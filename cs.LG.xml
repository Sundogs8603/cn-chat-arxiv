<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;HyperCATE&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#26435;&#37325;&#20849;&#20139;&#30340;&#26041;&#24335;&#23454;&#29616;&#31471;&#21040;&#31471;&#20449;&#24687;&#20849;&#20139;&#26469;&#35299;&#20915;&#29616;&#26377;CATE&#23398;&#20064;&#22120;&#20013;&#30340;&#26377;&#20559;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#22312;IHDP&#12289;ACIC-2016&#21644;Twins&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15984</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#30340;&#21160;&#24577;&#27835;&#30103;&#20449;&#24687;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Dynamic Inter-treatment Information Sharing for Heterogeneous Treatment Effects Estimation. (arXiv:2305.15984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;HyperCATE&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#26435;&#37325;&#20849;&#20139;&#30340;&#26041;&#24335;&#23454;&#29616;&#31471;&#21040;&#31471;&#20449;&#24687;&#20849;&#20139;&#26469;&#35299;&#20915;&#29616;&#26377;CATE&#23398;&#20064;&#22120;&#20013;&#30340;&#26377;&#20559;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#22312;IHDP&#12289;ACIC-2016&#21644;Twins&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#26377;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#23398;&#20064;&#32773;&#32570;&#20047;&#31471;&#21040;&#31471;&#27835;&#30103;&#20449;&#24687;&#20849;&#20139;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#24517;&#39035;&#23558;&#25968;&#25454;&#20998;&#21106;&#21040;&#28508;&#22312;&#32467;&#26524;&#20989;&#25968;&#20013;&#35757;&#32451;CATE&#23398;&#20064;&#22120;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20855;&#26377;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#30340;&#26377;&#20559;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;CATE&#23398;&#20064;&#22120;&#65292;&#20419;&#36827;&#27835;&#30103;&#32452;&#20043;&#38388;&#30340;&#21160;&#24577;&#31471;&#21040;&#31471;&#20449;&#24687;&#20849;&#20139;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#8220;&#36229;&#32593;&#32476;&#8221;&#30340;&#8220;&#36719;&#26435;&#37325;&#20849;&#20139;&#8221;&#65292;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#12289;&#26356;&#24555;&#35757;&#32451;&#21644;&#25913;&#36827;&#32467;&#26524;&#31561;&#20248;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;CATE&#23398;&#20064;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;HyperCATE&#8221;&#30340;&#26032;&#22411;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;CATE&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#24120;&#29992;CATE&#23398;&#20064;&#22120;&#30340;HyperCATE&#29256;&#26412;&#65292;&#24182;&#22312;IHDP&#12289;ACIC-2016&#21644;Twins&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing heterogeneous treatment effects learners, also known as conditional average treatment effects (CATE) learners, lack a general mechanism for end-to-end inter-treatment information sharing, and data have to be split among potential outcome functions to train CATE learners which can lead to biased estimates with limited observational datasets. To address this issue, we propose a novel deep learning-based framework to train CATE learners that facilitates dynamic end-to-end information sharing among treatment groups. The framework is based on \textit{soft weight sharing} of \textit{hypernetworks}, which offers advantages such as parameter efficiency, faster training, and improved results. The proposed framework complements existing CATE learners and introduces a new class of uncertainty-aware CATE learners that we refer to as \textit{HyperCATE}. We develop HyperCATE versions of commonly used CATE learners and evaluate them on IHDP, ACIC-2016, and Twins benchmarks. Our experimental 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#29992;&#20154;&#24037;&#27169;&#25311;&#30740;&#31350;&#37327;&#21270;&#35299;&#37322;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#22312;&#26377;&#29992;&#24615;&#65292;&#20197;&#35299;&#20915;&#35780;&#20272;&#35299;&#37322;&#36136;&#37327;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#20851;&#35299;&#37322;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15961</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#27169;&#25311;&#30740;&#31350;&#37327;&#21270;&#35299;&#37322;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#22312;&#26377;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Intrinsic Usefulness of Attributional Explanations for Graph Neural Networks with Artificial Simulatability Studies. (arXiv:2305.15961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#29992;&#20154;&#24037;&#27169;&#25311;&#30740;&#31350;&#37327;&#21270;&#35299;&#37322;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#22312;&#26377;&#29992;&#24615;&#65292;&#20197;&#35299;&#20915;&#35780;&#20272;&#35299;&#37322;&#36136;&#37327;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#20851;&#35299;&#37322;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#35780;&#20272;&#35299;&#37322;&#36136;&#37327;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#20026;&#19982;&#20154;&#31867;&#23454;&#39564;&#30456;&#20851;&#30340;&#39640;&#25104;&#26412;&#65292;&#36890;&#24120;&#20351;&#29992;&#21508;&#31181;&#20195;&#29702;&#24230;&#37327;&#26469;&#36817;&#20284;&#37327;&#21270;&#35299;&#37322;&#36136;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#24037;&#27169;&#25311;&#30740;&#31350;&#25193;&#23637;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#12290;&#25105;&#20204;&#20351;&#29992;&#25903;&#25345;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#25311;&#24615;&#30740;&#31350;&#65292;&#37327;&#21270;&#24402;&#22240;&#22270;&#35299;&#37322;&#30340;&#20869;&#22312;&#26377;&#29992;&#24615;&#65292;&#32780;&#19981;&#26159;&#26114;&#36149;&#30340;&#20154;&#31867;&#35797;&#39564;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;&#25152;&#25552;&#20986;&#30340;&#20998;&#26512;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#26368;&#26377;&#24847;&#20041;&#12290;&#25105;&#20204;&#36824;&#22312;&#23454;&#38469;&#30340;&#22270;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#30456;&#20851;&#35299;&#37322;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the increasing relevance of explainable AI, assessing the quality of explanations remains a challenging issue. Due to the high costs associated with human-subject experiments, various proxy metrics are often used to approximately quantify explanation quality. Generally, one possible interpretation of the quality of an explanation is its inherent value for teaching a related concept to a student. In this work, we extend artificial simulatability studies to the domain of graph neural networks. Instead of costly human trials, we use explanation-supervisable graph neural networks to perform simulatability studies to quantify the inherent usefulness of attributional graph explanations. We perform an extensive ablation study to investigate the conditions under which the proposed analyses are most meaningful. We additionally validate our methods applicability on real-world graph classification and regression datasets. We find that relevant explanations can significantly boost the samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#32593;&#32476;&#20013;&#30340;&#29420;&#31435;&#24490;&#29615;&#27169;&#22359;&#23398;&#20064;&#38271;&#31243;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#31454;&#20105;&#21147;&#65292;&#20026;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#25552;&#20379;&#20102;&#26032;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.15947</link><description>&lt;p&gt;
&#38271;&#20381;&#36182;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online learning of long range dependencies. (arXiv:2305.15947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#32593;&#32476;&#20013;&#30340;&#29420;&#31435;&#24490;&#29615;&#27169;&#22359;&#23398;&#20064;&#38271;&#31243;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#31454;&#20105;&#21147;&#65292;&#20026;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#25552;&#20379;&#20102;&#26032;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#26377;&#26395;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#38271;&#26399;&#20449;&#29992;&#20998;&#37197;&#65292;&#32780;&#24403;&#21069;&#31639;&#27861;&#35201;&#20040;&#19981;&#20855;&#22791;&#21487;&#25193;&#23637;&#24615;&#65292;&#35201;&#20040;&#26080;&#27861;&#23398;&#20064;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20165;&#23558;&#21333;&#27425;&#25512;&#26029;&#25152;&#38656;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#32763;&#20493;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#23618;&#32593;&#32476;&#20013;&#30340;&#29420;&#31435;&#24490;&#29615;&#27169;&#22359;&#21462;&#24471;&#20102;&#36825;&#20010;&#25104;&#26524;&#65292;&#36825;&#31181;&#32467;&#26500;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#38024;&#23545;&#21512;&#25104;&#35760;&#24518;&#38382;&#39064;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38271;&#31243;&#31454;&#25216;&#22330;&#22522;&#20934;&#22871;&#20214;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#65292;&#26641;&#31435;&#20102;&#22312;&#32447;&#23398;&#20064;&#30340;&#26032;&#26631;&#20934;&#12290;&#36825;&#31181;&#23398;&#20064;&#38271;&#31243;&#20381;&#36182;&#30340;&#33021;&#21147;&#20026;&#20102;&#35299;&#22823;&#33041;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#22312;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#24320;&#36767;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online learning holds the promise of enabling efficient long-term credit assignment in recurrent neural networks. However, current algorithms fall short of offline backpropagation by either not being scalable or failing to learn long-range dependencies. Here we present a high-performance online learning algorithm that merely doubles the memory and computational requirements of a single inference pass. We achieve this by leveraging independent recurrent modules in multi-layer networks, an architectural motif that has recently been shown to be particularly powerful. Experiments on synthetic memory problems and on the challenging long-range arena benchmark suite reveal that our algorithm performs competitively, establishing a new standard for what can be achieved through online learning. This ability to learn long-range dependencies offers a new perspective on learning in the brain and opens a promising avenue in neuromorphic computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15944</link><description>&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#23558;&#24744;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits. (arXiv:2305.15944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#25104;&#21151;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#21487;&#29992;&#20316;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#23558;&#20854;&#37325;&#26032;&#35299;&#37322;&#25104;&#20026;&#30005;&#36335;&#24418;&#24335;--&#36825;&#26159;&#19968;&#31181;&#20801;&#35768;&#26377;&#25928;&#36793;&#38469;&#21270;&#30340;&#32422;&#26463;&#35745;&#31639;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26041;&#27861;&#26469;&#33719;&#24471;&#26377;&#25928;&#30340;&#29983;&#25104;&#30005;&#36335;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#28608;&#27963;&#38480;&#21046;&#20026;&#38750;&#36127;&#25968;&#65292;&#21478;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#36755;&#20986;&#24179;&#26041;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#19981;&#20250;&#24433;&#21709;&#21040;&#39044;&#27979;&#33410;&#28857;&#36830;&#36793;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30005;&#36335;&#26694;&#26550;&#20351;&#24471;MLE&#30340;&#31934;&#30830;&#23398;&#20064;&#12289;&#26032;&#19977;&#20803;&#32452;&#30340;&#26377;&#25928;&#25277;&#26679;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#24471;&#20197;&#28385;&#36275;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25317;&#26377;&#25968;&#30334;&#19975;&#20010;&#23454;&#20307;&#30340;&#22270;&#19978;&#27604;&#21407;&#22987;&#30340;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some of the most successful knowledge graph embedding (KGE) models for link prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits -- constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#39532;&#23572;&#31185;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#20984;&#21644;&#24378;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#22810;&#23618;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#26696;&#20197;&#33719;&#24471;&#26368;&#20248;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#28040;&#38500;&#20102;&#20197;&#21069;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#26465;&#20214;&#12290;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#23545;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#25193;&#23637;&#26159;&#21407;&#21019;&#24615;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.15938</link><description>&lt;p&gt;
&#20855;&#26377;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#19968;&#38454;&#26041;&#27861;&#65306;&#20174;&#21152;&#36895;&#21040;&#21464;&#20998;&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
First Order Methods with Markovian Noise: from Acceleration to Variational Inequalities. (arXiv:2305.15938v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#39532;&#23572;&#31185;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#20984;&#21644;&#24378;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#22810;&#23618;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#26696;&#20197;&#33719;&#24471;&#26368;&#20248;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#28040;&#38500;&#20102;&#20197;&#21069;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#26465;&#20214;&#12290;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#23545;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#25193;&#23637;&#26159;&#21407;&#21019;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#28041;&#21450;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#29702;&#35770;&#20998;&#26512;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28085;&#30422;&#20102;&#38750;&#20984;&#21644;&#24378;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#23454;&#29616;&#19968;&#20010;&#20381;&#36182;&#20110;&#24213;&#23618;&#22122;&#22768;&#24207;&#21015;&#28151;&#21512;&#26102;&#38388;&#30340;&#26368;&#20248;(&#32447;&#24615;)&#20851;&#31995;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22810;&#23618;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#20801;&#35768;&#25105;&#20204;&#28040;&#38500;&#20197;&#21069;&#20851;&#20110;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#26465;&#20214;&#65292;&#20363;&#22914;&#38656;&#35201;&#26377;&#30028;&#22495;&#21644;&#22343;&#21248;&#26377;&#30028;&#38543;&#26426;&#26799;&#24230;&#12290;&#25105;&#20204;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#23545;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#25193;&#23637;&#26159;&#21407;&#21019;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21305;&#37197;&#24378;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#29702;&#35770;&#26368;&#20248;&#35299;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into stochastic optimization problems that involve Markovian noise. We present a unified approach for the theoretical analysis of first-order gradient methods for stochastic optimization and variational inequalities. Our approach covers scenarios for both non-convex and strongly convex minimization problems. To achieve an optimal (linear) dependence on the mixing time of the underlying noise sequence, we use the randomized batching scheme, which is based on the multilevel Monte Carlo method. Moreover, our technique allows us to eliminate the limiting assumptions of previous research on Markov noise, such as the need for a bounded domain and uniformly bounded stochastic gradients. Our extension to variational inequalities under Markovian noise is original. Additionally, we provide lower bounds that match the oracle complexity of our method in the case of strongly convex optimization problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#20165;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15936</link><description>&lt;p&gt;
&#20174;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs
&lt;/p&gt;
&lt;p&gt;
Learning DAGs from Data with Few Root Causes. (arXiv:2305.15936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15936
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#20165;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32447;&#24615;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEM)&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#32447;&#24615;SEM&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#32447;&#24615;&#21464;&#25442;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#19968;&#20010;&#30001;&#19982;&#33410;&#28857;&#20851;&#32852;&#30340;&#38543;&#26426;&#20540;&#26681;&#22240;(&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;)&#30340;&#31264;&#23494;&#36755;&#20837;&#21521;&#37327;&#35745;&#31639;&#25968;&#25454;&#12290;&#25105;&#20204;&#32771;&#34385;&#20165;&#23384;&#22312;&#20960;&#20010;&#26681;&#22240;(&#36817;&#20284;)&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#25968;&#25454;&#30340;&#27979;&#37327;&#20013;&#24341;&#20837;&#22122;&#22768;&#12290;&#20174;&#30452;&#35273;&#19978;&#35762;&#65292;&#36825;&#24847;&#21619;&#30528;DAG&#25968;&#25454;&#26159;&#30001;&#23569;&#25968;&#25968;&#25454;&#29983;&#25104;&#20107;&#20214;&#20135;&#29983;&#30340;&#65292;&#20854;&#25928;&#26524;&#36890;&#36807;DAG&#20256;&#25773;&#12290;&#25105;&#20204;&#22312;&#36825;&#31181;&#26032;&#35774;&#32622;&#20013;&#35777;&#26126;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#34920;&#26126;&#30495;&#27491;&#30340;DAG&#26159;&#26681;&#22240;&#21521;&#37327;L0-&#33539;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#21270;&#32773;&#12290;&#23545;&#20110;&#20855;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#65292;&#26377;&#21644;&#27809;&#26377;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;DAG&#23398;&#20064;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel perspective and algorithm for learning directed acyclic graphs (DAGs) from data generated by a linear structural equation model (SEM). First, we show that a linear SEM can be viewed as a linear transform that, in prior work, computes the data from a dense input vector of random valued root causes (as we will call them) associated with the nodes. Instead, we consider the case of (approximately) few root causes and also introduce noise in the measurement of the data. Intuitively, this means that the DAG data is produced by few data-generating events whose effect percolates through the DAG. We prove identifiability in this new setting and show that the true DAG is the global minimizer of the $L^0$-norm of the vector of root causes. For data with few root causes, with and without noise, we show superior performance compared to prior DAG learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#30340;&#31070;&#32463;&#36807;&#31243;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#20197;&#21450;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#30340;&#20256;&#32479;Meta-BO&#26041;&#27861;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.15930</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#31070;&#32463;&#36807;&#31243;&#30340;&#31471;&#21040;&#31471;Meta-Bayesian&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes. (arXiv:2305.15930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#30340;&#31070;&#32463;&#36807;&#31243;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#20197;&#21450;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#30340;&#20256;&#32479;Meta-BO&#26041;&#27861;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;Meta-Bayesian optimization&#65292;Meta-BO&#65289;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22320;&#29420;&#31435;&#20803;&#23398;&#20064;&#36807;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#65292;&#20294;&#26159;&#21516;&#26102;&#35757;&#32451;&#36825;&#20004;&#20010;&#32452;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;Meta-BO&#26694;&#26550;&#65292;&#36890;&#36807;Transformer&#20307;&#31995;&#32467;&#26500;&#23558;&#31070;&#32463;&#36807;&#31243;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20351;&#36825;&#31181;&#31471;&#21040;&#31471;&#26694;&#26550;&#20855;&#26377;&#22788;&#29702;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of Bayesian optimisation by leveraging data from related tasks. While previous methods successfully meta-learn either a surrogate model or an acquisition function independently, joint training of both components remains an open challenge. This paper proposes the first end-to-end differentiable meta-BO framework that generalises neural processes to learn acquisition functions via transformer architectures. We enable this end-to-end framework with reinforcement learning (RL) to tackle the lack of labelled acquisition data. Early on, we notice that training transformer-based neural processes from scratch with RL is challenging due to insufficient supervision, especially when rewards are sparse. We formalise this claim with a combinatorial analysis showing that the widely used notion of regret as a reward signal exhibits a logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we augment the RL 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#35270;&#22270;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36827;&#34892;&#25805;&#20316;&#24182;&#34920;&#29616;&#20986;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15927</link><description>&lt;p&gt;
&#29992;&#26368;&#20248;&#20256;&#36755;&#23398;&#20064;&#26377;&#21521;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Directed Graphical Models with Optimal Transport. (arXiv:2305.15927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15927
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#35270;&#22270;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36827;&#34892;&#25805;&#20316;&#24182;&#34920;&#29616;&#20986;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#27010;&#29575;&#26377;&#21521;&#22270;&#27169;&#22411;&#30340;&#21442;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#23384;&#22312;&#28508;&#22312;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#27809;&#26377;&#20851;&#20110;&#32467;&#26500;&#20381;&#36182;&#24615;&#25110;&#27169;&#22411;&#31867;&#30340;&#36827;&#19968;&#27493;&#20551;&#35774;&#65292;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#37117;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#23398;&#20064;&#26041;&#27861;&#22522;&#26412;&#19978;&#26159;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#37324;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#20010;&#26032;&#35270;&#22270;&#12290;&#36825;&#20010;&#35266;&#28857;&#25480;&#26435;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36816;&#20316;&#65292;&#32780;&#19981;&#20250;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#20570;&#20986;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#25110;&#35785;&#35832;&#20110;&#40657;&#31665;&#21464;&#20998;&#36817;&#20284;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25903;&#25345;&#23427;&#36890;&#36807;&#24191;&#27867;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#24674;&#22797;&#22522;&#20934;&#21442;&#25968;&#65292;&#32780;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#20063;&#34920;&#29616;&#24471;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the parameters of a probabilistic directed graphical model from incomplete data remains a long-standing challenge. This is because, in the presence of latent variables, both the likelihood function and posterior distribution are intractable without further assumptions about structural dependencies or model classes. While existing learning methods are fundamentally based on likelihood maximization, here we offer a new view of the parameter learning problem through the lens of optimal transport. This perspective licenses a framework that operates on many directed graphs without making unrealistic assumptions on the posterior over the latent variables or resorting to black-box variational approximations. We develop a theoretical framework and support it with extensive empirical evidence demonstrating the flexibility and versatility of our approach. Across experiments, we show that not only can our method recover the ground-truth parameters but it also performs competitively on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#23454;&#29616;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2305.15925</link><description>&lt;p&gt;
&#20851;&#20110;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability of Markov Switching Models. (arXiv:2305.15925v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#23454;&#29616;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#22240;&#20854;&#22312;&#21487;&#35299;&#37322;&#24615;&#25110;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20316;&#20026;&#23558;&#26368;&#36817;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#24207;&#21015;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#31532;&#19968;&#27493;&#30340;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;&#25105;&#20204;&#22312;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#25552;&#20986;&#20102;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifiability of latent variable models has recently gained interest in terms of its applications to interpretability or out of distribution generalisation. In this work, we study identifiability of Markov Switching Models as a first step towards extending recent results to sequential latent variable models. We present identifiability conditions within first-order Markov dependency structures, and parametrise the transition distribution via non-linear Gaussians. Our experiments showcase the applicability of our approach for regime-dependent causal discovery and high-dimensional time series segmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#24577;&#20018;&#34892;&#20998;&#31163;&#26694;&#26550;&#65292;&#22522;&#20110;&#23545;&#27604;&#20272;&#35745;&#36827;&#34892;&#33258;&#30417;&#30563;&#65292;&#20855;&#26377;&#26080;&#22806;&#37096;&#20449;&#21495;&#12289;&#24120;&#29992;&#25209;&#37327;&#22823;&#23567;&#12289;&#26679;&#26412;&#38598;&#33258;&#36523;&#28508;&#22312;&#31354;&#38388;&#31561;&#29305;&#28857;&#65292;&#21487;&#20197;&#35299;&#20915;&#26080;&#30417;&#30563;&#30340;&#20998;&#31163;&#23398;&#20064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#22788;&#29702;&#35821;&#20041;&#19978;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#25968;&#25454;&#35270;&#22270;&#65292;&#24182;&#22312;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15924</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#20272;&#35745;&#36827;&#34892;&#26080;&#27169;&#24577;&#20018;&#34892;&#20998;&#31163;&#65306;&#26679;&#26412;&#21644;&#39044;&#27979;&#28508;&#22312;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Sample and Predict Your Latent: Modality-free Sequential Disentanglement via Contrastive Estimation. (arXiv:2305.15924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#24577;&#20018;&#34892;&#20998;&#31163;&#26694;&#26550;&#65292;&#22522;&#20110;&#23545;&#27604;&#20272;&#35745;&#36827;&#34892;&#33258;&#30417;&#30563;&#65292;&#20855;&#26377;&#26080;&#22806;&#37096;&#20449;&#21495;&#12289;&#24120;&#29992;&#25209;&#37327;&#22823;&#23567;&#12289;&#26679;&#26412;&#38598;&#33258;&#36523;&#28508;&#22312;&#31354;&#38388;&#31561;&#29305;&#28857;&#65292;&#21487;&#20197;&#35299;&#20915;&#26080;&#30417;&#30563;&#30340;&#20998;&#31163;&#23398;&#20064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#22788;&#29702;&#35821;&#20041;&#19978;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#25968;&#25454;&#35270;&#22270;&#65292;&#24182;&#22312;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#20998;&#31163;&#23398;&#20064;&#19968;&#30452;&#26159;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#38590;&#39064;&#12290;&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#25216;&#26415;&#22312;&#26102;&#38388;&#20381;&#36182;&#22411;&#25968;&#25454;&#30340;&#39034;&#24207;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21518;&#32773;&#30340;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#27169;&#24577;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#38543;&#26426;&#25277;&#26679;&#65292;&#25110;&#32773;&#35299;&#20915;&#36741;&#21161;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20174;&#22522;&#30784;&#21464;&#20998;&#27169;&#22411;&#29983;&#25104;&#12289;&#37319;&#26679;&#21644;&#27604;&#36739;&#32463;&#39564;&#20998;&#24067;&#26469;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#12290;&#19982;&#29616;&#26377;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#27604;&#20272;&#35745;&#30340;&#33258;&#30417;&#30563;&#39034;&#24207;&#20998;&#31163;&#26694;&#26550;&#65292;&#27809;&#26377;&#22806;&#37096;&#20449;&#21495;&#65292;&#21516;&#26102;&#20351;&#29992;&#26222;&#36890;&#25209;&#37327;&#22823;&#23567;&#21644;&#26469;&#33258;&#28508;&#22312;&#31354;&#38388;&#26412;&#36523;&#30340;&#26679;&#26412;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#12289;&#39640;&#25928;&#30340;&#12289;&#26131;&#20110;&#32534;&#30721;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#35821;&#20041;&#19978;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#25968;&#25454;&#35270;&#22270;&#12290;&#25105;&#20204;&#22312;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21576;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised disentanglement is a long-standing challenge in representation learning. Recently, self-supervised techniques achieved impressive results in the sequential setting, where data is time-dependent. However, the latter methods employ modality-based data augmentations and random sampling or solve auxiliary tasks. In this work, we propose to avoid that by generating, sampling, and comparing empirical distributions from the underlying variational model. Unlike existing work, we introduce a self-supervised sequential disentanglement framework based on contrastive estimation with no external signals, while using common batch sizes and samples from the latent space itself. In practice, we propose a unified, efficient, and easy-to-code sampling strategy for semantically similar and dissimilar views of the data. We evaluate our approach on video, audio, and time series benchmarks. Our method presents state-of-the-art results in comparison to existing techniques. The code is available 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;GAN&#26469;&#23398;&#20064;&#19968;&#20010;&#21407;&#22411;&#26230;&#26684;&#19978;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21512;&#36866;&#30340;&#22810;&#27169;&#22411;&#31243;&#24207;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31934;&#24230;&#12290;GAN&#20284;&#20046;&#26159;&#22788;&#29702;&#22797;&#26434;&#32479;&#35745;&#21160;&#21147;&#23398;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.15920</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#22411;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#23398;&#20064;&#21644;&#31934;&#30830;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning and accurate generation of stochastic dynamics based on multi-model Generative Adversarial Networks. (arXiv:2305.15920v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;GAN&#26469;&#23398;&#20064;&#19968;&#20010;&#21407;&#22411;&#26230;&#26684;&#19978;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21512;&#36866;&#30340;&#22810;&#27169;&#22411;&#31243;&#24207;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31934;&#24230;&#12290;GAN&#20284;&#20046;&#26159;&#22788;&#29702;&#22797;&#26434;&#32479;&#35745;&#21160;&#21147;&#23398;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#24050;&#32463;&#22312;&#36828;&#31163;&#29289;&#29702;&#39046;&#22495;&#65292;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20351;&#29992;GAN&#26469;&#23398;&#20064;&#19968;&#20010;&#21407;&#22411;&#26230;&#26684;&#19978;&#30340;&#38543;&#26426;&#36807;&#31243;&#12290;&#36890;&#36807;&#21512;&#29702;&#22320;&#21521;&#21407;&#22987;&#25968;&#25454;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#25439;&#22833;&#20989;&#25968;&#30340;&#20540;&#24102;&#21040;&#20102;&#23427;&#20204;&#30340;&#29702;&#24819;&#20540;&#38468;&#36817;&#12290;&#28982;&#32780;&#65292;&#20687;&#23545;&#25239;&#24615;&#26041;&#27861;&#19968;&#26679;&#65292;&#38663;&#33633;&#20173;&#28982;&#23384;&#22312;&#12290;&#36825;&#20250;&#30772;&#22351;&#27169;&#22411;&#36873;&#25321;&#21644;&#29983;&#25104;&#36712;&#36857;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#19968;&#31181;&#21512;&#36866;&#30340;&#22810;&#27169;&#22411;&#31243;&#24207;&#65292;&#22312;&#27599;&#19968;&#27493;&#38543;&#26426;&#36873;&#25321;&#29983;&#25104;&#22120;&#25512;&#36827;&#38543;&#26426;&#36712;&#36857;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31934;&#24230;&#12290;&#22522;&#20110;&#20197;&#19978;&#21457;&#29616;&#65292;GAN&#20284;&#20046;&#26159;&#22788;&#29702;&#22797;&#26434;&#32479;&#35745;&#21160;&#21147;&#23398;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have shown immense potential in fields far from physics, such as in text and image generation. Here we use GANs to learn a prototypical stochastic process on a lattice. By suitably adding noise to the original data we succeed in bringing both the Generator and the Discriminator loss functions close to their ideal value. However, as typical for adversarial approaches, oscillations persist. This undermines model selection and the quality of the generated trajectory. We demonstrate that a suitable multi-model procedure where stochastic trajectories are advanced at each step upon randomly selecting a Generator leads to a remarkable increase in accuracy. Based on the reported findings GANs appears as a promising tool to tackle complex statistical dynamics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15912</link><description>&lt;p&gt;
&#25913;&#36827;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#30340;&#31070;&#32463;&#29305;&#24449;&#28608;&#27963;&#20540;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#21333;&#20010;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#12290;&#25105;&#20204;&#23558;ReLU&#21333;&#20803;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#23545;&#24212;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#31216;&#20026;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#38598;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29305;&#24449;&#28608;&#27963;&#38598;&#19982;ReLU&#32593;&#32476;&#20013;&#23398;&#20064;&#29305;&#24449;&#20043;&#38388;&#30340;&#26126;&#30830;&#32852;&#31995;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#25216;&#26415;&#22914;&#20309;&#35268;&#33539;&#21270;&#21644;&#31283;&#23450;SGD&#20248;&#21270;&#12290;&#21033;&#29992;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;ReLU&#32593;&#32476;&#20197;&#25913;&#36827;&#29305;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#20854;&#26377;&#29992;&#24615;&#65292;&#20351;&#29992;&#20102;&#19981;&#37027;&#20040;&#31934;&#24515;&#36873;&#25321;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#21644;&#26356;&#22823;&#30340;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26356;&#22909;&#30340;&#20248;&#21270;&#31283;&#23450;&#24615;&#65292;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#20004;&#20010;&#23436;&#20840;&#30456;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#36755;&#20986;&#24046;&#24322;&#21576;&#29616;&#20986;&#8220;&#21452;&#37325;&#38477;&#32500;&#8221;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26089;&#20572;&#27490;&#20934;&#21017;&#19982;&#25968;&#25454;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15907</link><description>&lt;p&gt;
&#21452;&#37325;&#38477;&#32500;&#29616;&#35937;&#30340;&#20559;&#24046;&#65306;&#19968;&#20010;&#20219;&#21153;&#12289;&#25968;&#25454;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Double Descent of Discrepancy: A Task-, Data-, and Model-Agnostic Phenomenon. (arXiv:2305.15907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#20004;&#20010;&#23436;&#20840;&#30456;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#36755;&#20986;&#24046;&#24322;&#21576;&#29616;&#20986;&#8220;&#21452;&#37325;&#38477;&#32500;&#8221;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26089;&#20572;&#27490;&#20934;&#21017;&#19982;&#25968;&#25454;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#20010;&#23436;&#20840;&#30456;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;&#21363;&#20351;&#29992;&#30456;&#21516;&#31639;&#27861;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#24182;&#20855;&#26377;&#30456;&#21516;&#26550;&#26500;&#20294;&#26159;&#21021;&#22987;&#21270;&#19981;&#21516;&#65289;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#36755;&#20986;&#24046;&#24322;&#21576;&#29616;&#20986;&#8220;&#21452;&#37325;&#38477;&#32500;&#8221;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#32593;&#32476;&#26550;&#26500;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#29616;&#35937;&#26222;&#36941;&#23384;&#22312;&#12290;&#21033;&#29992;&#36825;&#20010;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26089;&#20572;&#27490;&#20934;&#21017;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#29616;&#35937;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#21463;&#30410;&#20110;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we studied two identically-trained neural networks (i.e. networks with the same architecture, trained on the same dataset using the same algorithm, but with different initialization) and found that their outputs discrepancy on the training dataset exhibits a "double descent" phenomenon. We demonstrated through extensive experiments across various tasks, datasets, and network architectures that this phenomenon is prevalent. Leveraging this phenomenon, we proposed a new early stopping criterion and developed a new method for data quality assessment. Our results show that a phenomenon-driven approach can benefit deep learning research both in theoretical understanding and practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;MTCue&#65292;&#23427;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#35299;&#37322;&#20026;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#21487;&#36716;&#31227;&#24615;&#24182;&#23398;&#20250;&#20102;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#21033;&#29992;&#39069;&#22806;&#30340;&#25991;&#26412;&#23646;&#24615;&#65288;&#22914;&#31036;&#35980;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#21464;&#37327;&#65289;&#30340;&#25511;&#21046;&#12290;&#22312;&#22235;&#20010;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#65292;MTCue&#30340;&#32763;&#35793;&#36136;&#37327;&#26174;&#30528;&#25552;&#39640;&#65292;BLEU&#65288;+0.88&#65289;&#21644;Comet&#65288;+1.58&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.15904</link><description>&lt;p&gt;
MTCue&#65306;&#21033;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#26410;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#23398;&#20064;&#38646;&#26679;&#26412;&#25511;&#21046;&#39069;&#22806;&#25991;&#26412;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation. (arXiv:2305.15904v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;MTCue&#65292;&#23427;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#35299;&#37322;&#20026;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#21487;&#36716;&#31227;&#24615;&#24182;&#23398;&#20250;&#20102;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#21033;&#29992;&#39069;&#22806;&#30340;&#25991;&#26412;&#23646;&#24615;&#65288;&#22914;&#31036;&#35980;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#21464;&#37327;&#65289;&#30340;&#25511;&#21046;&#12290;&#22312;&#22235;&#20010;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#65292;MTCue&#30340;&#32763;&#35793;&#36136;&#37327;&#26174;&#30528;&#25552;&#39640;&#65292;BLEU&#65288;+0.88&#65289;&#21644;Comet&#65288;+1.58&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#21033;&#29992;&#25991;&#26412;&#20869;&#21644;&#25991;&#26412;&#22806;&#30340;&#19978;&#19979;&#25991;&#20173;&#26159;&#26426;&#22120;&#21644;&#20154;&#31867;&#32763;&#35793;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#36317;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22312;&#32763;&#35793;&#20013;&#25552;&#20379;&#20010;&#21035;&#23450;&#20041;&#33391;&#22909;&#31867;&#22411;&#30340;&#19978;&#19979;&#25991;&#65292;&#22914;&#21608;&#22260;&#30340;&#25991;&#26412;&#25110;&#31163;&#25955;&#30340;&#22806;&#37096;&#21464;&#37327;&#65288;&#22914;&#35828;&#35805;&#32773;&#30340;&#24615;&#21035;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MTCue&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26694;&#26550;&#65292;&#23427;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#65288;&#21253;&#25324;&#31163;&#25955;&#21464;&#37327;&#65289;&#35299;&#37322;&#20026;&#25991;&#26412;&#12290; MTCue&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#25277;&#35937;&#34920;&#36798;&#65292;&#21363;&#20351;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#35774;&#32622;&#21644;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#65292;&#20063;&#33021;&#23454;&#29616;&#21487;&#36716;&#31227;&#24615;&#24182;&#21033;&#29992;&#31867;&#20284;&#23646;&#24615;&#12290;&#25105;&#20204;&#19981;&#26029;&#35780;&#20272;MTCue&#22312;&#22235;&#20010;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#25991;&#26723;&#21644;&#20803;&#25968;&#25454;&#19978;&#19979;&#25991;&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#35805;&#39046;&#22495;&#12290;&#19982;&#21442;&#25968;&#21305;&#37197;&#30340;&#38750;&#19978;&#19979;&#25991;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#25552;&#39640;&#65292;BLEU&#65288;+0.88&#65289;&#21644;Comet&#65288;+1.58&#65289;&#12290;&#27492;&#22806;&#65292;MTCue&#25104;&#21151;&#22320;&#23398;&#20250;&#20102;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#21033;&#29992;&#39069;&#22806;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#35832;&#22914;&#31036;&#35980;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#21464;&#37327;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external variables like the speaker's gender. This work introduces MTCue, a novel neural machine translation (NMT) framework that interprets all context (including discrete variables) as text. MTCue learns an abstract representation of context, enabling transferability across different data settings and leveraging similar attributes in low-resource scenarios. With a focus on a dialogue domain with access to document and metadata context, we extensively evaluate MTCue in four language pairs in both translation directions. Our framework demonstrates significant improvements in translation quality over a parameter-matched non-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58). Moreover, MTCu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Feature Heterogeneity Distance(FHD)&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#27169;&#24335;Contrastive Convergence for Domain Generalization (CCDG) &#26469;&#23547;&#25214;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#27867;&#21270;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#21644;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2305.15889</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#24322;&#36136;&#24615;&#37327;&#21270;&#21644;&#23545;&#27604;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization. (arXiv:2305.15889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Feature Heterogeneity Distance(FHD)&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#27169;&#24335;Contrastive Convergence for Domain Generalization (CCDG) &#26469;&#23547;&#25214;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#27867;&#21270;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#21644;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;(DG)&#26159;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#20960;&#20010;&#28304;&#22495;&#35757;&#32451;&#20986;&#23545;&#26410;&#35265;&#36807;&#30446;&#26631;&#22495;&#36827;&#34892;&#26377;&#25928;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#39046;&#22495;&#26631;&#31614;-&#21363;&#27599;&#20010;&#25968;&#25454;&#28857;&#26469;&#33258;&#21738;&#20010;&#22495;&#33258;&#28982;&#23384;&#22312;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;DG&#31639;&#27861;&#23558;&#23427;&#20204;&#20316;&#20026;&#19968;&#31181;&#30417;&#30563;&#20449;&#24687;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39046;&#22495;&#20043;&#38388;&#32570;&#20047;&#24322;&#36136;&#24615;&#65292;&#21363;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21407;&#22987;&#30340;&#22495;&#26631;&#31614;&#21487;&#33021;&#19981;&#26159;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Feature Heterogeneity Distance(FHD)&#26469;&#34913;&#37327;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#27169;&#24335;CCDG&#65292;&#29992;&#20110;&#23547;&#25214;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#27867;&#21270;&#12290;&#22823;&#37327;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;FHD&#24230;&#37327;&#26631;&#20934;&#21644;CCDG&#27169;&#24335;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#21644;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) is a prevalent problem in real-world applications, which aims to train well-generalized models for unseen target domains by utilizing several source domains. Since domain labels, i.e., which domain each data point is sampled from, naturally exist, most DG algorithms treat them as a kind of supervision information to improve the generalization performance. However, the original domain labels may not be the optimal supervision signal due to the lack of domain heterogeneity, i.e., the diversity among domains. For example, a sample in one domain may be closer to another domain, its original label thus can be the noise to disturb the generalization learning. Although some methods try to solve it by re-dividing domains and applying the newly generated dividing pattern, the pattern they choose may not be the most heterogeneous due to the lack of the metric for heterogeneity. In this paper, we point out that domain heterogeneity mainly lies in variant features under 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31616;&#21270;&#24314;&#27169;&#26041;&#27861;GAROM&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#21442;&#25968;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15881</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#31616;&#21270;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Reduced Order Modelling. (arXiv:2305.15881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31616;&#21270;&#24314;&#27169;&#26041;&#27861;GAROM&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#21442;&#25968;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#31616;&#21270;&#24314;&#27169;&#26041;&#27861;&#8212;&#8212;GAROM&#12290;GAN&#22312;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;&#31616;&#21270;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;GAN&#21644;ROM&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#21442;&#25968;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#25105;&#20204;&#23558;&#37492;&#21035;&#22120;&#32593;&#32476;&#24314;&#27169;&#20026;&#33258;&#32534;&#30721;&#22120;&#65292;&#25552;&#21462;&#36755;&#20837;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#24182;&#23558;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20316;&#20026;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#32593;&#32476;&#30340;&#36755;&#20837;&#26465;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#25512;&#26029;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present GAROM, a new approach for reduced order modelling (ROM) based on generative adversarial networks (GANs). GANs have the potential to learn data distribution and generate more realistic data. While widely applied in many areas of deep learning, little research is done on their application for ROM, i.e. approximating a high-fidelity model with a simpler one. In this work, we combine the GAN and ROM framework, by introducing a data-driven generative adversarial model able to learn solutions to parametric differential equations. The latter is achieved by modelling the discriminator network as an autoencoder, extracting relevant features of the input, and applying a conditioning mechanism to the generator and discriminator networks specifying the differential equation parameters. We show how to apply our methodology for inference, provide experimental evidence of the model generalisation, and perform a convergence study of the method.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.15878</link><description>&lt;p&gt;
LFTK: &#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#30340;&#25163;&#24037;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
LFTK: Handcrafted Features in Computational Linguistics. (arXiv:2305.15878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#24050;&#32463;&#37492;&#23450;&#20986;&#20102;&#19968;&#32452;&#20016;&#23500;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#24110;&#21161;&#21508;&#31181;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#36825;&#20123;&#29305;&#24449;&#25968;&#37327;&#24222;&#22823;&#65292;&#22240;&#27492;&#38590;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#21644;&#21033;&#29992;&#29616;&#26377;&#30340;&#25163;&#24037;&#29305;&#24449;&#12290;&#21152;&#19978;&#22312;&#30740;&#31350;&#24037;&#20316;&#20013;&#23454;&#29616;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#36824;&#19981;&#23384;&#22312;&#20998;&#31867;&#26041;&#26696;&#25110;&#32773;&#32479;&#19968;&#25509;&#21463;&#30340;&#29305;&#24449;&#21517;&#31216;&#65292;&#36825;&#36896;&#25104;&#20102;&#19981;&#24517;&#35201;&#30340;&#28151;&#20081;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25163;&#24037;&#29305;&#24449;&#25552;&#21462;&#24211;&#37117;&#19981;&#26159;&#24320;&#28304;&#30340;&#65292;&#25110;&#32773;&#27809;&#26377;&#24471;&#21040;&#31215;&#26497;&#30340;&#32500;&#25252;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#38656;&#35201;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#36825;&#26679;&#30340;&#25552;&#21462;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#36807;&#21435;&#30340;&#25991;&#29486;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#65292;&#24182;&#25253;&#21578;&#20102;&#27599;&#20010;&#29305;&#24449;&#30340;&#28508;&#22312;&#29992;&#36884;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past research has identified a rich set of handcrafted linguistic features that can potentially assist various tasks. However, their extensive number makes it difficult to effectively select and utilize existing handcrafted features. Coupled with the problem of inconsistent implementation across research works, there has been no categorization scheme or generally-accepted feature names. This creates unwanted confusion. Also, most existing handcrafted feature extraction libraries are not open-source or not actively maintained. As a result, a researcher often has to build such an extraction system from the ground up.  We collect and categorize more than 220 popular handcrafted features grounded on past literature. Then, we conduct a correlation analysis study on several task-specific datasets and report the potential use cases of each feature. Lastly, we devise a multilingual handcrafted linguistic feature extraction system in a systematically expandable manner. We open-source our system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#23398;&#20064;&#20013;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;(IPS)&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#21487;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#35777;&#26126;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;IPS&#12290;</title><link>http://arxiv.org/abs/2305.15877</link><description>&lt;p&gt;
&#25351;&#25968;&#24179;&#28369;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exponential Smoothing for Off-Policy Learning. (arXiv:2305.15877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#23398;&#20064;&#20013;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;(IPS)&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#21487;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#35777;&#26126;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;IPS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#26469;&#23547;&#25214;&#25913;&#36827;&#30340;&#31574;&#30053;&#65292;&#36890;&#24120;&#20351;&#29992;&#35760;&#24405;&#30340;&#36172;&#21338;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;IPS&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#12290;&#35813;&#30028;&#38480;&#26159;&#21487;&#22788;&#29702;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#35777;&#26126;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23398;&#20064;&#20219;&#21153;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#21033;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#30028;&#38480;&#36866;&#29992;&#20110;&#26631;&#20934;IPS&#65292;&#22240;&#27492;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#20851;&#20110;&#20309;&#26102;&#27491;&#21017;&#21270;IPS&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#21363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#12290;&#36825;&#19982;&#22312;&#23454;&#36341;&#20013;&#65292;&#21098;&#36753;IPS&#24120;&#24120;&#27604;OPL&#20013;&#30340;&#26631;&#20934;IPS&#34920;&#29616;&#26356;&#22909;&#30340;&#20449;&#24565;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning (OPL) aims at finding improved policies from logged bandit data, often by minimizing the inverse propensity scoring (IPS) estimator of the risk. In this work, we investigate a smooth regularization for IPS, for which we derive a two-sided PAC-Bayes generalization bound. The bound is tractable, scalable, interpretable and provides learning certificates. In particular, it is also valid for standard IPS without making the assumption that the importance weights are bounded. We demonstrate the relevance of our approach and its favorable performance through a set of learning tasks. Since our bound holds for standard IPS, we are able to provide insight into when regularizing IPS is useful. Namely, we identify cases where regularization might not be needed. This goes against the belief that, in practice, clipped IPS often enjoys favorable performance than standard IPS in OPL.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#39318;&#20010;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#35770;&#65288;&#22914;ABC&#21644;NPE&#65289;&#20013;&#30001;&#20110;&#27169;&#22411;&#38169;&#35823;&#24341;&#36215;&#30340;&#19981;&#21487;&#38752;&#25512;&#35770;&#12290;&#36890;&#36807;&#32422;&#26463;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#19982;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#32479;&#35745;&#37327;&#26469;&#38450;&#27490;&#19981;&#21487;&#38752;&#25512;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26412;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15871</link><description>&lt;p&gt;
&#23398;&#20064;&#40065;&#26834;&#32479;&#35745;&#29992;&#20110;&#27169;&#22411;&#38169;&#35823;&#24773;&#20917;&#19979;&#30340;&#22522;&#20110;&#27169;&#25311;&#25512;&#35770;
&lt;/p&gt;
&lt;p&gt;
Learning Robust Statistics for Simulation-based Inference under Model Misspecification. (arXiv:2305.15871v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#39318;&#20010;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#35770;&#65288;&#22914;ABC&#21644;NPE&#65289;&#20013;&#30001;&#20110;&#27169;&#22411;&#38169;&#35823;&#24341;&#36215;&#30340;&#19981;&#21487;&#38752;&#25512;&#35770;&#12290;&#36890;&#36807;&#32422;&#26463;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#19982;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#32479;&#35745;&#37327;&#26469;&#38450;&#27490;&#19981;&#21487;&#38752;&#25512;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26412;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#35770;&#26041;&#27861;&#65288;&#22914;&#36817;&#20284;&#36125;&#21494;&#26031;&#35745;&#31639;&#65288;ABC&#65289;&#65292;&#21512;&#25104;&#20284;&#28982;&#24615;&#21644;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#65288;NPE&#65289;&#65289;&#20381;&#36182;&#20110;&#27169;&#25311;&#32479;&#35745;&#37327;&#20197;&#25512;&#26029;&#38590;&#20197;&#35745;&#31639;&#30340;&#20284;&#28982;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#22411;&#38169;&#35823;&#24773;&#20917;&#19979;&#20250;&#20135;&#29983;&#19981;&#21487;&#20449;&#21644;&#35823;&#23548;&#24615;&#30340;&#25512;&#35770;&#32467;&#26524;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#26469;&#22788;&#29702;&#36328;&#19981;&#21516;&#31867;&#21035;&#30340;SBI&#26041;&#27861;&#30340;&#27169;&#22411;&#38169;&#35823;&#24773;&#20917;&#12290;&#21033;&#29992;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#30830;&#23450;SBI&#20013;&#30340;&#35823;&#24046;&#31243;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#24809;&#32602;&#37027;&#20123;&#22686;&#21152;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#32479;&#35745;&#37327;&#12290;&#20197;NPE&#21644;ABC&#20026;&#24212;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#24037;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26469;&#33258;&#26080;&#32447;&#30005;&#20256;&#25773;&#39046;&#22495;&#30340;&#23454;&#38469;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-based inference (SBI) methods such as approximate Bayesian computation (ABC), synthetic likelihood, and neural posterior estimation (NPE) rely on simulating statistics to infer parameters of intractable likelihood models. However, such methods are known to yield untrustworthy and misleading inference outcomes under model misspecification, thus hindering their widespread applicability. In this work, we propose the first general approach to handle model misspecification that works across different classes of SBI methods. Leveraging the fact that the choice of statistics determines the degree of misspecification in SBI, we introduce a regularized loss function that penalises those statistics that increase the mismatch between the data and the model. Taking NPE and ABC as use cases, we demonstrate the superior performance of our method on high-dimensional time-series models that are artificially misspecified. We also apply our method to real data from the field of radio propagat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#37325;&#26500;&#22823;&#22411;&#39044;&#35757;&#32451;&#23884;&#20837;&#30697;&#38453;&#65292;&#20854;&#21487;&#20197;&#22312;&#25216;&#26415;&#39046;&#22495;&#20869;&#36798;&#21040;&#19982;&#21477;&#23376;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#36136;&#37327;&#65292;&#20294;&#22823;&#23567;&#20026;&#21518;&#32773;&#30340;&#20116;&#20998;&#20043;&#19968;&#65292;&#35745;&#31639;&#26102;&#38388;&#33021;&#24555;10&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.15867</link><description>&lt;p&gt;
&#25216;&#26415;&#39046;&#22495;&#26415;&#35821;&#21644;&#30701;&#35821;&#30340;&#25991;&#26412;&#34920;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Extracting Text Representations for Terms and Phrases in Technical Domains. (arXiv:2305.15867v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#37325;&#26500;&#22823;&#22411;&#39044;&#35757;&#32451;&#23884;&#20837;&#30697;&#38453;&#65292;&#20854;&#21487;&#20197;&#22312;&#25216;&#26415;&#39046;&#22495;&#20869;&#36798;&#21040;&#19982;&#21477;&#23376;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#36136;&#37327;&#65292;&#20294;&#22823;&#23567;&#20026;&#21518;&#32773;&#30340;&#20116;&#20998;&#20043;&#19968;&#65292;&#35745;&#31639;&#26102;&#38388;&#33021;&#24555;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#26415;&#35821;&#21644;&#30701;&#35821;&#30340;&#23494;&#38598;&#34920;&#31034;&#26159;&#38754;&#21521;&#39640;&#24230;&#25216;&#26415;&#39046;&#22495;&#30340;&#30693;&#35782;&#21457;&#29616;&#24179;&#21488;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#24120;&#29992;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#33258;&#30417;&#30563;&#35774;&#32622;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#23884;&#20837;&#25110;&#20351;&#29992;&#35757;&#32451;&#36807;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#23567;&#22411;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#26469;&#37325;&#26500;&#22823;&#22411;&#39044;&#35757;&#32451;&#23884;&#20837;&#30697;&#38453;&#12290;&#19982;&#38745;&#24577;&#23884;&#20837;&#30456;&#27604;&#65292;&#21477;&#23376;&#32534;&#30721;&#22120;&#19981;&#20250;&#21463;&#21040;&#35789;&#27719;&#22806;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#20294;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to summarization. Common approaches to create dense representations include training domain-specific embeddings with self-supervised setups or using sentence encoder models trained over similarity tasks. In contrast to static embeddings, sentence encoders do not suffer from the out-of-vocabulary (OOV) problem, but impose significant computational costs. In this paper, we propose a fully unsupervised approach to text encoding that consists of training small character-based models with the objective of reconstructing large pre-trained embedding matrices. Models trained with this approach can not only match the quality of sentence encoders in technical domains, but are 5 times smaller and up to 10 tim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#25512;&#29702;&#38382;&#39064;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#26469;&#23454;&#29616;&#20302;&#24310;&#36831;&#12289;&#39640;&#21487;&#38752;&#24615;&#30340;CNN&#20998;&#24067;&#24335;&#25512;&#29702;&#65292;&#33021;&#22815;&#20998;&#37197;&#23376;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#35777;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#36739;&#20302;&#30340;&#24310;&#36831;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15858</link><description>&lt;p&gt;
&#20302;&#24310;&#36831;&#21644;&#39640;&#21487;&#38752;&#24615;&#30340;CNN&#20998;&#24067;&#24335;&#25512;&#29702;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#20154;&#26426;&#32676;&#20307;
&lt;/p&gt;
&lt;p&gt;
LLHR: Low Latency and High Reliability CNN Distributed Inference for Resource-Constrained UAV Swarms. (arXiv:2305.15858v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#25512;&#29702;&#38382;&#39064;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#26469;&#23454;&#29616;&#20302;&#24310;&#36831;&#12289;&#39640;&#21487;&#38752;&#24615;&#30340;CNN&#20998;&#24067;&#24335;&#25512;&#29702;&#65292;&#33021;&#22815;&#20998;&#37197;&#23376;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#35777;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#36739;&#20302;&#30340;&#24310;&#36831;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#22312;&#30417;&#25511;&#12289;&#25628;&#25937;&#12289;&#29615;&#22659;&#30417;&#27979;&#31561;&#20851;&#38190;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#19981;&#31283;&#23450;&#30340;&#36830;&#25509;&#12289;&#26377;&#38480;&#30340;&#24102;&#23485;&#21644;&#33021;&#37327;&#20197;&#21450;&#20005;&#26684;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#65292;&#23558;&#25968;&#25454;&#22788;&#29702;&#35831;&#27714;&#21457;&#36865;&#21040;&#36828;&#31243;&#26381;&#21153;&#22120;&#30340;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#20999;&#23454;&#21487;&#34892;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#25512;&#29702;&#35831;&#27714;&#21010;&#20998;&#20026;&#23376;&#20219;&#21153;&#65292;&#26681;&#25454;&#21487;&#29992;&#36164;&#28304;&#23558;&#20854;&#20998;&#37197;&#32473;&#26080;&#20154;&#26426;&#32676;&#20307;&#24182;&#22312;&#26080;&#20154;&#26426;&#32676;&#20307;&#31227;&#21160;&#26102;&#20256;&#36755;&#20013;&#38388;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#35777;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#36739;&#20302;&#30340;&#24310;&#36831;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20302;&#24310;&#36831;&#21644;&#39640;&#21487;&#38752;&#24615; (LLHR)&#20998;&#24067;&#24335;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#32771;&#34385;&#20219;&#21153;&#37325;&#35201;&#24615;&#21644;&#32593;&#32476;&#26465;&#20214;&#30340;&#26368;&#20248;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24310;&#36831;&#12289;&#21487;&#38752;&#24615;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Unmanned Aerial Vehicles (UAVs) have shown impressive performance in many critical applications, such as surveillance, search and rescue operations, environmental monitoring, etc. In many of these applications, the UAVs capture images as well as other sensory data and then send the data processing requests to remote servers. Nevertheless, this approach is not always practical in real-time-based applications due to unstable connections, limited bandwidth, limited energy, and strict end-to-end latency. One promising solution is to divide the inference requests into subtasks that can be distributed among UAVs in a swarm based on the available resources. Moreover, these tasks create intermediate results that need to be transmitted reliably as the swarm moves to cover the area. Our system model deals with real-time requests, aiming to find the optimal transmission power that guarantees higher reliability and low latency. We formulate the Low Latency and High-Reliability (LLHR) dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39034;&#24207; Integrated Gradients&#65288;SIG&#65289;&#30340;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20854;&#20182;&#21333;&#35789;&#19981;&#21464;&#65292;&#20165;&#22312;&#22522;&#32447;&#21644;&#24863;&#20852;&#36259;&#30340;&#21333;&#35789;&#20043;&#38388;&#21019;&#24314;&#25554;&#20540;&#26469;&#35745;&#31639;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#29992;&#35757;&#32451;&#30340;&#20196;&#29260;&#8220;mask&#8221;&#26367;&#25442;&#22522;&#32447;&#20196;&#29260;&#8220;pad&#8221;&#26469;&#26174;&#30528;&#25913;&#21892;&#35299;&#37322;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15853</link><description>&lt;p&gt;
&#39034;&#24207;Integrated Gradients&#65306;&#19968;&#31181;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Integrated Gradients: a simple but effective method for explaining language models. (arXiv:2305.15853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39034;&#24207; Integrated Gradients&#65288;SIG&#65289;&#30340;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20854;&#20182;&#21333;&#35789;&#19981;&#21464;&#65292;&#20165;&#22312;&#22522;&#32447;&#21644;&#24863;&#20852;&#36259;&#30340;&#21333;&#35789;&#20043;&#38388;&#21019;&#24314;&#25554;&#20540;&#26469;&#35745;&#31639;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#29992;&#35757;&#32451;&#30340;&#20196;&#29260;&#8220;mask&#8221;&#26367;&#25442;&#22522;&#32447;&#20196;&#29260;&#8220;pad&#8221;&#26469;&#26174;&#30528;&#25913;&#21892;&#35299;&#37322;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#31181;&#35299;&#37322;&#26041;&#27861;&#65288;&#20363;&#22914;Integrated Gradients&#65288;IG&#65289;&#65289;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#25968;&#25454;&#21644;&#26080;&#20449;&#24687;&#22522;&#32447;&#20043;&#38388;&#30340;&#30452;&#32447;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21516;&#26102;&#20026;&#27599;&#20010;&#21477;&#23376;&#21333;&#35789;&#37327;&#20135;&#29983;&#36335;&#24452;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20174;&#25554;&#20540;&#35789;&#29983;&#25104;&#30340;&#21477;&#23376;&#27809;&#26377;&#26126;&#30830;&#30340;&#21547;&#20041;&#65292;&#25110;&#32773;&#19982;&#21407;&#22987;&#21477;&#23376;&#30456;&#27604;&#26377;&#26174;&#30528;&#19981;&#21516;&#30340;&#21547;&#20041;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#21477;&#23376;&#30340;&#21547;&#20041;&#23613;&#21487;&#33021;&#25509;&#36817;&#21407;&#22987;&#21477;&#23376;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39034;&#24207;Integrated Gradients&#65288;SIG&#65289;&#65292;&#23427;&#36890;&#36807;&#20445;&#25345;&#20854;&#20182;&#21333;&#35789;&#19981;&#21464;&#65292;&#20165;&#22312;&#22522;&#32447;&#21644;&#24863;&#20852;&#36259;&#30340;&#21333;&#35789;&#20043;&#38388;&#21019;&#24314;&#25554;&#20540;&#26469;&#35745;&#31639;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36824;&#24314;&#35758;&#29992;&#35757;&#32451;&#30340;&#20196;&#29260;&#8220;mask&#8221;&#26367;&#25442;&#22522;&#32447;&#20196;&#29260;&#8220;pad&#8221;&#12290;&#34429;&#28982;&#36825;&#21482;&#26159;&#23545;&#21407;&#22987;IG&#26041;&#27861;&#30340;&#31616;&#21333;&#25913;&#36827;&#65292;&#20294;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several explanation methods such as Integrated Gradients (IG) can be characterised as path-based methods, as they rely on a straight line between the data and an uninformative baseline. However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence. In order to keep the meaning of these sentences as close as possible to the original one, we propose Sequential Integrated Gradients (SIG), which computes the importance of each word in a sentence by keeping fixed every other words, only creating interpolations between the baseline and the word of interest. Moreover, inspired by the training procedure of several language models, we also propose to replace the baseline token "pad" with the trained token "mask". While being a simple improvement over the original IG method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.15852</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#65306;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#24819;&#30340;&#25991;&#26412;&#12290;&#33258;&#30456;&#30683;&#30462;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24187;&#35273;&#24418;&#24335;&#65292;&#25351;&#30340;&#26159;&#35821;&#35328;&#27169;&#22411;&#22312;&#21516;&#19968;&#35821;&#22659;&#20013;&#29983;&#25104;&#20004;&#20010;&#30683;&#30462;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#38024;&#23545;&#26368;&#20808;&#36827;&#12289;&#32463;&#36807;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#33258;&#30456;&#30683;&#30462;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12289;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#33879;&#21517;&#30340;&#36824;&#26159;&#19981;&#22826;&#20986;&#21517;&#30340;&#35805;&#39064;&#65292;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#30456;&#30683;&#30462;&#37117;&#32463;&#24120;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#37319;&#26679;&#30830;&#23450;&#24615;&#34892;&#21015;&#24335;&#21644;Pfaffian&#28857;&#36807;&#31243;&#30340;&#29366;&#24577;&#21450;&#20854;&#20248;&#21270;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.15851</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#37319;&#26679;&#30830;&#23450;&#24615;&#34892;&#21015;&#24335;&#21644;Pfaffian&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
On sampling determinantal and Pfaffian point processes on a quantum computer. (arXiv:2305.15851v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#37319;&#26679;&#30830;&#23450;&#24615;&#34892;&#21015;&#24335;&#21644;Pfaffian&#28857;&#36807;&#31243;&#30340;&#29366;&#24577;&#21450;&#20854;&#20248;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;(DPP) &#26368;&#26089;&#34987; Macchi &#20316;&#20026;&#37327;&#23376;&#20809;&#23398;&#27169;&#22411;&#24341;&#20837;&#65292;&#33258;&#37027;&#20197;&#21518;&#65292;&#23427;&#20204;&#24050;&#24191;&#27867;&#29992;&#20316;&#32479;&#35745;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#27169;&#22411;&#21644;&#23376;&#25277;&#26679;&#24037;&#20855;&#12290;&#22823;&#22810;&#25968;&#24212;&#29992;&#38656;&#35201;&#20174;DPP&#25277;&#26679;&#65292;&#32771;&#34385;&#21040;&#20854;&#37327;&#23376;&#36215;&#28304;&#65292;&#33258;&#28982;&#20250;&#24819;&#30693;&#36947;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#25277;&#26679;DPP&#26159;&#21542;&#27604;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#26356;&#23481;&#26131;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#26377;&#38480;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;DPP&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;$\{1,\dots,N\}$&#23376;&#38598;&#19978;&#30340;&#20998;&#24067;&#65292;&#30001;&#19968;&#20010;$N\times N$&#30340;Hermite&#20869;&#26680;&#30697;&#38453;&#21442;&#25968;&#21270;&#12290;&#26368;&#22522;&#26412;&#30340;&#37319;&#26679;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65292;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#20998;&#21035;&#38656;&#35201; $\mathcal{O}(N^3)$ &#21644; $\mathcal{O}(Nr^2)$ &#30340;&#25805;&#20316;&#25104;&#26412;&#65292;&#20854;&#20013;$r$&#26159;&#20869;&#26680;&#30697;&#38453;&#30340;&#31209;&#12290;&#26412;&#25991;&#26088;&#22312;&#35752;&#35770;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;DPP&#37319;&#26679;&#31639;&#27861;&#30340;&#29366;&#24577;&#21450;&#20854;&#20248;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
DPPs were introduced by Macchi as a model in quantum optics the 1970s. Since then, they have been widely used as models and subsampling tools in statistics and computer science. Most applications require sampling from a DPP, and given their quantum origin, it is natural to wonder whether sampling a DPP on a quantum computer is easier than on a classical one. We focus here on DPPs over a finite state space, which are distributions over the subsets of $\{1,\dots,N\}$ parametrized by an $N\times N$ Hermitian kernel matrix. Vanilla sampling consists in two steps, of respective costs $\mathcal{O}(N^3)$ and $\mathcal{O}(Nr^2)$ operations on a classical computer, where $r$ is the rank of the kernel matrix. A large first part of the current paper consists in explaining why the state-of-the-art in quantum simulation of fermionic systems already yields quantum DPP sampling algorithms. We then modify existing quantum circuits, and discuss their insertion in a full DPP sampling pipeline that start
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#29992;&#20110;&#20998;&#26512;Dropout&#21160;&#24577;&#30340;&#38543;&#26426;&#20462;&#25913;&#26041;&#31243;&#65292;&#30740;&#31350;&#20102;Dropout&#22914;&#20309;&#20419;&#36827;&#35782;&#21035;&#26356;&#24179;&#22374;&#30340;&#26497;&#20540;&#28857;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#23454;&#35777;&#20102;&#36870;&#26041;&#24046;-&#24179;&#22374;&#20851;&#31995;&#21644;&#28023;&#26862;&#30697;&#38453;-&#26041;&#24046;&#20851;&#31995;&#36143;&#31359;&#20110;Dropout&#30340;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.15850</link><description>&lt;p&gt;
&#38543;&#26426;&#20462;&#25913;&#26041;&#31243;&#21644;Dropout&#31639;&#27861;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Stochastic Modified Equations and Dynamics of Dropout Algorithm. (arXiv:2305.15850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#29992;&#20110;&#20998;&#26512;Dropout&#21160;&#24577;&#30340;&#38543;&#26426;&#20462;&#25913;&#26041;&#31243;&#65292;&#30740;&#31350;&#20102;Dropout&#22914;&#20309;&#20419;&#36827;&#35782;&#21035;&#26356;&#24179;&#22374;&#30340;&#26497;&#20540;&#28857;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#23454;&#35777;&#20102;&#36870;&#26041;&#24046;-&#24179;&#22374;&#20851;&#31995;&#21644;&#28023;&#26862;&#30697;&#38453;-&#26041;&#24046;&#20851;&#31995;&#36143;&#31359;&#20110;Dropout&#30340;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dropout&#26159;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20043;&#19968;&#65292;&#28982;&#32780;&#23427;&#30340;&#28508;&#22312;&#26426;&#21046;&#20197;&#21450;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#20173;&#19981;&#29978;&#20102;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#29992;&#20110;&#20998;&#26512;Dropout&#21160;&#24577;&#30340;&#38543;&#26426;&#20462;&#25913;&#26041;&#31243;&#65292;&#20854;&#20013;&#23427;&#30340;&#31163;&#25955;&#36845;&#20195;&#36807;&#31243;&#34987;&#19968;&#31867;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#25152;&#36817;&#20284;&#12290;&#20026;&#20102;&#30740;&#31350;Dropout&#22914;&#20309;&#20419;&#36827;&#35782;&#21035;&#26356;&#24179;&#22374;&#30340;&#26497;&#20540;&#28857;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#25512;&#23548;&#20986;&#30340;Dropout&#38543;&#26426;&#20462;&#25913;&#26041;&#31243;&#30340;&#22122;&#22768;&#32467;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;&#28023;&#26862;&#30697;&#38453;&#21644;&#21327;&#26041;&#24046;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#36827;&#34892;&#20960;&#20010;&#30452;&#35266;&#30340;&#36817;&#20284;&#65292;&#25105;&#20204;&#23454;&#35777;&#20102;&#36870;&#26041;&#24046;-&#24179;&#22374;&#20851;&#31995;&#21644;&#28023;&#26862;&#30697;&#38453;-&#26041;&#24046;&#20851;&#31995;&#36143;&#31359;&#20110;Dropout&#30340;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#36825;&#20123;&#29702;&#35770;&#21644;&#23454;&#35777;&#21457;&#29616;&#23545;&#20110;&#25105;&#20204;&#28145;&#20837;&#29702;&#35299;Dropout&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dropout is a widely utilized regularization technique in the training of neural networks, nevertheless, its underlying mechanism and its impact on achieving good generalization abilities remain poorly understood. In this work, we derive the stochastic modified equations for analyzing the dynamics of dropout, where its discrete iteration process is approximated by a class of stochastic differential equations. In order to investigate the underlying mechanism by which dropout facilitates the identification of flatter minima, we study the noise structure of the derived stochastic modified equation for dropout. By drawing upon the structural resemblance between the Hessian and covariance through several intuitive approximations, we empirically demonstrate the universal presence of the inverse variance-flatness relation and the Hessian-variance relation, throughout the training process of dropout. These theoretical and empirical findings make a substantial contribution to our understanding o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#23884;&#20837;&#65292;&#24182;&#35777;&#26126;&#20102;Barron&#31354;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#31867;&#20284;&#20110;Sobolev&#31354;&#38388;$H^m$&#12290;&#20854;&#20013;&#65292;&#20462;&#27491;&#21151;&#29575;&#21333;&#20301;&#28608;&#27963;&#20989;&#25968;&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#29305;&#21035;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.15839</link><description>&lt;p&gt;
&#20855;&#26377;&#39640;&#38454;&#28608;&#27963;&#20989;&#25968;&#30340;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Embeddings between Barron spaces with higher order activation functions. (arXiv:2305.15839v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#23884;&#20837;&#65292;&#24182;&#35777;&#26126;&#20102;Barron&#31354;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#31867;&#20284;&#20110;Sobolev&#31354;&#38388;$H^m$&#12290;&#20854;&#20013;&#65292;&#20462;&#27491;&#21151;&#29575;&#21333;&#20301;&#28608;&#27963;&#20989;&#25968;&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#29305;&#21035;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#23485;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#24615;&#36136;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#12290;&#20026;&#20102;&#20102;&#35299;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#23884;&#20837;&#12290;&#36890;&#36807;&#25552;&#20379;&#29992;&#20110;&#34920;&#31034;&#20989;&#25968;$f$&#30340;&#27979;&#37327;$\mu$&#19978;&#30340;&#25512;&#36827;&#26144;&#23556;&#26469;&#35777;&#26126;&#36825;&#20123;&#23884;&#20837;&#12290;&#19968;&#31181;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#28608;&#27963;&#20989;&#25968;&#26159;&#32473;&#23450;&#20026;$\operatorname{RePU}_s(x)=\max(0,x)^s$&#30340;&#20462;&#27491;&#21151;&#29575;&#21333;&#20301;($\operatorname{RePU}$)&#12290;&#23545;&#20110;&#35768;&#22810;&#24120;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21487;&#20197;&#20351;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340;&#27888;&#21202;&#20313;&#39033;&#23450;&#29702;&#26500;&#36896;&#25512;&#36827;&#26144;&#23556;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;&#30456;&#20851;Barron&#31354;&#38388;&#23884;&#20837;&#21040;&#20855;&#26377;$\operatorname{RePU}$&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;Barron&#31354;&#38388;&#20013;&#12290;&#27492;&#22806;&#65292;&#19982;$\operatorname{RePU}_s$&#30456;&#20851;&#30340;Barron&#31354;&#38388;&#20855;&#26377;&#31867;&#20284;&#20110;Sobolev&#31354;&#38388;$H^m$&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The approximation properties of infinitely wide shallow neural networks heavily depend on the choice of the activation function. To understand this influence, we study embeddings between Barron spaces with different activation functions. These embeddings are proven by providing push-forward maps on the measures $\mu$ used to represent functions $f$. An activation function of particular interest is the rectified power unit ($\operatorname{RePU}$) given by $\operatorname{RePU}_s(x)=\max(0,x)^s$. For many commonly used activation functions, the well-known Taylor remainder theorem can be used to construct a push-forward map, which allows us to prove the embedding of the associated Barron space into a Barron space with a $\operatorname{RePU}$ as activation function. Moreover, the Barron spaces associated with the $\operatorname{RePU}_s$ have a hierarchical structure similar to the Sobolev spaces $H^m$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#20013;&#20174;&#28857;&#20113;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#32467;&#26500;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15836</link><description>&lt;p&gt;
&#29992;&#20110;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#30340;&#28857;&#20113;&#22810;&#23610;&#24230;&#32593;&#26684;&#28210;&#26579;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks. (arXiv:2305.15836v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#20013;&#20174;&#28857;&#20113;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#32467;&#26500;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#32593;&#26684;&#34920;&#24449;&#65292;&#28982;&#21518;&#24212;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21487;&#29992;&#20110;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#65292;&#20294;&#20174;&#19981;&#35268;&#21017;&#30340;&#28857;&#20113;&#25968;&#25454;&#36716;&#25442;&#20026;&#23494;&#38598;&#30340;&#32593;&#26684;&#32467;&#26500;&#24120;&#24120;&#20250;&#23548;&#33268;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#36825;&#26159;&#30001;&#20110;&#28857;&#30340;&#31163;&#25955;&#21270;&#21644;&#32858;&#21512;&#36896;&#25104;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#32593;&#26684;&#28210;&#26579;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#65292;&#23427;&#21033;&#29992;&#26680;&#28857;&#21367;&#31215;&#30340;&#25551;&#36848;&#33021;&#21147;&#26469;&#25913;&#36827;&#32593;&#26684;&#28210;&#26579;&#36807;&#31243;&#20013;&#23616;&#37096;&#28857;&#20113;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22810;&#23610;&#24230;&#32593;&#26684;&#28210;&#26579;&#20844;&#24335;&#65292;&#20197;&#20219;&#24847;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861;&#23558;&#22810;&#23610;&#24230;&#29305;&#24449;&#26144;&#23556;&#34701;&#21512;&#21040;&#26816;&#27979;&#32593;&#32476;&#30340;&#21367;&#31215;&#39592;&#24178;&#20013;&#12290;&#25105;&#20204;&#22312; nuScenes &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35780;&#20272;&#20102;&#26816;&#27979;&#27773;&#36710;&#12289;&#21345;&#36710;&#21644;&#20844;&#20132;&#36710;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230; KPPillarsBEV &#32467;&#26500;&#21644; KPBEV &#32593;&#26684;&#28210;&#26579;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Architectures that first convert point clouds to a grid representation and then apply convolutional neural networks achieve good performance for radar-based object detection. However, the transfer from irregular point cloud data to a dense grid structure is often associated with a loss of information, due to the discretization and aggregation of points. In this paper, we propose a novel architecture, multi-scale KPPillarsBEV, that aims to mitigate the negative effects of grid rendering. Specifically, we propose a novel grid rendering method, KPBEV, which leverages the descriptive power of kernel point convolutions to improve the encoding of local point cloud contexts during grid rendering. In addition, we propose a general multi-scale grid rendering formulation to incorporate multi-scale feature maps into convolutional backbones of detection networks with arbitrary grid rendering methods. We perform extensive experiments on the nuScenes dataset and evaluate the methods in terms of dete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PDE+&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19981;&#20165;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#25110;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#36824;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#27425;&#21644;&#32593;&#32476;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15835</link><description>&lt;p&gt;
PDE+&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion. (arXiv:2305.15835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PDE+&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19981;&#20165;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#25110;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#36824;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#27425;&#21644;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#20998;&#24067;&#26102;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#20027;&#35201;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#33539;&#24335;&#30340;&#26041;&#24335;&#65292;&#22914;&#25968;&#25454;&#22686;&#24378;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#22122;&#22768;&#27880;&#20837;&#31561;&#65292;&#36825;&#20123;&#26041;&#27861;&#30001;&#20110;&#27169;&#22411;&#30340;&#38750;&#24179;&#28369;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;&#27867;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#30452;&#25509;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#20989;&#25968;&#26469;&#22686;&#24378;&#23427;&#65292;&#32780;&#19981;&#26159;&#32858;&#28966;&#20110;&#35843;&#25972;&#36755;&#20837;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#19982;&#29305;&#23450;PDE&#35299;&#30340;&#24179;&#28369;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21363;&#8220;&#36755;&#36816;&#26041;&#31243;&#8221;&#12290;&#36825;&#26679;&#24314;&#31435;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24635;&#20307;&#26694;&#26550;&#65292;&#23558;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#24341;&#20837;&#36755;&#36816;&#26041;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20854;&#35299;&#30340;&#24179;&#28369;&#24230;&#65292;&#20174;&#32780;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#27169;&#22359;&#20351;&#29992;&#65292;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#35757;&#32451;&#31639;&#27861;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#21644;&#35757;&#32451;&#36718;&#27425;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#25110;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization of neural networks is a central challenge in machine learning, especially concerning the performance under distributions that differ from training ones. Current methods, mainly based on the data-driven paradigm such as data augmentation, adversarial training, and noise injection, may encounter limited generalization due to model non-smoothness. In this paper, we propose to investigate generalization from a Partial Differential Equation (PDE) perspective, aiming to enhance it directly through the underlying function of neural networks, rather than focusing on adjusting input data. Specifically, we first establish the connection between neural network generalization and the smoothness of the solution to a specific PDE, namely ``transport equation''. Building upon this, we propose a general framework that introduces adaptive distributional diffusion into transport equation to enhance the smoothness of its solution, thereby improving generalization. In the context of neu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#65292;&#21363;&#26631;&#31614;&#20301;&#32622;&#20559;&#24046;&#65292;&#21363;&#38752;&#36817;&#26631;&#35760;&#33410;&#28857;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#34920;&#29616;&#26356;&#22909;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#26631;&#31614;&#25509;&#36817;&#24230;&#24471;&#20998;&#65292;&#21487;&#20197;&#37327;&#21270;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#26469;&#23398;&#20064;&#26080;&#26631;&#31614;&#20301;&#32622;&#20559;&#24046;&#30340;&#22270;&#32467;&#26500;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;GNNs&#65292;&#25104;&#21151;&#20943;&#36731;&#26631;&#31614;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15822</link><description>&lt;p&gt;
&#12298;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26631;&#31614;&#20301;&#32622;&#20559;&#24046;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Label Position Bias in Graph Neural Networks. (arXiv:2305.15822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#65292;&#21363;&#26631;&#31614;&#20301;&#32622;&#20559;&#24046;&#65292;&#21363;&#38752;&#36817;&#26631;&#35760;&#33410;&#28857;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#34920;&#29616;&#26356;&#22909;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#26631;&#31614;&#25509;&#36817;&#24230;&#24471;&#20998;&#65292;&#21487;&#20197;&#37327;&#21270;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#26469;&#23398;&#20064;&#26080;&#26631;&#31614;&#20301;&#32622;&#20559;&#24046;&#30340;&#22270;&#32467;&#26500;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;GNNs&#65292;&#25104;&#21151;&#20943;&#36731;&#26631;&#31614;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#24050;&#32463;&#25104;&#20026;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;GNNs&#23384;&#22312;&#26469;&#33258;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#25299;&#25169;&#30340;&#21508;&#31181;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#8212;&#8212;&#26631;&#31614;&#20301;&#32622;&#20559;&#24046;&#65292;&#23427;&#34920;&#26126;&#38752;&#36817;&#26631;&#35760;&#33410;&#28857;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#26631;&#31614;&#25509;&#36817;&#24230;&#24471;&#20998;&#65292;&#26469;&#37327;&#21270;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#21457;&#29616;&#23427;&#19982;&#24615;&#33021;&#24046;&#24322;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#20301;&#32622;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20248;&#21270;&#26694;&#26550;&#26469;&#23398;&#20064;&#26080;&#26631;&#31614;&#20301;&#32622;&#20559;&#24046;&#30340;&#22270;&#32467;&#26500;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340; GNNs&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#20248;&#20110;&#39592;&#24178;&#26041;&#27861;&#65292;&#32780;&#19988;&#26174;&#33879;&#20943;&#36731;&#20102; GNNs &#20013;&#30340;&#26631;&#31614;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a powerful tool for semi-supervised node classification tasks. However, recent studies have revealed various biases in GNNs stemming from both node features and graph topology. In this work, we uncover a new bias - label position bias, which indicates that the node closer to the labeled nodes tends to perform better. We introduce a new metric, the Label Proximity Score, to quantify this bias, and find that it is closely related to performance disparities. To address the label position bias, we propose a novel optimization framework for learning a label position unbiased graph structure, which can be applied to existing GNNs. Extensive experiments demonstrate that our proposed method not only outperforms backbone methods but also significantly mitigates the issue of label position bias in GNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LOB&#25968;&#25454;&#30340;&#24066;&#22330;&#20570;&#24066;RL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;LOB&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#28151;&#21512;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26041;&#27861;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.15821</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20174;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#36827;&#34892;&#20570;&#24066;
&lt;/p&gt;
&lt;p&gt;
Market Making with Deep Reinforcement Learning from Limit Order Books. (arXiv:2305.15821v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LOB&#25968;&#25454;&#30340;&#24066;&#22330;&#20570;&#24066;RL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;LOB&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#28151;&#21512;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26041;&#27861;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#20570;&#24066;&#65288;MM&#65289;&#26159;&#37327;&#21270;&#37329;&#34701;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#65292;&#20195;&#29702;&#21830;&#38656;&#35201;&#19981;&#26029;&#22320;&#20248;&#21270;&#35810;&#20215;&#21644;&#21483;&#20215;&#26469;&#25552;&#20379;&#27969;&#21160;&#24615;&#21644;&#36186;&#21462;&#21033;&#28070;&#12290;&#38480;&#20215;&#35746;&#21333;&#31807;&#65288;LOB&#65289;&#21253;&#21547;&#25152;&#26377;&#27963;&#36291;&#38480;&#20215;&#35746;&#21333;&#30340;&#20449;&#24687;&#65292;&#26159;&#20915;&#31574;&#21046;&#23450;&#30340;&#22522;&#30784;&#12290;&#28436;&#21270;&#30340;&#12289;&#39640;&#32500;&#30340;&#21644;&#20302;&#20449;&#22122;&#27604;&#30340;LOB&#25968;&#25454;&#30340;&#24314;&#27169;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;MM&#31574;&#30053;&#20381;&#36182;&#20110;&#24378;&#20551;&#35774;&#65292;&#22914;&#20215;&#26684;&#36807;&#31243;&#12289;&#35746;&#21333;&#21040;&#36798;&#36807;&#31243;&#31561;&#12290;&#20197;&#24448;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38656;&#35201;&#25163;&#21160;&#24314;&#31435;&#24066;&#22330;&#29305;&#24449;&#65292;&#36825;&#31181;&#20570;&#27861;&#19981;&#33021;&#24456;&#22909;&#22320;&#20195;&#34920;&#24066;&#22330;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LOB&#25968;&#25454;&#30340;&#24066;&#22330;&#20570;&#24066;RL&#20195;&#29702;&#12290;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;Attn-LOB&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#20174;LOB&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#25105;&#20204;&#20026;MM&#20219;&#21153;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#28151;&#21512;&#22870;&#21169;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32508;&#21512;&#30340;&#23454;&#39564;&#26469;&#27979;&#35797;&#24310;&#36831;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Market making (MM) is an important research topic in quantitative finance, the agent needs to continuously optimize ask and bid quotes to provide liquidity and make profits. The limit order book (LOB) contains information on all active limit orders, which is an essential basis for decision-making. The modeling of evolving, high-dimensional and low signal-to-noise ratio LOB data is a critical challenge. Traditional MM strategy relied on strong assumptions such as price process, order arrival process, etc. Previous reinforcement learning (RL) works handcrafted market features, which is insufficient to represent the market. This paper proposes a RL agent for market making with LOB data. We leverage a neural network with convolutional filters and attention mechanism (Attn-LOB) for feature extraction from LOB. We design a new continuous action space and a hybrid reward function for the MM task. Finally, we conduct comprehensive experiments on latency and interpretability, showing that our a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#38160;&#24230;&#24418;&#24335;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861; WSAM&#65292;&#29992;&#20110;&#25913;&#36827; Sharpness-Aware Minimization (SAM) &#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#25110;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15817</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65306;&#23558;&#38160;&#24230;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#30340;&#21152;&#26435;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term. (arXiv:2305.15817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#38160;&#24230;&#24418;&#24335;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861; WSAM&#65292;&#29992;&#20110;&#25913;&#36827; Sharpness-Aware Minimization (SAM) &#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#25110;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#26368;&#23567;&#20540;&#30340;&#24179;&#22374;&#24230;&#23494;&#20999;&#30456;&#20851;&#65292;&#23548;&#33268;&#21457;&#23637;&#20102;Sharpness-Aware Minimization (SAM)&#26469;&#23547;&#25214;&#26356;&#24179;&#22374;&#30340;&#26368;&#23567;&#20540;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102; SAM &#30340;&#25439;&#22833;&#20989;&#25968;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; WSAM&#65292;&#36890;&#36807;&#23558;&#38160;&#24230;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#36827;&#34892;&#25913;&#36827;&#12290;&#25105;&#20204;&#36890;&#36807;PAC&#21644;Bayes-PAC&#25216;&#26415;&#30340;&#32467;&#21512;&#35777;&#26126;&#20102;&#23427;&#30340;&#27867;&#21270;&#36793;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#23427;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982; vanilla optimizer&#12289;SAM &#21450;&#20854;&#21464;&#20307;&#30456;&#27604;&#65292;WSAM &#21462;&#24471;&#20102;&#25913;&#21892;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#25110;&#32773;&#33267;&#23569;&#26159;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#12290;&#20195;&#30721;&#21487;&#20174; https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) generalization is known to be closely related to the flatness of minima, leading to the development of Sharpness-Aware Minimization (SAM) for seeking flatter minima and better generalization. In this paper, we revisit the loss of SAM and propose a more general method, called WSAM, by incorporating sharpness as a regularization term. We prove its generalization bound through the combination of PAC and Bayes-PAC techniques, and evaluate its performance on various public datasets. The results demonstrate that WSAM achieves improved generalization, or is at least highly competitive, compared to the vanilla optimizer, SAM and its variants. The code is available at https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#37319;&#29992;&#30446;&#26631;&#26816;&#27979;&#25216;&#26415;&#22312;&#21307;&#30103;&#22270;&#20687;&#20013;&#21033;&#29992;YOLOv5&#27169;&#22411;&#36827;&#34892;&#32954;&#30284;&#30149;&#28790;&#30340;&#35782;&#21035;&#65292;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#22312;&#33016;&#29255;&#20013;&#25104;&#21151;&#23450;&#20301;&#20986;&#24694;&#24615;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.15813</link><description>&lt;p&gt;
&#21033;&#29992;&#29289;&#20307;&#26816;&#27979;&#25216;&#26415;&#36827;&#34892;&#32954;&#30284;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging object detection for the identification of lung cancer. (arXiv:2305.15813v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#37319;&#29992;&#30446;&#26631;&#26816;&#27979;&#25216;&#26415;&#22312;&#21307;&#30103;&#22270;&#20687;&#20013;&#21033;&#29992;YOLOv5&#27169;&#22411;&#36827;&#34892;&#32954;&#30284;&#30149;&#28790;&#30340;&#35782;&#21035;&#65292;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#22312;&#33016;&#29255;&#20013;&#25104;&#21151;&#23450;&#20301;&#20986;&#24694;&#24615;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#19968;&#30452;&#26159;&#20840;&#29699;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#38754;&#20020;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#26089;&#26399;&#26816;&#27979;&#23545;&#24739;&#32773;&#32467;&#26524;&#30340;&#25913;&#21892;&#30340;&#37325;&#35201;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29305;&#21035;&#26159;YOLOv5&#36825;&#20010;&#20808;&#36827;&#30446;&#26631;&#35782;&#21035;&#31995;&#32479;&#22312;&#21307;&#30103;&#22270;&#20687;&#20013;&#24212;&#29992;&#20110;&#32954;&#30284;&#35782;&#21035;&#26041;&#38754;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#31639;&#27861;&#65292;&#20174;Kaggle&#33719;&#21462;&#20102;&#30001;&#33016;&#37096;X&#20809;&#29255;&#21644;&#30456;&#24212;&#27880;&#37322;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;&#37319;&#29992;YOLOv5&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#30284;&#24615;&#32954;&#37096;&#30149;&#21464;&#12290;&#35757;&#32451;&#36807;&#31243;&#28041;&#21450;&#20248;&#21270;&#36229;&#21442;&#25968;&#24182;&#21033;&#29992;&#22686;&#24378;&#25216;&#26415;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;YOLOv5&#27169;&#22411;&#22312;&#37492;&#23450;&#32954;&#30284;&#30149;&#28790;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#26174;&#31034;&#20986;&#39640;&#20934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;&#23427;&#25104;&#21151;&#22320;&#22312;&#33016;&#29255;&#20013;&#23450;&#20301;&#20102;&#24694;&#24615;&#21306;&#22495;&#65292;&#32463;&#30001;&#19987;&#19994;&#21307;&#29983;&#30340;&#30830;&#35748;&#21518;&#24471;&#20986;&#21028;&#26029;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer poses a significant global public health challenge, emphasizing the importance of early detection for improved patient outcomes. Recent advancements in deep learning algorithms have shown promising results in medical image analysis. This study aims to explore the application of object detection particularly YOLOv5, an advanced object identification system, in medical imaging for lung cancer identification. To train and evaluate the algorithm, a dataset comprising chest X-rays and corresponding annotations was obtained from Kaggle. The YOLOv5 model was employed to train an algorithm capable of detecting cancerous lung lesions. The training process involved optimizing hyperparameters and utilizing augmentation techniques to enhance the model's performance. The trained YOLOv5 model exhibited exceptional proficiency in identifying lung cancer lesions, displaying high accuracy and recall rates. It successfully pinpointed malignant areas in chest radiographs, as validated by a se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;Grug&#65292;&#26088;&#22312;&#32479;&#19968;HGNN&#20013;&#30340;&#22270;&#24418;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#65292;&#24182;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#12289;&#38750;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#65292;&#32508;&#21512;&#25928;&#26524;&#21644;&#25928;&#29575;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15811</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#27491;&#21017;&#21270;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unifying gradient regularization for Heterogeneous Graph Neural Networks. (arXiv:2305.15811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;Grug&#65292;&#26088;&#22312;&#32479;&#19968;HGNN&#20013;&#30340;&#22270;&#24418;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#65292;&#24182;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#12289;&#38750;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#65292;&#32508;&#21512;&#25928;&#26524;&#21644;&#25928;&#29575;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24322;&#26500;&#22270;&#30340;&#34920;&#24449;&#12290;&#23613;&#31649;HGNN&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#20173;&#38754;&#20020;&#36807;&#24230;&#24179;&#28369;&#21644;&#38750;&#40065;&#26834;&#24615;&#31561;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#19987;&#27880;&#20110;&#22270;&#24418;&#25299;&#25169;&#25110;&#33410;&#28857;&#29305;&#24449;&#65292;&#32570;&#20047;&#32479;&#19968;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;Grug&#65292;&#26088;&#22312;&#32479;&#19968;HGNN&#20013;&#30340;&#22270;&#24418;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#65292;&#24182;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#12289;&#38750;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Grug&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Neural Networks (HGNNs) are a class of powerful deep learning methods widely used to learn representations of heterogeneous graphs. Despite the fast development of HGNNs, they still face some challenges such as over-smoothing, and non-robustness. Previous studies have shown that these problems can be reduced by using gradient regularization methods. However, the existing gradient regularization methods focus on either graph topology or node features. There is no universal approach to integrate these features, which severely affects the efficiency of regularization. In addition, the inclusion of gradient regularization into HGNNs sometimes leads to some problems, such as an unstable training process, increased complexity and insufficient coverage regularized information. Furthermore, there is still short of a complete theoretical analysis of the effects of gradient regularization on HGNNs. In this paper, we propose a novel gradient regularization method called Grug, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#24635;&#25104;&#26412;&#38480;&#21046;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20915;&#31574;&#38382;&#39064;&#65288;CBwK&#65289;&#65292;&#36890;&#36807;&#23545;&#26415;&#35821;&#36827;&#34892;&#37325;&#26032;&#32452;&#21512;&#65292;&#23545;CBwK&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#25903;&#25345;&#23567;&#20110;$T^{3/4}$&#30340;&#24635;&#25104;&#26412;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#23545;&#20598;&#31574;&#30053;&#23454;&#29616;&#20102;&#24179;&#31561;&#30340;&#25104;&#26412;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15807</link><description>&lt;p&gt;
&#24102;&#23567;&#24635;&#25104;&#26412;&#38480;&#21046;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20915;&#31574;&#38382;&#39064;&#19982;&#32972;&#21253;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#65292;&#21450;&#20854;&#23545;&#20844;&#24179;&#24615;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness. (arXiv:2305.15807v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#24635;&#25104;&#26412;&#38480;&#21046;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20915;&#31574;&#38382;&#39064;&#65288;CBwK&#65289;&#65292;&#36890;&#36807;&#23545;&#26415;&#35821;&#36827;&#34892;&#37325;&#26032;&#32452;&#21512;&#65292;&#23545;CBwK&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#25903;&#25345;&#23567;&#20110;$T^{3/4}$&#30340;&#24635;&#25104;&#26412;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#23545;&#20598;&#31574;&#30053;&#23454;&#29616;&#20102;&#24179;&#31561;&#30340;&#25104;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#24102;&#26377;&#32972;&#21253;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20915;&#31574;&#38382;&#39064;&#65288;CBwK&#65289;&#65292;&#27599;&#19968;&#36718;&#33719;&#24471;&#19968;&#20010;&#26631;&#37327;&#22870;&#21169;&#21644;&#19968;&#20010;&#21521;&#37327;&#20540;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#32047;&#35745;&#30340;&#22870;&#21169;&#65292;&#24182;&#30830;&#20445;&#32047;&#35745;&#25104;&#26412;&#20302;&#20110;&#26576;&#20010;&#39044;&#23450;&#30340;&#25104;&#26412;&#38480;&#21046;&#12290;&#25105;&#20204;&#20551;&#35774;&#29615;&#22659;&#26469;&#33258;&#19968;&#20010;&#36830;&#32493;&#38598;&#21512;&#65292;&#25104;&#26412;&#21487;&#20197;&#24102;&#31526;&#21495;&#65292;&#24182;&#19988;&#26410;&#30693;&#30340;&#26399;&#26395;&#22870;&#21169;&#21644;&#25104;&#26412;&#20989;&#25968;&#21487;&#20197;&#34987;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#36825;&#26159;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#20856;&#22411;&#20551;&#35774;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36804;&#20170;&#20026;&#27490;&#24635;&#25104;&#26412;&#32422;&#26463;&#33267;&#23569;&#35201;&#20026;$T^{3/4}$&#65292;&#20854;&#20013;$T$&#26159;&#36718;&#25968;&#65292;&#24182;&#19988;&#29978;&#33267;&#36890;&#24120;&#34987;&#20551;&#23450;&#20026;&#19982;$T$&#32447;&#24615;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21463;&#21040;&#40723;&#33310;&#65292;&#20351;&#29992;CBwK&#26469;&#24378;&#21046;&#23454;&#26045;&#23454;&#29616;&#32452;&#20043;&#38388;&#24179;&#22343;&#25104;&#26412;&#24179;&#31561;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65306;&#19982;&#30456;&#24212;&#25104;&#26412;&#32422;&#26463;&#30456;&#20851;&#30340;&#39044;&#31639;&#24212;&#23613;&#21487;&#33021;&#25509;&#36817;&#20110;&#38454;&#25968;&#20026;$\sqrt{T}$&#32423;&#21035;&#30340;&#33258;&#28982;&#20559;&#24046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#20598;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider contextual bandit problems with knapsacks [CBwK], a problem where at each round, a scalar reward is obtained and vector-valued costs are suffered. The learner aims to maximize the cumulative rewards while ensuring that the cumulative costs are lower than some predetermined cost constraints. We assume that contexts come from a continuous set, that costs can be signed, and that the expected reward and cost functions, while unknown, may be uniformly estimated -- a typical assumption in the literature. In this setting, total cost constraints had so far to be at least of order $T^{3/4}$, where $T$ is the number of rounds, and were even typically assumed to depend linearly on $T$. We are however motivated to use CBwK to impose a fairness constraint of equalized average costs between groups: the budget associated with the corresponding cost constraints should be as close as possible to the natural deviations, of order $\sqrt{T}$. To that end, we introduce a dual strategy based on 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15805</link><description>&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#29992;&#20110;&#39640;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. (arXiv:2305.15805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#38590;&#20197;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#20943;&#23569;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#22823;&#22810;&#25968;LLM&#20173;&#28982;&#22312;&#25152;&#26377;&#26631;&#35760;&#23545;&#20043;&#38388;&#37319;&#29992;&#27880;&#24847;&#23618;&#65292;&#20174;&#32780;&#20135;&#29983;&#20108;&#27425;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#26469;&#21160;&#24577;&#20462;&#21098;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21487;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30830;&#23450;&#21738;&#20123;&#26080;&#20851;&#30340;&#26631;&#35760;&#21487;&#20197;&#20174;&#19978;&#19979;&#25991;&#20013;&#21024;&#38500;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#35299;&#20915;&#20102;&#24615;&#33021;&#38382;&#39064;&#65292;&#32780;&#19988;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20026;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21098;&#26525;&#24378;&#24230;&#21487;&#20197;&#30001;&#31232;&#30095;&#24230;&#21442;&#25968;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity para
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340; Lucy-SKG &#23398;&#20064;&#29609;&#8220;&#28779;&#31661;&#32852;&#30431;&#8221;&#28216;&#25103;&#30340;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#35813;&#28216;&#25103;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#20004;&#20010;&#26426;&#22120;&#20154;&#65292;&#24182;&#25104;&#20026;&#20102;&#26368;&#20808;&#36827;&#30340;&#20195;&#29702;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2305.15801</link><description>&lt;p&gt;
Lucy-SKG&#65306;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39640;&#25928;&#23398;&#20064;&#29609;&#8220;&#28779;&#31661;&#32852;&#30431;&#8221;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep Reinforcement Learning. (arXiv:2305.15801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340; Lucy-SKG &#23398;&#20064;&#29609;&#8220;&#28779;&#31661;&#32852;&#30431;&#8221;&#28216;&#25103;&#30340;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#35813;&#28216;&#25103;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#20004;&#20010;&#26426;&#22120;&#20154;&#65292;&#24182;&#25104;&#20026;&#20102;&#26368;&#20808;&#36827;&#30340;&#20195;&#29702;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#39046;&#22495;&#21462;&#24471;&#36827;&#23637;&#30340;&#19968;&#31181;&#25104;&#21151;&#31574;&#30053;&#26159;&#23558;&#28216;&#25103;&#35270;&#20026;&#38382;&#39064;&#65292;&#36825;&#24050;&#34987;&#35777;&#26126;&#20250;&#23548;&#33268;&#21508;&#31181;&#37325;&#22823;&#31361;&#30772;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#30740;&#31350;&#8220;&#28779;&#31661;&#32852;&#30431;&#8221;&#28216;&#25103;&#65292;&#36825;&#26159;&#19968;&#27454;&#24191;&#21463;&#27426;&#36814;&#20294;&#30456;&#23545;&#26410;&#34987;&#28145;&#20837;&#25506;&#32034;&#30340;3D&#22810;&#20154;&#22312;&#32447;&#28216;&#25103;&#65292;&#20855;&#26377;&#29420;&#29305;&#30340;&#29289;&#29702;&#24341;&#25806;&#21644;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#65292;&#23545;&#20110;&#24320;&#21457;&#39640;&#25928;&#21644;&#39640;&#24615;&#33021;&#30340;&#28216;&#25103;&#20195;&#29702;&#31243;&#24207;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340; Lucy-SKG&#65292;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#29609;&#28779;&#31661;&#32852;&#30431;&#28216;&#25103;&#65292;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#26412;&#28216;&#25103;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#20004;&#20010;&#26426;&#22120;&#20154; Necto&#65288;2022&#24180;&#26426;&#22120;&#20154;&#20896;&#20891;&#65289;&#21644;&#20854;&#21518;&#32487; Nexto&#65292;&#20174;&#32780;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#20195;&#29702;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;a&#65289;&#24320;&#21457;&#20102;&#22870;&#21169;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#24211;&#65307;b&#65289;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#32908;&#32905;&#21453;&#39304;&#32452;&#21512;&#65292;&#25429;&#25417;&#22797;&#26434;&#22870;&#21169;&#31867;&#22411;&#30340;&#25928;&#29992;&#30340;&#26032;&#22411;&#21442;&#25968;&#21270;&#22870;&#21169;&#24418;&#29366;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A successful tactic that is followed by the scientific community for advancing AI is to treat games as problems, which has been proven to lead to various breakthroughs. We adapt this strategy in order to study Rocket League, a widely popular but rather under-explored 3D multiplayer video game with a distinct physics engine and complex dynamics that pose a significant challenge in developing efficient and high-performance game-playing agents. In this paper, we present Lucy-SKG, a Reinforcement Learning-based model that learned how to play Rocket League in a sample-efficient manner, outperforming by a notable margin the two highest-ranking bots in this game, namely Necto (2022 bot champion) and its successor Nexto, thus becoming a state-of-the-art agent. Our contributions include: a) the development of a reward analysis and visualization library, b) novel parameterizable reward shape functions that capture the utility of complex reward types via our proposed Kinesthetic Reward Combinatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#26550;&#26500;&#21387;&#32553;&#26041;&#27861;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;MAC&#21644;&#24310;&#36831;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#19982;&#20351;&#29992;&#26356;&#22810;&#36164;&#28304;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15798</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26550;&#26500;&#21387;&#32553;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Architectural Compression of Text-to-Image Diffusion Models. (arXiv:2305.15798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#26550;&#26500;&#21387;&#32553;&#26041;&#27861;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;MAC&#21644;&#24310;&#36831;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#19982;&#20351;&#29992;&#26356;&#22810;&#36164;&#28304;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65288;SDMs&#65289;&#20013;&#20986;&#33394;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#32467;&#26524;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36817;&#26399;&#20851;&#20110;&#39640;&#25928;SDMs&#30340;&#30740;&#31350;&#23558;&#37325;&#28857;&#25918;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#19978;&#12290;&#19982;&#36825;&#20123;&#26041;&#21521;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#65292;&#24378;&#35843;&#20102;&#32463;&#20856;&#26550;&#26500;&#21387;&#32553;&#22312;&#36890;&#29992;T2I&#21512;&#25104;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20174;SDMs&#30340;U-Net&#20013;&#21024;&#38500;&#20102;&#20960;&#20010;&#27531;&#24046;&#21644;&#27880;&#24847;&#21147;&#22359;&#65292;&#20351;&#21442;&#25968;&#25968;&#37327;&#12289;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;MAC&#21644;&#24310;&#36831;&#20943;&#23569;&#20102;&#36229;&#36807;30&#65285;&#12290;&#25105;&#20204;&#22312;&#21333;&#20010;A100 GPU&#19978;&#20165;&#20351;&#29992;0.22M LAION&#23545;&#36827;&#34892;&#33976;&#39311;&#39044;&#35757;&#32451;&#65288;&#23569;&#20110;&#20840;&#20307;&#35757;&#32451;&#23545;&#30340;0.1&#65285;&#65289;&#12290;&#23613;&#31649;&#20351;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#32039;&#20945;&#22411;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20256;&#36882;&#30340;&#30693;&#35782;&#27169;&#20223;&#21407;&#22987;SDM&#65292;&#24182;&#22312;&#23545;&#25239;&#36739;&#22823;&#30340;&#22810;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exceptional text-to-image (T2I) generation results of Stable Diffusion models (SDMs) come with substantial computational demands. To resolve this issue, recent research on efficient SDMs has prioritized reducing the number of sampling steps and utilizing network quantization. Orthogonal to these directions, this study highlights the power of classical architectural compression for general-purpose T2I synthesis by introducing block-removed knowledge-distilled SDMs (BK-SDMs). We eliminate several residual and attention blocks from the U-Net of SDMs, obtaining over a 30% reduction in the number of parameters, MACs per sampling step, and latency. We conduct distillation-based pretraining with only 0.22M LAION pairs (fewer than 0.1% of the full training pairs) on a single A100 GPU. Despite being trained with limited resources, our compact models can imitate the original SDM by benefiting from transferred knowledge and achieve competitive results against larger multi-billion parameter models
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;RFMS&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#30340;&#22788;&#29702;&#25968;&#21315;&#20010;&#31867;&#21035;&#30340;&#36229;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#36890;&#36807;&#38182;&#26631;&#36187;&#25490;&#24207;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#36873;&#25321;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#20855;&#26377;&#19982;&#34892;&#19994;&#26631;&#20934;&#30340;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#21644;&#26356;&#22810;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.15793</link><description>&lt;p&gt;
&#29305;&#24449;&#31354;&#38388;&#38477;&#32500;&#26041;&#27861;&#23545;&#20110;&#36229;&#39640;&#32500;&#24230;&#22810;&#31867;&#25968;&#25454;&#30340;&#30740;&#31350;&#65306;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#22810;&#36718;&#31579;&#36873;(RFMS)
&lt;/p&gt;
&lt;p&gt;
Feature space reduction method for ultrahigh-dimensional, multiclass data: Random forest-based multiround screening (RFMS). (arXiv:2305.15793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;RFMS&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#30340;&#22788;&#29702;&#25968;&#21315;&#20010;&#31867;&#21035;&#30340;&#36229;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#36890;&#36807;&#38182;&#26631;&#36187;&#25490;&#24207;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#36873;&#25321;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#20855;&#26377;&#19982;&#34892;&#19994;&#26631;&#20934;&#30340;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#21644;&#26356;&#22810;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#26377;&#22823;&#37327;&#38024;&#23545;&#25317;&#26377;&#25968;&#21313;&#19975;&#29305;&#24449;&#30340;&#36229;&#39640;&#32500;&#24230;&#25968;&#25454;&#30340;&#31579;&#36873;&#26041;&#27861;&#34987;&#21457;&#24067;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#25968;&#21315;&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#12290;&#22522;&#20110;&#22810;&#36890;&#36947;&#29983;&#29289;&#29305;&#24449;&#25968;&#25454;&#39564;&#35777;&#29992;&#25143;&#30340;&#39044;&#27979;&#27169;&#22411;&#20063;&#38754;&#20020;&#30528;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#22810;&#36718;&#31579;&#36873;(RFMS)&#65292;&#21487;&#20197;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#24212;&#29992;&#12290;&#35813;&#31639;&#27861;&#23558;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#20026;&#23567;&#30340;&#23376;&#38598;&#65292;&#24182;&#25191;&#34892;&#19968;&#31995;&#21015;&#37096;&#20998;&#27169;&#22411;&#26500;&#24314;&#12290;&#36825;&#20123;&#37096;&#20998;&#27169;&#22411;&#29992;&#20110;&#23454;&#29616;&#22522;&#20110;&#38182;&#26631;&#36187;&#25490;&#24207;&#21644;&#22522;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#20351;&#29992;&#21512;&#25104;&#29983;&#29289;&#29305;&#24449;&#31354;&#38388;&#29983;&#25104;&#22120;BiometricBlender&#26469;&#23545;RFMS&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#26681;&#25454;&#32467;&#26524;&#65292;RFMS&#19982;&#34892;&#19994;&#26631;&#20934;&#30340;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#21516;&#26102;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, numerous screening methods have been published for ultrahigh-dimensional data that contain hundreds of thousands of features; however, most of these features cannot handle data with thousands of classes. Prediction models built to authenticate users based on multichannel biometric data result in this type of problem. In this study, we present a novel method known as random forest-based multiround screening (RFMS) that can be effectively applied under such circumstances. The proposed algorithm divides the feature space into small subsets and executes a series of partial model builds. These partial models are used to implement tournament-based sorting and the selection of features based on their importance. To benchmark RFMS, a synthetic biometric feature space generator known as BiometricBlender is employed. Based on the results, the RFMS is on par with industry-standard feature screening methods while simultaneously possessing many advantages over these methods.
&lt;/p&gt;</description></item><item><title>IDEA&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#24378;&#39044;&#27979;&#24615;&#21644;&#36328;&#25915;&#20987;&#19981;&#21464;&#24615;&#30340;&#22240;&#26524;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#24418;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#19981;&#21464;&#22240;&#26524;&#38450;&#24481;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15792</link><description>&lt;p&gt;
IDEA&#65306;&#22270;&#24418;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#19981;&#21464;&#22240;&#26524;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
IDEA: Invariant Causal Defense for Graph Adversarial Robustness. (arXiv:2305.15792v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15792
&lt;/p&gt;
&lt;p&gt;
IDEA&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#24378;&#39044;&#27979;&#24615;&#21644;&#36328;&#25915;&#20987;&#19981;&#21464;&#24615;&#30340;&#22240;&#26524;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#24418;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#19981;&#21464;&#22240;&#26524;&#38450;&#24481;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#24341;&#36215;&#20102;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#21487;&#20197;&#25269;&#25239;&#19968;&#20123;&#25915;&#20987;&#65292;&#20294;&#22312;&#20854;&#20182;&#26410;&#30693;&#25915;&#20987;&#19979;&#20250;&#36973;&#21463;&#38590;&#20197;&#25215;&#21463;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#26377;&#38480;&#30340;&#35266;&#23519;&#21040;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#36827;&#34892;&#20248;&#21270;&#65288;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#25110;&#29305;&#23450;&#21551;&#21457;&#24335;&#26469;&#25913;&#21464;&#22270;&#24418;&#25110;&#27169;&#22411;&#32467;&#26500;&#65288;&#22270;&#32431;&#21270;&#25110;&#40065;&#26834;&#32858;&#21512;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#22240;&#26524;&#38450;&#24481;&#26041;&#27861;&#26469;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#65288;IDEA&#65289;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#20855;&#26377;&#24378;&#39044;&#27979;&#26631;&#31614;&#30340;&#22240;&#26524;&#29305;&#24449;&#65292;&#24182;&#19988;&#36328;&#25915;&#20987;&#20855;&#26377;&#19981;&#21464;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#22270;&#24418;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23545;&#22270;&#24418;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#19981;&#21464;&#24615;&#30446;&#26631;&#26469;&#23398;&#20064;&#22240;&#26524;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#23454;&#39564;&#21644;&#32508;&#21512;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;IDEA&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have achieved remarkable success in various tasks, however, their vulnerability to adversarial attacks raises concerns for the real-world applications. Existing defense methods can resist some attacks, but suffer unbearable performance degradation under other unknown attacks. This is due to their reliance on either limited observed adversarial examples to optimize (adversarial training) or specific heuristics to alter graph or model structures (graph purification or robust aggregation). In this paper, we propose an Invariant causal DEfense method against adversarial Attacks (IDEA), providing a new perspective to address this issue. The method aims to learn causal features that possess strong predictability for labels and invariant predictability across attacks, to achieve graph adversarial robustness. Through modeling and analyzing the causal relationships in graph adversarial attacks, we design two invariance objectives to learn the causal features. Extens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#38598;&#21512;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#25110;&#26377;&#38480;&#32500;&#21472;&#21152;&#27867;&#21270;&#27169;&#22411;&#20013;&#36873;&#25321;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#24615;&#33021;&#30340;&#26368;&#20248;&#21472;&#21152;&#27867;&#21270;&#19982;&#26368;&#20248;&#35299;&#24615;&#33021;&#30456;&#36817;&#12290;</title><link>http://arxiv.org/abs/2305.15786</link><description>&lt;p&gt;
&#23398;&#20064;&#38598;&#21512;&#31574;&#30053;&#30340;&#29702;&#35770;&#20445;&#35777;&#21450;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting. (arXiv:2305.15786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#38598;&#21512;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#25110;&#26377;&#38480;&#32500;&#21472;&#21152;&#27867;&#21270;&#27169;&#22411;&#20013;&#36873;&#25321;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#24615;&#33021;&#30340;&#26368;&#20248;&#21472;&#21152;&#27867;&#21270;&#19982;&#26368;&#20248;&#35299;&#24615;&#33021;&#30456;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#23569;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#38024;&#23545;&#40657;&#30418;&#22522;&#23398;&#20064;&#22120;&#30340;&#22823;&#22810;&#25968;&#38598;&#21512;&#26041;&#27861;&#37117;&#23646;&#20110;&#8220;&#21472;&#21152;&#27867;&#21270;&#8221;&#33539;&#30068;&#65292;&#21363;&#35757;&#32451;&#19968;&#20010;&#25509;&#21463;&#22522;&#23398;&#20064;&#22120;&#25512;&#29702;&#20316;&#20026;&#36755;&#20837;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#34429;&#28982;&#21472;&#21152;&#27867;&#21270;&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#29702;&#35770;&#24615;&#36136;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#19968;&#20010;&#26032;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#36873;&#25321;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#24615;&#33021;&#30340;&#8220;&#26377;&#38480;&#25110;&#26377;&#38480;&#32500;&#8221;&#21472;&#21152;&#27867;&#21270;&#20013;&#30340;&#26368;&#20339;&#21472;&#21152;&#27867;&#21270;&#24182;&#19981;&#27604;&#26368;&#20248;&#35299;&#34920;&#29616;&#8220;&#24046;&#24471;&#22810;&#8221;&#12290;&#36825;&#19968;&#32467;&#26524;&#21152;&#24378;&#21644;&#22823;&#22823;&#25193;&#23637;&#20102;Van der Laan&#31561;&#20154;&#65288;2007&#24180;&#65289;&#30340;&#32467;&#26524;&#12290;&#21463;&#21040;&#29702;&#35770;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#27010;&#29575;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#25935;&#24863;&#24615;&#30340;&#21472;&#21152;&#27867;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembling is among the most popular tools in machine learning (ML) due to its effectiveness in minimizing variance and thus improving generalization. Most ensembling methods for black-box base learners fall under the umbrella of "stacked generalization," namely training an ML algorithm that takes the inferences from the base learners as input. While stacking has been widely applied in practice, its theoretical properties are poorly understood. In this paper, we prove a novel result, showing that choosing the best stacked generalization from a (finite or finite-dimensional) family of stacked generalizations based on cross-validated performance does not perform "much worse" than the oracle best. Our result strengthens and significantly extends the results in Van der Laan et al. (2007). Inspired by the theoretical analysis, we further propose a particular family of stacked generalizations in the context of probabilistic forecasting, each one with a different sensitivity for how much the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25968;&#25454;&#22686;&#24378;&#65288;DDAug&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#25928;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#23398;&#20064;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#26377;&#21033;&#22686;&#24378;&#31574;&#30053;&#65292;&#26377;&#25928;&#19988;&#35745;&#31639;&#20195;&#20215;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.15777</link><description>&lt;p&gt;
&#36890;&#36807;MCTS&#36827;&#34892;&#21069;&#21015;&#33146;MRI&#20998;&#21106;&#30340;&#21160;&#24577;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Dynamic Data Augmentation via MCTS for Prostate MRI Segmentation. (arXiv:2305.15777v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25968;&#25454;&#22686;&#24378;&#65288;DDAug&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#25928;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#23398;&#20064;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#26377;&#21033;&#22686;&#24378;&#31574;&#30053;&#65292;&#26377;&#25928;&#19988;&#35745;&#31639;&#20195;&#20215;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36890;&#24120;&#30001;&#20110;&#26114;&#36149;&#30340;&#33719;&#21462;&#21644;&#27880;&#37322;&#36807;&#31243;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#21482;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24456;&#23481;&#26131;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#21508;&#31181;&#36716;&#25442;&#26469;&#22686;&#24378;&#21407;&#22987;&#25968;&#25454;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26032;&#25968;&#25454;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#19968;&#33268;&#30340;&#33719;&#21462;&#26041;&#27861;&#21644;&#25968;&#25454;&#20998;&#24067;&#65292;&#20026;&#19981;&#21516;&#25968;&#25454;&#38598;&#25163;&#21160;&#37197;&#32622;&#36890;&#29992;&#22686;&#24378;&#32452;&#21512;&#21644;&#21442;&#25968;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#26469;&#23398;&#20064;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#26377;&#21033;&#22686;&#24378;&#31574;&#30053;&#65292;&#20294;&#20250;&#20135;&#29983;&#22823;&#37327;GPU&#24320;&#38144;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#25968;&#25454;&#22686;&#24378;&#65288;DDAug&#65289;&#65292;&#35813;&#26041;&#27861;&#39640;&#25928;&#19988;&#35745;&#31639;&#20195;&#20215;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#30340;DDAug&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#32467;&#26500;&#26469;&#34920;&#31034;&#21508;&#31181;&#22686;&#24378;&#65292;&#24182;&#21033;&#29992;&#39640;&#25928;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#26356;&#26032;&#12289;&#20462;&#21098;&#21644;&#25277;&#26679;&#26641;&#12290;&#22240;&#27492;&#65292;
&lt;/p&gt;
&lt;p&gt;
Medical image data are often limited due to the expensive acquisition and annotation process. Hence, training a deep-learning model with only raw data can easily lead to overfitting. One solution to this problem is to augment the raw data with various transformations, improving the model's ability to generalize to new data. However, manually configuring a generic augmentation combination and parameters for different datasets is non-trivial due to inconsistent acquisition approaches and data distributions. Therefore, automatic data augmentation is proposed to learn favorable augmentation strategies for different datasets while incurring large GPU overhead. To this end, we present a novel method, called Dynamic Data Augmentation (DDAug), which is efficient and has negligible computation cost. Our DDAug develops a hierarchical tree structure to represent various augmentations and utilizes an efficient Monte-Carlo tree searching algorithm to update, prune, and sample the tree. As a result,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;AUC&#20248;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#21644;&#29702;&#35770;&#19978;&#37117;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.15776</link><description>&lt;p&gt;
&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;AUC&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
AUC Optimization from Multiple Unlabeled Datasets. (arXiv:2305.15776v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;AUC&#20248;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#21644;&#29702;&#35770;&#19978;&#37117;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#22312;&#32570;&#20047;&#23436;&#32654;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36171;&#20104;&#26426;&#22120;&#23398;&#20064;&#33021;&#21147;&#65292;&#36825;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26696;&#20363;&#20043;&#19968;&#26159;&#20165;&#20102;&#35299;&#31867;&#21035;&#20808;&#39564;&#30693;&#35782;&#30340;&#22810;&#20010;&#26410;&#26631;&#35760;(U)&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#25110;&#31216;&#20026;U^m&#23398;&#20064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#25104;&#23545;&#25490;&#21517;&#33021;&#21147;&#30340;AUC (ROC&#26354;&#32447;&#19979;&#38754;&#31215;) &#20248;&#21270;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;U^m-AUC&#65292;&#19968;&#31181;&#23558;U^m&#25968;&#25454;&#36716;&#25442;&#20026;&#22810;&#26631;&#35760;AUC&#20248;&#21270;&#38382;&#39064;&#24182;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#30340;AUC&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;U^m-AUC&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning aims to empower machine learning when the perfect supervision is unavailable, which has drawn great attention from researchers. Among various types of weak supervision, one of the most challenging cases is to learn from multiple unlabeled (U) datasets with only a little knowledge of the class priors, or U$^m$ learning for short. In this paper, we study the problem of building an AUC (area under ROC curve) optimization model from multiple unlabeled datasets, which maximizes the pairwise ranking ability of the classifier. We propose U$^m$-AUC, an AUC optimization approach that converts the U$^m$ data into a multi-label AUC optimization problem, and can be trained efficiently. We show that the proposed U$^m$-AUC is effective theoretically and empirically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#23398;&#20064;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#27010;&#24565;&#30340;Transformer&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15775</link><description>&lt;p&gt;
&#20197;&#27010;&#24565;&#20026;&#20013;&#24515;&#30340;Transformer&#65306;&#20855;&#26377;&#38754;&#21521;&#29289;&#20307;&#30340;&#27010;&#24565;&#23398;&#20064;&#65292;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability. (arXiv:2305.15775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#23398;&#20064;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#27010;&#24565;&#30340;Transformer&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#22823;&#22823;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35270;&#35273;&#12289;NLP&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#20102;&#24037;&#20855;&#26469;&#24110;&#21161;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#27010;&#24565;Transformer&#65288;CT&#65289;&#23558;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#20302;&#32423;&#36755;&#20837;&#29305;&#24449;&#27867;&#21270;&#21040;&#26356;&#25277;&#35937;&#30340;&#20013;&#38388;&#23618;&#28508;&#22312;&#27010;&#24565;&#65292;&#26356;&#22909;&#22320;&#20801;&#35768;&#20154;&#31867;&#20998;&#26512;&#21592;&#30452;&#25509;&#35780;&#20272;&#35299;&#37322;&#20851;&#20110;&#20219;&#20309;&#29305;&#23450;&#36755;&#20986;&#20998;&#31867;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;CT&#37319;&#29992;&#30340;&#27010;&#24565;&#23398;&#20064;&#40664;&#35748;&#20551;&#35774;&#31867;&#21035;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#37117;&#23545;&#34920;&#24449;&#35813;&#31867;&#21035;&#30340;&#27010;&#24565;&#20316;&#20986;&#20102;&#30456;&#21516;&#30340;&#36129;&#29486;&#65292;&#32780;&#20351;&#29992;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms have greatly improved the performance of deep-learning models on visual, NLP, and multimodal tasks while also providing tools to aid in the model's interpretability. In particular, attention scores over input regions or concrete image features can be used to measure how much the attended elements contribute to the model inference. The recently proposed Concept Transformer (CT) generalizes the Transformer attention mechanism from such low-level input features to more abstract, intermediate-level latent concepts that better allow human analysts to more directly assess an explanation for the reasoning of the model about any particular output classification. However, the concept learning employed by CT implicitly assumes that across every image in a class, each image patch makes the same contribution to concepts that characterize membership in that class. Instead of using the CT's image-patch-centric concepts, object-centric concepts could lead to better classification
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#23398;&#20064;&#30340;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#23398;&#20064;&#20013;&#22686;&#24378;&#24863;&#21463;&#37326;&#21644;&#34701;&#21512;&#19981;&#21516;&#23610;&#24230;&#29305;&#24449;&#30340;&#20248;&#28857;&#65292;&#20351;&#29992;&#20102;&#20613;&#37324;&#21494;&#21464;&#25442;&#12289;&#22855;&#24322;&#20540;&#20998;&#35299;&#12289;&#30697;&#38453;&#20056;&#27861;&#21644;&#21367;&#31215;&#22359;&#31561;&#22235;&#31181;&#19981;&#21516;&#30340;&#21464;&#25442;&#26426;&#21046;&#36827;&#34892;&#26500;&#24314;&#65292;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.15770</link><description>&lt;p&gt;
TLNets: &#22522;&#20110;&#21464;&#25442;&#23398;&#20064;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TLNets: Transformation Learning Networks for long-range time-series prediction. (arXiv:2305.15770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15770
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#23398;&#20064;&#30340;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#23398;&#20064;&#20013;&#22686;&#24378;&#24863;&#21463;&#37326;&#21644;&#34701;&#21512;&#19981;&#21516;&#23610;&#24230;&#29305;&#24449;&#30340;&#20248;&#28857;&#65292;&#20351;&#29992;&#20102;&#20613;&#37324;&#21494;&#21464;&#25442;&#12289;&#22855;&#24322;&#20540;&#20998;&#35299;&#12289;&#30697;&#38453;&#20056;&#27861;&#21644;&#21367;&#31215;&#22359;&#31561;&#22235;&#31181;&#19981;&#21516;&#30340;&#21464;&#25442;&#26426;&#21046;&#36827;&#34892;&#26500;&#24314;&#65292;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#35768;&#22810;&#23398;&#31185;&#39046;&#22495;&#37324;&#30340;&#19968;&#20010;&#24191;&#27867;&#38382;&#39064;&#65292;&#22914;&#27668;&#35937;&#23398;&#12289;&#20132;&#36890;&#30417;&#27979;&#12289;&#25237;&#36164;&#12289;&#33021;&#28304;&#29983;&#20135;&#21644;&#28040;&#36153;&#31561;&#12290;&#35768;&#22810;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#24050;&#32463;&#34987;&#24320;&#21457;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#35201;&#20040;&#24403;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#22686;&#21152;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#30340;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#26041;&#26696;&#65292;&#20855;&#26377;&#28508;&#21147;&#22312;&#23398;&#20064;&#20013;&#23454;&#29616;&#22686;&#24378;&#30340;&#24863;&#21463;&#37326;&#65292;&#24182;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#34701;&#21512;&#29305;&#24449;&#30340;&#22909;&#22788;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#21464;&#25442;&#26426;&#21046;&#20316;&#20026;&#26500;&#24314;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#30784;&#65292;&#21253;&#25324;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FT&#65289;&#12289;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#12289;&#30697;&#38453;&#20056;&#27861;&#21644;&#21367;&#31215;&#22359;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#20197;&#19978;&#26500;&#24314;&#22359;&#24320;&#21457;&#20102;&#22235;&#20010;&#23398;&#20064;&#27169;&#22411;&#65292;&#20998;&#21035;&#20026;FT-Matrix&#12289;FT-SVD&#12289;FT-Conv&#21644;Conv-SVD&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;FT&#21644;SVD&#27169;&#22411;&#20855;&#26377;&#25552;&#21462;&#20302;&#32500;&#34920;&#31034;&#20197;&#22686;&#24378;&#23398;&#20064;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#23454;&#38469;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#38271;&#26399;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series prediction is a prevalent issue across various disciplines, such as meteorology, traffic surveillance, investment, and energy production and consumption. Many statistical and machine-learning strategies have been developed to tackle this problem. However, these approaches either lack explainability or exhibit less satisfactory performance when the prediction horizon increases. To this end, we propose a novel plan for the designing of networks' architecture based on transformations, possessing the potential to achieve an enhanced receptive field in learning which brings benefits to fuse features across scales. In this context, we introduce four different transformation mechanisms as bases to construct the learning model including Fourier Transform (FT), Singular Value Decomposition (SVD), matrix multiplication and Conv block. Hence, we develop four learning models based on the above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD. Note that the FT and SVD b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.15759</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Latent Diffusion Models. (arXiv:2305.15759v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;(DMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30452;&#25509;&#22312;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;DMs&#30340;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#36825;&#23548;&#33268;&#30001;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#32452;&#21512;&#24615;&#23646;&#24615;&#65292;&#22823;&#37327;&#22122;&#38899;&#27880;&#20837;&#21040;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#12290;LDMs&#20351;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20943;&#23569;&#21040;&#26356;&#20302;&#32500;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#35757;&#32451;DMs&#26356;&#21152;&#39640;&#25928;&#21644;&#24555;&#36895;&#12290;&#19982;[Ghalebikesabi&#31561;&#20154;&#65292;2023]&#39044;&#20808;&#29992;&#20844;&#20849;&#25968;&#25454;&#39044;&#35757;&#32451;DMs&#65292;&#28982;&#21518;&#20877;&#29992;&#38544;&#31169;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#19981;&#21516;&#65292;&#25105;&#20204;&#20165;&#24494;&#35843;LDMs&#20013;&#19981;&#21516;&#23618;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#20197;&#33719;&#24471;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#25972;&#20010;DM&#24494;&#35843;&#65292;&#21487;&#20943;&#23569;&#22823;&#32422;96%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) are widely used for generating high-quality image datasets. However, since they operate directly in the high-dimensional pixel space, optimization of DMs is computationally expensive, requiring long training times. This contributes to large amounts of noise being injected into the differentially private learning process, due to the composability property of differential privacy. To address this challenge, we propose training Latent Diffusion Models (LDMs) with differential privacy. LDMs use powerful pre-trained autoencoders to reduce the high-dimensional pixel space to a much lower-dimensional latent space, making training DMs more efficient and fast. Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then fine-tunes them with private data, we fine-tune only the attention modules of LDMs at varying layers with privacy-sensitive data, reducing the number of trainable parameters by approximately 96% compared to fine-tuning the entire DM. We te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;UnionSNN&#65292;&#27880;&#20837;&#20102;&#37051;&#23621;&#36830;&#25509;&#20449;&#24687;&#65292;&#36890;&#36807;&#32852;&#21512;&#23376;&#22270;&#26469;&#32534;&#30721;&#39640;&#38454;&#36830;&#25509;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;1-WL&#21644;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;GNN&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.15747</link><description>&lt;p&gt;
Union Subgraph&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Union Subgraph Neural Networks. (arXiv:2305.15747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;UnionSNN&#65292;&#27880;&#20837;&#20102;&#37051;&#23621;&#36830;&#25509;&#20449;&#24687;&#65292;&#36890;&#36807;&#32852;&#21512;&#23376;&#22270;&#26469;&#32534;&#30721;&#39640;&#38454;&#36830;&#25509;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;1-WL&#21644;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#34987;&#24191;&#27867;&#29992;&#20110;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#30001;&#20110;&#23427;&#20204;&#36890;&#36807;&#36845;&#20195;&#20256;&#36882;&#28040;&#24687;&#26469;&#22788;&#29702;&#26377;&#26681;&#23376;&#26641;&#65292;&#22240;&#27492;&#26222;&#36890;&#30340;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#19978;&#38480;&#20026;1&#32500;Weisfeiler-Leman(1-WL)&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27880;&#20837;&#20174;&#26032;&#31867;&#22411;&#30340;&#23376;&#32467;&#26500;&#20013;&#25552;&#21462;&#30340;&#37051;&#23621;&#36830;&#25509;&#20449;&#24687;&#26469;&#22686;&#24378;GNNs&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#23616;&#37096;&#37051;&#22495;&#20013;&#23384;&#22312;&#30340;&#19981;&#21516;&#36830;&#25509;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#31216;&#20026;&#32852;&#21512;&#23376;&#22270;&#30340;&#23376;&#32467;&#26500;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#21040;&#19968;&#26465;&#36793;&#30340;1-&#36339;&#37051;&#23621;&#30340;&#23436;&#25972;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#30701;&#36335;&#24452;&#30340;&#23376;&#32467;&#26500;&#25551;&#36848;&#31526;&#65292;&#20855;&#26377;&#19977;&#20010;&#33391;&#22909;&#30340;&#24615;&#36136;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;&#32852;&#21512;&#23376;&#22270;&#20013;&#30340;&#39640;&#38454;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#27880;&#20837;&#32534;&#30721;&#37051;&#23621;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21363;Union Subgraph&#31070;&#32463;&#32593;&#32476;(UnionSNN)&#65292;&#23427;&#34987;&#35777;&#26126;&#22312;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#26041;&#38754;&#27604;1-WL&#20005;&#26684;&#26356;&#24378;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;UnionSNN&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are widely used for graph representation learning in many application domains. The expressiveness of vanilla GNNs is upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) test as they operate on rooted subtrees through iterative message passing. In this paper, we empower GNNs by injecting neighbor-connectivity information extracted from a new type of substructure. We first investigate different kinds of connectivities existing in a local neighborhood and identify a substructure called union subgraph, which is able to capture the complete picture of the 1-hop neighborhood of an edge. We then design a shortest-path-based substructure descriptor that possesses three nice properties and can effectively encode the high-order connectivities in union subgraphs. By infusing the encoded neighbor connectivities, we propose a novel model, namely Union Subgraph Neural Network (UnionSNN), which is proven to be strictly more powerful than 1-WL in distinguishing non-isom
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#28595;&#22823;&#21033;&#20122;&#26089;&#26399;&#21457;&#23637;&#26222;&#26597;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#20986;&#24109;&#24188;&#20799;&#22253;&#19982;&#20799;&#31461;&#21457;&#23637;&#33030;&#24369;&#24615;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22312;&#20986;&#24109;&#24188;&#20799;&#22253;&#27604;&#20363;&#36739;&#39640;&#30340;&#22320;&#21306;&#65292;&#20799;&#31461;&#33267;&#23569;&#23384;&#22312;&#19968;&#20010;&#21457;&#23637;&#24615;&#33030;&#24369;&#24615;&#30340;&#27604;&#20363;&#36739;&#20302;&#12290;&#20351;&#29992;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#21592;&#30830;&#23450;&#20102;&#26118;&#22763;&#20848;&#30465;&#20869;&#30340;&#19977;&#20010;&#19981;&#21516;&#32676;&#38598;&#65292;&#27599;&#20010;&#32676;&#38598;&#29305;&#28857;&#26159;&#19981;&#21516;&#30340;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#21464;&#37327;&#24433;&#21709;&#24188;&#20799;&#22253;&#20986;&#24109;&#19982;&#21457;&#23637;&#33030;&#24369;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.15746</link><description>&lt;p&gt;
&#35780;&#20272;&#28595;&#22823;&#21033;&#20122;&#26118;&#22763;&#20848;&#30465;&#20986;&#24109;&#24188;&#20799;&#22253;&#19982;&#20799;&#31461;&#21457;&#23637;&#33030;&#24369;&#24615;&#30340;&#31354;&#38388;&#32467;&#26500;&#65288;arXiv:2305.15746v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
Assessing the Spatial Structure of the Association between Attendance at Preschool and Childrens Developmental Vulnerabilities in Queensland Australia. (arXiv:2305.15746v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15746
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#28595;&#22823;&#21033;&#20122;&#26089;&#26399;&#21457;&#23637;&#26222;&#26597;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#20986;&#24109;&#24188;&#20799;&#22253;&#19982;&#20799;&#31461;&#21457;&#23637;&#33030;&#24369;&#24615;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22312;&#20986;&#24109;&#24188;&#20799;&#22253;&#27604;&#20363;&#36739;&#39640;&#30340;&#22320;&#21306;&#65292;&#20799;&#31461;&#33267;&#23569;&#23384;&#22312;&#19968;&#20010;&#21457;&#23637;&#24615;&#33030;&#24369;&#24615;&#30340;&#27604;&#20363;&#36739;&#20302;&#12290;&#20351;&#29992;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#21592;&#30830;&#23450;&#20102;&#26118;&#22763;&#20848;&#30465;&#20869;&#30340;&#19977;&#20010;&#19981;&#21516;&#32676;&#38598;&#65292;&#27599;&#20010;&#32676;&#38598;&#29305;&#28857;&#26159;&#19981;&#21516;&#30340;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#21464;&#37327;&#24433;&#21709;&#24188;&#20799;&#22253;&#20986;&#24109;&#19982;&#21457;&#23637;&#33030;&#24369;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20986;&#24109;&#24188;&#20799;&#22253;&#65288;&#20840;&#26085;&#21046;&#23398;&#26657;&#21069;&#19968;&#24180;&#65289;&#23545;&#20799;&#31461;&#22312;&#20854;&#31532;&#19968;&#24180;&#23398;&#26657;&#26399;&#38388;&#21457;&#23637;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#28595;&#22823;&#21033;&#20122;&#26089;&#26399;&#21457;&#23637;&#26222;&#26597;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#21457;&#29616;&#20986;&#24109;&#24188;&#20799;&#22253;&#27604;&#20363;&#36739;&#39640;&#30340;&#22320;&#21306;&#24448;&#24448;&#26377;&#36739;&#20302;&#27604;&#20363;&#30340;&#20799;&#31461;&#33267;&#23569;&#23384;&#22312;&#19968;&#20010;&#21457;&#23637;&#24615;&#33030;&#24369;&#24615;&#12290;&#21457;&#23637;&#24615;&#33030;&#24369;&#24615;&#21253;&#25324;&#19981;&#33021;&#24212;&#23545;&#25972;&#20010;&#23398;&#26657;&#26085;&#65288;&#30130;&#24811;&#12289;&#39269;&#39295;&#12289;&#31934;&#21147;&#20302;&#19979;&#65289;&#12289;&#19981;&#33021;&#19982;&#20182;&#20154;&#30456;&#22788;&#25110;&#32773;&#20855;&#26377;&#25915;&#20987;&#24615;&#65292;&#20197;&#21450;&#38405;&#35835;/&#20889;&#20316;&#25110;&#25968;&#23398;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#20123;&#21457;&#29616;&#24403;&#28982;&#20250;&#22240;&#22320;&#21306;&#32780;&#24322;&#12290;&#20351;&#29992;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#35782;&#21035;&#20986;&#26118;&#22763;&#20848;&#30465;&#20869;&#30340;&#19977;&#20010;&#19981;&#21516;&#30340;&#32676;&#38598;&#65292;&#27599;&#20010;&#32676;&#38598;&#30340;&#29305;&#28857;&#26159;&#19981;&#21516;&#30340;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#21464;&#37327;&#24433;&#21709;&#24188;&#20799;&#22253;&#20986;&#24109;&#19982;&#21457;&#23637;&#33030;&#24369;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20123;&#20998;&#26512;&#26377;&#21161;&#20110;&#29702;&#35299;&#39640;&#33030;&#24369;&#24615;&#22320;&#21306;&#21644;&#35813;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#20851;&#22914;&#20309;&#22312;&#36825;&#20123;&#21306;&#22495;&#24212;&#29992;&#36164;&#28304;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research explores the influence of preschool attendance (one year before full-time school) on the development of children during their first year of school. Using data collected by the Australian Early Development Census, the findings show that areas with high proportions of preschool attendance tended to have lower proportions of children with at least one developmental vulnerability. Developmental vulnerablities include not being able to cope with the school day (tired, hungry, low energy), unable to get along with others or aggressive behaviour, trouble with reading/writing or numbers. These findings, of course, vary by region. Using Data Analysis and Machine Learning, the researchers were able to identify three distinct clusters within Queensland, each characterised by different socio-demographic variables influencing the relationship between preschool attendance and developmental vulnerability. These analyses contribute to understanding regions with high vulnerability and the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;RAGE&#30340;&#28789;&#27963;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#21457;&#29616;&#22270;&#32593;&#32476;&#32467;&#26500;&#20013;&#30340;&#35299;&#37322;&#12290;&#35813;&#35299;&#37322;&#22120;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;GNN&#26550;&#26500;&#21644;&#22270;&#24418;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#20855;&#26377;&#36275;&#22815;&#20449;&#24687;&#20197;&#20351;&#20154;&#31867;&#39044;&#27979;&#22797;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15745</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#40065;&#26834;&#22411;&#21069;&#24207;&#22270;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Robust Ante-hoc Graph Explainer using Bilevel Optimization. (arXiv:2305.15745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;RAGE&#30340;&#28789;&#27963;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#21457;&#29616;&#22270;&#32593;&#32476;&#32467;&#26500;&#20013;&#30340;&#35299;&#37322;&#12290;&#35813;&#35299;&#37322;&#22120;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;GNN&#26550;&#26500;&#21644;&#22270;&#24418;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#20855;&#26377;&#36275;&#22815;&#20449;&#24687;&#20197;&#20351;&#20154;&#31867;&#39044;&#27979;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20915;&#31574;&#30340;&#35828;&#26126;&#23545;&#20110;&#22686;&#21152;&#36879;&#26126;&#24230;&#21644;&#25351;&#23548;&#20915;&#31574;&#25913;&#36827;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#22312;&#22270;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#65292;&#22240;&#20026;&#20915;&#31574;&#24448;&#24448;&#21462;&#20915;&#20110;&#32467;&#26500;&#21644;&#23646;&#24615;&#25968;&#25454;&#30340;&#32508;&#21512;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#25152;&#35859;&#30340;&#21518;&#24207;&#35299;&#37322;&#22120;&#19978;&#65292;&#30456;&#24212;&#30340;&#22909;&#30340;&#35299;&#37322;&#30340;&#26631;&#20934;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#65292;&#28789;&#27963;&#30340;&#40065;&#26834;&#22411;&#21069;&#24207;&#35299;&#37322;&#22120;&#65292;&#21517;&#20026;RAGE&#65288;Robust Ante-hoc Graph Explainer&#65289;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26469;&#21457;&#29616;&#19968;&#31867;&#24191;&#27867;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#65292;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;GNN&#26550;&#26500;&#21644;&#22270;&#24418;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#36275;&#22815;&#20449;&#24687;&#65292;&#20197;&#21551;&#29992;&#20154;&#31867;&#39044;&#27979;&#22797;&#21046;&#12290;&#23454;&#39564;&#34920;&#26126;RAGE&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#21457;&#29616;&#24544;&#23454;&#21644;&#40065;&#26834;&#35299;&#37322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining the decisions made by machine learning models for high-stakes applications is critical for increasing transparency and guiding improvements to these decisions. This is particularly true in the case of models for graphs, where decisions often depend on complex patterns combining rich structural and attribute data. While recent work has focused on designing so-called post-hoc explainers, the question of what constitutes a good explanation remains open. One intuitive property is that explanations should be sufficiently informative to enable humans to approximately reproduce the predictions given the data. However, we show that post-hoc explanations do not achieve this goal as their explanations are highly dependent on fixed model parameters (e.g., learned GNN weights). To address this challenge, this paper proposes RAGE (Robust Ante-hoc Graph Explainer), a novel and flexible ante-hoc explainer designed to discover explanations for a broad class of graph neural networks using bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#21464;&#37327;&#22788;&#29702;&#24773;&#20917;&#19979;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25512;&#26029;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.15742</link><description>&lt;p&gt;
&#26102;&#38388;&#21464;&#21270;&#22788;&#29702;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Generative Models for Time-Varying Treatments. (arXiv:2305.15742v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#21464;&#37327;&#22788;&#29702;&#24773;&#20917;&#19979;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25512;&#26029;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#26159;&#27979;&#35797;&#26032;&#30103;&#27861;&#30340;&#24120;&#29992;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#25928;&#24212;&#20250;&#25513;&#30422;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#37325;&#35201;&#30340;&#20010;&#20307;&#29305;&#24449;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26102;&#38388;&#35774;&#32622;&#20013;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#22788;&#29702;&#26159;&#26102;&#24207;&#30340;&#21644;&#26102;&#21464;&#30340;&#65292;&#23545;&#21453;&#20107;&#23454;&#20998;&#24067;&#20135;&#29983;&#20102;&#38169;&#32508;&#22797;&#26434;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#25429;&#33719;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#20801;&#35768;&#23545;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;&#36825;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23588;&#20854;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#36890;&#36807;&#36793;&#38469;&#32467;&#26500;&#27169;&#22411;&#35880;&#24910;&#22320;&#35299;&#20915;&#20102;&#35266;&#23519;&#25968;&#25454;&#21644;&#30446;&#26631;&#21453;&#20107;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating average causal effects is a common practice to test new treatments. However, the average effect ''masks'' important individual characteristics in the counterfactual distribution, which may lead to safety, fairness, and ethical concerns. This issue is exacerbated in the temporal setting, where the treatment is sequential and time-varying, leading to an intricate influence on the counterfactual distribution. In this paper, we propose a novel conditional generative modeling approach to capture the whole counterfactual distribution, allowing efficient inference on certain statistics of the counterfactual distribution. This makes the proposed approach particularly suitable for healthcare and public policy making. Our generative modeling approach carefully tackles the distribution mismatch in the observed data and the targeted counterfactual distribution via a marginal structural model. Our method outperforms state-of-the-art baselines on both synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#30693;&#35782;&#33976;&#39311;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#24402;&#22240;&#20110;&#20174;&#32769;&#24072;&#27169;&#22411;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#30340;&#31867;&#30456;&#20284;&#20449;&#24687;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.15734</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#33976;&#39311;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#19978;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Knowledge Distillation for Model Interpretability. (arXiv:2305.15734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#30693;&#35782;&#33976;&#39311;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#24402;&#22240;&#20110;&#20174;&#32769;&#24072;&#27169;&#22411;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#30340;&#31867;&#30456;&#20284;&#20449;&#24687;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30693;&#35782;&#33976;&#39311;(KD)&#20026;&#20309;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24050;&#34987;&#38416;&#26126;&#65292;&#20294;&#26159;&#24456;&#23569;&#26377;&#20154;&#30740;&#31350;&#38500;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#22806;KD&#30340;&#20854;&#20182;&#20248;&#28857;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35777;&#26126;KD&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32593;&#32476;&#20998;&#35299;&#31639;&#27861;&#20013;&#26816;&#27979;&#30340;&#27010;&#24565;&#26816;&#27979;&#22120;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#23450;&#37327;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#21487;&#35299;&#37322;&#24615;&#30340;&#25552;&#39640;&#24402;&#22240;&#20110;&#20174;&#32769;&#24072;&#27169;&#22411;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#30340;&#31867;&#30456;&#20284;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#36923;&#36753;&#22238;&#24402;&#33976;&#39311;&#30830;&#35748;&#20102;&#31867;&#30456;&#20284;&#20449;&#24687;&#30340;&#20256;&#36882;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#31867;&#30456;&#20284;&#20449;&#24687;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#20854;&#23384;&#22312;&#25110;&#32570;&#22833;&#20197;&#21450;&#30456;&#20284;&#20449;&#24687;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#19981;&#21516;&#30340;KD&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#19978;&#26816;&#26597;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#25968;&#25454;&#25345;&#26377;&#32773;&#20043;&#38388;&#30340;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#19978;&#20063;&#36827;&#34892;&#20102;&#23454;&#38469;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.15723</link><description>&lt;p&gt;
&#21033;&#29992;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#25968;&#25454;&#25345;&#26377;&#32773;&#20043;&#38388;&#30340;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning across Data Owners with Joint Differential Privacy. (arXiv:2305.15723v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#25968;&#25454;&#25345;&#26377;&#32773;&#20043;&#38388;&#30340;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#19978;&#20063;&#36827;&#34892;&#20102;&#23454;&#38469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#25345;&#26377;&#32773;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22330;&#26223;&#19979;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#22312;&#36825;&#31181;&#22330;&#26223;&#19979;&#65292;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#25345;&#26377;&#32773;$j$&#65292;&#20351;&#29992;$j$&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#26102;&#19981;&#32771;&#34385;&#38544;&#31169;&#38382;&#39064;&#65292;&#32780;&#20351;&#29992;&#20854;&#20182;&#25345;&#26377;&#32773;&#30340;&#25968;&#25454;&#21017;&#20250;&#25552;&#20379;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#38024;&#23545;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#38024;&#23545;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the setting in which data owners train machine learning models collaboratively under a privacy notion called joint differential privacy [Kearns et al., 2018]. In this setting, the model trained for each data owner $j$ uses $j$'s data without privacy consideration and other owners' data with differential privacy guarantees. This setting was initiated in [Jain et al., 2021] with a focus on linear regressions. In this paper, we study this setting for stochastic convex optimization (SCO). We present an algorithm that is a variant of DP-SGD [Song et al., 2013; Abadi et al., 2016] and provides theoretical bounds on its population loss. We compare our algorithm to several baselines and discuss for what parameter setups our algorithm is more preferred. We also empirically study joint differential privacy in the multi-class classification problem over two public datasets. Our empirical findings are well-connected to the insights from our theoretical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15722</link><description>&lt;p&gt;
&#38754;&#21521;&#20195;&#30721;&#28151;&#21512;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data. (arXiv:2305.15722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#20195;&#30721;&#28151;&#21512;&#8221;&#26159;&#25351;&#22312;&#21516;&#19968;&#27573;&#25991;&#26412;&#20013;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#30340;&#29616;&#35937;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#23384;&#22312;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#32435;&#12290;&#26816;&#27979;&#35821;&#35328;&#20013;&#30340;&#22806;&#26469;&#20803;&#32032;&#24182;&#27491;&#30830;&#22788;&#29702;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#35768;&#22810;&#20154;&#20351;&#29992;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#65292;&#20854;&#20013;&#20219;&#19968;&#35821;&#35328;&#37117;&#26080;&#27861;&#29702;&#35299;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20302;&#36164;&#28304;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#65292;&#24182;&#25552;&#39640;&#19981;&#21516;&#20195;&#30721;&#28151;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#39044;&#35757;&#32451;&#30340;&#19981;&#21516;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#20195;&#30721;&#28151;&#21512;&#27169;&#22411;&#65288;&#22914;HingBERT&#12289;HingRoBERTa&#12289;HingRoBERTa-Mixed&#12289;mBERT&#65289;&#21644;&#38750;&#20195;&#30721;&#28151;&#21512;&#27169;&#22411;&#65288;&#22914;AlBERT&#12289;BERT&#12289;RoBERTa&#65289;&#65292;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term "Code Mixed" refers to the use of more than one language in the same text. This phenomenon is predominantly observed on social media platforms, with an increasing amount of adaptation as time goes on. It is critical to detect foreign elements in a language and process them correctly, as a considerable number of individuals are using code-mixed languages that could not be comprehended by understanding one of those languages. In this work, we focus on low-resource Hindi-English code-mixed language and enhancing the performance of different code-mixed natural language processing tasks such as sentiment analysis, emotion recognition, and hate speech identification. We perform a comparative analysis of different Transformer-based language Models pre-trained using unsupervised approaches. We have included the code-mixed models like HingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like AlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-En
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeLoDy&#30340;&#39640;&#25928;&#31070;&#32463;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#23454;&#26102;&#12289;&#21333;CPU&#29615;&#22659;&#19979;&#29983;&#25104;&#21508;&#31181;&#39118;&#26684;&#21644;&#38271;&#24230;&#30340;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.15719</link><description>&lt;p&gt;
&#39640;&#25928;&#31070;&#32463;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Neural Music Generation. (arXiv:2305.15719v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15719
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeLoDy&#30340;&#39640;&#25928;&#31070;&#32463;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#23454;&#26102;&#12289;&#21333;CPU&#29615;&#22659;&#19979;&#29983;&#25104;&#21508;&#31181;&#39118;&#26684;&#21644;&#38271;&#24230;&#30340;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20808;&#36827;&#30340;MusicLM&#65292;&#38899;&#20048;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;MusicLM&#30001;&#19977;&#20010;LM&#30340;&#23618;&#27425;&#32467;&#26500;&#32452;&#25104;&#65292;&#20998;&#21035;&#29992;&#20110;&#35821;&#20041;&#12289;&#31895;&#30053;&#22768;&#23398;&#21644;&#32454;&#33410;&#22768;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;MusicLM&#36827;&#34892;&#37319;&#26679;&#38656;&#35201;&#36880;&#20010;&#22788;&#29702;&#36825;&#20123;LM&#20197;&#33719;&#24471;&#32454;&#31890;&#24230;&#30340;&#22768;&#23398;&#26631;&#35760;&#65292;&#36825;&#20351;&#24471;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#19981;&#36866;&#29992;&#20110;&#23454;&#26102;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MeLoDy&#65288;M&#20195;&#34920;&#38899;&#20048;&#65307;L&#20195;&#34920;LM&#65307;D&#20195;&#34920;&#25193;&#25955;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;LM&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#38899;&#39057;&#65292;&#21516;&#26102;&#22312;&#37319;&#26679;10&#31186;&#25110;30&#31186;&#38899;&#20048;&#26102;&#65292;&#20998;&#21035;&#20943;&#23569;&#20102;MusicLM&#20013;&#30340;95.7%&#25110;99.6%&#30340;&#21069;&#21521;&#20256;&#36882;&#12290;MeLoDy&#32487;&#25215;&#20102;MusicLM&#30340;&#35821;&#20041;&#24314;&#27169;&#30340;&#26368;&#39640;&#32423;&#21035;&#30340;LM&#65292;&#24182;&#24212;&#29992;&#20102;&#26032;&#39062;&#30340;&#21452;&#36335;&#24452;&#25193;&#25955;(DPD)&#27169;&#22411;&#21644;&#38899;&#39057;VAE-GAN&#26469;&#39640;&#25928;&#22320;&#23558;&#26465;&#20214;&#35821;&#20041;&#26631;&#35760;&#35299;&#30721;&#20026;&#27874;&#24418;&#12290;DPD&#26159;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#27839;&#30528;&#30830;&#23450;&#24615;&#36335;&#24452;&#20256;&#25773;&#39640;&#32423;&#21035;&#35821;&#20041;&#20449;&#24687;&#65292;&#27839;&#30528;&#38543;&#26426;&#36335;&#24452;&#20256;&#25773;&#20302;&#32423;&#22768;&#23398;&#32454;&#33410;&#12290;&#32463;&#39564;&#19978;&#65292;MeLoDy&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#19978;&#23454;&#26102;&#29983;&#25104;&#21508;&#31181;&#39118;&#26684;&#21644;&#38271;&#24230;&#30340;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#24314;&#27169;&#21333;&#27169;&#24577;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;VAE&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15708</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#22810;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Score-Based Multimodal Autoencoders. (arXiv:2305.15708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#24314;&#27169;&#21333;&#27169;&#24577;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;VAE&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31867;&#33021;&#22815;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#26500;&#24314;&#21487;&#22788;&#29702;&#21518;&#39564;&#30340;&#26377;&#21069;&#36884;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;&#20294;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27599;&#19968;&#20010;&#27169;&#24577;&#30340;&#29983;&#25104;&#36136;&#37327;&#37117;&#20250;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#21333;&#27169;&#24577;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20197;&#22686;&#24378;&#22810;&#27169;&#24577;VAE&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#20998;&#25968;&#27169;&#22411;&#30340;&#20316;&#29992;&#26159;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#21333;&#27169;&#24577;VAE&#21331;&#36234;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#23545;&#19981;&#21516;&#27169;&#24577;&#30340;&#19968;&#33268;&#24615;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Variational Autoencoders (VAEs) represent a promising group of generative models that facilitate the construction of a tractable posterior within the latent space, given multiple modalities. Daunhawer et al. (2022) demonstrate that as the number of modalities increases, the generative quality of each modality declines. In this study, we explore an alternative approach to enhance the generative performance of multimodal VAEs by jointly modeling the latent space of unimodal VAEs using score-based models (SBMs). The role of the SBM is to enforce multimodal coherence by learning the correlation among the latent variables. Consequently, our model combines the superior generative quality of unimodal VAEs with coherent integration across different modalities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30456;&#20284;&#24615;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861; pFedSim&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#30456;&#20284;&#24615;&#32858;&#21512;&#21644;&#27169;&#22411;&#35299;&#32806;&#20004;&#31181;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23558; NN &#27169;&#22411;&#35299;&#32806;&#20026;&#20010;&#24615;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#26412;&#22320;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#22312;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#19979;&#25913;&#21892;&#20102;FL&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15706</link><description>&lt;p&gt;
pFedSim: &#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#25512;&#36827;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
pFedSim: Similarity-Aware Model Aggregation Towards Personalized Federated Learning. (arXiv:2305.15706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30456;&#20284;&#24615;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861; pFedSim&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#30456;&#20284;&#24615;&#32858;&#21512;&#21644;&#27169;&#22411;&#35299;&#32806;&#20004;&#31181;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23558; NN &#27169;&#22411;&#35299;&#32806;&#20026;&#20010;&#24615;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#26412;&#22320;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#22312;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#19979;&#25913;&#21892;&#20102;FL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#33539;&#24335;&#20986;&#29616;&#20026;&#20102;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20165;&#23637;&#31034;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#21442;&#25968;&#32780;&#38750;&#21407;&#22987;&#25968;&#25454;&#12290;FL &#20013;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#22312;&#20110;&#20998;&#24067;&#20110;&#23458;&#25143;&#31471;&#30340;&#38750; IID&#65288;&#19981;&#21516;&#19988;&#29420;&#31435;&#20998;&#24067;&#65289;&#25968;&#25454;&#65288;&#21363;&#25968;&#25454;&#24322;&#26500;&#24615;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#20010;&#24615;&#21270; FL&#65288;pFL&#65289;&#26041;&#27861;&#65292;&#20363;&#22914;&#22522;&#20110;&#30456;&#20284;&#24615;&#32858;&#21512;&#21644;&#27169;&#22411;&#35299;&#32806;&#12290;&#21069;&#32773;&#20174;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#32858;&#21512;&#27169;&#22411;&#12290;&#21518;&#32773;&#23558;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#35299;&#32806;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#12290;&#20010;&#24615;&#21270;&#30001;&#20998;&#31867;&#22120;&#25429;&#33719;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#26159;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#33719;&#24471;&#30340;&#12290;&#20026;&#20102;&#25512;&#36827; pFL&#65292;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;pFedSim&#65288;&#22522;&#20110;&#27169;&#22411;&#30456;&#20284;&#24615;&#30340;pFL&#65289;&#31639;&#27861;&#65292;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558; NN &#27169;&#22411;&#35299;&#32806;&#20026;&#20010;&#24615;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#26412;&#22320;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#19979;&#25913;&#21892;&#20102;FL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The federated learning (FL) paradigm emerges to preserve data privacy during model training by only exposing clients' model parameters rather than original data. One of the biggest challenges in FL lies in the non-IID (not identical and independently distributed) data (a.k.a., data heterogeneity) distributed on clients. To address this challenge, various personalized FL (pFL) methods are proposed such as similarity-based aggregation and model decoupling. The former one aggregates models from clients of a similar data distribution. The later one decouples a neural network (NN) model into a feature extractor and a classifier. Personalization is captured by classifiers which are obtained by local training. To advance pFL, we propose a novel pFedSim (pFL based on model similarity) algorithm in this work by combining these two kinds of methods. More specifically, we decouple a NN model into a personalized feature extractor, obtained by aggregating models from similar clients, and a classifi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15703</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#22909;&#22788;&#65306;&#23567;&#25439;&#22833;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15703
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#26524;&#65292;&#20294;&#20854;&#20309;&#26102;&#20309;&#22320;&#26377;&#30410;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#22238;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#25105;&#20204;&#30340;&#36793;&#30028;&#20250;&#27604;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#26356;&#24378;&#12290;&#20316;&#20026;&#28909;&#36523;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#25104;&#26412;&#20998;&#24067;&#20250;&#22312;&#24773;&#22659;&#23637;&#24320;&#65288;CB&#65289;&#20013;&#23548;&#33268;&#23567;&#25439;&#22833;&#21518;&#24724;&#36793;&#30028;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#24067;&#24335;CB&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#22312;&#23454;&#35777;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#22312;&#32447;RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#29256;&#26412;&#31354;&#38388;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#34920;&#26684;MDP&#20013;&#23454;&#29616;&#20102;&#23567;&#25439;&#22833;&#21518;&#24724;&#65292;&#21516;&#26102;&#22312;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#20139;&#26377;&#23567;&#25439;&#22833;PAC&#36793;&#30028;&#12290;&#20197;&#31867;&#20284;&#30340;&#35265;&#35299;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#31163;&#32447;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#29616;&#23454;&#24212;&#29992;&#20013;&#25968;&#25454;&#38598;&#28418;&#31227;&#31561;&#36829;&#21453;&#25968;&#25454;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#20551;&#35774;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.15696</link><description>&lt;p&gt;
&#36890;&#36807;k&#26368;&#36817;&#37051;&#26469;&#26816;&#27979;&#25968;&#25454;&#38598;&#28418;&#31227;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Detecting Dataset Drift and Non-IID Sampling via k-Nearest Neighbors. (arXiv:2305.15696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#29616;&#23454;&#24212;&#29992;&#20013;&#25968;&#25454;&#38598;&#28418;&#31227;&#31561;&#36829;&#21453;&#25968;&#25454;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#20551;&#35774;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#26469;&#26816;&#27979;&#36829;&#21453;&#25968;&#25454;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#20551;&#35774;&#30340;&#26576;&#20123;&#24773;&#20917;&#12290;&#25105;&#20204;&#32771;&#34385;&#30340;&#20855;&#20307;&#36829;&#35268;&#24418;&#24335;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65306;&#20363;&#22914;&#65292;&#31034;&#20363;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#25490;&#24207;&#26041;&#24335;&#23548;&#33268;&#20960;&#20046;&#30456;&#37051;&#30340;&#31034;&#20363;&#30340;&#29305;&#24449;&#20540;&#36235;&#21521;&#30456;&#20284;&#65288;&#20363;&#22914;&#30001;&#20110;&#20998;&#24067;&#28418;&#31227;&#25110;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65289;&#12290;&#22522;&#20110;k&#26368;&#36817;&#37051;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#23457;&#35745;&#20219;&#20309;&#22810;&#21464;&#37327;&#25968;&#20540;&#25968;&#25454;&#20197;&#21450;&#20854;&#20182;&#21487;&#20197;&#29992;&#25968;&#20540;&#34920;&#31034;&#30340;&#25968;&#25454;&#31867;&#22411;&#65288;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#31561;&#65289;&#65292;&#20063;&#35768;&#21487;&#20197;&#20351;&#29992;&#27169;&#22411;&#23884;&#20837;&#34920;&#31034;&#12290;&#19982;&#29616;&#26377;&#30340;&#26816;&#27979;&#28418;&#31227;&#25110;&#33258;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#36866;&#29992;&#20110;&#26356;&#22810;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#21448;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#26816;&#27979;&#26356;&#24191;&#27867;&#30340;IID&#36829;&#35268;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a straightforward statistical test to detect certain violations of the assumption that the data are Independent and Identically Distributed (IID). The specific form of violation considered is common across real-world applications: whether the examples are ordered in the dataset such that almost adjacent examples tend to have more similar feature values (e.g. due to distributional drift, or attractive interactions between datapoints). Based on a k-Nearest Neighbors estimate, our approach can be used to audit any multivariate numeric data as well as other data types (image, text, audio, etc.) that can be numerically represented, perhaps with model embeddings. Compared with existing methods to detect drift or auto-correlation, our approach is both applicable to more types of data and also able to detect a wider variety of IID violations in practice. Code: https://github.com/cleanlab/cleanlab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#20989;&#25968;ANOVA&#26694;&#26550;&#21450;&#20854;&#22312;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36824;&#27010;&#36848;&#20102;&#20004;&#31181;&#26032;&#24320;&#21457;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;FANOVA&#21644;GAM&#30340;&#21487;&#35299;&#37322;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65288;FANGAM-EBM&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.15670</link><description>&lt;p&gt;
&#22522;&#20110;&#20989;&#25968;ANOVA&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;: &#31639;&#27861;&#21450;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Interpretable Machine Learning based on Functional ANOVA Framework: Algorithms and Comparisons. (arXiv:2305.15670v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#20989;&#25968;ANOVA&#26694;&#26550;&#21450;&#20854;&#22312;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36824;&#27010;&#36848;&#20102;&#20004;&#31181;&#26032;&#24320;&#21457;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;FANOVA&#21644;GAM&#30340;&#21487;&#35299;&#37322;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65288;FANGAM-EBM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#26089;&#26399;&#65292;&#37325;&#28857;&#26159;&#24320;&#21457;&#22797;&#26434;&#31639;&#27861;&#20197;&#33719;&#24471;&#26368;&#20339;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#29702;&#35299;&#21644;&#35299;&#37322;&#27169;&#22411;&#32467;&#26524;&#65292;&#24517;&#39035;&#20381;&#38752;&#20107;&#21518;&#35299;&#37322;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#24050;&#32463;&#34987;&#35777;&#26126;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#35748;&#35782;&#21040;&#21487;&#35299;&#37322;&#24615;&#21516;&#26679;&#37325;&#35201;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20570;&#20986;&#22949;&#21327;&#26469;&#24320;&#21457;&#22266;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#31639;&#27861;&#32780;&#19981;&#26159;&#36861;&#27714;&#26497;&#33268;&#39044;&#27979;&#34920;&#29616;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#37325;&#26032;&#21457;&#25496;&#20102;&#20989;&#25968;ANOVA&#20302;&#38454;&#27169;&#22411;&#30340;&#20351;&#29992;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#25991;&#29486;&#20013;&#24050;&#30693;&#12290;&#26412;&#25991;&#39318;&#20808;&#25551;&#36848;&#20102;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#20027;&#25928;&#24212;&#21644;&#20108;&#38454;&#30456;&#20114;&#20316;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#27010;&#36848;&#20102;&#20004;&#31181;&#26032;&#24320;&#21457;&#30340;&#25216;&#26415;:&#21487;&#35299;&#37322;&#30340;&#22686;&#24378;&#26426;&#22120;&#65288;EBM&#65289;&#65288;Lou&#31561;&#20154;&#65292;2013&#65289;&#21644;GAMI-Net&#65288;Yang&#31561;&#20154;&#65292;2021b)&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861;&#65292;&#21363;&#22522;&#20110;FANOVA&#21644;GAM&#30340;&#21487;&#35299;&#37322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65288;FANGAM-EBM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the early days of machine learning (ML), the emphasis was on developing complex algorithms to achieve best predictive performance. To understand and explain the model results, one had to rely on post hoc explainability techniques, which are known to have limitations. Recently, with the recognition that interpretability is just as important, researchers are compromising on small increases in predictive performance to develop algorithms that are inherently interpretable. While doing so, the ML community has rediscovered the use of low-order functional ANOVA (fANOVA) models that have been known in the statistical literature for some time. This paper starts with a description of challenges with post hoc explainability and reviews the fANOVA framework with a focus on main effects and second-order interactions. This is followed by an overview of two recently developed techniques: Explainable Boosting Machines or EBM (Lou et al., 2013) and GAMI-Net (Yang et al., 2021b). The paper proposes 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROTO&#30340;&#26032;&#22411;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#28436;&#21270;&#30340;&#35268;&#33539;&#21270;&#39033;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#27425;&#20248;&#12289;&#36866;&#24212;&#24615;&#26377;&#38480;&#21644;&#35745;&#31639;&#25928;&#29575;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;PROTO&#21487;&#20197;&#26497;&#22823;&#22320;&#36866;&#24212;&#21508;&#31181;&#26041;&#27861;&#65292;&#19988;&#20165;&#38656;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.15669</link><description>&lt;p&gt;
PROTO: &#36845;&#20195;&#31574;&#30053;&#35268;&#33539;&#21270;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning. (arXiv:2305.15669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15669
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROTO&#30340;&#26032;&#22411;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#28436;&#21270;&#30340;&#35268;&#33539;&#21270;&#39033;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#27425;&#20248;&#12289;&#36866;&#24212;&#24615;&#26377;&#38480;&#21644;&#35745;&#31639;&#25928;&#29575;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;PROTO&#21487;&#20197;&#26497;&#22823;&#22320;&#36866;&#24212;&#21508;&#31181;&#26041;&#27861;&#65292;&#19988;&#20165;&#38656;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#32467;&#21512;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#20248;&#28857;&#65292;&#25215;&#35834;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#31574;&#30053;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#23384;&#22312;&#24615;&#33021;&#27425;&#20248;&#12289;&#36866;&#24212;&#24615;&#26377;&#38480;&#21644;&#35745;&#31639;&#25928;&#29575;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PROTO&#65292;&#36890;&#36807;&#23558;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#19982;&#36845;&#20195;&#28436;&#21270;&#30340;&#35268;&#33539;&#21270;&#39033;&#30456;&#32467;&#21512;&#65292;&#20811;&#26381;&#20102;&#19978;&#36848;&#38480;&#21046;&#12290;PROTO&#36890;&#36807;&#25191;&#34892;&#20449;&#20219;&#21306;&#22495;&#26679;&#24335;&#26356;&#26032;&#65292;&#22312;&#28176;&#36827;&#25918;&#26494;&#32422;&#26463;&#24378;&#24230;&#30340;&#21516;&#26102;&#65292;&#20351;&#21021;&#22987;&#24494;&#35843;&#31283;&#23450;&#12289;&#26368;&#32456;&#24615;&#33021;&#26368;&#20248;&#12290;&#36890;&#36807;&#35843;&#25972;&#21482;&#26377;&#20960;&#34892;&#20195;&#30721;&#65292;PROTO&#21487;&#20197;&#23558;&#20219;&#20309;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#21644;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#26725;&#25509;&#36215;&#26469;&#65292;&#20174;&#32780;&#20135;&#29983;&#23545;&#21508;&#31181;&#26041;&#27861;&#30340;&#26497;&#24378;&#36866;&#24212;&#24615;&#12290;PROTO&#31616;&#21333;&#32780;&#20248;&#38597;&#65292;&#20165;&#24378;&#21152;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline-to-online reinforcement learning (RL), by combining the benefits of offline pretraining and online finetuning, promises enhanced sample efficiency and policy performance. However, existing methods, effective as they are, suffer from suboptimal performance, limited adaptability, and unsatisfactory computational efficiency. We propose a novel framework, PROTO, which overcomes the aforementioned limitations by augmenting the standard RL objective with an iteratively evolving regularization term. Performing a trust-region-style update, PROTO yields stable initial finetuning and optimal final performance by gradually evolving the regularization term to relax the constraint strength. By adjusting only a few lines of code, PROTO can bridge any offline policy pretraining and standard off-policy RL finetuning to form a powerful offline-to-online RL pathway, birthing great adaptability to diverse methods. Simple yet elegant, PROTO imposes minimal additional computation and enables highly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15659</link><description>&lt;p&gt;
&#22914;&#20309;&#36867;&#31163;&#38160;&#21270;&#30340;&#26497;&#23567;&#20540;&#28857;
&lt;/p&gt;
&lt;p&gt;
How to escape sharp minima. (arXiv:2305.15659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#20123;&#31639;&#27861;&#34987;&#35774;&#35745;&#29992;&#26469;&#21457;&#29616;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#31639;&#27861;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#25439;&#22833;&#20989;&#25968;&#28023;&#26862;&#30697;&#38453;&#30340;&#36857;&#26469;&#24230;&#37327;&#23427;&#30340;&#24179;&#22374;&#31243;&#24230;&#65292;&#24182;&#24418;&#24335;&#21270;&#23450;&#20041;&#20102;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#27010;&#24565;&#12290;&#22312;&#27492;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#12290;&#38024;&#23545;&#19968;&#33324;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;&#12290;&#31639;&#27861;&#30340;&#20027;&#35201;&#32452;&#20214;&#26159;&#20351;&#29992;&#20174;&#38543;&#26426;&#25200;&#21160;&#36845;&#20195;&#20013;&#35745;&#31639;&#30340;&#26799;&#24230;&#26469;&#20272;&#35745;&#23548;&#33268;&#26356;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#26041;&#21521;&#12290;&#23545;&#20110;&#25104;&#26412;&#20989;&#25968;&#26159;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#31639;&#27861;&#65292;&#21463;&#26368;&#36817;&#25552;&#20986;&#30340;&#23454;&#29992;&#31639;&#27861;&#8212;&#8212;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning applications have seen a remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this paradigm, this work formulates and studies the algorithmic question of how to find flat minima. As an initial effort, this work adopts the trace of hessian of the cost function as the measure of flatness, and formally defines the notion of approximate flat minima. Under this notion, we then design algorithms that find approximate flat minima efficiently. For general cost functions, we present a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima. For the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, support
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23569;&#22495;&#36890;&#29992;&#21270;&#26694;&#26550;&#21644;&#20803;&#33258;&#36866;&#24212;&#20219;&#21153;&#37319;&#26679;&#65288;MATS&#65289;&#36807;&#31243;&#65292;&#26088;&#22312;&#21033;&#29992;&#26497;&#23569;&#37327;&#30340;&#26032;&#20219;&#21153;&#22495;&#26469;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#33719;&#24471;&#30693;&#35782;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#22495;&#22806;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15644</link><description>&lt;p&gt;
&#20803;&#33258;&#36866;&#24212;&#20219;&#21153;&#37319;&#26679;&#23454;&#29616;&#23569;&#22495;&#36890;&#29992;&#21270;
&lt;/p&gt;
&lt;p&gt;
Meta Adaptive Task Sampling for Few-Domain Generalization. (arXiv:2305.15644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23569;&#22495;&#36890;&#29992;&#21270;&#26694;&#26550;&#21644;&#20803;&#33258;&#36866;&#24212;&#20219;&#21153;&#37319;&#26679;&#65288;MATS&#65289;&#36807;&#31243;&#65292;&#26088;&#22312;&#21033;&#29992;&#26497;&#23569;&#37327;&#30340;&#26032;&#20219;&#21153;&#22495;&#26469;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#33719;&#24471;&#30693;&#35782;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#22495;&#22806;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#30830;&#20445;&#27169;&#22411;&#20855;&#26377;&#36275;&#22815;&#30340;&#22495;&#22806;&#27867;&#21270;&#24615;&#33021;&#65292;&#20256;&#32479;&#30340;&#39046;&#22495;&#36890;&#29992;&#21270;&#26041;&#27861;&#24120;&#24120;&#38656;&#35201;&#22312;&#19981;&#21516;&#24213;&#23618;&#20998;&#24067;&#30340;&#22810;&#20010;&#28304;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#26377;&#22810;&#26679;&#21270;&#30340;&#35757;&#32451;&#20998;&#24067;&#12290;&#20294;&#30001;&#20110;&#39640;&#26114;&#30340;&#36153;&#29992;&#12289;&#38544;&#31169;&#38382;&#39064;&#25110;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#31561;&#21407;&#22240;&#65292;&#36890;&#24120;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#21162;&#21147;&#25165;&#33021;&#33719;&#24471;&#36275;&#22815;&#30340;&#24322;&#26500;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#22312;&#24863;&#30693;&#21040;&#30340;&#24322;&#36136;&#24615;&#21463;&#38480;&#26102;&#65292;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#30340;&#22495;&#22806;&#27867;&#21270;&#24615;&#33021;&#26159;&#19968;&#20010;&#26377;&#36259;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#23569;&#22495;&#36890;&#29992;&#21270;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#26497;&#23569;&#37327;&#30340;&#26032;&#20219;&#21153;&#22495;&#20013;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#20808;&#21069;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#33258;&#36866;&#24212;&#20219;&#21153;&#37319;&#26679;&#65288;MATS&#65289;&#36807;&#31243;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#35821;&#20041;&#21644;&#39046;&#22495;&#36716;&#31227;&#30456;&#24322;&#24615;&#26469;&#21306;&#20998;&#22522;&#30784;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure the out-of-distribution (OOD) generalization performance, traditional domain generalization (DG) methods resort to training on data from multiple sources with different underlying distributions. And the success of those DG methods largely depends on the fact that there are diverse training distributions. However, it usually needs great efforts to obtain enough heterogeneous data due to the high expenses, privacy issues or the scarcity of data. Thus an interesting yet seldom investigated problem arises: how to improve the OOD generalization performance when the perceived heterogeneity is limited. In this paper, we instantiate a new framework called few-domain generalization (FDG), which aims to learn a generalizable model from very few domains of novel tasks with the knowledge acquired from previous learning experiences on base tasks. Moreover, we propose a Meta Adaptive Task Sampling (MATS) procedure to differentiate base tasks according to their semantic and domain-shift sim
&lt;/p&gt;</description></item><item><title>FeDualEx&#26159;&#31532;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#19979;&#21516;&#26102;&#22788;&#29702;&#38797;&#28857;&#20248;&#21270;&#21644;&#22797;&#21512;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15643</link><description>&lt;p&gt;
&#32852;&#37030;&#22797;&#21512;&#38797;&#28857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated Composite Saddle Point Optimization. (arXiv:2305.15643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15643
&lt;/p&gt;
&lt;p&gt;
FeDualEx&#26159;&#31532;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#19979;&#21516;&#26102;&#22788;&#29702;&#38797;&#28857;&#20248;&#21270;&#21644;&#22797;&#21512;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#35299;&#20915;&#38797;&#28857;&#38382;&#39064;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#24179;&#28369;&#26080;&#32422;&#26463;&#30446;&#26631;&#20989;&#25968;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#24448;&#24448;&#28041;&#21450;&#32422;&#26463;&#25110;&#38750;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#36825;&#23548;&#33268;&#38656;&#35201;&#36827;&#34892;&#22797;&#21512;&#20248;&#21270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23545;&#20598;&#22806;&#25512;&#65288;FeDualEx&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39069;&#22806;&#27493;&#39588;&#30340;&#21407;&#22987;&#8212;&#23545;&#20598;&#31639;&#27861;&#65292;&#26159;&#31532;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#19979;&#21253;&#25324;&#20102;&#38797;&#28857;&#20248;&#21270;&#21644;&#22797;&#21512;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#25910;&#25947;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#37117;&#35777;&#26126;&#20102; FeDualEx &#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110; FeDualEx &#30340;&#39034;&#24207;&#29256;&#26412;&#65292;&#25105;&#20204;&#20063;&#20026;&#38543;&#26426;&#22797;&#21512;&#38797;&#28857;&#35774;&#32622;&#25552;&#20379;&#20102;&#36895;&#29575;&#65292;&#36825;&#26159;&#25454;&#25105;&#20204;&#25152;&#30693;&#20043;&#21069;&#30340;&#25991;&#29486;&#20013;&#27809;&#26377;&#25214;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) approaches for saddle point problems (SPP) have recently gained in popularity due to the critical role they play in machine learning (ML). Existing works mostly target smooth unconstrained objectives in Euclidean space, whereas ML problems often involve constraints or non-smooth regularization, which results in a need for composite optimization. Addressing these issues, we propose Federated Dual Extrapolation (FeDualEx), an extra-step primal-dual algorithm, which is the first of its kind that encompasses both saddle point optimization and composite objectives under the FL paradigm. Both the convergence analysis and the empirical evaluation demonstrate the effectiveness of FeDualEx in these challenging settings. In addition, even for the sequential version of FeDualEx, we provide rates for the stochastic composite saddle point setting which, to our knowledge, are not found in prior literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BiasCorr&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#23376;&#38598;&#26631;&#31614;&#32570;&#22833;&#26159;&#30001;&#20110;&#36873;&#25321;&#36807;&#31243;&#30340;&#32570;&#22833;&#38750;&#38543;&#26426;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20462;&#25913;&#21407;&#22987;&#35757;&#32451;&#38598;&#20351;&#20998;&#31867;&#22120;&#22312;&#32570;&#22833;&#38750;&#38543;&#26426;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#19979;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.15641</link><description>&lt;p&gt;
&#19968;&#31181;&#22312;&#32570;&#22833;&#38750;&#38543;&#26426;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#19979;&#30340;&#40065;&#26834;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Robust Classifier Under Missing-Not-At-Random Sample Selection Bias. (arXiv:2305.15641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15641
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BiasCorr&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#23376;&#38598;&#26631;&#31614;&#32570;&#22833;&#26159;&#30001;&#20110;&#36873;&#25321;&#36807;&#31243;&#30340;&#32570;&#22833;&#38750;&#38543;&#26426;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20462;&#25913;&#21407;&#22987;&#35757;&#32451;&#38598;&#20351;&#20998;&#31867;&#22120;&#22312;&#32570;&#22833;&#38750;&#38543;&#26426;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#19979;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#20559;&#31227;&#36890;&#24120;&#26159;&#30001;&#20110;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#36896;&#25104;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#30001;&#20110;&#26679;&#26412;&#38750;&#38543;&#26426;&#25277;&#26679;&#32780;&#23548;&#33268;&#30340;&#20559;&#24046;&#65292;&#21253;&#25324;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#31034;&#20363;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#26041;&#27861;&#29992;&#20110;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#19979;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#20294;&#24456;&#23569;&#28041;&#21450;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#23376;&#38598;&#26631;&#31614;&#32570;&#22833;&#26159;&#30001;&#20110;&#36873;&#25321;&#36807;&#31243;&#30340;&#32570;&#22833;&#38750;&#38543;&#26426;&#24615;&#12290;&#22312;&#32479;&#35745;&#23398;&#20013;&#65292;&#26684;&#26519;&#26041;&#27861;&#21033;&#29992;&#36923;&#36753;&#22238;&#24402;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#26469;&#34920;&#31034;&#36825;&#31181;&#31867;&#22411;&#30340;&#26679;&#26412;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#23558;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#22320;&#38598;&#25104;&#21040;&#40065;&#26834;&#20998;&#31867;&#26694;&#26550;&#20013;&#23545;&#20110;&#36825;&#31181;&#20559;&#24046;&#35774;&#32622;&#24182;&#19981;&#26377;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BiasCorr&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#21407;&#22987;&#35757;&#32451;&#38598;&#26469;&#20351;&#20998;&#31867;&#22120;&#22312;&#32570;&#22833;&#38750;&#38543;&#26426;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#19979;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#20559;&#24046;&#25552;&#20379;&#20102;BiasCorr&#30456;&#23545;&#20110;Greene&#26041;&#27861;&#30340;&#25913;&#36827;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BiasCorr&#22312;MNAR&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The shift between the training and testing distributions is commonly due to sample selection bias, a type of bias caused by non-random sampling of examples to be included in the training set. Although there are many approaches proposed to learn a classifier under sample selection bias, few address the case where a subset of labels in the training set are missing-not-at-random (MNAR) as a result of the selection process. In statistics, Greene's method formulates this type of sample selection with logistic regression as the prediction model. However, we find that simply integrating this method into a robust classification framework is not effective for this bias setting. In this paper, we propose BiasCorr, an algorithm that improves on Greene's method by modifying the original training set in order for a classifier to learn under MNAR sample selection bias. We provide theoretical guarantee for the improvement of BiasCorr over Greene's method by analyzing its bias. Experimental results on
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861; - &#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COT)&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#22522;&#20110;&#32463;&#39564;&#30340;&#21464;&#20307; - &#24102;&#38376;&#38480;&#30340;&#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COTT)&#65292;&#23427;&#20204;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#20266;&#26631;&#31614;&#36716;&#31227;&#35823;&#24046;&#26102;&#12290;</title><link>http://arxiv.org/abs/2305.15640</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#34920;&#24449;&#21306;&#20998;&#20110;&#20998;&#24067;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Characterizing Out-of-Distribution Error via Optimal Transport. (arXiv:2305.15640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861; - &#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COT)&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#22522;&#20110;&#32463;&#39564;&#30340;&#21464;&#20307; - &#24102;&#38376;&#38480;&#30340;&#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COTT)&#65292;&#23427;&#20204;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#20266;&#26631;&#31614;&#36716;&#31227;&#35823;&#24046;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#37096;&#32626;&#20013;&#65292;&#27809;&#22312;&#20998;&#24067;(out-of-distribution)&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#25552;&#20986;&#20102;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#22240;&#27492;&#39044;&#27979;&#27169;&#22411;&#22312;&#27809;&#26631;&#31614;&#30340;o
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) data poses serious challenges in deployed machine learning models, so methods of predicting a model's performance on OOD data without labels are important for machine learning safety. While a number of methods have been proposed by prior work, they often underestimate the actual error, sometimes by a large margin, which greatly impacts their applicability to real tasks. In this work, we identify pseudo-label shift, or the difference between the predicted and true OOD label distributions, as a key indicator to this underestimation. Based on this observation, we introduce a novel method for estimating model performance by leveraging optimal transport theory, Confidence Optimal Transport (COT), and show that it provably provides more robust error estimates in the presence of pseudo-label shift. Additionally, we introduce an empirically-motivated variant of COT, Confidence Optimal Transport with Thresholding (COTT), which applies thresholding to the individual tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#33021;&#37327;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#24191;&#20041;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#22810;&#31181;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#65292;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15639</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24191;&#20041;p-Laplacian&#27491;&#21017;&#21270;&#26694;&#26550;&#22270;&#21367;&#31215;&#32593;&#32476;: &#25910;&#25947;&#24615;&#12289;&#33021;&#37327;&#21160;&#24577;&#21644;&#38750;&#32447;&#24615;&#25193;&#25955;&#35757;&#32451;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion. (arXiv:2305.15639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#33021;&#37327;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#24191;&#20041;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#22810;&#31181;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#65292;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65288;pL-UFG&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#24314;&#31435;&#23545;&#20854;&#24615;&#36136;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;Framelet&#21367;&#31215;&#21518;&#38598;&#25104;p-Laplacian&#30340;&#38544;&#24335;&#23618;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;pL-UFG&#28176;&#36817;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;&#36890;&#36807;&#25506;&#32034;pL-UFG&#30340;&#24191;&#20041;Dirichlet&#33021;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Dirichlet&#33021;&#37327;&#20445;&#25345;&#38750;&#38646;&#65292;&#30830;&#20445;&#22312;pL-UFG&#25509;&#36817;&#25910;&#25947;&#26102;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21160;&#24577;&#33021;&#37327;&#35270;&#35282;&#38416;&#26126;&#20102;pL-UFG&#20013;&#30340;&#38544;&#24335;&#23618;&#19982;&#22270;Framelets&#21327;&#21516;&#24037;&#20316;&#65292;&#22686;&#24378;&#20102;&#35813;&#27169;&#22411;&#23545;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38544;&#24335;&#23618;&#21487;&#20197;&#34987;&#35299;&#37322;&#25104;&#24191;&#20041;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#22810;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;&#36825;&#20123;&#22810;&#26041;&#38754;&#30340;&#20998;&#26512;&#23548;&#33268;&#20102;&#32479;&#19968;&#30340;&#32467;&#35770;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a comprehensive theoretical analysis of graph p-Laplacian based framelet network (pL-UFG) to establish a solid understanding of its properties. We begin by conducting a convergence analysis of the p-Laplacian based implicit layer integrated after the framelet convolution, providing insights into the asymptotic behavior of pL-UFG. By exploring the generalized Dirichlet energy of pL-UFG, we demonstrate that the Dirichlet energy remains non-zero, ensuring the avoidance of over-smoothing issues in pL-UFG as it approaches convergence. Furthermore, we elucidate the dynamic energy perspective through which the implicit layer in pL-UFG synergizes with graph framelets, enhancing the model's adaptability to both homophilic and heterophilic data. Remarkably, we establish that the implicit layer can be interpreted as a generalized non-linear diffusion process, enabling training using diverse schemes. These multifaceted analyses lead to unified conclusions that provide novel insi
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#19968;&#23478;&#22823;&#22411;&#21307;&#38498;&#32593;&#32476;&#19982;&#23398;&#26415;&#30028;&#21644;&#39038;&#38382;&#21512;&#20316;&#65292;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#32467;&#26524;&#65292;&#21516;&#26102;&#23558;&#39044;&#27979;&#19982;&#21307;&#29983;&#30340;&#39044;&#27979;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#26356;&#22810;&#30340;&#24739;&#32773;&#20986;&#38498;&#24182;&#20943;&#23569;&#20102;&#20877;&#20837;&#38498;&#12290;</title><link>http://arxiv.org/abs/2305.15629</link><description>&lt;p&gt;
&#24739;&#32773;&#32467;&#26524;&#39044;&#27979;&#25913;&#21892;&#20102;&#22823;&#22411;&#21307;&#38498;&#32593;&#32476;&#30340;&#36816;&#33829;
&lt;/p&gt;
&lt;p&gt;
Patient Outcome Predictions Improve Operations at a Large Hospital Network. (arXiv:2305.15629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15629
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#19968;&#23478;&#22823;&#22411;&#21307;&#38498;&#32593;&#32476;&#19982;&#23398;&#26415;&#30028;&#21644;&#39038;&#38382;&#21512;&#20316;&#65292;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#32467;&#26524;&#65292;&#21516;&#26102;&#23558;&#39044;&#27979;&#19982;&#21307;&#29983;&#30340;&#39044;&#27979;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#26356;&#22810;&#30340;&#24739;&#32773;&#20986;&#38498;&#24182;&#20943;&#23569;&#20102;&#20877;&#20837;&#38498;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#33719;&#24471;&#20934;&#30830;&#30340;&#24739;&#32773;&#32467;&#26524;&#39044;&#27979;&#21487;&#20197;&#22686;&#24378;&#21307;&#21153;&#20154;&#21592;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#26368;&#32456;&#20351;&#21307;&#38498;&#30340;&#25152;&#26377;&#21033;&#30410;&#30456;&#20851;&#32773;&#21463;&#30410;&#12290;&#32654;&#22269;&#19968;&#23478;&#22823;&#22411;&#21307;&#38498;&#32593;&#32476;&#19968;&#30452;&#19982;&#23398;&#26415;&#30028;&#21644;&#39038;&#38382;&#21512;&#20316;&#65292;&#39044;&#27979;&#20854;&#19971;&#23478;&#21307;&#38498;&#25152;&#26377;&#20303;&#38498;&#24739;&#32773;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#32467;&#26524;&#12290;&#26041;&#27861;/&#32467;&#26524;&#65306;&#25105;&#20204;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#39044;&#27979;&#19979;&#19968;&#20010;24&#23567;&#26102;/48&#23567;&#26102;&#20986;&#38498;&#21644;&#37325;&#30151;&#30417;&#25252;&#23460;&#36716;&#31227;&#65292;&#20986;&#38498;&#27515;&#20129;&#29575;&#21644;&#20986;&#38498;&#23433;&#25490;&#30340;&#27010;&#29575;&#12290;&#25152;&#26377;&#27169;&#22411;&#37117;&#23454;&#29616;&#20102;&#39640;&#30340;&#22806;&#37096;&#26679;&#26412;AUC&#65288;75.7&#65285;-92.5&#65285;&#65289;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#26657;&#20934;&#12290;&#27492;&#22806;&#65292;&#23558;48&#23567;&#26102;&#20986;&#38498;&#39044;&#27979;&#19982;&#21307;&#29983;&#21516;&#26102;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#26356;&#22810;&#30340;&#24739;&#32773;&#20986;&#38498;&#65288;10&#65285;-28.7&#65285;&#65289;&#65292;&#24182;&#20943;&#23569;&#20102;7&#22825;/ 30&#22825;&#30340;&#20877;&#20837;&#38498;&#65288;p&#20540;&#23567;&#20110;0.001&#65289;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#27599;&#22825;&#26089;&#19978;&#25552;&#21462;&#25968;&#25454;&#24182;&#26356;&#26032;&#39044;&#27979;&#65292;&#20197;&#21450;&#29992;&#25143;&#21451;&#22909;&#22411;&#36719;&#20214;&#21644;&#24425;&#33394;&#35686;&#25253;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Problem definition: Access to accurate predictions of patients' outcomes can enhance medical staff's decision-making, which ultimately benefits all stakeholders in the hospitals. A large hospital network in the US has been collaborating with academics and consultants to predict short-term and long-term outcomes for all inpatients across their seven hospitals. Methodology/results: We develop machine learning models that predict the probabilities of next 24-hr/48-hr discharge and intensive care unit transfers, end-of-stay mortality and discharge dispositions. All models achieve high out-of-sample AUC (75.7%-92.5%) and are well calibrated. In addition, combining 48-hr discharge predictions with doctors' predictions simultaneously enables more patient discharges (10%-28.7%) and fewer 7-day/30-day readmissions ($p$-value $&lt;0.001$). We implement an automated pipeline that extracts data and updates predictions every morning, as well as user-friendly software and a color-coded alert system to 
&lt;/p&gt;</description></item><item><title>GFairHint&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#23398;&#20064;&#20844;&#24179;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#21407;&#22987;&#22270;&#23884;&#20837;&#36830;&#25509;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15622</link><description>&lt;p&gt;
GFairHint&#65306;&#36890;&#36807;&#20844;&#24179;&#24615;&#25552;&#31034;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
GFairHint: Improving Individual Fairness for Graph Neural Networks via Fairness Hint. (arXiv:2305.15622v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15622
&lt;/p&gt;
&lt;p&gt;
GFairHint&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#23398;&#20064;&#20844;&#24179;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#21407;&#22987;&#22270;&#23884;&#20837;&#36830;&#25509;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#38382;&#39064;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#20197;&#21450;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#25968;&#25454;&#23398;&#20064;&#19978;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;GNN&#20013;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#35768;&#22810;&#29616;&#26377;&#30340;&#30740;&#31350;&#22312;&#32676;&#20307;&#23618;&#38754;&#19978;&#25913;&#21892;&#20102;&#20844;&#24179;&#24615;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#20419;&#36827;&#20102;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#36825;&#20351;&#24471;&#30456;&#20284;&#30340;&#20010;&#20307;&#20855;&#26377;&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;&#20419;&#36827;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#29702;&#24819;&#26694;&#26550;&#24212;&#35813;&#65288;1&#65289;&#22312;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#24179;&#34913;&#65292;&#65288;2&#65289;&#36866;&#24212;&#20004;&#31181;&#24120;&#29992;&#30340;&#20010;&#20307;&#30456;&#20284;&#24615;&#24230;&#37327;&#65288;&#20174;&#22806;&#37096;&#27880;&#37322;&#21644;&#20174;&#36755;&#20837;&#29305;&#24449;&#35745;&#31639;&#65289;&#65292;&#65288;3&#65289;&#27178;&#36328;&#21508;&#31181;GNN&#27169;&#22411;&#36827;&#34892;&#25512;&#24191;&#65292;&#65288;4&#65289;&#20855;&#26377;&#39640;&#25928;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#27809;&#26377;&#23454;&#29616;&#25152;&#26377;&#30340;&#29702;&#24819;&#26465;&#20214;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;GFairHint&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36741;&#21161;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#23398;&#20064;&#20844;&#24179;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#21407;&#22987;&#22270;&#23884;&#20837;&#36830;&#25509;&#20197;&#22686;&#24378;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GFairHint&#22312;&#19981;&#29306;&#29298;&#22826;&#22810;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the growing concerns about fairness in machine learning and the impressive performance of Graph Neural Networks (GNNs) on graph data learning, algorithmic fairness in GNNs has attracted significant attention. While many existing studies improve fairness at the group level, only a few works promote individual fairness, which renders similar outcomes for similar individuals. A desirable framework that promotes individual fairness should (1) balance between fairness and performance, (2) accommodate two commonly-used individual similarity measures (externally annotated and computed from input features), (3) generalize across various GNN models, and (4) be computationally efficient. Unfortunately, none of the prior work achieves all the desirables. In this work, we propose a novel method, GFairHint, which promotes individual fairness in GNNs and achieves all aforementioned desirables. GFairHint learns fairness representations through an auxiliary link prediction task, and then concate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#30697;&#38453;&#20272;&#35745;&#26041;&#27861;&#65292;&#24403;MDP&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#26102;&#21487;&#26494;&#24347;&#35206;&#30422;&#26465;&#20214;&#38480;&#21046;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#29305;&#24449;&#34920;&#31034;&#30340;&#38656;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.15621</link><description>&lt;p&gt;
&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30697;&#38453;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Matrix Estimation for Offline Reinforcement Learning with Low-Rank Structure. (arXiv:2305.15621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#30697;&#38453;&#20272;&#35745;&#26041;&#27861;&#65292;&#24403;MDP&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#26102;&#21487;&#26494;&#24347;&#35206;&#30422;&#26465;&#20214;&#38480;&#21046;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#29305;&#24449;&#34920;&#31034;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#30697;&#38453;&#20272;&#35745;&#26041;&#27861;&#65292;&#24403;MDP&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#26102;&#33021;&#22815;&#26494;&#24347;state-action&#35206;&#30422;&#26465;&#20214;&#38480;&#21046;&#65292;&#19981;&#38656;&#35201;&#39044;&#20808;&#30693;&#36947;&#29305;&#24449;&#34920;&#31034;&#12290; &#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24046;&#24322;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#26377;&#38480;&#26679;&#26412;&#19979;&#30340;&#35823;&#24046;&#19978;&#30028;&#65292;&#24182;&#32473;&#20986;&#20102;&#20855;&#20307;&#20363;&#23376;&#26469;&#35777;&#26126;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider offline Reinforcement Learning (RL), where the agent does not interact with the environment and must rely on offline data collected using a behavior policy. Previous works provide policy evaluation guarantees when the target policy to be evaluated is covered by the behavior policy, that is, state-action pairs visited by the target policy must also be visited by the behavior policy. We show that when the MDP has a latent low-rank structure, this coverage condition can be relaxed. Building on the connection to weighted matrix completion with non-uniform observations, we propose an offline policy evaluation algorithm that leverages the low-rank structure to estimate the values of uncovered state-action pairs. Our algorithm does not require a known feature representation, and our finite-sample error bound involves a novel discrepancy measure quantifying the discrepancy between the behavior and target policies in the spectral space. We provide concrete examples where our algorit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#38477;&#23610;&#24230;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#20351;&#29992;&#25104;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21453;&#20559;&#32622;&#21644;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#26469;&#24674;&#22797;&#23384;&#22312;&#20559;&#35265;&#26679;&#26412;&#30340;&#30495;&#23454;&#29289;&#29702;&#32479;&#35745;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.15618</link><description>&lt;p&gt;
&#26368;&#20248;&#21270;&#20256;&#36755;&#21644;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65306;&#21453;&#20559;&#24046;&#65292;&#26465;&#20214;&#37319;&#26679;&#19979;&#30340;&#32479;&#35745;&#38477;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models. (arXiv:2305.15618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15618
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#38477;&#23610;&#24230;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#20351;&#29992;&#25104;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21453;&#20559;&#32622;&#21644;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#26469;&#24674;&#22797;&#23384;&#22312;&#20559;&#35265;&#26679;&#26412;&#30340;&#30495;&#23454;&#29289;&#29702;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#19981;&#25104;&#23545;&#25968;&#25454;&#30340;&#32479;&#35745;&#38477;&#23610;&#24230;&#30340;&#20004;&#38454;&#27573;&#27010;&#29575;&#26694;&#26550;&#12290;&#32479;&#35745;&#38477;&#23610;&#24230;&#36890;&#36807;&#19968;&#20010;&#27010;&#29575;&#26144;&#23556;&#26469;&#23558;&#20302;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#20174;&#65288;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#30340;&#65289;&#31895;&#31890;&#24230;&#25968;&#20540;&#26041;&#26696;&#36716;&#25442;&#20026;&#19982;&#39640;&#20445;&#30495;&#24230;&#26041;&#26696;&#19968;&#33268;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20018;&#32852;&#20004;&#20010;&#36716;&#25442;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#19968;&#20010;&#30001;&#26368;&#20248;&#20256;&#36755;&#22270;&#23454;&#29616;&#30340;&#21453;&#20559;&#32622;&#27493;&#39588;&#65292;&#20197;&#21450;&#19968;&#20010;&#30001;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#30340;&#19978;&#37319;&#26679;&#27493;&#39588;&#65292;&#24182;&#23558;&#26465;&#20214;&#37319;&#26679;&#21518;&#30340;&#27010;&#29575;&#20998;&#24067;&#32435;&#20837;&#35813;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#20351;&#29992;&#25104;&#23545;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#30830;&#23450;&#26465;&#20214;&#20998;&#24067;&#65292;&#24182;&#20174;&#23384;&#22312;&#20559;&#35265;&#30340;&#26679;&#26412;&#20013;&#30495;&#23454;&#22320;&#24674;&#22797;&#30456;&#20851;&#30340;&#29289;&#29702;&#32479;&#35745;&#20449;&#24687;&#12290;&#25105;&#20204;&#29992;&#19968;&#32500;&#21644;&#20108;&#32500;&#27969;&#20307;&#27969;&#21160;&#38382;&#39064;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29992;&#65292;&#36825;&#20123;&#38382;&#39064;&#20195;&#34920;&#20102;&#22825;&#27668;&#21644;&#27668;&#20505;&#25968;&#20540;&#27169;&#25311;&#20013;&#30340;&#26680;&#24515;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#20013;&#29983;&#25104;&#30495;&#23454;&#30340;&#39640;&#20998;&#36776;&#29575;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a two-stage probabilistic framework for statistical downscaling between unpaired data. Statistical downscaling seeks a probabilistic map to transform low-resolution data from a (possibly biased) coarse-grained numerical scheme to high-resolution data that is consistent with a high-fidelity scheme. Our framework tackles the problem by tandeming two transformations: a debiasing step that is performed by an optimal transport map, and an upsampling step that is achieved by a probabilistic diffusion model with \textit{a posteriori} conditional sampling. This approach characterizes a conditional distribution without the need for paired data, and faithfully recovers relevant physical statistics from biased samples. We demonstrate the utility of the proposed approach on one- and two-dimensional fluid flow problems, which are representative of the core difficulties present in numerical simulations of weather and climate. Our method produces realistic high-resolution outputs from lo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26234;&#33021;&#27969;&#25216;&#26415;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#36890;&#37327;&#30340;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#30340;AI&#25512;&#29702;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#21644;&#35299;&#30721;&#26102;&#38388;&#65292;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#65292;&#24182;&#38477;&#20302;&#20102;&#25972;&#20307;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.15617</link><description>&lt;p&gt;
&#22522;&#20110;&#26234;&#33021;&#27969;&#25216;&#26415;&#30340;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#30340;&#39640;&#36890;&#37327;AI&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
High-Throughput AI Inference for Medical Image Classification and Segmentation using Intelligent Streaming. (arXiv:2305.15617v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26234;&#33021;&#27969;&#25216;&#26415;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#36890;&#37327;&#30340;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#30340;AI&#25512;&#29702;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#21644;&#35299;&#30721;&#26102;&#38388;&#65292;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#65292;&#24182;&#38477;&#20302;&#20102;&#25972;&#20307;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20020;&#24202;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#37319;&#29992;&#22686;&#22810;&#65292;&#24102;&#23485;&#30340;&#38480;&#21046;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#27969;&#24335;&#20256;&#36755;&#22270;&#20687;&#25968;&#25454;&#26102;&#20986;&#29616;&#36890;&#20449;&#29942;&#39048;&#65292;&#20174;&#32780;&#24310;&#35823;&#24739;&#32773;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#22240;&#27492;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;AI&#20379;&#24212;&#21830;&#23558;&#38656;&#35201;&#26356;&#22823;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#36153;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26234;&#33021;&#27969;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#35268;&#27169;&#21270;&#30340;&#21152;&#36895;&#12289;&#25104;&#26412;&#25928;&#30410;&#12289;&#24102;&#23485;&#20248;&#21270;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;AI&#25512;&#29702;&#65292;&#20174;&#32780;&#29992;&#20110;&#20020;&#24202;&#20915;&#31574;&#21046;&#23450;&#12290;&#23545;&#20110;&#20998;&#31867;&#65292;&#26234;&#33021;&#27969;&#25216;&#26415;&#23558;&#25968;&#25454;&#20256;&#36755;&#20943;&#23569;&#20102;99.01%&#65292;&#35299;&#30721;&#26102;&#38388;&#20943;&#23569;&#20102;98.58%&#65292;&#21516;&#26102;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;27.43&#20493;&#12290;&#23545;&#20110;&#20998;&#21106;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#25968;&#25454;&#20256;&#36755;&#20943;&#23569;&#20102;90.32%&#65292;&#35299;&#30721;&#26102;&#38388;&#20943;&#23569;&#20102;90.26%&#65292;&#21516;&#26102;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;4.20&#20493;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26234;&#33021;&#27969;&#25216;&#26415;&#21487;&#20197;&#21152;&#24555;&#21608;&#36716;&#26102;&#38388;&#65292;&#38477;&#20302;&#25968;&#25454;&#21644;&#20256;&#36755;&#30340;&#24635;&#20307;&#25104;&#26412;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the adoption of AI systems within the clinical setup grows, limitations in bandwidth could create communication bottlenecks when streaming imaging data, leading to delays in patient diagnosis and treatment. As such, healthcare providers and AI vendors will require greater computational infrastructure, therefore dramatically increasing costs. To that end, we developed intelligent streaming, a state-of-the-art framework to enable accelerated, cost-effective, bandwidth-optimized, and computationally efficient AI inference for clinical decision making at scale. For classification, intelligent streaming reduced the data transmission by 99.01% and decoding time by 98.58%, while increasing throughput by 27.43x. For segmentation, our framework reduced data transmission by 90.32%, decoding time by 90.26%, while increasing throughput by 4.20x. Our work demonstrates that intelligent streaming results in faster turnaround times, and reduced overall cost of data and transmission, without negativ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26500;&#20445;&#25345;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#35201;&#20040;&#20445;&#25345;&#33021;&#37327;&#65292;&#35201;&#20040;&#22312;&#28145;&#24230;&#22686;&#21152;&#26102;&#20135;&#29983;&#27491;&#30340;&#32791;&#25955;&#65292;&#36825;&#35299;&#37322;&#20102;&#21487;&#36870;&#24615;&#21644;&#19981;&#21487;&#36870;&#24615;&#22312;&#32593;&#32476;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.15616</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#21487;&#36870;&#21644;&#19981;&#21487;&#36870;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Reversible and irreversible bracket-based dynamics for deep graph neural networks. (arXiv:2305.15616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26500;&#20445;&#25345;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#35201;&#20040;&#20445;&#25345;&#33021;&#37327;&#65292;&#35201;&#20040;&#22312;&#28145;&#24230;&#22686;&#21152;&#26102;&#20135;&#29983;&#27491;&#30340;&#32791;&#25955;&#65292;&#36825;&#35299;&#37322;&#20102;&#21487;&#36870;&#24615;&#21644;&#19981;&#21487;&#36870;&#24615;&#22312;&#32593;&#32476;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#32467;&#26500;&#20801;&#35768;&#35757;&#32451;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32780;&#19981;&#20250;&#36807;&#24230;&#20809;&#28369;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29289;&#29702;&#30340;&#20316;&#29992;&#23578;&#19981;&#28165;&#26970;&#65292;&#23613;&#31649;&#21487;&#36870;&#65288;&#20363;&#22914;&#21704;&#23494;&#39039;&#65289;&#21644;&#19981;&#21487;&#36870;&#65288;&#20363;&#22914;&#25193;&#25955;&#65289;&#29616;&#35937;&#30340;&#25104;&#21151;&#23454;&#20363;&#20135;&#29983;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#26426;&#21046;&#25130;&#28982;&#30456;&#21453;&#65292;&#24182;&#19988;&#30001;&#20110;&#32463;&#39564;&#19978;&#30340;&#31163;&#24320;&#25968;&#23398;&#29702;&#35770;&#32780;&#20986;&#29616;&#20102;&#36827;&#19968;&#27493;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#32467;&#26500;&#20445;&#25345;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#35201;&#20040;&#20445;&#25345;&#33021;&#37327;&#65292;&#35201;&#20040;&#22312;&#28145;&#24230;&#22686;&#21152;&#26102;&#20135;&#29983;&#27491;&#30340;&#32791;&#25955;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36825;&#37324;&#20351;&#29992;&#30340;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26694;&#26550;&#20801;&#35768;&#22266;&#26377;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#65292;&#36825;&#20123;&#32467;&#26500;&#23558;&#24403;&#21069;&#26550;&#26500;&#20013;&#30340;&#31163;&#24320;&#29702;&#35770;&#20869;&#23481;&#25918;&#22312;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#26356;&#22909;&#22320;&#38416;&#26126;&#20102;&#21487;&#36870;&#24615;&#21644;&#19981;&#21487;&#36870;&#24615;&#22312;&#32593;&#32476;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that physics-inspired architectures allow the training of deep graph neural networks (GNNs) without oversmoothing. The role of these physics is unclear, however, with successful examples of both reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable results despite diametrically opposed mechanisms, and further complications arising due to empirical departures from mathematical theory. This work presents a series of novel GNN architectures based upon structure-preserving bracket-based dynamical systems, which are provably guaranteed to either conserve energy or generate positive dissipation with increasing depth. It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36870;&#21521;&#24037;&#31243;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35757;&#32451;&#34920;&#31034;&#65292;&#21457;&#29616;SSL&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27491;&#21017;&#21270;&#39033;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#12290;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2305.15614</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reverse Engineering Self-Supervised Learning. (arXiv:2305.15614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36870;&#21521;&#24037;&#31243;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35757;&#32451;&#34920;&#31034;&#65292;&#21457;&#29616;SSL&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27491;&#21017;&#21270;&#39033;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#12290;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#20294;&#29702;&#35299;&#23398;&#20064;&#34920;&#31034;&#21450;&#20854;&#22522;&#30784;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;SSL&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#21253;&#25324;&#22810;&#31181;&#27169;&#22411;&#12289;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;SSL&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#20010;&#26377;&#36259;&#26041;&#38754;&#65306;&#23427;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#65292;&#36825;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#26159;&#30001;SSL&#30446;&#26631;&#30340;&#27491;&#21017;&#21270;&#39033;&#39537;&#21160;&#30340;&#12290;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#19981;&#20165;&#22686;&#24378;&#20102;&#19979;&#28216;&#20998;&#31867;&#65292;&#32780;&#19988;&#21387;&#32553;&#20102;&#25968;&#25454;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#31867;&#21035;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#34920;&#31034;&#19982;&#21508;&#31181;&#23618;&#27425;&#30340;&#35821;&#20041;&#31867;&#21035;&#23545;&#40784;&#65292;&#24182;&#19988;&#36825;&#31181;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a powerful tool in machine learning, but understanding the learned representations and their underlying mechanisms remains a challenge. This paper presents an in-depth empirical analysis of SSL-trained representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing aspect of the SSL training process: it inherently facilitates the clustering of samples with respect to semantic labels, which is surprisingly driven by the SSL objective's regularization term. This clustering process not only enhances downstream classification but also compresses the data information. Furthermore, we establish that SSL-trained representations align more closely with semantic classes rather than random classes. Remarkably, we show that learned representations align with semantic classes across various hierarchical levels, and this alignment increases during training and when moving deeper into the network. Our findings provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15613</link><description>&lt;p&gt;
&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;
&lt;/p&gt;
&lt;p&gt;
Deep Equivariant Hyperspheres. (arXiv:2305.15613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;nD&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;&#28857;&#20113;&#20998;&#26512;&#20013;&#31561;&#21464;&#20110;&#27491;&#20132;&#36716;&#25442;&#65292;&#21033;&#29992;&#20102;&#36229;&#29699;&#20307;&#21644;&#24120;&#35268;n&#21333;&#24418;&#20307;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#29702;&#35770;&#26041;&#38754;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36817;&#26399;&#21457;&#23637;&#30340;&#21487;&#25805;&#32437;3D&#29699;&#24418;&#31070;&#32463;&#20803;&#29702;&#35770;--&#22522;&#20110;&#29699;&#24418;&#20915;&#31574;&#38754;&#30340;SO&#65288;3&#65289;-&#31561;&#21464;&#28388;&#27874;&#22120;&#32452;&#65292;&#23558;&#35813;&#31070;&#32463;&#20803;&#25193;&#23637;&#21040;&#20102;nD&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#22534;&#21472;&#22312;&#22810;&#23618;&#20013;&#12290;&#21033;&#29992;ModelNet40&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#28508;&#22312;&#23454;&#29992;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach to learning nD features equivariant under orthogonal transformations for point cloud analysis, utilizing hyperspheres and regular n-simplexes. Our main contributions are theoretical and tackle major issues in geometric deep learning such as equivariance and invariance under geometric transformations. Namely, we enrich the recently developed theory of steerable 3D spherical neurons -- SO(3)-equivariant filter banks based on neurons with spherical decision surfaces -- by extending said neurons to nD, which we call deep equivariant hyperspheres, and enabling their stacking in multiple layers. Using the ModelNet40 benchmark, we experimentally verify our theoretical contributions and show a potential practical configuration of the proposed equivariant hyperspheres.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#20195;&#26367;&#23494;&#24230;&#27604;&#26469;&#20272;&#35745;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20004;&#32452;&#25968;&#25454;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15612</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning. (arXiv:2305.15612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#20195;&#26367;&#23494;&#24230;&#27604;&#26469;&#20272;&#35745;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20004;&#32452;&#25968;&#25454;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#31185;&#23398;&#19982;&#24037;&#31243;&#30340;&#22810;&#20010;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#39640;&#25928;&#22320;&#25214;&#21040;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#36890;&#24120;&#65292;&#19968;&#20010;&#27010;&#29575;&#22238;&#24402;&#27169;&#22411;&#65292;&#22914;&#39640;&#26031;&#36807;&#31243;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#34987;&#24191;&#27867;&#29992;&#20316;&#26367;&#20195;&#20989;&#25968;&#65292;&#29992;&#20110;&#27169;&#25311;&#22312;&#32473;&#23450;&#36755;&#20837;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#20989;&#25968;&#35780;&#20272;&#30340;&#26174;&#24335;&#20998;&#24067;&#12290;&#38500;&#20102;&#22522;&#20110;&#27010;&#29575;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#34987;&#25552;&#20986;&#26469;&#20272;&#35745;&#30456;&#23545;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#30456;&#23545;&#25509;&#36817;&#21644;&#30456;&#23545;&#36828;&#31163;&#30340;&#20004;&#32452;&#23494;&#24230;&#27604;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#65292;&#21487;&#20197;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#26469;&#20272;&#35745;&#36825;&#20004;&#32452;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#23494;&#24230;&#27604;&#12290;&#28982;&#32780;&#65292;&#27492;&#31574;&#30053;&#20013;&#20351;&#29992;&#30340;&#30417;&#30563;&#20998;&#31867;&#22120;&#20542;&#21521;&#20110;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization has attracted huge attention from diverse research areas in science and engineering, since it is capable of finding a global optimum of an expensive-to-evaluate black-box function efficiently. In general, a probabilistic regression model, e.g., Gaussian processes, random forests, and Bayesian neural networks, is widely used as a surrogate function to model an explicit distribution over function evaluations given an input to estimate and a training dataset. Beyond the probabilistic regression-based Bayesian optimization, density ratio estimation-based Bayesian optimization has been suggested in order to estimate a density ratio of the groups relatively close and relatively far to a global optimum. Developing this line of research further, a supervised classifier can be employed to estimate a class probability for the two groups instead of a density ratio. However, the supervised classifiers used in this strategy tend to be overconfident for a global solution candid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15611</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#35282;&#24230;&#21078;&#26512;&#29983;&#29289;&#25968;&#25454;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#65306;&#35266;&#28857;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#26159;&#21542;&#20855;&#26377;&#20174;&#23567;&#22270;&#20013;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#25512;&#24191;&#21040;&#21516;&#19968;&#39046;&#22495;&#30340;&#22823;&#22270;&#20013;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23588;&#20854;&#26159;&#24230;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#20013;&#65292;&#24230;&#25968;&#26159;&#26377;&#30028;&#30340;&#65292;&#22240;&#27492;&#24230;&#20998;&#24067;&#30340;&#20559;&#31227;&#24456;&#23567;&#12290;&#21363;&#20351;&#24230;&#20998;&#24067;&#20559;&#31227;&#24456;&#23567;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GNNs&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#26263;&#31034;&#26377;&#20854;&#20182;&#21407;&#22240;&#12290;&#20107;&#23454;&#19978;&#65292;&#20197;&#24448;&#23545;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#21508;&#31181;&#22270;&#23610;&#23544;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#25506;&#32034;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#20998;&#26512;&#22823;&#22810;&#38598;&#20013;&#22312;&#31354;&#38388;&#39046;&#22495;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#35889;&#35282;&#24230;&#21435;&#30740;&#31350;GNNs&#22312;&#29983;&#29289;&#22270;&#25968;&#25454;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#27169;&#25311;&#21508;&#31181;&#31867;&#22411;&#30340;&#24230;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#27979;&#35797;GNNs &#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38500;&#20102;&#24230;&#20998;&#24067;&#20559;&#31227;&#22806;&#65292;GNNs &#36824;&#23545;&#22270;&#22823;&#23567;&#21464;&#21270;&#24341;&#36215;&#30340;&#35889;&#20998;&#24067;&#20559;&#31227;&#24456;&#25935;&#24863;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;GNN&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#20855;&#26377;&#23610;&#23544;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20851;&#20110;GNNs&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#30340;&#26032;&#35266;&#28857;&#21644;&#23454;&#36341;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#21160;&#24577;&#20132;&#20114;&#27169;&#22411;&#26102;&#27604;&#38750;&#31561;&#21464;&#32593;&#32476;&#26356;&#20934;&#30830;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#24471;&#30693;&#65292;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#21382;&#21490;&#23884;&#20837;&#30340;&#31561;&#21464;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#21152;&#31934;&#30830;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.15603</link><description>&lt;p&gt;
&#29992;E&#65288;3&#65289;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25289;&#26684;&#26391;&#26085;&#27969;&#20307;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning Lagrangian Fluid Mechanics with E($3$)-Equivariant Graph Neural Networks. (arXiv:2305.15603v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#21160;&#24577;&#20132;&#20114;&#27169;&#22411;&#26102;&#27604;&#38750;&#31561;&#21464;&#32593;&#32476;&#26356;&#20934;&#30830;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#24471;&#30693;&#65292;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#21382;&#21490;&#23884;&#20837;&#30340;&#31561;&#21464;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#21152;&#31934;&#30830;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#31243;&#31995;&#32479;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#35777;&#26126;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#27604;&#38750;&#31561;&#21464;&#32593;&#32476;&#26377;&#28508;&#21147;&#23398;&#20064;&#26356;&#20934;&#30830;&#30340;&#21160;&#24577;&#20132;&#20114;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#24191;&#20026;&#30740;&#31350;&#30340;&#27969;&#20307;&#27969;&#21160;&#31995;&#32479;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21363;3D&#34928;&#20943;Taylor-Green&#28457;&#28065;&#21644;3D&#36870;Poiseuille&#27969;&#65292;&#24182;&#26681;&#25454;&#19981;&#21516;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;&#21160;&#33021;&#25110;Sinkhorn&#36317;&#31163;&#65289;&#35780;&#20272;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#19981;&#21516;&#30340;&#29289;&#29702;&#20449;&#24687;&#21382;&#21490;&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#31561;&#21464;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#30446;&#21069;&#35757;&#32451;&#21644;&#35780;&#20272;&#36895;&#24230;&#36739;&#24930;&#65292;&#20294;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#21382;&#21490;&#23884;&#20837;&#30340;&#31561;&#21464;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#20934;&#30830;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We contribute to the vastly growing field of machine learning for engineering systems by demonstrating that equivariant graph neural networks have the potential to learn more accurate dynamic-interaction models than their non-equivariant counterparts. We benchmark two well-studied fluid-flow systems, namely 3D decaying Taylor-Green vortex and 3D reverse Poiseuille flow, and evaluate the models based on different performance measures, such as kinetic energy or Sinkhorn distance. In addition, we investigate different embedding methods of physical-information histories for equivariant models. We find that while currently being rather slow to train and evaluate, equivariant models with our proposed history embeddings learn more accurate physical interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#20316;&#32773;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;&#65292;&#24182;&#23558;CIS&#32435;&#20837;&#21040;&#22870;&#21169;&#35774;&#35745;&#12289;&#21021;&#22987;&#29366;&#24577;&#37319;&#26679;&#21644;&#29366;&#24577;&#37325;&#32622;&#31243;&#24207;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.15602</link><description>&lt;p&gt;
&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65306;&#25913;&#36827;&#30340;&#37319;&#26679;&#25928;&#29575;&#12289;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#21644;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Control invariant set enhanced safe reinforcement learning: improved sampling efficiency, guaranteed stability and robustness. (arXiv:2305.15602v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#20316;&#32773;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;&#65292;&#24182;&#23558;CIS&#32435;&#20837;&#21040;&#22870;&#21169;&#35774;&#35745;&#12289;&#21021;&#22987;&#29366;&#24577;&#37319;&#26679;&#21644;&#29366;&#24577;&#37325;&#32622;&#31243;&#24207;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#22788;&#29702;&#30495;&#23454;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#23433;&#20840;&#32422;&#26463;&#32780;&#22791;&#21463;&#30633;&#30446;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65288;CIS&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20351;&#29992;CIS&#30340;&#26174;&#24335;&#24418;&#24335;&#26469;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#65306;&#31163;&#32447;&#21644;&#22312;&#32447;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;CIS&#34987;&#32435;&#20837;&#21040;&#22870;&#21169;&#35774;&#35745;&#12289;&#21021;&#22987;&#29366;&#24577;&#37319;&#26679;&#21644;&#29366;&#24577;&#37325;&#32622;&#31243;&#24207;&#20013;&#12290;&#36825;&#31181;&#32435;&#20837;CIS&#30340;&#26041;&#24335;&#26377;&#21161;&#20110;&#25552;&#39640;&#31163;&#32447;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#22312;&#32447;&#38454;&#27573;&#65292;&#24403;&#39044;&#27979;&#30340;&#19979;&#19968;&#27493;&#29366;&#24577;&#22312;CIS&#20043;&#22806;&#26102;&#65292;&#21363;&#24341;&#20837;&#23433;&#20840;&#20934;&#21017;&#26102;&#65292;RL&#23558;&#20250;&#37325;&#26032;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is an area of significant research interest, and safe RL in particular is attracting attention due to its ability to handle safety-driven constraints that are crucial for real-world applications. This work proposes a novel approach to RL training, called control invariant set (CIS) enhanced RL, which leverages the advantages of utilizing the explicit form of CIS to improve stability guarantees and sampling efficiency. Furthermore, the robustness of the proposed approach is investigated in the presence of uncertainty. The approach consists of two learning stages: offline and online. In the offline stage, CIS is incorporated into the reward design, initial state sampling, and state reset procedures. This incorporation of CIS facilitates improved sampling efficiency during the offline training process. In the online stage, RL is retrained whenever the predicted next step state is outside of the CIS, which serves as a stability criterion, by introducing a Safety
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#36719;&#25552;&#31034;&#21644;&#36890;&#36807;&#38543;&#26426;&#40550;&#40521;&#32676;&#20307;&#36827;&#34892;&#30340;&#31163;&#25955;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#25552;&#31034;&#25968;&#25454;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15594</link><description>&lt;p&gt;
&#38543;&#26426;&#40550;&#40521;&#32676;&#20307;&#65306;&#29992;&#24046;&#20998;&#38544;&#31169;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models. (arXiv:2305.15594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#36719;&#25552;&#31034;&#21644;&#36890;&#36807;&#38543;&#26426;&#40550;&#40521;&#32676;&#20307;&#36827;&#34892;&#30340;&#31163;&#25955;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#25552;&#31034;&#25968;&#25454;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290; &#28982;&#32780;&#65292;&#25552;&#31034;&#20013;&#21253;&#21547;&#30340;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#24341;&#36215;&#20102;&#38544;&#31169;&#38382;&#39064;&#12290;&#25991;&#31456;&#39318;&#20808;&#35777;&#26126;&#20102;&#36825;&#20123;&#38382;&#39064;&#26159;&#21512;&#29702;&#30340;&#65306;&#25105;&#20204;&#23545;&#29992;&#20110;&#25552;&#31034;LLMs&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#26377;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#31169;&#26377;&#30340;&#36719;&#25552;&#31034;&#21487;&#20197;&#36890;&#36807;&#19979;&#28216;&#25968;&#25454;&#30340;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#12290;&#32780;&#31163;&#25955;&#25552;&#31034;&#21017;&#38656;&#35201;&#29992;&#22810;&#20010;LLMs&#36827;&#34892;&#22024;&#26434;&#30340;&#34920;&#20915;&#65292;&#21363;&#38543;&#26426;&#40550;&#40521;&#32676;&#20307;&#65292;&#26469;&#23558;&#20854;&#30693;&#35782;&#20256;&#36882;&#21040;&#19968;&#20010;&#20844;&#20849;&#25552;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20849;&#20139;&#30693;&#35782;&#32456;&#36523;&#23398;&#20064;&#65288;SKILL&#65289;&#25361;&#25112;&#65292;&#37096;&#32626;&#20998;&#25955;&#30340;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#65292;&#23454;&#29616;&#19981;&#21516;&#20219;&#21153;&#23398;&#20064;&#30340;&#24182;&#34892;&#20849;&#20139;&#12290;&#36890;&#36807;&#36731;&#37327;&#32423;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#65288;LLL&#65289;&#20195;&#29702;&#20154;&#65292;&#26368;&#23567;&#21270;&#19987;&#19994;&#21270;&#20998;&#25968;&#26469;&#20419;&#36827;&#26377;&#25928;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2305.15591</link><description>&lt;p&gt;
&#20849;&#20139;&#30693;&#35782;&#36731;&#37327;&#32423;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#30340;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Lightweight Learner for Shared Knowledge Lifelong Learning. (arXiv:2305.15591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15591
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20849;&#20139;&#30693;&#35782;&#32456;&#36523;&#23398;&#20064;&#65288;SKILL&#65289;&#25361;&#25112;&#65292;&#37096;&#32626;&#20998;&#25955;&#30340;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#65292;&#23454;&#29616;&#19981;&#21516;&#20219;&#21153;&#23398;&#20064;&#30340;&#24182;&#34892;&#20849;&#20139;&#12290;&#36890;&#36807;&#36731;&#37327;&#32423;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#65288;LLL&#65289;&#20195;&#29702;&#20154;&#65292;&#26368;&#23567;&#21270;&#19987;&#19994;&#21270;&#20998;&#25968;&#26469;&#20419;&#36827;&#26377;&#25928;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32456;&#36523;&#23398;&#20064; (LL) &#20013;&#65292;&#20195;&#29702;&#20154;&#22312;&#36935;&#21040;&#26032;&#26465;&#20214;&#21644;&#20219;&#21153;&#26102;&#19981;&#26029;&#23398;&#20064;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340; LL &#20165;&#38480;&#20110;&#21333;&#20010;&#20195;&#29702;&#31243;&#24207;&#25353;&#39034;&#24207;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#21518;&#37096;&#32626;&#19987;&#29992; LL &#26426;&#22120;&#26469;&#20943;&#36731;&#26087;&#20219;&#21153;&#30340;&#36951;&#24536;&#65292;&#22240;&#27492;&#36825;&#26159;&#22522;&#26412;&#19978;&#32531;&#24930;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20849;&#20139;&#30693;&#35782;&#32456;&#36523;&#23398;&#20064; (SKILL) &#25361;&#25112;&#65292;&#23427;&#37096;&#32626;&#20998;&#25955;&#30340; LL &#20195;&#29702;&#31243;&#24207;&#32676;&#65292;&#27599;&#20010;&#20195;&#29702;&#31243;&#24207;&#25353;&#39034;&#24207;&#23398;&#20064;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#25152;&#26377;&#20195;&#29702;&#31243;&#24207;&#29420;&#31435;&#24182;&#24182;&#34892;&#36816;&#34892;&#12290;&#22312;&#23398;&#20064;&#20102;&#21508;&#33258;&#30340;&#20219;&#21153;&#21518;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#20998;&#25955;&#30340;&#36890;&#20449;&#32593;&#32476;&#20849;&#20139;&#21644; consol &#20182;&#20204;&#30340;&#30693;&#35782;&#65292;&#20197;&#20415;&#26368;&#32456;&#25152;&#26377;&#20195;&#29702;&#31243;&#24207;&#37117;&#21487;&#20197;&#25484;&#25569;&#25152;&#26377;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915; SKILL &#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36731;&#37327;&#32423;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064; (LLL) &#20195;&#29702;&#20154;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#20195;&#29702;&#31243;&#24207;&#30340;&#19987;&#19994;&#21270;&#20998;&#25968;&#26368;&#23567;&#21270;&#26469;&#20419;&#36827;&#26377;&#25928;&#30340;&#20849;&#20139;&#12290;&#22240;&#27492;&#65292;&#27599;&#20010; LLL &#20195;&#29702;&#31243;&#24207;&#37117;&#21253;&#25324;&#19968;&#20010;&#36890;&#29992;&#20219;&#21153;&#19981;&#21487;&#21464;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Lifelong Learning (LL), agents continually learn as they encounter new conditions and tasks. Most current LL is limited to a single agent that learns tasks sequentially. Dedicated LL machinery is then deployed to mitigate the forgetting of old tasks as new tasks are learned. This is inherently slow. We propose a new Shared Knowledge Lifelong Learning (SKILL) challenge, which deploys a decentralized population of LL agents that each sequentially learn different tasks, with all agents operating independently and in parallel. After learning their respective tasks, agents share and consolidate their knowledge over a decentralized communication network, so that, in the end, all agents can master all tasks. We present one solution to SKILL which uses Lightweight Lifelong Learning (LLL) agents, where the goal is to facilitate efficient sharing by minimizing the fraction of the agent that is specialized for any given task. Each LLL agent thus consists of a common task-agnostic immutable par
&lt;/p&gt;</description></item><item><title>&#27969;&#24418;&#25193;&#25955;&#22330;&#26159;&#19968;&#31181;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#29983;&#25104;&#36830;&#32493;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#29305;&#24449;&#20989;&#25968;&#23450;&#20041;&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;&#22352;&#26631;&#31995;&#65292;&#24182;&#19988;&#20351;&#29992;&#22810;&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#34920;&#31034;&#20989;&#25968;&#12290;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#36825;&#20123;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.15586</link><description>&lt;p&gt;
&#27969;&#24418;&#25193;&#25955;&#22330;
&lt;/p&gt;
&lt;p&gt;
Manifold Diffusion Fields. (arXiv:2305.15586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15586
&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#25193;&#25955;&#22330;&#26159;&#19968;&#31181;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#29983;&#25104;&#36830;&#32493;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#29305;&#24449;&#20989;&#25968;&#23450;&#20041;&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;&#22352;&#26631;&#31995;&#65292;&#24182;&#19988;&#20351;&#29992;&#22810;&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#34920;&#31034;&#20989;&#25968;&#12290;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#36825;&#20123;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#27969;&#24418;&#25193;&#25955;&#22330;&#65288;MDF&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#23450;&#20041;&#36830;&#32493;&#20989;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#35889;&#20960;&#20309;&#20998;&#26512;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#36890;&#36807;Laplace-Beltrami&#31639;&#23376;&#30340;&#29305;&#24449;&#20989;&#25968;&#23450;&#20041;&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;&#22352;&#26631;&#31995;&#12290;MDF&#20351;&#29992;&#22810;&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#26500;&#25104;&#30340;&#26174;&#24335;&#21442;&#25968;&#21270;&#26469;&#34920;&#31034;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#27969;&#24418;&#19978;&#23545;&#36830;&#32493;&#20989;&#25968;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#19988;&#23545;&#27969;&#24418;&#30340;&#21018;&#24615;&#21644;&#31561;&#36317;&#21464;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27969;&#24418;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;MDF&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#36825;&#20123;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Manifold Diffusion Fields (MDF), an approach to learn generative models of continuous functions defined over Riemannian manifolds. Leveraging insights from spectral geometry analysis, we define an intrinsic coordinate system on the manifold via the eigen-functions of the Laplace-Beltrami Operator. MDF represents functions using an explicit parametrization formed by a set of multiple input-output pairs. Our approach allows to sample continuous functions on manifolds and is invariant with respect to rigid and isometric transformations of the manifold. Empirical results on several datasets and manifolds show that MDF can capture distributions of such functions with better diversity and fidelity than previous approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#27491;&#26631;&#31614;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#30740;&#31350;&#26631;&#31614;&#20559;&#24046;&#30340;&#21327;&#35758;&#21644;&#26032;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15584</link><description>&lt;p&gt;
&#21333;&#19968;&#27491;&#26631;&#31614;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#20559;&#24046;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding Label Bias in Single Positive Multi-Label Learning. (arXiv:2305.15584v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#27491;&#26631;&#31614;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#30740;&#31350;&#26631;&#31614;&#20559;&#24046;&#30340;&#21327;&#35758;&#21644;&#26032;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#26631;&#27880;&#25968;&#25454;&#30340;&#26114;&#36149;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24471;&#21040;&#32531;&#35299;&#12290;&#21333;&#19968;&#27491;&#26631;&#31614;&#22810;&#26631;&#31614;&#65288;SPML&#65289;&#23398;&#20064;&#25351;&#20986;&#21482;&#38656;&#27599;&#20010;&#22270;&#20687;&#30830;&#23450;&#19968;&#20010;&#27491;&#26631;&#31614;&#21363;&#21487;&#35757;&#32451;&#26377;&#25928;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#12290;&#20294;&#29616;&#26377;&#30340;SPML&#22522;&#20934;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20174;&#20256;&#32479;&#22810;&#26631;&#31614;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#27491;&#26631;&#31614;&#24182;&#21076;&#38500;&#20854;&#20182;&#26631;&#31614;&#24471;&#21040;&#30340;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27491;&#26631;&#31614;&#30340;&#38543;&#26426;&#36873;&#25321;&#24456;&#19981;&#29616;&#23454;&#12290;&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;SPML&#20013;&#26631;&#31614;&#20559;&#24046;&#30340;&#21327;&#35758;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating data for multi-label classification is prohibitively expensive because every category of interest must be confirmed to be present or absent. Recent work on single positive multi-label (SPML) learning shows that it is possible to train effective multi-label classifiers using only one positive label per image. However, the standard benchmarks for SPML are derived from traditional multi-label classification datasets by retaining one positive label for each training example (chosen uniformly at random) and discarding all other labels. In realistic settings it is not likely that positive labels are chosen uniformly at random. This work introduces protocols for studying label bias in SPML and provides new empirical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;KL&#25955;&#24230;&#38477;&#20302;&#30340;&#26032;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#26679;&#26412;&#36827;&#34892;&#35745;&#31639;&#32780;&#19981;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#65292;&#20855;&#26377;&#27604;SVGD&#26356;&#31616;&#21333;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#23545;&#20110;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#20063;&#26377;&#20248;&#21270;&#65292;&#25552;&#21319;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.15577</link><description>&lt;p&gt;
&#37319;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#30340;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Variational Gradient Descent using Local Linear Models. (arXiv:2305.15577v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;KL&#25955;&#24230;&#38477;&#20302;&#30340;&#26032;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#26679;&#26412;&#36827;&#34892;&#35745;&#31639;&#32780;&#19981;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#65292;&#20855;&#26377;&#27604;SVGD&#26356;&#31616;&#21333;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#23545;&#20110;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#20063;&#26377;&#20248;&#21270;&#65292;&#25552;&#21319;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) &#33021;&#22815;&#27839;&#30528;&#36712;&#36857;&#20256;&#36755;&#31890;&#23376;&#65292;&#20174;&#32780;&#20943;&#23569;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#20294;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#26469;&#35745;&#31639;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SVGD&#35270;&#35282;&#65292;&#23558;&#20854;&#35270;&#20026;&#21453;&#21521;KL&#26799;&#24230;&#27969;&#30340;&#23616;&#37096;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#35270;&#35282;&#21551;&#21457;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#26469;&#23454;&#29616;&#30456;&#21516;&#30446;&#30340;&#30340;&#26032;&#20272;&#35745;&#22120;&#12290;&#36825;&#20123;&#25552;&#35758;&#30340;&#20272;&#35745;&#22120;&#21487;&#20197;&#20165;&#20351;&#29992;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;&#30340;&#26679;&#26412;&#36827;&#34892;&#35745;&#31639;&#65292;&#32780;&#19981;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#35758;&#30340;&#21464;&#20998;&#26799;&#24230;&#20272;&#35745;&#22120;&#21033;&#29992;&#20102;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#20272;&#35745;&#20559;&#24046;&#19982;SVGD&#30456;&#24403;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#20855;&#26377;&#35745;&#31639;&#31616;&#20415;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#39640;&#32500;&#26799;&#24230;&#27969;&#30340;&#20272;&#35745;&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#20010;&#20302;&#32500;&#20272;&#35745;&#38382;&#39064;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#22909;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;&#25105;&#20204;&#23545;&#25552;&#35758;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) can transport particles along trajectories that reduce the KL divergence between the target and particle distribution but requires the target score function to compute the update. We introduce a new perspective on SVGD that views it as a local estimator of the reversed KL gradient flow. This perspective inspires us to propose new estimators that use local linear models to achieve the same purpose. The proposed estimators can be computed using only samples from the target and particle distribution without needing the target score function. Our proposed variational gradient estimators utilize local linear models, resulting in computational simplicity while maintaining effectiveness comparable to SVGD in terms of estimation biases. Additionally, we demonstrate that under a mild assumption, the estimation of high-dimensional gradient flow can be translated into a lower-dimensional estimation problem, leading to improved estimation accuracy. We vali
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#36716;&#31227;&#31639;&#23376;&#30340;&#31070;&#32463;&#38543;&#26426;&#36807;&#31243;MNPs&#65292;&#36890;&#36807;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#22534;&#21472;&#31070;&#32463;&#21442;&#25968;&#21270;&#30340;&#31639;&#23376;&#26500;&#24314;&#65292;&#19981;&#24433;&#21709;&#19968;&#33268;&#24615;&#25110;&#28155;&#21152;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;MNPs&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15574</link><description>&lt;p&gt;
&#22522;&#20110;&#21151;&#33021;&#39532;&#23572;&#31185;&#22827;&#36716;&#31227;&#31639;&#23376;&#30340;&#28145;&#24230;&#38543;&#26426;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep Stochastic Processes via Functional Markov Transition Operators. (arXiv:2305.15574v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15574
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#36716;&#31227;&#31639;&#23376;&#30340;&#31070;&#32463;&#38543;&#26426;&#36807;&#31243;MNPs&#65292;&#36890;&#36807;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#22534;&#21472;&#31070;&#32463;&#21442;&#25968;&#21270;&#30340;&#31639;&#23376;&#26500;&#24314;&#65292;&#19981;&#24433;&#21709;&#19968;&#33268;&#24615;&#25110;&#28155;&#21152;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;MNPs&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#36807;&#31243;&#31867;&#21035;&#31216;&#20026;&#39532;&#23572;&#31185;&#22827;&#31070;&#32463;&#36807;&#31243;(MNPs)&#65292;&#36825;&#31181;&#38543;&#26426;&#36807;&#31243;&#36890;&#36807;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#22534;&#21472;&#31070;&#32463;&#21442;&#25968;&#21270;&#30340;&#39532;&#23572;&#31185;&#22827;&#36716;&#31227;&#31639;&#23376;&#26500;&#24314;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#39532;&#23572;&#31185;&#22827;&#36716;&#31227;&#31639;&#23376;&#21487;&#20197;&#20445;&#25345;&#38543;&#26426;&#36807;&#31243;&#30340;&#21487;&#20132;&#25442;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#22952;&#30861;&#19968;&#33268;&#24615;&#25110;&#28155;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#30340;&#36845;&#20195;&#26500;&#24314;&#20026;&#31070;&#32463;&#36807;&#31243;(NPs)&#30340;&#21407;&#22987;&#26694;&#26550;&#22686;&#21152;&#20102;&#23454;&#36136;&#24615;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35828;&#26126;MNPs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#27604;&#22522;&#20934;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Markov Neural Processes (MNPs), a new class of Stochastic Processes (SPs) which are constructed by stacking sequences of neural parameterised Markov transition operators in function space. We prove that these Markov transition operators can preserve the exchangeability and consistency of SPs. Therefore, the proposed iterative construction adds substantial flexibility and expressivity to the original framework of Neural Processes (NPs) without compromising consistency or adding restrictions. Our experiments demonstrate clear advantages of MNPs over baseline models on a variety of tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31574;&#30053;&#30340;&#34892;&#20026;&#21644;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#32479;&#35745;&#25968;&#25454;&#34920;&#26126;&#65292;&#21333;&#20010;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#30340;&#26412;&#22320;&#35299;&#27604;&#20840;&#23616;&#26041;&#27861;&#24674;&#22797;&#30340;&#39044;&#26399;&#20540;&#26356;&#22909;&#12290;M&#252;ller&#31561;&#20154;&#25552;&#20986;&#30340;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#22312;&#26377;&#22122;&#38899;&#21644;&#26080;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#37117;&#26377;&#25512;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.15572</link><description>&lt;p&gt;
&#26412;&#22320;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#34892;&#20026;&#21644;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Behavior and Convergence of Local Bayesian Optimization. (arXiv:2305.15572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31574;&#30053;&#30340;&#34892;&#20026;&#21644;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#32479;&#35745;&#25968;&#25454;&#34920;&#26126;&#65292;&#21333;&#20010;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#30340;&#26412;&#22320;&#35299;&#27604;&#20840;&#23616;&#26041;&#27861;&#24674;&#22797;&#30340;&#39044;&#26399;&#20540;&#26356;&#22909;&#12290;M&#252;ller&#31561;&#20154;&#25552;&#20986;&#30340;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#22312;&#26377;&#22122;&#38899;&#21644;&#26080;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#37117;&#26377;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#19968;&#39033;&#26368;&#26032;&#30340;&#21457;&#23637;&#26159;&#20351;&#29992;&#26412;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#19982;&#20256;&#32479;&#30340;&#20840;&#23616;&#31574;&#30053;&#30456;&#27604;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#25552;&#20379;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#25991;&#29486;&#20013;&#30340;&#8220;&#20256;&#32479;&#26234;&#24935;&#8221;&#26159;&#65292;&#19987;&#27880;&#20110;&#26412;&#22320;&#20248;&#21270;&#35268;&#36991;&#20102;&#32500;&#24230;&#35781;&#21650;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#20363;&#31243;&#30340;&#39044;&#26399;&#34892;&#20026;&#25110;&#25910;&#25947;&#24615;&#20102;&#35299;&#29978;&#23569;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#26412;&#22320;&#26041;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#21333;&#20010;&#26412;&#22320;&#35299;&#30340;&#32479;&#35745;&#25968;&#25454;&#19982;&#20174;&#20840;&#23616;&#26041;&#27861;&#24674;&#22797;&#30340;&#39044;&#26399;&#20540;&#30456;&#27604;&#38750;&#24120;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#30001;M&#252;ller&#31561;&#20154;&#25552;&#20986;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31639;&#27861;&#30340;&#31532;&#19968;&#27425;&#20005;&#26684;&#20998;&#26512;&#65292;&#24182;&#22312;&#26377;&#22122;&#38899;&#21644;&#26080;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent development in Bayesian optimization is the use of local optimization strategies, which can deliver strong empirical performance on high-dimensional problems compared to traditional global strategies. The "folk wisdom" in the literature is that the focus on local optimization sidesteps the curse of dimensionality; however, little is known concretely about the expected behavior or convergence of Bayesian local optimization routines. We first study the behavior of the local approach, and find that the statistics of individual local solutions of Gaussian process sample paths are surprisingly good compared to what we would expect to recover from global methods. We then present the first rigorous analysis of such a Bayesian local optimization algorithm recently proposed by M\"uller et al. (2021), and derive convergence rates in both the noisy and noiseless settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30452;&#25509;&#24212;&#29992;&#20110;&#21407;&#22987;&#38899;&#39057;&#25968;&#25454;&#26469;&#25506;&#32034;&#28508;&#22312;&#38899;&#39057;&#31354;&#38388;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15571</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36827;&#34892;&#28508;&#22312;&#38899;&#39057;&#31354;&#38388;&#25506;&#32034;&#30340;&#22768;&#38899;&#35774;&#35745;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Sound Design Strategies for Latent Audio Space Explorations Using Deep Learning Architectures. (arXiv:2305.15571v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30452;&#25509;&#24212;&#29992;&#20110;&#21407;&#22987;&#38899;&#39057;&#25968;&#25454;&#26469;&#25506;&#32034;&#28508;&#22312;&#38899;&#39057;&#31354;&#38388;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22768;&#38899;&#21644;&#38899;&#20048;&#35745;&#31639;&#26041;&#38754;&#30340;&#24212;&#29992;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#25216;&#26415;&#19982;&#23427;&#20204;&#22914;&#20309;&#34987;&#32435;&#20837;&#30495;&#23454;&#19990;&#30028;&#30340;&#33402;&#26415;&#23454;&#36341;&#20043;&#38388;&#20173;&#23384;&#22312;&#30528;&#19968;&#20123;&#32570;&#22833;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#8212;&#8212;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#20043;&#21069;&#65292;VAE&#24050;&#32463;&#34987;&#29992;&#20110;&#29983;&#25104;&#28508;&#22312;&#38899;&#33394;&#31354;&#38388;&#25110;&#31526;&#21495;&#38899;&#20048;&#20363;&#23376;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#26412;&#30740;&#31350;&#23558;VAE&#30452;&#25509;&#24212;&#29992;&#20110;&#21407;&#22987;&#38899;&#39057;&#25968;&#25454;&#32780;&#19981;&#26159;&#38899;&#39057;&#29305;&#24449;&#25552;&#21462;&#30340;&#38899;&#39057;&#25968;&#25454;&#19978;&#65292;&#26082;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#38899;&#39057;&#24405;&#38899;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research in Deep Learning applications in sound and music computing have gathered an interest in the recent years; however, there is still a missing link between these new technologies and on how they can be incorporated into real-world artistic practices. In this work, we explore a well-known Deep Learning architecture called Variational Autoencoders (VAEs). These architectures have been used in many areas for generating latent spaces where data points are organized so that similar data points locate closer to each other. Previously, VAEs have been used for generating latent timbre spaces or latent spaces of symbolic music excepts. Applying VAE to audio features of timbre requires a vocoder to transform the timbre generated by the network to an audio signal, which is computationally expensive. In this work, we apply VAEs to raw audio data directly while bypassing audio feature extraction. This approach allows the practitioners to use any audio recording while giving flexibility an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#24050;&#32463;&#35757;&#32451;&#30340;DNN&#20998;&#31867;&#22120;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#36755;&#20986;&#22788;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#36845;&#20195;&#22320;&#20026;&#27599;&#20010;&#31867;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#21019;&#24314;&#31867;&#21407;&#22411;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21407;&#22411;&#21450;&#20854;&#29305;&#24449;&#20851;&#31995;&#26469;&#25581;&#31034;&#20998;&#31867;&#22120;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.15563</link><description>&lt;p&gt;
&#31070;&#22855;&#30340;DNN&#20998;&#31867;&#22120;&#21450;&#20854;&#26080;&#38656;&#25968;&#25454;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fantastic DNN Classifiers and How to Identify them without Data. (arXiv:2305.15563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#24050;&#32463;&#35757;&#32451;&#30340;DNN&#20998;&#31867;&#22120;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#36755;&#20986;&#22788;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#36845;&#20195;&#22320;&#20026;&#27599;&#20010;&#31867;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#21019;&#24314;&#31867;&#21407;&#22411;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21407;&#22411;&#21450;&#20854;&#29305;&#24449;&#20851;&#31995;&#26469;&#25581;&#31034;&#20998;&#31867;&#22120;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31639;&#27861;&#21644;&#32467;&#26500;&#21487;&#20197;&#20174;&#31034;&#20363;&#25968;&#25454;&#21019;&#24314;&#20248;&#31168;&#30340;DNN&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#26356;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#27169;&#22411;&#20272;&#35745;&#65292;&#20174;&#32780;&#25552;&#39640;&#27979;&#35797;&#24615;&#33021;&#12290;&#30446;&#21069;&#23384;&#22312;&#30340;&#39044;&#27979;&#24191;&#20041;&#24615;&#33021;&#30340;&#26041;&#27861;&#22522;&#20110;&#20445;&#30041;&#27979;&#35797;&#31034;&#20363;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#21487;&#20197;&#22312;&#27809;&#26377;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#35757;&#32451;&#36807;&#30340;DNN&#20998;&#31867;&#22120;&#36136;&#37327;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#24050;&#32463;&#35757;&#32451;&#30340;DNN&#20998;&#31867;&#22120;&#30340;&#36136;&#37327;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#31034;&#20363;&#25968;&#25454;&#12290;&#25105;&#20204;&#35748;&#20026;DNN&#30001;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#29305;&#24449;&#20998;&#31867;&#22120;&#32452;&#25104;&#65307;&#23558;&#29305;&#24449;&#25552;&#21462;&#22120;&#36755;&#20986;&#39304;&#36865;&#21040;&#20998;&#31867;&#22120;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#32593;&#32476;&#36755;&#20986;&#22788;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#36845;&#20195;&#22320;&#20026;&#27599;&#20010;&#31867;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#21019;&#24314;&#31867;&#21407;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#21407;&#22411;&#21450;&#20854;&#29305;&#24449;&#20851;&#31995;&#26469;&#25581;&#31034;&#20998;&#31867;&#22120;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;:&#19968;&#20010;&#20351;&#29992;&#29305;&#24449;&#30340;
&lt;/p&gt;
&lt;p&gt;
Current algorithms and architecture can create excellent DNN classifier models from example data. In general, larger training datasets result in better model estimations, which improve test performance. Existing methods for predicting generalization performance are based on hold-out test examples. To the best of our knowledge, at present no method exists that can estimate the quality of a trained DNN classifier without test data. In this paper, we show that the quality of a trained DNN classifier can be assessed without any example data. We consider DNNs to be composed of a feature extractor and a feature classifier; the feature extractor's output is fed to the classifier. The proposed method iteratively creates class prototypes in the input space for each class by minimizing a cross-entropy loss function at the output of the network. We use these prototypes and their feature relationships to reveal the quality of the classifier. We have developed two metrics: one using the features of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#25490;&#24207;&#35270;&#20026;&#38477;&#32500;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#25490;&#24207;&#22312;&#33258;&#22238;&#24402;&#22270;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.15562</link><description>&lt;p&gt;
&#35753;&#31209;&#24207;&#26469;&#21040;&#65306;&#37325;&#26032;&#32771;&#34385;&#33258;&#22238;&#24402;&#22270;&#29983;&#25104;&#20013;&#30340;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Let There Be Order: Rethinking Ordering in Autoregressive Graph Generation. (arXiv:2305.15562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#25490;&#24207;&#35270;&#20026;&#38477;&#32500;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#25490;&#24207;&#22312;&#33258;&#22238;&#24402;&#22270;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#22270;&#29983;&#25104;&#20219;&#21153;&#28041;&#21450;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#32473;&#23450;&#19968;&#32452;&#36755;&#20837;&#26465;&#20214;&#30340;&#22270;&#12290;&#35768;&#22810;&#20197;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#36880;&#27493;&#29983;&#25104;&#22270;&#32452;&#20214;&#65292;&#22914;&#33410;&#28857;&#21644;&#36793;&#32536;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#36890;&#24120;&#32570;&#20047;&#20854;&#32452;&#20214;&#20043;&#38388;&#30340;&#33258;&#28982;&#39034;&#24207;&#65292;&#23558;&#22270;&#36716;&#25442;&#20026;&#19968;&#31995;&#21015;&#26631;&#35760;&#24182;&#19981;&#30452;&#25130;&#20102;&#24403;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#24037;&#20316;&#22823;&#22810;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#22270;&#36941;&#21382;&#26041;&#27861;&#65288;&#22914;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#25110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;DFS&#65289;&#65289;&#23558;&#22270;&#36716;&#25442;&#20026;&#24207;&#21015;&#65292;&#20294;&#25490;&#24207;&#23545;&#22270;&#30340;&#29983;&#25104;&#24433;&#21709;&#30340;&#38382;&#39064;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#34987;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#19979;&#20960;&#28857;&#23545;&#36825;&#20010;&#38382;&#39064;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;(1)&#24378;&#35843;&#20102;&#25490;&#24207;&#22312;&#33258;&#22238;&#24402;&#22270;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;(2) &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#25490;&#24207;&#35270;&#20026;&#38477;&#32500;&#38382;&#39064;&#65292;&#20174;&#32780;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25490;&#24207;&#21644;&#25152;&#29983;&#25104;&#22270;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;(3)&#24341;&#20837;&#20102; "la...
&lt;/p&gt;
&lt;p&gt;
Conditional graph generation tasks involve training a model to generate a graph given a set of input conditions. Many previous studies employ autoregressive models to incrementally generate graph components such as nodes and edges. However, as graphs typically lack a natural ordering among their components, converting a graph into a sequence of tokens is not straightforward. While prior works mostly rely on conventional heuristics or graph traversal methods like breadth-first search (BFS) or depth-first search (DFS) to convert graphs to sequences, the impact of ordering on graph generation has largely been unexplored. This paper contributes to this problem by: (1) highlighting the crucial role of ordering in autoregressive graph generation models, (2) proposing a novel theoretical framework that perceives ordering as a dimensionality reduction problem, thereby facilitating a deeper understanding of the relationship between orderings and generated graph accuracy, and (3) introducing "la
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.15560</link><description>&lt;p&gt;
&#22522;&#20110; Foundation Model APIs &#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#65306;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Synthetic Data via Foundation Model APIs 1: Images. (arXiv:2305.15560v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21487;&#20943;&#36731;&#38544;&#31169;&#38382;&#39064;&#12290;&#19982;&#24403;&#21069;&#20026;&#27492;&#20219;&#21153;&#35757;&#32451;&#23450;&#21046;&#27169;&#22411;&#30340;&#20570;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;API&#29983;&#25104;DP&#21512;&#25104;&#25968;&#25454;&#65288;DPSDA&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#22522;&#30784;&#27169;&#22411;&#35270;&#20026;&#40657;&#30418;&#24182;&#21482;&#21033;&#29992;&#20854;&#25512;&#29702;API&#12290;&#36825;&#20123;&#22522;&#20110;API&#30340;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#26356;&#23481;&#26131;&#37096;&#32626;&#65292;&#22914;&#26368;&#36817; API &#24212;&#29992;&#31243;&#24207;&#30340;&#28608;&#22686;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#36825;&#20123;&#26041;&#27861;&#36824;&#21487;&#20197;&#21033;&#29992;&#21487;&#36890;&#36807;&#20854;&#25512;&#29702;API&#35775;&#38382;&#20854;&#26435;&#37325;&#26410;&#21457;&#24067;&#30340;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27169;&#22411;&#35775;&#38382;&#26356;&#21152;&#20005;&#26684;&#65292;&#36824;&#38656;&#20445;&#25252;API&#25552;&#20379;&#21830;&#30340;&#38544;&#31169;&#65292;&#36825;&#23558;&#24102;&#26469;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; Private Evolution&#65288;PE&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;API&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#21021;&#22987;&#23454;&#29616;&#12290;PE&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#22914;CIFAR-10&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29992;&#21644;&#38544;&#31169;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;DP&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating differentially private (DP) synthetic data that closely resembles the original private data without leaking sensitive user information is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are accessible via their inference APIs while the model weights are unreleased. However, this comes with greater challenges due to strictly more restrictive model access and the additional need to protect privacy from the API provider.  In this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its ini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#38754;&#21521;&#38543;&#26426;&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#30340;&#22312;&#32447;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20046;&#26368;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#25968;&#23383;&#27169;&#25311;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.15558</link><description>&lt;p&gt;
&#38754;&#21521;&#24102;&#26377;&#38271;&#26399;&#32422;&#26463;&#30340;&#38543;&#26426;&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#30340;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Optimization for Randomized Network Resource Allocation with Long-Term Constraints. (arXiv:2305.15558v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#38754;&#21521;&#38543;&#26426;&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#30340;&#22312;&#32447;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20046;&#26368;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#25968;&#23383;&#27169;&#25311;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#22312;&#32447;&#36164;&#28304;&#39044;&#30041;&#38382;&#39064;&#12290;&#32593;&#32476;&#30001;&#20004;&#20010;&#35745;&#31639;&#33410;&#28857;&#32452;&#25104;&#65292;&#36890;&#36807;&#26412;&#22320;&#36890;&#20449;&#38142;&#36335;&#36830;&#25509;&#12290;&#31995;&#32479;&#22312;&#31163;&#25955;&#26102;&#38388;&#20869;&#36816;&#34892;&#65307;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#31649;&#29702;&#21592;&#20250;&#22312;&#23454;&#38469;&#20316;&#19994;&#35831;&#27714;&#20043;&#21069;&#20026;&#26381;&#21153;&#22120;&#39044;&#30041;&#36164;&#28304;&#65292;&#36825;&#20123;&#39044;&#30041;&#20250;&#20135;&#29983;&#25104;&#26412;&#12290;&#28982;&#21518;&#65292;&#22312;&#35266;&#23519;&#21040;&#23458;&#25143;&#31471;&#35831;&#27714;&#20043;&#21518;&#65292;&#20316;&#19994;&#21487;&#33021;&#20250;&#20174;&#19968;&#20010;&#26381;&#21153;&#22120;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#26381;&#21153;&#22120;&#65292;&#20197;&#26368;&#22909;&#22320;&#36866;&#24212;&#38656;&#27714;&#65292;&#20294;&#36825;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#20256;&#36755;&#25104;&#26412;&#12290;&#22914;&#26524;&#26080;&#27861;&#28385;&#36275;&#26576;&#20123;&#20316;&#19994;&#35831;&#27714;&#65292;&#21017;&#20250;&#20135;&#29983;&#36829;&#35268;&#25104;&#26412;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#34987;&#38459;&#27490;&#30340;&#20316;&#19994;&#25903;&#20184;&#25104;&#26412;&#12290;&#30446;&#26631;&#26159;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#26368;&#23567;&#21270;&#24635;&#39044;&#35746;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#19968;&#23450;&#39044;&#31639;&#38480;&#21046;&#19979;&#32500;&#25252;&#32047;&#31215;&#36829;&#35268;&#21644;&#20256;&#36755;&#25104;&#26412;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21453;&#22797;&#21338;&#24328;&#38382;&#39064;&#65292;&#38024;&#23545;&#19968;&#31995;&#21015;&#25552;&#35758;&#30340;&#31574;&#30053;&#25353;&#38543;&#26426;&#39034;&#24207;&#36827;&#34892;&#39044;&#35746;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#20197;&#26399;&#26395;&#30340;&#24635;&#25104;&#26412;&#20026;&#22522;&#30784;&#65292;&#20026;&#20219;&#20309;&#26377;&#38480;&#30340;T&#26102;&#38388;&#27573;&#12290;&#25968;&#23383;&#27169;&#25311;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#20960;&#31181;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study an optimal online resource reservation problem in a simple communication network. The network is composed of two compute nodes linked by a local communication link. The system operates in discrete time; at each time slot, the administrator reserves resources for servers before the actual job requests are known. A cost is incurred for the reservations made. Then, after the client requests are observed, jobs may be transferred from one server to the other to best accommodate the demands by incurring an additional transport cost. If certain job requests cannot be satisfied, there is a violation that engenders a cost to pay for each of the blocked jobs. The goal is to minimize the overall reservation cost over finite horizons while maintaining the cumulative violation and transport costs under a certain budget limit. To study this problem, we first formalize it as a repeated game against nature where the reservations are drawn randomly according to a sequence of pro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#65292;&#20351;&#24471;&#23398;&#20064;&#36895;&#29575;&#38543;&#30528;&#26410;&#30693;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#21152;&#32039;&#23494;&#12290;</title><link>http://arxiv.org/abs/2305.15557</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#23398;&#20064;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#29575;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Non-Parametric Learning of Stochastic Differential Equations with Fast Rates of Convergence. (arXiv:2305.15557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15557
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#65292;&#20351;&#24471;&#23398;&#20064;&#36895;&#29575;&#38543;&#30528;&#26410;&#30693;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#21152;&#32039;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#26469;&#35782;&#21035;&#38750;&#32447;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#33539;&#24335;&#20381;&#36182;&#20110;&#29366;&#24577;&#30340;&#31163;&#25955;&#26102;&#38388;&#35266;&#27979;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#30456;&#24212;&#30340;Fokker-Planck&#26041;&#31243;&#30340;&#22522;&#20110;RKHS&#30340;&#36817;&#20284;&#25311;&#21512;&#21040;&#36825;&#20123;&#35266;&#27979;&#20540;&#65292;&#20174;&#32780;&#24471;&#20986;&#29702;&#35770;&#23398;&#20064;&#36895;&#29575;&#30340;&#20272;&#35745;&#20540;&#65292;&#36825;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#24403;&#26410;&#30693;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#36234;&#39640;&#26102;&#65292;&#29702;&#35770;&#20272;&#35745;&#20540;&#36234;&#26469;&#36234;&#32039;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#20869;&#26680;&#30340;&#65292;&#22240;&#27492;&#31163;&#32447;&#39044;&#22788;&#29702;&#21487;&#20197;&#22312;&#21407;&#21017;&#19978;&#24471;&#21040;&#26377;&#25928;&#30340;&#25968;&#20540;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel non-parametric learning paradigm for the identification of drift and diffusion coefficients of non-linear stochastic differential equations, which relies upon discrete-time observations of the state. The key idea essentially consists of fitting a RKHS-based approximation of the corresponding Fokker-Planck equation to such observations, yielding theoretical estimates of learning rates which, unlike previous works, become increasingly tighter when the regularity of the unknown drift and diffusion coefficients becomes higher. Our method being kernel-based, offline pre-processing may in principle be profitably leveraged to enable efficient numerical implementation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#37319;&#29992;&#26041;&#24046;&#32553;&#20943;&#21644;&#33258;&#36866;&#24212;&#25191;&#34892;&#31574;&#30053;&#36716;&#25442;&#25216;&#26415;&#65292;&#22312;&#30701;&#28903;&#21270;&#26102;&#38388;MDPs&#19978;&#23454;&#29616;&#20102;&#36951;&#25022;&#26368;&#20248;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#24615;&#21644;&#38656;&#35201;&#20184;&#20986;&#39640;&#26114;&#20869;&#23384;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15546</link><description>&lt;p&gt;
&#30701;&#28903;&#21270;&#26102;&#38388;MDPs&#19978;&#20855;&#26377;&#36951;&#25022;&#26368;&#20248;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time. (arXiv:2305.15546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15546
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#37319;&#29992;&#26041;&#24046;&#32553;&#20943;&#21644;&#33258;&#36866;&#24212;&#25191;&#34892;&#31574;&#30053;&#36716;&#25442;&#25216;&#26415;&#65292;&#22312;&#30701;&#28903;&#21270;&#26102;&#38388;MDPs&#19978;&#23454;&#29616;&#20102;&#36951;&#25022;&#26368;&#20248;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#24615;&#21644;&#38656;&#35201;&#20184;&#20986;&#39640;&#26114;&#20869;&#23384;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#22312;&#32447;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#26080;&#38480;&#26102;&#27573;&#25240;&#25187;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#12290;&#29616;&#26377;&#31639;&#27861;&#35201;&#20040;&#26080;&#27861;&#23454;&#29616;&#36951;&#25022;&#26368;&#20248;&#24615;&#65292;&#35201;&#20040;&#38656;&#35201;&#20184;&#20986;&#39640;&#26114;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#22312;&#29616;&#26377;&#30340;&#26368;&#20248;&#31639;&#27861;&#20013;&#65292;&#20026;&#20102;&#23454;&#29616;&#26368;&#20248;&#26679;&#26412;&#25928;&#29575;&#65292;&#25152;&#26377;&#31639;&#27861;&#37117;&#35201;&#32463;&#36807;&#36739;&#38271;&#30340;&#28903;&#21270;&#26102;&#38388;&#65292;&#21363;&#21482;&#26377;&#26679;&#26412;&#23481;&#37327;&#36229;&#36807;&#19968;&#20010;&#39640;&#38408;&#20540;&#25165;&#33021;&#20445;&#35777;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#26041;&#24046;&#32553;&#20943;&#21644;&#19968;&#31181;&#24930;&#32780;&#33258;&#36866;&#24212;&#30340;&#25191;&#34892;&#31574;&#30053;&#36716;&#25442;&#25216;&#26415;&#12290;&#36825;&#26159;&#25240;&#25187;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#20855;&#26377;&#36951;&#25022;&#26368;&#20248;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#20302;&#28903;&#21270;&#26102;&#38388;&#30340;&#39069;&#22806;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial problem in reinforcement learning is learning the optimal policy. We study this in tabular infinite-horizon discounted Markov decision processes under the online setting. The existing algorithms either fail to achieve regret optimality or have to incur a high memory and computational cost. In addition, existing optimal algorithms all require a long burn-in time in order to achieve optimal sample efficiency, i.e., their optimality is not guaranteed unless sample size surpasses a high threshold. We address both open problems by introducing a model-free algorithm that employs variance reduction and a novel technique that switches the execution policy in a slow-yet-adaptive manner. This is the first regret-optimal model-free algorithm in the discounted setting, with the additional benefit of a low burn-in time.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; TOAST &#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20803;&#32032;&#24182;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15542</link><description>&lt;p&gt;
&#32858;&#28966;&#26159;&#36801;&#31227;&#23398;&#20064;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Refocusing Is Key to Transfer Learning. (arXiv:2305.15542v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; TOAST &#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20803;&#32032;&#24182;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#28041;&#21450;&#23558;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36866;&#24212;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#24120;&#24120;&#26080;&#27861;&#32858;&#28966;&#20110;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;-Top-Down Attention Steering&#65288;TOAST&#65289;&#65292;&#23427;&#20445;&#25345;&#39044;&#20808;&#35757;&#32451;&#30340;&#39592;&#24178;&#32467;&#26500;&#19981;&#21464;&#65292;&#21516;&#26102;&#36873;&#25321;&#36755;&#20986;&#20013;&#19982;&#20219;&#21153;&#26377;&#20851;&#30340;&#20803;&#32032;&#65292;&#24182;&#23558;&#23427;&#20204;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#20197;&#24341;&#23548;&#20854;&#27880;&#24847;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#12290;&#20165;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;TOAST&#22312;&#35768;&#22810;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#12289;LoRA&#21644;&#25552;&#31034;&#24494;&#35843;&#30456;&#27604;&#65292;TOAST&#22312;&#19968;&#31995;&#21015;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65288;&#20363;&#22914;&#65292;&#22312; FGVC &#19978;&#20174; 81.1% &#25552;&#39640;&#21040; 86.2%&#65289;&#26174;&#30528;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;TOAST&#22312;&#25351;&#20196;&#36319;&#38543;&#26041;&#38754;&#20063;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340; Alpaca &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning involves adapting a pre-trained model to novel downstream tasks. However, we observe that current transfer learning methods often fail to focus on task-relevant features. In this work, we emphasize the importance of refocusing the attention in transfer learning. We introduce Top-Down Attention Steering (TOAST), a novel transfer learning algorithm that keeps the pre-trained backbone frozen, while selecting the task-relevant elements in the output and feeding them back to the model to steer its attention to the task-specific features. By refocusing the attention only, TOAST achieves state-of-the-art results on a number of transfer learning benchmarks, while having a small portion of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt tuning, TOAST substantially improves performance across a range of fine-grained visual classification datasets (e.g., 81.1% -&gt; 86.2% on FGVC). TOAST also outperforms the fully fine-tuned Alpaca model on instruction-following
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#25277;&#26679;&#65292;&#36807;&#28388;&#25481;&#19981;&#31526;&#21512;&#26368;&#32456;&#29992;&#25143;&#36873;&#25321;&#30340;&#24230;&#37327;&#25351;&#26631;&#30340;&#21512;&#25104;&#25968;&#25454;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;</title><link>http://arxiv.org/abs/2305.15538</link><description>&lt;p&gt;
&#25913;&#21892;&#36873;&#25321;&#24615;&#24230;&#37327;&#30340;&#31169;&#26377;&#21512;&#25104;&#25968;&#25454;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Post-processing Private Synthetic Data for Improving Utility on Selected Measures. (arXiv:2305.15538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#25277;&#26679;&#65292;&#36807;&#28388;&#25481;&#19981;&#31526;&#21512;&#26368;&#32456;&#29992;&#25143;&#36873;&#25321;&#30340;&#24230;&#37327;&#25351;&#26631;&#30340;&#21512;&#25104;&#25968;&#25454;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31169;&#26377;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#24573;&#30053;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#26159;&#26368;&#32456;&#29992;&#25143;&#21487;&#33021;&#26377;&#29305;&#23450;&#30340;&#38656;&#27714;&#65292;&#21512;&#25104;&#25968;&#25454;&#24517;&#39035;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#25968;&#25454;&#30340;&#19979;&#28216;&#29992;&#36884;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#36873;&#25321;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#25552;&#39640;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#28041;&#21450;&#20174;&#21512;&#25104;&#25968;&#25454;&#20013;&#37325;&#26032;&#25277;&#26679;&#65292;&#36807;&#28388;&#25481;&#19981;&#28385;&#36275;&#25152;&#36873;&#25928;&#29992;&#24230;&#37327;&#30340;&#26679;&#26412;&#65292;&#20351;&#29992;&#26377;&#25928;&#30340;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#23547;&#25214;&#26368;&#20248;&#30340;&#37325;&#26032;&#25277;&#26679;&#26435;&#37325;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22987;&#32456;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#20013;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing private synthetic data generation algorithms are agnostic to downstream tasks. However, end users may have specific requirements that the synthetic data must satisfy. Failure to meet these requirements could significantly reduce the utility of the data for downstream use. We introduce a post-processing technique that improves the utility of the synthetic data with respect to measures selected by the end user, while preserving strong privacy guarantees and dataset quality. Our technique involves resampling from the synthetic data to filter out samples that do not meet the selected utility measures, using an efficient stochastic first-order algorithm to find optimal resampling weights. Through comprehensive numerical experiments, we demonstrate that our approach consistently improves the utility of synthetic data across multiple benchmark datasets and state-of-the-art synthetic data generation algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#37327;&#21270;Seq2seq&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24863;&#30693;&#33539;&#25968;&#34928;&#20943;&#25216;&#26415;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#37327;&#21270;&#33539;&#22260;&#22238;&#20256;&#26102;&#32570;&#20047;&#27491;&#21017;&#21270;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26356;&#26159;&#22914;&#27492;&#12290;</title><link>http://arxiv.org/abs/2305.15536</link><description>&lt;p&gt;
RAND:&#29992;&#20110;&#37327;&#21270;Seq2seq&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24863;&#30693;&#33539;&#25968;&#34928;&#20943;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
RAND: Robustness Aware Norm Decay For Quantized Seq2seq Models. (arXiv:2305.15536v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#37327;&#21270;Seq2seq&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24863;&#30693;&#33539;&#25968;&#34928;&#20943;&#25216;&#26415;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#37327;&#21270;&#33539;&#22260;&#22238;&#20256;&#26102;&#32570;&#20047;&#27491;&#21017;&#21270;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26356;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#27169;&#22411;&#21387;&#32553;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#37327;&#21270;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#38477;&#20302;&#22823;&#22411;&#27169;&#22411;&#30340;&#22823;&#23567;&#12289;&#20869;&#23384;&#35775;&#38382;&#21644;&#35745;&#31639;&#36127;&#36733;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#27969;&#34892;&#25216;&#26415;&#22312;4&#20301;Seq2seq&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22810;&#20010;&#35821;&#38899;&#35782;&#21035;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#22522;&#20110;&#22122;&#22768;&#30340;&#37327;&#21270;&#25216;&#26415;QAT&#22312;&#37327;&#21270;&#33539;&#22260;&#22238;&#20256;&#26102;&#32570;&#20047;&#27491;&#21017;&#21270;&#20449;&#21495;&#26102;&#20250;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid increase in the size of neural networks, model compression has become an important area of research. Quantization is an effective technique at decreasing the model size, memory access, and compute load of large models. Despite recent advances in quantization aware training (QAT) technique, most papers present evaluations that are focused on computer vision tasks, which have different training dynamics compared to sequence tasks. In this paper, we first benchmark the impact of popular techniques such as straight through estimator, pseudo-quantization noise, learnable scale parameter, clipping, etc. on 4-bit seq2seq models across a suite of speech recognition datasets ranging from 1,000 hours to 1 million hours, as well as one machine translation dataset to illustrate its applicability outside of speech.  Through the experiments, we report that noise based QAT suffers when there is insufficient regularization signal flowing back to the quantization scale. We propose low co
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#25913;&#21892;&#25628;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20195;&#34920;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;Pinterest&#24179;&#21488;&#19978;&#23454;&#39564;&#21644;&#37096;&#32626;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#26679;&#21270;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#32654;&#23481;&#21644;&#26102;&#23578;&#31867;&#21035;&#20013;&#19981;&#21516;&#32932;&#33394;&#30340;&#20195;&#34920;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15534</link><description>&lt;p&gt;
&#22312;&#25628;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#22312;&#32447;&#34920;&#31034;&#24456;&#37325;&#35201;&#65306;&#23454;&#29992;&#30340;&#31471;&#21040;&#31471;&#22810;&#26679;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation Online Matters: Practical End-to-End Diversification in Search and Recommender Systems. (arXiv:2305.15534v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15534
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25913;&#21892;&#25628;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20195;&#34920;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;Pinterest&#24179;&#21488;&#19978;&#23454;&#39564;&#21644;&#37096;&#32626;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#26679;&#21270;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#32654;&#23481;&#21644;&#26102;&#23578;&#31867;&#21035;&#20013;&#19981;&#21516;&#32932;&#33394;&#30340;&#20195;&#34920;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#24179;&#21488;&#22312;&#21508;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#20013;&#30340;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#65292;&#29992;&#25143;&#32463;&#24120;&#34920;&#36798;&#24076;&#26395;&#22312;&#20869;&#23481;&#20013;&#24863;&#21463;&#21040;&#33258;&#24049;&#30340;&#20195;&#34920;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#25628;&#32034;&#32467;&#26524;&#21644;&#25512;&#33616;&#20013;&#30340;&#20195;&#34920;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31471;&#21040;&#31471;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#22810;&#26679;&#21270;&#20869;&#23481;&#22312;&#36825;&#20123;&#31995;&#32479;&#30340;&#21508;&#20010;&#38454;&#27573;&#20013;&#27969;&#21160;&#65292;&#20174;&#26816;&#32034;&#21040;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;Pinterest&#24179;&#21488;&#30340;&#29983;&#20135;&#30028;&#38754;&#20013;&#24320;&#21457;&#12289;&#23454;&#39564;&#21644;&#37096;&#32626;&#21487;&#25193;&#23637;&#30340;&#22810;&#26679;&#21270;&#26426;&#21046;&#65292;&#21253;&#25324;&#25628;&#32034;&#12289;&#30456;&#20851;&#20135;&#21697;&#21644;&#26032;&#29992;&#25143;&#20027;&#39029;&#65292;&#20197;&#25913;&#21892;&#32654;&#23481;&#21644;&#26102;&#23578;&#20869;&#23481;&#20013;&#19981;&#21516;&#32932;&#33394;&#30340;&#20195;&#34920;&#24615;&#12290;&#29983;&#20135;&#31995;&#32479;&#20013;&#30340;&#22810;&#26679;&#21270;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#30830;&#23450;&#20250;&#35302;&#21457;&#22810;&#26679;&#21270;&#30340;&#35831;&#27714;&#65292;&#22312;&#26816;&#32034;&#38454;&#27573;&#30830;&#20445;&#20174;&#22823;&#22411;&#20869;&#23481;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#21040;&#22810;&#26679;&#21270;&#30340;&#20869;&#23481;&#65292;&#26368;&#21518;&#65292;&#22312;&#25490;&#21517;&#38454;&#27573;&#20197;&#33258;&#25105;&#35843;&#25972;&#30340;&#26041;&#24335;&#24179;&#34913;&#22810;&#26679;&#24615;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#20351;&#29992;Strong-O&#24320;&#22987;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of online platforms continues to grow across all demographics, users often express a desire to feel represented in the content. To improve representation in search results and recommendations, we introduce end-to-end diversification, ensuring that diverse content flows throughout the various stages of these systems, from retrieval to ranking. We develop, experiment, and deploy scalable diversification mechanisms in multiple production surfaces on the Pinterest platform, including Search, Related Products, and New User Homefeed, to improve the representation of different skin tones in beauty and fashion content. Diversification in production systems includes three components: identifying requests that will trigger diversification, ensuring diverse content is retrieved from the large content corpus during the retrieval stage, and finally, balancing the diversity-utility trade-off in a self-adjusting manner in the ranking stage. Our approaches, which evolved from using Strong-O
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#32534;&#36753;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#12290;&#36890;&#36807;&#32534;&#36753;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#20462;&#22797;&#39044;&#27979;&#38169;&#35823;&#65292;&#24182;&#19981;&#24433;&#21709;&#20854;&#20182;&#26410;&#21463;&#24433;&#21709;&#30340;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;GNN&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.15529</link><description>&lt;p&gt;
&#21487;&#32534;&#36753;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Editable Graph Neural Network for Node Classifications. (arXiv:2305.15529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#32534;&#36753;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#12290;&#36890;&#36807;&#32534;&#36753;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#20462;&#22797;&#39044;&#27979;&#38169;&#35823;&#65292;&#24182;&#19981;&#24433;&#21709;&#20854;&#20182;&#26410;&#21463;&#24433;&#21709;&#30340;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;GNN&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#35768;&#22810;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#37329;&#34701;&#32593;&#32476;&#20013;&#30340;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#21644;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#21463;&#35757;&#32451;&#30340;GNN&#20173;&#28982;&#20250;&#20986;&#29616;&#38169;&#35823;&#65292;&#24182;&#19988;&#36825;&#20123;&#38169;&#35823;&#21487;&#33021;&#23545;&#31038;&#20250;&#36896;&#25104;&#20005;&#37325;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290; &#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#8220;&#27169;&#22411;&#32534;&#36753;&#8221;&#24050;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#35813;&#26041;&#27861;&#22312;&#32416;&#27491;&#38169;&#35823;&#39044;&#27979;&#26102;&#19981;&#24433;&#21709;&#26410;&#34987;&#35302;&#21450;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;GNN&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;GNN&#30340;&#27169;&#22411;&#32534;&#36753;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20250;&#26174;&#33879;&#38477;&#20302;GNN&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#65288;&#39640;&#36798;50&#65285;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#65289;&#65292;&#32780;&#22312;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20013;&#21482;&#26377;&#36731;&#24494;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#32972;&#21518;&#30340;&#21407;&#29702;&#26159;GNN&#20013;&#30340;&#33410;&#28857;&#32858;&#21512;&#23558;&#20256;&#25773;&#32534;&#36753;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Despite Graph Neural Networks (GNNs) have achieved prominent success in many graph-based learning problem, such as credit risk assessment in financial networks and fake news detection in social networks. However, the trained GNNs still make errors and these errors may cause serious negative impact on society. \textit{Model editing}, which corrects the model behavior on wrongly predicted target samples while leaving model predictions unchanged on unrelated samples, has garnered significant interest in the fields of computer vision and natural language processing. However, model editing for graph neural networks (GNNs) is rarely explored, despite GNNs' widespread applicability. To fill the gap, we first observe that existing model editing methods significantly deteriorate prediction accuracy (up to $50\%$ accuracy drop) in GNNs while a slight accuracy drop in multi-layer perception (MLP). The rationale behind this observation is that the node aggregation in GNNs will spread the editing e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#20581;&#24247;&#24212;&#29992;&#65292;&#21482;&#38656;&#23569;&#37327;&#35843;&#25972;&#20415;&#33021;&#25429;&#25417;&#20581;&#24247;&#39046;&#22495;&#30340;&#25968;&#23383;&#25968;&#25454;&#24182;&#22312;&#20020;&#24202;&#21644;&#20581;&#24247;&#29615;&#22659;&#19979;&#25512;&#29702;&#21450;&#21442;&#19982;&#21508;&#39033;&#20581;&#24247;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15525</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#20581;&#24247;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Few-Shot Health Learners. (arXiv:2305.15525v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#20581;&#24247;&#24212;&#29992;&#65292;&#21482;&#38656;&#23569;&#37327;&#35843;&#25972;&#20415;&#33021;&#25429;&#25417;&#20581;&#24247;&#39046;&#22495;&#30340;&#25968;&#23383;&#25968;&#25454;&#24182;&#22312;&#20020;&#24202;&#21644;&#20581;&#24247;&#29615;&#22659;&#19979;&#25512;&#29702;&#21450;&#21442;&#19982;&#21508;&#39033;&#20581;&#24247;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25429;&#25417;&#23454;&#29616;&#23454;&#38469;&#20219;&#21153;&#20013;&#26377;&#29992;&#30340;&#20016;&#23500;&#27010;&#24565;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#35821;&#35328;&#30340;&#27169;&#22411;&#20855;&#26377;&#23616;&#38480;&#24615;&#12290;&#20581;&#24247;&#24212;&#29992;&#35201;&#27714;&#27169;&#22411;&#22312;&#25968;&#23383;&#25968;&#25454;(&#20363;&#22914;&#65292;&#20020;&#24202;&#39046;&#22495;&#20013;&#30340;&#29983;&#21629;&#20307;&#24449;&#12289;&#23454;&#39564;&#23460;&#20540;&#65307;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#27493;&#25968;&#12289;&#36816;&#21160;)&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#25968;&#23383;&#25968;&#25454;&#22312;&#29616;&#26377;&#35757;&#32451;&#35821;&#26009;&#20013;&#24456;&#38590;&#25110;&#19981;&#33021;&#29992;&#25991;&#26412;&#36731;&#26494;&#34920;&#36798;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#38656;&#36827;&#34892;&#23569;&#37327;&#35843;&#25972;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20415;&#33021;&#22815;&#23558;&#21508;&#31181;&#29983;&#29702;&#21644;&#34892;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19982;&#22810;&#31181;&#20581;&#24247;&#20219;&#21153;&#32852;&#31995;&#36215;&#26469;&#65292;&#36866;&#29992;&#20110;&#20020;&#24202;&#21644;&#20581;&#24247;&#29615;&#22659;&#12290;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#21307;&#30103;&#20256;&#24863;&#22120;&#35760;&#24405;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#33021;&#21147;&#65292;&#24182;&#24212;&#29992;&#20110;&#24515;&#33039;&#20449;&#21495;&#20998;&#26512;&#12289;&#29289;&#29702;&#27963;&#21160;&#35782;&#21035;&#12289;&#20195;&#35874;&#35745;&#31639;(&#20363;&#22914;&#65292;&#29123;&#28903;&#30340;&#21345;&#36335;&#37324;)&#20197;&#21450;&#21387;&#21147;&#25253;&#21578;&#21644;&#24515;&#29702;&#20581;&#24247;&#31579;&#26597;&#30340;&#20272;&#35745;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can capture rich representations of concepts that are useful for real-world tasks. However, language alone is limited. While existing LLMs excel at text-based inferences, health applications require that models be grounded in numerical data (e.g., vital signs, laboratory values in clinical domains; steps, movement in the wellness domain) that is not easily or readily expressed as text in existing training corpus. We demonstrate that with only few-shot tuning, a large language model is capable of grounding various physiological and behavioral time-series data and making meaningful inferences on numerous health tasks for both clinical and wellness contexts. Using data from wearable and medical sensor recordings, we evaluate these capabilities on the tasks of cardiac signal analysis, physical activity recognition, metabolic calculation (e.g., calories burned), and estimation of stress reports and mental health screeners.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$p$-NormSoftmax&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15508</link><description>&lt;p&gt;
&#36890;&#36807;&#20107;&#21518;&#23545;&#25968;&#24402;&#19968;&#21270;&#21644;&#28201;&#24230;&#32553;&#25918;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving selective classification performance of deep neural networks through post-hoc logit normalization and temperature scaling. (arXiv:2305.15508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$p$-NormSoftmax&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#28508;&#22312;&#38169;&#35823;&#36890;&#36807;&#25918;&#24323;&#20302;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#38024;&#23545;&#30340;&#26159;&#20248;&#21270;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#35823;&#20998;&#31867;&#26816;&#27979;&#24615;&#33021;&#65292;&#21363;&#36890;&#36807;&#23558;&#26356;&#39640;&#30340;&#32622;&#20449;&#24230;&#20540;&#20998;&#37197;&#32473;&#27491;&#30830;&#30340;&#39044;&#27979;&#26469;&#21306;&#20998;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;$p$-NormSoftmax&#65292;&#36890;&#36807;&#23545;&#25968;&#36827;&#34892;$p$-&#33539;&#25968;&#24402;&#19968;&#21270;&#21644;&#28201;&#24230;&#32553;&#25918;&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of selective classification for deep neural networks, where a model is allowed to abstain from low-confidence predictions to avoid potential errors. Specifically, we tackle the problem of optimizing the confidence estimator of a fixed classifier, aiming to enhance its misclassification detection performance, i.e., its ability to discriminate between correct and incorrect predictions by assigning higher confidence values to the correct ones. Previous work has found that different classifiers exhibit varying levels of misclassification detection performance, particularly when using the maximum softmax probability (MSP) as a measure of confidence. However, we argue that these findings are mainly due to a sub-optimal confidence estimator being used for each model. To overcome this issue, we propose a simple and efficient post-hoc confidence estimator, named $p$-NormSoftmax, which consists of transforming the logits through $p$-norm normalization and tempera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#20108;&#27425;&#27969;&#24418;&#23545;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#36827;&#34892;&#27491;&#21017;&#27169;&#22411;&#31616;&#21270;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#20302;&#32500;&#24615;&#65292;&#24182;&#22312;&#36229;&#20986;&#20854;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15490</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#20108;&#27425;&#27969;&#24418;&#36827;&#34892;&#21704;&#23494;&#39039;&#31995;&#32479;&#27491;&#21017;&#27169;&#22411;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Symplectic model reduction of Hamiltonian systems using data-driven quadratic manifolds. (arXiv:2305.15490v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#20108;&#27425;&#27969;&#24418;&#23545;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#36827;&#34892;&#27491;&#21017;&#27169;&#22411;&#31616;&#21270;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#20302;&#32500;&#24615;&#65292;&#24182;&#22312;&#36229;&#20986;&#20854;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#20108;&#27425;&#27969;&#24418;&#23545;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#36827;&#34892;&#27491;&#21017;&#27169;&#22411;&#31616;&#21270;&#12290;&#20256;&#32479;&#30340;&#27491;&#21017;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#37319;&#29992;&#32447;&#24615;&#27491;&#21017;&#23376;&#31354;&#38388;&#34920;&#31034;&#39640;&#32500;&#31995;&#32479;&#29366;&#24577;&#30340;&#20943;&#23569;&#32500;&#24230;&#22352;&#26631;&#31995;&#12290;&#34429;&#28982;&#36825;&#20123;&#36817;&#20284;&#32771;&#34385;&#20102;&#21704;&#23494;&#39039;&#31995;&#32479;&#27491;&#21017;&#24615;&#36136;&#65292;&#20294;&#26159;&#36817;&#20284;&#30340;&#32447;&#24615;&#24615;&#20351;&#24471;&#26080;&#27861;&#36798;&#21040;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#26368;&#36817;&#24320;&#21457;&#30340;&#20108;&#27425;&#27969;&#24418;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#27599;&#19968;&#31181;&#26041;&#27861;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#22312;&#29366;&#24577;&#36817;&#20284;&#20013;&#21152;&#20837;&#20108;&#27425;&#39033;&#65292;&#36825;&#26159;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#31034;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#20302;&#32500;&#24615;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#36229;&#20986;&#20854;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#35774;&#32622;&#20013;&#21457;&#20986;&#39044;&#27979;&#26102;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents two novel approaches for the symplectic model reduction of high-dimensional Hamiltonian systems using data-driven quadratic manifolds. Classical symplectic model reduction approaches employ linear symplectic subspaces for representing the high-dimensional system states in a reduced-dimensional coordinate system. While these approximations respect the symplectic nature of Hamiltonian systems, the linearity of the approximation imposes a fundamental limitation to the accuracy that can be achieved. We propose two different model reduction methods based on recently developed quadratic manifolds, each presenting its own advantages and limitations. The addition of quadratic terms in the state approximation, which sits at the heart of the proposed methodologies, enables us to better represent intrinsic low-dimensionality in the problem at hand. Both approaches are effective for issuing predictions in settings well outside the range of their training data while providing mor
&lt;/p&gt;</description></item><item><title>SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;</title><link>http://arxiv.org/abs/2305.15486</link><description>&lt;p&gt;
SPRING: GPT-4&#36890;&#36807;&#23398;&#20064;&#35770;&#25991;&#21644;&#25512;&#29702;&#22312;&#28216;&#25103;&#20013;&#34920;&#29616;&#36229;&#36807;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning. (arXiv:2305.15486v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15486
&lt;/p&gt;
&lt;p&gt;
SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#30001;&#20110;&#20854;&#22810;&#20219;&#21153;&#12289;&#28145;&#24230;&#25506;&#32034;&#21644;&#30446;&#26631;&#20248;&#20808;&#32423;&#35201;&#27714;&#65292;&#23545;AI&#31639;&#27861;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#28216;&#25103;&#26041;&#38754;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#23427;&#22312;&#20687;Crafter&#25110;Minecraft&#36825;&#26679;&#22797;&#26434;&#30340;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SPRING&#65292;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#32473;&#23450;LaTeX&#28304;&#20316;&#20026;&#28216;&#25103;&#35821;&#22659;&#21644;&#20195;&#29702;&#24403;&#21069;&#35266;&#23519;&#30340;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;SPRING&#26694;&#26550;&#21033;&#29992;&#20855;&#26377;&#28216;&#25103;&#30456;&#20851;&#38382;&#39064;&#30340;&#23450;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20316;&#20026;&#33410;&#28857;&#21644;&#20381;&#36182;&#20851;&#31995;&#20316;&#20026;&#36793;&#12290;&#36890;&#36807;&#25353;&#25299;&#25169;&#39034;&#24207;&#36941;&#21382;DAG&#24182;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#30340;LLM&#21709;&#24212;&#26469;&#30830;&#23450;&#22312;&#29615;&#22659;&#20013;&#37319;&#21462;&#30340;&#26368;&#20248;&#34892;&#21160;&#65292;LLM&#23545;&#26368;&#32456;&#33410;&#28857;&#30340;&#31572;&#26696;&#30452;&#25509;&#36716;&#21270;&#20026;&#29615;&#22659;&#34892;&#21160;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;
&lt;/p&gt;
&lt;p&gt;
Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#22312;&#24179;&#34913;&#23545;&#25239;&#27169;&#22411;&#19979;&#30340;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#65292;&#22312;&#35813;&#27169;&#22411;&#19979;&#65292;&#30740;&#31350;&#32773;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#24179;&#34913;&#29366;&#24577;&#19979;&#65292;&#22238;&#31572;&#37027;&#20123;&#20197;&#21069;&#34987;&#35748;&#20026;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#33258;&#36866;&#24212;&#26597;&#35810;&#21464;&#24471;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.15452</link><description>&lt;p&gt;
&#24179;&#34913;&#23545;&#25239;&#27169;&#22411;&#19979;&#30340;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Adaptive Data Analysis in a Balanced Adversarial Model. (arXiv:2305.15452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#22312;&#24179;&#34913;&#23545;&#25239;&#27169;&#22411;&#19979;&#30340;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#65292;&#22312;&#35813;&#27169;&#22411;&#19979;&#65292;&#30740;&#31350;&#32773;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#24179;&#34913;&#29366;&#24577;&#19979;&#65292;&#22238;&#31572;&#37027;&#20123;&#20197;&#21069;&#34987;&#35748;&#20026;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#33258;&#36866;&#24212;&#26597;&#35810;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#26426;&#21046;&#33719;&#21462;n&#20010;&#26469;&#33258;&#26410;&#30693;&#20998;&#24067;D&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#65292;&#24182;&#38656;&#35201;&#23545;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#32479;&#35745;&#26597;&#35810;&#25552;&#20379;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;&#22312;&#19968;&#20010;&#25317;&#26377;&#21333;&#21521;&#20989;&#25968;&#30340;&#22522;&#30784;&#19978;&#65292;Hardt&#21644;Ullman(FOCS 2014)&#20197;&#21450;Steinke&#21644;Ullman(COLT 2015)&#34920;&#26126;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22238;&#31572;&#36229;&#36807;&#920;(n^2)&#20010;&#36866;&#24212;&#24615;&#26597;&#35810;&#26159;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36127;&#38754;&#32467;&#26524;&#24378;&#28872;&#20381;&#36182;&#20110;&#19968;&#20010;&#23545;&#25239;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#24471;&#23545;&#25239;&#20998;&#26512;&#21592;&#27604;&#26426;&#21046;&#26126;&#26174;&#22788;&#20110;&#20248;&#21183;&#22320;&#20301;&#65292;&#22240;&#20026;&#36873;&#25321;&#33258;&#36866;&#24212;&#26597;&#35810;&#30340;&#20998;&#26512;&#21592;&#20063;&#36873;&#25321;&#20102;&#22522;&#30784;&#20998;&#24067;D&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#20250;&#23545;&#25152;&#24471;&#21040;&#30340;&#38590;&#24230;&#32467;&#26524;&#30340;&#36866;&#29992;&#24615;&#25552;&#20986;&#38382;&#39064;&#8212;&#8212;&#20855;&#26377;&#22522;&#30784;&#20998;&#24067;D&#30340;&#23436;&#20840;&#30693;&#35782;&#30340;&#20998;&#26512;&#21592;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#24517;&#35201;&#21521;&#20165;&#25345;&#26377;&#26377;&#38480;&#25968;&#37327;D&#26679;&#26412;&#30340;&#26426;&#21046;&#21457;&#20986;&#32479;&#35745;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In adaptive data analysis, a mechanism gets $n$ i.i.d. samples from an unknown distribution $D$, and is required to provide accurate estimations to a sequence of adaptively chosen statistical queries with respect to $D$. Hardt and Ullman (FOCS 2014) and Steinke and Ullman (COLT 2015) showed that in general, it is computationally hard to answer more than $\Theta(n^2)$ adaptive queries, assuming the existence of one-way functions.  However, these negative results strongly rely on an adversarial model that significantly advantages the adversarial analyst over the mechanism, as the analyst, who chooses the adaptive queries, also chooses the underlying distribution $D$. This imbalance raises questions with respect to the applicability of the obtained hardness results -- an analyst who has complete knowledge of the underlying distribution $D$ would have little need, if at all, to issue statistical queries to a mechanism which only holds a finite number of samples from $D$.  We consider more 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#28145;&#24230;&#23398;&#20064;-&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#27861;&#29992;&#20110;&#21306;&#22495;&#20379;&#28909;&#32593;&#26684;&#20013;&#30340;&#27010;&#29575;&#29366;&#24577;&#20272;&#35745;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#20102;&#35813;&#26041;&#27861;&#30340;&#35745;&#31639;&#36807;&#31243;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15445</link><description>&lt;p&gt;
&#29992;&#20110;&#21306;&#22495;&#20379;&#28909;&#32593;&#27010;&#29575;&#29366;&#24577;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;-&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-enabled MCMC for Probabilistic State Estimation in District Heating Grids. (arXiv:2305.15445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#28145;&#24230;&#23398;&#20064;-&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#27861;&#29992;&#20110;&#21306;&#22495;&#20379;&#28909;&#32593;&#26684;&#20013;&#30340;&#27010;&#29575;&#29366;&#24577;&#20272;&#35745;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#20102;&#35813;&#26041;&#27861;&#30340;&#35745;&#31639;&#36807;&#31243;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28789;&#27963;&#30340;&#22478;&#24066;&#20379;&#28909;&#32593;&#26684;&#26159;&#26410;&#26469;&#20302;&#30899;&#33021;&#28304;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#30740;&#31350;&#36825;&#20123;&#32593;&#26684;&#20013;&#30340;&#27010;&#29575;&#29366;&#24577;&#20272;&#35745;&#65292;&#21363;&#22522;&#20110;&#37096;&#20998;&#29366;&#24577;&#27979;&#37327;&#26469;&#20272;&#35745;&#25152;&#26377;&#32593;&#26684;&#29366;&#24577;&#21464;&#37327;&#65288;&#20363;&#22914;&#21387;&#21147;&#12289;&#28201;&#24230;&#21644;&#36136;&#37327;&#27969;&#37327;&#65289;&#30340;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#21518;&#39564;&#29366;&#24577;&#20998;&#24067;&#19981;&#23646;&#20110;&#26631;&#20934;&#27010;&#29575;&#20998;&#24067;&#31867;&#21035;&#65292;&#22240;&#27492;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#27861;&#22312;&#32593;&#32476;&#28909;&#20132;&#25442;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#22312;&#32593;&#26684;&#29366;&#24577;&#31354;&#38388;&#20013;&#35780;&#20272;&#26679;&#26412;&#20197;&#20272;&#35745;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#12290;&#23558;&#28909;&#20132;&#25442;&#26679;&#26412;&#36716;&#25442;&#20026;&#32593;&#26684;&#29366;&#24577;&#30340;&#38750;&#32447;&#24615;&#26041;&#31243;&#27714;&#35299;&#20351;&#24471;&#35813;&#26041;&#27861;&#30340;&#35745;&#31639;&#36127;&#25285;&#37325;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#21152;&#36895;&#27492;&#36807;&#31243;&#65292;&#35813;&#32593;&#32476;&#34987;&#35757;&#32451;&#20197;&#36924;&#36817;&#31934;&#30830;&#20294;&#36739;&#24930;&#30340;&#38750;&#32447;&#24615;&#27714;&#35299;&#22120;&#30340;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#39640;&#24230;&#31934;&#30830;&#30340;&#21518;&#39564;&#20998;&#24067;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible district heating grids form an important part of future, low-carbon energy systems. We examine probabilistic state estimation in such grids, i.e., we aim to estimate the posterior probability distribution over all grid state variables such as pressures, temperatures, and mass flows conditional on measurements of a subset of these states. Since the posterior state distribution does not belong to a standard class of probability distributions, we use Markov Chain Monte Carlo (MCMC) sampling in the space of network heat exchanges and evaluate the samples in the grid state space to estimate the posterior. Converting the heat exchange samples into grid states by solving the non-linear grid equations makes this approach computationally burdensome. However, we propose to speed it up by employing a deep neural network that is trained to approximate the solution of the exact but slow non-linear solver. This novel approach is shown to deliver highly accurate posterior distributions both 
&lt;/p&gt;</description></item><item><title>PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15444</link><description>&lt;p&gt;
PromptNER: &#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15444
&lt;/p&gt;
&lt;p&gt;
PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29616;&#22312;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29616;&#25104;&#26041;&#27861;&#65292;&#20026;&#21508;&#31181;&#32463;&#20856;&#30340;NLP&#38382;&#39064;&#25552;&#20379;&#20102;&#23569;&#37327;&#26679;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26041;&#38754;&#20173;&#36828;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#31471;&#21040;&#31471;&#32467;&#26500;&#29702;&#35299;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#22312;&#26631;&#20934;&#26631;&#35760;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptNER&#65292;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#23569;&#26679;&#26412;&#21644;&#36328;&#39046;&#22495;NER&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;&#20026;&#20102;&#36866;&#24212;&#20219;&#20309;&#26032;&#30340;NER&#20219;&#21153;&#65292;PromptNER&#38656;&#35201;&#25552;&#20379;&#19968;&#32452;&#23454;&#20307;&#23450;&#20041;&#65292;&#38500;&#22522;&#26412;&#30340;&#23569;&#26679;&#26412;&#26679;&#20363;&#20197;&#22806;&#12290;&#32473;&#23450;&#36755;&#20837;&#21477;&#23376;&#65292;PromptNER&#25552;&#31034;LLM&#29983;&#25104;&#19968;&#20010;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#37322;&#65292;&#35777;&#26126;&#23427;&#20204;&#19982;&#25552;&#20379;&#30340;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#30340;&#20860;&#23481;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PromptNER&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;WikiAnn&#25968;&#25454;&#38598;&#19978;&#20026;&#36328;&#39046;&#22495;NER&#35774;&#23450;&#20102;&#26032;&#30340;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23569;&#37327;&#25968;&#25454;&#36827;&#34892;&#26032;&#22411;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#36716;&#31227;&#23398;&#20064;&#21644;&#36827;&#21270;&#39532;&#23572;&#21487;&#22827;&#33945;&#29305;&#21345;&#32599;&#38142;&#37319;&#26679;&#31639;&#27861;&#65292;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#36866;&#24212;&#24230;&#26223;&#35266;&#65292;&#20174;&#32780;&#21152;&#36895;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#27979;&#35797;&#21608;&#26399;&#12290;</title><link>http://arxiv.org/abs/2305.15441</link><description>&lt;p&gt;
&#29992;&#36827;&#21270;&#37319;&#26679;&#25913;&#21892;&#22522;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Improving few-shot learning-based protein engineering with evolutionary sampling. (arXiv:2305.15441v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23569;&#37327;&#25968;&#25454;&#36827;&#34892;&#26032;&#22411;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#36716;&#31227;&#23398;&#20064;&#21644;&#36827;&#21270;&#39532;&#23572;&#21487;&#22827;&#33945;&#29305;&#21345;&#32599;&#38142;&#37319;&#26679;&#31639;&#27861;&#65292;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#36866;&#24212;&#24230;&#26223;&#35266;&#65292;&#20174;&#32780;&#21152;&#36895;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#27979;&#35797;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26032;&#22411;&#21151;&#33021;&#34507;&#30333;&#36136;&#20173;&#28982;&#26159;&#19968;&#20010;&#32531;&#24930;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#36825;&#26159;&#30001;&#20110;&#21508;&#31181;&#34507;&#30333;&#36136;&#24037;&#31243;&#25361;&#25112;; &#29305;&#21035;&#26159;&#65292;&#22312;&#32473;&#23450;&#30340;&#23454;&#39564;&#20013;&#21487;&#20197;&#27979;&#35797;&#30340;&#34507;&#30333;&#21464;&#20307;&#25968;&#37327;&#36828;&#36828;&#19981;&#21450;&#25972;&#20010;&#24207;&#21015;&#31354;&#38388;&#30340;&#24191;&#38420;&#65292;&#23548;&#33268;&#20302;&#21629;&#20013;&#29575;&#21644;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#27979;&#35797;&#21608;&#26399;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#26469;&#35774;&#35745;&#26032;&#22411;&#34507;&#30333;&#36136;&#65292;&#26088;&#22312;&#21152;&#36895;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#27979;&#35797;&#21608;&#26399;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#19968;&#20010;&#23567;&#19988;&#20559;&#26012;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#32422;$10^5$&#25968;&#25454;&#28857;&#65292;$&lt;1\%$&#31215;&#26497;&#32467;&#26524;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#19968;&#31181;&#21322;&#30417;&#30563;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#25152;&#38656;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#31163;&#25955;&#36866;&#24212;&#24230;&#26223;&#35266;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#36827;&#21270;&#39532;&#23572;&#21487;&#22827;&#33945;&#29305;&#21345;&#32599;&#38142;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#36866;&#24212;&#24230;&#26223;&#35266;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;&#39640;&#36866;&#24212;&#24230;&#22522;&#22240;&#24182;&#36827;&#34892;&#31579;&#36873;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing novel functional proteins remains a slow and expensive process due to a variety of protein engineering challenges; in particular, the number of protein variants that can be experimentally tested in a given assay pales in comparison to the vastness of the overall sequence space, resulting in low hit rates and expensive wet lab testing cycles. In this paper, we propose a few-shot learning approach to novel protein design that aims to accelerate the expensive wet lab testing cycle and is capable of leveraging a training dataset that is both small and skewed ($\approx 10^5$ datapoints, $&lt; 1\%$ positive hits). Our approach is composed of two parts: a semi-supervised transfer learning approach to generate a discrete fitness landscape for a desired protein function and a novel evolutionary Monte Carlo Markov Chain sampling algorithm to more efficiently explore the fitness landscape. We demonstrate the performance of our approach by experimentally screening predicted high fitness gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32858;&#31867;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24456;&#22909;&#22320;&#24212;&#23545;&#20102;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15431</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#21033;&#29992;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring and Exploiting Data Heterogeneity in Recommendation. (arXiv:2305.15431v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32858;&#31867;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24456;&#22909;&#22320;&#24212;&#23545;&#20102;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#25968;&#25454;&#26159;&#25968;&#25454;&#39537;&#21160;&#25512;&#33616;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#25968;&#25454;&#24322;&#36136;&#24615;&#26159;&#22823;&#25968;&#25454;&#30340;&#20869;&#22312;&#29305;&#24615;&#65292;&#22312;&#29616;&#23454;&#25512;&#33616;&#31995;&#32479;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;&#23427;&#21453;&#26144;&#20102;&#23376;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#23646;&#24615;&#30340;&#24046;&#24322;&#12290;&#24573;&#30053;&#25512;&#33616;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21487;&#33021;&#20250;&#38480;&#21046;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25439;&#23475;&#23376;&#20154;&#21475;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20351;&#27169;&#22411;&#35823;&#23548;&#25968;&#25454;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#22312;&#25512;&#33616;&#30028;&#24182;&#27809;&#26377;&#21463;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#23427;&#28608;&#21457;&#25105;&#20204;&#20805;&#20998;&#25506;&#32034;&#21644;&#21033;&#29992;&#24322;&#36136;&#24615;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#24182;&#36741;&#21161;&#25968;&#25454;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#25506;&#35752;&#20102;&#25512;&#33616;&#25968;&#25454;&#20013;&#20004;&#31867;&#20856;&#22411;&#30340;&#24322;&#36136;&#24615;&#65292;&#21363;&#39044;&#27979;&#26426;&#21046;&#21644;&#21327;&#21464;&#37327;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21452;&#23618;&#32858;&#31867;&#26041;&#27861;&#25506;&#32034;&#24322;&#36136;&#24615;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26426;&#21046;&#21033;&#29992;&#20102;&#25366;&#25496;&#20986;&#26469;&#30340;&#24322;&#36136;&#24615;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive amounts of data are the foundation of data-driven recommendation models. As an inherent nature of big data, data heterogeneity widely exists in real-world recommendation systems. It reflects the differences in the properties among sub-populations. Ignoring the heterogeneity in recommendation data could limit the performance of recommendation models, hurt the sub-populational robustness, and make the models misled by biases. However, data heterogeneity has not attracted substantial attention in the recommendation community. Therefore, it inspires us to adequately explore and exploit heterogeneity for solving the above problems and assisting data analysis. In this work, we focus on exploring two representative categories of heterogeneity in recommendation data that is the heterogeneity of prediction mechanism and covariate distribution and propose an algorithm that explores the heterogeneity through a bilevel clustering method. Furthermore, the uncovered heterogeneity is exploite
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24102;&#26377;&#39069;&#22806;&#20837;&#21475;&#26377;&#30028;&#32422;&#26463;&#30340;&#25237;&#24433;&#30697;&#38453;&#36924;&#36817;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#39564;&#34920;&#26126;&#22312;&#31038;&#21306;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15430</link><description>&lt;p&gt;
&#24102;&#30028;&#25237;&#24433;&#30697;&#38453;&#36924;&#36817;&#21450;&#20854;&#22312;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bounded Projection Matrix Approximation with Applications to Community Detection. (arXiv:2305.15430v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24102;&#26377;&#39069;&#22806;&#20837;&#21475;&#26377;&#30028;&#32422;&#26463;&#30340;&#25237;&#24433;&#30697;&#38453;&#36924;&#36817;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#39564;&#34920;&#26126;&#22312;&#31038;&#21306;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#24102;&#26377;&#39069;&#22806;&#20837;&#21475;&#26377;&#30028;&#32422;&#26463;&#30340;&#25237;&#24433;&#30697;&#38453;&#36924;&#36817;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#31639;&#27861;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20984;&#22870;&#21169;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#31639;&#27861;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21322;&#23450;&#26494;&#24347;&#21644;&#35889;&#32858;&#31867;&#31561;&#31454;&#20105;&#26041;&#27861;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is an important problem in unsupervised learning. This paper proposes to solve a projection matrix approximation problem with an additional entrywise bounded constraint. Algorithmically, we introduce a new differentiable convex penalty and derive an alternating direction method of multipliers (ADMM) algorithm. Theoretically, we establish the convergence properties of the proposed algorithm. Numerical experiments demonstrate the superiority of our algorithm over its competitors, such as the semi-definite relaxation method and spectral clustering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#34928;&#20943;&#32423;&#32852;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;DC-UCB&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15428</link><description>&lt;p&gt;
&#22522;&#20110;&#34928;&#20943;&#32423;&#32852;&#27169;&#22411;&#30340;&#22312;&#32447;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Online Influence Maximization under Decreasing Cascade Model. (arXiv:2305.15428v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#34928;&#20943;&#32423;&#32852;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;DC-UCB&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#34928;&#20943;&#32423;&#32852;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#24066;&#22330;&#39281;&#21644;&#30340;&#24120;&#35265;&#29616;&#35937;&#65292;&#23545;&#29420;&#31435;&#32423;&#32852;&#27169;&#22411;&#36827;&#34892;&#20102;&#25512;&#24191;&#12290;&#22312;&#34928;&#20943;&#32423;&#32852;&#27169;&#22411;&#19979;&#65292;&#24433;&#21709;&#23581;&#35797;&#25104;&#21151;&#30340;&#27010;&#29575;&#20250;&#22312;&#20043;&#21069;&#22833;&#36133;&#30340;&#22522;&#30784;&#19978;&#20943;&#23567;&#12290;&#36825;&#31181;&#24433;&#21709;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#34987;&#29420;&#31435;&#32423;&#32852;&#27169;&#22411;&#21644;&#32447;&#24615;&#38408;&#20540;&#27169;&#22411;&#25152;&#24573;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DC-UCB&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#19982;IC&#27169;&#22411;&#19978;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30456;&#21516;&#12290;&#25105;&#20204;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online influence maximization (OIM) under a new model of decreasing cascade (DC). This model is a generalization of the independent cascade (IC) model by considering the common phenomenon of market saturation. In DC, the chance of an influence attempt being successful reduces with previous failures. The effect is neglected by previous OIM works under IC and linear threshold models. We propose the DC-UCB algorithm to solve this problem, which achieves a regret bound of the same order as the state-of-the-art works on the IC model. Extensive experiments on both synthetic and real datasets show the effectiveness of our algorithm.
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#22240;&#20026;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#33021;&#20250;&#23548;&#33268;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#36825;&#24433;&#21709;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;</title><link>http://arxiv.org/abs/2305.15425</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Language Model Tokenizers Introduce Unfairness Between Languages. (arXiv:2305.15425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15425
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#22240;&#20026;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#33021;&#20250;&#23548;&#33268;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#36825;&#24433;&#21709;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#65292;&#21363;&#20351;&#27809;&#26377;&#26126;&#30830;&#20026;&#27492;&#36827;&#34892;&#36807;&#35757;&#32451;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#36755;&#20986;&#36136;&#37327;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20998;&#35789;&#38454;&#27573;&#20986;&#29616;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#22788;&#29702;&#24046;&#24322;&#65292;&#29978;&#33267;&#22312;&#27169;&#22411;&#34987;&#35843;&#29992;&#20043;&#21069;&#23601;&#24050;&#32463;&#20986;&#29616;&#20102;&#12290;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#20197;&#26377;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#24046;&#24322;&#21487;&#39640;&#36798;15&#20493;&#12290;&#36825;&#20123;&#24046;&#24322;&#22312;&#25105;&#20204;&#35780;&#20272;&#30340;17&#31181;&#20998;&#35789;&#22120;&#20013;&#20173;&#28982;&#23384;&#22312;&#65292;&#21363;&#20351;&#23427;&#20204;&#26159;&#26377;&#24847;&#20026;&#22810;&#35821;&#35328;&#25903;&#25345;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#26576;&#20123;&#35821;&#35328;&#23545;&#30340;&#23383;&#31526;&#32423;&#21644;&#23383;&#33410;&#32423;&#27169;&#22411;&#20063;&#26174;&#31034;&#20986;4&#20493;&#20197;&#19978;&#30340;&#32534;&#30721;&#38271;&#24230;&#24046;&#24322;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent language models have shown impressive multilingual performance, even when not explicitly trained for it. Despite this, concerns have been raised about the quality of their outputs across different languages. In this paper, we show how disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked. The same text translated into different languages can have drastically different tokenization lengths, with differences up to 15 times in some cases. These disparities persist across the 17 tokenizers we evaluate, even if they are intentionally trained for multilingual support. Character-level and byte-level models also exhibit over 4 times the difference in the encoding length for some language pairs. This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided as context to the m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#29356;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#65292;&#20998;&#31867;&#31934;&#24230;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.15424</link><description>&lt;p&gt;
PulseNet: &#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#36827;&#34892;&#29356;ECG&#20449;&#21495;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines. (arXiv:2305.15424v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#29356;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#65292;&#20998;&#31867;&#31934;&#24230;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#29356;&#30340;&#24515;&#30005;&#22270;(ECG)&#38656;&#35201;&#29087;&#32451;&#30340;&#20861;&#21307;&#65292;&#20294;&#30446;&#21069;&#21487;&#29992;&#30340;&#20861;&#21307;&#24515;&#33039;&#30149;&#19987;&#23478;&#29992;&#20110;ECG&#35299;&#35835;&#21644;&#35786;&#26029;&#25903;&#25345;&#30340;&#25968;&#37327;&#26377;&#38480;&#12290;&#24320;&#21457;&#33258;&#21160;&#35780;&#20272;ECG&#24207;&#21015;&#30340;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20020;&#24202;&#21307;&#29983;&#23454;&#26102;&#32467;&#26524;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#26469;&#25913;&#21892;&#20861;&#21307;&#25252;&#29702;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#23558;&#29356;&#30340;&#24515;&#30005;&#22270;&#24207;&#21015;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;&#23558;ECG&#35760;&#24405;&#36716;&#25442;&#20026;8&#31186;&#30340;&#31532;&#20108;&#23548;&#32852;&#24207;&#21015;&#65292;&#26681;&#25454;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#25110;&#22810;&#31181;&#24515;&#33039;&#24322;&#24120;&#23558;&#20854;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;&#35757;&#32451;ECG&#24207;&#21015;&#20351;&#29992;RandomAugmentECG&#36827;&#34892;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#35813;&#39033;&#30446;&#23454;&#29616;&#30340;&#26032;&#22686;&#24378;&#24211;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#22359;&#20351;&#29992;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#36716;&#25442;&#25104;2D scalogram&#12290;2D scalogram&#20351;&#29992;&#20108;&#20803;CNN&#20998;&#31867;&#22120;&#20998;&#31867;&#25104;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating canine electrocardiograms (ECG) require skilled veterinarians, but current availability of veterinary cardiologists for ECG interpretation and diagnostic support is limited. Developing tools for automated assessment of ECG sequences can improve veterinary care by providing clinicians real-time results and decision support tools. We implement a deep convolutional neural network (CNN) approach for classifying canine electrocardiogram sequences as either normal or abnormal. ECG records are converted into 8 second Lead II sequences and classified as either normal (no evidence of cardiac abnormalities) or abnormal (presence of one or more cardiac abnormalities). For training ECG sequences are randomly augmented using RandomAugmentECG, a new augmentation library implemented specifically for this project. Each chunk is then is converted using a continuous wavelet transform into a 2D scalogram. The 2D scalogram are then classified as either normal or abnormal by a binary CNN classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21307;&#23398;&#25104;&#20687;&#20013;&#22270;&#20687;&#21512;&#25104;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;GAN&#22312;&#33041;&#37096;&#36328;&#27169;&#24577;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;GAN&#21487;&#20197;&#36890;&#36807;&#21367;&#31215;&#32593;&#32476;&#29983;&#25104;&#32570;&#22833;&#27169;&#24577;&#30340;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#37492;&#21035;&#22120;&#35782;&#21035;&#30495;&#23454;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.15421</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#33041;&#37096;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks for Brain Images Synthesis: A Review. (arXiv:2305.15421v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21307;&#23398;&#25104;&#20687;&#20013;&#22270;&#20687;&#21512;&#25104;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;GAN&#22312;&#33041;&#37096;&#36328;&#27169;&#24577;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;GAN&#21487;&#20197;&#36890;&#36807;&#21367;&#31215;&#32593;&#32476;&#29983;&#25104;&#32570;&#22833;&#27169;&#24577;&#30340;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#37492;&#21035;&#22120;&#35782;&#21035;&#30495;&#23454;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#22270;&#20687;&#21512;&#25104;&#26159;&#20174;&#21478;&#19968;&#24133;&#22270;&#20687;(&#24207;&#21015;&#12289;&#27169;&#24577;)&#20272;&#31639;&#20986;&#19968;&#24133;&#22270;&#20687;(&#24207;&#21015;&#12289;&#27169;&#24577;)&#30340;&#36807;&#31243;&#12290;&#22810;&#27169;&#24577;&#25104;&#20687;&#23545;&#20110;&#21307;&#23398;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#19981;&#21516;&#27169;&#24577;&#30340;&#22270;&#20687;&#25552;&#20379;&#20102;&#22810;&#26679;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#25429;&#25417;&#21508;&#31181;&#29305;&#24449;&#65292;&#32780;&#22810;&#27425;&#31579;&#26597;&#21017;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#36130;&#21147;&#65292;&#21516;&#26102;&#30001;&#25918;&#23556;&#31185;&#21307;&#24072;&#36827;&#34892;&#25253;&#21578;&#12290;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#33021;&#22815;&#20154;&#20026;&#22320;&#29983;&#25104;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#33258;&#21160;&#25429;&#33719;&#21644;&#25552;&#21462;&#39640;&#32500;&#29305;&#24449;&#12290;&#23588;&#20854;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#20316;&#20026;&#26368;&#27969;&#34892;&#30340;&#22522;&#20110;&#29983;&#25104;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20043;&#19968;&#65292;&#20351;&#29992;&#21367;&#31215;&#32593;&#32476;&#20316;&#20026;&#29983;&#25104;&#22120;&#65292;&#24182;&#36890;&#36807;&#37492;&#21035;&#22120;&#32593;&#32476;&#35782;&#21035;&#20272;&#31639;&#30340;&#22270;&#20687;&#26159;&#21542;&#20026;&#30495;&#23454;&#22270;&#20687;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;GAN&#22312;&#33041;&#37096;&#36328;&#27169;&#24577;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;CT&#21040;PET&#12289;CT&#21040;MRI&#12289;MRI&#21040;PET&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In medical imaging, image synthesis is the estimation process of one image (sequence, modality) from another image (sequence, modality). Since images with different modalities provide diverse biomarkers and capture various features, multi-modality imaging is crucial in medicine. While multi-screening is expensive, costly, and time-consuming to report by radiologists, image synthesis methods are capable of artificially generating missing modalities. Deep learning models can automatically capture and extract the high dimensional features. Especially, generative adversarial network (GAN) as one of the most popular generative-based deep learning methods, uses convolutional networks as generators, and estimated images are discriminated as true or false based on a discriminator network. This review provides brain image synthesis via GANs. We summarized the recent developments of GANs for cross-modality brain image synthesis including CT to PET, CT to MRI, MRI to PET, and vice versa.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29109;&#24863;&#30693;&#30456;&#20284;&#24615;&#30340;&#24179;&#34913;&#32858;&#31867;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;EASB&#65292;&#24182;&#22312;&#23454;&#38469;&#40657;&#33394;&#32032;&#30244;&#21307;&#23398;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.15417</link><description>&lt;p&gt;
&#29109;&#24863;&#30693;&#30456;&#20284;&#24230;&#29992;&#20110;&#24179;&#34913;&#32858;&#31867;: &#20197;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Entropy-Aware Similarity for Balanced Clustering: A Case Study with Melanoma Detection. (arXiv:2305.15417v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29109;&#24863;&#30693;&#30456;&#20284;&#24615;&#30340;&#24179;&#34913;&#32858;&#31867;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;EASB&#65292;&#24182;&#22312;&#23454;&#38469;&#40657;&#33394;&#32032;&#30244;&#21307;&#23398;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#25968;&#25454;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#19968;&#32452;&#25968;&#25454;&#28857;&#20998;&#25104;&#22810;&#20010;&#32452;&#12290;&#23427;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#20013;&#33267;&#20851;&#37325;&#35201;&#20294;&#35201;&#27714;&#20005;&#26684;&#30340;&#35838;&#39064;&#12290;&#20854;&#25104;&#21151;&#24212;&#29992;&#28085;&#30422;&#20102;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#32858;&#31867;&#25216;&#26415;&#38656;&#35201;&#22312;&#29305;&#23450;&#30340;&#24212;&#29992;&#20013;&#32771;&#34385;&#24179;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#32858;&#31867;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29109;&#24863;&#30693;&#30456;&#20284;&#24615;&#30340;&#24179;&#34913;&#32858;&#31867;&#26032;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#23450;&#20041;&#20026;&#24179;&#34913;&#24230;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#29992;&#20110;&#24179;&#34913;&#32858;&#31867;&#30340;&#29109;&#24863;&#30693;&#30456;&#20284;&#24230;&#65288;EASB&#65289;&#65292;&#36890;&#36807;&#23436;&#21892;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#20114;&#34917;&#32858;&#31867;&#21644;&#23558;&#29109;&#32467;&#21512;&#21040;&#26032;&#30340;&#30456;&#20284;&#24230;&#20844;&#24335;&#20013;&#65292;&#35813;&#20844;&#24335;&#32771;&#34385;&#20102;&#35282;&#24230;&#24046;&#24322;&#21644;&#36317;&#31163;&#65292;&#20174;&#32780;&#22312;&#32858;&#31867;&#36807;&#31243;&#20013;&#26368;&#22823;&#38480;&#24230;&#22320;&#23454;&#29616;&#24179;&#34913;&#12290;&#26412;&#25991;&#36824;&#38024;&#23545;&#23454;&#38469;&#40657;&#33394;&#32032;&#30244;&#21307;&#23398;&#25968;&#25454;&#36827;&#34892;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#65292;&#20855;&#20307;&#22320;&#35828;&#26159;&#20351;&#29992;&#20102;&#22269;&#38469;&#30382;&#32932;&#25104;&#20687;&#21512;&#20316;&#32452;&#32455;&#25552;&#20379;&#30340;&#22269;&#38469;&#40657;&#33394;&#32032;&#30244;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering data is an unsupervised learning approach that aims to divide a set of data points into multiple groups. It is a crucial yet demanding subject in machine learning and data mining. Its successful applications span various fields. However, conventional clustering techniques necessitate the consideration of balance significance in specific applications. Therefore, this paper addresses the challenge of imbalanced clustering problems and presents a new method for balanced clustering by utilizing entropy-aware similarity, which can be defined as the degree of balances. We have coined the term, entropy-aware similarity for balanced clustering (EASB), which maximizes balance during clustering by complementary clustering of unbalanced data and incorporating entropy in a novel similarity formula that accounts for both angular differences and distances. The effectiveness of the proposed approach is evaluated on actual melanoma medial data, specifically the International Skin Imaging Co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#36807;&#28193;&#37329;&#23646;&#21450;&#20854;&#27687;&#21270;&#29289;&#30340;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20013;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#30456;&#65292;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#34920;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#23545;X&#23556;&#32447;&#34893;&#23556;&#21644;&#26230;&#20307;&#32467;&#26500;&#30830;&#23450;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.15410</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#36807;&#28193;&#37329;&#23646;X&#23556;&#32447;&#34893;&#23556;&#30456;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Machine learning-assisted close-set X-ray diffraction phase identification of transition metals. (arXiv:2305.15410v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#36807;&#28193;&#37329;&#23646;&#21450;&#20854;&#27687;&#21270;&#29289;&#30340;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20013;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#30456;&#65292;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#34920;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#23545;X&#23556;&#32447;&#34893;&#23556;&#21644;&#26230;&#20307;&#32467;&#26500;&#30830;&#23450;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#34987;&#24212;&#29992;&#20110;X&#23556;&#32447;&#34893;&#23556;&#30456;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#36807;&#28193;&#37329;&#23646;&#21450;&#20854;&#27687;&#21270;&#29289;&#30340;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20013;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#30456;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#22810;&#31181;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#23545;X&#23556;&#32447;&#34893;&#23556;&#21644;&#26230;&#20307;&#32467;&#26500;&#30830;&#23450;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#24320;&#28304;&#23454;&#29616;&#65306;https://github.com/maxnygma/NeuralXRD.
&lt;/p&gt;
&lt;p&gt;
Machine learning has been applied to the problem of X-ray diffraction phase prediction with promising results. In this paper, we describe a method for using machine learning to predict crystal structure phases from X-ray diffraction data of transition metals and their oxides. We evaluate the performance of our method and compare the variety of its settings. Our results demonstrate that the proposed machine learning framework achieves competitive performance. This demonstrates the potential for machine learning to significantly impact the field of X-ray diffraction and crystal structure determination. Open-source implementation: https://github.com/maxnygma/NeuralXRD.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;&#65288;CoWorld&#65289;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#26465;&#20214;&#19979;&#35270;&#35273;RL&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24819;&#27861;&#26159;&#20351;&#29992;&#26131;&#20110;&#20132;&#20114;&#30340;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#36741;&#21161;RL&#27169;&#22411;&#20316;&#20026;&#31163;&#32447;&#31574;&#30053;&#30340;&#22312;&#32447;&#8220;&#27979;&#35797;&#24202;&#8221;&#65292;&#24182;&#25191;&#34892;&#22495;&#21327;&#20316;&#34920;&#31034;&#23398;&#20064;&#21644;&#22495;&#21327;&#20316;&#34892;&#20026;&#23398;&#20064;&#65292;&#32531;&#35299;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#20215;&#20540;&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15260</link><description>&lt;p&gt;
&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;: &#19968;&#31181;&#22312;&#32447;&#31163;&#32447;&#36716;&#31227;RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative World Models: An Online-Offline Transfer RL Approach. (arXiv:2305.15260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;&#65288;CoWorld&#65289;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#26465;&#20214;&#19979;&#35270;&#35273;RL&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24819;&#27861;&#26159;&#20351;&#29992;&#26131;&#20110;&#20132;&#20114;&#30340;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#36741;&#21161;RL&#27169;&#22411;&#20316;&#20026;&#31163;&#32447;&#31574;&#30053;&#30340;&#22312;&#32447;&#8220;&#27979;&#35797;&#24202;&#8221;&#65292;&#24182;&#25191;&#34892;&#22495;&#21327;&#20316;&#34920;&#31034;&#23398;&#20064;&#21644;&#22495;&#21327;&#20316;&#34892;&#20026;&#23398;&#20064;&#65292;&#32531;&#35299;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#20215;&#20540;&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#30001;&#20110;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#21644;&#20215;&#20540;&#20989;&#25968;&#20013;&#30340;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;&#65288;CoWorld&#65289;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#26465;&#20214;&#19979;&#35270;&#35273;RL&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24819;&#27861;&#26159;&#20351;&#29992;&#26131;&#20110;&#20132;&#20114;&#12289;&#29616;&#25104;&#30340;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#36741;&#21161;RL&#27169;&#22411;&#20316;&#20026;&#31163;&#32447;&#31574;&#30053;&#22312;&#30446;&#26631;&#22495;&#20013;&#23398;&#20064;&#30340;&#22312;&#32447;&#8220;&#27979;&#35797;&#24202;&#8221;&#65292;&#36825;&#20026;&#20215;&#20540;&#20989;&#25968;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#32422;&#26463;&#8212;&#8212;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#24819;&#22312;&#19981;&#22952;&#30861;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#30340;&#21160;&#20316;&#25506;&#32034;&#30340;&#24773;&#20917;&#19979;&#32531;&#35299;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#20215;&#20540;&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CoWorld&#25191;&#34892;&#22495;&#21327;&#20316;&#34920;&#31034;&#23398;&#20064;&#20197;&#24357;&#21512;&#22312;&#32447;&#21644;&#31163;&#32447;&#38544;&#34255;&#29366;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#23427;&#25191;&#34892;&#22495;&#21327;&#20316;&#34892;&#20026;&#23398;&#20064;&#65292;&#20351;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#22806;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#22312;&#32447;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training visual reinforcement learning (RL) models in offline datasets is challenging due to overfitting issues in representation learning and overestimation problems in value function. In this paper, we propose a transfer learning method called Collaborative World Models (CoWorld) to improve the performance of visual RL under offline conditions. The core idea is to use an easy-to-interact, off-the-shelf simulator to train an auxiliary RL model as the online "test bed" for the offline policy learned in the target domain, which provides a flexible constraint for the value function -- Intuitively, we want to mitigate the overestimation problem of value functions outside the offline data distribution without impeding the exploration of actions with potential advantages. Specifically, CoWorld performs domain-collaborative representation learning to bridge the gap between online and offline hidden state distributions. Furthermore, it performs domain-collaborative behavior learning that enab
&lt;/p&gt;</description></item><item><title>SyNDock&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#32452;&#35013;&#31934;&#30830;&#30340;&#22810;&#32858;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#65292;&#20855;&#26377;&#23398;&#20064;&#20840;&#23616;&#36716;&#25442;&#38382;&#39064;&#65292;&#21487;&#35757;&#32451;&#30340;&#20108;&#27493;SE&#65288;3&#65289;&#31639;&#27861;&#31561;&#20248;&#28857;</title><link>http://arxiv.org/abs/2305.15156</link><description>&lt;p&gt;
SyNDock: &#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#32676;&#21516;&#27493;&#36827;&#34892;N&#20010;&#21018;&#24615;&#34507;&#30333;&#36136;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
SyNDock: N Rigid Protein Docking via Learnable Group Synchronization. (arXiv:2305.15156v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15156
&lt;/p&gt;
&lt;p&gt;
SyNDock&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#32452;&#35013;&#31934;&#30830;&#30340;&#22810;&#32858;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#65292;&#20855;&#26377;&#23398;&#20064;&#20840;&#23616;&#36716;&#25442;&#38382;&#39064;&#65292;&#21487;&#35757;&#32451;&#30340;&#20108;&#27493;SE&#65288;3&#65289;&#31639;&#27861;&#31561;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#36807;&#31243;&#30340;&#35843;&#25511;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32454;&#32990;&#20869;&#30340;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#65292;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#23427;&#20204;&#30340;&#19977;&#32500;&#32467;&#26500;&#20197;&#38416;&#26126;&#22522;&#30784;&#26426;&#21046;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26694;&#26550;SyNDock&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#24555;&#36895;&#32452;&#35013;&#20986;&#31934;&#30830;&#30340;&#22810;&#32858;&#22797;&#21512;&#29289;&#65292;&#23637;&#31034;&#20102;&#21487;&#20197;&#36229;&#36234;&#25110;&#19982;&#26368;&#26032;&#30340;&#39640;&#32423;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;SyNDock&#20855;&#26377;&#20043;&#21069;&#26041;&#27861;&#20013;&#27809;&#26377;&#30340;&#20960;&#20010;&#21560;&#24341;&#20154;&#30340;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;SyNDock&#23558;&#22810;&#32858;&#34507;&#30333;&#36136;&#23545;&#25509;&#23450;&#20026;&#23398;&#20064;&#20840;&#23616;&#36716;&#25442;&#30340;&#38382;&#39064;&#65292;&#20197;&#25972;&#20307;&#25551;&#32472;&#22797;&#21512;&#29289;&#30340;&#38142;&#21333;&#20803;&#30340;&#25918;&#32622;&#65292;&#23454;&#29616;&#20197;&#23398;&#20064;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#27425;&#65292;SyNDock&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#20108;&#27493;SE&#65288;3&#65289;&#31639;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
The regulation of various cellular processes heavily relies on the protein complexes within a living cell, necessitating a comprehensive understanding of their three-dimensional structures to elucidate the underlying mechanisms. While neural docking techniques have exhibited promising outcomes in binary protein docking, the application of advanced neural architectures to multimeric protein docking remains uncertain. This study introduces SyNDock, an automated framework that swiftly assembles precise multimeric complexes within seconds, showcasing performance that can potentially surpass or be on par with recent advanced approaches. SyNDock possesses several appealing advantages not present in previous approaches. Firstly, SyNDock formulates multimeric protein docking as a problem of learning global transformations to holistically depict the placement of chain units of a complex, enabling a learning-centric solution. Secondly, SyNDock proposes a trainable two-step SE(3) algorithm, invol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2305.15151</link><description>&lt;p&gt;
&#30693;&#35782;&#35774;&#35745;&#65306;&#36890;&#36807;&#30693;&#35782;&#25552;&#28860;&#25512;&#21160;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement. (arXiv:2305.15151v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#65292;&#23547;&#25214;&#25240;&#21472;&#20026;&#25152;&#26399;&#26395;&#32467;&#26500;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#24050;&#32463;&#21462;&#24471;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#30053;&#20102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#37325;&#35201;&#24615;&#65292;&#26410;&#33021;&#35206;&#30422;&#24191;&#27867;&#30340;&#34507;&#30333;&#36136;&#31354;&#38388;&#65292;&#24182;&#19988;&#27809;&#26377;&#34701;&#20837;&#24120;&#35265;&#30340;&#34507;&#30333;&#36136;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#26469;&#33410;&#30465;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;CATH&#12289;TS50&#21644;TS500&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#30693;&#35782;&#35774;&#35745;&#26041;&#27861;&#22312;CATH&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;PiFold&#26041;&#27861;&#32422;9&#65285;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30693;&#35782;&#35774;&#35745;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown competitive performance in protein design that aims to find the amino acid sequence folding into the desired structure. However, most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. After witnessing the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, and TS500 datasets and our results show that our Knowledge-Design method outperforms the previous PiFold method by approximately 9\% on the CATH dataset. Specifically, Knowledge-Design is the first method that achieves 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26550;&#26500;&#20462;&#25913;&#21644;&#26032;&#39062;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.15001</link><description>&lt;p&gt;
&#22797;&#25968;&#20540;&#33258;&#32534;&#30721;&#22120;&#23545;&#29289;&#20307;&#21457;&#29616;&#30340;&#23545;&#27604;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Contrastive Training of Complex-Valued Autoencoders for Object Discovery. (arXiv:2305.15001v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15001
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26550;&#26500;&#20462;&#25913;&#21644;&#26032;&#39062;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#20013;&#24515;&#27169;&#22411;&#20351;&#29992;&#25554;&#27133;&#21644;&#27880;&#24847;&#21147;&#36335;&#30001;&#36827;&#34892;&#32465;&#23450;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#26377;&#20960;&#20010;&#27010;&#24565;&#24615;&#30340;&#23616;&#38480;&#24615;&#65306;&#25554;&#27133;&#30340;&#25968;&#37327;&#26159;&#30828;&#32534;&#30721;&#30340;&#65307;&#25152;&#26377;&#25554;&#27133;&#30340;&#23481;&#37327;&#30456;&#31561;&#65307;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65307;&#25554;&#27133;&#20869;&#27809;&#26377;&#30446;&#26631;&#32423;&#21035;&#30340;&#20851;&#31995;&#22240;&#32032;&#12290;&#21407;&#21017;&#19978;&#65292;&#22522;&#20110;&#21516;&#27493;&#24615;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22797;&#25968;&#20540;&#28608;&#27963;&#22312;&#20854;&#30456;&#20301;&#20998;&#37327;&#20013;&#23384;&#20648;&#32465;&#23450;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#22522;&#20110;&#21516;&#27493;&#24615;&#30340;&#27169;&#22411;&#30340;&#24037;&#20316;&#31034;&#20363;&#21482;&#26159;&#26368;&#36817;&#25165;&#26377;&#65292;&#32780;&#19988;&#23454;&#38469;&#19978;&#20173;&#28982;&#38480;&#20110;&#29609;&#20855;&#28784;&#24230;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#23384;&#20648;&#19981;&#21040;&#19977;&#20010;&#29289;&#20307;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26550;&#26500;&#20462;&#25913;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26497;&#22823;&#22320;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#21457;&#29616;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art object-centric models use slots and attention-based routing for binding. However, this class of models has several conceptual limitations: the number of slots is hardwired; all slots have equal capacity; training has high computational cost; there are no object-level relational factors within slots. Synchrony-based models in principle can address these limitations by using complex-valued activations which store binding information in their phase components. However, working examples of such synchrony-based models have been developed only very recently, and are still limited to toy grayscale datasets and simultaneous storage of less than three objects in practice. Here we introduce architectural modifications and a novel contrastive learning method that greatly improve the state-of-the-art synchrony-based model. For the first time, we obtain a class of synchrony-based models capable of discovering objects in an unsupervised manner in multi-object color datasets 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#35299;&#37322;&#20026;&#22522;&#20110;&#25928;&#29992;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20026;&#32534;&#30721;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26174;&#31034;&#30340;&#20559;&#22909;&#30340;&#24207;&#25968;&#25928;&#29992;&#20989;&#25968;&#65292;&#21487;&#20197;&#23558;SGD&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#23398;&#20064;&#21160;&#24577;&#35270;&#20026;&#23558;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21040;&#26368;&#20248;&#25928;&#29992;&#20989;&#25968;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#35774;&#35745;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2305.14859</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29992;-&#27010;&#29575;&#23545;&#20598;
&lt;/p&gt;
&lt;p&gt;
Utility-Probability Duality of Neural Networks. (arXiv:2305.14859v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14859
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#35299;&#37322;&#20026;&#22522;&#20110;&#25928;&#29992;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20026;&#32534;&#30721;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26174;&#31034;&#30340;&#20559;&#22909;&#30340;&#24207;&#25968;&#25928;&#29992;&#20989;&#25968;&#65292;&#21487;&#20197;&#23558;SGD&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#23398;&#20064;&#21160;&#24577;&#35270;&#20026;&#23558;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21040;&#26368;&#20248;&#25928;&#29992;&#20989;&#25968;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#35774;&#35745;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#25311;&#21512;&#25152;&#38656;&#36755;&#20986;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#35768;&#22810;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#24726;&#35770;&#29616;&#35937;&#35753;&#20154;&#20204;&#24576;&#30097;&#36825;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#35299;&#37322;&#26159;&#21542;&#33021;&#30495;&#27491;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#35299;&#37322;&#20026;&#22522;&#20110;&#25928;&#29992;&#30340;&#35299;&#37322;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#23558;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#19981;&#35299;&#37322;&#20026;&#27010;&#29575;&#27169;&#22411;&#65292;&#32780;&#35299;&#37322;&#20026;&#32534;&#30721;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26174;&#31034;&#30340;&#20559;&#22909;&#30340;&#24207;&#25968;&#25928;&#29992;&#20989;&#25968;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#23545;&#24212;&#20110;&#19968;&#20010;&#25928;&#29992;&#23398;&#20064;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#25152;&#26377;&#20855;&#26377;softmax&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#30340;SGD&#23398;&#20064;&#21160;&#24577;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#23558;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21040;&#26368;&#20248;&#25928;&#29992;&#20989;&#25968;&#12290;&#36825;&#20010;&#26694;&#26550;&#19981;&#20165;&#25552;&#20379;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#30340;&#26032;&#35299;&#37322;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#35774;&#35745;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is typically understood that the training of modern neural networks is a process of fitting the probability distribution of desired output. However, recent paradoxical observations in a number of language generation tasks let one wonder if this canonical probability-based explanation can really account for the empirical success of deep learning.  To resolve this issue, we propose an alternative utility-based explanation to the standard supervised learning procedure in deep learning. The basic idea is to interpret the learned neural network not as a probability model but as an ordinal utility function that encodes the preference revealed in training data. In this perspective, training of the neural network corresponds to a utility learning process. Specifically, we show that for all neural networks with softmax outputs, the SGD learning dynamic of maximum likelihood estimation (MLE) can be seen as an iteration process that optimizes the neural network toward an optimal utility functi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#12290;</title><link>http://arxiv.org/abs/2305.14749</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-State RNA Design with Geometric Multi-Graph Neural Networks. (arXiv:2305.14749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;RNA&#35774;&#35745;&#22312;&#21512;&#25104;&#29983;&#29289;&#23398;&#21644;&#27835;&#30103;&#24320;&#21457;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;RNA&#22810;&#26679;&#30340;&#29983;&#29289;&#23398;&#21151;&#33021;&#30340;&#22522;&#30784;&#26159;&#23427;&#30340;&#26500;&#35937;&#28789;&#27963;&#24615;&#65292;&#20351;&#21333;&#19968;&#24207;&#21015;&#33021;&#22815;&#37319;&#29992;&#22810;&#31181;&#19981;&#21516;&#30340;&#19977;&#32500;&#32467;&#26500;&#29366;&#24577;&#12290;&#30446;&#21069;&#65292;&#35745;&#31639;&#29983;&#29289;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#32463;&#24120;&#34987;&#25552;&#20986;&#20026;&#36870;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#37319;&#29992;&#21333;&#19968;&#39044;&#26399;&#32467;&#26500;&#26500;&#35937;&#26469;&#35774;&#35745;&#24207;&#21015;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;gRNAde&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#19968;&#32452;&#19977;&#32500;RNA&#39592;&#26550;&#32467;&#26500;&#25805;&#20316;&#30340;&#20960;&#20309;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#19977;&#32500;RNA&#35774;&#35745;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;gRNAde&#30340;&#25928;&#29992;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/chaitjo/geometric-rna-design&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational RNA design has broad applications across synthetic biology and therapeutic development. Fundamental to the diverse biological functions of RNA is its conformational flexibility, enabling single sequences to adopt a variety of distinct 3D states. Currently, computational biomolecule design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired structural conformation. In this work, we propose gRNAde, a geometric RNA design pipeline that operates on sets of 3D RNA backbone structures to explicitly account for and reflect RNA conformational diversity in its designs. We demonstrate the utility of gRNAde for improving native sequence recovery over single-state approaches on a new large-scale 3D RNA design dataset, especially for multi-state and structurally diverse RNAs. Our code is available at https://github.com/chaitjo/geometric-rna-design
&lt;/p&gt;</description></item><item><title>AdvFunMatch&#26159;&#19968;&#31181;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19968;&#33268;&#25945;&#23398;&#30340;&#26041;&#24335;&#65292;&#22312;&#21305;&#37197;&#25104;&#21151;&#25968;&#25454;&#28857;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#29699;&#24418;&#31354;&#38388;&#20869;&#21305;&#37197;&#25152;&#26377;&#25968;&#25454;&#28857;&#30340;&#20998;&#24067;</title><link>http://arxiv.org/abs/2305.14700</link><description>&lt;p&gt;
AdvFunMatch: &#24403;&#19968;&#33268;&#30340;&#25945;&#23398;&#36935;&#35265;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
AdvFunMatch: When Consistent Teaching Meets Adversarial Robustness. (arXiv:2305.14700v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14700
&lt;/p&gt;
&lt;p&gt;
AdvFunMatch&#26159;&#19968;&#31181;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19968;&#33268;&#25945;&#23398;&#30340;&#26041;&#24335;&#65292;&#22312;&#21305;&#37197;&#25104;&#21151;&#25968;&#25454;&#28857;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#29699;&#24418;&#31354;&#38388;&#20869;&#21305;&#37197;&#25152;&#26377;&#25968;&#25454;&#28857;&#30340;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#19968;&#33268;&#30340;&#25945;&#23398;&#8221;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#33539;&#20363;&#65292;&#22312;&#36825;&#31181;&#33539;&#20363;&#19979;&#65292;&#23398;&#29983;&#21644;&#25945;&#24072;&#27169;&#22411;&#25509;&#25910;&#30456;&#21516;&#30340;&#36755;&#20837;&#65292;&#24182;&#23558;&#30693;&#35782;&#33976;&#39311;&#35270;&#20026;&#20989;&#25968;&#21305;&#37197;&#20219;&#21153;&#65288;FunMatch&#65289;&#12290;&#28982;&#32780;&#65292;FunMatch &#30340;&#19968;&#20010;&#38480;&#21046;&#26159;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#27169;&#22411;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#23545;&#25239;&#20989;&#25968;&#21305;&#37197;&#65288;AdvFunMatch&#65289;&#65292;&#35813;&#31574;&#30053;&#26088;&#22312;&#22312;&#19968;&#33268;&#25945;&#23398;&#30340;&#21069;&#25552;&#19979;&#65292;&#21305;&#37197;&#35757;&#32451;&#25968;&#25454; $\ell_p$-&#33539;&#25968;&#29699;&#20869;&#30340;&#25152;&#26377;&#25968;&#25454;&#28857;&#30340;&#20998;&#24067;&#12290;AdvFunMatch &#34987;&#21046;&#23450;&#20026;&#26497;&#23567;&#21270;-&#26497;&#22823;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#23427;&#30830;&#23450;&#20102;&#26368;&#22823;&#21270;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388; KL &#25955;&#24230;&#30340;&#26368;&#22351;&#23454;&#20363;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#19981;&#21305;&#37197;&#23454;&#20363;&#8221;&#65289;&#65292;&#28982;&#21518;&#21305;&#37197;&#36825;&#20123;&#19981;&#21305;&#37197;&#23454;&#20363;&#19978;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AdvFunMatch &#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#24378;&#23545;&#25239;&#24615;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
\emph{Consistent teaching} is an effective paradigm for implementing knowledge distillation (KD), where both student and teacher models receive identical inputs, and KD is treated as a function matching task (FunMatch). However, one limitation of FunMatch is that it does not account for the transfer of adversarial robustness, a model's resistance to adversarial attacks. To tackle this problem, we propose a simple but effective strategy called Adversarial Function Matching (AdvFunMatch), which aims to match distributions for all data points within the $\ell_p$-norm ball of the training data, in accordance with consistent teaching. Formulated as a min-max optimization problem, AdvFunMatch identifies the worst-case instances that maximizes the KL-divergence between teacher and student model outputs, which we refer to as "mismatched examples," and then matches the outputs on these mismatched examples. Our experimental results show that AdvFunMatch effectively produces student models with b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#36882;&#24402;&#22270;&#20687;&#37197;&#20934;&#32593;&#32476;ORRN&#65292;&#38024;&#23545;&#32954;4DCT&#22270;&#20687;&#30340;&#21464;&#24418;&#20272;&#35745;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#36882;&#24402;&#37197;&#20934;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#21628;&#21560;&#21644;&#24515;&#36339;&#31561;&#22120;&#23448;&#36816;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.14673</link><description>&lt;p&gt;
ORRN&#65306;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#36882;&#24402;&#37197;&#20934;&#32593;&#32476;&#65292;&#29992;&#20110;&#21628;&#21560;&#36816;&#21160;&#21464;&#24418;&#30340;&#32954;4DCT&#22270;&#20687;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
ORRN: An ODE-based Recursive Registration Network for Deformable Respiratory Motion Estimation with Lung 4DCT Images. (arXiv:2305.14673v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#36882;&#24402;&#22270;&#20687;&#37197;&#20934;&#32593;&#32476;ORRN&#65292;&#38024;&#23545;&#32954;4DCT&#22270;&#20687;&#30340;&#21464;&#24418;&#20272;&#35745;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#36882;&#24402;&#37197;&#20934;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#21628;&#21560;&#21644;&#24515;&#36339;&#31561;&#22120;&#23448;&#36816;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#65288;DIR&#65289;&#22312;&#37327;&#21270;&#21307;&#23398;&#25968;&#25454;&#21464;&#24418;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#27880;&#20876;&#19968;&#23545;&#21307;&#23398;&#22270;&#20687;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;4D&#65288;3D +&#26102;&#38388;&#65289;&#21307;&#23398;&#25968;&#25454;&#20013;&#65292;&#22120;&#23448;&#36816;&#21160;&#65292;&#22914;&#21628;&#21560;&#36816;&#21160;&#21644;&#24515;&#36339;&#31561;&#65292;&#26080;&#27861;&#36890;&#36807;&#25104;&#23545;&#26041;&#27861;&#26377;&#25928;&#22320;&#24314;&#27169;&#65292;&#22240;&#20026;&#23427;&#20204;&#38024;&#23545;&#22270;&#20687;&#23545;&#36827;&#34892;&#20248;&#21270;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#22312;&#32771;&#34385;4D&#25968;&#25454;&#26102;&#24517;&#35201;&#30340;&#22120;&#23448;&#36816;&#21160;&#27169;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ORRN&#65292;&#19968;&#31181;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#36882;&#24402;&#22270;&#20687;&#37197;&#20934;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#23398;&#20250;&#20102;&#20026;&#27169;&#25311;4D&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#21464;&#24418;&#32780;&#20272;&#35745;&#26102;&#21464;&#30340;&#20307;&#32032;&#36895;&#24230;&#30340;ODE&#12290;&#23427;&#37319;&#29992;&#36882;&#24402;&#37197;&#20934;&#31574;&#30053;&#65292;&#36890;&#36807;ODE&#23545;&#20307;&#32032;&#36895;&#24230;&#36827;&#34892;&#31215;&#20998;&#26469;&#36880;&#27493;&#20272;&#35745;&#21464;&#24418;&#22330;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#32954;4DCT&#25968;&#25454;&#38598;DIRLab&#21644;CREATIS&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23436;&#25104;&#20004;&#20010;&#20219;&#21153;&#65306;1&#65289;&#23558;&#25152;&#26377;&#22270;&#20687;&#19982;&#21442;&#32771;&#22270;&#20687;&#37197;&#20934;&#65307;
&lt;/p&gt;
&lt;p&gt;
Deformable Image Registration (DIR) plays a significant role in quantifying deformation in medical data. Recent Deep Learning methods have shown promising accuracy and speedup for registering a pair of medical images. However, in 4D (3D + time) medical data, organ motion, such as respiratory motion and heart beating, can not be effectively modeled by pair-wise methods as they were optimized for image pairs but did not consider the organ motion patterns necessary when considering 4D data. This paper presents ORRN, an Ordinary Differential Equations (ODE)-based recursive image registration network. Our network learns to estimate time-varying voxel velocities for an ODE that models deformation in 4D image data. It adopts a recursive registration strategy to progressively estimate a deformation field through ODE integration of voxel velocities. We evaluate the proposed method on two publicly available lung 4DCT datasets, DIRLab and CREATIS, for two tasks: 1) registering all images to the e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#22352;&#26631;&#22810;&#23618;&#27425;&#20248;&#21270;&#26041;&#27861;&#30340;&#35299;&#20915;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20351;&#29992;PINNs&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38382;&#39064;&#65292;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14477</link><description>&lt;p&gt;
&#22359;&#22352;&#26631;&#22810;&#23618;&#27425;&#20248;&#21270;&#26041;&#27861;&#21450;&#20854;&#22312;&#22522;&#20110;&#29289;&#29702;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Block-Coordinate Approach of Multi-level Optimization with an Application to Physics-Informed Neural Networks. (arXiv:2305.14477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#22352;&#26631;&#22810;&#23618;&#27425;&#20248;&#21270;&#26041;&#27861;&#30340;&#35299;&#20915;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20351;&#29992;PINNs&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38382;&#39064;&#65292;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#27425;&#26041;&#27861;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#35745;&#31639;&#20248;&#21183;&#24182;&#21033;&#29992;&#20102;&#28041;&#21450;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#12290;&#22312;&#23558;&#22810;&#23618;&#27425;&#26041;&#27861;&#20174;&#22359;&#22352;&#26631;&#30340;&#35270;&#35282;&#37325;&#26032;&#35299;&#37322;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#22810;&#23618;&#27425;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#35780;&#20272;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#20351;&#29992;&#29289;&#29702;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20960;&#20010;&#27979;&#35797;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26356;&#22909;&#35299;&#20915;&#26041;&#26696;&#21644;&#26174;&#30528;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-level methods are widely used for the solution of large-scale problems, because of their computational advantages and exploitation of the complementarity between the involved sub-problems. After a re-interpretation of multi-level methods from a block-coordinate point of view, we propose a multi-level algorithm for the solution of nonlinear optimization problems and analyze its evaluation complexity. We apply it to the solution of partial differential equations using physics-informed neural networks (PINNs) and show on a few test problems that the approach results in better solutions and significant computational savings
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38480;&#23398;&#20064;&#19981;&#21516;&#30340;&#36830;&#32493;&#25216;&#33021;&#65292;&#22312;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20855;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14377</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#22312;&#29699;&#38754;&#19978;&#23398;&#20064;&#36830;&#32493;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Discovery of Continuous Skills on a Sphere. (arXiv:2305.14377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38480;&#23398;&#20064;&#19981;&#21516;&#30340;&#36830;&#32493;&#25216;&#33021;&#65292;&#22312;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20855;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20102;&#35768;&#22810;&#31181;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19981;&#21516;&#30340;&#25216;&#33021;&#24110;&#21161;&#26426;&#22120;&#20154;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#65292;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;&#22870;&#21169;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#23398;&#20064;&#30340;&#26159;&#26377;&#38480;&#25968;&#37327;&#30340;&#31163;&#25955;&#25216;&#33021;&#65292;&#22240;&#27492;&#23427;&#20204;&#23637;&#29616;&#20986;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#20063;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;&#22312;&#29699;&#38754;&#19978;&#23398;&#20064;&#36830;&#32493;&#25216;&#33021;&#30340;&#21457;&#29616;&#8221;&#65292;&#21487;&#20197;&#23398;&#20064;&#26080;&#38480;&#31181;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25216;&#33021;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#25216;&#33021;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#30340;&#65292;&#32780;&#27599;&#31181;&#25216;&#33021;&#37117;&#23545;&#24212;&#20110;&#29699;&#38754;&#19978;&#30340;&#19968;&#20010;&#36830;&#32493;&#20540;&#12290;&#30001;&#20110;&#25216;&#33021;&#22312;DISCS&#20013;&#30340;&#34920;&#31034;&#26159;&#36830;&#32493;&#30340;&#65292;&#25152;&#20197;&#21487;&#20197;&#23398;&#20064;&#26080;&#38480;&#22810;&#31181;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#26041;&#27861;&#21644;DISCS&#24212;&#29992;&#20110;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;DISCS&#21487;&#20197;&#27604;&#20854;&#20182;&#26041;&#27861;&#23398;&#20064;&#21040;&#26356;&#22810;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods for learning diverse skills to generate various behaviors without external rewards have been actively studied as a form of unsupervised reinforcement learning. However, most of the existing methods learn a finite number of discrete skills, and thus the variety of behaviors that can be exhibited with the learned skills is limited. In this paper, we propose a novel method for learning potentially an infinite number of different skills, which is named discovery of continuous skills on a sphere (DISCS). In DISCS, skills are learned by maximizing mutual information between skills and states, and each skill corresponds to a continuous value on a sphere. Because the representations of skills in DISCS are continuous, infinitely diverse skills could be learned. We examine existing methods and DISCS in the MuJoCo Ant robot control environments and show that DISCS can learn much more diverse skills than the other methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#8212;&#8212;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.14258</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#65306;&#32479;&#19968;&#30340;&#37096;&#20998;AUC&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised AUC Optimization: A Unified Partial AUC Approach. (arXiv:2305.14258v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#8212;&#8212;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33719;&#21462;&#23436;&#32654;&#30340;&#30417;&#30563;&#36890;&#24120;&#24456;&#22256;&#38590;&#65292;&#29616;&#23454;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#38754;&#20020;&#19981;&#20934;&#30830;&#12289;&#19981;&#23436;&#25972;&#25110;&#19981;&#31934;&#30830;&#30340;&#30417;&#30563;&#65292;&#32479;&#31216;&#20026;&#24369;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#28085;&#30422;&#20102;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#12290;&#22312;WSAUC&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#26368;&#23567;&#21270;&#21463;&#27745;&#26579;&#38598;&#21512;&#19978;AUC&#39118;&#38505;&#30340;&#24120;&#35265;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#19982;&#30495;&#23454;AUC&#19968;&#33268;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#65292;&#21363;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#23427;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#22312;&#23384;&#22312;&#27745;&#26579;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#20316;&#29992;&#12290;WSAUC&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since acquiring perfect supervision is usually difficult, real-world machine learning tasks often confront inaccurate, incomplete, or inexact supervision, collectively referred to as weak supervision. In this work, we present WSAUC, a unified framework for weakly supervised AUC optimization problems, which covers noisy label learning, positive-unlabeled learning, multi-instance learning, and semi-supervised learning scenarios. Within the WSAUC framework, we first frame the AUC optimization problems in various weakly supervised scenarios as a common formulation of minimizing the AUC risk on contaminated sets, and demonstrate that the empirical risk minimization problems are consistent with the true AUC. Then, we introduce a new type of partial AUC, specifically, the reversed partial AUC (rpAUC), which serves as a robust training objective for AUC maximization in the presence of contaminated labels. WSAUC offers a universal solution for AUC optimization in various weakly supervised scena
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#38899;&#22788;&#29702;&#25216;&#26415;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#26679;&#26412;(NLS)&#20998;&#26512;&#23396;&#29420;&#30151;&#20799;&#31461;&#30340;&#21475;&#35821;&#21457;&#23637;&#27700;&#24179;&#65292;&#33021;&#22815;&#20998;&#31867;&#20986;&#20799;&#31461;&#21644;&#25104;&#20154;&#35821;&#38899;&#65292;&#20197;&#21450;&#35821;&#38899;&#21644;&#38750;&#35821;&#35328;&#21457;&#22768;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14117</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#23884;&#20837;&#29702;&#35299;&#23396;&#29420;&#30151;&#20799;&#31461;&#21475;&#35821;&#35821;&#35328;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings. (arXiv:2305.14117v1 [eess.AS] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#38899;&#22788;&#29702;&#25216;&#26415;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#26679;&#26412;(NLS)&#20998;&#26512;&#23396;&#29420;&#30151;&#20799;&#31461;&#30340;&#21475;&#35821;&#21457;&#23637;&#27700;&#24179;&#65292;&#33021;&#22815;&#20998;&#31867;&#20986;&#20799;&#31461;&#21644;&#25104;&#20154;&#35821;&#38899;&#65292;&#20197;&#21450;&#35821;&#38899;&#21644;&#38750;&#35821;&#35328;&#21457;&#22768;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22788;&#29702;&#25216;&#26415;&#23545;&#20110;&#20998;&#26512;&#23396;&#29420;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#20799;&#31461;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#21457;&#23637;&#24456;&#26377;&#24110;&#21161;&#65292;&#36825;&#20123;&#23401;&#23376;&#22312;&#33719;&#24471;&#36825;&#20123;&#25216;&#33021;&#26041;&#38754;&#24120;&#24120;&#26159;&#22810;&#26679;&#24615;&#21644;&#24310;&#36831;&#30340;&#12290;&#23613;&#26089;&#35782;&#21035;&#21644;&#24178;&#39044;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#22914;&#25252;&#29702;&#20154;&#21592;&#25253;&#21578;&#23545;&#20110;&#25152;&#38656;&#34892;&#20026;&#34920;&#22411;&#25551;&#36848;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26679;&#26412;(NLS)&#20998;&#26512;&#24050;&#33719;&#24471;&#20851;&#27880;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#34917;&#20805;&#25163;&#27573;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#20998;&#26512;NLS&#20013;&#23396;&#29420;&#30151;&#20799;&#31461;&#30340;&#21475;&#35821;&#33021;&#21147;&#27700;&#24179;&#26102;&#24050;&#32463;&#21046;&#23450;&#20102;&#22522;&#20934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#38899;&#22788;&#29702;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20998;&#31867;&#26469;&#25903;&#25345;&#20799;&#31461;&#21475;&#35821;&#35821;&#35328;&#21457;&#23637;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#36776;&#20799;&#31461;&#21644;&#25104;&#20154;&#35821;&#38899;&#65292;&#20197;&#21450;&#35821;&#38899;&#21644;&#38750;&#35821;&#35328;&#21457;&#22768;&#65292;&#20854;F1&#23439;&#24179;&#22343;&#20998;&#21035;&#20026;82.6&#65285;&#21644;67.8&#65285;&#65292;&#24378;&#35843;&#20102;ASD&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech processing techniques are useful for analyzing speech and language development in children with Autism Spectrum Disorder (ASD), who are often varied and delayed in acquiring these skills. Early identification and intervention are crucial, but traditional assessment methodologies such as caregiver reports are not adequate for the requisite behavioral phenotyping. Natural Language Sample (NLS) analysis has gained attention as a promising complement. Researchers have developed benchmarks for spoken language capabilities in children with ASD, obtainable through the analysis of NLS. This paper proposes applications of speech processing technologies in support of automated assessment of children's spoken language development by classification between child and adult speech and between speech and nonverbal vocalization in NLS, with respective F1 macro scores of 82.6% and 67.8%, underscoring the potential for accurate and scalable tools for ASD research and clinical use.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#23618;&#27425;&#27969;&#27700;&#32447;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25429;&#33719;&#19981;&#21516;&#27969;&#27700;&#32447;&#32452;&#20214;&#20043;&#38388;&#30340;&#28145;&#24230;&#20132;&#20114;&#65292;&#24182;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#35774;&#32622;&#20013;&#20351;&#29992;&#36825;&#20123;&#23884;&#20837;&#29992;&#20110;&#25628;&#32034;&#26368;&#20339;&#27969;&#27700;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.14009</link><description>&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#23618;&#27425;&#27969;&#27700;&#32447;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Deep Pipeline Embeddings for AutoML. (arXiv:2305.14009v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14009
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#23618;&#27425;&#27969;&#27700;&#32447;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25429;&#33719;&#19981;&#21516;&#27969;&#27700;&#32447;&#32452;&#20214;&#20043;&#38388;&#30340;&#28145;&#24230;&#20132;&#20114;&#65292;&#24182;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#35774;&#32622;&#20013;&#20351;&#29992;&#36825;&#20123;&#23884;&#20837;&#29992;&#20110;&#25628;&#32034;&#26368;&#20339;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#37096;&#32626;&#38656;&#35201;&#26497;&#23569;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26469;&#25512;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;AutoML&#32972;&#21518;&#30340;&#26680;&#24515;&#25216;&#26415;&#25361;&#25112;&#26159;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#27969;&#27700;&#32447;&#65288;&#20363;&#22914;&#65292;&#39044;&#22788;&#29702;&#12289;&#22686;&#24378;&#12289;&#27169;&#22411;&#12289;&#20248;&#21270;&#22120;&#31561;&#30340;&#36873;&#25321;&#65289;&#12290;&#29616;&#26377;&#30340;&#27969;&#27700;&#32447;&#20248;&#21270;&#25216;&#26415;&#26410;&#33021;&#25506;&#32034;&#27969;&#27700;&#32447;&#38454;&#27573;/&#32452;&#20214;&#20043;&#38388;&#30340;&#28145;&#23618;&#20114;&#21160;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#25429;&#25417;&#20102;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#32452;&#20214;&#20043;&#38388;&#30340;&#28145;&#24230;&#20132;&#20114;&#12290;&#25105;&#20204;&#23558;&#27969;&#27700;&#32447;&#23884;&#20837;&#21040;&#19968;&#20010;&#28508;&#22312;&#34920;&#31034;&#20013;&#65292;&#36890;&#36807;&#26032;&#30340;&#32452;&#20214;&#32534;&#30721;&#22120;&#26426;&#21046;&#36827;&#34892;&#12290;&#20026;&#20102;&#25628;&#32034;&#26368;&#20339;&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#35774;&#32622;&#20869;&#20351;&#29992;&#36825;&#20123;&#27969;&#27700;&#32447;&#23884;&#20837;&#20110;&#28145;&#23618;&#26680;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#20043;&#20869;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#22810;&#26679;&#21270;&#25910;&#38598;&#25968;&#25454;&#20013;&#30340;&#31649;&#36947;&#30340;&#29616;&#26377;&#35780;&#20272;&#36827;&#34892;&#20803;&#23398;&#20064;&#27969;&#27700;&#32447;&#23884;&#20837;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Machine Learning (AutoML) is a promising direction for democratizing AI by automatically deploying Machine Learning systems with minimal human expertise. The core technical challenge behind AutoML is optimizing the pipelines of Machine Learning systems (e.g. the choice of preprocessing, augmentations, models, optimizers, etc.). Existing Pipeline Optimization techniques fail to explore deep interactions between pipeline stages/components. As a remedy, this paper proposes a novel neural architecture that captures the deep interaction between the components of a Machine Learning pipeline. We propose embedding pipelines into a latent representation through a novel per-component encoder mechanism. To search for optimal pipelines, such pipeline embeddings are used within deep-kernel Gaussian Process surrogates inside a Bayesian Optimization setup. Furthermore, we meta-learn the parameters of the pipeline embedding network using existing evaluations of pipelines on diverse collectio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#20174;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35282;&#24230;&#35762;&#36848;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#19981;&#20844;&#27491;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#26694;&#26550;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#20844;&#24179;&#24615;&#25351;&#26631;&#20197;&#31526;&#21512;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.13938</link><description>&lt;p&gt;
&#36890;&#36807;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35270;&#35282;&#35762;&#36848;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#65306;&#25110;&#35859;&#27861;&#24459;&#38750;&#20915;&#31574;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is not a Decision Tree. (arXiv:2305.13938v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#20174;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35282;&#24230;&#35762;&#36848;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#19981;&#20844;&#27491;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#26694;&#26550;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#20844;&#24179;&#24615;&#25351;&#26631;&#20197;&#31526;&#21512;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#32654;&#24863;&#21040;&#19981;&#20844;&#24179;&#21644;&#27495;&#35270;&#30340;&#38382;&#39064;&#26368;&#36817;&#24341;&#36215;&#20102;&#27861;&#24459;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#32773;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31639;&#27861;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#20197;&#21450;&#27861;&#24459;&#19978;&#30340;&#27495;&#35270;&#21644;&#24179;&#31561;&#27010;&#24565;&#20043;&#38388;&#30340;&#37325;&#21472;&#31243;&#24230;&#36890;&#24120;&#19981;&#28165;&#26970;&#65292;&#23548;&#33268;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#27861;&#24459;&#20043;&#38388;&#30340;&#35823;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#19982;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#27010;&#24565;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#21512;&#20197;&#21450;&#23427;&#20204;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#27431;&#30431;&#26696;&#20363;&#27861;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#31639;&#27861;&#19981;&#20844;&#24179;&#30340;&#20856;&#22411;&#20363;&#23376;&#65292;&#25214;&#20986;&#19982;&#27431;&#30431;&#26696;&#20363;&#27861;&#30340;&#31867;&#27604;&#20043;&#22788;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#21644;AI&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#31526;&#21512;&#27431;&#30431;&#30340;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312; CAFe II &#21644; LPI &#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23558;&#26657;&#27491;&#26102;&#38388;&#32553;&#30701;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.13869</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#36229;&#23548;&#32447;&#24615;&#21152;&#36895;&#22120;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator. (arXiv:2305.13869v1 [physics.acc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312; CAFe II &#21644; LPI &#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23558;&#26657;&#27491;&#26102;&#38388;&#32553;&#30701;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#23548;&#32447;&#24615;&#21152;&#36895;&#22120;&#26159;&#29616;&#20195;&#31185;&#23398;&#30740;&#31350;&#30340;&#39640;&#24230;&#28789;&#27963;&#30340;&#35774;&#26045;&#65292;&#38656;&#35201;&#27599;&#21608;&#37325;&#26032;&#37197;&#32622;&#21644;&#35843;&#25972;&#12290;&#22240;&#27492;&#65292;&#26368;&#23567;&#21270;&#35774;&#32622;&#26102;&#38388;&#23545;&#20110;&#25552;&#20379;&#20805;&#36275;&#30340;&#23454;&#39564;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#36719; actor-critic(TBSAC)&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#20110;&#30495;&#27491;&#30340;&#21152;&#36895;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#25511;&#21046;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20998;&#21035;&#22312;&#20013;&#22269;&#36229;&#37325;&#20803;&#32032;&#21152;&#36895;&#22120;&#35774;&#26045;(CAFe II)&#21644;&#19968;&#20010;&#36731;&#36136;&#31890;&#23376;&#27880;&#20837;&#22120;(LPI)&#20013;&#25191;&#34892;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#20856;&#22411;&#26463;&#27969;&#25511;&#21046;&#20219;&#21153;&#12290;&#22312;CAFe II&#30340;&#19977;&#20010;&#20302;&#28201;&#27169;&#22359;&#20013;&#20998;&#21035;&#25191;&#34892;&#20102;&#36712;&#36947;&#26657;&#27491;&#20219;&#21153;&#65292;&#35843;&#35856;&#25152;&#38656;&#26102;&#38388;&#24050;&#32463;&#20943;&#23569;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#65292;&#26657;&#27491;&#21518;&#30340;RMS&#20540;&#37117;&#23567;&#20110;1&#27627;&#31859;&#12290;&#21478;&#19968;&#20010;&#20256;&#36755;&#25928;&#29575;&#20248;&#21270;&#20219;&#21153;&#22312;CAFe II&#30340;&#21152;&#36895;&#22120;&#27573;LPI&#20013;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot. To validate the effectiveness of our method, two different typical beam control tasks were performed on China Accelerator Facility for Superheavy Elements (CAFe II) and a light particle injector(LPI) respectively. The orbit correction tasks were performed in three cryomodules in CAFe II seperately, the time required for tuning has been reduced to one-tenth of that needed by human experts, and the RMS values of the corrected orbit were all less than 1mm. The other transmission efficiency optimization task was conducted in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#65292;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#21327;&#21516;&#20316;&#29992;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.13447</link><description>&lt;p&gt;
&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65306;&#20197;&#21860;&#37202;&#33457;&#20998;&#31867;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Regularization Through Simultaneous Learning: A Case Study for Hop Classification. (arXiv:2305.13447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#65292;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#21327;&#21516;&#20316;&#29992;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#25311;&#21512;&#20173;&#28982;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#65292;&#23548;&#33268;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#37319;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26159;&#25269;&#21046;&#36825;&#19968;&#25361;&#25112;&#30340;&#24120;&#35265;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65306;Simultaneous Learning&#65292;&#23427;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#65292;&#19987;&#38376;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#19982;&#30446;&#26631;&#25968;&#25454;&#38598;&#21327;&#21516;&#24037;&#20316;&#65292;&#20174;&#32780;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#30340;&#26368;&#32456;&#23618;&#36827;&#34892;&#25112;&#30053;&#24615;&#20462;&#25913;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#65292;&#26080;&#38656;&#23558;&#23427;&#20204;&#35270;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#21253;&#25324;&#32452;&#38388;&#24809;&#32602;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;InceptionV3&#21644;ResNet50&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#25351;&#23450;&#20102;UFOP-HVD&#21860;&#37202;&#33457;&#21494;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overfitting remains a prevalent challenge in deep neural networks, leading to suboptimal real-world performance. Employing regularization techniques is a common strategy to counter this challenge, improving model generalization. This paper proposes Simultaneous Learning, a novel regularization approach drawing on Transfer Learning and Multi-task Learning principles, applied specifically to the classification of hop varieties - an integral component of beer production. Our approach harnesses the power of auxiliary datasets in synergy with the target dataset to amplify the acquisition of highly relevant features. Through a strategic modification of the model's final layer, we enable the simultaneous classification of both datasets without the necessity to treat them as disparate tasks. To realize this, we formulate a loss function that includes an inter-group penalty. We conducted experimental evaluations using the InceptionV3 and ResNet50 models, designating the UFOP-HVD hop leaf datase
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EMNS/Imz/ Corpus&#30340;&#24773;&#24863;&#21333;&#35828;&#32773;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#20132;&#20114;&#24335;&#21465;&#36848;&#39537;&#21160;&#31995;&#32479;&#20013;&#23545;&#35805;&#30340;&#34920;&#29616;&#21147;&#21644;&#24773;&#24863;&#36136;&#37327;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#20256;&#36798;&#24773;&#24863;&#21644;&#34920;&#29616;&#21147;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#23588;&#20854;&#22312;&#20849;&#20139;&#24773;&#24863;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.13137</link><description>&lt;p&gt;
EMNS / Imz / Corpus&#65306;&#24773;&#24863;&#21333;&#35828;&#32773;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#28216;&#25103;&#12289;&#30005;&#35270;&#21644;&#28459;&#30011;&#20013;&#30340;&#21465;&#36848;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
EMNS /Imz/ Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels. (arXiv:2305.13137v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EMNS/Imz/ Corpus&#30340;&#24773;&#24863;&#21333;&#35828;&#32773;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#20132;&#20114;&#24335;&#21465;&#36848;&#39537;&#21160;&#31995;&#32479;&#20013;&#23545;&#35805;&#30340;&#34920;&#29616;&#21147;&#21644;&#24773;&#24863;&#36136;&#37327;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#20256;&#36798;&#24773;&#24863;&#21644;&#34920;&#29616;&#21147;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#23588;&#20854;&#22312;&#20849;&#20139;&#24773;&#24863;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21040;&#35821;&#38899;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#20110;&#36866;&#24212;&#23545;&#35805;&#32972;&#26223;&#21644;&#24773;&#24863;&#35821;&#27668;&#30340;&#33258;&#28982;&#24773;&#24863;&#35821;&#38899;&#30340;&#38656;&#27714;&#12290;&#24773;&#24863;&#21465;&#36848;&#25925;&#20107;&#65288;EMNS&#65289;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29420;&#29305;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#20132;&#20114;&#24335;&#21465;&#36848;&#39537;&#21160;&#31995;&#32479;&#20013;&#23545;&#35805;&#30340;&#34920;&#29616;&#21147;&#21644;&#24773;&#24863;&#36136;&#37327;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;&#19968;&#20301;&#22899;&#24615;&#28436;&#35828;&#32773;&#35762;&#36848;&#26631;&#35760;&#35805;&#35821;&#30340;2.3&#23567;&#26102;&#24405;&#38899;&#65292;&#28085;&#30422;&#20102;&#20843;&#31181;&#34920;&#28436;&#24773;&#24863;&#29366;&#24577;&#65292;&#20998;&#24067;&#22343;&#21248;&#65292;&#26041;&#24046;&#20026;0.68&#65285;&#65292;&#20197;&#21450;&#34920;&#29616;&#21147;&#27700;&#24179;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#35789;&#37325;&#38899;&#26631;&#31614;&#12290;&#23545;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#38899;&#39057;&#26679;&#26412;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;EMNS&#35821;&#26009;&#24211;&#22312;&#20934;&#30830;&#20256;&#36798;&#24773;&#24863;&#21644;&#34920;&#29616;&#21147;&#26041;&#38754;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#20998;&#12290;&#23427;&#22312;&#34920;&#36798;&#20849;&#20139;&#24773;&#24863;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#25968;&#25454;&#38598;&#65292;&#24182;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#30495;&#23454;&#27700;&#24179;&#12290;&#19968;&#20010;&#20998;&#31867;&#20219;&#21153;&#35777;&#23454;&#20102;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing adoption of text-to-speech technologies has led to a growing demand for natural and emotive voices that adapt to a conversation's context and emotional tone. The Emotive Narrative Storytelling (EMNS) corpus is a unique speech dataset created to enhance conversations' expressiveness and emotive quality in interactive narrative-driven systems. The corpus consists of a 2.3-hour recording featuring a female speaker delivering labelled utterances. It encompasses eight acted emotional states, evenly distributed with a variance of 0.68%, along with expressiveness levels and natural language descriptions with word emphasis labels. The evaluation of audio samples from different datasets revealed that the EMNS corpus achieved the highest average scores in accurately conveying emotions and demonstrating expressiveness. It outperformed other datasets in conveying shared emotions and achieved comparable levels of genuineness. A classification task confirmed the accurate representatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXACT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#36827;&#34892;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12289;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12997</link><description>&lt;p&gt;
EXACT&#65306;&#29992;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#20840;&#38754;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EXACT: Extensive Attack for Split Learning. (arXiv:2305.12997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXACT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#36827;&#34892;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12289;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#35757;&#32451;&#21644;&#37096;&#32626;&#21033;&#29992;&#31169;&#20154;&#20449;&#24687;&#30340;&#27169;&#22411;&#12290;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#20351;&#25105;&#20204;&#22312;&#25512;&#26029;&#26399;&#38388;&#23436;&#20840;&#36991;&#20813;&#19982;&#31532;&#19977;&#26041;&#26381;&#21153;&#22120;&#20849;&#20139;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#19982;&#26381;&#21153;&#22120;&#31471;&#30456;&#27604;&#65292;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#36890;&#24120;&#36739;&#19981;&#20934;&#30830;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#21482;&#20381;&#36182;&#20110;&#19968;&#23567;&#32452;&#35774;&#22791;&#29305;&#24449;&#19988;&#38656;&#35201;&#36275;&#22815;&#23567;&#25165;&#33021;&#22312;&#32456;&#31471;&#29992;&#25143;&#35774;&#22791;&#19978;&#39640;&#25928;&#36816;&#34892;&#12290;&#20998;&#24067;&#24335;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#65292;&#23558;&#19968;&#20010;&#22823;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#25104;&#20004;&#37096;&#20998;&#65292;&#22823;&#37096;&#20998;&#20301;&#20110;&#26381;&#21153;&#22120;&#31471;&#65292;&#23567;&#37096;&#20998;&#22312;&#35774;&#22791;&#19978;&#25191;&#34892;&#65292;&#26088;&#22312;&#25972;&#21512;&#31169;&#26377;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#38656;&#35201;&#22312;&#20998;&#30028;&#22788;&#20132;&#25442;&#26799;&#24230;&#65292;&#36825;&#21487;&#33021;&#32534;&#30721;&#31169;&#26377;&#29305;&#24449;&#25110;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EXACT&#65288;Extensive Attack for Split Learning&#65289;&#30340;&#26032;&#39062;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#27867;&#30340;&#22122;&#22768;&#23454;&#29616;&#23433;&#20840;&#30340;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving machine learning (PPML) can help us train and deploy models that utilize private information. In particular, on-device Machine Learning allows us to completely avoid sharing information with a third-party server during inference. However, on-device models are typically less accurate when compared to the server counterparts due to the fact that (1) they typically only rely on a small set of on-device features and (2) they need to be small enough to run efficiently on end-user devices. Split Learning (SL) is a promising approach that can overcome these limitations. In SL, a large machine learning model is divided into two parts, with the bigger part residing on the server-side and a smaller part executing on-device, aiming to incorporate the private features. However, end-to-end training of such models requires exchanging gradients at the cut layer, which might encode private features or labels. In this paper, we provide insights into potential privacy risks associated
&lt;/p&gt;</description></item><item><title>FIT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#23558;&#25968;&#25454;&#26631;&#35760;&#20998;&#32452;&#65292;&#20351;&#29992;&#23616;&#37096;&#23618;&#21644;&#20840;&#23616;&#23618;&#36827;&#34892;&#25805;&#20316;&#12290;&#36890;&#36807;&#20132;&#38169;&#20351;&#29992;&#36825;&#20123;&#23618;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#20419;&#36827;&#20449;&#24687;&#20132;&#25442;&#65292;FIT&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12689</link><description>&lt;p&gt;
FIT&#65306;&#36828;&#31243;&#20132;&#38169;Transformer
&lt;/p&gt;
&lt;p&gt;
FIT: Far-reaching Interleaved Transformers. (arXiv:2305.12689v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12689
&lt;/p&gt;
&lt;p&gt;
FIT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#23558;&#25968;&#25454;&#26631;&#35760;&#20998;&#32452;&#65292;&#20351;&#29992;&#23616;&#37096;&#23618;&#21644;&#20840;&#23616;&#23618;&#36827;&#34892;&#25805;&#20316;&#12290;&#36890;&#36807;&#20132;&#38169;&#20351;&#29992;&#36825;&#20123;&#23618;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#20419;&#36827;&#20449;&#24687;&#20132;&#25442;&#65292;FIT&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FIT&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#33258;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#12290;&#19982;&#21407;&#22987;Transformer&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#26631;&#35760;&#20998;&#25104;&#32452;&#65292;&#27599;&#20010;&#32452;&#26159;&#19968;&#20010;&#36739;&#30701;&#30340;&#26631;&#35760;&#24207;&#21015;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;Transformer&#23618;&#65306;&#23616;&#37096;&#23618;&#22312;&#27599;&#20010;&#32452;&#20869;&#25805;&#20316;&#25968;&#25454;&#26631;&#35760;&#65292;&#32780;&#20840;&#23616;&#23618;&#22312;&#19968;&#20010;&#26356;&#23567;&#30340;&#24341;&#20837;&#30340;&#28508;&#22312;&#26631;&#35760;&#38598;&#21512;&#19978;&#25805;&#20316;&#12290;&#36825;&#20123;&#23618;&#21253;&#25324;&#19982;&#26631;&#20934;Transformer&#30456;&#21516;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#23618;&#65292;&#34987;&#20132;&#38169;&#20351;&#29992;&#65292;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#22312;&#21516;&#19968;&#32452;&#20869;&#25968;&#25454;&#21644;&#28508;&#22312;&#26631;&#35760;&#20043;&#38388;&#20419;&#36827;&#20449;&#24687;&#20132;&#25442;&#12290;&#27599;&#20010;&#22823;&#23567;&#20026;n&#30340;&#32452;&#20869;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#20026;$O(n^2)$&#65292;&#20294;&#23545;&#20110;&#38271;&#24230;&#20026;L&#30340;&#24207;&#21015;&#65292;&#21487;&#20197;&#22312;&#20840;&#23616;&#33539;&#22260;&#20869;&#36798;&#21040;$O(L^{{4}/{3}})$&#12290;&#36890;&#36807;&#26356;&#22810;&#22320;&#20381;&#36182;&#25191;&#34892;&#20351;&#29992;&#26356;&#23567;&#28508;&#22312;&#26631;&#35760;&#38598;&#21512;&#30340;&#20840;&#23616;&#23618;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;FIT&#26159;&#19968;&#31181;&#22810;&#29992;&#36884;&#30340;&#26550;&#26500;&#65292;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FIT: a transformer-based architecture with efficient self-attention and adaptive computation. Unlike original transformers, which operate on a single sequence of data tokens, we divide the data tokens into groups, with each group being a shorter sequence of tokens. We employ two types of transformer layers: local layers operate on data tokens within each group, while global layers operate on a smaller set of introduced latent tokens. These layers, comprising the same set of self-attention and feed-forward layers as standard transformers, are interleaved, and cross-attention is used to facilitate information exchange between data and latent tokens within the same group. The attention complexity is $O(n^2)$ locally within each group of size $n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. The efficiency can be further enhanced by relying more on global layers that perform adaptive computation using a smaller set of latent tokens. FIT is a versatile arch
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#27169;&#22411;&#65292;&#31216;&#20026;GNSTODE&#65292;&#36890;&#36807;&#21033;&#29992;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#25551;&#36848;&#20102;&#31890;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#38388;&#21644;&#19981;&#21516;&#31354;&#38388;&#26465;&#20214;&#19979;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.12334</link><description>&lt;p&gt;
&#37319;&#29992;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#21160;&#24577;&#29289;&#29702;&#31995;&#32479;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Towards Complex Dynamic Physics System Simulation with Graph Neural ODEs. (arXiv:2305.12334v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#27169;&#22411;&#65292;&#31216;&#20026;GNSTODE&#65292;&#36890;&#36807;&#21033;&#29992;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#25551;&#36848;&#20102;&#31890;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#38388;&#21644;&#19981;&#21516;&#31354;&#38388;&#26465;&#20214;&#19979;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#24456;&#24378;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#29702;&#35299;&#30495;&#23454;&#30340;&#29289;&#29702;&#19990;&#30028;&#65292;&#22240;&#27492;&#23398;&#20064;&#27169;&#25311;&#22797;&#26434;&#30340;&#31890;&#23376;&#31995;&#32479;&#26159;&#19968;&#20010;&#24456;&#26377;&#21069;&#36884;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#29289;&#29702;&#19990;&#30028;&#30340;&#22797;&#26434;&#35268;&#24459;&#32473;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#22914;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#20043;&#38388;&#30340;&#19981;&#21516;&#31354;&#38388;&#20381;&#36182;&#24615;&#20197;&#21450;&#19981;&#21516;&#26102;&#38388;&#25139;&#20043;&#38388;&#31890;&#23376;&#31995;&#32479;&#29366;&#24577;&#30340;&#19981;&#21516;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#36825;&#20123;&#22240;&#32032;&#20915;&#23450;&#20102;&#31890;&#23376;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#21644;&#29289;&#29702;&#31995;&#32479;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#26041;&#27861;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#26080;&#27861;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#25311;&#32467;&#26524;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#30340;&#29289;&#29702;&#27861;&#21017;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#27169;&#22411;&#8212;&#8212;&#20855;&#26377;&#26102;&#31354;&#24314;&#27169;&#30340;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#65288;GNSTODE&#65289;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#25551;&#36848;&#20102;&#31890;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#38388;&#21644;&#19981;&#21516;&#31354;&#38388;&#26465;&#20214;&#19979;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The great learning ability of deep learning models facilitates us to comprehend the real physical world, making learning to simulate complicated particle systems a promising endeavour. However, the complex laws of the physical world pose significant challenges to the learning based simulations, such as the varying spatial dependencies between interacting particles and varying temporal dependencies between particle system states in different time stamps, which dominate particles' interacting behaviour and the physical systems' evolution patterns. Existing learning based simulation methods fail to fully account for the complexities, making them unable to yield satisfactory simulations. To better comprehend the complex physical laws, this paper proposes a novel learning based simulation model- Graph Networks with Spatial-Temporal neural Ordinary Equations (GNSTODE)- that characterizes the varying spatial and temporal dependencies in particle systems using a united end-to-end framework. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;Transformer&#32467;&#26500;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#20837;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.12095</link><description>&lt;p&gt;
&#20351;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20877;&#27425;&#21331;&#36234;&#65306;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer
&lt;/p&gt;
&lt;p&gt;
Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer. (arXiv:2305.12095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;Transformer&#32467;&#26500;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#20837;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;Transformer&#21644;MLP&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#20248;&#21183;&#12290;&#23613;&#31649;&#22312;NLP&#21644;CV&#26041;&#38754;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;MLP&#30456;&#27604;&#65292;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;Transformer&#65292;&#21363;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#65288;CARD&#65289;&#65292;&#20197;&#35299;&#20915;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;CARD&#24341;&#20837;&#20102;&#21452;Transformer&#32467;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#20449;&#21495;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#22810;&#20010;&#21464;&#37327;&#22312;&#26102;&#38388;&#19978;&#30340;&#21160;&#24577;&#20381;&#36182;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20943;&#36731;&#28508;&#22312;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;&#36825;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#39044;&#27979;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#38271;&#26399;&#21644;&#30701;&#26399;&#39044;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;CARD&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the great power of deep learning methods, particularly Transformer and MLP, for time series forecasting. Despite its success in NLP and CV, many studies found that Transformer is less effective than MLP for time series forecasting. In this work, we design a special Transformer, i.e., channel-aligned robust dual Transformer (CARD for short), that addresses key shortcomings of Transformer in time series forecasting. First, CARD introduces a dual Transformer structure that allows it to capture both temporal correlations among signals and dynamical dependence among multiple variables over time. Second, we introduce a robust loss function for time series forecasting to alleviate the potential overfitting issue. This new loss function weights the importance of forecasting over a finite horizon based on prediction uncertainties. Our evaluation of multiple long-term and short-term forecasting datasets demonstrates that CARD significantly outperforms state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11442</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Text Classification via Self-Supervised Tuning. (arXiv:2305.11442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65307;&#35201;&#20040;&#20381;&#36182;&#20110;&#22823;&#37327;&#30456;&#20851;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;&#33258;&#30417;&#30563;&#35843;&#25972;&#12290;&#36890;&#36807;&#25506;&#32034;&#33258;&#30001;&#25991;&#26412;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#31216;&#20026;&#39318;&#21477;&#39044;&#27979;&#65292;&#20197;&#24357;&#21512;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35843;&#25972;&#27169;&#22411;&#20197;&#23398;&#20064;&#26681;&#25454;&#21097;&#20313;&#25991;&#26412;&#26469;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#21518;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#22914;&#20027;&#39064;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;10&#20010;&#20219;&#21153;&#20013;&#30340;7&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RHIP&#65289;&#65292;&#36890;&#36807;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#35299;&#20915;&#20102;&#20840;&#29699;&#35268;&#27169;&#30340;MDPs&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#22312;&#35895;&#27468;&#22320;&#22270;&#20013;&#23454;&#29616;&#20102;16-24%&#30340;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.11290</link><description>&lt;p&gt;
&#35895;&#27468;&#22320;&#22270;&#20013;&#30340;&#22823;&#35268;&#27169;&#21487;&#25193;&#23637;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Massively Scalable Inverse Reinforcement Learning in Google Maps. (arXiv:2305.11290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RHIP&#65289;&#65292;&#36890;&#36807;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#35299;&#20915;&#20102;&#20840;&#29699;&#35268;&#27169;&#30340;MDPs&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#22312;&#35895;&#27468;&#22320;&#22270;&#20013;&#23454;&#29616;&#20102;16-24%&#30340;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#20154;&#31867;&#28508;&#22312;&#20559;&#22909;&#26159;&#36335;&#32447;&#25512;&#33616;&#20013;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#20840;&#29699;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36807;&#21435;&#30340;&#30740;&#31350;&#20026;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#21019;&#24314;&#20102;&#36234;&#26469;&#36234;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#23578;&#26410;&#25104;&#21151;&#25193;&#23637;&#21040;&#19990;&#30028;&#35268;&#27169;&#30340;MDP&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#65292;&#20998;&#21035;&#28041;&#21450;&#25968;&#20159;&#20010;&#29366;&#24577;&#12289;&#36712;&#36857;&#21644;&#21442;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25913;&#36827;&#65292;&#32858;&#28966;&#20110;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#65292;&#31361;&#30772;&#20197;&#24448;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36870;&#21521;&#35268;&#21010;&#36882;&#36827;&#22320;&#24179;&#38754;(RHIP)&#65292;&#23427;&#21487;&#20197;&#27010;&#25324;&#29616;&#26377;&#30340;&#24037;&#20316;&#65292;&#24182;&#36890;&#36807;&#20854;&#35268;&#21010;&#27700;&#24179;&#25511;&#21046;&#20851;&#38190;&#24615;&#33021;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;16-24%&#30340;&#25913;&#36827;&#65292;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#26159;&#36804;&#20170;&#20026;&#27490;&#23454;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#26368;&#22823;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#26356;&#22909;&#30340;&#23548;&#33322;&#34892;&#20026;&#21644;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing for humans' latent preferences is a grand challenge in route recommendation, where globally-scalable solutions remain an open problem. Although past work created increasingly general solutions for the application of inverse reinforcement learning (IRL), these have not been successfully scaled to world-sized MDPs, large datasets, and highly parameterized models; respectively hundreds of millions of states, trajectories, and parameters. In this work, we surpass previous limitations through a series of advancements focused on graph compression, parallelization, and problem initialization based on dominant eigenvectors. We introduce Receding Horizon Inverse Planning (RHIP), which generalizes existing work and enables control of key performance trade-offs via its planning horizon. Our policy achieves a 16-24% improvement in global route quality, and, to our knowledge, represents the largest instance of IRL in a real-world setting to date. Our results show critical benefits to mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#33258;&#20027;&#28293;&#23556;&#27785;&#31215;&#34180;&#33180;&#30340;&#20202;&#22120;&#65292;&#21033;&#29992;Python&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#25511;&#21046;&#34180;&#33180;&#32452;&#25104;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#27493;&#20240;&#12290;</title><link>http://arxiv.org/abs/2305.11122</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#20809;&#23398;&#31561;&#31163;&#23376;&#20307;&#21457;&#23556;&#30340;&#33258;&#20027;&#28293;&#23556;&#21512;&#25104;&#34180;&#33180;&#27694;&#21270;&#29289;
&lt;/p&gt;
&lt;p&gt;
Autonomous sputter synthesis of thin film nitrides with composition controlled by Bayesian optimization of optical plasma emission. (arXiv:2305.11122v2 [physics.app-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#33258;&#20027;&#28293;&#23556;&#27785;&#31215;&#34180;&#33180;&#30340;&#20202;&#22120;&#65292;&#21033;&#29992;Python&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#25511;&#21046;&#34180;&#33180;&#32452;&#25104;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#27493;&#20240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#23454;&#39564;&#24050;&#25104;&#20026;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#27493;&#20240;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#34429;&#28982;&#33258;&#20027;&#21512;&#25104;&#30340;&#20202;&#22120;&#22312;&#20998;&#23376;&#21644;&#32858;&#21512;&#29289;&#31185;&#23398;&#20013;&#24050;&#32463;&#24456;&#27969;&#34892;&#65292;&#20294;&#26159;&#29992;&#20110;&#28151;&#21512;&#26448;&#26009;&#21644;&#32435;&#31859;&#39063;&#31890;&#30340;&#28342;&#28082;&#22788;&#29702;&#65292;&#29289;&#29702;&#27668;&#30456;&#27785;&#31215;&#30340;&#33258;&#20027;&#24037;&#20855;&#21364;&#24456;&#23569;&#35265;&#65292;&#20294;&#23545;&#20110;&#21322;&#23548;&#20307;&#34892;&#19994;&#32780;&#35328;&#21364;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#33258;&#20027;&#28293;&#23556;&#27785;&#31215;&#34180;&#33180;&#30340;&#20202;&#22120;&#35774;&#35745;&#21644;&#23454;&#26045;&#65292;&#21033;&#29992;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;&#28293;&#23556;&#21453;&#24212;&#22120;&#12289;Python&#12289;&#20809;&#30005;&#21457;&#23556;&#20809;&#35889;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#25511;&#21046;&#34180;&#33180;&#32452;&#25104;&#12290;&#25105;&#20204;&#23558;&#30001;&#20803;&#32032;&#38156;&#21644;&#38043;&#22312;&#27694;&#27668;&#27668;&#27675;&#19979;&#20849;&#28293;&#23556;&#26102;&#30417;&#27979;&#21040;&#30340;&#21457;&#23556;&#32447;&#20316;&#20026;&#20809;&#35889;&#25968;&#25454;&#65292;&#24182;&#23558;&#34180;&#33180;&#25104;&#20998;&#65288;&#30001;X&#33639;&#20809;&#27861;&#27979;&#37327;&#65289;&#24314;&#27169;&#20026;&#21457;&#23556;&#32447;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#30001;OES&#25552;&#20379;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#25511;&#21046;&#31639;&#27861;&#22312;&#28293;&#23556;&#21151;&#29575;&#30340;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#20197;&#21046;&#36896;&#31526;&#21512;&#29992;&#25143;&#23450;&#20041;&#30340;&#32452;&#25104;&#30340;&#34180;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous experimentation has emerged as an efficient approach to accelerate the pace of materials discovery. Although instruments for autonomous synthesis have become popular in molecular and polymer science, solution processing of hybrid materials and nanoparticles, examples of autonomous tools for physical vapour deposition are scarce yet important for the semiconductor industry. Here, we report the design and implementation of an autonomous instrument for sputter deposition of thin films with controlled composition, leveraging a highly automated sputtering reactor custom-controlled by Python, optical emission spectroscopy (OES), and Bayesian optimization algorithm. We modeled film composition, measured by x-ray fluorescence, as a linear function of emission lines monitored during the co-sputtering from elemental Zn and Ti targets in N$_2$ atmosphere. A Bayesian control algorithm, informed by OES, navigates the space of sputtering power to fabricate films with user-defined composit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.10235</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#30340;&#38544;&#34255;&#39118;&#38505;&#65306;&#20851;&#20110;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#24320;&#25918;&#24335;&#29615;&#22659;&#65288;&#22914;API&#12289;&#24320;&#28304;&#27169;&#22411;&#21644;&#25554;&#20214;&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#32570;&#20047;&#20840;&#38754;&#35752;&#35770;&#21644;&#20998;&#26512;&#28508;&#22312;&#39118;&#38505;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#20294;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;LLMs&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#26469;&#22788;&#29702;&#22823;&#37327;&#26597;&#35810;/&#21709;&#24212;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;ChatGPT&#12289;LLaMA&#21644;OPT&#22312;&#20869;&#30340;&#20027;&#27969;LLMs&#36827;&#34892;&#20102;100&#22810;&#19975;&#20010;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#26680;&#24515;&#21253;&#25324;&#25968;&#25454;&#21407;&#35821;&#65292;&#38543;&#21518;&#26159;&#33258;&#21160;&#35299;&#37322;&#22120;&#65292;&#35780;&#20272;&#36825;&#20123;LLMs&#22312;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#24230;&#37327;&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#12289;&#20063;&#35768;&#26159;&#19981;&#24184;&#30340;&#32467;&#35770;&#65292;&#36825;&#20123;&#32467;&#35770;&#30456;&#24403;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
&lt;/p&gt;</description></item><item><title>sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.08711</link><description>&lt;p&gt;
sustain.AI: &#19968;&#31181;&#20998;&#26512;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
sustain.AI: a Recommender System to analyze Sustainability Reports. (arXiv:2305.08711v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08711
&lt;/p&gt;
&lt;p&gt;
sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;sustain.AI&#65292;&#36825;&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#20102;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;BERT&#30340;&#32534;&#30721;&#27169;&#22359;&#19982;&#22810;&#26631;&#31614;&#20998;&#31867;&#22836;&#30456;&#32467;&#21512;&#65292;&#23558;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#19982;&#20840;&#29699;&#25253;&#21578;&#20513;&#35758;&#65288;GRI&#65289;&#26631;&#20934;&#20013;&#30340;&#30456;&#24212;&#27861;&#24459;&#27861;&#35268;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26032;&#39062;&#30340;&#24503;&#22269;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22987;&#32456;&#23454;&#29616;&#20102;&#19982;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#26356;&#39640;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;sustain.AI&#24050;&#32463;&#20844;&#24320;&#22312;https://sustain.ki.nrw/&#19978;&#25552;&#20379;&#32473;&#25152;&#26377;&#20154;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present $\text{sustain.AI}$, an intelligent, context-aware recommender system that assists auditors and financial investors as well as the general public to efficiently analyze companies' sustainability reports. The tool leverages an end-to-end trainable architecture that couples a BERT-based encoding module with a multi-label classification head to match relevant text passages from sustainability reports to their respective law regulations from the Global Reporting Initiative (GRI) standards. We evaluate our model on two novel German sustainability reporting data sets and consistently achieve a significantly higher recommendation performance compared to multiple strong baselines. Furthermore, $\text{sustain.AI}$ is publicly available for everyone at https://sustain.ki.nrw/.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#23458;&#25143;&#20195;&#29702;&#37197;&#23545;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;$215\%$&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.08594</link><description>&lt;p&gt;
&#36890;&#36807;&#26234;&#33021;&#23458;&#25143;&#20195;&#29702;&#37197;&#23545;&#25913;&#21892;&#21628;&#21483;&#20013;&#24515;&#30340;&#23458;&#25143;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
Improving Customer Experience in Call Centers with Intelligent Customer-Agent Pairing. (arXiv:2305.08594v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08594
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#23458;&#25143;&#20195;&#29702;&#37197;&#23545;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;$215\%$&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28385;&#24847;&#30340;&#23458;&#25143;&#23545;&#20110;&#19968;&#20010;&#30408;&#21033;&#30340;&#32452;&#32455;&#25110;&#20844;&#21496;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#25552;&#39640;&#23458;&#25143;&#20307;&#39564;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#20248;&#21270;&#21628;&#21483;&#20013;&#24515;&#30340;&#21151;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19982;&#20840;&#22269;&#26368;&#22823;&#30340;&#30005;&#20449;&#21644;&#20114;&#32852;&#32593;&#25509;&#20837;&#20379;&#24212;&#21830;&#21512;&#20316;&#65292;&#23558;&#23458;&#25143;&#20195;&#29702;&#37197;&#23545;&#38382;&#39064;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#35268;&#21017;-based&#26041;&#27861;&#65292;&#24615;&#33021;&#26377;$215\%$&#30340;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Customer experience plays a critical role for a profitable organisation or company. A satisfied customer for a company corresponds to higher rates of customer retention, and better representation in the market. One way to improve customer experience is to optimize the functionality of its call center. In this work, we have collaborated with the largest provider of telecommunications and Internet access in the country, and we formulate the customer-agent pairing problem as a machine learning problem. The proposed learning-based method causes a significant improvement in performance of about $215\%$ compared to a rule-based method.
&lt;/p&gt;</description></item><item><title>MaxViT-UNet&#26159;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#22312;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.08396</link><description>&lt;p&gt;
MaxViT-UNet: &#22810;&#36724;&#27880;&#24847;&#21147;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08396
&lt;/p&gt;
&lt;p&gt;
MaxViT-UNet&#26159;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#22312;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#21367;&#31215;&#31639;&#23376;&#30340;&#23616;&#37096;&#24615;&#36136;&#25233;&#21046;&#20102;CNNs&#25429;&#25417;&#20840;&#23616;&#21644;&#38271;&#31243;&#20132;&#20114;&#12290;&#26368;&#36817;&#65292;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#31038;&#21306;&#21644;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#20294;&#26159;&#65292;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#21644;&#32570;&#20047;CNN&#31867;&#24402;&#32435;&#20559;&#24046;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MaxViT-UNet&#65292;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#25552;&#20986;&#30340;&#28151;&#21512;&#35299;&#30721;&#22120;&#65292;&#36824;&#22522;&#20110;MaxViT-block&#65292;&#26088;&#22312;&#22312;&#27599;&#20010;&#35299;&#30721;&#38454;&#27573;&#26368;&#23567;&#21270;&#35745;&#31639;&#36127;&#25285;&#19979;&#21033;&#29992;&#21367;&#31215;&#21644;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#21147;&#37327;&#12290;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#30340;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;&#28151;&#21512;&#35299;&#30721;&#22120;&#22359;&#26368;&#21021;&#36890;&#36807;&#19978;&#37319;&#26679;&#20256;&#36755;&#20302;&#23618;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks have made significant strides in medical image analysis in recent years. However, the local nature of the convolution operator inhibits the CNNs from capturing global and long-range interactions. Recently, Transformers have gained popularity in the computer vision community and also medical image segmentation. But scalability issues of self-attention mechanism and lack of the CNN like inductive bias have limited their adoption. In this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision transformer for medical image segmentation. The proposed hybrid decoder, also based on MaxViT-block, is designed to harness the power of convolution and self-attention mechanism at each decoding stage with minimal computational burden. The multi-axis self-attention in each decoder stage helps in differentiating between the object and background regions much more efficiently. The hybrid decoder block initially fuses the lower level features upsampled via tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2305.07759</link><description>&lt;p&gt;
TinyStories: &#35821;&#35328;&#27169;&#22411;&#33021;&#31616;&#23567;&#21040;&#20160;&#20040;&#31243;&#24230;&#21364;&#20381;&#28982;&#33021;&#22815;&#35762;&#36848;&#36830;&#36143;&#30340;&#33521;&#25991;&#25925;&#20107;&#65311;
&lt;/p&gt;
&lt;p&gt;
TinyStories: How Small Can Language Models Be and Still Speak Coherent English?. (arXiv:2305.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#23567;&#22411;&#21270;&#26102;&#32463;&#24120;&#38590;&#20197;&#20135;&#29983;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026; TinyStories &#30340;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#35268;&#27169;&#23567;&#12289;&#22797;&#26434;&#24230;&#20302;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#30701;&#25925;&#20107;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).  In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet stil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Masked Audio Text Encoders&#65288;MATE&#65289;&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#20351;&#29992;MATE&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36827;&#34892;&#22810;&#27169;&#24577;&#25171;&#20998;&#65292;&#21363;&#20351;&#22312;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#19979;&#23601;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.07677</link><description>&lt;p&gt;
Masked Audio Text Encoders &#22312;&#22810;&#27169;&#24577;&#37325;&#25171;&#20998;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Audio Text Encoders are Effective Multi-Modal Rescorers. (arXiv:2305.07677v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Masked Audio Text Encoders&#65288;MATE&#65289;&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#20351;&#29992;MATE&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36827;&#34892;&#22810;&#27169;&#24577;&#25171;&#20998;&#65292;&#21363;&#20351;&#22312;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#19979;&#23601;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#20108;&#27425;&#25171;&#20998;&#38750;&#24120;&#26377;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Masked Audio Text Encoder&#65288;MATE&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#26469;&#26377;&#25928;&#22320;&#23545;&#40784;&#21508;&#31181;&#27169;&#24577;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#37325;&#26032;&#25171;&#20998;&#22120;&#23545;ASR&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#24456;&#26377;&#22909;&#22788;&#12290;&#19982;&#20165;&#25991;&#26412;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#22495;&#20869;&#25968;&#25454;&#32452;&#19978;&#65292;MATE &#21487;&#20197;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;4&#65285;-16&#65285;&#65292;&#22312;&#22495;&#22806;&#25968;&#25454;&#32452;&#19978;&#21487;&#23558;WER&#38477;&#20302;3&#65285;-7&#65285;&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;0.8&#23567;&#26102;&#65289;&#65292;MATE&#23601;&#21487;&#20197;&#23558;WER&#27604;&#19968;&#27425;&#25171;&#20998;&#30340;&#22522;&#32447;&#38477;&#20302;8&#65285;-23&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of the ASR system when target domain data is unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and 3%-7% on out-of-domain datasets, over the text-only baseline. Additionally, with very limited amount of training data (0.8 hours), MATE achieves a WER reduction of 8%-23% over the first-pass baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#39281;&#21644;&#38750;&#21333;&#35843;&#28608;&#27963;&#20989;&#25968;&#65288;SGELU&#12289;SSiLU&#21644;SMish&#65289;&#65292;&#23427;&#20204;&#30001;GELU&#12289;SiLU&#12289;Mish&#21450;ReLU&#30340;&#27491;&#37096;&#20998;&#32452;&#25104;&#65292;&#33021;&#22815;&#22312;CIFAR-100&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#20013;&#23637;&#29616;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07537</link><description>&lt;p&gt;
&#39281;&#21644;&#38750;&#21333;&#35843;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Saturated Non-Monotonic Activation Functions. (arXiv:2305.07537v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#39281;&#21644;&#38750;&#21333;&#35843;&#28608;&#27963;&#20989;&#25968;&#65288;SGELU&#12289;SSiLU&#21644;SMish&#65289;&#65292;&#23427;&#20204;&#30001;GELU&#12289;SiLU&#12289;Mish&#21450;ReLU&#30340;&#27491;&#37096;&#20998;&#32452;&#25104;&#65292;&#33021;&#22815;&#22312;CIFAR-100&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#20013;&#23637;&#29616;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;&#12289;&#28789;&#27963;&#24615;&#24378;&#30340;&#28608;&#27963;&#20989;&#25968;&#37117;&#26159;&#21333;&#35843;&#20989;&#25968;&#65292;&#20294;&#19968;&#20123;&#38750;&#21333;&#35843;&#28608;&#27963;&#20989;&#25968;&#27491;&#22312;&#34987;&#25506;&#32034;&#24182;&#23637;&#29616;&#20986;&#24456;&#26377;&#21069;&#26223;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;: SGELU&#12289;SSiLU&#21644;SMish&#12290;&#36825;&#20123;&#28608;&#27963;&#20989;&#25968;&#26159;&#30001;GELU&#12289;SiLU&#12289;Mish&#20197;&#21450;ReLU&#30340;&#27491;&#37096;&#20998;&#32452;&#25104;&#65292;&#24182;&#22312;CIFAR-100&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation functions are essential to deep learning networks. Popular and versatile activation functions are mostly monotonic functions, some non-monotonic activation functions are being explored and show promising performance. But by introducing non-monotonicity, they also alter the positive input, which is proved to be unnecessary by the success of ReLU and its variants. In this paper, we double down on the non-monotonic activation functions' development and propose the Saturated Gaussian Error Linear Units by combining the characteristics of ReLU and non-monotonic activation functions. We present three new activation functions built with our proposed method: SGELU, SSiLU, and SMish, which are composed of the negative portion of GELU, SiLU, and Mish, respectively, and ReLU's positive portion. The results of image classification experiments on CIFAR-100 indicate that our proposed activation functions are highly effective and outperform state-of-the-art baselines across multiple deep l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#20854;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#35299;&#20915;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#38382;&#39064;&#24471;&#21040;&#23450;&#20041;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.06348</link><description>&lt;p&gt;
&#24102;&#27010;&#29575;&#24577;&#23556;&#21644;&#26680;&#24179;&#22343;&#23884;&#20837;&#30340;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised learning with probabilistic morphisms and kernel mean embeddings. (arXiv:2305.06348v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#20854;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#35299;&#20915;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#38382;&#39064;&#24471;&#21040;&#23450;&#20041;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;X&#21644;&#26631;&#31614;&#31354;&#38388;Y&#12290; &#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#24517;&#39035;&#27491;&#30830;&#22320;&#24230;&#37327;&#21487;&#33021;&#39044;&#27979;&#22120;&#30340;&#20551;&#35774;&#31354;&#38388;H&#20013;&#30340;&#20803;&#32032;&#19982;&#30417;&#31649;&#36816;&#31639;&#31526;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#30417;&#31649;&#36816;&#31639;&#31526;&#21487;&#33021;&#19981;&#23646;&#20110;H&#12290; &#20026;&#20102;&#23450;&#20041;&#27491;&#30830;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#22312;&#25237;&#24433;&#928;X&#65306;X&#215;Y&#8594;X&#30456;&#23545;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#119883;&#215;&#119884;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#956;Y| X&#30340;&#29305;&#27530;&#24615;&#36136;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20316;&#20026;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#22914;&#26524;Y&#26159;&#19968;&#20010;&#20855;&#26377;Borel &#963;-&#20195;&#25968; BY&#30340;&#21487;&#20998;&#30340;&#21487;&#24230;&#37327;&#21270;&#25299;&#25169;&#31354;&#38388;&#65292;&#21017;&#25552;&#20986;&#20102;&#20851;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#30456;&#23545;&#20110;&#25237;&#24433;&#928;X&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#956;Y| X&#30340;&#21478;&#19968;&#31181;&#29305;&#27530;&#24615;&#36136;&#30340;&#34920;&#24449;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\mathcal{X}$ and a label space $\mathcal{Y}$, which are measurable spaces. A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure $\mu$ on $\mathcal{X} \times \mathcal{Y}$ relative to the projection $\Pi_{\mathcal{X}}: \mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solution of a linear operator equation. If $\mathcal{Y}$ is a separable metrizable topological space with the Borel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$, I propose another characterization of a regular conditional probability measure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#20195;&#30721;&#29255;&#27573;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#20174;&#20195;&#30721;&#20013;&#20998;&#31867;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#20195;&#30721;&#30340;&#22810;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.05379</link><description>&lt;p&gt;
TASTY&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26102;&#31354;&#22797;&#26434;&#24230;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TASTY: A Transformer based Approach to Space and Time complexitY. (arXiv:2305.05379v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#20195;&#30721;&#29255;&#27573;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#20174;&#20195;&#30721;&#20013;&#20998;&#31867;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#20195;&#30721;&#30340;&#22810;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#38750;&#24120;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#22914;&#20195;&#30721;&#30340;&#23436;&#21892;&#12289;&#20195;&#30721;&#30340;&#34917;&#20840;&#21644;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#38598;&#65292;&#20174;&#20195;&#30721;&#20013;&#20998;&#31867;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#20219;&#21153;&#36824;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#20165;&#38480;&#20110;Java&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#20195;&#30721;&#29255;&#27573;&#26631;&#35760;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65288;&#30446;&#21069;&#26159;Python&#21644;C ++&#25968;&#25454;&#38598;&#65292;&#19981;&#20037;&#23558;&#21457;&#24067;C&#65292;C&#65283;&#21644;JavaScript&#25968;&#25454;&#38598;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#35745;&#31639;&#24211;&#21644;&#24037;&#20855;&#20165;&#36866;&#29992;&#20110;&#23569;&#25968;&#29992;&#20363;&#12290;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20419;&#20351;&#36816;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#20195;&#30721;&#30340;&#22810;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27515;&#20195;&#30721;&#28040;&#38500;&#21644;&#22686;&#21152;LM&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#26102;&#38388;&#22797;&#26434;&#24615;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#35758;&#20351;&#29992;LM&#26469;&#23547;&#25214;&#31354;&#38388;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code based Language Models (LMs) have shown very promising results in the field of software engineering with applications such as code refinement, code completion and generation. However, the task of time and space complexity classification from code has not been extensively explored due to a lack of datasets, with prior endeavors being limited to Java. In this project, we aim to address these gaps by creating a labelled dataset of code snippets spanning multiple languages (Python and C++ datasets currently, with C, C#, and JavaScript datasets being released shortly). We find that existing time complexity calculation libraries and tools only apply to a limited number of use-cases. The lack of a well-defined rule based system motivates the application of several recently proposed code-based LMs. We demonstrate the effectiveness of dead code elimination and increasing the maximum sequence length of LMs. In addition to time complexity, we propose to use LMs to find space complexities from
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;</title><link>http://arxiv.org/abs/2305.03403</link><description>&lt;p&gt;
GPT&#29992;&#20110;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#24341;&#20837;CAAFE&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03403
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21151;&#33021;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21517;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;CAAFE&#65289;&#65292;&#23427;&#21033;&#29992;LLM&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#29992;&#20110;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;Python&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#29983;&#25104;&#29305;&#24449;&#30340;&#25928;&#29992;&#35828;&#26126;&#12290;&#23613;&#31649;&#26041;&#27861;&#35770;&#19978;&#24456;&#31616;&#21333;&#65292;&#20294;CAAFE&#25552;&#39640;&#20102;14&#20010;&#25968;&#25454;&#38598;&#20013;11&#20010;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#19982;2&#20010;&#25968;&#25454;&#38598;&#24182;&#21015;&#65292;&#21482;&#26377;1&#20010;&#25968;&#25454;&#38598;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;ROC AUC&#34920;&#29616;&#20174;0.798&#25552;&#21319;&#33267;0.822&#12290;&#23545;&#20110;&#25152;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#25913;&#36827;&#19982;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;AUC 0.782&#65289;&#20195;&#26367;&#36923;&#36753;&#22238;&#24402;&#65288;AUC 0.754&#65289;&#25152;&#33719;&#24471;&#30340;&#24179;&#22343;&#25913;&#36827;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#26412;&#27969;&#21305;&#37197;&#31639;&#27861;&#65292;&#22312;&#28385;&#36275;&#27491;&#30830;&#30340;&#36793;&#32536;&#32422;&#26463;&#30340;&#26465;&#20214;&#19979;&#65292;&#21033;&#29992;&#23567;&#25209;&#37327;&#32806;&#21512;&#23558;&#27969;&#36827;&#34892;&#30699;&#27491;&#65292;&#20174;&#32780;&#20351;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#20302;&#32500;&#20195;&#20215;&#30340;&#36816;&#36755;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.14772</link><description>&lt;p&gt;
&#22810;&#26679;&#26412;&#27969;&#21305;&#37197;&#65306;&#21033;&#29992;&#23567;&#25209;&#37327;&#32806;&#21512;&#23558;&#27969;&#36827;&#34892;&#30699;&#27491;
&lt;/p&gt;
&lt;p&gt;
Multisample Flow Matching: Straightening Flows with Minibatch Couplings. (arXiv:2304.14772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14772
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#26412;&#27969;&#21305;&#37197;&#31639;&#27861;&#65292;&#22312;&#28385;&#36275;&#27491;&#30830;&#30340;&#36793;&#32536;&#32422;&#26463;&#30340;&#26465;&#20214;&#19979;&#65292;&#21033;&#29992;&#23567;&#25209;&#37327;&#32806;&#21512;&#23558;&#27969;&#36827;&#34892;&#30699;&#27491;&#65292;&#20174;&#32780;&#20351;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#20302;&#32500;&#20195;&#20215;&#30340;&#36816;&#36755;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#27169;&#25311;&#30340;&#36830;&#32493;&#26102;&#38388;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#26500;&#24314;&#20102;&#20174;&#22122;&#22768;&#20998;&#24067;&#21040;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#27010;&#29575;&#36335;&#24452;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697;&#65292;&#22914;&#27969;&#21305;&#37197;&#65292;&#23548;&#20986;&#20102;&#26368;&#36866;&#21512;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#20381;&#36182;&#20110;&#29420;&#31435;&#30340;&#25968;&#25454;&#21644;&#22122;&#22768;&#26679;&#26412;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#22522;&#30784;&#32467;&#26500;&#26469;&#26500;&#24314;&#27010;&#29575;&#36335;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#26412;&#27969;&#21305;&#37197;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#25968;&#25454;&#21644;&#22122;&#22768;&#26679;&#26412;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#32806;&#21512;&#65292;&#21516;&#26102;&#28385;&#36275;&#27491;&#30830;&#30340;&#36793;&#32536;&#32422;&#26463;&#12290;&#22312;&#38750;&#24120;&#23567;&#30340;&#24320;&#38144;&#19979;&#65292;&#36825;&#31181;&#27867;&#21270;&#20351;&#25105;&#20204;&#33021;&#22815;(i) &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38477;&#20302;&#26799;&#24230;&#26041;&#24046;&#65292;(ii) &#33719;&#24471;&#26356;&#21152;&#30452;&#25509;&#30340;&#27969;&#65292;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26356;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;(iii) &#33719;&#24471;&#26356;&#20302;&#32500;&#20195;&#20215;&#30340;&#36816;&#36755;&#22270;&#65292;&#36825;&#22312;&#29983;&#25104;&#27169;&#22411;&#20043;&#22806;&#20063;&#26377;&#24212;&#29992;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20197;&#27010;&#24565;&#19978;&#31616;&#21333;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#23567;&#25209;&#37327;&#32806;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-free methods for training continuous-time generative models construct probability paths that go between noise distributions and individual data samples. Recent works, such as Flow Matching, derived paths that are optimal for each data sample. However, these algorithms rely on independent data and noise samples, and do not exploit underlying structure in the data distribution for constructing probability paths. We propose Multisample Flow Matching, a more general framework that uses non-trivial couplings between data and noise samples while satisfying the correct marginal constraints. At very small overhead costs, this generalization allows us to (i) reduce gradient variance during training, (ii) obtain straighter flows for the learned vector field, which allows us to generate high-quality samples using fewer function evaluations, and (iii) obtain transport maps with lower cost in high dimensions, which has applications beyond generative modeling. Importantly, we do so in a c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;"&#21464;&#33394;&#40857;"&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#21152;&#32784;&#29992;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#25552;&#20379;&#30340;&#33391;&#24615;&#22270;&#20687;&#21644;&#26377;&#27602;&#22270;&#20687;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12961</link><description>&lt;p&gt;
&#21464;&#33394;&#40857;: &#36866;&#24212;&#23545;&#31561;&#38236;&#20687;&#20197;&#26893;&#20837;&#32784;&#29992;&#21518;&#38376;&#26469;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning. (arXiv:2304.12961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;"&#21464;&#33394;&#40857;"&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#21152;&#32784;&#29992;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#25552;&#20379;&#30340;&#33391;&#24615;&#22270;&#20687;&#21644;&#26377;&#27602;&#22270;&#20687;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#19978;&#20256;&#20854;&#26412;&#22320;&#27169;&#22411;&#21040;&#20013;&#24515;&#26381;&#21153;&#22120;&#20197;&#32858;&#21512;&#25104;&#20840;&#23616;&#27169;&#22411;&#12290;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#20197;&#36890;&#36807;&#19978;&#20256;&#26377;&#27602;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#22312;&#20840;&#23616;&#27169;&#22411;&#20013;&#26893;&#20837;&#21518;&#38376;&#65292;&#23548;&#33268;&#20855;&#26377;&#29305;&#23450;&#27169;&#24335;&#30340;&#22270;&#20687;&#34987;&#38169;&#35823;&#20998;&#31867;&#20026;&#26576;&#20123;&#30446;&#26631;&#26631;&#31614;&#12290;&#24403;&#21069;&#25915;&#20987;&#26893;&#20837;&#30340;&#21518;&#38376;&#26159;&#19981;&#32784;&#29992;&#30340;&#65292;&#19968;&#26086;&#25915;&#20987;&#32773;&#20572;&#27490;&#27169;&#22411;&#20013;&#27602;&#65292;&#20415;&#20250;&#36805;&#36895;&#28040;&#22833;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;FL&#21518;&#38376;&#30340;&#32784;&#29992;&#24615;&#19982;&#33391;&#24615;&#22270;&#35937;&#21644;&#26377;&#27602;&#22270;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;(&#21363;&#22312;&#26412;&#22320;&#35757;&#32451;&#26399;&#38388;&#26631;&#31614;&#34987;&#32763;&#36716;&#20026;&#30446;&#26631;&#26631;&#31614;&#30340;&#22270;&#20687;)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21457;&#29616;&#21407;&#22987;&#22270;&#35937;&#21644;&#26377;&#27602;&#22270;&#35937;&#30340;&#30446;&#26631;&#26631;&#31614;&#23545;&#21518;&#38376;&#30340;&#32784;&#20037;&#24615;&#26377;&#20851;&#38190;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#65292;&#31216;&#20026;"&#21464;&#33394;&#40857;"&#65292;&#23427;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#24433;&#21709;&#26469;&#23454;&#29616;&#26356;&#32784;&#29992;&#30340;&#21518;&#38376;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a federated learning (FL) system, distributed clients upload their local models to a central server to aggregate into a global model. Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing images with specific patterns to be misclassified into some target labels. Backdoors planted by current attacks are not durable, and vanish quickly once the attackers stop model poisoning. In this paper, we investigate the connection between the durability of FL backdoors and the relationships between benign images and poisoned images (i.e., the images whose labels are flipped to the target label during local training). Specifically, benign images with the original and the target labels of the poisoned images are found to have key effects on backdoor durability. Consequently, we propose a novel attack, Chameleon, which utilizes contrastive learning to further amplify such effects towards a more durable backdoor. Extensive experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ProbDR&#21464;&#20998;&#26694;&#26550;&#65292;&#23558;&#32463;&#20856;&#38477;&#32500;&#31639;&#27861;&#35299;&#37322;&#20026;&#27010;&#29575;&#25512;&#26029;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#35777;&#25454;&#19979;&#30028;&#26469;&#23436;&#25104;&#25512;&#26029;&#25805;&#20316;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#21487;&#20197;&#23436;&#25104;&#24120;&#35268;&#38477;&#32500;&#31639;&#27861;&#65292;&#36824;&#25903;&#25345;&#20351;&#29992;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#38477;&#32500;&#25805;&#20316;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.07658</link><description>&lt;p&gt;
&#20316;&#20026;&#27010;&#29575;&#25512;&#26029;&#30340;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduction as Probabilistic Inference. (arXiv:2304.07658v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07658
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ProbDR&#21464;&#20998;&#26694;&#26550;&#65292;&#23558;&#32463;&#20856;&#38477;&#32500;&#31639;&#27861;&#35299;&#37322;&#20026;&#27010;&#29575;&#25512;&#26029;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#35777;&#25454;&#19979;&#30028;&#26469;&#23436;&#25104;&#25512;&#26029;&#25805;&#20316;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#21487;&#20197;&#23436;&#25104;&#24120;&#35268;&#38477;&#32500;&#31639;&#27861;&#65292;&#36824;&#25903;&#25345;&#20351;&#29992;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#38477;&#32500;&#25805;&#20316;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#32500;&#31639;&#27861;&#23558;&#39640;&#32500;&#25968;&#25454;&#21387;&#32553;&#21040;&#20302;&#32500;&#34920;&#31034;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#38477;&#32500;&#26159;&#35768;&#22810;&#20998;&#26512;&#27969;&#31243;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#23454;&#29616;&#20102;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270;&#12289;&#22122;&#22768;&#38477;&#20302;&#21644;&#39640;&#25928;&#30340;&#19979;&#28216;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ProbDR&#21464;&#20998;&#26694;&#26550;&#65292;&#23558;&#24191;&#27867;&#30340;&#32463;&#20856;DR&#31639;&#27861;&#35299;&#37322;&#20026;&#35813;&#26694;&#26550;&#20013;&#30340;&#27010;&#29575;&#25512;&#26029;&#31639;&#27861;&#12290;ProbDR&#21253;&#25324;PCA&#12289;CMDS&#12289;LLE&#12289;LE&#12289;MVU&#12289;&#25193;&#25955;&#26144;&#23556;&#12289;kPCA&#12289;Isomap&#12289;(t-)SNE&#21644;UMAP&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#19968;&#20010;&#20302;&#32500;&#28508;&#21464;&#37327;&#29992;&#20110;&#26500;&#24314;&#21327;&#26041;&#24046;&#12289;&#31934;&#24230;&#25110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#12290;&#25512;&#26029;&#26159;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#35777;&#25454;&#19979;&#30028;&#26469;&#23436;&#25104;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#25903;&#25345;&#20351;&#29992;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#65288;PPL&#65289;&#36827;&#34892;DR&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#21487;&#20197;&#23436;&#25104;&#24120;&#35268;DR&#31639;&#27861;&#30340;&#25805;&#20316;&#65292;&#24182;&#36171;&#20104;&#20102;&#23427;&#36890;&#36807;&#27010;&#29575;&#21464;&#20998;&#25512;&#26029;&#30340;&#24378;&#22823;&#34920;&#36798;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction (DR) algorithms compress high-dimensional data into a lower dimensional representation while preserving important features of the data. DR is a critical step in many analysis pipelines as it enables visualisation, noise reduction and efficient downstream processing of the data. In this work, we introduce the ProbDR variational framework, which interprets a wide range of classical DR algorithms as probabilistic inference algorithms in this framework. ProbDR encompasses PCA, CMDS, LLE, LE, MVU, diffusion maps, kPCA, Isomap, (t-)SNE, and UMAP. In our framework, a low-dimensional latent variable is used to construct a covariance, precision, or a graph Laplacian matrix, which can be used as part of a generative model for the data. Inference is done by optimizing an evidence lower bound. We demonstrate the internal consistency of our framework and show that it enables the use of probabilistic programming languages (PPLs) for DR. Additionally, we illustrate that the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07163</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;Bandit&#26041;&#27861;&#30340;&#26174;&#24335;&#22609;&#24418;&#22806;&#37096;&#24314;&#35758;&#31639;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning. (arXiv:2304.07163v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#22806;&#37096;&#25110;&#19987;&#23478;&#30340;&#24314;&#35758;&#34701;&#20837;&#21040;&#23398;&#20064;&#24403;&#20013;&#12290;&#26412;&#25991;&#23558;&#23558;&#23558;&#27492;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#31181;&#22810;&#33218;&#36172;&#21338;&#26426;&#31216;&#20026;&#22609;&#24418;&#36172;&#21338;&#26426;&#65288;shaping-bandits&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;LQR&#21644;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19977;&#31181;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason a
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SELFIES&#35821;&#35328;&#27169;&#22411;&#30340;SELFormer&#26550;&#26500;&#65292;&#21033;&#29992;&#35813;&#26550;&#26500;&#21487;&#26377;&#25928;&#23398;&#20064;&#28789;&#27963;&#12289;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#30456;&#27604;&#20854;&#20182;&#21516;&#31867;&#26041;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2304.04662</link><description>&lt;p&gt;
SELFormer&#65306;&#21033;&#29992;SELFIES&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SELFormer: Molecular Representation Learning via SELFIES Language Models. (arXiv:2304.04662v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SELFIES&#35821;&#35328;&#27169;&#22411;&#30340;SELFormer&#26550;&#26500;&#65292;&#21033;&#29992;&#35813;&#26550;&#26500;&#21487;&#26377;&#25928;&#23398;&#20064;&#28789;&#27963;&#12289;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#30456;&#27604;&#20854;&#20182;&#21516;&#31867;&#26041;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#24191;&#38420;&#30340;&#21270;&#23398;&#31354;&#38388;&#36827;&#34892;&#33258;&#21160;&#21270;&#30340;&#35745;&#31639;&#20998;&#26512;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#30340;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#20540;&#34920;&#36798;&#24335;&#12290;&#19968;&#31181;&#26377;&#25928;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#22788;&#29702;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#21270;&#23398;&#26631;&#27880;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#21033;&#29992;SMILES&#26631;&#27880;&#23454;&#29616;&#27492;&#30446;&#30340;; &#28982;&#32780;&#65292;SMILES&#26631;&#27880;&#23384;&#22312;&#35768;&#22810;&#19982;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#27490;&#27169;&#22411;&#26377;&#25928;&#22320;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#30693;&#35782;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SELFormer&#65292;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#21270;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;100&#65285;&#26377;&#25928;&#65292;&#32039;&#20945;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#31526;&#21495;SELFIES&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#23398;&#20064;&#28789;&#27963;&#19988;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290; SELFormer&#22312;&#22823;&#22411;&#21270;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#39033;&#20998;&#23376;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated computational analysis of the vast chemical space is critical for numerous fields of research such as drug discovery and material science. Representation learning techniques have recently been employed with the primary objective of generating compact and informative numerical expressions of complex data. One approach to efficiently learn molecular representations is processing string-based notations of chemicals via natural language processing (NLP) algorithms. Majority of the methods proposed so far utilize SMILES notations for this purpose; however, SMILES is associated with numerous problems related to validity and robustness, which may prevent the model from effectively uncovering the knowledge hidden in the data. In this study, we propose SELFormer, a transformer architecture-based chemical language model that utilizes a 100% valid, compact and expressive notation, SELFIES, as input, in order to learn flexible and high-quality molecular representations. SELFormer is pre-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22270;&#35770;&#30340;&#35821;&#35328;&#21051;&#30011;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20004;&#31181;&#24773;&#24418;&#19979;&#65292;&#32431;&#31929;&#21644;&#36817;&#20284;&#30340;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#30683;&#30462;&#22270;$G$&#26469;&#25429;&#25417; $\mathcal{H}$ &#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#21457;&#29616;&#20998;&#25968;&#22242;&#25968;&#21644;&#22242;&#25968;&#26159;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#23545;&#20854;&#36827;&#34892;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.03996</link><description>&lt;p&gt;
&#29992;&#22270;&#35770;&#32479;&#19968;&#21051;&#30011;&#24046;&#20998;&#38544;&#31169;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Unified Characterization of Private Learnability via Graph Theory. (arXiv:2304.03996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22270;&#35770;&#30340;&#35821;&#35328;&#21051;&#30011;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20004;&#31181;&#24773;&#24418;&#19979;&#65292;&#32431;&#31929;&#21644;&#36817;&#20284;&#30340;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#30683;&#30462;&#22270;$G$&#26469;&#25429;&#25417; $\mathcal{H}$ &#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#21457;&#29616;&#20998;&#25968;&#22242;&#25968;&#21644;&#22242;&#25968;&#26159;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#23545;&#20854;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#21051;&#30011;&#32431;&#31929;&#30340;&#21644;&#36817;&#20284;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22270;&#35770;&#30340;&#35821;&#35328;:&#23545;&#20110;&#19968;&#20010;&#27010;&#24565;&#31867; $\mathcal{H}$,&#25105;&#20204;&#23450;&#20041;&#20102; $\mathcal{H}$ &#30340;&#30683;&#30462;&#22270; $G$&#12290;&#23427;&#30340;&#39030;&#28857;&#26159;&#21487;&#23454;&#29616;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;&#26524;&#20004;&#20010;&#25968;&#25454;&#38598; $S$&#65292;$S'$ &#30456;&#20114;&#30683;&#30462;(&#21363;&#65292;&#22312; $S$ &#21644; $S'$ &#20013;&#26377;&#19968;&#20010;&#28857; $x$ &#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#35760;)&#65292;&#21017;&#23427;&#20204;&#20043;&#38388;&#26377;&#19968;&#26465;&#36793;&#36830;&#25509;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;$G$ &#30340;&#32452;&#21512;&#32467;&#26500;&#19982;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#32431;&#31929;&#30340;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#30340;&#25429;&#33719;&#20026; $G$ &#30340;&#20998;&#25968;&#22242;&#25968;&#12290;&#22312;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#30340;&#25429;&#33719;&#20026; $G$ &#30340;&#22242;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#21487;&#23398;&#20064;&#24615;&#30340;&#22270;&#35770;&#32500;&#24230;&#65306;&#22242;&#32500;&#21644;&#20998;&#25968;&#22242;&#32500;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#30683;&#30462;&#22270;&#30340;&#19968;&#20123;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#26469;&#20272;&#35745; $G$ &#30340;&#36825;&#20123;&#24230;&#37327;&#65292;&#36890;&#36807;&#36825;&#20123;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20960;&#31181;&#27010;&#24565;&#31867;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a unified framework for characterizing pure and approximate differentially private (DP) learnabiliity. The framework uses the language of graph theory: for a concept class $\mathcal{H}$, we define the contradiction graph $G$ of $\mathcal{H}$. It vertices are realizable datasets, and two datasets $S,S'$ are connected by an edge if they contradict each other (i.e., there is a point $x$ that is labeled differently in $S$ and $S'$). Our main finding is that the combinatorial structure of $G$ is deeply related to learning $\mathcal{H}$ under DP. Learning $\mathcal{H}$ under pure DP is captured by the fractional clique number of $G$. Learning $\mathcal{H}$ under approximate DP is captured by the clique number of $G$. Consequently, we identify graph-theoretic dimensions that characterize DP learnability: the clique dimension and fractional clique dimension. Along the way, we reveal properties of the contradiction graph which may be of independent interest. We also suggest several o
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#19979;&#22810;&#26631;&#31614;&#25490;&#21517;&#38382;&#39064;&#22312;&#25209;&#22788;&#29702;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#24182;&#39318;&#27425;&#32473;&#20986;&#22522;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#30340;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.03337</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#26631;&#31614;&#25490;&#21517;&#30340;&#21487;&#23398;&#20064;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Learnability of Multilabel Ranking. (arXiv:2304.03337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03337
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#19979;&#22810;&#26631;&#31614;&#25490;&#21517;&#38382;&#39064;&#22312;&#25209;&#22788;&#29702;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#24182;&#39318;&#27425;&#32473;&#20986;&#22522;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#30340;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#22810;&#26631;&#31614;&#25490;&#21517;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#32593;&#32476;&#25628;&#32034;&#12289;&#26032;&#38395;&#25253;&#36947;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;&#20851;&#20110;&#22810;&#26631;&#31614;&#25490;&#21517;&#35774;&#32622;&#20013;&#21487;&#23398;&#20064;&#24615;&#30340;&#26368;&#22522;&#26412;&#38382;&#39064;&#20173;&#26410;&#35299;&#31572;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#19979;&#22810;&#26631;&#31614;&#25490;&#21517;&#38382;&#39064;&#22312;&#25209;&#22788;&#29702;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#21516;&#26102;&#20063;&#39318;&#27425;&#32473;&#20986;&#20102;&#22522;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#30340;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilabel ranking is a central task in machine learning with widespread applications to web search, news stories, recommender systems, etc. However, the most fundamental question of learnability in a multilabel ranking setting remains unanswered. In this paper, we characterize the learnability of multilabel ranking problems in both the batch and online settings for a large family of ranking losses. Along the way, we also give the first equivalence class of ranking losses based on learnability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#31070;&#32463;&#24352;&#37327;&#32593;&#32476;&#65288;ANTN&#65289;&#26469;&#26725;&#25509;&#24352;&#37327;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#22810;&#20307;&#37327;&#23376;&#27169;&#25311;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#31934;&#24230;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.01996</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#31070;&#32463;&#24352;&#37327;&#32593;&#32476;: &#26725;&#25509;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#32593;&#32476;&#36827;&#34892;&#22810;&#20307;&#37327;&#23376;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Neural TensorNet: Bridging Neural Networks and Tensor Networks for Quantum Many-Body Simulation. (arXiv:2304.01996v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#31070;&#32463;&#24352;&#37327;&#32593;&#32476;&#65288;ANTN&#65289;&#26469;&#26725;&#25509;&#24352;&#37327;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#22810;&#20307;&#37327;&#23376;&#27169;&#25311;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#31934;&#24230;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20307;&#37327;&#23376;&#29289;&#29702;&#30340;&#27169;&#25311;&#23545;&#20110;&#29702;&#35299;&#22522;&#30784;&#31185;&#23398;&#21450;&#24212;&#29992;&#20110;&#37327;&#23376;&#26448;&#26009;&#35774;&#35745;&#21644;&#37327;&#23376;&#25216;&#26415;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#22823;&#23567;&#38543;&#31890;&#23376;&#25968;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#30452;&#25509;&#27169;&#25311;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#24352;&#37327;&#32593;&#32476;&#21644;&#31070;&#32463;&#32593;&#32476;&#26159;&#36817;&#20284;&#27169;&#25311;&#30340;&#20004;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20294;&#22312;&#34920;&#36798;&#33021;&#21147;&#21644;&#20248;&#21270;&#26041;&#38754;&#21508;&#33258;&#26377;&#20854;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26550;&#26500;&#8212;&#8212;&#33258;&#22238;&#24402;&#31070;&#32463;&#24352;&#37327;&#32593;&#32476;&#65288;ANTN&#65289;&#65292;&#23427;&#26725;&#25509;&#20102;&#24352;&#37327;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ANTN&#29992;&#20110;&#21442;&#25968;&#21270;&#20855;&#26377;&#31934;&#30830;&#37319;&#26679;&#30340;&#24402;&#19968;&#21270;&#27874;&#20989;&#25968;&#65292;&#25193;&#23637;&#20102;&#24352;&#37327;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#21147;&#65292;&#32487;&#25215;&#20102;&#35768;&#22810;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#22312;&#20108;&#32500; $J_1$-$J_2$ &#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;ANTN&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24352;&#37327;&#32593;&#32476;&#26041;&#27861;&#65292;&#24182;&#22312;&#29616;&#26377;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#20013;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum many-body physics simulation has important impacts on understanding fundamental science and has applications to quantum materials design and quantum technology. However, due to the exponentially growing size of the Hilbert space with respect to the particle number, a direct simulation is intractable. While representing quantum states with tensor networks and neural networks are the two state-of-the-art methods for approximate simulations, each has its own limitations in terms of expressivity and optimization. To address these challenges, we develop a novel architecture, Autoregressive Neural TensorNet (ANTN), which bridges tensor networks and autoregressive neural networks. We show that Autoregressive Neural TensorNet parameterizes normalized wavefunctions with exact sampling, generalizes the expressivity of tensor networks and autoregressive neural networks, and inherits a variety of symmetries from autoregressive neural networks. We demonstrate our approach on the 2D $J_1$-$J
&lt;/p&gt;</description></item><item><title>LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.00457</link><description>&lt;p&gt;
LLMMaps&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#23618;&#35780;&#20215;&#30340;&#21487;&#35270;&#21270;&#38544;&#21947;
&lt;/p&gt;
&lt;p&gt;
LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00457
&lt;/p&gt;
&lt;p&gt;
LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;&#27169;&#22411;&#22312;&#21709;&#24212;&#20013;&#26292;&#38706;&#20986;&#19981;&#27491;&#30830;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#24517;&#39035;&#37319;&#29992;&#21220;&#22859;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#34429;&#28982;LLM&#22312;&#29305;&#23450;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#22522;&#20110;&#38382;&#31572;(Q&amp;A)&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#36825;&#20123;&#35780;&#20272;&#36890;&#24120;&#20165;&#25253;&#21578;&#25972;&#20010;&#39046;&#22495;&#30340;&#21333;&#20010;&#20934;&#30830;&#24230;&#25968;&#23383;&#65292;&#36825;&#19968;&#31243;&#24207;&#22312;&#36879;&#26126;&#24230;&#21644;&#27169;&#22411;&#25913;&#36827;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20998;&#23618;&#35780;&#20272;&#21487;&#20197;&#25581;&#31034;&#21487;&#33021;&#26356;&#23481;&#26131;&#21457;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#35780;&#20272;LLMs&#30340;&#39118;&#38505;&#24182;&#25351;&#23548;&#23427;&#20204;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#20026;&#25903;&#25345;&#36825;&#26679;&#30340;&#20998;&#23618;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMMaps&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;Q&amp;A&#25968;&#25454;&#38598;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;LLMMaps&#25552;&#20379;&#20102;&#23545;LLMs&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#20998;&#24067;&#30340;&#35814;&#32454;&#27934;&#23519;&#65292;&#20801;&#35768;&#29992;&#25143;&#25918;&#22823;&#39046;&#22495;&#30340;&#29305;&#23450;&#37096;&#20998;&#24182;&#25506;&#32034;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMMaps&#26377;&#21161;&#20110;&#35782;&#21035;&#20986;&#26356;&#23481;&#26131;&#20986;&#29616;LLM&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20197;&#25913;&#21892;&#36825;&#20123;&#39046;&#22495;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing and demonstrated impressive capabilities in various tasks. Unfortunately, they are prone to hallucinations, where the model exposes incorrect or false information in its responses, which renders diligent evaluation approaches mandatory. While LLM performance in specific knowledge fields is often evaluated based on question and answer (Q&amp;A) datasets, such evaluations usually report only a single accuracy number for the entire field, a procedure which is problematic with respect to transparency and model improvement. A stratified evaluation could instead reveal subfields, where hallucinations are more likely to occur and thus help to better assess LLMs' risks and guide their further development. To support such stratified evaluations, we propose LLMMaps as a novel visualization technique that enables users to evaluate LLMs' performance with respect to Q&amp;A datasets. LLMMaps provide detailed insights into LLMs' kn
&lt;/p&gt;</description></item><item><title>&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17580</link><description>&lt;p&gt;
HuggingGPT: &#22312;HugingFace&#20013;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20249;&#20276;&#35299;&#20915;AI&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17580
&lt;/p&gt;
&lt;p&gt;
&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22797;&#26434;AI&#20219;&#21153;&#26159;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#31649;&#29702;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#20197;&#35299;&#20915;AI&#20219;&#21153;&#65292;&#35821;&#35328;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#26469;&#36171;&#33021;&#23427;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#26681;&#25454;HuggingFace&#20013;&#21487;&#29992;&#30340;&#27169;&#22411;&#21151;&#33021;&#25551;&#36848;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#22312;&#36873;&#23450;AI&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#65288;ANN-BP&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29028;&#26609;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;ANN-BP&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#26631;&#31614;&#26367;&#20195;&#26041;&#26696;&#65292;&#23558;&#26609;&#23376;&#31283;&#23450;&#24615;&#25193;&#23637;&#21040;&#22235;&#20010;&#31867;&#21035;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16524</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#31070;&#32463;&#32593;&#32476;BP&#26550;&#26500;&#30340;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#29028;&#26609;&#31283;&#23450;&#24615;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Ensemble Learning Model on Artificial Neural Network-Backpropagation (ANN-BP) Architecture for Coal Pillar Stability Classification. (arXiv:2303.16524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#65288;ANN-BP&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29028;&#26609;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;ANN-BP&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#26631;&#31614;&#26367;&#20195;&#26041;&#26696;&#65292;&#23558;&#26609;&#23376;&#31283;&#23450;&#24615;&#25193;&#23637;&#21040;&#22235;&#20010;&#31867;&#21035;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29028;&#26609;&#26159;&#30830;&#20445;&#22320;&#19979;&#30828;&#23721;&#30719;&#23665;&#23433;&#20840;&#30340;&#37325;&#35201;&#32467;&#26500;&#21333;&#20803;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#23545;&#22320;&#19979;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#36827;&#34892;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#19968;&#20010;&#24120;&#29992;&#30340;&#35780;&#20272;&#26609;&#23376;&#31283;&#23450;&#24615;&#30340;&#25351;&#26631;&#26159;&#23433;&#20840;&#31995;&#25968;&#65288;SF&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20351;&#29992;SF&#36827;&#34892;&#26609;&#23376;&#31283;&#23450;&#24615;&#35780;&#20272;&#26102;&#65292;&#24120;&#24120;&#20986;&#29616;&#28165;&#26224;&#30340;&#36793;&#30028;&#19981;&#21487;&#38752;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#65288;ANN-BP&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#22312;&#26609;&#23376;&#31283;&#23450;&#24615;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#26609;&#23376;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#26377;&#19977;&#31181;ANN-BP&#65292;&#20998;&#21035;&#30001;&#20854;&#28608;&#27963;&#20989;&#25968;&#21306;&#20998;&#65306;ANN-BP ReLU&#12289;ANN-BP ELU&#21644;ANN-BP GELU&#12290;&#26412;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#19982;SF&#30340;&#36866;&#24212;&#24615;&#26469;&#32771;&#34385;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#26609;&#23376;&#31283;&#23450;&#24615;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#22833;&#36133;&#12289;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#23436;&#22909;&#12289;&#19981;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#22833;&#36133;&#21644;&#19981;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#23436;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pillars are important structural units used to ensure mining safety in underground hard rock mines. Therefore, precise predictions regarding the stability of underground pillars are required. One common index that is often used to assess pillar stability is the Safety Factor (SF). Unfortunately, such crisp boundaries in pillar stability assessment using SF are unreliable. This paper presents a novel application of Artificial Neural Network-Backpropagation (ANN-BP) and Deep Ensemble Learning for pillar stability classification. There are three types of ANN-BP used for the classification of pillar stability distinguished by their activation functions: ANN-BP ReLU, ANN-BP ELU, and ANN-BP GELU. This research also presents a new labeling alternative for pillar stability by considering its suitability with the SF. Thus, pillar stability is expanded into four categories: failed with a suitable safety factor, intact with a suitable safety factor, failed without a suitable safety factor, and in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;</title><link>http://arxiv.org/abs/2303.16372</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#30340;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26102;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#30340;&#35821;&#20041;&#20445;&#35777;&#24378;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23548;&#20986;&#38750;&#28176;&#36827;&#37327;&#32423;&#19979;&#30028;&#26469;&#30740;&#31350;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21644;&#24230;&#37327;&#38544;&#31169;&#65288;mDP&#65289;&#30340;&#23398;&#20064;&#22120;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#23545;mDP&#30340;&#20998;&#26512;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#23545;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;DP-SGD&#21644;Projected Noisy SGD&#36827;&#34892;&#20102;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#25193;&#23637;&#38544;&#31169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16317</link><description>&lt;p&gt;
PCA-Net&#65306;&#25805;&#20316;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#19978;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;PCA-Net&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#23427;&#23558;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#36924;&#36817;&#28508;&#22312;&#30340;&#31639;&#23376;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#36817;&#20284;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#25913;&#36827;&#24182;&#26174;&#30528;&#25193;&#23637;&#20102;&#27492;&#26041;&#21521;&#30340;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312;&#23450;&#24615;&#30028;&#38480;&#26041;&#38754;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#26032;&#39062;&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#22312;&#23545;&#28508;&#22312;&#31639;&#23376;&#21644;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#26368;&#23567;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#12290;&#22312;&#23450;&#37327;&#38480;&#21046;&#26041;&#38754;&#65292;&#26412;&#25991;&#35782;&#21035;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#20004;&#20010;&#28508;&#22312;&#38556;&#30861;&#65292;&#36890;&#36807;&#23548;&#20986;&#19979;&#30028;&#36827;&#34892;&#20102;&#20005;&#26684;&#35777;&#26126;&#65292;&#31532;&#19968;&#20010;&#38556;&#30861;&#19982;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#26377;&#20851;&#65292;&#30001;PCA&#29305;&#24449;&#20540;&#30340;&#32531;&#24930;&#34928;&#20943;&#26469;&#34913;&#37327;&#65307;&#21478;&#19968;&#20010;&#38556;&#30861;&#28041;&#21450;&#26080;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#30340;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#22522;&#20110;&#26102;&#38388;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24320;&#25918;&#38598;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21160;&#24577;&#20256;&#25773;&#36866;&#24403;&#20449;&#24687;&#21644;&#36991;&#20813;&#30693;&#35782;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2303.15015</link><description>&lt;p&gt;
&#22522;&#20110;&#24320;&#25918;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Open Temporal Graph Neural Networks. (arXiv:2303.15015v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#22522;&#20110;&#26102;&#38388;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24320;&#25918;&#38598;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21160;&#24577;&#20256;&#25773;&#36866;&#24403;&#20449;&#24687;&#21644;&#36991;&#20813;&#30693;&#35782;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22522;&#20110;&#26102;&#38388;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#20551;&#35774;&#26159;&#33410;&#28857;&#31867;&#21035;&#38598;&#21512;&#26159;&#23553;&#38381;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#24448;&#24448;&#20250;&#38754;&#23545;&#21160;&#24577;&#22686;&#21152;&#31867;&#21035;&#30340;&#24320;&#25918;&#38598;&#38382;&#39064;&#12290;&#23545;&#29616;&#26377;&#30340;&#21160;&#24577;GNN&#26041;&#27861;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#65306;(i) &#22914;&#20309;&#22312;&#24320;&#25918;&#30340;&#26102;&#38388;&#22270;&#20013;&#21160;&#24577;&#20256;&#25773;&#36866;&#24403;&#30340;&#20449;&#24687;&#65292;&#20854;&#20013;&#26032;&#31867;&#33410;&#28857;&#24448;&#24448;&#19982;&#26087;&#31867;&#33410;&#28857;&#30456;&#36830;&#12290;&#36825;&#31181;&#24773;&#20917;&#23558;&#23548;&#33268;&#19968;&#20010;&#23574;&#38160;&#30340;&#30683;&#30462;&#12290;&#36825;&#26159;&#22240;&#20026;&#20856;&#22411;&#30340;GNN&#20542;&#21521;&#20110;&#20351;&#36830;&#25509;&#33410;&#28857;&#30340;&#23884;&#20837;&#21464;&#24471;&#30456;&#20284;&#65292;&#32780;&#25105;&#20204;&#24076;&#26395;&#36825;&#20004;&#20010;&#20132;&#20114;&#33410;&#28857;&#30340;&#23884;&#20837;&#26159;&#21487;&#21306;&#20998;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#23646;&#20110;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;(ii)&#22914;&#20309;&#22312;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#26102;&#36991;&#20813;&#22312;&#26102;&#38388;&#22270;&#20013;&#20986;&#29616;&#30340;&#26087;&#31867;&#21035;&#30340;&#28798;&#38590;&#24615;&#30693;&#35782;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24320;&#25918;&#26102;&#38388;&#22270;&#30340;&#36890;&#29992;&#21644;&#26377;&#21407;&#21017;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) for temporal graphs have recently attracted increasing attentions, where a common assumption is that the class set for nodes is closed. However, in real-world scenarios, it often faces the open set problem with the dynamically increased class set as the time passes by. This will bring two big challenges to the existing dynamic GNN methods: (i) How to dynamically propagate appropriate information in an open temporal graph, where new class nodes are often linked to old class nodes. This case will lead to a sharp contradiction. This is because typical GNNs are prone to make the embeddings of connected nodes become similar, while we expect the embeddings of these two interactive nodes to be distinguishable since they belong to different classes. (ii) How to avoid catastrophic knowledge forgetting over old classes when learning new classes occurred in temporal graphs. In this paper, we propose a general and principled learning approach for open temporal graphs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ISimDL&#65292;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#26377;&#25928;&#35780;&#20272;&#20102;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.08035</link><description>&lt;p&gt;
ISimDL: &#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#39537;&#21160;&#30340;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#24378;&#20581;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning. (arXiv:2303.08035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ISimDL&#65292;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#26377;&#25928;&#35780;&#20272;&#20102;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;(DL)&#31995;&#32479;&#24050;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#38656;&#35201;&#19987;&#29992;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#21644;&#33455;&#29255;&#12290;&#22312;&#32435;&#31859;&#26102;&#20195;&#65292;&#35774;&#22791;&#36234;&#26469;&#36234;&#23481;&#26131;&#21463;&#21040;&#27704;&#20037;&#24615;&#21644;&#30636;&#21464;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#27492;&#31867;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#20102;&#35299;&#31070;&#32463;&#21152;&#36895;&#22120;&#33455;&#29255;&#20013;&#30340;&#25925;&#38556;&#22914;&#20309;&#22312;DL&#24212;&#29992;&#32423;&#21035;&#19978;&#34920;&#29616;&#20026;&#38169;&#35823;&#65292;&#20854;&#20013;&#25925;&#38556;&#21487;&#33021;&#23548;&#33268;&#26080;&#27861;&#26816;&#27979;&#21644;&#24674;&#22797;&#30340;&#38169;&#35823;&#12290;&#20351;&#29992;&#25925;&#38556;&#27880;&#20837;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22312;&#36719;&#20214;&#32423;&#21035;&#20462;&#25913;&#31070;&#32463;&#20803;&#26435;&#37325;&#21644;&#36755;&#20986;&#26469;&#25191;&#34892;DL&#31995;&#32479;&#30340;&#38887;&#24615;&#30740;&#31350;&#65292;&#23601;&#22909;&#20687;&#30828;&#20214;&#21463;&#21040;&#30636;&#21464;&#25925;&#38556;&#30340;&#24433;&#21709;&#19968;&#26679;&#12290;&#29616;&#26377;&#30340;&#25925;&#38556;&#27169;&#22411;&#20943;&#23569;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#20998;&#26512;&#26356;&#24555;&#65292;&#20294;&#38656;&#35201;&#35813;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#19988;&#19981;&#20801;&#35768;&#36827;&#19968;&#27493;&#20998;&#26512;&#31579;&#36873;&#20986;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ISimDL&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#24182;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#12290;ISimDL&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#30528;&#20943;&#23569;&#20102;&#25925;&#38556;&#27880;&#20837;&#20998;&#26512;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#65292;&#21516;&#26102;&#20173;&#30830;&#20445;&#36275;&#22815;&#35206;&#30422;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;ISimDL&#24212;&#29992;&#20110;&#20195;&#34920;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#23427;&#25552;&#20379;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#25925;&#38556;&#27880;&#20837;&#26041;&#27861;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) systems have proliferated in many applications, requiring specialized hardware accelerators and chips. In the nano-era, devices have become increasingly more susceptible to permanent and transient faults. Therefore, we need an efficient methodology for analyzing the resilience of advanced DL systems against such faults, and understand how the faults in neural accelerator chips manifest as errors at the DL application level, where faults can lead to undetectable and unrecoverable errors. Using fault injection, we can perform resilience investigations of the DL system by modifying neuron weights and outputs at the software-level, as if the hardware had been affected by a transient fault. Existing fault models reduce the search space, allowing faster analysis, but requiring a-priori knowledge on the model, and not allowing further analysis of the filtered-out search space. Therefore, we propose ISimDL, a novel methodology that employs neuron sensitivity to generate impo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#29992;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#26367;&#20195;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#24182;&#36866;&#29992;&#20110;&#24066;&#22330;&#35774;&#35745;&#21644;&#26356;&#29616;&#23454;&#22320;&#24314;&#27169;&#24066;&#22330;&#21442;&#19982;&#32773;&#12290;</title><link>http://arxiv.org/abs/2303.01772</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning. (arXiv:2303.01772v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#29992;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#26367;&#20195;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#24182;&#36866;&#29992;&#20110;&#24066;&#22330;&#35774;&#35745;&#21644;&#26356;&#29616;&#23454;&#22320;&#24314;&#27169;&#24066;&#22330;&#21442;&#19982;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#24066;&#22330;&#21487;&#33021;&#20250;&#20026;&#24066;&#22330;&#21442;&#19982;&#32773;&#30340;&#19981;&#33391;&#34892;&#20026;&#25552;&#20379;&#28608;&#21169;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#39044;&#27979;&#33021;&#28304;&#24066;&#22330;&#21442;&#19982;&#32773;&#39044;&#26399;&#34892;&#20026;&#30340;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#35768;&#22810;&#19982;&#31995;&#32479;&#30340;&#20132;&#20114;&#25165;&#33021;&#25910;&#25947;&#65292;&#32780;&#30005;&#21147;&#31995;&#32479;&#29615;&#22659;&#36890;&#24120;&#21253;&#25324;&#24191;&#27867;&#30340;&#35745;&#31639;&#65292;&#20363;&#22914;&#29992;&#20110;&#24066;&#22330;&#28165;&#31639;&#30340;&#26368;&#20248;&#21151;&#29575;&#27969;&#37327;&#65288;OPF&#65289;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#28304;&#24066;&#22330;&#30340;&#27169;&#22411;&#32473;&#22522;&#26412;&#30340;MARL&#31639;&#27861;&#65292;&#36825;&#20010;&#27169;&#22411;&#37319;&#29992;&#20102;&#23398;&#20064;&#30340;OPF&#36817;&#20284;&#20540;&#21644;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#12290;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20351;&#24471;OPF&#30340;&#26126;&#30830;&#35299;&#20915;&#21464;&#24471;&#19981;&#24517;&#35201;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#36824;&#23558;&#35757;&#32451;&#26102;&#38388;&#38477;&#20302;&#20102;&#32422;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#20294;&#20195;&#20215;&#26159;&#30053;&#24494;&#26356;&#24046;&#30340;&#32435;&#20160;&#22343;&#34913;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#22312;&#24212;&#29992;&#26159;&#24066;&#22330;&#35774;&#35745;&#65292;&#26356;&#29616;&#23454;&#22320;&#23545;&#24066;&#22330;&#21442;&#19982;&#32773;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#23545;&#24066;&#22330;&#21160;&#24577;&#30340;&#25913;&#36827;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy markets can provide incentives for undesired behavior of market participants. Multi-agent Reinforcement learning (MARL) is a promising new approach to predicting the expected behavior of energy market participants. However, reinforcement learning requires many interactions with the system to converge, and the power system environment often consists of extensive computations, e.g., optimal power flow (OPF) calculation for market clearing. To tackle this complexity, we provide a model of the energy market to a basic MARL algorithm in the form of a learned OPF approximation and explicit market rules. The learned OPF surrogate model makes an explicit solving of the OPF completely unnecessary. Our experiments demonstrate that the model additionally reduces training time by about one order of magnitude but at the cost of a slightly worse approximation of the Nash equilibrium. Potential applications of our method are market design, more realistic modeling of market participants, and an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#36880;&#27493;&#23545;&#25239;&#24615;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;SCRM&#65289;&#8221;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#37096;&#32626;&#23398;&#20064;&#31574;&#30053;&#22810;&#27425;&#24182;&#33719;&#21462;&#26032;&#25968;&#25454;&#65292;&#21033;&#29992;&#26032;&#30340;&#23545;&#25239;&#24615;&#20272;&#35745;&#22120;&#21644;&#37325;&#21551;&#31574;&#30053;&#65292;&#21487;&#20197;&#25913;&#21892; CRM &#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.12120</link><description>&lt;p&gt;
&#36880;&#27493;&#23545;&#25239;&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sequential Counterfactual Risk Minimization. (arXiv:2302.12120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#36880;&#27493;&#23545;&#25239;&#24615;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;SCRM&#65289;&#8221;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#37096;&#32626;&#23398;&#20064;&#31574;&#30053;&#22810;&#27425;&#24182;&#33719;&#21462;&#26032;&#25968;&#25454;&#65292;&#21033;&#29992;&#26032;&#30340;&#23545;&#25239;&#24615;&#20272;&#35745;&#22120;&#21644;&#37325;&#21551;&#31574;&#30053;&#65292;&#21487;&#20197;&#25913;&#21892; CRM &#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#39118;&#38505;&#26368;&#23567;&#21270;&#26159;&#22788;&#29702;&#35760;&#24405;&#36793;&#24102;&#21453;&#39304;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#25913;&#36827;&#35760;&#24405;&#31574;&#30053;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33021;&#22815;&#22810;&#27425;&#37096;&#32626;&#23398;&#20064;&#31574;&#30053;&#21644;&#33719;&#21462;&#26032;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#25193;&#23637; CRM &#21407;&#21017;&#21450;&#20854;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#8220;&#36880;&#27493;&#23545;&#25239;&#24615;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;SCRM&#65289;&#8221;&#36825;&#19968;&#22330;&#26223;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#20272;&#35745;&#22120;&#65292;&#24182;&#36890;&#36807;&#31867;&#20284;&#20110;&#21152;&#36895;&#20248;&#21270;&#26041;&#27861;&#20013;&#30340;&#37325;&#21551;&#31574;&#30053;&#30340;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#21487;&#20197;&#25913;&#21892; CRM &#22312;&#36229;&#37327;&#39118;&#38505;&#21644;&#21518;&#24724;&#29575;&#26041;&#38754;&#34920;&#29616;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#36824;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#34892;&#21160;&#35774;&#32622;&#19979;&#25552;&#20379;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102; CRM &#30340;&#22810;&#27425;&#37096;&#32626;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Risk Minimization (CRM) is a framework for dealing with the logged bandit feedback problem, where the goal is to improve a logging policy using offline data. In this paper, we explore the case where it is possible to deploy learned policies multiple times and acquire new data. We extend the CRM principle and its theory to this scenario, which we call "Sequential Counterfactual Risk Minimization (SCRM)." We introduce a novel counterfactual estimator and identify conditions that can improve the performance of CRM in terms of excess risk and regret rates, by using an analysis similar to restart strategies in accelerated optimization methods. We also provide an empirical evaluation of our method in both discrete and continuous action settings, and demonstrate the benefits of multiple deployments of CRM.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11939</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451; LM &#36827;&#34892;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#21644;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#19982; NLP &#21644; CV &#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#39046;&#22495;&#37319;&#29992;&#32479;&#19968;&#27169;&#22411;&#21363;&#21487;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#20173;&#28982;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22914;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#38459;&#30861;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#22823;&#37327;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36991;&#20813;&#25913;&#21464;&#39044;&#35757;&#32451;&#35821;&#35328;&#25110;&#22270;&#20687;&#27169;&#22411;&#20013;&#27531;&#24046;&#22359;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#21521;&#20256;&#36882;&#23618;&#12290;&#36825;&#31181;&#27169;&#22411;&#34987;&#31216;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (FPT)&#65292;&#36890;&#36807;&#23545;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FPT &#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#38024;&#23545;&#32479;&#35745;&#32858;&#31867;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#22797;&#21046;&#32858;&#31867;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21253;&#25324;&#21033;&#29992;&#36817;&#20284;&#31639;&#27861;&#32452;&#21512;&#38382;&#39064;&#30340;&#40657;&#30418;&#26041;&#24335;&#35299;&#20915;&#32479;&#35745;$k$-medians&#12289;&#32479;&#35745;$k$-means&#21644;&#32479;&#35745;$k$-centers&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#34920;&#31034;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#20989;&#25968;&#21644;&#35823;&#24046;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.10359</link><description>&lt;p&gt;
&#21487;&#22797;&#21046;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Replicable Clustering. (arXiv:2302.10359v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#38024;&#23545;&#32479;&#35745;&#32858;&#31867;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#22797;&#21046;&#32858;&#31867;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21253;&#25324;&#21033;&#29992;&#36817;&#20284;&#31639;&#27861;&#32452;&#21512;&#38382;&#39064;&#30340;&#40657;&#30418;&#26041;&#24335;&#35299;&#20915;&#32479;&#35745;$k$-medians&#12289;&#32479;&#35745;$k$-means&#21644;&#32479;&#35745;$k$-centers&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#34920;&#31034;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#20989;&#25968;&#21644;&#35823;&#24046;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26368;&#36817;&#30001;Impagliazzo&#31561;&#20154;[2022]&#24341;&#20837;&#30340;&#21487;&#22797;&#21046;&#24615;&#27010;&#24565;&#19979;&#35774;&#35745;&#20102;&#22312;&#32479;&#35745;&#32858;&#31867;&#20013;&#21487;&#22797;&#21046;&#30340;&#31639;&#27861;&#12290;&#26681;&#25454;&#36825;&#20010;&#23450;&#20041;&#65292;&#22914;&#26524;&#19968;&#20010;&#32858;&#31867;&#31639;&#27861;&#26159;&#21487;&#22797;&#21046;&#30340;&#65292;&#37027;&#20040;&#22312;&#21516;&#19968;&#20998;&#24067;&#30340;&#20004;&#20010;&#19981;&#21516;&#36755;&#20837;&#19978;&#25191;&#34892;&#26102;&#65292;&#21482;&#35201;&#20854;&#20869;&#37096;&#38543;&#26426;&#24615;&#22312;&#25191;&#34892;&#20013;&#24471;&#21040;&#20849;&#20139;&#65292;&#23601;&#33021;&#39640;&#27010;&#29575;&#22320;&#20135;&#29983;&#23436;&#20840;&#30456;&#21516;&#30340;&#26679;&#26412;&#31354;&#38388;&#20998;&#21306;&#12290;&#25105;&#20204;&#36890;&#36807;&#40657;&#30418;&#30340;&#26041;&#24335;&#21033;&#29992;&#32452;&#21512;&#23545;&#24212;&#38382;&#39064;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#20026;&#32479;&#35745;$k$-medians&#12289;&#32479;&#35745;$k$-means&#21644;&#32479;&#35745;$k$-centers&#38382;&#39064;&#25552;&#20986;&#20102;&#36825;&#26679;&#30340;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21487;&#22797;&#21046;&#30340;$O(1)$-&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#36866;&#29992;&#20110;&#32479;&#35745;&#27431;&#20960;&#37324;&#24471;$k$-medians ($k$-means)&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\operatorname{poly}(d)$&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;$O(1)$-&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#22312;&#32479;&#35745;&#27431;&#20960;&#37324;&#24471;$k$-centers$&#26102;&#20855;&#26377;&#39069;&#22806;&#30340;$O(1)$-&#21152;&#24615;&#35823;&#24046;&#65292;&#23613;&#31649;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\exp(d)$&#12290;
&lt;/p&gt;
&lt;p&gt;
We design replicable algorithms in the context of statistical clustering under the recently introduced notion of replicability from Impagliazzo et al. [2022]. According to this definition, a clustering algorithm is replicable if, with high probability, its output induces the exact same partition of the sample space after two executions on different inputs drawn from the same distribution, when its internal randomness is shared across the executions. We propose such algorithms for the statistical $k$-medians, statistical $k$-means, and statistical $k$-centers problems by utilizing approximation routines for their combinatorial counterparts in a black-box manner. In particular, we demonstrate a replicable $O(1)$-approximation algorithm for statistical Euclidean $k$-medians ($k$-means) with $\operatorname{poly}(d)$ sample complexity. We also describe an $O(1)$-approximation algorithm with an additional $O(1)$-additive error for statistical Euclidean $k$-centers, albeit with $\exp(d)$ samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23616;&#37096;&#36845;&#20195;&#32454;&#21270;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#21253;&#25324;&#22270;&#20687;&#34701;&#21512;&#12289;&#29289;&#20307;&#27880;&#20837;&#12289;&#32441;&#29702;&#26367;&#25442;&#31561;&#22810;&#31181;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#27880;&#25110;&#35757;&#32451;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2302.10167</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Compositing with Pretrained Diffusion Models. (arXiv:2302.10167v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23616;&#37096;&#36845;&#20195;&#32454;&#21270;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#21253;&#25324;&#22270;&#20687;&#34701;&#21512;&#12289;&#29289;&#20307;&#27880;&#20837;&#12289;&#32441;&#29702;&#26367;&#25442;&#31561;&#22810;&#31181;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#27880;&#25110;&#35757;&#32451;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#26465;&#20214;&#22270;&#20687;&#32534;&#36753;&#21151;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#25193;&#23637;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#29992;&#20110;&#21508;&#31181;&#36328;&#39046;&#22495;&#21512;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#34701;&#21512;&#65292;&#29289;&#20307;&#27880;&#20837;&#65292;&#32441;&#29702;&#26367;&#25442;&#65292;&#29978;&#33267;&#21253;&#25324;CG2Real&#30340;&#32763;&#35793;&#25110;&#39118;&#26684;&#21270;&#12290;&#25105;&#20204;&#37319;&#29992;&#23616;&#37096;&#36845;&#20195;&#32454;&#21270;&#26041;&#26696;&#65292;&#23558;&#27880;&#20837;&#30340;&#23545;&#35937;&#19982;&#32972;&#26223;&#22330;&#26223;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#24182;&#33021;&#22815;&#25511;&#21046;&#23545;&#35937;&#21487;&#33021;&#32463;&#21382;&#30340;&#31243;&#24230;&#21644;&#31867;&#22411;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23450;&#24615;&#21644;&#23450;&#37327;&#27604;&#36739;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#32467;&#26524;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#36924;&#30495;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#27880;&#25110;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have enabled high-quality, conditional image editing capabilities. We propose to expand their arsenal, and demonstrate that off-the-shelf diffusion models can be used for a wide range of cross-domain compositing tasks. Among numerous others, these include image blending, object immersion, texture-replacement and even CG2Real translation or stylization. We employ a localized, iterative refinement scheme which infuses the injected objects with contextual information derived from the background scene, and enables control over the degree and types of changes the object may undergo. We conduct a range of qualitative and quantitative comparisons to prior work, and exhibit that our method produces higher quality and realistic results without requiring any annotations or training. Finally, we demonstrate how our method may be used for data augmentation of downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#35745;&#31639;&#21644;&#21521;&#37327;&#25215;&#35834;&#30340;&#25308;&#21344;&#24237;&#25269;&#25239;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;RAM&#31192;&#23494;&#20849;&#20139;&#23558;&#26412;&#22320;&#26356;&#26032;&#20998;&#21106;&#25104;&#36739;&#23567;&#23376;&#21521;&#37327;&#65292;&#24182;&#20351;&#29992;&#21452;&#37325;RAMP&#20849;&#20139;&#25216;&#26415;&#23454;&#29616;&#25104;&#23545;&#36317;&#31163;&#30340;&#23433;&#20840;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2302.09913</link><description>&lt;p&gt;
&#22522;&#20110;&#32534;&#30721;&#35745;&#31639;&#21644;&#21521;&#37327;&#25215;&#35834;&#30340;&#25308;&#21344;&#24237;&#25269;&#25239;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064; (arXiv:2302.09913v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
ByzSecAgg: A Byzantine-Resistant Secure Aggregation Scheme for Federated Learning Based on Coded Computing and Vector Commitment. (arXiv:2302.09913v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#35745;&#31639;&#21644;&#21521;&#37327;&#25215;&#35834;&#30340;&#25308;&#21344;&#24237;&#25269;&#25239;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;RAM&#31192;&#23494;&#20849;&#20139;&#23558;&#26412;&#22320;&#26356;&#26032;&#20998;&#21106;&#25104;&#36739;&#23567;&#23376;&#21521;&#37327;&#65292;&#24182;&#20351;&#29992;&#21452;&#37325;RAMP&#20849;&#20139;&#25216;&#26415;&#23454;&#29616;&#25104;&#23545;&#36317;&#31163;&#30340;&#23433;&#20840;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26041;&#26696;&#65292;&#21487;&#20197;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#38544;&#31169;&#27844;&#38706;&#12290;&#36825;&#31181;&#26041;&#26696;&#36890;&#36807;&#22788;&#29702;&#21333;&#20010;&#26356;&#26032;&#26469;&#31649;&#29702;&#23545;&#25239;&#34892;&#20026;&#65292;&#24182;&#22312;&#25269;&#24481;&#20018;&#36890;&#33410;&#28857;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#23545;&#26356;&#26032;&#21521;&#37327;&#36827;&#34892;&#23433;&#20840;&#31192;&#23494;&#20849;&#20139;&#30340;&#36890;&#20449;&#36127;&#36733;&#21487;&#33021;&#38750;&#24120;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26412;&#22320;&#26356;&#26032;&#20998;&#21106;&#25104;&#36739;&#23567;&#23376;&#21521;&#37327;&#24182;&#20351;&#29992;RAM&#31192;&#23494;&#20849;&#20139;&#30340;&#26041;&#26696;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#20849;&#20139;&#26041;&#27861;&#26080;&#27861;&#36827;&#34892;&#21452;&#32447;&#24615;&#35745;&#31639;&#65292;&#20363;&#22914;&#38656;&#35201;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#25104;&#23545;&#36317;&#31163;&#35745;&#31639;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#20250;&#36816;&#34892;&#21478;&#19968;&#36718;RAMP&#20849;&#20139;&#65292;&#35813;&#20849;&#20139;&#20855;&#26377;&#19981;&#21516;&#30340;&#25968;&#25454;&#23884;&#20837;&#20854;&#20013;&#12290;&#36825;&#31181;&#21463;&#32534;&#30721;&#35745;&#31639;&#24605;&#24819;&#21551;&#21457;&#30340;&#25216;&#26415;&#23454;&#29616;&#20102;&#25104;&#23545;&#36317;&#31163;&#30340;&#23433;&#20840;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an efficient secure aggregation scheme for federated learning that is protected against Byzantine attacks and privacy leakages. Processing individual updates to manage adversarial behavior, while preserving privacy of data against colluding nodes, requires some sort of secure secret sharing. However, communication load for secret sharing of long vectors of updates can be very high. To resolve this issue, in the proposed scheme, local updates are partitioned into smaller sub-vectors and shared using ramp secret sharing. However, this sharing method does not admit bi-linear computations, such as pairwise distance calculations, needed by outlier-detection algorithms. To overcome this issue, each user runs another round of ramp sharing, with different embedding of data in the sharing polynomial. This technique, motivated by ideas from coded computing, enables secure computation of pairwise distance. In addition, to maintain the integrity and privacy of the local u
&lt;/p&gt;</description></item><item><title>TAMUNA&#26159;&#39318;&#20010;&#32852;&#21512;&#21033;&#29992;&#32593;&#32476;&#21387;&#32553;&#21644;&#23569;&#37327;&#36890;&#20449;&#37197;&#21512;&#21152;&#36895;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#20801;&#35768;&#37096;&#20998;&#21442;&#19982;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09832</link><description>&lt;p&gt;
TAMUNA: &#24102;&#26377;&#23616;&#37096;&#35757;&#32451;&#12289;&#21387;&#32553;&#21644;&#37096;&#20998;&#21442;&#19982;&#30340;&#21452;&#20493;&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TAMUNA: Doubly Accelerated Federated Learning with Local Training, Compression, and Partial Participation. (arXiv:2302.09832v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09832
&lt;/p&gt;
&lt;p&gt;
TAMUNA&#26159;&#39318;&#20010;&#32852;&#21512;&#21033;&#29992;&#32593;&#32476;&#21387;&#32553;&#21644;&#23569;&#37327;&#36890;&#20449;&#37197;&#21512;&#21152;&#36895;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#20801;&#35768;&#37096;&#20998;&#21442;&#19982;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#22823;&#37327;&#29992;&#25143;&#21512;&#20316;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#20182;&#20204;&#20132;&#26367;&#36827;&#34892;&#26412;&#22320;&#35745;&#31639;&#21644;&#19982;&#36828;&#31243;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#12290;&#36890;&#20449;&#26159;&#35813;&#35774;&#32622;&#20013;&#30340;&#20027;&#35201;&#29942;&#39048;&#65292;&#23427;&#21487;&#20197;&#24930;&#19988;&#26114;&#36149;&#12290;&#20026;&#20102;&#20943;&#23569;&#36890;&#20449;&#36127;&#36733;&#24182;&#21152;&#36895;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65292;&#20351;&#29992;&#20004;&#31181;&#31574;&#30053;&#24456;&#21463;&#27426;&#36814;&#65306;1&#65289;&#26356;&#23569;&#22320;&#36890;&#20449;&#65292;&#21363;&#22312;&#36890;&#20449;&#36718;&#20043;&#38388;&#25191;&#34892;&#20960;&#20010;&#26412;&#22320;&#35745;&#31639;&#30340;&#36845;&#20195;&#65307;2&#65289;&#20256;&#36755;&#21387;&#32553;&#20449;&#24687;&#32780;&#19981;&#26159;&#23436;&#25972;&#32500;&#24230;&#30340;&#30690;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TAMUNA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#32852;&#21512;&#21033;&#29992;&#36825;&#20004;&#31181;&#31574;&#30053;&#65292;&#21516;&#26102;&#20801;&#35768;&#37096;&#20998;&#21442;&#19982;&#12290;TAMUNA&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, a large number of users collaborate to learn a global model. They alternate local computations and communication with a distant server. Communication, which can be slow and costly, is the main bottleneck in this setting. In addition to communication-efficiency, a robust algorithm should allow for partial participation, the desirable feature that not all clients need to participate to every round of the training process. To reduce the communication load and therefore accelerate distributed gradient descent, two strategies are popular: 1) communicate less frequently; that is, perform several iterations of local computations between the communication rounds; and 2) communicate compressed information instead of full-dimensional vectors. We propose TAMUNA, the first algorithm for distributed optimization and federated learning, which harnesses these two strategies jointly and allows for partial participation. TAMUNA converges linearly to an exact solution in the stron
&lt;/p&gt;</description></item><item><title>InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08624</link><description>&lt;p&gt;
InstructABSA: &#22522;&#20110;&#25351;&#20196;&#23398;&#20064;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08624
&lt;/p&gt;
&lt;p&gt;
InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;InstructABSA&#65292;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;Aspect Based Sentiment Analysis (ABSA) &#25152;&#26377;&#23376;&#20219;&#21153;&#65288;Aspect Term Extraction (ATE)&#65292;Aspect Term Sentiment Classification (ATSC)&#65292;&#20197;&#21450;Joint Task modeling&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#24341;&#20837;&#20102;&#27491;&#38754;&#12289;&#36127;&#38754;&#12289;&#21644;&#20013;&#24615;&#30340;&#20363;&#23376;&#65292;&#24182;&#20351;&#29992;&#25351;&#20196;&#26469;&#35843;&#25972;&#27599;&#20010;ABSA&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#65288;Tk-Instruct&#65289;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#22312;Sem Eval 2014&#12289;2015&#21644;2016&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;ABSA&#23376;&#20219;&#21153;&#65288;ATE&#12289;ATSC&#21644;Joint Task&#65289;&#19978;&#65292;InstructABSA&#22312;&#24615;&#33021;&#19978;&#37117;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#34920;&#29616;&#36229;&#36807;&#20102;7&#20493;&#22823;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;Rest14 ATE&#23376;&#20219;&#21153;&#19978;&#65292;InstructABSA&#36229;&#36807;&#20102;SOTA 7.31%&#30340;&#24471;&#20998;&#65292;Rest15 ATSC&#23376;&#20219;&#21153;&#19978;&#20063;&#26377;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;Lapt14 Joint Task&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;8.63%&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;InstructABSA&#20855;&#26377;&#24378;&#22823;&#30340;&#26032;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) for each ABSA subtask, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by 8.63% points. Our results also suggest a strong generalization ability to new domains across all three subtasks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#38750;&#24179;&#31283;&#32447;&#24615;&#36172;&#33218;&#27169;&#22411;&#65292;&#32467;&#21512;&#25805;&#20316;&#21382;&#21490;&#20449;&#24687;&#19988;&#20855;&#26377;&#20004;&#20010;&#21442;&#25968;&#12290;&#23427;&#20197;&#24674;&#22797;&#24179;&#31283;&#30340;&#32447;&#24615;&#36172;&#33218;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#19988;&#22312;&#24050;&#30693;&#31383;&#21475;&#22823;&#23567;&#21644;&#25351;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#24490;&#29615;&#31574;&#30053;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;OFUL&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2302.08345</link><description>&lt;p&gt;
&#24102;&#20869;&#23384;&#30340;&#32447;&#24615;&#36172;&#33218;&#65306;&#20174;&#34928;&#36864;&#21040;&#23835;&#36215;
&lt;/p&gt;
&lt;p&gt;
Linear Bandits with Memory: from Rotting to Rising. (arXiv:2302.08345v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#38750;&#24179;&#31283;&#32447;&#24615;&#36172;&#33218;&#27169;&#22411;&#65292;&#32467;&#21512;&#25805;&#20316;&#21382;&#21490;&#20449;&#24687;&#19988;&#20855;&#26377;&#20004;&#20010;&#21442;&#25968;&#12290;&#23427;&#20197;&#24674;&#22797;&#24179;&#31283;&#30340;&#32447;&#24615;&#36172;&#33218;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#19988;&#22312;&#24050;&#30693;&#31383;&#21475;&#22823;&#23567;&#21644;&#25351;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#24490;&#29615;&#31574;&#30053;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;OFUL&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#24179;&#31283;&#29616;&#35937;&#65288;&#22914;&#25512;&#33616;&#20013;&#30340;&#39281;&#21644;&#25928;&#24212;&#65289;&#22823;&#22810;&#20351;&#29992;&#26377;&#38480;&#33218;&#36172;&#33218;&#26469;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#26356;&#21916;&#27426;&#20351;&#29992;&#20855;&#26377;&#26356;&#20016;&#23500;&#34892;&#21160;&#31354;&#38388;&#30340;&#32447;&#24615;&#36172;&#33218;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#24179;&#31283;&#32447;&#24615;&#36172;&#33218;&#27169;&#22411;&#65292;&#20854;&#20013;&#24403;&#21069;&#22870;&#21169;&#21463;&#23398;&#20064;&#32773;&#20197;&#22266;&#23450;&#22823;&#23567;&#30340;&#31383;&#21475;&#20869;&#30340;&#20808;&#21069;&#25805;&#20316;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#24674;&#22797;&#24179;&#31283;&#30340;&#32447;&#24615;&#36172;&#33218;&#65292;&#21033;&#29992;&#20004;&#20010;&#21442;&#25968;&#65306;&#31383;&#21475;&#22823;&#23567;$ m \geq 0 $&#21644;&#25351;&#25968;$ \gamma $&#65292;&#23427;&#25429;&#25417;&#29616;&#35937;&#30340;&#34928;&#36864;&#65288;$ \gamma &lt;0 $&#65289;&#25110;&#23835;&#36215;&#65288;$ \gamma&gt; 0 $&#65289;&#30340;&#24615;&#36136;&#12290;&#24403;&#21516;&#26102;&#30693;&#36947;$ m $&#21644;$ \gamma $&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#38024;&#23545;&#24490;&#29615;&#31574;&#30053;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;OFUL&#21464;&#20307;&#12290;&#36890;&#36807;&#36873;&#25321;&#24490;&#29615;&#38271;&#24230;&#20197;&#22312;&#36924;&#36817;&#21644;&#20272;&#35745;&#35823;&#24046;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#26368;&#20339;&#24207;&#21015;&#30340;&#36951;&#25022;&#22823;&#23567;&#20026;$\sqrt{d}\,(m+1)^{\frac{1}{2}+\max\{\gamma,0\}}\,T^{3/4}$ &#65288;&#24573;&#30053;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonstationary phenomena, such as satiation effects in recommendations, have mostly been modeled using bandits with finitely many arms. However, the richer action space provided by linear bandits is often preferred in practice. In this work, we introduce a novel nonstationary linear bandit model, where current rewards are influenced by the learner's past actions in a fixed-size window. Our model, which recovers stationary linear bandits as a special case, leverages two parameters: the window size $m \ge 0$, and an exponent $\gamma$ that captures the rotting ($\gamma &lt; 0)$ or rising ($\gamma &gt; 0$) nature of the phenomenon. When both $m$ and $\gamma$ are known, we propose and analyze a variant of OFUL which minimizes regret against cycling policies. By choosing the cycle length so as to trade-off approximation and estimation errors, we then prove a bound of order $\sqrt{d}\,(m+1)^{\frac{1}{2}+\max\{\gamma,0\}}\,T^{3/4}$ (ignoring log factors) on the regret against the optimal sequence of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38656;&#27714;&#20391;&#31649;&#29702;&#65288;DSM&#65289;&#26041;&#27861;&#65292;&#21363;&#25511;&#21046;&#22823;&#37327;&#30005;&#27668;&#35774;&#22791;&#36981;&#24490;&#25152;&#38656;&#28040;&#36153;&#20449;&#21495;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;MD-MFC&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30452;&#25509;&#35299;&#20915;&#30446;&#26631;&#36319;&#36394;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.08190</link><description>&lt;p&gt;
&#21033;&#29992;&#22343;&#20540;&#22330;&#23398;&#20064;&#37325;&#26032;&#26500;&#24819;&#38656;&#27714;&#20391;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reimagining Demand-Side Management with Mean Field Learning. (arXiv:2302.08190v2 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38656;&#27714;&#20391;&#31649;&#29702;&#65288;DSM&#65289;&#26041;&#27861;&#65292;&#21363;&#25511;&#21046;&#22823;&#37327;&#30005;&#27668;&#35774;&#22791;&#36981;&#24490;&#25152;&#38656;&#28040;&#36153;&#20449;&#21495;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;MD-MFC&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30452;&#25509;&#35299;&#20915;&#30446;&#26631;&#36319;&#36394;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24179;&#34913;&#20379;&#38656;&#30340;&#21516;&#26102;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#32435;&#20837;&#30005;&#32593;&#26159;&#19968;&#20010;&#22797;&#26434;&#38382;&#39064;&#65292;&#37492;&#20110;&#20854;&#38388;&#27463;&#24615;&#12290;&#38656;&#27714;&#20391;&#31649;&#29702;&#65288;DSM&#65289;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DSM&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#25511;&#21046;&#22823;&#37327;&#30005;&#27668;&#35774;&#22791;&#36981;&#24490;&#25152;&#38656;&#28040;&#36153;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20854;&#24314;&#27169;&#20026;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#27573;&#39532;&#23572;&#31185;&#22827;&#22343;&#20540;&#22330;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861;&#65292;MD-MFC&#65292;&#20026;&#20984;&#24615;&#21644;Lipschitz&#30446;&#26631;&#20989;&#25968;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;MD-MFC&#19982;&#29616;&#26377;&#36127;&#36733;&#25511;&#21046;&#25991;&#29486;&#30340;&#21306;&#21035;&#22312;&#20110;&#20854;&#22312;&#19981;&#20351;&#29992;&#20027;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#35299;&#20915;&#30446;&#26631;&#36319;&#36394;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#38236;&#20687;&#19979;&#38477;&#26041;&#26696;&#19978;&#30340;&#38750;&#26631;&#20934;Bregman&#36317;&#31163;&#20801;&#35768;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#33719;&#24471;&#31616;&#21333;&#30340;&#38381;&#24335;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#33324;&#30340;&#22343;&#20540;&#22330;&#21338;&#24328;&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#27492;&#38382;&#39064;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating renewable energy into the power grid while balancing supply and demand is a complex issue, given its intermittent nature. Demand side management (DSM) offers solutions to this challenge. We propose a new method for DSM, in particular the problem of controlling a large population of electrical devices to follow a desired consumption signal. We model it as a finite horizon Markovian mean field control problem. We develop a new algorithm, MD-MFC, which provides theoretical guarantees for convex and Lipschitz objective functions. What distinguishes MD-MFC from the existing load control literature is its effectiveness in directly solving the target tracking problem without resorting to regularization techniques on the main problem. A non-standard Bregman divergence on a mirror descent scheme allows dynamic programming to be used to obtain simple closed-form solutions. In addition, we show that general mean-field game algorithms can be applied to this problem, which expands the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Projec and Probe&#65288;Pro$^2$&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#25554;&#20540;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#26469;&#36866;&#24212;&#30446;&#26631;&#20998;&#24067;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.05441</link><description>&lt;p&gt;
&#36890;&#36807;&#25554;&#20540;&#27491;&#20132;&#29305;&#24449;&#23454;&#29616;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#30340;Projec and Probe&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features. (arXiv:2302.05441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Projec and Probe&#65288;Pro$^2$&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#25554;&#20540;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#26469;&#36866;&#24212;&#30446;&#26631;&#20998;&#24067;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23569;&#37327;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26159;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#20998;&#24067;&#21464;&#21270;&#30340;&#19968;&#20010;&#26377;&#25928;&#19988;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#25968;&#25454;&#26631;&#31614;&#21487;&#33021;&#24456;&#38590;&#33719;&#24471;&#65292;&#22240;&#27492;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#25968;&#37327;&#30340;&#30446;&#26631;&#25968;&#25454;&#28857;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#21033;&#29992;&#26497;&#23567;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#12289;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#26469;&#36866;&#24212;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Projec and Probe&#65288;Pro$^2$&#65289;&#65292;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#25237;&#24433;&#65292;&#23558;&#39044;&#35757;&#32451;&#23884;&#20837;&#26144;&#23556;&#21040;&#27491;&#20132;&#26041;&#21521;&#19978;&#65292;&#21516;&#26102;&#21487;&#39044;&#27979;&#28304;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#12290;&#36825;&#19968;&#27493;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#39044;&#27979;&#29305;&#24449;&#65292;&#20197;&#20415;&#22312;&#20998;&#24067;&#21464;&#21270;&#21518;&#20173;&#26377;&#19968;&#20123;&#29305;&#24449;&#26159;&#26377;&#29992;&#30340;&#12290;&#25509;&#30528;&#65292;Pro$^2$&#21033;&#29992;&#23569;&#37327;&#30446;&#26631;&#25968;&#25454;&#22312;&#36825;&#20123;&#25237;&#24433;&#29305;&#24449;&#20043;&#19978;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;Pro$^2$&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#25928;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning with a small amount of target data is an effective and common approach to adapting a pre-trained model to distribution shifts. In some situations, target data labels may be expensive to obtain, so we may only have access to a limited number of target data points. To make the most of a very small target dataset, we propose a lightweight, sample-efficient approach that learns a diverse set of features and adapts to a target distribution by interpolating these features. Our approach, Project and Probe (Pro$^2$), first learns a linear projection that maps a pre-trained embedding onto orthogonal directions while being predictive of labels in the source dataset. The goal of this step is to learn a variety of predictive features, so that at least some of them remain useful after distribution shift. Pro$^2$ then learns a linear classifier on top of these projected features using a small target dataset. Theoretically, we find that Pro$^2$ results in more sample-efficient gener
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25191;&#34892;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#21169;&#20869;&#23481;&#21019;&#20316;&#32773;&#21019;&#24314;&#22810;&#26679;&#24615;&#20869;&#23481;&#65292;&#20197;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25512;&#33616;&#31995;&#32479;&#30340;&#34920;&#28436;&#24615;&#36136;&#21644;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#31574;&#30053;&#21464;&#21270;&#65292;&#24182;&#23545;&#20869;&#23481;&#30340;&#21516;&#36136;&#24615;&#36827;&#34892;&#24809;&#32602;&#12290;</title><link>http://arxiv.org/abs/2302.04336</link><description>&lt;p&gt;
&#25191;&#34892;&#25512;&#33616;&#65306;&#36890;&#36807;&#31574;&#30053;&#28608;&#21169;&#23454;&#29616;&#22810;&#26679;&#24615;&#20869;&#23481;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Performative Recommendation: Diversifying Content via Strategic Incentives. (arXiv:2302.04336v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25191;&#34892;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#21169;&#20869;&#23481;&#21019;&#20316;&#32773;&#21019;&#24314;&#22810;&#26679;&#24615;&#20869;&#23481;&#65292;&#20197;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25512;&#33616;&#31995;&#32479;&#30340;&#34920;&#28436;&#24615;&#36136;&#21644;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#31574;&#30053;&#21464;&#21270;&#65292;&#24182;&#23545;&#20869;&#23481;&#30340;&#21516;&#36136;&#24615;&#36827;&#34892;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21521;&#29992;&#25143;&#25512;&#33616;&#30456;&#20851;&#32852;&#30340;&#20869;&#23481;&#65292;&#20294;&#26159;&#20248;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#24448;&#24448;&#23548;&#33268;&#25512;&#33616;&#30340;&#20869;&#23481;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;&#22914;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#65292;&#21487;&#20197;&#36890;&#36807;&#21576;&#29616;&#26356;&#22810;&#26679;&#30340;&#39033;&#30446;&#26469;&#25552;&#39640;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#20026;&#20102;&#20419;&#36827;&#22810;&#26679;&#24615;&#30340;&#20135;&#29983;&#21644;&#24310;&#32493;&#65292;&#31995;&#32479;&#24517;&#39035;&#40723;&#21169;&#20869;&#23481;&#21019;&#36896;&#32773;&#21019;&#36896;&#22810;&#26679;&#24615;&#20869;&#23481;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#25512;&#33616;&#31995;&#32479;&#30340;&#34920;&#28436;&#24615;&#36136;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#23398;&#20064;&#26469;&#28608;&#21169;&#20869;&#23481;&#21019;&#20316;&#32773;&#21019;&#24314;&#22810;&#26679;&#24615;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#31574;&#30053;&#21464;&#21270;&#65292;&#24182;&#23545;&#20869;&#23481;&#30340;&#21516;&#36136;&#24615;&#36827;&#34892;&#24809;&#32602;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#21487;&#20197;&#28608;&#21169;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#19978;&#23454;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary goal in recommendation is to suggest relevant content to users, but optimizing for accuracy often results in recommendations that lack diversity. To remedy this, conventional approaches such as re-ranking improve diversity by presenting more diverse items. Here we argue that to promote inherent and prolonged diversity, the system must encourage its creation. Towards this, we harness the performative nature of recommendation, and show how learning can incentivize strategic content creators to create diverse content. Our approach relies on a novel form of regularization that anticipates strategic changes to content, and penalizes for content homogeneity. We provide analytic and empirical results that demonstrate when and how diversity can be incentivized, and experimentally demonstrate the utility of our approach on synthetic and semi-synthetic data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;CVaR&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#38024;&#23545;&#22810;&#33218;&#32769;&#34382;&#26426;&#21644;&#26631;&#31614;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#19978;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20271;&#24681;&#26031;&#22374;&#22870;&#21169;&#31639;&#27861;&#21644;&#22522;&#20110;&#20215;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2302.03201</link><description>&lt;p&gt;
&#22522;&#20110;CVaR&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#26368;&#23567;&#21270;&#39118;&#38505;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Near-Minimax-Optimal Risk-Sensitive Reinforcement Learning with CVaR. (arXiv:2302.03201v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;CVaR&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#38024;&#23545;&#22810;&#33218;&#32769;&#34382;&#26426;&#21644;&#26631;&#31614;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#19978;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20271;&#24681;&#26031;&#22374;&#22870;&#21169;&#31639;&#27861;&#21644;&#22522;&#20110;&#20215;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning)&#30340;&#30446;&#26631;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;(CVaR)&#65292;&#24182;&#38024;&#23545;&#22810;&#33218;&#32769;&#34382;&#26426;&#21644;&#26631;&#31614;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19978;&#32622;&#20449;&#30028;&#31639;&#27861;&#30340;&#20271;&#24681;&#26031;&#22374;&#22870;&#21169;&#31639;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#36830;&#32493;&#24615;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20248;&#25110;&#32773;&#25509;&#36817;&#26368;&#20248;&#30340;&#39118;&#38505;&#12290;&#36825;&#20123;&#31639;&#27861;&#37117;&#26159;&#22522;&#20110;CVaR&#25152;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study risk-sensitive Reinforcement Learning (RL), focusing on the objective of Conditional Value at Risk (CVaR) with risk tolerance $\tau$. Starting with multi-arm bandits (MABs), we show the minimax CVaR regret rate is $\Omega(\sqrt{\tau^{-1}AK})$, where $A$ is the number of actions and $K$ is the number of episodes, and that it is achieved by an Upper Confidence Bound algorithm with a novel Bernstein bonus. For online RL in tabular Markov Decision Processes (MDPs), we show a minimax regret lower bound of $\Omega(\sqrt{\tau^{-1}SAK})$ (with normalized cumulative rewards), where $S$ is the number of states, and we propose a novel bonus-driven Value Iteration procedure. We show that our algorithm achieves the optimal regret of $\widetilde O(\sqrt{\tau^{-1}SAK})$ under a continuity assumption and in general attains a near-optimal regret of $\widetilde O(\tau^{-1}\sqrt{SAK})$, which is minimax-optimal for constant $\tau$. This improves on the best available bounds. By di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#35760;&#24518;&#30340;&#20803;&#23398;&#20064;&#22312;&#38750;&#24179;&#31283;&#20998;&#24067;&#19978;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#21644;&#21160;&#20316;-&#35266;&#23519;&#24207;&#21015;&#65292;&#30740;&#31350;&#34920;&#26126;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#31070;&#32463;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#36924;&#36817;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#65292;&#24182;&#25191;&#34892;&#28508;&#22312;&#21442;&#25968;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2302.03067</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#30340;&#20803;&#23398;&#20064;&#22312;&#38750;&#24179;&#31283;&#20998;&#24067;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Memory-Based Meta-Learning on Non-Stationary Distributions. (arXiv:2302.03067v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#35760;&#24518;&#30340;&#20803;&#23398;&#20064;&#22312;&#38750;&#24179;&#31283;&#20998;&#24067;&#19978;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#21644;&#21160;&#20316;-&#35266;&#23519;&#24207;&#21015;&#65292;&#30740;&#31350;&#34920;&#26126;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#31070;&#32463;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#36924;&#36817;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#65292;&#24182;&#25191;&#34892;&#28508;&#22312;&#21442;&#25968;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#30340;&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#36924;&#36817;&#36125;&#21494;&#26031;&#26368;&#20248;&#39044;&#27979;&#22120;&#30340;&#25216;&#26415;&#12290;&#22312;&#30456;&#24403;&#19968;&#33324;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39034;&#24207;&#39044;&#27979;&#35823;&#24046;&#65288;&#30001;&#23545;&#25968;&#25439;&#22833;&#24230;&#37327;&#65289;&#20250;&#23548;&#33268;&#38544;&#24335;&#20803;&#23398;&#20064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#24403;&#21069;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#26696;&#33021;&#21542;&#23454;&#29616;&#36825;&#31181;&#35299;&#37322;&#30340;&#28145;&#24230;&#12290;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#26410;&#35266;&#23519;&#21040;&#30340;&#20999;&#25442;&#28857;&#30340;&#20998;&#27573;&#24179;&#31283;&#28304;&#65292;&#24456;&#21487;&#33021;&#25429;&#25417;&#21040;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#21644;&#21160;&#20316;-&#35266;&#23519;&#24207;&#21015;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#31070;&#32463;&#27169;&#22411;&#65288;&#21253;&#25324;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#12289;LSTM&#21644;RNN&#65289;&#21487;&#20197;&#23398;&#20064;&#20934;&#30830;&#22320;&#36924;&#36817;&#24050;&#30693;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#65292;&#24182;&#34920;&#29616;&#24471;&#22909;&#20687;&#22312;&#27599;&#20010;&#27573;&#20869;&#23545;&#28508;&#22312;&#20999;&#25442;&#28857;&#21644;&#25511;&#21046;&#25968;&#25454;&#20998;&#24067;&#30340;&#28508;&#22312;&#21442;&#25968;&#25191;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-based meta-learning is a technique for approximating Bayes-optimal predictors. Under fairly general conditions, minimizing sequential prediction error, measured by the log loss, leads to implicit meta-learning. The goal of this work is to investigate how far this interpretation can be realized by current sequence prediction models and training regimes. The focus is on piecewise stationary sources with unobserved switching-points, which arguably capture an important characteristic of natural language and action-observation sequences in partially observable environments. We show that various types of memory-based neural models, including Transformers, LSTMs, and RNNs can learn to accurately approximate known Bayes-optimal algorithms and behave as if performing Bayesian inference over the latent switching-points and the latent parameters governing the data distribution within each segment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#23454;&#29616;&#32676;&#32452;&#21512;&#26469;&#30740;&#31350;&#26222;&#36866;&#24615;&#20551;&#35774;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36890;&#36807;&#20219;&#24847;&#26377;&#38480;&#32676;&#26469;&#23454;&#29616;&#32452;&#21512;&#65292;&#20174;&#32780;&#23436;&#20840;&#25551;&#36848;&#32593;&#32476;&#22312;&#27492;&#20219;&#21153;&#19978;&#23398;&#20064;&#30340;&#30005;&#36335;&#21644;&#29305;&#24449;&#26063;&#12290;</title><link>http://arxiv.org/abs/2302.03025</link><description>&lt;p&gt;
&#19968;&#31181;&#26222;&#36866;&#24615;&#30340;&#29609;&#20855;&#27169;&#22411;&#65306;&#36870;&#21521;&#24037;&#31243;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#32676;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations. (arXiv:2302.03025v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#23454;&#29616;&#32676;&#32452;&#21512;&#26469;&#30740;&#31350;&#26222;&#36866;&#24615;&#20551;&#35774;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36890;&#36807;&#20219;&#24847;&#26377;&#38480;&#32676;&#26469;&#23454;&#29616;&#32452;&#21512;&#65292;&#20174;&#32780;&#23436;&#20840;&#25551;&#36848;&#32593;&#32476;&#22312;&#27492;&#20219;&#21153;&#19978;&#23398;&#20064;&#30340;&#30005;&#36335;&#21644;&#29305;&#24449;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36866;&#24615;&#26159;&#26426;&#26800;&#35299;&#37322;&#24615;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;--&#19981;&#21516;&#30340;&#27169;&#22411;&#22312;&#31867;&#20284;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#23398;&#20064;&#30456;&#20284;&#30340;&#29305;&#24449;&#21644;&#30005;&#36335;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#23454;&#29616;&#32676;&#32452;&#21512;&#26469;&#30740;&#31350;&#26222;&#36866;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#23398;&#34920;&#31034;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36890;&#36807;&#20219;&#24847;&#26377;&#38480;&#32676;&#26469;&#23454;&#29616;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#27169;&#22411;&#30340;&#36923;&#36753;&#21644;&#26435;&#37325;&#26469;&#23637;&#31034;&#32593;&#32476;&#22987;&#32456;&#23398;&#20064;&#27492;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#28040;&#34701;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#22312;&#19981;&#21516;&#32676;&#19978;&#30340;&#19981;&#21516;&#26550;&#26500;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#21457;&#29616;&#26222;&#36866;&#24615;&#30340;&#35777;&#25454;&#19981;&#19968;&#65306;&#20351;&#29992;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#23436;&#20840;&#25551;&#36848;&#32593;&#32476;&#22312;&#27492;&#20219;&#21153;&#19978;&#23398;&#20064;&#30340;&#30005;&#36335;&#21644;&#29305;&#24449;&#26063;&#65292;&#20294;&#23545;&#20110;&#32473;&#23450;&#30340;&#32593;&#32476;&#65292;&#23398;&#20064;&#30340;&#31934;&#30830;&#30005;&#36335;&#20197;&#21450;&#23427;&#20204;&#30340;&#21457;&#23637;&#39034;&#24207;&#26159;&#20219;&#24847;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#24182;&#34892;&#36866;&#37197;&#22120;&#26469;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#65292;&#23454;&#39564;&#34920;&#26126;&#21487;&#20197;&#38477;&#20302;&#32422;90%&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#36817;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02949</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#21442;&#25968;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Adaptive Parameterization of Deep Learning Models for Federated Learning. (arXiv:2302.02949v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#24182;&#34892;&#36866;&#37197;&#22120;&#26469;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#65292;&#23454;&#39564;&#34920;&#26126;&#21487;&#20197;&#38477;&#20302;&#32422;90%&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#36817;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#38656;&#35201;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23450;&#26399;&#20132;&#25442;&#27169;&#22411;&#21442;&#25968;&#25110;&#26799;&#24230;&#65292;&#23548;&#33268;&#36890;&#20449;&#24320;&#38144;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#24182;&#34892;&#36866;&#37197;&#22120;&#26469;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36866;&#37197;&#22120;&#21487;&#20197;&#38477;&#20302;&#32422;90%&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#36817;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#20063;&#25506;&#35752;&#20102;&#36866;&#37197;&#22120;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#36328;&#36793;&#30028;&#21644;&#36328;&#35774;&#22791;&#22330;&#26223;&#21450;&#19981;&#21516;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20998;&#24067;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning offers a way to train deep neural networks in a distributed fashion. While this addresses limitations related to distributed data, it incurs a communication overhead as the model parameters or gradients need to be exchanged regularly during training. This can be an issue with large scale distribution of learning tasks and negate the benefit of the respective resource distribution. In this paper, we we propose to utilise parallel Adapters for Federated Learning. Using various datasets, we show that Adapters can be incorporated to different Federated Learning techniques. We highlight that our approach can achieve similar inference performance compared to training the full model while reducing the communication overhead by roughly 90%. We further explore the applicability of Adapters in cross-silo and cross-device settings, as well as different non-IID data distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#8212;&#8212;Guided Adversarial Training (GAT)&#65292;&#23427;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#25552;&#39640;&#27169;&#22411;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02907</link><description>&lt;p&gt;
GAT&#65306;&#24102; Pareto &#26368;&#20248;&#36741;&#21161;&#20219;&#21153;&#30340;&#24341;&#23548;&#24335;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks. (arXiv:2302.02907v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#8212;&#8212;Guided Adversarial Training (GAT)&#65292;&#23427;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#25552;&#39640;&#27169;&#22411;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#65292;&#21033;&#29992;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#30830;&#31435;&#30340;&#22909;&#26041;&#27861;&#65292;&#20294;&#26159;&#23427;&#35201;&#20184;&#20986;&#25968;&#25454;&#25910;&#38598;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#20195;&#20215;&#21644;&#35757;&#32451;&#27169;&#22411;&#30340;&#37325;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Guided Adversarial Training (GAT)&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#30340;&#26032;&#22411;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#12290;&#22312;&#23545;&#25239;&#35757;&#32451;&#30340;&#26497;&#23567;&#21270;&#26368;&#22823;&#21270;&#20248;&#21270;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21333;&#20219;&#21153;&#27169;&#22411;&#25193;&#23637;&#20026;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36328;&#22810;&#20010;&#20219;&#21153;&#30340;&#26799;&#24230;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#26469;&#39537;&#21160;&#25439;&#22833;&#20248;&#21270;&#12290;GAT&#21033;&#29992;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#36741;&#21161;&#20219;&#21153;&#65306;&#33258;&#30417;&#30563;&#20219;&#21153;&#65292;&#20854;&#20013;&#26631;&#31614;&#26159;&#33258;&#21160;&#29983;&#25104;&#30340;&#65292;&#21644;&#39046;&#22495;&#30693;&#35782;&#20219;&#21153;&#65292;&#20854;&#20013;&#20154;&#31867;&#19987;&#23478;&#25552;&#20379;&#39069;&#22806;&#30340;&#26631;&#31614;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;GAT&#23558; CheXpert &#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#30340;&#40065;&#26834;&#24615; AUC &#20174;50% &#25552;&#39640;&#21040;83%&#65292;&#22312; CIFAR-10 &#19978;&#65292;GAT &#36229;&#36807;&#20102;&#20843;&#31181;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;Resnet&#21487;&#20197;&#36798;&#21040;56.21% &#30340;&#40065;&#26834;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While leveraging additional training data is well established to improve adversarial robustness, it incurs the unavoidable cost of data collection and the heavy computation to train models. To mitigate the costs, we propose Guided Adversarial Training (GAT), a novel adversarial training technique that exploits auxiliary tasks under a limited set of training data. Our approach extends single-task models into multi-task models during the min-max optimization of adversarial training, and drives the loss optimization with a regularization of the gradient curvature across multiple tasks. GAT leverages two types of auxiliary tasks: self-supervised tasks, where the labels are generated automatically, and domain-knowledge tasks, where human experts provide additional labels. Experimentally, GAT increases the robust AUC of CheXpert medical imaging dataset from 50% to 83% and On CIFAR-10, GAT outperforms eight state-of-the-art adversarial training and achieves 56.21% robust accuracy with Resnet-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#31639;&#27861;&#26469;&#25552;&#39640;&#24191;&#21578;&#20998;&#37197;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#36807;&#24230;&#20445;&#23432;&#65292;&#24182;&#19988;&#22312;&#19981;&#33391;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.01827</link><description>&lt;p&gt;
&#39044;&#27979;&#19982;&#22312;&#32447;&#24191;&#21578;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Online Ad Allocation with Predictions. (arXiv:2302.01827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#31639;&#27861;&#26469;&#25552;&#39640;&#24191;&#21578;&#20998;&#37197;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#36807;&#24230;&#20445;&#23432;&#65292;&#24182;&#19988;&#22312;&#19981;&#33391;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#24191;&#21578;&#19982;&#19968;&#33324;&#21270;&#20998;&#37197;&#38382;&#39064;&#26159;&#20004;&#20010;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#22312;&#32447;&#25171;&#21253;&#38382;&#39064;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#24191;&#21578;&#20998;&#37197;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#22312;&#36825;&#20004;&#20010;&#38382;&#39064;&#20013;&#65292;&#24191;&#21578;&#23637;&#31034;&#20250;&#22312;&#32447;&#21040;&#36798;&#65292;&#24517;&#39035;&#31435;&#21363;&#20998;&#37197;&#32473;&#21463;&#39044;&#31639;&#38480;&#21046;&#30340;&#24191;&#21578;&#23458;&#25143;&#12290;&#24050;&#30693;&#30340;&#26368;&#20248;&#31454;&#20105;&#27604;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#29616;&#23454;&#36755;&#20837;&#30340;&#21487;&#39044;&#27979;&#24615;&#21644;&#36890;&#24120;&#36739;&#24179;&#31283;&#30340;&#29305;&#24615;&#65292;&#21487;&#33021;&#36807;&#20110;&#20445;&#23432;&#12290;&#37492;&#20110;&#36825;&#19968;&#24046;&#24322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#38024;&#23545;&#36825;&#20004;&#20010;&#38382;&#39064;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#21487;&#20197;&#22312;&#26368;&#22351;&#24773;&#20917;&#20043;&#22806;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;Feldman&#31561;&#20154;&#65288;2009&#65289;&#30340;&#24037;&#20316;&#65292;&#24182;&#19982;Mahdian&#31561;&#20154;&#65288;2007&#65289;&#31867;&#20284;&#65292;&#20182;&#20204;&#26159;&#31532;&#19968;&#20010;&#20026;&#30456;&#20851;&#20294;&#26356;&#20855;&#32467;&#26500;&#30340;&#24191;&#21578;&#35789;&#38382;&#39064;&#24320;&#21457;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#30340;&#20154;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#21033;&#29992;&#33391;&#22909;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#23545;&#24046;&#39044;&#27979;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Display Ads and the generalized assignment problem are two well-studied online packing problems with important applications in ad allocation and other areas. In both problems, ad impressions arrive online and have to be allocated immediately to budget-constrained advertisers. Worst-case algorithms that achieve the ideal competitive ratio are known, but might act overly conservative given the predictable and usually tame nature of real-world input. Given this discrepancy, we develop an algorithm for both problems that incorporate machine-learned predictions and can thus improve the performance beyond the worst-case. Our algorithm is based on the work of Feldman et al. (2009) and similar in nature to Mahdian et al. (2007) who were the first to develop a learning-augmented algorithm for the related, but more structured Ad Words problem. We use a novel analysis to show that our algorithm is able to capitalize on a good prediction, while being robust against poor predictions. We experimenta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QCS-SGM+&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGM)&#20316;&#20026;&#38544;&#24335;&#20808;&#39564;&#36827;&#34892;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;(QCS)&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#19968;&#33324;&#30697;&#38453;&#12290;&#36825;&#20010;&#31639;&#27861;&#35299;&#20915;&#20102;&#22312;&#31895;&#31961;&#37327;&#21270;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00919</link><description>&lt;p&gt;
QCM-SGM+: &#22522;&#20110;&#24471;&#20998;&#29983;&#25104;&#27169;&#22411;&#30340;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
QCM-SGM+: Improved Quantized Compressed Sensing With Score-Based Generative Models. (arXiv:2302.00919v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QCS-SGM+&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGM)&#20316;&#20026;&#38544;&#24335;&#20808;&#39564;&#36827;&#34892;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;(QCS)&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#19968;&#33324;&#30697;&#38453;&#12290;&#36825;&#20010;&#31639;&#27861;&#35299;&#20915;&#20102;&#22312;&#31895;&#31961;&#37327;&#21270;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#21387;&#32553;&#24863;&#30693;&#36807;&#31243;&#20013;&#65292;&#33719;&#24471;&#30340;&#27979;&#37327;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#22312;&#20256;&#36755;&#25110;&#23384;&#20648;&#21069;&#38480;&#21046;&#20026;&#26377;&#38480;&#27604;&#29305;&#30340;&#37327;&#21270;&#12290;&#36825;&#20010;&#38750;&#32447;&#24615;&#37327;&#21270;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#24674;&#22797;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#26497;&#24230;&#31895;&#31961;&#30340;&#37327;&#21270;&#22914;1&#27604;&#29305;&#19979;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;QCS-SGM&#30340;&#26377;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGM)&#20316;&#20026;&#38544;&#24335;&#20808;&#39564;&#36827;&#34892;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;(QCS)&#12290;&#30001;&#20110;SGM&#22312;&#25429;&#25417;&#33258;&#28982;&#20449;&#21495;&#30340;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;QCS-SGM&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;QCS&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;QCS-SGM&#23616;&#38480;&#20110;(&#36817;&#20284;)&#34892;&#27491;&#20132;&#20256;&#24863;&#30697;&#38453;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20998;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QCS-SGM+&#30340;&#39640;&#32423;&#21464;&#20307;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#19968;&#33324;&#30697;&#38453;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20284;&#28982;&#20998;&#25968;&#35745;&#31639;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#35266;&#28857;&#65292;&#20854;&#20013;&#35745;&#31639;&#26399;&#26395;&#24471;&#20998;&#20197;&#35299;&#20915;&#27599;&#20010;&#27979;&#37327;&#30340;&#32467;&#26500;&#38750;&#27491;&#20132;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical compressed sensing (CS), the obtained measurements typically necessitate quantization to a limited number of bits prior to transmission or storage. This nonlinear quantization process poses significant recovery challenges, particularly with extreme coarse quantization such as 1-bit. Recently, an efficient algorithm called QCS-SGM was proposed for quantized CS (QCS) which utilizes score-based generative models (SGM) as an implicit prior. Due to the adeptness of SGM in capturing the intricate structures of natural signals, QCS-SGM substantially outperforms previous QCS methods. However, QCS-SGM is constrained to (approximately) row-orthogonal sensing matrices as the computation of the likelihood score becomes intractable otherwise. To address this limitation, we introduce an advanced variant of QCS-SGM, termed QCS-SGM+, capable of handling general matrices effectively. The key idea is a Bayesian inference perspective on the likelihood score computation, wherein an expectatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24130;&#24459;&#35268;&#24459;&#30340;Deep Power Laws&#65288;DPL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36229;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#34920;&#26684;&#12289;&#22270;&#20687;&#21644;NLP&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.00441</link><description>&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24130;&#24459;&#27861;&#21017;
&lt;/p&gt;
&lt;p&gt;
Power Laws for Hyperparameter Optimization. (arXiv:2302.00441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24130;&#24459;&#35268;&#24459;&#30340;Deep Power Laws&#65288;DPL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36229;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#34920;&#26684;&#12289;&#22270;&#20687;&#21644;NLP&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#23376;&#39046;&#22495;&#65292;&#23427;&#19987;&#27880;&#20110;&#35843;&#25972;&#25152;&#36873;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#26377;&#19968;&#31995;&#21015;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#23398;&#20064;&#26354;&#32447;&#30340;&#32553;&#25918;&#35268;&#24459;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;Deep Power Laws&#65288;DPL&#65289;&#65292;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#36981;&#24490;&#19968;&#20010;&#24130;&#24459;&#32553;&#25918;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#28784;&#30418;&#35780;&#20272;&#21160;&#24577;&#20915;&#23450;&#26242;&#20572;&#21644;&#22686;&#37327;&#35757;&#32451;&#21738;&#20123;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#19982;3&#20010;&#22522;&#20934;&#30456;&#20851;&#30340;&#34920;&#26684;&#65292;&#22270;&#20687;&#21644;NLP&#25968;&#25454;&#38598;&#19978;&#19982;7&#31181;&#26368;&#20808;&#36827;&#30340;&#31454;&#20105;&#23545;&#25163;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#28085;&#30422;59&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#33719;&#24471;&#20102;&#27604;&#25152;&#26377;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#20219;&#20309;&#26102;&#20505;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization is an important subfield of machine learning that focuses on tuning the hyperparameters of a chosen algorithm to achieve peak performance. Recently, there has been a stream of methods that tackle the issue of hyperparameter optimization, however, most of the methods do not exploit the scaling law property of learning curves. In this work, we propose Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern. Our method dynamically decides which configurations to pause and train incrementally by making use of gray-box evaluations. We compare our method against 7 state-of-the-art competitors on 3 benchmarks related to tabular, image, and NLP datasets covering 59 diverse tasks. Our method achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#23618;&#22270;&#21367;&#31215;&#26680;&#26426;&#22120;(GCKM)&#65292;&#36890;&#36807;&#22534;&#21472;&#22810;&#20010;&#27973;&#26680;&#26426;&#22120;&#65292;&#35813;&#26426;&#22120;&#33021;&#22815;&#22312;&#22270;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#24182;&#19988;&#22312;&#21487;&#29992;&#26631;&#31614;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2301.13764</link><description>&lt;p&gt;
&#24102;&#26377;&#22270;&#21367;&#31215;&#26680;&#26426;&#22120;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Classification with Graph Convolutional Kernel Machines. (arXiv:2301.13764v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13764
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#23618;&#22270;&#21367;&#31215;&#26680;&#26426;&#22120;(GCKM)&#65292;&#36890;&#36807;&#22534;&#21472;&#22810;&#20010;&#27973;&#26680;&#26426;&#22120;&#65292;&#35813;&#26426;&#22120;&#33021;&#22815;&#22312;&#22270;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#24182;&#19988;&#22312;&#21487;&#29992;&#26631;&#31614;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20013;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#30340;&#28145;&#23618;&#22270;&#21367;&#31215;&#26680;&#26426;&#22120;(GCKM)&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#26680;&#26426;&#22120;&#26469;&#22312;&#19968;&#20010;&#19968;&#36339;&#37051;&#22495;&#20869;&#20256;&#25773;&#33410;&#28857;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;Fenchel-Young&#19981;&#31561;&#24335;&#30340;&#35270;&#35282;&#26469;&#25351;&#23450;&#21322;&#30417;&#30563;&#20998;&#31867;&#26680;&#26426;&#22120;&#12290;&#36890;&#36807;&#22534;&#21472;&#22810;&#20010;&#27973;&#26680;&#26426;&#22120;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#28145;&#24230;&#22270;&#21367;&#31215;&#26680;&#26426;&#22120;&#12290;&#22312;&#23637;&#31034;&#20102;&#26080;&#30417;&#30563;&#23618;&#21644;&#21322;&#30417;&#30563;&#23618;&#20998;&#21035;&#23545;&#24212;&#20110;&#32858;&#21512;&#33410;&#28857;&#29305;&#24449;&#30340;&#29305;&#24449;&#20540;&#38382;&#39064;&#21644;&#32447;&#24615;&#31995;&#32479;&#20043;&#21518;&#65292;&#25105;&#20204;&#22312;&#23545;&#20598;&#21464;&#37327;&#20013;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#31639;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21516;&#36136;&#21644;&#24322;&#36136;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#21487;&#29992;&#26631;&#31614;&#24456;&#23569;&#26102;&#65292;GCKM&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep Graph Convolutional Kernel Machine (GCKM) for semi-supervised node classification in graphs. First, we introduce an unsupervised kernel machine propagating the node features in a one-hop neighbourhood. Then, we specify a semi-supervised classification kernel machine through the lens of the Fenchel-Young inequality. The deep graph convolutional kernel machine is obtained by stacking multiple shallow kernel machines. After showing that unsupervised and semi-supervised layer corresponds to an eigenvalue problem and a linear system on the aggregated node features, respectively, we derive an efficient end-to-end training algorithm in the dual variables. Numerical experiments demonstrate that our approach is competitive with state-of-the-art graph neural networks for homophilious and heterophilious benchmark datasets. Notably, GCKM achieves superior performance when very few labels are available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21407;&#22411;&#20998;&#26512;&#30340;&#27010;&#29575;&#21021;&#22987;&#21270;&#31574;&#30053; AA ++&#65292;&#33021;&#22815;&#22312;13&#20010;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2301.13748</link><description>&lt;p&gt;
&#21407;&#22411;&#20998;&#26512;++&#65306;&#37325;&#26032;&#24605;&#32771;&#21021;&#22987;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Archetypal Analysis++: Rethinking the Initialization Strategy. (arXiv:2301.13748v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21407;&#22411;&#20998;&#26512;&#30340;&#27010;&#29575;&#21021;&#22987;&#21270;&#31574;&#30053; AA ++&#65292;&#33021;&#22815;&#22312;13&#20010;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#20998;&#26512;&#26159;&#19968;&#31181;&#24102;&#26377;&#20984;&#24615;&#32422;&#26463;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#12290;&#30001;&#20110;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#65292;&#22909;&#30340;&#21021;&#22987;&#21270;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#32463;&#24120;&#20351;&#29992;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#35201;&#20040;&#20135;&#29983;&#27425;&#20248;&#30340;&#36215;&#22987;&#28857;&#65292;&#35201;&#20040;&#23481;&#26131;&#38519;&#20837;&#19981;&#33391;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22411;&#20998;&#26512;++&#65288;AA ++&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#21407;&#22411;&#20998;&#26512;&#30340;&#27010;&#29575;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#23427;&#26681;&#25454;&#28857;&#23545;&#30446;&#26631;&#30340;&#24433;&#21709;&#39034;&#24207;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#31867;&#20284;&#20110;$k$-means++&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#35748;&#20026;$k$-means++&#24050;&#36817;&#36924;&#36817;&#20102;&#25152;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;$k$-means++&#30340;&#39640;&#25928;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26041;&#27861;&#24212;&#29992;&#20110;AA++&#12290;&#22312;&#23545;13&#20010;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#24182;&#32771;&#34385;&#20004;&#20010;&#39044;&#22788;&#29702;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;AA++&#20960;&#20046;&#24635;&#26159;&#20248;&#20110;&#25152;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#21253;&#25324;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Archetypal analysis is a matrix factorization method with convexity constraints. Due to local minima, a good initialization is essential, but frequently used initialization methods yield either sub-optimal starting points or are prone to get stuck in poor local minima. In this paper, we propose archetypal analysis++ (AA++), a probabilistic initialization strategy for archetypal analysis that sequentially samples points based on their influence on the objective, similar to $k$-means++. In fact, we argue that $k$-means++ already approximates the proposed initialization method. Furthermore, we suggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++. In an extensive empirical evaluation of 13 real-world data sets of varying sizes and dimensionalities and considering two pre-processing strategies, we show that AA++ nearly always outperforms all baselines, including the most frequently used ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992; GFlowNets &#30340;&#29702;&#35770;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#36830;&#32493;&#25110;&#28151;&#21512;&#29366;&#24577;&#31354;&#38388;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#38750; GFlowNet &#22522;&#32447;&#30456;&#27604;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#32467;&#26524;&#65292;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#23558; GFlowNets &#24212;&#29992;&#20110;&#27010;&#29575;&#25512;&#29702;&#21644;&#21508;&#31181;&#24314;&#27169;&#35774;&#32622;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2301.12594</link><description>&lt;p&gt;
&#36830;&#32493;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A theory of continuous generative flow networks. (arXiv:2301.12594v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992; GFlowNets &#30340;&#29702;&#35770;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#36830;&#32493;&#25110;&#28151;&#21512;&#29366;&#24577;&#31354;&#38388;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#38750; GFlowNet &#22522;&#32447;&#30456;&#27604;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#32467;&#26524;&#65292;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#23558; GFlowNets &#24212;&#29992;&#20110;&#27010;&#29575;&#25512;&#29702;&#21644;&#21508;&#31181;&#24314;&#27169;&#35774;&#32622;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#19968;&#31181;&#24120;&#35268;&#21270;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#65292;&#34987;&#35757;&#32451;&#29992;&#20110;&#20174;&#32452;&#21512;&#23545;&#35937;&#30340;&#26410;&#24402;&#19968;&#21270;&#30446;&#26631;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;GFlowNets &#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#21040;&#30446;&#21069;&#20026;&#27490;&#19968;&#30452;&#26159;&#23427;&#20204;&#20165;&#36866;&#29992;&#20110;&#31163;&#25955;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992; GFlowNets &#30340;&#29702;&#35770;&#65292;&#23427;&#21253;&#25324;&#29616;&#26377;&#30340;&#31163;&#25955; GFlowNets &#21644;&#36830;&#32493;&#25110;&#28151;&#21512;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340; GFlowNets&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#30446;&#26631;&#36827;&#34892;&#23454;&#39564;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#29702;&#35770;&#30340;&#20851;&#38190;&#28857;&#20197;&#21450;&#21508;&#31181;&#20551;&#35774;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#20043;&#21069;&#30740;&#31350;&#30340;&#20219;&#21153;&#20013;&#65292;&#35777;&#26126;&#20102;&#20851;&#20110;&#31163;&#25955; GFlowNets &#30340;&#35266;&#23519;&#32467;&#26524;&#22914;&#20309;&#36716;&#21270;&#20026;&#36830;&#32493;&#24773;&#20917;&#65292;&#24182;&#19982;&#38750; GFlowNet &#22522;&#32447;&#30456;&#27604;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#23558; GFlowNets &#24212;&#29992;&#20110;&#27010;&#29575;&#25512;&#29702;&#21644;&#21508;&#31181;&#24314;&#27169;&#35774;&#32622;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative flow networks (GFlowNets) are amortized variational inference algorithms that are trained to sample from unnormalized target distributions over compositional objects. A key limitation of GFlowNets until this time has been that they are restricted to discrete spaces. We present a theory for generalized GFlowNets, which encompasses both existing discrete GFlowNets and ones with continuous or hybrid state spaces, and perform experiments with two goals in mind. First, we illustrate critical points of the theory and the importance of various assumptions. Second, we empirically demonstrate how observations about discrete GFlowNets transfer to the continuous case and show strong results compared to non-GFlowNet baselines on several previously studied tasks. This work greatly widens the perspectives for the application of GFlowNets in probabilistic inference and various modeling settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#27491;&#21521;&#36807;&#31243;&#20197;&#26368;&#23567;&#21270;&#29983;&#25104;&#36712;&#36857;&#30340;&#26354;&#29575;&#65292;&#26469;&#20248;&#21270;ODE/SDE-based&#29983;&#25104;&#27169;&#22411;&#30340;&#37319;&#26679;&#36895;&#24230;&#65292;&#23454;&#39564;&#34920;&#26126;&#27492;&#26041;&#27861;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2301.12003</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;ODE&#29983;&#25104;&#27169;&#22411;&#36712;&#36857;&#30340;&#26354;&#29575;
&lt;/p&gt;
&lt;p&gt;
Minimizing Trajectory Curvature of ODE-based Generative Models. (arXiv:2301.12003v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#27491;&#21521;&#36807;&#31243;&#20197;&#26368;&#23567;&#21270;&#29983;&#25104;&#36712;&#36857;&#30340;&#26354;&#29575;&#65292;&#26469;&#20248;&#21270;ODE/SDE-based&#29983;&#25104;&#27169;&#22411;&#30340;&#37319;&#26679;&#36895;&#24230;&#65292;&#23454;&#39564;&#34920;&#26126;&#27492;&#26041;&#27861;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;ODE / SDE&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#25193;&#25955;&#27169;&#22411;&#65292;&#30697;&#24418;&#27969;&#21644;&#27969;&#21305;&#37197;&#65292;&#23558;&#29983;&#25104;&#36807;&#31243;&#23450;&#20041;&#20026;&#22266;&#23450;&#27491;&#21521;&#36807;&#31243;&#30340;&#26102;&#38388;&#21453;&#28436;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#25968;&#20540;&#27169;&#25311;&#38656;&#35201;&#22810;&#27425;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#65292;&#23548;&#33268;&#37319;&#26679;&#36895;&#24230;&#32531;&#24930;&#12290;&#25105;&#20204;&#35748;&#20026;&#21407;&#22240;&#22312;&#20110;&#23398;&#20064;&#21040;&#30340;&#29983;&#25104;&#36712;&#36857;&#20855;&#26377;&#24456;&#39640;&#30340;&#26354;&#29575;&#65292;&#22240;&#20026;&#23427;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#25130;&#26029;&#35823;&#24046;&#30452;&#25509;&#30456;&#20851;&#12290;&#22522;&#20110;&#27491;&#21521;&#36807;&#31243;&#21644;&#26354;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27491;&#21521;&#36807;&#31243;&#20197;&#26368;&#23567;&#21270;&#29983;&#25104;&#36712;&#36857;&#30340;&#26354;&#29575;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;ODE / SDE&#27169;&#25311;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#27604;&#20808;&#21069;&#27169;&#22411;&#26356;&#20302;&#30340;&#26354;&#29575;&#65292;&#24182;&#22240;&#27492;&#38477;&#20302;&#20102;&#37319;&#26679;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent ODE/SDE-based generative models, such as diffusion models, rectified flows, and flow matching, define a generative process as a time reversal of a fixed forward process. Even though these models show impressive performance on large-scale datasets, numerical simulation requires multiple evaluations of a neural network, leading to a slow sampling speed. We attribute the reason to the high curvature of the learned generative trajectories, as it is directly related to the truncation error of a numerical solver. Based on the relationship between the forward process and the curvature, here we present an efficient method of training the forward process to minimize the curvature of generative trajectories without any ODE/SDE simulation. Experiments show that our method achieves a lower curvature than previous models and, therefore, decreased sampling costs while maintaining competitive performance. Code is available at https://github.com/sangyun884/fast-ode.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09656</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#35299;&#37322;&#65306;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#23545;&#40784;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22240;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#29983;&#20135;&#21644;&#28040;&#36153;&#26041;&#24335;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;XAI&#25216;&#26415;&#24448;&#24448;&#38590;&#20197;&#20351;&#29992;&#24182;&#32570;&#20047;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#20351;AI&#35299;&#37322;&#20855;&#26377;&#36873;&#25321;&#24615;&#65288;&#36825;&#26159;&#20154;&#31867;&#35299;&#37322;&#30340;&#22522;&#26412;&#23646;&#24615;&#20043;&#19968;&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#36890;&#36807;&#26681;&#25454;&#25509;&#25910;&#26041;&#30340;&#20559;&#22909;&#26377;&#36873;&#25321;&#24615;&#22320;&#21576;&#29616;&#22823;&#37327;&#27169;&#22411;&#21407;&#22240;&#30340;&#23376;&#38598;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#26679;&#26412;&#19978;&#30340;&#20154;&#31867;&#36755;&#20837;&#26469;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#24320;&#36767;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#36873;&#25321;&#24615;&#30446;&#26631;&#12289;&#36755;&#20837;&#31867;&#22411;&#31561;&#12290;&#20316;&#20026;&#19968;&#20010;&#23637;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#26469;&#25506;&#32034;&#22522;&#20110;&#20915;&#31574;&#32773;&#35748;&#20026;&#30456;&#20851;&#30340;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#26816;&#26597;&#20174;&#22823;&#19968;&#32452;&#27169;&#22411;&#21407;&#22240;&#20013;&#36873;&#25321;&#30340;&#19977;&#20010;&#23376;&#38598;&#19982;&#26410;&#36873;&#25321;&#30340;&#23376;&#38598;&#30456;&#27604;&#65292;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a vast collection of explainable AI (XAI) algorithms have been developed in recent years, they are often criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective -- a fundamental property of human explanations -- by selectively presenting a subset from a large set of model reasons based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small sample. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three out of a bro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20256;&#32479;&#25311;&#21512;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#33258;&#21160;&#30005;&#23481;&#32806;&#21512;&#35782;&#21035;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#24102;&#26377;&#34394;&#20551;&#28857;&#30340;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#29305;&#23450;QD&#29420;&#31435;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2301.08654</link><description>&lt;p&gt;
&#37327;&#23376;&#28857;&#31995;&#32479;&#30005;&#23481;&#32806;&#21512;&#30340;&#33258;&#21160;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Automated extraction of capacitive coupling for quantum dot systems. (arXiv:2301.08654v2 [cond-mat.mes-hall] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08654
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20256;&#32479;&#25311;&#21512;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#33258;&#21160;&#30005;&#23481;&#32806;&#21512;&#35782;&#21035;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#24102;&#26377;&#34394;&#20551;&#28857;&#30340;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#29305;&#23450;QD&#29420;&#31435;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#30005;&#26497;&#23450;&#20041;&#30340;&#37327;&#23376;&#28857;&#20316;&#20026;&#37327;&#23376;&#35745;&#31639;&#24179;&#21488;&#26377;&#30528;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36817;&#26399;&#35774;&#22791;&#23384;&#22312;&#19968;&#31995;&#21015;&#28508;&#22312;&#32570;&#38519;&#65292;&#22312;QD&#22120;&#20214;&#30340;&#35843;&#25511;&#21644;&#25805;&#20316;&#20013;&#38656;&#35201;&#21152;&#20197;&#35299;&#20915;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#23450;&#20041;&#21644;&#25511;&#21046;QD&#37327;&#23376;&#27604;&#29305;&#30340;&#37329;&#23646;&#26629;&#20043;&#38388;&#30340;&#30005;&#23481;&#20018;&#25200;&#12290;&#20026;&#20102;&#34917;&#20607;&#30005;&#23481;&#32806;&#21512;&#24182;&#23454;&#29616;&#29305;&#23450;QD&#29420;&#31435;&#25511;&#21046;&#65292;&#21487;&#20197;&#20351;&#29992;&#34394;&#25311;&#38376;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#33258;&#21160;&#30005;&#23481;&#32806;&#21512;&#35782;&#21035;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#20256;&#32479;&#25311;&#21512;&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#21508;&#33258;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20132;&#21449;&#30005;&#23481;&#27979;&#37327;&#22914;&#20309;&#29992;&#20110;&#35782;&#21035;&#22312;&#35843;&#25972;&#23454;&#39564;&#35774;&#22791;&#26102;&#26377;&#26102;&#24418;&#25104;&#30340;&#34394;&#20551;QD&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#33258;&#20027;&#26631;&#35760;&#22788;&#20110;&#25805;&#20316;&#33539;&#22260;&#38468;&#36817;&#30340;&#24102;&#26377;&#34394;&#20551;&#28857;&#30340;&#35774;&#22791;&#65292;&#36825;&#23545;&#20110;&#21487;&#38752;&#35843;&#25972;&#33267;&#21512;&#36866;&#21306;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gate-defined quantum dots (QDs) have appealing attributes as a quantum computing platform. However, near-term devices possess a range of possible imperfections that need to be accounted for during the tuning and operation of QD devices. One such problem is the capacitive cross-talk between the metallic gates that define and control QD qubits. A way to compensate for the capacitive cross-talk and enable targeted control of specific QDs independent of coupling is by the use of virtual gates. Here, we demonstrate a reliable automated capacitive coupling identification method that combines machine learning with traditional fitting to take advantage of the desirable properties of each. We also show how the cross-capacitance measurement may be used for the identification of spurious QDs sometimes formed during tuning experimental devices. Our systems can autonomously flag devices with spurious dots near the operating regime, which is crucial information for reliable tuning to a regime suitab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23427;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20351;&#24471;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#24456;&#22810;&#25104;&#21151;&#65292;&#20294;&#26159;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20250;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2301.05860</link><description>&lt;p&gt;
&#22270;&#23884;&#20837;&#23398;&#20064;&#30340;&#29616;&#29366;&#21644;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
State of the Art and Potentialities of Graph-level Learning. (arXiv:2301.05860v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23427;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20351;&#24471;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#24456;&#22810;&#25104;&#21151;&#65292;&#20294;&#26159;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20250;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#29616;&#20102;&#20851;&#31995;&#25968;&#25454;&#30340;&#19968;&#31181;&#20248;&#36234;&#33021;&#21147;&#65292;&#22914;&#21270;&#21512;&#29289;&#12289;&#34507;&#30333;&#36136;&#21644;&#31038;&#20132;&#32593;&#32476;&#31561;&#12290;&#22240;&#27492;&#65292;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#24050;&#24212;&#29992;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#21253;&#25324;&#27604;&#36739;&#12289;&#22238;&#24402;&#12289;&#20998;&#31867;&#31561;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#19968;&#32452;&#22270;&#30340;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#65292;&#22914;&#20122;&#32467;&#26500;&#12290;&#20294;&#36825;&#20123;&#26041;&#27861;&#34429;&#28982;&#21463;&#30410;&#20110;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#24120;&#24120;&#22240;&#26080;&#27861;&#36991;&#20813;&#22270;&#21516;&#26500;&#38382;&#39064;&#32780;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#21644;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#65292;&#24110;&#21161;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#28145;&#24230;&#22270;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#36896;&#25104;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#20840;&#38754;&#30340;&#32508;&#36848;&#26469;&#22238;&#39038;&#20174;&#20256;&#32479;&#23398;&#20064;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22270;&#23884;&#20837;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs have a superior ability to represent relational data, like chemical compounds, proteins, and social networks. Hence, graph-level learning, which takes a set of graphs as input, has been applied to many tasks including comparison, regression, classification, and more. Traditional approaches to learning a set of graphs heavily rely on hand-crafted features, such as substructures. But while these methods benefit from good interpretability, they often suffer from computational bottlenecks as they cannot skirt the graph isomorphism problem. Conversely, deep learning has helped graph-level learning adapt to the growing scale of graphs by extracting features automatically and encoding graphs into low-dimensional representations. As a result, these deep graph learning methods have been responsible for many successes. Yet, there is no comprehensive survey that reviews graph-level learning starting with traditional learning and moving through to the deep learning approaches. This article 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#65288;QTD&#65289;&#22312;&#19968;&#23450;&#29366;&#24577;&#19979;&#30340;&#25910;&#25947;&#27010;&#29575;&#20026;1&#65292;&#24314;&#31435;&#20102;QTD&#19982;&#38750;&#32447;&#24615;&#24494;&#20998;&#21253;&#21547;&#24335;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2301.04462</link><description>&lt;p&gt;
&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Quantile Temporal-Difference Learning. (arXiv:2301.04462v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#65288;QTD&#65289;&#22312;&#19968;&#23450;&#29366;&#24577;&#19979;&#30340;&#25910;&#25947;&#27010;&#29575;&#20026;1&#65292;&#24314;&#31435;&#20102;QTD&#19982;&#38750;&#32447;&#24615;&#24494;&#20998;&#21253;&#21547;&#24335;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#65288;QTD&#65289;&#65292;&#35813;&#31639;&#27861;&#24050;&#25104;&#20026;&#22810;&#20010;&#25104;&#21151;&#30340;&#24378;&#21270;&#23398;&#20064;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23613;&#31649;&#22312;&#23454;&#35777;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;QTD&#30340;&#29702;&#35770;&#35748;&#35782;&#19968;&#30452;&#38590;&#20197;&#25417;&#25720;&#12290;&#19982;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#38543;&#26426;&#36924;&#36817;&#24037;&#20855;&#26469;&#36827;&#34892;&#20998;&#26512;&#30340;&#32463;&#20856;TD&#23398;&#20064;&#19981;&#21516;&#65292;QTD&#30340;&#26356;&#26032;&#24182;&#19981;&#36817;&#20284;&#20110;&#25910;&#32553;&#31639;&#23376;&#65292;&#39640;&#24230;&#38750;&#32447;&#24615;&#24182;&#19988;&#21487;&#33021;&#20855;&#26377;&#22810;&#20010;&#19981;&#21160;&#28857;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#32467;&#26524;&#26159;&#35777;&#26126;&#22312;&#19982;&#19968;&#31867;&#21160;&#24577;&#35268;&#21010;&#31243;&#24207;&#30340;&#19981;&#21160;&#28857;&#30456;&#24212;&#30340;&#29366;&#24577;&#19979;&#65292;QTD&#30340;&#25910;&#25947;&#27010;&#29575;&#20026;1&#65292;&#20174;&#32780;&#35753;QTD&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#30830;&#23450;&#24615;&#30340;&#22522;&#30784;&#12290;&#35777;&#26126;&#36890;&#36807;&#38543;&#26426;&#36924;&#36817;&#29702;&#35770;&#21644;&#38750;&#20809;&#28369;&#20998;&#26512;&#23558;QTD&#19982;&#38750;&#32447;&#24615;&#24494;&#20998;&#21253;&#21547;&#24335;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyse quantile temporal-difference learning (QTD), a distributional reinforcement learning algorithm that has proven to be a key component in several successful large-scale applications of reinforcement learning. Despite these empirical successes, a theoretical understanding of QTD has proven elusive until now. Unlike classical TD learning, which can be analysed with standard stochastic approximation tools, QTD updates do not approximate contraction mappings, are highly non-linear, and may have multiple fixed points. The core result of this paper is a proof of convergence to the fixed points of a related family of dynamic programming procedures with probability 1, putting QTD on firm theoretical footing. The proof establishes connections between QTD and non-linear differential inclusions through stochastic approximation theory and non-smooth analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.00815</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23156;&#20799;&#33041;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A attention way in Explainable methods for infant brain. (arXiv:2301.00815v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an explainable geometric deep network that enhances discriminative representation extraction by end-to-end learning of explanation factors, which is a more intuitive strategy to inversely assure fine-grained explainability, suitable for high-dimensional data in neuroimaging and neuroscience studies containing noisy, redundant, and task-irrelevant information.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#23398;&#31185;&#24212;&#29992;&#20013;&#37096;&#32626;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#20934;&#30830;&#19988;&#65288;&#26356;&#37325;&#35201;&#30340;&#26159;&#65289;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20197;&#20107;&#21518;&#26041;&#24335;&#35299;&#37322;&#32593;&#32476;&#36755;&#20986;&#65292;&#38544;&#21547;&#22320;&#20551;&#35774;&#24544;&#23454;&#30340;&#35299;&#37322;&#26469;&#33258;&#20934;&#30830;&#30340;&#39044;&#27979;/&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30456;&#21453;&#30340;&#35266;&#28857;&#65292;&#21363;&#35299;&#37322;&#25552;&#21319;&#65288;&#29978;&#33267;&#20915;&#23450;&#65289;&#20998;&#31867;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#21487;&#33021;&#26159;&#19968;&#31181;&#26356;&#30452;&#35266;&#30340;&#31574;&#30053;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20363;&#22914;&#22312;&#37027;&#20123;&#21253;&#21547;&#22122;&#22768;&#65292;&#20887;&#20313;&#21644;&#20219;&#21153;&#26080;&#20851;&#20449;&#24687;&#30340;&#39640;&#32500;&#25968;&#25454;&#30340;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying reliable deep learning techniques in interdisciplinary applications needs learned models to output accurate and ({even more importantly}) explainable predictions. Existing approaches typically explicate network outputs in a post-hoc fashion, under an implicit assumption that faithful explanations come from accurate predictions/classifications. We have an opposite claim that explanations boost (or even determine) classification. That is, end-to-end learning of explanation factors to augment discriminative representation extraction could be a more intuitive strategy to inversely assure fine-grained explainability, e.g., in those neuroimaging and neuroscience studies with high-dimensional data containing noisy, redundant, and task-irrelevant information. In this paper, we propose such an explainable geometric deep network dubbed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#21333;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#30001;&#35838;&#31243;&#23450;&#20041;&#30340;&#22810;&#20219;&#21153;&#38382;&#39064;&#65292;&#35777;&#26126;&#22312;&#35838;&#31243;&#26377;&#36731;&#24494;&#27491;&#21017;&#21270;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#20381;&#27425;&#35299;&#20915;&#27599;&#20010;&#20219;&#21153;&#27604;&#30452;&#25509;&#35299;&#20915;&#21407;&#22987;&#21333;&#20219;&#21153;&#26356;&#21152;&#35745;&#31639;&#19978;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2212.12809</link><description>&lt;p&gt;
&#36890;&#36807;&#35838;&#31243;&#29702;&#35299;&#21333;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24230;&#25910;&#30410;
&lt;/p&gt;
&lt;p&gt;
Understanding the Complexity Gains of Single-Task RL with a Curriculum. (arXiv:2212.12809v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#21333;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#30001;&#35838;&#31243;&#23450;&#20041;&#30340;&#22810;&#20219;&#21153;&#38382;&#39064;&#65292;&#35777;&#26126;&#22312;&#35838;&#31243;&#26377;&#36731;&#24494;&#27491;&#21017;&#21270;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#20381;&#27425;&#35299;&#20915;&#27599;&#20010;&#20219;&#21153;&#27604;&#30452;&#25509;&#35299;&#20915;&#21407;&#22987;&#21333;&#20219;&#21153;&#26356;&#21152;&#35745;&#31639;&#19978;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#32463;&#36807;&#33391;&#22909;&#35774;&#35745;&#30340;&#22870;&#21169;&#26426;&#21046;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#25552;&#20986;&#20351;&#29992;&#19987;&#38376;&#30340;&#25506;&#32034;&#31574;&#30053;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#21478;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#23558;&#20854;&#37325;&#26032;&#26500;&#36896;&#20026;&#19968;&#20010;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20219;&#21153;&#31354;&#38388;&#19981;&#20165;&#21253;&#21547;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#36824;&#21253;&#21547;&#38544;&#21547;&#30340;&#35838;&#31243;&#20316;&#20026;&#36741;&#21161;&#12290;&#36825;&#26679;&#30340;&#37325;&#26032;&#26500;&#36896;&#25171;&#24320;&#20102;&#20351;&#29992;&#29616;&#26377;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#35299;&#20915;&#21333;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#21333;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#30001;&#35838;&#31243;&#23450;&#20041;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#35838;&#31243;&#26377;&#36731;&#24494;&#27491;&#21017;&#21270;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20381;&#27425;&#35299;&#20915;&#22810;&#20219;&#21153;RL&#38382;&#39064;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#27604;&#20174;&#22836;&#24320;&#22987;&#35299;&#20915;&#21407;&#22987;&#21333;&#20219;&#21153;&#38382;&#39064;&#26356;&#21152;&#35745;&#31639;&#19978;&#39640;&#25928;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#25506;&#32034;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) problems can be challenging without well-shaped rewards. Prior work on provably efficient RL methods generally proposes to address this issue with dedicated exploration strategies. However, another way to tackle this challenge is to reformulate it as a multi-task RL problem, where the task space contains not only the challenging task of interest but also easier tasks that implicitly function as a curriculum. Such a reformulation opens up the possibility of running existing multi-task RL methods as a more efficient alternative to solving a single challenging task from scratch. In this work, we provide a theoretical framework that reformulates a single-task RL problem as a multi-task RL problem defined by a curriculum. Under mild regularity conditions on the curriculum, we show that sequentially solving each task in the multi-task RL problem is more computationally efficient than solving the original single-task problem, without any explicit exploration bonuse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20272;&#31639;&#25968;&#25454;&#27969;&#24418;&#30340;&#32500;&#24230;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2212.12611</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26263;&#20013;&#35782;&#21035;&#25968;&#25454;&#27969;&#24418;&#30340;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Your diffusion model secretly knows the dimension of the data manifold. (arXiv:2212.12611v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20272;&#31639;&#25968;&#25454;&#27969;&#24418;&#30340;&#32500;&#24230;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35757;&#32451;&#36807;&#30340;&#25193;&#25955;&#27169;&#22411;&#20272;&#31639;&#25968;&#25454;&#27969;&#24418;&#32500;&#24230;&#30340;&#26032;&#26694;&#26550;&#12290;&#25193;&#25955;&#27169;&#22411;&#36880;&#28176;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#26799;&#24230;&#65292;&#21363;&#22122;&#22768;&#27745;&#26579;&#29256;&#26412;&#30340;&#23545;&#25968;&#23494;&#24230;&#30340;&#26799;&#24230;&#65292;&#19981;&#21516;&#32423;&#21035;&#30340;&#27745;&#26579;&#31243;&#24230;&#23545;&#24212;&#19981;&#21516;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#25968;&#25454;&#38598;&#32858;&#28966;&#20110;&#39640;&#32500;&#29615;&#22659;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#27969;&#24418;&#65292;&#37027;&#20040;&#38543;&#30528;&#22122;&#22768;&#27745;&#26579;&#31243;&#24230;&#30340;&#38477;&#20302;&#65292;&#26799;&#24230;&#20250;&#25351;&#21521;&#27969;&#24418;&#65292;&#22240;&#20026;&#36825;&#20010;&#26041;&#21521;&#26159;&#26368;&#22823;&#20284;&#28982;&#22686;&#21152;&#30340;&#26041;&#21521;&#12290;&#22240;&#27492;&#65292;&#22312;&#27745;&#26579;&#31243;&#24230;&#36739;&#20302;&#26102;&#65292;&#25193;&#25955;&#27169;&#22411;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#27969;&#24418;&#27491;&#24120;&#21521;&#37327;&#30340;&#36924;&#36817;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20272;&#35745;&#20999;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;&#20063;&#23601;&#26159;&#25968;&#25454;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#27969;&#24418;&#32500;&#24230;&#30340;&#31532;&#19968;&#20010;&#20272;&#31639;&#22120;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#24050;&#32463;&#25104;&#29087;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel framework for estimating the dimension of the data manifold using a trained diffusion model. A diffusion model approximates the score function i.e. the gradient of the log density of a noise-corrupted version of the target distribution for varying levels of corruption. We prove that, if the data concentrates around a manifold embedded in the high-dimensional ambient space, then as the level of corruption decreases, the score function points towards the manifold, as this direction becomes the direction of maximal likelihood increase. Therefore, for small levels of corruption, the diffusion model provides us with access to an approximation of the normal bundle of the data manifold. This allows us to estimate the dimension of the tangent space, thus, the intrinsic dimension of the data manifold. To the best of our knowledge, our method is the first estimator of the data manifold dimension based on diffusion models and it outperforms well established statis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10823</link><description>&lt;p&gt;
&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#20381;&#36182;&#32467;&#26500;&#21270;&#27880;&#37322;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#26399;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#30340;RE&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#36890;&#36807;RE&#30446;&#26631;&#39044;&#35757;&#32451;&#20851;&#31995;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23427;&#38459;&#27490;RE&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#26088;&#22312;&#24357;&#21512;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#12290;&#30001;&#20110;&#22312;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20851;&#31995;&#21487;&#33021;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36731;&#26494;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#65292;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#25991;&#26723;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#26174;&#30528;&#25552;&#39640;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;BLOOM&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#65292;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#35821;&#35328;&#19978;&#65292;&#24182;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25552;&#31034;&#24615;&#33021;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2212.09535</link><description>&lt;p&gt;
BLOOM+1&#65306;&#20026;&#38646;&#26679;&#26412;&#25552;&#31034;&#28155;&#21152;&#35821;&#35328;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting. (arXiv:2212.09535v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;BLOOM&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#65292;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#35821;&#35328;&#19978;&#65292;&#24182;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25552;&#31034;&#24615;&#33021;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLOOM&#27169;&#22411;&#26159;&#19968;&#20010;&#22823;&#22411;&#20844;&#24320;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#20854;&#39044;&#35757;&#32451;&#20165;&#38480;&#20110;46&#31181;&#35821;&#35328;&#12290;&#20026;&#20102;&#23558;BLOOM&#30340;&#22909;&#22788;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#36807;&#39640;&#30340;&#25104;&#26412;&#65292;&#26377;&#24517;&#35201;&#23558;BLOOM&#36866;&#24212;&#21040;&#26032;&#30340;&#35821;&#35328;&#19978;&#12290;&#26412;&#25991;&#23558;&#29616;&#26377;&#30340;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#24212;&#29992;&#20110;BLOOM&#65292;&#24182;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#23545;&#20854;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#36866;&#24212;&#23545;&#20110;&#25552;&#39640;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#26159;&#26377;&#25928;&#30340;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#24615;&#33021;&#19981;&#20250;&#21463;&#21040;&#35821;&#35328;&#29305;&#23450;&#24615;&#30340;&#26174;&#30528;&#24433;&#21709;&#65292;&#22914;&#20070;&#20889;&#31995;&#32479;&#12290;&#23427;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;&#25105;&#20204;&#36824;&#21521;BLOOMZ&#28155;&#21152;&#20102;&#26032;&#35821;&#35328;&#65292;&#36825;&#26159;BLOOM&#30340;&#22810;&#20219;&#21153;&#24494;&#35843;&#29256;&#26412;&#65292;&#33021;&#22815;&#36319;&#38543;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2212.08049</link><description>&lt;p&gt;
&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;
&lt;/p&gt;
&lt;p&gt;
Sliced Optimal Partial Transport. (arXiv:2212.08049v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21464;&#24471;&#26497;&#20854;&#27969;&#34892;&#12290;OT&#38382;&#39064;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#28304;&#21644;&#30446;&#26631;&#27979;&#24230;&#30340;&#24635;&#36136;&#37327;&#30456;&#31561;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#65288;OPT&#65289;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#30340;&#26041;&#27861;&#12290;&#19982;OT&#38382;&#39064;&#31867;&#20284;&#65292;OPT&#30340;&#35745;&#31639;&#20381;&#36182;&#20110;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;&#36890;&#24120;&#22312;&#39640;&#32500;&#24230;&#20013;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#35745;&#31639;&#19978;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;OPT&#38382;&#39064;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#36981;&#24490;&#20999;&#29255;OT&#36317;&#31163;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#21033;&#29992;&#20999;&#29255;&#23450;&#20041;&#20102;&#20999;&#29255;OPT&#36317;&#31163;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20999;&#29255;OPT-based&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#20540;&#23454;&#39564;&#20013;&#30340;&#35745;&#31639;&#21644;&#31934;&#24230;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;Sliced-OPT&#22312;&#22122;&#22768;&#28857;&#20113;&#37197;&#20934;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;CALIME&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#29420;&#31435;&#24615;&#30340;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#21021;&#22987;&#26041;&#27861;&#30340;&#40657;&#30418;&#27169;&#22411;&#27169;&#25311;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#31283;&#23450;&#24615;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.05256</link><description>&lt;p&gt;
CALIME: &#22240;&#26524;&#24863;&#30693;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;-&#26080;&#20851;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
CALIME: Causality-Aware Local Interpretable Model-Agnostic Explanations. (arXiv:2212.05256v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;CALIME&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#29420;&#31435;&#24615;&#30340;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#21021;&#22987;&#26041;&#27861;&#30340;&#40657;&#30418;&#27169;&#22411;&#27169;&#25311;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#31283;&#23450;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#32570;&#28857;&#26159;&#20551;&#35774;&#29305;&#24449;&#29420;&#31435;&#24615;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#22686;&#21152;&#20449;&#20219;&#24182;&#24110;&#21161;&#29992;&#25143;&#35780;&#20272;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#26126;&#30830;&#22320;&#22312;&#22260;&#32469;&#36755;&#20837;&#23454;&#20363;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#32534;&#30721;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#37322;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#20223;&#40657;&#30418;&#23376;&#30340;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#26041;&#38754;&#22343;&#27604;&#21021;&#22987;&#26041;&#27861;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant drawback of eXplainable Artificial Intelligence (XAI) approaches is the assumption of feature independence. This paper focuses on integrating causal knowledge in XAI methods to increase trust and help users assess explanations' quality. We propose a novel extension to a widely used local and model-agnostic explainer that explicitly encodes causal relationships in the data generated around the input instance to explain. Extensive experiments show that our method achieves superior performance comparing the initial one for both the fidelity in mimicking the black-box and the stability of the explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26080;&#26684;&#26629;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;&#30340;&#26368;&#32456;&#21518;&#39564;&#36755;&#20986;&#65292;&#19982;&#20351;&#29992;N-best&#21015;&#34920;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#35757;&#32451;&#26399;&#38388;&#28040;&#38500;&#20102;&#20551;&#35774;&#29983;&#25104;&#30340;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#22312;&#21333;&#35789;&#38169;&#35823;&#29575;&#19978;&#34920;&#29616;&#20063;&#26377;6.5&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.04325</link><description>&lt;p&gt;
&#26080;&#26684;&#26629;&#24207;&#21015;&#37492;&#21035;&#35757;&#32451;&#29992;&#20110;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;
&lt;/p&gt;
&lt;p&gt;
Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers. (arXiv:2212.04325v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26080;&#26684;&#26629;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;&#30340;&#26368;&#32456;&#21518;&#39564;&#36755;&#20986;&#65292;&#19982;&#20351;&#29992;N-best&#21015;&#34920;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#35757;&#32451;&#26399;&#38388;&#28040;&#38500;&#20102;&#20551;&#35774;&#29983;&#25104;&#30340;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#22312;&#21333;&#35789;&#38169;&#35823;&#29575;&#19978;&#34920;&#29616;&#20063;&#26377;6.5&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;RNN-Transducer&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#26080;&#26684;&#26629;&#24207;&#21015;&#37492;&#21035;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#33719;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;RNN-Transducer&#20013;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26080;&#26684;&#26629;&#35757;&#32451;&#30446;&#26631;&#65292;&#21363;&#26080;&#26684;&#26629;&#26368;&#22823;&#20114;&#20449;&#24687;&#12289;&#26080;&#26684;&#26629;&#27573;&#32423;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#21644;&#26080;&#26684;&#26629;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65292;&#29992;&#20110;&#20855;&#26377;&#26377;&#38480;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#30340;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;&#30340;&#26368;&#32456;&#21518;&#39564;&#36755;&#20986;&#12290;&#19982;&#20351;&#29992;N-best&#21015;&#34920;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#35757;&#32451;&#26399;&#38388;&#28040;&#38500;&#20102;&#20551;&#35774;&#29983;&#25104;&#30340;&#35299;&#30721;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24207;&#21015;&#32423;&#20132;&#21449;&#29109;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#21333;&#35789;&#38169;&#35823;&#29575;&#19978;&#33719;&#24471;&#20102;&#39640;&#36798;6.5&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#19982;&#22522;&#20110;N-best&#21015;&#34920;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#30446;&#26631;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;N-best&#21015;&#34920;&#20013;&#20855;&#26377;&#19968;&#20123;&#22122;&#22768;&#21644;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, RNN-Transducers have achieved remarkable results on various automatic speech recognition tasks. However, lattice-free sequence discriminative training methods, which obtain superior performance in hybrid models, are rarely investigated in RNN-Transducers. In this work, we propose three lattice-free training objectives, namely lattice-free maximum mutual information, lattice-free segment-level minimum Bayes risk, and lattice-free minimum Bayes risk, which are used for the final posterior output of the phoneme-based neural transducer with a limited context dependency. Compared to criteria using N-best lists, lattice-free methods eliminate the decoding step for hypotheses generation during training, which leads to more efficient training. Experimental results show that lattice-free methods gain up to 6.5% relative improvement in word error rate compared to a sequence-level cross-entropy trained model. Compared to the N-best-list based minimum Bayes risk objectives, lattice-free 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30495;&#23454;&#35821;&#38899;&#21644;&#21512;&#25104;&#35821;&#38899;&#30340;&#20998;&#24067;&#65292;&#20351;&#29992;&#32479;&#35745;&#23398;&#26041;&#27861;&#37327;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;10%&#30340;&#36317;&#31163;&#32553;&#23567;&#12290;</title><link>http://arxiv.org/abs/2211.16049</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#32553;&#23567;&#21512;&#25104;&#35821;&#38899;&#21644;&#30495;&#23454;&#35821;&#38899;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Evaluating and reducing the distance between synthetic and real speech distributions. (arXiv:2211.16049v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30495;&#23454;&#35821;&#38899;&#21644;&#21512;&#25104;&#35821;&#38899;&#30340;&#20998;&#24067;&#65292;&#20351;&#29992;&#32479;&#35745;&#23398;&#26041;&#27861;&#37327;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;10%&#30340;&#36317;&#31163;&#32553;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#20195;TTS&#31995;&#32479;&#21487;&#20197;&#20135;&#29983;&#33258;&#28982;&#27969;&#30021;&#30340;&#35821;&#38899;&#65292;&#20294;&#23427;&#20204;&#20173;&#26080;&#27861;&#22797;&#29616;&#33258;&#28982;&#35821;&#38899;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#20840;&#37096;&#22810;&#26679;&#24615;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#32452;&#21457;&#38899;&#32773;&#25152;&#33021;&#20135;&#29983;&#30340;&#25152;&#26377;&#21487;&#33021;&#30495;&#23454;&#35821;&#38899;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#20351;&#29992;&#29305;&#23450;TTS&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#30340;&#25152;&#26377;&#21512;&#25104;&#26679;&#26412;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#19982;&#21457;&#38899;&#32773;&#23646;&#24615;&#12289;&#35821;&#38899;&#38901;&#24459;&#21644;&#22768;&#23398;&#29615;&#22659;&#30456;&#20851;&#30340;&#35805;&#35821;&#27700;&#24179;&#32479;&#35745;&#20449;&#24687;&#26469;&#37327;&#21270;&#30495;&#23454;&#35821;&#38899;&#21644;&#21512;&#25104;&#35821;&#38899;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#20351;&#29992;Wasserstein&#36317;&#31163;&#35780;&#20272;&#36825;&#20123;&#32479;&#35745;&#20449;&#24687;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#26102;&#25552;&#20379;&#22522;&#20934;&#20540;&#65292;&#25105;&#20204;&#32553;&#23567;&#20102;&#36825;&#20123;&#36317;&#31163;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26469;&#37327;&#21270;&#25972;&#20307;&#20998;&#24067;&#36317;&#31163;&#30340;&#25913;&#36827;&#24773;&#20917;&#12290;&#22312;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#20013;&#65292;&#20998;&#24067;&#36317;&#31163;&#32553;&#23567;&#20102;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While modern Text-to-Speech (TTS) systems can produce natural-sounding speech, they remain unable to reproduce the full diversity found in natural speech data. We consider the distribution of all possible real speech samples that could be generated by these speakers alongside the distribution of all synthetic samples that could be generated for the same set of speakers, using a particular TTS system. We set out to quantify the distance between real and synthetic speech via a range of utterance-level statistics related to properties of the speaker, speech prosody and acoustic environment. Differences in the distribution of these statistics are evaluated using the Wasserstein distance. We reduce these distances by providing ground-truth values at generation time, and quantify the improvements to the overall distribution distance, approximated using an automatic speech recognition system. Our best system achieves a 10\% reduction in distribution distance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#36135;&#24065;&#23450;&#20215;&#26041;&#26696;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#25972;&#20154;&#24037;&#36135;&#24065;&#25910;&#36153;&#30340;&#37325;&#22797;&#21338;&#24328;&#35774;&#32622;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#31995;&#32479;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.14793</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#36135;&#24065;&#20248;&#21270;&#36335;&#30001;&#30340;&#23450;&#20215;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Data-driven Pricing Scheme for Optimal Routing through Artificial Currencies. (arXiv:2211.14793v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#36135;&#24065;&#23450;&#20215;&#26041;&#26696;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#25972;&#20154;&#24037;&#36135;&#24065;&#25910;&#36153;&#30340;&#37325;&#22797;&#21338;&#24328;&#35774;&#32622;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#31995;&#32479;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#31995;&#32479;&#36890;&#24120;&#22240;&#33258;&#31169;&#29992;&#25143;&#30340;&#19981;&#21463;&#25511;&#21046;&#34892;&#20026;&#32780;&#36973;&#21463;&#39640;&#20195;&#20215;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#25104;&#26412;&#30456;&#27604;&#30001;&#38598;&#20013;&#25511;&#21046;&#31995;&#32479;&#20248;&#21270;&#25511;&#21046;&#22120;&#25152;&#33021;&#23454;&#29616;&#30340;&#25104;&#26412;&#35201;&#39640;&#24471;&#22810;&#12290;&#36135;&#24065;&#25910;&#36153;&#26041;&#26696;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#33258;&#31169;&#29992;&#25143;&#30340;&#34892;&#20026;&#19982;&#31995;&#32479;&#26368;&#20248;&#30456;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#22312;&#25910;&#20837;&#26041;&#38754;&#27495;&#35270;&#20154;&#21475;&#12290;&#20154;&#24037;&#36135;&#24065;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#35777;&#20154;&#21475;&#30340;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#26159;&#22522;&#20110;&#34892;&#20026;&#27169;&#22411;&#30340;&#65292;&#21487;&#33021;&#19982;&#23454;&#38469;&#23454;&#29616;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37325;&#22797;&#21338;&#24328;&#35774;&#32622;&#20013;&#33258;&#21160;&#35843;&#25972;&#20154;&#24037;&#36135;&#24065;&#25910;&#36153;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#19968;&#31181;&#24179;&#34892;&#24359;&#35774;&#32622;&#65292;&#29992;&#25143;&#27599;&#22825;&#20174;&#20010;&#20154;&#36215;&#28857;&#21040;&#20010;&#20154;&#32456;&#28857;&#36890;&#21220;&#65292;&#36873;&#25321;&#19968;&#26465;&#36335;&#32447;&#65292;&#20132;&#25442;&#19968;&#31181;&#20154;&#24037;&#36135;&#24065;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobility systems often suffer from a high price of anarchy due to the uncontrolled behavior of selfish users. This may result in societal costs that are significantly higher compared to what could be achieved by a centralized system-optimal controller. Monetary tolling schemes can effectively align the behavior of selfish users with the system-optimum. Yet, they inevitably discriminate the population in terms of income. Artificial currencies were recently presented as an effective alternative that can achieve the same performance, whilst guaranteeing fairness among the population. However, those studies were based on behavioral models that may differ from practical implementations. This paper presents a data-driven approach to automatically adapt artificial-currency tolls within repetitive-game settings. We first consider a parallel-arc setting whereby users commute on a daily basis from an individual origin to an individual destination, choosing a route in exchange of an artificial-cu
&lt;/p&gt;</description></item><item><title>MEGAN&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#22810;&#20010;&#36890;&#36947;&#20013;&#20135;&#29983;&#33410;&#28857;&#21644;&#36793;&#30340;&#35828;&#26126;&#24615;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23545;&#20110;&#25913;&#36827;&#22270;&#22238;&#24402;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#23427;&#26159;&#23436;&#20840;&#21487;&#24494;&#30340;&#65292;&#21487;&#20197;&#22312;&#35299;&#37322;&#30417;&#30563;&#26041;&#24335;&#19979;&#20027;&#21160;&#22320;&#35757;&#32451;&#35828;&#26126;&#12290;</title><link>http://arxiv.org/abs/2211.13236</link><description>&lt;p&gt;
MEGAN: &#22810;&#35299;&#37322;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MEGAN: Multi-Explanation Graph Attention Network. (arXiv:2211.13236v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13236
&lt;/p&gt;
&lt;p&gt;
MEGAN&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#22810;&#20010;&#36890;&#36947;&#20013;&#20135;&#29983;&#33410;&#28857;&#21644;&#36793;&#30340;&#35828;&#26126;&#24615;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23545;&#20110;&#25913;&#36827;&#22270;&#22238;&#24402;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#23427;&#26159;&#23436;&#20840;&#21487;&#24494;&#30340;&#65292;&#21487;&#20197;&#22312;&#35299;&#37322;&#30417;&#30563;&#26041;&#24335;&#19979;&#20027;&#21160;&#22320;&#35757;&#32451;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35299;&#37322;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;MEGAN&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22270;&#21487;&#35299;&#37322;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#21487;&#20197;&#27839;&#22810;&#20010;&#36890;&#36947;&#20135;&#29983;&#33410;&#28857;&#21644;&#36793;&#30340;&#35828;&#26126;&#24615;&#35299;&#37322;&#65292;&#20854;&#25968;&#37327;&#29420;&#31435;&#20110;&#20219;&#21153;&#35268;&#26684;&#35828;&#26126;&#12290;&#36825;&#23545;&#20110;&#25913;&#36827;&#22270;&#22238;&#24402;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#21487;&#20197;&#23558;&#35299;&#37322;&#20998;&#20026;&#30456;&#23545;&#20110;&#21442;&#32771;&#20540;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32593;&#32476;&#26159;&#23436;&#20840;&#21487;&#24494;&#30340;&#65292;&#21487;&#20197;&#22312;&#35299;&#37322;&#30417;&#30563;&#26041;&#24335;&#19979;&#20027;&#21160;&#22320;&#35757;&#32451;&#35828;&#26126;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#24050;&#30693;&#22320;&#38754;&#30495;&#30456;&#35828;&#26126;&#30340;&#21512;&#25104;&#22270;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#21333;&#19968;&#21644;&#22810;&#35299;&#37322;&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#22312;&#35299;&#37322;&#30417;&#30563;&#26399;&#38388;&#23454;&#29616;&#20102;&#25509;&#36817;&#23436;&#32654;&#30340;&#35299;&#37322;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-explanation graph attention network (MEGAN). Unlike existing graph explainability methods, our network can produce node and edge attributional explanations along multiple channels, the number of which is independent of task specifications. This proves crucial to improve the interpretability of graph regression predictions, as explanations can be split into positive and negative evidence w.r.t to a reference value. Additionally, our attention-based network is fully differentiable and explanations can actively be trained in an explanation-supervised manner. We first validate our model on a synthetic graph regression dataset with known ground-truth explanations. Our network outperforms existing baseline explainability methods for the single- as well as the multi-explanation case, achieving near-perfect explanation accuracy during explanation supervision. Finally, we demonstrate our model's capabilities on multiple real-world datasets. We find that our model produces spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;</title><link>http://arxiv.org/abs/2211.08486</link><description>&lt;p&gt;
&#38646;&#20559;&#32622;&#26631;&#37327;&#19981;&#21464;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Scalar Invariant Networks with Zero Bias. (arXiv:2211.08486v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26435;&#37325;&#19968;&#26679;&#65292;&#20559;&#32622;&#39033;&#20063;&#26159;&#35768;&#22810;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;(&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;)&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#20154;&#20204;&#35748;&#20026;&#20559;&#24046;&#33021;&#26377;&#25928;&#22320;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#33021;&#21147;&#26469;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#25105;&#20204;&#20174;&#31532;&#19968;&#21407;&#29702;&#32771;&#34385;&#22270;&#20687;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#20998;&#24067;&#20197;&#21450;&#27169;&#22411;&#24212;&#20855;&#26377;&#30340;&#19968;&#20123;&#26399;&#26395;&#29305;&#24615;&#65292;&#21017;&#20559;&#24046;&#21487;&#20197;&#23436;&#20840;&#24573;&#30053;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21487;&#33021;&#19982;&#24102;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31216;&#20026;&#26631;&#37327;(&#20056;&#27861;)&#19981;&#21464;&#24615;&#30340;&#33391;&#22909;&#23646;&#24615;&#65292;&#36825;&#20351;&#24471;&#24403;&#25913;&#21464;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#27604;&#24230;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26631;&#37327;&#19981;&#21464;&#24615;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#8230;
&lt;/p&gt;
&lt;p&gt;
Just like weights, bias terms are the learnable parameters of many popular machine learning models, including neural networks. Biases are believed to effectively increase the representational power of neural networks to solve a wide range of tasks in computer vision. However, we argue that if we consider the intrinsic distribution of images in the input space as well as some desired properties a model should have from the first principles, biases can be completely ignored in addressing many image-related tasks, such as image classification. Our observation indicates that zero-bias neural networks could perform comparably to neural networks with bias at least on practical image classification tasks. In addition, we prove that zero-bias neural networks possess a nice property called scalar (multiplication) invariance, which allows the prediction of neural networks remains the same when altering the contrast of the input image. We then extend scalar invariance to more general cases that a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;&#26041;&#27861;&#65288;NBW&#65289;&#65292;&#36890;&#36807;&#20248;&#21270; $f$ -&#20998;&#24067;&#30340;&#21464;&#20998;&#34920;&#31034;&#65292;&#30452;&#25509;&#20272;&#35745;&#28304;&#21644;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#30340;&#23494;&#24230;&#27604;&#65292;&#33719;&#24471;&#20102;&#26435;&#37325;&#65292;&#29992;&#20110;&#20272;&#35745;&#20219;&#24847;&#28151;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.07533</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Generalized Balancing Weights via Deep Neural Networks. (arXiv:2211.07533v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;&#26041;&#27861;&#65288;NBW&#65289;&#65292;&#36890;&#36807;&#20248;&#21270; $f$ -&#20998;&#24067;&#30340;&#21464;&#20998;&#34920;&#31034;&#65292;&#30452;&#25509;&#20272;&#35745;&#28304;&#21644;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#30340;&#23494;&#24230;&#27604;&#65292;&#33719;&#24471;&#20102;&#26435;&#37325;&#65292;&#29992;&#20110;&#20272;&#35745;&#20219;&#24847;&#28151;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#26159;&#35768;&#22810;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20013;&#24515;&#38382;&#39064;&#12290;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#21327;&#21464;&#37327;&#30340;&#26435;&#37325;&#65292;&#20351;&#24471;&#25968;&#25454;&#30340;&#20998;&#24067;&#31867;&#20284;&#20110;&#38543;&#26426;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#24179;&#34913;&#26435;&#37325;&#65288;NBW&#65289;&#30340;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;&#65292;&#20197;&#20272;&#35745;&#20219;&#24847;&#28151;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#20248;&#21270; $f$ -&#20998;&#24067;&#30340;&#21464;&#20998;&#34920;&#31034;&#65292;&#30452;&#25509;&#20272;&#35745;&#28304;&#21644;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#30340;&#23494;&#24230;&#27604;&#65292;&#33719;&#24471;&#20102;&#26435;&#37325;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102; $\alpha$-&#24046;&#24322;&#20316;&#20026;&#20248;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#20854;&#22320;&#38754;&#23454;&#20917;&#20540;&#21644;&#26080;&#20559;&#23567;&#25209;&#37327;&#26799;&#24230;&#30340;&#20272;&#35745;&#22120;&#65292;&#32780;&#19988;&#23545;&#20110;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20197;&#19979;&#20004;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#24179;&#34913;&#26435;&#37325;&#65306;&#25552;&#39640;&#24179;&#34913;&#26435;&#37325;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26816;&#26597;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating causal effects from observational data is a central problem in many domains. A general approach is to balance covariates with weights such that the distribution of the data mimics randomization. We present generalized balancing weights, Neural Balancing Weights (NBW), to estimate the causal effects of an arbitrary mixture of discrete and continuous interventions. The weights were obtained through direct estimation of the density ratio between the source and balanced distributions by optimizing the variational representation of $f$-divergence. For this, we selected $\alpha$-divergence as it presents efficient optimization because it has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients; moreover, it is advantageous for the vanishing-gradient problem. In addition, we provide the following two methods for estimating the balancing weights: improving the generalization performance of the balancing weights and checking 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#32780;&#21516;&#27493;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;FedCL&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#29992;&#25143;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#36718;&#20013;&#20027;&#21160;&#21644;&#21516;&#27493;&#22320;&#23433;&#25490;&#29992;&#25143;&#23398;&#20064;&#36895;&#24230;&#65292;&#23558;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#36817;&#20284;&#20026;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2211.07248</link><description>&lt;p&gt;
FedCL&#65306;&#32852;&#37030;&#22810;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65292;&#20197;&#21516;&#27493;&#30456;&#20851;&#29992;&#25143;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
FedCL: Federated Multi-Phase Curriculum Learning to Synchronously Correlate User Heterogeneity. (arXiv:2211.07248v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#32780;&#21516;&#27493;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;FedCL&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#29992;&#25143;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#36718;&#20013;&#20027;&#21160;&#21644;&#21516;&#27493;&#22320;&#23433;&#25490;&#29992;&#25143;&#23398;&#20064;&#36895;&#24230;&#65292;&#23558;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#36817;&#20284;&#20026;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#20840;&#23616;&#27169;&#22411;&#36845;&#20195;&#22320;&#25910;&#38598;&#26412;&#22320;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#35775;&#38382;&#20854;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#22788;&#29702;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#28418;&#31227;&#65292;&#20174;&#32780;&#38590;&#20197;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24403;&#21069;&#26041;&#27861;&#37319;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#22914;&#30693;&#35782;&#33976;&#39311;&#12289;&#21152;&#26435;&#27169;&#22411;&#32858;&#21512;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#31216;&#20026;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24050;&#32463;&#21457;&#29983;&#25110;&#34987;&#20302;&#20272;&#27169;&#22411;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#26657;&#20934;&#29992;&#25143;&#27169;&#22411;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20027;&#21160;&#32780;&#21516;&#27493;&#30340;&#30456;&#20851;&#26041;&#27861;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#29992;&#25143;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#27599;&#36718;&#20013;&#20027;&#21160;&#21644;&#21516;&#27493;&#22320;&#23433;&#25490;&#29992;&#25143;&#23398;&#20064;&#36895;&#24230;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#36817;&#20284;&#20026;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a decentralized learning method used to train machine learning algorithms. In FL, a global model iteratively collects the parameters of local models without accessing their local data. However, a significant challenge in FL is handling the heterogeneity of local data distribution, which often results in a drifted global model that is difficult to converge. To address this issue, current methods employ different strategies such as knowledge distillation, weighted model aggregation, and multi-task learning. These approaches are referred to as asynchronous FL, as they align user models either locally or post-hoc, where model drift has already occurred or has been underestimated. In this paper, we propose an active and synchronous correlation approach to address the challenge of user heterogeneity in FL. Specifically, our approach aims to approximate FL as standard deep learning by actively and synchronously scheduling user learning pace in each round with a dyna
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2211.05985</link><description>&lt;p&gt;
&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#26469;&#35299;&#37322;&#21644;&#26816;&#27979;&#20581;&#24247;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#26159;&#24403;&#20170;&#31038;&#20250;&#30340;&#19968;&#22823;&#38382;&#39064;&#65292;&#35768;&#22810;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#21162;&#21147;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#22825;&#21019;&#36896;&#30340;&#34394;&#20551;&#20449;&#24687;&#25968;&#37327;&#24040;&#22823;&#65292;&#23558;&#27492;&#20219;&#21153;&#30041;&#32473;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#22810;&#24180;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65292;&#20294;&#20170;&#22825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#20026;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#28155;&#21152;&#19968;&#20010;&#26032;&#23618;&#27425;&#65307;&#20351;&#29992;&#20855;&#26377;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36825;&#31687;&#25991;&#31456;&#21487;&#20197;&#26631;&#35760;&#20026;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#35768;&#22810;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#26032;&#27880;&#37322;&#26041;&#26696;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#40065;&#26834;&#25439;&#22833;&#30340;&#25918;&#26494;&#19979;&#36866;&#24403;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#40065;&#26834;&#25439;&#22833;&#30340;&#25918;&#23485;&#20351;&#24471;VC&#20998;&#31867;&#21487;&#36866;&#24403;&#22320;&#29992;PAC&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#20110;&#23545;&#25239;&#40065;&#26834;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#26032;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.05656</link><description>&lt;p&gt;
&#22312;&#24179;&#22343;&#40065;&#26834;&#24615;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#36866;&#24403;&#21487;&#23398;&#20064;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Proper Learnability between Average- and Worst-case Robustness. (arXiv:2211.05656v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#40065;&#26834;&#25439;&#22833;&#30340;&#25918;&#26494;&#19979;&#36866;&#24403;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#40065;&#26834;&#25439;&#22833;&#30340;&#25918;&#23485;&#20351;&#24471;VC&#20998;&#31867;&#21487;&#36866;&#24403;&#22320;&#29992;PAC&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#20110;&#23545;&#25239;&#40065;&#26834;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#26032;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Montasser&#31561;&#20154;[2019]&#34920;&#26126;&#65292;&#26377;&#38480;VC&#32500;&#24230;&#19981;&#36275;&#20197;&#23454;&#29616;&#36866;&#24403;&#30340;&#23545;&#25239;&#40065;&#26834;PAC&#23398;&#20064;&#12290;&#37492;&#20110;&#36825;&#31181;&#22256;&#38590;&#65292;&#23398;&#30028;&#24320;&#22987;&#30740;&#31350;&#22312;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#40065;&#26834;&#25439;&#22833;&#30340;&#25918;&#23485;&#19979;&#30340;&#36866;&#24403;&#23398;&#20064;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31995;&#21015;&#40065;&#26834;&#25439;&#22833;&#30340;&#25918;&#23485;&#65292;&#20351;&#24471;VC&#20998;&#31867;&#21487;&#36866;&#24403;&#22320;&#29992;PAC&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#25509;&#36817;&#20110;&#26631;&#20934;PAC&#23398;&#20064;&#35774;&#32622;&#25152;&#38656;&#30340;&#22797;&#26434;&#24230;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31181;&#29616;&#26377;&#30340;&#21644;&#33258;&#28982;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#40065;&#26834;&#25439;&#22833;&#30340;&#25918;&#23485;&#65292;&#26377;&#38480;&#30340;VC&#32500;&#24230;&#19981;&#36275;&#20197;&#23454;&#29616;&#36866;&#24403;&#30340;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23545;&#20110;&#23545;&#25239;&#40065;&#26834;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#26032;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Montasser et al. [2019] showed that finite VC dimension is not sufficient for proper adversarially robust PAC learning. In light of this hardness, there is a growing effort to study what type of relaxations to the adversarially robust PAC learning setup can enable proper learnability. In this work, we initiate the study of proper learning under relaxations of the worst-case robust loss. We give a family of robust loss relaxations under which VC classes are properly PAC learnable with sample complexity close to what one would require in the standard PAC learning setup. On the other hand, we show that for an existing and natural relaxation of the worst-case robust loss, finite VC dimension is not sufficient for proper learning. Lastly, we give new generalization guarantees for the adversarially robust empirical risk minimizer.
&lt;/p&gt;</description></item><item><title>PAD-Net&#26159;&#19968;&#20010;&#37096;&#20998;&#21160;&#24577;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#23558;&#20887;&#20313;&#30340;&#21160;&#24577;&#21442;&#25968;&#36716;&#25442;&#20026;&#38745;&#24577;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#32593;&#32476;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05528</link><description>&lt;p&gt;
PAD-Net&#65306;&#29992;&#20110;&#21160;&#24577;&#32593;&#32476;&#30340;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PAD-Net: An Efficient Framework for Dynamic Networks. (arXiv:2211.05528v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05528
&lt;/p&gt;
&lt;p&gt;
PAD-Net&#26159;&#19968;&#20010;&#37096;&#20998;&#21160;&#24577;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#23558;&#20887;&#20313;&#30340;&#21160;&#24577;&#21442;&#25968;&#36716;&#25442;&#20026;&#38745;&#24577;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#32593;&#32476;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32593;&#32476;&#65292;&#20363;&#22914;&#21160;&#24577;&#21367;&#31215;&#65288;DY-Conv&#65289;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#65292;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23454;&#29616;&#21160;&#24577;&#32593;&#32476;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#23558;&#32473;&#23450;&#30340;&#38745;&#24577;&#23618;&#36716;&#25442;&#20026;&#23436;&#20840;&#21160;&#24577;&#30340;&#23618;&#65292;&#20854;&#20013;&#25152;&#26377;&#21442;&#25968;&#37117;&#26159;&#21160;&#24577;&#30340;&#65288;&#33267;&#23569;&#22312;&#21333;&#20010;&#23618;&#20869;&#65289;&#24182;&#38543;&#36755;&#20837;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#23436;&#20840;&#21160;&#24577;&#30340;&#35774;&#32622;&#21487;&#33021;&#20250;&#23548;&#33268;&#20887;&#20313;&#21442;&#25968;&#21644;&#39640;&#37096;&#32626;&#25104;&#26412;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21160;&#24577;&#32593;&#32476;&#22312;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25361;&#25112;&#21160;&#24577;&#32593;&#32476;&#30340;&#22522;&#26412;&#24120;&#35782;&#65292;&#24182;&#25552;&#20986;&#37096;&#20998;&#21160;&#24577;&#32593;&#32476;&#65292;&#21363;PAD-Net&#65292;&#20197;&#23558;&#20887;&#20313;&#21160;&#24577;&#21442;&#25968;&#36716;&#25442;&#20026;&#38745;&#24577;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#36845;&#20195;&#27169;&#24335;&#20998;&#21306;&#26469;&#26377;&#25928;&#22320;&#20998;&#21306;&#21160;&#24577;&#21644;&#38745;&#24577;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#20840;&#38754;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model's representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#20004;&#20010;&#38454;&#27573;&#65292;&#20998;&#21035;&#35299;&#20915;&#20102;&#39640;&#26031;&#25968;&#25454;&#24211;&#30340;&#30456;&#20851;&#24615;&#26816;&#27979;&#21644;&#37096;&#20998;&#23545;&#40784;&#24674;&#22797;&#38382;&#39064;&#65307;&#25552;&#20986;&#30340;&#26816;&#27979;&#22120;&#34920;&#29616;&#26356;&#20339;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#35770;&#25216;&#26415;&#26469;&#32465;&#23450;&#30456;&#20851;&#24615;&#26631;&#20934;&#19979;&#30340;&#38169;&#35823;&#27010;&#29575;&#65292;&#24182;&#33021;&#24674;&#22797;&#32473;&#23450;&#25968;&#25454;&#24211;&#30340;&#37096;&#20998;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2211.01069</link><description>&lt;p&gt;
&#39640;&#26031;&#25968;&#25454;&#24211;&#30340;&#30456;&#20851;&#24615;&#26816;&#27979;&#19982;&#23545;&#40784;&#24674;&#22797;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Correlation Detection and Alignment Recovery of Gaussian Databases. (arXiv:2211.01069v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#20004;&#20010;&#38454;&#27573;&#65292;&#20998;&#21035;&#35299;&#20915;&#20102;&#39640;&#26031;&#25968;&#25454;&#24211;&#30340;&#30456;&#20851;&#24615;&#26816;&#27979;&#21644;&#37096;&#20998;&#23545;&#40784;&#24674;&#22797;&#38382;&#39064;&#65307;&#25552;&#20986;&#30340;&#26816;&#27979;&#22120;&#34920;&#29616;&#26356;&#20339;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#35770;&#25216;&#26415;&#26469;&#32465;&#23450;&#30456;&#20851;&#24615;&#26631;&#20934;&#19979;&#30340;&#38169;&#35823;&#27010;&#29575;&#65292;&#24182;&#33021;&#24674;&#22797;&#32473;&#23450;&#25968;&#25454;&#24211;&#30340;&#37096;&#20998;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#26031;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26816;&#27979;&#21644;&#37096;&#20998;&#23545;&#40784;&#22238;&#22797;&#38382;&#39064;&#12290;&#25105;&#20204;&#20026;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#24314;&#31435;&#20102;&#19978;&#19979;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26816;&#27979;&#22120;&#22312;&#26576;&#20123;&#29305;&#23450;&#21442;&#25968;&#36873;&#25321;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#26816;&#27979;&#22120;&#34987;&#25509;&#21463;&#20026;&#30456;&#20851;&#24615;&#26102;&#65292;&#31639;&#27861;&#36824;&#20250;&#24674;&#22797;&#32473;&#23450;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#37096;&#20998;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose an efficient two-stage algorithm solving a joint problem of correlation detection and partial alignment recovery between two Gaussian databases. Correlation detection is a hypothesis testing problem; under the null hypothesis, the databases are independent, and under the alternate hypothesis, they are correlated, under an unknown row permutation. We develop bounds on the type-I and type-II error probabilities, and show that the analyzed detector performs better than a recently proposed detector, at least for some specific parameter choices. Since the proposed detector relies on a statistic, which is a sum of dependent indicator random variables, then in order to bound the type-I probability of error, we develop a novel graph-theoretic technique for bounding the $k$-th order moments of such statistics. When the databases are accepted as correlated, the algorithm also recovers some partial alignment between the given databases. We also propose two more algorithms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#29256;&#26412;&#30340;&#21487;&#35757;&#32451;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#33021;&#22815;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#25191;&#34892;&#21367;&#31215;&#65292;&#21487;&#20197;&#35753;CNN&#29992;&#20110;&#26356;&#24191;&#27867;&#12289;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.13416</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#21487;&#35757;&#32451;&#36830;&#32493;&#21367;&#31215;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Continuous Convolutional Trainable Filter for Modelling Unstructured Data. (arXiv:2210.13416v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#29256;&#26412;&#30340;&#21487;&#35757;&#32451;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#33021;&#22815;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#25191;&#34892;&#21367;&#31215;&#65292;&#21487;&#20197;&#35753;CNN&#29992;&#20110;&#26356;&#24191;&#27867;&#12289;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26550;&#26500;&#20043;&#19968;&#12290;CNN&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#26159;&#21487;&#35757;&#32451;&#28388;&#27874;&#22120;&#65292;&#34920;&#31034;&#20026;&#31163;&#25955;&#32593;&#26684;&#65292;&#22312;&#31163;&#25955;&#36755;&#20837;&#25968;&#25454;&#19978;&#25191;&#34892;&#21367;&#31215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#29256;&#26412;&#30340;&#21487;&#35757;&#32451;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#36825;&#20010;&#26032;&#30340;&#26694;&#26550;&#20801;&#35768;&#25506;&#32034;CNN&#22312;&#31163;&#25955;&#22495;&#20043;&#22806;&#30340;&#26356;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#35768;&#22810;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#25552;&#20379;&#37325;&#35201;&#30340;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36830;&#32493;&#28388;&#27874;&#22120;&#30340;&#20934;&#30830;&#24230;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#31163;&#25955;&#28388;&#27874;&#22120;&#30456;&#23218;&#32654;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26500;&#24314;&#22359;&#29992;&#20110;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26469;&#35299;&#20915;&#38750;&#32467;&#26500;&#21270;&#22495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Network (CNN) is one of the most important architectures in deep learning. The fundamental building block of a CNN is a trainable filter, represented as a discrete grid, used to perform convolution on discrete input data. In this work, we propose a continuous version of a trainable convolutional filter able to work also with unstructured data. This new framework allows exploring CNNs beyond discrete domains, enlarging the usage of this important learning technique for many more complex problems. Our experiments show that the continuous filter can achieve a level of accuracy comparable to the state-of-the-art discrete filter, and that it can be used in current deep learning architectures as a building block to solve problems with unstructured domains as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#23545;&#25968;&#31934;&#24230;transformer&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#23545;&#25968;&#31934;&#24230;transformer&#37117;&#21487;&#20197;&#31561;&#25928;&#22320;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#65292;&#25193;&#23637;&#20102;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2210.02671</link><description>&lt;p&gt;
&#34920;&#36798;&#23545;&#25968;&#31934;&#24230;&#21464;&#25442;&#22120;&#30340;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
A Logic for Expressing Log-Precision Transformers. (arXiv:2210.02671v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#23545;&#25968;&#31934;&#24230;transformer&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#23545;&#25968;&#31934;&#24230;transformer&#37117;&#21487;&#20197;&#31561;&#25928;&#22320;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#65292;&#25193;&#23637;&#20102;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#25551;&#36848;&#23427;&#20204;&#21487;&#20197;&#22312;&#26576;&#20123;&#36755;&#20837;&#25991;&#26412;&#19978;&#35299;&#20915;&#30340;&#36923;&#36753;&#35268;&#21017;&#31867;&#22411;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#22312;&#38271;&#24230;&#20026;n&#30340;&#35821;&#22659;&#19978;&#35745;&#31639;&#21069;&#21521;&#20256;&#36882;&#30340;&#23545;&#25968;&#31934;&#24230;transformer&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#23545;&#25968;&#31934;&#24230;transformer&#37117;&#21487;&#20197;&#31561;&#25928;&#22320;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26631;&#20934;&#30340;&#20840;&#31216;&#21644;&#23384;&#22312;&#37327;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
One way to interpret the reasoning power of transformer-based language models is to describe the types of logical rules they can resolve over some input text. Recently, Chiang et al. (2023) showed that finite-precision transformers can be equivalently expressed in a generalization of first-order logic. However, finite-precision transformers are a weak transformer variant because, as we show, a single head can only attend to a constant number of tokens and, in particular, cannot represent uniform attention. Since attending broadly is a core capability for transformers, we ask whether a minimally more expressive model that can attend universally can also be characterized in logic. To this end, we analyze transformers whose forward pass is computed in $\log n$ precision on contexts of length $n$. We prove that any log-precision transformer can be equivalently expressed as a first-order logic sentence that, in addition to standard universal and existential quantifiers, may also contain maj
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#28151;&#21512;&#35268;&#21010;&#22120;&#36827;&#34892;&#20219;&#21153;&#24863;&#30693;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#22312;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;93.59%&#30340;&#25972;&#20307;&#26631;&#20934;&#21270;&#24615;&#33021;&#65292;&#36229;&#36807;&#20043;&#21069;&#30340;&#22522;&#32447;&#65292;&#26159;&#35299;&#20915;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#25552;&#21319;&#30340;&#19968;&#39033;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2209.12016</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#24320;&#22987;&#25484;&#25569;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels. (arXiv:2209.12016v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#28151;&#21512;&#35268;&#21010;&#22120;&#36827;&#34892;&#20219;&#21153;&#24863;&#30693;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#22312;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;93.59%&#30340;&#25972;&#20307;&#26631;&#20934;&#21270;&#24615;&#33021;&#65292;&#36229;&#36807;&#20043;&#21069;&#30340;&#22522;&#32447;&#65292;&#26159;&#35299;&#20915;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#25552;&#21319;&#30340;&#19968;&#39033;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#35273;&#24863;&#30693;&#25968;&#25454;&#20013;&#25511;&#21046;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25104;&#21151;&#65292;&#20294;&#38656;&#35201;&#26234;&#33021;&#20307;&#19982;&#29615;&#22659;&#20043;&#38388;&#22823;&#37327;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#20132;&#20114;&#21644;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#20197;&#26356;&#24555;&#22320;&#36866;&#24212;&#26410;&#26469;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#25152;&#26174;&#31034;&#30340;&#37027;&#26679;&#65292;&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#31574;&#30053;&#26159;&#21542;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#22312;&#35270;&#35273;&#25511;&#21046;&#29615;&#22659;&#20013;&#23588;&#20854;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#39044;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#24182;&#32467;&#21512;&#26032;&#25552;&#20986;&#30340;&#28151;&#21512;&#35268;&#21010;&#22120;Dyna-MPC&#26469;&#36827;&#34892;&#20219;&#21153;&#24863;&#30693;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33719;&#24471;&#20102;93.59%&#30340;&#25972;&#20307;&#26631;&#20934;&#21270;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#22522;&#32447;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#28040;&#34701;&#30740;&#31350;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;Dyna-MPC&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#20102;&#35299;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#35273;&#25511;&#21046;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#32463;&#39564;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, as shown in the Unsupervised RL Benchmark (URLB; Laskin et al. 2021), whether current unsupervised strategies can improve generalization capabilities is still unclear, especially in visual control settings. In this work, we study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains 93.59% overall normalized performance, surpassing previous baselines by a staggering margin. The approach is empirically evaluated through a large-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Q-learning&#20915;&#31574;Transformer&#65288;QDT&#65289;&#65292;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#30340;&#20248;&#28857;&#21019;&#36896;&#32541;&#21512;&#20989;&#25968;&#20197;&#20174;&#27425;&#20248;&#25968;&#25454;&#20013;&#23398;&#20064;&#26368;&#20248;&#25919;&#31574;&#65292;&#24182;&#21033;&#29992;DT&#30340;transformer&#26550;&#26500;&#36827;&#34892;&#26465;&#20214;&#31574;&#30053;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;QDT&#27604;DT&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2209.03993</link><description>&lt;p&gt;
Q-&#23398;&#20064;&#20915;&#31574;Transformer&#65306;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#36827;&#34892;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#30340;&#31163;&#32447;RL
&lt;/p&gt;
&lt;p&gt;
Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline RL. (arXiv:2209.03993v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Q-learning&#20915;&#31574;Transformer&#65288;QDT&#65289;&#65292;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#30340;&#20248;&#28857;&#21019;&#36896;&#32541;&#21512;&#20989;&#25968;&#20197;&#20174;&#27425;&#20248;&#25968;&#25454;&#20013;&#23398;&#20064;&#26368;&#20248;&#25919;&#31574;&#65292;&#24182;&#21033;&#29992;DT&#30340;transformer&#26550;&#26500;&#36827;&#34892;&#26465;&#20214;&#31574;&#30053;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;QDT&#27604;DT&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;&#26465;&#20214;&#31574;&#30053;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#20855;&#26377;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;&#20915;&#31574;Transformer&#65288;DT&#65289;&#32467;&#21512;&#20102;&#26465;&#20214;&#31574;&#30053;&#26041;&#27861;&#21644;Transformer&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;&#19982;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;DT&#32570;&#23569;&#32541;&#21512;&#33021;&#21147;&#8212;&#8212;&#31163;&#32447;RL&#23398;&#20064;&#26368;&#20248;&#25919;&#31574;&#20381;&#36182;&#20110;&#26469;&#33258;&#27425;&#20248;&#36712;&#36857;&#30340;&#25968;&#25454;&#65292;&#36825;&#19968;&#28857;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#65288;&#22914;Q-learning&#65289;&#30340;&#20256;&#32479;RL&#26041;&#27861;&#27809;&#26377;&#30456;&#21516;&#30340;&#38480;&#21046;&#65307;&#28982;&#32780;&#65292;&#22312;&#31163;&#32447;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#23588;&#20854;&#26159;&#24403;&#20182;&#20204;&#20381;&#36182;&#20989;&#25968;&#36924;&#36817;&#26102;&#65292;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#19981;&#31283;&#23450;&#30340;&#23398;&#20064;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Q-learning&#20915;&#31574;Transformer&#65288;QDT&#65289;&#26469;&#35299;&#20915;DT&#30340;&#32570;&#28857;&#65292;&#23427;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#65288;Q-learning&#65289;&#30340;&#20248;&#28857;&#21019;&#36896;&#20102;&#19968;&#20010;&#32541;&#21512;&#20989;&#25968;&#65292;&#20174;&#32780;&#20351;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#25104;&#20026;&#21487;&#33021;&#12290;&#21516;&#26102;&#65292;QDT&#21033;&#29992;DT&#30340;transformer&#26550;&#26500;&#36827;&#34892;&#26465;&#20214;&#31574;&#30053;&#24314;&#27169;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;QDT&#32988;&#36807;DT&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that tackling offline reinforcement learning (RL) with a conditional policy produces promising results. The Decision Transformer (DT) combines the conditional policy approach and a transformer architecture, showing competitive performance against several benchmarks. However, DT lacks stitching ability -- one of the critical abilities for offline RL to learn the optimal policy from sub-optimal trajectories. This issue becomes particularly significant when the offline dataset only contains sub-optimal trajectories. On the other hand, the conventional RL approaches based on Dynamic Programming (such as Q-learning) do not have the same limitation; however, they suffer from unstable learning behaviours, especially when they rely on function approximation in an off-policy learning setting. In this paper, we propose the Q-learning Decision Transformer (QDT) to address the shortcomings of DT by leveraging the benefits of Dynamic Programming (Q-learning). It utilises the
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#39640;&#25928;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;DICE&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#24615;&#23398;&#20064;&#30446;&#26631;&#21644;&#29305;&#27530;&#26631;&#35760;&#65292;&#20849;&#21516;&#35757;&#32451;&#23454;&#20307;&#25552;&#21450;&#21644;&#20107;&#20214;&#25277;&#21462;&#31561;&#36741;&#21161;&#20219;&#21153;&#65292;&#25152;&#25552;&#20986;&#30340;MACCROBAT-EE&#25968;&#25454;&#38598;&#20026;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#25552;&#20379;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2208.07989</link><description>&lt;p&gt;
DICE&#65306;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
DICE: Data-Efficient Clinical Event Extraction with Generative Models. (arXiv:2208.07989v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07989
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#39640;&#25928;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;DICE&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#24615;&#23398;&#20064;&#30446;&#26631;&#21644;&#29305;&#27530;&#26631;&#35760;&#65292;&#20849;&#21516;&#35757;&#32451;&#23454;&#20307;&#25552;&#21450;&#21644;&#20107;&#20214;&#25277;&#21462;&#31561;&#36741;&#21161;&#20219;&#21153;&#65292;&#25152;&#25552;&#20986;&#30340;MACCROBAT-EE&#25968;&#25454;&#38598;&#20026;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#25552;&#20379;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#25968;&#37327;&#20247;&#22810;&#21644;&#23454;&#20307;&#30028;&#38480;&#27169;&#31946;&#65292;&#20351;&#24471;&#36825;&#39033;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DICE&#65292;&#19968;&#31181;&#31283;&#20581;&#12289;&#39640;&#25928;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#29983;&#25104;&#27169;&#22411;&#12290;DICE&#23558;&#20107;&#20214;&#25277;&#21462;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#23545;&#27604;&#24615;&#23398;&#20064;&#30446;&#26631;&#65292;&#20197;&#20934;&#30830;&#30830;&#23450;&#29983;&#29289;&#21307;&#23398;&#25552;&#21450;&#30340;&#36793;&#30028;&#12290;DICE&#36824;&#32852;&#21512;&#35757;&#32451;&#36741;&#21161;&#25552;&#21450;&#26631;&#35782;&#20219;&#21153;&#21644;&#20107;&#20214;&#25277;&#21462;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#30830;&#23450;&#23454;&#20307;&#25552;&#21450;&#36793;&#30028;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#20837;&#29305;&#27530;&#30340;&#26631;&#35760;&#26469;&#20316;&#20026;&#35302;&#21457;&#22120;&#21644;&#21442;&#25968;&#20505;&#36873;&#39033;&#65292;&#20197;&#21253;&#21547;&#20854;&#21508;&#33258;&#30340;&#20219;&#21153;&#20013;&#30340;&#30830;&#23450;&#23454;&#20307;&#38382;&#39064;&#12290;&#20026;&#20102;&#23545;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#26681;&#25454;&#29616;&#26377;&#30340;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;MACCRO&#65292;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#21442;&#25968;&#25209;&#27880;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#25968;&#25454;&#38598;MACCROBAT-EE&#12290;
&lt;/p&gt;
&lt;p&gt;
Event extraction for the clinical domain is an under-explored research area. The lack of training data along with the high volume of domain-specific terminologies with vague entity boundaries makes the task especially challenging. In this paper, we introduce DICE, a robust and data-efficient generative model for clinical event extraction. DICE frames event extraction as a conditional generation problem and introduces a contrastive learning objective to accurately decide the boundaries of biomedical mentions. DICE also trains an auxiliary mention identification task jointly with event extraction tasks to better identify entity mention boundaries, and further introduces special markers to incorporate identified entity mentions as trigger and argument candidates for their respective tasks. To benchmark clinical event extraction, we compose MACCROBAT-EE, the first clinical event extraction dataset with argument annotation, based on an existing clinical information extraction dataset MACCRO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411; (dlglm) &#21450;&#20854;&#22312;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#30340;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#36755;&#20837;&#29305;&#24449;&#21644;&#21709;&#24212;&#30340;&#21487;&#24573;&#30053;&#21644;&#19981;&#21487;&#24573;&#30053;&#30340;&#32570;&#22833;&#27169;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#27169;&#25311;&#35777;&#26126;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#12290;</title><link>http://arxiv.org/abs/2207.08911</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21450;&#20854;&#22312;&#32570;&#22833;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deeply-Learned Generalized Linear Models with Missing Data. (arXiv:2207.08911v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411; (dlglm) &#21450;&#20854;&#22312;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#30340;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#36755;&#20837;&#29305;&#24449;&#21644;&#21709;&#24212;&#30340;&#21487;&#24573;&#30053;&#21644;&#19981;&#21487;&#24573;&#30053;&#30340;&#32570;&#22833;&#27169;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#27169;&#25311;&#35777;&#26126;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#22312;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#26174;&#33879;&#24212;&#29992;&#65292;&#20294;&#29616;&#20195;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#25968;&#25454;&#30340;&#26356;&#21152;&#26222;&#36941;&#21644;&#22797;&#26434;&#24615;&#32473;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;(dlglm)&#30340;&#27491;&#24335;&#22788;&#29702;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26102;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#36755;&#20837;&#29305;&#24449;&#21644;&#21709;&#24212;&#30340;&#21487;&#24573;&#30053;&#21644;&#19981;&#21487;&#24573;&#30053;&#30340;&#32570;&#22833;&#27169;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#27169;&#25311;&#35777;&#26126;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20197;UCI&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38134;&#34892;&#33829;&#38144;&#25968;&#25454;&#38598;&#20026;&#20363;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) methods have dramatically increased in popularity in recent years, with significant growth in their application to supervised learning problems in the biomedical sciences. However, the greater prevalence and complexity of missing data in modern biomedical datasets present significant challenges for DL methods. Here, we provide a formal treatment of missing data in the context of deeply learned generalized linear models, a supervised DL architecture for regression and classification problems. We propose a new architecture, \textit{dlglm}, that is one of the first to be able to flexibly account for both ignorable and non-ignorable patterns of missingness in input features and response at training time. We demonstrate through statistical simulation that our method outperforms existing approaches for supervised learning tasks in the presence of missing not at random (MNAR) missingness. We conclude with a case study of a Bank Marketing dataset from the UCI Machine Learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;TF-Codec&#20026;&#19968;&#31181;&#36866;&#29992;&#20110;&#20302;&#24310;&#36831;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#28508;&#22495;&#39044;&#27979;&#32534;&#30721;&#23436;&#20840;&#28040;&#38500;&#20102;&#26102;&#38388;&#20887;&#20313;&#65292;&#37319;&#29992;&#21487;&#23398;&#20064;&#30340;&#26102;&#38388;&#39057;&#29575;&#36755;&#20837;&#21387;&#32553;&#21644;&#22522;&#20110;&#36317;&#31163;&#21040;&#36719;&#26144;&#23556;&#21644;Gumbel-Softmax&#30340;&#21487;&#24494;&#37327;&#21270;&#26041;&#26696;&#65292;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#25351;&#26631;&#19978;&#22343;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2207.08363</link><description>&lt;p&gt;
&#28508;&#22495;&#39044;&#27979;&#31070;&#32463;&#35821;&#38899;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Latent-Domain Predictive Neural Speech Coding. (arXiv:2207.08363v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;TF-Codec&#20026;&#19968;&#31181;&#36866;&#29992;&#20110;&#20302;&#24310;&#36831;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#28508;&#22495;&#39044;&#27979;&#32534;&#30721;&#23436;&#20840;&#28040;&#38500;&#20102;&#26102;&#38388;&#20887;&#20313;&#65292;&#37319;&#29992;&#21487;&#23398;&#20064;&#30340;&#26102;&#38388;&#39057;&#29575;&#36755;&#20837;&#21387;&#32553;&#21644;&#22522;&#20110;&#36317;&#31163;&#21040;&#36719;&#26144;&#23556;&#21644;Gumbel-Softmax&#30340;&#21487;&#24494;&#37327;&#21270;&#26041;&#26696;&#65292;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#25351;&#26631;&#19978;&#22343;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#31070;&#32463;&#38899;&#39057;/&#35821;&#38899;&#32534;&#30721;&#23637;&#29616;&#20986;&#22312;&#36828;&#20302;&#20110;&#20256;&#32479;&#26041;&#27861;&#27604;&#29305;&#29575;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#38899;&#39057;/&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#37319;&#29992;&#22768;&#23398;&#29305;&#24449;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#30450;&#30446;&#29305;&#24449;&#36827;&#34892;&#32534;&#30721;&#65292;&#20173;&#23384;&#22312;&#32534;&#30721;&#29305;&#24449;&#20013;&#30340;&#26102;&#38388;&#20887;&#20313;&#65292;&#26412;&#25991;&#23558;&#28508;&#22495;&#39044;&#27979;&#32534;&#30721;&#24341;&#20837;VQ-VAE&#26694;&#26550;&#20013;&#65292;&#20197;&#23436;&#20840;&#28040;&#38500;&#36825;&#20123;&#20887;&#20313;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20302;&#24310;&#36831;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#35821;&#38899;&#32534;&#30721;&#22120;TF-Codec&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26681;&#25454;&#36807;&#21435;&#37327;&#21270;&#28508;&#24577;&#24103;&#30340;&#39044;&#27979;&#65292;&#23545;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#28040;&#38500;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#26102;&#38388;&#39057;&#29575;&#36755;&#20837;&#21387;&#32553;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#27604;&#29305;&#29575;&#19979;&#23545;&#20027;&#35201;&#39057;&#29575;&#21644;&#32454;&#33410;&#30340;&#20851;&#27880;&#12290;&#22522;&#20110;&#36317;&#31163;&#21040;&#36719;&#26144;&#23556;&#21644;Gumbel-Softmax&#30340;&#21487;&#24494;&#37327;&#21270;&#26041;&#26696;&#29992;&#20110;&#37327;&#21270;/&#35299;&#30721;&#28508;&#22495;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;TF-Codec&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural audio/speech coding has recently demonstrated its capability to deliver high quality at much lower bitrates than traditional methods. However, existing neural audio/speech codecs employ either acoustic features or learned blind features with a convolutional neural network for encoding, by which there are still temporal redundancies within encoded features. This paper introduces latent-domain predictive coding into the VQ-VAE framework to fully remove such redundancies and proposes the TF-Codec for low-latency neural speech coding in an end-to-end manner. Specifically, the extracted features are encoded conditioned on a prediction from past quantized latent frames so that temporal correlations are further removed. Moreover, we introduce a learnable compression on the time-frequency input to adaptively adjust the attention paid to main frequencies and details at different bitrates. A differentiable vector quantization scheme based on distance-to-soft mapping and Gumbel-Softmax is 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#22312;&#30701;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#23454;&#29616;&#20102;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#65292;&#24182;&#39564;&#35777;&#20102;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;</title><link>http://arxiv.org/abs/2207.06983</link><description>&lt;p&gt;
&#22810;&#36712;&#38899;&#20048; Transformer
&lt;/p&gt;
&lt;p&gt;
Multitrack Music Transformer. (arXiv:2207.06983v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#22312;&#30701;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#23454;&#29616;&#20102;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#65292;&#24182;&#39564;&#35777;&#20102;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20351;&#29992; Transformer &#27169;&#22411;&#29983;&#25104;&#22810;&#36712;&#38899;&#20048;&#30340;&#26041;&#27861;&#22312;&#20048;&#22120;&#25968;&#37327;&#12289;&#38899;&#20048;&#29255;&#27573;&#38271;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#26377;&#38480;&#21046;&#65292;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#24050;&#26377;&#34920;&#31034;&#26041;&#24335;&#38656;&#35201;&#38271;&#24230;&#36739;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;&#20174;&#32780;&#38656;&#35201;&#26356;&#22810;&#30340;&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#21516;&#26102;&#20351;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#26356;&#30701;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340; Multitrack Music Transformer&#65288;MMT&#65289;&#19982;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#65292;&#22312;&#20027;&#35266;&#21548;&#27979;&#35797;&#20013;&#25490;&#22312;&#20004;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#27169;&#22411;&#20043;&#38388;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#19978;&#37117;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#23454;&#26102;&#21363;&#20852;&#21019;&#20316;&#25110;&#25509;&#36817;&#23454;&#26102;&#30340;&#21019;&#24847;&#24212;&#29992;&#20013;&#26356;&#20026;&#23454;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for generating multitrack music with transformer models have been limited in terms of the number of instruments, the length of the music segments and slow inference. This is partly due to the memory requirements of the lengthy input sequences necessitated by existing representations. In this work, we propose a new multitrack music representation that allows a diverse set of instruments while keeping a short sequence length. Our proposed Multitrack Music Transformer (MMT) achieves comparable performance with state-of-the-art systems, landing in between two recently proposed models in a subjective listening test, while achieving substantial speedups and memory reductions over both, making the method attractive for real time improvisation or near real time creative applications. Further, we propose a new measure for analyzing musical self-attention and show that the trained model attends more to notes that form a consonant interval with the current note and to notes th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.13872</link><description>&lt;p&gt;
&#21518;&#39564;&#27010;&#24565;&#35299;&#37322;&#20309;&#26102;&#21487;&#35782;&#21035;&#65311;
&lt;/p&gt;
&lt;p&gt;
When are Post-hoc Conceptual Explanations Identifiable?. (arXiv:2206.13872v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23884;&#20837;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#27010;&#24565;&#35299;&#37322;&#26469;&#29702;&#35299;&#21644;&#20998;&#35299;&#65292;&#36825;&#31181;&#38656;&#27714;&#22312;&#35299;&#37322;&#20013;&#19981;&#21253;&#21547;&#26377;&#25928;&#27010;&#24565;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26174;&#33879;&#12290;&#20026;&#20102;&#25552;&#20379;&#21518;&#39564;&#35299;&#37322;&#65292;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#20250;&#22312;&#24050;&#35757;&#32451;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#37322;&#24615;&#24378;&#30340;&#27010;&#24565;&#65292;&#20363;&#22914;&#29289;&#20307;&#24418;&#29366;&#25110;&#39068;&#33394;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#35748;&#20026;&#27010;&#24565;&#21457;&#29616;&#24212;&#35813;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#34987;&#35777;&#26126;&#22320;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20316;&#20026;&#19968;&#20010;&#36215;&#28857;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#23558;&#27010;&#24565;&#21457;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#20363;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65289;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#38750;&#39640;&#26031;&#20998;&#24067;&#30340;&#29420;&#31435;&#27010;&#24565;&#26469;&#38416;&#26126;&#36825;&#19968;&#28857;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#21487;&#35777;&#26126;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest in understanding and factorizing learned embedding spaces through conceptual explanations is steadily growing. When no human concept labels are available, concept discovery methods search trained embedding spaces for interpretable concepts like object shape or color that can be used to provide post-hoc explanations for decisions. Unlike previous work, we argue that concept discovery should be identifiable, meaning that a number of known concepts can be provably recovered to guarantee reliability of the explanations. As a starting point, we explicitly make the connection between concept discovery and classical methods like Principal Component Analysis and Independent Component Analysis by showing that they can recover independent concepts with non-Gaussian distributions. For dependent concepts, we propose two novel approaches that exploit functional compositionality properties of image-generating processes. Our provably identifiable concept discovery methods substantially outpe
&lt;/p&gt;</description></item><item><title>Grid-SiPhyR&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#37325;&#26500;&#30340;&#28165;&#27905;&#33021;&#28304;&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#29289;&#29702;&#21551;&#21457;&#24335;&#33293;&#20837;&#26041;&#27861;&#20248;&#21270;&#32452;&#21512;&#38382;&#39064;&#24182;&#28385;&#36275;&#20851;&#38190;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2206.06789</link><description>&lt;p&gt;
Grid-SiPhyR&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#20013;&#32452;&#21512;&#38382;&#39064;&#30340;&#20248;&#21270;(arXiv:2206.06789v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Grid-SiPhyR: An end-to-end learning to optimize framework for combinatorial problems in power systems. (arXiv:2206.06789v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06789
&lt;/p&gt;
&lt;p&gt;
Grid-SiPhyR&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#37325;&#26500;&#30340;&#28165;&#27905;&#33021;&#28304;&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#29289;&#29702;&#21551;&#21457;&#24335;&#33293;&#20837;&#26041;&#27861;&#20248;&#21270;&#32452;&#21512;&#38382;&#39064;&#24182;&#28385;&#36275;&#20851;&#38190;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#38382;&#39064;&#22312;&#20915;&#31574;&#21046;&#23450;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#21253;&#25324;&#31163;&#25955;&#35774;&#22791;&#35774;&#32622;&#21644;&#35774;&#35745;&#21442;&#25968;&#12289;&#21333;&#20803;&#29983;&#20135;&#12289;&#24320;&#20851;&#20013;&#30340;&#24320;/&#20851;&#25110;&#26159;/&#21542;&#20915;&#31574;&#12289;&#36335;&#30001;&#21644;&#31038;&#20132;&#32593;&#32476;&#31561;&#39046;&#22495;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#24456;&#24120;&#35265;&#65292;&#20294;&#32463;&#20856;&#30340;&#32452;&#21512;&#20248;&#21270;&#20248;&#21270;&#26041;&#27861;&#22312;&#21160;&#24577;&#21644;&#20851;&#38190;&#23433;&#20840;&#29615;&#22659;&#19979;&#36827;&#34892;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;&#20915;&#31574;&#21046;&#23450;&#20173;&#28982;&#36807;&#20110;&#32531;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SiPhyR&#65288;&#21457;&#38899;&#65306;cipher&#65289;&#30340;&#29289;&#29702;&#23398;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#23398;&#20064;&#20248;&#21270;&#32452;&#21512;&#38382;&#39064;&#12290;SiPhyR&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#23398;&#21551;&#21457;&#30340;&#22235;&#33293;&#20116;&#20837;&#26041;&#27861;&#26469;&#22788;&#29702;&#32452;&#21512;&#20248;&#21270;&#30340;&#25361;&#25112;&#65292;&#22312;&#21487;&#24494;&#20998;&#26694;&#26550;&#20869;&#20855;&#26377;&#23433;&#20840;&#24615;&#20851;&#38190;&#32422;&#26463;&#30340;&#35748;&#35777;&#21487;&#28385;&#36275;&#24615;&#12290;&#25105;&#20204;&#22312;&#28165;&#27905;&#33021;&#28304;&#31995;&#32479;&#30340;&#26032;&#20852;&#33539;&#20363;&#8212;&#8212;&#21160;&#24577;&#37325;&#26500;&#19978;&#23637;&#31034;&#20102;SiPhyR&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#36825;&#37324;&#65292;&#30005;&#32593;&#30340;&#25299;&#25169;&#21644;&#21151;&#29575;&#27969;&#34987;&#20248;&#21270;&#20197;&#26368;&#22823;&#21270;&#28165;&#27905;&#33021;&#28304;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed integer problems are ubiquitous in decision making, from discrete device settings and design parameters, unit production, and on/off or yes/no decision in switches, routing, and social networks. Despite their prevalence, classical optimization approaches for combinatorial optimization remain prohibitively slow for fast and accurate decision making in dynamic and safety-critical environments with hard constraints. To address this gap, we propose SiPhyR (pronounced: cipher), a physics-informed machine learning framework for end-to-end learning to optimize for combinatorial problems. SiPhyR employs a novel physics-informed rounding approach to tackle the challenge of combinatorial optimization within a differentiable framework that has certified satisfiability of safety-critical constraints. We demonstrate the effectiveness of SiPhyR on an emerging paradigm for clean energy systems: dynamic reconfiguration, where the topology of the electric grid and power flow are optimized so as t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#30340;&#22270;&#20687;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#31639;&#20840;&#29699;&#36139;&#22256;&#23454;&#39564;&#20013;&#30340;&#27835;&#30103;&#25928;&#26524;&#24322;&#36136;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.06417</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#27835;&#30103;&#25928;&#26524;&#24322;&#36136;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Image-based Treatment Effect Heterogeneity. (arXiv:2206.06417v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#30340;&#22270;&#20687;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#31639;&#20840;&#29699;&#36139;&#22256;&#23454;&#39564;&#20013;&#30340;&#27835;&#30103;&#25928;&#26524;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;(RCT)&#34987;&#35748;&#20026;&#26159;&#35780;&#20272;&#24178;&#39044;&#25514;&#26045;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;(ATE)&#30340;&#37329; standard&#12290;&#20854;&#20013;&#19968;&#20010;&#20351;&#29992;RCT&#30340;&#26041;&#27861;&#26159;&#30740;&#31350;&#20840;&#29699;&#36139;&#22256;&#30340;&#21407;&#22240;&#65292;&#36825;&#20063;&#26159;2019&#24180;&#35834;&#36125;&#23572;&#32426;&#24565;&#22870;&#39041;&#32473;&#26460;&#24343;&#27931;&#12289;&#29677;&#32435;&#21513;&#21644;&#20811;&#33713;&#40664;&#30340;&#21407;&#22240;&#12290;&#20026;&#20102;&#20998;&#26512;ATE&#21608;&#22260;&#30340;&#25928;&#26524;&#21464;&#21270;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#26465;&#20214;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;(CATE)&#36827;&#34892;&#26465;&#20214;&#31579;&#36873;&#21644;&#20998;&#26512;&#65292;&#32780;&#35813;&#31579;&#36873;&#21644;&#20998;&#26512;&#25152;&#37319;&#38598;&#30340;&#34920;&#26684;&#21464;&#37327;&#65292;&#20363;&#22914;&#24180;&#40836;&#21644;&#26063;&#35028;&#65292;&#36890;&#24120;&#20165;&#22312;RCT&#25968;&#25454;&#25910;&#38598;&#26399;&#38388;&#36827;&#34892;&#27979;&#37327;&#12290;&#34429;&#28982;&#36825;&#20123;&#21464;&#37327;&#26159;&#35299;&#26512;CATE&#30340;&#20851;&#38190;&#65292;&#20294;&#20165;&#20351;&#29992;&#36825;&#20123;&#21464;&#37327;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#25928;&#26524;&#21464;&#24322;&#30340;&#21382;&#21490;&#12289;&#22320;&#29702;&#25110;&#37051;&#22495;&#29305;&#23450;&#22240;&#32032;&#65292;&#22240;&#20026;&#34920;&#26684;RCT&#25968;&#25454;&#36890;&#24120;&#20165;&#22312;&#23454;&#39564;&#26399;&#38388;&#38468;&#36817;&#34987;&#35266;&#23519;&#21040;&#12290;&#22312;&#20840;&#29699;&#36139;&#22256;&#30740;&#31350;&#20013;&#65292;&#22914;&#26524;&#23454;&#39564;&#21333;&#20803;&#30340;&#20301;&#32622;&#22823;&#33268;&#24050;&#30693;&#65292;&#21355;&#26143;&#22270;&#20687;&#21487;&#20197;&#25552;&#20379;&#31397;&#35270;&#23454;&#39564;&#21333;&#20803;&#26356;&#24191;&#27867;&#32972;&#26223;&#30340;&#31383;&#21475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#65292;&#29992;&#20110;&#20272;&#31639;&#27835;&#30103;&#25928;&#26524;&#24322;&#36136;&#24615;(ITEH)&#65292;&#24182;&#35777;&#26126;&#20102;&#20182;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#21360;&#24230;&#23454;&#39564;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized controlled trials (RCTs) are considered the gold standard for estimating the average treatment effect (ATE) of interventions. One use of RCTs is to study the causes of global poverty -- a subject explicitly cited in the 2019 Nobel Memorial Prize awarded to Duflo, Banerjee, and Kremer "for their experimental approach to alleviating global poverty." Because the ATE is a population summary, anti-poverty experiments often seek to unpack the effect variation around the ATE by conditioning (CATE) on tabular variables such as age and ethnicity that were measured during the RCT data collection. Although such variables are key to unpacking CATE, using only such variables may fail to capture historical, geographical, or neighborhood-specific contributors to effect variation, as tabular RCT data are often only observed near the time of the experiment. In global poverty research, when the location of the experiment units is approximately known, satellite imagery can provide a window int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#35328;&#35821;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HARD-Training&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#19968;&#20010;&#24102;&#26377;&#26412;&#22320;&#20851;&#27880;&#26426;&#21046;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#20004;&#31867;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20013;&#26816;&#27979;&#21644;&#39044;&#27979;&#37325;&#24230;&#25233;&#37057;&#30151;&#12290;</title><link>http://arxiv.org/abs/2206.01542</link><description>&lt;p&gt;
&#20174;&#35328;&#35821;&#20013;&#26816;&#27979;&#37325;&#24230;&#25233;&#37057;&#30151;&#65306;&#19968;&#31181;&#26032;&#30340;HARD-Training&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Detecting the Severity of Major Depressive Disorder from Speech: A Novel HARD-Training Methodology. (arXiv:2206.01542v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#35328;&#35821;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HARD-Training&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#19968;&#20010;&#24102;&#26377;&#26412;&#22320;&#20851;&#27880;&#26426;&#21046;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#20004;&#31867;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20013;&#26816;&#27979;&#21644;&#39044;&#27979;&#37325;&#24230;&#25233;&#37057;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#24230;&#25233;&#37057;&#30151;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#12289;&#20276;&#38543;&#39640;&#26114;&#31038;&#20250;&#32463;&#27982;&#25104;&#26412;&#30340;&#20840;&#29699;&#24615;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#12290;&#39044;&#27979;&#21644;&#33258;&#21160;&#26816;&#27979;&#37325;&#24230;&#25233;&#37057;&#30151;&#21487;&#20197;&#23545;&#31038;&#20250;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#35328;&#35821;&#20316;&#20026;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#12289;&#26131;&#20110;&#25910;&#38598;&#30340;&#20449;&#21495;&#65292;&#26159;&#36741;&#21161;&#35786;&#26029;&#21644;&#35780;&#20272;&#37325;&#24230;&#25233;&#37057;&#30151;&#30340;&#26377;&#24076;&#26395;&#26631;&#35760;&#12290;&#26412;&#25991;&#25910;&#38598;&#20102;RADAR-MDD&#30740;&#31350;&#35745;&#21010;&#20013;&#20316;&#20026;&#19968;&#37096;&#20998;&#30340;&#35328;&#35821;&#26679;&#26412;&#12290;RADAR-MDD&#26159;&#19968;&#20010;&#35266;&#23519;&#24615;&#38431;&#21015;&#30740;&#31350;&#65292;&#20854;&#20013;&#25910;&#38598;&#20102;&#35199;&#29677;&#29273;&#12289;&#33521;&#22269;&#21644;&#33655;&#20848;&#26377;&#25233;&#37057;&#30151;&#21490;&#30340;&#20010;&#20154;&#30340;&#35328;&#35821;&#21644;&#20854;&#20182;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#26412;&#25991;&#23558;RADAR-MDD&#35328;&#35821;&#35821;&#26009;&#24211;&#20316;&#20026;&#23454;&#39564;&#26694;&#26550;&#65292;&#27979;&#35797;&#20102;&#19968;&#31181;&#20855;&#26377;&#26412;&#22320;&#27880;&#24847;&#26426;&#21046;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#20004;&#31867;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#33539;&#24335;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;HARD-Training&#12290;
&lt;/p&gt;
&lt;p&gt;
Major Depressive Disorder (MDD) is a common worldwide mental health issue with high associated socioeconomic costs. The prediction and automatic detection of MDD can, therefore, make a huge impact on society. Speech, as a non-invasive, easy to collect signal, is a promising marker to aid the diagnosis and assessment of MDD. In this regard, speech samples were collected as part of the Remote Assessment of Disease and Relapse in Major Depressive Disorder (RADAR-MDD) research programme. RADAR-MDD was an observational cohort study in which speech and other digital biomarkers were collected from a cohort of individuals with a history of MDD in Spain, United Kingdom and the Netherlands. In this paper, the RADAR-MDD speech corpus was taken as an experimental framework to test the efficacy of a Sequence-to-Sequence model with a local attention mechanism in a two-class depression severity classification paradigm. Additionally, a novel training method, HARD-Training, is proposed. It is a methodo
&lt;/p&gt;</description></item><item><title>ForestPrune&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#20462;&#21098;&#28145;&#24230;&#22270;&#23618;&#26469;&#20248;&#21270;&#26641;&#38598;&#25104;&#30340;&#26032;&#39062;&#31639;&#27861;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#22312;&#20013;&#31561;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#21644;&#38598;&#25104;&#20013;&#26174;&#33879;&#21387;&#32553;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#30340;&#39044;&#27979;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2206.00128</link><description>&lt;p&gt;
ForestPrune: &#32039;&#20945;&#30340;&#28145;&#24230;&#21487;&#25511;&#26641;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
ForestPrune: Compact Depth-Controlled Tree Ensembles. (arXiv:2206.00128v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00128
&lt;/p&gt;
&lt;p&gt;
ForestPrune&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#20462;&#21098;&#28145;&#24230;&#22270;&#23618;&#26469;&#20248;&#21270;&#26641;&#38598;&#25104;&#30340;&#26032;&#39062;&#31639;&#27861;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#22312;&#20013;&#31561;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#21644;&#38598;&#25104;&#20013;&#26174;&#33879;&#21387;&#32553;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#30340;&#39044;&#27979;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#38598;&#25104;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#21487;&#33021;&#20250;&#21464;&#24471;&#24222;&#22823;&#32780;&#38590;&#20197;&#25511;&#21046;&#12290;&#36825;&#20123;&#38598;&#25104;&#36890;&#24120;&#20250;&#36827;&#34892;&#21518;&#22788;&#29702;(&#20462;&#21098;)&#65292;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#24182;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#26694;&#26550;ForestPrune&#65292;&#36890;&#36807;&#20174;&#21333;&#20010;&#26641;&#20013;&#20462;&#21098;&#28145;&#24230;&#22270;&#23618;&#65292;&#23545;&#26641;&#38598;&#25104;&#36827;&#34892;&#21518;&#22788;&#29702;&#12290;&#30001;&#20110;&#20915;&#31574;&#26641;&#20013;&#33410;&#28857;&#25968;&#37327;&#38543;&#30528;&#26641;&#30340;&#28145;&#24230;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#28145;&#26641;&#30340;&#20462;&#21098;&#21487;&#26174;&#33879;&#21387;&#32553;&#38598;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;ForestPrune&#19979;&#39640;&#25928;&#22320;&#33719;&#24471;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#24120;&#21487;&#20197;&#22312;&#20013;&#31561;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#21644;&#38598;&#25104;&#20013;&#22312;&#20960;&#31186;&#38047;&#20869;&#33719;&#24471;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#25968;&#19975;&#34892;&#21644;&#25968;&#30334;&#26869;&#26641;&#65292;&#32467;&#26524;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ForestPrune&#20135;&#29983;&#30340;&#31616;&#27905;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#21518;&#22788;&#29702;&#31639;&#27861;&#25552;&#21462;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tree ensembles are powerful models that achieve excellent predictive performances, but can grow to unwieldy sizes. These ensembles are often post-processed (pruned) to reduce memory footprint and improve interpretability. We present ForestPrune, a novel optimization framework to post-process tree ensembles by pruning depth layers from individual trees. Since the number of nodes in a decision tree increases exponentially with tree depth, pruning deep trees drastically compactifies ensembles. We develop a specialized optimization algorithm to efficiently obtain high-quality solutions to problems under ForestPrune. Our algorithm typically reaches good solutions in seconds for medium-size datasets and ensembles, with 10000s of rows and 100s of trees, resulting in significant speedups over existing approaches. Our experiments demonstrate that ForestPrune produces parsimonious models that outperform models extracted by existing post-processing algorithms.
&lt;/p&gt;</description></item><item><title>SOM-CPC&#26159;&#19968;&#31181;&#21033;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#32452;&#32455;&#30340;&#20108;&#32500;&#27969;&#24418;&#20013;&#21487;&#35270;&#21270;&#39640;&#36895;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#20110;&#20854;&#20182;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#22909;&#22320;&#29702;&#35299;&#28508;&#22312;&#27169;&#24335;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2205.15875</link><description>&lt;p&gt;
SOM-CPC: &#21033;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20026;&#39640;&#36895;&#26102;&#38388;&#24207;&#21015;&#32467;&#26500;&#21270;&#34920;&#31034;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
SOM-CPC: Unsupervised Contrastive Learning with Self-Organizing Maps for Structured Representations of High-Rate Time Series. (arXiv:2205.15875v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15875
&lt;/p&gt;
&lt;p&gt;
SOM-CPC&#26159;&#19968;&#31181;&#21033;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#32452;&#32455;&#30340;&#20108;&#32500;&#27969;&#24418;&#20013;&#21487;&#35270;&#21270;&#39640;&#36895;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#20110;&#20854;&#20182;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#22909;&#22320;&#29702;&#35299;&#28508;&#22312;&#27169;&#24335;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#19981;&#26029;&#22686;&#22810;&#30340;&#20256;&#24863;&#22120;&#65292;&#36830;&#32493;&#30417;&#27979;&#24050;&#32463;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#25152;&#33719;&#21462;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#26159;&#39640;&#32500;&#30340;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#34920;&#29616;&#21147;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#38477;&#32500;&#30340;&#28909;&#38376;&#36873;&#25321;&#65292;&#20294;&#25152;&#24471;&#21040;&#30340;&#28508;&#22312;&#31354;&#38388;&#36890;&#24120;&#38590;&#20197;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SOM-CPC &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26377;&#32452;&#32455;&#30340;&#20108;&#32500;&#27969;&#24418;&#20013;&#21487;&#35270;&#21270;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#26356;&#39640;&#32500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#23454;&#38469;&#22330;&#26223;&#20013;&#23578;&#26410;&#34987;&#24191;&#27867;&#25506;&#32034;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21363;&#39640;&#36895;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#65288;&#21253;&#25324;&#29983;&#29702;&#25968;&#25454;&#21644;&#38899;&#39057;&#35760;&#24405;&#65289;&#19978;&#23637;&#31034;&#20102; SOM-CPC &#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65288;&#22914;&#22522;&#20110; DL &#30340;&#29305;&#24449;&#25552;&#21462;&#65292;&#38543;&#21518;&#26159;&#24120;&#35268;&#38477;&#32500;&#25216;&#26415;&#65292;&#20197;&#21450;&#21516;&#26102;&#20248;&#21270; DL &#27169;&#22411;&#21644;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOM&#65289;&#30340;&#27169;&#22411;&#65289;&#12290;SOM-CPC &#20855;&#26377;&#26356;&#22909;&#22320;&#29702;&#35299;&#39640;&#36895;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#28508;&#22312;&#27169;&#24335;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous monitoring with an ever-increasing number of sensors has become ubiquitous across many application domains. However, acquired time series are typically high-dimensional and difficult to interpret. Expressive deep learning (DL) models have gained popularity for dimensionality reduction, but the resulting latent space often remains difficult to interpret. In this work we propose SOM-CPC, a model that visualizes data in an organized 2D manifold, while preserving higher-dimensional information. We address a largely unexplored and challenging set of scenarios comprising high-rate time series, and show on both synthetic and real-life data (physiological data and audio recordings) that SOM-CPC outperforms strong baselines like DL-based feature extraction, followed by conventional dimensionality reduction techniques, and models that jointly optimize a DL model and a Self-Organizing Map (SOM). SOM-CPC has great potential to acquire a better understanding of latent patterns in high-ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#39033;&#20851;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#21098;&#26525;&#21644;&#20923;&#32467;&#32593;&#32476;&#21442;&#25968;&#20943;&#23569;&#24050;&#35757;&#32451;&#26435;&#37325;&#25968;&#37327;&#30340;&#35843;&#26597;&#30740;&#31350;&#12290;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#21021;&#22987;&#21270;&#26102;&#30340;&#21098;&#26525;&#12289;&#22870;&#21169;&#24425;&#31080;&#21644;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#65292;&#32780;&#20923;&#32467;&#26435;&#37325;&#21516;&#26679;&#33021;&#22815;&#20943;&#23569;&#24050;&#35757;&#32451;&#30340;&#26435;&#37325;&#25968;&#37327;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#38477;&#20302;&#23384;&#20648;&#21644;&#20256;&#36755;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2205.08099</link><description>&lt;p&gt;
&#21098;&#26525;&#21644;&#20923;&#32467;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#37096;&#20998;&#20197;&#36827;&#34892;&#38477;&#32500;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;: &#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey. (arXiv:2205.08099v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#39033;&#20851;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#21098;&#26525;&#21644;&#20923;&#32467;&#32593;&#32476;&#21442;&#25968;&#20943;&#23569;&#24050;&#35757;&#32451;&#26435;&#37325;&#25968;&#37327;&#30340;&#35843;&#26597;&#30740;&#31350;&#12290;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#21021;&#22987;&#21270;&#26102;&#30340;&#21098;&#26525;&#12289;&#22870;&#21169;&#24425;&#31080;&#21644;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#65292;&#32780;&#20923;&#32467;&#26435;&#37325;&#21516;&#26679;&#33021;&#22815;&#20943;&#23569;&#24050;&#35757;&#32451;&#30340;&#26435;&#37325;&#25968;&#37327;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#38477;&#20302;&#23384;&#20648;&#21644;&#20256;&#36755;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#35745;&#25968;&#36798;&#21040;&#20102;&#21315;&#20159;&#32423;&#21035;&#12290;&#35757;&#32451;&#12289;&#23384;&#20648;&#21644;&#20256;&#36755;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#32791;&#36153;&#33021;&#37327;&#21644;&#26102;&#38388;&#30340;&#65292;&#22240;&#27492;&#20195;&#20215;&#24456;&#39640;&#12290;&#20854;&#20013;&#24456;&#22823;&#19968;&#37096;&#20998;&#25104;&#26412;&#26159;&#30001;&#32593;&#32476;&#30340;&#35757;&#32451;&#24341;&#36215;&#30340;&#12290;&#27169;&#22411;&#21387;&#32553;&#21487;&#20197;&#38477;&#20302;&#23384;&#20648;&#21644;&#20256;&#36755;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#21069;&#21521;&#25110;&#21518;&#21521;&#20256;&#36882;&#20013;&#30340;&#35745;&#31639;&#25968;&#37327;&#65292;&#36827;&#19968;&#27493;&#20351;&#35757;&#32451;&#26356;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#26102;&#21387;&#32553;&#32593;&#32476;&#24182;&#20445;&#25345;&#39640;&#24615;&#33021;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#26159;&#20851;&#20110;&#22914;&#20309;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#20943;&#23569;&#24050;&#35757;&#32451;&#26435;&#37325;&#25968;&#37327;&#30340;&#26041;&#27861;&#30340;&#35843;&#26597;&#30740;&#31350;&#12290;&#20171;&#32461;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#23558;&#32593;&#32476;&#21442;&#25968;&#35774;&#32622;&#20026;&#38646;&#65292;&#36825;&#34987;&#31216;&#20026;&#21098;&#26525;&#12290;&#25152;&#20171;&#32461;&#30340;&#21098;&#26525;&#26041;&#27861;&#34987;&#24402;&#31867;&#20026;&#21021;&#22987;&#21270;&#26102;&#30340;&#21098;&#26525;&#12289;&#22870;&#21169;&#24425;&#31080;&#21644;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#26102;&#20923;&#32467;&#32593;&#32476;&#19968;&#37096;&#20998;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20923;&#32467;&#26435;&#37325;&#65292;&#20063;&#21487;&#20197;&#20943;&#23569;&#24050;&#35757;&#32451;&#30340;&#26435;&#37325;&#25968;&#37327;&#12290;&#22312;&#26412;&#27425;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38477;&#32500;&#20248;&#21270;&#35757;&#32451;&#20013;&#21098;&#26525;&#21644;&#20923;&#32467;&#32593;&#32476;&#21442;&#25968;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art deep learning models have a parameter count that reaches into the billions. Training, storing and transferring such models is energy and time consuming, thus costly. A big part of these costs is caused by training the network. Model compression lowers storage and transfer costs, and can further make training more efficient by decreasing the number of computations in the forward and/or backward pass. Thus, compressing networks also at training time while maintaining a high performance is an important research topic. This work is a survey on methods which reduce the number of trained weights in deep learning models throughout the training. Most of the introduced methods set network parameters to zero which is called pruning. The presented pruning approaches are categorized into pruning at initialization, lottery tickets and dynamic sparse training. Moreover, we discuss methods that freeze parts of a network at its random initialization. By freezing weights, the number of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#36866;&#24212;&#31163;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#29366;&#24577;&#20215;&#20540;&#20989;&#25968;&#26469;&#20026;&#26426;&#22120;&#20154;&#22312;&#30495;&#23454;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#25506;&#32034;&#25552;&#20379;&#25351;&#23548;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#31232;&#30095;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2204.03140</link><description>&lt;p&gt;
&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#36827;&#34892;&#26426;&#22120;&#20154;&#25506;&#32034;&#30340;&#22312;&#32447;&#33258;&#36866;&#24212;&#31163;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation with Online Adaptation for Robot Exploration in Challenging Environments. (arXiv:2204.03140v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#36866;&#24212;&#31163;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#29366;&#24577;&#20215;&#20540;&#20989;&#25968;&#26469;&#20026;&#26426;&#22120;&#20154;&#22312;&#30495;&#23454;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#25506;&#32034;&#25552;&#20379;&#25351;&#23548;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#31232;&#30095;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#25506;&#32034;&#20855;&#26377;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#20449;&#24687;&#22686;&#30410;&#25110;&#22522;&#20110;&#21069;&#27839;&#30340;&#20256;&#32479;&#25506;&#32034;&#20165;&#20381;&#36182;&#20110;&#26426;&#22120;&#20154;&#24403;&#21069;&#29366;&#24577;&#26469;&#30830;&#23450;&#21363;&#26102;&#25506;&#32034;&#30446;&#26631;&#65292;&#32570;&#20047;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#20215;&#20540;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#23548;&#33268;&#25506;&#32034;&#20915;&#31574;&#20302;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23398;&#20064;&#22914;&#20309;&#34913;&#37327;&#8220;&#22909;&#8221;&#29366;&#24577;&#65288;&#20197;&#29366;&#24577;&#20215;&#20540;&#20989;&#25968;&#34913;&#37327;&#65289;&#65292;&#20026;&#26426;&#22120;&#20154;&#22312;&#30495;&#23454;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#25506;&#32034;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#24037;&#20316;&#21046;&#23450;&#20026;&#26426;&#22120;&#20154;&#25506;&#32034;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPERE&#65289;&#38382;&#39064;&#12290;&#23427;&#21253;&#25324;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#31163;&#32447;&#33945;&#29305;&#21345;&#32599;&#35757;&#32451;&#65292;&#24182;&#25191;&#34892;&#26102;&#38388;&#24046;&#20998;&#65288;TD&#65289;&#22312;&#32447;&#33258;&#36866;&#24212;&#26469;&#20248;&#21270;&#32463;&#36807;&#35757;&#32451;&#30340;&#20215;&#20540;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#24863;&#22120;&#20449;&#24687;&#35206;&#30422;&#29575;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#31232;&#30095;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#30340;&#20215;&#20540;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#20570;&#20986;&#39640;&#25928;&#30340;&#25506;&#32034;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous exploration has many important applications. However, classic information gain-based or frontier-based exploration only relies on the robot current state to determine the immediate exploration goal, which lacks the capability of predicting the value of future states and thus leads to inefficient exploration decisions. This paper presents a method to learn how "good" states are, measured by the state value function, to provide a guidance for robot exploration in real-world challenging environments. We formulate our work as an off-policy evaluation (OPE) problem for robot exploration (OPERE). It consists of offline Monte-Carlo training on real-world data and performs Temporal Difference (TD) online adaptation to optimize the trained value estimator. We also design an intrinsic reward function based on sensor information coverage to enable the robot to gain more information with sparse extrinsic rewards. Results show that our method enables the robot to predict the value of fut
&lt;/p&gt;</description></item><item><title>HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2203.03691</link><description>&lt;p&gt;
HyperMixer&#65306;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#20302;&#25104;&#26412;Transformer&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03691
&lt;/p&gt;
&lt;p&gt;
HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#39318;&#36873;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#26412;&#30456;&#24403;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36755;&#20837;&#38271;&#24230;&#26041;&#38754;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#33021;&#38590;&#20197;&#35843;&#25972;&#12290;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31616;&#21333;&#30340;&#22522;&#20110;MLP&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26550;&#26500;&#65288;&#20363;&#22914;MLPMixer&#65289;&#36890;&#36807;&#38745;&#24577;&#30340;MLP&#29420;&#31435;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#29305;&#24449;&#65292;&#32780;&#36807;&#20110;&#33073;&#31163;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25152;&#38656;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21363;HyperMixer&#65292;&#23427;&#20351;&#29992;&#36229;&#32593;&#32476;&#21160;&#24577;&#22320;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#26367;&#20195;&#30340;&#22522;&#20110;MLP&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;Transformer&#23218;&#32654;&#12290;&#19982;Transformer&#19981;&#21516;&#65292;HyperMixer&#22312;&#22788;&#29702;&#26102;&#38388;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#20855;&#26377;&#22823;&#22823;&#38477;&#20302;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#26102;&#38388;&#23610;&#24230;Actor Critic&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22686;&#30410;&#23450;&#29702;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#24050;&#32463;&#36798;&#21040;&#20102;&#26368;&#20339;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2203.02591</link><description>&lt;p&gt;
&#21333;&#26102;&#38388;&#23610;&#24230;Actor Critic&#30340;&#23567;&#22686;&#30410;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Small Gain Analysis of Single Timescale Actor Critic. (arXiv:2203.02591v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#26102;&#38388;&#23610;&#24230;Actor Critic&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22686;&#30410;&#23450;&#29702;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#24050;&#32463;&#36798;&#21040;&#20102;&#26368;&#20339;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#20351;&#29992;&#27604;&#20363;&#27493;&#38271;&#21644;&#27599;&#20010;Actor&#27493;&#39588;&#20174;&#31283;&#23450;&#20998;&#24067;&#20013;&#36873;&#25321;&#19968;&#20010;&#26679;&#26412;&#36827;&#34892;&#21333;&#20010;Critic&#26356;&#26032;&#30340;Actor-Critic&#29256;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#23567;&#22686;&#30410;&#23450;&#29702;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25214;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#24471;&#21040;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#30340;&#25216;&#26415;&#27700;&#24179;&#25552;&#39640;&#21040;&#20102;$O(\mu^{-2}\epsilon^{-2})$&#65292;&#21363;&#26597;&#25214;&#19968;&#20010;$\epsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#65292;&#20854;&#20013;$\mu$&#26159;&#19982;&#35780;&#35770;&#23478;&#30456;&#20851;&#30340;&#26465;&#20214;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a version of actor-critic which uses proportional step-sizes and only one critic update with a single sample from the stationary distribution per actor step. We provide an analysis of this method using the small-gain theorem. Specifically, we prove that this method can be used to find a stationary point, and that the resulting sample complexity improves the state of the art for actor-critic methods to $O \left(\mu^{-2} \epsilon^{-2} \right)$ to find an $\epsilon$-approximate stationary point where $\mu$ is the condition number associated with the critic.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30417;&#30563;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#31867;&#39044;&#27979;&#21644;&#29305;&#24449;&#22270;&#26469;&#30417;&#30563;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#30340;&#27973;&#23618;&#65292;&#36890;&#36807;&#22522;&#20110;&#25439;&#22833;&#30340;&#26435;&#37325;&#20998;&#37197;&#31574;&#30053;&#33258;&#36866;&#24212;&#24179;&#34913;&#21508;&#20010;&#27973;&#23618;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2202.07846</link><description>&lt;p&gt;
&#28145;&#24230;&#30417;&#30563;&#19979;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation with Deep Supervision. (arXiv:2202.07846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30417;&#30563;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#31867;&#39044;&#27979;&#21644;&#29305;&#24449;&#22270;&#26469;&#30417;&#30563;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#30340;&#27973;&#23618;&#65292;&#36890;&#36807;&#22522;&#20110;&#25439;&#22833;&#30340;&#26435;&#37325;&#20998;&#37197;&#31574;&#30053;&#33258;&#36866;&#24212;&#24179;&#34913;&#21508;&#20010;&#27973;&#23618;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26088;&#22312;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#24040;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#25552;&#21319;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#25945;&#24072;&#39044;&#27979;&#20165;&#29992;&#20110;&#20026;&#23398;&#29983;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#25552;&#20379;&#30417;&#30563;&#20449;&#21495;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#27973;&#23618;&#23398;&#29983;&#27169;&#22411;&#22312;&#36880;&#23618;&#21453;&#21521;&#20256;&#25773;&#26102;&#32570;&#20047;&#20934;&#30830;&#30340;&#35757;&#32451;&#25351;&#23548;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#30417;&#30563;&#19979;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;DSKD&#65289;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#31867;&#39044;&#27979;&#21644;&#29305;&#24449;&#22270;&#26469;&#30417;&#30563;&#27973;&#23618;&#23398;&#29983;&#23618;&#30340;&#35757;&#32451;&#12290;&#22312;DSKD&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#25439;&#22833;&#30340;&#26435;&#37325;&#20998;&#37197;&#31574;&#30053;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24179;&#34913;&#27599;&#20010;&#27973;&#23618;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;&#21508;&#31181;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#23545;CIFAR-100&#21644;TinyImageNet&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#34920;&#26126;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation aims to enhance the performance of a lightweight student model by exploiting the knowledge from a pre-trained cumbersome teacher model. However, in the traditional knowledge distillation, teacher predictions are only used to provide the supervisory signal for the last layer of the student model, which may result in those shallow student layers lacking accurate training guidance in the layer-by-layer back propagation and thus hinders effective knowledge transfer. To address this issue, we propose Deeply-Supervised Knowledge Distillation (DSKD), which fully utilizes class predictions and feature maps of the teacher model to supervise the training of shallow student layers. A loss-based weight allocation strategy is developed in DSKD to adaptively balance the learning process of each shallow layer, so as to further improve the student performance. Extensive experiments on CIFAR-100 and TinyImageNet with various teacher-student models show significantly performance, 
&lt;/p&gt;</description></item><item><title>pNLP-Mixer&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;MLP-Mixer&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#23884;&#20837;&#23618;&#65292;&#29992;&#20110;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21487;&#20197;&#36798;&#21040;&#22522;&#20110;transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21364;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2202.04350</link><description>&lt;p&gt;
pNLP-Mixer&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#20840;MLP&#26550;&#26500;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
pNLP-Mixer: an Efficient all-MLP Architecture for Language. (arXiv:2202.04350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04350
&lt;/p&gt;
&lt;p&gt;
pNLP-Mixer&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;MLP-Mixer&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#23884;&#20837;&#23618;&#65292;&#29992;&#20110;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21487;&#20197;&#36798;&#21040;&#22522;&#20110;transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21364;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;&#22312;&#26234;&#33021;&#25163;&#34920;&#31561;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#23436;&#20840;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22823;&#23567;&#21644;&#25512;&#29702;&#25104;&#26412;&#12290;&#20316;&#20026;Transformer&#26550;&#26500;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#26368;&#36817;&#20851;&#20110;&#39640;&#25928;NLP&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26435;&#37325;&#39640;&#25928;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20806;&#23383;&#33410;&#32423;&#30340;&#27169;&#22411;&#22823;&#23567;&#20013;&#33719;&#24471;&#31616;&#21333;&#20219;&#21153;(&#22914;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;)&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;pNLP-Mixer&#26550;&#26500;&#65292;&#19968;&#31181;&#29992;&#20110;&#35774;&#22791;&#19978;NLP&#30340;&#26080;&#23884;&#20837;MLP-Mixer&#27169;&#22411;&#65292;&#30001;&#20110;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#25237;&#24433;&#23618;&#65292;&#22240;&#27492;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22810;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;MTOP&#21644;multiATIS&#19978;&#35780;&#20272;&#20102;&#19968;&#20010;&#22823;&#23567;&#20165;&#20026;1&#20806;&#23383;&#33410;&#30340;pNLP-Mixer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#37327;&#21270;&#27169;&#22411;&#22312;MTOP&#21644;multi-ATIS&#19978;&#23454;&#29616;&#20102;mBERT&#30340;99.4&#65285;&#21644;97.8&#65285;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#36164;&#28304;&#20165;&#20026;mBERT&#30340;170&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models based on transformer architecture have drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot filling and intent classification, with model sizes in the order of the megabyte. This work introduces the pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP that achieves high weight-efficiency thanks to a novel projection layer. We evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves 99.4% and 97.8% the performance of mBERT on MTOP and multi-ATIS, while using 170x fewer 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861; FedGCN&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451; GCN &#27169;&#22411;&#36827;&#34892;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#23454;&#29616;&#25910;&#25947;&#24555;&#65292;&#36890;&#20449;&#37327;&#23567;&#65292;&#21516;&#26102;&#36824;&#33021;&#22815;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2201.12433</link><description>&lt;p&gt;
FedGCN&#65306;&#32852;&#37030;&#35757;&#32451;&#20013;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#25910;&#25947;&#19982;&#36890;&#20449;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
FedGCN: Convergence and Communication Tradeoffs in Federated Training of Graph Convolutional Networks. (arXiv:2201.12433v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12433
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861; FedGCN&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451; GCN &#27169;&#22411;&#36827;&#34892;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#23454;&#29616;&#25910;&#25947;&#24555;&#65292;&#36890;&#20449;&#37327;&#23567;&#65292;&#21516;&#26102;&#36824;&#33021;&#22815;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#20998;&#24067;&#20110;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#22270;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#22240;&#20854;&#22270;&#30340;&#35268;&#27169;&#21644;&#25968;&#25454;&#20445;&#30041;&#35268;&#23450;&#30340;&#21407;&#22240;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36830;&#25509;&#22270;&#33410;&#28857;&#30340;&#36328;&#23458;&#25143;&#31471;&#36793;&#32536;&#65292;&#21333;&#20010;&#36830;&#25509;&#22270;&#19981;&#33021;&#34987;&#20998;&#21035;&#20998;&#38548;&#21040;&#22810;&#20010;&#23458;&#25143;&#31471;&#12290;&#22240;&#27492;&#65292;&#22312;&#21333;&#20010;&#22270;&#19978;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#22411;&#20250;&#23548;&#33268;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36890;&#20449;&#24320;&#38144;&#24040;&#22823;&#25110;&#35757;&#32451;&#20013;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FedGCN&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#26469;&#35757;&#32451;&#29992;&#20110;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#30340;GCN&#27169;&#22411;&#65292;&#32780;&#19988;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#32780;&#19988;&#36890;&#20449;&#37327;&#36739;&#23567;&#12290;&#19982;&#20043;&#21069;&#38656;&#35201;&#22312;&#27599;&#20010;&#35757;&#32451;&#36718;&#27425;&#20013;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;FedGCN&#23458;&#25143;&#31471;&#20165;&#22312;&#19968;&#20010;&#39044;&#35757;&#32451;&#27493;&#39588;&#20013;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#26469;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedGCN&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#38598;&#20013;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#36890;&#20449;&#37327;&#26126;&#26174;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for training models on graphs distributed across multiple clients have recently grown in popularity, due to the size of these graphs as well as regulations on keeping data where it is generated. However, a single connected graph cannot be disjointly partitioned onto multiple clients due to the cross-client edges connecting graph nodes. Thus, distributed methods for training a model on a single graph incur either significant communication overhead between clients or a loss of available information to the training. We introduce the Federated Graph Convolutional Network (FedGCN) algorithm, which uses federated learning to train GCN models for semi-supervised node classification with fast convergence and little communication. Compared to prior methods that require communication among clients at each training round, FedGCN clients only communicate with the central server in one pre-training step, greatly reducing communication costs and allowing the use of homomorphic encryption to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#24179;&#28369;&#21644;&#24179;&#28369;&#20984;&#20248;&#21270;&#20013;&#65292;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#65288;SMD&#65289;&#22312;&#20869;&#25554;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#30456;&#23545;&#24179;&#28369;&#20984;&#20248;&#21270;&#20013;&#65292;&#20351;&#29992;&#24658;&#23450;&#27493;&#38271;&#30340;SMD&#20855;&#26377;&#26032;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#23545;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#26041;&#26696;&#8212;&#8212;&#38236;&#20687;&#38543;&#26426;Polyak&#27493;&#38271;&#65288;mSPS&#65289;&#12290;&#36825;&#20123;&#32467;&#26524;&#26159;&#39318;&#20010;&#22312;&#20869;&#25554;&#19979;&#23545;&#25351;&#25968;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#22266;&#23450;&#25110;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2110.15412</link><description>&lt;p&gt;
&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#65306;&#36890;&#36807;&#38236;&#20687;&#38543;&#26426;Polyak&#27493;&#38271;&#30340;&#33258;&#36866;&#24212;&#21464;&#20307;&#36827;&#34892;&#25910;&#25947;&#20998;&#26512;&#65288;&#26356;&#26032;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize. (arXiv:2110.15412v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#24179;&#28369;&#21644;&#24179;&#28369;&#20984;&#20248;&#21270;&#20013;&#65292;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#65288;SMD&#65289;&#22312;&#20869;&#25554;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#30456;&#23545;&#24179;&#28369;&#20984;&#20248;&#21270;&#20013;&#65292;&#20351;&#29992;&#24658;&#23450;&#27493;&#38271;&#30340;SMD&#20855;&#26377;&#26032;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#23545;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#26041;&#26696;&#8212;&#8212;&#38236;&#20687;&#38543;&#26426;Polyak&#27493;&#38271;&#65288;mSPS&#65289;&#12290;&#36825;&#20123;&#32467;&#26524;&#26159;&#39318;&#20010;&#22312;&#20869;&#25554;&#19979;&#23545;&#25351;&#25968;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#22266;&#23450;&#25110;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#30456;&#23545;&#24179;&#28369;&#21644;&#24179;&#28369;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#65288;SMD&#65289;&#22312;&#20869;&#25554;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#30456;&#23545;&#24179;&#28369;&#20984;&#20248;&#21270;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#24658;&#23450;&#27493;&#38271;&#30340;SMD&#30340;&#26032;&#25910;&#25947;&#20445;&#35777;&#12290;&#23545;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#26041;&#26696;&#8212;&#8212;&#38236;&#20687;&#38543;&#26426;Polyak&#27493;&#38271;&#65288;mSPS&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#32467;&#26524;&#37117;&#19981;&#20570;&#26377;&#30028;&#26799;&#24230;&#20551;&#35774;&#25110;&#26377;&#30028;&#26041;&#24046;&#20551;&#35774;&#65292;&#24182;&#19988;&#25105;&#20204;&#26174;&#31034;&#20102;&#22312;&#20869;&#25554;&#19979;&#36880;&#28176;&#28040;&#22833;&#30340;&#37051;&#22495;&#20869;&#30340;&#25910;&#25947;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#22266;&#23450;&#25110;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#25351;&#25968;&#26799;&#24230;&#31639;&#27861;&#26469;&#35828;&#65292;&#37117;&#26159;&#39318;&#27425;&#22312;&#20869;&#25554;&#19979;&#33719;&#24471;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;mSPS&#23558;&#26368;&#36817;&#25552;&#20986;&#30340;&#38543;&#26426;Polyak&#27493;&#38271;&#65288;SPS&#65289;&#65288;Loizou&#31561;&#20154;&#65292;2021&#65289;&#25512;&#24191;&#21040;&#38236;&#20687;&#19979;&#38477;&#65292;&#24182;&#22312;&#32487;&#25215;&#38236;&#20687;&#19979;&#38477;&#30340;&#20248;&#28857;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#23454;&#29992;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the convergence of stochastic mirror descent (SMD) under interpolation in relatively smooth and smooth convex optimization. In relatively smooth convex optimization we provide new convergence guarantees for SMD with a constant stepsize. For smooth convex optimization we propose a new adaptive stepsize scheme -- the mirror stochastic Polyak stepsize (mSPS). Notably, our convergence results in both settings do not make bounded gradient assumptions or bounded variance assumptions, and we show convergence to a neighborhood that vanishes under interpolation. Consequently, these results correspond to the first convergence guarantees under interpolation for the exponentiated gradient algorithm for fixed or adaptive stepsizes. mSPS generalizes the recently proposed stochastic Polyak stepsize (SPS) (Loizou et al. 2021) to mirror descent and remains both practical and efficient for modern machine learning applications while inheriting the benefits of mirror descent. We complement 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20855;&#21464;&#37327;&#21435;&#38500;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#20559;&#24046;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.01438</link><description>&lt;p&gt;
&#22522;&#20110;&#24037;&#20855;&#21464;&#37327;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#39537;&#21160;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Instrumental Variable-Driven Domain Generalization with Unobserved Confounders. (arXiv:2110.01438v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.01438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20855;&#21464;&#37327;&#21435;&#38500;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#20559;&#24046;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#27867;&#21270;&#26088;&#22312;&#20174;&#22810;&#20010;&#28304;&#39046;&#22495;&#20013;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#22312;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23398;&#20064;&#20855;&#26377;&#19981;&#21464;&#36793;&#38469;&#20998;&#24067;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#26631;&#31614;&#32473;&#23450;&#36755;&#20837;&#29305;&#24449;&#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#19981;&#21464;&#24615;&#23545;&#20110;&#26410;&#30693;&#39046;&#22495;&#30340;&#39044;&#27979;&#26356;&#20026;&#22522;&#26412;&#12290;&#21516;&#26102;&#65292;&#24433;&#21709;&#36755;&#20837;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#23384;&#22312;&#20250;&#23548;&#33268;&#20266;&#30456;&#20851;&#24615;&#65292;&#24182;&#38459;&#30861;&#21253;&#21547;&#26465;&#20214;&#20998;&#24067;&#20013;&#21253;&#21547;&#19981;&#21464;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#22240;&#26524;&#24615;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#39046;&#22495;&#30340;&#36755;&#20837;&#29305;&#24449;&#26159;&#20854;&#20182;&#39046;&#22495;&#30340;&#26377;&#25928;&#24037;&#20855;&#21464;&#37327;&#12290;&#21463;&#27492;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20855;&#21464;&#37327;&#39537;&#21160;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861; (IV-DG)&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#23398;&#20064;&#21435;&#38500;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) aims to learn from multiple source domains a model that can generalize well on unseen target domains. Existing DG methods mainly learn the representations with invariant marginal distribution of the input features, however, the invariance of the conditional distribution of the labels given the input features is more essential for unknown domain prediction. Meanwhile, the existing of unobserved confounders which affect the input features and labels simultaneously cause spurious correlation and hinder the learning of the invariant relationship contained in the conditional distribution. Interestingly, with a causal view on the data generating process, we find that the input features of one domain are valid instrumental variables for other domains. Inspired by this finding, we propose an instrumental variable-driven DG method (IV-DG) by removing the bias of the unobserved confounders with two-stage learning. In the first stage, it learns the conditional distribut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#8212;&#8212;&#36125;&#21494;&#26031;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#65292;&#26088;&#22312;&#35299;&#20915;&#26631;&#20934;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#28040;&#38500;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#20110;&#26377;&#38480;&#23485;&#24230;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#65292;&#24182;&#20445;&#30041;&#26631;&#20934;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#30340;&#31616;&#21333;&#24615;&#12290;</title><link>http://arxiv.org/abs/2108.13097</link><description>&lt;p&gt;
&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#29702;&#35770;&#32473;&#20986;&#20102;&#26680;&#26041;&#27861;&#30340;&#28145;&#24230;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A theory of representation learning in deep neural networks gives a deep generalisation of kernel methods. (arXiv:2108.13097v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.13097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#8212;&#8212;&#36125;&#21494;&#26031;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#65292;&#26088;&#22312;&#35299;&#20915;&#26631;&#20934;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#28040;&#38500;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#20110;&#26377;&#38480;&#23485;&#24230;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#65292;&#24182;&#20445;&#30041;&#26631;&#20934;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#30340;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#25104;&#21151;&#22522;&#20110;&#23427;&#20204;&#36328;&#22810;&#20010;&#23618;&#27425;&#23545;&#36755;&#20837;&#36827;&#34892;&#21464;&#25442;&#20197;&#24314;&#31435;&#33391;&#22909;&#30340;&#39640;&#32423;&#34920;&#31034;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24120;&#35268;&#30340;&#29702;&#35770;&#26041;&#27861;&#65288;&#27491;&#24335;&#20026;NNGPs&#65289;&#28041;&#21450;&#26080;&#38480;&#23485;&#38480;&#21046;&#28040;&#38500;&#20102;&#34920;&#31034;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38480;&#23485;&#38480;&#21046;&#8212;&#8212;&#36125;&#21494;&#26031;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#65292;&#23427;&#23637;&#29616;&#20102;&#22312;&#26377;&#38480;&#23485;&#24230;&#27169;&#22411;&#20013;&#38236;&#20687;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#19968;&#20123;&#26631;&#20934;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#30340;&#31616;&#21333;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#36125;&#21494;&#26031;&#34920;&#31034;&#23398;&#20064;&#26497;&#38480;&#19979;&#30340;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#65288;DGPs&#65289;&#20855;&#26377;&#30830;&#20999;&#30340;&#22810;&#20803;&#39640;&#26031;&#21518;&#39564;&#20998;&#24067;&#65292;&#21518;&#39564;&#21327;&#26041;&#24046;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#19968;&#31181;&#21487;&#35299;&#37322;&#30446;&#26631;&#24471;&#21040;&#65292;&#35813;&#30446;&#26631;&#32467;&#21512;&#20102;&#22686;&#24378;&#24615;&#33021;&#30340;&#23545;&#25968;&#20284;&#28982;&#21644;&#19968;&#31995;&#21015;&#30340;KL-&#25955;&#24230;&#65292;&#20351;&#24471;&#21518;&#39564;&#20998;&#24067;&#25509;&#36817;&#20808;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
The successes of modern deep machine learning methods are founded on their ability to transform inputs across multiple layers to build good high-level representations. It is therefore critical to understand this process of representation learning. However, standard theoretical approaches (formally NNGPs) involving infinite width limits eliminate representation learning. We therefore develop a new infinite width limit, the Bayesian representation learning limit, that exhibits representation learning mirroring that in finite-width models, yet at the same time, retains some of the simplicity of standard infinite-width limits. In particular, we show that Deep Gaussian processes (DGPs) in the Bayesian representation learning limit have exactly multivariate Gaussian posteriors, and the posterior covariances can be obtained by optimizing an interpretable objective combining a log-likelihood to improve performance with a series of KL-divergences which keep the posteriors close to the prior. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeepFreight&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#22810;&#27573;&#36135;&#36816;&#37197;&#36865;&#31639;&#27861;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#36710;&#38431;&#35843;&#24230;&#21644;&#21253;&#35065;&#21305;&#37197;&#65292;&#30830;&#20445;100%&#30340;&#20132;&#20184;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2103.03450</link><description>&lt;p&gt;
DeepFreight&#65306;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38598;&#25104;&#20110;&#22810;&#27425;&#36716;&#31227;&#21345;&#36710;&#36135;&#36816;&#37197;&#36865;&#20013;
&lt;/p&gt;
&lt;p&gt;
DeepFreight: Integrating Deep Reinforcement Learning and Mixed Integer Programming for Multi-transfer Truck Freight Delivery. (arXiv:2103.03450v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeepFreight&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#22810;&#27573;&#36135;&#36816;&#37197;&#36865;&#31639;&#27861;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#36710;&#38431;&#35843;&#24230;&#21644;&#21253;&#35065;&#21305;&#37197;&#65292;&#30830;&#20445;100%&#30340;&#20132;&#20184;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36135;&#36816;&#20132;&#20184;&#38656;&#27714;&#21644;&#36816;&#36153;&#19981;&#26029;&#22686;&#21152;&#65292;&#26234;&#33021;&#25511;&#21046;&#36710;&#38431;&#20197;&#23454;&#29616;&#39640;&#25928;&#21644;&#33410;&#32422;&#25104;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeepFreight&#65292;&#19968;&#31181;&#22522;&#20110;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27573;&#36135;&#36816;&#37197;&#36865;&#31639;&#27861;&#65292;&#21253;&#25324;&#21345;&#36710;&#35843;&#24230;&#21644;&#21253;&#35065;&#21305;&#37197;&#20004;&#20010;&#32039;&#23494;&#21327;&#20316;&#30340;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21033;&#29992;&#31216;&#20026;QMIX&#30340;&#28145;&#24230;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#19968;&#31181;&#35843;&#24230;&#31574;&#30053;&#65292;&#36890;&#36807;&#35813;&#31574;&#30053;&#21487;&#20197;&#24471;&#21040;&#19982;&#20132;&#20184;&#35831;&#27714;&#30456;&#20851;&#30340;&#36710;&#38431;&#30340;&#22810;&#27493;&#32852;&#21512;&#36710;&#36742;&#35843;&#24230;&#20915;&#31574;&#12290;&#28982;&#21518;&#25191;&#34892;&#39640;&#25928;&#30340;&#22810;&#27425;&#36716;&#31227;&#21305;&#37197;&#31639;&#27861;&#23558;&#20132;&#20184;&#35831;&#27714;&#20998;&#37197;&#32473;&#21345;&#36710;&#12290;&#27492;&#22806;&#65292;DeepFreight&#19982;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20248;&#21270;&#22120;&#38598;&#25104;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#25193;&#23637;&#24615;&#24378;&#65292;&#21487;&#30830;&#20445;100&#65285;&#30340;&#20132;&#20184;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#20302;&#30340;&#24310;&#36831;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the freight delivery demands and shipping costs increasing rapidly, intelligent control of fleets to enable efficient and cost-conscious solutions becomes an important problem. In this paper, we propose DeepFreight, a model-free deep-reinforcement-learning-based algorithm for multi-transfer freight delivery, which includes two closely-collaborative components: truck-dispatch and package-matching. Specifically, a deep multi-agent reinforcement learning framework called QMIX is leveraged to learn a dispatch policy, with which we can obtain the multi-step joint vehicle dispatch decisions for the fleet with respect to the delivery requests. Then an efficient multi-transfer matching algorithm is executed to assign the delivery requests to the trucks. Also, DeepFreight is integrated with a Mixed-Integer Linear Programming optimizer for further optimization. The evaluation results show that the proposed system is highly scalable and ensures a 100\% delivery success while maintaining low 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#25913;&#36827;&#25968;&#25454;&#21516;&#21270;&#31639;&#27861;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#19968;&#33324;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#38750;&#39640;&#26031;&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2010.09694</link><description>&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Assimilation Networks. (arXiv:2010.09694v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.09694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#25913;&#36827;&#25968;&#25454;&#21516;&#21270;&#31639;&#27861;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#19968;&#33324;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#38750;&#39640;&#26031;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#31995;&#32479;&#30340;&#25968;&#23398;&#34920;&#31034;&#21644;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#22024;&#26434;&#35266;&#23519;&#26469;&#39044;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#29366;&#24577;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22522;&#20110;&#39640;&#26031;&#35823;&#24046;&#32479;&#35745;&#21644;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#32447;&#24615;&#21270;&#65292;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#25913;&#36827;&#36825;&#20123;&#26041;&#27861;&#20173;&#26377;&#24453;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23427;&#25512;&#24191;&#20102;&#24490;&#29615;Elman&#32593;&#32476;&#21644;&#25968;&#25454;&#21516;&#21270;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#32473;&#23450;&#22024;&#26434;&#35266;&#23519;&#19979;&#36817;&#20284;&#20110;&#19968;&#31995;&#21015;&#20808;&#39564;&#21644;&#21518;&#39564;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#30340;&#26500;&#36896;&#20351;&#20854;&#21487;&#29992;&#20110;&#19968;&#33324;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#38750;&#39640;&#26031;&#23494;&#24230;&#12290;&#22312;&#22522;&#20110;&#33879;&#21517;&#30340;Lorenz-95&#31995;&#32479;&#21644;&#39640;&#26031;&#35823;&#24046;&#32479;&#35745;&#30340;&#25968;&#23383;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#20998;&#26512;&#21644;&#20256;&#25773;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;EnKF&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data assimilation (DA) aims at forecasting the state of a dynamical system by combining a mathematical representation of the system with noisy observations taking into account their uncertainties. State of the art methods are based on the Gaussian error statistics and the linearization of the non-linear dynamics which may lead to sub-optimal methods. In this respect, there are still open questions how to improve these methods. In this paper, we propose a fully data driven deep learning architecture generalizing recurrent Elman networks and data assimilation algorithms which approximate a sequence of prior and posterior densities conditioned on noisy observations. By construction our approach can be used for general nonlinear dynamics and non-Gaussian densities. On numerical experiments based on the well-known Lorenz-95 system and with Gaussian error statistics, our architecture achieves comparable performance to EnKF on both the analysis and the propagation of probability density funct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;UCB&#26368;&#20248;&#25289;&#33218;&#31574;&#30053;&#65292;&#25104;&#26412;&#20026;$\sqrt{\log T}$&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#27492;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2008.09312</link><description>&lt;p&gt;
UCB Bandits&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#25915;&#20987;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Near Optimal Adversarial Attack on UCB Bandits. (arXiv:2008.09312v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.09312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;UCB&#26368;&#20248;&#25289;&#33218;&#31574;&#30053;&#65292;&#25104;&#26412;&#20026;$\sqrt{\log T}$&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#27492;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#22870;&#21169;&#21463;&#21040;&#23545;&#25239;&#24615;&#30772;&#22351;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#36890;&#36807;&#25805;&#20316;UCB&#21407;&#21017;&#26469;&#25289;&#21160;&#19968;&#20123;&#38750;&#26368;&#20248;&#30446;&#26631;&#33218;$T-o(T)$&#27425;&#65292;&#32047;&#31215;&#25104;&#26412;&#30340;&#26631;&#24230;&#20026;$\sqrt{\log T}$&#65292;&#20854;&#20013;$T$&#20026;&#22238;&#21512;&#25968;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#32047;&#31215;&#25915;&#20987;&#25104;&#26412;&#30340;&#31532;&#19968;&#20010;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#19982;&#25105;&#20204;&#30340;&#19978;&#30028;&#21305;&#37197;&#65292;&#38500;&#20102;$\log\log T$&#22240;&#23376;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a stochastic multi-arm bandit problem where rewards are subject to adversarial corruption. We propose a novel attack strategy that manipulates a UCB principle into pulling some non-optimal target arm $T - o(T)$ times with a cumulative cost that scales as $\sqrt{\log T}$, where $T$ is the number of rounds. We also prove the first lower bound on the cumulative attack cost. Our lower bound matches our upper bound up to $\log \log T$ factors, showing our attack to be near optimal.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24212;&#29992;&#23433;&#20840;&#32858;&#21512;&#21518;&#65292;&#21333;&#20010;&#35757;&#32451;&#38598;&#30340;&#36136;&#37327;&#20449;&#24687;&#20173;&#21487;&#33021;&#34987;&#25512;&#26029;&#24182;&#24402;&#22240;&#20110;&#20855;&#20307;&#21442;&#19982;&#32773;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22270;&#20687;&#35782;&#21035;&#23454;&#39564;&#25214;&#20986;&#20102;&#21442;&#19982;&#32773;&#30456;&#23545;&#30340;&#36136;&#37327;&#25490;&#24207;&#65292;&#36827;&#32780;&#29992;&#20110;&#26816;&#27979;&#19981;&#33391;&#34892;&#20026;&#12289;&#31283;&#23450;&#35757;&#32451;&#24615;&#33021;&#20197;&#21450;&#27979;&#37327;&#21442;&#19982;&#32773;&#30340;&#20010;&#20154;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2007.06236</link><description>&lt;p&gt;
&#24102;&#26377;&#23433;&#20840;&#32858;&#21512;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36136;&#37327;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Quality Inference in Federated Learning with Secure Aggregation. (arXiv:2007.06236v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.06236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24212;&#29992;&#23433;&#20840;&#32858;&#21512;&#21518;&#65292;&#21333;&#20010;&#35757;&#32451;&#38598;&#30340;&#36136;&#37327;&#20449;&#24687;&#20173;&#21487;&#33021;&#34987;&#25512;&#26029;&#24182;&#24402;&#22240;&#20110;&#20855;&#20307;&#21442;&#19982;&#32773;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22270;&#20687;&#35782;&#21035;&#23454;&#39564;&#25214;&#20986;&#20102;&#21442;&#19982;&#32773;&#30456;&#23545;&#30340;&#36136;&#37327;&#25490;&#24207;&#65292;&#36827;&#32780;&#29992;&#20110;&#26816;&#27979;&#19981;&#33391;&#34892;&#20026;&#12289;&#31283;&#23450;&#35757;&#32451;&#24615;&#33021;&#20197;&#21450;&#27979;&#37327;&#21442;&#19982;&#32773;&#30340;&#20010;&#20154;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#38556;&#20010;&#20154;&#21644;&#21830;&#19994;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#26426;&#23494;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26082;&#32771;&#34385;&#20102;&#25928;&#29575;&#65292;&#20063;&#27880;&#37325;&#19981;&#20849;&#20139;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26426;&#21046;&#20173;&#21487;&#33021;&#27844;&#28431;&#25935;&#24863;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#23433;&#20840;&#32858;&#21512;&#26469;&#38450;&#27490;&#24402;&#22240;&#20110;&#29305;&#23450;&#21442;&#19982;&#32773;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#21333;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#26174;&#31034;&#21363;&#20351;&#24212;&#29992;&#20102;&#23433;&#20840;&#32858;&#21512;&#65292;&#36825;&#26679;&#30340;&#36136;&#37327;&#20449;&#24687;&#20173;&#21487;&#33021;&#34987;&#25512;&#26029;&#24182;&#24402;&#22240;&#20110;&#20855;&#20307;&#21442;&#19982;&#32773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#22270;&#20687;&#35782;&#21035;&#23454;&#39564;&#65292;&#25105;&#20204;&#25512;&#26029;&#21442;&#19982;&#32773;&#30340;&#30456;&#23545;&#36136;&#37327;&#25490;&#24207;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25512;&#26029;&#20986;&#30340;&#36136;&#37327;&#20449;&#24687;&#24212;&#29992;&#20110;&#26816;&#27979;&#19981;&#33391;&#34892;&#20026;&#12289;&#31283;&#23450;&#35757;&#32451;&#24615;&#33021;&#20197;&#21450;&#27979;&#37327;&#21442;&#19982;&#32773;&#30340;&#20010;&#20154;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning algorithms are developed both for efficiency reasons and to ensure the privacy and confidentiality of personal and business data, respectively. Despite no data being shared explicitly, recent studies showed that the mechanism could still leak sensitive information. Hence, secure aggregation is utilized in many real-world scenarios to prevent attribution to specific participants. In this paper, we focus on the quality of individual training datasets and show that such quality information could be inferred and attributed to specific participants even when secure aggregation is applied. Specifically, through a series of image recognition experiments, we infer the relative quality ordering of participants. Moreover, we apply the inferred quality information to detect misbehaviours, to stabilize training performance, and to measure the individual contributions of participants.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#32447;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(LRNNs)&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#26102;&#21464;&#20989;&#25968;f(t)&#12290;&#36890;&#36807;&#26816;&#26597;&#32593;&#32476;&#36716;&#31227;&#30697;&#38453;&#30340;&#20027;&#35201;&#29305;&#24449;&#20540;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LRNN&#30340;&#35268;&#27169;&#12290;LRNNs&#20855;&#26377;&#20197;&#26925;&#22278;&#36712;&#36857;&#32467;&#26463;&#30340;&#26377;&#36259;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#39044;&#27979;&#36827;&#19968;&#27493;&#30340;&#20540;&#21644;&#20989;&#25968;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/1802.03308</link><description>&lt;p&gt;
&#32447;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
The Power of Linear Recurrent Neural Networks. (arXiv:1802.03308v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1802.03308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#32447;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(LRNNs)&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#26102;&#21464;&#20989;&#25968;f(t)&#12290;&#36890;&#36807;&#26816;&#26597;&#32593;&#32476;&#36716;&#31227;&#30697;&#38453;&#30340;&#20027;&#35201;&#29305;&#24449;&#20540;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LRNN&#30340;&#35268;&#27169;&#12290;LRNNs&#20855;&#26377;&#20197;&#26925;&#22278;&#36712;&#36857;&#32467;&#26463;&#30340;&#26377;&#36259;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#39044;&#27979;&#36827;&#19968;&#27493;&#30340;&#20540;&#21644;&#20989;&#25968;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26159;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;autoregressive linear,&#21363;&#32447;&#24615;&#28608;&#27963;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(LRNNs)&#21487;&#20197;&#36924;&#36817;&#30001;&#22810;&#20010;&#20989;&#25968;&#20540;&#32473;&#20986;&#30340;&#20219;&#20309;&#26102;&#21464;&#20989;&#25968;f(t)&#12290;&#36924;&#36817;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#35299;&#20915;&#19968;&#20010;&#32447;&#24615;&#26041;&#31243;&#32452;&#26469;&#26377;&#25928;&#23398;&#20064;&#65307;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#25110;&#31867;&#20284;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#21487;&#33021;&#26159;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#36890;&#36807;&#26816;&#26597;&#32593;&#32476;&#36716;&#31227;&#30697;&#38453;&#30340;&#39057;&#35889;&#65292;&#21363;&#23427;&#30340;&#29305;&#24449;&#20540;&#65292;&#21482;&#21462;&#26368;&#30456;&#20851;&#30340;&#32452;&#20214;&#65292;&#21487;&#20197;&#22312;&#19968;&#27493;&#20013;&#26174;&#33879;&#38477;&#20302;LRNN&#30340;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#32593;&#32476;&#26435;&#37325;&#65292;&#36824;&#21487;&#20197;&#23398;&#20064;&#32593;&#32476;&#26550;&#26500;&#12290;LRNNs&#20855;&#26377;&#26377;&#36259;&#30340;&#29305;&#24615;&#65306;&#23427;&#20204;&#26368;&#32456;&#20250;&#20197;&#26925;&#22278;&#36712;&#36857;&#32467;&#26463;&#65292;&#24182;&#20801;&#35768;&#39044;&#27979;&#36827;&#19968;&#27493;&#30340;&#20540;&#21644;&#20989;&#25968;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#28436;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks are a powerful means to cope with time series. We show how autoregressive linear, i.e., linearly activated recurrent neural networks (LRNNs) can approximate any time-dependent function f(t) given by a number of function values. The approximation can effectively be learned by simply solving a linear equation system; no backpropagation or similar methods are needed. Furthermore, and this is probably the main contribution of this article, the size of an LRNN can be reduced significantly in one step after inspecting the spectrum of the network transition matrix, i.e., its eigenvalues, by taking only the most relevant components. Therefore, in contrast to other approaches, we do not only learn network weights but also the network architecture. LRNNs have interesting properties: They end up in ellipse trajectories in the long run and allow the prediction of further values and compact representations of functions. We demonstrate this by several experiments, among the
&lt;/p&gt;</description></item></channel></rss>