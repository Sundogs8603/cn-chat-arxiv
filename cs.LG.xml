<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#31639;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26410;&#32463;&#30740;&#31350;&#30340;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.07535</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#20165;&#20973;&#23569;&#37327;&#27979;&#35797;&#26679;&#26412;&#25913;&#21892;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift. (arXiv:2310.07535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07535
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#31639;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26410;&#32463;&#30740;&#31350;&#30340;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#34920;&#29616;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30830;&#20445;&#19981;&#21516;&#25935;&#24863;&#32676;&#20307;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#28041;&#21450;&#21040;&#35832;&#22914;&#21009;&#20107;&#21496;&#27861;&#31561;&#31038;&#20250;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25805;&#20316;&#65292;&#21482;&#26377;&#19968;&#23567;&#32452;&#26080;&#26631;&#31614;&#30340;&#27979;&#35797;&#26679;&#26412;&#21644;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#35757;&#32451;&#38598;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#22522;&#20110;&#26032;&#22411;&#22797;&#21512;&#21152;&#26435;&#29109;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#20960;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#22312;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#22312;&#25105;&#20204;&#25152;&#30693;&#30340;&#33539;&#22260;&#20869;&#23578;&#26410;&#30740;&#31350;&#36807;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covariate shift in the test data can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups in such settings is of paramount importance due to societal implications like criminal justice. We operate under the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards this problem, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#40657;&#30418;&#23376;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#65292;&#23458;&#35266;&#35780;&#20272;&#20102;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07534</link><description>&lt;p&gt;
XAI&#26041;&#27861;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human-Centered Evaluation of XAI Methods. (arXiv:2310.07534v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07534
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#40657;&#30418;&#23376;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#65292;&#23458;&#35266;&#35780;&#20272;&#20102;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#35299;&#26512;&#28145;&#24230;&#23398;&#20064;&#20013;&#25152;&#35859;&#30340;&#8220;&#40657;&#30418;&#23376;&#8221;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#37322;&#21508;&#31181;&#20219;&#21153;&#30340;&#20915;&#31574;&#12290;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20250;&#35782;&#21035;&#24182;&#24378;&#35843;&#23545;&#20998;&#31867;&#22120;&#39044;&#27979;&#24433;&#21709;&#26368;&#22823;&#30340;&#20851;&#38190;&#20687;&#32032;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#20154;&#31867;&#34892;&#20026;&#30456;&#20284;&#65306;&#24403;&#25105;&#20204;&#34987;&#35201;&#27714;&#35299;&#37322;&#20998;&#31867;&#22270;&#20687;&#30340;&#29702;&#30001;&#26102;&#65292;&#25105;&#20204;&#36890;&#24120;&#20250;&#25351;&#20986;&#26368;&#26174;&#33879;&#30340;&#29305;&#24449;&#25110;&#26041;&#38754;&#12290;&#21033;&#29992;&#36825;&#31181;&#31867;&#20284;&#24615;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35797;&#22270;&#23458;&#35266;&#22320;&#35780;&#20272;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#65288;1&#65289;&#20856;&#22411;&#23616;&#37096;&#32593;&#32476;&#12289;&#65288;2&#65289;&#36974;&#25377;&#21644;&#65288;3&#65289;&#23618;&#27425;&#30456;&#20851;&#20256;&#25773;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#25152;&#31361;&#20986;&#30340;&#21306;&#22495;&#21487;&#33021;&#24046;&#24322;&#24456;&#22823;&#65292;&#20294;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called "black boxes" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#22312;&#20989;&#25968;&#36924;&#36817;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;PQCs&#30340;&#26174;&#24335;&#26500;&#36896;&#21450;&#20854;&#23545;&#36830;&#32493;&#21644;&#24179;&#28369;&#20989;&#25968;&#36924;&#36817;&#30340;&#23450;&#37327;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.07528</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#22312;&#20989;&#25968;&#36924;&#36817;&#20013;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Provable Advantage of Parameterized Quantum Circuit in Function Approximation. (arXiv:2310.07528v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#22312;&#20989;&#25968;&#36924;&#36817;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;PQCs&#30340;&#26174;&#24335;&#26500;&#36896;&#21450;&#20854;&#23545;&#36830;&#32493;&#21644;&#24179;&#28369;&#20989;&#25968;&#36924;&#36817;&#30340;&#23450;&#37327;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65288;PQCs&#65289;&#22312;&#23436;&#25104;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#36890;&#36807;&#20989;&#25968;&#36924;&#36817;&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;PQCs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#20197;&#24448;&#23545;&#20110;PQCs&#30340;&#26222;&#36866;&#36924;&#36817;&#23450;&#29702;&#20027;&#35201;&#26159;&#38750;&#26500;&#36896;&#24615;&#30340;&#65292;&#30001;&#27492;&#24341;&#21457;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;PQCs&#38656;&#35201;&#22810;&#22823;&#25165;&#33021;&#20197;&#32473;&#23450;&#35823;&#24046;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;PQCs&#30340;&#26174;&#24335;&#26500;&#36896;&#65292;&#29992;&#20110;&#36924;&#36817;&#36830;&#32493;&#21644;&#24179;&#28369;&#20989;&#25968;&#65292;&#24182;&#24314;&#31435;&#20102;PQCs&#30340;&#23485;&#24230;&#12289;&#28145;&#24230;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#30340;&#23450;&#37327;&#36924;&#36817;&#35823;&#24046;&#30028;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#37327;&#23376;&#20449;&#21495;&#22788;&#29702;&#21644;&#37193;&#32447;&#24615;&#32452;&#21512;&#30340;&#25216;&#26415;&#26469;&#26500;&#36896;&#23454;&#29616;&#22810;&#20803;&#22810;&#39033;&#24335;&#30340;PQCs&#12290;&#25105;&#20204;&#20351;&#29992;&#20271;&#24681;&#26031;&#22374;&#22810;&#39033;&#24335;&#21644;&#23616;&#37096;&#27888;&#21202;&#36817;&#20284;&#25216;&#26415;&#23454;&#29616;&#20840;&#23616;&#21644;&#23616;&#37096;&#36924;&#36817;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the power of parameterized quantum circuits (PQCs) in accomplishing machine learning tasks is one of the most important questions in quantum machine learning. In this paper, we analyze the expressivity of PQCs through the lens of function approximation. Previously established universal approximation theorems for PQCs are mainly nonconstructive, leading us to the following question: How large do the PQCs need to be to approximate the target function up to a given error? We exhibit explicit constructions of data re-uploading PQCs for approximating continuous and smooth functions and establish quantitative approximation error bounds in terms of the width, the depth and the number of trainable parameters of the PQCs. To achieve this, we utilize techniques from quantum signal processing and linear combinations of unitaries to construct PQCs that implement multivariate polynomials. We implement global and local approximation techniques using Bernstein polynomials and local Tayl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22240;&#26524;&#22270;&#20808;&#39564;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21644;&#20998;&#35299;&#21160;&#24577;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#33258;&#28982;&#22320;&#35774;&#35745;&#20808;&#39564;&#65292;&#24182;&#19988;&#26681;&#25454;&#20808;&#39564;&#30693;&#35782;&#31243;&#24230;&#36830;&#25509;&#20102;&#36951;&#25022;&#29575;&#19982;&#36125;&#21494;&#26031;&#36951;&#25022;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07518</link><description>&lt;p&gt;
&#21033;&#29992;&#21518;&#39564;&#37319;&#26679;&#21644;&#22240;&#26524;&#22270;&#20808;&#39564;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning. (arXiv:2310.07518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22240;&#26524;&#22270;&#20808;&#39564;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21644;&#20998;&#35299;&#21160;&#24577;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#33258;&#28982;&#22320;&#35774;&#35745;&#20808;&#39564;&#65292;&#24182;&#19988;&#26681;&#25454;&#20808;&#39564;&#30693;&#35782;&#31243;&#24230;&#36830;&#25509;&#20102;&#36951;&#25022;&#29575;&#19982;&#36125;&#21494;&#26031;&#36951;&#25022;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#36716;&#31227;&#21160;&#24577;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#20808;&#39564;&#36890;&#24120;&#34987;&#25351;&#23450;&#20026;&#29615;&#22659;&#21464;&#37327;&#30340;&#65288;&#37096;&#20998;&#65289;&#22240;&#26524;&#22270;&#65292;&#30456;&#27604;&#23454;&#36341;&#20013;&#40635;&#28902;&#30340;&#21442;&#25968;&#20998;&#24067;&#31867;&#21035;&#25351;&#23450;&#26356;&#21152;&#33258;&#28982;&#65292;&#20363;&#22914;&#22312;&#21307;&#30103;&#27835;&#30103;&#30740;&#31350;&#20013;&#21015;&#20986;&#29983;&#29289;&#29305;&#24449;&#20043;&#38388;&#30340;&#24050;&#30693;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#21517;&#20026;C-PSRL&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#23398;&#20064;&#26356;&#39640;&#23618;&#30340;&#23436;&#25972;&#22240;&#26524;&#22270;&#21644;&#26356;&#20302;&#23618;&#23548;&#33268;&#30340;&#20998;&#35299;&#21160;&#24577;&#30340;&#21442;&#25968;&#12290;&#23545;&#20110;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;&#36125;&#21494;&#26031;&#36951;&#25022;&#30340;&#20998;&#26512;&#65292;&#26126;&#30830;&#22320;&#23558;&#36951;&#25022;&#29575;&#19982;&#20808;&#39564;&#30693;&#35782;&#31243;&#24230;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Posterior sampling allows the exploitation of prior knowledge of the environment's transition dynamics to improve the sample efficiency of reinforcement learning. The prior is typically specified as a class of parametric distributions, a task that can be cumbersome in practice, often resulting in the choice of uninformative priors. In this work, we propose a novel posterior sampling approach in which the prior is given as a (partial) causal graph over the environment's variables. The latter is often more natural to design, such as listing known causal dependencies between biometric features in a medical treatment study. Specifically, we propose a hierarchical Bayesian procedure, called C-PSRL, simultaneously learning the full causal graph at the higher level and the parameters of the resulting factored dynamics at the lower level. For this procedure, we provide an analysis of its Bayesian regret, which explicitly connects the regret rate with the degree of prior knowledge. Our numerica
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20559;&#24046;&#20851;&#31995;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.07511</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#24046;&#20851;&#31995;&#23398;&#20064;&#23454;&#29616;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#30340;&#32479;&#19968;&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes via Deviation Relationship Learning. (arXiv:2310.07511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07511
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20559;&#24046;&#20851;&#31995;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;&#22120;&#21487;&#20197;&#25214;&#21040;&#19982;&#32972;&#26223;&#19981;&#31526;&#30340;&#30446;&#26631;&#20316;&#20026;&#28508;&#22312;&#30446;&#26631;&#12290;&#37492;&#20110;&#22320;&#29699;&#24322;&#24120;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#30340;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#22120;&#24212;&#35813;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#65292;&#24182;&#19988;&#23545;&#20110;&#26032;&#30340;&#22320;&#29699;&#35266;&#27979;&#28304;&#21644;&#24322;&#24120;&#31867;&#22411;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#20165;&#38480;&#20110;&#21333;&#19968;&#27169;&#24577;&#21644;&#21333;&#19968;&#22330;&#26223;&#65292;&#22240;&#20026;&#23427;&#20204;&#26088;&#22312;&#23398;&#20064;&#19981;&#26029;&#21464;&#21270;&#30340;&#32972;&#26223;&#20998;&#24067;&#12290;&#22312;&#26222;&#36941;&#30340;&#24322;&#24120;&#20559;&#24046;&#27169;&#24335;&#30340;&#28608;&#21457;&#19979;&#65292;&#21363;&#24322;&#24120;&#23637;&#29616;&#20986;&#19982;&#20854;&#23616;&#37096;&#29615;&#22659;&#30340;&#20559;&#24046;&#29305;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#29305;&#24449;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#22522;&#20110;&#20559;&#24046;&#20851;&#31995;&#30340;&#26080;&#21521;&#21452;&#23618;&#22270;&#65292;&#20854;&#20013;&#24322;&#24120;&#35780;&#20998;&#34987;&#24314;&#27169;&#20026;&#22312;&#32972;&#26223;&#21644;&#27491;&#24120;&#23545;&#35937;&#30340;&#27169;&#24335;&#32473;&#23450;&#19979;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#28982;&#21518;&#65292;&#23398;&#20064;&#30446;&#26631;&#34987;&#34920;&#31034;&#20026;&#19968;&#20010;&#26465;&#20214;&#27010;&#29575;&#25490;&#24207;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23454;&#20363;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remote sensing anomaly detector can find the objects deviating from the background as potential targets. Given the diversity in earth anomaly types, a unified anomaly detector across modalities and scenes should be cost-effective and flexible to new earth observation sources and anomaly types. However, the current anomaly detectors are limited to a single modality and single scene, since they aim to learn the varying background distribution. Motivated by the universal anomaly deviation pattern, in that anomalies exhibit deviations from their local context, we exploit this characteristic to build a unified anomaly detector. Firstly, we reformulate the anomaly detection task as an undirected bilayer graph based on the deviation relationship, where the anomaly score is modeled as the conditional probability, given the pattern of the background and normal objects. The learning objective is then expressed as a conditional probability ranking problem. Furthermore, we design an instantiation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#25968;&#25454;&#21442;&#25968;&#21270;&#26550;&#26500;&#65288;HMN&#65289;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21387;&#32553;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#23384;&#20648;&#22312;&#19977;&#23618;&#32467;&#26500;&#20013;&#65292;HMN&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#38598;&#32423;&#21035;&#12289;&#31867;&#21035;&#32423;&#21035;&#21644;&#26679;&#26412;&#32423;&#21035;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.07506</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#23618;&#29305;&#24449;&#20849;&#20139;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#38598;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation. (arXiv:2310.07506v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#25968;&#25454;&#21442;&#25968;&#21270;&#26550;&#26500;&#65288;HMN&#65289;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21387;&#32553;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#23384;&#20648;&#22312;&#19977;&#23618;&#32467;&#26500;&#20013;&#65292;HMN&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#38598;&#32423;&#21035;&#12289;&#31867;&#21035;&#32423;&#21035;&#21644;&#26679;&#26412;&#32423;&#21035;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#65292;&#25968;&#25454;&#21387;&#32553;&#65288;DC&#65289;&#26088;&#22312;&#21512;&#25104;&#19968;&#20010;&#26174;&#33879;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#39640;&#24615;&#33021;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#25968;&#25454;&#21442;&#25968;&#21270;&#22686;&#24378;DC&#65292;&#23558;&#25968;&#25454;&#21387;&#32553;&#20026;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23481;&#22120;&#32780;&#19981;&#26159;&#20687;&#32032;&#31354;&#38388;&#12290;&#25968;&#25454;&#21442;&#25968;&#21270;&#30340;&#30452;&#35273;&#26159;&#32534;&#30721;&#22270;&#20687;&#30340;&#20849;&#20139;&#29305;&#24449;&#65292;&#20197;&#36991;&#20813;&#39069;&#22806;&#30340;&#23384;&#20648;&#25104;&#26412;&#12290;&#26412;&#25991;&#35748;&#35782;&#21040;&#30001;&#20110;&#20998;&#31867;&#31995;&#32479;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#65292;&#22270;&#20687;&#20197;&#20998;&#23618;&#26041;&#24335;&#20849;&#20139;&#20849;&#21516;&#30340;&#29305;&#24449;&#65292;&#36825;&#26159;&#24403;&#21069;&#25968;&#25454;&#21442;&#25968;&#21270;&#26041;&#27861;&#25152;&#24573;&#35270;&#30340;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20351;DC&#19982;&#36825;&#31181;&#20998;&#23618;&#24615;&#36136;&#23545;&#40784;&#65292;&#24182;&#22312;&#25968;&#25454;&#23481;&#22120;&#20869;&#37096;&#40723;&#21169;&#26356;&#39640;&#25928;&#30340;&#20449;&#24687;&#20849;&#20139;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#21442;&#25968;&#21270;&#26550;&#26500;&#65292;&#20998;&#23618;&#35760;&#24518;&#32593;&#32476;&#65288;HMN&#65289;&#12290;HMN&#23558;&#21387;&#32553;&#25968;&#25454;&#23384;&#20648;&#22312;&#19977;&#23618;&#32467;&#26500;&#20013;&#65292;&#34920;&#31034;&#25968;&#25454;&#38598;&#32423;&#21035;&#12289;&#31867;&#21035;&#32423;&#21035;&#21644;&#26679;&#26412;&#32423;&#21035;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a real-world dataset, data condensation (DC) aims to synthesize a significantly smaller dataset that captures the knowledge of this dataset for model training with high performance. Recent works propose to enhance DC with data parameterization, which condenses data into parameterized data containers rather than pixel space. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods. To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level fea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#23454;&#26102;&#24863;&#30693;&#33021;&#21147;&#30340;IoT&#32593;&#32476;&#35774;&#35745;&#30340;&#22522;&#20110;&#26679;&#26412;&#39537;&#21160;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#37319;&#26679;&#36807;&#31243;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#24615;&#65292;&#24182;&#35299;&#20915;&#33021;&#25928;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07497</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26412;&#39537;&#21160;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#33021;&#25928;&#21644;&#23454;&#26102;IoT&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Sample-Driven Federated Learning for Energy-Efficient and Real-Time IoT Sensing. (arXiv:2310.07497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#23454;&#26102;&#24863;&#30693;&#33021;&#21147;&#30340;IoT&#32593;&#32476;&#35774;&#35745;&#30340;&#22522;&#20110;&#26679;&#26412;&#39537;&#21160;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#37319;&#26679;&#36807;&#31243;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#24615;&#65292;&#24182;&#35299;&#20915;&#33021;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#21069;&#27839;&#26041;&#27861;&#22312;&#25910;&#25947;&#20998;&#26512;&#20013;&#20005;&#37325;&#20381;&#36182;&#20110;&#29702;&#24819;&#26465;&#20214;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;IoT&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#19982;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#30340;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#25968;&#25454;&#29305;&#24449;&#30340;&#20840;&#38754;&#33539;&#22260;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#23454;&#26102;&#24863;&#30693;&#33021;&#21147;&#30340;IoT&#32593;&#32476;&#35774;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#30001;&#29992;&#25143;&#25968;&#25454;&#37319;&#26679;&#36807;&#31243;&#24341;&#36215;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#36890;&#36807;&#26377;&#25928;&#22320;&#25511;&#21046;&#36825;&#20010;&#37319;&#26679;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#37319;&#26679;&#36807;&#31243;&#21516;&#26102;&#20943;&#23569;&#36807;&#25311;&#21512;&#21644;&#26368;&#22823;&#21270;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#26367;&#20195;&#20248;&#21270;&#38382;&#39064;&#25797;&#38271;&#22788;&#29702;&#33021;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of Federated Learning (FL) systems, recent cutting-edge methods heavily rely on ideal conditions convergence analysis. Specifically, these approaches assume that the training datasets on IoT devices possess similar attributes to the global data distribution. However, this approach fails to capture the full spectrum of data characteristics in real-time sensing FL systems. In order to overcome this limitation, we suggest a new approach system specifically designed for IoT networks with real-time sensing capabilities. Our approach takes into account the generalization gap due to the user's data sampling process. By effectively controlling this sampling process, we can mitigate the overfitting issue and improve overall accuracy. In particular, We first formulate an optimization problem that harnesses the sampling process to concurrently reduce overfitting while maximizing accuracy. In pursuit of this objective, our surrogate optimization problem is adept at handling energy ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27169;&#22411;&#32858;&#31867;&#20998;&#26512;&#20010;&#20307;&#29983;&#24577;&#30636;&#26102;&#35780;&#20272;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#20316;&#32773;&#27604;&#36739;&#20102;&#20004;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20869;&#22312;&#32858;&#31867;&#35780;&#20272;&#19982;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.07491</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#20010;&#20307;&#29983;&#24577;&#30636;&#26102;&#35780;&#20272;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32858;&#31867;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Model-based Clustering of Individuals' Ecological Momentary Assessment Time-series Data for Improving Forecasting Performance. (arXiv:2310.07491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27169;&#22411;&#32858;&#31867;&#20998;&#26512;&#20010;&#20307;&#29983;&#24577;&#30636;&#26102;&#35780;&#20272;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#20316;&#32773;&#27604;&#36739;&#20102;&#20004;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20869;&#22312;&#32858;&#31867;&#35780;&#20272;&#19982;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#24577;&#30636;&#26102;&#35780;&#20272;&#65288;EMA&#65289;&#30740;&#31350;&#65292;&#25910;&#38598;&#20102;&#36328;&#22810;&#20010;&#20010;&#20307;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#19981;&#26029;&#30417;&#27979;&#24773;&#32490;&#34892;&#20026;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#25968;&#25454;&#36890;&#24120;&#22312;&#20010;&#20307;&#23618;&#38754;&#19978;&#36827;&#34892;&#20998;&#26512;&#65292;&#20351;&#29992;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#35748;&#20026;&#36890;&#36807;&#32858;&#31867;&#23558;&#31867;&#20284;&#20010;&#20307;&#30340;&#38468;&#21152;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#65292;&#21487;&#33021;&#20250;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#21892;&#20010;&#20307;&#30340;&#25551;&#36848;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#26368;&#30456;&#20284;&#30340;&#20010;&#20307;&#20998;&#32452;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#20123;&#20449;&#24687;&#29992;&#20110;&#22522;&#20110;&#32676;&#20307;&#30340;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#20010;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#31532;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#27169;&#22411;&#25552;&#21462;&#21442;&#25968;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#21017;&#36890;&#36807;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20869;&#22312;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#65288;&#20363;&#22914;&#36718;&#24275;&#31995;&#25968;&#65289;&#20197;&#21450;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through Ecological Momentary Assessment (EMA) studies, a number of time-series data is collected across multiple individuals, continuously monitoring various items of emotional behavior. Such complex data is commonly analyzed in an individual level, using personalized models. However, it is believed that additional information of similar individuals is likely to enhance these models leading to better individuals' description. Thus, clustering is investigated with an aim to group together the most similar individuals, and subsequently use this information in group-based models in order to improve individuals' predictive performance. More specifically, two model-based clustering approaches are examined, where the first is using model-extracted parameters of personalized models, whereas the second is optimized on the model-based forecasting performance. Both methods are then analyzed using intrinsic clustering evaluation measures (e.g. Silhouette coefficients) as well as the performance o
&lt;/p&gt;</description></item><item><title>KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.07488</link><description>&lt;p&gt;
KwaiYiiMath: &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07488
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KwaiYiiMath&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22686;&#24378;&#20102;KwaiYiiBase1&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20013;&#23567;&#23398;&#25968;&#23398;&#27979;&#35797;&#38598;&#65288;&#21629;&#21517;&#20026;KMath&#65289;&#65292;&#21253;&#21547;188&#20010;&#20363;&#23376;&#65292;&#29992;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;KwaiYiiMath&#22312;GSM8k&#12289;CMath&#21644;KMath&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;Galerkin&#26041;&#26696;&#30340;&#38750;&#32447;&#24615;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25345;&#21704;&#23494;&#39039;&#37327;&#21644;&#20854;&#20182;&#25968;&#37327;&#30340;&#23432;&#24658;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#35745;&#31639;&#26174;&#24335;&#23884;&#20837;&#21040;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#35299;&#22330;&#30340;&#27969;&#24418;&#19978;&#26469;&#20445;&#35777;&#25968;&#37327;&#30340;&#23432;&#24658;&#12290;</title><link>http://arxiv.org/abs/2310.07485</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#23884;&#20837;&#29992;&#20110;&#20445;&#25345;&#21704;&#23494;&#39039;&#37327;&#21644;&#20854;&#20182;&#37327;&#30340;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;Galerkin&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Nonlinear embeddings for conserving Hamiltonians and other quantities with Neural Galerkin schemes. (arXiv:2310.07485v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;Galerkin&#26041;&#26696;&#30340;&#38750;&#32447;&#24615;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25345;&#21704;&#23494;&#39039;&#37327;&#21644;&#20854;&#20182;&#25968;&#37327;&#30340;&#23432;&#24658;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#35745;&#31639;&#26174;&#24335;&#23884;&#20837;&#21040;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#35299;&#22330;&#30340;&#27969;&#24418;&#19978;&#26469;&#20445;&#35777;&#25968;&#37327;&#30340;&#23432;&#24658;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#22312;&#20351;&#29992;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#65288;&#22914;&#28145;&#24230;&#32593;&#32476;&#65289;&#23545;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#36827;&#34892;&#36817;&#20284;&#26102;&#65292;&#20445;&#25345;&#21704;&#23494;&#39039;&#37327;&#12289;&#36136;&#37327;&#21644;&#21160;&#37327;&#31561;&#25968;&#37327;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#22522;&#20110;Dirac-Frenkel&#21464;&#20998;&#21407;&#29702;&#30340;&#31070;&#32463;Galerkin&#26041;&#26696;&#65292;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#35757;&#32451;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#20165;&#20165;&#28155;&#21152;&#22312;&#36830;&#32493;&#26102;&#38388;&#19978;&#26088;&#22312;&#20445;&#25345;&#25968;&#37327;&#30340;&#32422;&#26463;&#26159;&#19981;&#20805;&#20998;&#30340;&#65292;&#22240;&#20026;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#24847;&#21619;&#30528;&#21363;&#20351;&#23545;&#35299;&#22330;&#26159;&#32447;&#24615;&#30340;&#25968;&#37327;&#20063;&#21464;&#25104;&#20102;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#65292;&#22240;&#27492;&#22312;&#26102;&#38388;&#19978;&#31163;&#25955;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#31070;&#32463;Galerkin&#26041;&#26696;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#35745;&#31639;&#19968;&#20010;&#26174;&#24335;&#23884;&#20837;&#21040;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#35299;&#22330;&#30340;&#27969;&#24418;&#19978;&#65292;&#20197;&#20445;&#35777;&#25968;&#37327;&#30340;&#23432;&#24658;&#12290;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#19982;&#26631;&#20934;&#30340;&#26174;&#24335;&#21644;&#38544;&#24335;&#26102;&#38388;&#31215;&#20998;&#26041;&#26696;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on the conservation of quantities such as Hamiltonians, mass, and momentum when solution fields of partial differential equations are approximated with nonlinear parametrizations such as deep networks. The proposed approach builds on Neural Galerkin schemes that are based on the Dirac--Frenkel variational principle to train nonlinear parametrizations sequentially in time. We first show that only adding constraints that aim to conserve quantities in continuous time can be insufficient because the nonlinear dependence on the parameters implies that even quantities that are linear in the solution fields become nonlinear in the parameters and thus are challenging to discretize in time. Instead, we propose Neural Galerkin schemes that compute at each time step an explicit embedding onto the manifold of nonlinearly parametrized solution fields to guarantee conservation of quantities. The embeddings can be combined with standard explicit and implicit time integration schemes
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#29983;&#29289;&#26631;&#24535;&#29289;&#32452;&#32455;&#24418;&#24577;&#23398;&#21457;&#29616;&#32773;&#27169;&#22411;&#65292;&#21033;&#29992;&#20840;&#20999;&#29255;&#22270;&#20687;&#39044;&#27979;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#20013;&#20116;&#20010;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#29366;&#24577;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#21333;&#31867;&#20998;&#31867;&#32435;&#20837;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#23454;&#20363;&#32423;&#21035;&#30417;&#30563;&#65292;&#22312;&#25552;&#39640;&#29983;&#29289;&#26631;&#24535;&#29289;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.07464</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#29366;&#24577;&#24182;&#21457;&#29616;&#30456;&#20851;&#30340;&#32452;&#32455;&#24418;&#24577;&#23398;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Predicts Biomarker Status and Discovers Related Histomorphology Characteristics for Low-Grade Glioma. (arXiv:2310.07464v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#29983;&#29289;&#26631;&#24535;&#29289;&#32452;&#32455;&#24418;&#24577;&#23398;&#21457;&#29616;&#32773;&#27169;&#22411;&#65292;&#21033;&#29992;&#20840;&#20999;&#29255;&#22270;&#20687;&#39044;&#27979;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#20013;&#20116;&#20010;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#29366;&#24577;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#21333;&#31867;&#20998;&#31867;&#32435;&#20837;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#23454;&#20363;&#32423;&#21035;&#30417;&#30563;&#65292;&#22312;&#25552;&#39640;&#29983;&#29289;&#26631;&#24535;&#29289;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#26631;&#24535;&#29289;&#26816;&#27979;&#26159;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#65288;LGG&#65289;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LGG&#29983;&#29289;&#26631;&#24535;&#29289;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#26114;&#36149;&#32780;&#22797;&#26434;&#30340;&#20998;&#23376;&#36951;&#20256;&#23398;&#27979;&#35797;&#65292;&#38656;&#35201;&#19987;&#19994;&#20154;&#21592;&#20998;&#26512;&#32467;&#26524;&#65292;&#19988;&#24120;&#24120;&#23384;&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#24046;&#24322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#21363;&#22522;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26694;&#26550;&#30340;&#22810;&#29983;&#29289;&#26631;&#24535;&#29289;&#32452;&#32455;&#24418;&#24577;&#23398;&#21457;&#29616;&#32773;&#65288;Multi-Beholder&#65289;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#33487;&#26408;&#31934;-&#20234;&#32418;&#26579;&#33394;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;&#20999;&#29255;&#32423;&#29983;&#29289;&#26631;&#24535;&#29289;&#29366;&#24577;&#26631;&#31614;&#39044;&#27979;LGG&#20013;&#20116;&#20010;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23558;&#21333;&#31867;&#20998;&#31867;&#34701;&#20837;MIL&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#31034;&#20363;&#20266;&#26631;&#31614;&#65292;&#29992;&#20110;&#23454;&#20363;&#32423;&#21035;&#30417;&#30563;&#65292;&#36825;&#26497;&#22823;&#22320;&#34917;&#20805;&#20102;&#20999;&#29255;&#32423;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#20102;&#29983;&#29289;&#26631;&#24535;&#29289;&#39044;&#27979;&#24615;&#33021;&#12290; Multi-Beholder&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomarker detection is an indispensable part in the diagnosis and treatment of low-grade glioma (LGG). However, current LGG biomarker detection methods rely on expensive and complex molecular genetic testing, for which professionals are required to analyze the results, and intra-rater variability is often reported. To overcome these challenges, we propose an interpretable deep learning pipeline, a Multi-Biomarker Histomorphology Discoverer (Multi-Beholder) model based on the multiple instance learning (MIL) framework, to predict the status of five biomarkers in LGG using only hematoxylin and eosin-stained whole slide images and slide-level biomarker status labels. Specifically, by incorporating the one-class classification into the MIL framework, accurate instance pseudo-labeling is realized for instance-level supervision, which greatly complements the slide-level labels and improves the biomarker prediction performance. Multi-Beholder demonstrates superior prediction performance and g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20998;&#26512;&#20102;&#20581;&#24247;&#20010;&#20307;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#65292;&#24182;&#35782;&#21035;&#20986;&#38543;&#24180;&#40836;&#22686;&#38271;&#21628;&#21560;&#29575;&#30340;&#19979;&#38477;&#21450;SDANN&#20540;&#24322;&#24120;&#39640;&#20316;&#20026;&#32769;&#24180;&#20154;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.07463</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25581;&#31034;&#20581;&#24247;&#34928;&#32769;&#36807;&#31243;&#20013;&#30340;&#24515;&#30005;&#22270;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncovering ECG Changes during Healthy Aging using Explainable AI. (arXiv:2310.07463v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20998;&#26512;&#20102;&#20581;&#24247;&#20010;&#20307;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#65292;&#24182;&#35782;&#21035;&#20986;&#38543;&#24180;&#40836;&#22686;&#38271;&#21628;&#21560;&#29575;&#30340;&#19979;&#38477;&#21450;SDANN&#20540;&#24322;&#24120;&#39640;&#20316;&#20026;&#32769;&#24180;&#20154;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#20173;&#28982;&#26159;&#20840;&#29699;&#39046;&#20808;&#30340;&#27515;&#22240;&#12290;&#36825;&#38656;&#35201;&#23545;&#24515;&#33039;&#34928;&#32769;&#36807;&#31243;&#26377;&#28145;&#20837;&#30340;&#20102;&#35299;&#65292;&#20197;&#35786;&#26029;&#24515;&#34880;&#31649;&#20581;&#24247;&#29366;&#20917;&#30340;&#38480;&#21046;&#12290;&#20256;&#32479;&#19978;&#65292;&#23545;&#20010;&#20307;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#29305;&#24449;&#38543;&#24180;&#40836;&#21464;&#21270;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#36825;&#20123;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29305;&#24449;&#34429;&#28982;&#26377;&#20449;&#24687;&#37327;&#65292;&#20294;&#21487;&#33021;&#25513;&#30422;&#20102;&#24213;&#23618;&#25968;&#25454;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20998;&#26512;&#26469;&#33258;&#20581;&#24247;&#20010;&#20307;&#30340;ECG&#25968;&#25454;&#65292;&#21253;&#25324;&#21407;&#22987;&#20449;&#21495;&#21644;ECG&#29305;&#24449;&#26684;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#26469;&#35782;&#21035;&#23545;&#20110;&#21306;&#20998;&#24180;&#40836;&#32452;&#21035;&#26368;&#26377;&#36776;&#21035;&#21147;&#30340;ECG&#29305;&#24449;&#25110;&#21407;&#22987;&#20449;&#21495;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#19982;&#22522;&#20110;&#26641;&#30340;&#20998;&#31867;&#22120;&#25581;&#31034;&#20102;&#38543;&#24180;&#40836;&#22686;&#38271;&#21628;&#21560;&#29575;&#19979;&#38477;&#65292;&#24182;&#35782;&#21035;&#20986;SDANN&#20540;&#24322;&#24120;&#39640;&#20316;&#20026;&#32769;&#24180;&#20154;&#30340;&#25351;&#26631;&#65292;&#21487;&#23558;&#20854;&#19982;&#24180;&#36731;&#20154;&#21306;&#20998;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular diseases remain the leading global cause of mortality. This necessitates a profound understanding of heart aging processes to diagnose constraints in cardiovascular fitness. Traditionally, most of such insights have been drawn from the analysis of electrocardiogram (ECG) feature changes of individuals as they age. However, these features, while informative, may potentially obscure underlying data relationships. In this paper, we employ a deep-learning model and a tree-based model to analyze ECG data from a robust dataset of healthy individuals across varying ages in both raw signals and ECG feature format. Explainable AI techniques are then used to identify ECG features or raw signal characteristics are most discriminative for distinguishing between age groups. Our analysis with tree-based classifiers reveal age-related declines in inferred breathing rates and identifies notably high SDANN values as indicative of elderly individuals, distinguishing them from younger adul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#22495;&#20998;&#35299;&#21644;&#25299;&#25169;&#23884;&#20837;&#22120;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#22320;&#36136;&#20648;&#23384;&#24212;&#29992;&#20013;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07461</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22320;&#36136;&#30899;&#21644;&#33021;&#28304;&#20648;&#23384;&#30340;&#39640;&#25928;&#26426;&#22120;&#23398;&#20064;&#26367;&#20195;&#21697;
&lt;/p&gt;
&lt;p&gt;
Efficient machine-learning surrogates for large-scale geological carbon and energy storage. (arXiv:2310.07461v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#22495;&#20998;&#35299;&#21644;&#25299;&#25169;&#23884;&#20837;&#22120;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#22320;&#36136;&#20648;&#23384;&#24212;&#29992;&#20013;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#36136;&#30899;&#21644;&#33021;&#28304;&#20648;&#23384;&#23545;&#20110;&#23454;&#29616;&#20928;&#38646;&#30899;&#25490;&#25918;&#21644;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22320;&#36136;&#22240;&#32032;&#21644;&#25805;&#20316;&#38480;&#21046;&#65292;&#23427;&#20204;&#38754;&#20020;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#33021;&#24341;&#21457;&#22320;&#38663;&#20107;&#20214;&#25110;&#22320;&#19979;&#27700;&#27745;&#26579;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#20197;&#39640;&#25928;&#22320;&#31649;&#29702;&#24191;&#27867;&#30340;&#20648;&#23618;&#27169;&#22411;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22320;&#36136;&#30899;&#20648;&#23384;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22823;&#35268;&#27169;&#20998;&#26512;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#26159;&#19968;&#20010;&#38556;&#30861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22495;&#20998;&#35299;&#21644;&#25299;&#25169;&#23884;&#20837;&#22120;&#26469;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#65292;&#20197;&#38142;&#25509;&#26102;&#31354;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#27169;&#22411;&#39046;&#22495;&#20869;&#23545;&#26410;&#32463;&#35757;&#32451;&#30340;&#25968;&#25454;&#36827;&#34892;&#31934;&#30830;&#39044;&#27979;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#22320;&#36136;&#20648;&#23384;&#24212;&#29992;&#20013;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geological carbon and energy storage are pivotal for achieving net-zero carbon emissions and addressing climate change. However, they face uncertainties due to geological factors and operational limitations, resulting in possibilities of induced seismic events or groundwater contamination. To overcome these challenges, we propose a specialized machine-learning (ML) model to manage extensive reservoir models efficiently.  While ML approaches hold promise for geological carbon storage, the substantial computational resources required for large-scale analysis are the obstacle. We've developed a method to reduce the training cost for deep neural operator models, using domain decomposition and a topology embedder to link spatio-temporal points. This approach allows accurate predictions within the model's domain, even for untrained data, enhancing ML efficiency for large-scale geological storage applications.
&lt;/p&gt;</description></item><item><title>ProbTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#21516;&#21644;&#27604;&#36739;&#23450;&#21046;&#31070;&#32463;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#21183;&#21644;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.07446</link><description>&lt;p&gt;
ProbTS&#65306;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#28145;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32479;&#19968;&#24037;&#20855;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;
ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting. (arXiv:2310.07446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07446
&lt;/p&gt;
&lt;p&gt;
ProbTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#21516;&#21644;&#27604;&#36739;&#23450;&#21046;&#31070;&#32463;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#21183;&#21644;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#36825;&#20010;&#39046;&#22495;&#20998;&#21270;&#25104;&#20102;&#20004;&#20010;&#26174;&#33879;&#30340;&#20998;&#25903;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20026;&#26102;&#38388;&#24207;&#21015;&#23450;&#21046;&#29305;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#20998;&#25903;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#25968;&#25454;&#24773;&#26223;&#12289;&#26041;&#27861;&#35770;&#28966;&#28857;&#21644;&#35299;&#30721;&#26041;&#26696;&#19978;&#30340;&#24046;&#24322;&#25552;&#20986;&#20102;&#28145;&#20837;&#32780;&#26410;&#34987;&#25506;&#32034;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#40511;&#27807;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ProbTS&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#21327;&#21516;&#21644;&#27604;&#36739;&#36825;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#25903;&#12290;ProbTS&#20855;&#22791;&#32479;&#19968;&#30340;&#25968;&#25454;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#27169;&#22411;&#27169;&#22359;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#22120;&#27169;&#22359;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#37325;&#26032;&#23457;&#35270;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20004;&#20010;&#20998;&#25903;&#30340;&#39046;&#20808;&#26041;&#27861;&#12290;&#36890;&#36807;ProbTS&#30340;&#23457;&#26597;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#12289;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further explorat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25903;&#30340;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#24052;&#40654;&#38654;&#38718;&#30340;&#21457;&#29983;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#27668;&#35937;&#22270;&#30340;&#19981;&#21516;&#29305;&#24449;&#31354;&#38388;&#23610;&#24230;&#26469;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#24180;&#26399;&#30340;&#27668;&#35937;&#21464;&#37327;&#21644;&#22320;&#38754;&#33021;&#35265;&#24230;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#36825;&#20123;&#26032;&#26550;&#26500;&#25552;&#39640;&#20102;&#32593;&#32476;&#24615;&#33021;&#65292;&#22312;&#26410;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30450;&#39044;&#27979;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#39044;&#27979;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.07437</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20998;&#25903;&#30340;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#31354;&#38388;&#23610;&#24230;&#30340;&#27668;&#35937;&#22270;&#39044;&#27979;&#24052;&#40654;&#38654;&#38718;&#30340;&#21457;&#29983;
&lt;/p&gt;
&lt;p&gt;
A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales. (arXiv:2310.07437v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25903;&#30340;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#24052;&#40654;&#38654;&#38718;&#30340;&#21457;&#29983;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#27668;&#35937;&#22270;&#30340;&#19981;&#21516;&#29305;&#24449;&#31354;&#38388;&#23610;&#24230;&#26469;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#24180;&#26399;&#30340;&#27668;&#35937;&#21464;&#37327;&#21644;&#22320;&#38754;&#33021;&#35265;&#24230;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#36825;&#20123;&#26032;&#26550;&#26500;&#25552;&#39640;&#20102;&#32593;&#32476;&#24615;&#33021;&#65292;&#22312;&#26410;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30450;&#39044;&#27979;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#39044;&#27979;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#39044;&#27979;&#20302;&#33021;&#35265;&#24230;&#20107;&#20214;&#25110;&#38654;&#38718;&#30340;&#21457;&#29983;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#22810;&#24180;&#26399;&#30340;&#21508;&#31181;&#27668;&#35937;&#21644;&#27700;&#25991;&#21464;&#37327;&#30340;&#21306;&#22495;&#26085;&#24120;&#22270;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#23558;&#22320;&#38754;&#33021;&#35265;&#24230;&#35266;&#27979;&#20316;&#20026;&#30446;&#26631;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20445;&#30041;&#19981;&#21516;&#36755;&#20837;&#29305;&#24449;&#30340;&#29305;&#24449;&#31354;&#38388;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#36817;&#24320;&#21457;&#20102;&#20004;&#31181;&#38024;&#23545;&#24052;&#40654;&#38654;&#38718;&#30340;&#20998;&#25903;&#26550;&#26500;&#12290;&#36825;&#20123;&#26032;&#26550;&#26500;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#22312;&#39564;&#35777;&#21644;&#30450;&#39044;&#27979;&#35780;&#20272;&#20013;&#20135;&#29983;&#20102;&#21512;&#29702;&#30340;&#20998;&#25968;&#65292;&#20351;&#29992;&#20102;2021&#24180;&#21644;2022&#24180;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#27809;&#26377;&#22312;&#35757;&#32451;&#21644;&#39564;&#35777;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A deep learning platform has been developed to forecast the occurrence of the low visibility events or hazes. It is trained by using multi-decadal daily regional maps of various meteorological and hydrological variables as input features and surface visibility observations as the targets. To better preserve the characteristic spatial information of different input features for training, two branched architectures have recently been developed for the case of Paris hazes. These new architectures have improved the performance of the network, producing reasonable scores in both validation and a blind forecasting evaluation using the data of 2021 and 2022 that have not been used in the training and validation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26497;&#31471;&#20107;&#20214;&#39044;&#27979;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#28151;&#21512;&#20998;&#24067;&#21644;&#37325;&#21442;&#25968;&#21270; GP &#20998;&#24067;&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#23545;&#26497;&#31471;&#20107;&#20214;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2310.07435</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26497;&#31471;&#20107;&#20214;&#39044;&#27979;&#30340;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Mixture Model for Extreme Events Forecasting in Time Series Data. (arXiv:2310.07435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07435
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26497;&#31471;&#20107;&#20214;&#39044;&#27979;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#28151;&#21512;&#20998;&#24067;&#21644;&#37325;&#21442;&#25968;&#21270; GP &#20998;&#24067;&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#23545;&#26497;&#31471;&#20107;&#20214;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20027;&#39064;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#22825;&#27668;&#39044;&#25253;&#12289;&#20132;&#36890;&#25511;&#21046;&#21644;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26497;&#31471;&#20540;&#36890;&#24120;&#23545;&#20154;&#31867;&#21644;&#33258;&#28982;&#31995;&#32479;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#20294;&#30001;&#20110;&#20854;&#32597;&#35265;&#24615;&#65292;&#39044;&#27979;&#36825;&#20123;&#20540;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22522;&#20110;&#26497;&#20540;&#29702;&#35770; (EVT) &#30340;&#32479;&#35745;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#24314;&#27169;&#26497;&#20540;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#24191;&#20041;&#24085;&#32047;&#25176; (GP) &#20998;&#24067;&#29992;&#20110;&#24314;&#27169;&#36229;&#36807;&#38408;&#20540;&#30340;&#20998;&#24067;&#12290;&#20026;&#20102;&#20811;&#26381;&#28145;&#24230;&#23398;&#20064;&#22312;&#22788;&#29702;&#37325;&#23614;&#25968;&#25454;&#26041;&#38754;&#30340;&#19981;&#29702;&#24819;&#34920;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#23545;&#26497;&#31471;&#20107;&#20214;&#20851;&#27880;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#28145;&#24230;&#26497;&#31471;&#28151;&#21512;&#27169;&#22411; (DEMMA) &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#65306;1) &#22522;&#20110;&#38556;&#30861;&#27169;&#22411;&#30340;&#24191;&#20041;&#28151;&#21512;&#20998;&#24067;&#21644;&#19982;&#26497;&#31471;&#38408;&#20540;&#26080;&#20851;&#30340;&#37325;&#21442;&#25968;&#21270; GP &#20998;&#24067;&#24418;&#24335;&#65292;2) &#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#26497;&#31471;&#20107;&#20214;&#30340;&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time Series Forecasting (TSF) is a widely researched topic with broad applications in weather forecasting, traffic control, and stock price prediction. Extreme values in time series often significantly impact human and natural systems, but predicting them is challenging due to their rare occurrence. Statistical methods based on Extreme Value Theory (EVT) provide a systematic approach to modeling the distribution of extremes, particularly the Generalized Pareto (GP) distribution for modeling the distribution of exceedances beyond a threshold. To overcome the subpar performance of deep learning in dealing with heavy-tailed data, we propose a novel framework to enhance the focus on extreme events. Specifically, we propose a Deep Extreme Mixture Model with Autoencoder (DEMMA) for time series prediction. The model comprises two main modules: 1) a generalized mixture distribution based on the Hurdle model and a reparameterized GP distribution form independent of the extreme threshold, 2) an 
&lt;/p&gt;</description></item><item><title>HealthWalk&#36890;&#36807;&#38598;&#25104;&#20256;&#24863;&#22120;&#21040;&#28378;&#36718;&#34892;&#36208;&#32773;&#35774;&#35745;&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#28378;&#36718;&#34892;&#36208;&#32773;&#29992;&#25143;&#23039;&#21183;&#19981;&#22909;&#23548;&#33268;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20854;&#20182;&#26377;&#36259;&#30340;&#29992;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.07434</link><description>&lt;p&gt;
HealthWalk&#65306;&#36890;&#36807;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#28378;&#36718;&#34892;&#36208;&#32773;&#36741;&#21161;&#20419;&#36827;&#20581;&#24247;&#19982;&#31227;&#21160;&#24615;
&lt;/p&gt;
&lt;p&gt;
HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator Walker Assistance. (arXiv:2310.07434v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07434
&lt;/p&gt;
&lt;p&gt;
HealthWalk&#36890;&#36807;&#38598;&#25104;&#20256;&#24863;&#22120;&#21040;&#28378;&#36718;&#34892;&#36208;&#32773;&#35774;&#35745;&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#28378;&#36718;&#34892;&#36208;&#32773;&#29992;&#25143;&#23039;&#21183;&#19981;&#22909;&#23548;&#33268;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20854;&#20182;&#26377;&#36259;&#30340;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28378;&#36718;&#34892;&#36208;&#32773;&#33021;&#22815;&#24110;&#21161;&#36523;&#20307;&#21463;&#38480;&#21046;&#30340;&#20154;&#25552;&#39640;&#31227;&#21160;&#33021;&#21147;&#65292;&#24182;&#32473;&#20182;&#20204;&#20449;&#24515;&#21644;&#29420;&#31435;&#24615;&#65292;&#20351;&#20182;&#20204;&#33021;&#26356;&#38271;&#26102;&#38388;&#22320;&#21442;&#19982;&#31038;&#20250;&#12290;&#28982;&#32780;&#65292;&#28378;&#36718;&#34892;&#36208;&#32773;&#29992;&#25143;&#24448;&#24448;&#23039;&#21183;&#19981;&#22909;&#65292;&#23548;&#33268;&#36827;&#19968;&#27493;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#26368;&#22351;&#30340;&#24773;&#20917;&#26159;&#25684;&#20498;&#12290;&#23558;&#20256;&#24863;&#22120;&#38598;&#25104;&#21040;&#28378;&#36718;&#34892;&#36208;&#32773;&#35774;&#35745;&#20013;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#20854;&#20182;&#19968;&#20123;&#26377;&#36259;&#30340;&#29992;&#20363;&#12290;&#26412;&#25991;&#31616;&#35201;&#20171;&#32461;&#20102;&#29616;&#26377;&#31995;&#32479;&#20197;&#21450;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#25105;&#20204;&#26089;&#26399;&#30340;HealthWalk&#28378;&#36718;&#34892;&#36208;&#32773;&#21407;&#22411;&#65292;&#29992;&#20110;&#19982;&#32769;&#24180;&#20154;&#12289;&#39118;&#28287;&#30149;&#12289;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#21644;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#20197;&#21450;&#35270;&#21147;&#38556;&#30861;&#32773;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rollator walkers allow people with physical limitations to increase their mobility and give them the confidence and independence to participate in society for longer. However, rollator walker users often have poor posture, leading to further health problems and, in the worst case, falls. Integrating sensors into rollator walker designs can help to address this problem and results in a platform that allows several other interesting use cases. This paper briefly overviews existing systems and the current research directions and challenges in this field. We also present our early HealthWalk rollator walker prototype for data collection with older people, rheumatism, multiple sclerosis and Parkinson patients, and individuals with visual impairments.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07433</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#25240;&#25187;&#35843;&#24230;&#20174;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07433
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#35266;&#23519;&#21644;&#27169;&#20223;&#26469;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#12290;&#23545;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#30340;&#22823;&#37327;&#26080;&#26631;&#31614;&#35270;&#39057;&#28436;&#31034;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#38656;&#35201;&#22312;&#27809;&#26377;&#35775;&#38382;&#20854;&#21160;&#20316;&#30340;&#24773;&#20917;&#19979;&#27169;&#20223;&#19987;&#23478;&#65292;&#36825;&#26159;&#19968;&#31181;&#31216;&#20026;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;&#65288;ILfO&#65289;&#30340;&#25361;&#25112;&#12290;&#35299;&#20915;ILfO&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#20854;&#36716;&#21270;&#20026;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#21644;&#19987;&#23478;&#35266;&#23519;&#20013;&#35745;&#31639;&#20986;&#30340;&#20195;&#29702;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20855;&#26377;&#36827;&#23637;&#20381;&#36182;&#24615;&#23646;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65307;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#38656;&#35201;&#22312;&#25484;&#25569;&#21518;&#32493;&#34892;&#20026;&#20043;&#21069;&#20808;&#23398;&#20064;&#19987;&#23478;&#30340;&#21069;&#24207;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#20998;&#37197;&#32473;&#21518;&#32493;&#27493;&#39588;&#30340;&#22870;&#21169;&#20449;&#21495;&#22952;&#30861;&#20102;&#23545;&#21021;&#22987;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;ILfO&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#25484;&#25569;&#26089;&#26399;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
&lt;/p&gt;</description></item><item><title>&#38750;&#22238;&#28335;&#22270;&#31070;&#32463;&#32593;&#32476;(NBA-GNN)&#36890;&#36807;&#19981;&#32771;&#34385;&#20808;&#21069;&#35775;&#38382;&#33410;&#28857;&#30340;&#28040;&#24687;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#26412;&#22320;&#26356;&#26032;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07430</link><description>&lt;p&gt;
&#38750;&#22238;&#28335;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Non-backtracking Graph Neural Networks. (arXiv:2310.07430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07430
&lt;/p&gt;
&lt;p&gt;
&#38750;&#22238;&#28335;&#22270;&#31070;&#32463;&#32593;&#32476;(NBA-GNN)&#36890;&#36807;&#19981;&#32771;&#34385;&#20808;&#21069;&#35775;&#38382;&#33410;&#28857;&#30340;&#28040;&#24687;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#26412;&#22320;&#26356;&#26032;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33879;&#21517;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#24687;&#20256;&#36882;&#26356;&#26032;&#20801;&#35768;&#20351;&#29992;&#26412;&#22320;&#21644;&#35745;&#31639;&#19978;&#21487;&#36319;&#36394;&#30340;&#26356;&#26032;&#26469;&#34920;&#31034;&#22823;&#35268;&#27169;&#22270;&#12290;&#28982;&#32780;&#65292;&#26412;&#22320;&#26356;&#26032;&#21463;&#21040;&#22238;&#28335;&#30340;&#24433;&#21709;&#65292;&#21363;&#28040;&#24687;&#36890;&#36807;&#21516;&#19968;&#26465;&#36793;&#20004;&#27425;&#27969;&#21160;&#24182;&#37325;&#35775;&#20808;&#21069;&#35775;&#38382;&#30340;&#33410;&#28857;&#12290;&#30001;&#20110;&#28040;&#24687;&#27969;&#30340;&#25968;&#37327;&#38543;&#30528;&#26356;&#26032;&#30340;&#27425;&#25968;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#65292;&#26412;&#22320;&#26356;&#26032;&#20013;&#30340;&#20887;&#20313;&#38459;&#30861;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#28040;&#24687;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#38750;&#22238;&#28335;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;NBA-GNN&#65289;&#35299;&#20915;&#20102;&#36825;&#31181;&#20887;&#20313;&#65292;&#35813;&#32593;&#32476;&#22312;&#26356;&#26032;&#28040;&#24687;&#26102;&#19981;&#32771;&#34385;&#20808;&#21069;&#35775;&#38382;&#33410;&#28857;&#30340;&#28040;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;NBA-GNN&#22914;&#20309;&#32531;&#35299;GNN&#30340;&#36807;&#24230;&#21387;&#32553;&#65292;&#24182;&#24314;&#31435;&#20102;NBA-GNN&#21644;&#38750;&#22238;&#28335;&#26356;&#26032;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#24674;&#22797;&#26041;&#38754;&#20986;&#33394;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;NBA-
&lt;/p&gt;
&lt;p&gt;
The celebrated message-passing updates for graph neural networks allow the representation of large-scale graphs with local and computationally tractable updates. However, the local updates suffer from backtracking, i.e., a message flows through the same edge twice and revisits the previously visited node. Since the number of message flows increases exponentially with the number of updates, the redundancy in local updates prevents the graph neural network from accurately recognizing a particular message flow for downstream tasks. In this work, we propose to resolve such a redundancy via the non-backtracking graph neural network (NBA-GNN) that updates a message without incorporating the message from the previously visited node. We further investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive performance of non-backtracking updates for stochastic block model recovery. We empirically verify the effectiveness of our NBA-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Quantum Gramian Angular Field (QGAF)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#23558;&#32929;&#31080;&#22238;&#25253;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35757;&#32451;&#30340;&#20108;&#32500;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;QGAF&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.07427</link><description>&lt;p&gt;
&#22686;&#24378;&#37327;&#23376;&#39044;&#27979;&#33021;&#21147;&#65306;&#21033;&#29992;&#37327;&#23376;Gramian&#35282;&#24230;&#22330;&#21644;CNN&#36827;&#34892;&#32929;&#31080;&#22238;&#25253;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions. (arXiv:2310.07427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Quantum Gramian Angular Field (QGAF)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#23558;&#32929;&#31080;&#22238;&#25253;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35757;&#32451;&#30340;&#20108;&#32500;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;QGAF&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Quantum Gramian Angular Field (QGAF)&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#36890;&#36807;&#35774;&#35745;&#29305;&#23450;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#32929;&#31080;&#22238;&#25253;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35757;&#32451;&#30340;&#20108;&#32500;&#22270;&#20687;&#12290;&#19982;&#32463;&#20856;&#30340;Gramian Angular Field (GAF)&#26041;&#27861;&#19981;&#21516;&#65292;QGAF&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#28040;&#38500;&#20102;&#25968;&#25454;&#24402;&#19968;&#21270;&#21644;&#21453;&#20313;&#24358;&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#31616;&#21270;&#20102;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21040;&#20108;&#32500;&#22270;&#20687;&#30340;&#36716;&#25442;&#36807;&#31243;&#12290;&#20026;&#20102;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20013;&#22269;A&#32929;&#24066;&#22330;&#12289;&#39321;&#28207;&#32929;&#24066;&#21644;&#32654;&#22269;&#32929;&#24066;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#32463;&#20856;&#30340;GAF&#26041;&#27861;&#30456;&#27604;&#65292;QGAF&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a time series forecasting method named Quantum Gramian Angular Field (QGAF). This approach merges the advantages of quantum computing technology with deep learning, aiming to enhance the precision of time series classification and forecasting. We successfully transformed stock return time series data into two-dimensional images suitable for Convolutional Neural Network (CNN) training by designing specific quantum circuits. Distinct from the classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in eliminating the need for data normalization and inverse cosine calculations, simplifying the transformation process from time series data to two-dimensional images. To validate the effectiveness of this method, we conducted experiments on datasets from three major stock markets: the China A-share market, the Hong Kong stock market, and the US stock market. Experimental results revealed that compared to the classical GAF method, the QGAF approach significantly improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#23545;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#65292;&#24182;&#19988;&#26410;&#21450;&#26102;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#23558;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#36825;&#20026;&#35299;&#20915;&#39640;&#37325;&#25918;&#27604;&#22256;&#22659;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.07418</link><description>&lt;p&gt;
&#37325;&#23457;&#35270;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#65306;&#25968;&#25454;&#12289;&#27169;&#22359;&#21644;&#35757;&#32451;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages. (arXiv:2310.07418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#23545;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#65292;&#24182;&#19988;&#26410;&#21450;&#26102;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#23558;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#36825;&#20026;&#35299;&#20915;&#39640;&#37325;&#25918;&#27604;&#22256;&#22659;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22609;&#24615;&#65292;&#31070;&#32463;&#32593;&#32476;&#38543;&#26032;&#25968;&#25454;&#28436;&#36827;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;(VRL)&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#37325;&#32622;&#21644;&#27491;&#21017;&#21270;&#31561;&#26041;&#27861;&#21487;&#33021;&#33021;&#22815;&#32531;&#35299;&#21487;&#22609;&#24615;&#25439;&#22833;&#65292;&#20294;VRL&#26694;&#26550;&#20869;&#21508;&#31181;&#32452;&#20214;&#23545;&#20195;&#29702;&#30340;&#21487;&#22609;&#24615;&#30340;&#24433;&#21709;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#32463;&#39564;&#24615;&#25506;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#20010;&#20027;&#35201;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#26041;&#38754;&#65292;&#24182;&#24471;&#20986;&#20197;&#19979;&#26377;&#28145;&#20837;&#35265;&#35299;&#30340;&#32467;&#35770;&#65306;(1)&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65307;(2)&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#38459;&#30861;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#29942;&#39048;&#65307;(3)&#22312;&#26089;&#26399;&#38454;&#27573;&#27809;&#26377;&#21450;&#26102;&#24178;&#39044;&#20197;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#65292;&#20854;&#25439;&#22833;&#23558;&#21464;&#24471;&#28798;&#38590;&#24615;&#12290;&#36825;&#20123;&#35265;&#35299;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#39640;&#37325;&#25918;&#27604;&#65288;RR&#65289;&#22256;&#22659;&#30340;&#26032;&#31574;&#30053;&#65292;&#20854;&#20013;&#21152;&#21095;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#22952;&#30861;&#20102;&#36890;&#36807;&#22686;&#21152;&#37325;&#25918;&#25968;&#37327;&#24102;&#26469;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#30340;&#29616;&#29366;&#19982;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20197;&#23454;&#29616;&#35299;&#37322;&#24615;&#12289;&#21487;&#39564;&#35777;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#23545;&#40784;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07417</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#21487;&#20197;&#33719;&#24471;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What can knowledge graph alignment gain with Neuro-Symbolic learning approaches?. (arXiv:2310.07417v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#30340;&#29616;&#29366;&#19982;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20197;&#23454;&#29616;&#35299;&#37322;&#24615;&#12289;&#21487;&#39564;&#35777;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#23545;&#40784;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#35768;&#22810;&#25968;&#25454;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#30784;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#34920;&#31034;&#25968;&#25454;&#19982;&#20854;&#24847;&#20041;&#21644;&#19978;&#19979;&#25991;&#30340;&#20851;&#32852;&#12290;&#23545;&#40784;&#19981;&#21516;&#39046;&#22495;&#21644;&#25552;&#20379;&#32773;&#30340;&#30693;&#35782;&#22270;&#35889;&#26159;&#20026;&#20102;&#23454;&#29616;&#26356;&#20840;&#38754;&#21644;&#19968;&#20307;&#21270;&#30340;&#34920;&#31034;&#12290;&#30446;&#21069;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#31639;&#27861;&#30340;&#19968;&#20010;&#20005;&#37325;&#23616;&#38480;&#26159;&#23427;&#20204;&#26080;&#27861;&#23558;&#36923;&#36753;&#24605;&#32771;&#21644;&#25512;&#29702;&#19982;&#35789;&#27719;&#12289;&#32467;&#26500;&#21644;&#35821;&#20041;&#25968;&#25454;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20511;&#37492;&#20102;&#23427;&#20204;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#33391;&#22909;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#37322;&#24615;&#12289;&#25512;&#29702;&#33021;&#21147;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#28151;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#27169;&#22411;&#26377;&#26395;&#23558;&#36923;&#36753;&#21644;&#25968;&#25454;&#35270;&#35282;&#25972;&#21512;&#36215;&#26469;&#65292;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#39640;&#36136;&#37327;&#23545;&#40784;&#32467;&#26524;&#65292;&#24182;&#25903;&#25345;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#39564;&#35777;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#24182;&#25506;&#35752;&#20102;&#31070;&#32463;&#31526;&#21495;&#25972;&#21512;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#20102;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KG) are the backbone of many data-intensive applications since they can represent data coupled with its meaning and context. Aligning KGs across different domains and providers is necessary to afford a fuller and integrated representation. A severe limitation of current KG alignment (KGA) algorithms is that they fail to articulate logical thinking and reasoning with lexical, structural, and semantic data learning. Deep learning models are increasingly popular for KGA inspired by their good performance in other tasks, but they suffer from limitations in explainability, reasoning, and data efficiency. Hybrid neurosymbolic learning models hold the promise of integrating logical and data perspectives to produce high-quality alignments that are explainable and support validation through human-centric approaches. This paper examines the current state of the art in KGA and explores the potential for neurosymbolic integration, highlighting promising research directions for co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Voronoi&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25317;&#25380;&#35270;&#39057;&#20013;&#24494;&#35266;&#32423;&#21035;&#22320;&#35782;&#21035;&#25512;&#21160;&#34892;&#20026;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#25317;&#25380;&#22330;&#26223;&#20013;&#25512;&#21160;&#34892;&#20026;&#30340;&#27169;&#24335;&#21644;&#20114;&#21160;&#65292;&#24182;&#20026;&#25317;&#25380;&#31649;&#29702;&#31574;&#30053;&#21644;&#20154;&#27969;&#37327;&#20248;&#21270;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.07416</link><description>&lt;p&gt;
&#22522;&#20110;Voronoi&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#22312;&#25317;&#25380;&#35270;&#39057;&#20013;&#25512;&#21160;&#20154;&#21592;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Novel Voronoi-based Convolutional Neural Network Framework for Pushing Person Detection in Crowd Videos. (arXiv:2310.07416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Voronoi&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25317;&#25380;&#35270;&#39057;&#20013;&#24494;&#35266;&#32423;&#21035;&#22320;&#35782;&#21035;&#25512;&#21160;&#34892;&#20026;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#25317;&#25380;&#22330;&#26223;&#20013;&#25512;&#21160;&#34892;&#20026;&#30340;&#27169;&#24335;&#21644;&#20114;&#21160;&#65292;&#24182;&#20026;&#25317;&#25380;&#31649;&#29702;&#31574;&#30053;&#21644;&#20154;&#27969;&#37327;&#20248;&#21270;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#25317;&#25380;&#22330;&#26223;&#20013;&#25512;&#21160;&#34892;&#20026;&#30340;&#24494;&#35266;&#21160;&#24577;&#21487;&#20197;&#20026;&#25317;&#25380;&#30340;&#27169;&#24335;&#21644;&#20114;&#21160;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#35782;&#21035;&#25317;&#25380;&#35270;&#39057;&#20013;&#30340;&#25512;&#21160;&#23454;&#20363;&#65292;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36825;&#31181;&#34892;&#20026;&#21457;&#29983;&#30340;&#26102;&#38388;&#12289;&#22320;&#28857;&#21644;&#21407;&#22240;&#12290;&#36825;&#23545;&#20110;&#21019;&#24314;&#26356;&#26377;&#25928;&#30340;&#25317;&#25380;&#31649;&#29702;&#31574;&#30053;&#12289;&#20248;&#21270;&#20154;&#27969;&#37327;&#21644;&#25552;&#21319;&#25972;&#20307;&#25317;&#25380;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#22312;&#24494;&#35266;&#23618;&#38754;&#19978;&#35782;&#21035;&#25512;&#21160;&#34892;&#20026;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#26041;&#27861;&#26080;&#27861;&#26816;&#27979;&#21040;&#36825;&#31181;&#24494;&#35266;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25317;&#25380;&#35270;&#39057;&#20013;&#24494;&#35266;&#32423;&#21035;&#22320;&#35782;&#21035;&#25512;&#21160;&#34892;&#20026;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;i&#65289;&#29305;&#24449;&#25552;&#21462;&#21644;ii&#65289;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#32452;&#20214;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;Voronoi&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#19982;&#36755;&#20837;&#35270;&#39057;&#20013;&#30340;&#27599;&#20010;&#20154;&#30456;&#20851;&#30340;&#23616;&#37096;&#21306;&#22495;&#12290;&#38543;&#21518;&#65292;&#23558;&#36825;&#20123;&#21306;&#22495;&#36755;&#20837;&#21040;&#27169;&#22411;&#20013;&#36827;&#34892;&#26631;&#35760;&#21644;&#25512;&#21160;&#34892;&#20026;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing the microscopic dynamics of pushing behavior within crowds can offer valuable insights into crowd patterns and interactions. By identifying instances of pushing in crowd videos, a deeper understanding of when, where, and why such behavior occurs can be achieved. This knowledge is crucial to creating more effective crowd management strategies, optimizing crowd flow, and enhancing overall crowd experiences. However, manually identifying pushing behavior at the microscopic level is challenging, and the existing automatic approaches cannot detect such microscopic behavior. Thus, this article introduces a novel automatic framework for identifying pushing in videos of crowds on a microscopic level. The framework comprises two main components: i) Feature extraction and ii) Video labeling. In the feature extraction component, a new Voronoi-based method is developed for determining the local regions associated with each person in the input video. Subsequently, these regions are fed in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.07402</link><description>&lt;p&gt;
NuTime: &#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#30740;&#31350;&#26174;&#31034;&#20986;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#38480;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#25968;&#21315;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25968;&#20540;&#29305;&#24615;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#30334;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#37319;&#29992;Transformer&#26550;&#26500;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#21010;&#20998;&#20026;&#38750;&#37325;&#21472;&#31383;&#21475;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#31383;&#21475;&#30340;&#26631;&#20934;&#21270;&#24418;&#29366;&#21644;&#20004;&#20010;&#26631;&#37327;&#20540;&#34920;&#31034;&#27599;&#20010;&#31383;&#21475;&#20869;&#30340;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#12290;&#20026;&#20102;&#23558;&#21487;&#33021;&#20855;&#26377;&#20219;&#24847;&#25968;&#20540;&#23610;&#24230;&#30340;&#26631;&#37327;&#20540;&#23884;&#20837;&#21040;&#39640;&#32500;&#21521;&#37327;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#26522;&#20030;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#37327;&#20540;&#23610;&#24230;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#25552;&#20986;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20302;&#32500;&#26680;&#20989;&#25968;&#65292;&#20197;&#20248;&#21270;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26426;&#22120;&#20154;&#36229;&#22768;&#25511;&#21046;&#22120;&#65292;&#35299;&#20915;&#20102;&#22312;&#19981;&#21516;&#24739;&#32773;&#20013;&#20248;&#21270;&#36229;&#22768;&#22270;&#20687;&#36136;&#37327;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07392</link><description>&lt;p&gt;
&#28145;&#24230;&#26680;&#21644;&#22270;&#20687;&#36136;&#37327;&#20272;&#35745;&#22120;&#29992;&#20110;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26426;&#22120;&#20154;&#36229;&#22768;&#25511;&#21046;&#22120;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Kernel and Image Quality Estimators for Optimizing Robotic Ultrasound Controller using Bayesian Optimization. (arXiv:2310.07392v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07392
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20302;&#32500;&#26680;&#20989;&#25968;&#65292;&#20197;&#20248;&#21270;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26426;&#22120;&#20154;&#36229;&#22768;&#25511;&#21046;&#22120;&#65292;&#35299;&#20915;&#20102;&#22312;&#19981;&#21516;&#24739;&#32773;&#20013;&#20248;&#21270;&#36229;&#22768;&#22270;&#20687;&#36136;&#37327;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#21307;&#23398;&#25104;&#20687;&#27169;&#24335;&#65292;&#38656;&#35201;&#19987;&#23478;&#36229;&#22768;&#25216;&#24072;&#26681;&#25454;&#33719;&#21462;&#30340;&#22270;&#20687;&#25163;&#21160;&#25805;&#32437;&#36229;&#22768;&#25506;&#22836;&#12290;&#33258;&#20027;&#26426;&#22120;&#20154;&#36229;&#22768;&#65288;A-RUS&#65289;&#26159;&#20943;&#36731;&#36229;&#22768;&#25216;&#24072;&#24037;&#20316;&#36127;&#25285;&#30340;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;A-RUS&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#19981;&#21516;&#24739;&#32773;&#20013;&#20248;&#21270;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#36229;&#22768;&#22270;&#20687;&#36136;&#37327;&#12290;&#36825;&#38656;&#35201;&#23545;&#35299;&#21078;&#23398;&#26377;&#25152;&#20102;&#35299;&#65292;&#35782;&#21035;&#38169;&#35823;&#28304;&#20197;&#21450;&#20934;&#30830;&#30340;&#25506;&#22836;&#20301;&#32622;&#12289;&#26041;&#21521;&#21644;&#21387;&#21147;&#12290;&#22312;&#20248;&#21270;&#19982;&#26426;&#22120;&#20154;&#25506;&#22836;&#25511;&#21046;&#22120;&#30456;&#20851;&#30340;&#36825;&#20123;&#21442;&#25968;&#26102;&#65292;&#26679;&#26412;&#25928;&#29575;&#38750;&#24120;&#37325;&#35201;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#26679;&#26412;&#25928;&#29575;&#39640;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#26368;&#36817;&#24050;&#32463;&#24212;&#29992;&#20110;&#20248;&#21270;&#25506;&#22836;&#30340;&#20108;&#32500;&#36816;&#21160;&#12290;&#28982;&#32780;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#26679;&#26412;&#25928;&#29575;&#65292;&#20197;&#36866;&#29992;&#20110;&#25506;&#22836;&#30340;&#39640;&#32500;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#19968;&#20010;&#20302;&#32500;&#26680;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultrasound is a commonly used medical imaging modality that requires expert sonographers to manually maneuver the ultrasound probe based on the acquired image. Autonomous Robotic Ultrasound (A-RUS) is an appealing alternative to this manual procedure in order to reduce sonographers' workload. The key challenge to A-RUS is optimizing the ultrasound image quality for the region of interest across different patients. This requires knowledge of anatomy, recognition of error sources and precise probe position, orientation and pressure. Sample efficiency is important while optimizing these parameters associated with the robotized probe controller. Bayesian Optimization (BO), a sample-efficient optimization framework, has recently been applied to optimize the 2D motion of the probe. Nevertheless, further improvements are needed to improve the sample efficiency for high-dimensional control of the probe. We aim to overcome this problem by using a neural network to learn a low-dimensional kernel
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#30382;&#32932;&#30284;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07380</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#21644;&#33030;&#24369;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Histopathological Image Classification and Vulnerability Analysis using Federated Learning. (arXiv:2310.07380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#30382;&#32932;&#30284;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#26159;&#26426;&#22120;&#23398;&#20064;(Machine Learning&#65292;ML)&#30340;&#20027;&#35201;&#24212;&#29992;&#20043;&#19968;&#12290;&#20256;&#32479;&#19978;&#65292;ML&#27169;&#22411;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#35757;&#32451;&#65292;&#36890;&#36807;&#27719;&#24635;&#26469;&#33258;&#21508;&#20010;&#20998;&#24067;&#24335;&#35774;&#22791;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;&#26032;&#29983;&#25104;&#25968;&#25454;&#30340;&#32467;&#26524;&#12290;&#36825;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#22240;&#20026;&#27169;&#22411;&#21487;&#20197;&#35775;&#38382;&#25935;&#24863;&#30340;&#29992;&#25143;&#20449;&#24687;&#65292;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;(FL)&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#20840;&#23616;&#27169;&#22411;&#23558;&#20854;&#21103;&#26412;&#21457;&#36865;&#32473;&#25152;&#26377;&#23458;&#25143;&#31471;&#65292;&#36825;&#20123;&#23458;&#25143;&#31471;&#35757;&#32451;&#36825;&#20123;&#21103;&#26412;&#65292;&#24182;&#23558;&#26356;&#26032;(&#26435;&#37325;)&#21457;&#36865;&#22238;&#32473;&#20840;&#23616;&#27169;&#22411;&#12290;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#20840;&#23616;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#20934;&#30830;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#65292;&#22240;&#20026;&#35757;&#32451;&#26159;&#22312;&#23458;&#25143;&#31471;&#35774;&#22791;&#19978;&#26412;&#22320;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#20840;&#23616;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#19968;&#20010;&#30382;&#32932;&#30284;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#24773;&#20917;&#12290;&#21313;&#20010;&#23458;&#25143;&#31471;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#20854;&#20013;&#19968;&#20010;&#25925;&#24847;&#24341;&#20837;&#20102;&#32763;&#36716;&#26631;&#31614;&#20316;&#20026;&#25915;&#20987;&#12290;&#36825;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare is one of the foremost applications of machine learning (ML). Traditionally, ML models are trained by central servers, which aggregate data from various distributed devices to forecast the results for newly generated data. This is a major concern as models can access sensitive user information, which raises privacy concerns. A federated learning (FL) approach can help address this issue: A global model sends its copy to all clients who train these copies, and the clients send the updates (weights) back to it. Over time, the global model improves and becomes more accurate. Data privacy is protected during training, as it is conducted locally on the clients' devices.  However, the global model is susceptible to data poisoning. We develop a privacy-preserving FL technique for a skin cancer dataset and show that the model is prone to data poisoning attacks. Ten clients train the model, but one of them intentionally introduces flipped labels as an attack. This reduces the accurac
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;CAUSE&#65289;&#26159;&#19968;&#20010;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#27010;&#24565;&#32858;&#31867;&#34920;&#20316;&#20026;&#20013;&#20171;&#65292;&#24182;&#19982;&#27010;&#24565;&#33258;&#30417;&#30563;&#23398;&#20064;&#24314;&#31435;&#32852;&#31995;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#20998;&#21106;&#20013;&#36866;&#24403;&#32858;&#31867;&#27700;&#24179;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07379</link><description>&lt;p&gt;
&#22240;&#26524;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Causal Unsupervised Semantic Segmentation. (arXiv:2310.07379v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07379
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;CAUSE&#65289;&#26159;&#19968;&#20010;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#27010;&#24565;&#32858;&#31867;&#34920;&#20316;&#20026;&#20013;&#20171;&#65292;&#24182;&#19982;&#27010;&#24565;&#33258;&#30417;&#30563;&#23398;&#20064;&#24314;&#31435;&#32852;&#31995;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#20998;&#21106;&#20013;&#36866;&#24403;&#32858;&#31867;&#27700;&#24179;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26088;&#22312;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#35821;&#20041;&#20998;&#32452;&#12290;&#38543;&#30528;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#20986;&#29616;&#65292;&#21508;&#31181;&#26694;&#26550;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#35757;&#32451;&#26080;&#30417;&#30563;&#23494;&#38598;&#39044;&#27979;&#30340;&#39044;&#27979;&#22836;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#30830;&#23450;&#29992;&#20110;&#20998;&#21106;&#27010;&#24565;&#25152;&#38656;&#30340;&#36866;&#24403;&#32858;&#31867;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22240;&#26524;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;CAUSE&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#22240;&#26524;&#25512;&#26029;&#30340;&#35265;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26725;&#25509;&#20102;&#38754;&#21521;&#24178;&#39044;&#30340;&#26041;&#27861;&#65288;&#21363;&#21069;&#38376;&#35843;&#25972;&#65289;&#65292;&#20197;&#23450;&#20041;&#36866;&#21512;&#26080;&#30417;&#30563;&#39044;&#27979;&#30340;&#20004;&#27493;&#20219;&#21153;&#12290;&#31532;&#19968;&#27493;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#27010;&#24565;&#32858;&#31867;&#34920;&#20316;&#20026;&#20013;&#20171;&#65292;&#20197;&#31163;&#25955;&#24418;&#24335;&#34920;&#31034;&#19981;&#21516;&#31890;&#24230;&#23618;&#27425;&#19978;&#30340;&#21487;&#33021;&#27010;&#24565;&#21407;&#22411;&#12290;&#28982;&#21518;&#65292;&#20013;&#20171;&#19982;&#38543;&#21518;&#30340;&#27010;&#24565;&#33258;&#30417;&#30563;&#23398;&#20064;&#24314;&#31435;&#20102;&#26126;&#30830;&#30340;&#32852;&#31995;...
&lt;/p&gt;
&lt;p&gt;
Unsupervised semantic segmentation aims to achieve high-quality semantic grouping without human-labeled annotations. With the advent of self-supervised pre-training, various frameworks utilize the pre-trained features to train prediction heads for unsupervised dense prediction. However, a significant challenge in this unsupervised setup is determining the appropriate level of clustering required for segmenting concepts. To address it, we propose a novel framework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages insights from causal inference. Specifically, we bridge intervention-oriented approach (i.e., frontdoor adjustment) to define suitable two-step tasks for unsupervised prediction. The first step involves constructing a concept clusterbook as a mediator, which represents possible concept prototypes at different levels of granularity in a discretized form. Then, the mediator establishes an explicit link to the subsequent concept-wise self-supervised learning for 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#20809;&#23376;&#23398;&#39046;&#22495;&#39318;&#27425;&#20351;&#29992;&#20809;&#23376;&#23454;&#39564;&#20272;&#35745;&#20102;&#37327;&#23376;&#33258;&#28982;&#26799;&#24230;&#65288;QNG&#65289;&#65292;&#24182;&#36890;&#36807;&#33719;&#24471;He-H$^+$&#38451;&#31163;&#23376;&#30340;&#35299;&#31163;&#26354;&#32447;&#24182;&#36798;&#21040;&#21270;&#23398;&#31934;&#24230;&#65292;&#39564;&#35777;&#20102;QNG&#20248;&#21270;&#22312;&#20809;&#23376;&#35774;&#22791;&#19978;&#30340;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07371</link><description>&lt;p&gt;
&#20809;&#23376;&#23398;&#20013;&#30340;&#23454;&#39564;&#37327;&#23376;&#33258;&#28982;&#26799;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Experimental quantum natural gradient optimization in photonics. (arXiv:2310.07371v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07371
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#20809;&#23376;&#23398;&#39046;&#22495;&#39318;&#27425;&#20351;&#29992;&#20809;&#23376;&#23454;&#39564;&#20272;&#35745;&#20102;&#37327;&#23376;&#33258;&#28982;&#26799;&#24230;&#65288;QNG&#65289;&#65292;&#24182;&#36890;&#36807;&#33719;&#24471;He-H$^+$&#38451;&#31163;&#23376;&#30340;&#35299;&#31163;&#26354;&#32447;&#24182;&#36798;&#21040;&#21270;&#23398;&#31934;&#24230;&#65292;&#39564;&#35777;&#20102;QNG&#20248;&#21270;&#22312;&#20809;&#23376;&#35774;&#22791;&#19978;&#30340;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQA&#65289;&#23558;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#21644;&#32463;&#20856;&#20248;&#21270;&#22120;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#22122;&#22768;&#20013;&#38388;&#23610;&#24230;&#37327;&#23376;&#26102;&#20195;&#30340;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;VQA&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20248;&#21270;&#26041;&#27861;&#12290;&#30456;&#27604;&#26080;&#26799;&#24230;&#21644;&#26222;&#36890;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#37327;&#23376;&#33258;&#28982;&#26799;&#24230;&#65288;QNG&#65289;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#65292;&#26356;&#23481;&#26131;&#36991;&#20813;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#20174;&#32780;&#20943;&#23569;&#30005;&#36335;&#25191;&#34892;&#25104;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#20840;&#21487;&#32534;&#31243;&#30340;&#20809;&#23376;&#33455;&#29255;&#39318;&#27425;&#22312;&#20809;&#23376;&#23398;&#20013;&#23454;&#39564;&#20272;&#35745;&#20102;QNG&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;He-H$^+$&#38451;&#31163;&#23376;&#30340;&#35299;&#31163;&#26354;&#32447;&#65292;&#24182;&#36798;&#21040;&#20102;&#21270;&#23398;&#31934;&#24230;&#65292;&#39564;&#35777;&#20102;QNG&#20248;&#21270;&#22312;&#20809;&#23376;&#35774;&#22791;&#19978;&#30340;&#36229;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#22312;&#20809;&#23376;&#23398;&#20013;&#21033;&#29992;QNG&#23454;&#29616;&#23454;&#29992;&#30340;&#36817;&#26399;&#37327;&#23376;&#24212;&#29992;&#24320;&#36767;&#20102;&#26032;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational quantum algorithms (VQAs) combining the advantages of parameterized quantum circuits and classical optimizers, promise practical quantum applications in the Noisy Intermediate-Scale Quantum era. The performance of VQAs heavily depends on the optimization method. Compared with gradient-free and ordinary gradient descent methods, the quantum natural gradient (QNG), which mirrors the geometric structure of the parameter space, can achieve faster convergence and avoid local minima more easily, thereby reducing the cost of circuit executions. We utilized a fully programmable photonic chip to experimentally estimate the QNG in photonics for the first time. We obtained the dissociation curve of the He-H$^+$ cation and achieved chemical accuracy, verifying the outperformance of QNG optimization on a photonic device. Our work opens up a vista of utilizing QNG in photonics to implement practical near-term quantum applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#30340;&#26680;&#36817;&#20284;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#24471;&#20986;&#20102;&#23574;&#38160;&#25351;&#25968;&#30028;&#38480;&#65292;&#25903;&#25345;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#27604;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#26356;&#20855;&#20449;&#24687;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07370</link><description>&lt;p&gt;
&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;: &#26126;&#30830;&#24418;&#24335;&#21644;&#23574;&#38160;&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
Orthogonal Random Features: Explicit Forms and Sharp Inequalities. (arXiv:2310.07370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#30340;&#26680;&#36817;&#20284;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#24471;&#20986;&#20102;&#23574;&#38160;&#25351;&#25968;&#30028;&#38480;&#65292;&#25903;&#25345;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#27604;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#26356;&#20855;&#20449;&#24687;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#36890;&#36807;&#38543;&#26426;&#21270;&#25216;&#26415;&#34987;&#24341;&#20837;&#20197;&#25193;&#23637;&#26680;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#21644;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#34987;&#29992;&#26469;&#36817;&#20284;&#27969;&#34892;&#30340;&#39640;&#26031;&#26680;&#12290;&#21069;&#32773;&#36890;&#36807;&#38543;&#26426;&#39640;&#26031;&#30697;&#38453;&#25191;&#34892;&#65292;&#24182;&#22312;&#24179;&#22343;&#21518;&#24471;&#21040;&#20102;&#23436;&#20840;&#31526;&#21512;&#39640;&#26031;&#26680;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22522;&#20110;&#29992;&#21040;Haar&#27491;&#20132;&#30697;&#38453;&#30340;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#30340;&#26680;&#36817;&#20284;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#25105;&#20204;&#20351;&#29992;&#24402;&#19968;&#21270;&#36125;&#22622;&#23572;&#20989;&#25968;&#25552;&#20379;&#20102;&#36825;&#20123;&#37327;&#30340;&#26126;&#30830;&#34920;&#36798;&#24335;&#65292;&#24182;&#25512;&#23548;&#20102;&#25903;&#25345;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#27604;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#23574;&#38160;&#25351;&#25968;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random features have been introduced to scale up kernel methods via randomization techniques. In particular, random Fourier features and orthogonal random features were used to approximate the popular Gaussian kernel. The former is performed by a random Gaussian matrix and leads exactly to the Gaussian kernel after averaging. In this work, we analyze the bias and the variance of the kernel approximation based on orthogonal random features which makes use of Haar orthogonal matrices. We provide explicit expressions for these quantities using normalized Bessel functions and derive sharp exponential bounds supporting the view that orthogonal random features are more informative than random Fourier features.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#20013;&#25913;&#36827;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21019;&#26032;&#31639;&#27861;&#19981;&#20165;&#26159;&#39318;&#20010;&#38750;&#20132;&#20114;&#24335;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#32780;&#19988;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.07367</link><description>&lt;p&gt;
&#22312;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#20013;&#25913;&#36827;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model. (arXiv:2310.07367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#20013;&#25913;&#36827;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21019;&#26032;&#31639;&#27861;&#19981;&#20165;&#26159;&#39318;&#20010;&#38750;&#20132;&#20114;&#24335;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#32780;&#19988;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#20013;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#38750;&#20132;&#20114;&#24335;&#21644;&#39034;&#24207;&#26412;&#22320;&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33719;&#21462;&#22522;&#30784;&#21442;&#25968;&#20026;1&#31232;&#30095;&#30340;&#19979;&#30028;&#19978;&#65292;&#23558;&#36825;&#20123;&#19979;&#30028;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;k&#31232;&#30095;&#24773;&#20917;&#35777;&#26126;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#26159;&#21542;&#23384;&#22312;&#39640;&#25928;&#30340;&#38750;&#20132;&#20114;&#24335;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#22312;&#949;&#38750;&#20132;&#20114;&#24335;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#23376;&#39640;&#26031;&#25968;&#25454;&#30340;L2&#33539;&#25968;&#20272;&#35745;&#35823;&#24046;&#25552;&#20986;&#20102;&#19968;&#20010;&#19979;&#30028;&#937;(sqrt(dklogd)/(sqrtn&#949;))&#65292;&#20854;&#20013;n&#26159;&#26679;&#26412;&#37327;&#65292;d&#26159;&#31354;&#38388;&#30340;&#32500;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38750;&#20132;&#20114;&#24335;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#36825;&#26159;&#35813;&#38382;&#39064;&#30340;&#39318;&#20010;&#38750;&#20132;&#20114;&#24335;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#12290;&#36825;&#20010;&#31639;&#27861;&#21487;&#20197;&#20135;&#29983;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#20316;&#20026;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we revisit the problem of sparse linear regression in the local differential privacy (LDP) model. Existing research in the non-interactive and sequentially local models has focused on obtaining the lower bounds for the case where the underlying parameter is $1$-sparse, and extending such bounds to the more general $k$-sparse case has proven to be challenging. Moreover, it is unclear whether efficient non-interactive LDP (NLDP) algorithms exist. To address these issues, we first consider the problem in the $\epsilon$ non-interactive LDP model and provide a lower bound of $\Omega(\frac{\sqrt{dk\log d}}{\sqrt{n}\epsilon})$ on the $\ell_2$-norm estimation error for sub-Gaussian data, where $n$ is the sample size and $d$ is the dimension of the space. We propose an innovative NLDP algorithm, the very first of its kind for the problem. As a remarkable outcome, this algorithm also yields a novel and highly efficient estimator as a valuable by-product. Our algorithm achieves an 
&lt;/p&gt;</description></item><item><title>&#22312;&#22270;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#20013;&#65292;GraphControl&#36890;&#36807;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;&#23454;&#29616;&#20102;&#23545;&#36890;&#29992;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#36801;&#31227;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#22270;&#22495;&#38388;&#30340;&#23646;&#24615;&#35821;&#20041;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07365</link><description>&lt;p&gt;
GraphControl:&#20026;&#22270;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning. (arXiv:2310.07365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07365
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#20013;&#65292;GraphControl&#36890;&#36807;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;&#23454;&#29616;&#20102;&#23545;&#36890;&#29992;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#36801;&#31227;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#22270;&#22495;&#38388;&#30340;&#23646;&#24615;&#35821;&#20041;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#19990;&#30028;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#36825;&#31181;&#25968;&#25454;&#27169;&#22411;&#20102;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20026;&#21508;&#31181;Web&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;Web&#19978;&#27599;&#22825;&#28044;&#29616;&#30340;&#26080;&#26631;&#31614;&#22270;&#25968;&#25454;&#20026;&#36825;&#20123;&#24212;&#29992;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22270;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#20174;&#20016;&#23500;&#30340;&#26080;&#26631;&#31614;&#22270;&#25968;&#25454;&#20013;&#33719;&#24471;&#36890;&#29992;&#30693;&#35782;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;Web&#24212;&#29992;&#65292;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#65292;&#25552;&#39640;&#19979;&#28216;&#65288;&#30446;&#26631;&#65289;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#34920;&#38754;&#19978;&#30475;&#36215;&#26469;&#30456;&#20284;&#30340;&#39046;&#22495;&#20013;&#65292;&#19981;&#21516;&#30340;&#22270;&#22312;&#23646;&#24615;&#35821;&#20041;&#26041;&#38754;&#20063;&#21487;&#33021;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#36825;&#32473;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36801;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#22256;&#38590;&#65292;&#29978;&#33267;&#26159;&#19981;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20363;&#22914;&#65292;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#38468;&#21152;&#29305;&#23450;&#20219;&#21153;&#33410;&#28857;&#20449;&#24687;&#65288;&#29305;&#24322;&#24615;&#65289;&#36890;&#24120;&#20250;&#34987;&#26377;&#24847;&#30465;&#30053;&#65292;&#20197;&#20415;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#65288;&#21487;&#36801;&#31227;&#24615;&#65289;&#12290;&#36825;&#31181;&#26435;&#34913;&#34987;&#31216;&#20026;
&lt;/p&gt;
&lt;p&gt;
Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;GAN-CNN&#27169;&#22411;&#20174;&#19977;&#32500;&#32467;&#26500;&#30913;&#20849;&#25391;&#22270;&#20687;&#65288;sMRI&#65289;&#35786;&#26029;&#21452;&#30456;&#24773;&#24863;&#38556;&#30861;&#65292;&#21462;&#24471;&#20102;&#27604;&#20808;&#21069;&#24037;&#20316;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#25935;&#24863;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.07359</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;GAN-CNN&#26041;&#27861;&#20174;3D&#32467;&#26500;&#30913;&#20849;&#25391;&#22270;&#20687;&#35786;&#26029;&#21452;&#30456;&#24773;&#24863;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance Images Using a Hybrid GAN-CNN Method. (arXiv:2310.07359v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;GAN-CNN&#27169;&#22411;&#20174;&#19977;&#32500;&#32467;&#26500;&#30913;&#20849;&#25391;&#22270;&#20687;&#65288;sMRI&#65289;&#35786;&#26029;&#21452;&#30456;&#24773;&#24863;&#38556;&#30861;&#65292;&#21462;&#24471;&#20102;&#27604;&#20808;&#21069;&#24037;&#20316;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#25935;&#24863;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#30456;&#24773;&#24863;&#38556;&#30861;&#65288;BD&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#30340;&#20130;&#22859;&#21644;&#25233;&#37057;&#21608;&#26399;&#36827;&#34892;&#35786;&#26029;&#30340;&#31934;&#31070;&#30142;&#30149;&#12290;&#30001;&#20110;&#35786;&#26029;BD&#20381;&#36182;&#20110;&#38271;&#26102;&#38388;&#20869;&#30340;&#20027;&#35266;&#34892;&#20026;&#35780;&#20272;&#65292;&#22522;&#20110;&#23458;&#35266;&#26631;&#20934;&#30340;&#21487;&#38752;&#35786;&#26029;&#24182;&#19981;&#30452;&#25509;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#28151;&#21512;GAN-CNN&#27169;&#22411;&#65292;&#20174;&#19977;&#32500;&#32467;&#26500;&#30913;&#20849;&#25391;&#22270;&#20687;&#65288;sMRI&#65289;&#35786;&#26029;BD&#26469;&#24212;&#23545;&#19978;&#36848;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#20351;&#29992;sMRI&#26679;&#26412;&#32780;&#19981;&#26159;&#20256;&#32479;&#25968;&#25454;&#38598;&#65288;&#22914;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#65292;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#21644;&#34892;&#20026;&#30151;&#29366;&#65289;&#26469;&#35786;&#26029;BD&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22788;&#29702;sMRI&#26679;&#26412;&#26102;&#36890;&#24120;&#36935;&#21040;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#36824;&#36890;&#36807;&#20351;&#29992;5&#20493;&#20132;&#21449;&#39564;&#35777;&#26469;&#27979;&#35797;&#19981;&#21516;&#22686;&#24378;&#27604;&#20363;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32467;&#26524;&#65292;&#26412;&#30740;&#31350;&#33719;&#24471;&#20102;75.8&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;60.3&#65285;&#30340;&#25935;&#24863;&#24615;&#21644;82.5&#65285;&#30340;&#29305;&#24322;&#24615;&#65292;&#27604;&#20808;&#21069;&#24037;&#20316;&#39640;&#20986;3-5&#65285;&#65292;&#21516;&#26102;&#20351;&#29992;&#19981;&#21040;6&#65285;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bipolar Disorder (BD) is a psychiatric condition diagnosed by repetitive cycles of hypomania and depression. Since diagnosing BD relies on subjective behavioral assessments over a long period, a solid diagnosis based on objective criteria is not straightforward. The current study responded to the described obstacle by proposing a hybrid GAN-CNN model to diagnose BD from 3-D structural MRI Images (sMRI). The novelty of this study stems from diagnosing BD from sMRI samples rather than conventional datasets such as functional MRI (fMRI), electroencephalography (EEG), and behavioral symptoms while removing the data insufficiency usually encountered when dealing with sMRI samples. The impact of various augmentation ratios is also tested using 5-fold cross-validation. Based on the results, this study obtains an accuracy rate of 75.8%, a sensitivity of 60.3%, and a specificity of 82.5%, which are 3-5% higher than prior work while utilizing less than 6% sample counts. Next, it is demonstrated 
&lt;/p&gt;</description></item><item><title>IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.07355</link><description>&lt;p&gt;
IMITATE: &#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training. (arXiv:2310.07355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07355
&lt;/p&gt;
&lt;p&gt;
IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#39046;&#22495;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#20174;&#20020;&#24202;&#25253;&#21578;&#21644;&#30456;&#20851;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#24573;&#35270;&#20102;&#21033;&#29992;&#20020;&#24202;&#25253;&#21578;&#22266;&#26377;&#30340;&#23618;&#32423;&#32467;&#26500;&#30340;&#26426;&#20250;&#65292;&#36825;&#20123;&#25253;&#21578;&#36890;&#24120;&#34987;&#20998;&#20026;&#25551;&#36848;&#24615;&#20869;&#23481;&#30340;&#8220;&#21457;&#29616;&#8221;&#21644;&#32467;&#35770;&#24615;&#35266;&#23519;&#30340;&#8220;&#21360;&#35937;&#8221;&#12290;&#24403;&#21069;&#30340;&#21307;&#23398;VLP&#26041;&#27861;&#24448;&#24448;&#23558;&#25253;&#21578;&#31616;&#21270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23454;&#20307;&#25110;&#20998;&#25955;&#30340;&#26631;&#35760;&#65292;&#32780;&#27809;&#26377;&#21033;&#29992;&#36825;&#31181;&#20016;&#23500;&#30340;&#12289;&#32467;&#26500;&#21270;&#30340;&#26684;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;VLP&#26694;&#26550;&#65292;&#21517;&#20026;IMITATE&#65292;&#29992;&#20110;&#20174;&#21307;&#23398;&#25253;&#21578;&#20013;&#23398;&#20064;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20174;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#20998;&#21035;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furth
&lt;/p&gt;</description></item><item><title>Atom-Motif Contrastive Transformer (AMCT) is proposed to address the issue of overlooking motif interactions in Molecular Property Prediction (MPP). AMCT explores both atom-level and motif-level interactions, enhancing the effectiveness of MPP.</title><link>http://arxiv.org/abs/2310.07351</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#23376;-&#20027;&#39064;&#23545;&#27604;&#30340;&#36716;&#25442;&#22120;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Atom-Motif Contrastive Transformer for Molecular Property Prediction. (arXiv:2310.07351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07351
&lt;/p&gt;
&lt;p&gt;
Atom-Motif Contrastive Transformer (AMCT) is proposed to address the issue of overlooking motif interactions in Molecular Property Prediction (MPP). AMCT explores both atom-level and motif-level interactions, enhancing the effectiveness of MPP.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#25551;&#36848;&#22270;&#33410;&#28857;&#65288;&#21363;&#20998;&#23376;&#20013;&#30340;&#21407;&#23376;&#65289;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#36739;&#39640;&#65292;&#22270;&#36716;&#25442;&#65288;GT&#65289;&#27169;&#22411;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#65288;MPP&#65289;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;GT&#30340;&#26041;&#27861;&#36890;&#24120;&#25506;&#32034;&#25104;&#23545;&#21407;&#23376;&#20043;&#38388;&#30340;&#22522;&#26412;&#20132;&#20114;&#65292;&#22240;&#27492;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#20998;&#23376;&#20013;&#20851;&#38190;&#32452;&#20998;&#65288;&#20363;&#22914;&#30001;&#20960;&#20010;&#21407;&#23376;&#32452;&#25104;&#30340;&#21151;&#33021;&#22522;&#22242;&#65289;&#20043;&#38388;&#30340;&#37325;&#35201;&#20132;&#20114;&#12290;&#30001;&#20110;&#20998;&#23376;&#20013;&#30340;&#27169;&#24335;&#23545;&#20110;&#30830;&#23450;&#20998;&#23376;&#24615;&#36136;&#65288;&#20363;&#22914;&#27602;&#24615;&#21644;&#28342;&#35299;&#24230;&#65289;&#38750;&#24120;&#37325;&#35201;&#65292;&#24573;&#35270;&#27169;&#24335;&#38388;&#30340;&#20132;&#20114;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#20102;MPP&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21407;&#23376;-&#20027;&#39064;&#23545;&#27604;&#30340;&#36716;&#25442;&#22120;&#65288;AMCT&#65289;&#65292;&#23427;&#19981;&#20165;&#25506;&#32034;&#20102;&#21407;&#23376;&#32423;&#21035;&#30340;&#20132;&#20114;&#65292;&#36824;&#32771;&#34385;&#20102;&#20027;&#39064;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Graph Transformer (GT) models have been widely used in the task of Molecular Property Prediction (MPP) due to their high reliability in characterizing the latent relationship among graph nodes (i.e., the atoms in a molecule). However, most existing GT-based methods usually explore the basic interactions between pairwise atoms, and thus they fail to consider the important interactions among critical motifs (e.g., functional groups consisted of several atoms) of molecules. As motifs in a molecule are significant patterns that are of great importance for determining molecular properties (e.g., toxicity and solubility), overlooking motif interactions inevitably hinders the effectiveness of MPP. To address this issue, we propose a novel Atom-Motif Contrastive Transformer (AMCT), which not only explores the atom-level interactions but also considers the motif-level interactions. Since the representations of atoms and motifs for a given molecule are actually two different views of t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;ELECTRA&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36741;&#21161;&#27169;&#22411;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#24179;&#28369;&#20027;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#30340;ELECTRA&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#36741;&#21161;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.07347</link><description>&lt;p&gt;
&#39640;&#25928;&#39044;&#35757;&#32451;&#30340;&#24555;&#36895;ELECTRA
&lt;/p&gt;
&lt;p&gt;
Fast-ELECTRA for Efficient Pre-training. (arXiv:2310.07347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07347
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;ELECTRA&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36741;&#21161;&#27169;&#22411;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#24179;&#28369;&#20027;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#30340;ELECTRA&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#36741;&#21161;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ELECTRA&#36890;&#36807;&#26816;&#27979;&#24207;&#21015;&#20013;&#34987;&#36741;&#21161;&#27169;&#22411;&#26367;&#25442;&#30340;&#26631;&#35760;&#26469;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#34429;&#28982;ELECTRA&#22823;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#20294;&#20854;&#28508;&#21147;&#21463;&#21040;&#20102;&#36741;&#21161;&#27169;&#22411;&#24102;&#26469;&#30340;&#35757;&#32451;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;&#19982;&#20027;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#30340;&#27169;&#22411;&#20165;&#29992;&#20110;&#36741;&#21161;&#20027;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#21518;&#34987;&#20002;&#24323;&#12290;&#36825;&#23548;&#33268;&#22823;&#37327;&#30340;&#35757;&#32451;&#25104;&#26412;&#34987;&#30333;&#30333;&#28010;&#36153;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fast-ELECTRA&#65292;&#23427;&#21033;&#29992;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36741;&#21161;&#27169;&#22411;&#12290;&#20026;&#20102;&#26500;&#24314;&#20027;&#27169;&#22411;&#30340;&#23398;&#20064;&#35838;&#31243;&#65292;&#25105;&#20204;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#21644;&#36882;&#20943;&#30340;&#26041;&#27861;&#24179;&#28369;&#20854;&#36755;&#20986;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;ELECTRA&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#36741;&#21161;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#38477;&#20302;&#20102;&#27169;&#22411;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07338</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Foundation Models for Learning on Tabular Data. (arXiv:2310.07338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#25903;&#25745;&#30528;&#20247;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#24320;&#21457;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#26377;&#25928;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#30446;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#35201;&#20040;&#32570;&#20047;&#30452;&#25509;&#25351;&#20196;&#36319;&#38543;&#26032;&#20219;&#21153;&#30340;&#25903;&#25345;&#65292;&#35201;&#20040;&#24573;&#35270;&#20174;&#19981;&#21516;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#22522;&#30784;&#30693;&#35782;&#21644;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;TabFMs&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32463;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#30446;&#26631;&#22312;&#22823;&#33539;&#22260;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#26041;&#27861;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24378;&#35843;&#20102;TabFM&#30340;&#26377;&#25928;&#24615;&#65306;&#23427;&#19981;&#20165;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#25512;&#29702;&#31561;&#36981;&#24490;&#25351;&#20196;&#30340;&#20219;&#21153;&#20013;&#26126;&#26174;&#20986;&#33394;&#65292;
&lt;/p&gt;
&lt;p&gt;
Learning on tabular data underpins numerous real-world applications. Despite considerable efforts in developing effective learning models for tabular data, current transferable tabular models remain in their infancy, limited by either the lack of support for direct instruction following in new tasks or the neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs) to overcome these limitations. TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets. This approach endows TabFMs with a profound understanding and universal capabilities essential for learning on tabular data. Our evaluations underscore TabFM's effectiveness: not only does it significantly excel in instruction-following tasks like zero-shot and in-context inference
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#31038;&#20132;&#36816;&#21160;&#28508;&#31354;&#38388;&#26469;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#23548;&#33322;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#23545;&#26426;&#22120;&#20154;&#30340;&#24847;&#35782;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#24847;&#35782;&#30340;&#34701;&#20837;&#33021;&#22815;&#25913;&#21892;&#23548;&#33322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07335</link><description>&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#25506;&#32034;&#31038;&#20132;&#36816;&#21160;&#28508;&#31354;&#38388;&#21644;&#20154;&#31867;&#24847;&#35782;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments. (arXiv:2310.07335v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#31038;&#20132;&#36816;&#21160;&#28508;&#31354;&#38388;&#26469;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#23548;&#33322;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#23545;&#26426;&#22120;&#20154;&#30340;&#24847;&#35782;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#24847;&#35782;&#30340;&#34701;&#20837;&#33021;&#22815;&#25913;&#21892;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31038;&#20132;&#36816;&#21160;&#28508;&#31354;&#38388;&#20013;&#23398;&#20064;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#31038;&#20132;&#23548;&#33322;&#25351;&#26631;&#30340;&#26174;&#33879;&#25913;&#21892;&#65292;&#22914;&#25104;&#21151;&#29575;&#12289;&#23548;&#33322;&#26102;&#38388;&#21644;&#36712;&#36857;&#38271;&#24230;&#65292;&#21516;&#26102;&#20135;&#29983;&#20102;&#26356;&#24179;&#32531;&#30340;&#36712;&#36857;&#65288;&#20943;&#23569;&#25238;&#21160;&#21644;&#35282;&#24230;&#20559;&#24046;&#65289;&#21644;&#26356;&#20855;&#39044;&#35265;&#24615;&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#19982;&#22522;&#20934;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#23558;&#20154;&#31867;&#23545;&#26426;&#22120;&#20154;&#30340;&#24847;&#35782;&#27010;&#24565;&#24341;&#20837;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#26694;&#26550;&#20013;&#65292;&#26174;&#31034;&#20986;&#20154;&#31867;&#24847;&#35782;&#30340;&#34701;&#20837;&#33021;&#22815;&#20943;&#23569;&#36712;&#36857;&#38271;&#24230;&#21644;&#24179;&#28369;&#36712;&#36857;&#65292;&#22240;&#20026;&#20154;&#31867;&#33021;&#22815;&#19982;&#26426;&#22120;&#20154;&#31215;&#26497;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a novel approach to social robot navigation by learning to generate robot controls from a social motion latent space. By leveraging this social motion latent space, the proposed method achieves significant improvements in social navigation metrics such as success rate, navigation time, and trajectory length while producing smoother (less jerk and angular deviations) and more anticipatory trajectories. The superiority of the proposed method is demonstrated through comparison with baseline models in various scenarios. Additionally, the concept of humans' awareness towards the robot is introduced into the social robot navigation framework, showing that incorporating human awareness leads to shorter and smoother trajectories owing to humans' ability to positively interact with the robot.
&lt;/p&gt;</description></item><item><title>&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.07325</link><description>&lt;p&gt;
&#30452;&#25509;&#36923;&#36753;&#23646;&#24615;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65306;gelu-4l&#20013;&#30340;&#20869;&#23384;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07325
&lt;/p&gt;
&lt;p&gt;
&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;4&#23618;transformer&#20013;&#20869;&#23384;&#31649;&#29702;&#30340;&#20855;&#20307;&#35777;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32452;&#20214;&#19968;&#33268;&#22320;&#31227;&#38500;&#21069;&#38754;&#32452;&#20214;&#30340;&#36755;&#20986;&#65292;&#36825;&#26159;&#19968;&#31181;&#28165;&#29702;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#24615;&#25216;&#26415;Direct Logit Attribution&#25552;&#20379;&#20102;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#31181;&#25216;&#26415;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#28165;&#29702;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#21147;&#21464;&#21387;&#22120;&#35786;&#26029;&#30340;&#22810;&#36890;&#36947;&#36830;&#32493;&#25968;&#25454;&#20132;&#21449;&#25552;&#21462;&#65288;MCDC&#65289;&#32467;&#26500;&#65292;&#20197;&#20840;&#38754;&#21033;&#29992;&#39034;&#24207;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27880;&#24847;&#21147;&#65288;1DCNN-attention&#65289;&#26426;&#21046;&#20197;&#38477;&#20302;&#31354;&#38388;&#22797;&#26434;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;MCDC&#30340;&#26377;&#25928;&#24615;&#21644;&#26356;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07323</link><description>&lt;p&gt;
&#20351;&#29992;1DCNN-attention&#36827;&#34892;&#22810;&#36890;&#36947;&#36830;&#32493;&#25968;&#25454;&#20132;&#21449;&#25552;&#21462;&#20197;&#35786;&#26029;&#30005;&#21147;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer. (arXiv:2310.07323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#21147;&#21464;&#21387;&#22120;&#35786;&#26029;&#30340;&#22810;&#36890;&#36947;&#36830;&#32493;&#25968;&#25454;&#20132;&#21449;&#25552;&#21462;&#65288;MCDC&#65289;&#32467;&#26500;&#65292;&#20197;&#20840;&#38754;&#21033;&#29992;&#39034;&#24207;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27880;&#24847;&#21147;&#65288;1DCNN-attention&#65289;&#26426;&#21046;&#20197;&#38477;&#20302;&#31354;&#38388;&#22797;&#26434;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;MCDC&#30340;&#26377;&#25928;&#24615;&#21644;&#26356;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#21464;&#21387;&#22120;&#22312;&#30005;&#32593;&#22522;&#30784;&#35774;&#26045;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20854;&#35786;&#26029;&#23545;&#20110;&#20445;&#25345;&#31283;&#23450;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#21464;&#21387;&#22120;&#35786;&#26029;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#31163;&#25955;&#30340;&#28342;&#35299;&#27668;&#20307;&#20998;&#26512;&#19978;&#65292;&#24573;&#35270;&#20102;&#22810;&#36890;&#36947;&#36830;&#32493;&#25968;&#25454;&#30340;&#28145;&#23618;&#29305;&#24449;&#25552;&#21462;&#12290;&#26410;&#34987;&#21033;&#29992;&#30340;&#39034;&#24207;&#25968;&#25454;&#21253;&#21547;&#21453;&#26144;&#21464;&#21387;&#22120;&#29366;&#24577;&#30340;&#37325;&#35201;&#26102;&#38388;&#20449;&#24687;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#36890;&#36947;&#36830;&#32493;&#25968;&#25454;&#20132;&#21449;&#25552;&#21462;&#65288;MCDC&#65289;&#30340;&#32467;&#26500;&#65292;&#20197;&#20840;&#38754;&#21033;&#29992;&#20869;&#22312;&#29305;&#24615;&#24182;&#35780;&#20272;&#21464;&#21387;&#22120;&#30340;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#36866;&#24212;&#21464;&#21387;&#22120;&#35786;&#26029;&#22330;&#26223;&#65292;&#24341;&#20837;&#20102;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27880;&#24847;&#21147;&#65288;1DCNN-attention&#65289;&#26426;&#21046;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#38477;&#20302;&#20102;&#31354;&#38388;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;&#39564;&#35777;&#20102;MCDC&#30340;&#26377;&#25928;&#24615;&#21644;&#26356;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Power transformer plays a critical role in grid infrastructure, and its diagnosis is paramount for maintaining stable operation. However, the current methods for transformer diagnosis focus on discrete dissolved gas analysis, neglecting deep feature extraction of multichannel consecutive data. The unutilized sequential data contains the significant temporal information reflecting the transformer condition. In light of this, the structure of multichannel consecutive data cross-extraction (MCDC) is proposed in this article in order to comprehensively exploit the intrinsic characteristic and evaluate the states of transformer. Moreover, for the better accommodation in scenario of transformer diagnosis, one dimensional convolution neural network attention (1DCNN-attention) mechanism is introduced and offers a more efficient solution given the simplified spatial complexity. Finally, the effectiveness of MCDC and the superior generalization ability, compared with other algorithms, are valida
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07321</link><description>&lt;p&gt;
&#20851;&#20110;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#22312;&#36890;&#29992;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#19978;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#31361;&#26174;&#20102;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#32771;&#23519;&#25968;&#25454;&#22810;&#26679;&#24615;&#39640;&#20110;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#39046;&#22495;&#25991;&#26412;&#30340;&#24503;&#35821;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#26088;&#22312;&#21253;&#21547;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21442;&#25968;&#33539;&#22260;&#20174;122M&#21040;750M&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20351;&#29992;&#36136;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#20808;&#21069;&#26368;&#20808;&#36827;&#32467;&#26524;&#19978;&#25552;&#20986;&#20102;&#39640;&#36798;4.45%&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;https://huggingface.co/ikim-uk-essen&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25308;&#21344;&#24237;&#23481;&#38169;&#30340;&#20998;&#25955;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#28151;&#21512;&#21644;&#20540;&#30340;&#25130;&#26029;&#23454;&#29616;&#20102;&#23545;&#25308;&#21344;&#24237;&#20195;&#29702;&#30340;&#24674;&#22797;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07320</link><description>&lt;p&gt;
&#25308;&#21344;&#24237;&#23481;&#38169;&#30340;&#20998;&#25955;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Resilient Decentralized Multi-Armed Bandits. (arXiv:2310.07320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07320
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25308;&#21344;&#24237;&#23481;&#38169;&#30340;&#20998;&#25955;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#28151;&#21512;&#21644;&#20540;&#30340;&#25130;&#26029;&#23454;&#29616;&#20102;&#23545;&#25308;&#21344;&#24237;&#20195;&#29702;&#30340;&#24674;&#22797;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#25955;&#24335;&#21512;&#20316;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#27599;&#20010;&#20195;&#29702;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#22870;&#21169;&#27969;&#65292;&#35797;&#22270;&#19982;&#20854;&#20182;&#20195;&#29702;&#20132;&#25442;&#20449;&#24687;&#20197;&#36873;&#25321;&#19968;&#31995;&#21015;&#25163;&#33218;&#20197;&#26368;&#23567;&#21270;&#36951;&#25022;&#12290;&#19982;&#29420;&#31435;&#36816;&#34892;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#31561;&#22810;&#33218;&#36172;&#21338;&#26426;&#26041;&#27861;&#30456;&#27604;&#65292;&#21327;&#20316;&#22330;&#26223;&#20013;&#30340;&#20195;&#29702;&#21487;&#20197;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26410;&#30693;&#27604;&#20363;&#30340;&#20195;&#29702;&#21487;&#33021;&#26159;&#25308;&#21344;&#24237;&#65288;&#21363;&#65292;&#20197;&#22870;&#21169;&#22343;&#20540;&#20272;&#35745;&#25110;&#32622;&#20449;&#24230;&#38598;&#30340;&#24418;&#24335;&#20256;&#36882;&#20219;&#24847;&#38169;&#35823;&#20449;&#24687;&#65289;&#26102;&#24674;&#22797;&#27492;&#31867;&#31361;&#20986;&#34892;&#20026;&#12290;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#27169;&#25311;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#30340;&#25915;&#20987;&#32773;&#65292;&#21521;&#25512;&#33616;&#31995;&#32479;&#20013;&#25554;&#20837;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#31574;&#21010;&#32773;&#65292;&#25110;&#32773;&#37329;&#34701;&#24066;&#22330;&#30340;&#25805;&#32437;&#32773;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#23436;&#20840;&#20998;&#25955;&#30340;&#20855;&#26377;&#23481;&#38169;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#20195;&#29702;&#38388;&#30340;&#20449;&#24687;&#28151;&#21512;&#27493;&#39588;&#19982;&#19981;&#19968;&#33268;&#21644;&#26497;&#31471;&#20540;&#30340;&#25130;&#26029;&#30456;&#32467;&#21512;&#12290;&#36825;&#20010;&#25130;&#26029;&#27493;&#39588;&#20351;&#25105;&#20204;&#33021;&#22815;&#24314;&#31435;
&lt;/p&gt;
&lt;p&gt;
In decentralized cooperative multi-armed bandits (MAB), each agent observes a distinct stream of rewards, and seeks to exchange information with others to select a sequence of arms so as to minimize its regret. Agents in the cooperative setting can outperform a single agent running a MAB method such as Upper-Confidence Bound (UCB) independently. In this work, we study how to recover such salient behavior when an unknown fraction of the agents can be Byzantine, that is, communicate arbitrarily wrong information in the form of reward mean-estimates or confidence sets. This framework can be used to model attackers in computer networks, instigators of offensive content into recommender systems, or manipulators of financial markets. Our key contribution is the development of a fully decentralized resilient upper confidence bound (UCB) algorithm that fuses an information mixing step among agents with a truncation of inconsistent and extreme values. This truncation step enables us to establis
&lt;/p&gt;</description></item><item><title>METRO&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26368;&#23567;&#27169;&#26495;&#36827;&#34892;&#21453;&#24212;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#28335;&#28304;&#39044;&#27979;&#20013;&#30340;&#35745;&#31639;&#24320;&#38144;&#22823;&#21644;&#35299;&#37322;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07313</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#30340;&#28335;&#28304;&#39044;&#27979;&#30340;&#20998;&#23376;&#32534;&#36753;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
Molecule-Edit Templates for Efficient and Accurate Retrosynthesis Prediction. (arXiv:2310.07313v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07313
&lt;/p&gt;
&lt;p&gt;
METRO&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26368;&#23567;&#27169;&#26495;&#36827;&#34892;&#21453;&#24212;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#28335;&#28304;&#39044;&#27979;&#20013;&#30340;&#35745;&#31639;&#24320;&#38144;&#22823;&#21644;&#35299;&#37322;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28335;&#28304;&#28041;&#21450;&#30830;&#23450;&#20174;&#31616;&#21333;&#21069;&#20307;&#21512;&#25104;&#22797;&#26434;&#20998;&#23376;&#30340;&#19968;&#31995;&#21015;&#21453;&#24212;&#12290;&#30001;&#20110;&#36825;&#22312;&#26377;&#26426;&#21270;&#23398;&#20013;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#39044;&#27979;&#32473;&#23450;&#30446;&#26631;&#20998;&#23376;&#30340;&#21487;&#33021;&#21453;&#24212;&#24213;&#29289;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#20998;&#20026;&#22522;&#20110;&#27169;&#26495;&#21644;&#22522;&#20110;&#38750;&#27169;&#26495;&#20004;&#31867;&#12290;&#21069;&#32773;&#39640;&#25928;&#20294;&#20381;&#36182;&#20110;&#22823;&#37327;&#39044;&#23450;&#20041;&#30340;&#21453;&#24212;&#27169;&#24335;&#65292;&#32780;&#21518;&#32773;&#34429;&#28982;&#26356;&#28789;&#27963;&#65292;&#20294;&#35745;&#31639;&#24320;&#38144;&#22823;&#19988;&#35299;&#37322;&#24615;&#36739;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;METRO&#65288;&#29992;&#20110;&#28335;&#28304;&#21512;&#25104;&#30340;&#20998;&#23376;&#32534;&#36753;&#27169;&#26495;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#26368;&#23567;&#27169;&#26495;&#36827;&#34892;&#21453;&#24212;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#31616;&#21270;&#20102;&#21453;&#24212;&#27169;&#24335;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis involves determining a sequence of reactions to synthesize complex molecules from simpler precursors. As this poses a challenge in organic chemistry, machine learning has offered solutions, particularly for predicting possible reaction substrates for a given target molecule. These solutions mainly fall into template-based and template-free categories. The former is efficient but relies on a vast set of predefined reaction patterns, while the latter, though more flexible, can be computationally intensive and less interpretable. To address these issues, we introduce METRO (Molecule-Edit Templates for RetrOsynthesis), a machine-learning model that predicts reactions using minimal templates - simplified reaction patterns capturing only essential molecular changes - reducing computational overhead and achieving state-of-the-art results on standard benchmarks.
&lt;/p&gt;</description></item><item><title>WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07312</link><description>&lt;p&gt;
WiGenAI: &#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26080;&#32447;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#32455;
&lt;/p&gt;
&lt;p&gt;
WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07312
&lt;/p&gt;
&lt;p&gt;
WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-3&#21644;&#31283;&#23450;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#23454;&#29616;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#21521;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21457;&#23637;&#12290;&#20174;&#25968;&#25454;&#36890;&#20449;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#35745;&#23558;&#24191;&#27867;&#24212;&#29992;&#20110;&#26410;&#26469;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#26032;&#19968;&#20195;&#20013;&#65292;&#24378;&#35843;&#20102;&#22312;&#26032;&#20852;&#36890;&#20449;&#22330;&#26223;&#20013;&#38656;&#35201;&#26032;&#39062;&#30340;AI&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#20171;&#32461;&#20102;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#20855;&#26377;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose de
&lt;/p&gt;</description></item><item><title>SNOiC&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;&#36719;&#26631;&#31614;&#21644;&#22122;&#22768;&#28151;&#21512;&#30340;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#20943;&#23569;&#20559;&#24046;&#21644;&#29983;&#25104;&#20266;&#25968;&#25454;&#26469;&#25552;&#39640;&#35782;&#21035;&#24320;&#25918;&#24847;&#22270;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07306</link><description>&lt;p&gt;
SNOiC: &#22522;&#20110;&#36719;&#26631;&#31614;&#21644;&#22122;&#22768;&#28151;&#21512;&#30340;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification Model. (arXiv:2310.07306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07306
&lt;/p&gt;
&lt;p&gt;
SNOiC&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;&#36719;&#26631;&#31614;&#21644;&#22122;&#22768;&#28151;&#21512;&#30340;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#20943;&#23569;&#20559;&#24046;&#21644;&#29983;&#25104;&#20266;&#25968;&#25454;&#26469;&#25552;&#39640;&#35782;&#21035;&#24320;&#25918;&#24847;&#22270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#26631;&#31614;&#21644;&#22122;&#22768;&#28151;&#21512;&#30340;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#27169;&#22411;&#65288;SNOiC&#65289;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#22823;&#22810;&#20351;&#29992;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#24320;&#25918;&#24847;&#22270;&#65292;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#26377;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#24320;&#25918;&#24847;&#22270;&#31867;&#21035;&#38656;&#35201;&#26356;&#22810;&#21487;&#29992;&#25968;&#25454;&#30340;&#38656;&#27714;&#20063;&#26159;&#29616;&#26377;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#38480;&#21046;&#12290;SNOiC&#23558;&#36719;&#26631;&#31614;&#21644;&#22122;&#22768;&#28151;&#21512;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20943;&#23569;&#20559;&#24046;&#24182;&#20026;&#24320;&#25918;&#24847;&#22270;&#31867;&#21035;&#29983;&#25104;&#20266;&#25968;&#25454;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SNOiC&#27169;&#22411;&#22312;&#35782;&#21035;&#24320;&#25918;&#24847;&#22270;&#26041;&#38754;&#30340;&#26368;&#20302;&#21644;&#26368;&#39640;&#24615;&#33021;&#20998;&#21035;&#20026;68.72&#65285;&#21644;94.71&#65285;&#12290;&#27492;&#22806;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;SNOiC&#27169;&#22411;&#22312;&#35782;&#21035;&#24320;&#25918;&#24847;&#22270;&#26041;&#38754;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;0.93&#65285;&#65288;&#26368;&#20302;&#65289;&#21644;12.76&#65285;&#65288;&#26368;&#39640;&#65289;&#12290;&#36890;&#36807;&#20998;&#26512;&#25152;&#25552;&#20986;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#21442;&#25968;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#36824;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#20854;&#20013;
&lt;/p&gt;
&lt;p&gt;
This paper presents a Soft Labeling and Noisy Mixup-based open intent classification model (SNOiC). Most of the previous works have used threshold-based methods to identify open intents, which are prone to overfitting and may produce biased predictions. Additionally, the need for more available data for an open intent class presents another limitation for these existing models. SNOiC combines Soft Labeling and Noisy Mixup strategies to reduce the biasing and generate pseudo-data for open intent class. The experimental results on four benchmark datasets show that the SNOiC model achieves a minimum and maximum performance of 68.72\% and 94.71\%, respectively, in identifying open intents. Moreover, compared to state-of-the-art models, the SNOiC model improves the performance of identifying open intents by 0.93\% (minimum) and 12.76\% (maximum). The model's efficacy is further established by analyzing various parameters used in the proposed model. An ablation study is also conducted, which
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#27604;&#20363;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#22810;&#31181;&#20010;&#20154;&#23646;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#38544;&#31169;&#27844;&#38706;&#30340;&#26032;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.07298</link><description>&lt;p&gt;
&#36229;&#36234;&#35760;&#24518;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#26469;&#20405;&#29359;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Beyond Memorization: Violating Privacy Via Inference with Large Language Models. (arXiv:2310.07298v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07298
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#27604;&#20363;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#22810;&#31181;&#20010;&#20154;&#23646;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#38544;&#31169;&#27844;&#38706;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38544;&#31169;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#21462;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#19978;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#24050;&#22823;&#24133;&#22686;&#24378;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#24403;&#21069;&#30340;LLMs&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#20174;&#25512;&#29702;&#26102;&#32473;&#20986;&#30340;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#26469;&#20405;&#29359;&#20010;&#20154;&#38544;&#31169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#39044;&#35757;&#32451;LLMs&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;Reddit&#20010;&#20154;&#36164;&#26009;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26174;&#31034;&#24403;&#21069;&#30340;LLMs&#21487;&#20197;&#25512;&#26029;&#20986;&#21508;&#31181;&#21508;&#26679;&#30340;&#20010;&#20154;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#20301;&#32622;&#12289;&#25910;&#20837;&#12289;&#24615;&#21035;&#65289;&#65292;&#22312;&#25104;&#26412;&#65288;100&#20493;&#65289;&#21644;&#26102;&#38388;&#65288;240&#20493;&#65289;&#19978;&#20165;&#38656;&#20154;&#31867;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#36798;&#21040;&#20102;&#26368;&#39640;1&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;85&#65285;&#65292;&#26368;&#39640;3&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;95.8&#65285;&#12290;&#38543;&#30528;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#30001;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20114;&#21160;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20405;&#29359;&#38544;&#31169;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#36890;&#36807;&#20284;&#20046;&#26080;&#20851;&#30340;&#23545;&#35805;&#35797;&#22270;&#25552;&#21462;&#20010;&#20154;&#20449;&#24687;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#34892;&#20026;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#24471;&#20998;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32791;&#26102;&#19988;&#35745;&#31639;&#23494;&#38598;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#22312;D4RL&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#36807;25&#20493;&#30340;&#21160;&#20316;&#37319;&#26679;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.07297</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#34892;&#20026;&#23454;&#29616;&#24471;&#20998;&#27491;&#21017;&#21270;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score Regularized Policy Optimization through Diffusion Behavior. (arXiv:2310.07297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07297
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#34892;&#20026;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#24471;&#20998;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32791;&#26102;&#19988;&#35745;&#31639;&#23494;&#38598;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#22312;D4RL&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#36807;25&#20493;&#30340;&#21160;&#20316;&#37319;&#26679;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#24314;&#27169;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#36825;&#20805;&#20998;&#23637;&#29616;&#20102;&#20854;&#22312;&#34920;&#36798;&#24322;&#36136;&#34892;&#20026;&#31574;&#30053;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25193;&#25955;&#31574;&#30053;&#20013;&#37319;&#26679;&#38750;&#24120;&#32531;&#24930;&#65292;&#22240;&#20026;&#38656;&#35201;&#25968;&#21313;&#21040;&#25968;&#30334;&#27425;&#36845;&#20195;&#25512;&#29702;&#27493;&#39588;&#26469;&#36827;&#34892;&#19968;&#27425;&#21160;&#20316;&#37319;&#26679;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35780;&#35770;&#23478;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#34892;&#20026;&#27169;&#22411;&#20013;&#25552;&#21462;&#39640;&#25928;&#30830;&#23450;&#24615;&#25512;&#29702;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21033;&#29992;&#21518;&#32773;&#30452;&#25509;&#23545;&#31574;&#30053;&#26799;&#24230;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20351;&#29992;&#34892;&#20026;&#20998;&#24067;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#36807;&#31243;&#20013;&#20805;&#20998;&#21457;&#25381;&#20102;&#25193;&#25955;&#24314;&#27169;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#65292;&#21516;&#26102;&#23436;&#20840;&#32469;&#36807;&#20102;&#35745;&#31639;&#23494;&#38598;&#21644;&#32791;&#26102;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#26696;&#12290;&#22312;D4RL&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21160;&#20316;&#37319;&#26679;&#36895;&#24230;&#25552;&#39640;&#20102;&#36229;&#36807;25&#20493;&#65292;&#30456;&#27604;&#20110;&#21508;&#31181;&#39046;&#20808;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#22312;&#36816;&#21160;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion ta
&lt;/p&gt;</description></item><item><title>BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07276</link><description>&lt;p&gt;
BioT5&#65306;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#36328;&#27169;&#24577;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07276
&lt;/p&gt;
&lt;p&gt;
BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21033;&#29992;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#25972;&#21512;&#26469;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#29983;&#25104;&#26080;&#25928;&#30340;&#20998;&#23376;SMILES&#12289;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#20197;&#21450;&#23545;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#31561;&#37327;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;BioT5&#65292;&#23427;&#36890;&#36807;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#29983;&#29289;&#23398;&#20013;&#30340;&#36328;&#27169;&#24577;&#25972;&#21512;&#12290;BioT5&#21033;&#29992;SELFIES&#36827;&#34892;100%&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#29983;&#29289;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#29289;&#23454;&#20307;&#21608;&#22260;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;BioT5&#21306;&#20998;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20449;&#24687;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;BioT5&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;Sharpness-Aware Minimization&#65288;SAM&#65289;&#30456;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#19968;&#23450;&#25968;&#25454;&#27169;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#27867;&#21270;&#26356;&#22909;&#30340;&#31354;&#30333;&#65292;&#35299;&#37322;&#20102;SAM&#30340;&#20248;&#21183;&#65292;&#23588;&#20854;&#26159;&#22312;&#26089;&#26399;&#38454;&#27573;&#38450;&#27490;&#22122;&#22768;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07269</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#27604;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#22909;&#22320;&#25512;&#24191;&#65311;(arXiv:2310.07269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Why Does Sharpness-Aware Minimization Generalize Better Than SGD?. (arXiv:2310.07269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07269
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;Sharpness-Aware Minimization&#65288;SAM&#65289;&#30456;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#19968;&#23450;&#25968;&#25454;&#27169;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#27867;&#21270;&#26356;&#22909;&#30340;&#31354;&#30333;&#65292;&#35299;&#37322;&#20102;SAM&#30340;&#20248;&#21183;&#65292;&#23588;&#20854;&#26159;&#22312;&#26089;&#26399;&#38454;&#27573;&#38450;&#27490;&#22122;&#22768;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#25311;&#21512;&#30340;&#25361;&#25112;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#23427;&#25351;&#30340;&#26159;&#27169;&#22411;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#26080;&#27861;&#25512;&#24191;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;SAM&#30340;&#24037;&#20316;&#26041;&#24335;&#20173;&#28982;&#32570;&#20047;&#28145;&#20837;&#29702;&#35299;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#20026;&#20160;&#20040;&#22312;&#29305;&#23450;&#25968;&#25454;&#27169;&#22411;&#21644;&#20004;&#23618;&#21367;&#31215;ReLU&#32593;&#32476;&#20013;&#65292;SAM&#27604;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#22909;&#22320;&#25512;&#24191;&#65292;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25152;&#30740;&#31350;&#38382;&#39064;&#30340;&#25439;&#22833;&#26223;&#35266;&#26159;&#38750;&#20809;&#28369;&#30340;&#65292;&#22240;&#27492;&#30446;&#21069;&#20851;&#20110;SAM&#25104;&#21151;&#30340;&#35299;&#37322;&#22522;&#20110;Hessian&#20449;&#24687;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35299;&#37322;&#20102;SAM&#30340;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#23427;&#22312;&#26089;&#26399;&#38454;&#27573;&#38450;&#27490;&#20102;&#22122;&#22768;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of overfitting, in which the model memorizes the training data and fails to generalize to test data, has become increasingly significant in the training of large neural networks. To tackle this challenge, Sharpness-Aware Minimization (SAM) has emerged as a promising training method, which can improve the generalization of neural networks even in the presence of label noise. However, a deep understanding of how SAM works, especially in the setting of nonlinear neural networks and classification tasks, remains largely missing. This paper fills this gap by demonstrating why SAM generalizes better than Stochastic Gradient Descent (SGD) for a certain data model and two-layer convolutional ReLU networks. The loss landscape of our studied problem is nonsmooth, thus current explanations for the success of SAM based on the Hessian information are insufficient. Our result explains the benefits of SAM, particularly its ability to prevent noise learning in the early stages, thereby f
&lt;/p&gt;</description></item><item><title>RaftFed&#26159;&#19968;&#31181;&#38754;&#21521;&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#30340;&#36731;&#37327;&#32423;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#20013;&#23384;&#22312;&#30340;&#21487;&#38752;&#24615;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.07268</link><description>&lt;p&gt;
RaftFed:&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#29992;&#20110;&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RaftFed: A Lightweight Federated Learning Framework for Vehicular Crowd Intelligence. (arXiv:2310.07268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07268
&lt;/p&gt;
&lt;p&gt;
RaftFed&#26159;&#19968;&#31181;&#38754;&#21521;&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#30340;&#36731;&#37327;&#32423;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#20013;&#23384;&#22312;&#30340;&#21487;&#38752;&#24615;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36890;&#36807;&#20808;&#36827;&#30340;&#36710;&#36733;&#33258;&#32452;&#32455;&#32593;&#32476;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25903;&#25345;&#65292;&#21508;&#31181;&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#24212;&#29992;&#24320;&#22987;&#20986;&#29616;&#65292;&#22914;&#21327;&#20316;&#24863;&#30693;&#12289;&#23450;&#20301;&#21644;&#21046;&#22270;&#12290;&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#24212;&#29992;&#30340;&#21327;&#20316;&#24615;&#36890;&#24120;&#38656;&#35201;&#21442;&#19982;&#32773;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#65292;&#20174;&#32780;&#24418;&#25104;&#32593;&#32476;&#33539;&#22260;&#20869;&#30340;&#26234;&#33021;&#12290;&#22914;&#20309;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36825;&#20010;&#36807;&#31243;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#65292;&#20294;&#23558;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#36866;&#24212;&#20110;&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#39318;&#20808;&#65292;&#30001;&#20110;&#23384;&#22312;&#20449;&#36947;&#26465;&#20214;&#19981;&#21033;&#30340;&#20572;&#28382;&#32773;&#65292;&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#20013;&#30340;&#38598;&#20013;&#24335;&#27169;&#22411;&#32858;&#21512;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#23481;&#26131;&#21463;&#21040;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#32780;&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21152;&#21095;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RaftFed&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#20445;&#25252;&#38544;&#31169;&#30340;&#36710;&#36733;&#32676;&#20307;&#26234;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RaftFed&#22312;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicular crowd intelligence (VCI) is an emerging research field. Facilitated by state-of-the-art vehicular ad-hoc networks and artificial intelligence, various VCI applications come to place, e.g., collaborative sensing, positioning, and mapping. The collaborative property of VCI applications generally requires data to be shared among participants, thus forming network-wide intelligence. How to fulfill this process without compromising data privacy remains a challenging issue. Although federated learning (FL) is a promising tool to solve the problem, adapting conventional FL frameworks to VCI is nontrivial. First, the centralized model aggregation is unreliable in VCI because of the existence of stragglers with unfavorable channel conditions. Second, existing FL schemes are vulnerable to Non-IID data, which is intensified by the data heterogeneity in VCI. This paper proposes a novel federated learning framework called RaftFed to facilitate privacy-preserving VCI. The experimental resu
&lt;/p&gt;</description></item><item><title>&#26412;&#31995;&#32479;&#24615;&#32508;&#36848;&#20998;&#26512;&#20102;&#22522;&#20110;&#20005;&#37325;&#31243;&#24230;&#23545;&#21457;&#38899;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#26368;&#26377;&#25928;&#30340;&#29305;&#24449;&#38598;&#21512;&#21644;&#31867;&#22411;&#20197;&#21450;&#26368;&#20339;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.07264</link><description>&lt;p&gt;
&#22522;&#20110;&#20005;&#37325;&#31243;&#24230;&#30340;&#21457;&#38899;&#38556;&#30861;&#20998;&#31867;&#12290;&#19968;&#39033;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification of Dysarthria based on the Levels of Severity. A Systematic Review. (arXiv:2310.07264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31995;&#32479;&#24615;&#32508;&#36848;&#20998;&#26512;&#20102;&#22522;&#20110;&#20005;&#37325;&#31243;&#24230;&#23545;&#21457;&#38899;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#26368;&#26377;&#25928;&#30340;&#29305;&#24449;&#38598;&#21512;&#21644;&#31867;&#22411;&#20197;&#21450;&#26368;&#20339;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#38899;&#38556;&#30861;&#26159;&#19968;&#31181;&#31070;&#32463;&#24615;&#35821;&#35328;&#38556;&#30861;&#65292;&#20005;&#37325;&#24433;&#21709;&#24739;&#32773;&#30340;&#27807;&#36890;&#33021;&#21147;&#21644;&#29983;&#27963;&#36136;&#37327;&#12290;&#20934;&#30830;&#21644;&#23458;&#35266;&#22320;&#23545;&#21457;&#38899;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#21644;&#30830;&#23450;&#20854;&#20005;&#37325;&#31243;&#24230;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#24178;&#39044;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#35328;&#35821;&#30149;&#29702;&#23398;&#23478;&#35780;&#20272;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#24448;&#24448;&#26159;&#20027;&#35266;&#30340;&#12289;&#32791;&#26102;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#22240;&#20174;&#19994;&#20154;&#21592;&#32780;&#24322;&#12290;&#26032;&#20852;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#25552;&#20379;&#26356;&#23458;&#35266;&#30340;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#30340;&#28508;&#21147;&#65292;&#22686;&#24378;&#20102;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#31995;&#32479;&#24615;&#32508;&#36848;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#30446;&#21069;&#29992;&#20110;&#22522;&#20110;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#21457;&#38899;&#38556;&#30861;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#32508;&#36848;&#23558;&#37325;&#28857;&#30830;&#23450;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#24739;&#32773;&#20998;&#31867;&#30340;&#26368;&#26377;&#25928;&#30340;&#29305;&#24449;&#38598;&#21512;&#21644;&#31867;&#22411;&#65292;&#24182;&#35780;&#20272;&#26368;&#20339;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;&#25105;&#20204;&#23558;&#31995;&#32479;&#22320;&#23457;&#26597;&#29616;&#26377;&#30340;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dysarthria is a neurological speech disorder that can significantly impact affected individuals' communication abilities and overall quality of life. The accurate and objective classification of dysarthria and the determination of its severity are crucial for effective therapeutic intervention. While traditional assessments by speech-language pathologists (SLPs) are common, they are often subjective, time-consuming, and can vary between practitioners. Emerging machine learning-based models have shown the potential to provide a more objective dysarthria assessment, enhancing diagnostic accuracy and reliability. This systematic review aims to comprehensively analyze current methodologies for classifying dysarthria based on severity levels. Specifically, this review will focus on determining the most effective set and type of features that can be used for automatic patient classification and evaluating the best AI techniques for this purpose. We will systematically review the literature o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#20998;&#21106;&#19978;&#20351;&#29992;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#23545;&#36830;&#32493;&#20998;&#27573;&#22810;&#39033;&#24335;&#20989;&#25968;&#30340;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#23637;&#24320;&#31995;&#25968;&#36827;&#34892;&#32534;&#30721;&#30340;&#26032;&#39062;ReLU NN&#26367;&#20195;&#27169;&#22411;&#26500;&#36896;&#65292;&#19982;&#22522;&#20110;ReLU NN&#27169;&#25311;&#22810;&#39033;&#24335;&#30340;&#26500;&#36896;&#30456;&#27604;&#65292;&#22312;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.07261</link><description>&lt;p&gt;
&#28145;&#24230;ReLU&#32593;&#32476;&#21644;&#39640;&#38454;&#26377;&#38480;&#20803;&#26041;&#27861;II&#65306;&#20999;&#27604;&#38634;&#22827;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Deep ReLU networks and high-order finite element methods II: Chebyshev emulation. (arXiv:2310.07261v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#20998;&#21106;&#19978;&#20351;&#29992;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#23545;&#36830;&#32493;&#20998;&#27573;&#22810;&#39033;&#24335;&#20989;&#25968;&#30340;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#23637;&#24320;&#31995;&#25968;&#36827;&#34892;&#32534;&#30721;&#30340;&#26032;&#39062;ReLU NN&#26367;&#20195;&#27169;&#22411;&#26500;&#36896;&#65292;&#19982;&#22522;&#20110;ReLU NN&#27169;&#25311;&#22810;&#39033;&#24335;&#30340;&#26500;&#36896;&#30456;&#27604;&#65292;&#22312;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#26377;&#30028;&#21306;&#38388;$(a,b)$&#19978;&#30340;&#20219;&#24847;&#26377;&#38480;&#20998;&#21106;$\mathcal{T}$&#19978;&#65292;&#36830;&#32493;&#30340;&#12289;&#20998;&#27573;&#22810;&#39033;&#24335;&#20989;&#25968;&#30340;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22312;&#23450;&#20041;NN&#30340;&#21442;&#25968;&#25968;&#37327;&#26041;&#38754;&#23545;Sobolev&#33539;&#25968;&#30340;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;ReLU NN&#26367;&#20195;&#27169;&#22411;&#26500;&#36896;&#65292;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#23637;&#24320;&#31995;&#25968;&#23545;&#36817;&#20284;&#20989;&#25968;&#36827;&#34892;&#32534;&#30721;&#12290;&#21487;&#20197;&#36890;&#36807;&#23558;&#20989;&#25968;&#22312;Clenshaw-Curtis&#28857;&#19978;&#30340;&#20540;&#20351;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#36870;&#21464;&#25442;&#36731;&#26494;&#22320;&#35745;&#31639;&#20986;&#20999;&#27604;&#38634;&#22827;&#31995;&#25968;&#12290;&#22312;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#33719;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ReLU NN&#27169;&#25311;&#22810;&#39033;&#24335;&#30340;&#26500;&#36896;[Opschoor&#65292;Petersen&#65292;Schwab&#65292;2020]&#30340;&#30028;&#38480;&#12290;&#25152;&#26377;&#27169;&#25311;&#19978;&#30028;&#37117;&#26126;&#30830;&#22320;&#19982;&#21306;&#38388;&#30340;&#65288;&#20219;&#24847;&#65289;&#20998;&#21106;&#12289;&#30446;&#26631;&#27169;&#25311;&#31934;&#24230;&#21644;&#20998;&#21106;&#20013;&#27599;&#20010;&#20803;&#32032;&#30340;&#22810;&#39033;&#24335;&#27425;&#25968;&#26377;&#20851;&#12290;&#25552;&#20379;&#20102;ReLU NN&#27169;&#25311;&#35823;&#24046;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expression rates and stability in Sobolev norms of deep ReLU neural networks (NNs) in terms of the number of parameters defining the NN for continuous, piecewise polynomial functions, on arbitrary, finite partitions $\mathcal{T}$ of a bounded interval $(a,b)$ are addressed. Novel constructions of ReLU NN surrogates encoding the approximated functions in terms of Chebyshev polynomial expansion coefficients are developed. Chebyshev coefficients can be computed easily from the values of the function in the Clenshaw--Curtis points using the inverse fast Fourier transform. Bounds on expression rates and stability that are superior to those of constructions based on ReLU NN emulations of monomials considered in [Opschoor, Petersen, Schwab, 2020] are obtained. All emulation bounds are explicit in terms of the (arbitrary) partition of the interval, the target emulation accuracy and the polynomial degree in each element of the partition. ReLU NN emulation error estimates are provided for variou
&lt;/p&gt;</description></item><item><title>ADMEOOD&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#30340;&#36229;&#20998;&#24067;&#22522;&#20934;&#65292;&#21253;&#21547;27&#20010;&#33647;&#29289;&#23646;&#24615;&#21644;&#20004;&#31181;&#25968;&#25454;&#36716;&#31227;&#65288;&#22122;&#22768;&#36716;&#31227;&#21644;&#27010;&#24565;&#20914;&#31361;&#28418;&#31227;&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.07253</link><description>&lt;p&gt;
ADMEOOD: &#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#30340;&#36229;&#20998;&#24067;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction. (arXiv:2310.07253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07253
&lt;/p&gt;
&lt;p&gt;
ADMEOOD&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#30340;&#36229;&#20998;&#24067;&#22522;&#20934;&#65292;&#21253;&#21547;27&#20010;&#33647;&#29289;&#23646;&#24615;&#21644;&#20004;&#31181;&#25968;&#25454;&#36716;&#31227;&#65288;&#22122;&#22768;&#36716;&#31227;&#21644;&#27010;&#24565;&#20914;&#31361;&#28418;&#31227;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#24471;&#20934;&#30830;&#26377;&#25928;&#30340;&#33647;&#29289;&#20998;&#23376;&#20449;&#24687;&#26159;&#19968;&#39033;&#20851;&#38190;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;100&#24180;&#26469;&#65292;&#21270;&#23398;&#30693;&#35782;&#21644;&#20449;&#24687;&#26469;&#33258;&#19981;&#21516;&#22320;&#21306;&#12289;&#23454;&#39564;&#23460;&#21644;&#23454;&#39564;&#30446;&#30340;&#30340;&#31215;&#32047;&#12290;&#22312;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#38382;&#39064;&#20013;&#65292;&#22122;&#38899;&#21644;&#19981;&#19968;&#33268;&#24615;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24369;&#40065;&#26834;&#24615;&#21644;&#19981;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ADMEOOD, &#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#30340;&#31995;&#32479;&#36229;&#20998;&#24067;&#25968;&#25454;&#38598;&#31574;&#21010;&#32773;&#21644;&#22522;&#20934;&#12290;ADMEOOD&#20174;Chembl&#21644;&#30456;&#20851;&#25991;&#29486;&#20013;&#33719;&#24471;&#20102;27&#20010;&#33647;&#29289;&#23646;&#24615;&#12290;&#21478;&#22806;&#65292;&#23427;&#36824;&#21253;&#25324;&#20004;&#31181;&#36229;&#20998;&#24067;&#25968;&#25454;&#36716;&#31227;&#65306;&#22122;&#22768;&#36716;&#31227;&#21644;&#27010;&#24565;&#20914;&#31361;&#28418;&#31227;&#65288;CCD&#65289;&#12290;&#22122;&#22768;&#36716;&#31227;&#36890;&#36807;&#23558;&#29615;&#22659;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#32622;&#20449;&#27700;&#24179;&#26469;&#21709;&#24212;&#22122;&#22768;&#27700;&#24179;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;CCD&#25551;&#36848;&#20102;&#21407;&#22987;&#25968;&#25454;&#20013;&#26631;&#31614;&#19981;&#19968;&#33268;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining accurate and valid information for drug molecules is a crucial and challenging task. However, chemical knowledge and information have been accumulated over the past 100 years from various regions, laboratories, and experimental purposes. Little has been explored in terms of the out-of-distribution (OOD) problem with noise and inconsistency, which may lead to weak robustness and unsatisfied performance. This study proposes a novel benchmark ADMEOOD, a systematic OOD dataset curator and benchmark specifically designed for drug property prediction. ADMEOOD obtained 27 ADME (Absorption, Distribution, Metabolism, Excretion) drug properties from Chembl and relevant literature. Additionally, it includes two kinds of OOD data shifts: Noise Shift and Concept Conflict Drift (CCD). Noise Shift responds to the noise level by categorizing the environment into different confidence levels. On the other hand, CCD describes the data which has inconsistent label among the original data. Finall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;CNN&#21644;GRU&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;GRU&#30340;&#35299;&#30721;&#22120;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MSCOCO&#21644;Flickr30k&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2310.07252</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;CNN&#21644;GRU&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Pre-trained CNNs and GRU-Based Attention for Image Caption Generation. (arXiv:2310.07252v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;CNN&#21644;GRU&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;GRU&#30340;&#35299;&#30721;&#22120;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MSCOCO&#21644;Flickr30k&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20026;&#22270;&#20687;&#29983;&#25104;&#25991;&#26412;&#25551;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;GRU&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#32534;&#30721;&#22120;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;GRU&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35299;&#30721;&#22120;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;Bahdanau&#27880;&#24847;&#21147;&#27169;&#22411;&#19982;GRU&#35299;&#30721;&#22120;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#22270;&#20687;&#37096;&#20998;&#12290;&#25105;&#20204;&#20351;&#29992;MSCOCO&#21644;Flickr30k&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#24357;&#21512;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#21487;&#20197;&#25193;&#23637;&#21040;&#29305;&#23450;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image captioning is a challenging task involving generating a textual description for an image using computer vision and natural language processing techniques. This paper proposes a deep neural framework for image caption generation using a GRU-based attention mechanism. Our approach employs multiple pre-trained convolutional neural networks as the encoder to extract features from the image and a GRU-based language model as the decoder to generate descriptive sentences. To improve performance, we integrate the Bahdanau attention model with the GRU decoder to enable learning to focus on specific image parts. We evaluate our approach using the MSCOCO and Flickr30k datasets and show that it achieves competitive scores compared to state-of-the-art methods. Our proposed framework can bridge the gap between computer vision and natural language and can be extended to specific domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20174;&#24050;&#26377;&#30340;&#27169;&#24577;&#29983;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#22312;BraTS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07250</link><description>&lt;p&gt;
&#22312;BraTS&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20174;&#24050;&#26377;&#30340;&#27169;&#24577;&#29983;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset. (arXiv:2310.07250v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20174;&#24050;&#26377;&#30340;&#27169;&#24577;&#29983;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#22312;BraTS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24230;&#20405;&#34989;&#24615;&#21644;&#33268;&#21629;&#30340;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#33041;&#30284;&#12290;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#30001;&#20110;&#20854;&#26080;&#21019;&#21644;&#26080;&#36752;&#23556;&#24615;&#36136;&#65292;&#22312;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#24739;&#32773;&#30340;&#35786;&#26029;&#12289;&#27835;&#30103;&#35745;&#21010;&#21644;&#38543;&#35775;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22269;&#38469;&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;&#20026;&#21033;&#29992;&#22235;&#31181;&#32467;&#26500;&#24615;MRI&#25195;&#25551;&#65288;T1&#12289;T1Gd&#12289;T2&#12289;T2-FLAIR&#65289;&#20934;&#30830;&#39640;&#25928;&#22320;&#20998;&#21106;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#20122;&#21306;&#22495;&#25552;&#20379;&#20102;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#22235;&#20010;MRI&#24207;&#21015;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21512;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#21033;&#29992;&#24320;&#28304;&#30340;GAN&#26041;&#27861;&#65292;&#20197;&#20219;&#19977;&#20010;MRI&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#29983;&#25104;&#32570;&#22833;&#30340;&#31532;&#22235;&#20010;&#32467;&#26500;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36129;&#29486;&#32473;&#20102;&#31038;&#21306;&#39537;&#21160;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;GaNDLF&#65289;&#65292;&#24182;&#22312;&#21512;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic resonance imaging (MRI) plays a significant role in the diagnosis, treatment planning, and follow-up of glioblastoma patients due to its non-invasive and radiation-free nature. The International Brain Tumor Segmentation (BraTS) challenge has contributed to generating numerous AI algorithms to accurately and efficiently segment glioblastoma sub-compartments using four structural (T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not always be available. To address this issue, Generative Adversarial Networks (GANs) can be used to synthesize the missing MRI sequences. In this paper, we implement and utilize an open-source GAN approach that takes any three MRI sequences as input to generate the missing fourth structural sequence. Our proposed approach is contributed to the community-driven generally nuanced deep learning framework (GaNDLF) and demonstrates promising results in synthesizing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Pix2Pix GANs&#23545;&#24694;&#21155;&#22825;&#27668;&#20013;&#30340;&#20154;&#32676;&#22270;&#20687;&#36827;&#34892;&#21435;&#22122;&#26469;&#25552;&#39640;&#20154;&#32676;&#35745;&#25968;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#28040;&#38500;&#22122;&#22768;&#21644;&#27169;&#31946;&#25928;&#26524;&#65292;&#24182;&#22312;&#25512;&#26029;&#24341;&#25806;&#20013;&#24212;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#26469;&#20272;&#35745;&#20154;&#32676;&#23494;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38656;&#35201;&#39640;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#26102;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07245</link><description>&lt;p&gt;
&#21033;&#29992;Pix2Pix GANs&#36827;&#34892;&#24694;&#21155;&#22825;&#27668;&#20013;&#30340;&#20154;&#32676;&#35745;&#25968;&#30340;&#22270;&#20687;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs. (arXiv:2310.07245v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Pix2Pix GANs&#23545;&#24694;&#21155;&#22825;&#27668;&#20013;&#30340;&#20154;&#32676;&#22270;&#20687;&#36827;&#34892;&#21435;&#22122;&#26469;&#25552;&#39640;&#20154;&#32676;&#35745;&#25968;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#28040;&#38500;&#22122;&#22768;&#21644;&#27169;&#31946;&#25928;&#26524;&#65292;&#24182;&#22312;&#25512;&#26029;&#24341;&#25806;&#20013;&#24212;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#26469;&#20272;&#35745;&#20154;&#32676;&#23494;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38656;&#35201;&#39640;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#26102;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20154;&#32676;&#35745;&#25968;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20272;&#35745;&#20154;&#32676;&#23494;&#24230;&#12290;&#27169;&#22411;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#32452;&#25104;&#20154;&#32676;&#22270;&#20687;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#22312;&#24694;&#21155;&#22825;&#27668;&#22914;&#38654;&#12289;&#23576;&#22303;&#21644;&#20302;&#20809;&#26465;&#20214;&#19979;&#65292;&#22122;&#22768;&#21644;&#27169;&#31946;&#22270;&#20687;&#21487;&#33021;&#20250;&#20005;&#37325;&#38477;&#20302;&#25512;&#26029;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20808;&#20351;&#29992;Pix2Pix&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#23545;&#20154;&#32676;&#22270;&#20687;&#36827;&#34892;&#21435;&#22122;&#65292;&#28982;&#21518;&#23558;&#20854;&#20256;&#36882;&#32473;&#35745;&#25968;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#20174;&#21407;&#22987;&#20154;&#32676;&#22270;&#20687;&#29983;&#25104;&#30340;&#21512;&#25104;&#22122;&#22768;&#22270;&#20687;&#35757;&#32451;Pix2Pix&#32593;&#32476;&#65292;&#28982;&#21518;&#23558;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#29992;&#20110;&#25512;&#26029;&#24341;&#25806;&#20197;&#20272;&#35745;&#19981;&#21487;&#35265;&#30340;&#22122;&#22768;&#20154;&#32676;&#22270;&#20687;&#20013;&#30340;&#20154;&#32676;&#23494;&#24230;&#12290;&#20351;&#29992;JHU-Crowd&#25968;&#25454;&#38598;&#23545;&#24615;&#33021;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#39564;&#35777;&#25152;&#25552;&#26041;&#27861;&#22312;&#39640;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#35201;&#27714;&#26102;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual crowd counting estimates the density of the crowd using deep learning models such as convolution neural networks (CNNs). The performance of the model heavily relies on the quality of the training data that constitutes crowd images. In harsh weather such as fog, dust, and low light conditions, the inference performance may severely degrade on the noisy and blur images. In this paper, we propose the use of Pix2Pix generative adversarial network (GAN) to first denoise the crowd images prior to passing them to the counting model. A Pix2Pix network is trained using synthetic noisy images generated from original crowd images and then the pretrained generator is then used in the inference engine to estimate the crowd density in unseen, noisy crowd images. The performance is tested on JHU-Crowd dataset to validate the significance of the proposed method particularly when high reliability and accuracy are required.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#32467;&#26500;&#20013;&#35010;&#32441;&#30340;&#25193;&#23637;&#65292;&#24182;&#25104;&#21151;&#22320;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20808;&#39564;&#20998;&#24067;&#29992;&#20110;&#36125;&#21494;&#26031;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.07241</link><description>&lt;p&gt;
&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#38543;&#26426;&#35010;&#32441;&#25193;&#23637;&#36807;&#31243;&#30340;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Surrogate modeling for stochastic crack growth processes in structural health monitoring applications. (arXiv:2310.07241v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#32467;&#26500;&#20013;&#35010;&#32441;&#30340;&#25193;&#23637;&#65292;&#24182;&#25104;&#21151;&#22320;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20808;&#39564;&#20998;&#24067;&#29992;&#20110;&#36125;&#21494;&#26031;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30130;&#21171;&#35010;&#32441;&#25193;&#23637;&#26159;&#37329;&#23646;&#32467;&#26500;&#20013;&#26368;&#24120;&#35265;&#30340;&#19968;&#31181;&#30772;&#22351;&#31867;&#22411;&#65292;&#23545;&#20854;&#21487;&#38752;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26368;&#36817;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#39046;&#22495;&#30340;&#36827;&#23637;&#20419;&#20351;&#20351;&#29992;&#32467;&#26500;&#21709;&#24212;&#25968;&#25454;&#26469;&#39044;&#27979;&#19981;&#30830;&#23450;&#26465;&#20214;&#19979;&#26410;&#26469;&#30340;&#35010;&#32441;&#25193;&#23637;&#65292;&#20197;&#23454;&#29616;&#21521;&#39044;&#27979;&#24615;&#32500;&#20462;&#30340;&#36807;&#28193;&#12290;&#20934;&#30830;&#22320;&#34920;&#31034;&#38543;&#26426;&#35010;&#32441;&#25193;&#23637;&#36807;&#31243;&#20013;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#22312;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#38543;&#26426;&#35010;&#32441;&#25193;&#23637;&#24314;&#27169;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#32771;&#34385;&#20102;&#26448;&#26009;&#21644;&#36733;&#33655;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#27010;&#29575;&#20195;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#32534;&#30721;&#36825;&#20123;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#37319;&#29992;&#20102;&#21463;&#28508;&#21464;&#37327;&#24314;&#27169;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#20351;&#20195;&#29702;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20026;&#19981;&#21516;&#30340;&#36125;&#21494;&#26031;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#20219;&#21153;&#29983;&#25104;&#20808;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fatigue crack growth is one of the most common types of deterioration in metal structures with significant implications on their reliability. Recent advances in Structural Health Monitoring (SHM) have motivated the use of structural response data to predict future crack growth under uncertainty, in order to enable a transition towards predictive maintenance. Accurately representing different sources of uncertainty in stochastic crack growth (SCG) processes is a non-trivial task. The present work builds on previous research on physics-based SCG modeling under both material and load-related uncertainty. The aim here is to construct computationally efficient, probabilistic surrogate models for SCG processes that successfully encode these different sources of uncertainty. An approach inspired by latent variable modeling is employed that utilizes Gaussian Process (GP) regression models to enable the surrogates to be used to generate prior distributions for different Bayesian SHM tasks as th
&lt;/p&gt;</description></item><item><title>CacheGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#21387;&#32553;&#26469;&#20943;&#23569;LLM&#30340;&#32593;&#32476;&#33719;&#21462;&#21644;&#22788;&#29702;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.07240</link><description>&lt;p&gt;
CacheGen&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#24555;&#36895;&#19978;&#19979;&#25991;&#21152;&#36733;
&lt;/p&gt;
&lt;p&gt;
CacheGen: Fast Context Loading for Language Model Applications. (arXiv:2310.07240v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07240
&lt;/p&gt;
&lt;p&gt;
CacheGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#21387;&#32553;&#26469;&#20943;&#23569;LLM&#30340;&#32593;&#32476;&#33719;&#21462;&#21644;&#22788;&#29702;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25215;&#25285;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#20854;&#36755;&#20837;&#23558;&#25972;&#21512;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#24212;&#23545;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#25110;&#29992;&#25143;&#29305;&#23450;&#30340;&#23545;&#35805;&#21382;&#21490;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#23545;&#20110;&#21709;&#24212;&#24335;&#30340;LLM&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#25152;&#26377;&#19978;&#19979;&#25991;&#34987;&#33719;&#21462;&#21644;LLM&#22788;&#29702;&#20043;&#21069;&#65292;&#26080;&#27861;&#29983;&#25104;&#20219;&#20309;&#20869;&#23481;&#12290;&#29616;&#26377;&#31995;&#32479;&#20165;&#36890;&#36807;&#20248;&#21270;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#35745;&#31639;&#24310;&#36831;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#32531;&#23384;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#20013;&#38388;&#38190;&#20540;&#29305;&#24449;&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#19978;&#19979;&#25991;&#33719;&#21462;&#30340;&#32593;&#32476;&#24310;&#36831;&#26356;&#38271;&#65288;&#20363;&#22914;&#65292;&#38190;&#20540;&#29305;&#24449;&#28040;&#32791;&#30340;&#24102;&#23485;&#27604;&#25991;&#26412;&#19978;&#19979;&#25991;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CacheGen&#65292;&#20197;&#26368;&#23567;&#21270;LLM&#19978;&#19979;&#25991;&#33719;&#21462;&#21644;&#22788;&#29702;&#30340;&#24310;&#36831;&#12290;CacheGen&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#29305;&#24449;&#21387;&#32553;&#20026;&#26356;&#32039;&#20945;&#30340;&#27604;&#29305;&#27969;&#34920;&#31034;&#65292;&#20943;&#23569;&#20102;&#20256;&#36755;&#25152;&#38656;&#30340;&#24102;&#23485;&#12290;&#32534;&#30721;&#22120;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#37327;&#21270;&#21644;......
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing (e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).  This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Graph Attention Network (GAT)&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26631;&#20934;&#21021;&#22987;&#21270;&#19979;&#30340;GAT&#20013;&#39640;&#27604;&#20363;&#30340;&#21442;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24456;&#38590;&#25913;&#21464;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;GAT&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#20351;&#24471;&#28145;&#23618;&#32593;&#32476;&#26356;&#23481;&#26131;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#27604;&#26631;&#20934;&#21021;&#22987;&#21270;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.07235</link><description>&lt;p&gt;
GATs&#26159;&#21542;&#22833;&#34913;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are GATs Out of Balance?. (arXiv:2310.07235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Graph Attention Network (GAT)&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26631;&#20934;&#21021;&#22987;&#21270;&#19979;&#30340;GAT&#20013;&#39640;&#27604;&#20363;&#30340;&#21442;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24456;&#38590;&#25913;&#21464;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;GAT&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#20351;&#24471;&#28145;&#23618;&#32593;&#32476;&#26356;&#23481;&#26131;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#27604;&#26631;&#20934;&#21021;&#22987;&#21270;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35745;&#31639;&#33021;&#21147;&#24050;&#32463;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#30340;&#20248;&#21270;&#21644;&#23398;&#20064;&#21160;&#24577;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20173;&#28982;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38024;&#23545;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;GNN&#26550;&#26500;&#65292;&#20854;&#20013;&#33410;&#28857;&#30340;&#37051;&#22495;&#32858;&#21512;&#30001;&#21442;&#25968;&#21270;&#30340;&#27880;&#24847;&#21147;&#31995;&#25968;&#21152;&#26435;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;GAT&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#36825;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26631;&#20934;&#21021;&#22987;&#21270;&#19979;&#30340;GAT&#20013;&#39640;&#27604;&#20363;&#30340;&#21442;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24456;&#38590;&#25913;&#21464;&#12290;&#36825;&#31181;&#25928;&#24212;&#22312;&#28145;&#23618;&#30340;GAT&#20013;&#34987;&#25918;&#22823;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#35201;&#26126;&#26174;&#24046;&#20110;&#27973;&#23618;&#30340;GAT&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24179;&#34913;GAT&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861; i) &#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20256;&#25773;&#26799;&#24230;&#65292;&#20174;&#32780;&#20351;&#28145;&#23618;&#32593;&#32476;&#21487;&#35757;&#32451;&#65292;ii) &#30456;&#27604;&#20110;&#26631;&#20934;&#21021;&#22987;&#21270;&#65292;&#21487;&#20197;&#23454;&#29616;&#35757;&#32451;&#21644;&#25910;&#25947;&#26102;&#38388;&#30340;&#26174;&#33879;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. O
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#20998;&#23618;&#20998;&#35299;&#65292;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#65292;&#35299;&#20915;&#20102;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#25972;&#21512;&#21644;&#39044;&#27979;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07234</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#20998;&#23618;&#20998;&#35299;&#65306;&#37325;&#26032;&#24605;&#32771;&#27169;&#31946;&#30340;&#27425;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality. (arXiv:2310.07234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#20998;&#23618;&#20998;&#35299;&#65292;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#65292;&#35299;&#20915;&#20102;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#25972;&#21512;&#21644;&#39044;&#27979;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#23398;&#20064;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#30693;&#35782;&#36827;&#34892;&#19979;&#28216;&#25345;&#32493;&#23398;&#20064;&#30340;&#26032;&#20852;&#26041;&#21521;&#65292;&#22312;&#21463;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#19979;&#20960;&#20046;&#36798;&#21040;&#20102;&#24615;&#33021;&#23792;&#20540;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26356;&#21152;&#29616;&#23454;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#65292;&#24403;&#21069;&#30340;&#31574;&#30053;&#26410;&#33021;&#21457;&#25381;&#20986;&#20854;&#20840;&#37096;&#28508;&#21147;&#65292;&#32780;&#36825;&#22312;&#23454;&#36341;&#20013;&#22788;&#29702;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#36890;&#36807;&#25552;&#31034;&#21442;&#25968;&#34987;&#32435;&#20837;&#21040;&#25351;&#31034;&#34920;&#31034;&#20013;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#30001;&#26410;&#25351;&#31034;&#30340;&#34920;&#31034;&#36827;&#34892;&#39044;&#27979;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#20013;&#23545;&#25345;&#32493;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20998;&#23618;&#32452;&#25104;&#37096;&#20998;&#65306;&#20219;&#21153;&#20869;&#39044;&#27979;&#12289;&#20219;&#21153;&#36523;&#20221;&#25512;&#26029;&#21644;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#27979;&#12290;&#26681;&#25454;&#36825;&#20123;&#23454;&#35777;&#21644;&#29702;&#35770;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hierarchical Decomposition&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical research reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Following these empirical and theoretical insights, we propose Hierarchical Decomposition 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34507;&#30333;&#36136;&#29255;&#27573;&#21644;&#21608;&#22260;&#23545;&#40784;&#30340;&#33258;&#30417;&#30563;&#21475;&#34955;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#24182;&#29983;&#25104;&#22823;&#37327;&#30340;&#37197;&#20307;-&#21463;&#20307;&#30456;&#20114;&#20316;&#29992;&#22797;&#21512;&#29289;&#12290;</title><link>http://arxiv.org/abs/2310.07229</link><description>&lt;p&gt;
&#36890;&#36807;&#34507;&#30333;&#36136;&#29255;&#27573;&#21608;&#22260;&#23545;&#40784;&#23454;&#29616;&#33258;&#30417;&#30563;&#21475;&#34955;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment. (arXiv:2310.07229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34507;&#30333;&#36136;&#29255;&#27573;&#21644;&#21608;&#22260;&#23545;&#40784;&#30340;&#33258;&#30417;&#30563;&#21475;&#34955;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#24182;&#29983;&#25104;&#22823;&#37327;&#30340;&#37197;&#20307;-&#21463;&#20307;&#30456;&#20114;&#20316;&#29992;&#22797;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#34955;&#34920;&#31034;&#22312;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22914;&#33647;&#29289;&#21487;&#35064;&#38706;&#24615;&#20272;&#35745;&#12289;&#37197;&#20307;&#20146;&#21644;&#21147;&#39044;&#27979;&#21644;&#26032;&#33647;&#35774;&#35745;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20960;&#20309;&#29305;&#24449;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36890;&#24120;&#23558;&#21475;&#34955;&#29420;&#31435;&#20110;&#37197;&#20307;&#22788;&#29702;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#22522;&#26412;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21475;&#34955;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20511;&#21161;&#39640;&#20998;&#36776;&#29575;&#21407;&#23376;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#30693;&#35782;&#65292;&#24182;&#36741;&#20197;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#23567;&#20998;&#23376;&#34920;&#31034;&#12290;&#23558;&#34507;&#30333;&#36136;&#32467;&#26500;&#20998;&#27573;&#20026;&#31867;&#20284;&#33647;&#29289;&#30340;&#29255;&#27573;&#21450;&#20854;&#30456;&#24212;&#30340;&#21475;&#34955;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#37197;&#20307;-&#21463;&#20307;&#30456;&#20114;&#20316;&#29992;&#27169;&#25311;&#65292;&#20174;&#32780;&#29983;&#25104;&#20102;&#36229;&#36807;500&#19975;&#20010;&#22797;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pocket representations play a vital role in various biomedical applications, such as druggability estimation, ligand affinity prediction, and de novo drug design. While existing geometric features and pretrained representations have demonstrated promising results, they usually treat pockets independent of ligands, neglecting the fundamental interactions between them. However, the limited pocket-ligand complex structures available in the PDB database (less than 100 thousand non-redundant pairs) hampers large-scale pretraining endeavors for interaction modeling. To address this constraint, we propose a novel pocket pretraining approach that leverages knowledge from high-resolution atomic protein structures, assisted by highly effective pretrained small molecule representations. By segmenting protein structures into drug-like fragments and their corresponding pockets, we obtain a reasonable simulation of ligand-receptor interactions, resulting in the generation of over 5 million complexes
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;MODIS&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;LULC&#31867;&#21035;&#30340;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#12290;&#36890;&#36807;&#28155;&#21152;&#22320;&#29702;&#21152;&#22320;&#24418;&#21644;&#27668;&#20505;&#36741;&#21161;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07223</link><description>&lt;p&gt;
&#29992;MODIS&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#21644;&#36741;&#21161;&#25968;&#25454;&#36827;&#34892;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data. (arXiv:2310.07223v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;MODIS&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;LULC&#31867;&#21035;&#30340;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#12290;&#36890;&#36807;&#28155;&#21152;&#22320;&#29702;&#21152;&#22320;&#24418;&#21644;&#27668;&#20505;&#36741;&#21161;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#25968;&#25454;&#20013;LULC&#31867;&#21035;&#36890;&#24120;&#23384;&#22312;&#28151;&#21512;&#24773;&#20917;&#65292;&#20809;&#35889;&#20998;&#31163;&#26159;&#19968;&#31181;&#20174;&#28151;&#21512;&#20687;&#32032;&#20013;&#25552;&#21462;&#20449;&#24687;&#21040;&#20854;&#32452;&#25104;LULC&#31867;&#22411;&#21644;&#30456;&#24212;&#20016;&#24230;&#20998;&#25968;&#30340;&#25216;&#26415;&#12290;&#20256;&#32479;&#19978;&#65292;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#35201;&#20040;&#20381;&#36182;&#20110;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#32463;&#20856;&#26041;&#27861;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#36991;&#20813;&#26174;&#24335;&#25104;&#20998;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20063;&#23601;&#26159;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#65288;BSU&#65289;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;BSU&#30740;&#31350;&#20391;&#37325;&#20110;&#21333;&#20010;&#26102;&#38388;&#27493;&#30340;&#39640;&#20809;&#35889;&#25968;&#25454;&#65292;&#28982;&#32780;&#19982;&#22810;&#20809;&#35889;&#25968;&#25454;&#30456;&#27604;&#65292;&#20854;&#33719;&#21462;&#25104;&#26412;&#20173;&#28982;&#30456;&#24403;&#39640;&#26114;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20379;&#20102;&#31532;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;DL&#27169;&#22411;&#36827;&#34892;LULC&#31867;&#21035;&#30340;BSU&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#28155;&#21152;&#22320;&#29702;&#21152;&#22320;&#24418;&#65288;geo-topographic&#65289;&#21644;&#27668;&#20505;&#36741;&#21161;&#20449;&#24687;&#26469;&#25552;&#21319;&#22522;&#20110;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20809;&#35889;&#26102;&#38388;&#36755;&#20837;&#25968;&#25454;&#19982;&#22320;&#29702;&#26102;&#24577;&#25968;&#25454;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC) types. Spectral unmixing is a technique to extract information from mixed pixels into their constituent LULC types and corresponding abundance fractions. Traditionally, solving this task has relied on either classical methods that require prior knowledge of endmembers or machine learning methods that avoid explicit endmembers calculation, also known as blind spectral unmixing (BSU). Most BSU studies based on Deep Learning (DL) focus on one time-step hyperspectral data, yet its acquisition remains quite costly compared with multispectral data. To our knowledge, here we provide the first study on BSU of LULC classes using multispectral time series data with DL models. We further boost the performance of a Long-Short Term Memory (LSTM)-based model by incorporating geographic plus topographic (geo-topographic) and climatic ancillary information. Our experiments show that combining spectral-temporal input data togeth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#23398;&#30340;&#31639;&#27861;&#27969;&#31243;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#35786;&#26029;&#36816;&#21160;&#23039;&#21183;&#38382;&#39064;&#24182;&#25552;&#20379;&#30699;&#27491;&#24314;&#35758;&#65292;&#36890;&#36807;&#23039;&#21183;&#35782;&#21035;&#12289;&#37325;&#22797;&#27425;&#25968;&#35745;&#31639;&#21644;&#21160;&#20316;&#28436;&#21464;&#36319;&#36394;&#23454;&#29616;&#12290;&#35813;&#31995;&#32479;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#36816;&#21160;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#35774;&#22791;&#22914;&#26234;&#33021;&#25163;&#26426;&#25552;&#20379;&#23454;&#26102;&#24314;&#35758;&#65292;&#20351;&#33258;&#25105;&#32451;&#20064;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#21463;&#20260;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.07221</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#23398;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#23039;&#21183;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Using Learnable Physics for Real-Time Exercise Form Recommendations. (arXiv:2310.07221v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#23398;&#30340;&#31639;&#27861;&#27969;&#31243;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#35786;&#26029;&#36816;&#21160;&#23039;&#21183;&#38382;&#39064;&#24182;&#25552;&#20379;&#30699;&#27491;&#24314;&#35758;&#65292;&#36890;&#36807;&#23039;&#21183;&#35782;&#21035;&#12289;&#37325;&#22797;&#27425;&#25968;&#35745;&#31639;&#21644;&#21160;&#20316;&#28436;&#21464;&#36319;&#36394;&#23454;&#29616;&#12290;&#35813;&#31995;&#32479;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#36816;&#21160;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#35774;&#22791;&#22914;&#26234;&#33021;&#25163;&#26426;&#25552;&#20379;&#23454;&#26102;&#24314;&#35758;&#65292;&#20351;&#33258;&#25105;&#32451;&#20064;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#21463;&#20260;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#22909;&#30340;&#23039;&#21183;&#21644;&#24418;&#24335;&#23545;&#20110;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#36816;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#21363;&#20351;&#22312;&#20581;&#36523;&#25151;&#29615;&#22659;&#19979;&#65292;&#25945;&#32451;&#21487;&#33021;&#26080;&#27861;&#21450;&#26102;&#25552;&#20379;&#21453;&#39304;&#12290;&#22240;&#27492;&#65292;&#24247;&#22797;&#30103;&#27861;&#21644;&#20581;&#36523;&#35757;&#32451;&#21487;&#20197;&#20174;&#25552;&#20379;&#23454;&#26102;&#35780;&#20272;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#27969;&#31243;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#23545;&#36816;&#21160;&#25216;&#26415;&#20013;&#30340;&#38382;&#39064;&#36827;&#34892;&#35786;&#26029;&#65292;&#24182;&#32473;&#20986;&#30699;&#27491;&#24314;&#35758;&#65292;&#20855;&#26377;&#39640;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;MediaPipe&#36827;&#34892;&#23039;&#21183;&#35782;&#21035;&#65292;&#20351;&#29992;&#23792;&#20540;&#31361;&#20986;&#26816;&#27979;&#35745;&#31639;&#37325;&#22797;&#27425;&#25968;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#27169;&#25311;&#22120;&#36319;&#36394;&#27599;&#20010;&#36816;&#21160;&#30340;&#21160;&#20316;&#28436;&#21464;&#12290;&#26681;&#25454;&#19982;&#20856;&#22411;&#23398;&#20064;&#21160;&#20316;&#30340;&#20559;&#24046;&#65292;&#22522;&#20110;&#32479;&#35745;&#23398;&#20064;&#23545;&#27979;&#35797;&#35270;&#39057;&#36827;&#34892;&#35786;&#26029;&#12290;&#35813;&#31995;&#32479;&#22312;&#20845;&#20010;&#20840;&#36523;&#21644;&#19978;&#21322;&#36523;&#36816;&#21160;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#20302;&#25104;&#26412;&#35774;&#22791;&#22914;&#26234;&#33021;&#25163;&#26426;&#25552;&#20379;&#30340;&#23454;&#26102;&#24314;&#35758;&#65292;&#36816;&#21160;&#32773;&#21487;&#20197;&#32416;&#27491;&#28508;&#22312;&#30340;&#38169;&#35823;&#65292;&#20351;&#33258;&#25105;&#32451;&#20064;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#21463;&#20260;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Good posture and form are essential for safe and productive exercising. Even in gym settings, trainers may not be readily available for feedback. Rehabilitation therapies and fitness workouts can thus benefit from recommender systems that provide real-time evaluation. In this paper, we present an algorithmic pipeline that can diagnose problems in exercise techniques and offer corrective recommendations, with high sensitivity and specificity in real-time. We use MediaPipe for pose recognition, count repetitions using peak-prominence detection, and use a learnable physics simulator to track motion evolution for each exercise. A test video is diagnosed based on deviations from the prototypical learned motion using statistical learning. The system is evaluated on six full and upper body exercises. These real-time recommendations, counseled via low-cost equipment like smartphones, will allow exercisers to rectify potential mistakes making self-practice feasible while reducing the risk of wo
&lt;/p&gt;</description></item><item><title>COPlanner&#26159;&#19968;&#20010;&#22522;&#20110;&#35268;&#21010;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20445;&#23432;&#30340;&#27169;&#22411;&#28436;&#31639;&#21644;&#20048;&#35266;&#30340;&#29615;&#22659;&#25506;&#32034;&#65292;COPlanner&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26469;&#35299;&#20915;&#21160;&#21147;&#23398;&#27169;&#22411;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.07220</link><description>&lt;p&gt;
COPlanner: &#20445;&#23432;&#30340;&#27169;&#22411;&#35268;&#21010;&#21644;&#20048;&#35266;&#30340;&#29615;&#22659;&#25506;&#32034;&#20026;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL. (arXiv:2310.07220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07220
&lt;/p&gt;
&lt;p&gt;
COPlanner&#26159;&#19968;&#20010;&#22522;&#20110;&#35268;&#21010;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20445;&#23432;&#30340;&#27169;&#22411;&#28436;&#31639;&#21644;&#20048;&#35266;&#30340;&#29615;&#22659;&#25506;&#32034;&#65292;COPlanner&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26469;&#35299;&#20915;&#21160;&#21147;&#23398;&#27169;&#22411;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dyna-style&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21253;&#21547;&#20004;&#20010;&#38454;&#27573;&#65306;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#30340;&#27169;&#22411;&#28436;&#31639;&#38454;&#27573;&#21644;&#20351;&#29992;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#30495;&#23454;&#29615;&#22659;&#25506;&#32034;&#20197;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#65292;&#38590;&#20813;&#20250;&#23398;&#20064;&#21040;&#19968;&#20010;&#20855;&#26377;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#30340;&#19981;&#23436;&#32654;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36827;&#32780;&#21487;&#33021;&#35823;&#23548;&#31574;&#30053;&#23398;&#20064;&#24182;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COPlanner&#30340;&#22522;&#20110;&#35268;&#21010;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#20934;&#30830;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#38382;&#39064;&#65292;&#20854;&#37319;&#29992;&#20445;&#23432;&#30340;&#27169;&#22411;&#28436;&#31639;&#21644;&#20048;&#35266;&#30340;&#29615;&#22659;&#25506;&#32034;&#12290;COPlanner&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#24341;&#23548;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;UP-MPC&#65289;&#32452;&#20214;&#36827;&#34892;&#22810;&#27493;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#35268;&#21010;&#12290;&#36825;&#20010;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#22312;&#27169;&#22411;&#28436;&#31639;&#26399;&#38388;&#20316;&#20026;&#24809;&#32602;&#22240;&#32032;&#65292;&#22312;&#30495;&#23454;&#29615;&#22659;&#25506;&#32034;&#26399;&#38388;&#20316;&#20026;&#22870;&#21169;&#22240;&#32032;&#65292;&#20197;&#36873;&#25321;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;COPlanner&#21487;&#20197;&#21516;&#26102;&#20445;&#35777;&#20445;&#23432;&#30340;&#27169;&#22411;&#28436;&#31639;&#21644;&#20048;&#35266;&#30340;&#29615;&#22659;&#25506;&#32034;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. In this paper, we propose $\texttt{COPlanner}$, a planning-driven framework for model-based methods to address the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. $\texttt{COPlanner}$ leverages an uncertainty-aware policy-guided model predictive control (UP-MPC) component to plan for multi-step uncertainty estimation. This estimated uncertainty then serves as a penalty during model rollouts and as a bonus during real environment exploration respectively, to choose actions. Consequently, $\texttt{COPlanner}$ 
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#22810;&#20010;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2310.07219</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Improved Membership Inference Attacks Against Language Classification Models. (arXiv:2310.07219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07219
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#22810;&#20010;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20855;&#26377;&#38646;&#21806;&#12289;&#21046;&#36896;&#12289;&#20581;&#24247;&#31561;&#35768;&#22810;&#39046;&#22495;&#30340;&#29992;&#20363;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#37319;&#29992;&#30340;&#22686;&#21152;&#65292;&#24050;&#32463;&#21457;&#29616;&#20102;&#30456;&#20851;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#23545;&#20351;&#29992;&#20854;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30340;&#20154;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#23545;&#20110;&#26159;&#21542;&#20351;&#29992;&#12289;&#37096;&#32626;&#25110;&#20849;&#20139;&#27169;&#22411;&#20570;&#20986;&#30693;&#24773;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#23545;&#27169;&#22411;&#36827;&#34892;&#19968;&#20010;&#25110;&#22810;&#20010;&#24050;&#30693;&#25915;&#20987;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#20026;&#19981;&#21516;&#25968;&#25454;&#23376;&#38598;&#29983;&#25104;&#35768;&#22810;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#37117;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence systems are prevalent in everyday life, with use cases in retail, manufacturing, health, and many other fields. With the rise in AI adoption, associated risks have been identified, including privacy risks to the people whose data was used to train models. Assessing the privacy risks of machine learning models is crucial to enabling knowledgeable decisions on whether to use, deploy, or share a model. A common approach to privacy risk assessment is to run one or more known attacks against the model and measure their success rate. We present a novel framework for running membership inference attacks against classification models. Our framework takes advantage of the ensemble method, generating many specialized attack models for different subsets of the data. We show that this approach achieves higher accuracy than either a single attack model or an attack model per class label, both on classical and language classification tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#30828;&#20214;&#32422;&#26463;&#34701;&#20837;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#25216;&#26415;&#20013;&#65292;&#23454;&#29616;&#22312;&#24494;&#22411;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#37096;&#32626;&#39640;&#25928;&#20934;&#30830;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07217</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#24494;&#22411;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#20010;&#30828;&#20214;&#32422;&#26463;&#22686;&#24378;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices. (arXiv:2310.07217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07217
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#30828;&#20214;&#32422;&#26463;&#34701;&#20837;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#25216;&#26415;&#20013;&#65292;&#23454;&#29616;&#22312;&#24494;&#22411;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#37096;&#32626;&#39640;&#25928;&#20934;&#30830;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#35745;&#31639;&#39046;&#22495;&#30340;&#36805;&#36895;&#22686;&#38271;&#21019;&#36896;&#20102;&#23545;&#33021;&#22815;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#20856;&#22411;&#30340;&#29289;&#32852;&#32593;&#32456;&#31471;&#33410;&#28857;&#26469;&#35828;&#24448;&#24448;&#36807;&#20110;&#22797;&#26434;&#19988;&#35745;&#31639;&#23494;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#35774;&#35745;&#33258;&#21160;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#20849;&#21516;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NAS&#25216;&#26415;&#38656;&#35201;&#35768;&#22810;&#36845;&#20195;&#25165;&#33021;&#20135;&#29983;&#31526;&#21512;&#29305;&#23450;&#30828;&#20214;&#32422;&#26463;&#30340;&#32593;&#32476;&#65292;&#22914;&#30828;&#20214;&#19978;&#21487;&#29992;&#30340;&#26368;&#22823;&#20869;&#23384;&#25110;&#30446;&#26631;&#24212;&#29992;&#20801;&#35768;&#30340;&#26368;&#22823;&#24310;&#36831;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#32422;&#26463;&#34701;&#20837;&#25152;&#35859;&#30340;&#21487;&#24494;&#20998;NAS&#20248;&#21270;&#26041;&#27861;&#20013;&#65292;&#20174;&#32780;&#33021;&#22815;&#19968;&#27425;&#29983;&#25104;&#31526;&#21512;&#29992;&#25143;&#23450;&#20041;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#32422;&#26463;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid proliferation of computing domains relying on Internet of Things (IoT) devices has created a pressing need for efficient and accurate deep-learning (DL) models that can run on low-power devices. However, traditional DL models tend to be too complex and computationally intensive for typical IoT end-nodes. To address this challenge, Neural Architecture Search (NAS) has emerged as a popular design automation technique for co-optimizing the accuracy and complexity of deep neural networks. Nevertheless, existing NAS techniques require many iterations to produce a network that adheres to specific hardware constraints, such as the maximum memory available on the hardware or the maximum latency allowed by the target application. In this work, we propose a novel approach to incorporate multiple constraints into so-called Differentiable NAS optimization methods, which allows the generation, in a single shot, of a model that respects user-defined constraints on both memory and latency i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#40654;&#26364;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#26500;&#24314;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07216</link><description>&lt;p&gt;
&#22312;&#27969;&#24418;&#19978;&#36890;&#36807;&#40654;&#26364;&#25193;&#25955;&#36807;&#31243;&#30340;&#28151;&#21512;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes. (arXiv:2310.07216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07216
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#40654;&#26364;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#26500;&#24314;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#24314;&#27169;&#25968;&#25454;&#30340;&#20998;&#24067;&#23545;&#20110;&#26469;&#33258;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#30340;&#35768;&#22810;&#24212;&#29992;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27969;&#24418;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#30528;&#35745;&#31639;&#22797;&#26434;&#30340;&#25955;&#24230;&#25110;&#20381;&#36182;&#20110;&#28909;&#26680;&#30340;&#36817;&#20284;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#38480;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#31616;&#21333;&#20960;&#20309;&#24418;&#29366;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#38459;&#30861;&#20102;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#40654;&#26364;&#25193;&#25955;&#28151;&#21512;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#27969;&#24418;&#19978;&#26500;&#24314;&#29983;&#25104;&#36807;&#31243;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#32452;&#20197;&#31471;&#28857;&#26465;&#20214;&#25193;&#25955;&#36807;&#31243;&#20316;&#20026;&#28151;&#21512;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20808;&#21069;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#26041;&#27861;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#29983;&#25104;&#36807;&#31243;&#30340;&#29305;&#24615;&#26159;&#23427;&#30340;&#28418;&#31227;&#23548;&#21521;&#19982;&#27969;&#24418;&#30340;&#20960;&#20309;&#24418;&#29366;&#30456;&#23545;&#24212;&#30340;&#26368;&#21487;&#33021;&#30340;&#32456;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;&#23398;&#20064;&#28151;&#21512;&#36807;&#31243;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#19968;&#33324;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#24471;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the distribution of data on Riemannian manifolds is crucial for modeling data from non-Euclidean space, which is required by many applications from diverse scientific fields. Yet, existing generative models on manifolds suffer from expensive divergence computation or rely on approximations of heat kernel. These limitations restrict their applicability to simple geometries and hinder scalability to high dimensions. In this work, we introduce the Riemannian Diffusion Mixture, a principled framework for building a generative process on manifolds as a mixture of endpoint-conditioned diffusion processes instead of relying on the denoising approach of previous diffusion models, for which the generative process is characterized by its drift guiding toward the most probable endpoint with respect to the geometry of the manifold. We further propose a simple yet efficient training objective for learning the mixture process, that is readily applicable to general manifolds. Our method outp
&lt;/p&gt;</description></item><item><title>&#27491;&#21017;&#21270;&#31574;&#30053;&#36845;&#20195;&#21644;&#29275;&#39039;-&#25289;&#24343;&#26862;&#26041;&#27861;&#22312;&#20351;&#29992;&#24378;&#20984;&#20989;&#25968;&#23545;&#36125;&#23572;&#26364;&#26041;&#31243;&#24179;&#28369;&#21270;&#30340;&#26465;&#20214;&#19979;&#20005;&#26684;&#31561;&#20215;&#65292;&#20026;&#27491;&#21017;&#21270;&#31574;&#30053;&#36845;&#20195;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#25910;&#25947;&#34892;&#20026;&#25552;&#20379;&#20102;&#32479;&#19968;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#21644;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07211</link><description>&lt;p&gt;
&#32553;&#23567;&#29275;&#39039;-&#25289;&#24343;&#26862;&#26041;&#27861;&#21644;&#27491;&#35268;&#21270;&#31574;&#30053;&#36845;&#20195;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap between Newton-Raphson Method and Regularized Policy Iteration. (arXiv:2310.07211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07211
&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#31574;&#30053;&#36845;&#20195;&#21644;&#29275;&#39039;-&#25289;&#24343;&#26862;&#26041;&#27861;&#22312;&#20351;&#29992;&#24378;&#20984;&#20989;&#25968;&#23545;&#36125;&#23572;&#26364;&#26041;&#31243;&#24179;&#28369;&#21270;&#30340;&#26465;&#20214;&#19979;&#20005;&#26684;&#31561;&#20215;&#65292;&#20026;&#27491;&#21017;&#21270;&#31574;&#30053;&#36845;&#20195;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#25910;&#25947;&#34892;&#20026;&#25552;&#20379;&#20102;&#32479;&#19968;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#21644;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#26159;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#26368;&#37325;&#35201;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26159;&#27491;&#21017;&#21270;&#31574;&#30053;&#36845;&#20195;&#30340;&#19968;&#20010;&#29305;&#20363;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#39033;&#36873;&#25321;&#20026;Shannon&#29109;&#12290;&#23613;&#31649;&#27491;&#21017;&#21270;&#31574;&#30053;&#36845;&#20195;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#24378;&#20984;&#20989;&#25968;&#23545;&#36125;&#23572;&#26364;&#26041;&#31243;&#24179;&#28369;&#21270;&#30340;&#26465;&#20214;&#19979;&#65292;&#27491;&#21017;&#21270;&#31574;&#30053;&#36845;&#20195;&#22312;&#20005;&#26684;&#24847;&#20041;&#19978;&#31561;&#20215;&#20110;&#26631;&#20934;&#30340;&#29275;&#39039;-&#25289;&#24343;&#26862;&#26041;&#27861;&#12290;&#36825;&#31181;&#31561;&#20215;&#24615;&#20026;&#27491;&#21017;&#21270;&#31574;&#30053;&#36845;&#20195;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#25910;&#25947;&#34892;&#20026;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#35777;&#26126;&#27491;&#21017;&#21270;&#31574;&#30053;&#36845;&#20195;&#20855;&#26377;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$\gamma$&#65288;&#25240;&#25187;&#22240;&#23376;&#65289;&#12290;&#27492;&#22806;&#65292;&#19968;&#26086;&#36827;&#20837;&#26368;&#20248;&#20540;&#21608;&#22260;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#35813;&#31639;&#27861;&#23558;&#20108;&#27425;&#25910;&#25947;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#27491;&#21017;&#21270;&#31574;&#30053;&#36845;&#20195;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#21363;&#26377;&#38480;&#30340;&#27491;-----------&#27492;&#22788;&#30465;&#30053;&#37096;&#20998;&#20869;&#23481;---------------
&lt;/p&gt;
&lt;p&gt;
Regularization is one of the most important techniques in reinforcement learning algorithms. The well-known soft actor-critic algorithm is a special case of regularized policy iteration where the regularizer is chosen as Shannon entropy. Despite some empirical success of regularized policy iteration, its theoretical underpinnings remain unclear. This paper proves that regularized policy iteration is strictly equivalent to the standard Newton-Raphson method in the condition of smoothing out Bellman equation with strongly convex functions. This equivalence lays the foundation of a unified analysis for both global and local convergence behaviors of regularized policy iteration. We prove that regularized policy iteration has global linear convergence with the rate being $\gamma$ (discount factor). Furthermore, this algorithm converges quadratically once it enters a local region around the optimal value. We also show that a modified version of regularized policy iteration, i.e., with finite
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24456;&#23569;&#32771;&#34385;&#22806;&#37096;&#24178;&#25200;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#31435;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#26469;&#35299;&#20915;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24178;&#25200;&#65292;&#24182;&#35777;&#26126;&#20102;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07207</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#24615;&#24178;&#25200;&#19979;&#30340;&#40065;&#26834;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Safe Reinforcement Learning under Adversarial Disturbances. (arXiv:2310.07207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24456;&#23569;&#32771;&#34385;&#22806;&#37096;&#24178;&#25200;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#31435;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#26469;&#35299;&#20915;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24178;&#25200;&#65292;&#24182;&#35777;&#26126;&#20102;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#26159;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#25511;&#21046;&#20219;&#21153;&#26102;&#30340;&#39318;&#35201;&#20851;&#27880;&#28857;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#22806;&#37096;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24456;&#23569;&#32771;&#34385;&#22806;&#37096;&#24178;&#25200;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#36866;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#24212;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24178;&#25200;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#36845;&#20195;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#40065;&#26834;&#19981;&#21464;&#38598;&#65292;&#21363;&#23433;&#20840;&#38598;&#30340;&#23376;&#38598;&#65292;&#20854;&#20013;&#21482;&#26377;&#29366;&#24577;&#22312;&#36825;&#20010;&#38598;&#21512;&#20869;&#25165;&#33021;&#23454;&#29616;&#25345;&#20037;&#23433;&#20840;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#21033;&#29992;Hamilton-Jacobi&#21487;&#36798;&#24615;&#20998;&#26512;&#20013;&#30340;&#23433;&#20840;&#20540;&#20989;&#25968;&#24314;&#31435;&#19968;&#20010;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#65292;&#20854;&#20013;&#20027;&#35282;&#65288;&#21363;&#25511;&#21046;&#36755;&#20837;&#65289;&#26088;&#22312;&#20445;&#25345;&#23433;&#20840;&#65292;&#32780;&#23545;&#25163;&#65288;&#21363;&#22806;&#37096;&#24178;&#25200;&#65289;&#35797;&#22270;&#30772;&#22351;&#23433;&#20840;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#21333;&#35843;&#25910;&#25947;&#21040;m&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is a primary concern when applying reinforcement learning to real-world control tasks, especially in the presence of external disturbances. However, existing safe reinforcement learning algorithms rarely account for external disturbances, limiting their applicability and robustness in practice. To address this challenge, this paper proposes a robust safe reinforcement learning framework that tackles worst-case disturbances. First, this paper presents a policy iteration scheme to solve for the robust invariant set, i.e., a subset of the safe set, where persistent safety is only possible for states within. The key idea is to establish a two-player zero-sum game by leveraging the safety value function in Hamilton-Jacobi reachability analysis, in which the protagonist (i.e., control inputs) aims to maintain safety and the adversary (i.e., external disturbances) tries to break down safety. This paper proves that the proposed policy iteration algorithm converges monotonically to the m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#20171;&#32461;&#26368;&#26032;&#30340;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#28085;&#30422;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#27010;&#24565;&#21644;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#24635;&#32467;&#20102;&#20010;&#20154;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#21644;&#21453;&#28436;&#31561;&#37325;&#35201;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.07204</link><description>&lt;p&gt;
&#35270;&#35273;&#35745;&#31639;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
State of the Art on Diffusion Models for Visual Computing. (arXiv:2310.07204v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07204
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#20171;&#32461;&#26368;&#26032;&#30340;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#28085;&#30422;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#27010;&#24565;&#21644;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#24635;&#32467;&#20102;&#20010;&#20154;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#21644;&#21453;&#28436;&#31561;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#23427;&#20026;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#22330;&#26223;&#30340;&#29983;&#25104;&#12289;&#32534;&#36753;&#21644;&#37325;&#24314;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#26159;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#39318;&#36873;&#26550;&#26500;&#12290;&#20165;&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#24037;&#20855;&#21644;&#24212;&#29992;&#30340;&#25991;&#29486;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#24182;&#19988;&#28041;&#21450;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#30456;&#20851;&#35770;&#25991;&#27599;&#22825;&#37117;&#22312;arXiv&#19978;&#21457;&#34920;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#20351;&#24471;&#36319;&#19978;&#25152;&#26377;&#26368;&#26032;&#21457;&#23637;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20221;&#26368;&#26032;&#25216;&#26415;&#25253;&#21578;&#65288;STAR&#65289;&#30340;&#30446;&#26631;&#26159;&#20171;&#32461;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#25968;&#23398;&#27010;&#24565;&#12289;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#29616;&#32454;&#33410;&#21644;&#35774;&#35745;&#36873;&#25321;&#65292;&#20197;&#21450;&#27010;&#36848;&#36825;&#20123;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#12289;&#21453;&#28436;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Mor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#32593;&#32476;&#25552;&#21319;&#23398;&#20064;&#25216;&#26415;&#21644;&#24341;&#20837;&#20998;&#22359;&#35757;&#32451;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;LDPC&#30721;&#35299;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35823;&#30721;&#24213;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#20102;LDPC&#30721;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07194</link><description>&lt;p&gt;
&#25552;&#21319;LDPC&#30721;&#23398;&#20064;&#20197;&#25913;&#21892;&#35823;&#30721;&#24213;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Boosting Learning for LDPC Codes to Improve the Error-Floor Performance. (arXiv:2310.07194v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#32593;&#32476;&#25552;&#21319;&#23398;&#20064;&#25216;&#26415;&#21644;&#24341;&#20837;&#20998;&#22359;&#35757;&#32451;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;LDPC&#30721;&#35299;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35823;&#30721;&#24213;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#20102;LDPC&#30721;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#32416;&#38169;&#33021;&#21147;&#21644;&#31616;&#21333;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#20302;&#23494;&#24230;&#22855;&#20598;&#26657;&#39564;&#65288;LDPC&#65289;&#30721;&#24050;&#25104;&#21151;&#21830;&#29992;&#21270;&#20110;&#36890;&#20449;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;LDPC&#30721;&#30340;&#35823;&#30721;&#24213;&#29616;&#35937;&#20351;&#24471;&#22312;&#23454;&#29616;&#26497;&#20302;&#35823;&#30721;&#29575;&#21644;&#22312;&#23545;&#36229;&#39640;&#21487;&#38752;&#24615;&#26377;&#35201;&#27714;&#30340;&#22330;&#26223;&#20013;&#24212;&#29992;LDPC&#30721;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#23545;&#35823;&#30721;&#24213;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31070;&#32463;min-sum&#65288;NMS&#65289;&#35299;&#30721;&#22120;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#32593;&#32476;&#30340;&#25552;&#21319;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#23558;&#35299;&#30721;&#32593;&#32476;&#20998;&#20026;&#20004;&#20010;&#32593;&#32476;&#65292;&#24182;&#35757;&#32451;&#21518;&#32493;&#32593;&#32476;&#19987;&#38376;&#29992;&#20110;&#23545;&#31532;&#19968;&#20010;&#32593;&#32476;&#20013;&#35299;&#30721;&#22833;&#36133;&#30340;&#26410;&#32416;&#27491;&#30721;&#23383;&#36827;&#34892;&#35782;&#21035;&#12290;&#20854;&#27425;&#65292;&#20026;&#35299;&#20915;&#35757;&#32451;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20998;&#22359;&#35757;&#32451;&#26041;&#26696;&#65292;&#23616;&#37096;&#35757;&#32451;&#19968;&#22359;&#26435;&#37325;&#65292;&#24182;&#37325;&#26032;&#35757;&#32451;&#21069;&#19968;&#22359;&#26435;&#37325;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;NMS&#35299;&#30721;&#22120;&#20998;&#37197;&#32473;&#27599;&#20010;&#26410;&#35299;&#30721;&#30340;&#30721;&#23383;&#30340;&#30456;&#20851;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35823;&#30721;&#24213;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-density parity-check (LDPC) codes have been successfully commercialized in communication systems due to their strong error correction ability and simple decoding process. However, the error-floor phenomenon of LDPC codes, in which the error rate stops decreasing rapidly at a certain level, poses challenges in achieving extremely low error rates and the application of LDPC codes in scenarios demanding ultra high reliability. In this work, we propose training methods to optimize neural min-sum (NMS) decoders that are robust to the error-floor. Firstly, by leveraging the boosting learning technique of ensemble networks, we divide the decoding network into two networks and train the post network to be specialized for uncorrected codewords that failed in the first network. Secondly, to address the vanishing gradient issue in training, we introduce a block-wise training schedule that locally trains a block of weights while retraining the preceding block. Lastly, we show that assigning di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#21644;&#23485;&#24230;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#21482;&#26377;&#28145;&#24230;&#36235;&#36817;&#26080;&#31351;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#25165;&#21487;&#33021;&#36798;&#21040;&#27604;&#29109;&#25968;&#26356;&#22909;&#30340;&#36895;&#24230;&#65292;&#32780;&#22266;&#23450;&#28145;&#24230;&#24182;&#35753;&#23485;&#24230;&#36235;&#36817;&#26080;&#31351;&#22823;&#21017;&#27809;&#26377;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.07190</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65306;&#28145;&#23618;&#12289;&#27973;&#23618;&#36824;&#26159;&#20013;&#38388;&#23618;&#65311;
&lt;/p&gt;
&lt;p&gt;
Neural networks: deep, shallow, or in between?. (arXiv:2310.07190v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#21644;&#23485;&#24230;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#21482;&#26377;&#28145;&#24230;&#36235;&#36817;&#26080;&#31351;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#25165;&#21487;&#33021;&#36798;&#21040;&#27604;&#29109;&#25968;&#26356;&#22909;&#30340;&#36895;&#24230;&#65292;&#32780;&#22266;&#23450;&#28145;&#24230;&#24182;&#35753;&#23485;&#24230;&#36235;&#36817;&#26080;&#31351;&#22823;&#21017;&#27809;&#26377;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#36890;&#36807;&#23485;&#24230;&#20026;W&#12289;&#28145;&#24230;&#20026;l&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#28385;&#36275;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;&#36755;&#20986;&#36827;&#34892;&#36817;&#20284;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#23545;&#25968;&#22240;&#23376;&#35299;&#38500;&#65292;&#20165;&#26377;&#28145;&#24230;l&#36235;&#36817;&#26080;&#31351;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#25165;&#26377;&#21487;&#33021;&#36798;&#21040;&#27604;&#29109;&#25968;&#26356;&#22909;&#30340;&#36895;&#24230;&#65292;&#32780;&#22914;&#26524;&#22266;&#23450;&#28145;&#24230;&#28982;&#21518;&#35753;&#23485;&#24230;W&#36235;&#36817;&#26080;&#31351;&#22823;&#65292;&#21017;&#27809;&#26377;&#20219;&#20309;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give estimates from below for the error of approximation of a compact subset from a Banach space by the outputs of feed-forward neural networks with width W, depth l and Lipschitz activation functions. We show that, modulo logarithmic factors, rates better that entropy numbers' rates are possibly attainable only for neural networks for which the depth l goes to infinity, and that there is no gain if we fix the depth and let the width W go to infinity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;Cox&#37096;&#20998;&#32447;&#24615;&#22238;&#24402;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#30284;&#30151;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26680;&#26426;&#22120;&#26041;&#27861;&#25551;&#36848;&#22797;&#26434;&#30340;&#29983;&#23384;&#21644;&#39044;&#27979;&#22240;&#23376;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#27491;&#21017;&#21270;&#21152;&#26435;&#26680;&#26426;&#22120;&#26041;&#27861;&#33258;&#21160;&#21435;&#38500;&#19981;&#30456;&#20851;&#30340;&#22240;&#23376;&#12290;&#19982;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07187</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;Cox&#37096;&#20998;&#32447;&#24615;&#22238;&#24402;&#65306;&#26500;&#24314;&#30284;&#30151;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Kernel Cox partially linear regression: building predictive models for cancer patients' survival. (arXiv:2310.07187v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;Cox&#37096;&#20998;&#32447;&#24615;&#22238;&#24402;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#30284;&#30151;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26680;&#26426;&#22120;&#26041;&#27861;&#25551;&#36848;&#22797;&#26434;&#30340;&#29983;&#23384;&#21644;&#39044;&#27979;&#22240;&#23376;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#27491;&#21017;&#21270;&#21152;&#26435;&#26680;&#26426;&#22120;&#26041;&#27861;&#33258;&#21160;&#21435;&#38500;&#19981;&#30456;&#20851;&#30340;&#22240;&#23376;&#12290;&#19982;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#24739;&#32773;&#30340;&#29983;&#23384;&#23384;&#22312;&#24191;&#27867;&#30340;&#24322;&#36136;&#24615;&#65292;&#20174;&#20960;&#20010;&#26376;&#21040;&#20960;&#21313;&#24180;&#19981;&#31561;&#12290;&#20026;&#20102;&#20934;&#30830;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#24314;&#31435;&#19968;&#20010;&#33021;&#22815;&#23558;&#24739;&#32773;&#30340;&#20998;&#23376;&#29305;&#24449;&#19982;&#29983;&#23384;&#24773;&#20917;&#20851;&#32852;&#36215;&#26469;&#30340;&#31934;&#30830;&#39044;&#27979;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#29983;&#23384;&#19982;&#39640;&#32500;&#20998;&#23376;&#39044;&#27979;&#22240;&#23376;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#36827;&#34892;&#38750;&#21442;&#25968;&#24314;&#27169;&#21644;&#21435;&#38500;&#19981;&#30456;&#20851;&#30340;&#39044;&#27979;&#22240;&#23376;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#26680;Cox&#27604;&#20363;&#39118;&#38505;&#21322;&#21442;&#25968;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#21152;&#26435;&#26680;&#26426;&#22120;&#65288;RegGKM&#65289;&#26041;&#27861;&#26469;&#25311;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#26680;&#26426;&#22120;&#26041;&#27861;&#25551;&#36848;&#29983;&#23384;&#21644;&#39044;&#27979;&#22240;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;LASSO&#24809;&#32602;&#33258;&#21160;&#21435;&#38500;&#19981;&#30456;&#20851;&#30340;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#39044;&#27979;&#22240;&#23376;&#12290;&#20026;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#32500;&#31639;&#27861;&#12290;&#19982;&#27169;&#25311;&#20013;&#30340;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24635;&#26159;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wide heterogeneity exists in cancer patients' survival, ranging from a few months to several decades. To accurately predict clinical outcomes, it is vital to build an accurate predictive model that relates patients' molecular profiles with patients' survival. With complex relationships between survival and high-dimensional molecular predictors, it is challenging to conduct non-parametric modeling and irrelevant predictors removing simultaneously. In this paper, we build a kernel Cox proportional hazards semi-parametric model and propose a novel regularized garrotized kernel machine (RegGKM) method to fit the model. We use the kernel machine method to describe the complex relationship between survival and predictors, while automatically removing irrelevant parametric and non-parametric predictors through a LASSO penalty. An efficient high-dimensional algorithm is developed for the proposed method. Comparison with other competing methods in simulation shows that the proposed method alway
&lt;/p&gt;</description></item><item><title>NeuroInspect&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#21487;&#35299;&#37322;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#30830;&#23450;&#32593;&#32476;&#20013;&#23548;&#33268;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#24182;&#21487;&#35270;&#21270;&#23884;&#20837;&#20854;&#20013;&#30340;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#24341;&#20837;&#20102;CLIP-Illusion&#26469;&#29983;&#25104;&#29305;&#24449;&#22270;&#20687;&#65292;&#24182;&#20197;&#31867;&#20026;&#26465;&#20214;&#26469;&#32771;&#23519;&#31070;&#32463;&#20803;&#19982;&#20915;&#31574;&#23618;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07184</link><description>&lt;p&gt;
NeuroInspect&#65306;&#36890;&#36807;&#31867;&#26465;&#20214;&#21487;&#35270;&#21270;&#23454;&#29616;&#30340;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#35843;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations. (arXiv:2310.07184v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07184
&lt;/p&gt;
&lt;p&gt;
NeuroInspect&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#21487;&#35299;&#37322;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#30830;&#23450;&#32593;&#32476;&#20013;&#23548;&#33268;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#24182;&#21487;&#35270;&#21270;&#23884;&#20837;&#20854;&#20013;&#30340;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#24341;&#20837;&#20102;CLIP-Illusion&#26469;&#29983;&#25104;&#29305;&#24449;&#22270;&#20687;&#65292;&#24182;&#20197;&#31867;&#20026;&#26465;&#20214;&#26469;&#32771;&#23519;&#31070;&#32463;&#20803;&#19982;&#20915;&#31574;&#23618;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#20986;&#38169;&#12290;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#28145;&#24230;&#23398;&#20064;&#20174;&#19994;&#32773;&#20351;&#29992;&#26377;&#25928;&#30340;&#35843;&#35797;&#24037;&#20855;&#26469;&#35299;&#37322;&#32593;&#32476;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35843;&#35797;&#26041;&#27861;&#24120;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#25110;&#35843;&#25972;&#20915;&#31574;&#36807;&#31243;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuroInspect&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#21487;&#35299;&#37322;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#65306;&#21453;&#20107;&#23454;&#35299;&#37322;&#12289;&#29305;&#24449;&#21487;&#35270;&#21270;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#21066;&#20943;&#12290;&#25105;&#20204;&#30340;&#35843;&#35797;&#26694;&#26550;&#39318;&#20808;&#30830;&#23450;&#32593;&#32476;&#20013;&#23548;&#33268;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#65292;&#28982;&#21518;&#21487;&#35270;&#21270;&#23884;&#20837;&#22312;&#36825;&#20123;&#31070;&#32463;&#20803;&#20013;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#20154;&#31867;&#35299;&#37322;&#12290;&#20026;&#20102;&#25552;&#20379;&#36825;&#20123;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CLIP-Illusion&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#20195;&#34920;&#29305;&#24449;&#30340;&#22270;&#20687;&#65292;&#24182;&#20197;&#31867;&#20026;&#26465;&#20214;&#26469;&#32771;&#23519;&#31070;&#32463;&#20803;&#19982;&#20915;&#31574;&#23618;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite deep learning (DL) has achieved remarkable progress in various domains, the DL models are still prone to making mistakes. This issue necessitates effective debugging tools for DL practitioners to interpret the decision-making process within the networks. However, existing debugging methods often demand extra data or adjustments to the decision process, limiting their applicability. To tackle this problem, we present NeuroInspect, an interpretable neuron-based debugging framework with three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. Our debugging framework first pinpoints neurons responsible for mistakes in the network and then visualizes features embedded in the neurons to be human-interpretable. To provide these explanations, we introduce CLIP-Illusion, a novel feature visualization method that generates images representing features conditioned on classes to examine the connection between neurons and the decision layer. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAM-OCTA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#21644;&#25552;&#31034;&#28857;&#29983;&#25104;&#31574;&#30053;&#65292;&#22312;OCTA&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#35757;&#32451;&#20013;&#21487;&#33021;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#35270;&#32593;&#33180;&#34880;&#31649;&#12289;&#40644;&#26001;&#26080;&#34880;&#31649;&#21306;&#12289;&#27611;&#32454;&#34880;&#31649;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#31561;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#23616;&#37096;&#34880;&#31649;&#20998;&#21106;&#21644;&#21160;&#33033;-&#38745;&#33033;&#20998;&#21106;&#31561;&#20197;&#21069;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07183</link><description>&lt;p&gt;
SAM-OCTA: &#29992;&#20110;OCTA&#22270;&#20687;&#20998;&#21106;&#30340;&#20998;&#27573;&#36741;&#21161;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation. (arXiv:2310.07183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAM-OCTA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#21644;&#25552;&#31034;&#28857;&#29983;&#25104;&#31574;&#30053;&#65292;&#22312;OCTA&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#35757;&#32451;&#20013;&#21487;&#33021;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#35270;&#32593;&#33180;&#34880;&#31649;&#12289;&#40644;&#26001;&#26080;&#34880;&#31649;&#21306;&#12289;&#27611;&#32454;&#34880;&#31649;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#31561;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#23616;&#37096;&#34880;&#31649;&#20998;&#21106;&#21644;&#21160;&#33033;-&#38745;&#33033;&#20998;&#21106;&#31561;&#20197;&#21069;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#34880;&#27969;&#21160;&#21147;&#23398;(OCTA)&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#20998;&#21106;&#29305;&#23450;&#30446;&#26631;&#26159;&#24517;&#35201;&#30340;&#25805;&#20316;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#38480;&#23450;&#26679;&#26412;&#25968;&#65288;&#22823;&#32422;&#25968;&#30334;&#20010;&#65289;&#30340;&#26377;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#36827;&#34892;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#25552;&#31034;&#28857;&#29983;&#25104;&#31574;&#30053;&#65292;&#20197;&#22788;&#29702;OCTA&#25968;&#25454;&#38598;&#19978;&#30340;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#21629;&#21517;&#20026;SAM-OCTA&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;OCTA-500&#21644;ROSE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#25110;&#25509;&#36817;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#24615;&#33021;&#25351;&#26631;&#12290;&#35814;&#32454;&#35752;&#35770;&#20102;&#25552;&#31034;&#28857;&#23545;&#35270;&#32593;&#33180;&#34880;&#31649;&#12289;&#40644;&#26001;&#26080;&#34880;&#31649;&#21306;&#12289;&#27611;&#32454;&#34880;&#31649;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#20998;&#21106;&#20219;&#21153;&#30340;&#25928;&#26524;&#21644;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;SAM-OCTA&#23454;&#29616;&#20102;&#23616;&#37096;&#34880;&#31649;&#20998;&#21106;&#21644;&#26377;&#25928;&#30340;&#21160;&#33033;-&#38745;&#33033;&#20998;&#21106;&#65292;&#36825;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 and ROSE datasets. This method achieves or approaches state-of-the-art segmentation performance metrics. The effect and applicability of prompt points are discussed in detail for the retinal vessel, foveal avascular zone, capillary, artery, and vein segmentation tasks. Furthermore, SAM-OCTA accomplishes local vessel segmentation and effective artery-vein segmentation, which was not well-solved in previous works. The cod
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07177</link><description>&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07177
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#33021;&#21147;&#24046;&#36317;&#26102;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#65288;OSD&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#20016;&#23500;&#30340;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#25345;&#32493;&#26356;&#26032;&#65288;&#22810;&#20010;&#65289;&#33609;&#31295;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#21463;&#20869;&#23384;&#38480;&#21046;&#65292;&#20856;&#22411;&#30340;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#30340;&#21097;&#20313;&#35745;&#31639;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#22312;&#32447;&#37325;&#26032;&#35757;&#32451;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#25104;&#26412;&#20445;&#25345;&#20013;&#24615;&#12290;&#30001;&#20110;LLM&#26381;&#21153;&#30340;&#26597;&#35810;&#20998;&#24067;&#30456;&#23545;&#31616;&#21333;&#65292;&#26681;&#25454;&#26597;&#35810;&#20998;&#24067;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#20351;&#33609;&#31295;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31070;&#32463;&#25490;&#24207;&#32593;&#32476;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#20855;&#26377;&#26080;&#35823;&#24046;&#19988;&#21487;&#24494;&#20998;&#30340;&#20132;&#25442;&#20989;&#25968;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#32622;&#25442;&#31561;&#21464;Transformer&#32593;&#32476;&#26469;&#25429;&#25417;&#36755;&#20837;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25490;&#24207;&#22522;&#20934;&#19978;&#34920;&#29616;&#20248;&#20110;&#25110;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.07174</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#35823;&#24046;&#30340;&#21487;&#24494;&#20998;&#20132;&#25442;&#20989;&#25968;&#30340;&#24191;&#20041;&#31070;&#32463;&#25490;&#24207;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions. (arXiv:2310.07174v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31070;&#32463;&#25490;&#24207;&#32593;&#32476;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#20855;&#26377;&#26080;&#35823;&#24046;&#19988;&#21487;&#24494;&#20998;&#30340;&#20132;&#25442;&#20989;&#25968;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#32622;&#25442;&#31561;&#21464;Transformer&#32593;&#32476;&#26469;&#25429;&#25417;&#36755;&#20837;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25490;&#24207;&#22522;&#20934;&#19978;&#34920;&#29616;&#20248;&#20110;&#25110;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#24207;&#26159;&#25152;&#26377;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#12290;&#38500;&#20102;&#20256;&#32479;&#25490;&#24207;&#31639;&#27861;&#30340;&#38382;&#39064;&#34920;&#36848;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#25490;&#24207;&#32593;&#32476;&#32771;&#34385;&#20102;&#26356;&#25277;&#35937;&#20294;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#36755;&#20837;&#65292;&#20363;&#22914;&#22810;&#20301;&#25968;&#23383;&#22270;&#20687;&#21644;&#22270;&#20687;&#29255;&#27573;&#12290;&#20026;&#20102;&#23398;&#20064;&#20174;&#39640;&#32500;&#36755;&#20837;&#21040;&#27425;&#24207;&#21464;&#37327;&#30340;&#26144;&#23556;&#65292;&#38656;&#35201;&#20445;&#35777;&#25490;&#24207;&#32593;&#32476;&#30340;&#21487;&#24494;&#20998;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#20132;&#25442;&#20989;&#25968;&#23450;&#20041;&#19968;&#20010;&#26580;&#21270;&#35823;&#24046;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26080;&#35823;&#24046;&#30340;&#20132;&#25442;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#28385;&#36275;&#38750;&#20943;&#21644;&#21487;&#24494;&#20998;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20102;&#20855;&#26377;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32622;&#25442;&#31561;&#21464;Transformer&#32593;&#32476;&#65292;&#20197;&#25429;&#25417;&#32473;&#23450;&#36755;&#20837;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#20854;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#33021;&#21147;&#12290;&#22312;&#22810;&#26679;&#30340;&#25490;&#24207;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25110;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds non-decreasing and differentiability conditions. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#35757;&#32451;&#20998;&#24067;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#19981;&#21305;&#37197;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07171</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#20998;&#24067;&#22810;&#26679;&#21270;&#23454;&#29616;&#32852;&#37030;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Federated Generalization via Information-Theoretic Distribution Diversification. (arXiv:2310.07171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07171
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#35757;&#32451;&#20998;&#24067;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#19981;&#21305;&#37197;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#22312;&#26080;&#38656;&#30452;&#25509;&#25968;&#25454;&#20849;&#20139;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#21516;&#27169;&#22411;&#35757;&#32451;&#30340;&#33021;&#21147;&#32780;&#26085;&#30410;&#31361;&#20986;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24040;&#22823;&#24046;&#24322;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25361;&#25112;&#65292;&#23545;FL&#30340;&#27867;&#21270;&#33021;&#21147;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#24403;&#24182;&#38750;&#25152;&#26377;&#23458;&#25143;&#31471;&#37117;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#24773;&#20917;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#36825;&#26159;&#30001;&#20110;&#19981;&#31283;&#23450;&#30340;&#32593;&#32476;&#36830;&#25509;&#25110;&#26377;&#38480;&#30340;&#35745;&#31639;&#33021;&#21147;&#32780;&#24120;&#35265;&#12290;&#36825;&#21487;&#33021;&#26497;&#22823;&#22320;&#22797;&#26434;&#21270;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#22823;&#37327;&#30740;&#31350;&#38598;&#20013;&#22312;&#28041;&#21450;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#24046;&#36317;&#38382;&#39064;&#19978;&#65292;&#20294;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#38750;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#21364;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;&#22810;&#35270;&#22270;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#29305;&#24449;&#19979;&#38477;&#25366;&#25496;&#21644;&#37096;&#32626;&#35270;&#22270;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#32858;&#31867;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#35270;&#22270;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#34987;&#31216;&#20026;&#8220;&#30456;&#20284;&#31354;&#38388;&#8221;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07166</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22810;&#35270;&#22270;&#23376;&#31354;&#38388;&#32858;&#31867;&#19982;&#20998;&#23618;&#29305;&#24449;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent. (arXiv:2310.07166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;&#22810;&#35270;&#22270;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#29305;&#24449;&#19979;&#38477;&#25366;&#25496;&#21644;&#37096;&#32626;&#35270;&#22270;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#32858;&#31867;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#35270;&#22270;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#34987;&#31216;&#20026;&#8220;&#30456;&#20284;&#31354;&#38388;&#8221;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#32858;&#31867;&#22240;&#20854;&#33021;&#22815;&#20174;&#22810;&#20010;&#20449;&#24687;&#28304;&#20013;&#32858;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#20844;&#20849;&#20107;&#21153;&#20013;&#26377;&#21069;&#26223;&#30340;&#28508;&#21147;&#32780;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#36817;&#26399;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38590;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;&#22312;&#35797;&#22270;&#23545;&#19981;&#21516;&#35270;&#22270;&#30340;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#26102;&#65292;&#20986;&#29616;&#20102;&#19968;&#20010;&#24120;&#35265;&#30340;&#22256;&#22659;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#29305;&#24449;&#19979;&#38477;&#25366;&#25496;&#21644;&#37096;&#32626;&#35270;&#22270;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#65288;&#31532;&#19968;&#38454;&#27573;&#65289;&#12290;&#36825;&#20010;&#28508;&#22312;&#31354;&#38388;&#39318;&#27425;&#34987;&#35270;&#20026;&#8220;&#30456;&#20284;&#31354;&#38388;&#8221;&#65292;&#22240;&#20026;&#23427;&#25581;&#31034;&#20102;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#26576;&#20123;&#30456;&#20851;&#24615;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#31867;&#21035;&#30340;&#29420;&#28909;&#32534;&#30721;&#20063;&#21487;&#20197;&#34987;&#31216;&#20026;&#32456;&#27490;&#38454;&#27573;&#30340;&#30456;&#20284;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#31639;&#27861;&#36215;&#28304;&#20110;k-means&#32858;&#31867;&#21644;&#35889;&#32858;&#31867;&#65292;&#36825;&#23548;&#33268;&#20102;&#31435;&#26041;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering has attracted growing attention owing to its capabilities of aggregating information from various sources and its promising horizons in public affairs. Up till now, many advanced approaches have been proposed in recent literature. However, there are several ongoing difficulties to be tackled. One common dilemma occurs while attempting to align the features of different views. We dig out as well as deploy the dependency amongst views through hierarchical feature descent, which leads to a common latent space( STAGE 1). This latent space, for the first time of its kind, is regarded as a 'resemblance space', as it reveals certain correlations and dependencies of different views. To be exact, the one-hot encoding of a category can also be referred to as a resemblance space in its terminal phase. Moreover, due to the intrinsic fact that most of the existing multi-view clustering algorithms stem from k-means clustering and spectral clustering, this results in cubic time 
&lt;/p&gt;</description></item><item><title>LLark&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#27169;&#24577;&#26550;&#26500;&#23454;&#29616;&#38899;&#20048;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#19978;&#21305;&#37197;&#25110;&#36229;&#20986;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#39640;&#24230;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.07160</link><description>&lt;p&gt;
LLark: &#19968;&#31181;&#29992;&#20110;&#38899;&#20048;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLark: A Multimodal Foundation Model for Music. (arXiv:2310.07160v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07160
&lt;/p&gt;
&lt;p&gt;
LLark&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#27169;&#24577;&#26550;&#26500;&#23454;&#29616;&#38899;&#20048;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#19978;&#21305;&#37197;&#25110;&#36229;&#20986;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#39640;&#24230;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#20855;&#26377;&#29420;&#29305;&#19988;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#23545;&#20110;&#19987;&#19994;&#20154;&#22763;&#21644;&#29616;&#26377;&#30340;AI&#31995;&#32479;&#26469;&#35828;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#30456;&#23545;&#20110;&#20854;&#20182;&#24418;&#24335;&#30340;&#38899;&#39057;&#21576;&#29616;&#20986;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLark&#65292;&#19968;&#31181;&#38024;&#23545;&#38899;&#20048;&#29702;&#35299;&#30340;&#25351;&#20196;&#35843;&#35856;&#22810;&#27169;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#22686;&#24378;&#22810;&#26679;&#21270;&#30340;&#24320;&#28304;&#38899;&#20048;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#25351;&#20196;&#35843;&#35856;&#26684;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#29992;&#20110;LLark&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#22312;&#23545;&#19977;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#65288;&#38899;&#20048;&#29702;&#35299;&#12289;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#65289;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38899;&#20048;&#29702;&#35299;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#19978;&#19982;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#36229;&#20986;&#65292;&#24182;&#19988;&#22312;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#20154;&#31867;&#19982;&#27169;&#22411;&#30340;&#21709;&#24212;&#26174;&#31034;&#20986;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;LLark&#23436;&#20840;&#26159;&#26681;&#25454;&#24320;&#28304;&#38899;&#20048;&#25968;&#25454;&#21644;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;TEE&#20445;&#25252;&#19979;&#35774;&#22791;&#31471;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;TSDP&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#31363;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07152</link><description>&lt;p&gt;
&#35774;&#22791;&#22806;&#27809;&#26377;&#38544;&#31169;&#65306;&#20851;&#20110;TEE&#20445;&#25252;&#19979;&#30340;&#35774;&#22791;&#31471;&#26426;&#22120;&#23398;&#20064;&#30340;&#65288;&#19981;&#65289;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML. (arXiv:2310.07152v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;TEE&#20445;&#25252;&#19979;&#35774;&#22791;&#31471;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;TSDP&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#31363;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#31471;&#26426;&#22120;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#65306;DNN&#27169;&#22411;&#21487;&#20197;&#34987;&#35774;&#22791;&#29992;&#25143;&#30333;&#30418;&#35775;&#38382;&#12290;&#22522;&#20110;&#30333;&#30418;&#20449;&#24687;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#27169;&#22411;&#31363;&#21462;&#21644;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#26469;&#20445;&#25252;&#35774;&#22791;&#31471;&#30340;DNN&#27169;&#22411;&#26088;&#22312;&#23558;&#65288;&#26131;&#20110;&#36827;&#34892;&#30340;&#65289;&#30333;&#30418;&#25915;&#20987;&#38477;&#32423;&#20026;&#65288;&#26356;&#38590;&#36827;&#34892;&#30340;&#65289;&#40657;&#30418;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#32570;&#28857;&#26159;&#22823;&#22823;&#22686;&#21152;&#20102;&#24310;&#36831;&#65288;&#39640;&#36798;50&#20493;&#65289;&#12290;&#20026;&#20102;&#21152;&#36895;&#20351;&#29992;GPU&#36827;&#34892;TEE&#20445;&#25252;&#30340;DNN&#35745;&#31639;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#20960;&#31181;&#27169;&#22411;&#20998;&#21306;&#25216;&#26415;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#34987;&#31216;&#20026;TEE&#20445;&#25252;&#30340;DNN&#20998;&#21306;&#65288;TSDP&#65289;&#65292;&#23558;DNN&#27169;&#22411;&#20998;&#20026;&#20004;&#20010;&#37096;&#20998;&#65292;&#23558;&#38544;&#31169;&#19981;&#25935;&#24863;&#30340;&#37096;&#20998;&#21368;&#36733;&#21040;GPU&#19978;&#65292;&#21516;&#26102;&#23558;&#38544;&#31169;&#25935;&#24863;&#30340;&#37096;&#20998;&#20445;&#25252;&#22312;TEE&#20869;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;TSDP&#35299;&#20915;&#26041;&#26696;&#22312;&#21508;&#31181;DNN&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#19978;&#36827;&#34892;&#20102;&#27169;&#22411;&#31363;&#21462;&#21644;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;TSDP&#35299;&#20915;&#26041;&#26696;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#31363;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device ML introduces new security challenges: DNN models become white-box accessible to device users. Based on white-box information, adversaries can conduct effective model stealing (MS) and membership inference attack (MIA). Using Trusted Execution Environments (TEEs) to shield on-device DNN models aims to downgrade (easy) white-box attacks to (harder) black-box attacks. However, one major shortcoming is the sharply increased latency (up to 50X). To accelerate TEE-shield DNN computation with GPUs, researchers proposed several model partition techniques. These solutions, referred to as TEE-Shielded DNN Partition (TSDP), partition a DNN model into two parts, offloading the privacy-insensitive part to the GPU while shielding the privacy-sensitive part within the TEE. This paper benchmarks existing TSDP solutions using both MS and MIA across a variety of DNN models, datasets, and metrics. We show important findings that existing TSDP solutions are vulnerable to privacy-stealing attack
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QFT&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;LLMs&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07147</link><description>&lt;p&gt;
QFT: &#20351;&#29992;&#21487;&#25215;&#25285;&#36164;&#28304;&#23545;LLMs&#36827;&#34892;&#37327;&#21270;&#20840;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources. (arXiv:2310.07147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07147
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QFT&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;LLMs&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#23545;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#36825;&#19968;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#29616;&#26377;&#30340;&#21162;&#21147;&#37117;&#38598;&#20013;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#19978;&#65292;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#20840;&#21442;&#25968;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QFT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;LLMs&#30340;&#37327;&#21270;&#20840;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#25439;&#23475;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#20869;&#23384;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#26032;&#39062;&#30340;&#24605;&#24819;&#65306;&#65288;i&#65289;&#25105;&#20204;&#37319;&#29992;&#39640;&#25928;&#30340;Lion&#20248;&#21270;&#22120;&#65292;&#20165;&#36319;&#36394;&#21160;&#37327;&#24182;&#20855;&#26377;&#27599;&#20010;&#21442;&#25968;&#19968;&#33268;&#30340;&#26356;&#26032;&#24133;&#24230;&#65292;&#36825;&#23545;&#20110;&#31283;&#20581;&#30340;&#37327;&#21270;&#26159;&#19968;&#31181;&#20869;&#22312;&#20248;&#21183;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#23558;&#25152;&#26377;&#27169;&#22411;&#29366;&#24577;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20197;&#25972;&#25968;&#20540;&#23384;&#20648;&#65292;&#21516;&#26102;&#25552;&#20379;&#26799;&#24230;&#27969;&#21644;&#21442;&#25968;&#26356;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pre-trained models on downstream datasets provides further significant performance gains, but this process has been challenging due to its extraordinary resource requirements. To this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately, fail to capitalize on the powerful potential of full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized Full-parameter Tuning framework for LLMs that enables memory-efficient fine-tuning without harming performance. Our framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the momentum and has consistent update magnitudes for each parameter, an inherent advantage for robust quantization; and (ii) we quantize all model states and store them as integer values, and present a gradient flow and parameter update s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32431;&#20928;&#28436;&#31034;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#36807;&#31243;&#23545;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#30340;&#28508;&#22312;&#25200;&#21160;&#36827;&#34892;&#20928;&#21270;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#20223;&#23398;&#20064;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07143</link><description>&lt;p&gt;
&#20174;&#32431;&#20928;&#28436;&#31034;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Purified Demonstration. (arXiv:2310.07143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32431;&#20928;&#28436;&#31034;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#36807;&#31243;&#23545;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#30340;&#28508;&#22312;&#25200;&#21160;&#36827;&#34892;&#20928;&#21270;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#20223;&#23398;&#20064;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20551;&#35774;&#19987;&#23478;&#28436;&#31034;&#26159;&#26368;&#20248;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#19987;&#23478;&#28436;&#31034;&#36890;&#24120;&#26159;&#19981;&#23436;&#32654;&#30340;&#65292;&#23548;&#33268;&#22312;&#26377;&#25928;&#24212;&#29992;&#27169;&#20223;&#23398;&#20064;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#30528;&#30524;&#20110;&#20248;&#21270;&#19981;&#23436;&#32654;&#28436;&#31034;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#19968;&#23450;&#27604;&#20363;&#30340;&#26368;&#20248;&#28436;&#31034;&#26469;&#20445;&#35777;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#20928;&#21270;&#28508;&#22312;&#25200;&#21160;&#24182;&#38543;&#21518;&#20174;&#32431;&#20928;&#28436;&#31034;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#21463;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#30340;&#20004;&#27493;&#20928;&#21270;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#27491;&#21521;&#25193;&#25955;&#36807;&#31243;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#22122;&#22768;&#26469;&#26377;&#25928;&#22320;&#24179;&#28369;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#30340;&#28508;&#22312;&#25200;&#21160;&#12290;&#38543;&#21518;&#65292;&#36827;&#34892;&#36870;&#21521;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning has emerged as a promising approach for addressing sequential decision-making problems, with the assumption that expert demonstrations are optimal. However, in real-world scenarios, expert demonstrations are often imperfect, leading to challenges in effectively applying imitation learning. While existing research has focused on optimizing with imperfect demonstrations, the training typically requires a certain proportion of optimal demonstrations to guarantee performance. To tackle these problems, we propose to purify the potential perturbations in imperfect demonstrations and subsequently conduct imitation learning from purified demonstrations. Motivated by the success of diffusion models, we introduce a two-step purification via the diffusion process. In the first step, we apply a forward diffusion process to effectively smooth out the potential perturbations in imperfect demonstrations by introducing additional noise. Subsequently, a reverse generative process is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411; AE-smnsMLC&#65292;&#29992;&#20110;&#35299;&#20915;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#23558;&#23646;&#24615;&#20540;&#25552;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21482;&#26377;&#23646;&#24615;&#20540;&#27880;&#37322;&#30340;&#23454;&#38469;&#22330;&#26223;&#65292;&#32780;&#26080;&#38656;&#23646;&#24615;&#20540;&#30340;&#20301;&#32622;&#20449;&#24687;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.07137</link><description>&lt;p&gt;
AE-smnsMLC&#65306;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction. (arXiv:2310.07137v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411; AE-smnsMLC&#65292;&#29992;&#20110;&#35299;&#20915;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#23558;&#23646;&#24615;&#20540;&#25552;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21482;&#26377;&#23646;&#24615;&#20540;&#27880;&#37322;&#30340;&#23454;&#38469;&#22330;&#26223;&#65292;&#32780;&#26080;&#38656;&#23646;&#24615;&#20540;&#30340;&#20301;&#32622;&#20449;&#24687;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#22312;&#30005;&#23376;&#21830;&#21153;&#31561;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22914;&#20135;&#21697;&#25628;&#32034;&#21644;&#25512;&#33616;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#38656;&#35201;&#26356;&#22810;&#27880;&#37322;&#26469;&#26631;&#27880;&#20135;&#21697;&#25991;&#26412;&#20013;&#20540;&#30340;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#27599;&#20010;&#20135;&#21697;&#21482;&#26377;&#23646;&#24615;&#20540;&#30340;&#24369;&#26631;&#27880;&#65292;&#32780;&#27809;&#26377;&#23427;&#20204;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#20351;&#29992;&#20135;&#21697;&#25991;&#26412;&#65288;&#21363;&#20135;&#21697;&#26631;&#39064;&#21644;&#25551;&#36848;&#65289;&#65292;&#32780;&#19981;&#32771;&#34385;&#32473;&#23450;&#20135;&#21697;&#30340;&#22810;&#20010;&#23646;&#24615;&#20540;&#19982;&#20854;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#36830;&#25509;&#65292;&#36825;&#21487;&#20197;&#24110;&#21161;&#23646;&#24615;&#20540;&#25552;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#65292;&#20854;&#20013;&#21482;&#26377;&#23646;&#24615;&#20540;&#30340;&#27880;&#37322;&#21487;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#65288;&#21363;&#27809;&#26377;&#23646;&#24615;&#20540;&#20301;&#32622;&#20449;&#24687;&#30340;&#27880;&#37322;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Product attribute value extraction plays an important role for many real-world applications in e-Commerce such as product search and recommendation. Previous methods treat it as a sequence labeling task that needs more annotation for position of values in the product text. This limits their application to real-world scenario in which only attribute values are weakly-annotated for each product without their position. Moreover, these methods only use product text (i.e., product title and description) and do not consider the semantic connection between the multiple attribute values of a given product and its text, which can help attribute value extraction. In this paper, we reformulate this task as a multi-label classification task that can be applied for real-world scenario in which only annotation of attribute values is available to train models (i.e., annotation of positional information of attribute values is not available). We propose a classification model with semantic matching and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#30456;&#23545;&#27979;&#35797;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#19968;&#38454;&#21644;&#20108;&#38454;&#38543;&#26426;&#20248;&#21183;&#65292;&#24182;&#20511;&#37492;&#20102;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#25968;&#23398;&#37329;&#34701;&#20013;&#24120;&#29992;&#30340;&#24179;&#22343;&#39118;&#38505;&#27169;&#22411;&#12290;&#22312;&#32473;&#23450;&#25351;&#23450;&#24230;&#37327;&#37327;&#21270;&#30340;&#38450;&#25252;&#26639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;&#21463;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#21644;&#36873;&#25321;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;"&#24230;&#37327;&#32452;&#21512;"&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#32452;&#21512;&#30340;&#38543;&#26426;&#20248;&#21183;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.07132</link><description>&lt;p&gt;
&#22312;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#32479;&#35745;&#26174;&#33879;&#24615;
&lt;/p&gt;
&lt;p&gt;
Risk Assessment and Statistical Significance in the Age of Foundation Models. (arXiv:2310.07132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#30456;&#23545;&#27979;&#35797;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#19968;&#38454;&#21644;&#20108;&#38454;&#38543;&#26426;&#20248;&#21183;&#65292;&#24182;&#20511;&#37492;&#20102;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#25968;&#23398;&#37329;&#34701;&#20013;&#24120;&#29992;&#30340;&#24179;&#22343;&#39118;&#38505;&#27169;&#22411;&#12290;&#22312;&#32473;&#23450;&#25351;&#23450;&#24230;&#37327;&#37327;&#21270;&#30340;&#38450;&#25252;&#26639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;&#21463;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#21644;&#36873;&#25321;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;"&#24230;&#37327;&#32452;&#21512;"&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#32452;&#21512;&#30340;&#38543;&#26426;&#20248;&#21183;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#31038;&#20250;&#25216;&#26415;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#22522;&#20110;&#23454;&#38469;&#38543;&#26426;&#21464;&#37327;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#38543;&#26426;&#20248;&#21183;&#30340;&#26032;&#30340;&#32479;&#35745;&#30456;&#23545;&#27979;&#35797;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20010;&#27979;&#35797;&#20013;&#30340;&#20108;&#38454;&#32479;&#35745;&#19982;&#22312;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#25968;&#23398;&#37329;&#34701;&#20013;&#24120;&#29992;&#30340;&#24179;&#22343;&#39118;&#38505;&#27169;&#22411;&#30456;&#32852;&#31995;&#65292;&#29992;&#20110;&#22312;&#36873;&#25321;&#26041;&#26696;&#26102;&#24179;&#34913;&#39118;&#38505;&#21644;&#25928;&#29992;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#27491;&#24335;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#32473;&#23450;&#30001;&#25351;&#23450;&#24230;&#37327;&#37327;&#21270;&#30340;&#38450;&#25252;&#26639;&#12290;&#21463;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#21644;&#36873;&#25321;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;"&#24230;&#37327;&#32452;&#21512;"&#65292;&#20316;&#20026;&#32858;&#21512;&#19968;&#31995;&#21015;&#24230;&#37327;&#30340;&#25163;&#27573;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#32452;&#21512;&#30340;&#38543;&#26426;&#20248;&#21183;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#22312;&#29702;&#35770;&#19978;&#30001;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#30340;&#28176;&#36817;&#20998;&#26512;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPEHF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#29616;&#26377;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.07123</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation for Human Feedback. (arXiv:2310.07123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPEHF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#29616;&#26377;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31163;&#32447;&#35757;&#32451;&#21644;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#30340;&#32553;&#23567;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#36890;&#36807;&#20165;&#20351;&#29992;&#31163;&#32447;&#36712;&#36857;&#20272;&#35745;&#30446;&#26631;&#65288;&#35780;&#20272;&#65289;&#31574;&#30053;&#30340;&#24615;&#33021;&#21644;/&#25110;&#25490;&#21517;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#21644;&#31574;&#30053;&#27979;&#35797;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#65292;&#22312;&#22312;&#32447;&#37096;&#32626;&#25104;&#26412;&#36739;&#39640;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#65288;HF&#65289;&#20449;&#21495;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;HF&#21487;&#33021;&#20250;&#21463;&#21040;&#22810;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#21482;&#26159;&#31232;&#30095;&#21487;&#29992;&#30340;&#65307;&#32780;&#19981;&#21516;&#20110;&#20195;&#29702;&#23450;&#20041;&#30340;&#29615;&#22659;&#22870;&#21169;&#65288;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#65289;&#65292;&#29615;&#22659;&#22870;&#21169;&#36890;&#24120;&#26159;&#22312;&#21442;&#25968;&#20989;&#25968;&#25110;&#20998;&#24067;&#19978;&#20915;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;HF&#20449;&#21495;&#30340;&#24615;&#36136;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;OPE&#20272;&#35745;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;HF&#30340;OPE&#26694;&#26550;&#65292;&#23427;&#37325;&#26032;&#20351;&#29992;&#29616;&#26377;&#30340;OPE&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#35780;&#20272;HF&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;
&lt;/p&gt;
&lt;p&gt;
Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and is only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we deve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;(DLMs)&#30340;&#20998;&#23618;&#32467;&#26500;&#19982;&#20154;&#33041;&#35821;&#35328;&#29702;&#35299;&#30340;&#26102;&#38388;&#21160;&#24577;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#37319;&#29992;&#30005;&#23376;&#30382;&#23618;&#22270;(ECoG)&#25968;&#25454;&#26469;&#20248;&#21270;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20026;&#28145;&#20837;&#20102;&#35299;&#20154;&#33041;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2310.07106</link><description>&lt;p&gt;
&#12298;&#20154;&#33041;&#20013;&#35821;&#35328;&#22788;&#29702;&#30340;&#26102;&#24577;&#32467;&#26500;&#19982;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#32467;&#26500;&#30456;&#31526;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models. (arXiv:2310.07106v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;(DLMs)&#30340;&#20998;&#23618;&#32467;&#26500;&#19982;&#20154;&#33041;&#35821;&#35328;&#29702;&#35299;&#30340;&#26102;&#38388;&#21160;&#24577;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#37319;&#29992;&#30005;&#23376;&#30382;&#23618;&#22270;(ECoG)&#25968;&#25454;&#26469;&#20248;&#21270;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20026;&#28145;&#20837;&#20102;&#35299;&#20154;&#33041;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;(DLMs)&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35299;&#20154;&#33041;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#30340;&#26032;&#30340;&#35745;&#31639;&#33539;&#24335;&#12290;&#19982;&#20256;&#32479;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#27169;&#22411;&#19981;&#21516;&#65292;DLMs&#20351;&#29992;&#20998;&#23618;&#30340;&#36830;&#32493;&#25968;&#20540;&#21521;&#37327;&#24207;&#21015;&#26469;&#34920;&#31034;&#21333;&#35789;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#24471;&#35832;&#22810;&#26032;&#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#65292;&#22914;&#31867;&#20154;&#29983;&#25104;&#25991;&#26412;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;DLMs&#30340;&#20998;&#23618;&#32467;&#26500;&#21487;&#33021;&#29992;&#20110;&#27169;&#25311;&#22823;&#33041;&#35821;&#35328;&#29702;&#35299;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;DLM&#23618;&#27425;&#28145;&#24230;&#19982;&#26368;&#33021;&#39044;&#27979;&#20154;&#33041;&#30340;&#23618;&#27425;&#26102;&#38388;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20043;&#25152;&#20197;&#33021;&#22815;&#22312;&#26102;&#38388;&#19978;&#35299;&#26512;&#20986;&#27599;&#20010;&#23618;&#27425;&#30340;&#20248;&#21183;&#22312;&#20110;&#25105;&#20204;&#37319;&#29992;&#20102;&#30005;&#23376;&#30382;&#36136;&#22270;(ECoG)&#25968;&#25454;&#65292;&#20854;&#26102;&#38388;&#20998;&#36776;&#29575;&#27604;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#31561;&#26080;&#25439;&#27979;&#37327;&#26041;&#27861;&#26356;&#39640;&#12290;&#25105;&#20204;&#21033;&#29992;ECoG&#20174;&#21442;&#19982;&#32773;&#21548;30&#20998;&#38047;&#21465;&#36848;&#26102;&#35760;&#24405;&#31070;&#32463;&#27963;&#21160;&#65292;&#21516;&#26102;&#23558;&#30456;&#21516;&#21465;&#36848;&#25552;&#20379;&#32473;&#39640;&#24615;&#33021;DLM(GPT2-XL)&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Language Models (DLMs) provide a novel computational paradigm for understanding the mechanisms of natural language processing in the human brain. Unlike traditional psycholinguistic models, DLMs use layered sequences of continuous numerical vectors to represent words and context, allowing a plethora of emerging applications such as human-like text generation. In this paper we show evidence that the layered hierarchy of DLMs may be used to model the temporal dynamics of language comprehension in the brain by demonstrating a strong correlation between DLM layer depth and the time at which layers are most predictive of the human brain. Our ability to temporally resolve individual layers benefits from our use of electrocorticography (ECoG) data, which has a much higher temporal resolution than noninvasive methods like fMRI. Using ECoG, we record neural activity from participants listening to a 30-minute narrative while also feeding the same narrative to a high-performing DLM (GPT2-XL)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25195;&#25551;&#38376;&#30005;&#38236;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#20108;&#32500;&#30005;&#23376;&#27668;&#20307;&#65288;2DEGs&#65289;&#32972;&#26223;&#21183;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#12289;&#32454;&#32990;&#31070;&#32463;&#32593;&#32476;&#21644;&#36827;&#21270;&#25628;&#32034;&#31639;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#36827;&#21270;&#25628;&#32034;&#31639;&#27861;&#22312;&#32570;&#38519;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#19981;&#20165;&#25512;&#21160;&#20102;&#23545;2DEGs&#30340;&#29702;&#35299;&#65292;&#36824;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#25506;&#32034;&#37327;&#23376;&#26448;&#26009;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#23545;&#37327;&#23376;&#35745;&#31639;&#21644;&#32435;&#31859;&#30005;&#23376;&#23398;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.07089</link><description>&lt;p&gt;
&#20108;&#32500;&#30005;&#23376;&#27668;&#20307;&#20013;&#32972;&#26223;&#21183;&#33021;&#20272;&#35745;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Methods for Background Potential Estimation in 2DEGs. (arXiv:2310.07089v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25195;&#25551;&#38376;&#30005;&#38236;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#20108;&#32500;&#30005;&#23376;&#27668;&#20307;&#65288;2DEGs&#65289;&#32972;&#26223;&#21183;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#12289;&#32454;&#32990;&#31070;&#32463;&#32593;&#32476;&#21644;&#36827;&#21270;&#25628;&#32034;&#31639;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#36827;&#21270;&#25628;&#32034;&#31639;&#27861;&#22312;&#32570;&#38519;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#19981;&#20165;&#25512;&#21160;&#20102;&#23545;2DEGs&#30340;&#29702;&#35299;&#65292;&#36824;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#25506;&#32034;&#37327;&#23376;&#26448;&#26009;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#23545;&#37327;&#23376;&#35745;&#31639;&#21644;&#32435;&#31859;&#30005;&#23376;&#23398;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#23376;&#25928;&#24212;&#35774;&#22791;&#21644;&#26448;&#26009;&#39046;&#22495;&#65292;&#20108;&#32500;&#30005;&#23376;&#27668;&#20307;&#65288;2DEGs&#65289;&#26159;&#19968;&#31181;&#22522;&#30784;&#32467;&#26500;&#65292;&#25215;&#35834;&#30528;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;2DEGs&#20013;&#30340;&#26434;&#36136;&#21644;&#32570;&#38519;&#23384;&#22312;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#24433;&#21709;&#36733;&#27969;&#23376;&#36801;&#31227;&#29575;&#12289;&#23548;&#30005;&#24615;&#21644;&#37327;&#23376;&#30456;&#24178;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#25195;&#25551;&#38376;&#30005;&#38236;&#65288;SGM&#65289;&#30340;&#33021;&#21147;&#65292;&#37319;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;SGM&#25968;&#25454;&#20013;&#20272;&#35745;2DEGs&#30340;&#32972;&#26223;&#21183;&#33021;&#65306;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#65292;&#32454;&#32990;&#31070;&#32463;&#32593;&#32476;&#21644;&#36827;&#21270;&#25628;&#32034;&#12290;&#23613;&#31649;&#23384;&#22312;&#25968;&#25454;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#36827;&#21270;&#25628;&#32034;&#31639;&#27861;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#32570;&#38519;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#19981;&#20165;&#25512;&#36827;&#20102;&#25105;&#20204;&#23545;2DEGs&#30340;&#29702;&#35299;&#65292;&#20063;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#25506;&#32034;&#37327;&#23376;&#26448;&#26009;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#23545;&#37327;&#23376;&#35745;&#31639;&#21644;&#32435;&#31859;&#30005;&#23376;&#23398;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of quantum-effect devices and materials, two-dimensional electron gases (2DEGs) stand as fundamental structures that promise transformative technologies. However, the presence of impurities and defects in 2DEGs poses substantial challenges, impacting carrier mobility, conductivity, and quantum coherence time. To address this, we harness the power of scanning gate microscopy (SGM) and employ three distinct machine learning techniques to estimate the background potential of 2DEGs from SGM data: image-to-image translation using generative adversarial neural networks, cellular neural network, and evolutionary search. Our findings, despite data constraints, highlight the effectiveness of an evolutionary search algorithm in this context, offering a novel approach for defect analysis. This work not only advances our understanding of 2DEGs but also underscores the potential of machine learning in probing quantum materials, with implications for quantum computing and nanoelectronic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#27010;&#29575;&#27969;&#21160;ODE&#27169;&#22411;&#25506;&#31350;&#20102;&#23494;&#24230;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#23494;&#24230;&#20272;&#35745;&#23545;&#39640;&#22797;&#26434;&#24230;&#12289;&#39640;&#20284;&#28982;&#24230;&#30340;&#25915;&#20987;&#26159;&#40065;&#26834;&#30340;&#65292;&#21516;&#26102;&#23545;&#25239;&#26679;&#26412;&#20063;&#20855;&#26377;&#35821;&#20041;&#19978;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.07084</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#27969;&#21160;ODE&#25506;&#31350;&#23494;&#24230;&#20272;&#35745;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Adversarial Robustness of Density Estimation Using the Probability Flow ODE. (arXiv:2310.07084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#27010;&#29575;&#27969;&#21160;ODE&#27169;&#22411;&#25506;&#31350;&#20102;&#23494;&#24230;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#23494;&#24230;&#20272;&#35745;&#23545;&#39640;&#22797;&#26434;&#24230;&#12289;&#39640;&#20284;&#28982;&#24230;&#30340;&#25915;&#20987;&#26159;&#40065;&#26834;&#30340;&#65292;&#21516;&#26102;&#23545;&#25239;&#26679;&#26412;&#20063;&#20855;&#26377;&#35821;&#20041;&#19978;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#20854;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#37319;&#26679;&#33021;&#21147;&#20043;&#22806;&#65292;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#21363;&#23545;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#26597;&#35810;&#26679;&#26412;&#36827;&#34892;&#26080;&#20559;&#23494;&#24230;&#20272;&#35745;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#27969;&#21160;(PF)&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#23545;&#23494;&#24230;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#26679;&#26412;&#30340;&#21387;&#32553;&#22823;&#23567;&#29992;&#20316;&#20854;&#22797;&#26434;&#24615;&#30340;&#34913;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#20845;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#23545;&#25968;&#20284;&#28982;&#26368;&#22823;&#21270;&#25915;&#20987;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#21453;&#21521;&#31215;&#20998;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#20351;&#29992;PF ODE&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#23545;&#39640;&#22797;&#26434;&#24230;&#12289;&#39640;&#20284;&#28982;&#24230;&#25915;&#20987;&#26159;&#40065;&#26834;&#30340;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23545;&#25239;&#26679;&#26412;&#26159;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#65292;&#36825;&#19982;&#40065;&#26834;&#20272;&#35745;&#22120;&#30340;&#39044;&#26399;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond their impressive sampling capabilities, score-based diffusion models offer a powerful analysis tool in the form of unbiased density estimation of a query sample under the training data distribution. In this work, we investigate the robustness of density estimation using the probability flow (PF) neural ordinary differential equation (ODE) model against gradient-based likelihood maximization attacks and the relation to sample complexity, where the compressed size of a sample is used as a measure of its complexity. We introduce and evaluate six gradient-based log-likelihood maximization attacks, including a novel reverse integration attack. Our experimental evaluations on CIFAR-10 show that density estimation using the PF ODE is robust against high-complexity, high-likelihood attacks, and that in some cases adversarial samples are semantically meaningful, as expected from a robust estimator.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#20248;&#21270;&#31639;&#27861;&#30340;&#26368;&#20339;&#21021;&#22987;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#36890;&#36807;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#23398;&#20064;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#21021;&#22987;&#21270;&#30340;&#35745;&#31639;&#24615;&#33021;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27714;&#35299;&#26102;&#38388;&#65292;&#24182;&#19988;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.07082</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#28040;&#38500;&#22522;&#20110;&#20998;&#35299;&#30340;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#20154;&#31867;&#22240;&#32032;&#65306;&#31532;&#20108;&#37096;&#20998;&#12290;&#23398;&#20064;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taking the human out of decomposition-based optimization via artificial intelligence: Part II. Learning to initialize. (arXiv:2310.07082v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#20248;&#21270;&#31639;&#27861;&#30340;&#26368;&#20339;&#21021;&#22987;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#36890;&#36807;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#23398;&#20064;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#21021;&#22987;&#21270;&#30340;&#35745;&#31639;&#24615;&#33021;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27714;&#35299;&#26102;&#38388;&#65292;&#24182;&#19988;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#20219;&#21153;&#20013;&#65292;&#32463;&#24120;&#38656;&#35201;&#35299;&#20915;&#22823;&#35268;&#27169;&#20248;&#21270;&#38382;&#39064;&#12290;&#20998;&#35299;&#27861;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#65292;&#20294;&#20854;&#23454;&#26045;&#20855;&#26377;&#22810;&#20010;&#38590;&#20197;&#37197;&#32622;&#30340;&#27493;&#39588;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20339;&#21021;&#22987;&#21270;&#65292;&#20197;&#26368;&#23567;&#21270;&#35745;&#31639;&#26102;&#38388;&#12290;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#65292;&#39044;&#27979;&#32473;&#23450;&#21021;&#22987;&#21270;&#30340;&#35745;&#31639;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#24191;&#20041;Benders&#20998;&#35299;&#30340;&#21021;&#22987;&#21270;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#38382;&#39064;&#12290;&#26367;&#20195;&#27169;&#22411;&#29992;&#20110;&#23547;&#25214;&#24212;&#22312;&#20027;&#38382;&#39064;&#20013;&#28155;&#21152;&#30340;&#21021;&#22987;&#21106;&#30340;&#26368;&#20339;&#25968;&#37327;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27714;&#35299;&#26102;&#38388;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#20943;&#23569;&#23398;&#20064;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The repeated solution of large-scale optimization problems arises frequently in process systems engineering tasks. Decomposition-based solution methods have been widely used to reduce the corresponding computational time, yet their implementation has multiple steps that are difficult to configure. We propose a machine learning approach to learn the optimal initialization of such algorithms which minimizes the computational time. Active and supervised learning is used to learn a surrogate model that predicts the computational performance for a given initialization. We apply this approach to the initialization of Generalized Benders Decomposition for the solution of mixed integer model predictive control problems. The surrogate models are used to find the optimal number of initial cuts that should be added in the master problem. The results show that the proposed approach can lead to a significant reduction in solution time, and active learning can reduce the data required for learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;BDFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#21306;&#22359;&#38142;&#36827;&#34892;&#20998;&#25955;&#24335;&#27169;&#22411;&#39564;&#35777;&#21644;&#23457;&#35745;&#65292;&#35299;&#20915;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#34394;&#20551;&#27169;&#22411;&#21644;&#25968;&#25454;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07079</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#23433;&#20840;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Secure Decentralized Learning with Blockchain. (arXiv:2310.07079v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;BDFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#21306;&#22359;&#38142;&#36827;&#34892;&#20998;&#25955;&#24335;&#27169;&#22411;&#39564;&#35777;&#21644;&#23457;&#35745;&#65292;&#35299;&#20915;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#34394;&#20551;&#27169;&#22411;&#21644;&#25968;&#25454;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#20248;&#21270;&#36890;&#20449;&#25928;&#29575;&#12290;&#20026;&#20102;&#36991;&#20813;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#21333;&#28857;&#25925;&#38556;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#65292;&#21033;&#29992;&#28857;&#23545;&#28857;&#36890;&#20449;&#36827;&#34892;&#27169;&#22411;&#32858;&#21512;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#20998;&#24067;&#24335;&#20010;&#20154;&#35774;&#22791;&#19978;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23481;&#26131;&#21463;&#21040;&#20849;&#20139;&#34394;&#20551;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#25915;&#20987;&#32773;&#30340;&#25915;&#20987;&#12290;&#22914;&#26524;&#23384;&#22312;&#19968;&#32452;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#20182;&#20204;&#21487;&#33021;&#36890;&#36807;&#36827;&#34892;&#25237;&#27602;&#25915;&#20987;&#26469;&#25439;&#23475;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;DFL&#20013;&#65292;&#23458;&#25143;&#31471;&#36890;&#24120;&#32570;&#20047;&#28608;&#21169;&#26469;&#36129;&#29486;&#20182;&#20204;&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;BDFL&#65289;&#65292;&#23427;&#21033;&#29992;&#21306;&#22359;&#38142;&#36827;&#34892;&#20998;&#25955;&#24335;&#27169;&#22411;&#39564;&#35777;&#21644;&#23457;&#35745;&#12290;BDFL&#21253;&#25324;&#19968;&#20010;&#23457;&#26680;&#22996;&#21592;&#20250;&#29992;&#20110;&#27169;&#22411;&#39564;&#35777;&#65292;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a well-known paradigm of distributed machine learning on mobile and IoT devices, which preserves data privacy and optimizes communication efficiency. To avoid the single point of failure problem in FL, decentralized federated learning (DFL) has been proposed to use peer-to-peer communication for model aggregation, which has been considered an attractive solution for machine learning tasks on distributed personal devices. However, this process is vulnerable to attackers who share false models and data. If there exists a group of malicious clients, they might harm the performance of the model by carrying out a poisoning attack. In addition, in DFL, clients often lack the incentives to contribute their computing powers to do model training. In this paper, we proposed Blockchain-based Decentralized Federated Learning (BDFL), which leverages a blockchain for decentralized model verification and auditing. BDFL includes an auditor committee for model verification, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25299;&#23637;&#20102;&#23545;COVID-19&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#35745;&#21644;&#24378;&#21270;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#21457;&#29616;&#22312;&#23567;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#19987;&#19994;&#20998;&#31867;&#22120;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#26377;&#38480;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25239;&#20869;&#23481;&#26469;&#22686;&#24378;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.07078</link><description>&lt;p&gt;
&#36890;&#36807;&#25239;&#20869;&#23481;&#37319;&#26679;&#23545;COVID-19&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#35745;&#21644;&#24378;&#21270;
&lt;/p&gt;
&lt;p&gt;
Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling. (arXiv:2310.07078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25299;&#23637;&#20102;&#23545;COVID-19&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#35745;&#21644;&#24378;&#21270;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#21457;&#29616;&#22312;&#23567;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#19987;&#19994;&#20998;&#31867;&#22120;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#26377;&#38480;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25239;&#20869;&#23481;&#26469;&#22686;&#24378;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26377;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#23427;&#35748;&#20026;&#22312;&#23567;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#39640;&#24230;&#19987;&#19994;&#30340;&#32597;&#35265;&#20869;&#23481;&#20998;&#31867;&#22120;&#36890;&#24120;&#23545;&#37326;&#22806;&#35266;&#23519;&#21040;&#30340;&#36127;&#38754;&#31867;&#21035;&#30340;&#20016;&#23500;&#24615;&#21644;&#35805;&#39064;&#22810;&#26679;&#24615;&#65288;&#31216;&#20026;&#25239;&#20869;&#23481;&#65289;&#26377;&#38480;&#26292;&#38706;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#22312;&#27979;&#35797;&#38598;&#19978;&#35266;&#23519;&#21040;&#30340;&#24378;&#22823;&#24615;&#33021;&#21487;&#33021;&#26080;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#26377;&#25928;&#36716;&#21270;&#12290;&#22312;COVID-19&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#37326;&#22806;&#23457;&#35745;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#20960;&#20010;&#37325;&#35201;&#24341;&#29992;&#30340;&#26368;&#36817;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#37326;&#22806;&#35780;&#20272;&#26102;&#23481;&#26131;&#21463;&#21040;&#25239;&#20869;&#23481;&#30340;&#25915;&#20987;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#27969;&#31243;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#36890;&#36807;&#25361;&#25112;&#24615;&#30340;&#25239;&#20869;&#23481;&#19981;&#26029;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#24378;&#21270;&#36825;&#20123;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper makes two key contributions. First, it argues that highly specialized rare content classifiers trained on small data typically have limited exposure to the richness and topical diversity of the negative class (dubbed anticontent) as observed in the wild. As a result, these classifiers' strong performance observed on the test set may not translate into real-world settings. In the context of COVID-19 misinformation detection, we conduct an in-the-wild audit of multiple datasets and demonstrate that models trained with several prominently cited recent datasets are vulnerable to anticontent when evaluated in the wild. Second, we present a novel active learning pipeline that requires zero manual annotation and iteratively augments the training data with challenging anticontent, robustifying these classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#30830;&#23450;&#26159;&#20351;&#29992;&#25972;&#20307;&#24335;&#35299;&#27861;&#36824;&#26159;&#22522;&#20110;&#20998;&#35299;&#30340;&#35299;&#27861;&#30340;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35813;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#20984;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.07068</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#22522;&#20110;&#20998;&#35299;&#30340;&#20248;&#21270;&#26041;&#27861;&#30340;&#33258;&#21160;&#21270;&#36873;&#25321;&#65306;&#31532;&#19968;&#37096;&#20998; - &#23398;&#20064;&#20309;&#26102;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Taking the human out of decomposition-based optimization via artificial intelligence: Part I. Learning when to decompose. (arXiv:2310.07068v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#30830;&#23450;&#26159;&#20351;&#29992;&#25972;&#20307;&#24335;&#35299;&#27861;&#36824;&#26159;&#22522;&#20110;&#20998;&#35299;&#30340;&#35299;&#27861;&#30340;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35813;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#20984;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#30830;&#23450;&#26159;&#20351;&#29992;&#25972;&#20307;&#24335;&#35299;&#27861;&#36824;&#26159;&#22522;&#20110;&#20998;&#35299;&#30340;&#35299;&#27861;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#20248;&#21270;&#38382;&#39064;&#34987;&#34920;&#31034;&#20026;&#19968;&#20010;&#22270;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#29305;&#24449;&#25429;&#25417;&#38382;&#39064;&#30340;&#21464;&#37327;&#21644;&#32422;&#26463;&#20043;&#38388;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#20851;&#32852;&#12290;&#22522;&#20110;&#36825;&#31181;&#34920;&#31034;&#65292;&#24314;&#31435;&#19968;&#20010;&#22270;&#20998;&#31867;&#22120;&#26469;&#30830;&#23450;&#32473;&#23450;&#38382;&#39064;&#30340;&#26368;&#20339;&#35299;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29992;&#20110;&#24320;&#21457;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#30830;&#23450;&#26159;&#21542;&#24212;&#20351;&#29992;&#20998;&#25903;&#23450;&#30028;&#27861;&#25110;&#22806;&#37096;&#36924;&#36817;&#31639;&#27861;&#26469;&#35299;&#20915;&#20984;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#23398;&#20064;&#21040;&#30340;&#20998;&#31867;&#22120;&#32435;&#20837;&#29616;&#26377;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#27714;&#35299;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a graph classification approach for automatically determining whether to use a monolithic or a decomposition-based solution method. In this approach, an optimization problem is represented as a graph that captures the structural and functional coupling among the variables and constraints of the problem via an appropriate set of features. Given this representation, a graph classifier is built to determine the best solution method for a given problem. The proposed approach is used to develop a classifier that determines whether a convex Mixed Integer Nonlinear Programming problem should be solved using branch and bound or the outer approximation algorithm. Finally, it is shown how the learned classifier can be incorporated into existing mixed integer optimization solvers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#23398;&#27169;&#22411;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#36827;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#39046;&#22495;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07062</link><description>&lt;p&gt;
&#22768;&#23398;&#27169;&#22411;&#34701;&#21512;&#29992;&#20110;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Acoustic Model Fusion for End-to-end Speech Recognition. (arXiv:2310.07062v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#23398;&#27169;&#22411;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#36827;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#39046;&#22495;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#31471;&#21040;&#31471;&#65288;E2E&#65289;ASR&#31995;&#32479;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#23558;&#20934;&#30830;&#24230;&#25552;&#21319;&#21040;&#19968;&#20010;&#26032;&#30340;&#27700;&#24179;&#12290;E2E&#31995;&#32479;&#22312;&#35757;&#32451;&#35821;&#38899;-&#25991;&#26412;&#23545;&#30340;&#21516;&#26102;&#38544;&#24335;&#22320;&#24314;&#27169;&#20102;&#25152;&#26377;&#20256;&#32479;&#30340;ASR&#32452;&#20214;&#65292;&#22914;&#22768;&#23398;&#27169;&#22411;&#65288;AM&#65289;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#12290;&#23613;&#31649;&#36825;&#31181;&#31616;&#21270;&#30340;&#31995;&#32479;&#26550;&#26500;&#65292;&#23558;&#19968;&#20010;&#19987;&#38376;&#22312;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#29420;&#31435;LM&#34701;&#21512;&#21040;E2E&#31995;&#32479;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;LM&#34701;&#21512;&#30340;&#24212;&#29992;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#28857;&#65292;&#20363;&#22914;&#26080;&#27861;&#35299;&#20915;&#20869;&#37096;AM&#22266;&#26377;&#30340;&#39046;&#22495;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#21463;&#21040;LM&#34701;&#21512;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22806;&#37096;AM&#34701;&#21512;&#21040;E2E&#31995;&#32479;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#39046;&#22495;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#26045;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#27979;&#35797;&#38598;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#35789;&#38169;&#35823;&#29575;&#19979;&#38477;&#65292;&#26368;&#39640;&#36798;14.3&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;AM&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#39069;&#22806;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning and automatic speech recognition (ASR) have enabled the end-to-end (E2E) ASR system and boosted the accuracy to a new level. The E2E systems implicitly model all conventional ASR components, such as the acoustic model (AM) and the language model (LM), in a single network trained on audio-text pairs. Despite this simpler system architecture, fusing a separate LM, trained exclusively on text corpora, into the E2E system has proven to be beneficial. However, the application of LM fusion presents certain drawbacks, such as its inability to address the domain mismatch issue inherent to the internal AM. Drawing inspiration from the concept of LM fusion, we propose the integration of an external AM into the E2E system to better address the domain mismatch. By implementing this novel approach, we have achieved a significant reduction in the word error rate, with an impressive drop of up to 14.3% across varied test sets. We also discovered that this AM fusion ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07059</link><description>&lt;p&gt;
DKEC: &#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#30005;&#23376;&#30149;&#21382;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#32463;&#24120;&#38754;&#20020;&#38271;&#23614;&#26631;&#31614;&#20998;&#24067;&#65292;&#21363;&#32597;&#35265;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#23569;&#20110;&#39057;&#32321;&#31867;&#21035;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#23618;&#27425;&#21270;&#26631;&#31614;&#32467;&#26500;&#26469;&#25214;&#21040;&#37325;&#35201;&#29305;&#24449;&#65292;&#20294;&#22823;&#22810;&#25968;&#24573;&#30053;&#20102;&#20174;&#21307;&#23398;&#25351;&#21335;&#20013;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#22686;&#24378;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#30340;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#21019;&#26032;&#28857;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22522;&#20110;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#32467;&#21512;&#24322;&#26500;&#22270;&#21644;&#39046;&#22495;&#26412;&#20307;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#65288;2&#65289;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#30456;&#20284;&#24615;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DKEC&#65306;RAA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#24613;&#25937;&#26381;&#21153;&#65288;EMS&#65289;&#20107;&#20214;&#30340;4,417&#20010;&#24739;&#32773;&#25252;&#29702;&#25253;&#21578;&#30340;&#25910;&#38598;&#65292;&#21644;&#26469;&#33258;53898&#25253;&#21578;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the 
&lt;/p&gt;</description></item><item><title>FedMFS&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#35299;&#20915;&#20102;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07048</link><description>&lt;p&gt;
FedMFS: &#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#30340;&#32852;&#37030;&#22810;&#27169;&#24577;&#34701;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication. (arXiv:2310.07048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07048
&lt;/p&gt;
&lt;p&gt;
FedMFS&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#35299;&#20915;&#20102;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#20165;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#32780;&#19981;&#35775;&#38382;&#12289;&#20405;&#29359;&#25110;&#27844;&#38706;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#20351;&#23458;&#25143;&#33021;&#22815;&#21512;&#20316;&#12290;&#22312;&#29289;&#32852;&#32593;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#32452;&#21512;&#21644;&#34701;&#21512;&#33539;&#24335;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;&#19968;&#65289;&#35299;&#20915;&#30001;&#20110;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#24341;&#36215;&#30340;&#38382;&#39064;&#65307;&#65288;&#20108;&#65289;&#35774;&#35745;&#19968;&#31181;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#21516;&#26102;&#26368;&#22823;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;FedMFS&#65292;&#21487;&#20197;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;Shapley&#20540;&#26469;&#37327;&#21270;&#27599;&#20010;&#27169;&#24577;&#30340;&#36129;&#29486;&#21644;&#27169;&#24577;&#27169;&#22411;&#22823;&#23567;&#26469;&#34913;&#37327;&#36890;&#20449;&#24320;&#38144;&#65292;&#20197;&#20415;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning (ML) paradigm that enables clients to collaborate without accessing, infringing upon, or leaking original user data by sharing only model parameters. In the Internet of Things (IoT), edge devices are increasingly leveraging multimodal data compositions and fusion paradigms to enhance model performance. However, in FL applications, two main challenges remain open: (i) addressing the issues caused by heterogeneous clients lacking specific modalities and (ii) devising an optimal modality upload strategy to minimize communication overhead while maximizing learning performance. In this paper, we propose Federated Multimodal Fusion learning with Selective modality communication (FedMFS), a new multimodal fusion FL methodology that can tackle the above mentioned challenges. The key idea is to utilize Shapley values to quantify each modality's contribution and modality model size to gauge communication overhead, so that each client can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#28070;&#39537;&#21160;&#27969;&#22833;&#38450;&#27490;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20010;&#20307;&#39038;&#23458;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#65288;CLV&#65289;&#26469;&#23450;&#20301;&#26368;&#26377;&#20215;&#20540;&#30340;&#39038;&#23458;&#65292;&#24182;&#37319;&#29992;&#39044;&#27979;&#21644;&#20248;&#21270;&#26694;&#26550;&#36827;&#34892;&#35299;&#20915;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;12&#20010;&#27969;&#22833;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24179;&#22343;&#21033;&#28070;&#32489;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.07047</link><description>&lt;p&gt;
&#21033;&#28070;&#39537;&#21160;&#27969;&#22833;&#38450;&#27490;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A predict-and-optimize approach to profit-driven churn prevention. (arXiv:2310.07047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#28070;&#39537;&#21160;&#27969;&#22833;&#38450;&#27490;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20010;&#20307;&#39038;&#23458;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#65288;CLV&#65289;&#26469;&#23450;&#20301;&#26368;&#26377;&#20215;&#20540;&#30340;&#39038;&#23458;&#65292;&#24182;&#37319;&#29992;&#39044;&#27979;&#21644;&#20248;&#21270;&#26694;&#26550;&#36827;&#34892;&#35299;&#20915;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;12&#20010;&#27969;&#22833;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24179;&#22343;&#21033;&#28070;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#28070;&#39537;&#21160;&#27969;&#22833;&#38450;&#27490;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#23450;&#20301;&#39038;&#23458;&#29992;&#20110;&#30041;&#23384;&#27963;&#21160;&#30340;&#20219;&#21153;&#26500;&#24314;&#20026;&#19968;&#20010;&#21518;&#24724;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;&#20010;&#20307;&#39038;&#23458;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#65288;CLV&#65289;&#30830;&#20445;&#21482;&#26377;&#26368;&#26377;&#20215;&#20540;&#30340;&#39038;&#23458;&#34987;&#23450;&#20301;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#35768;&#22810;&#21033;&#28070;&#39537;&#21160;&#31574;&#30053;&#20851;&#27880;&#27969;&#22833;&#27010;&#29575;&#21516;&#26102;&#32771;&#34385;&#24179;&#22343;CLV&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#24102;&#26469;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#31526;&#21512;&#39044;&#27979;&#21644;&#20248;&#21270;&#65288;PnO&#65289;&#26694;&#26550;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#39640;&#25928;&#35299;&#20915;&#12290;&#26469;&#33258;12&#20010;&#27969;&#22833;&#39044;&#27979;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#24179;&#22343;&#21033;&#28070;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel predict-and-optimize method for profit-driven churn prevention. We frame the task of targeting customers for a retention campaign as a regret minimization problem. The main objective is to leverage individual customer lifetime values (CLVs) to ensure that only the most valuable customers are targeted. In contrast, many profit-driven strategies focus on churn probabilities while considering average CLVs. This often results in significant information loss due to data aggregation. Our proposed model aligns with the guidelines of Predict-and-Optimize (PnO) frameworks and can be efficiently solved using stochastic gradient descent methods. Results from 12 churn prediction datasets underscore the effectiveness of our approach, which achieves the best average performance compared to other well-established strategies in terms of average profit.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#35757;&#32451;&#33258;&#30417;&#30563;&#22522;&#30784;&#27169;&#22411;&#24182;&#22312;&#22823;&#22411;&#20020;&#24202;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#26088;&#22312;&#35757;&#32451;&#26368;&#22823;&#30340;&#23398;&#26415;&#22522;&#30784;&#27169;&#22411;&#24182;&#35780;&#20272;&#26368;&#26174;&#33879;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07033</link><description>&lt;p&gt;
&#20581;&#24247;&#31995;&#32479;&#35268;&#27169;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#8212;&#8212;&#26469;&#33258;30&#20159;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images. (arXiv:2310.07033v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07033
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#35757;&#32451;&#33258;&#30417;&#30563;&#22522;&#30784;&#27169;&#22411;&#24182;&#22312;&#22823;&#22411;&#20020;&#24202;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#26088;&#22312;&#35757;&#32451;&#26368;&#22823;&#30340;&#23398;&#26415;&#22522;&#30784;&#27169;&#22411;&#24182;&#35780;&#20272;&#26368;&#26174;&#33879;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31361;&#30772;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#32780;&#21487;&#20197;&#25512;&#24191;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#34429;&#28982;&#36825;&#31181;&#35757;&#32451;&#33539;&#24335;&#38750;&#24120;&#36866;&#21512;&#21307;&#23398;&#39046;&#22495;&#65292;&#22240;&#20026;&#26631;&#27880;&#24448;&#24448;&#31232;&#32570;&#65292;&#20294;&#26159;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#30149;&#29702;&#23398;&#39046;&#22495;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#20197;&#24448;&#30340;&#30149;&#29702;&#23398;&#33258;&#30417;&#30563;&#23398;&#20064;&#24037;&#20316;&#21033;&#29992;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#35780;&#20272;&#19979;&#28216;&#24615;&#33021;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#26368;&#22823;&#30340;&#23398;&#26415;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#20020;&#24202;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#35780;&#20272;&#19979;&#28216;&#24615;&#33021;&#26469;&#35780;&#20272;&#26368;&#26174;&#33879;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#36229;&#36807;423,000&#20010;&#26174;&#24494;&#38236;&#24187;&#28783;&#29255;&#30340;30&#20159;&#24352;&#22270;&#20687;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#21644;D&#26469;&#39044;&#35757;&#32451;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in self-supervised learning have enabled the use of large unlabeled datasets to train visual foundation models that can generalize to a variety of downstream tasks. While this training paradigm is well suited for the medical domain where annotations are scarce, large-scale pre-training in the medical domain, and in particular pathology, has not been extensively studied. Previous work in self-supervised learning in pathology has leveraged smaller datasets for both pre-training and evaluating downstream performance. The aim of this project is to train the largest academic foundation model and benchmark the most prominent self-supervised learning algorithms by pre-training and evaluating downstream performance on large clinical pathology datasets. We collected the largest pathology dataset to date, consisting of over 3 billion images from over 423 thousand microscopy slides. We compared pre-training of visual transformer models using the masked autoencoder (MAE) and D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#32467;&#26500;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#35782;&#21035;&#65292;&#24182;&#24212;&#29992;&#20110;&#38899;&#39057;&#22788;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#35856;&#27874;&#20998;&#26512;&#21644;&#36882;&#24402;&#24314;&#27169;&#26041;&#24335;&#65292;&#35813;&#27169;&#22411;&#22312;&#26102;&#38388;&#39057;&#29575;&#22495;&#20013;&#23545;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#39057;&#29575;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#26102;&#22495;&#21644;&#39057;&#35889;&#20998;&#36776;&#29575;&#65292;&#33021;&#22815;&#24555;&#36895;&#12289;&#31283;&#20581;&#12289;&#31934;&#30830;&#22320;&#36827;&#34892;&#20108;&#38454;&#20248;&#21270;&#65292;&#26080;&#38656;&#26174;&#24335;&#35745;&#31639;&#28023;&#26862;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2310.07032</link><description>&lt;p&gt;
&#31070;&#32463;&#35856;&#27874;&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#32467;&#26500;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#35782;&#21035;&#21450;&#20854;&#22312;&#38899;&#39057;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Neural Harmonium: An Interpretable Deep Structure for Nonlinear Dynamic System Identification with Application to Audio Processing. (arXiv:2310.07032v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#32467;&#26500;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#35782;&#21035;&#65292;&#24182;&#24212;&#29992;&#20110;&#38899;&#39057;&#22788;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#35856;&#27874;&#20998;&#26512;&#21644;&#36882;&#24402;&#24314;&#27169;&#26041;&#24335;&#65292;&#35813;&#27169;&#22411;&#22312;&#26102;&#38388;&#39057;&#29575;&#22495;&#20013;&#23545;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#39057;&#29575;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#26102;&#22495;&#21644;&#39057;&#35889;&#20998;&#36776;&#29575;&#65292;&#33021;&#22815;&#24555;&#36895;&#12289;&#31283;&#20581;&#12289;&#31934;&#30830;&#22320;&#36827;&#34892;&#20108;&#38454;&#20248;&#21270;&#65292;&#26080;&#38656;&#26174;&#24335;&#35745;&#31639;&#28023;&#26862;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#12290;&#21487;&#35299;&#37322;&#24615;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#25581;&#31034;&#20854;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22240;&#26524;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#32467;&#26500;&#65292;&#29992;&#20110;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#35856;&#27874;&#20998;&#26512;&#65292;&#22312;&#26102;&#38388;&#39057;&#29575;&#22495;&#20013;&#23545;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#26102;&#22495;&#21644;&#39057;&#35889;&#20998;&#36776;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#20197;&#36882;&#24402;&#26041;&#24335;&#26500;&#24314;&#65292;&#20801;&#35768;&#24555;&#36895;&#12289;&#31283;&#20581;&#21644;&#31934;&#30830;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#26080;&#38656;&#26174;&#24335;&#35745;&#31639;&#28023;&#26862;&#30697;&#38453;&#12290;&#20026;&#20102;&#20811;&#26381;&#31995;&#32479;&#26500;&#24314;&#27169;&#22359;&#23548;&#33268;&#30340;&#39640;&#32500;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#39057;&#29575;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#21644;&#39564;&#35777;&#65292;&#20197;&#28385;&#36275;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the interpretability of deep neural networks has recently gained increased attention, especially when the power of deep learning is leveraged to solve problems in physics. Interpretability helps us understand a model's ability to generalize and reveal its limitations. In this paper, we introduce a causal interpretable deep structure for modeling dynamic systems. Our proposed model makes use of the harmonic analysis by modeling the system in a time-frequency domain while maintaining high temporal and spectral resolution. Moreover, the model is built in an order recursive manner which allows for fast, robust, and exact second order optimization without the need for an explicit Hessian calculation. To circumvent the resulting high dimensionality of the building blocks of our system, a neural network is designed to identify the frequency interdependencies. The proposed model is illustrated and validated on nonlinear system identification problems as required for audio signal proc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20020;&#24202;&#33041;&#30005;&#22270;&#30340;&#30740;&#31350;&#26469;&#25506;&#35752;&#22823;&#33041;&#30340;&#29983;&#29289;&#24180;&#40836;&#21160;&#24577;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#29366;&#24577;&#21644;&#29305;&#36136;&#20551;&#35828;&#20197;&#21450;&#33041;&#30149;&#21464;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.07029</link><description>&lt;p&gt;
&#22823;&#33041;&#24180;&#40836;&#20877;&#35775;&#65306;&#29992;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#22823;&#33041;&#24180;&#40836;&#21160;&#24577;&#30340;&#29366;&#24577;&#20551;&#35828;&#19982;&#29305;&#36136;&#20551;&#35828;
&lt;/p&gt;
&lt;p&gt;
Brain Age Revisited: Investigating the State vs. Trait Hypotheses of EEG-derived Brain-Age Dynamics with Deep Learning. (arXiv:2310.07029v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07029
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20020;&#24202;&#33041;&#30005;&#22270;&#30340;&#30740;&#31350;&#26469;&#25506;&#35752;&#22823;&#33041;&#30340;&#29983;&#29289;&#24180;&#40836;&#21160;&#24577;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#29366;&#24577;&#21644;&#29305;&#36136;&#20551;&#35828;&#20197;&#21450;&#33041;&#30149;&#21464;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30340;&#29983;&#29289;&#24180;&#40836;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#26377;&#28508;&#21147;&#30340;&#31070;&#32463;&#23398;&#26174;&#33879;&#29983;&#29289;&#26631;&#35760;&#29289;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#32437;&#21521;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#30340;&#26368;&#26032;&#32467;&#26524;&#24341;&#21457;&#20102;&#23545;&#20854;&#35299;&#37322;&#30340;&#36136;&#30097;&#12290;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22823;&#33041;&#30340;&#29983;&#29289;&#24180;&#40836;&#26159;&#21542;&#25351;&#31034;&#20102;&#33041;&#30149;&#21464;&#65292;&#24182;&#19988;&#22823;&#33041;&#24180;&#40836;&#30340;&#21464;&#21270;&#26159;&#21542;&#19982;&#35786;&#26029;&#20986;&#30340;&#30149;&#29702;&#30456;&#20851;&#65288;&#29366;&#24577;&#20551;&#35828;&#65289;&#12290;&#25110;&#32773;&#65292;&#22823;&#33041;&#24180;&#40836;&#30340;&#24046;&#24322;&#26159;&#21542;&#26159;&#27599;&#20010;&#20010;&#20307;&#29420;&#29305;&#30340;&#31283;&#23450;&#29305;&#24449;&#65288;&#29305;&#36136;&#20551;&#35828;&#65289;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#20020;&#24202;&#33041;&#30005;&#22270;&#30340;&#22823;&#33041;&#34928;&#32769;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#23545;&#20043;&#21069;&#22522;&#20110;&#26680;&#30913;&#20849;&#25391;&#25104;&#20687;&#30340;&#35843;&#26597;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#26102;&#24207;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#36827;&#34892;&#24180;&#40836;&#22238;&#24402;&#20219;&#21153;&#12290;&#25105;&#20204;&#29992;&#34987;&#26126;&#30830;&#26631;&#35760;&#20026;&#38750;&#30149;&#29702;&#30340;Temple&#22823;&#23398;&#21307;&#38498;&#33041;&#30005;&#22270;&#35821;&#26009;&#24211;&#65288;TUEG&#65289;&#35760;&#24405;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#38750;&#30149;&#29702;&#21644;&#30149;&#29702;&#20027;&#39064;&#30340;&#35760;&#24405;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain's biological age has been considered as a promising candidate for a neurologically significant biomarker. However, recent results based on longitudinal magnetic resonance imaging data have raised questions on its interpretation. A central question is whether an increased biological age of the brain is indicative of brain pathology and if changes in brain age correlate with diagnosed pathology (state hypothesis). Alternatively, could the discrepancy in brain age be a stable characteristic unique to each individual (trait hypothesis)? To address this question, we present a comprehensive study on brain aging based on clinical EEG, which is complementary to previous MRI-based investigations. We apply a state-of-the-art Temporal Convolutional Network (TCN) to the task of age regression. We train on recordings of the Temple University Hospital EEG Corpus (TUEG) explicitly labeled as non-pathological and evaluate on recordings of subjects with non-pathological as well as pathologica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;Deepfake&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#32972;&#26223;&#22122;&#22768;&#21644;&#23398;&#20064;&#21306;&#20998;&#24615;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07028</link><description>&lt;p&gt;
&#20351;&#29992;&#32454;&#31890;&#24230;&#29305;&#24449;&#36827;&#34892;&#38754;&#37096;&#20266;&#36896;&#30340;Deepfake&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Facial Forgery-based Deepfake Detection using Fine-Grained Features. (arXiv:2310.07028v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;Deepfake&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#32972;&#26223;&#22122;&#22768;&#21644;&#23398;&#20064;&#21306;&#20998;&#24615;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Deepfake&#30340;&#38754;&#37096;&#20266;&#36896;&#24050;&#32463;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#24182;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#31038;&#20250;&#20851;&#20999;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;Deepfake&#26816;&#27979;&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#22810;&#25968;&#26041;&#27861;&#23558;Deepfake&#26816;&#27979;&#24314;&#27169;&#20026;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#39592;&#24178;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26550;&#26500;&#36827;&#34892;&#20108;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#22312;Deepfake&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#38750;&#24120;&#39640;&#30340;&#25928;&#26524;&#65292;&#26354;&#32447;&#19979;&#38754;&#31215;(AUC)&#39640;&#36798;0.99&#12290;&#28982;&#32780;&#65292;&#24403;&#34987;&#24212;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;Deepfake&#25805;&#32437;&#25216;&#26415;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#23398;&#20064;&#26356;&#21152;&#24494;&#22937;&#12289;&#23616;&#37096;&#21644;&#26377;&#36776;&#21035;&#21147;&#30340;&#29305;&#24449;&#26469;&#36827;&#34892;Deepfake&#26816;&#27979;&#12290;&#26412;&#25991;&#23558;Deepfake&#26816;&#27979;&#38382;&#39064;&#35270;&#20026;&#32454;&#31890;&#24230;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32454;&#31890;&#24230;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26377;&#25928;&#22320;&#25233;&#21046;&#32972;&#26223;&#22122;&#22768;&#21644;&#23398;&#20064;&#21306;&#20998;&#24615;&#29305;&#24449;&#26469;&#23398;&#20064;&#24494;&#22937;&#19988;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Facial forgery by deepfakes has caused major security risks and raised severe societal concerns. As a countermeasure, a number of deepfake detection methods have been proposed. Most of them model deepfake detection as a binary classification problem using a backbone convolutional neural network (CNN) architecture pretrained for the task. These CNN-based methods have demonstrated very high efficacy in deepfake detection with the Area under the Curve (AUC) as high as $0.99$. However, the performance of these methods degrades significantly when evaluated across datasets and deepfake manipulation techniques. This draws our attention towards learning more subtle, local, and discriminative features for deepfake detection. In this paper, we formulate deepfake detection as a fine-grained classification problem and propose a new fine-grained solution to it. Specifically, our method is based on learning subtle and generalizable features by effectively suppressing background noise and learning di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#22312;&#21307;&#23398;VLP&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#29992;&#30495;&#23454;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#26367;&#25442;&#30495;&#23454;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.07027</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65306;&#32469;&#36807;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Utilizing Synthetic Data for Medical Vision-Language Pre-training: Bypassing the Need for Real Images. (arXiv:2310.07027v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#22312;&#21307;&#23398;VLP&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#29992;&#30495;&#23454;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#26367;&#25442;&#30495;&#23454;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#20174;&#21307;&#23398;&#22270;&#20687;&#21644;&#37197;&#23545;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#32852;&#21512;&#23398;&#20064;&#34920;&#31034;&#12290;&#23427;&#36890;&#24120;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#39044;&#35757;&#32451;&#12290;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#20165;&#20351;&#29992;&#20174;&#30495;&#23454;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#26469;&#23454;&#29616;VLP&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#22823;&#37327;&#37197;&#23545;&#21644;&#31579;&#36873;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#38656;&#35201;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#21307;&#23398;VLP&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#26469;&#23457;&#26597;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#29992;&#30495;&#23454;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#26367;&#25442;&#30495;&#23454;&#21307;&#23398;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;VLP&#31639;&#27861;&#19987;&#38376;&#22312;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#35780;&#20272;&#28085;&#30422;&#20102;&#19977;&#20010;&#36830;&#32493;&#20219;&#21153;&#65292;&#21363;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Vision-Language Pre-training (VLP) learns representations jointly from medical images and paired radiology reports. It typically requires large-scale paired image-text datasets to achieve effective pre-training for both the image encoder and text encoder. The advent of text-guided generative models raises a compelling question: Can VLP be implemented solely with synthetic images generated from genuine radiology reports, thereby mitigating the need for extensively pairing and curating image-text datasets? In this work, we scrutinize this very question by examining the feasibility and effectiveness of employing synthetic images for medical VLP. We replace real medical images with their synthetic equivalents, generated from authentic medical reports. Utilizing three state-of-the-art VLP algorithms, we exclusively train on these synthetic samples. Our empirical evaluation across three subsequent tasks, namely image classification, semantic segmentation and object detection, reveals
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#36890;&#36807;&#22810;&#39033;&#30740;&#31350;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07023</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25366;&#25496;&#23439;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Automatic Macro Mining from Interaction Traces at Scale. (arXiv:2310.07023v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#36890;&#36807;&#22810;&#39033;&#30740;&#31350;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23439;&#20219;&#21153;&#26159;&#25105;&#20204;&#26085;&#24120;&#25163;&#26426;&#27963;&#21160;&#30340;&#26500;&#24314;&#22359;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#8220;&#30331;&#24405;&#8221;&#25110;&#8220;&#39044;&#23450;&#33322;&#29677;&#8221;&#65289;&#12290;&#26377;&#25928;&#22320;&#25552;&#21462;&#23439;&#20219;&#21153;&#23545;&#20110;&#29702;&#35299;&#31227;&#21160;&#20132;&#20114;&#21644;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23439;&#20219;&#21153;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#24456;&#38590;&#25552;&#21462;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#30001;&#22810;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#21516;&#26102;&#21448;&#38544;&#34255;&#22312;&#24212;&#29992;&#30340;&#32534;&#31243;&#32452;&#20214;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#20174;&#38543;&#26426;&#21644;&#29992;&#25143;&#31574;&#21010;&#30340;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23439;&#20219;&#21153;&#33258;&#21160;&#26631;&#35760;&#20102;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#19988;&#21487;&#20197;&#23436;&#20840;&#25191;&#34892;&#12290;&#20026;&#20102;&#26816;&#39564;&#25552;&#21462;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#39033;&#30740;&#31350;&#65292;&#21253;&#25324;&#29992;&#25143;&#35780;&#20272;&#12289;&#19982;&#20154;&#24037;&#31574;&#21010;&#20219;&#21153;&#30340;&#27604;&#36739;&#20998;&#26512;&#20197;&#21450;&#23545;&#36825;&#20123;&#23439;&#20219;&#21153;&#30340;&#33258;&#21160;&#25191;&#34892;&#12290;&#36825;&#20123;&#23454;&#39564;&#21644;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Macros are building block tasks of our everyday smartphone activity (e.g., "login", or "booking a flight"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of the app. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. To examine the quality of extraction, we conduct multiple studies, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses show the effectiveness of our approach and the usefulness of extracted macros in various dow
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24555;&#36895;&#27169;&#22359;&#21270;&#20803;&#23398;&#20064;&#30340;&#31070;&#32463;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#32452;&#21512;&#65292;&#38544;&#24335;&#22320;&#32534;&#30721;&#26102;&#38388;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#19968;&#20010;&#19978;&#19979;&#25991;&#20013;&#25512;&#26029;&#20851;&#31995;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07015</link><description>&lt;p&gt;
&#24555;&#36895;&#27169;&#22359;&#21270;&#20803;&#23398;&#20064;&#30340;&#31070;&#32463;&#20851;&#31995;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural Relational Inference with Fast Modular Meta-learning. (arXiv:2310.07015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24555;&#36895;&#27169;&#22359;&#21270;&#20803;&#23398;&#20064;&#30340;&#31070;&#32463;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#32452;&#21512;&#65292;&#38544;&#24335;&#22320;&#32534;&#30721;&#26102;&#38388;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#19968;&#20010;&#19978;&#19979;&#25991;&#20013;&#25512;&#26029;&#20851;&#31995;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26159;&#29992;&#20110;&#21253;&#21547;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#35768;&#22810;&#21160;&#24577;&#31995;&#32479;&#30340;&#26377;&#25928;&#27169;&#22411;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;GNN&#24212;&#29992;&#20551;&#35774;&#23454;&#20307;&#21644;&#20851;&#31995;&#21482;&#26377;&#19968;&#31181;&#31867;&#22411;&#65292;&#20294;&#35768;&#22810;&#24773;&#20917;&#28041;&#21450;&#22810;&#31181;&#31867;&#22411;&#30340;&#20132;&#20114;&#12290;&#20851;&#31995;&#25512;&#29702;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#36825;&#20123;&#20132;&#20114;&#24182;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20851;&#31995;&#25512;&#29702;&#35270;&#20026;&#27169;&#22359;&#21270;&#20803;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#27169;&#22359;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#32452;&#21512;&#26469;&#35299;&#20915;&#35768;&#22810;&#20219;&#21153;&#12290;&#36825;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#20801;&#35768;&#25105;&#20204;&#38544;&#24335;&#22320;&#32534;&#30721;&#26102;&#38388;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#19968;&#20010;&#19978;&#19979;&#25991;&#20013;&#25512;&#26029;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#29420;&#31435;&#22320;&#25512;&#26029;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#33021;&#21147;&#12290;&#23558;&#25512;&#29702;&#20316;&#20026;&#20803;&#23398;&#20064;&#30340;&#20869;&#24490;&#29615;&#20248;&#21270;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#26356;&#21152;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#24182;&#33021;&#22815;&#20272;&#35745;&#25105;&#20204;&#19981;&#30452;&#25509;&#35266;&#27979;&#21040;&#30340;&#23454;&#20307;&#30340;&#29366;&#24577;&#65292;&#20294;&#21487;&#20197;&#20174;&#23427;&#20204;&#23545;&#35266;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#20013;&#25512;&#26029;&#20986;&#23427;&#20204;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Graph neural networks} (GNNs) are effective models for many dynamical systems consisting of entities and relations. Although most GNN applications assume a single type of entity and relation, many situations involve multiple types of interactions. \textit{Relational inference} is the problem of inferring these interactions and learning the dynamics from observational data. We frame relational inference as a \textit{modular meta-learning} problem, where neural modules are trained to be composed in different ways to solve many tasks. This meta-learning framework allows us to implicitly encode time invariance and infer relations in context of one another rather than independently, which increases inference capacity. Framing inference as the inner-loop optimization of meta-learning leads to a model-based approach that is more data-efficient and capable of estimating the state of entities that we do not observe directly, but whose existence can be inferred from their effect on obser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#29983;&#25104;&#30340;&#20505;&#36873;&#31572;&#26696;&#22522;&#20110;&#20854;&#31867;&#22411;&#36827;&#34892;&#36807;&#28388;&#21644;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#19988;&#23545;&#20110;&#21547;&#26377;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07008</link><description>&lt;p&gt;
&#31572;&#26696;&#20505;&#36873;&#31867;&#22411;&#36873;&#25321;&#65306;&#38381;&#20070;&#38382;&#31572;&#20013;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#28385;&#36275;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs. (arXiv:2310.07008v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#29983;&#25104;&#30340;&#20505;&#36873;&#31572;&#26696;&#22522;&#20110;&#20854;&#31867;&#22411;&#36827;&#34892;&#36807;&#28388;&#21644;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#19988;&#23545;&#20110;&#21547;&#26377;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;T5&#25110;BART&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#23481;&#37327;&#26377;&#38480;&#65292;&#23545;&#20110;&#21253;&#21547;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#65292;&#36136;&#37327;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#30784;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26681;&#25454;&#20505;&#36873;&#31572;&#26696;&#30340;&#31867;&#22411;&#65288;&#26469;&#33258;Wikidata&#30340;"instance_of"&#23646;&#24615;&#65289;&#36827;&#34892;&#31579;&#36873;&#21644;&#37325;&#26032;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield promising results in the Knowledge Graph Question Answering (KGQA) task. However, the capacity of the models is limited and the quality decreases for questions with less popular entities. In this paper, we present a novel approach which works on top of the pre-trained Text-to-Text QA system to address this issue. Our simple yet effective method performs filtering and re-ranking of generated candidates based on their types derived from Wikidata "instance_of" property.
&lt;/p&gt;</description></item><item><title>&#22768;&#38899;&#25915;&#20987;&#26159;&#19968;&#31181;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#35789;&#21457;&#38899;&#30456;&#20284;&#24615;&#26469;&#27450;&#39575;&#29992;&#25143;&#65292;&#32780;Sound-skwatter&#26159;&#19968;&#31181;&#22810;&#35821;&#35328;&#30340;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#22768;&#38899;&#30456;&#20284;&#24615;&#29983;&#25104;&#22768;&#38899;&#25915;&#20987;&#20505;&#36873;&#39033;&#65292;&#20855;&#26377;&#36739;&#39640;&#36136;&#37327;&#21644;&#36328;&#35821;&#35328;&#24212;&#29992;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Sound-skwatter&#33021;&#22815;&#21457;&#29616;&#22823;&#37327;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#26410;&#30693;&#30340;&#24050;&#30693;&#21644;&#26032;&#22411;&#21516;&#38899;&#35789;&#20505;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.07005</link><description>&lt;p&gt;
&#22768;&#38899;&#25915;&#20987;&#39044;&#38450;&#30340;AI&#36741;&#21161;&#29983;&#25104;&#22120;Sound-skwatter&#65288;&#20320;&#26159;&#25351;Sound-squatter&#21527;&#65311;&#65289;
&lt;/p&gt;
&lt;p&gt;
Sound-skwatter (Did You Mean: Sound-squatter?) AI-powered Generator for Phishing Prevention. (arXiv:2310.07005v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07005
&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#25915;&#20987;&#26159;&#19968;&#31181;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#35789;&#21457;&#38899;&#30456;&#20284;&#24615;&#26469;&#27450;&#39575;&#29992;&#25143;&#65292;&#32780;Sound-skwatter&#26159;&#19968;&#31181;&#22810;&#35821;&#35328;&#30340;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#22768;&#38899;&#30456;&#20284;&#24615;&#29983;&#25104;&#22768;&#38899;&#25915;&#20987;&#20505;&#36873;&#39033;&#65292;&#20855;&#26377;&#36739;&#39640;&#36136;&#37327;&#21644;&#36328;&#35821;&#35328;&#24212;&#29992;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Sound-skwatter&#33021;&#22815;&#21457;&#29616;&#22823;&#37327;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#26410;&#30693;&#30340;&#24050;&#30693;&#21644;&#26032;&#22411;&#21516;&#38899;&#35789;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#25915;&#20987;&#26159;&#36890;&#36807;&#21033;&#29992;&#21333;&#35789;&#21457;&#38899;&#30456;&#20284;&#24615;&#26469;&#27450;&#39575;&#29992;&#25143;&#36827;&#20837;&#24694;&#24847;&#36164;&#28304;&#30340;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#12290;&#23545;&#22768;&#38899;&#25915;&#20987;&#20505;&#36873;&#39033;&#36827;&#34892;&#20027;&#21160;&#38450;&#24481;&#26159;&#24456;&#22797;&#26434;&#30340;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#20381;&#36182;&#20110;&#25163;&#21160;&#32534;&#36753;&#30340;&#21516;&#38899;&#35789;&#21015;&#34920;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35821;&#35328;&#30340;AI&#31995;&#32479;Sound-skwatter&#65292;&#29992;&#20110;&#29983;&#25104;&#22768;&#38899;&#25915;&#20987;&#20505;&#36873;&#39033;&#36827;&#34892;&#20027;&#21160;&#38450;&#24481;&#12290;Sound-skwatter&#21033;&#29992;&#36716;&#25442;&#32593;&#32476;&#21644;&#22768;&#23398;&#27169;&#22411;&#30340;&#21019;&#26032;&#22810;&#27169;&#24577;&#32452;&#21512;&#26469;&#23398;&#20064;&#22768;&#38899;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Sound-skwatter&#33021;&#22815;&#33258;&#21160;&#21015;&#20986;&#24050;&#30693;&#30340;&#21516;&#38899;&#35789;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#39640;&#36136;&#37327;&#30340;&#20505;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#36328;&#35821;&#35328;&#22768;&#38899;&#25915;&#20987;&#65292;&#21363;&#24403;&#38405;&#35835;&#32773;&#21644;&#21548;&#32773;&#20351;&#29992;&#19981;&#21516;&#35821;&#35328;&#26102;&#65292;&#25903;&#25345;&#20219;&#24847;&#35821;&#35328;&#32452;&#21512;&#12290;&#25105;&#20204;&#23558;Sound-skwatter&#24212;&#29992;&#20110;&#20197;&#22495;&#21517;&#21163;&#25345;&#20026;&#22522;&#30784;&#30340;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#32422;10&#65285;&#30340;&#29983;&#25104;&#22495;&#21517;&#22312;&#29616;&#23454;&#20013;&#23384;&#22312;&#65292;&#20854;&#20013;&#32477;&#22823;&#37096;&#20998;&#23545;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound-squatting is a phishing attack that tricks users into malicious resources by exploiting similarities in the pronunciation of words. Proactive defense against sound-squatting candidates is complex, and existing solutions rely on manually curated lists of homophones. We here introduce Sound-skwatter, a multi-language AI-based system that generates sound-squatting candidates for proactive defense. Sound-skwatter relies on an innovative multi-modal combination of Transformers Networks and acoustic models to learn sound similarities. We show that Sound-skwatter can automatically list known homophones and thousands of high-quality candidates. In addition, it covers cross-language sound-squatting, i.e., when the reader and the listener speak different languages, supporting any combination of languages. We apply Sound-skwatter to network-centric phishing via squatted domain names. We find ~ 10% of the generated domains exist in the wild, the vast majority unknown to protection solutions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;CarDS-Plus ECG&#24179;&#21488;&#30340;&#24320;&#21457;&#65292;&#35813;&#24179;&#21488;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#22810;&#24179;&#21488;&#31995;&#32479;&#65292;&#26088;&#22312;&#24555;&#36895;&#37096;&#32626;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24515;&#30005;&#22270;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#20020;&#24202;&#35843;&#26597;&#21644;&#21307;&#30103;&#26381;&#21153;&#20132;&#20184;&#12290;</title><link>http://arxiv.org/abs/2310.07000</link><description>&lt;p&gt;
CarDS-Plus ECG&#24179;&#21488;&#65306;&#24320;&#21457;&#21644;&#21487;&#34892;&#24615;&#35780;&#20272;&#19968;&#20010;&#36866;&#29992;&#20110;&#20415;&#25658;&#24335;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#24515;&#30005;&#22270;&#30340;&#22810;&#24179;&#21488;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
CarDS-Plus ECG Platform: Development and Feasibility Evaluation of a Multiplatform Artificial Intelligence Toolkit for Portable and Wearable Device Electrocardiograms. (arXiv:2310.07000v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;CarDS-Plus ECG&#24179;&#21488;&#30340;&#24320;&#21457;&#65292;&#35813;&#24179;&#21488;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#22810;&#24179;&#21488;&#31995;&#32479;&#65292;&#26088;&#22312;&#24555;&#36895;&#37096;&#32626;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24515;&#30005;&#22270;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#20020;&#24202;&#35843;&#26597;&#21644;&#21307;&#30103;&#26381;&#21153;&#20132;&#20184;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#29615;&#22659;&#19979;&#65292;&#23558;&#21487;&#31359;&#25140;&#21644;&#20415;&#25658;&#25216;&#26415;&#25972;&#21512;&#36827;&#26469;&#20026;&#20010;&#24615;&#21270;&#30340;&#20581;&#24247;&#30417;&#27979;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#26426;&#20250;&#12290;&#20687;Apple Watch&#12289;FitBit&#21644;AliveCor KardiaMobile&#36825;&#26679;&#30340;&#35774;&#22791;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#22797;&#26434;&#20581;&#24247;&#25968;&#25454;&#27969;&#30340;&#33719;&#21462;&#21644;&#22788;&#29702;&#12290;&#22312;&#36825;&#20123;&#35774;&#22791;&#25910;&#38598;&#30340;&#21508;&#31181;&#25968;&#25454;&#20013;&#65292;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#35760;&#24405;&#24050;&#32463;&#25104;&#20026;&#30417;&#27979;&#24515;&#34880;&#31649;&#20581;&#24247;&#30340;&#37325;&#35201;&#20449;&#24687;&#26469;&#28304;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#35299;&#35835;&#36825;&#20123;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26377;&#21161;&#20110;&#20020;&#24202;&#35786;&#26029;&#21644;&#32597;&#35265;&#24515;&#33039;&#30142;&#30149;&#30340;&#26816;&#27979;&#12290;&#36825;&#39033;&#35774;&#35745;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#22810;&#24179;&#21488;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#26088;&#22312;&#24555;&#36895;&#37096;&#32626;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24515;&#30005;&#22270;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#20020;&#24202;&#35843;&#26597;&#21644;&#21307;&#30103;&#26381;&#21153;&#20132;&#20184;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#35774;&#35745;&#32771;&#34385;&#22240;&#32032;&#65292;&#24182;&#23558;&#20854;&#19982;&#29305;&#23450;&#24212;&#29992;&#30456;&#19968;&#33268;&#65292;&#24320;&#21457;&#20102;&#25968;&#25454;&#27969;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of modern healthcare, the integration of wearable &amp; portable technology provides a unique opportunity for personalized health monitoring in the community. Devices like the Apple Watch, FitBit, and AliveCor KardiaMobile have revolutionized the acquisition and processing of intricate health data streams. Amidst the variety of data collected by these gadgets, single-lead electrocardiogram (ECG) recordings have emerged as a crucial source of information for monitoring cardiovascular health. There has been significant advances in artificial intelligence capable of interpreting these 1-lead ECGs, facilitating clinical diagnosis as well as the detection of rare cardiac disorders. This design study describes the development of an innovative multiplatform system aimed at the rapid deployment of AI-based ECG solutions for clinical investigation &amp; care delivery. The study examines design considerations, aligning them with specific applications, develops data flow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36829;&#21453;&#26399;&#26395;&#26426;&#21046;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38477;&#20302;&#29992;&#25143;&#39044;&#27979;&#35823;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#23384;&#20648;&#21644;&#26816;&#32034;&#36829;&#21453;&#29992;&#25143;&#26399;&#26395;&#30340;&#20107;&#23454;&#21487;&#20197;&#20351;&#27169;&#22411;&#20197;&#31867;&#20284;&#20154;&#31867;&#23398;&#20064;&#29702;&#35770;&#30340;&#26041;&#24335;&#20102;&#35299;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2310.06983</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#35748;&#30693;&#25552;&#31034;&#36829;&#21453;&#26399;&#26395;&#38477;&#20302;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24515;&#26234;&#29702;&#35770;&#39044;&#27979;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models. (arXiv:2310.06983v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36829;&#21453;&#26399;&#26395;&#26426;&#21046;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38477;&#20302;&#29992;&#25143;&#39044;&#27979;&#35823;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#23384;&#20648;&#21644;&#26816;&#32034;&#36829;&#21453;&#29992;&#25143;&#26399;&#26395;&#30340;&#20107;&#23454;&#21487;&#20197;&#20351;&#27169;&#22411;&#20197;&#31867;&#20284;&#20154;&#31867;&#23398;&#20064;&#29702;&#35770;&#30340;&#26041;&#24335;&#20102;&#35299;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#24515;&#26234;&#29702;&#35770;(ToM)&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#27700;&#24179;&#12290;&#23558;&#19981;&#21487;&#35266;&#23519;&#30340;&#24515;&#29702;&#29366;&#24577;&#24402;&#22240;&#20110;&#20182;&#20154;&#23545;&#20110;&#20154;&#31867;&#31038;&#20250;&#35748;&#30693;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#22312;&#20010;&#20307;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;(AIs)&#20043;&#38388;&#30340;&#22996;&#25176;-&#20195;&#29702;&#20851;&#31995;&#20013;&#21487;&#33021;&#21516;&#26679;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#21457;&#23637;&#24515;&#29702;&#23398;&#20013;&#30740;&#31350;&#30340;&#26426;&#21046;&#65292;&#21363;&#36829;&#21453;&#26399;&#26395;(VoE)&#65292;&#22914;&#20309;&#23454;&#29616;&#20197;&#36890;&#36807;&#21033;&#29992;&#26032;&#29983;&#30340;ToM&#21151;&#33021;&#26469;&#38477;&#20302;LLM&#23545;&#29992;&#25143;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#8220;&#20803;&#35748;&#30693;&#25552;&#31034;&#8221;&#26694;&#26550;&#65292;&#20197;&#22312;AI&#36741;&#23548;&#21592;&#30340;&#24773;&#22659;&#20013;&#24212;&#29992;VoE&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#22312;LLM&#23545;&#29992;&#25143;&#26399;&#26395;&#34987;&#36829;&#21453;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#33021;&#22815;&#20197;&#19982;&#20154;&#31867;&#23398;&#20064;&#29702;&#35770;&#30456;&#31526;&#30340;&#26041;&#24335;&#20102;&#35299;&#29992;&#25143;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24314;&#27169;&#29992;&#25143;&#24515;&#29702;&#30340;&#28508;&#22312;&#21361;&#38505;&#21644;&#22686;&#24378;&#26426;&#20250;&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research shows that Large Language Models (LLMs) exhibit a compelling level of proficiency in Theory of Mind (ToM) tasks. This ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and Artificial Intelligences (AIs). In this paper, we explore how a mechanism studied in developmental psychology known as Violation of Expectation (VoE) can be implemented to reduce errors in LLM prediction about users by leveraging emergent ToM affordances. And we introduce a \textit{metacognitive prompting} framework to apply VoE in the context of an AI tutor. By storing and retrieving facts derived in cases where LLM expectation about the user was violated, we find that LLMs are able to learn about users in ways that echo theories of human learning. Finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#24335;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;PDD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#22810;&#20010;&#23567;&#30340;&#21512;&#25104;&#22270;&#20687;&#38598;&#65292;&#24182;&#22312;&#36825;&#20123;&#23376;&#38598;&#30340;&#32047;&#35745;&#32852;&#21512;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25968;&#25454;&#38598;&#33976;&#39311;&#25216;&#26415;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06982</link><description>&lt;p&gt;
&#25968;&#25454;&#33976;&#39311;&#23601;&#20687;&#20239;&#29305;&#21152;&#19968;&#26679;&#65306;&#22810;&#27425;&#33976;&#39311;&#33719;&#24471;&#26356;&#22909;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality. (arXiv:2310.06982v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#24335;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;PDD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#22810;&#20010;&#23567;&#30340;&#21512;&#25104;&#22270;&#20687;&#38598;&#65292;&#24182;&#22312;&#36825;&#20123;&#23376;&#38598;&#30340;&#32047;&#35745;&#32852;&#21512;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25968;&#25454;&#38598;&#33976;&#39311;&#25216;&#26415;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#23567;&#32452;&#20855;&#26377;&#19982;&#23436;&#25972;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#65292;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#25216;&#26415;&#23384;&#22312;&#32570;&#38519;&#65292;&#19982;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#35757;&#32451;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20165;&#20351;&#29992;&#19968;&#20010;&#21512;&#25104;&#23376;&#38598;&#36827;&#34892;&#33976;&#39311;&#23558;&#26080;&#27861;&#33719;&#24471;&#26368;&#20339;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#36825;&#26159;&#22240;&#20026;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21457;&#29983;&#20102;&#26174;&#33879;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#22810;&#20010;&#21512;&#25104;&#23376;&#38598;&#26469;&#25429;&#25417;&#35757;&#32451;&#30340;&#19981;&#21516;&#38454;&#27573;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28176;&#36827;&#24335;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;Progressive Dataset Distillation&#65292;&#31616;&#31216;PDD&#65289;&#12290;PDD&#21512;&#25104;&#22810;&#20010;&#23567;&#30340;&#21512;&#25104;&#22270;&#20687;&#38598;&#65292;&#27599;&#20010;&#38598;&#21512;&#37117;&#26159;&#20197;&#21069;&#38598;&#21512;&#20026;&#26465;&#20214;&#65292;&#24182;&#22312;&#36825;&#20123;&#23376;&#38598;&#30340;&#32047;&#35745;&#32852;&#21512;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation aims to minimize the time and memory needed for training deep networks on large datasets, by creating a small set of synthetic images that has a similar generalization performance to that of the full dataset. However, current dataset distillation techniques fall short, showing a notable performance gap when compared to training on the original data. In this work, we are the first to argue that using just one synthetic subset for distillation will not yield optimal generalization performance. This is because the training dynamics of deep networks drastically change during the training. Hence, multiple synthetic subsets are required to capture the training dynamics at different phases of training. To address this issue, we propose Progressive Dataset Distillation (PDD). PDD synthesizes multiple small sets of synthetic images, each conditioned on the previous sets, and trains the model on the cumulative union of these subsets without requiring additional training time
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#21644;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#65288;QDP&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#37327;&#23376;&#24179;&#21488;&#19978;&#23454;&#26045;&#65292;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#27844;&#38706;&#21644;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#20840;&#38754;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2310.06973</link><description>&lt;p&gt;
&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Quantum Machine Learning with Differential Privacy. (arXiv:2310.06973v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#21644;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#65288;QDP&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#37327;&#23376;&#24179;&#21488;&#19978;&#23454;&#26045;&#65292;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#27844;&#38706;&#21644;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#20840;&#38754;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#19978;&#23454;&#26045;&#20154;&#24037;&#26234;&#33021;&#26102;&#65292;&#38544;&#31169;&#20445;&#25252;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26377;&#20960;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#26080;&#20811;&#38534;&#23450;&#29702;&#65292;&#37327;&#23376;&#35745;&#31639;&#22312;&#26412;&#36136;&#19978;&#26356;&#23433;&#20840;&#65292;&#22240;&#27492;&#25104;&#20026;&#26368;&#29702;&#24819;&#30340;&#35745;&#31639;&#24179;&#21488;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#26366;&#29420;&#31435;&#30740;&#31350;&#36807;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#21644;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#65288;QDP&#65289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#23578;&#26410;&#21516;&#26102;&#35299;&#20915;QFL&#21644;QDP&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#36825;&#20123;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#22312;&#37327;&#23376;&#24179;&#21488;&#19978;&#23454;&#26045;&#65292;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#27844;&#38706;&#65288;QFL&#65289;&#21644;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#65288;QDP&#65289;&#30340;&#20840;&#38754;&#20445;&#25252;&#12290;&#36825;&#31181;&#23454;&#29616;&#25215;&#35834;&#26356;&#39640;&#25928;&#21644;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The preservation of privacy is a critical concern in the implementation of artificial intelligence on sensitive training data. There are several techniques to preserve data privacy but quantum computations are inherently more secure due to the no-cloning theorem, resulting in a most desirable computational platform on top of the potential quantum advantages. There have been prior works in protecting data privacy by Quantum Federated Learning (QFL) and Quantum Differential Privacy (QDP) studied independently. However, to the best of our knowledge, no prior work has addressed both QFL and QDP together yet. Here, we propose to combine these privacy-preserving methods and implement them on the quantum platform, so that we can achieve comprehensive protection against data leakage (QFL) and model inversion attacks (QDP). This implementation promises more efficient and secure artificial intelligence. In this paper, we present a successful implementation of these privacy-preservation methods b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#30340;&#26032;&#30340;&#25191;&#34892;&#26694;&#26550;&#65306;&#27946;&#27700;&#21644;&#22238;&#22768;&#32593;&#32476;&#12290;&#36890;&#36807;&#27874;&#29366;&#28608;&#27963;&#27169;&#24335;&#65292;&#23427;&#21487;&#20197;&#22312;&#25972;&#20010;&#22270;&#20013;&#20256;&#36882;&#28040;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#26356;&#22823;&#30340;&#22270;&#24418;&#12290;</title><link>http://arxiv.org/abs/2310.06970</link><description>&lt;p&gt;
&#27946;&#27700;&#21644;&#22238;&#22768;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#31639;&#27861;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing. (arXiv:2310.06970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#30340;&#26032;&#30340;&#25191;&#34892;&#26694;&#26550;&#65306;&#27946;&#27700;&#21644;&#22238;&#22768;&#32593;&#32476;&#12290;&#36890;&#36807;&#27874;&#29366;&#28608;&#27963;&#27169;&#24335;&#65292;&#23427;&#21487;&#20197;&#22312;&#25972;&#20010;&#22270;&#20013;&#20256;&#36882;&#28040;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#26356;&#22823;&#30340;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#23398;&#20064;&#31639;&#27861;&#30340;&#33258;&#28982;&#36873;&#25321;&#12290;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#25277;&#35937;&#32780;&#22810;&#21151;&#33021;&#30340;&#22270;&#32467;&#26500;&#30452;&#25509;&#34920;&#31034;&#20219;&#21153;&#65292;&#24182;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#36755;&#20837;&#12290;&#36825;&#20026;&#31639;&#27861;&#30340;&#25193;&#23637;&#21644;&#22806;&#25512;&#21040;&#26356;&#22823;&#30340;&#22270;&#24418;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#37325;&#35201;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#25552;&#20986;&#20102;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;i&#65289;&#22914;&#20309;&#20351;&#33410;&#28857;&#33021;&#22815;&#22312;&#32473;&#23450;&#30340;&#22270;&#20013;&#25910;&#38598;&#25152;&#38656;&#30340;&#20449;&#24687;&#65288;&#21363;&#8220;&#20449;&#24687;&#20132;&#25442;&#8221;&#65289;&#65292;&#21363;&#20351;&#33410;&#28857;&#36317;&#31163;&#24456;&#36828;&#65307;ii&#65289;&#25105;&#20204;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#25191;&#34892;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#35813;&#20449;&#24687;&#20132;&#25442;&#20197;&#20415;&#22806;&#25512;&#21040;&#26356;&#22823;&#30340;&#22270;&#22823;&#23567;&#65288;&#21363;&#8220;&#22806;&#25512;&#26102;&#30340;&#31639;&#27861;&#23545;&#40784;&#8221;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25191;&#34892;&#26694;&#26550;&#65292;&#21463;&#20998;&#24067;&#24335;&#31639;&#27861;&#30340;&#35774;&#35745;&#21407;&#21017;&#21551;&#21457;&#65306;&#27946;&#27700;&#21644;&#22238;&#22768;&#32593;&#32476;&#12290;&#23427;&#20197;&#27874;&#29366;&#28608;&#27963;&#27169;&#24335;&#23558;&#28040;&#24687;&#20256;&#25773;&#21040;&#25972;&#20010;&#22270;&#20013;&#65292;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#23454;&#20363;&#12290;&#36890;&#36807;&#23427;&#30340;&#31232;&#30095;&#20294;&#24182;&#34892;&#28608;&#27963;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks are a natural fit for learning algorithms. They can directly represent tasks through an abstract but versatile graph structure and handle inputs of different sizes. This opens up the possibility for scaling and extrapolation to larger graphs, one of the most important advantages of an algorithm. However, this raises two core questions i) How can we enable nodes to gather the required information in a given graph ($\textit{information exchange}$), even if is far away and ii) How can we design an execution framework which enables this information exchange for extrapolation to larger graph sizes ($\textit{algorithmic alignment for extrapolation}$). We propose a new execution framework that is inspired by the design principles of distributed algorithms: Flood and Echo Net. It propagates messages through the entire graph in a wave like activation pattern, which naturally generalizes to larger instances. Through its sparse but parallel activations it is provably more ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#24615;&#30340;&#25919;&#31574;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#25919;&#31574;&#23398;&#20064;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#23454;&#24773;&#22659;&#20013;&#26080;&#27861;&#28385;&#36275;&#20551;&#35774;&#30340;&#38590;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22686;&#37327;&#20542;&#21521;&#20998;&#25968;&#31574;&#30053;&#35843;&#25972;&#20542;&#21521;&#20998;&#25968;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.06969</link><description>&lt;p&gt;
&#26080;&#20559;&#24615;&#25919;&#31574;&#23398;&#20064;&#19982;&#35266;&#27979;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Positivity-free Policy Learning with Observational Data. (arXiv:2310.06969v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#24615;&#30340;&#25919;&#31574;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#25919;&#31574;&#23398;&#20064;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#23454;&#24773;&#22659;&#20013;&#26080;&#27861;&#28385;&#36275;&#20551;&#35774;&#30340;&#38590;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22686;&#37327;&#20542;&#21521;&#20998;&#25968;&#31574;&#30053;&#35843;&#25972;&#20542;&#21521;&#20998;&#25968;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#25919;&#31574;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#38750;&#24120;&#37325;&#35201;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#26368;&#20248;&#30340;&#22788;&#29702;&#20998;&#37197;&#31574;&#30053;&#65292;&#21516;&#26102;&#28385;&#36275;&#29305;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#39044;&#31639;&#21644;&#31616;&#21333;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#20559;&#24615;&#65288;&#38543;&#26426;&#65289;&#25919;&#31574;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#24212;&#23545;&#29616;&#23454;&#24773;&#22659;&#20013;&#26080;&#27861;&#28385;&#36275;&#20551;&#35774;&#30340;&#22256;&#22659;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22686;&#37327;&#20542;&#21521;&#20998;&#25968;&#31574;&#30053;&#26469;&#35843;&#25972;&#20542;&#21521;&#20998;&#25968;&#20540;&#65292;&#32780;&#19981;&#26159;&#32473;&#27835;&#30103;&#20998;&#37197;&#22266;&#23450;&#20540;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#22686;&#37327;&#20542;&#21521;&#20998;&#25968;&#31574;&#30053;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#24314;&#31435;&#20102;&#35782;&#21035;&#26465;&#20214;&#65292;&#21033;&#29992;&#21322;&#21442;&#25968;&#25928;&#29575;&#29702;&#35770;&#25552;&#20986;&#20102;&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#29575;&#30340;&#39640;&#25928;&#20272;&#35745;&#22120;&#65292;&#21363;&#20351;&#26159;&#19982;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#26412;&#25991;&#23545;&#25919;&#31574;&#23398;&#20064;&#30340;&#29702;&#35770;&#20445;&#35777;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#24182;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy learning utilizing observational data is pivotal across various domains, with the objective of learning the optimal treatment assignment policy while adhering to specific constraints such as fairness, budget, and simplicity. This study introduces a novel positivity-free (stochastic) policy learning framework designed to address the challenges posed by the impracticality of the positivity assumption in real-world scenarios. This framework leverages incremental propensity score policies to adjust propensity score values instead of assigning fixed values to treatments. We characterize these incremental propensity score policies and establish identification conditions, employing semiparametric efficiency theory to propose efficient estimators capable of achieving rapid convergence rates, even when integrated with advanced machine learning algorithms. This paper provides a thorough exploration of the theoretical guarantees associated with policy learning and validates the proposed fr
&lt;/p&gt;</description></item><item><title>ObjectComposer&#26159;&#19968;&#31181;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#19968;&#33268;&#22320;&#29983;&#25104;&#22810;&#20010;&#23545;&#35937;&#30340;&#32452;&#21512;&#12290;&#36825;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23545;&#35937;&#22806;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06968</link><description>&lt;p&gt;
ObjectComposer: &#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#19968;&#33268;&#29983;&#25104;&#22810;&#20010;&#23545;&#35937;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ObjectComposer: Consistent Generation of Multiple Objects Without Fine-tuning. (arXiv:2310.06968v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06968
&lt;/p&gt;
&lt;p&gt;
ObjectComposer&#26159;&#19968;&#31181;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#19968;&#33268;&#22320;&#29983;&#25104;&#22810;&#20010;&#23545;&#35937;&#30340;&#32452;&#21512;&#12290;&#36825;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23545;&#35937;&#22806;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20174;&#25991;&#26412;&#25552;&#31034;&#20013;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#19968;&#33268;&#22320;&#29983;&#25104;&#30456;&#21516;&#22806;&#35266;&#30340;&#23545;&#35937;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#19968;&#33268;&#30340;&#23545;&#35937;&#29983;&#25104;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#22914;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#35282;&#33394;&#21644;&#22330;&#26223;&#30340;&#28459;&#30011;&#20070;&#25554;&#22270;&#38750;&#24120;&#37325;&#35201;&#12290;&#35768;&#22810;&#26041;&#27861;&#36890;&#36807;&#23545;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#36827;&#34892;&#24494;&#35843;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#36731;&#37327;&#32423;&#24494;&#35843;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#21644;&#23454;&#26102;&#36816;&#34892;&#26102;&#20063;&#21487;&#33021;&#20195;&#20215;&#36807;&#39640;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;ObjectComposer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#30456;&#20284;&#30340;&#22810;&#20010;&#23545;&#35937;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#26368;&#36817;&#30340;BLIP-Diffusion&#27169;&#22411;&#20043;&#19978;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#21442;&#32771;&#22270;&#20687;&#29983;&#25104;&#21333;&#20010;&#23545;&#35937;&#30340;&#22270;&#20687;&#12290;ObjectComposer&#21487;&#20197;&#23454;&#29616;&#29983;&#25104;&#21253;&#21547; &#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image generative models can generate high-fidelity images from text prompts. However, these models struggle to consistently generate the same objects in different contexts with the same appearance. Consistent object generation is important to many downstream tasks like generating comic book illustrations with consistent characters and setting. Numerous approaches attempt to solve this problem by extending the vocabulary of diffusion models through fine-tuning. However, even lightweight fine-tuning approaches can be prohibitively expensive to run at scale and in real-time. We introduce a method called ObjectComposer for generating compositions of multiple objects that resemble user-specified images. Our approach is training-free, leveraging the abilities of preexisting models. We build upon the recent BLIP-Diffusion model, which can generate images of single objects specified by reference images. ObjectComposer enables the consistent generation of compositions containing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#31867;&#35282;&#24230;&#35780;&#20272;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;&#36825;&#20123;&#23454;&#39564;&#19981;&#20165;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#35780;&#20272;&#21508;&#31181;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36824;&#26159;&#26368;&#20840;&#38754;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.06966</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#30340;&#20998;&#31867;&#22120;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#20154;&#31867;&#20013;&#24515;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Interpretability of Part-Prototype Based Classifiers: A Human Centric Analysis. (arXiv:2310.06966v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#31867;&#35282;&#24230;&#35780;&#20272;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;&#36825;&#20123;&#23454;&#39564;&#19981;&#20165;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#35780;&#20272;&#21508;&#31181;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36824;&#26159;&#26368;&#20840;&#38754;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#30340;&#32593;&#32476;&#24050;&#25104;&#20026;&#35768;&#22810;&#40657;&#30418;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#21487;&#35299;&#37322;&#24615;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#20154;&#31867;&#29992;&#25143;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20174;&#20154;&#31867;&#35282;&#24230;&#35780;&#20272;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#21487;&#25805;&#20316;&#30340;&#24230;&#37327;&#21644;&#23454;&#39564;&#32452;&#25104;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#23427;&#20204;&#19981;&#20165;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35780;&#20272;&#21508;&#31181;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32780;&#19988;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#26368;&#20840;&#38754;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Part-prototype networks have recently become methods of interest as an interpretable alternative to many of the current black-box image classifiers. However, the interpretability of these methods from the perspective of human users has not been sufficiently explored. In this work, we have devised a framework for evaluating the interpretability of part-prototype-based models from a human perspective. The proposed framework consists of three actionable metrics and experiments. To demonstrate the usefulness of our framework, we performed an extensive set of experiments using Amazon Mechanical Turk. They not only show the capability of our framework in assessing the interpretability of various part-prototype-based models, but they also are, to the best of our knowledge, the most comprehensive work on evaluating such methods in a unified framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#29616;&#20195;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#37096;&#20998;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#25269;&#25239;&#21147;&#65292;&#20026;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26356;&#23433;&#20840;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.06958</link><description>&lt;p&gt;
&#27604;&#36739;&#29616;&#20195;&#26080;&#21442;&#32771;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Comparing the robustness of modern no-reference image- and video-quality metrics to adversarial attacks. (arXiv:2310.06958v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#29616;&#20195;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#37096;&#20998;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#25269;&#25239;&#21147;&#65292;&#20026;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26356;&#23433;&#20840;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#21464;&#24471;&#26356;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#20998;&#25968;&#20294;&#19981;&#25913;&#21892;&#35270;&#35273;&#36136;&#37327;&#12290;&#29616;&#26377;&#30340;&#36136;&#37327;&#24230;&#37327;&#22522;&#20934;&#23558;&#20854;&#24615;&#33021;&#19982;&#20027;&#35266;&#36136;&#37327;&#30456;&#20851;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20063;&#26159;&#19968;&#20010;&#20540;&#24471;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#20195;&#24230;&#37327;&#26041;&#27861;&#23545;&#19981;&#21516;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20123;&#25915;&#20987;&#23545;15&#20010;&#26080;&#21442;&#32771;&#22270;&#20687;/&#35270;&#39057;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#19968;&#20123;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#25269;&#25239;&#21147;&#65292;&#20351;&#23427;&#20204;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20351;&#29992;&#27604;&#23481;&#26131;&#21463;&#25915;&#20987;&#30340;&#26041;&#27861;&#26356;&#23433;&#20840;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#25509;&#21463;&#30740;&#31350;&#20154;&#21592;&#25552;&#20132;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#20351;&#20182;&#20204;&#30340;&#26041;&#27861;&#23545;&#25915;&#20987;&#26356;&#21152;&#40065;&#26834;&#65292;&#25110;&#32773;&#20026;&#20182;&#20204;&#23547;&#25214;&#31526;&#21512;&#38656;&#27714;&#30340;&#40065;&#26834;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays neural-network-based image- and video-quality metrics show better performance compared to traditional methods. However, they also became more vulnerable to adversarial attacks that increase metrics' scores without improving visual quality. The existing benchmarks of quality metrics compare their performance in terms of correlation with subjective quality and calculation time. However, the adversarial robustness of image-quality metrics is also an area worth researching. In this paper, we analyse modern metrics' robustness to different adversarial attacks. We adopted adversarial attacks from computer vision tasks and compared attacks' efficiency against 15 no-reference image/video-quality metrics. Some metrics showed high resistance to adversarial attacks which makes their usage in benchmarks safer than vulnerable metrics. The benchmark accepts new metrics submissions for researchers who want to make their metrics more robust to attacks or to find such metrics for their needs. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#25955;&#20808;&#39564;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#37325;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#25968;&#25454;&#20445;&#30495;&#24230;&#37325;&#24314;&#36807;&#31243;&#65292;&#20197;&#21450;&#37319;&#29992;Nesterov&#21160;&#37327;&#21152;&#36895;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#21058;&#37327;CT&#19979;&#20986;&#33394;&#30340;&#22270;&#20687;&#37325;&#24314;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06949</link><description>&lt;p&gt;
&#20302;&#21058;&#37327;CT&#30340;&#25193;&#25955;&#20808;&#39564;&#27491;&#21017;&#21270;&#36845;&#20195;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Diffusion Prior Regularized Iterative Reconstruction for Low-dose CT. (arXiv:2310.06949v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#25955;&#20808;&#39564;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#37325;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#25968;&#25454;&#20445;&#30495;&#24230;&#37325;&#24314;&#36807;&#31243;&#65292;&#20197;&#21450;&#37319;&#29992;Nesterov&#21160;&#37327;&#21152;&#36895;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#21058;&#37327;CT&#19979;&#20986;&#33394;&#30340;&#22270;&#20687;&#37325;&#24314;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#28041;&#21450;&#24739;&#32773;&#25509;&#21463;&#30005;&#31163;&#36752;&#23556;&#12290;&#20026;&#20102;&#20943;&#23569;&#36752;&#23556;&#21058;&#37327;&#65292;&#25105;&#20204;&#21487;&#20197;&#38477;&#20302;X&#23556;&#32447;&#20809;&#23376;&#35745;&#25968;&#25110;&#32773;&#19979;&#37319;&#26679;&#25237;&#24433;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#24335;&#32463;&#24120;&#20250;&#25439;&#23475;&#22270;&#20687;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#20808;&#39564;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#37325;&#24314;&#31639;&#27861;&#12290;&#20511;&#37492;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#20986;&#33394;&#25104;&#20687;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20248;&#20808;&#32771;&#34385;&#25968;&#25454;&#20445;&#30495;&#24230;&#30340;&#37325;&#24314;&#36807;&#31243;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#34701;&#21512;&#21033;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#30340;&#20248;&#28857;&#65292;&#22312;&#26080;&#30417;&#30563;&#26694;&#26550;&#20013;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#37325;&#24314;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;Nesterov&#21160;&#37327;&#21152;&#36895;&#25216;&#26415;&#12290;&#36825;&#31181;&#22686;&#24378;&#26377;&#21161;&#20110;&#22312;&#26356;&#23569;&#30340;&#27493;&#39588;&#20013;&#23454;&#29616;&#20248;&#31168;&#30340;&#25193;&#25955;&#37319;&#26679;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#23454;&#39564;&#25152;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#39640;&#28165;CT&#22270;&#20687;&#37325;&#24314;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#36884;&#24452;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#36752;&#23556;&#21058;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computed tomography (CT) involves a patient's exposure to ionizing radiation. To reduce the radiation dose, we can either lower the X-ray photon count or down-sample projection views. However, either of the ways often compromises image quality. To address this challenge, here we introduce an iterative reconstruction algorithm regularized by a diffusion prior. Drawing on the exceptional imaging prowess of the denoising diffusion probabilistic model (DDPM), we merge it with a reconstruction procedure that prioritizes data fidelity. This fusion capitalizes on the merits of both techniques, delivering exceptional reconstruction results in an unsupervised framework. To further enhance the efficiency of the reconstruction process, we incorporate the Nesterov momentum acceleration technique. This enhancement facilitates superior diffusion sampling in fewer steps. As demonstrated in our experiments, our method offers a potential pathway to high-definition CT image reconstruction with minimized
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#35774;&#35745;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#26102;&#38388;&#34892;&#20026;&#21644;&#25552;&#21462;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#40065;&#26834;&#12289;&#22522;&#20110;&#29289;&#29702;&#30340;&#24037;&#19994;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#32593;&#32476;&#25915;&#20987;&#30340;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2310.06948</link><description>&lt;p&gt;
&#19968;&#31181;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#29992;&#20110;&#40065;&#26834;&#30340;&#12289;&#22522;&#20110;&#29289;&#29702;&#30340;&#24037;&#19994;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#32593;&#32476;&#25915;&#20987;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Variational Autoencoder Framework for Robust, Physics-Informed Cyberattack Recognition in Industrial Cyber-Physical Systems. (arXiv:2310.06948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#35774;&#35745;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#26102;&#38388;&#34892;&#20026;&#21644;&#25552;&#21462;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#40065;&#26834;&#12289;&#22522;&#20110;&#29289;&#29702;&#30340;&#24037;&#19994;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#32593;&#32476;&#25915;&#20987;&#30340;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#36890;&#20449;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#26080;&#32447;&#32593;&#32476;&#65292;&#24037;&#19994;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#30340;&#32593;&#32476;&#23433;&#20840;&#24341;&#36215;&#20102;&#24040;&#22823;&#20851;&#27880;&#12290;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#34987;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#25915;&#20987;&#65292;&#20294;&#24456;&#23569;&#26377;&#26041;&#27861;&#19987;&#27880;&#20110;&#21306;&#20998;&#32593;&#32476;&#25915;&#20987;&#21644;&#35774;&#22791;&#25925;&#38556;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#12289;&#35786;&#26029;&#21644;&#23450;&#20301;&#19968;&#31181;&#31216;&#20026;&#38544;&#34109;&#25915;&#20987;&#30340;&#32593;&#32476;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#30340;&#32593;&#32476;&#25915;&#20987;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#28151;&#21512;&#35774;&#35745;&#12290;&#35813;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#32771;&#34385;&#20102;&#19968;&#20010;&#36890;&#29992;&#29289;&#29702;&#31995;&#32479;&#30340;&#26102;&#38388;&#34892;&#20026;&#65292;&#20174;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#29305;&#24449;&#29992;&#20110;&#26816;&#27979;&#38544;&#34109;&#25915;&#20987;&#12289;&#21306;&#20998;&#23427;&#20204;&#19982;&#35774;&#22791;&#25925;&#38556;&#20197;&#21450;&#23450;&#20301;&#25915;&#20987;/&#25925;&#38556;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;&#20223;&#30495;&#23454;&#39564;&#20013;&#30340;&#23454;&#38469;&#27169;&#25311;&#30740;&#31350;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybersecurity of Industrial Cyber-Physical Systems is drawing significant concerns as data communication increasingly leverages wireless networks. A lot of data-driven methods were develope for detecting cyberattacks, but few are focused on distinguishing them from equipment faults. In this paper, we develop a data-driven framework that can be used to detect, diagnose, and localize a type of cyberattack called covert attacks on networked industrial control systems. The framework has a hybrid design that combines a variational autoencoder (VAE), a recurrent neural network (RNN), and a Deep Neural Network (DNN). This data-driven framework considers the temporal behavior of a generic physical system that extracts features from the time series of the sensor measurements that can be used for detecting covert attacks, distinguishing them from equipment faults, as well as localize the attack/fault. We evaluate the performance of the proposed method through a realistic simulation study on a ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23041;&#32961;&#27979;&#35797;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#20915;&#31574;&#36807;&#31243;&#25903;&#25345;&#29305;&#24037;&#34892;&#21160;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#23041;&#32961;&#34892;&#20026;&#33021;&#21147;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06936</link><description>&lt;p&gt;
LLMs&#26432;&#27515;&#20102;&#33050;&#26412;&#23567;&#23376;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#19979;&#30340;&#29305;&#24037;&#22914;&#20309;&#25913;&#21464;&#32593;&#32476;&#23041;&#32961;&#27979;&#35797;&#30340;&#26684;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing. (arXiv:2310.06936v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23041;&#32961;&#27979;&#35797;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#20915;&#31574;&#36807;&#31243;&#25903;&#25345;&#29305;&#24037;&#34892;&#21160;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#23041;&#32961;&#34892;&#20026;&#33021;&#21147;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#23041;&#32961;&#12289;&#29983;&#25104;&#26377;&#20851;&#24037;&#20855;&#20449;&#24687;&#21644;&#33258;&#21160;&#21270;&#32593;&#32476;&#25915;&#20987;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#25163;&#21160;&#25506;&#32034;&#20102;LLMs&#22312;&#25903;&#25345;&#29305;&#23450;&#23041;&#32961;&#30456;&#20851;&#25805;&#20316;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20915;&#31574;&#36807;&#31243;&#33258;&#21160;&#21270;&#24212;&#29992;&#20110;&#32593;&#32476;&#25915;&#20987;&#34892;&#21160;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#23041;&#32961;&#34892;&#21160;&#30340;&#35745;&#21010;-&#34892;&#21160;-&#25253;&#21578;&#24490;&#29615;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#20197;&#21450;&#25351;&#23548;&#22810;&#39033;&#34892;&#21160;&#30340;&#36830;&#38145;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;LLM&#22312;&#25105;&#20204;&#23637;&#31034;&#30340;&#30701;&#26399;&#25915;&#20987;&#20013;&#30340;&#32593;&#32476;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#23601;&#24341;&#21457;&#21487;&#25191;&#34892;&#21709;&#24212;&#30340;&#25552;&#31034;&#35774;&#35745;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;LLMs&#23545;&#23041;&#32961;&#26684;&#23616;&#30340;&#28508;&#22312;&#24433;&#21709;&#20197;&#21450;&#20351;&#29992;LLMs&#21152;&#36895;&#23041;&#32961;&#34892;&#20026;&#33021;&#21147;&#30340;&#20262;&#29702;&#32771;&#34385;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#23041;&#32961;&#20013;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#20294;&#20196;&#20154;&#25285;&#24551;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;LLM&#35299;&#20915;&#26356;&#22797;&#26434;&#21644;&#25345;&#32493;&#24615;&#23041;&#32961;&#30340;&#33021;&#21147;&#36824;&#26377;&#24453;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the potential of Large Language Models (LLMs) to reason about threats, generate information about tools, and automate cyber campaigns. We begin with a manual exploration of LLMs in supporting specific threat-related actions and decisions. We proceed by automating the decision process in a cyber campaign. We present prompt engineering approaches for a plan-act-report loop for one action of a threat campaign and and a prompt chaining design that directs the sequential decision process of a multi-action campaign. We assess the extent of LLM's cyber-specific knowledge w.r.t the short campaign we demonstrate and provide insights into prompt design for eliciting actionable responses. We discuss the potential impact of LLMs on the threat landscape and the ethical considerations of using LLMs for accelerating threat actor capabilities. We report a promising, yet concerning, application of generative AI to cyber threats. However, the LLM's capabilities to deal with mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#38452;&#24433;&#26799;&#24230;&#19979;&#38477;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#27979;&#37327;&#37327;&#23376;&#38452;&#24433;&#26679;&#26412;&#21644;&#27169;&#25311;&#24577;&#30340;&#26799;&#24230;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.06935</link><description>&lt;p&gt;
&#37327;&#23376;&#38452;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#37327;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Quantum Shadow Gradient Descent for Quantum Learning. (arXiv:2310.06935v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#38452;&#24433;&#26799;&#24230;&#19979;&#38477;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#27979;&#37327;&#37327;&#23376;&#38452;&#24433;&#26679;&#26412;&#21644;&#27169;&#25311;&#24577;&#30340;&#26799;&#24230;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#38452;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;QSGD&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19968;&#27425;&#24615;&#25805;&#20316;&#30340;&#20248;&#28857;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26679;&#26412;&#22797;&#21046;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#20351;&#29992;&#31934;&#30830;&#26799;&#24230;&#35745;&#31639;&#30340;&#29702;&#24819;&#26356;&#26032;&#35268;&#21017;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#37327;&#23376;&#38452;&#24433;&#26679;&#26412;&#65288;QSS&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#29983;&#25104;&#37327;&#23376;&#38452;&#24433;&#32780;&#19981;&#26159;&#29616;&#26377;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#32463;&#20856;&#38452;&#24433;&#12290;&#36890;&#36807;&#27979;&#37327;&#37327;&#23376;&#38452;&#24433;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#35745;&#31639;&#26102;&#22240;&#32500;&#24230;&#21576;&#25351;&#25968;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;&#20316;&#20026;&#31532;&#20108;&#20010;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26356;&#19968;&#33324;&#30340;&#38750;&#20056;&#31215;&#24418;&#24335;&#30340;&#27169;&#25311;&#21464;&#20998;&#21704;&#23494;&#39039;&#37327;&#65292;&#24418;&#24335;&#20026;$\exp\{i\sum_j \theta_j A_j\}$&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#21487;&#20197;&#29992;&#26131;&#20110;&#27979;&#37327;&#30340;&#21333;&#21442;&#25968;&#27169;&#25311;&#24577;&#30340;&#26799;&#24230;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new procedure called quantum shadow gradient descent (QSGD) that addresses these key challenges. Our method has the benefits of a one-shot approach, in not requiring any sample duplication while having a convergence rate comparable to the ideal update rule using exact gradient computation. We propose a new technique for generating quantum shadow samples (QSS), which generates quantum shadows as opposed to classical shadows used in existing works. With classical shadows, the computations are typically performed on classical computers and, hence, are prohibitive since the dimension grows exponentially. Our approach resolves this issue by measurements of quantum shadows. As the second main contribution, we study more general non-product ansatz of the form $\exp\{i\sum_j \theta_j A_j\}$ that model variational Hamiltonians. We prove that the gradient can be written in terms of the gradient of single-parameter ansatzes that can be easily measured. Our proof is based on 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21547;&#26377;93&#20010;&#20070;&#31821;&#21644;&#23545;&#24212;&#26377;&#22768;&#20070;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26377;&#22768;&#20070;&#25991;&#26412;&#20013;&#30340;&#38901;&#24459;&#23646;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#38901;&#24459;&#19982;&#20154;&#31867;&#26391;&#35835;&#27604;&#21830;&#19994;&#32423;TTS&#31995;&#32479;&#26356;&#30456;&#20851;&#65292;&#24182;&#19988;&#20154;&#20204;&#26356;&#21916;&#27426;&#38901;&#24459;&#22686;&#24378;&#30340;&#26377;&#22768;&#20070;&#26391;&#35835;&#12290;</title><link>http://arxiv.org/abs/2310.06930</link><description>&lt;p&gt;
&#12298;&#26377;&#22768;&#20070;&#30340;&#38901;&#24459;&#20998;&#26512;&#12299;
&lt;/p&gt;
&lt;p&gt;
Prosody Analysis of Audiobooks. (arXiv:2310.06930v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21547;&#26377;93&#20010;&#20070;&#31821;&#21644;&#23545;&#24212;&#26377;&#22768;&#20070;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26377;&#22768;&#20070;&#25991;&#26412;&#20013;&#30340;&#38901;&#24459;&#23646;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#38901;&#24459;&#19982;&#20154;&#31867;&#26391;&#35835;&#27604;&#21830;&#19994;&#32423;TTS&#31995;&#32479;&#26356;&#30456;&#20851;&#65292;&#24182;&#19988;&#20154;&#20204;&#26356;&#21916;&#27426;&#38901;&#24459;&#22686;&#24378;&#30340;&#26377;&#22768;&#20070;&#26391;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20351;&#24471;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#33258;&#28982;&#38899;&#25928;&#30340;&#38899;&#39057;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#26377;&#22768;&#20070;&#26391;&#35835;&#28041;&#21450;&#21040;&#35835;&#32773;&#30340;&#25103;&#21095;&#24615;&#22768;&#38899;&#21644;&#35821;&#35843;&#65292;&#26356;&#22810;&#22320;&#20381;&#36182;&#24773;&#24863;&#12289;&#23545;&#35805;&#21644;&#21465;&#36848;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;93&#26412;&#20070;&#19982;&#20854;&#23545;&#24212;&#30340;&#26377;&#22768;&#20070;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#21465;&#36848;&#25991;&#26412;&#20013;&#39044;&#27979;&#38901;&#24459;&#23646;&#24615;&#65288;&#38899;&#39640;&#12289;&#38899;&#37327;&#21644;&#35821;&#36895;&#65289;&#65292;&#24182;&#20351;&#29992;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#39044;&#27979;&#30340;&#38901;&#24459;&#23646;&#24615;&#19982;&#20154;&#31867;&#26391;&#35835;&#30340;&#30456;&#20851;&#24615;&#35201;&#36828;&#39640;&#20110;&#21830;&#19994;&#32423;TTS&#31995;&#32479;&#30340;&#32467;&#26524;&#65306;&#22312;24&#26412;&#20070;&#20013;&#65292;&#25105;&#20204;&#39044;&#27979;&#30340;&#38899;&#39640;&#23545;22&#26412;&#20070;&#30340;&#20154;&#31867;&#38405;&#35835;&#26356;&#20855;&#30456;&#20851;&#24615;&#65292;&#32780;&#25105;&#20204;&#39044;&#27979;&#30340;&#38899;&#37327;&#23646;&#24615;&#23545;23&#26412;&#20070;&#30340;&#20154;&#31867;&#38405;&#35835;&#26356;&#21152;&#30456;&#20284;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#65292;&#20197;&#37327;&#21270;&#20154;&#20204;&#26356;&#21916;&#27426;&#38901;&#24459;&#22686;&#24378;&#30340;&#26377;&#22768;&#20070;&#26391;&#35835;&#36824;&#26159;&#21830;&#19994;&#32423;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-speech have made it possible to generate natural-sounding audio from text. However, audiobook narrations involve dramatic vocalizations and intonations by the reader, with greater reliance on emotions, dialogues, and descriptions in the narrative. Using our dataset of 93 aligned book-audiobook pairs, we present improved models for prosody prediction properties (pitch, volume, and rate of speech) from narrative text using language modeling. Our predicted prosody attributes correlate much better with human audiobook readings than results from a state-of-the-art commercial TTS system: our predicted pitch shows a higher correlation with human reading for 22 out of the 24 books, while our predicted volume attribute proves more similar to human reading for 23 out of the 24 books. Finally, we present a human evaluation study to quantify the extent that people prefer prosody-enhanced audiobook readings over commercial text-to-speech systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23431;&#23449;&#23398;&#36229;&#20998;&#36776;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#19981;&#21516;&#23610;&#24230;&#30340;&#37325;&#35201;&#24615;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#24182;&#19988;&#19982;&#23567;&#23610;&#24230;&#29305;&#24449;&#22810;&#26679;&#24615;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.06929</link><description>&lt;p&gt;
&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#38543;&#26426;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#22312;&#23431;&#23449;&#23398;&#27169;&#25311;&#20013;&#23454;&#29616;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Stochastic Super-resolution of Cosmological Simulations with Denoising Diffusion Models. (arXiv:2310.06929v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23431;&#23449;&#23398;&#36229;&#20998;&#36776;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#19981;&#21516;&#23610;&#24230;&#30340;&#37325;&#35201;&#24615;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#24182;&#19988;&#19982;&#23567;&#23610;&#24230;&#29305;&#24449;&#22810;&#26679;&#24615;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22686;&#24378;&#20302;&#20998;&#36776;&#29575;&#30340;&#23431;&#23449;&#23398;&#27169;&#25311;&#30340;&#23567;&#23610;&#24230;&#20449;&#24687;&#65292;&#36825;&#34987;&#31216;&#20026;&#8220;&#36229;&#20998;&#36776;&#29575;&#8221;&#20219;&#21153;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#23431;&#23449;&#23398;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20381;&#36182;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#39640;&#24230;&#36924;&#30495;&#30340;&#32467;&#26524;&#65292;&#20294;&#23384;&#22312;&#21508;&#31181;&#32570;&#28857;&#65288;&#20363;&#22914;&#20302;&#26679;&#26412;&#22810;&#26679;&#24615;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26469;&#23454;&#29616;&#36229;&#20998;&#36776;&#29575;&#30340;&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#39044;&#27979;&#65288;&#20316;&#20026;&#20108;&#32500;&#30340;&#39318;&#20010;&#27010;&#24565;&#35777;&#26126;&#65289;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#23567;&#23610;&#24230;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#28388;&#27874;&#22686;&#24378;&#8221;&#35757;&#32451;&#26041;&#27861;&#65292;&#37325;&#26032;&#20998;&#37197;&#20102;&#20687;&#32032;&#32423;&#35757;&#32451;&#30446;&#26631;&#20013;&#19981;&#21516;&#23610;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#30334;&#20998;&#20043;&#19968;&#27700;&#24179;&#19968;&#33268;&#30340;&#21151;&#29575;&#35889;&#65292;&#24182;&#19988;&#33021;&#22815;&#37325;&#29616;&#19982;&#23567;&#23610;&#24230;&#29305;&#24449;&#22810;&#26679;&#24615;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning models have been successfully employed for augmenting low-resolution cosmological simulations with small-scale information, a task known as "super-resolution". So far, these cosmological super-resolution models have relied on generative adversarial networks (GANs), which can achieve highly realistic results, but suffer from various shortcomings (e.g. low sample diversity). We introduce denoising diffusion models as a powerful generative model for super-resolving cosmic large-scale structure predictions (as a first proof-of-concept in two dimensions). To obtain accurate results down to small scales, we develop a new "filter-boosted" training approach that redistributes the importance of different scales in the pixel-wise training objective. We demonstrate that our model not only produces convincing super-resolution images and power spectra consistent at the percent level, but is also able to reproduce the diversity of small-scale features consistent with a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PICProp&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24378;&#22823;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#65292;&#24182;&#19988;&#36890;&#36807;&#20256;&#25773;&#32622;&#20449;&#24230;&#23454;&#29616;&#20102;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2310.06923</link><description>&lt;p&gt;
PICProp&#65306;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#29289;&#29702;&#20449;&#24687;&#32622;&#20449;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification. (arXiv:2310.06923v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PICProp&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24378;&#22823;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#65292;&#24182;&#19988;&#36890;&#36807;&#20256;&#25773;&#32622;&#20449;&#24230;&#23454;&#29616;&#20102;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#65292;&#26631;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#23384;&#22312;&#30528;&#25345;&#20037;&#30340;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#38656;&#35201;&#23545;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#20570;&#20986;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20808;&#39564;&#30340;&#36873;&#25321;&#65292;&#24182;&#19988;&#21518;&#39564;&#21482;&#33021;&#20197;&#36817;&#20284;&#30340;&#26041;&#24335;&#36827;&#34892;&#37319;&#26679;&#65292;&#20174;&#32780;&#30001;&#20110;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#36817;&#20284;&#31934;&#24230;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#23545;&#30830;&#23450;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#12290;&#21363;&#65292;&#22312;&#25972;&#20010;&#21306;&#22495;&#20013;&#20197;&#27010;&#29575;&#25285;&#20445;&#30340;&#24418;&#24335;&#20256;&#25773;&#32622;&#20449;&#24230;&#65292;&#20197;&#36798;&#21040;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#20449;&#24687;&#32622;&#20449;&#20256;&#25773;&#65288;PICProp&#65289;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#26469;&#35745;&#31639;&#19968;&#20010;&#26377;&#25928;&#30340;CI&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#23450;&#29702;&#20197;&#21450;&#38024;&#23545;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#35745;&#31639;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. Indicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost. This paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem. That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees. We propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions. We provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#65292;&#21033;&#29992;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.06918</link><description>&lt;p&gt;
&#29992;&#32858;&#28966;-&#20449;&#24687;&#29109;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE. (arXiv:2310.06918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#65292;&#21033;&#29992;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SimCSE&#30340;&#26368;&#26032;&#25104;&#21151;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#21477;&#23376;&#34920;&#31034;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;SimCSE&#30340;&#21407;&#22987;&#34920;&#36798;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#20013;&#30828;&#36127;&#26679;&#26412;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;SimCSE&#19982;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#22312;&#23545;&#27604;&#30446;&#26631;&#20013;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#35843;&#33410;&#39033;&#65292;&#38477;&#20302;&#19982;&#26131;&#36127;&#26679;&#26412;&#30456;&#20851;&#30340;&#25439;&#22833;&#65292;&#24182;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#20110;&#22256;&#38590;&#36127;&#26679;&#26412;&#12290;&#22312;&#21508;&#31181;STS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#12289;&#34920;&#31034;&#23545;&#40784;&#24615;&#21644;&#19968;&#33268;&#24615;&#26041;&#38754;&#25913;&#36827;&#20102;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that combines SimCSE with hard negative mining, aiming to enhance the quality of sentence embeddings. The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives. Experimentation on various STS benchmarks shows that our method improves sentence embeddings in terms of Spearman's correlation and representation alignment and uniformity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#31532;&#22235;&#20195;&#33521;&#29305;&#23572;&#38887;&#24615;&#22788;&#29702;&#22120;&#36827;&#34892;&#20998;&#24067;&#24335;&#36801;&#31227;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33521;&#29305;&#23572;&#39640;&#32423;&#30697;&#38453;&#25193;&#23637;&#65288;AMX&#65289;&#21644;Horovod&#22312;Image Classification TensorFlow&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06916</link><description>&lt;p&gt;
&#21033;&#29992;&#31532;&#22235;&#20195;&#33521;&#29305;&#23572;&#38887;&#24615;&#22788;&#29702;&#22120;&#36827;&#34892;&#20998;&#24067;&#24335;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributed Transfer Learning with 4th Gen Intel Xeon Processors. (arXiv:2310.06916v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#31532;&#22235;&#20195;&#33521;&#29305;&#23572;&#38887;&#24615;&#22788;&#29702;&#22120;&#36827;&#34892;&#20998;&#24067;&#24335;&#36801;&#31227;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33521;&#29305;&#23572;&#39640;&#32423;&#30697;&#38453;&#25193;&#23637;&#65288;AMX&#65289;&#21644;Horovod&#22312;Image Classification TensorFlow&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33521;&#29305;&#23572;&#38887;&#24615;&#22788;&#29702;&#22120;&#65292;&#29305;&#21035;&#26159;&#31532;&#22235;&#20195;&#33521;&#29305;&#23572;&#38887;&#24615;&#21487;&#25193;&#23637;&#22788;&#29702;&#22120;&#65292;&#25171;&#30772;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#21363;&#35757;&#32451;&#20027;&#35201;&#20381;&#36182;&#20110;GPU&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#33521;&#29305;&#23572;&#39640;&#32423;&#30697;&#38453;&#25193;&#23637;&#65288;AMX&#65289;&#21644;Horovod&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;Image Classification TensorFlow&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore how transfer learning, coupled with Intel Xeon, specifically 4th Gen Intel Xeon scalable processor, defies the conventional belief that training is primarily GPU-dependent. We present a case study where we achieved near state-of-the-art accuracy for image classification on a publicly available Image Classification TensorFlow dataset using Intel Advanced Matrix Extensions(AMX) and distributed training with Horovod.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#27491;&#21017;&#21270;&#21644;&#29289;&#29702;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33258;&#21160;&#27169;&#22411;&#21457;&#29616;&#65292;&#25506;&#32034;&#20102;&#38750;&#32447;&#24615;&#22238;&#24402;&#20013;&#31232;&#30095;&#22238;&#24402;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#28508;&#21147;&#65292;&#24182;&#38024;&#23545;&#26448;&#26009;&#24314;&#27169;&#25552;&#20986;&#20102;&#24120;&#35265;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.06872</link><description>&lt;p&gt;
&#20851;&#20110;&#31232;&#30095;&#22238;&#24402;&#12289;Lp&#27491;&#21017;&#21270;&#21644;&#33258;&#21160;&#27169;&#22411;&#21457;&#29616;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On sparse regression, Lp-regularization, and automated model discovery. (arXiv:2310.06872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06872
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#27491;&#21017;&#21270;&#21644;&#29289;&#29702;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33258;&#21160;&#27169;&#22411;&#21457;&#29616;&#65292;&#25506;&#32034;&#20102;&#38750;&#32447;&#24615;&#22238;&#24402;&#20013;&#31232;&#30095;&#22238;&#24402;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#28508;&#21147;&#65292;&#24182;&#38024;&#23545;&#26448;&#26009;&#24314;&#27169;&#25552;&#20986;&#20102;&#24120;&#35265;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#22238;&#24402;&#21644;&#29305;&#24449;&#25552;&#21462;&#26159;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#36827;&#34892;&#30693;&#35782;&#21457;&#29616;&#30340;&#22522;&#30784;&#12290;&#23427;&#20204;&#30340;&#30446;&#26631;&#26159;&#21457;&#29616;&#33021;&#22815;&#25552;&#20379;&#31185;&#23398;&#21464;&#37327;&#20043;&#38388;&#31616;&#21333;&#20851;&#31995;&#30340;&#21487;&#35299;&#37322;&#21644;&#39044;&#27979;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#32447;&#24615;&#22238;&#24402;&#30340;&#27169;&#22411;&#21457;&#29616;&#26041;&#38754;&#30340;&#32479;&#35745;&#24037;&#20855;&#24050;&#32463;&#24471;&#21040;&#24456;&#22909;&#22320;&#24314;&#31435;&#65292;&#20294;&#22312;&#26448;&#26009;&#24314;&#27169;&#20013;&#23558;&#20854;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#22238;&#24402;&#26159;&#39640;&#24230;&#38382;&#39064;&#29305;&#23450;&#19988;&#29702;&#35299;&#19981;&#22815;&#20805;&#20998;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#27169;&#22411;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#24341;&#20837;&#31232;&#30095;&#24615;&#65306;&#27491;&#21017;&#21270;&#21644;&#29289;&#29702;&#32422;&#26463;&#12290;&#25105;&#20204;&#23558;Lp&#27491;&#21017;&#21270;&#30340;&#27010;&#24565;&#19982;&#22522;&#20110;&#26412;&#39046;&#22495;&#22312;&#36816;&#21160;&#23398;&#21644;&#28909;&#21147;&#23398;&#26041;&#38754;&#30340;&#30693;&#35782;&#30340;&#26500;&#25104;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#23545;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36827;&#34892;&#20102;&#20960;&#21315;&#27425;&#21457;&#29616;&#36816;&#34892;&#26469;&#25512;&#26029;&#20986;&#20849;&#21516;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#36235;&#21183;&#65306;L2&#27491;&#21017;&#21270;&#25110;&#23725;&#22238;&#24402;&#26159;&#26368;&#20339;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse regression and feature extraction are the cornerstones of knowledge discovery from massive data. Their goal is to discover interpretable and predictive models that provide simple relationships among scientific variables. While the statistical tools for model discovery are well established in the context of linear regression, their generalization to nonlinear regression in material modeling is highly problem-specific and insufficiently understood. Here we explore the potential of neural networks for automatic model discovery and induce sparsity by a hybrid approach that combines two strategies: regularization and physical constraints. We integrate the concept of Lp regularization for subset selection with constitutive neural networks that leverage our domain knowledge in kinematics and thermodynamics. We train our networks with both, synthetic and real data, and perform several thousand discovery runs to infer common guidelines and trends: L2 regularization or ridge regression is
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#26102;H.264&#25511;&#21046;&#22120;&#65292;&#21033;&#29992;&#21363;&#26102;&#36890;&#36947;&#36136;&#37327;&#25968;&#25454;&#21644;&#35270;&#39057;&#22359;&#21160;&#24577;&#20272;&#35745;&#26368;&#20248;&#32534;&#30721;&#22120;&#21442;&#25968;&#65292;&#20197;&#23454;&#26102;&#24310;&#36831;&#21487;&#24573;&#30053;&#30340;&#26041;&#24335;&#32500;&#25345;&#32534;&#30721;&#35270;&#39057;&#27604;&#29305;&#29575;&#30053;&#20302;&#20110;&#21487;&#29992;&#36890;&#36947;&#27604;&#29305;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.06857</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#32447;&#32593;&#32476;&#23454;&#26102;&#27969;&#23186;&#20307;&#36895;&#29575;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Real-Time Rate Control for Live Streaming on Wireless Networks. (arXiv:2310.06857v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06857
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#26102;H.264&#25511;&#21046;&#22120;&#65292;&#21033;&#29992;&#21363;&#26102;&#36890;&#36947;&#36136;&#37327;&#25968;&#25454;&#21644;&#35270;&#39057;&#22359;&#21160;&#24577;&#20272;&#35745;&#26368;&#20248;&#32534;&#30721;&#22120;&#21442;&#25968;&#65292;&#20197;&#23454;&#26102;&#24310;&#36831;&#21487;&#24573;&#30053;&#30340;&#26041;&#24335;&#32500;&#25345;&#32534;&#30721;&#35270;&#39057;&#27604;&#29305;&#29575;&#30053;&#20302;&#20110;&#21487;&#29992;&#36890;&#36947;&#27604;&#29305;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#39640;&#36136;&#37327;&#35270;&#39057;&#20869;&#23481;&#32473;&#26080;&#32447;&#29992;&#25143;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21160;&#24577;&#35270;&#39057;&#20869;&#23481;&#21644;&#26080;&#32447;&#34928;&#33853;&#25928;&#24212;&#24341;&#36215;&#30340;&#27874;&#21160;&#36890;&#36947;&#36895;&#29575;&#65292;&#30830;&#20445;&#19968;&#33268;&#30340;&#35270;&#39057;&#36136;&#37327;&#38754;&#20020;&#25361;&#25112;&#12290;&#27425;&#20248;&#30340;&#32534;&#30721;&#22120;&#21442;&#25968;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#35270;&#39057;&#36136;&#37327;&#25439;&#22833;&#65292;&#22240;&#20026;&#24102;&#23485;&#21033;&#29992;&#19981;&#36275;&#25110;&#30001;&#20110;&#21253;&#20002;&#22833;&#24341;&#20837;&#35270;&#39057;&#20266;&#24433;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#26102;H.264&#25511;&#21046;&#22120;&#12290;&#35813;&#25511;&#21046;&#22120;&#21033;&#29992;&#20174;&#29289;&#29702;&#23618;&#33719;&#24471;&#30340;&#21363;&#26102;&#36890;&#36947;&#36136;&#37327;&#25968;&#25454;&#21644;&#35270;&#39057;&#22359;&#21160;&#24577;&#20272;&#35745;&#26368;&#20248;&#32534;&#30721;&#22120;&#21442;&#25968;&#65292;&#23454;&#26102;&#24310;&#36831;&#21487;&#24573;&#30053;&#12290;&#20854;&#30446;&#26631;&#26159;&#32500;&#25345;&#32534;&#30721;&#35270;&#39057;&#27604;&#29305;&#29575;&#30053;&#20302;&#20110;&#21487;&#29992;&#36890;&#36947;&#27604;&#29305;&#29575;&#12290;&#22312;QCIF&#25968;&#25454;&#38598;&#21644;&#26469;&#33258;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#38543;&#26426;&#35270;&#39057;&#30340;&#22810;&#26679;&#36873;&#25321;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing wireless users with high-quality video content has become increasingly important. However, ensuring consistent video quality poses challenges due to variable encoded bitrate caused by dynamic video content and fluctuating channel bitrate caused by wireless fading effects. Suboptimal selection of encoder parameters can lead to video quality loss due to underutilized bandwidth or the introduction of video artifacts due to packet loss. To address this, a real-time deep learning based H.264 controller is proposed. This controller leverages instantaneous channel quality data driven from the physical layer, along with the video chunk, to dynamically estimate the optimal encoder parameters with a negligible delay in real-time. The objective is to maintain an encoded video bitrate slightly below the available channel bitrate. Experimental results, conducted on both QCIF dataset and a diverse selection of random videos from public datasets, validate the effectiveness of the approach. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#20013;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;GABAttack&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20248;&#21270;&#21518;&#38376;&#35302;&#21457;&#22120;&#21442;&#25968;&#30340;&#20540;&#21644;&#20301;&#32622;&#65292;&#20197;&#23454;&#29616;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#31713;&#25913;&#12290;</title><link>http://arxiv.org/abs/2310.06855</link><description>&lt;p&gt;
&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#23545;&#20110;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification. (arXiv:2310.06855v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#20013;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;GABAttack&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20248;&#21270;&#21518;&#38376;&#35302;&#21457;&#22120;&#21442;&#25968;&#30340;&#20540;&#21644;&#20301;&#32622;&#65292;&#20197;&#23454;&#29616;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#31713;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#20849;&#21516;&#20026;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#32452;&#32455;&#30340;&#20840;&#23616;&#27169;&#22411;&#30340;&#23398;&#20064;&#20570;&#20986;&#36129;&#29486;&#12290;&#36825;&#31181;&#23398;&#20064;&#26041;&#26696;&#20419;&#36827;&#20102;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#38544;&#31169;&#24182;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#31561;&#24212;&#29992;&#20013;&#65292;&#36825;&#26377;&#21161;&#20110;&#38544;&#34255;&#32593;&#32476;&#30340;&#28431;&#27934;&#21644;&#24369;&#28857;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#23545;&#21518;&#38376;&#25915;&#20987;&#26159;&#33030;&#24369;&#30340;&#65292;&#21363;&#23545;&#25163;&#21521;&#20840;&#23616;&#27169;&#22411;&#20013;&#27880;&#20837;&#31713;&#25913;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;&#36825;&#20123;&#26356;&#26032;&#22312;&#20840;&#23616;&#27169;&#22411;&#20013;&#27880;&#20837;&#20102;&#26174;&#33879;&#30340;&#21151;&#33021;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#30340;&#36755;&#20837;&#27169;&#24335;&#26469;&#21551;&#21160;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#27169;&#22411;&#23545;&#20110;&#36825;&#20123;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GABAttack&#65292;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#38024;&#23545;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;GABAttack&#21033;&#29992;&#36951;&#20256;&#31639;&#27861;&#26469;&#20248;&#21270;&#21518;&#38376;&#35302;&#21457;&#22120;&#21442;&#25968;&#30340;&#20540;&#21644;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enables multiple clients to collaboratively contribute to the learning of a global model orchestrated by a central server. This learning scheme promotes clients' data privacy and requires reduced communication overheads. In an application like network traffic classification, this helps hide the network vulnerabilities and weakness points. However, federated learning is susceptible to backdoor attacks, in which adversaries inject manipulated model updates into the global model. These updates inject a salient functionality in the global model that can be launched with specific input patterns. Nonetheless, the vulnerability of network traffic classification models based on federated learning to these attacks remains unexplored. In this paper, we propose GABAttack, a novel genetic algorithm-based backdoor attack against federated learning for network traffic classification. GABAttack utilizes a genetic algorithm to optimize the values and locations of backdoor trigger pa
&lt;/p&gt;</description></item><item><title>DeepCrysTet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;3D&#22235;&#38754;&#20307;&#32593;&#26684;&#34920;&#31034;&#26230;&#20307;&#32467;&#26500;&#65292;&#39044;&#27979;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2310.06852</link><description>&lt;p&gt;
DeepCrysTet: &#20351;&#29992;&#22235;&#38754;&#20307;&#32593;&#26684;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#26230;&#20307;&#26448;&#26009;&#24615;&#36136;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepCrysTet: A Deep Learning Approach Using Tetrahedral Mesh for Predicting Properties of Crystalline Materials. (arXiv:2310.06852v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06852
&lt;/p&gt;
&lt;p&gt;
DeepCrysTet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;3D&#22235;&#38754;&#20307;&#32593;&#26684;&#34920;&#31034;&#26230;&#20307;&#32467;&#26500;&#65292;&#39044;&#27979;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#39044;&#27979;&#26448;&#26009;&#24615;&#36136;&#20197;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#30001;&#20110;&#26448;&#26009;&#24615;&#36136;&#21463;&#26230;&#20307;&#32467;&#26500;&#30340;&#24378;&#28872;&#24433;&#21709;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#23558;&#26230;&#20307;&#32467;&#26500;&#36716;&#21270;&#20026;&#29992;&#20110;&#36755;&#20837;ML&#27169;&#22411;&#30340;&#29305;&#24449;&#12290;&#30446;&#21069;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#26230;&#20307;&#32467;&#26500;&#36716;&#21270;&#20026;&#22270;&#24418;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#39044;&#27979;&#20854;&#24615;&#36136;&#12290;&#19968;&#20123;GNN&#27169;&#22411;&#65292;&#22914;&#26230;&#20307;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CGCNN&#65289;&#21644;&#21407;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ALIGNN&#65289;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#39640;&#24230;&#20934;&#30830;&#30340;&#26448;&#26009;&#24615;&#36136;&#39044;&#27979;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#25104;&#21151;&#65292;&#20351;&#29992;&#22270;&#24418;&#26469;&#34920;&#31034;&#26230;&#20307;&#32467;&#26500;&#23384;&#22312;&#26126;&#26174;&#30340;&#38480;&#21046;&#65292;&#21363;&#20002;&#22833;&#26230;&#20307;&#32467;&#26500;&#30340;&#19977;&#32500;&#65288;3D&#65289;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepCrysTet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26448;&#26009;&#24615;&#36136;&#65292;&#23427;&#20351;&#29992;&#30001;Delaunay&#19977;&#35282;&#21078;&#20998;&#29983;&#25104;&#30340;3D&#22235;&#38754;&#20307;&#32593;&#26684;&#34920;&#31034;&#26230;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) is becoming increasingly popular for predicting material properties to accelerate materials discovery. Because material properties are strongly affected by its crystal structure, a key issue is converting the crystal structure into the features for input to the ML model. Currently, the most common method is to convert the crystal structure into a graph and predicting its properties using a graph neural network (GNN). Some GNN models, such as crystal graph convolutional neural network (CGCNN) and atomistic line graph neural network (ALIGNN), have achieved highly accurate predictions of material properties. Despite these successes, using a graph to represent a crystal structure has the notable limitation of losing the crystal structure's three-dimensional (3D) information. In this work, we propose DeepCrysTet, a novel deep learning approach for predicting material properties, which uses crystal structures represented as a 3D tetrahedral mesh generated by Delaunay te
&lt;/p&gt;</description></item><item><title>DeepTriNet&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32423;&#27880;&#24847;&#21147;&#30340;DeepLabv3+&#26550;&#26500;&#65292;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;SENets&#21644;TAUs&#26725;&#25509;&#20102;&#32534;&#35299;&#30721;&#22120;&#36755;&#20986;&#19982;&#30456;&#20851;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#21516;&#26102;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30830;&#23450;&#20102;&#26356;&#37325;&#35201;&#21644;&#26356;&#36890;&#29992;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.06848</link><description>&lt;p&gt;
DeepTriNet:&#19968;&#31181;&#22522;&#20110;&#19977;&#32423;&#27880;&#24847;&#21147;&#30340;DeepLabv3+&#32467;&#26500;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DeepTriNet: A Tri-Level Attention Based DeepLabv3+ Architecture for Semantic Segmentation of Satellite Images. (arXiv:2310.06848v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06848
&lt;/p&gt;
&lt;p&gt;
DeepTriNet&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32423;&#27880;&#24847;&#21147;&#30340;DeepLabv3+&#26550;&#26500;&#65292;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;SENets&#21644;TAUs&#26725;&#25509;&#20102;&#32534;&#35299;&#30721;&#22120;&#36755;&#20986;&#19982;&#30456;&#20851;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#21516;&#26102;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30830;&#23450;&#20102;&#26356;&#37325;&#35201;&#21644;&#26356;&#36890;&#29992;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#22270;&#20687;&#30340;&#20998;&#21106;&#22312;&#36965;&#24863;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#21355;&#26143;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#38754;&#20020;&#35782;&#21035;&#23567;&#23610;&#24230;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#24573;&#30053;&#20102;&#24213;&#23618;&#32593;&#32476;&#30340;&#20302;&#32423;&#29305;&#24449;&#21644;&#30001;&#19981;&#21516;&#29305;&#24449;&#22270;&#21253;&#21547;&#19981;&#21516;&#25968;&#37327;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19977;&#32423;&#27880;&#24847;&#21147;&#30340;DeepLabv3+&#26550;&#26500;&#65288;DeepTriNet&#65289;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#26041;&#27861;&#32467;&#21512;&#20102;&#25380;&#21387;&#28608;&#21169;&#32593;&#32476;&#65288;SENets&#65289;&#21644;&#19977;&#32423;&#27880;&#24847;&#21147;&#21333;&#20803;&#65288;TAUs&#65289;&#19982;&#26222;&#36890;&#30340;DeepLabv3+&#26550;&#26500;&#65292;&#20854;&#20013;TAUs&#29992;&#20110;&#24357;&#21512;&#32534;&#35299;&#30721;&#22120;&#36755;&#20986;&#19982;SENets&#29992;&#20110;&#32473;&#30456;&#20851;&#29305;&#24449;&#20998;&#37197;&#26356;&#22810;&#26435;&#37325;&#20043;&#38388;&#30340;&#35821;&#20041;&#29305;&#24449;&#24046;&#36317;&#12290;&#25152;&#25552;&#20986;&#30340;DeepTriNet&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#25214;&#21040;&#21738;&#20123;&#29305;&#24449;&#26159;&#26356;&#30456;&#20851;&#19988;&#26356;&#36890;&#29992;&#30340;&#65292;&#32780;&#19981;&#26159;&#25105;&#20204;&#23545;&#23427;&#20204;&#36827;&#34892;&#27880;&#37322;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;DeepTriNet&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The segmentation of satellite images is crucial in remote sensing applications. Existing methods face challenges in recognizing small-scale objects in satellite images for semantic segmentation primarily due to ignoring the low-level characteristics of the underlying network and due to containing distinct amounts of information by different feature maps. Thus, in this research, a tri-level attention-based DeepLabv3+ architecture (DeepTriNet) is proposed for the semantic segmentation of satellite images. The proposed hybrid method combines squeeze-and-excitation networks (SENets) and tri-level attention units (TAUs) with the vanilla DeepLabv3+ architecture, where the TAUs are used to bridge the semantic feature gap among encoders output and the SENets used to put more weight on relevant features. The proposed DeepTriNet finds which features are the more relevant and more generalized way by its self-supervision rather we annotate them. The study showed that the proposed DeepTriNet perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;EfficientNet&#32972;&#39592;&#30340;U-Net++&#26550;&#26500;&#65292;&#36890;&#36807;&#28145;&#24230;&#30417;&#30563;&#21644;&#37325;&#35774;&#35745;&#30340;&#36339;&#36291;&#36830;&#25509;&#65292;&#25552;&#39640;&#20102;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#24314;&#31569;&#29289;&#25552;&#21462;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.06847</link><description>&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#22522;&#20110;EfficientNet&#30340;U-Net++&#26550;&#26500;&#29992;&#20110;&#33258;&#21160;&#24314;&#31569;&#29289;&#25552;&#21462;&#30340;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Performance Analysis of Various EfficientNet Based U-Net++ Architecture for Automatic Building Extraction from High Resolution Satellite Images. (arXiv:2310.06847v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;EfficientNet&#32972;&#39592;&#30340;U-Net++&#26550;&#26500;&#65292;&#36890;&#36807;&#28145;&#24230;&#30417;&#30563;&#21644;&#37325;&#35774;&#35745;&#30340;&#36339;&#36291;&#36830;&#25509;&#65292;&#25552;&#39640;&#20102;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#24314;&#31569;&#29289;&#25552;&#21462;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#29289;&#25552;&#21462;&#26159;&#36965;&#24863;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24314;&#31569;&#29289;&#25552;&#21462;&#30340;&#24212;&#29992;&#20005;&#37325;&#20381;&#36182;&#20110;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#23384;&#22312;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#24046;&#36317;&#38480;&#21046;&#65292;&#21487;&#33021;&#23548;&#33268;&#20998;&#21106;&#32467;&#26524;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#20197;&#39640;&#20934;&#30830;&#29575;&#25552;&#21462;&#24314;&#31569;&#29289;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;EfficientNet&#32972;&#39592;&#30340;&#22810;&#31181;U-Net++&#32467;&#26500;&#12290;&#35813;&#35774;&#35745;&#30340;&#32593;&#32476;&#22522;&#20110;U-Net&#65292;&#22312;&#28145;&#24230;&#30417;&#30563;&#12289;&#24222;&#22823;&#30340;&#37325;&#35774;&#35745;&#30340;&#36339;&#36291;&#36830;&#25509;&#31561;&#26041;&#38754;&#25552;&#39640;&#27169;&#22411;&#30340;&#28789;&#25935;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#32972;&#26223;&#20013;&#19982;&#30456;&#20851;&#29305;&#24449;&#21306;&#22495;&#26080;&#20851;&#30340;&#24433;&#21709;&#12290;&#22312;&#35757;&#32451;&#32593;&#32476;&#26102;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;EfficientNet&#32972;&#39592;&#32534;&#30721;&#22120;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#25552;&#21462;&#26356;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#24314;&#35758;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#23574;&#31471;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building extraction is an essential component of study in the science of remote sensing, and applications for building extraction heavily rely on semantic segmentation of high-resolution remote sensing imagery. Semantic information extraction gap constraints in the present deep learning based approaches, however can result in inadequate segmentation outcomes. To address this issue and extract buildings with high accuracy, various efficientNet backbone based U-Net++ has been proposed in this study. The designed network, based on U-Net, can improve the sensitivity of the model by deep supervision, voluminous redesigned skip-connections and hence reducing the influence of irrelevant feature areas in the background. Various effecientNet backbone based encoders have been employed when training the network to enhance the capacity of the model to extract more relevant feature. According on the experimental findings, the suggested model significantly outperforms previous cutting-edge approache
&lt;/p&gt;</description></item><item><title>RobustEdge&#26159;&#19968;&#31181;&#20302;&#21151;&#32791;&#12289;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#20113;&#36793;&#31995;&#32479;&#20013;&#33021;&#37327;&#28010;&#36153;&#21644;&#35745;&#31639;&#24320;&#38144;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06845</link><description>&lt;p&gt;
RobustEdge&#65306;&#38754;&#21521;&#20113;&#36793;&#31995;&#32479;&#30340;&#20302;&#21151;&#32791;&#23545;&#25239;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RobustEdge: Low Power Adversarial Detection for Cloud-Edge Systems. (arXiv:2310.06845v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06845
&lt;/p&gt;
&lt;p&gt;
RobustEdge&#26159;&#19968;&#31181;&#20302;&#21151;&#32791;&#12289;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#20113;&#36793;&#31995;&#32479;&#20013;&#33021;&#37327;&#28010;&#36153;&#21644;&#35745;&#31639;&#24320;&#38144;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#20113;&#36793;&#22330;&#26223;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#36827;&#34892;&#25968;&#25454;&#37319;&#38598;&#65292;&#25317;&#26377;&#36275;&#22815;&#36164;&#28304;&#30340;&#20113;&#31995;&#32479;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#65292;&#23545;&#25239;&#40065;&#26834;&#24615;&#23545;&#20110;&#21487;&#38752;&#24615;&#21644;&#26222;&#21450;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#25239;&#26816;&#27979;&#26159;&#20808;&#21069;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#20027;&#35201;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#20808;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#20013;&#65292;&#26816;&#27979;&#22120;&#38468;&#21152;&#22312;&#20998;&#31867;&#22120;&#27169;&#22411;&#19978;&#65292;&#26816;&#27979;&#22120;&#21644;&#20998;&#31867;&#22120;&#19968;&#36215;&#36827;&#34892;&#23545;&#25239;&#26816;&#27979;&#65292;&#36825;&#38656;&#35201;&#24456;&#39640;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22312;&#20302;&#21151;&#32791;&#36793;&#32536;&#35774;&#22791;&#19978;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#21482;&#33021;&#22312;&#20113;&#31471;&#36827;&#34892;&#23545;&#25239;&#26816;&#27979;&#65292;&#32780;&#19981;&#33021;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#36973;&#21463;&#23545;&#25239;&#25915;&#20987;&#26102;&#65292;&#19981;&#21033;&#30340;&#23545;&#25239;&#26679;&#26412;&#24517;&#39035;&#20256;&#36865;&#21040;&#20113;&#31471;&#65292;&#23548;&#33268;&#36793;&#32536;&#35774;&#22791;&#30340;&#33021;&#37327;&#28010;&#36153;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#20302;&#21151;&#32791;&#12289;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical cloud-edge scenarios, where a resource constrained edge performs data acquisition and a cloud system (having sufficient resources) performs inference tasks with a deep neural network (DNN), adversarial robustness is critical for reliability and ubiquitous deployment. Adversarial detection is a prime adversarial defence technique used in prior literature. However, in prior detection works, the detector is attached to the classifier model and both detector and classifier work in tandem to perform adversarial detection that requires a high computational overhead which is not available at the low-power edge. Therefore, prior works can only perform adversarial detection at the cloud and not at the edge. This means that in case of adversarial attacks, the unfavourable adversarial samples must be communicated to the cloud which leads to energy wastage at the edge device. Therefore, a low-power edge-friendly adversarial detection method is required to improve the energy efficiency
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;DNNs&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#19978;&#20855;&#26377;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#21644;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#33021;&#21147;&#30340;&#26041;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#25512;&#36827;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25216;&#26415;&#21644;&#25552;&#21319;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.06841</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#65306;&#24615;&#33021;&#35780;&#20272;&#21644;&#22312;&#36793;&#32536;&#35774;&#22791;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Malware Classification using Deep Neural Networks: Performance Evaluation and Applications in Edge Devices. (arXiv:2310.06841v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06841
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;DNNs&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#19978;&#20855;&#26377;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#21644;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#33021;&#21147;&#30340;&#26041;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#25512;&#36827;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25216;&#26415;&#21644;&#25552;&#21319;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#24694;&#24847;&#36719;&#20214;&#30340;&#26816;&#27979;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#24403;&#21069;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#30340;&#33539;&#22260;&#19981;&#26029;&#25193;&#22823;&#65292;&#26377;&#24517;&#35201;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;&#21487;&#20197;&#35774;&#35745;&#21644;&#35757;&#32451;&#22810;&#20010;DNN&#26550;&#26500;&#20197;&#26816;&#27979;&#21644;&#20998;&#31867;&#24694;&#24847;&#36719;&#20214;&#20108;&#36827;&#21046;&#25991;&#20214;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;DNNs&#22312;&#20934;&#30830;&#20998;&#31867;&#24694;&#24847;&#36719;&#20214;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#24694;&#24847;&#36719;&#20214;&#35266;&#23519;&#21040;&#39640;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#23558;&#36825;&#20123;DNN&#27169;&#22411;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#20197;&#23454;&#29616;&#23454;&#26102;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#22823;&#22411;&#29289;&#32852;&#32593;&#31995;&#32479;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#21644;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#33021;&#21147;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#25512;&#36827;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25216;&#26415;&#20855;&#26377;&#36129;&#29486;&#65292;&#24182;&#24378;&#35843;&#20102;&#23558;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#25972;&#21512;&#21040;&#26089;&#26399;&#26816;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing extent of malware attacks in the present day along with the difficulty in detecting modern malware, it is necessary to evaluate the effectiveness and performance of Deep Neural Networks (DNNs) for malware classification. Multiple DNN architectures can be designed and trained to detect and classify malware binaries. Results demonstrate the potential of DNNs in accurately classifying malware with high accuracy rates observed across different malware types. Additionally, the feasibility of deploying these DNN models on edge devices to enable real-time classification, particularly in resource-constrained scenarios proves to be integral to large IoT systems. By optimizing model architectures and leveraging edge computing capabilities, the proposed methodologies achieve efficient performance even with limited resources. This study contributes to advancing malware detection techniques and emphasizes the significance of integrating cybersecurity measures for the early detec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#36229;&#32500;&#35745;&#31639;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#23454;&#29616;&#35745;&#31639;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#20445;&#23494;&#24615;&#65292;&#20026;&#39640;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.06840</link><description>&lt;p&gt;
&#36229;&#32500;&#35745;&#31639;&#20316;&#20026;&#39640;&#25928;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#30340;&#25937;&#25588;
&lt;/p&gt;
&lt;p&gt;
Hyperdimensional Computing as a Rescue for Efficient Privacy-Preserving Machine Learning-as-a-Service. (arXiv:2310.06840v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36229;&#32500;&#35745;&#31639;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#23454;&#29616;&#35745;&#31639;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#20445;&#23494;&#24615;&#65292;&#20026;&#39640;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20316;&#20026;&#20113;&#26381;&#21153;&#25552;&#20379;&#65292;&#23458;&#25143;&#23558;&#33258;&#24049;&#30340;&#25968;&#25454;&#21457;&#36865;&#32473;&#26381;&#21153;&#25552;&#20379;&#21830;&#20197;&#33719;&#21462;&#32467;&#26524;&#12290;&#36825;&#31181;&#35774;&#32622;&#26159;&#24120;&#35265;&#30340;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#39640;&#20215;&#20540;&#65292;&#20294;&#23427;&#35201;&#27714;&#23458;&#25143;&#25918;&#24323;&#26597;&#35810;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#30340;&#38544;&#31169;&#12290;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#26159;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#12290;&#20351;&#29992;HE&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#21487;&#20197;&#25509;&#25910;&#21152;&#23494;&#30340;&#25968;&#25454;&#20316;&#20026;&#26597;&#35810;&#65292;&#24182;&#36816;&#34892;&#27169;&#22411;&#32780;&#26080;&#38656;&#35299;&#23494;&#12290;&#32467;&#26524;&#20445;&#25345;&#21152;&#23494;&#65292;&#21482;&#26377;&#23458;&#25143;&#21487;&#20197;&#35299;&#23494;&#12290;&#25152;&#26377;&#36825;&#20123;&#22909;&#22788;&#24102;&#26469;&#30340;&#20195;&#20215;&#26159;&#35745;&#31639;&#25104;&#26412;&#65292;&#22240;&#20026;HE&#23558;&#31616;&#21333;&#30340;&#28014;&#28857;&#36816;&#31639;&#36716;&#21270;&#20026;&#38271;&#65288;&#24230;&#36229;&#36807;1024&#65289;&#22810;&#39033;&#24335;&#20043;&#38388;&#30340;&#35745;&#31639;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#20102;&#20026;&#22312;&#21152;&#23494;&#25968;&#25454;&#19978;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;HE&#21448;&#20877;&#27425;&#25918;&#22823;&#20102;&#24050;&#32463;&#24456;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#38459;&#30861;&#20102;&#24615;&#33021;&#25913;&#36827;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36229;&#32500;&#35745;&#31639;&#21487;&#20197;&#20316;&#20026;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#30340;&#25937;&#25588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are often provisioned as a cloud-based service where the clients send their data to the service provider to obtain the result. This setting is commonplace due to the high value of the models, but it requires the clients to forfeit the privacy that the query data may contain. Homomorphic encryption (HE) is a promising technique to address this adversity. With HE, the service provider can take encrypted data as a query and run the model without decrypting it. The result remains encrypted, and only the client can decrypt it. All these benefits come at the cost of computational cost because HE turns simple floating-point arithmetic into the computation between long (degree over 1024) polynomials. Previous work has proposed to tailor deep neural networks for efficient computation over encrypted data, but already high computational cost is again amplified by HE, hindering performance improvement. In this paper we show hyperdimensional computing can be a rescue for pri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21046;&#36896;&#19994;&#30340;AI&#23413;&#21270;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26356;&#26032;AI&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#25345;&#32493;&#25913;&#36827;&#20915;&#31574;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;CBEAL&#30340;&#38598;&#25104;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22320;&#25351;&#23548;&#25968;&#25454;&#33719;&#21462;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#26524;&#30340;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.06306</link><description>&lt;p&gt;
&#38754;&#21521;&#21046;&#36896;&#19994;&#30340;AI&#23413;&#21270;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing. (arXiv:2310.06306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21046;&#36896;&#19994;&#30340;AI&#23413;&#21270;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26356;&#26032;AI&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#25345;&#32493;&#25913;&#36827;&#20915;&#31574;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;CBEAL&#30340;&#38598;&#25104;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22320;&#25351;&#23548;&#25968;&#25454;&#33719;&#21462;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#26524;&#30340;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#24863;&#30693;&#21644;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#22522;&#20110;AI&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#22914;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#38459;&#30861;&#20102;&#31163;&#32447;&#35757;&#32451;&#30340;AI&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;AI&#27169;&#22411;&#20250;&#36890;&#36807;&#27969;&#24335;&#25968;&#25454;&#36827;&#34892;&#22312;&#32447;&#26356;&#26032;&#20197;&#25345;&#32493;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#37322;&#32422;&#26463;&#65292;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#36873;&#25321;&#29992;&#20110;&#26356;&#26032;&#30340;&#20248;&#36136;&#27969;&#24335;&#26679;&#26412;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#25991;&#29486;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#20851;&#27880;&#19981;&#36275;&#25110;&#36807;&#24230;&#34920;&#31034;&#30340;&#21306;&#22495;&#26469;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#21046;&#36896;&#32972;&#26223;&#19979;&#24179;&#34913;&#36825;&#20123;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;AI&#23398;&#20064;&#21040;&#30340;&#19968;&#20123;&#33719;&#21462;&#20934;&#21017;&#21487;&#20197;&#21160;&#24577;&#36866;&#24212;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#22987;&#32456;&#22788;&#29702;&#39057;&#32321;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#25104;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;CBEAL&#65292;&#19987;&#38376;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;&#25506;&#32034;&#25110;&#21033;&#29992;&#12290;&#20195;&#29702;&#30340;&#26435;&#37325;&#26681;&#25454;&#20915;&#31574;&#26377;&#25928;&#24615;&#36827;&#34892;&#35843;&#25972;&#12290;CBEAL&#21487;&#20197;&#20248;&#21270;&#22320;&#25351;&#23548;&#25968;&#25454;&#33719;&#21462;&#65292;&#23454;&#29616;&#25968;&#25454;&#25928;&#26524;&#30340;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online sensing and computational resources in Industrial Cyber-physical Systems (ICPS) facilitate AI-driven decision-making. Yet, issues with data quality, such as imbalanced classes, hinder AI models trained offline. To address this, AI models are updated online with streaming data for continuous improvement. Supervised learning models, however, face challenges in selecting quality streaming samples for updates due to annotation constraints. Active learning methods in literature offer solutions by focusing on under-represented or well-represented regions. Balancing these strategies in changing manufacturing contexts is challenging. Some acquisition criteria learned by AI dynamically adapt but may not consistently handle frequent changes. We introduce an ensemble active learning method, CBEAL, employing active learning agents specifically for exploration or exploitation. Weights of agents are adjusted based on agent decision effectiveness. CBEAL optimally guides data acquisition, minim
&lt;/p&gt;</description></item><item><title>MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.06282</link><description>&lt;p&gt;
MuseChat:&#19968;&#31181;&#35270;&#39057;&#23545;&#35805;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06282
&lt;/p&gt;
&lt;p&gt;
MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MuseChat&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#20010;&#29420;&#29305;&#30340;&#24179;&#21488;&#19981;&#20165;&#25552;&#20379;&#20114;&#21160;&#29992;&#25143;&#21442;&#19982;&#65292;&#36824;&#20026;&#36755;&#20837;&#30340;&#35270;&#39057;&#25552;&#20379;&#20102;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25913;&#36827;&#21644;&#20010;&#24615;&#21270;&#20182;&#20204;&#30340;&#38899;&#20048;&#36873;&#25321;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#20197;&#21069;&#30340;&#31995;&#32479;&#20027;&#35201;&#24378;&#35843;&#20869;&#23481;&#30340;&#20860;&#23481;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#29992;&#25143;&#20010;&#20307;&#20559;&#22909;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#20363;&#22914;&#65292;&#25152;&#26377;&#30340;&#25968;&#25454;&#38598;&#37117;&#21482;&#25552;&#20379;&#22522;&#26412;&#30340;&#38899;&#20048;-&#35270;&#39057;&#37197;&#23545;&#65292;&#25110;&#32773;&#24102;&#26377;&#38899;&#20048;&#25551;&#36848;&#30340;&#37197;&#23545;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#35805;&#21512;&#25104;&#26041;&#27861;&#65292;&#27169;&#25311;&#20102;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#20004;&#36718;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#20132;&#20114;&#20013;&#65292;&#29992;&#25143;&#25552;&#20132;&#19968;&#20010;&#35270;&#39057;&#32473;&#31995;&#32479;&#65292;&#31995;&#32479;&#20250;&#25552;&#20379;&#19968;&#20010;&#21512;&#36866;&#30340;&#38899;&#20048;&#29255;&#27573;&#65292;&#24182;&#38468;&#24102;&#35299;&#37322;&#12290;&#20043;&#21518;&#65292;&#29992;&#25143;&#20250;&#34920;&#36798;&#20182;&#20204;&#23545;&#38899;&#20048;&#30340;&#20559;&#22909;&#65292;&#31995;&#32479;&#20250;&#21576;&#29616;&#19968;&#20010;&#25913;&#36827;&#21518;&#30340;&#38899;&#20048;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recomme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#20013;&#21363;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.06200</link><description>&lt;p&gt;
&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#21450;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Prompt Tuning for Automated Neuron Explanations. (arXiv:2310.06200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#20013;&#21363;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#21450;&#20854;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#24182;&#27809;&#26377;&#21516;&#27493;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#23427;&#20204;&#30340;&#20010;&#20307;&#31070;&#32463;&#20803;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;LLM&#12290;&#25105;&#20204;&#22312;&#21069;&#20154;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#22914;&#20309;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#20013;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29983;&#25104;&#35299;&#37322;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#26356;&#33258;&#28982;&#30340;&#26041;&#24335;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#25105;&#20204;&#26032;&#25552;&#31034;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances have greatly increased the capabilities of large language models (LLMs), but our understanding of the models and their safety has not progressed as fast. In this paper we aim to understand LLMs deeper by studying their individual neurons. We build upon previous work showing large language models such as GPT-4 can be useful in explaining what each neuron in a language model does. Specifically, we analyze the effect of the prompt used to generate explanations and show that reformatting the explanation prompt in a more natural way can significantly improve neuron explanation quality and greatly reduce computational cost. We demonstrate the effects of our new prompts in three different ways, incorporating both automated and human evaluations.
&lt;/p&gt;</description></item><item><title>&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;HyperAttention&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#21442;&#25968;&#26469;&#34913;&#37327;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#36731;&#26494;&#38598;&#25104;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05869</link><description>&lt;p&gt;
&#36229;&#32423;&#20851;&#27880;&#21147;&#65306;&#36817;&#20284;&#32447;&#24615;&#26102;&#38388;&#19979;&#30340;&#38271;&#19978;&#19979;&#25991;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
HyperAttention: Long-context Attention in Near-Linear Time. (arXiv:2310.05869v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05869
&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;HyperAttention&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#21442;&#25968;&#26469;&#34913;&#37327;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#36731;&#26494;&#38598;&#25104;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperAttention&#30340;&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#26085;&#30410;&#22797;&#26434;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#38500;&#38750;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#26465;&#30446;&#34987;&#38480;&#21046;&#25110;&#30697;&#38453;&#20855;&#26377;&#20302;&#31283;&#23450;&#31209;&#65292;&#21542;&#21017;&#20108;&#27425;&#26102;&#38388;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21442;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#65306;&#65288;1&#65289;&#26631;&#20934;&#21270;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#30340;&#26368;&#22823;&#21015;&#33539;&#25968;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;&#26816;&#27979;&#21644;&#21024;&#38500;&#22823;&#26465;&#30446;&#21518;&#65292;&#38750;&#26631;&#20934;&#21270;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#34892;&#33539;&#25968;&#30340;&#27604;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#32454;&#31890;&#24230;&#30340;&#21442;&#25968;&#26469;&#25429;&#25417;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;&#23613;&#31649;&#20808;&#21069;&#23384;&#22312;&#19979;&#30028;&#65292;&#20294;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#19968;&#20010;&#32447;&#24615;&#26102;&#38388;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21363;&#20351;&#30697;&#38453;&#20855;&#26377;&#26080;&#30028;&#30340;&#26465;&#30446;&#25110;&#36739;&#22823;&#30340;&#31283;&#23450;&#31209;&#65292;&#21482;&#35201;&#19978;&#36848;&#21442;&#25968;&#36739;&#23567;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#36731;&#26494;&#23481;&#32435;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#65292;&#29305;&#21035;&#26159;FlashAttention&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.05797</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20107;&#21518;&#35299;&#37322;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#21019;&#26032;&#65292;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#34429;&#28982;LLM&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#20294;&#20854;&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#35299;&#37322;&#25216;&#26415;&#65292;&#20294;&#24456;&#22810;&#25216;&#26415;&#35201;&#27714;&#23545;&#27169;&#22411;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#26435;&#38480;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#20984;&#26174;&#20102;&#19979;&#19968;&#20195;&#20107;&#21518;&#35299;&#37322;&#22120;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;LLM&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;i&#65289;&#22522;&#20110;&#25200;&#21160;&#30340;ICL&#65292;ii&#65289;&#22522;&#20110;&#39044;&#27979;&#30340;ICL&#65292;iii&#65289;&#22522;&#20110;&#25351;&#20196;&#30340;ICL&#65292;&#21644;iv&#65289;&#22522;&#20110;&#35299;&#37322;&#30340;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#36965;&#24863;&#25216;&#26415;&#20998;&#26512;&#20102;&#26031;&#37324;&#20848;&#21345;&#21644;&#36234;&#21335;&#20004;&#20010;&#28909;&#24102;&#22269;&#23478;&#30340;&#38477;&#38632;&#27169;&#24335;&#21644;&#27700;&#24211;&#27700;&#22495;&#33539;&#22260;&#30340;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#29702;&#35299;&#36965;&#24863;&#38477;&#38632;&#25968;&#25454;&#19982;&#27700;&#22495;&#33539;&#22260;&#21160;&#24577;&#21464;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.05682</link><description>&lt;p&gt;
&#20351;&#29992;Google Earth Engine (GEE)&#20998;&#26512;&#36873;&#23450;&#30340;&#20004;&#20010;&#28909;&#24102;&#22269;&#23478;&#8212;&#8212;&#26031;&#37324;&#20848;&#21345;&#21644;&#36234;&#21335;&#30340;&#38477;&#38632;&#21464;&#24322;&#24615;&#21644;&#27700;&#22495;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
Analysis of Rainfall Variability and Water Extent of Selected Hydropower Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical Countries, Sri Lanka and Vietnam. (arXiv:2310.05682v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#36965;&#24863;&#25216;&#26415;&#20998;&#26512;&#20102;&#26031;&#37324;&#20848;&#21345;&#21644;&#36234;&#21335;&#20004;&#20010;&#28909;&#24102;&#22269;&#23478;&#30340;&#38477;&#38632;&#27169;&#24335;&#21644;&#27700;&#24211;&#27700;&#22495;&#33539;&#22260;&#30340;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#29702;&#35299;&#36965;&#24863;&#38477;&#38632;&#25968;&#25454;&#19982;&#27700;&#22495;&#33539;&#22260;&#21160;&#24577;&#21464;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#36965;&#24863;&#25216;&#26415;&#23545;&#36234;&#21335;&#21644;&#26031;&#37324;&#20848;&#21345;&#20004;&#20010;&#28909;&#24102;&#23395;&#39118;&#22269;&#23478;&#30340;&#38477;&#38632;&#27169;&#24335;&#21644;&#36873;&#23450;&#30340;&#27700;&#30005;&#27700;&#24211;&#27700;&#22495;&#33539;&#22260;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#36965;&#24863;&#38477;&#38632;&#25968;&#25454;&#19982;&#27700;&#24211;&#27700;&#22495;&#33539;&#22260;&#65288;&#26376;&#24230;&#65289;&#21160;&#24577;&#21464;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20998;&#26512;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#20809;&#23398;&#24433;&#20687;&#21644;Sentinel-1&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#25968;&#25454;&#35266;&#27979;&#21644;&#30417;&#27979;&#19981;&#21516;&#22825;&#27668;&#26465;&#20214;&#19979;&#27700;&#20307;&#65292;&#29305;&#21035;&#26159;&#23395;&#39118;&#23395;&#33410;&#12290;&#30830;&#23450;&#20102;&#20004;&#22269;&#30340;&#24180;&#22343;&#38477;&#38632;&#37327;&#65292;&#24182;&#20351;&#29992;1981&#24180;&#33267;2022&#24180;&#30340;&#27668;&#20505;&#28798;&#23475;&#32452;&#32418;&#22806;&#38477;&#38632;&#25968;&#25454;&#38598;&#65288;CHIRPS&#65289;&#22312;&#21306;&#22495;&#21644;&#27700;&#24211;&#27969;&#22495;&#32423;&#21035;&#19978;&#32771;&#23519;&#20102;&#26376;&#22343;&#38477;&#38632;&#30340;&#26102;&#31354;&#21464;&#21270;&#12290;&#20351;&#29992;2017&#24180;&#33267;2022&#24180;&#30340;Sentinel-1 SAR&#22320;&#38754;&#33539;&#22260;&#26816;&#27979;&#65288;GRD&#65289;&#22270;&#20687;&#33719;&#21462;&#20102;&#36873;&#23450;&#27700;&#24211;&#30340;&#27700;&#22495;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a comprehensive remote sensing analysis of rainfall patterns and selected hydropower reservoir water extent in two tropical monsoon countries, Vietnam and Sri Lanka. The aim is to understand the relationship between remotely sensed rainfall data and the dynamic changes (monthly) in reservoir water extent. The analysis utilizes high-resolution optical imagery and Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water bodies during different weather conditions, especially during the monsoon season. The average annual rainfall for both countries is determined, and spatiotemporal variations in monthly average rainfall are examined at regional and reservoir basin levels using the Climate Hazards Group InfraRed Precipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents are derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected (GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are pre-processed an
&lt;/p&gt;</description></item><item><title>&#22810;&#27493;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#30446;&#26631;&#26469;&#35757;&#32451;&#19968;&#27493;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#26102;&#19968;&#27493;&#39044;&#27979;&#35823;&#24046;&#30340;&#32047;&#31215;&#38382;&#39064;&#65292;&#24182;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05672</link><description>&lt;p&gt;
&#22810;&#27493;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-timestep models for Model-based Reinforcement Learning. (arXiv:2310.05672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05672
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#30446;&#26631;&#26469;&#35757;&#32451;&#19968;&#27493;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#26102;&#19968;&#27493;&#39044;&#27979;&#35823;&#24046;&#30340;&#32047;&#31215;&#38382;&#39064;&#65292;&#24182;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#20381;&#36182;&#20110;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#27169;&#25311;&#36712;&#36857;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#30340;&#22686;&#38271;&#65292;&#19968;&#27493;&#39044;&#27979;&#35823;&#24046;&#30340;&#32047;&#31215;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#30446;&#26631;&#26469;&#35757;&#32451;&#19968;&#27493;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#21508;&#31181;&#26410;&#26469;&#26102;&#38388;&#27573;&#19978;&#30340;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65288;&#20363;&#22914;&#65292;&#36127;&#23545;&#25968;&#20284;&#28982;&#65289;&#30340;&#21152;&#26435;&#21644;&#12290;&#25105;&#20204;&#25506;&#32034;&#21644;&#27979;&#35797;&#20102;&#19968;&#31995;&#21015;&#26435;&#37325;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#25351;&#25968;&#34928;&#20943;&#26435;&#37325;&#23548;&#33268;&#27169;&#22411;&#22312;&#38271;&#26102;&#38388;&#27573;&#30340;R2&#24471;&#20998;&#26174;&#33879;&#25552;&#39640;&#12290;&#24403;&#27169;&#22411;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#36825;&#31181;&#25913;&#36827;&#23588;&#20026;&#26126;&#26174;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#32431;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#36845;&#20195;&#25209;&#37327;RL&#22330;&#26223;&#20013;&#20351;&#29992;&#36719;&#20214;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;SAC&#65289;&#20195;&#29702;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#22810;&#27493;&#27169;&#22411;&#20248;&#20110;&#25110;&#19982;&#26631;&#20934;&#30340;&#19968;&#27493;&#27169;&#22411;&#30456;&#21305;&#37197;&#12290;&#36825;&#22312;&#32771;&#34385;&#29615;&#22659;&#30340;&#22122;&#22768;&#21464;&#20307;&#20013;&#23588;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
In model-based reinforcement learning (MBRL), most algorithms rely on simulating trajectories from one-step dynamics models learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as length of the trajectory grows. In this paper we tackle this issue by using a multi-timestep objective to train one-step models. Our objective is a weighted sum of a loss function (e.g., negative log-likelihood) at various future horizons. We explore and test a range of weights profiles. We find that exponentially decaying weights lead to models that significantly improve the long-horizon R2 score. This improvement is particularly noticeable when the models were evaluated on noisy data. Finally, using a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, we found that our multi-timestep models outperform or match standard one-step models. This was especially evident in a noisy variant of the considered envi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.05518</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;LSTD&#21644;&#38543;&#26426;&#29305;&#24449;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#24615;&#33021;&#21463;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#36807;&#21442;&#25968;&#21270;&#21644;&#20854;&#24102;&#26469;&#30340;&#22909;&#22788;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24773;&#20917;&#21017;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#21442;&#25968;&#20010;&#25968;&#19982;&#35775;&#38382;&#29366;&#24577;&#20010;&#25968;&#20043;&#27604;&#23450;&#20041;&#20026;&#20851;&#38190;&#22240;&#32032;&#65292;&#24403;&#35813;&#27604;&#20540;&#22823;&#20110;1&#26102;&#31216;&#20026;&#36807;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#21363;&#22312;&#21442;&#25968;/&#29366;&#24577;&#27604;&#20026;1&#38468;&#36817;&#20250;&#31361;&#28982;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#26080;&#38480;&#22823;&#30340;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#24335;&#25805;&#20316;&#32593;&#32476;&#30340;&#25391;&#21160;&#22768;&#23398;&#39057;&#21709;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20195;&#34920;&#24615;&#25391;&#21160;&#22768;&#23398;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#39057;&#21709;&#27169;&#25311;&#65292;&#26377;&#21161;&#20110;&#35774;&#35745;&#20248;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.05469</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#24335;&#25805;&#20316;&#32593;&#32476;&#30340;&#25391;&#21160;&#22768;&#23398;&#39057;&#21709;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Vibroacoustic Frequency Response Prediction with Query-based Operator Networks. (arXiv:2310.05469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#24335;&#25805;&#20316;&#32593;&#32476;&#30340;&#25391;&#21160;&#22768;&#23398;&#39057;&#21709;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20195;&#34920;&#24615;&#25391;&#21160;&#22768;&#23398;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#39057;&#21709;&#27169;&#25311;&#65292;&#26377;&#21161;&#20110;&#35774;&#35745;&#20248;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#39134;&#26426;&#12289;&#27773;&#36710;&#21644;&#25151;&#23627;&#31561;&#26426;&#26800;&#32467;&#26500;&#20013;&#30340;&#25391;&#21160;&#22768;&#23398;&#27874;&#20256;&#25773;&#23545;&#20445;&#35777;&#29992;&#25143;&#30340;&#20581;&#24247;&#21644;&#33298;&#36866;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20998;&#26512;&#36825;&#20123;&#31995;&#32479;&#65292;&#35774;&#35745;&#24072;&#21644;&#24037;&#31243;&#24072;&#20027;&#35201;&#32771;&#34385;&#39057;&#22495;&#20013;&#30340;&#21160;&#24577;&#21709;&#24212;&#65292;&#36825;&#36890;&#36807;&#20687;&#26377;&#38480;&#20803;&#26041;&#27861;&#36825;&#26679;&#30340;&#26114;&#36149;&#25968;&#20540;&#27169;&#25311;&#26469;&#35745;&#31639;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#26367;&#20195;&#27169;&#22411;&#25215;&#35834;&#21152;&#36895;&#36825;&#20123;&#27169;&#25311;&#65292;&#20174;&#32780;&#20419;&#36827;&#35774;&#35745;&#20248;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#31561;&#20219;&#21153;&#30340;&#23454;&#26045;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#20195;&#34920;&#24615;&#25391;&#21160;&#22768;&#23398;&#38382;&#39064;&#65306;&#39044;&#27979;&#24102;&#26377;&#19981;&#21516;&#24418;&#24335;&#38262;&#36793;&#30340;&#25391;&#21160;&#26495;&#30340;&#39057;&#21709;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#20849;&#35745;12,000&#20010;&#26495;&#20960;&#20309;&#24418;&#29366;&#20197;&#21450;&#30456;&#24212;&#30340;&#25968;&#20540;&#35299;&#65292;&#24182;&#24341;&#20837;&#20102;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#39044;&#27979;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#39057;&#21709;&#39044;&#27979;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#29575;&#26597;&#35810;&#25805;&#20316;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding vibroacoustic wave propagation in mechanical structures like airplanes, cars and houses is crucial to ensure health and comfort of their users. To analyze such systems, designers and engineers primarily consider the dynamic response in the frequency domain, which is computed through expensive numerical simulations like the finite element method. In contrast, data-driven surrogate models offer the promise of speeding up these simulations, thereby facilitating tasks like design optimization, uncertainty quantification, and design space exploration. We present a structured benchmark for a representative vibroacoustic problem: Predicting the frequency response for vibrating plates with varying forms of beadings. The benchmark features a total of 12,000 plate geometries with an associated numerical solution and introduces evaluation metrics to quantify the prediction quality. To address the frequency response prediction task, we propose a novel frequency query operator model, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#36827;&#34892;&#20102;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#26088;&#22312;&#20248;&#21270;&#25968;&#25454;&#22788;&#29702;&#21644;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.05350</link><description>&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#39640;&#25928;&#21442;&#25968;&#25628;&#32034;&#21644;&#24182;&#34892;&#24615;&#30340;&#25193;&#23637;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training. (arXiv:2310.05350v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#36827;&#34892;&#20102;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#26088;&#22312;&#20248;&#21270;&#25968;&#25454;&#22788;&#29702;&#21644;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21152;&#36895;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#20869;&#23384;&#38480;&#21046;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#30830;&#23450;&#20102;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#65288;&#22914;&#35757;&#32451;&#21644;&#25512;&#29702;&#65289;&#22312;&#21487;&#25509;&#21463;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#25191;&#34892;&#30340;&#35268;&#27169;&#12290;&#22914;&#20170;&#65292;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#24102;&#26377;&#39640;&#36895;&#20114;&#36830;&#30340;GPU&#21152;&#36895;&#39640;&#24615;&#33021;&#35745;&#31639;&#26426;&#12290;&#38543;&#30528;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;AI&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#20869;&#23384;&#38656;&#27714;&#20063;&#22312;&#19981;&#26029;&#22686;&#38271;&#12290;&#36825;&#20123;&#25361;&#25112;&#20419;&#20351;&#24320;&#21457;&#20986;&#20998;&#24067;&#24335;&#31639;&#27861;&#21644;&#22522;&#20110;&#30005;&#36335;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#22810;&#33410;&#28857;&#29615;&#22659;&#20013;&#36880;&#27493;&#25193;&#23637;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#26412;&#20989;&#25968;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#65292;&#24182;&#23558;&#26356;&#22810;&#21442;&#25968;&#23384;&#20648;&#22312;&#19968;&#23450;&#25968;&#37327;&#30340;&#21487;&#29992;&#36164;&#28304;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20248;&#21270;&#25968;&#25454;&#22788;&#29702;&#21644;&#39044;&#35757;&#32451;&#30340;&#19968;&#32452;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI accelerator processing capabilities and memory constraints largely dictate the scale in which machine learning workloads (e.g., training and inference) can be executed within a desirable time frame. Training a state of the art, transformer-based model today requires use of GPU-accelerated high performance computers with high-speed interconnects. As datasets and models continue to increase in size, computational requirements and memory demands for AI also continue to grow. These challenges have inspired the development of distributed algorithm and circuit-based optimization techniques that enable the ability to progressively scale models in multi-node environments, efficiently minimize neural network cost functions for faster convergence, and store more parameters into a set number of available resources. In our research project, we focus on parallel and distributed machine learning algorithm development, specifically for optimizing the data processing and pre-training of a set of 5 
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#29289;&#29702;&#30452;&#35273;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#23454;&#39564;&#20013;&#30340;&#21147;&#23398;&#35268;&#24459;&#12290;&#36890;&#36807;&#23545;&#31890;&#23376;&#36712;&#36857;&#30340;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23545;&#31216;&#24615;&#21644;&#38750;&#30456;&#21516;&#31890;&#23376;&#20043;&#38388;&#30340;&#38750;&#20114;&#36870;&#21147;&#65292;&#24182;&#25552;&#21462;&#20102;&#27599;&#20010;&#31890;&#23376;&#30340;&#36136;&#37327;&#21644;&#30005;&#33655;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25351;&#31034;&#20986;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#20013;&#23384;&#22312;&#36229;&#20986;&#24403;&#21069;&#29702;&#35770;&#20998;&#36776;&#29575;&#30340;&#26032;&#29289;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24341;&#23548;&#22810;&#20307;&#31995;&#32479;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.05273</link><description>&lt;p&gt;
&#22312;&#22810;&#20307;&#31995;&#32479;&#20013;&#23398;&#20064;&#21147;&#23398;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Learning force laws in many-body systems. (arXiv:2310.05273v1 [physics.plasm-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05273
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#29289;&#29702;&#30452;&#35273;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#23454;&#39564;&#20013;&#30340;&#21147;&#23398;&#35268;&#24459;&#12290;&#36890;&#36807;&#23545;&#31890;&#23376;&#36712;&#36857;&#30340;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23545;&#31216;&#24615;&#21644;&#38750;&#30456;&#21516;&#31890;&#23376;&#20043;&#38388;&#30340;&#38750;&#20114;&#36870;&#21147;&#65292;&#24182;&#25552;&#21462;&#20102;&#27599;&#20010;&#31890;&#23376;&#30340;&#36136;&#37327;&#21644;&#30005;&#33655;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25351;&#31034;&#20986;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#20013;&#23384;&#22312;&#36229;&#20986;&#24403;&#21069;&#29702;&#35770;&#20998;&#36776;&#29575;&#30340;&#26032;&#29289;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24341;&#23548;&#22810;&#20307;&#31995;&#32479;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#33258;&#28982;&#31995;&#32479;&#30340;&#31185;&#23398;&#35268;&#24459;&#21487;&#33021;&#27604;&#25105;&#20204;&#30340;&#30452;&#35273;&#26356;&#22797;&#26434;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#29616;&#35268;&#24459;&#30340;&#26041;&#27861;&#24517;&#39035;&#25913;&#21464;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21487;&#20197;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#20854;&#32467;&#26500;&#24212;&#35813;&#31526;&#21512;&#22522;&#26412;&#30340;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#20197;&#25552;&#20379;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#29289;&#29702;&#30452;&#35273;&#30340;ML&#26041;&#27861;&#65292;&#20197;&#25512;&#26029;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#23454;&#39564;&#20013;&#30340;&#21147;&#23398;&#27861;&#21017;&#12290;&#36890;&#36807;&#23545;3D&#31890;&#23376;&#36712;&#36857;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#21644;&#38750;&#30456;&#21516;&#31890;&#23376;&#20043;&#38388;&#30340;&#26377;&#25928;&#38750;&#20114;&#36870;&#21147;&#65292;&#24182;&#25552;&#21462;&#20986;&#27599;&#20010;&#31890;&#23376;&#30340;&#36136;&#37327;&#21644;&#30005;&#33655;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65288;R^2 &gt; 0.99&#65289;&#25351;&#31034;&#20986;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#20013;&#36229;&#20986;&#24403;&#21069;&#29702;&#35770;&#20998;&#36776;&#29575;&#30340;&#26032;&#29289;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26041;&#27861;&#22914;&#20309;&#24341;&#23548;&#22810;&#20307;&#31995;&#32479;&#20013;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific laws describing natural systems may be more complex than our intuition can handle, and thus how we discover laws must change. Machine learning (ML) models can analyze large quantities of data, but their structure should match the underlying physical constraints to provide useful insight. Here we demonstrate a ML approach that incorporates such physical intuition to infer force laws in dusty plasma experiments. Trained on 3D particle trajectories, the model accounts for inherent symmetries and non-identical particles, accurately learns the effective non-reciprocal forces between particles, and extracts each particle's mass and charge. The model's accuracy (R^2 &gt; 0.99) points to new physics in dusty plasma beyond the resolution of current theories and demonstrates how ML-powered approaches can guide new routes of scientific discovery in many-body systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#22312;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#30340;&#21516;&#26102;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.05052</link><description>&lt;p&gt;
&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#23398;&#20064;&#32454;&#32990;&#20869;&#21644;&#32454;&#32990;&#38388;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions. (arXiv:2310.05052v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#22312;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#30340;&#21516;&#26102;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#23545;&#30005;&#27744;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#30446;&#21069;&#65292;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20381;&#36182;&#20110;&#29305;&#23450;&#30446;&#26631;&#30005;&#27744;&#30340;&#26089;&#26399;&#30005;&#20449;&#21495;&#26469;&#39044;&#27979;&#23427;&#20204;&#30340;&#23551;&#21629;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#19981;&#36275;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#29305;&#23450;&#32769;&#21270;&#26465;&#20214;&#24320;&#21457;&#30340;&#65292;&#36825;&#19981;&#20165;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#27169;&#22411;&#33021;&#21147;&#65292;&#32780;&#19988;&#38477;&#20302;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#36864;&#21270;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#20854;&#20182;&#26465;&#20214;&#19979;&#21487;&#29992;&#30340;&#20016;&#23500;&#21382;&#21490;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26126;&#30830;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#23427;&#20204;&#30340;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#26469;&#39044;&#27979;&#30446;&#26631;&#30005;&#27744;&#30340;&#23551;&#21629;&#12290;&#36890;&#36807;&#36825;&#31181;&#32454;&#32990;&#38388;&#24046;&#24322;&#65292;&#25105;&#20204;&#19981;&#20165;&#25193;&#23637;&#20102;&#29305;&#24449;&#31354;&#38388;&#65292;&#36824;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;
Battery life prediction holds significant practical value for battery research and development. Currently, many data-driven models rely on early electrical signals from specific target batteries to predict their lifespan. A common shortfall is that most existing methods are developed based on specific aging conditions, which not only limits their model's capability but also diminishes their effectiveness in predicting degradation under varied conditions. As a result, these models often miss out on fully benefiting from the rich historical data available under other conditions. Here, to address above, we introduce an approach that explicitly captures differences between electrical signals of a target battery and a reference battery, irrespective of their materials and aging conditions, to forecast the target battery life. Through this inter-cell difference, we not only enhance the feature space but also pave the way for a universal battery life prediction framework. Remarkably, our mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#26816;&#27979;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32454;&#32990;&#24418;&#29366;&#36817;&#20284;&#20026;&#23450;&#21521;&#26925;&#22278;&#24182;&#20351;&#29992;&#36890;&#29992;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#22120;&#26469;&#35782;&#21035;&#32454;&#32990;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#32990;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20943;&#36731;&#20102;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.04895</link><description>&lt;p&gt;
&#21033;&#29992;&#26925;&#22278;&#36793;&#30028;&#26694;&#36827;&#34892;&#32454;&#32990;&#36319;&#36394;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cell Tracking-by-detection using Elliptical Bounding Boxes. (arXiv:2310.04895v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#26816;&#27979;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32454;&#32990;&#24418;&#29366;&#36817;&#20284;&#20026;&#23450;&#21521;&#26925;&#22278;&#24182;&#20351;&#29992;&#36890;&#29992;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#22120;&#26469;&#35782;&#21035;&#32454;&#32990;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#32990;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20943;&#36731;&#20102;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#23545;&#20110;&#29983;&#29289;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#27169;&#22411;&#28436;&#36827;&#30340;&#36319;&#36394;&#65292;&#36890;&#24120;&#36890;&#36807;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#22312;&#22270;&#20687;&#24103;&#19978;&#26816;&#27979;&#21644;&#36319;&#36394;&#32454;&#32990;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#32791;&#26102;&#19988;&#38656;&#35201;&#19987;&#19994;&#30340;&#26631;&#27880;&#21592;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#26816;&#27979;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#23558;&#32454;&#32990;&#24418;&#29366;&#36817;&#20284;&#20026;&#23450;&#21521;&#26925;&#22278;&#65292;&#28982;&#21518;&#20351;&#29992;&#36890;&#29992;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#22120;&#26469;&#35782;&#21035;&#27599;&#20010;&#24103;&#20013;&#30340;&#32454;&#32990;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20381;&#38752;&#19968;&#20010;&#20840;&#23616;&#25968;&#25454;&#20851;&#32852;&#31639;&#27861;&#65292;&#20351;&#29992;&#27010;&#29575;&#36317;&#31163;&#24230;&#37327;&#26469;&#25506;&#32034;&#32454;&#32990;&#30340;&#26102;&#38388;&#30456;&#20284;&#24615;&#65292;&#32771;&#34385;&#21040;&#26925;&#22278;&#19982;&#20108;&#32500;&#39640;&#26031;&#20998;&#24067;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cell detection and tracking are paramount for bio-analysis. Recent approaches rely on the tracking-by-model evolution paradigm, which usually consists of training end-to-end deep learning models to detect and track the cells on the frames with promising results. However, such methods require extensive amounts of annotated data, which is time-consuming to obtain and often requires specialized annotators. This work proposes a new approach based on the classical tracking-by-detection paradigm that alleviates the requirement of annotated data. More precisely, it approximates the cell shapes as oriented ellipses and then uses generic-purpose oriented object detectors to identify the cells in each frame. We then rely on a global data association algorithm that explores temporal cell similarity using probability distance metrics, considering that the ellipses relate to two-dimensional Gaussian distributions. Our results show that our method can achieve detection and tracking results competiti
&lt;/p&gt;</description></item><item><title>LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.04673</link><description>&lt;p&gt;
LauraGPT&#65306;&#20351;&#29992;GPT&#36827;&#34892;&#21548;&#12289;&#20851;&#27880;&#12289;&#29702;&#35299;&#21644;&#20877;&#29983;&#38899;&#39057;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT. (arXiv:2310.04673v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04673
&lt;/p&gt;
&lt;p&gt;
LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#31867;&#20284;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20197;&#21069;&#25552;&#20986;&#30340;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#32570;&#20047;&#20805;&#20998;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#35782;&#21035;&#21644;&#29702;&#35299;&#38899;&#39057;&#20869;&#23481;&#30340;&#20219;&#21153;&#65292;&#35201;&#20040;&#26126;&#26174;&#19981;&#21450;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65288;SOTA&#65289;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LauraGPT&#65292;&#19968;&#20010;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#32479;&#19968;GPT&#27169;&#22411;&#12290;LauraGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#38899;&#39057;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#22312;&#20219;&#24847;&#27169;&#24335;&#19979;&#29983;&#25104;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#36827;&#34892;&#19982;&#20869;&#23481;&#12289;&#35821;&#20041;&#12289;&#35821;&#38899;&#23398;&#21644;&#38899;&#39057;&#20449;&#21495;&#20998;&#26512;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20123;&#20540;&#24471;&#27880;&#24847;&#30340;&#20219;&#21153;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#38899;&#22686;&#24378;&#12289;&#33258;&#21160;&#38899;&#39057;&#25429;&#33719;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio capt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.04395</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25552;&#39640;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;$\theta$&#21644;&#25968;&#25454;$y$&#30340;&#27010;&#29575;&#32852;&#21512;&#27169;&#22411;$p(\theta, y)$&#20013;&#30340;&#36890;&#29992;&#23545;&#31216;&#24615;&#65292;&#25913;&#36827;&#20102;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#65288;ABI&#65289;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#31616;&#35328;&#20043;&#65292;&#25105;&#20204;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#65292;&#24182;&#22522;&#20110;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#12290;&#22312;&#23436;&#32654;&#36817;&#20284;&#24773;&#20917;&#19979;&#65292;&#36793;&#38469;&#20284;&#28982;&#22312;&#25152;&#26377;&#21442;&#25968;&#20540;&#19978;&#37117;&#26159;&#24120;&#25968;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#35823;&#24046;&#23548;&#33268;&#19981;&#21516;&#21442;&#25968;&#20540;&#30340;&#36793;&#38469;&#20284;&#28982;&#20272;&#35745;&#20013;&#23384;&#22312;&#19981;&#21487;&#21462;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24418;&#24335;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#26174;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#20284;&#28982;&#65289;&#30340;&#21452;&#23792;&#29609;&#20855;&#38382;&#39064;&#21644;&#20855;&#26377;&#38544;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#27169;&#25311;&#65289;&#30340;&#29616;&#23454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31471;&#21040;&#31471;&#32593;&#32476;&#33258;&#21160;&#21270;&#33041;&#26426;&#25509;&#21475;&#20013;&#30340;&#39044;&#22788;&#29702;&#21644;&#36890;&#36947;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#26512;&#21151;&#33021;&#24615;&#33041;&#36830;&#25509;&#21644;&#35782;&#21035;&#33041;&#27963;&#21160;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03748</link><description>&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#20013;&#30340;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;
&lt;/p&gt;
&lt;p&gt;
Phase Synchrony Component Self-Organization in Brain Computer Interface. (arXiv:2310.03748v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31471;&#21040;&#31471;&#32593;&#32476;&#33258;&#21160;&#21270;&#33041;&#26426;&#25509;&#21475;&#20013;&#30340;&#39044;&#22788;&#29702;&#21644;&#36890;&#36947;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#26512;&#21151;&#33021;&#24615;&#33041;&#36830;&#25509;&#21644;&#35782;&#21035;&#33041;&#27963;&#21160;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#21516;&#27493;&#20449;&#24687;&#22312;&#20998;&#26512;&#21151;&#33021;&#24615;&#33041;&#36830;&#25509;&#21644;&#35782;&#21035;&#33041;&#27963;&#21160;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#27969;&#31243;&#30001;&#39044;&#22788;&#29702;&#12289;&#36873;&#25321;&#33041;&#30005;&#37319;&#38598;&#36890;&#36947;&#21644;&#30456;&#20301;&#38145;&#23450;&#20540;&#65288;PLV&#65289;&#35745;&#31639;&#32452;&#25104;&#65292;&#22312;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#35813;&#27969;&#31243;&#26159;&#25163;&#21160;&#30340;&#19988;&#20381;&#36182;&#19987;&#23478;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#20415;&#21033;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#37319;&#29992;&#20102;&#19968;&#33324;&#30340;&#19982;&#25968;&#25454;&#26080;&#20851;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#26469;&#25233;&#21046;&#22122;&#22768;&#65292;&#38459;&#30861;&#20102;&#26356;&#37325;&#35201;&#30340;&#30456;&#20301;&#21516;&#27493;&#29616;&#35937;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;&#30340;&#27010;&#24565;&#65292;&#23427;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#25968;&#25454;&#30456;&#20851;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#65292;&#20174;&#32780;&#33258;&#21160;&#21270;&#39044;&#22788;&#29702;&#21644;&#36890;&#36947;&#36873;&#25321;&#31243;&#24207;&#12290;&#22522;&#20110;&#36825;&#20010;&#27010;&#24565;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#31471;&#21040;&#31471;&#32593;&#32476;&#65292;&#30452;&#25509;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Phase synchrony information plays a crucial role in analyzing functional brain connectivity and identifying brain activities. A widely adopted feature extraction pipeline, composed of preprocessing, selection of EEG acquisition channels, and phase locking value (PLV) calculation, has achieved success in motor imagery classification (MI). However, this pipeline is manual and reliant on expert knowledge, limiting its convenience and adaptability to different application scenarios. Moreover, most studies have employed mediocre data-independent spatial filters to suppress noise, impeding the exploration of more significant phase synchronization phenomena. To address the issues, we propose the concept of phase synchrony component self-organization, which enables the adaptive learning of data-dependent spatial filters for automating both the preprocessing and channel selection procedures. Based on this concept, the first deep learning end-to-end network is developed, which directly extracts 
&lt;/p&gt;</description></item><item><title>HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00113</link><description>&lt;p&gt;
HyperMask: &#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00113
&lt;/p&gt;
&lt;p&gt;
HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#39034;&#24207;&#35757;&#32451;&#26102;&#65292;&#24448;&#24448;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#26368;&#26377;&#25928;&#30340;&#20043;&#19968;&#26159;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36229;&#32593;&#32476;&#26681;&#25454;&#20219;&#21153;&#30340;&#29305;&#24449;&#29983;&#25104;&#30446;&#26631;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#20027;&#35201;&#38480;&#21046;&#26159;&#36229;&#32593;&#32476;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#21487;&#20197;&#20135;&#29983;&#23436;&#20840;&#19981;&#21516;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#22240;&#27492;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#21333;&#29420;&#35299;&#20915;&#30340;&#12290;&#27169;&#22411;&#22312;&#23398;&#20064;&#21518;&#32493;&#20219;&#21153;&#26102;&#19981;&#20351;&#29992;&#20043;&#21069;&#20219;&#21153;&#25152;&#20851;&#32852;&#30340;&#32593;&#32476;&#20449;&#24687;&#65292;&#24182;&#23454;&#38469;&#19978;&#20135;&#29983;&#20102;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#24425;&#31080;&#31080;&#35777;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#35748;&#20026;&#23384;&#22312;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#65288;&#21363;&#20013;&#22870;&#31080;&#65289;&#65292;&#21487;&#20197;&#20445;&#25345;&#23436;&#25972;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperMask&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#12290;&#36229;&#32593;&#32476;&#20135;&#29983;&#21322;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#20197;&#33719;&#21462;&#30446;&#26631;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"RAFA"&#30340;&#21407;&#21017;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;LLM&#20013;&#23558;&#25512;&#29702;&#35270;&#20026;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#36125;&#21494;&#26031;&#38382;&#39064;&#65292;&#21327;&#35843;&#25512;&#29702;&#21644;&#34892;&#21160;&#12290;&#36890;&#36807;&#19968;&#20010;&#25552;&#31034;&#27169;&#26495;&#36827;&#34892;&#25512;&#29702;&#65292;&#23398;&#20064;&#24182;&#21046;&#23450;&#26410;&#26469;&#30340;&#36712;&#36857;&#35268;&#21010;&#65292;&#28982;&#21518;&#22312;&#27599;&#19968;&#27493;&#20013;&#37319;&#21462;&#35745;&#21010;&#36712;&#36857;&#30340;&#21021;&#22987;&#34892;&#21160;&#24182;&#37325;&#26032;&#35268;&#21010;&#26410;&#26469;&#36712;&#36857;&#12290;&#36825;&#20010;&#26694;&#26550;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.17382</link><description>&lt;p&gt;
&#26410;&#26469;&#30340;&#21407;&#22240;&#65292;&#29616;&#22312;&#30340;&#34892;&#21160;&#65306;&#19968;&#31181;&#21487;&#35777;&#26126;&#26679;&#26412;&#25928;&#29575;&#30340;&#33258;&#20027;LLM&#26234;&#33021;&#20307;&#30340;&#21407;&#21017;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency. (arXiv:2309.17382v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"RAFA"&#30340;&#21407;&#21017;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;LLM&#20013;&#23558;&#25512;&#29702;&#35270;&#20026;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#36125;&#21494;&#26031;&#38382;&#39064;&#65292;&#21327;&#35843;&#25512;&#29702;&#21644;&#34892;&#21160;&#12290;&#36890;&#36807;&#19968;&#20010;&#25552;&#31034;&#27169;&#26495;&#36827;&#34892;&#25512;&#29702;&#65292;&#23398;&#20064;&#24182;&#21046;&#23450;&#26410;&#26469;&#30340;&#36712;&#36857;&#35268;&#21010;&#65292;&#28982;&#21518;&#22312;&#27599;&#19968;&#27493;&#20013;&#37319;&#21462;&#35745;&#21010;&#36712;&#36857;&#30340;&#21021;&#22987;&#34892;&#21160;&#24182;&#37325;&#26032;&#35268;&#21010;&#26410;&#26469;&#36712;&#36857;&#12290;&#36825;&#20010;&#26694;&#26550;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23558;&#25512;&#29702;&#36716;&#21270;&#20026;&#34892;&#21160;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#20309;&#36890;&#36807;&#25512;&#29702;&#30340;&#20869;&#37096;&#26426;&#21046;&#22312;&#19982;&#22806;&#37096;&#29615;&#22659;&#30340;&#26368;&#23569;&#20132;&#20114;&#27425;&#25968;&#20869;&#21487;&#35777;&#26126;&#22320;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21487;&#35777;&#26126;&#36951;&#25022;&#20445;&#35777;&#30340;&#21407;&#21017;&#26694;&#26550;&#26469;&#21327;&#35843;&#25512;&#29702;&#21644;&#34892;&#21160;&#65292;&#31216;&#20043;&#20026;&#8220;&#20026;&#26410;&#26469;&#32780;&#25512;&#29702;&#65292;&#20026;&#29616;&#22312;&#32780;&#34892;&#21160;&#8221;&#65288;RAFA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25512;&#29702;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#23398;&#20064;&#24182;&#21046;&#23450;&#26410;&#26469;&#30340;&#38271;&#26399;&#36712;&#36857;&#35268;&#21010;&#65288;&#8220;&#20026;&#26410;&#26469;&#32780;&#25512;&#29702;&#8221;&#65289;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;LLM&#26234;&#33021;&#20307;&#37319;&#21462;&#35745;&#21010;&#36712;&#36857;&#30340;&#21021;&#22987;&#34892;&#21160;&#65288;&#8220;&#20026;&#29616;&#22312;&#32780;&#34892;&#21160;&#8221;&#65289;&#65292;&#23558;&#25910;&#38598;&#21040;&#30340;&#21453;&#39304;&#23384;&#20648;&#22312;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#65292;&#24182;&#37325;&#26032;&#35843;&#29992;&#25512;&#29702;&#36807;&#31243;&#20174;&#26032;&#29366;&#24577;&#37325;&#26032;&#35268;&#21010;&#26410;&#26469;&#30340;&#36712;&#36857;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;LLM&#20013;&#30340;&#25512;&#29702;&#35270;&#20026;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#36125;&#21494;&#26031;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" (\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.  The key idea is to cast reasoning in LLMs as learning and planning in Bayes
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;ACGAN-GNNExplainer&#65292;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#20511;&#21161;&#37492;&#21035;&#22120;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16918</link><description>&lt;p&gt;
ACGAN-GNNExplainer&#65306;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36741;&#21161;&#26465;&#20214;&#29983;&#25104;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks. (arXiv:2309.16918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;ACGAN-GNNExplainer&#65292;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#20511;&#21161;&#37492;&#21035;&#22120;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#24182;&#23454;&#29616;&#21487;&#38752;&#30340;&#20915;&#31574;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#35768;&#22810;GNN&#35299;&#37322;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#29305;&#23450;&#23454;&#20363;&#30340;&#20381;&#36182;&#24615;&#65292;&#23545;&#26410;&#35265;&#36807;&#30340;&#22270;&#30340;&#19968;&#33324;&#24615;&#19981;&#36275;&#65292;&#21487;&#33021;&#20135;&#29983;&#26080;&#25928;&#30340;&#35299;&#37322;&#20197;&#21450;&#29983;&#25104;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#19981;&#20805;&#20998;&#30340;&#20445;&#30495;&#24230;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#35299;&#37322;&#22120;ACGAN-GNNExplainer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#36816;&#29992;&#37492;&#21035;&#22120;&#26469;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#30830;&#20445;&#35299;&#37322;&#30340;&#20445;&#30495;&#24230;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35780;&#20272;&#20998;&#21035;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have proven their efficacy in a variety of real-world applications, but their underlying mechanisms remain a mystery. To address this challenge and enable reliable decision-making, many GNN explainers have been proposed in recent years. However, these methods often encounter limitations, including their dependence on specific instances, lack of generalizability to unseen graphs, producing potentially invalid explanations, and yielding inadequate fidelity. To overcome these limitations, we, in this paper, introduce the Auxiliary Classifier Generative Adversarial Network (ACGAN) into the field of GNN explanation and propose a new GNN explainer dubbed~\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. Experimental evaluations conducted on both synthetic and real-world graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#24182;&#37319;&#29992;&#21435;&#20559;&#24046;&#26041;&#27861;&#32416;&#27491;&#39044;&#27979;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16598</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#21449;&#39044;&#27979;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Cross-Prediction-Powered Inference. (arXiv:2309.16598v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#24182;&#37319;&#29992;&#21435;&#20559;&#24046;&#26041;&#27861;&#32416;&#27491;&#39044;&#27979;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#28982;&#32780;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#32463;&#24120;&#38656;&#35201;&#32321;&#29712;&#30340;&#20154;&#24037;&#26631;&#27880;&#25110;&#32773;&#32531;&#24930;&#26114;&#36149;&#30340;&#31185;&#23398;&#27979;&#37327;&#12290;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#31934;&#23494;&#30340;&#39044;&#27979;&#25216;&#26415;&#21487;&#20197;&#24555;&#36895;&#12289;&#24265;&#20215;&#22320;&#20135;&#29983;&#22823;&#37327;&#39044;&#27979;&#26631;&#31614;&#65307;&#20363;&#22914;&#65292;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34987;&#29992;&#26469;&#34917;&#20805;&#23454;&#39564;&#24471;&#21040;&#30340;&#32467;&#26500;&#65292;&#21355;&#26143;&#22270;&#20687;&#39044;&#27979;&#30340;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#34987;&#29992;&#26469;&#34917;&#20805;&#20934;&#30830;&#30340;&#35843;&#26597;&#25968;&#25454;&#31561;&#12290;&#30001;&#20110;&#39044;&#27979;&#20855;&#26377;&#19981;&#23436;&#32654;&#21644;&#28508;&#22312;&#20559;&#24046;&#30340;&#29305;&#28857;&#65292;&#36825;&#31181;&#20570;&#27861;&#23545;&#19979;&#28216;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#20102;&#36136;&#30097;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#19968;&#20010;&#23567;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22823;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20132;&#21449;&#39044;&#27979;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#24182;&#24212;&#29992;&#19968;&#31181;&#21435;&#20559;&#24046;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reliable data-driven decision-making hinges on high-quality labeled data, the acquisition of quality labels often involves laborious human annotations or slow and expensive scientific measurements. Machine learning is becoming an appealing alternative as sophisticated predictive techniques are being used to quickly and cheaply produce large amounts of predicted labels; e.g., predicted protein structures are used to supplement experimentally derived structures, predictions of socioeconomic indicators from satellite imagery are used to supplement accurate survey data, and so on. Since predictions are imperfect and potentially biased, this practice brings into question the validity of downstream inferences. We introduce cross-prediction: a method for valid inference powered by machine learning. With a small labeled dataset and a large unlabeled dataset, cross-prediction imputes the missing labels via machine learning and applies a form of debiasing to remedy the prediction inaccurac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LPML&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;&#12290;&#36890;&#36807;&#23558;Chain-of-Thought&#26041;&#27861;&#21644;Python REPL&#19982;&#35813;&#26631;&#35760;&#35821;&#35328;&#32467;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#21033;&#29992;Python&#35745;&#31639;&#32416;&#27491;&#38169;&#35823;&#21644;&#35299;&#20915;&#25361;&#25112;&#24615;&#25968;&#23398;&#38382;&#39064;&#65292;&#32780;&#21482;&#38656;&#35201;&#38646;&#26679;&#26412;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.13078</link><description>&lt;p&gt;
LPML: &#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
LPML: LLM-Prompting Markup Language for Mathematical Reasoning. (arXiv:2309.13078v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LPML&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;&#12290;&#36890;&#36807;&#23558;Chain-of-Thought&#26041;&#27861;&#21644;Python REPL&#19982;&#35813;&#26631;&#35760;&#35821;&#35328;&#32467;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#21033;&#29992;Python&#35745;&#31639;&#32416;&#27491;&#38169;&#35823;&#21644;&#35299;&#20915;&#25361;&#25112;&#24615;&#25968;&#23398;&#38382;&#39064;&#65292;&#32780;&#21482;&#38656;&#35201;&#38646;&#26679;&#26412;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#26102;&#65292;&#35299;&#20915;LLMs&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#25512;&#29702;&#21644;&#35745;&#31639;&#38169;&#35823;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;Chain-of-Thought&#65288;CoT&#65289;&#26041;&#27861;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;Python REPL&#65289;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#29983;&#25104;&#31867;&#20284;XML&#26631;&#35760;&#35821;&#35328;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;CoT&#21644;&#22806;&#37096;&#24037;&#20855;&#65292;&#24182;&#25511;&#21046;LLMs&#30340;&#19981;&#33391;&#34892;&#20026;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;LLMs&#21487;&#20197;&#21033;&#29992;Python&#35745;&#31639;&#26469;&#32416;&#27491;CoT&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;ChatGPT&#65288;GPT-3.5&#65289;&#26469;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#26631;&#35760;&#35821;&#35328;&#23558;CoT&#21644;Python REPL&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#32534;&#20889;&#26631;&#35760;&#35821;&#35328;&#65292;&#24182;&#36827;&#34892;&#39640;&#32423;&#25968;&#23398;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20813;&#30123;&#30340;&#27010;&#29575;&#65292;&#25552;&#20986;&#20102;&#20813;&#30123;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#21450;&#949;-&#26377;&#30028;&#20813;&#30123;&#30340;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#20511;&#21161;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20272;&#35745;&#21463;&#30410;&#27010;&#29575;&#65292;&#24182;&#24471;&#21040;&#27604;&#29616;&#26377;&#36793;&#30028;&#26356;&#32039;&#23494;&#30340;&#27010;&#29575;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#20171;&#32461;&#20102;&#38388;&#25509;&#20813;&#30123;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#26410;&#27979;&#37327;&#28151;&#28102;&#30340;&#20813;&#30123;&#27010;&#29575;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11942</link><description>&lt;p&gt;
&#20851;&#20110;&#20813;&#30123;&#30340;&#27010;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Probability of Immunity. (arXiv:2309.11942v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20813;&#30123;&#30340;&#27010;&#29575;&#65292;&#25552;&#20986;&#20102;&#20813;&#30123;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#21450;&#949;-&#26377;&#30028;&#20813;&#30123;&#30340;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#20511;&#21161;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20272;&#35745;&#21463;&#30410;&#27010;&#29575;&#65292;&#24182;&#24471;&#21040;&#27604;&#29616;&#26377;&#36793;&#30028;&#26356;&#32039;&#23494;&#30340;&#27010;&#29575;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#20171;&#32461;&#20102;&#38388;&#25509;&#20813;&#30123;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#26410;&#27979;&#37327;&#28151;&#28102;&#30340;&#20813;&#30123;&#27010;&#29575;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#30740;&#31350;&#20813;&#30123;&#30340;&#27010;&#29575;&#65292;&#21363;&#26080;&#35770;&#26292;&#38706;&#19982;&#21542;&#65292;&#25928;&#26524;&#37117;&#20250;&#21457;&#29983;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#20813;&#30123;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#20197;&#21450;&#949;-&#26377;&#30028;&#20813;&#30123;&#30340;&#26465;&#20214;&#65292;&#21069;&#32773;&#20801;&#35768;&#25105;&#20204;&#20174;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#20272;&#35745;&#21463;&#30410;&#30340;&#27010;&#29575;&#65288;&#21363;&#21482;&#26377;&#22312;&#26292;&#38706;&#30340;&#24773;&#20917;&#19979;&#25928;&#26524;&#25165;&#20250;&#21457;&#29983;&#65289;&#65292;&#21518;&#32773;&#20801;&#35768;&#25105;&#20204;&#24471;&#21040;&#27604;&#29616;&#26377;&#30340;&#36793;&#30028;&#26356;&#32039;&#23494;&#30340;&#21463;&#30410;&#27010;&#29575;&#36793;&#30028;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#38388;&#25509;&#20813;&#30123;&#30340;&#27010;&#24565;&#65288;&#36890;&#36807;&#20171;&#36136;&#65289;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#21069;&#36848;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#26410;&#27979;&#37327;&#28151;&#28102;&#24773;&#20917;&#19979;&#36827;&#34892;&#20813;&#30123;&#27010;&#29575;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is devoted to the study of the probability of immunity, i.e. the effect occurs whether exposed or not. We derive necessary and sufficient conditions for non-immunity and $\epsilon$-bounded immunity, i.e. the probability of immunity is zero and $\epsilon$-bounded, respectively. The former allows us to estimate the probability of benefit (i.e., the effect occurs if and only if exposed) from a randomized controlled trial, and the latter allows us to produce bounds of the probability of benefit that are tighter than the existing ones. We also introduce the concept of indirect immunity (i.e., through a mediator) and repeat our previous analysis for it. Finally, we propose a method for sensitivity analysis of the probability of immunity under unmeasured confounding.
&lt;/p&gt;</description></item><item><title>DiscoverPath&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#35770;&#25991;&#25628;&#32034;&#24341;&#25806;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24615;&#26631;&#27880;&#20174;&#25991;&#31456;&#25688;&#35201;&#20013;&#25552;&#21462;&#26415;&#35821;&#21644;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#32473;&#29992;&#25143;&#19968;&#20010;&#20851;&#27880;&#26597;&#35810;&#23454;&#20307;&#21450;&#20854;&#37051;&#36817;&#33410;&#28857;&#30340;&#23376;&#22270;&#65292;&#20197;&#21450;&#26597;&#35810;&#25512;&#33616;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24490;&#24207;&#28176;&#36827;&#22320;&#32454;&#21270;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2309.01808</link><description>&lt;p&gt;
DiscoverPath&#65306;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#30340;&#30693;&#35782;&#32454;&#21270;&#21644;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research. (arXiv:2309.01808v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01808
&lt;/p&gt;
&lt;p&gt;
DiscoverPath&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#35770;&#25991;&#25628;&#32034;&#24341;&#25806;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24615;&#26631;&#27880;&#20174;&#25991;&#31456;&#25688;&#35201;&#20013;&#25552;&#21462;&#26415;&#35821;&#21644;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#32473;&#29992;&#25143;&#19968;&#20010;&#20851;&#27880;&#26597;&#35810;&#23454;&#20307;&#21450;&#20854;&#37051;&#36817;&#33410;&#28857;&#30340;&#23376;&#22270;&#65292;&#20197;&#21450;&#26597;&#35810;&#25512;&#33616;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24490;&#24207;&#28176;&#36827;&#22320;&#32454;&#21270;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20986;&#29256;&#29289;&#30340;&#25351;&#25968;&#22686;&#38271;&#38656;&#35201;&#39640;&#32423;&#24037;&#20855;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#25991;&#31456;&#26816;&#32034;&#65292;&#23588;&#20854;&#22312;&#36328;&#23398;&#31185;&#39046;&#22495;&#20013;&#65292;&#19981;&#21516;&#30340;&#26415;&#35821;&#34987;&#29992;&#26469;&#25551;&#36848;&#30456;&#20284;&#30340;&#30740;&#31350;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#24341;&#25806;&#24448;&#24448;&#26080;&#27861;&#24110;&#21161;&#37027;&#20123;&#23545;&#29305;&#23450;&#26415;&#35821;&#19981;&#29087;&#24713;&#30340;&#29992;&#25143;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#35770;&#25991;&#25628;&#32034;&#24341;&#25806;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#22312;&#21457;&#29616;&#30456;&#20851;&#26597;&#35810;&#21644;&#25991;&#31456;&#26041;&#38754;&#30340;&#20307;&#39564;&#12290;&#35813;&#31995;&#32479;&#34987;&#31216;&#20026;DiscoverPath&#65292;&#37319;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#26469;&#20174;&#25991;&#31456;&#25688;&#35201;&#20013;&#25552;&#21462;&#26415;&#35821;&#21644;&#20851;&#31995;&#65292;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#12290;&#20026;&#20102;&#20943;&#23569;&#20449;&#24687;&#36229;&#36733;&#65292;DiscoverPath&#32473;&#29992;&#25143;&#23637;&#31034;&#20102;&#19968;&#20010;&#20851;&#27880;&#26597;&#35810;&#23454;&#20307;&#21450;&#20854;&#37051;&#36817;&#33410;&#28857;&#30340;&#23376;&#22270;&#65292;&#24182;&#19988;&#36824;&#32467;&#21512;&#20102;&#26597;&#35810;&#25512;&#33616;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24490;&#24207;&#28176;&#36827;&#22320;&#32454;&#21270;&#26597;&#35810;&#12290;&#35813;&#31995;&#32479;&#37197;&#22791;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth in scholarly publications necessitates advanced tools for efficient article retrieval, especially in interdisciplinary fields where diverse terminologies are used to describe similar research. Traditional keyword-based search engines often fall short in assisting users who may not be familiar with specific terminologies. To address this, we present a knowledge graph-based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, dubbed DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a KG. To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes and incorporates a query recommendation system, enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical Us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15321</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#8220;&#26333;&#20809;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#65292;&#32570;&#20047;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#23545;&#37319;&#26679;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#24314;&#27169;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;&#39044;&#27979;&#35823;&#24046;&#24402;&#22240;&#20026;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#38500;&#20102;&#38416;&#26126;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;Epsilon Scaling&#65292;&#20197;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Epsilon Scaling&#36890;&#36807;&#32553;&#23567;&#32593;&#32476;&#36755;&#20986;&#65288;Epsilon&#65289;&#26126;&#30830;&#22320;&#23558;&#37319;&#26679;&#36712;&#36857;&#31227;&#36817;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#12290;&#22312;&#21508;&#31181;&#25193;&#25955;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
&lt;/p&gt;</description></item><item><title>InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;</title><link>http://arxiv.org/abs/2308.12067</link><description>&lt;p&gt;
InstructionGPT-4: &#19968;&#20010;200&#25351;&#20196;&#33539;&#24335;&#29992;&#20110;&#24494;&#35843;MiniGPT-4
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12067
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#33719;&#21462;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65306;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30417;&#30563;&#24335;&#35270;&#35273;-&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#37327;&#30340;&#39640;&#36136;&#37327;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InstructionGPT-4&#65292;&#23427;&#32463;&#36807;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;200&#20010;&#20363;&#23376;&#65292;&#32422;&#21344;MiniGPT-4&#23545;&#40784;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#30340;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#30340;6%&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20960;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#22522;&#20110;&#36825;&#20123;&#24230;&#37327;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36873;&#25321;&#22120;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#30340;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;InstructionGPT-4&#22312;&#21508;&#31181;&#35780;&#20272;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#12289;GPT-4&#20559;&#22909;&#65289;&#19978;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11838</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#26657;&#20934;&#38382;&#39064;&#65292;&#23613;&#31649;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26694;&#26550;&#26469;&#25913;&#21892;&#26657;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#26657;&#20934;&#23646;&#24615;&#30340;&#30740;&#31350;&#26377;&#28857;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20840;&#38754;&#25506;&#32034;&#26657;&#20934;&#23646;&#24615;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#23613;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#12290;&#25105;&#20204;&#29305;&#21035;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;NATS-Bench&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#20102;90&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;12&#20010;&#20854;&#20182;&#26657;&#20934;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;117,702&#20010;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#22238;&#31572;&#35813;&#39046;&#22495;&#19968;&#20123;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26657;&#20934;&#33021;&#21542;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65311;&#65288;ii&#65289;&#33021;&#21542;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#20808;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#19968;&#32452;&#29305;&#24449;&#26144;&#23556;&#65292;&#20877;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#20844;&#24335;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13149</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions. (arXiv:2307.13149v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#20808;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#19968;&#32452;&#29305;&#24449;&#26144;&#23556;&#65292;&#20877;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#20844;&#24335;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#36890;&#24120;&#34987;&#35748;&#20026;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20004;&#27493;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19987;&#23478;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#65292;&#20854;&#20013;&#23624;&#26381;&#26354;&#38754;&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#19968;&#32452;&#21333;&#21464;&#37327;&#29305;&#24449;&#26144;&#23556;&#26469;&#34920;&#31034;&#30340;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#36825;&#32452;&#21333;&#21464;&#37327;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#20989;&#25968;&#37325;&#26032;&#35299;&#37322;&#20026;&#25968;&#23398;&#24418;&#24335;&#12290;&#36825;&#31181;&#20998;&#32780;&#27835;&#20043;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#37325;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;&#20174;&#23454;&#38469;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#25552;&#39640;&#20102;&#29992;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#26448;&#26009;&#30340;&#23646;&#24615;&#65288;&#22914;&#20984;&#24615;&#21644;&#23545;&#31216;&#24615;&#65289;&#26377;&#19968;&#20010;&#20855;&#20307;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#26500;&#24314;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#65292;&#20174;&#32780;&#23558;&#23545;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.06555</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#36924;&#36817;&#65306;&#20174;ReLU&#21040;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#26500;&#24314;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#65292;&#20174;&#32780;&#23558;&#23545;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23450;&#20041;&#20102;&#19968;&#20010;&#28608;&#27963;&#20989;&#25968;&#38598;&#21512;A&#65292;&#21253;&#25324;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;ReLU&#12289;LeakyReLU&#12289;ReLU^2&#12289;ELU&#12289;SELU&#12289;Softplus&#12289;GELU&#12289;SiLU&#12289;Swish&#12289;Mish&#12289;Sigmoid&#12289;Tanh&#12289;Arctan&#12289;Softsign&#12289;dSiLU&#21644;SRS&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#24847;&#28608;&#27963;&#20989;&#25968;varrho&#8712;A&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#20197;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#12290;&#36825;&#19968;&#21457;&#29616;&#20351;&#24471;&#22823;&#37096;&#20998;&#23545;&#20110;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#33021;&#22815;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#65292;&#23613;&#31649;&#38656;&#35201;&#31245;&#22823;&#30340;&#24120;&#25968;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.
&lt;/p&gt;</description></item><item><title>&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#65292;&#26469;&#24314;&#31435;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36755;&#20986;&#30340;&#26356;&#22362;&#23454;&#22522;&#30784;&#65292;&#24182;&#35299;&#20915;&#38169;&#35823;&#20449;&#24687;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02796</link><description>&lt;p&gt;
VerifAI&#65306;&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
VerifAI: Verified Generative AI. (arXiv:2307.02796v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02796
&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#65292;&#26469;&#24314;&#31435;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36755;&#20986;&#30340;&#26356;&#22362;&#23454;&#22522;&#30784;&#65292;&#24182;&#35299;&#20915;&#38169;&#35823;&#20449;&#24687;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#23545;&#20110;&#20854;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#20173;&#22312;&#22686;&#38271;&#12290;&#36825;&#31181;&#19981;&#20934;&#30830;&#24615;&#21487;&#33021;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#22914;&#38169;&#35823;&#20915;&#31574;&#65292;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#65292;&#20405;&#29359;&#38544;&#31169;&#65292;&#27861;&#24459;&#36131;&#20219;&#31561;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#36827;&#34892;&#24212;&#23545;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#65292;&#22914;&#36879;&#26126;&#24230;&#65292;&#38544;&#31169;&#20445;&#25252;&#65292;&#20559;&#35265;&#32531;&#35299;&#20197;&#21450;&#31038;&#20250;&#21644;&#29615;&#22659;&#36131;&#20219;&#31561;&#65292;&#20294;&#30001;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#36215;&#30340;&#38169;&#35823;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#20174;&#25968;&#25454;&#31649;&#29702;&#30340;&#35282;&#24230;&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#12290;&#36825;&#21253;&#25324;&#20998;&#26512;&#26469;&#33258;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#21253;&#25324;&#25991;&#26412;&#25991;&#20214;&#65292;&#34920;&#26684;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36755;&#20986;&#22880;&#23450;&#26356;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;&#35299;&#20915;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#39564;&#35777;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. Such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. Although efforts to address these risks are underway, including explainable AI and responsible AI practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative AI will remain a significant challenge. We propose that verifying the outputs of generative AI from a data management perspective is an emerging issue for generative AI. This involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. By doing so, we can establish a stronger foundation for evaluating the outputs of generative AI models. Such an approach ca
&lt;/p&gt;</description></item><item><title>&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02484</link><description>&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02484
&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#65292;&#23427;&#26159;&#29616;&#26377;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;DT&#22768;&#31216;&#33021;&#22815;&#29983;&#25104;&#26368;&#20339;&#36712;&#36857;&#65292;&#20294;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#23427;&#22312;&#36712;&#36857;&#25340;&#25509;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36712;&#36857;&#25340;&#25509;&#26159;&#25351;&#20174;&#19968;&#32452;&#27425;&#20248;&#36712;&#36857;&#20013;&#29983;&#25104;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;&#25552;&#20986;&#30340;EDT&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;DT&#20013;&#32500;&#25252;&#30340;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#20174;&#32780;&#20351;&#33258;&#24049;&#19982;&#20247;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#36712;&#36857;&#26159;&#26368;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#38271;&#30340;&#21382;&#21490;&#65292;&#24403;&#24403;&#21069;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#30701;&#30340;&#21382;&#21490;&#26469;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#26356;&#20248;&#30340;&#36712;&#36857;&#36827;&#34892;&#8220;&#25340;&#25509;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#33021;&#22815;&#22635;&#34917;&#22522;&#20110;DT&#21644;&#22522;&#20110;Q-Learning&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;EDT&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
&lt;/p&gt;</description></item><item><title>ECG-QA&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#24515;&#30005;&#22270;&#20998;&#26512;&#35774;&#35745;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#28085;&#30422;&#24191;&#27867;&#20020;&#24202;&#30456;&#20851;ECG&#20027;&#39064;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#22810;&#26679;&#21270;&#30340;ECG&#35299;&#35835;&#38382;&#39064;&#12290;&#36825;&#19968;&#36164;&#28304;&#23558;&#20026;&#26410;&#26469;&#30340;&#21307;&#30103;&#20445;&#20581;&#38382;&#31572;&#30740;&#31350;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.15681</link><description>&lt;p&gt;
ECG-QA&#65306;&#32467;&#21512;&#24515;&#30005;&#22270;&#30340;&#32508;&#21512;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram. (arXiv:2306.15681v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15681
&lt;/p&gt;
&lt;p&gt;
ECG-QA&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#24515;&#30005;&#22270;&#20998;&#26512;&#35774;&#35745;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#28085;&#30422;&#24191;&#27867;&#20020;&#24202;&#30456;&#20851;ECG&#20027;&#39064;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#22810;&#26679;&#21270;&#30340;ECG&#35299;&#35835;&#38382;&#39064;&#12290;&#36825;&#19968;&#36164;&#28304;&#23558;&#20026;&#26410;&#26469;&#30340;&#21307;&#30103;&#20445;&#20581;&#38382;&#31572;&#30740;&#31350;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#38382;&#31572;&#38382;&#39064;&#65288;QA&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21307;&#30103;&#20445;&#20581;QA&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#21307;&#23398;&#24433;&#20687;&#12289;&#20020;&#24202;&#35760;&#24405;&#25110;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#34920;&#19978;&#12290;&#36825;&#20351;&#24471;&#23558;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#19982;&#36825;&#20123;&#31995;&#32479;&#30456;&#32467;&#21512;&#30340;&#24040;&#22823;&#28508;&#21147;&#20960;&#20046;&#26410;&#34987;&#21033;&#29992;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ECG-QA&#65292;&#36825;&#26159;&#19987;&#38376;&#38024;&#23545;ECG&#20998;&#26512;&#35774;&#35745;&#30340;&#31532;&#19968;&#20010;QA&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20849;70&#20010;&#28085;&#30422;&#20102;&#24191;&#27867;&#20020;&#24202;&#30456;&#20851;ECG&#20027;&#39064;&#30340;&#38382;&#39064;&#27169;&#26495;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#32463;&#36807;&#19968;&#21517;ECG&#19987;&#23478;&#30340;&#39564;&#35777;&#65292;&#20197;&#30830;&#20445;&#20854;&#20020;&#24202;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22810;&#26679;&#21270;&#30340;ECG&#35299;&#35835;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#23545;&#20004;&#20010;&#19981;&#21516;&#30340;ECG&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#35768;&#22810;&#23454;&#39564;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30456;&#20449;ECG-QA&#23558;&#25104;&#20026;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#20379;&#30740;&#31350;&#32773;&#20204;&#25506;&#32034;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) in the field of healthcare has received much attention due to significant advancements in natural language processing. However, existing healthcare QA datasets primarily focus on medical images, clinical notes, or structured electronic health record tables. This leaves the vast potential of combining electrocardiogram (ECG) data with these systems largely untapped. To address this gap, we present ECG-QA, the first QA dataset specifically designed for ECG analysis. The dataset comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility. As a result, our dataset includes diverse ECG interpretation questions, including those that require a comparative analysis of two different ECGs. In addition, we have conducted numerous experiments to provide valuable insights for future research directions. We believe that ECG-QA will serve as a valuable resource for the de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#26102;&#21453;&#39304;&#25511;&#21046;InAs/GaAs&#37327;&#23376;&#28857;&#29983;&#38271;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12898</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#26102;&#21453;&#39304;&#25511;&#21046;InAs/GaAs&#37327;&#23376;&#28857;&#29983;&#38271;
&lt;/p&gt;
&lt;p&gt;
Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots. (arXiv:2306.12898v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12898
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#26102;&#21453;&#39304;&#25511;&#21046;InAs/GaAs&#37327;&#23376;&#28857;&#29983;&#38271;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32452;&#35013;&#30340;InAs / GaAs&#37327;&#23376;&#28857;&#65288;QDs&#65289;&#20855;&#26377;&#29992;&#20110;&#24320;&#21457;&#21508;&#31181;&#20809;&#30005;&#23376;&#22120;&#20214;&#30340;&#26497;&#39640;&#20215;&#20540;&#12290;&#24314;&#31435;&#29305;&#23450;&#23494;&#24230;&#30340;QDs&#30340;&#36807;&#31243;&#21442;&#25968;&#26159;&#19968;&#20010;&#22810;&#32500;&#20248;&#21270;&#25361;&#25112;&#65292;&#36890;&#24120;&#36890;&#36807;&#32791;&#26102;&#21644;&#36845;&#20195;&#30340;&#35797;&#38169;&#26469;&#35299;&#20915;&#12290;&#22312;&#27492;&#65292;&#20316;&#32773;&#20351;&#29992;&#22522;&#20110;3D ResNet&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#19987;&#38376;&#35757;&#32451;RHEED&#35270;&#39057;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#34920;&#38754;&#24418;&#35980;&#30340;&#23454;&#26102;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-assembled InAs/GaAs quantum dots (QDs) have properties highly valuable for developing various optoelectronic devices such as QD lasers and single photon sources. The applications strongly rely on the density and quality of these dots, which has motivated studies of the growth process control to realize high-quality epi-wafers and devices. Establishing the process parameters in molecular beam epitaxy (MBE) for a specific density of QDs is a multidimensional optimization challenge, usually addressed through time-consuming and iterative trial-and-error. Meanwhile, reflective high-energy electron diffraction (RHEED) has been widely used to capture a wealth of growth information in situ. However, it still faces the challenges of extracting information from noisy and overlapping images. Here, based on 3D ResNet, we developed a machine learning (ML) model specially designed for training RHEED videos instead of static images and providing real-time feedback on surface morphologies for pro
&lt;/p&gt;</description></item><item><title>SIDDMs&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#38544;&#24335;&#21644;&#26174;&#24335;&#22240;&#23376;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#24555;&#36895;&#25910;&#25947;&#19988;&#19968;&#23450;&#31243;&#24230;&#19978;&#20445;&#35777;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.12511</link><description>&lt;p&gt;
&#21322;&#38544;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;SIDDMs&#65289;
&lt;/p&gt;
&lt;p&gt;
Semi-Implicit Denoising Diffusion Models (SIDDMs). (arXiv:2306.12511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12511
&lt;/p&gt;
&lt;p&gt;
SIDDMs&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#38544;&#24335;&#21644;&#26174;&#24335;&#22240;&#23376;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#24555;&#36895;&#25910;&#25947;&#19988;&#19968;&#23450;&#31243;&#24230;&#19978;&#20445;&#35777;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#37327;&#27491;&#22312;&#22686;&#21152;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#24555;&#36895;&#37319;&#26679;&#32780;&#19981;&#29306;&#29298;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#65288;&#22914; DDPMS&#65289;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#65292;&#20016;&#23500;&#22810;&#26679;&#30340;&#26679;&#26412;&#65292;&#20294;&#21463;&#36845;&#20195;&#27493;&#39588;&#25968;&#37327;&#30340;&#22266;&#26377;&#38480;&#21046;&#32780;&#36895;&#24230;&#36739;&#24930;&#12290;Denoising Diffusion Generative Adversarial Networks (DDGAN) &#35797;&#22270;&#36890;&#36807;&#38598;&#25104; GAN &#27169;&#22411;&#29992;&#20110;&#25193;&#25955;&#36807;&#31243;&#30340;&#36739;&#22823;&#36339;&#36291;&#26469;&#35268;&#36991;&#27492;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#65292;DDGAN &#36935;&#21040;&#20102;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#38544;&#24335;&#21644;&#26174;&#24335;&#22240;&#23376;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21033;&#29992;&#38544;&#24335;&#27169;&#22411;&#26469;&#21305;&#37197;&#22024;&#26434;&#25968;&#25454;&#30340;&#36793;&#32536;&#20998;&#24067;&#21644;&#21069;&#21521;&#25193;&#25955;&#30340;&#26174;&#24335;&#26465;&#20214;&#20998;&#24067;&#12290;&#36825;&#31181;&#32452;&#21512;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#21305;&#37197;&#32852;&#21512;&#21435;&#22122;&#20998;&#24067;&#12290;&#19982; DDPMS &#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21322;&#38544;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;SIDDMs&#65289;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#25152;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the proliferation of generative models, achieving fast sampling during inference without compromising sample diversity and quality remains challenging. Existing models such as Denoising Diffusion Probabilistic Models (DDPM) deliver high-quality, diverse samples but are slowed by an inherently high number of iterative steps. The Denoising Diffusion Generative Adversarial Networks (DDGAN) attempted to circumvent this limitation by integrating a GAN model for larger jumps in the diffusion process. However, DDGAN encountered scalability limitations when applied to large datasets. To address these limitations, we introduce a novel approach that tackles the problem by matching implicit and explicit factors. More specifically, our approach involves utilizing an implicit model to match the marginal distributions of noisy data and the explicit conditional distribution of the forward diffusion. This combination allows us to effectively match the joint denoising distributions. Unlike DDPM
&lt;/p&gt;</description></item><item><title>DCdetector&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#21452;&#37325;&#20851;&#27880;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#21033;&#29992;&#21452;&#37325;&#20851;&#27880;&#19981;&#23545;&#31216;&#35774;&#35745;&#21644;&#32431;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#34920;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;&#24322;&#24120;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.10347</link><description>&lt;p&gt;
DCdetector: &#21452;&#37325;&#20851;&#27880;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection. (arXiv:2306.10347v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10347
&lt;/p&gt;
&lt;p&gt;
DCdetector&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#21452;&#37325;&#20851;&#27880;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#21033;&#29992;&#21452;&#37325;&#20851;&#27880;&#19981;&#23545;&#31216;&#35774;&#35745;&#21644;&#32431;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#34920;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;&#24322;&#24120;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#30446;&#26631;&#26159;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#20986;&#19982;&#27491;&#24120;&#26679;&#26412;&#20998;&#24067;&#26377;&#24046;&#24322;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#26368;&#22522;&#26412;&#25361;&#25112;&#26159;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#24322;&#24120;&#30340;&#34920;&#31034;&#26144;&#23556;&#12290;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#20173;&#28982;&#20027;&#23548;&#30528;&#35813;&#39046;&#22495;&#65292;&#20294;&#26159;&#20351;&#29992;&#24322;&#24120;&#25968;&#25454;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#21487;&#33021;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#25214;&#21040;&#19968;&#31181;&#33021;&#22815;&#26126;&#26174;&#21306;&#20998;&#20219;&#20309;&#23454;&#20363;&#30340;&#34920;&#31034;&#65292;&#36825;&#21487;&#20197;&#20026;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#26356;&#33258;&#28982;&#21644;&#26377;&#21069;&#26223;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DCdetector&#65292;&#19968;&#31181;&#22810;&#23610;&#24230;&#21452;&#37325;&#20851;&#27880;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;DCdetector&#21033;&#29992;&#26032;&#39062;&#30340;&#21452;&#37325;&#20851;&#27880;&#19981;&#23545;&#31216;&#35774;&#35745;&#21019;&#24314;&#32622;&#25442;&#29615;&#22659;&#65292;&#24182;&#20351;&#29992;&#32431;&#23545;&#27604;&#25439;&#22833;&#26469;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32622;&#25442;&#19981;&#21464;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is critical for a wide range of applications. It aims to identify deviant samples from the normal sample distribution in time series. The most fundamental challenge for this task is to learn a representation map that enables effective discrimination of anomalies. Reconstruction-based methods still dominate, but the representation learning with anomalies might hurt the performance with its large abnormal loss. On the other hand, contrastive learning aims to find a representation that can clearly distinguish any instance from the others, which can bring a more natural and promising representation for time series anomaly detection. In this paper, we propose DCdetector, a multi-scale dual attention contrastive representation learning model. DCdetector utilizes a novel dual attention asymmetric design to create the permutated environment and pure contrastive loss to guide the learning process, thus learning a permutation invariant representation with superior d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.09442</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#23454;&#29616;&#32418;&#38431;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;&#19982;&#24314;&#31435;
&lt;/p&gt;
&lt;p&gt;
Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#27602;&#25110;&#19981;&#35802;&#23454;&#38472;&#36848;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#20102;&#24037;&#20855;&#20197;&#35843;&#26597;&#26377;&#23475;&#36755;&#20986;&#65292;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#34429;&#28982;&#36825;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#30340;&#26377;&#20215;&#20540;&#27493;&#39588;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#38024;&#23545;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21482;&#26377;&#39044;&#20808;&#30693;&#36947;&#26377;&#23475;&#34892;&#20026;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#36339;&#36807;&#20102;&#32418;&#38431;&#34892;&#21160;&#30340;&#26680;&#24515;&#25361;&#25112;&#65306;&#24320;&#21457;&#27169;&#22411;&#21487;&#33021;&#23637;&#31034;&#30340;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#23384;&#22312;&#26102;&#65292;&#32418;&#38431;&#34892;&#21160;&#30340;&#36793;&#38469;&#20215;&#20540;&#26377;&#38480;&#65292;&#22240;&#20026;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#36755;&#20986;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20551;&#35774;&#23545;&#25163;&#20174;&#39640;&#32423;&#12289;&#25277;&#35937;&#30340;&#19981;&#33391;&#34892;&#20026;&#35268;&#33539;&#20986;&#21457;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32418;&#38431;&#34892;&#21160;&#12290;&#32418;&#38431;&#24212;&#35813;&#22312;&#31934;&#21270;/&#25193;&#23637;&#27492;&#35268;&#33539;&#30340;&#21516;&#26102;&#23545;&#25239;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;VHH&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#27979;&#25239;&#21407;-&#25239;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;VHHs&#30340;&#32467;&#26500;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;573,891&#20010;&#25239;&#21407;-VHH&#23545;&#65292;&#24182;&#19988;&#26159;&#24403;&#21069;&#20844;&#24320;&#25968;&#25454;&#38598;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2306.03329</link><description>&lt;p&gt;
AVIDa-hIL6&#65306;&#22522;&#20110;&#19968;&#21482;&#34987;&#20813;&#30123;&#32650;&#39548;&#30340;&#22823;&#35268;&#27169;VHH&#25968;&#25454;&#38598;&#29992;&#20110;&#39044;&#27979;&#25239;&#21407; - &#25239;&#20307;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
AVIDa-hIL6: A Large-Scale VHH Dataset Produced from an Immunized Alpaca for Predicting Antigen-Antibody Interactions. (arXiv:2306.03329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03329
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;VHH&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#27979;&#25239;&#21407;-&#25239;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;VHHs&#30340;&#32467;&#26500;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;573,891&#20010;&#25239;&#21407;-VHH&#23545;&#65292;&#24182;&#19988;&#26159;&#24403;&#21069;&#20844;&#24320;&#25968;&#25454;&#38598;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#24050;&#32463;&#25104;&#20026;&#27835;&#30103;&#20154;&#31867;&#30142;&#30149;&#30340;&#37325;&#35201;&#33647;&#29289;&#12290;&#20026;&#20102;&#21152;&#36895;&#27835;&#30103;&#25239;&#20307;&#30340;&#21457;&#29616;&#65292;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#25239;&#20307;&#20505;&#36873;&#21644;&#30446;&#26631;&#25239;&#21407;&#65288;&#22914;&#30149;&#27602;&#21644;&#32454;&#33740;&#65289;&#20043;&#38388;&#30340;&#29305;&#23450;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#26126;&#26174;&#30340;&#38480;&#21046;&#65292;&#22914;&#35268;&#27169;&#23567;&#65292;&#32570;&#20047;&#38750;&#32467;&#21512;&#26679;&#26412;&#21644;&#20934;&#30830;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AVIDa-hIL6, &#19968;&#20010;&#22823;&#35268;&#27169;&#30340;VHH&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#27979;&#20855;&#26377;&#20154;&#31867;&#30333;&#32454;&#32990;&#20171;&#32032;-6&#65288;IL-6&#65289;&#34507;&#30333;&#65292;&#20316;&#20026;&#25239;&#21407;&#30340;VHHs&#30340;&#25239;&#21407; - &#25239;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;VHHs&#30340;&#31616;&#21333;&#32467;&#26500;&#65292;&#26377;&#21033;&#20110;&#36890;&#36807;DNA&#27979;&#24207;&#25216;&#26415;&#35782;&#21035;&#20840;&#38271;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;AVIDa-hIL6&#21253;&#21547;573,891&#20010;&#25239;&#21407; - VHH&#23545;&#65292;&#20854;&#20013;62,067&#23545;&#26159;&#32467;&#21512;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies have become an important class of therapeutic agents to treat human diseases. To accelerate therapeutic antibody discovery, computational methods, especially machine learning, have attracted considerable interest for predicting specific interactions between antibody candidates and target antigens such as viruses and bacteria. However, the publicly available datasets in existing works have notable limitations, such as small sizes and the lack of non-binding samples and exact amino acid sequences. To overcome these limitations, we have developed AVIDa-hIL6, a large-scale dataset for predicting antigen-antibody interactions in the variable domain of heavy chain of heavy chain antibodies (VHHs), produced from an alpaca immunized with the human interleukin-6 (IL-6) protein, as antigens. By leveraging the simple structure of VHHs, which facilitates identification of full-length amino acid sequences by DNA sequencing technology, AVIDa-hIL6 contains 573,891 antigen-VHH pairs with am
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#32467;&#21512;&#20102;&#25968;&#25454;&#20013;&#32534;&#30721;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#22810;&#27493;&#21644;&#38271;&#31243;&#39044;&#27979;&#33021;&#21147;&#65292;&#20855;&#26377;&#28789;&#27963;&#30340;&#37319;&#26679;&#36712;&#36857;&#21644;&#25240;&#34935;&#24615;&#33021;&#19982;&#21152;&#36895;&#37319;&#26679;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#21487;&#22312;&#26102;&#31354;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01984</link><description>&lt;p&gt;
DYffusion&#65306;&#38754;&#21521;&#26102;&#31354;&#39044;&#27979;&#30340;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting. (arXiv:2306.01984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01984
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#32467;&#21512;&#20102;&#25968;&#25454;&#20013;&#32534;&#30721;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#22810;&#27493;&#21644;&#38271;&#31243;&#39044;&#27979;&#33021;&#21147;&#65292;&#20855;&#26377;&#28789;&#27963;&#30340;&#37319;&#26679;&#36712;&#36857;&#21644;&#25240;&#34935;&#24615;&#33021;&#19982;&#21152;&#36895;&#37319;&#26679;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#21487;&#22312;&#26102;&#31354;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#22320;&#29983;&#25104;&#25968;&#25454;&#21644;&#20570;&#20986;&#39044;&#27979;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#26159;&#20026;&#38745;&#24577;&#22270;&#20687;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#29992;&#20110;&#21160;&#24577;&#39044;&#27979;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#32534;&#30721;&#22312;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#30452;&#25509;&#23558;&#20854;&#19982;&#32593;&#32476;&#20013;&#30340;&#25193;&#25955;&#27493;&#39588;&#32806;&#21512;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38543;&#26426;&#30340;&#12289;&#26102;&#38388;&#26465;&#20214;&#30340;&#25554;&#20540;&#22120;&#21644;&#19968;&#20010;&#39592;&#24178;&#39044;&#27979;&#32593;&#32476;&#65292;&#20998;&#21035;&#27169;&#20223;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#36807;&#31243;&#12290;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#22810;&#27493;&#21644;&#38271;&#31243;&#39044;&#27979;&#33021;&#21147;&#65292;&#20801;&#35768;&#39640;&#24230;&#28789;&#27963;&#30340;&#36830;&#32493;&#26102;&#38388;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#25240;&#34935;&#24615;&#33021;&#19982;&#21152;&#36895;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#38754;&#21521;&#21160;&#24577;&#30340;&#25193;&#25955;&#36807;&#31243;&#24378;&#21152;&#20102;&#24378;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#30456;&#27604;&#20256;&#32479;&#22522;&#20110;&#39640;&#26031;&#22122;&#22768;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27010;&#29575;&#28369;&#38634;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While diffusion models can successfully generate data and make predictions, they are predominantly designed for static images. We propose an approach for training diffusion models for dynamics forecasting that leverages the temporal dynamics encoded in the data, directly coupling it with the diffusion steps in the network. We train a stochastic, time-conditioned interpolator and a backbone forecaster network that mimic the forward and reverse processes of conventional diffusion models, respectively. This design choice naturally encodes multi-step and long-range forecasting capabilities, allowing for highly flexible, continuous-time sampling trajectories and the ability to trade-off performance with accelerated sampling at inference time. In addition, the dynamics-informed diffusion process imposes a strong inductive bias, allowing for improved computational efficiency compared to traditional Gaussian noise-based diffusion models. Our approach performs competitively on probabilistic ski
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#36153;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#20102;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01276</link><description>&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#20013;&#23545;&#31216;&#25506;&#32034;&#26159;&#20813;&#36153;&#30340;&#65281;
&lt;/p&gt;
&lt;p&gt;
Symmetric Exploration in Combinatorial Optimization is Free!. (arXiv:2306.01276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#36153;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#20102;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20813;&#36153;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#26469;&#22686;&#24378;&#20219;&#20309;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20445;&#30041;&#22870;&#21169;&#30340;&#21464;&#25442;&#26469;&#22686;&#24378;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#12290;&#35813;&#31639;&#27861;&#21487;&#33021;&#20855;&#26377;&#24433;&#21709;&#21147;&#65292;&#22240;&#20026;&#23427;&#31616;&#21333;&#65292;&#26131;&#20110;&#19982;&#29616;&#26377;&#27714;&#35299;&#22120;&#38598;&#25104;&#65292;&#24182;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;NP&#38590;&#30340;&#36335;&#32447;&#20248;&#21270;&#65292;&#35843;&#24230;&#20248;&#21270;&#21644;&#26032;&#22411;&#20998;&#23376;&#20248;&#21270;&#30340;&#24191;&#27867;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36731;&#26494;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;DRL&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep reinforcement learning (DRL) has shown promise in solving combinatorial optimization (CO) problems. However, they often require a large number of evaluations on the objective function, which can be time-consuming in real-world scenarios. To address this issue, we propose a "free" technique to enhance the performance of any deep reinforcement learning (DRL) solver by exploiting symmetry without requiring additional objective function evaluations. Our key idea is to augment the training of DRL-based combinatorial optimization solvers by reward-preserving transformations. The proposed algorithm is likely to be impactful since it is simple, easy to integrate with existing solvers, and applicable to a wide range of combinatorial optimization tasks. Extensive empirical evaluations on NP-hard routing optimization, scheduling optimization, and de novo molecular optimization confirm that our method effortlessly improves the sample efficiency of state-of-the-art DRL algorithms. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16340</link><description>&lt;p&gt;
&#20998;&#27573;&#24490;&#29615;Transformer:&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#25972;&#20010;&#24207;&#21015;&#21010;&#20998;&#25104;&#33509;&#24178;&#27573;&#12290;&#28982;&#21518;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#32467;&#26500;&#30340;&#31070;&#32463;&#20803;&#26469;&#32858;&#21512;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#36739;&#20302;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#30340;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#27169;&#22411;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;Attention&#26426;&#21046;&#23545;&#21333;&#20010;&#27573;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#65292;&#23427;&#23558;&#20998;&#27573;Attention&#21644;&#24490;&#29615;Attention&#30456;&#32467;&#21512;&#12290;&#23427;&#20351;&#29992;&#24490;&#29615;accumulate and fire&#65288;RAF&#65289;&#23618;&#22312;&#30456;&#37051;&#27573;&#20043;&#38388;&#22788;&#29702;&#20449;&#24687;&#12290;&#36890;&#36807;&#26356;&#26032;key&#30340;&#20135;&#21697;&#26469;&#34917;&#20607;&#20943;&#23569;Attention&#31383;&#21475;&#38271;&#24230;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;FACTSCORE&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20998;&#35299;&#20026;&#21407;&#23376;&#20107;&#23454;&#65292;&#24182;&#35745;&#31639;&#34987;&#21487;&#38752;&#30693;&#35782;&#28304;&#25903;&#25345;&#30340;&#27604;&#20363;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#20013;&#20165;&#26377;58%&#30340;ChatGPT&#20256;&#35760;&#36798;&#21040;&#20102;&#39640;&#27700;&#24179;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;&#24378;&#35821;&#35328;&#27169;&#22411;&#20272;&#35745;FACTSCORE&#65292;&#35823;&#24046;&#29575;&#20302;&#20110;2%&#12290;</title><link>http://arxiv.org/abs/2305.14251</link><description>&lt;p&gt;
FActScore: &#23545;&#38271;&#25991;&#26412;&#29983;&#25104;&#20013;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#32454;&#31890;&#24230;&#21407;&#23376;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. (arXiv:2305.14251v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;FACTSCORE&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20998;&#35299;&#20026;&#21407;&#23376;&#20107;&#23454;&#65292;&#24182;&#35745;&#31639;&#34987;&#21487;&#38752;&#30693;&#35782;&#28304;&#25903;&#25345;&#30340;&#27604;&#20363;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#20013;&#20165;&#26377;58%&#30340;ChatGPT&#20256;&#35760;&#36798;&#21040;&#20102;&#39640;&#27700;&#24179;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;&#24378;&#35821;&#35328;&#27169;&#22411;&#20272;&#35745;FACTSCORE&#65292;&#35823;&#24046;&#29575;&#20302;&#20110;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#26159;&#19968;&#39033;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#65288;1&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#36890;&#24120;&#21253;&#21547;&#25903;&#25345;&#21644;&#19981;&#25903;&#25345;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#20108;&#20803;&#21028;&#26029;&#36136;&#37327;&#19981;&#36275;&#65292;&#65288;2&#65289;&#20154;&#24037;&#35780;&#20272;&#32791;&#26102;&#19988;&#25104;&#26412;&#39640;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FACTSCORE&#65292;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#23558;&#29983;&#25104;&#20869;&#23481;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#21407;&#23376;&#20107;&#23454;&#65292;&#24182;&#35745;&#31639;&#34987;&#21487;&#38752;&#30693;&#35782;&#28304;&#25903;&#25345;&#30340;&#21407;&#23376;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#24471;&#20986;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#65288;InstructGPT&#12289;ChatGPT&#21644;&#22686;&#24378;&#25552;&#21462;PerplexityAI&#65289;&#29983;&#25104;&#30340;&#20154;&#29289;&#20256;&#35760;&#30340;FACTSCORE&#65292;&#24182;&#25253;&#36947;&#20102;&#26032;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#36825;&#26679;&#30340;&#32454;&#31890;&#24230;&#35780;&#20998;&#30340;&#38656;&#27714;&#65288;&#20363;&#22914;&#65292;ChatGPT&#21482;&#36798;&#21040;58%&#65289;&#12290;&#30001;&#20110;&#20154;&#24037;&#35780;&#20272;&#36153;&#26102;&#36153;&#21147;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#26816;&#32034;&#21644;&#24378;&#35821;&#35328;&#27169;&#22411;&#20272;&#35745;FACTSCORE&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#65292;&#35823;&#24046;&#29575;&#19981;&#36229;&#36807;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13172</link><description>&lt;p&gt;
&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33021;&#22815;&#35757;&#32451;&#20986;&#34920;&#29616;&#20248;&#31168;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#20854;&#20445;&#25345;&#30456;&#20851;&#24615;&#21644;&#32416;&#27491;&#38169;&#35823;&#30340;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#30830;&#23450;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#20960;&#24180;&#20986;&#29616;&#20102;&#35768;&#22810;&#32534;&#36753;LLMs&#30340;&#25216;&#26415;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#39640;&#25928;&#22320;&#25913;&#21464;LLMs&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#19981;&#23545;&#20854;&#20182;&#36755;&#20837;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;LLMs&#27169;&#22411;&#32534;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#23450;&#20041;&#21644;&#30456;&#20851;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20197;&#21450;&#23545;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26356;&#24378;&#22823;&#30340;&#35780;&#20272;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#25216;&#26415;&#22266;&#26377;&#30340;&#25345;&#20037;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#27599;&#31181;&#32534;&#36753;&#25216;&#26415;&#30340;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#24110;&#21161;&#31038;&#21306;&#22312;LLMs&#30340;&#31649;&#29702;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in ma
&lt;/p&gt;</description></item><item><title>BertRLFuzzer&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#24037;&#20855;&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#65292;BertRLFuzzer&#30456;&#23545;&#20110;&#20854;&#20182;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#12289;&#26032;&#28431;&#27934;&#21457;&#29616;&#21644;&#25915;&#20987;&#29575;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.12534</link><description>&lt;p&gt;
BertRLFuzzer: &#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer
&lt;/p&gt;
&lt;p&gt;
BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer. (arXiv:2305.12534v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12534
&lt;/p&gt;
&lt;p&gt;
BertRLFuzzer&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#24037;&#20855;&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#65292;BertRLFuzzer&#30456;&#23545;&#20110;&#20854;&#20182;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#12289;&#26032;&#28431;&#27934;&#21457;&#29616;&#21644;&#25915;&#20987;&#29575;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;BertRLFuzzer&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;BertRLFuzzer&#30340;&#24037;&#20316;&#21407;&#29702;&#22914;&#19979;&#65306;&#32473;&#23450;&#19968;&#32452;&#31181;&#23376;&#36755;&#20837;&#65292;Fuzzer&#23545;&#23427;&#20204;&#25191;&#34892;&#36981;&#24490;&#35821;&#27861;&#24182;&#24341;&#21457;&#25915;&#20987;&#30340;&#21464;&#24322;&#25805;&#20316;&#65292;&#20197;&#29983;&#25104;&#20505;&#36873;&#25915;&#20987;&#21521;&#37327;&#12290;BertRLFuzzer&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#39640;&#25928;&#23398;&#20064;&#36981;&#24490;&#35821;&#27861;&#21644;&#24341;&#21457;&#25915;&#20987;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#12290;&#20026;&#20102;&#39564;&#35777;BertRLFuzzer&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20849;&#35745;13&#20010;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;9&#20010;&#21463;&#23475;&#32773;&#32593;&#31449;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#28041;&#21450;&#36229;&#36807;16K&#34892;&#30340;&#28304;&#20195;&#30721;&#12290;&#30456;&#23545;&#20110;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#24037;&#20855;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#30340;&#26174;&#33879;&#25913;&#36827;&#65288;&#20943;&#23569;54&#65285;&#65289;&#65292;&#21457;&#29616;&#30340;&#26032;&#28431;&#27934;&#65288;17&#20010;&#26032;&#28431;&#27934;&#65289;&#21644;&#25915;&#20987;&#29575;&#65288;&#29983;&#25104;&#30340;&#25915;&#20987;&#21521;&#37327;&#22686;&#21152;&#20102;4.4&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL) based fuzzer aimed at finding security vulnerabilities for Web applications. BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performs grammar-adhering and attack-provoking mutation operations on them to generate candidate attack vectors. The key insight of BertRLFuzzer is the use of RL with a BERT model as an agent to guide the fuzzer to efficiently learn grammar-adhering and attack-provoking mutation operators. In order to establish the efficacy of BertRLFuzzer we compare it against a total of 13 black box and white box fuzzers over a benchmark of 9 victim websites with over 16K LOC. We observed a significant improvement, relative to the nearest competing tool, in terms of time to first attack (54% less), new vulnerabilities found (17 new vulnerabilities), and attack rate (4.4% more attack vectors generated).
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11141</link><description>&lt;p&gt;
Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11141
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;Clifford&#32676;&#65292;&#23427;&#26159;Clifford&#20195;&#25968;&#20013;&#30340;&#19968;&#20010;&#23376;&#32676;&#65292;&#20854;&#23450;&#20041;&#32463;&#36807;&#35843;&#25972;&#20197;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;&#20027;&#35201;&#22320;&#65292;&#35813;&#32676;&#30340;&#20316;&#29992;&#24418;&#25104;&#20102;&#19968;&#20010;&#27491;&#20132;&#33258;&#21516;&#26500;&#65292;&#25193;&#23637;&#21040;&#25972;&#20010;Clifford&#20195;&#25968;&#65292;&#21516;&#26102;&#23562;&#37325;&#22810;&#30690;&#20998;&#32423;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#24212;&#20110;&#22810;&#30690;&#20998;&#35299;&#30340;&#22810;&#20010;&#38750;&#31561;&#20215;&#23376;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#20316;&#29992;&#19981;&#20165;&#23562;&#37325;Clifford&#20195;&#25968;&#30340;&#21521;&#37327;&#31354;&#38388;&#32467;&#26500;&#65292;&#36824;&#23562;&#37325;&#20854;&#20056;&#27861;&#32467;&#26500;&#65292;&#21363;&#20960;&#20309;&#20056;&#31215;&#12290;&#36825;&#20123;&#21457;&#29616;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#22312;&#20219;&#24847;&#32500;&#30340;&#20869;&#31215;&#31354;&#38388;&#20013;&#20248;&#38597;&#22320;&#25512;&#24191;&#30340;&#34920;&#36798;&#23618;&#12290;&#25105;&#20204;&#29305;&#21035;&#23637;&#31034;&#20102;&#20174;&#19968;&#20010;sin
&lt;/p&gt;
&lt;p&gt;
We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;$\lambda$-\textit{equitune}&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09900</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;$\lambda$-\textit{equitune}&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#26159;&#22522;&#30784;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697; \cite{basu2022equi} &#21644; \cite{kaba2022equivariance} &#20998;&#21035;&#25552;&#20986;&#20102;&#20351;&#29992;&#20174;&#32676;&#21464;&#25442;&#36755;&#20837;&#24471;&#21040;&#30340;&#29305;&#24449;&#30340;&#32676;&#24179;&#22343;&#20540;&#65288;\textit{equitune}&#65289;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#20174;&#19981;&#31561;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#21462;&#31561;&#21464;&#36755;&#20986;&#12290;&#34429;&#28982; \cite{kaba2022equivariance} &#21482;&#20851;&#27880;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#33391;&#22909;&#30340;&#24494;&#35843;&#32467;&#26524;&#19979;&#65292;\textit{equitune} &#22312;&#31561;&#21464;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#26576;&#20123;&#36716;&#25442;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#65292;&#32780;&#23545;&#20854;&#36827;&#34892;&#31616;&#21333;&#24179;&#22343;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#30340;$\lambda$-\textit{equitune} &#26041;&#27861;&#12290;&#36825;&#20123;&#26435;&#37325;&#26159;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging (\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\lambda$-\textit{equitune} that averages the features using \textit{importance weights}, $\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08732</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26222;&#36890;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21333;&#29420;&#22788;&#29702;&#30693;&#35782;&#23494;&#38598;&#22411;NLP&#20219;&#21153;&#30340;&#33021;&#21147;&#19981;&#36275;&#65292;&#22240;&#27492;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;&#22806;&#37096;&#30693;&#35782;&#38598;&#25104;&#21040;PLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;PLM&#21487;&#33021;&#24050;&#32463;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20294;&#22312;&#24212;&#29992;&#21040;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#23427;&#20204;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;PLMs&#20013;&#28155;&#21152;&#19968;&#20010;&#22914;&#8220;&#25454;&#25105;&#25152;&#30693;&#8221;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#35797;&#22270;&#22238;&#39038;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#27169;&#22411;&#20197;&#36827;&#34892;&#30693;&#35782;&#25972;&#21512;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#30693;&#35782;&#21453;&#24605;&#24212;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;RoBERTa&#12289;DeBERTa&#21644;GPT-3&#12290;&#22312;&#20845;&#20010;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;.....
&lt;/p&gt;
&lt;p&gt;
Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like "As far as I know" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.05658</link><description>&lt;p&gt;
TidyBot: &#24212;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#26426;&#22120;&#20154;&#29289;&#29702;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#29289;&#29702;&#36741;&#21161;&#65292;&#23427;&#24517;&#39035;&#23398;&#20064;&#29992;&#25143;&#30340;&#20010;&#20154;&#21916;&#22909;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#25441;&#36215;&#29289;&#21697;&#24182;&#23558;&#20854;&#25918;&#22238;&#21407;&#22788;&#26469;&#25972;&#29702;&#25151;&#38388;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#30830;&#23450;&#27599;&#20010;&#29289;&#21697;&#30340;&#27491;&#30830;&#20301;&#32622;&#65292;&#22240;&#20026;&#20154;&#20204;&#30340;&#21916;&#22909;&#21487;&#20197;&#22240;&#20010;&#20154;&#21697;&#21619;&#25110;&#25991;&#21270;&#32972;&#26223;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#25277;&#23625;&#37324;&#65292;&#32780;&#21478;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#26550;&#23376;&#19978;&#12290;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#29305;&#23450;&#20154;&#30340;&#20808;&#21069;&#20132;&#20114;&#23398;&#20064;&#36825;&#26679;&#30340;&#21916;&#22909;&#65292;&#32780;&#21482;&#38656;&#35201;&#20960;&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#22522;&#20110;&#35821;&#35328;&#30340;&#35268;&#21010;&#21644;&#24863;&#30693;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#24191;&#27867;&#36866;&#29992;&#20110;&#26410;&#26469;&#20132;&#20114;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#21462;&#24471;&#20102;91.2%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#24615;&#24352;&#37327;CUR&#20998;&#35299;&#65288;RTCUR&#65289;&#65292;&#29992;&#20110;Tucker&#31209;&#35774;&#32622;&#19979;&#30340;&#22823;&#35268;&#27169;&#38750;&#20984;TRPCA&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#26367;&#25237;&#24433;&#30340;&#26694;&#26550;&#21644;&#24352;&#37327;CUR&#20998;&#35299;&#65292;&#24555;&#36895;&#23454;&#29616;&#23545;&#31232;&#30095;&#25439;&#22351;&#30340;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#65292;&#24182;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.04080</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#24352;&#37327;CUR&#20998;&#35299;&#65306;&#23545;&#31232;&#30095;&#25439;&#22351;&#36827;&#34892;&#24555;&#36895;&#20302;Tucker&#31209;&#24352;&#37327;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Tensor CUR Decompositions: Rapid Low-Tucker-Rank Tensor Recovery with Sparse Corruption. (arXiv:2305.04080v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#24615;&#24352;&#37327;CUR&#20998;&#35299;&#65288;RTCUR&#65289;&#65292;&#29992;&#20110;Tucker&#31209;&#35774;&#32622;&#19979;&#30340;&#22823;&#35268;&#27169;&#38750;&#20984;TRPCA&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#26367;&#25237;&#24433;&#30340;&#26694;&#26550;&#21644;&#24352;&#37327;CUR&#20998;&#35299;&#65292;&#24555;&#36895;&#23454;&#29616;&#23545;&#31232;&#30095;&#25439;&#22351;&#30340;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#65292;&#24182;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24352;&#37327;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;TRPCA&#65289;&#38382;&#39064;&#65292;&#23427;&#26159;&#30697;&#38453;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;RPCA&#65289;&#30340;&#24352;&#37327;&#25193;&#23637;&#65292;&#26088;&#22312;&#23558;&#32473;&#23450;&#30340;&#24352;&#37327;&#20998;&#35299;&#20026;&#22522;&#30784;&#20302;&#31209;&#20998;&#37327;&#21644;&#31232;&#30095;&#24322;&#24120;&#20998;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#24615;&#24352;&#37327;CUR&#20998;&#35299;&#65288;RTCUR&#65289;&#65292;&#29992;&#20110;Tucker&#31209;&#35774;&#32622;&#19979;&#30340;&#22823;&#35268;&#27169;&#38750;&#20984;TRPCA&#38382;&#39064;&#12290;RTCUR&#26159;&#22312;&#20132;&#26367;&#25237;&#24433;&#30340;&#26694;&#26550;&#19979;&#24320;&#21457;&#20986;&#26469;&#30340;&#65292;&#23427;&#22312;&#20302;&#31209;&#24352;&#37327;&#38598;&#21644;&#31232;&#30095;&#24352;&#37327;&#38598;&#20043;&#38388;&#36827;&#34892;&#25237;&#24433;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#24352;&#37327;CUR&#20998;&#35299;&#65292;&#22312;&#27599;&#20010;&#25237;&#24433;&#20013;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#24320;&#21457;&#20102;&#22235;&#20010;RTCUR&#21464;&#20307;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;RTCUR&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#20248;&#21183;&#65292;&#20197;&#23545;&#25239;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the tensor robust principal component analysis (TRPCA) problem, a tensorial extension of matrix robust principal component analysis (RPCA), that aims to split the given tensor into an underlying low-rank component and a sparse outlier component. This work proposes a fast algorithm, called Robust Tensor CUR Decompositions (RTCUR), for large-scale non-convex TRPCA problems under the Tucker rank setting. RTCUR is developed within a framework of alternating projections that projects between the set of low-rank tensors and the set of sparse tensors. We utilize the recently developed tensor CUR decomposition to substantially reduce the computational complexity in each projection. In addition, we develop four variants of RTCUR for different application settings. We demonstrate the effectiveness and computational advantages of RTCUR against state-of-the-art methods on both synthetic and real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;ReLU&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26032;&#22411;MILP&#37327;&#23376;&#20998;&#35299;&#26041;&#27861;&#65292;&#23588;&#20854;&#38024;&#23545;&#35299;&#20915;&#37327;&#23376;&#20301;&#21487;&#29992;&#24615;&#12289;&#22122;&#22768;&#21644;&#35823;&#24046;&#38480;&#21046;&#65292;&#21487;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#19982;&#31934;&#30830;&#24230;&#65292;&#23545;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#30340;&#21457;&#23637;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.00472</link><description>&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#20013;&#29992;&#20110;ReLU&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#39640;&#25928;MILP&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Efficient MILP Decomposition in Quantum Computing for ReLU Network Robustness. (arXiv:2305.00472v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;ReLU&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26032;&#22411;MILP&#37327;&#23376;&#20998;&#35299;&#26041;&#27861;&#65292;&#23588;&#20854;&#38024;&#23545;&#35299;&#20915;&#37327;&#23376;&#20301;&#21487;&#29992;&#24615;&#12289;&#22122;&#22768;&#21644;&#35823;&#24046;&#38480;&#21046;&#65292;&#21487;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#19982;&#31934;&#30830;&#24230;&#65292;&#23545;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#30340;&#21457;&#23637;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#65292;&#22914;&#22024;&#26434;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#65292;&#20026;&#35299;&#20915;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#37327;&#23376;&#20301;&#30340;&#21487;&#29992;&#24615;&#12289;&#22122;&#22768;&#21644;&#35823;&#24046;&#30340;&#38480;&#21046;&#23545;&#23454;&#38469;&#23454;&#26045;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;MILP&#20998;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#21407;&#22987;&#38382;&#39064;&#22823;&#23567;&#24182;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#21487;&#29992;&#30340;NISQ&#35774;&#22791;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#23558;&#21407;&#22987;&#38382;&#39064;&#20998;&#35299;&#25104;&#26356;&#23567;&#30340;&#23376;&#38382;&#39064;&#65292;&#28982;&#21518;&#20351;&#29992;&#32452;&#21512;&#30340;&#37327;&#23376;-&#32463;&#20856;&#30828;&#20214;&#26041;&#27861;&#36845;&#20195;&#22320;&#35299;&#20915;&#36825;&#20123;&#23376;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;Benders&#21644;Dantzig-Wolfe&#26041;&#27861;&#30340;MILP&#20998;&#35299;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;Benders&#25152;&#38656;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#32780;Dantzig-Wolfe&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#20445;&#25345;&#19981;&#21464;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;Dantzig-Wolfe&#20998;&#35299;&#23545;&#35777;&#26126;ReLU&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26102;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;MILP&#20998;&#35299;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#32463;&#20856;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging quantum computing technologies, such as Noisy Intermediate-Scale Quantum (NISQ) devices, offer potential advancements in solving mathematical optimization problems. However, limitations in qubit availability, noise, and errors pose challenges for practical implementation. In this study, we examine two decomposition methods for Mixed-Integer Linear Programming (MILP) designed to reduce the original problem size and utilize available NISQ devices more efficiently. We concentrate on breaking down the original problem into smaller subproblems, which are then solved iteratively using a combined quantum-classical hardware approach. We conduct a detailed analysis for the decomposition of MILP with Benders and Dantzig-Wolfe methods. In our analysis, we show that the number of qubits required to solve Benders is exponentially large in the worst-case, while remains constant for Dantzig-Wolfe. Additionally, we leverage Dantzig-Wolfe decomposition on the use-case of certifying the robustn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;</title><link>http://arxiv.org/abs/2304.14933</link><description>&lt;p&gt;
&#19968;&#39033;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#34701;&#21512;&#65288;&#20363;&#22914;&#25554;&#20540;&#25110;&#20219;&#21153;&#31639;&#26415;&#65289;&#23558;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#22810;&#20010;&#27169;&#22411;&#21512;&#24182;&#20197;&#29983;&#25104;&#22810;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25216;&#26415;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#25104;&#21151;&#65292;&#20854;&#20013;&#27169;&#22411;&#26159;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#21644;&#30456;&#21516;&#30340;&#21021;&#22987;&#21270;&#19979;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#34701;&#21512;&#65292;&#23558;&#27492;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#36827;&#34892;&#30740;&#31350;&#65292;&#22312;&#35813;&#30446;&#26631;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#30340;transformer&#21512;&#24182;&#21040;&#29305;&#23450;&#27169;&#24577;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#24433;&#21709;&#27169;&#22411;&#34701;&#21512;&#21518;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#21021;&#22987;&#21270;&#12289;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#26469;&#21305;&#37197;&#27169;&#24577;&#19981;&#21487;&#30693;&#22522;&#32447;&#30340;&#24615;&#33021;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#65289;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper, we expand on this concept to a multimodal setup by merging transformers trained on different modalities. Furthermore, we conduct our study for a novel goal where we can merge vision, language, and cross-modal transformers of a modality-specific architecture to create a parameter-efficient modality-agnostic architecture. Through comprehensive experiments, we systematically investigate the key factors impacting model performance after merging, including initialization, merging mechanisms, and model architectures. Our analysis leads to an effective training recipe for matching the performance of the modality-agnostic baseline (i.e. pre-trained from scratch) via model merging. Our code is availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#23558;&#24103;&#36716;&#21270;&#20026;&#26102;&#38388;&#24207;&#21015;&#24182;&#20351;&#29992;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#31639;&#27861;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2304.06841</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#35270;&#39057;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Video alignment using unsupervised learning of local and global features. (arXiv:2304.06841v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#23558;&#24103;&#36716;&#21270;&#20026;&#26102;&#38388;&#24207;&#21015;&#24182;&#20351;&#29992;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#31639;&#27861;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#35270;&#39057;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21363;&#21305;&#37197;&#21253;&#21547;&#30456;&#20284;&#27963;&#21160;&#30340;&#19968;&#23545;&#35270;&#39057;&#30340;&#24103;&#12290;&#35270;&#39057;&#23545;&#40784;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#65292;&#23613;&#31649;&#20004;&#20010;&#35270;&#39057;&#20043;&#38388;&#30340;&#25191;&#34892;&#36807;&#31243;&#21644;&#22806;&#35266;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#20173;&#38656;&#35201;&#24314;&#31435;&#31934;&#30830;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24103;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20154;&#29289;&#26816;&#27979;&#12289;&#23039;&#24577;&#20272;&#35745;&#21644;VGG&#32593;&#32476;&#19977;&#31181;&#26426;&#22120;&#35270;&#35273;&#24037;&#20855;&#20026;&#27599;&#20010;&#35270;&#39057;&#24103;&#24341;&#20837;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#22788;&#29702;&#21644;&#32452;&#21512;&#20197;&#26500;&#24314;&#20195;&#34920;&#35270;&#39057;&#30340;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#12290;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#30340;&#26032;&#29256;&#26412;&#65288;Diagonalized Dynamic Time Warping, DDTW&#65289;&#23545;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#28857;&#22312;&#20110;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#20219;&#20309;&#26032;&#31867;&#22411;&#30340;&#27963;&#21160;&#32780;&#26080;&#38656;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we tackle the problem of video alignment, the process of matching the frames of a pair of videos containing similar actions. The main challenge in video alignment is that accurate correspondence should be established despite the differences in the execution processes and appearances between the two videos. We introduce an unsupervised method for alignment that uses global and local features of the frames. In particular, we introduce effective features for each video frame by means of three machine vision tools: person detection, pose estimation, and VGG network. Then the features are processed and combined to construct a multidimensional time series that represent the video. The resulting time series are used to align videos of the same actions using a novel version of dynamic time warping named Diagonalized Dynamic Time Warping(DDTW). The main advantage of our approach is that no training is required, which makes it applicable for any new type of action without any need
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#37325;&#23545;&#27604;&#25439;&#22833;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;IMU&#23450;&#21521;&#20272;&#35745;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06548</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26680;&#37325;&#23545;&#27604;&#25439;&#22833;&#30340;IMU&#23450;&#21521;&#20272;&#35745;&#65306;&#26799;&#24230;&#19979;&#38477;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-kernel Correntropy-based Orientation Estimation of IMUs: Gradient Descent Methods. (arXiv:2304.06548v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#37325;&#23545;&#27604;&#25439;&#22833;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;IMU&#23450;&#21521;&#20272;&#35745;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMUs&#65289;&#30340;&#23450;&#21521;&#20272;&#35745;&#65306;&#37325;&#23545;&#27604;&#26799;&#24230;&#19979;&#38477;&#65288;CGD&#65289;&#21644;&#37325;&#23545;&#27604;&#35299;&#32806;&#23450;&#21521;&#20272;&#35745;&#65288;CDOE&#65289;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#35299;&#32806;&#23450;&#21521;&#20272;&#35745;&#65288;DOE&#65289;&#65292;&#20381;&#36182;&#20110;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20934;&#21017;&#65292;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#22806;&#37096;&#21152;&#36895;&#24230;&#21644;&#30913;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#22122;&#22768;&#36981;&#24490;&#19968;&#31181;&#37325;&#23614;&#20998;&#24067;&#26102;&#65292;&#22810;&#26680;&#37325;&#23545;&#27604;&#25439;&#22833;&#65288;MKCL&#65289;&#26159;&#26368;&#20248;&#30340;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23384;&#22312;&#20219;&#24847;&#22823;&#30340;&#31163;&#32676;&#20540;&#65292;MKCL&#30340;&#20272;&#35745;&#35823;&#24046;&#20063;&#26159;&#26377;&#30028;&#30340;&#12290;&#36890;&#36807;&#29992;MKCL&#26367;&#25442;&#26631;&#20934;MSE&#25104;&#26412;&#20989;&#25968;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;CGD&#21644;CDOE&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#20256;&#24863;&#22120;&#35774;&#32622;&#21644;&#36816;&#21160;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CGD&#21644;CDOE&#22312;&#31934;&#24230;&#12289;&#20581;&#22766;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two computationally efficient algorithms for the orientation estimation of inertial measurement units (IMUs): the correntropy-based gradient descent (CGD) and the correntropy-based decoupled orientation estimation (CDOE). Traditional methods, such as gradient descent (GD) and decoupled orientation estimation (DOE), rely on the mean squared error (MSE) criterion, making them vulnerable to external acceleration and magnetic interference. To address this issue, we demonstrate that the multi-kernel correntropy loss (MKCL) is an optimal objective function for maximum likelihood estimation (MLE) when the noise follows a type of heavy-tailed distribution. In certain situations, the estimation error of the MKCL is bounded even in the presence of arbitrarily large outliers. By replacing the standard MSE cost function with MKCL, we develop the CGD and CDOE algorithms. We evaluate the effectiveness of our proposed methods by comparing them with existing algorithms in various s
&lt;/p&gt;</description></item><item><title>SGDP&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#27969;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39044;&#21462;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;LBA&#22686;&#37327;&#27969;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#28151;&#21512;&#29305;&#24449;&#26469;&#36827;&#34892;&#25968;&#25454;&#39044;&#21462;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03864</link><description>&lt;p&gt;
SGDP: &#19968;&#31181;&#22522;&#20110;&#27969;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39044;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
SGDP: A Stream-Graph Neural Network Based Data Prefetcher. (arXiv:2304.03864v1 [cs.OS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03864
&lt;/p&gt;
&lt;p&gt;
SGDP&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#27969;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39044;&#21462;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;LBA&#22686;&#37327;&#27969;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#28151;&#21512;&#29305;&#24449;&#26469;&#36827;&#34892;&#25968;&#25454;&#39044;&#21462;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39044;&#21462;&#23545;&#20110;&#23384;&#20648;&#31995;&#32479;&#20248;&#21270;&#21644;&#35775;&#38382;&#24615;&#33021;&#25552;&#21319;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#39044;&#21462;&#22120;&#36866;&#29992;&#20110;&#25366;&#25496;&#39034;&#24207;&#36923;&#36753;&#22359;&#22320;&#22336;&#65288;LBA&#65289;&#30340;&#35775;&#38382;&#27169;&#24335;&#65292;&#20294;&#26159;&#26080;&#27861;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22797;&#26434;&#38750;&#39034;&#24207;&#27169;&#24335;&#12290;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#39044;&#21462;&#22120;&#35206;&#30422;&#20102;&#26356;&#22810;&#30340;LBA&#35775;&#38382;&#65292;&#20294;&#26159;&#23427;&#20204;&#19981;&#36275;&#20197;&#20805;&#20998;&#32771;&#34385;LBA&#22686;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#23548;&#33268;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#27969;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39044;&#21462;&#22120;&#65288;SGDP&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SGDP&#20351;&#29992;&#21152;&#26435;&#26377;&#21521;&#22270;&#32467;&#26500;&#26469;&#24314;&#27169;LBA&#22686;&#37327;&#27969;&#20197;&#34920;&#31034;LBA&#22686;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#19968;&#27493;&#25552;&#21462;&#28151;&#21512;&#29305;&#24449;&#36827;&#34892;&#25968;&#25454;&#39044;&#21462;&#12290;&#25105;&#20204;&#23545;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;SGDP&#22312;&#21629;&#20013;&#29575;&#26041;&#38754;&#20248;&#20110;SOTA&#26041;&#27861;6.21&#65285;&#65292;&#25928;&#26524;&#20063;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data prefetching is important for storage system optimization and access performance improvement. Traditional prefetchers work well for mining access patterns of sequential logical block address (LBA) but cannot handle complex non-sequential patterns that commonly exist in real-world applications. The state-of-the-art (SOTA) learning-based prefetchers cover more LBA accesses. However, they do not adequately consider the spatial interdependencies between LBA deltas, which leads to limited performance and robustness. This paper proposes a novel Stream-Graph neural network-based Data Prefetcher (SGDP). Specifically, SGDP models LBA delta streams using a weighted directed graph structure to represent interactive relations among LBA deltas and further extracts hybrid features by graph neural networks for data prefetching. We conduct extensive experiments on eight real-world datasets. Empirical results verify that SGDP outperforms the SOTA methods in terms of the hit ratio by 6.21%, the effe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#23646;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#20256;&#25773;&#27880;&#20837;&#30340;&#22806;&#35266;&#21040;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.17546</link><description>&lt;p&gt;
PAIR-Diffusion: &#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models. (arXiv:2303.17546v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#23646;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#20256;&#25773;&#27880;&#20837;&#30340;&#22806;&#35266;&#21040;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#21457;&#23637;&#36805;&#36895;&#12290;&#20197;&#21069;&#30340;&#20316;&#21697;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#26041;&#24335;&#36827;&#34892;&#25511;&#21046;&#21644;&#32534;&#36753;&#22270;&#20687;&#65292;&#26576;&#20123;&#20316;&#21697;&#20351;&#29992;&#39640;&#32423;&#26465;&#20214;&#65288;&#20363;&#22914;&#25991;&#26412;&#65289;&#65292;&#32780;&#20854;&#20182;&#20316;&#21697;&#20351;&#29992;&#20302;&#32423;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20316;&#21697;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#19981;&#21516;&#23545;&#35937;&#30340;&#23646;&#24615;&#36827;&#34892;&#31934;&#32454;&#21270;&#25511;&#21046;&#65292;&#21363;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#12290;&#26412;&#25991;&#23558;&#22270;&#20687;&#35270;&#20026;&#30001;&#22810;&#20010;&#23545;&#35937;&#32452;&#25104;&#65292;&#27599;&#20010;&#23545;&#35937;&#30001;&#19981;&#21516;&#23646;&#24615;&#23450;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#32467;&#26500;&#21644;&#22806;&#35266;&#26159;&#26368;&#30452;&#35266;&#19988;&#26368;&#26377;&#29992;&#20110;&#32534;&#36753;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#65288;PAIR-Diffusion&#65289;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20174;&#22270;&#20687;&#20013;&#26126;&#30830;&#25552;&#21462;&#30340;&#32467;&#26500;&#21644;&#22806;&#35266;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#23545;&#35937;&#21644;&#20840;&#23616;&#32423;&#21035;&#23558;&#21442;&#32771;&#22270;&#20687;&#30340;&#22806;&#35266;&#27880;&#20837;&#36755;&#20837;&#22270;&#20687;&#20013;&#12290;&#27492;&#22806;&#65292;PAIR-Diffusion&#33258;&#21160;&#23558;&#27880;&#20837;&#30340;&#22806;&#35266;&#20256;&#25773;&#21040;&#36755;&#20837;&#22270;&#20687;&#20013;&#20855;&#26377;&#31867;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image editing using diffusion models has witnessed extremely fast-paced growth recently. There are various ways in which previous works enable controlling and editing images. Some works use high-level conditioning such as text, while others use low-level conditioning. Nevertheless, most of them lack fine-grained control over the properties of the different objects present in the image, i.e. object-level image editing. In this work, we consider an image as a composition of multiple objects, each defined by various properties. Out of these properties, we identify structure and appearance as the most intuitive to understand and useful for editing purposes. We propose Structure-and-Appearance Paired Diffusion model (PAIR-Diffusion), which is trained using structure and appearance information explicitly extracted from the images. The proposed model enables users to inject a reference image's appearance into the input image at both the object and global levels. Additionally, PAIR-Diffusion a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#31283;&#23450;&#24615;&#26469;&#22238;&#31572;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.13093</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Probabilistic Stability of Stochastic Gradient Descent. (arXiv:2303.13093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#31283;&#23450;&#24615;&#26469;&#22238;&#31572;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#24320;&#25918;&#38382;&#39064;&#26159;&#22914;&#20309;&#23450;&#20041;&#21644;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#25509;&#36817;&#22266;&#23450;&#28857;&#30340;&#31283;&#23450;&#24615;&#12290;&#20256;&#32479;&#25991;&#29486;&#20381;&#36182;&#20110;&#21442;&#25968;&#32479;&#35745;&#30697;&#65292;&#29305;&#21035;&#26159;&#21442;&#25968;&#26041;&#24046;&#30340;&#25910;&#25947;&#26469;&#37327;&#21270;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;SGD&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;\textit{&#27010;&#29575;&#25910;&#25947;}&#26465;&#20214;&#26469;&#23450;&#20041;SGD&#30340;\textit{&#27010;&#29575;&#31283;&#23450;&#24615;}&#12290;&#25552;&#20986;&#30340;&#31283;&#23450;&#24615;&#30452;&#25509;&#22238;&#31572;&#20102;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#22312;&#27010;&#29575;&#24615;&#31283;&#23450;&#24615;&#30340;&#38236;&#22836;&#19979;&#65292;SGD&#25165;&#34920;&#29616;&#20986;&#20016;&#23500;&#32780;&#23454;&#38469;&#30456;&#20851;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#22914;&#23436;&#20840;&#22833;&#21435;&#31283;&#23450;&#24615;&#38454;&#27573;&#12289;&#19981;&#27491;&#30830;&#23398;&#20064;&#38454;&#27573;&#12289;&#25910;&#25947;&#21040;&#20302;&#31209;&#38797;&#28857;&#38454;&#27573;&#21644;&#27491;&#30830;&#23398;&#20064;&#38454;&#27573;&#12290;&#24403;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36825;&#20123;&#30456;&#22270;&#24847;&#21619;&#30528;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#30340;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental open problem in deep learning theory is how to define and understand the stability of stochastic gradient descent (SGD) close to a fixed point. Conventional literature relies on the convergence of statistical moments, esp., the variance, of the parameters to quantify the stability. We revisit the definition of stability for SGD and use the \textit{convergence in probability} condition to define the \textit{probabilistic stability} of SGD. The proposed stability directly answers a fundamental question in deep learning theory: how SGD selects a meaningful solution for a neural network from an enormous number of solutions that may overfit badly. To achieve this, we show that only under the lens of probabilistic stability does SGD exhibit rich and practically relevant phases of learning, such as the phases of the complete loss of stability, incorrect learning, convergence to low-rank saddles, and correct learning. When applied to a neural network, these phase diagrams imply t
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#22686;&#37327;&#26356;&#26032;&#21644;&#27493;&#39588;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20027;&#35201;&#22270;&#27169;&#22411;&#21644;&#26500;&#24314;&#27969;&#31243;&#38656;&#27714;&#65292;&#23545;&#26500;&#24314;&#39640;&#36136;&#37327;&#30693;&#35782;&#22270;&#35889;&#30340;&#24517;&#35201;&#27493;&#39588;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2302.11509</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#65306;&#29616;&#29366;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Construction of Knowledge Graphs: State and Challenges. (arXiv:2302.11509v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11509
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#22686;&#37327;&#26356;&#26032;&#21644;&#27493;&#39588;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20027;&#35201;&#22270;&#27169;&#22411;&#21644;&#26500;&#24314;&#27969;&#31243;&#38656;&#27714;&#65292;&#23545;&#26500;&#24314;&#39640;&#36136;&#37327;&#30693;&#35782;&#22270;&#35889;&#30340;&#24517;&#35201;&#27493;&#39588;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30693;&#35782;&#22270;&#35889;&#22312;&#35832;&#22810;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#38382;&#31572;&#31995;&#32479;&#65292;&#26500;&#24314;&#21644;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#22270;&#35889;&#30340;&#36890;&#29992;&#27969;&#31243;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#25991;&#26412;&#65289;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#25968;&#25454;&#24211;&#65289;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#21508;&#20010;&#27493;&#39588;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#30446;&#21069;&#23545;&#20110;&#22686;&#37327;&#26356;&#26032;&#30693;&#35782;&#22270;&#35889;&#21644;&#21508;&#20010;&#27493;&#39588;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#39318;&#20808;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#20027;&#35201;&#22270;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#26410;&#26469;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#27969;&#31243;&#30340;&#20027;&#35201;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30693;&#35782;&#22270;&#35889;&#25152;&#38656;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#21253;&#25324;&#20803;&#25968;&#25454;&#31649;&#29702;&#12289;&#26412;&#20307;&#21457;&#23637;&#21644;&#36136;&#37327;&#20445;&#35777;&#31561;&#20132;&#21449;&#20027;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19982;&#24050;&#20171;&#32461;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#30340;&#29305;&#23450;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
With knowledge graphs (KGs) at the center of numerous applications such as recommender systems and question answering, the need for generalized pipelines to construct and continuously update such KGs is increasing. While the individual steps that are necessary to create KGs from unstructured (e.g. text) and structured data sources (e.g. databases) are mostly well-researched for their one-shot execution, their adoption for incremental KG updates and the interplay of the individual steps have hardly been investigated in a systematic manner so far. In this work, we first discuss the main graph models for KGs and introduce the major requirement for future KG construction pipelines. Next, we provide an overview of the necessary steps to build high-quality KGs, including cross-cutting topics such as metadata management, ontology development, and quality assurance. We then evaluate the state of the art of KG construction w.r.t the introduced requirements for specific popular KGs as well as so
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;ProbConserv&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23432;&#24658;&#32422;&#26463;&#19982;&#36125;&#21494;&#26031;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23558;&#23432;&#24658;&#32422;&#26463;&#32435;&#20837;&#36890;&#29992;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#20197;&#20415;&#22312;&#23398;&#20064;&#39640;&#38590;&#24230;&#30340;PDE&#36816;&#31639;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.11002</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#20197;&#36981;&#23432;&#23432;&#24658;&#23450;&#24459;&#30340;&#29289;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Physical Models that Can Respect Conservation Laws. (arXiv:2302.11002v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11002
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;ProbConserv&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23432;&#24658;&#32422;&#26463;&#19982;&#36125;&#21494;&#26031;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23558;&#23432;&#24658;&#32422;&#26463;&#32435;&#20837;&#36890;&#29992;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#20197;&#20415;&#22312;&#23398;&#20064;&#39640;&#38590;&#24230;&#30340;PDE&#36816;&#31639;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#36817;&#19968;&#20123;&#24037;&#20316;&#38598;&#20013;&#22312;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#20449;&#24687;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#20854;&#20013;&#35768;&#22810;&#24037;&#20316;&#38598;&#20013;&#22312;&#30456;&#23545;&#8220;&#23481;&#26131;&#8221;&#30340;PDE&#31639;&#23376;&#65288;&#20363;&#22914;&#26925;&#22278;&#21644;&#25243;&#29289;&#32447;&#65289;&#19978;&#65292;&#32780;&#30456;&#23545;&#8220;&#22256;&#38590;&#8221;&#30340;PDE&#31639;&#23376;&#65288;&#20363;&#22914;&#21452;&#26354;&#32447;&#65289;&#21017;&#36739;&#23569;&#12290;&#22312;&#25968;&#20540;PDE&#26041;&#38754;&#65292;&#21518;&#19968;&#31181;&#38382;&#39064;&#31867;&#38656;&#35201;&#25511;&#21046;&#19968;&#31181;&#20307;&#31215;&#20803;&#32032;&#25110;&#23432;&#24658;&#32422;&#26463;&#31867;&#22411;&#65292;&#36825;&#34987;&#35270;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#25215;&#35834;&#65292;&#38656;&#35201;&#26080;&#32541;&#22320;&#23558;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in scientific machine learning (SciML) has focused on incorporating partial differential equation (PDE) information into the learning process. Much of this work has focused on relatively ``easy'' PDE operators (e.g., elliptic and parabolic), with less emphasis on relatively ``hard'' PDE operators (e.g., hyperbolic). Within numerical PDEs, the latter problem class requires control of a type of volume element or conservation constraint, which is known to be challenging. Delivering on the promise of SciML requires seamlessly incorporating both types of problems into the learning process. To address this issue, we propose ProbConserv, a framework for incorporating conservation constraints into a generic SciML architecture. To do so, ProbConserv combines the integral form of a conservation law with a Bayesian update. We provide a detailed analysis of ProbConserv on learning with the Generalized Porous Medium Equation (GPME), a widely-applicable parameterized family of PDEs that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#26223;&#35266;&#20998;&#26512;&#26469;&#39044;&#27979;&#36719;&#20214;&#39033;&#30446;&#20581;&#24247;&#29366;&#20917;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#24182;&#36873;&#25321;&#26356;&#22909;&#30340;&#23398;&#20064;&#25511;&#21046;&#21442;&#25968;&#65292;&#36890;&#36807;niSNEAK&#24037;&#20855;&#33021;&#22815;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#24555;&#19988;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#26356;&#20934;&#30830;&#30340;&#37197;&#32622;&#65292;&#20943;&#23569;&#20102;&#39044;&#27979;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2301.06577</link><description>&lt;p&gt;
&#23398;&#20064;&#23569;&#37327;&#25968;&#25454;&#65306;&#20851;&#20110;&#20351;&#29992;&#26223;&#35266;&#20998;&#26512;&#26469;&#39044;&#27979;&#36719;&#20214;&#39033;&#30446;&#20581;&#24247;&#29366;&#20917;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health. (arXiv:2301.06577v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06577
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#26223;&#35266;&#20998;&#26512;&#26469;&#39044;&#27979;&#36719;&#20214;&#39033;&#30446;&#20581;&#24247;&#29366;&#20917;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#24182;&#36873;&#25321;&#26356;&#22909;&#30340;&#23398;&#20064;&#25511;&#21046;&#21442;&#25968;&#65292;&#36890;&#36807;niSNEAK&#24037;&#20855;&#33021;&#22815;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#24555;&#19988;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#26356;&#20934;&#30830;&#30340;&#37197;&#32622;&#65292;&#20943;&#23569;&#20102;&#39044;&#27979;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#36719;&#20214;&#20998;&#26512;&#21487;&#33021;&#20250;&#20986;&#29616;&#35768;&#22810;&#38169;&#35823;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#23398;&#20064;&#24320;&#28304;&#39033;&#30446;&#20581;&#24247;&#29366;&#20917;&#30340;&#39044;&#27979;&#22120;&#65288;&#20363;&#22914;&#65292;&#22312;&#21313;&#20108;&#20010;&#26376;&#21518;&#30340;&#24050;&#20851;&#38381;&#25289;&#21462;&#35831;&#27714;&#30340;&#25968;&#37327;&#65289;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#38750;&#24120;&#23569;&#65288;&#20363;&#22914;&#65292;&#20116;&#24180;&#30340;&#25968;&#25454;&#65292;&#27599;&#20010;&#26376;&#25910;&#38598;&#19968;&#27425;&#65292;&#21482;&#26377;60&#34892;&#30340;&#35757;&#32451;&#25968;&#25454;&#65289;&#12290;&#20174;&#22914;&#27492;&#23567;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#35768;&#22810;&#39044;&#27979;&#38169;&#35823;&#12290;&#36825;&#20123;&#38169;&#35823;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#26356;&#22909;&#30340;&#23398;&#20064;&#25511;&#21046;&#21442;&#25968;&#36827;&#34892;&#8220;&#26223;&#35266;&#20998;&#26512;&#8221;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;niSNEAK&#24037;&#20855;(a)~&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25214;&#21040;&#36229;&#21442;&#25968;&#30340;&#19968;&#33324;&#26223;&#35266;&#65307;&#28982;&#21518;(b)~&#20174;&#27599;&#20010;&#37096;&#20998;&#20013;&#25506;&#32034;&#20960;&#20010;&#20195;&#34920;&#24615;&#26679;&#26412;&#12290;niSNEAK&#27604;&#20043;&#21069;&#30340;&#26368;&#26032;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;&#65288;&#20363;&#22914;FLASH&#12289;HYPEROPT&#12289;OPTUNA&#65289;&#26356;&#24555;&#19988;&#26356;&#26377;&#25928;&#12290;niSNEAK&#25214;&#21040;&#30340;&#37197;&#32622;&#38169;&#35823;&#36828;&#36828;&#23569;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#39033;&#30446;&#20581;&#24247;&#25351;&#26631;&#65288;&#22914;$C$=&#25552;&#20132;&#27425;&#25968;&#65307;$I$=&#24050;&#20851;&#38381;&#30340;&#25289;&#21462;&#35831;&#27714;&#27425;&#25968;&#65289;&#65292;niSNEAK&#30340;&#38169;&#35823;&#35201;&#23569;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g. the number of closed pull requests in twelve months time). The training data for this task may be very small (e.g. five years of data, collected every month means just 60 rows of training data). The models generated from such tiny data sets can make many prediction errors.  Those errors can be tamed by a {\em landscape analysis} that selects better learner control parameters. Our niSNEAK tool (a)~clusters the data to find the general landscape of the hyperparameters; then (b)~explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g. FLASH, HYPEROPT, OPTUNA).  The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as $C$= number of commits; $I$=number of clos
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22522;&#20110;&#22330;&#26223;&#21644;&#23545;&#35937;&#30340;&#22270;&#20687;-&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#31867;&#22411;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.05174</link><description>&lt;p&gt;
&#22522;&#20110;&#22330;&#26223;&#21644;&#23545;&#35937;&#30340;&#22270;&#20687;-&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;&#65306;&#19968;&#39033;&#21487;&#22797;&#29616;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study. (arXiv:2301.05174v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05174
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22522;&#20110;&#22330;&#26223;&#21644;&#23545;&#35937;&#30340;&#22270;&#20687;-&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#31867;&#22411;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#36328;&#27169;&#24577;&#26816;&#32034;&#65288;CMR&#65289;&#26041;&#27861;&#35201;&#20040;&#32858;&#28966;&#20110;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#27599;&#20010;&#25991;&#26723;&#25551;&#32472;&#25110;&#25551;&#36848;&#19968;&#20010;&#21333;&#19968;&#23545;&#35937;&#65292;&#35201;&#20040;&#32858;&#28966;&#20110;&#20197;&#22330;&#26223;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#27599;&#20010;&#22270;&#20687;&#25551;&#32472;&#25110;&#25551;&#36848;&#30456;&#20114;&#20851;&#32852;&#30340;&#22810;&#20010;&#23545;&#35937;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#22330;&#26223;&#12290;&#25105;&#20204;&#35748;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;CMR&#27169;&#22411;&#24212;&#35813;&#22312;&#20004;&#31181;&#25968;&#25454;&#38598;&#31867;&#22411;&#19978;&#37117;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;CMR&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#32467;&#26524;&#30340;&#21487;&#22797;&#29616;&#24615;&#21450;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#31867;&#22411;&#19978;&#30340;&#27867;&#21270;&#24615;&#23578;&#26410;&#34987;&#30740;&#31350;&#36807;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#31354;&#30333;&#65292;&#24182;&#20851;&#27880;&#24403;&#22312;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#21644;&#20197;&#22330;&#26223;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#65292;&#26368;&#20808;&#36827;&#30340;CMR&#32467;&#26524;&#30340;&#21487;&#22797;&#29616;&#24615;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20004;&#31181;&#20855;&#26377;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;CMR&#27169;&#22411;&#65306;&#65288;i&#65289;CLIP&#65307;&#20197;&#21450;&#65288;ii&#65289;X-VLM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20004;&#20010;&#20197;&#22330;&#26223;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#30830;&#23450;&#20102;&#25152;&#36873;&#27169;&#22411;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#30340;&#30456;&#23545;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most approaches to cross-modal retrieval (CMR) focus either on object-centric datasets, meaning that each document depicts or describes a single object, or on scene-centric datasets, meaning that each image depicts or describes a complex scene that involves multiple objects and relations between them. We posit that a robust CMR model should generalize well across both dataset types. Despite recent advances in CMR, the reproducibility of the results and their generalizability across different dataset types has not been studied before. We address this gap and focus on the reproducibility of the state-of-the-art CMR results when evaluated on object-centric and scene-centric datasets. We select two state-of-the-art CMR models with different architectures: (i) CLIP; and (ii) X-VLM. Additionally, we select two scene-centric datasets, and three object-centric datasets, and determine the relative performance of the selected models on these datasets. We focus on reproducibility, replicability, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#36793;&#32536;&#35270;&#39057;&#20998;&#26512;&#65288;EVA&#65289;&#30340;&#24212;&#29992;&#12289;&#31995;&#32479;&#21644;&#25903;&#25345;&#25216;&#26415;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#21644;&#20114;&#32852;&#35774;&#22791;&#30340;&#26222;&#21450;&#65292;EVA&#20316;&#20026;&#19968;&#31181;&#22312;&#32593;&#32476;&#36793;&#32536;&#22788;&#29702;&#35270;&#39057;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2211.15751</link><description>&lt;p&gt;
&#36793;&#32536;&#35270;&#39057;&#20998;&#26512;&#65306;&#24212;&#29992;&#12289;&#31995;&#32479;&#21644;&#25903;&#25345;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Edge Video Analytics: A Survey on Applications, Systems and Enabling Techniques. (arXiv:2211.15751v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15751
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#36793;&#32536;&#35270;&#39057;&#20998;&#26512;&#65288;EVA&#65289;&#30340;&#24212;&#29992;&#12289;&#31995;&#32479;&#21644;&#25903;&#25345;&#25216;&#26415;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#21644;&#20114;&#32852;&#35774;&#22791;&#30340;&#26222;&#21450;&#65292;EVA&#20316;&#20026;&#19968;&#31181;&#22312;&#32593;&#32476;&#36793;&#32536;&#22788;&#29702;&#35270;&#39057;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20316;&#20026;&#20840;&#29699;&#25968;&#23383;&#20449;&#24687;&#29190;&#28856;&#30340;&#20851;&#38190;&#39537;&#21160;&#21147;&#65292;&#21487;&#20197;&#20026;&#20154;&#31867;&#31038;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#22909;&#22788;&#12290;&#25919;&#24220;&#21644;&#20225;&#19994;&#27491;&#22312;&#37096;&#32626;&#26080;&#25968;&#25668;&#20687;&#22836;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#25191;&#27861;&#12289;&#24212;&#24613;&#31649;&#29702;&#12289;&#20132;&#36890;&#25511;&#21046;&#21644;&#23433;&#20840;&#30417;&#25511;&#65292;&#36825;&#20123;&#37117;&#26159;&#30001;&#35270;&#39057;&#20998;&#26512;&#65288;VA&#65289;&#25152;&#25903;&#25345;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#23545;&#35937;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#36319;&#36394;&#30340;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#12290;&#21516;&#26102;&#65292;&#38543;&#30528;&#20114;&#32852;&#35774;&#22791;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#27599;&#22825;&#37117;&#20250;&#20135;&#29983;&#28023;&#37327;&#25968;&#25454;&#65292;&#36229;&#36234;&#20102;&#20113;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#36793;&#32536;&#35745;&#31639;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#23427;&#23558;&#24037;&#20316;&#36127;&#36733;&#21644;&#26381;&#21153;&#20174;&#32593;&#32476;&#26680;&#24515;&#31227;&#21160;&#21040;&#32593;&#32476;&#36793;&#32536;&#65292;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26032;&#20132;&#21449;&#39046;&#22495;&#65292;&#21363;&#36793;&#32536;&#35270;&#39057;&#20998;&#26512;&#65288;EVA&#65289;&#65292;&#24320;&#22987;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#21482;&#26377;&#23569;&#25968;&#26494;&#25955;&#30456;&#20851;&#30340;&#32508;&#36848;&#23384;&#22312;&#12290;EVA&#30340;&#22522;&#26412;&#27010;&#24565;&#65288;&#20363;&#22914;
&lt;/p&gt;
&lt;p&gt;
Video, as a key driver in the global explosion of digital information, can create tremendous benefits for human society. Governments and enterprises are deploying innumerable cameras for a variety of applications, e.g., law enforcement, emergency management, traffic control, and security surveillance, all facilitated by video analytics (VA). This trend is spurred by the rapid advancement of deep learning (DL), which enables more precise models for object classification, detection, and tracking. Meanwhile, with the proliferation of Internet-connected devices, massive amounts of data are generated daily, overwhelming the cloud. Edge computing, an emerging paradigm that moves workloads and services from the network core to the network edge, has been widely recognized as a promising solution. The resulting new intersection, edge video analytics (EVA), begins to attract widespread attention. Nevertheless, only a few loosely-related surveys exist on this topic. The basic concepts of EVA (e.g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#23427;&#21033;&#29992;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#26469;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#20855;&#26377;&#35782;&#21035;&#28508;&#22312;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#22823;&#33041;&#30340;&#32593;&#32476;&#24418;&#24335;&#65292;&#33021;&#22815;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#33041;&#32593;&#32476;&#20197;&#22270;&#24418;&#26041;&#24335;&#34920;&#31034;&#65292;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.12421</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#31070;&#32463;&#31185;&#23398;&#65306;&#20851;&#20110;&#25968;&#25454;&#25910;&#38598;&#19982;&#22522;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Network Neuroscience: On Data Collection and Benchmark. (arXiv:2211.12421v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#23427;&#21033;&#29992;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#26469;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#20855;&#26377;&#35782;&#21035;&#28508;&#22312;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#22823;&#33041;&#30340;&#32593;&#32476;&#24418;&#24335;&#65292;&#33021;&#22815;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#33041;&#32593;&#32476;&#20197;&#22270;&#24418;&#26041;&#24335;&#34920;&#31034;&#65292;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#34987;&#29992;&#20110;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#19988;&#22312;&#35782;&#21035;&#38463;&#23572;&#33576;&#28023;&#40664;&#27663;&#30151;&#12289;&#24085;&#37329;&#26862;&#30151;&#21644;&#33258;&#38381;&#30151;&#31561;&#28508;&#22312;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#20197;&#33041;&#32593;&#32476;&#30340;&#24418;&#24335;&#26469;&#30740;&#31350;&#22823;&#33041;&#30340;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#20316;&#20026;&#19968;&#20010;&#22270;&#24418;&#34920;&#31034;&#30340;&#33041;&#32593;&#32476;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#25506;&#32034;&#12290;&#20854;&#20013;&#20027;&#35201;&#30340;&#22256;&#38590;&#22312;&#20110;&#22797;&#26434;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive and quality collection of functional human brain \emph{network} data for potential research in the intersection of neuroscience, machine learning, and graph analytics. Anatomical and functional MRI images have been used to understand the functional connectivity of the human brain and are particularly important in identifying underlying neurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism. Recently, the study of the brain in the form of brain networks using machine learning and graph analytics has become increasingly popular, especially to predict the early onset of these conditions. A brain network, represented as a graph, retains rich structural and positional information that traditional examination methods are unable to capture. However, the lack of publicly accessible brain network data prevents researchers from data-driven explorations. One of the main difficulties lies in the complicated domain-specific preprocessing steps 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#31163;&#32447;&#26816;&#27979;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#33258;&#21160;&#21464;&#28857;&#26816;&#27979;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#26631;&#20934;CUSUM&#20998;&#31867;&#22120;&#24615;&#33021;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2211.03860</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#33258;&#21160;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automatic Change-Point Detection in Time Series via Deep Learning. (arXiv:2211.03860v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#31163;&#32447;&#26816;&#27979;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#33258;&#21160;&#21464;&#28857;&#26816;&#27979;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#26631;&#20934;CUSUM&#20998;&#31867;&#22120;&#24615;&#33021;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#21464;&#28857;&#26816;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#31163;&#32447;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#37327;&#21270;&#27492;&#26041;&#27861;&#30340;&#35823;&#24046;&#29575;&#21450;&#20854;&#19982;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#20851;&#31995;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#24615;&#33021;&#20063;&#21487;&#19982;&#29992;&#20110;&#26816;&#27979;&#20013;&#21464;&#21270;&#30340;&#26631;&#20934;CUSUM&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting change-points in data is challenging because of the range of possible types of change and types of behaviour of data when there is no change. Statistically efficient methods for detecting a change will depend on both of these features, and it can be difficult for a practitioner to develop an appropriate detection method for their application of interest. We show how to automatically generate new offline detection methods based on training a neural network. Our approach is motivated by many existing tests for the presence of a change-point being representable by a simple neural network, and thus a neural network trained with sufficient data should have performance at least as good as these methods. We present theory that quantifies the error rate for such an approach, and how it depends on the amount of training data. Empirical results show that, even with limited training data, its performance is competitive with the standard CUSUM-based classifier for detecting a change in m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#30340;&#25915;&#20987;&#20197;&#21450;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#36867;&#36991;&#38450;&#24481;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#23454;&#23041;&#32961;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.13660</link><description>&lt;p&gt;
SpacePhish: &#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#22120;&#30340;&#25915;&#20987;&#30340;&#36867;&#36991;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
SpacePhish: The Evasion-space of Adversarial Attacks against Phishing Website Detectors using Machine Learning. (arXiv:2210.13660v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#30340;&#25915;&#20987;&#20197;&#21450;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#36867;&#36991;&#38450;&#24481;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#23454;&#23041;&#32961;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#25991;&#29486;&#35201;&#20040;&#23637;&#31034;&#33021;&#22815;&#30772;&#22351;&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#35201;&#20040;&#23637;&#31034;&#33021;&#22815;&#25269;&#24481;&#22823;&#37096;&#20998;&#25915;&#20987;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#32771;&#34385;&#25915;&#20987;&#25110;&#38450;&#24481;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#25239;&#26679;&#26412;&#36890;&#24120;&#26159;&#22312;&#8220;&#29305;&#24449;&#31354;&#38388;&#8221;&#20013;&#29983;&#25104;&#30340;&#65292;&#36825;&#20351;&#24471;&#30456;&#20851;&#35780;&#20272;&#30340;&#20215;&#20540;&#20540;&#24471;&#24576;&#30097;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#30446;&#21069;&#30340;&#24773;&#20917;&#19981;&#20801;&#35768;&#20272;&#35745;&#23545;&#25239;&#25915;&#20987;&#25152;&#24102;&#26469;&#30340;&#30495;&#23454;&#23041;&#32961;&#65292;&#36825;&#23548;&#33268;&#32570;&#20047;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#26412;&#25991;&#26088;&#22312;&#28548;&#28165;&#36825;&#31181;&#22256;&#24785;&#12290;&#36890;&#36807;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#8220;&#36867;&#36991;&#31354;&#38388;&#8221;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#33021;&#22815;&#24341;&#20837;&#23545;&#25239;&#25200;&#21160;&#20197;&#27450;&#39575;&#26426;&#22120;&#23398;&#20064;-&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#30340;&#31354;&#38388;-&#35777;&#26126;&#21363;&#20351;&#22312;&#8220;&#29305;&#24449;&#31354;&#38388;&#8221;&#20013;&#30340;&#25200;&#21160;&#20063;&#26159;&#26377;&#29992;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25551;&#36848;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;-&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#30340;&#36867;&#36991;&#25915;&#20987;&#30340;&#29616;&#23454;&#23041;&#32961;&#27169;&#22411;&#65292;&#36825;&#31181;&#25915;&#20987;&#23481;&#26131;&#23454;&#26045;&#65292;&#22240;&#27492;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing literature on adversarial Machine Learning (ML) focuses either on showing attacks that break every ML model, or defenses that withstand most attacks. Unfortunately, little consideration is given to the actual feasibility of the attack or the defense. Moreover, adversarial samples are often crafted in the "feature-space", making the corresponding evaluations of questionable value. Simply put, the current situation does not allow to estimate the actual threat posed by adversarial attacks, leading to a lack of secure ML systems.  We aim to clarify such confusion in this paper. By considering the application of ML for Phishing Website Detection (PWD), we formalize the "evasion-space" in which an adversarial perturbation can be introduced to fool a ML-PWD -- demonstrating that even perturbations in the "feature-space" are useful. Then, we propose a realistic threat model describing evasion attacks against ML-PWD that are cheap to stage, and hence intrinsically more attractive for r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#26041;&#27861;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#12289;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#37051;&#25509;&#30697;&#38453;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#25581;&#31034;&#20102;&#32447;&#24615;GNN&#19982;&#38750;&#32447;&#24615;ReLU-GNN&#24615;&#33021;&#30456;&#24403;&#30340;&#29616;&#35937;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2210.09809</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#20999;&#32447;&#26680;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#65292;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel. (arXiv:2210.09809v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#26041;&#27861;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#12289;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#37051;&#25509;&#30697;&#38453;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#25581;&#31034;&#20102;&#32447;&#24615;GNN&#19982;&#38750;&#32447;&#24615;ReLU-GNN&#24615;&#33021;&#30456;&#24403;&#30340;&#29616;&#35937;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#36890;&#36807;&#20351;&#29992;&#8220;&#22270;&#21367;&#31215;&#8221;&#26469;&#32858;&#21512;&#30456;&#37051;&#33410;&#28857;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#26550;&#26500;&#65288;&#20363;&#22914;&#28145;&#24230;&#21644;&#28608;&#27963;&#20989;&#25968;&#65289;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#27599;&#20010;&#35774;&#35745;&#36873;&#25321;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30340;&#21367;&#31215;&#24050;&#25104;&#20026;&#20027;&#27969;&#36873;&#25321;&#65292;&#20854;&#20013;&#23545;&#37051;&#25509;&#30697;&#38453;&#36827;&#34892;&#23545;&#31216;&#24402;&#19968;&#21270;&#26159;&#26368;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#34892;&#24402;&#19968;&#21270;&#30340;&#37051;&#25509;&#30697;&#38453;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#23613;&#31649;GNN&#30340;&#20351;&#29992;&#38750;&#24120;&#24191;&#27867;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#20005;&#26684;&#30340;&#29702;&#35770;&#30740;&#31350;&#20851;&#20110;&#36825;&#20123;&#21367;&#31215;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#26080;&#27861;&#35299;&#37322;&#36825;&#31181;&#34892;&#20026;&#12290;&#21516;&#26679;&#65292;&#32447;&#24615;GNN&#30340;&#24615;&#33021;&#19982;&#38750;&#32447;&#24615;ReLU-GNN&#30340;&#24615;&#33021;&#30456;&#24403;&#30340;&#32463;&#39564;&#35266;&#23519;&#20063;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution' in conjunction with a suitable choice for the network architecture, such as depth and activation functions. Therefore, understanding the influence of each of the design choice on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix as the most widely adopted one. However, some empirical studies show that row normalization of the adjacency matrix outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolutions, that could explain this behavior. Similarly, the empirical observation of the linear GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory.  In this work, we theoretically a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20998;&#26512;&#21943;&#23376;&#30340;&#36712;&#36857;&#65292;&#35782;&#21035;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#24080;&#25143;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#20420;&#32599;&#26031;&#21943;&#23376;&#19982;&#26377;&#26426;&#29992;&#25143;&#65292;&#21487;&#25552;&#20379;&#22269;&#23478;&#36190;&#21161;&#24433;&#21709;&#27963;&#21160;&#30340;&#26089;&#26399;&#35686;&#25253;&#65292;&#24182;&#23545;&#20445;&#25252;&#27665;&#20027;&#36827;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2210.08786</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#34920;&#24449;&#21644;&#26816;&#27979;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Characterizing and Detecting State-Sponsored Troll Activity on Social Media. (arXiv:2210.08786v5 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20998;&#26512;&#21943;&#23376;&#30340;&#36712;&#36857;&#65292;&#35782;&#21035;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#24080;&#25143;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#20420;&#32599;&#26031;&#21943;&#23376;&#19982;&#26377;&#26426;&#29992;&#25143;&#65292;&#21487;&#25552;&#20379;&#22269;&#23478;&#36190;&#21161;&#24433;&#21709;&#27963;&#21160;&#30340;&#26089;&#26399;&#35686;&#25253;&#65292;&#24182;&#23545;&#20445;&#25252;&#27665;&#20027;&#36827;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#22312;&#24433;&#21709;&#27963;&#21160;&#20013;&#36816;&#33829;&#30340;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#26159;&#30740;&#31350;&#31038;&#21306;&#30340;&#19968;&#20010;&#20851;&#38190;&#32780;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#20854;&#24433;&#21709;&#36229;&#20986;&#20102;&#22312;&#32447;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;AI&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20998;&#26512;&#21943;&#23376;&#30340;&#20998;&#20139;&#27963;&#21160;&#24207;&#21015;&#25110;&#36712;&#36857;&#65292;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#35782;&#21035;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#24080;&#25143;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;LSTM&#30340;&#20998;&#31867;&#22120;&#23558;&#24080;&#25143;&#30340;&#36712;&#36857;&#20998;&#31867;&#20026;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#25110;&#26377;&#26426;&#30340;&#21512;&#27861;&#29992;&#25143;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#20998;&#31867;&#21518;&#30340;&#36712;&#36857;&#35745;&#31639;&#19968;&#31181;&#25351;&#26631;&#65292;&#31216;&#20026;&#8220;&#21943;&#23376;&#35780;&#20998;&#8221;&#65292;&#26469;&#37327;&#21270;&#24080;&#25143;&#30340;&#34892;&#20026;&#19982;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#30340;&#30456;&#20284;&#31243;&#24230;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;2016&#24180;&#32654;&#22269;&#24635;&#32479;&#22823;&#36873;&#26399;&#38388;&#30340;&#20420;&#32599;&#26031;&#24178;&#39044;&#27963;&#21160;&#12290;&#25105;&#20204;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#24080;&#25143;&#36712;&#36857;&#65292;AUC&#25509;&#36817;99&#65285;&#65292;&#24182;&#20934;&#30830;&#20998;&#31867;&#20420;&#32599;&#26031;&#21943;&#23376;&#21644;&#26377;&#26426;&#29992;&#25143;&#65292;F1&#20998;&#25968;&#20998;&#21035;&#20026;0.95&#21644;0.91&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#31995;&#32479;&#20013;&#65292;&#25552;&#20379;&#22269;&#23478;&#36190;&#21161;&#24433;&#21709;&#27963;&#21160;&#30340;&#26089;&#26399;&#35686;&#25253;&#65292;&#24182;&#23545;&#20445;&#25252;&#27665;&#20027;&#36827;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of state-sponsored trolls operating in influence campaigns is a critical and unsolved challenge for the research community, which has significant implications beyond the online realm. To address this challenge, we propose a new AI-based solution that identifies state-sponsored troll accounts by analyzing their sharing activity sequences, or trajectories, through a two-step process. First, we classify accounts' trajectories using an LSTM-based classifier as belonging to either a state-sponsored troll or an organic, legitimate user. Second, we utilize the classified trajectories to compute a metric, named ``Troll Score'', to quantify the extent to which an account behaves like a state-sponsored troll. To evaluate our approach, we examine the Russian interference campaign during the 2016 U.S. Presidential election. The results of our experiments show that our method can identify account trajectories with an AUC close to 99% and accurately classify Russian trolls and organic 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#24635;&#32467;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#30740;&#31350;&#65292;&#21253;&#25324;&#39640;&#25928;&#37319;&#26679;&#12289;&#20284;&#28982;&#20272;&#35745;&#19982;&#29305;&#27530;&#32467;&#26500;&#25968;&#25454;&#22788;&#29702;&#65292;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#24182;&#22238;&#39038;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2209.00796</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65306;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v10 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#24635;&#32467;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#30740;&#31350;&#65292;&#21253;&#25324;&#39640;&#25928;&#37319;&#26679;&#12289;&#20284;&#28982;&#20272;&#35745;&#19982;&#29305;&#27530;&#32467;&#26500;&#25968;&#25454;&#22788;&#29702;&#65292;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#24182;&#22238;&#39038;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31867;&#20855;&#26377;&#35760;&#24405;&#24615;&#33021;&#30340;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#21644;&#20998;&#23376;&#35774;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#36825;&#20221;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#25193;&#23637;&#30740;&#31350;&#65292;&#23558;&#30740;&#31350;&#20998;&#31867;&#20026;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65306;&#39640;&#25928;&#37319;&#26679;&#12289;&#25913;&#36827;&#30340;&#20284;&#28982;&#20272;&#35745;&#21644;&#22788;&#29702;&#20855;&#26377;&#29305;&#27530;&#32467;&#26500;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#20197;&#23454;&#29616;&#22686;&#24378;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22238;&#39038;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12289;&#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#20197;&#21450;&#20854;&#20182;&#31185;&#23398;&#23398;&#31185;&#30340;&#36328;&#23398;&#31185;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#20010;&#35843;&#30740;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#32972;&#26223;&#30340;&#28145;&#20837;&#20102;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;&#29616;&#29366;&#65292;&#30830;&#23450;&#20851;&#38190;&#30340;&#30740;&#31350;&#37325;&#28857;&#65292;&#25351;&#26126;&#21487;&#33021;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/Yan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#34892;&#35299;&#37322;&#19982;&#34987;&#36951;&#24536;&#26435;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#23569;&#37327;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#35302;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#26356;&#26032;&#26102;&#65292;&#30446;&#21069;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#29983;&#25104;&#30340;&#21487;&#34892;&#35299;&#37322;&#21487;&#33021;&#22833;&#25928;&#12290;</title><link>http://arxiv.org/abs/2208.14137</link><description>&lt;p&gt;
&#35770;&#21487;&#34892;&#35299;&#37322;&#19982;&#34987;&#36951;&#24536;&#26435;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
On the Trade-Off between Actionable Explanations and the Right to be Forgotten. (arXiv:2208.14137v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#34892;&#35299;&#37322;&#19982;&#34987;&#36951;&#24536;&#26435;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#23569;&#37327;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#35302;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#26356;&#26032;&#26102;&#65292;&#30446;&#21069;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#29983;&#25104;&#30340;&#21487;&#34892;&#35299;&#37322;&#21487;&#33021;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#20915;&#31574;&#32773;&#25552;&#20986;&#20102;&#26356;&#20005;&#26684;&#30340;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#65288;&#20363;&#22914;GDPR&#12289;CCPA&#65289;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#21407;&#21017;&#26159;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#65292;&#21363;&#29992;&#25143;&#26377;&#26435;&#35201;&#27714;&#21024;&#38500;&#20854;&#25968;&#25454;&#12290;&#21478;&#19968;&#20010;&#20851;&#38190;&#21407;&#21017;&#26159;&#21487;&#34892;&#35299;&#37322;&#26435;&#65292;&#20063;&#31216;&#20026;&#31639;&#27861;&#36861;&#32034;&#26435;&#65292;&#20801;&#35768;&#29992;&#25143;&#25764;&#38144;&#19981;&#21033;&#30340;&#20915;&#23450;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#23578;&#19981;&#28165;&#26970;&#36825;&#20004;&#20010;&#21407;&#21017;&#26159;&#21542;&#21487;&#20197;&#21516;&#26102;&#23454;&#26045;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#36861;&#32034;&#26080;&#25928;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#35282;&#24230;&#20998;&#26512;&#20102;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#29983;&#25104;&#30340;&#36861;&#32034;&#24456;&#21487;&#33021;&#20250;&#22312;&#23569;&#37327;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#65288;&#20363;&#22914;1&#25110;2&#20010;&#65289;&#24341;&#21457;&#23545;&#39044;&#27979;&#27169;&#22411;&#30340;&#26356;&#26032;&#26102;&#22833;&#25928;&#12290;&#23545;&#20110;&#19981;&#21487;&#21306;&#20998;&#27169;&#22411;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning (ML) models are increasingly being deployed in high-stakes applications, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA). One key principle is the "right to be forgotten" which gives users the right to have their data deleted. Another key principle is the right to an actionable explanation, also known as algorithmic recourse, allowing users to reverse unfavorable decisions. To date, it is unknown whether these two principles can be operationalized simultaneously. Therefore, we introduce and study the problem of recourse invalidation in the context of data deletion requests. More specifically, we theoretically and empirically analyze the behavior of popular state-of-the-art algorithms and demonstrate that the recourses generated by these algorithms are likely to be invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant updates of the predictive model. For the setting of differentiable models, we suggest a fra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#19979;&#22823;&#22411;&#27169;&#22411;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#20307;&#23376;&#27169;&#22411;&#35757;&#32451;&#65292;&#22312;&#19981;&#36829;&#21453;&#38544;&#31169;&#25215;&#35834;&#30340;&#21069;&#25552;&#19979;&#65292;&#20351;&#24369;&#20294;&#37325;&#35201;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#21442;&#19982;&#21040;&#21327;&#20316;&#35757;&#32451;&#20013;&#12290;</title><link>http://arxiv.org/abs/2208.13141</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#36827;&#34892;&#20027;&#20307;&#23376;&#27169;&#22411;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Large Models at the Edge via Principal Sub-Model Training. (arXiv:2208.13141v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#19979;&#22823;&#22411;&#27169;&#22411;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#20307;&#23376;&#27169;&#22411;&#35757;&#32451;&#65292;&#22312;&#19981;&#36829;&#21453;&#38544;&#31169;&#25215;&#35834;&#30340;&#21069;&#25552;&#19979;&#65292;&#20351;&#24369;&#20294;&#37325;&#35201;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#21442;&#19982;&#21040;&#21327;&#20316;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#21327;&#20316;&#35757;&#32451;&#65292;&#26080;&#38656;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#65292;&#20063;&#26080;&#38656;&#20256;&#36755;&#33267;&#20013;&#24515;&#26381;&#21153;&#22120;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#35768;&#22810;&#36793;&#32536;&#23458;&#25143;&#31471;&#32570;&#20047;&#36275;&#22815;&#30340;&#35745;&#31639;&#12289;&#20869;&#23384;&#25110;&#36890;&#20449;&#33021;&#21147;&#65292;&#32852;&#37030;&#23398;&#20064;&#22823;&#35268;&#27169;&#27169;&#22411;&#20173;&#38754;&#20020;&#37325;&#22823;&#29942;&#39048;&#12290;&#20026;&#20102;&#20445;&#25345;&#24369;&#20294;&#37325;&#35201;&#30340;&#23458;&#25143;&#31471;&#21442;&#19982;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#32771;&#34385;&#20102;&#24322;&#26500;&#23458;&#25143;&#31471;&#35774;&#32622;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65307;&#35201;&#20040;&#23558;&#35757;&#32451;&#20219;&#21153;&#36716;&#31227;&#21040;&#26381;&#21153;&#31471;&#12290;&#28982;&#32780;&#65292;&#24322;&#26500;&#23458;&#25143;&#31471;&#35774;&#32622;&#35201;&#27714;&#26576;&#20123;&#23458;&#25143;&#31471;&#35757;&#32451;&#23436;&#25972;&#27169;&#22411;&#65292;&#36825;&#19982;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#32622;&#19981;&#19968;&#33268;&#65307;&#32780;&#21518;&#32773;&#22312;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#25110;&#26631;&#31614;&#26102;&#36829;&#21453;&#20102;FL&#30340;&#38544;&#31169;&#25215;&#35834;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#29616;&#23454;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23569;&#25506;&#32034;&#30340;&#36328;&#35774;&#22791;FL&#35774;&#32622;&#65292;&#20854;&#20013;&#27809;&#26377;&#20219;&#20309;&#23458;&#25143;&#31471;&#38656;&#35201;&#35757;&#32451;&#23436;&#25972;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is emerging as a popular, promising decentralized learning framework that enables collaborative training among clients, with no need to share private data between them or to a centralized server. However, considering many edge clients do not have sufficient computing, memory, or communication capabilities, federated learning of large models still faces significant bottlenecks. To keep such weak but crucial clients in the loop, prior works either consider a heterogeneous-client setting where clients train models with different sizes; or offload training to the server. However, the heterogeneous-client setting requires some clients to train full model, which is not aligned with the resource-constrained setting; while the latter ones break privacy promises in FL when sharing intermediate representations or labels with the server. To overcome these limitations, in this work, we formulate a realistic, but much less explored, cross-device FL setting in which no client
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27880;&#37325;&#20844;&#27491;&#24615;&#30340;&#33402;&#26415;&#23637;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#23616;&#37096;&#32422;&#26463;&#22270;&#21305;&#37197;&#21644;&#20215;&#20540;&#23548;&#21521;&#30340;&#36164;&#28304;&#20998;&#37197;&#65292;&#23454;&#29616;&#20844;&#20849;&#33402;&#26415;&#23637;&#35272;&#30340;&#31574;&#21010;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;Schelling&#27169;&#22411;&#26500;&#24314;&#25104;&#26412;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#35780;&#20998;&#20989;&#25968;&#65292;&#36719;&#20998;&#37197;&#33402;&#26415;&#20316;&#21697;&#21040;&#20844;&#20849;&#31354;&#38388;&#65292;&#20197;&#20943;&#23569;&#20869;&#37096;&#32676;&#20307;&#20559;&#22909;&#12289;&#28385;&#36275;&#26368;&#20302;&#20195;&#34920;&#24615;&#21644;&#26333;&#20809;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2207.14367</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#32422;&#26463;&#22270;&#21305;&#37197;&#30340;&#27880;&#37325;&#20844;&#27491;&#24615;&#30340;&#33402;&#26415;&#23637;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Equity-Aware Recommender System for Curating Art Exhibits Based on Locally-Constrained Graph Matching. (arXiv:2207.14367v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14367
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27880;&#37325;&#20844;&#27491;&#24615;&#30340;&#33402;&#26415;&#23637;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#23616;&#37096;&#32422;&#26463;&#22270;&#21305;&#37197;&#21644;&#20215;&#20540;&#23548;&#21521;&#30340;&#36164;&#28304;&#20998;&#37197;&#65292;&#23454;&#29616;&#20844;&#20849;&#33402;&#26415;&#23637;&#35272;&#30340;&#31574;&#21010;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;Schelling&#27169;&#22411;&#26500;&#24314;&#25104;&#26412;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#35780;&#20998;&#20989;&#25968;&#65292;&#36719;&#20998;&#37197;&#33402;&#26415;&#20316;&#21697;&#21040;&#20844;&#20849;&#31354;&#38388;&#65292;&#20197;&#20943;&#23569;&#20869;&#37096;&#32676;&#20307;&#20559;&#22909;&#12289;&#28385;&#36275;&#26368;&#20302;&#20195;&#34920;&#24615;&#21644;&#26333;&#20809;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#33402;&#26415;&#22609;&#36896;&#20102;&#25105;&#20204;&#20849;&#20139;&#30340;&#31354;&#38388;&#12290;&#20844;&#20849;&#33402;&#26415;&#24212;&#35813;&#19982;&#31038;&#21306;&#21644;&#32972;&#26223;&#30456;&#20851;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#30693;&#21517;&#26426;&#26500;&#30340;&#33402;&#26415;&#20316;&#21697;&#20559;&#21521;&#20110;&#36807;&#26102;&#30340;&#25991;&#21270;&#35268;&#33539;&#21644;&#20256;&#32479;&#31038;&#32676;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#20197;&#20869;&#32622;&#20844;&#27491;&#30446;&#26631;&#21644;&#23616;&#37096;&#22522;&#20110;&#20215;&#20540;&#30340;&#26377;&#38480;&#36164;&#28304;&#20998;&#37197;&#26469;&#31574;&#21010;&#20844;&#20849;&#33402;&#26415;&#23637;&#35272;&#12290;&#25105;&#20204;&#21033;&#29992;Schelling&#30340;&#31181;&#26063;&#20998;&#31163;&#27169;&#22411;&#26500;&#24314;&#20102;&#25104;&#26412;&#30697;&#38453;&#12290;&#20351;&#29992;&#25104;&#26412;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#24471;&#21040;&#20102;&#19968;&#20010;&#36719;&#20998;&#37197;&#30697;&#38453;&#30340;&#35780;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;&#31243;&#24207;&#20197;&#19968;&#31181;&#26041;&#24335;&#23558;&#33402;&#26415;&#20316;&#21697;&#20998;&#37197;&#32473;&#20844;&#20849;&#31354;&#38388;&#65292;&#20197;&#38477;&#20302;&#8220;&#20869;&#37096;&#32676;&#20307;&#8221;&#20559;&#22909;&#65292;&#24182;&#28385;&#36275;&#26368;&#20302;&#20195;&#34920;&#24615;&#21644;&#26333;&#20809;&#26631;&#20934;&#12290;&#25105;&#20204;&#20511;&#37492;&#29616;&#26377;&#30340;&#25991;&#29486;&#20026;&#31639;&#27861;&#36755;&#20986;&#24320;&#21457;&#20102;&#19968;&#20010;&#20844;&#27491;&#24615;&#25351;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#20174;&#31574;&#23637;&#21644;&#20844;&#27491;&#24615;&#30340;&#35282;&#24230;&#35752;&#35770;&#20854;&#28508;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public art shapes our shared spaces. Public art should speak to community and context, and yet, recent work has demonstrated numerous instances of art in prominent institutions favoring outdated cultural norms and legacy communities. Motivated by this, we develop a novel recommender system to curate public art exhibits with built-in equity objectives and a local value-based allocation of constrained resources. We develop a cost matrix by drawing on Schelling's model of segregation. Using the cost matrix as an input, the scoring function is optimized via a projected gradient descent to obtain a soft assignment matrix. Our optimization program allocates artwork to public spaces in a way that de-prioritizes "in-group" preferences, by satisfying minimum representation and exposure criteria. We draw on existing literature to develop a fairness metric for our algorithmic output, and we assess the effectiveness of our approach and discuss its potential pitfalls from both a curatorial and equi
&lt;/p&gt;</description></item><item><title>DataPerf&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#20027;&#23548;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#36890;&#36807;&#31454;&#20105;&#12289;&#21487;&#27604;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#20419;&#36827;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2207.10062</link><description>&lt;p&gt;
DataPerf&#65306;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DataPerf: Benchmarks for Data-Centric AI Development. (arXiv:2207.10062v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10062
&lt;/p&gt;
&lt;p&gt;
DataPerf&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#20027;&#23548;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#36890;&#36807;&#31454;&#20105;&#12289;&#21487;&#27604;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#20419;&#36827;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#19968;&#30452;&#27880;&#37325;&#27169;&#22411;&#32780;&#19981;&#26159;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#33879;&#21517;&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#32780;&#24573;&#35270;&#20102;&#24213;&#23618;&#38382;&#39064;&#30340;&#24191;&#24230;&#12289;&#38590;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#24573;&#35270;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#23548;&#33268;&#20102;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#12289;&#20559;&#35265;&#21644;&#33030;&#24369;&#24615;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#36798;&#21040;&#20102;&#39281;&#21644;&#29366;&#24577;&#65292;&#38459;&#30861;&#20102;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DataPerf&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#20027;&#23548;&#30340;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#20013;&#24515;&#31639;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31454;&#20105;&#12289;&#21487;&#27604;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#20419;&#36827;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#21019;&#26032;&#12290;&#25105;&#20204;&#20351;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#33021;&#22815;&#36845;&#20195;&#25968;&#25454;&#38598;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#24320;&#25918;&#30340;&#22312;&#32447;&#24179;&#21488;&#65292;&#20197;&#25903;&#25345;&#36825;&#31181;&#36845;&#20195;&#24320;&#21457;&#12290;DataPerf&#30340;&#31532;&#19968;&#20010;&#36845;&#20195;&#21253;&#21547;&#20102;&#20116;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33719;&#21462;&#31561;&#22810;&#31181;&#25968;&#25454;&#20013;&#24515;&#25216;&#26415;&#12289;&#20219;&#21153;&#21644;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning research has long focused on models rather than datasets, and prominent datasets are used for common ML tasks without regard to the breadth, difficulty, and faithfulness of the underlying problems. Neglecting the fundamental importance of data has given rise to inaccuracy, bias, and fragility in real-world applications, and research is hindered by saturation across existing dataset benchmarks. In response, we present DataPerf, a community-led benchmark suite for evaluating ML datasets and data-centric algorithms. We aim to foster innovation in data-centric AI through competition, comparability, and reproducibility. We enable the ML community to iterate on datasets, instead of just architectures, and we provide an open, online platform with multiple rounds of challenges to support this iterative development. The first iteration of DataPerf contains five benchmarks covering a wide spectrum of data-centric techniques, tasks, and modalities in vision, speech, acquisition, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#35843;&#30740;&#65292;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;341&#31687;&#35770;&#25991;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#24178;&#39044;&#36807;&#31243;&#21644;&#24212;&#29992;&#25216;&#26415;&#65292;&#24182;&#35843;&#26597;&#20102;&#35780;&#20272;&#26041;&#27861;&#12290;&#36825;&#23558;&#20026;&#20174;&#19994;&#32773;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#26032;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#26102;&#25552;&#20379;&#26377;&#30410;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2207.07068</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#20559;&#24046;&#32531;&#35299;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey. (arXiv:2207.07068v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#35843;&#30740;&#65292;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;341&#31687;&#35770;&#25991;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#24178;&#39044;&#36807;&#31243;&#21644;&#24212;&#29992;&#25216;&#26415;&#65292;&#24182;&#35843;&#26597;&#20102;&#35780;&#20272;&#26041;&#27861;&#12290;&#36825;&#23558;&#20026;&#20174;&#19994;&#32773;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#26032;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#26102;&#25552;&#20379;&#26377;&#30410;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#24615;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#30740;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#24635;&#35745;341&#31687;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20559;&#24046;&#32531;&#35299;&#30340;&#30740;&#31350;&#35770;&#25991;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#23427;&#20204;&#30340;&#24178;&#39044;&#36807;&#31243;&#65288;&#39044;&#22788;&#29702;&#12289;&#22788;&#22788;&#29702;&#12289;&#21518;&#22788;&#29702;&#65289;&#21644;&#24212;&#29992;&#25216;&#26415;&#36827;&#34892;&#21306;&#20998;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#35780;&#20272;&#26041;&#24335;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#22522;&#20110;&#25152;&#25910;&#38598;&#21040;&#30340;&#35265;&#35299;&#65288;&#20363;&#22914;&#65292;&#26368;&#27969;&#34892;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#26159;&#20160;&#20040;&#65311;&#26377;&#22810;&#23569;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65311;&#65289;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#25903;&#25345;&#20174;&#19994;&#32773;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#26032;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#26102;&#20316;&#20986;&#26126;&#26234;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive survey of bias mitigation methods for achieving fairness in Machine Learning (ML) models. We collect a total of 341 publications concerning bias mitigation for ML classifiers. These methods can be distinguished based on their intervention procedure (i.e., pre-processing, in-processing, post-processing) and the technique they apply. We investigate how existing bias mitigation methods are evaluated in the literature. In particular, we consider datasets, metrics and benchmarking. Based on the gathered insights (e.g., What is the most popular fairness metric? How many datasets are used for evaluating bias mitigation methods?), we hope to support practitioners in making informed choices when developing and evaluating new bias mitigation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;&#20248;&#21270;&#26399;&#26395;&#20934;&#30830;&#29575;&#65292;&#21462;&#24471;&#20102;&#24378;&#26377;&#21147;&#30340;&#26367;&#20195;&#20998;&#31867;&#25439;&#22833;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.09615</link><description>&lt;p&gt;
EXACT: &#22914;&#20309;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXACT: How to Train Your Accuracy. (arXiv:2205.09615v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;&#20248;&#21270;&#26399;&#26395;&#20934;&#30830;&#29575;&#65292;&#21462;&#24471;&#20102;&#24378;&#26377;&#21147;&#30340;&#26367;&#20195;&#20998;&#31867;&#25439;&#22833;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#20219;&#21153;&#36890;&#24120;&#20250;&#20197;&#20934;&#30830;&#29575;&#20316;&#20026;&#35780;&#20272;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#29575;&#26159;&#19981;&#36830;&#32493;&#30340;&#65292;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#26799;&#24230;&#19978;&#21319;&#36827;&#34892;&#20248;&#21270;&#12290;&#27969;&#34892;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#12289;&#38128;&#38142;&#25439;&#22833;&#25110;&#20854;&#20182;&#26367;&#20195;&#25439;&#22833;&#26469;&#20248;&#21270;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27169;&#22411;&#30340;&#36755;&#20986;&#24341;&#20837;&#38543;&#26426;&#24615;&#24182;&#20248;&#21270;&#26399;&#26395;&#20934;&#30830;&#29575;&#65292;&#21363;&#38543;&#26426;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26041;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#20998;&#31867;&#25439;&#22833;&#30340;&#24378;&#26377;&#21147;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification tasks are usually evaluated in terms of accuracy. However, accuracy is discontinuous and cannot be directly optimized using gradient ascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate losses, which can lead to suboptimal results. In this paper, we propose a new optimization framework by introducing stochasticity to a model's output and optimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive experiments on linear models and deep image classification show that the proposed optimization method is a powerful alternative to widely used classification losses.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#21644;&#26080;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#22312;&#26376;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;TCN&#12289;LSTM&#21644;CNN&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#19978;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#19988;&#30452;&#25509;&#22312;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#30340;TCN&#21644;LSTM&#22312;&#26576;&#20123;&#39044;&#27979;&#26102;&#27573;&#30340;&#24615;&#33021;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2203.11196</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26376;&#24230;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27493;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series. (arXiv:2203.11196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#21644;&#26080;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#22312;&#26376;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;TCN&#12289;LSTM&#21644;CNN&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#19978;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#19988;&#30452;&#25509;&#22312;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#30340;TCN&#21644;LSTM&#22312;&#26576;&#20123;&#39044;&#27979;&#26102;&#27573;&#30340;&#24615;&#33021;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#34987;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#28982;&#32780;&#23545;&#20110;&#26376;&#24230;&#26102;&#38388;&#24207;&#21015;&#30340;&#24615;&#33021;&#39044;&#27979;&#65292;&#32570;&#20047;&#35777;&#25454;&#12290;&#26412;&#25991;&#26088;&#22312;&#27604;&#36739;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#21644;&#27809;&#26377;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#22312;&#26376;&#24230;&#39044;&#27979;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#22238;&#31572;&#19977;&#20010;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;M4&#21644;M3&#31454;&#36187;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;TCN&#12289;LSTM&#21644;CNN&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#19978;&#36801;&#31227;&#23398;&#20064;&#24448;&#24448;&#33021;&#22815;&#36229;&#36807;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23545;&#20110;&#26576;&#20123;&#39044;&#27979;&#26102;&#27573;&#65292;&#30452;&#25509;&#22312;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#30340;TCN&#21644;LSTM&#30340;&#24615;&#33021;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning and transfer learning models are being used to generate time series forecasts; however, there is scarce evidence about their performance prediction that it is more evident for monthly time series. The purpose of this paper is to compare Deep Learning models with transfer learning and without transfer learning and other traditional methods used for monthly forecasts to answer three questions about the suitability of Deep Learning and Transfer Learning to generate predictions of time series. Time series of M4 and M3 competitions were used for the experiments. The results suggest that deep learning models based on TCN, LSTM, and CNN with transfer learning tend to surpass the performance prediction of other traditional methods. On the other hand, TCN and LSTM, trained directly on the target time series, got similar or better performance than traditional methods for some forecast horizons.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36798;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#20165;&#20855;&#26377;&#19968;&#20010;&#38544;&#21547;&#23618;&#21644;&#19968;&#20010;&#36755;&#20986;&#32500;&#24230;&#20197;&#21450;&#20165;&#20855;&#26377;&#19968;&#20010;&#36127;&#12289;&#38646;&#21644;&#19968;&#20010;&#27491;&#26435;&#37325;&#25110;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#65292;&#23427;&#26159;NP&#38590;&#24230;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.07941</link><description>&lt;p&gt;
&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36798;&#24615;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reachability In Simple Neural Networks. (arXiv:2203.07941v3 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36798;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#20165;&#20855;&#26377;&#19968;&#20010;&#38544;&#21547;&#23618;&#21644;&#19968;&#20010;&#36755;&#20986;&#32500;&#24230;&#20197;&#21450;&#20165;&#20855;&#26377;&#19968;&#20010;&#36127;&#12289;&#38646;&#21644;&#19968;&#20010;&#27491;&#26435;&#37325;&#25110;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#65292;&#23427;&#26159;NP&#38590;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#65288;&#28145;&#24230;&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36798;&#24615;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65306;&#22312;&#32473;&#23450;&#19968;&#20123;&#26377;&#25928;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#26159;&#21542;&#35745;&#31639;&#20986;&#26377;&#25928;&#36755;&#20986;&#65311;&#26368;&#36817;&#26377;&#20154;&#22768;&#31216;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#30001;&#32447;&#24615;&#19981;&#31561;&#24335;&#30340;&#21512;&#21462;&#32452;&#25104;&#30340;&#36755;&#20837;/&#36755;&#20986;&#32500;&#24230;&#30340;&#35268;&#33539;&#65292;&#35813;&#38382;&#39064;&#26159;NP&#23436;&#20840;&#38382;&#39064;&#12290; &#25105;&#20204;&#24635;&#32467;&#20102;&#35777;&#26126;&#24182;&#20462;&#22797;&#20102;&#21407;&#22987;&#19978;&#30028;&#21644;&#19979;&#30028;&#35777;&#26126;&#20013;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;&#21463;&#21040;&#36890;&#29992;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NP&#38590;&#24230;&#24050;&#32463;&#36866;&#29992;&#20110;&#31616;&#21333;&#35268;&#33539;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#21463;&#38480;&#31867;&#12290;&#20801;&#35768;&#19968;&#20010;&#38544;&#34255;&#23618;&#21644;&#19968;&#20010;&#36755;&#20986;&#32500;&#25968;&#20197;&#21450;&#20165;&#20855;&#26377;&#19968;&#20010;&#36127;&#12289;&#38646;&#21644;&#19968;&#20010;&#27491;&#26435;&#37325;&#25110;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#23601;&#36275;&#20197;&#30830;&#20445;NP&#38590;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30740;&#31350;&#30340;&#36825;&#20010;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35752;&#35770;&#21644;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the complexity of the reachability problem for (deep) neural networks: does it compute valid output given some valid input? It was recently claimed that the problem is NP-complete for general neural networks and specifications over the input/output dimension given by conjunctions of linear inequalities. We recapitulate the proof and repair some flaws in the original upper and lower bound proofs. Motivated by the general result, we show that NP-hardness already holds for restricted classes of simple specifications and neural networks. Allowing for a single hidden layer and an output dimension of one as well as neural networks with just one negative, zero and one positive weight or bias is sufficient to ensure NP-hardness. Additionally, we give a thorough discussion and outlook of possible extensions for this direction of research on neural network verification.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#26377;&#25928;&#22320;&#31649;&#29702;&#36861;&#32034;&#25104;&#26412;&#19982;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.06768</link><description>&lt;p&gt;
&#27010;&#29575;&#40065;&#26834;&#24615;&#36861;&#32034;&#65306;&#22312;&#31639;&#27861;&#36861;&#32034;&#20013;&#26435;&#34913;&#25104;&#26412;&#19982;&#40065;&#26834;&#24615;&#30340;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse. (arXiv:2203.06768v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#26377;&#25928;&#22320;&#31649;&#29702;&#36861;&#32034;&#25104;&#26412;&#19982;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20570;&#37325;&#35201;&#20915;&#31574;&#65292;&#30830;&#20445;&#37027;&#20123;&#21463;&#21040;&#27169;&#22411;&#39044;&#27979;&#24433;&#21709;&#65288;&#20363;&#22914;&#34987;&#25298;&#36151;&#27454;&#65289;&#30340;&#20010;&#20307;&#26377;&#19968;&#31181;&#34917;&#25937;&#30340;&#25163;&#27573;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20026;&#21463;&#24433;&#21709;&#30340;&#20010;&#20307;&#26500;&#24314;&#36861;&#32034;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20135;&#29983;&#30340;&#36861;&#32034;&#35201;&#20040;&#23454;&#29616;&#25104;&#26412;&#36739;&#20302;&#65288;&#21363;&#26131;&#20110;&#23454;&#26045;&#65289;&#65292;&#35201;&#20040;&#23545;&#23567;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65288;&#21363;&#36861;&#32034;&#30340;&#22122;&#22768;&#23454;&#29616;&#65289;&#65292;&#20294;&#30001;&#20110;&#36861;&#32034;&#25104;&#26412;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#22825;&#28982;&#30340;&#26435;&#34913;&#65292;&#20004;&#32773;&#19981;&#33021;&#20860;&#24471;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#27809;&#26377;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#23548;&#33322;&#19978;&#36848;&#20114;&#25442;&#20043;&#38388;&#30340;&#20219;&#20309;&#26426;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#31532;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26377;&#25928;&#22320;&#31649;&#29702;&#36861;&#32034;&#25104;&#26412;&#19982;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;Probabilistically ROBust rEcours
&lt;/p&gt;
&lt;p&gt;
As machine learning models are increasingly being employed to make consequential decisions in real-world settings, it becomes critical to ensure that individuals who are adversely impacted (e.g., loan denied) by the predictions of these models are provided with a means for recourse. While several approaches have been proposed to construct recourses for affected individuals, the recourses output by these methods either achieve low costs (i.e., ease-of-implementation) or robustness to small perturbations (i.e., noisy implementations of recourses), but not both due to the inherent trade-offs between the recourse costs and robustness. Furthermore, prior approaches do not provide end users with any agency over navigating the aforementioned trade-offs. In this work, we address the above challenges by proposing the first algorithmic framework which enables users to effectively manage the recourse cost vs. robustness trade-offs. More specifically, our framework Probabilistically ROBust rEcours
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#21407;&#22411;DBSCAN&#32858;&#31867;&#31639;&#27861;&#65288;IPD&#65289;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#20219;&#24847;&#24418;&#29366;&#32858;&#31867;&#65292;&#24182;&#20026;&#27599;&#20010;&#31751;&#36873;&#25321;&#20102;&#19968;&#32452;&#20195;&#34920;&#12290;</title><link>http://arxiv.org/abs/2202.07870</link><description>&lt;p&gt;
IPD&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#22686;&#37327;&#21407;&#22411;DBSCAN&#32858;&#31867;&#26041;&#27861;&#65292;&#20855;&#26377;&#31751;&#20195;&#34920;
&lt;/p&gt;
&lt;p&gt;
IPD:An Incremental Prototype based DBSCAN for large-scale data with cluster representatives. (arXiv:2202.07870v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07870
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#21407;&#22411;DBSCAN&#32858;&#31867;&#31639;&#27861;&#65288;IPD&#65289;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#20219;&#24847;&#24418;&#29366;&#32858;&#31867;&#65292;&#24182;&#20026;&#27599;&#20010;&#31751;&#36873;&#25321;&#20102;&#19968;&#32452;&#20195;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DBSCAN&#26159;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#22522;&#30784;&#32858;&#31867;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#22823;&#25968;&#25454;&#26102;&#19981;&#21487;&#34892;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#36136;&#24515;&#30340;&#32858;&#31867;&#23545;&#20110;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#26410;&#32463;&#22788;&#29702;&#30340;&#25968;&#25454;&#28857;&#21487;&#20197;&#34987;&#26631;&#35760;&#20026;&#26368;&#36817;&#30340;&#36136;&#24515;&#12290;&#28982;&#32780;&#65292;&#23427;&#26080;&#27861;&#26816;&#27979;&#38750;&#29699;&#24418;&#32858;&#31867;&#12290;&#23545;&#20110;&#22823;&#22411;&#25968;&#25454;&#65292;&#23384;&#20648;&#21644;&#35745;&#31639;&#27599;&#20010;&#26679;&#26412;&#30340;&#26631;&#31614;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#24403;&#38656;&#35201;&#20449;&#24687;&#26102;&#65292;&#21487;&#20197;&#36880;&#27493;&#36827;&#34892;&#12290;&#24403;&#32858;&#31867;&#20316;&#20026;&#35782;&#21035;&#31751;&#20195;&#34920;&#30340;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#26368;&#36817;&#20195;&#34920;&#30340;&#31751;&#26631;&#31614;&#20026;&#26597;&#35810;&#25552;&#20379;&#26381;&#21153;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#21407;&#22411;DBSCAN&#65288;IPD&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#35782;&#21035;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#20219;&#24847;&#24418;&#29366;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#23427;&#20026;&#27599;&#20010;&#31751;&#36873;&#25321;&#20102;&#19968;&#32452;&#20195;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
DBSCAN is a fundamental density-based clustering technique that identifies any arbitrary shape of the clusters. However, it becomes infeasible while handling big data. On the other hand, centroid-based clustering is important for detecting patterns in a dataset since unprocessed data points can be labeled to their nearest centroid. However, it can not detect non-spherical clusters. For a large data, it is not feasible to store and compute labels of every samples. These can be done as and when the information is required. The purpose can be accomplished when clustering act as a tool to identify cluster representatives and query is served by assigning cluster labels of nearest representative. In this paper, we propose an Incremental Prototype-based DBSCAN (IPD) algorithm which is designed to identify arbitrary-shaped clusters for large-scale data. Additionally, it chooses a set of representatives for each cluster.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#30340;&#29305;&#28857;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#21487;&#20197;&#34920;&#31034;&#35813;&#31867;&#20013;&#30340;&#20219;&#20309;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#34920;&#31034;&#36825;&#31867;&#20989;&#25968;&#25152;&#38656;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#30028;&#38480;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#22266;&#23450;&#26550;&#26500;&#30340;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#25968;&#25454;&#26679;&#26412;&#22823;&#23567;&#21576;&#22810;&#39033;&#24335;&#20851;&#31995;&#65292;&#32780;&#36755;&#20837;&#32500;&#24230;&#21644;&#32593;&#32476;&#26550;&#26500;&#30340;&#22823;&#23567;&#34987;&#35748;&#20026;&#26159;&#22266;&#23450;&#24120;&#25968;&#12290;</title><link>http://arxiv.org/abs/2111.08117</link><description>&lt;p&gt;
&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#32467;&#26500;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural networks with linear threshold activations: structure and algorithms. (arXiv:2111.08117v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#30340;&#29305;&#28857;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#21487;&#20197;&#34920;&#31034;&#35813;&#31867;&#20013;&#30340;&#20219;&#20309;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#34920;&#31034;&#36825;&#31867;&#20989;&#25968;&#25152;&#38656;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#30028;&#38480;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#22266;&#23450;&#26550;&#26500;&#30340;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#25968;&#25454;&#26679;&#26412;&#22823;&#23567;&#21576;&#22810;&#39033;&#24335;&#20851;&#31995;&#65292;&#32780;&#36755;&#20837;&#32500;&#24230;&#21644;&#32593;&#32476;&#26550;&#26500;&#30340;&#22823;&#23567;&#34987;&#35748;&#20026;&#26159;&#22266;&#23450;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32467;&#26524;&#12290;&#25105;&#20204;&#31934;&#30830;&#22320;&#25551;&#36848;&#20102;&#21487;&#20197;&#30001;&#36825;&#26679;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#20989;&#25968;&#31867;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#29992;2&#20010;&#38544;&#34255;&#23618;&#34920;&#31034;&#35813;&#31867;&#20013;&#30340;&#20219;&#20309;&#21487;&#34920;&#31034;&#20989;&#25968;&#26082;&#26159;&#24517;&#35201;&#30340;&#20063;&#26159;&#20805;&#20998;&#30340;&#12290;&#36825;&#26159;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#32771;&#34385;&#21040;&#26368;&#36817;&#20351;&#29992;&#20854;&#20182;&#27969;&#34892;&#28608;&#27963;&#20989;&#25968;&#22914;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#31934;&#30830;&#21487;&#34920;&#31034;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#34920;&#31034;&#35813;&#31867;&#20013;&#20219;&#20309;&#20989;&#25968;&#25152;&#38656;&#30340;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#31934;&#30830;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#22266;&#23450;&#26550;&#26500;&#30340;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#38382;&#39064;&#65292;&#20197;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;&#22914;&#26524;&#23558;&#36755;&#20837;&#32500;&#24230;&#21644;&#32593;&#32476;&#26550;&#26500;&#30340;&#22823;&#23567;&#35270;&#20026;&#22266;&#23450;&#24120;&#25968;&#65292;&#21017;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#26159;&#22810;&#39033;&#24335;&#26102;&#38388;&#12290;&#35813;&#31639;&#27861;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#21487;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we present new results on neural networks with linear threshold activation functions. We precisely characterize the class of functions that are representable by such neural networks and show that 2 hidden layers are necessary and sufficient to represent any function representable in the class. This is a surprising result in the light of recent exact representability investigations for neural networks using other popular activation functions like rectified linear units (ReLU). We also give precise bounds on the sizes of the neural networks required to represent any function in the class. Finally, we design an algorithm to solve the empirical risk minimization (ERM) problem to global optimality for these neural networks with a fixed architecture. The algorithm's running time is polynomial in the size of the data sample, if the input dimension and the size of the network architecture are considered fixed constants. The algorithm is unique in the sense that it works for any
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#33258;&#20027;&#25506;&#32034;&#21644;&#23548;&#33322;&#19981;&#21516;&#30340;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#21644;&#22270;&#20687;&#30340;&#25299;&#25169;&#35760;&#24518;&#65292;&#20351;&#29992;&#20449;&#24687;&#29942;&#39048;&#32422;&#26463;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19981;&#21040;20&#20998;&#38047;&#20869;&#36890;&#36807;&#35270;&#35273;&#30446;&#26631;&#34920;&#31034;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#25506;&#32034;&#24182;&#21457;&#29616;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2104.05859</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#30446;&#26631;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#24320;&#25918;&#19990;&#30028;&#23548;&#33322;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Rapid Exploration for Open-World Navigation with Latent Goal Models. (arXiv:2104.05859v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.05859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#33258;&#20027;&#25506;&#32034;&#21644;&#23548;&#33322;&#19981;&#21516;&#30340;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#21644;&#22270;&#20687;&#30340;&#25299;&#25169;&#35760;&#24518;&#65292;&#20351;&#29992;&#20449;&#24687;&#29942;&#39048;&#32422;&#26463;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19981;&#21040;20&#20998;&#38047;&#20869;&#36890;&#36807;&#35270;&#35273;&#30446;&#26631;&#34920;&#31034;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#25506;&#32034;&#24182;&#21457;&#29616;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#20027;&#25506;&#32034;&#21644;&#23548;&#33322;&#22810;&#26679;&#21270;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#36317;&#31163;&#21644;&#21160;&#20316;&#30340;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#22270;&#20687;&#25299;&#25169;&#35760;&#24518;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#29942;&#39048;&#26469;&#35268;&#33539;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24471;&#21040;(i)&#32039;&#20945;&#30340;&#30446;&#26631;&#35270;&#35273;&#34920;&#31034;&#65292;(ii)&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;(iii)&#29992;&#20110;&#25506;&#32034;&#30340;&#21487;&#34892;&#30446;&#26631;&#37319;&#26679;&#26426;&#21046;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#31163;&#32447;&#32463;&#39564;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33719;&#24471;&#20102;&#23545;&#20110;&#20219;&#21153;&#26080;&#20851;&#24178;&#25200;&#29289;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#35270;&#35273;&#30446;&#26631;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#19990;&#30028;&#25506;&#32034;&#22330;&#26223;&#20013;&#21033;&#29992;&#31227;&#21160;&#22320;&#38754;&#26426;&#22120;&#20154;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#36317;&#31163;&#39640;&#36798;80&#31859;&#30340;&#30446;&#26631;&#22270;&#20687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20854;&#34920;&#31034;&#22312;&#19981;&#21040;20&#20998;&#38047;&#20869;&#25506;&#32034;&#24182;&#21457;&#29616;&#20102;&#30446;&#26631;&#65292;&#21363;&#20351;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#38556;&#30861;&#21644;&#22825;&#27668;&#26465;&#20214;&#19979;&#20063;&#33021;&#23454;&#29616;&#12290;&#35831;&#26597;&#30475;&#39033;&#30446;&#32593;&#31449;&#19978;&#30340;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a robotic learning system for autonomous exploration and navigation in diverse, open-world environments. At the core of our method is a learned latent variable model of distances and actions, along with a non-parametric topological memory of images. We use an information bottleneck to regularize the learned policy, giving us (i) a compact visual representation of goals, (ii) improved generalization capabilities, and (iii) a mechanism for sampling feasible goals for exploration. Trained on a large offline dataset of prior experience, the model acquires a representation of visual goals that is robust to task-irrelevant distractors. We demonstrate our method on a mobile ground robot in open-world exploration scenarios. Given an image of a goal that is up to 80 meters away, our method leverages its representation to explore and discover the goal in under 20 minutes, even amidst previously-unseen obstacles and weather conditions. Please check out the project website for videos o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#20998;&#26512;&#27773;&#36710;&#29992;&#25143;&#30340;&#29305;&#24449;&#23545;&#20572;&#36710;&#26102;&#38271;&#30340;&#24433;&#21709;&#65292;&#37319;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#20854;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#36816;&#29992;Garson&#31639;&#27861;&#21644;LIME&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2008.01674</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22478;&#24066;&#22303;&#22320;&#20351;&#29992;&#20013;&#30340;&#20572;&#36710;&#26102;&#38271;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach for Modelling Parking Duration in Urban Land-use. (arXiv:2008.01674v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.01674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#20998;&#26512;&#27773;&#36710;&#29992;&#25143;&#30340;&#29305;&#24449;&#23545;&#20572;&#36710;&#26102;&#38271;&#30340;&#24433;&#21709;&#65292;&#37319;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#20854;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#36816;&#29992;Garson&#31639;&#27861;&#21644;LIME&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20572;&#36710;&#26159;&#21457;&#23637;&#20013;&#22269;&#23478;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#12290;&#38543;&#30528;&#36710;&#36742;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#38656;&#35201;&#20026;&#20572;&#36710;&#20998;&#37197;&#26356;&#22810;&#30340;&#22478;&#24066;&#22303;&#22320;&#12290;&#28982;&#32780;&#65292;&#22312;&#21360;&#24230;&#31561;&#21457;&#23637;&#20013;&#22269;&#23478;&#65292;&#20572;&#36710;&#38382;&#39064;&#19968;&#30452;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#27773;&#36710;&#29992;&#25143;&#30340;&#31038;&#20250;&#32463;&#27982;&#21644;&#20986;&#34892;&#29305;&#24449;&#23545;&#20572;&#36710;&#26102;&#38271;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#26469;&#25429;&#25417;&#39550;&#39542;&#21592;&#29305;&#24449;&#21644;&#20572;&#36710;&#26102;&#38271;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;ANNs&#22312;&#23398;&#20064;&#21644;&#35782;&#21035;&#21442;&#25968;&#20043;&#38388;&#30340;&#36830;&#25509;&#19978;&#38750;&#24120;&#39640;&#25928;&#65292;&#20197;&#26368;&#20339;&#39044;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#40657;&#30418;&#29305;&#24615;&#65292;ANNs&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;Garson&#31639;&#27861;&#21644;&#23616;&#37096;&#21487;&#35299;&#37322;&#27169;&#22411;&#19981;&#21463;&#38480;&#30340;&#35299;&#37322;&#65288;LIME&#65289;&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#12290;LIME&#36890;&#36807;&#23558;&#23616;&#37096;&#25968;&#25454;&#19982;&#24320;&#21457;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#23616;&#37096;&#36924;&#36817;&#65292;&#23637;&#31034;&#20102;&#20219;&#20309;&#20998;&#31867;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parking is an inevitable issue in the fast-growing developing countries. Increasing number of vehicles require more and more urban land to be allocated for parking. However, a little attention has been conferred to the parking issues in developing countries like India. This study proposes a model for analysing the influence of car users' socioeconomic and travel characteristics on parking duration. Specifically, artificial neural networks (ANNs) is deployed to capture the interrelationship between driver characteristics and parking duration. ANNs are highly efficient in learning and recognizing connections between parameters for best prediction of an outcome. Since, utility of ANNs has been critically limited due to its Black Box nature, the study involves the use of Garson algorithm and Local interpretable model-agnostic explanations (LIME) for model interpretations. LIME shows the prediction for any classification, by approximating it locally with the developed interpretable model. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#29420;&#31435;&#24615;&#27979;&#35797;&#23454;&#29616;&#20102;&#26222;&#36941;&#19968;&#33268;&#30340;k&#26679;&#26412;&#26816;&#39564;&#65292;&#24182;&#19988;&#21457;&#29616;&#38750;&#21442;&#25968;&#29420;&#31435;&#24615;&#27979;&#35797;&#36890;&#24120;&#27604;&#22810;&#20803;&#26041;&#24046;&#20998;&#26512;(MANOVA)&#27979;&#35797;&#22312;&#39640;&#26031;&#20998;&#24067;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/1910.08883</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#21644;&#26222;&#36941;&#19968;&#33268;&#30340;k&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and universally consistent k-sample tests. (arXiv:1910.08883v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#29420;&#31435;&#24615;&#27979;&#35797;&#23454;&#29616;&#20102;&#26222;&#36941;&#19968;&#33268;&#30340;k&#26679;&#26412;&#26816;&#39564;&#65292;&#24182;&#19988;&#21457;&#29616;&#38750;&#21442;&#25968;&#29420;&#31435;&#24615;&#27979;&#35797;&#36890;&#24120;&#27604;&#22810;&#20803;&#26041;&#24046;&#20998;&#26512;(MANOVA)&#27979;&#35797;&#22312;&#39640;&#26031;&#20998;&#24067;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
k&#26679;&#26412;&#26816;&#39564;&#38382;&#39064;&#28041;&#21450;&#30830;&#23450;$k$&#32452;&#25968;&#25454;&#28857;&#26159;&#21542;&#37117;&#26469;&#33258;&#21516;&#19968;&#20010;&#20998;&#24067;&#12290;&#23613;&#31649;&#22810;&#20803;&#26041;&#24046;&#20998;&#26512;(MANOVA)&#26159;&#29983;&#29289;&#21307;&#23398;&#20013;&#24120;&#29992;&#30340;k&#26679;&#26412;&#26816;&#39564;&#26041;&#27861;&#65292;&#20294;&#23427;&#20381;&#36182;&#20110;&#24378;&#22823;&#19988;&#36890;&#24120;&#19981;&#21512;&#36866;&#30340;&#21442;&#25968;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#29420;&#31435;&#24615;&#27979;&#35797;&#21644;k&#26679;&#26412;&#27979;&#35797;&#23494;&#20999;&#30456;&#20851;&#65292;&#19968;&#20123;&#26222;&#36941;&#19968;&#33268;&#30340;&#39640;&#32500;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#22914;&#36317;&#31163;&#30456;&#20851;(Discrepancy)&#21644;Hilbert-Schmidt&#29420;&#31435;&#24615;&#20934;&#21017;(Hsic)&#65292;&#20855;&#26377;&#22362;&#23454;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29420;&#31435;&#24615;&#27979;&#35797;&#23454;&#29616;&#20102;&#26222;&#36941;&#19968;&#33268;&#30340;k&#26679;&#26412;&#26816;&#39564;&#65292;&#24182;&#19988;k&#26679;&#26412;&#32479;&#35745;&#37327;&#65292;&#22914;Energy&#21644;Maximum Mean Discrepancy(MMD)&#65292;&#19982;Discrepancy&#23436;&#20840;&#31561;&#20215;&#12290;&#23545;&#38750;&#21442;&#25968;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#23427;&#20204;&#36890;&#24120;&#27604;&#27969;&#34892;&#30340;MANOVA&#27979;&#35797;&#34920;&#29616;&#26356;&#22909;&#65292;&#21363;&#20351;&#22312;&#39640;&#26031;&#20998;&#24067;&#30340;&#22330;&#26223;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
The k-sample testing problem involves determining whether $k$ groups of data points are each drawn from the same distribution. The standard method for k-sample testing in biomedicine is Multivariate analysis of variance (MANOVA), despite that it depends on strong, and often unsuitable, parametric assumptions. Moreover, independence testing and k-sample testing are closely related, and several universally consistent high-dimensional independence tests such as distance correlation (Dcorr) and Hilbert-Schmidt-Independence-Criterion (Hsic) enjoy solid theoretical and empirical properties. In this paper, we prove that independence tests achieve universally consistent k-sample testing and that k-sample statistics such as Energy and Maximum Mean Discrepancy (MMD) are precisely equivalent to Dcorr. An empirical evaluation of nonparametric independence tests showed that they generally perform better than the popular MANOVA test, even in Gaussian distributed scenarios. The evaluation included se
&lt;/p&gt;</description></item></channel></rss>