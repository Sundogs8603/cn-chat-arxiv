<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#21046;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#40736;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#35757;&#32451;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#12289;&#25913;&#36827;&#32593;&#32476;&#26550;&#26500;&#21644;&#21442;&#25968;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;&#21475;&#26381;&#21644;&#38745;&#33033;&#32473;&#33647;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#39044;&#27979;&#26356;&#22810;&#32456;&#28857;&#21644;&#22788;&#29702;&#19981;&#21516;&#30340;&#21327;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.09167</link><description>&lt;p&gt;
&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#21046;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#22823;&#40736;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Deep Neural Network -- Mechanistic Hybrid Model to Predict Pharmacokinetics in Rat. (arXiv:2310.09167v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09167
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#21046;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#40736;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#35757;&#32451;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#12289;&#25913;&#36827;&#32593;&#32476;&#26550;&#26500;&#21644;&#21442;&#25968;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;&#21475;&#26381;&#21644;&#38745;&#33033;&#32473;&#33647;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#39044;&#27979;&#26356;&#22810;&#32456;&#28857;&#21644;&#22788;&#29702;&#19981;&#21516;&#30340;&#21327;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#20998;&#23376;&#33647;&#29289;&#25110;&#20892;&#33647;&#30340;&#30740;&#21457;&#20013;&#65292;&#37325;&#35201;&#30340;&#19968;&#20010;&#26041;&#38754;&#23601;&#26159;&#23427;&#20204;&#22312;&#38745;&#33033;&#21644;&#21475;&#26381;&#32473;&#33647;&#21518;&#30340;&#20840;&#36523;&#21487;&#29992;&#24615;&#12290;&#20174;&#20505;&#36873;&#21270;&#21512;&#29289;&#30340;&#21270;&#23398;&#32467;&#26500;&#39044;&#27979;&#20840;&#36523;&#21487;&#29992;&#24615;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#35753;&#33647;&#29289;&#25110;&#20892;&#33647;&#30340;&#30740;&#21457;&#38598;&#20013;&#22312;&#20855;&#26377;&#33391;&#22909;&#21160;&#21147;&#23398;&#29305;&#24615;&#30340;&#21270;&#21512;&#29289;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21487;&#29992;&#24615;&#26159;&#20998;&#23376;&#24615;&#36136;&#12289;&#29983;&#29289;&#23398;&#21644;&#29983;&#29702;&#23398;&#20043;&#38388;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#31232;&#32570;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#20808;&#21069;&#24320;&#21457;&#30340;&#28151;&#21512;&#27169;&#22411;[34]&#12290;&#25105;&#20204;&#23558;&#24635;&#30340;&#21475;&#26381;&#26292;&#38706;&#30340;&#20013;&#20540;&#25240;&#21472;&#35823;&#24046;&#20174;2.85&#38477;&#20302;&#21040;2.35&#65292;&#23558;&#38745;&#33033;&#32473;&#33647;&#30340;&#35823;&#24046;&#20174;1.95&#38477;&#20302;&#21040;1.62&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20197;&#21450;&#26426;&#21046;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#23454;&#29616;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#20854;&#20182;&#32456;&#28857;&#21644;&#22788;&#29702;&#19981;&#21516;&#30340;&#21327;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important aspect in the development of small molecules as drugs or agro-chemicals is their systemic availability after intravenous and oral administration.The prediction of the systemic availability from the chemical structure of a poten-tial candidate is highly desirable, as it allows to focus the drug or agrochemicaldevelopment on compounds with a favorable kinetic profile. However, such pre-dictions are challenging as the availability is the result of the complex interplaybetween molecular properties, biology and physiology and training data is rare.In this work we improve the hybrid model developed earlier [34]. We reducethe median fold change error for the total oral exposure from 2.85 to 2.35 andfor intravenous administration from 1.95 to 1.62. This is achieved by trainingon a larger data set, improving the neural network architecture as well as theparametrization of mechanistic model. Further, we extend our approach to predictadditional endpoints and to handle different covar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17207</link><description>&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#65306;&#23545;&#20869;&#23384;&#20026;&#22522;&#30784;&#30340;&#26234;&#33021;&#20307;&#22312;&#26080;&#23613;&#20219;&#21153;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes. (arXiv:2309.17207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#20171;&#32461;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#27979;&#35797;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#29305;&#21035;&#26159;&#23558;&#38376;&#24490;&#29615;&#21333;&#20803;(GRU)&#19982;Transformer-XL(TrXL)&#30456;&#27604;&#65292;&#23427;&#20204;&#23545;&#20110;&#35760;&#24518;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#12289;&#25239;&#22122;&#22768;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#37319;&#29992;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#21363;Mortar Mayhem&#12289;Mystery Path&#21644;Searing Spotlights&#12290;&#36825;&#20123;&#26368;&#21021;&#26159;&#26377;&#38480;&#30340;&#29615;&#22659;&#34987;&#25512;&#24191;&#20026;&#26032;&#39062;&#30340;&#26080;&#23613;&#20219;&#21153;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#21160;&#35838;&#31243;&#65292;&#20174;&#36710;&#28216;&#25103;"I packed my bag"&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#36825;&#20123;&#26080;&#23613;&#20219;&#21153;&#19981;&#20165;&#26377;&#21161;&#20110;&#35780;&#20272;&#25928;&#29575;&#65292;&#32780;&#19988;&#26377;&#36259;&#22320;&#35780;&#20272;&#20102;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;&#29616;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#35760;&#24518;&#22522;&#20934;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;TrXL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;&#26412;&#23454;&#29616;&#21033;&#29992;TrXL&#20316;&#20026;&#20197;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#20351;&#29992;&#30340;&#24773;&#33410;&#24615;&#35760;&#24518;&#12290;&#22312;&#26377;&#38480;&#29615;&#22659;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Memory Gym introduces a unique benchmark designed to test Deep Reinforcement Learning agents, specifically comparing Gated Recurrent Unit (GRU) against Transformer-XL (TrXL), on their ability to memorize long sequences, withstand noise, and generalize. It features partially observable 2D environments with discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights. These originally finite environments are extrapolated to novel endless tasks that act as an automatic curriculum, drawing inspiration from the car game ``I packed my bag". These endless tasks are not only beneficial for evaluating efficiency but also intriguingly valuable for assessing the effectiveness of approaches in memory-based agents. Given the scarcity of publicly available memory baselines, we contribute an implementation driven by TrXL and Proximal Policy Optimization. This implementation leverages TrXL as episodic memory using a sliding window approach. In our experiments on the finite environment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.16741</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#36890;&#36807;&#28508;&#31354;&#38388;&#25237;&#24433;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections. (arXiv:2309.16741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20844;&#21496;&#36890;&#24120;&#22788;&#29702;&#21644;&#23384;&#20648;&#20135;&#29983;&#36830;&#32493;&#19988;&#39640;&#39057;&#30340;&#25968;&#21313;&#20159;&#26465;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20026;&#20102;&#25903;&#25345;&#39640;&#25928;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#26816;&#32034;&#65292;&#20986;&#29616;&#20102;&#19987;&#38376;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24211;&#21644;&#31995;&#32479;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25903;&#25345;&#36890;&#36807;&#31867;&#20284;&#20110;&#32422;&#26463;&#21270;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#30340;&#26684;&#24335;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32034;&#24341;&#21644;&#26597;&#35810;&#65292;&#20197;&#23454;&#29616;&#20687;&#8220;&#26376;&#24230;&#20215;&#26684;&#22238;&#25253;&#22823;&#20110;5%&#30340;&#32929;&#31080;&#8221;&#36825;&#26679;&#30340;&#26597;&#35810;&#65292;&#24182;&#20197;&#20005;&#26684;&#30340;&#26684;&#24335;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26597;&#35810;&#19981;&#33021;&#25429;&#25417;&#21040;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#24448;&#24448;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#25110;&#35821;&#35328;&#65288;&#20363;&#22914;&#8220;&#22788;&#20110;&#20302;&#27874;&#21160;&#24615;&#29366;&#24577;&#30340;&#32929;&#31080;&#8221;&#65289;&#26356;&#22909;&#22320;&#25551;&#36848;&#12290;&#32780;&#19988;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#25152;&#38656;&#30340;&#23384;&#20648;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#26816;&#32034;&#22797;&#26434;&#24230;&#24448;&#24448;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#28436;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#28508;&#31354;&#38388;&#25237;&#24433;&#21487;&#20197;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial firms commonly process and store billions of time-series data, generated continuously and at a high frequency. To support efficient data storage and retrieval, specialized time-series databases and systems have emerged. These databases support indexing and querying of time-series by a constrained Structured Query Language(SQL)-like format to enable queries like "Stocks with monthly price returns greater than 5%", and expressed in rigid formats. However, such queries do not capture the intrinsic complexity of high dimensional time-series data, which can often be better described by images or language (e.g., "A stock in low volatility regime"). Moreover, the required storage, computational time, and retrieval complexity to search in the time-series space are often non-trivial. In this paper, we propose and demonstrate a framework to store multi-modal data for financial time-series in a lower-dimensional latent space using deep encoders, such that the latent space projections ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.15224</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Collaborative Watermarking for Adversarial Speech Synthesis. (arXiv:2309.15224v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#38899;&#21512;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#25216;&#26415;&#19981;&#20165;&#25509;&#36817;&#20154;&#31867;&#30340;&#33258;&#28982;&#24230;&#65292;&#32780;&#19988;&#33021;&#22815;&#20197;&#23569;&#37327;&#25968;&#25454;&#36827;&#34892;&#21363;&#26102;&#35821;&#38899;&#20811;&#38534;&#65292;&#24182;&#19988;&#20511;&#21161;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#39640;&#24230;&#21487;&#35775;&#38382;&#24615;&#12290;&#24403;&#28982;&#65292;&#29983;&#25104;&#20869;&#23481;&#30340;&#28508;&#22312;&#27867;&#28389;&#24341;&#36215;&#20102;&#23545;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#21644;&#27700;&#21360;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#30340;&#30740;&#31350;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#21644;&#27450;&#39575;&#23545;&#31574;&#25361;&#25112;&#65288;ASVspoof&#65289;&#19978;&#65292;&#35813;&#25361;&#25112;&#19987;&#27880;&#20110;&#34987;&#21160;&#23545;&#31574;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#35282;&#24230;&#20986;&#21457;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#22312;&#19981;&#24178;&#25200;&#20154;&#31867;&#21548;&#20247;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#36890;&#36807;&#21327;&#21516;&#26426;&#22120;&#26816;&#27979;&#21040;&#29983;&#25104;&#35821;&#38899;&#30340;&#27700;&#21360;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;ASVspoof 2021&#22522;&#32447;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#30340;HiFi-GAN&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in neural speech synthesis have brought us technology that is not only close to human naturalness, but is also capable of instant voice cloning with little data, and is highly accessible with pre-trained models available. Naturally, the potential flood of generated content raises the need for synthetic speech detection and watermarking. Recently, considerable research effort in synthetic speech detection has been related to the Automatic Speaker Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on passive countermeasures. This paper takes a complementary view to generated speech detection: a synthesis system should make an active effort to watermark the generated speech in a way that aids detection by another machine, but remains transparent to a human listener. We propose a collaborative training scheme for synthetic speech watermarking and show that a HiFi-GAN neural vocoder collaborating with the ASVspoof 2021 baseline countermeasure models consis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#26631;&#20934;&#21270;&#27969;&#24341;&#20837;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#28789;&#27963;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.09222</link><description>&lt;p&gt;
&#21452;&#37325;&#26631;&#20934;&#21270;&#27969;&#65306;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Double Normalizing Flows: Flexible Bayesian Gaussian Process ODEs Learning. (arXiv:2309.09222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09222
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#26631;&#20934;&#21270;&#27969;&#24341;&#20837;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#28789;&#27963;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39640;&#26031;&#36807;&#31243;&#34987;&#29992;&#26469;&#24314;&#27169;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#30340;&#21521;&#37327;&#22330;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#36125;&#21494;&#26031;&#25512;&#26029;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31561;&#20219;&#21153;&#65292;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#22312;&#20855;&#26377;&#38750;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#32422;&#26463;&#20808;&#39564;&#21644;&#22343;&#20540;&#22330;&#21518;&#39564;&#21487;&#33021;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#20934;&#21270;&#27969;&#26469;&#37325;&#26032;&#21442;&#25968;&#21270;ODE&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#26356;&#28789;&#27963;&#12289;&#26356;&#34920;&#36798;&#24615;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26631;&#20934;&#21270;&#27969;&#30340;&#35299;&#26512;&#21487;&#35745;&#31639;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;GP ODE&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#29983;&#25104;&#19968;&#20010;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#12290;&#36890;&#36807;&#36825;&#20123;&#26631;&#20934;&#21270;&#27969;&#30340;&#21452;&#37325;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#20013;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Gaussian processes have been utilized to model the vector field of continuous dynamical systems. Bayesian inference for such models \cite{hegde2022variational} has been extensively studied and has been applied in tasks such as time series prediction, providing uncertain estimates. However, previous Gaussian Process Ordinary Differential Equation (ODE) models may underperform on datasets with non-Gaussian process priors, as their constrained priors and mean-field posteriors may lack flexibility. To address this limitation, we incorporate normalizing flows to reparameterize the vector field of ODEs, resulting in a more flexible and expressive prior distribution. Additionally, due to the analytically tractable probability density functions of normalizing flows, we apply them to the posterior inference of GP ODEs, generating a non-Gaussian posterior. Through these dual applications of normalizing flows, our model improves accuracy and uncertainty estimates for Bayesian Gaussian P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#26448;&#26009;&#22312;&#22823;&#21464;&#24418;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#20855;&#26377;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#31283;&#20581;&#22320;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.15640</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#22797;&#26434;&#36229;&#24377;&#24615;&#22266;&#20307;&#30340;&#32452;&#20998;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Identifying Constitutive Parameters for Complex Hyperelastic Solids using Physics-Informed Neural Networks. (arXiv:2308.15640v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#26448;&#26009;&#22312;&#22823;&#21464;&#24418;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#20855;&#26377;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#31283;&#20581;&#22320;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#31243;&#21644;&#29983;&#29289;&#26448;&#26009;&#20013;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#21644;&#26426;&#26800;&#34892;&#20026;&#30340;&#26448;&#26009;&#20013;&#65292;&#35782;&#21035;&#32452;&#20998;&#21442;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20026;&#27492;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24403;&#21069;&#30340;&#26694;&#26550;&#36890;&#24120;&#20165;&#38480;&#20110;&#22522;&#26412;&#30340;&#32452;&#20998;&#23450;&#24459;&#65292;&#24182;&#22312;&#19982;&#23454;&#39564;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#36935;&#21040;&#23454;&#38469;&#32422;&#26463;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;PINN&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#36719;&#26448;&#26009;&#30340;&#26448;&#26009;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#21576;&#29616;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#12290;&#35813;&#27169;&#22411;&#24378;&#35843;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;PINN&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#22330;&#21464;&#24418;&#21644;&#21152;&#36733;&#21382;&#21490;&#65292;&#20197;&#30830;&#20445;&#31639;&#27861;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#20173;&#28982;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying constitutive parameters in engineering and biological materials, particularly those with intricate geometries and mechanical behaviors, remains a longstanding challenge. The recent advent of Physics-Informed Neural Networks (PINNs) offers promising solutions, but current frameworks are often limited to basic constitutive laws and encounter practical constraints when combined with experimental data. In this paper, we introduce a new PINN-based framework designed to identify material parameters for soft materials, specifically those exhibiting complex constitutive behaviors, under large deformation in plane stress conditions. Distinctively, our model emphasizes training PINNs with multi-modal time-dependent experimental datasets consisting of full-field deformation and loading history, ensuring algorithm robustness even amidst noisy data. Our results reveal that our framework can accurately identify constitutive parameters of the incompressible Arruda-Boyce model for samples 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#36716;&#25442;&#20026;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.13815</link><description>&lt;p&gt;
SyMOT-Flow: &#23398;&#20064;&#20004;&#20010;&#20219;&#24847;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#27969;&#21160;&#21450;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
SyMOT-Flow: Learning optimal transport flow for two arbitrary distributions with maximum mean discrepancy. (arXiv:2308.13815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#36716;&#25442;&#20026;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25214;&#21040;&#20004;&#20010;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36716;&#25442;&#23545;&#20110;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#21644;&#25191;&#34892;&#23494;&#24230;&#20272;&#35745;&#12289;&#26679;&#26412;&#29983;&#25104;&#21644;&#32479;&#35745;&#25512;&#26029;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#19968;&#20010;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#20197;&#33719;&#24471;&#19968;&#20010;&#30701;&#36317;&#31163;&#21644;&#21487;&#35299;&#37322;&#30340;&#36716;&#25442;&#12290;&#24471;&#21040;&#30340;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#20302;&#32500;&#31034;&#20363;&#21644;&#39640;&#32500;&#29983;&#25104;&#26679;&#26412;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a transformation between two unknown probability distributions from samples is crucial for modeling complex data distributions and perform tasks such as density estimation, sample generation, and statistical inference. One powerful framework for such transformations is normalizing flow, which transforms an unknown distribution into a standard normal distribution using an invertible network. In this paper, we introduce a novel model called SyMOT-Flow that trains an invertible transformation by minimizing the symmetric maximum mean discrepancy between samples from two unknown distributions, and we incorporate an optimal transport cost as regularization to obtain a short-distance and interpretable transformation. The resulted transformation leads to more stable and accurate sample generation. We establish several theoretical results for the proposed model and demonstrate its effectiveness with low-dimensional illustrative examples as well as high-dimensional generative samples obt
&lt;/p&gt;</description></item><item><title>SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.04365</link><description>&lt;p&gt;
SLEM&#65306;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#36335;&#24452;&#24314;&#27169;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04365
&lt;/p&gt;
&lt;p&gt;
SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#31185;&#23398;&#30340;&#20851;&#38190;&#30446;&#26631;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24471;&#20986;&#20851;&#20110;&#23545;&#20551;&#23450;&#24178;&#39044;&#30340;&#39044;&#27979;&#30340;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;&#36335;&#24452;&#27169;&#22411;&#12289;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEMs)&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#33021;&#22815;&#26126;&#30830;&#22320;&#25351;&#23450;&#20851;&#20110;&#29616;&#35937;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#20551;&#35774;&#12290;&#19982;DAGs&#19981;&#21516;&#65292;SEMs&#20551;&#35774;&#32447;&#24615;&#20851;&#31995;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20989;&#25968;&#38169;&#35823;&#35268;&#33539;&#65292;&#20174;&#32780;&#38459;&#30861;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#21487;&#38752;&#30340;&#25928;&#26524;&#22823;&#23567;&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;&#65288;SLEM&#65289;&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#38598;&#25104;&#30340;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;SLEM&#33021;&#22815;&#25552;&#20379;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#22312;&#19982;SEMs&#36827;&#34892;&#32447;&#24615;&#27169;&#22411;&#27604;&#36739;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#20248;&#20110;SEMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#19968;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#24182;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#26032;&#30340;&#35823;&#24046;&#30028;&#12290;&#20854;&#26041;&#27861;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.16164</link><description>&lt;p&gt;
&#22312;RKHS&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#23494;&#24230;&#27604;&#29575;
&lt;/p&gt;
&lt;p&gt;
Adaptive learning of density ratios in RKHS. (arXiv:2307.16164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#19968;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#24182;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#26032;&#30340;&#35823;&#24046;&#30028;&#12290;&#20854;&#26041;&#27861;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#23494;&#24230;&#35266;&#27979;&#20013;&#20272;&#35745;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#30340;&#27604;&#29575;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#24212;&#29992;&#21253;&#25324;&#21452;&#26679;&#26412;&#26816;&#39564;&#12289;&#20998;&#27495;&#20272;&#35745;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#21327;&#21464;&#37327;&#36716;&#31227;&#36866;&#24212;&#12289;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#21644;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#22823;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#20204;&#36890;&#36807;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#26368;&#23567;&#21270;&#30495;&#23454;&#23494;&#24230;&#27604;&#29575;&#19982;&#27169;&#22411;&#20043;&#38388;&#30340;&#27491;&#21017;Bregman&#36317;&#31163;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26032;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;Lepskii&#31867;&#22411;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#22312;&#19981;&#30693;&#36947;&#23494;&#24230;&#27604;&#29575;&#30340;&#27491;&#21017;&#24615;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#35823;&#24046;&#30028;&#12290;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#20540;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#21033;&#29992;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.10875</link><description>&lt;p&gt;
&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Risk-optimized Outlier Removal for Robust Point Cloud Classification. (arXiv:2307.10875v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#21033;&#29992;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#28145;&#24230;&#27169;&#22411;&#22312;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#26159;&#28857;&#20113;&#22122;&#22768;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;(PointCVaR)&#65292;&#23427;&#21487;&#20197;&#20351;&#26631;&#20934;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36827;&#34892;&#24402;&#22240;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#28857;&#30340;&#39118;&#38505;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540; (CVaR) &#20316;&#20026;&#30446;&#26631;&#65292;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#35266;&#23519;&#21040;&#28857;&#20113;&#22122;&#22768;&#28857;&#24448;&#24448;&#32858;&#38598;&#22312;&#39118;&#38505;&#20998;&#24067;&#30340;&#23614;&#37096;&#65292;&#39057;&#29575;&#20302;&#20294;&#39118;&#38505;&#27700;&#24179;&#39640;&#65292;&#20174;&#32780;&#23545;&#20998;&#31867;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24178;&#25200;&#12290;&#23613;&#31649;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21364;&#33021;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#21463;&#26410;&#30693;&#24178;&#25200;&#21644;&#38459;&#23612;&#24433;&#21709;&#26102;&#23398;&#20064;&#21040;&#20869;&#37096;&#21160;&#24577;&#35299;&#26512;&#39033;&#30340;&#20266;&#21704;&#23494;&#39039;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#65292;&#21363;&#20351;&#38590;&#20197;&#25214;&#21040;&#25200;&#21160;&#35299;&#26512;&#39033;&#65292;&#20063;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#21160;&#24577;&#65292;&#23545;&#20110;&#20854;&#20182;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#24773;&#20917;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22235;&#38454;&#23545;&#31216;&#31215;&#20998;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06920</link><description>&lt;p&gt;
&#20266;&#21704;&#23494;&#39039;&#31995;&#32479;&#36776;&#35782;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Hamiltonian system identification. (arXiv:2305.06920v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#21463;&#26410;&#30693;&#24178;&#25200;&#21644;&#38459;&#23612;&#24433;&#21709;&#26102;&#23398;&#20064;&#21040;&#20869;&#37096;&#21160;&#24577;&#35299;&#26512;&#39033;&#30340;&#20266;&#21704;&#23494;&#39039;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#65292;&#21363;&#20351;&#38590;&#20197;&#25214;&#21040;&#25200;&#21160;&#35299;&#26512;&#39033;&#65292;&#20063;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#21160;&#24577;&#65292;&#23545;&#20110;&#20854;&#20182;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#24773;&#20917;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22235;&#38454;&#23545;&#31216;&#31215;&#20998;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#26102;&#65292;&#30830;&#23450;&#29289;&#29702;&#31995;&#32479;&#30340;&#22522;&#26412;&#21160;&#24577;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#21487;&#20197;&#24314;&#27169;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#20551;&#35774;&#19968;&#23450;&#30340;&#20266;&#21704;&#23494;&#39039;&#24418;&#24335;&#65292;&#21363;&#20351;&#27169;&#22411;&#22312;&#31995;&#32479;&#21463;&#21040;&#26410;&#30693;&#38459;&#23612;&#21644;&#22806;&#25200;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#20063;&#33021;&#22815;&#23398;&#20064;&#21040;&#20869;&#37096;&#21160;&#24577;&#30340;&#35299;&#26512;&#39033;&#12290;&#22312;&#38590;&#20197;&#25214;&#21040;&#25200;&#21160;&#35299;&#26512;&#39033;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36825;&#20123;&#39033;&#30340;&#28151;&#21512;&#27169;&#22411;&#20173;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#23601;&#20687;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#19968;&#26679;&#12290;&#36825;&#20351;&#24471;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#20854;&#20182;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#20351;&#29992;&#22235;&#38454;&#23545;&#31216;&#31215;&#20998;&#26041;&#26696;&#65292;&#36991;&#20813;&#35757;&#32451;&#20013;&#30340;&#23454;&#38469;&#31215;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#22914;&#20309;&#25552;&#39640;&#24615;&#33021;&#30340;&#21508;&#31181;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the underlying dynamics of physical systems can be challenging when only provided with observational data. In this work, we consider systems that can be modelled as first-order ordinary differential equations. By assuming a certain pseudo-Hamiltonian formulation, we are able to learn the analytic terms of internal dynamics even if the model is trained on data where the system is affected by unknown damping and external disturbances. In cases where it is difficult to find analytic terms for the disturbances, a hybrid model that uses a neural network to learn these can still accurately identify the dynamics of the system as if under ideal conditions. This makes the models applicable in situations where other system identification models fail. Furthermore, we propose to use a fourth-order symmetric integration scheme in the loss function and avoid actual integration in the training, and demonstrate on varied examples how this leads to increased performance on noisy data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#20197;&#35299;&#20915;&#20197;&#24448;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02803</link><description>&lt;p&gt;
&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#24352;&#37327;PCA
&lt;/p&gt;
&lt;p&gt;
Tensor PCA from basis in tensor space. (arXiv:2305.02803v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#20197;&#35299;&#20915;&#20197;&#24448;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#20197;&#21069;&#36890;&#36807;&#36845;&#20195;&#27714;&#35299;&#20248;&#21270;&#38382;&#39064;&#26469;&#25552;&#21462;&#20302;&#32500;&#23376;&#31354;&#38388;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20174;&#23454;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#20013;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#65292;&#20174;&#32780;&#23558;&#22522;&#30784;&#30340;&#23548;&#20986;&#38382;&#39064;&#36716;&#21270;&#20026;&#29305;&#24449;&#20540;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#24773;&#20917;&#30340;&#23548;&#20986;&#65306;i&#65289;&#20174;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#20013;&#23548;&#20986;&#22522;&#30784;&#65307;ii&#65289;&#23548;&#20986;&#31209;&#20026;1&#30340;&#22522;&#30784;&#65307;iii&#65289;&#20174;&#23376;&#31354;&#38388;&#20013;&#23548;&#20986;&#22522;&#30784;&#12290;&#29305;&#21035;&#26159;&#65292;&#35777;&#26126;&#20102;&#23454;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#26041;&#31243;&#19982;&#26631;&#20934;&#30697;&#38453;&#29305;&#24449;&#20540;&#26041;&#31243;&#30340;&#31561;&#20215;&#24615;&#12290;&#38024;&#23545;&#25152;&#32771;&#34385;&#30340;&#19977;&#31181;&#24773;&#20917;&#65292;&#37319;&#29992;&#20102;&#23376;&#31354;&#38388;&#26041;&#27861;&#26469;&#23548;&#20986;&#24352;&#37327;PCA&#12290;&#22522;&#20110;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to present a mathematical framework for tensor PCA. The proposed approach is able to overcome the limitations of previous methods that extract a low dimensional subspace by iteratively solving an optimization problem. The core of the proposed approach is the derivation of a basis in tensor space from a real self-adjoint tensor operator, thus reducing the problem of deriving a basis to an eigenvalue problem. Three different cases have been studied to derive: i) a basis from a self-adjoint tensor operator; ii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence between eigenvalue equation for a real self-adjoint tensor operator and standard matrix eigenvalue equation has been proven. For all the three cases considered, a subspace approach has been adopted to derive a tensor PCA. Experiments on image datasets validate the proposed mathematical framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#65292;PHNN&#34920;&#29616;&#26356;&#20026;&#20248;&#36234;&#65292;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#24773;&#20917;&#24182;&#21487;&#20998;&#21035;&#24471;&#21040;&#19977;&#20010;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2304.14374</link><description>&lt;p&gt;
&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Hamiltonian neural networks for learning partial differential equations. (arXiv:2304.14374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#65292;PHNN&#34920;&#29616;&#26356;&#20026;&#20248;&#36234;&#65292;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#24773;&#20917;&#24182;&#21487;&#20998;&#21035;&#24471;&#21040;&#19977;&#20010;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#26469;&#23398;&#20064;&#21487;&#20197;&#29992;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;&#26412;&#25991;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#25152;&#24471;&#27169;&#22411;&#30001;&#39640;&#36798;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#27169;&#25311;&#20195;&#34920;&#23432;&#24658;&#12289;&#32791;&#25955;&#21644;&#22806;&#21147;&#30340;&#39033;&#20197;&#21450;&#21487;&#20197;&#23398;&#20064;&#25110;&#20026;&#20808;&#21069;&#30693;&#35782;&#30340;&#31163;&#25955;&#21367;&#31215;&#31639;&#23376;&#26500;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;PHNN&#30456;&#27604;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PHNN&#27169;&#22411;&#30001;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#32452;&#25104;&#65292;&#21487;&#20197;&#20998;&#21035;&#30740;&#31350;&#36825;&#20123;&#37096;&#20998;&#20197;&#33719;&#24471;&#23545;&#31995;&#32479;&#30340;&#27934;&#23519;&#65292;&#24182;&#19988;&#21363;&#20351;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#65292;&#25152;&#23398;&#24471;&#30340;&#27169;&#22411;&#20173;&#28982;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Hamiltonian neural networks (PHNN) were recently introduced for learning dynamical systems that can be modelled by ordinary differential equations. In this paper, we extend the method to partial differential equations. The resulting model is comprised of up to three neural networks, modelling terms representing conservation, dissipation and external forces, and discrete convolution operators that can either be learned or be prior knowledge. We demonstrate numerically the superior performance of PHNN compared to a baseline model that models the full dynamics by a single neural network. Moreover, since the PHNN model consists of three parts with different physical interpretations, these can be studied separately to gain insight into the system, and the learned model is applicable also if external forces are removed or changed.
&lt;/p&gt;</description></item><item><title>&#21516;&#28304;&#24615;&#21407;&#21017;&#19981;&#19968;&#23450;&#26159;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#21807;&#19968;&#21407;&#22240;&#65307;&#26412;&#25991;&#25552;&#20986;Contextual Stochastic Block Model for Homophily (CSBM-H)&#20197;&#28145;&#20837;&#30740;&#31350;&#21516;&#28304;&#24615;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.14274</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#23545;&#33410;&#28857;&#20998;&#31867;&#26377;&#24110;&#21161;&#65306;&#30740;&#31350;&#21516;&#28304;&#24615;&#21407;&#21017;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability. (arXiv:2304.14274v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14274
&lt;/p&gt;
&lt;p&gt;
&#21516;&#28304;&#24615;&#21407;&#21017;&#19981;&#19968;&#23450;&#26159;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#21807;&#19968;&#21407;&#22240;&#65307;&#26412;&#25991;&#25552;&#20986;Contextual Stochastic Block Model for Homophily (CSBM-H)&#20197;&#28145;&#20837;&#30740;&#31350;&#21516;&#28304;&#24615;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#28304;&#24615;&#21407;&#21017;&#25351;&#30456;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#26356;&#26377;&#21487;&#33021;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#65288;NC&#65289;&#20219;&#21153;&#19978;&#24615;&#33021;&#20248;&#36234;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#25552;&#20986;&#29702;&#35770;&#32467;&#26524;&#35748;&#20026;&#65292;&#21363;&#20351;&#21516;&#28304;&#24615;&#21407;&#21017;&#34987;&#25171;&#30772;&#65292;&#21482;&#35201;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#33410;&#28857;&#20998;&#20139;&#30456;&#20284;&#30340;&#37051;&#23621;&#27169;&#24335;&#65292;GNN&#30340;&#20248;&#21183;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#23545;&#21516;&#28304;&#24615;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#35770;&#28857;&#20165;&#32771;&#34385;&#20102;&#21516;&#31867;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#24573;&#30053;&#20102;&#36328;&#31867;&#21035;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#36825;&#26159;&#30740;&#31350;&#21516;&#28304;&#24615;&#25928;&#24212;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20363;&#23376;&#35777;&#26126;&#20102;&#19978;&#36848;&#19981;&#36275;&#65292;&#24182;&#35748;&#20026;&#21487;&#21306;&#20998;&#24615;&#30340;&#29702;&#24819;&#24773;&#20917;&#26159;&#21516;&#31867;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#23567;&#20110;&#36328;&#31867;&#21035;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#36825;&#20010;&#24819;&#27861;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#21516;&#28304;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Contextual Stochastic Block Model for Homophily (CSBM-H)&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Homophily principle, i.e. nodes with the same labels are more likely to be connected, was believed to be the main reason for the performance superiority of Graph Neural Networks (GNNs) over Neural Networks (NNs) on Node Classification (NC) tasks. Recently, people have developed theoretical results arguing that, even though the homophily principle is broken, the advantage of GNNs can still hold as long as nodes from the same class share similar neighborhood patterns, which questions the validity of homophily. However, this argument only considers intra-class Node Distinguishability (ND) and ignores inter-class ND, which is insufficient to study the effect of homophily. In this paper, we first demonstrate the aforementioned insufficiency with examples and argue that an ideal situation for ND is to have smaller intra-class ND than inter-class ND. To formulate this idea and have a better understanding of homophily, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) and def
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#8212;&#8212;&#19978;&#19979;&#25991;&#22871;&#32034;&#65292;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#25311;&#21512;&#33021;&#21147;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31232;&#30095;&#25311;&#21512;&#65292;&#24182;&#19988;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2302.00878</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22871;&#32034;&#65306;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#23454;&#29616;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The contextual lasso: Sparse linear models via deep neural networks. (arXiv:2302.00878v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#8212;&#8212;&#19978;&#19979;&#25991;&#22871;&#32034;&#65292;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#25311;&#21512;&#33021;&#21147;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31232;&#30095;&#25311;&#21512;&#65292;&#24182;&#19988;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26631;&#20934;&#24037;&#20855;&#65292;&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#24378;&#22823;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#19978;&#19979;&#25991;&#22871;&#32034;&#26159;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#23427;&#23558;&#36755;&#20837;&#29305;&#24449;&#20998;&#25104;&#21487;&#35299;&#37322;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#20004;&#32452;&#65292;&#24182;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#36827;&#34892;&#31232;&#30095;&#25311;&#21512;&#65292;&#21516;&#26102;&#20854;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#36825;&#20010;&#36807;&#31243;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#38656;&#21442;&#25968;&#22320;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse linear models are a gold standard tool for interpretable machine learning, a field of emerging importance as predictive models permeate decision-making in many domains. Unfortunately, sparse linear models are far less flexible as functions of their input features than black-box models like deep neural networks. With this capability gap in mind, we study a not-uncommon situation where the input features dichotomize into two groups: explanatory features, which are candidates for inclusion as variables in an interpretable model, and contextual features, which select from the candidate variables and determine their effects. This dichotomy leads us to the contextual lasso, a new statistical estimator that fits a sparse linear model to the explanatory features such that the sparsity pattern and coefficients vary as a function of the contextual features. The fitting process learns this function nonparametrically via a deep neural network. To attain sparse coefficients, we train the net
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06950</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v5 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20248;&#21270;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21464;&#25442;&#32534;&#30721;&#33539;&#24335;&#65292;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#20449;&#24687;&#29109;&#32534;&#30721;&#65292;&#28982;&#21518;&#20877;&#26144;&#23556;&#22238;&#25968;&#25454;&#31354;&#38388;&#36827;&#34892;&#37325;&#26500;&#12290;&#19982;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#31070;&#32463;&#21387;&#32553;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35299;&#30721;&#22120;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#20869;&#23481;&#8221;&#28508;&#21464;&#37327;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20250;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#24182;&#21033;&#29992;&#35813;&#21464;&#37327;&#23384;&#20648;&#22270;&#20687;&#20449;&#24687;&#12290;&#20915;&#23450;&#25193;&#25955;&#36807;&#31243;&#30340;&#21097;&#20313;&#8220;&#32441;&#29702;&#8221;&#21464;&#37327;&#20250;&#22312;&#35299;&#30721;&#26102;&#21512;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#26681;&#25454;&#24863;&#30693;&#24230;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#28041;&#21450;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#36739;&#20110;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional "content" latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining "texture" variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based mode
&lt;/p&gt;</description></item></channel></rss>