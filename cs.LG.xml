<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#26368;&#20339;&#21452;&#36194;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20248;&#24615;&#24046;&#36317;&#21463;&#21040;&#19979;&#30028;&#38480;&#21046;&#26102;&#36951;&#25022;&#20026;$O(\log(T))$&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#36793;&#32536;&#26465;&#20214;&#26469;&#25551;&#36848;&#27425;&#20248;&#24615;&#24046;&#36317;&#23545;&#38382;&#39064;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03219</link><description>&lt;p&gt;
LC-Tsalis-INF: &#24191;&#20041;&#26368;&#20339;&#21452;&#36194;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#26368;&#20339;&#21452;&#36194;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20248;&#24615;&#24046;&#36317;&#21463;&#21040;&#19979;&#30028;&#38480;&#21046;&#26102;&#36951;&#25022;&#20026;$O(\log(T))$&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#36793;&#32536;&#26465;&#20214;&#26469;&#25551;&#36848;&#27425;&#20248;&#24615;&#24046;&#36317;&#23545;&#38382;&#39064;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20855;&#26377;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#32972;&#26223;&#30340;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#29616;&#26377;&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#20339;&#21452;&#36194;&#65288;BoBW&#65289;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#22312;&#38543;&#26426;&#21306;&#22495;&#20013;&#28385;&#36275;$O(\log^2(T))$&#65292;&#20854;&#20013;$T$&#20026;&#22238;&#21512;&#25968;&#65292;&#20854;&#27425;&#20248;&#24615;&#24046;&#36317;&#30001;&#27491;&#24120;&#25968;&#19979;&#30028;&#65292;&#21516;&#26102;&#22312;&#23545;&#25239;&#24615;&#21306;&#22495;&#20013;&#28385;&#36275;$O(\sqrt{T})$&#12290;&#28982;&#32780;&#65292;&#23545;$T$&#30340;&#20381;&#36182;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#24182;&#19988;&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#20551;&#35774;&#21487;&#20197;&#25918;&#23485;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#24403;&#27425;&#20248;&#24615;&#24046;&#36317;&#21463;&#21040;&#19979;&#30028;&#38480;&#21046;&#26102;&#65292;&#20854;&#36951;&#25022;&#28385;&#36275;$O(\log(T))$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36793;&#32536;&#26465;&#20214;&#65292;&#21363;&#23545;&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#19968;&#20010;&#26356;&#28201;&#21644;&#30340;&#20551;&#35774;&#12290;&#35813;&#26465;&#20214;&#20351;&#29992;&#21442;&#25968;$\beta \in (0, \infty]$&#34920;&#24449;&#19982;&#27425;&#20248;&#24615;&#24046;&#36317;&#30456;&#20851;&#30340;&#38382;&#39064;&#38590;&#24230;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#35813;&#31639;&#27861;&#30340;&#36951;&#25022;&#28385;&#36275;$O\left(\
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03219v1 Announce Type: new  Abstract: This study considers the linear contextual bandit problem with independent and identically distributed (i.i.d.) contexts. In this problem, existing studies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets satisfy $O(\log^2(T))$ for the number of rounds $T$ in a stochastic regime with a suboptimality gap lower-bounded by a positive constant, while satisfying $O(\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has room for improvement, and the suboptimality-gap assumption can be relaxed. For this issue, this study proposes an algorithm whose regret satisfies $O(\log(T))$ in the setting when the suboptimality gap is lower-bounded. Furthermore, we introduce a margin condition, a milder assumption on the suboptimality gap. That condition characterizes the problem difficulty linked to the suboptimality gap using a parameter $\beta \in (0, \infty]$. We then show that the algorithm's regret satisfies $O\left(\
&lt;/p&gt;</description></item><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>&#20027;&#21160;&#25512;&#26029;&#26159;&#19968;&#31181;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30830;&#23450;&#26368;&#26377;&#21033;&#20110;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#26469;&#26377;&#25928;&#21033;&#29992;&#39044;&#31639;&#65292;&#23454;&#29616;&#27604;&#29616;&#26377;&#22522;&#32447;&#26356;&#23569;&#26679;&#26412;&#30340;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03208</link><description>&lt;p&gt;
&#20027;&#21160;&#32479;&#35745;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Active Statistical Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03208
&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25512;&#26029;&#26159;&#19968;&#31181;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30830;&#23450;&#26368;&#26377;&#21033;&#20110;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#26469;&#26377;&#25928;&#21033;&#29992;&#39044;&#31639;&#65292;&#23454;&#29616;&#27604;&#29616;&#26377;&#22522;&#32447;&#26356;&#23569;&#26679;&#26412;&#30340;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20027;&#21160;&#23398;&#20064;&#27010;&#24565;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20027;&#21160;&#25512;&#26029;&#8212;&#8212;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#25968;&#25454;&#25910;&#38598;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;&#20551;&#35774;&#23545;&#21487;&#25910;&#38598;&#30340;&#26631;&#31614;&#25968;&#37327;&#26377;&#39044;&#31639;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30830;&#23450;&#21738;&#20123;&#25968;&#25454;&#28857;&#26368;&#26377;&#21033;&#20110;&#26631;&#35760;&#65292;&#20174;&#32780;&#26377;&#25928;&#21033;&#29992;&#39044;&#31639;&#12290;&#20854;&#36816;&#20316;&#26041;&#24335;&#22522;&#20110;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#30452;&#35273;&#65306;&#20248;&#20808;&#25910;&#38598;&#27169;&#22411;&#34920;&#29616;&#20986;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#28857;&#30340;&#26631;&#31614;&#65292;&#24182;&#22312;&#27169;&#22411;&#34920;&#29616;&#20986;&#33258;&#20449;&#26102;&#20381;&#36182;&#20110;&#20854;&#39044;&#27979;&#12290;&#20027;&#21160;&#25512;&#26029;&#26500;&#24314;&#20102;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#20551;&#35774;&#26816;&#39564;&#65292;&#21516;&#26102;&#21033;&#29992;&#20219;&#20309;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#22788;&#29702;&#20219;&#20309;&#25968;&#25454;&#20998;&#24067;&#12290;&#20851;&#38190;&#28857;&#22312;&#20110;&#65292;&#23427;&#33021;&#20197;&#27604;&#20381;&#36182;&#20110;&#38750;&#33258;&#36866;&#24212;&#25910;&#38598;&#25968;&#25454;&#30340;&#29616;&#26377;&#22522;&#32447;&#26356;&#23569;&#30340;&#26679;&#26412;&#36798;&#21040;&#30456;&#21516;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#20110;&#30456;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03208v1 Announce Type: cross  Abstract: Inspired by the concept of active learning, we propose active inference$\unicode{x2013}$a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model's predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number 
&lt;/p&gt;</description></item><item><title>&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#30340;&#21442;&#25968;&#21270;&#35821;&#35328;&#27169;&#22411;LMs&#65292;&#20316;&#32773;&#20027;&#24352;&#20351;&#29992;&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#20316;&#20026;&#19979;&#19968;&#20195;LMs&#65292;&#20197;&#25552;&#39640;&#21487;&#38752;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03187</link><description>&lt;p&gt;
&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;&#21487;&#38752;&#12289;&#36866;&#24212;&#24615;&#24378;&#19988;&#21487;&#36861;&#28335;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reliable, Adaptable, and Attributable Language Models with Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03187
&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#30340;&#21442;&#25968;&#21270;&#35821;&#35328;&#27169;&#22411;LMs&#65292;&#20316;&#32773;&#20027;&#24352;&#20351;&#29992;&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#20316;&#20026;&#19979;&#19968;&#20195;LMs&#65292;&#20197;&#25552;&#39640;&#21487;&#38752;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#28023;&#37327;&#32593;&#32476;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28789;&#27963;&#24615;&#21644;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#24187;&#35273;&#12289;&#38590;&#20197;&#36866;&#24212;&#26032;&#25968;&#25454;&#20998;&#24067;&#21644;&#32570;&#20047;&#21487;&#39564;&#35777;&#24615;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20027;&#24352;&#29992;&#20855;&#22791;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#21462;&#20195;&#21442;&#25968;&#21270;LMs&#20316;&#20026;&#19979;&#19968;&#20195;LMs&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25972;&#21512;&#22823;&#35268;&#27169;&#25968;&#25454;&#23384;&#20648;&#65292;&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#21487;&#20197;&#26356;&#21152;&#21487;&#38752;&#12289;&#36866;&#24212;&#24615;&#26356;&#24378;&#12289;&#21487;&#36861;&#28335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03187v1 Announce Type: cross  Abstract: Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose
&lt;/p&gt;</description></item><item><title>&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03185</link><description>&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Preventing Reward Hacking with Occupancy Measure Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03185
&lt;/p&gt;
&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#29702;&#26681;&#25454;&#19968;&#20010;&#8220;&#20195;&#29702;&#8221;&#22870;&#21169;&#20989;&#25968;&#65288;&#21487;&#33021;&#26159;&#25163;&#21160;&#25351;&#23450;&#25110;&#23398;&#20064;&#30340;&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30456;&#23545;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#22870;&#21169;&#21364;&#34920;&#29616;&#31967;&#31957;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#22870;&#21169;&#27450;&#39575;&#12290;&#30001;&#20110;&#30830;&#20445;&#20195;&#29702;&#21644;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#33391;&#22909;&#23545;&#40784;&#26497;&#20026;&#22256;&#38590;&#65292;&#39044;&#38450;&#22870;&#21169;&#27450;&#39575;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20445;&#23432;&#22320;&#20248;&#21270;&#20195;&#29702;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#20110;&#36890;&#36807;&#24809;&#32602;&#20182;&#20204;&#30340;&#34892;&#20026;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#24378;&#21046;&#35753;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#34920;&#29616;&#31867;&#20284;&#20110;&#8220;&#23433;&#20840;&#8221;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#34892;&#20026;&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#65292;&#22240;&#20026;&#22312;&#21333;&#20010;&#29366;&#24577;&#19979;&#34892;&#20026;&#20998;&#24067;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#65292;&#32780;&#36739;&#22823;&#30340;&#21464;&#21270;&#21487;&#33021;&#24182;&#19981;&#20195;&#34920;&#20219;&#20309;&#21361;&#38505;&#27963;&#21160;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#26159;&#65292;&#24403;&#22870;&#21169;&#27450;&#39575;&#26102;&#65292;&#20195;&#29702;&#35775;&#38382;&#30340;&#29366;&#24577;&#19982;&#23433;&#20840;&#31574;&#30053;&#36798;&#21040;&#30340;&#29366;&#24577;&#25130;&#28982;&#19981;&#21516;&#65292;&#23548;&#33268;&#29366;&#24577;&#21344;&#29992;&#24230;&#30340;&#24040;&#22823;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03185v1 Announce Type: cross  Abstract: Reward hacking occurs when an agent performs very well with respect to a "proxy" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward. Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively. Prior work has particularly focused on enforcing the learned policy to behave similarly to a "safe" policy by penalizing the KL divergence between their action distributions (AD). However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity. Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM
&lt;/p&gt;</description></item><item><title>Transformers&#33021;&#22815;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;Transformer&#21487;&#20197;&#22312; logistic &#22238;&#24402;&#20219;&#21153;&#20013;&#36817;&#20284;&#23454;&#29616;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#21363;&#20351;&#26159;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;Transformer&#20063;&#21487;&#20197;&#23454;&#29616;&#30697;&#38453;&#27714;&#36870;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;</title><link>https://arxiv.org/abs/2403.03183</link><description>&lt;p&gt;
Transformers&#33021;&#22810;&#22909;&#22320;&#27169;&#25311; Newton &#26041;&#27861;&#19978;&#19979;&#25991;&#20013;&#30340;&#34920;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Well Can Transformers Emulate In-context Newton's Method?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03183
&lt;/p&gt;
&lt;p&gt;
Transformers&#33021;&#22815;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;Transformer&#21487;&#20197;&#22312; logistic &#22238;&#24402;&#20219;&#21153;&#20013;&#36817;&#20284;&#23454;&#29616;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#21363;&#20351;&#26159;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;Transformer&#20063;&#21487;&#20197;&#23454;&#29616;&#30697;&#38453;&#27714;&#36870;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;&#22522;&#30784;&#26426;&#21046;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformers&#21487;&#20197;&#23454;&#29616;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#29978;&#33267;&#23545;&#20110;&#32447;&#24615;&#22238;&#24402;&#30340;&#24773;&#20917;&#65292;&#21487;&#20197;&#23454;&#29616;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Transformer&#26159;&#21542;&#33021;&#22815;&#25191;&#34892;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#32447;&#24615;&#22238;&#24402;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30830;&#23450;&#20855;&#26377;ReLU&#23618;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;Transformer&#21487;&#20197;&#36817;&#20284;&#23454;&#29616;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36923;&#36753;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#23545;&#25968;&#21040;&#38169;&#35823;&#26356;&#22810;&#30340;&#23618;&#21487;&#20197;&#36798;&#21040;$\epsilon$&#35823;&#24046;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#20165;&#20855;&#26377;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;Transformer&#20063;&#21487;&#20197;&#22312;&#20165;&#20004;&#23618;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30697;&#38453;&#27714;&#36870;&#30340;&#29275;&#39039;&#36845;&#20195;&#30340;&#21333;&#27493;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;Transformer&#26550;&#26500;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03183v1 Announce Type: cross  Abstract: Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms. Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression. In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression. We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve $\epsilon$ error with only a logarithmic to the error more layers. As a by-product we demonstrate the ability of even linear attention-only Transformers in implementing a single step of Newton's iteration for matrix inversion with merely two layers. These results suggest the ability of the Transformer architecture to im
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30690;&#37327;&#37327;&#21270;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;VQ-BeT&#65289;&#30340;&#22810;&#21151;&#33021;&#34892;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#36830;&#32493;&#21160;&#20316;&#36827;&#34892;&#26631;&#35760;&#21270;&#22788;&#29702;&#22810;&#27169;&#24577;&#21160;&#20316;&#39044;&#27979;&#12289;&#26465;&#20214;&#29983;&#25104;&#21644;&#37096;&#20998;&#35266;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.03181</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#21160;&#20316;&#30340;&#34892;&#20026;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Behavior Generation with Latent Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03181
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30690;&#37327;&#37327;&#21270;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;VQ-BeT&#65289;&#30340;&#22810;&#21151;&#33021;&#34892;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#36830;&#32493;&#21160;&#20316;&#36827;&#34892;&#26631;&#35760;&#21270;&#22788;&#29702;&#22810;&#27169;&#24577;&#21160;&#20316;&#39044;&#27979;&#12289;&#26465;&#20214;&#29983;&#25104;&#21644;&#37096;&#20998;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22797;&#26434;&#34892;&#20026;&#30340;&#29983;&#25104;&#24314;&#27169;&#19968;&#30452;&#26159;&#20915;&#31574;&#21046;&#23450;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#19982;&#35821;&#35328;&#25110;&#22270;&#20687;&#29983;&#25104;&#19981;&#21516;&#65292;&#20915;&#31574;&#21046;&#23450;&#38656;&#35201;&#24314;&#27169;&#21160;&#20316; - &#36830;&#32493;&#20540;&#21521;&#37327;&#65292;&#20854;&#22312;&#20998;&#24067;&#19978;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#21487;&#33021;&#26469;&#33258;&#26410;&#32463;&#31579;&#36873;&#30340;&#26469;&#28304;&#65292;&#22312;&#39034;&#24207;&#39044;&#27979;&#20013;&#29983;&#25104;&#35823;&#24046;&#21487;&#33021;&#20250;&#30456;&#20114;&#32047;&#31215;&#12290;&#26368;&#36817;&#19968;&#31867;&#31216;&#20026;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;BeT&#65289;&#30340;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;k-means&#32858;&#31867;&#23545;&#21160;&#20316;&#36827;&#34892;&#31163;&#25955;&#21270;&#20197;&#25429;&#25417;&#19981;&#21516;&#27169;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;k-means&#22312;&#22788;&#29702;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#25110;&#38271;&#24207;&#21015;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#32570;&#20047;&#26799;&#24230;&#20449;&#24687;&#65292;&#22240;&#27492;BeT&#22312;&#24314;&#27169;&#38271;&#36317;&#31163;&#21160;&#20316;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30690;&#37327;&#37327;&#21270;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;VQ-BeT&#65289;&#30340;&#22810;&#21151;&#33021;&#34892;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#21160;&#20316;&#39044;&#27979;&#12289;&#26465;&#20214;&#29983;&#25104;&#21644;&#37096;&#20998;&#35266;&#23519;&#12290;VQ-BeT&#36890;&#36807;&#23545;&#36830;&#32493;&#21160;&#20316;&#36827;&#34892;&#26631;&#35760;&#21270;&#26469;&#22686;&#24378;BeT
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03181v1 Announce Type: cross  Abstract: Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making. Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#27927;&#29260;&#21160;&#37327;&#26799;&#24230;&#26041;&#27861;&#25193;&#23637;&#21040;&#26377;&#38480;&#21644;&#24378;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;&#38024;&#23545;&#24378;&#20984;&#35774;&#32622;&#30340;&#27927;&#29260;&#21160;&#37327;&#26041;&#27861;&#30340;&#20998;&#26512;&#65292;&#36798;&#21040;&#20102;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/nT^2)$&#12290;</title><link>https://arxiv.org/abs/2403.03180</link><description>&lt;p&gt;
&#29992;&#20110;&#20984;&#20248;&#21270;&#30340;&#27927;&#29260;&#21160;&#37327;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Shuffling Momentum Gradient Algorithm for Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#27927;&#29260;&#21160;&#37327;&#26799;&#24230;&#26041;&#27861;&#25193;&#23637;&#21040;&#26377;&#38480;&#21644;&#24378;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;&#38024;&#23545;&#24378;&#20984;&#35774;&#32622;&#30340;&#27927;&#29260;&#21160;&#37327;&#26041;&#27861;&#30340;&#20998;&#26512;&#65292;&#36798;&#21040;&#20102;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/nT^2)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65288;SGD&#65289;&#21450;&#20854;&#38543;&#26426;&#21464;&#20307;&#24050;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#30001;&#22823;&#35268;&#27169;&#24212;&#29992;&#21644;&#22823;&#25968;&#25454;&#38598;&#20135;&#29983;&#30340;&#26377;&#38480;&#21644;&#20248;&#21270;&#38382;&#39064;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#22788;&#29702;&#22823;&#35268;&#27169;&#24212;&#29992;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#30740;&#31350;SGD&#21450;&#20854;&#27927;&#29260;&#21464;&#20307;&#30340;&#29702;&#35770;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#26377;&#38480;&#30340;&#24037;&#20316;&#28041;&#21450;&#20102;&#20854;&#27927;&#29260;&#21160;&#37327;&#21464;&#20307;&#65292;&#21253;&#25324;&#29992;&#20110;&#38750;&#20984;&#38382;&#39064;&#30340;&#27927;&#29260;&#37325;&#37327;&#29699;&#21160;&#37327;&#26041;&#26696;&#21644;&#29992;&#20110;&#20984;&#35774;&#32622;&#30340;Nesterov&#21160;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;Tran&#31561;&#20154;&#65288;2021&#24180;&#65289;&#25152;&#24320;&#21457;&#30340;&#27927;&#29260;&#21160;&#37327;&#26799;&#24230;&#26041;&#27861;&#30340;&#20998;&#26512;&#25299;&#23637;&#21040;&#26377;&#38480;&#21644;&#24378;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#38024;&#23545;&#24378;&#20984;&#35774;&#32622;&#30340;&#27927;&#29260;&#21160;&#37327;&#26041;&#27861;&#30340;&#20998;&#26512;&#65292;&#36798;&#21040;&#20102;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/nT^2)$&#65292;&#20854;&#20013;$n$&#26159;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03180v1 Announce Type: cross  Abstract: The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants. However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings. In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20849;&#35782;&#26426;&#21046;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#24819;&#35937;&#26694;&#26550;&#24341;&#23548;&#26234;&#33021;&#20307;&#36798;&#25104;&#20849;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03172</link><description>&lt;p&gt;
&#20351;&#29992;&#30446;&#26631;&#24819;&#35937;&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03172
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20849;&#35782;&#26426;&#21046;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#24819;&#35937;&#26694;&#26550;&#24341;&#23548;&#26234;&#33021;&#20307;&#36798;&#25104;&#20849;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36798;&#25104;&#19968;&#33268;&#24847;&#35265;&#23545;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23436;&#25104;&#21327;&#20316;&#20219;&#21153;&#65292;&#26234;&#33021;&#20307;&#38656;&#35201;&#21327;&#35843;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#32852;&#21512;&#21160;&#20316;&#65292;&#20197;&#26368;&#22823;&#21270;&#22242;&#38431;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#19981;&#26126;&#30830;&#32771;&#34385;&#19968;&#33268;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#21327;&#35843;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20849;&#35782;&#26426;&#21046;&#65292;&#20197;&#26126;&#30830;&#21327;&#35843;&#22810;&#20010;&#26234;&#33021;&#20307;&#12290;&#25552;&#20986;&#30340;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#24819;&#35937;&#65288;MAGI&#65289;&#26694;&#26550;&#24341;&#23548;&#26234;&#33021;&#20307;&#36890;&#36807;&#24819;&#35937;&#20986;&#30340;&#20849;&#21516;&#30446;&#26631;&#36798;&#25104;&#19968;&#33268;&#12290;&#20849;&#21516;&#30446;&#26631;&#26159;&#19968;&#20010;&#20855;&#26377;&#39640;&#20215;&#20540;&#30340;&#21487;&#23454;&#29616;&#29366;&#24577;&#65292;&#36890;&#36807;&#20174;&#26410;&#26469;&#29366;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#33719;&#24471;&#12290;&#25105;&#20204;&#30452;&#25509;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#23545;&#27492;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27169;&#22411;&#26041;&#27861;&#20013;&#24120;&#29992;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#27493;&#39588;&#31574;&#30053;&#23637;&#24320;&#24341;&#36215;&#30340;&#8220;&#32500;&#24230;&#28798;&#38590;&#8221;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#39640;&#25928;&#30340;&#20849;&#35782;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03172v1 Announce Type: new  Abstract: Reaching consensus is key to multi-agent coordination. To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward. However, current cooperative multi-agent reinforcement learning (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem. In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal. The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states. We directly model this distribution with a self-supervised generative model, thus alleviating the "curse of dimensinality" problem induced by multi-agent multi-step policy rollout commonly used in model-based methods. We show that such efficient c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21464;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#25511;&#21046;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#21644;&#26465;&#20214;&#25968;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#20445;&#35777;&#25968;&#25454;&#22312;&#31232;&#30095;&#22495;&#20013;&#20855;&#26377;&#33391;&#22909;&#34920;&#31034;&#30340;&#20248;&#21270;&#21464;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.03168</link><description>&lt;p&gt;
&#23398;&#20064;&#26174;&#24335;&#26465;&#20214;&#21270;&#31232;&#30095;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Learning Explicitly Conditioned Sparsifying Transforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21464;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#25511;&#21046;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#21644;&#26465;&#20214;&#25968;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#20445;&#35777;&#25968;&#25454;&#22312;&#31232;&#30095;&#22495;&#20013;&#20855;&#26377;&#33391;&#22909;&#34920;&#31034;&#30340;&#20248;&#21270;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#31232;&#30095;&#21270;&#21464;&#25442;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24191;&#20026;&#20154;&#30693;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#26576;&#20123;&#21464;&#25442;&#22495;&#20013;&#25214;&#21040;&#20449;&#21495;&#30340;&#32467;&#26500;&#31232;&#30095;&#34920;&#31034;&#12290;&#23613;&#31649;&#20687;DCT&#21644;&#23567;&#27874;&#36825;&#26679;&#30340;&#32463;&#20856;&#21464;&#25442;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#26368;&#36817;&#22312;&#19968;&#31995;&#21015;&#35770;&#25991;&#20013;&#24050;&#32463;&#20998;&#26512;&#20102;&#23398;&#20064;&#20445;&#35777;&#25968;&#25454;&#22312;&#31232;&#30095;&#22495;&#20013;&#20855;&#26377;&#33391;&#22909;&#34920;&#31034;&#30340;&#26368;&#20248;&#21464;&#25442;&#12290;&#23398;&#20064;&#26041;&#22359;&#21464;&#25442;&#30340;&#26465;&#20214;&#25968;&#21644;&#34920;&#31034;&#33021;&#21147;&#36890;&#24120;&#26159;&#20114;&#34917;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#21487;&#33021;&#22312;&#32473;&#23450;&#30340;&#20248;&#21270;&#27169;&#22411;&#20013;&#19981;&#33021;&#26126;&#30830;&#25511;&#21046;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21464;&#25442;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#24378;&#21046;&#22312;&#23398;&#20064;&#21464;&#25442;&#30340;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#21644;&#26465;&#20214;&#25968;&#19978;&#36827;&#34892;&#26174;&#24335;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#30830;&#35748;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03168v1 Announce Type: cross  Abstract: Sparsifying transforms became in the last decades widely known tools for finding structured sparse representations of signals in certain transform domains. Despite the popularity of classical transforms such as DCT and Wavelet, learning optimal transforms that guarantee good representations of data into the sparse domain has been recently analyzed in a series of papers. Typically, the conditioning number and representation ability are complementary key features of learning square transforms that may not be explicitly controlled in a given optimization model. Unlike the existing approaches from the literature, in our paper, we consider a new sparsifying transform model that enforces explicit control over the data representation quality and the condition number of the learned transforms. We confirm through numerical experiments that our model presents better numerical behavior than the state-of-the-art.
&lt;/p&gt;</description></item><item><title>PalmProbNet &#26159;&#19968;&#31181;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#20998;&#26512;&#39640;&#20998;&#36776;&#29575;&#26080;&#20154;&#26426;&#22270;&#20687;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21380;&#29916;&#22810;&#23572;&#38632;&#26519;&#20013;&#26816;&#27979;&#26837;&#27016;&#26641;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#26837;&#27016;&#26641;&#26816;&#27979;&#65292;&#26377;&#25928;&#30830;&#23450;&#26837;&#27016;&#26641;&#22312;&#28151;&#21512;&#28909;&#24102;&#38632;&#26519;&#20013;&#30340;&#23384;&#22312;&#21644;&#20301;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.03161</link><description>&lt;p&gt;
PalmProbNet&#65306;&#19968;&#31181;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#29702;&#35299;&#21380;&#29916;&#22810;&#23572;&#28909;&#24102;&#38632;&#26519;&#20013;&#26837;&#27016;&#20998;&#24067;&#30340;&#27010;&#29575;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03161
&lt;/p&gt;
&lt;p&gt;
PalmProbNet &#26159;&#19968;&#31181;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#20998;&#26512;&#39640;&#20998;&#36776;&#29575;&#26080;&#20154;&#26426;&#22270;&#20687;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21380;&#29916;&#22810;&#23572;&#38632;&#26519;&#20013;&#26816;&#27979;&#26837;&#27016;&#26641;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#26837;&#27016;&#26641;&#26816;&#27979;&#65292;&#26377;&#25928;&#30830;&#23450;&#26837;&#27016;&#26641;&#22312;&#28151;&#21512;&#28909;&#24102;&#38632;&#26519;&#20013;&#30340;&#23384;&#22312;&#21644;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26837;&#27016;&#26641;&#22312;&#28909;&#24102;&#26862;&#26519;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23545;&#20110;&#20154;&#31867;&#21644;&#37326;&#29983;&#21160;&#29289;&#26469;&#35828;&#26159;&#37325;&#35201;&#36164;&#28304;&#12290;&#28909;&#24102;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#29702;&#35299;&#26837;&#27016;&#26641;&#30340;&#20998;&#24067;&#21644;&#20016;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#28909;&#24102;&#38632;&#26519;&#30340;&#22270;&#20687;&#20013;&#20934;&#30830;&#35782;&#21035;&#21644;&#23450;&#20301;&#26837;&#27016;&#26641;&#30001;&#20110;&#23494;&#38598;&#26893;&#34987;&#12289;&#37325;&#21472;&#26641;&#20896;&#20197;&#21450;&#28151;&#21512;&#26862;&#26519;&#26223;&#35266;&#20013;&#30340;&#20809;&#29031;&#26465;&#20214;&#21464;&#21270;&#32780;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PalmProbNet&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#39640;&#20998;&#36776;&#29575;&#30340;&#26080;&#20154;&#26426;&#33719;&#21462;&#30340;&#27491;&#23556;&#24433;&#20687;&#22270;&#20687;&#65292;&#20174;&#32780;&#22312;&#21380;&#29916;&#22810;&#23572;&#38632;&#26519;&#30340;&#23494;&#38598;&#26641;&#20896;&#20013;&#26816;&#27979;&#26837;&#27016;&#26641;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#33258;&#21160;&#26837;&#27016;&#26641;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#30830;&#23450;&#28151;&#21512;&#28909;&#24102;&#38632;&#26519;&#20013;&#30340;&#26837;&#27016;&#26641;&#23384;&#22312;&#21644;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#22987;&#20110;&#20174;&#26080;&#20154;&#26426;&#22270;&#20687;&#29983;&#25104;&#27491;&#23556;&#24433;&#20687;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#21644;&#26631;&#35760;&#26837;&#27016;&#26641;&#21644;&#38750;&#26837;&#27016;&#26641;&#22270;&#20687;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03161v1 Announce Type: cross  Abstract: Palms play an outsized role in tropical forests and are important resources for humans and wildlife. A central question in tropical ecosystems is understanding palm distribution and abundance. However, accurately identifying and localizing palms in geospatial imagery presents significant challenges due to dense vegetation, overlapping canopies, and variable lighting conditions in mixed-forest landscapes. Addressing this, we introduce PalmProbNet, a probabilistic approach utilizing transfer learning to analyze high-resolution UAV-derived orthomosaic imagery, enabling the detection of palm trees within the dense canopy of the Ecuadorian Rainforest. This approach represents a substantial advancement in automated palm detection, effectively pinpointing palm presence and locality in mixed tropical rainforests. Our process begins by generating an orthomosaic image from UAV images, from which we extract and label palm and non-palm image patch
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;NOMA&#22686;&#24378;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#23558;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#19982;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#38750;IID&#26465;&#20214;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.03157</link><description>&lt;p&gt;
&#22312;NOMA&#22686;&#24378;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#37325;&#26032;&#24605;&#32771;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Clustered Federated Learning in NOMA Enhanced Wireless Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;NOMA&#22686;&#24378;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#23558;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#19982;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#38750;IID&#26465;&#20214;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26032;&#22411;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#26041;&#27861;&#19982;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#65288;NOMA&#65289;&#30456;&#32467;&#21512;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#38598;&#19979;&#30340;&#20248;&#21183;&#65292;&#20854;&#20013;&#22810;&#20010;&#35774;&#22791;&#21442;&#19982;&#20855;&#26377;&#26102;&#38388;&#38480;&#21046;&#21644;&#26377;&#38480;&#23376;&#20449;&#36947;&#25968;&#37327;&#30340;&#32858;&#21512;&#12290;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#20102;&#34913;&#37327;&#25968;&#25454;&#20998;&#24067;&#20013;&#38750;IID&#31243;&#24230;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#22312;&#27492;&#20043;&#21518;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#38750;IID&#26465;&#20214;&#25152;&#24102;&#26469;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#20102;&#21508;&#39033;&#24615;&#36136;&#12290;&#20855;&#20307;&#22320;&#65292;&#29992;&#25143;&#30340;&#25968;&#25454;&#20998;&#24067;&#34987;&#21442;&#25968;&#21270;&#20026;&#38598;&#20013;&#21442;&#25968;&#65292;&#24182;&#20351;&#29992;&#35889;&#32858;&#31867;&#36827;&#34892;&#20998;&#32452;&#65292;Dirichlet&#20998;&#24067;&#20316;&#20026;&#20808;&#39564;&#12290;&#23545;&#27867;&#21270;&#24046;&#36317;&#21644;&#25910;&#25947;&#36895;&#29575;&#30340;&#25506;&#35752;&#25351;&#23548;&#20102;&#36890;&#36807;&#22522;&#20110;&#21305;&#37197;&#30340;&#31639;&#27861;&#35774;&#35745;&#23376;&#20449;&#36947;&#20998;&#37197;&#65292;&#24182;&#23454;&#29616;&#21151;&#29575;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03157v1 Announce Type: cross  Abstract: This study explores the benefits of integrating the novel clustered federated learning (CFL) approach with non-orthogonal multiple access (NOMA) under non-independent and identically distributed (non-IID) datasets, where multiple devices participate in the aggregation with time limitations and a finite number of sub-channels. A detailed theoretical analysis of the generalization gap that measures the degree of non-IID in the data distribution is presented. Following that, solutions to address the challenges posed by non-IID conditions are proposed with the analysis of the properties. Specifically, users' data distributions are parameterized as concentration parameters and grouped using spectral clustering, with Dirichlet distribution serving as the prior. The investigation into the generalization gap and convergence rate guides the design of sub-channel assignments through the matching-based algorithm, and the power allocation is achie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#21387;&#32553;&#27169;&#22411;HQARF&#65292;&#29992;&#20110;&#23545;&#23556;&#39057;&#20449;&#21495;&#36827;&#34892;&#21387;&#32553;&#21644;&#20998;&#31867;&#65292;&#20197;&#38477;&#20302;&#25968;&#25454;&#20256;&#36755;&#30340;&#24102;&#23485;&#21644;&#24310;&#36831;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.03150</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21387;&#32553;&#29992;&#20110;&#23556;&#39057;&#20449;&#21495;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep-Learned Compression for Radio-Frequency Signal Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03150
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#21387;&#32553;&#27169;&#22411;HQARF&#65292;&#29992;&#20110;&#23545;&#23556;&#39057;&#20449;&#21495;&#36827;&#34892;&#21387;&#32553;&#21644;&#20998;&#31867;&#65292;&#20197;&#38477;&#20302;&#25968;&#25454;&#20256;&#36755;&#30340;&#24102;&#23485;&#21644;&#24310;&#36831;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20195;&#34562;&#31389;&#27010;&#24565;&#20381;&#36182;&#20110;&#22788;&#29702;&#22823;&#37327;&#30340;&#23556;&#39057;&#65288;RF&#65289;&#26679;&#26412;&#65292;&#21253;&#25324;&#36830;&#25509;&#22522;&#20110;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#30005;&#65288;SDR&#65289;&#30340;&#34562;&#31389;&#21069;&#31471;&#30340;&#23556;&#39057;&#25509;&#20837;&#32593;&#32476;&#65288;RAN&#65289;&#21644;&#29992;&#20110;&#22788;&#29702;&#39057;&#35889;&#30456;&#20851;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#21387;&#32553;&#65288;DLC&#65289;&#27169;&#22411;HQARF&#65292;&#29992;&#20110;&#21387;&#32553;&#30001;6&#31181;&#35843;&#21046;&#31867;&#21035;&#32452;&#25104;&#30340;&#23556;&#39057;&#20449;&#21495;&#30340;&#22797;&#20540;&#26679;&#26412;&#12290;&#25105;&#20204;&#27491;&#22312;&#35780;&#20272;HQARF&#23545;&#35757;&#32451;&#25512;&#26029;&#23556;&#39057;&#20449;&#21495;&#35843;&#21046;&#31867;&#21035;&#30340;AI&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#31364;&#24102;&#23556;&#39057;&#26679;&#26412;&#30340;&#21387;&#32553;&#23558;&#20801;&#35768;&#26377;&#25928;&#36827;&#34892;&#35757;&#32451;&#21644;&#29616;&#22330;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03150v1 Announce Type: new  Abstract: Next-generation cellular concepts rely on the processing of large quantities of radio-frequency (RF) samples. This includes Radio Access Networks (RAN) connecting the cellular front-end based on software defined radios (SDRs) and a framework for the AI processing of spectrum-related data. The RF data collected by the dense RAN radio units and spectrum sensors may need to be jointly processed for intelligent decision making. Moving large amounts of data to AI agents may result in significant bandwidth and latency costs. We propose a deep learned compression (DLC) model, HQARF, based on learned vector quantization (VQ), to compress the complex-valued samples of RF signals comprised of 6 modulation classes. We are assessing the effects of HQARF on the performance of an AI model trained to infer the modulation class of the RF signal. Compression of narrow-band RF samples for the training and off-the-site inference will allow for an efficient
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25308;&#21344;&#24237;-&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;InferGuard&#65292;&#29992;&#20110;&#38450;&#24481;&#23458;&#25143;&#31471;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#25512;&#26029;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.03149</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#32531;&#35299;&#23458;&#25143;&#31471;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25308;&#21344;&#24237;-&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;InferGuard&#65292;&#29992;&#20110;&#38450;&#24481;&#23458;&#25143;&#31471;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#25512;&#26029;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#25581;&#31034;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26366;&#34987;&#35748;&#20026;&#23433;&#20840;&#30340;&#28431;&#27934;&#65292;&#22240;&#20026;&#23458;&#25143;&#31471;&#19981;&#21521;&#26381;&#21153;&#22120;&#20849;&#20139;&#20854;&#31169;&#26377;&#25968;&#25454;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#36973;&#21463;&#35832;&#22914;&#23458;&#25143;&#31471;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#25512;&#26029;&#25915;&#20987;&#31561;&#25915;&#20987;&#65292;&#27492;&#25915;&#20987;&#21487;&#20197;&#35753;&#24694;&#24847;&#23458;&#25143;&#31471;&#37325;&#29616;&#21463;&#23475;&#32773;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25308;&#21344;&#24237;-&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;InferGuard&#65292;&#26088;&#22312;&#38450;&#24481;&#23458;&#25143;&#31471;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#25512;&#26029;&#25915;&#20987;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;InferGuard&#20013;&#65292;&#26381;&#21153;&#22120;&#39318;&#20808;&#35745;&#31639;&#20854;&#25910;&#21040;&#30340;&#25152;&#26377;&#27169;&#22411;&#26356;&#26032;&#30340;&#22352;&#26631;&#20013;&#20301;&#25968;&#12290;&#22914;&#26524;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26032;&#19982;&#35745;&#31639;&#20986;&#30340;&#20013;&#20301;&#25968;&#26356;&#26032;&#26174;&#33879;&#20559;&#31163;&#65292;&#21017;&#35270;&#20026;&#24694;&#24847;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;InferGuard&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03149v1 Announce Type: cross  Abstract: Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack.   In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client's model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five benchmark dat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#65292;&#36890;&#36807;&#21452;&#37325;&#22343;&#20540;&#25945;&#24072;&#32467;&#26500;&#36991;&#20813;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21644;&#20016;&#23500;&#26080;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.03145</link><description>&lt;p&gt;
&#21452;&#37325;&#22343;&#20540;&#25945;&#24072;&#65306;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#30340;&#26080;&#20559;&#21322;&#30417;&#30563;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03145
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#65292;&#36890;&#36807;&#21452;&#37325;&#22343;&#20540;&#25945;&#24072;&#32467;&#26500;&#36991;&#20813;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21644;&#20016;&#23500;&#26080;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#65288;AVSL&#65289;&#26088;&#22312;&#22312;&#35270;&#39057;&#24103;&#20013;&#23450;&#20301;&#22768;&#38899;&#23545;&#35937;&#65292;&#32473;&#23450;&#37197;&#23545;&#30340;&#38899;&#39057;&#21098;&#36753;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#38899;&#39057;-&#35270;&#35273;&#23545;&#24212;&#20851;&#31995;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#36793;&#30028;&#26694;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24456;&#38590;&#23454;&#29616;&#31934;&#30830;&#30340;&#23450;&#20301;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#29289;&#20307;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#36793;&#30028;&#27169;&#31946;&#21644;&#35823;&#25253;&#12290;&#27492;&#22806;&#65292;&#26420;&#32032;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#22312;&#20805;&#20998;&#21033;&#29992;&#20016;&#23500;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#20449;&#24687;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;AVSL&#30340;&#26032;&#39062;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#21452;&#37325;&#22343;&#20540;&#25945;&#24072;&#65288;DMT&#65289;&#65292;&#21253;&#25324;&#20004;&#20010;&#25945;&#24072;-&#23398;&#29983;&#32467;&#26500;&#65292;&#20197;&#35268;&#36991;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#20004;&#20010;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#34987;&#29992;&#26469;&#36890;&#36807;&#23427;&#20204;&#39044;&#27979;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#36807;&#28388;&#22024;&#26434;&#26679;&#26412;&#65292;&#28982;&#21518;&#36890;&#36807;&#20132;&#21449;&#23427;&#20204;&#30340;&#32622;&#20449;&#22270;&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03145v1 Announce Type: cross  Abstract: Audio-Visual Source Localization (AVSL) aims to locate sounding objects within video frames given the paired audio clips. Existing methods predominantly rely on self-supervised contrastive learning of audio-visual correspondence. Without any bounding-box annotations, they struggle to achieve precise localization, especially for small objects, and suffer from blurry boundaries and false positives. Moreover, the naive semi-supervised method is poor in fully leveraging the information of abundant unlabeled data. In this paper, we propose a novel semi-supervised learning framework for AVSL, namely Dual Mean-Teacher (DMT), comprising two teacher-student structures to circumvent the confirmation bias issue. Specifically, two teachers, pre-trained on limited labeled data, are employed to filter out noisy samples via the consensus between their predictions, and then generate high-quality pseudo-labels by intersecting their confidence maps. The
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#38598;&#25104;&#27169;&#22411;&#36890;&#36807;&#31616;&#21333;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21363;&#21487;&#23454;&#29616;&#22312;&#25152;&#26377;&#36755;&#20837;&#21644;&#35757;&#32451;&#26102;&#21051;&#37117;&#20445;&#25345;&#31561;&#21464;&#24615;&#65292;&#36825;&#31181;&#31561;&#21464;&#24615;&#22312;&#31163;&#24320;&#27969;&#24418;&#26102;&#21644;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#19979;&#30340;&#20219;&#20309;&#26550;&#26500;&#37117;&#33021;&#20445;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.03103</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#27169;&#22411;&#20013;&#30340;&#26032;&#22411;&#31561;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Emergent Equivariance in Deep Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03103
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#27169;&#22411;&#36890;&#36807;&#31616;&#21333;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21363;&#21487;&#23454;&#29616;&#22312;&#25152;&#26377;&#36755;&#20837;&#21644;&#35757;&#32451;&#26102;&#21051;&#37117;&#20445;&#25345;&#31561;&#21464;&#24615;&#65292;&#36825;&#31181;&#31561;&#21464;&#24615;&#22312;&#31163;&#24320;&#27969;&#24418;&#26102;&#21644;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#19979;&#30340;&#20219;&#20309;&#26550;&#26500;&#37117;&#33021;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#38598;&#25104;&#27169;&#22411;&#26159;&#26263;&#20013;&#31561;&#21464;&#30340;&#27169;&#22411;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#31616;&#21333;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#65292;&#28145;&#24230;&#38598;&#25104;&#27169;&#22411;&#22312;&#25152;&#26377;&#36755;&#20837;&#21644;&#25152;&#26377;&#35757;&#32451;&#26102;&#21051;&#37117;&#21464;&#24471;&#31561;&#21464;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#31561;&#21464;&#24615;&#22312;&#31163;&#24320;&#27969;&#24418;&#26102;&#21644;&#22312;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#19979;&#30340;&#20219;&#20309;&#26550;&#26500;&#37117;&#20445;&#25345;&#12290;&#36825;&#31181;&#31561;&#21464;&#24615;&#26159;&#26032;&#20852;&#30340;&#65292;&#22240;&#20026;&#21333;&#20010;&#38598;&#25104;&#25104;&#21592;&#30340;&#39044;&#27979;&#24182;&#38750;&#31561;&#21464;&#65292;&#20294;&#23427;&#20204;&#30340;&#38598;&#20307;&#39044;&#27979;&#26159;&#31561;&#21464;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#20999;&#32447;&#26680;&#29702;&#35770;&#25512;&#23548;&#20102;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#35814;&#32454;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03103v1 Announce Type: new  Abstract: We demonstrate that deep ensembles are secretly equivariant models. More precisely, we show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments.
&lt;/p&gt;</description></item><item><title>KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03101</link><description>&lt;p&gt;
KnowAgent: &#30693;&#35782;&#22686;&#24378;&#35268;&#21010;&#29992;&#20110;&#22522;&#20110;LLM&#30340;Agent
&lt;/p&gt;
&lt;p&gt;
KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03101
&lt;/p&gt;
&lt;p&gt;
KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#26102;&#20173;&#26377;&#25152;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#19982;&#29615;&#22659;&#20114;&#21160;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#21160;&#20316;&#26102;&#12290;&#36825;&#31181;&#19981;&#36275;&#20027;&#35201;&#26469;&#33258;&#20110;&#35821;&#35328;Agent&#20013;&#32570;&#20047;&#20869;&#32622;&#21160;&#20316;&#30693;&#35782;&#65292;&#23548;&#33268;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;&#35268;&#21010;&#36712;&#36857;&#65292;&#20174;&#32780;&#23548;&#33268;&#35268;&#21010;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KnowAgent&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#26469;&#22686;&#24378;LLM&#35268;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;KnowAgent&#37319;&#29992;&#20102;&#19968;&#20010;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#19968;&#20010;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#38480;&#21046;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#34892;&#21160;&#36335;&#24452;&#65292;&#23454;&#29616;&#26356;&#21512;&#29702;&#30340;&#36712;&#36857;&#21512;&#25104;&#65292;&#36827;&#32780;&#25552;&#39640;&#35821;&#35328;Agent&#30340;&#35745;&#21010;&#24615;&#33021;&#12290;&#22522;&#20110;HotpotQA&#21644;ALFWorld&#30340;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19981;&#21516;&#30340;&#20027;&#24178;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
&lt;/p&gt;</description></item><item><title>NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;</title><link>https://arxiv.org/abs/2403.03100</link><description>&lt;p&gt;
NaturalSpeech 3: &#21033;&#29992;&#20998;&#35299;&#32534;&#35299;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03100
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#22312;&#35821;&#38899;&#36136;&#37327;&#12289;&#30456;&#20284;&#24230;&#21644;&#38901;&#24459;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#37492;&#20110;&#35821;&#38899;&#22797;&#26434;&#22320;&#21253;&#21547;&#21508;&#31181;&#23646;&#24615;&#65288;&#20363;&#22914;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#65289;&#65292;&#32473;&#29983;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#23558;&#35821;&#38899;&#22240;&#23376;&#20998;&#35299;&#20026;&#20195;&#34920;&#19981;&#21516;&#23646;&#24615;&#30340;&#21508;&#20010;&#23376;&#31354;&#38388;&#65292;&#24182;&#21333;&#29420;&#29983;&#25104;&#23427;&#20204;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NaturalSpeech 3&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#20998;&#35299;&#21521;&#37327;&#37327;&#21270;&#65288;FVQ&#65289;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#20998;&#35299;&#20026;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#30340;&#23376;&#31354;&#38388;&#65307;2) &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#65292;&#26681;&#25454;&#20854;&#30456;&#24212;&#30340;&#25552;&#31034;&#29983;&#25104;&#27599;&#20010;&#23376;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#20998;&#35299;&#35774;&#35745;&#65292;NaturalSpeech 3&#33021;&#22815;ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
&lt;/p&gt;</description></item><item><title>VQSynergy&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#26426;&#21046;&#20197;&#21450;&#20854;&#20182;&#21019;&#26032;&#25216;&#26415;&#25552;&#39640;&#20102;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#39640;&#26031;&#22122;&#22768;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.03089</link><description>&lt;p&gt;
VQSynergy: &#21033;&#29992;&#30690;&#37327;&#37327;&#21270;&#26426;&#21046;&#36827;&#34892;&#31283;&#20581;&#30340;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03089
&lt;/p&gt;
&lt;p&gt;
VQSynergy&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#26426;&#21046;&#20197;&#21450;&#20854;&#20182;&#21019;&#26032;&#25216;&#26415;&#25552;&#39640;&#20102;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#39640;&#26031;&#22122;&#22768;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#39640;&#36890;&#37327;&#31579;&#36873;&#21644;&#35745;&#31639;&#21019;&#26032;&#30340;&#20986;&#29616;&#24341;&#39046;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#25506;&#32034;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#26041;&#27861;&#23398;&#30340;&#36716;&#21464;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;VQSynergy&#65292;&#36825;&#26159;&#19968;&#20010;&#37319;&#29992;&#20102;&#30690;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#26426;&#21046;&#12289;&#19982;&#38376;&#25511;&#27531;&#24046;&#21644;&#37327;&#36523;&#23450;&#21046;&#30340;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#26031;&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;VQSynergy&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#22797;&#26434;&#19988;&#24120;&#24120;&#22024;&#26434;&#30340;&#33647;&#29289;&#21327;&#21516;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03089v1 Announce Type: cross  Abstract: The pursuit of optimizing cancer therapies is significantly advanced by the accurate prediction of drug synergy. Traditional methods, such as clinical trials, are reliable yet encumbered by extensive time and financial demands. The emergence of high-throughput screening and computational innovations has heralded a shift towards more efficient methodologies for exploring drug interactions. In this study, we present VQSynergy, a novel framework that employs the Vector Quantization (VQ) mechanism, integrated with gated residuals and a tailored attention mechanism, to enhance the precision and generalizability of drug synergy predictions. Our findings demonstrate that VQSynergy surpasses existing models in terms of robustness, particularly under Gaussian noise conditions, highlighting its superior performance and utility in the complex and often noisy domain of drug synergy research. This study underscores the potential of VQSynergy in rev
&lt;/p&gt;</description></item><item><title>&#26694;&#26550;&#25552;&#20986;&#20102;&#21484;&#22238;&#23548;&#21521;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#20803;&#27169;&#22411;(GAMM)&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#26368;&#22823;&#21270;&#36807;&#21435;&#30693;&#35782;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03082</link><description>&lt;p&gt;
&#20855;&#26377;&#29983;&#25104;&#23545;&#25239;&#20803;&#27169;&#22411;&#30340;&#21484;&#22238;&#23548;&#21521;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Recall-Oriented Continual Learning with Generative Adversarial Meta-Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03082
&lt;/p&gt;
&lt;p&gt;
&#26694;&#26550;&#25552;&#20986;&#20102;&#21484;&#22238;&#23548;&#21521;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#20803;&#27169;&#22411;(GAMM)&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#26368;&#22823;&#21270;&#36807;&#21435;&#30693;&#35782;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#22256;&#22659;&#26159;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#20197;&#21069;&#20219;&#21153;&#24615;&#33021;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21484;&#22238;&#23548;&#21521;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#22823;&#33041;&#20998;&#31163;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26426;&#21046;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#20004;&#32423;&#20307;&#31995;&#32467;&#26500;&#65292;&#20854;&#20013;&#25512;&#29702;&#32593;&#32476;&#26377;&#25928;&#22320;&#33719;&#21462;&#26032;&#30693;&#35782;&#65292;&#32780;&#29983;&#25104;&#32593;&#32476;&#22312;&#38656;&#35201;&#26102;&#22238;&#39038;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#26368;&#22823;&#21270;&#36807;&#21435;&#30693;&#35782;&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#34920;&#31034;&#21462;&#20915;&#20110;&#30693;&#35782;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#22686;&#37327;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#32780;&#19981;&#26159;&#20219;&#21153;&#30340;&#36755;&#20837;&#25968;&#25454;&#26679;&#26412;&#30340;&#29983;&#25104;&#23545;&#25239;&#20803;&#27169;&#22411;&#65288;GAMM&#65289;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03082v1 Announce Type: cross  Abstract: The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks. In this paper, we propose the recall-oriented continual learning framework to address this challenge. Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary. In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task. Through our experiments, we show that our framework not only 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Brenier&#30340;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#31070;&#32463;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#28508;&#22312;&#20989;&#25968;$u$&#65292;&#20174;&#26368;&#26032;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#36827;&#23637;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;</title><link>https://arxiv.org/abs/2403.03071</link><description>&lt;p&gt;
&#35770;Brenier&#30340;&#26497;&#20998;&#35299;&#30340;&#31070;&#32463;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
On a Neural Implementation of Brenier's Polar Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03071
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Brenier&#30340;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#31070;&#32463;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#28508;&#22312;&#20989;&#25968;$u$&#65292;&#20174;&#26368;&#26032;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#36827;&#23637;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1991&#24180;&#65292;Brenier&#35777;&#26126;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#23558;$QR$&#20998;&#35299;&#65288;&#20998;&#20026;&#21322;&#27491;&#23450;&#30697;&#38453;$\times$&#37193;&#30697;&#38453;&#65289;&#25512;&#24191;&#21040;&#20219;&#24847;&#30690;&#37327;&#22330;$F:\mathbb{R}^d\rightarrow \mathbb{R}^d$&#12290;&#36825;&#20010;&#34987;&#31216;&#20026;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#20219;&#24847;&#22330;$F$&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#20984;&#20989;&#25968;$u$&#30340;&#26799;&#24230;&#19982;&#20445;&#27979;&#24230;&#26144;&#23556;$M$&#30340;&#22797;&#21512;&#65292;&#21363;$F=\nabla u \circ M$&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#20855;&#26377;&#28145;&#36828;&#29702;&#35770;&#24847;&#20041;&#30340;&#32467;&#26524;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#33021;&#30340;&#24212;&#29992;&#12290;&#35813;&#23450;&#29702;&#19982;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#29702;&#35770;&#23494;&#20999;&#30456;&#20851;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#28508;&#22312;&#20989;&#25968;$u$&#21442;&#25968;&#21270;&#20026;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#12290;&#26144;&#23556;$M$&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;$u^*$&#65292;&#21363;$u$&#30340;&#20984;&#20849;&#36717;&#65292;&#36880;&#28857;&#35745;&#31639;&#24471;&#21040;&#65292;&#21363;$M=\nabla u^* \circ F$&#65292;&#25110;&#32773;&#20316;&#20026;&#36741;&#21161;&#32593;&#32476;&#23398;&#20064;&#24471;&#21040;&#12290;&#22240;&#20026;$M$&#22312;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03071v1 Announce Type: cross  Abstract: In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for square matrices -- factored as PSD $\times$ unitary -- to any vector field $F:\mathbb{R}^d\rightarrow \mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\nabla u \circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\nabla u^* \circ F$, or learned as an auxiliary network. Because $M$ is, in gene
&lt;/p&gt;</description></item><item><title>&#32570;&#22833;&#25968;&#25454;&#22686;&#21152;&#20102;&#27169;&#22411;&#23545;&#28508;&#22312;&#21464;&#37327;&#21518;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#8212;&#8212;&#22522;&#20110;&#26377;&#38480;&#21464;&#20998;&#28151;&#21512;&#21644;&#22522;&#20110;&#22635;&#34917;&#30340;&#21464;&#20998;&#28151;&#21512;&#20998;&#24067;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20272;&#35745;VAE&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03069</link><description>&lt;p&gt;
&#29992;&#28151;&#21512;&#21464;&#20998;&#23478;&#26063;&#25913;&#36827;&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20272;&#35745;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03069
&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#22686;&#21152;&#20102;&#27169;&#22411;&#23545;&#28508;&#22312;&#21464;&#37327;&#21518;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#8212;&#8212;&#22522;&#20110;&#26377;&#38480;&#21464;&#20998;&#28151;&#21512;&#21644;&#22522;&#20110;&#22635;&#34917;&#30340;&#21464;&#20998;&#28151;&#21512;&#20998;&#24067;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20272;&#35745;VAE&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32570;&#22833;&#25968;&#25454;&#20250;&#22686;&#21152;&#27169;&#22411;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#65292;&#19982;&#23436;&#20840;&#35266;&#27979;&#30340;&#24773;&#20917;&#30456;&#27604;&#12290;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#21487;&#33021;&#20250;&#30001;&#20110;&#21464;&#20998;&#20998;&#24067;&#21644;&#27169;&#22411;&#21518;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#32780;&#23545;&#27169;&#22411;&#25311;&#21512;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#22522;&#20110;&#65288;i&#65289;&#26377;&#38480;&#21464;&#20998;&#28151;&#21512;&#21644;&#65288;ii&#65289;&#22522;&#20110;&#22635;&#34917;&#30340;&#21464;&#20998;&#28151;&#21512;&#20998;&#24067;&#30340;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#22686;&#21152;&#30340;&#21518;&#39564;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23545;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#34920;&#26126;&#21464;&#20998;&#28151;&#21512;&#22312;&#25913;&#36827;&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20272;&#35745;VAE&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03069v1 Announce Type: new  Abstract: We consider the task of estimating variational autoencoders (VAEs) when the training data is incomplete. We show that missing data increases the complexity of the model's posterior distribution over the latent variables compared to the fully-observed case. The increased complexity may adversely affect the fit of the model due to a mismatch between the variational and model posterior distributions. We introduce two strategies based on (i) finite variational-mixture and (ii) imputation-based variational-mixture distributions to address the increased posterior complexity. Through a comprehensive evaluation of the proposed approaches, we show that variational mixtures are effective at improving the accuracy of VAE estimation from incomplete data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#20108;&#27425;&#32593;&#32476;&#31995;&#32479;&#20013;&#25910;&#25947;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#30340;&#21487;&#25193;&#23637;&#20998;&#24067;&#24335;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#38543;&#30528;&#36890;&#20449;&#21644;&#25511;&#21046;&#33539;&#22260;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#24046;&#36317;&#20250;&#25351;&#25968;&#32423;&#20943;&#23567;&#20026;&#38646;&#65292;&#24182;&#23637;&#31034;&#20102;&#22686;&#21152;&#36890;&#20449;&#33539;&#22260;&#22914;&#20309;&#22686;&#24378;&#31995;&#32479;&#31283;&#23450;&#24615;&#21644;&#25581;&#31034;&#20851;&#38190;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.03055</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#38480;&#36890;&#20449;&#33539;&#22260;&#30340;&#32447;&#24615;&#20108;&#27425;&#32593;&#32476;&#25511;&#21046;&#30340;&#20998;&#24067;&#24335;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03055
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#20108;&#27425;&#32593;&#32476;&#31995;&#32479;&#20013;&#25910;&#25947;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#30340;&#21487;&#25193;&#23637;&#20998;&#24067;&#24335;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#38543;&#30528;&#36890;&#20449;&#21644;&#25511;&#21046;&#33539;&#22260;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#24046;&#36317;&#20250;&#25351;&#25968;&#32423;&#20943;&#23567;&#20026;&#38646;&#65292;&#24182;&#23637;&#31034;&#20102;&#22686;&#21152;&#36890;&#20449;&#33539;&#22260;&#22914;&#20309;&#22686;&#24378;&#31995;&#32479;&#31283;&#23450;&#24615;&#21644;&#25581;&#31034;&#20851;&#38190;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#20108;&#27425;&#32593;&#32476;&#31995;&#32479;&#20013;&#25910;&#25947;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;&#26234;&#33021;&#20307;&#22312;&#29305;&#23450;&#32593;&#32476;&#20013;&#36827;&#34892;&#20132;&#20114;&#65292;&#21463;&#38480;&#20110;&#26412;&#22320;&#36890;&#20449;&#32422;&#26463;&#65292;&#24847;&#21619;&#30528;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#19982;&#26377;&#38480;&#25968;&#37327;&#30340;&#30456;&#37051;&#26234;&#33021;&#20307;&#20132;&#25442;&#20449;&#24687;&#12290;&#22312;&#32593;&#32476;&#30340;&#24213;&#23618;&#22270;&#20013;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#22312;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#35774;&#32622;&#20013;&#26681;&#25454;&#20854;&#38468;&#36817;&#37051;&#23621;&#30340;&#29366;&#24577;&#23454;&#26045;&#20854;&#25511;&#21046;&#36755;&#20837;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21482;&#20351;&#29992;&#23616;&#37096;&#20449;&#24687;&#21363;&#21487;&#36817;&#20284;&#31934;&#30830;&#26799;&#24230;&#26159;&#21487;&#33021;&#30340;&#12290;&#19982;&#38598;&#20013;&#24335;&#26368;&#20248;&#25511;&#21046;&#22120;&#30456;&#27604;&#65292;&#38543;&#30528;&#36890;&#20449;&#21644;&#25511;&#21046;&#33539;&#22260;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#24046;&#36317;&#25351;&#25968;&#32423;&#20943;&#23567;&#20026;&#38646;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22686;&#21152;&#36890;&#20449;&#33539;&#22260;&#21487;&#20197;&#22686;&#24378;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#31995;&#32479;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#26435;&#34913;&#12290;&#27169;&#25311;&#32467;&#26524;&#39564;&#35777;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03055v1 Announce Type: cross  Abstract: This paper proposes a scalable distributed policy gradient method and proves its convergence to near-optimal solution in multi-agent linear quadratic networked systems. The agents engage within a specified network under local communication constraints, implying that each agent can only exchange information with a limited number of neighboring agents. On the underlying graph of the network, each agent implements its control input depending on its nearby neighbors' states in the linear quadratic control setting. We show that it is possible to approximate the exact gradient only using local information. Compared with the centralized optimal controller, the performance gap decreases to zero exponentially as the communication and control ranges increase. We also demonstrate how increasing the communication range enhances system stability in the gradient descent process, thereby elucidating a critical trade-off. The simulation results verify
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;EgoPack&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#32467;&#21512;&#20849;&#20139;&#26102;&#38388;&#24314;&#27169;&#21644;&#26368;&#23567;&#24320;&#38144;&#65292;&#25903;&#25345;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#25216;&#33021;&#26102;&#36827;&#34892;&#21512;&#20316;&#65292;&#20026;&#26234;&#33021;&#26426;&#22120;&#25552;&#20379;&#20840;&#38754;&#30340;&#35270;&#39057;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03037</link><description>&lt;p&gt;
&#19968;&#20010;&#20805;&#28385;&#25216;&#33021;&#30340;&#32972;&#21253;&#65306;&#20855;&#26377;&#22810;&#20803;&#20219;&#21153;&#35270;&#35282;&#30340;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;EgoPack&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#32467;&#21512;&#20849;&#20139;&#26102;&#38388;&#24314;&#27169;&#21644;&#26368;&#23567;&#24320;&#38144;&#65292;&#25903;&#25345;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#25216;&#33021;&#26102;&#36827;&#34892;&#21512;&#20316;&#65292;&#20026;&#26234;&#33021;&#26426;&#22120;&#25552;&#20379;&#20840;&#38754;&#30340;&#35270;&#39057;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.03037v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#20154;&#31867;&#23545;&#35270;&#39057;&#27969;&#30340;&#29702;&#35299;&#33258;&#28982;&#32780;&#28982;&#26159;&#24191;&#27867;&#30340;&#65306;&#22312;&#30701;&#30701;&#30340;&#30636;&#38388;&#20869;&#65292;&#25105;&#20204;&#33021;&#22815;&#29702;&#35299;&#21457;&#29983;&#20102;&#20160;&#20040;&#65292;&#23545;&#35937;&#30340;&#30456;&#20851;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#39044;&#27979;&#25509;&#19979;&#26469;&#30340;&#23558;&#26469;&#65292;&#25152;&#26377;&#36825;&#20123;&#19968;&#27425;&#24615;&#23436;&#25104;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35201;&#26377;&#25928;&#22320;&#23558;&#36825;&#26679;&#30340;&#25972;&#20307;&#24863;&#30693;&#36716;&#31227;&#21040;&#26234;&#33021;&#26426;&#22120;&#20013;&#65292;&#23398;&#20064;&#30456;&#20851;&#27010;&#24565;&#21644;&#25552;&#28860;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#30340;&#30693;&#35782;&#20043;&#38388;&#30340;&#20851;&#31995;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#65292;&#20197;&#20415;&#22312;&#23398;&#20064;&#26032;&#25216;&#33021;&#26102;&#21327;&#21516;&#21033;&#29992;&#23427;&#20204;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#34892;&#20026;&#30340;&#20849;&#20139;&#26102;&#38388;&#24314;&#27169;&#19982;&#26368;&#23567;&#24320;&#38144;&#30456;&#32467;&#21512;&#65292;&#20197;&#25903;&#25345;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#24182;&#22312;&#23398;&#20064;&#26032;&#25216;&#33021;&#26102;&#36827;&#34892;&#21512;&#20316;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EgoPack&#65292;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#21019;&#24314;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21487;&#20197;&#25658;&#24102;&#30340;&#20219;&#21153;&#35270;&#35282;&#38598;&#21512;&#65292;&#24182;&#21487;&#29992;&#20316;&#39069;&#22806;&#35265;&#35299;&#30340;&#28508;&#22312;&#26469;&#28304;&#65292;&#23601;&#20687;&#19968;&#20010;&#32972;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03037v1 Announce Type: cross  Abstract: Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.03020</link><description>&lt;p&gt;
SplAgger&#65306;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#21106;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
SplAgger: Split Aggregation for Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#26680;&#24515;&#30446;&#26631;&#26159;&#21019;&#24314;&#33021;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#36825;&#20123;&#26234;&#33021;&#20307;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19968;&#31867;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34987;&#31216;&#20026;&#40657;&#30418;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#29616;&#25104;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19982;&#20043;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#21478;&#19968;&#31867;&#26041;&#27861;&#65292;&#23427;&#20204;&#26126;&#30830;&#22320;&#25512;&#26029;&#20986;&#26410;&#30693;&#20219;&#21153;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;&#24207;&#21015;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#20219;&#21153;&#25512;&#26029;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#35777;&#26126;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03020v1 Announce Type: cross  Abstract: A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end. In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task. These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods. However, recent evidence suggests that task inference objectives are unnecessary in practice. Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;sgRNA&#35774;&#35745;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.03018</link><description>&lt;p&gt;
CRISPR&#65306;&#38598;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CRISPR: Ensemble Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03018
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;sgRNA&#35774;&#35745;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#22522;&#22240;&#32534;&#36753;&#25216;&#26415;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;CRISPR&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#39044;&#27979;&#21333;&#23548;RNA (sgRNAs)&#30340;&#38774;&#21521;&#25928;&#21147;&#21644;&#38750;&#38774;&#21521;&#25935;&#24863;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;sgRNA&#35774;&#35745;&#65292;&#35813;&#26041;&#27861;&#26082;&#20934;&#30830;&#21448;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20197;&#29983;&#25104;&#21333;&#19968;&#19988;&#26356;&#20581;&#22766;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22522;&#20934;sgRNA&#35774;&#35745;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03018v1 Announce Type: new  Abstract: Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) is a gene editing technology that has revolutionized the fields of biology and medicine. However, one of the challenges of using CRISPR is predicting the on-target efficacy and off-target sensitivity of single-guide RNAs (sgRNAs). This is because most existing methods are trained on separate datasets with different genes and cells, which limits their generalizability. In this paper, we propose a novel ensemble learning method for sgRNA design that is accurate and generalizable. Our method combines the predictions of multiple machine learning models to produce a single, more robust prediction. This approach allows us to learn from a wider range of data, which improves the generalizability of our model. We evaluated our method on a benchmark dataset of sgRNA designs and found that it outperformed existing methods in terms of both accuracy and generalizability. Our results s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#38598;&#25104;&#26641;&#26469;&#32531;&#35299;&#24694;&#24847;URL&#26816;&#27979;&#22120;&#20013;&#30340;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#65292;&#20174;&#32780;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#38598;&#25104;&#38450;&#24481;&#26426;&#21046;&#20197;&#38450;&#33539;&#28508;&#22312;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.02995</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#26641;&#22312;&#24694;&#24847;URL&#26816;&#27979;&#22120;&#20013;&#32531;&#35299;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#38598;&#25104;&#26641;&#26469;&#32531;&#35299;&#24694;&#24847;URL&#26816;&#27979;&#22120;&#20013;&#30340;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#65292;&#20174;&#32780;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#38598;&#25104;&#38450;&#24481;&#26426;&#21046;&#20197;&#38450;&#33539;&#28508;&#22312;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;URL&#25552;&#20379;&#20102;&#36328;&#21508;&#34892;&#19994;&#65288;&#21253;&#25324;&#20132;&#36890;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#33021;&#28304;&#21644;&#38134;&#34892;&#19994;&#65289;&#30340;&#23545;&#25239;&#24615;&#26426;&#20250;&#65292;&#21487;&#33021;&#23545;&#19994;&#21153;&#36816;&#33829;&#36896;&#25104;&#37325;&#22823;&#25439;&#23475;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#36825;&#20123;URL&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#65307;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#28041;&#21450;&#25805;&#32437;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#26631;&#31614;&#65292;&#22914;&#26631;&#31614;&#32763;&#36716;&#65288;LF&#65289;&#65292;&#23558;&#33391;&#24615;&#26631;&#31614;&#26356;&#25913;&#20026;&#24694;&#24847;&#26631;&#31614;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#36825;&#31181;&#25805;&#32437;&#23548;&#33268;&#35823;&#20998;&#31867;&#65292;&#24182;&#23548;&#33268;&#27169;&#22411;&#34892;&#20026;&#19981;&#27491;&#30830;&#12290;&#22240;&#27492;&#65292;&#22312;ML&#27169;&#22411;&#26550;&#26500;&#20013;&#38598;&#25104;&#38450;&#24481;&#26426;&#21046;&#25104;&#20026;&#21152;&#22266;&#28508;&#22312;&#25915;&#20987;&#30340;&#24517;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#22312;&#20351;&#29992;&#38598;&#25104;&#26641;&#36827;&#34892;URL&#26816;&#27979;&#32972;&#26223;&#19979;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#38416;&#26126;&#27492;&#31867;&#25915;&#20987;&#32972;&#21518;&#30340;&#21160;&#26426;&#65292;&#31361;&#20986;&#25915;&#20987;&#32773;&#30340;&#35282;&#33394;&#65292;&#24182;&#24378;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02995v1 Announce Type: cross  Abstract: Malicious URLs provide adversarial opportunities across various industries, including transportation, healthcare, energy, and banking which could be detrimental to business operations. Consequently, the detection of these URLs is of crucial importance; however, current Machine Learning (ML) models are susceptible to backdoor attacks. These attacks involve manipulating a small percentage of training data labels, such as Label Flipping (LF), which changes benign labels to malicious ones and vice versa. This manipulation results in misclassification and leads to incorrect model behavior. Therefore, integrating defense mechanisms into the architecture of ML models becomes an imperative consideration to fortify against potential attacks.   The focus of this study is on backdoor attacks in the context of URL detection using ensemble trees. By illuminating the motivations behind such attacks, highlighting the roles of attackers, and emphasizi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35745;&#31639;&#26426;&#32593;&#32476;&#39046;&#22495;&#20013;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#20005;&#37325;&#24615;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#26631;&#31614;&#32763;&#36716;&#21644;&#29305;&#24449;&#20013;&#27602;&#65292;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02983</link><description>&lt;p&gt;
&#21463;&#25915;&#20987;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#36890;&#36807;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#25581;&#31034;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Under Attack: Exposing Vulnerabilities through Data Poisoning Attacks in Computer Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35745;&#31639;&#26426;&#32593;&#32476;&#39046;&#22495;&#20013;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#20005;&#37325;&#24615;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#26631;&#31614;&#32763;&#36716;&#21644;&#29305;&#24449;&#20013;&#27602;&#65292;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02983v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;(ML)&#26041;&#27861;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#35774;&#22791;&#25110;&#36793;&#32536;&#26381;&#21153;&#22120;&#21327;&#20316;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20132;&#25442;&#21407;&#22987;&#25968;&#25454;&#12290;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#35757;&#32451;&#21644;&#27169;&#22411;&#26356;&#26032;&#20849;&#20139;&#36807;&#31243;&#20013;&#65292;&#25968;&#25454;&#21644;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19981;&#21516;&#30340;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#35745;&#31639;&#26426;&#32593;&#32476;&#39046;&#22495;&#20013;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#20005;&#37325;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#23481;&#26131;&#23454;&#26045;&#20294;&#24456;&#38590;&#26816;&#27979;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#65292;&#21363;&#26631;&#31614;&#32763;&#36716;(LF)&#21644;&#29305;&#24449;&#20013;&#27602;(FP)&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#22312;LF&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#32763;&#36716;&#20102;&#33391;&#24615;&#25968;&#25454;&#30340;&#26631;&#31614;&#65292;&#24182;&#22312;&#25805;&#32437;&#21518;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#23545;&#20110;FP&#65292;&#25105;&#20204;&#38543;&#26426;&#25805;&#32437;&#20102;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30830;&#23450;&#30340;&#39640;&#36129;&#29486;&#29305;&#24449;&#12290;&#35813;&#23454;&#39564;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#20026;&#19982;&#35745;&#31639;&#26426;&#32593;&#32476;&#30456;&#20851;&#30340;CIC&#21644;UNSW&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02983v1 Announce Type: cross  Abstract: Federated Learning (FL) is a machine learning (ML) approach that enables multiple decentralized devices or edge servers to collaboratively train a shared model without exchanging raw data. During the training and sharing of model updates between clients and servers, data and models are susceptible to different data-poisoning attacks.   In this study, our motivation is to explore the severity of data poisoning attacks in the computer network domain because they are easy to implement but difficult to detect. We considered two types of data-poisoning attacks, label flipping (LF) and feature poisoning (FP), and applied them with a novel approach. In LF, we randomly flipped the labels of benign data and trained the model on the manipulated data. For FP, we randomly manipulated the highly contributing features determined using the Random Forest algorithm. The datasets used in this experiment were CIC and UNSW related to computer networks. We
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20154;&#31867;&#32422;&#26463;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20849;&#20139;&#33258;&#27835;&#26041;&#24335;&#19979;&#30340;&#23454;&#26102;&#21327;&#20316;&#65292;&#30446;&#30340;&#26159;&#25903;&#25345;&#20154;&#31867;&#25805;&#20316;&#32773;&#25191;&#34892;&#20849;&#20139;&#20219;&#21153;&#24182;&#26368;&#23567;&#21270;&#20854;&#19981;&#36866;&#24863;&#12290;</title><link>https://arxiv.org/abs/2403.02974</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#39304;&#22312;&#32447;&#23398;&#20064;&#20154;&#31867;&#32422;&#26463;&#30340;&#20849;&#20139;&#33258;&#27835;
&lt;/p&gt;
&lt;p&gt;
Online Learning of Human Constraints from Feedback in Shared Autonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20154;&#31867;&#32422;&#26463;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20849;&#20139;&#33258;&#27835;&#26041;&#24335;&#19979;&#30340;&#23454;&#26102;&#21327;&#20316;&#65292;&#30446;&#30340;&#26159;&#25903;&#25345;&#20154;&#31867;&#25805;&#20316;&#32773;&#25191;&#34892;&#20849;&#20139;&#20219;&#21153;&#24182;&#26368;&#23567;&#21270;&#20854;&#19981;&#36866;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20154;&#31867;&#23454;&#26102;&#21327;&#20316;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#24335;&#26159;&#21463;&#21040;&#19981;&#21516;&#29289;&#29702;&#32422;&#26463;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20851;&#27880;&#20110;&#23398;&#20064;&#21327;&#20316;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#25110;&#32773;&#22914;&#20309;&#22312;&#21442;&#19982;&#30340;&#20195;&#29702;&#20043;&#38388;&#20998;&#37197;&#21644;&#20998;&#21457;&#23376;&#20219;&#21153;&#26469;&#25191;&#34892;&#20027;&#35201;&#20219;&#21153;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#20154;&#31867;&#32422;&#26463;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#19981;&#21516;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#22810;&#26679;&#21270;&#34892;&#20026;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#22312;&#20849;&#20139;&#33258;&#27835;&#26041;&#24335;&#19979;&#30340;&#21327;&#20316;&#31867;&#22411;&#65292;&#20854;&#20013;&#20154;&#31867;&#25805;&#20316;&#32773;&#21644;&#36741;&#21161;&#26426;&#22120;&#20154;&#21516;&#26102;&#22312;&#21516;&#19968;&#20219;&#21153;&#31354;&#38388;&#20869;&#34892;&#21160;&#65292;&#24444;&#27492;&#30340;&#34892;&#21160;&#20250;&#30456;&#20114;&#24433;&#21709;&#12290;&#36741;&#21161;&#20195;&#29702;&#30340;&#20219;&#21153;&#26159;&#36890;&#36807;&#23613;&#21487;&#33021;&#25903;&#25345;&#20154;&#31867;&#65292;&#26082;&#38477;&#20302;&#24037;&#20316;&#37327;&#20063;&#26368;&#23567;&#21270;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#19981;&#36866;&#24863;&#65292;&#26469;&#22686;&#24378;&#20154;&#31867;&#25191;&#34892;&#20849;&#20139;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#22411;&#21161;&#25163;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02974v1 Announce Type: cross  Abstract: Real-time collaboration with humans poses challenges due to the different behavior patterns of humans resulting from diverse physical constraints. Existing works typically focus on learning safety constraints for collaboration, or how to divide and distribute the subtasks between the participating agents to carry out the main task. In contrast, we propose to learn a human constraints model that, in addition, considers the diverse behaviors of different human operators. We consider a type of collaboration in a shared-autonomy fashion, where both a human operator and an assistive robot act simultaneously in the same task space that affects each other's actions. The task of the assistive agent is to augment the skill of humans to perform a shared task by supporting humans as much as possible, both in terms of reducing the workload and minimizing the discomfort for the human operator. Therefore, we propose an augmentative assistant agent c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Hamiltonian&#30340;&#26412;&#22320;&#24615;&#27979;&#35797;&#20316;&#20026;&#23646;&#24615;&#27979;&#35797;&#38382;&#39064;&#65292;&#37325;&#28857;&#22312;&#20110;&#30830;&#23450;&#26410;&#30693;&#30340;$n$&#27604;&#29305;Hamiltonian&#26159;&#21542;&#26159;$k$&#23616;&#37096;&#30340;&#65292;&#36890;&#36807;&#23545;$H$&#30340;&#26102;&#38388;&#28436;&#21270;&#36827;&#34892;&#35775;&#38382;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02968</link><description>&lt;p&gt;
Hamiltonian&#24615;&#36136;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Hamiltonian Property Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Hamiltonian&#30340;&#26412;&#22320;&#24615;&#27979;&#35797;&#20316;&#20026;&#23646;&#24615;&#27979;&#35797;&#38382;&#39064;&#65292;&#37325;&#28857;&#22312;&#20110;&#30830;&#23450;&#26410;&#30693;&#30340;$n$&#27604;&#29305;Hamiltonian&#26159;&#21542;&#26159;$k$&#23616;&#37096;&#30340;&#65292;&#36890;&#36807;&#23545;$H$&#30340;&#26102;&#38388;&#28436;&#21270;&#36827;&#34892;&#35775;&#38382;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Hamiltonian&#26412;&#22320;&#24615;&#27979;&#35797;&#20316;&#20026;&#19968;&#20010;&#23646;&#24615;&#27979;&#35797;&#38382;&#39064;&#65292;&#21363;&#30830;&#23450;&#19968;&#20010;&#26410;&#30693;&#30340;$n$&#27604;&#29305;Hamiltonian $H$&#26159;&#21542;&#26159;$k$&#23616;&#37096;&#30340;&#65292;&#25110;&#32773;&#19982;&#25152;&#26377;$k$&#23616;&#37096;Hamiltonian&#37117;&#30456;&#36317;$\varepsilon$&#65292;&#24182;&#36890;&#36807;&#23545;$H$&#30340;&#26102;&#38388;&#28436;&#21270;&#36827;&#34892;&#35775;&#38382;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02968v1 Announce Type: cross  Abstract: Locality is a fundamental feature of many physical time evolutions. Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution. However, no protocols to rigorously test whether an unknown Hamiltonian is local were known. We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\tilde{\Omega}(2^n)$ many time evolution queries and an expected total evolution time of $\tilde{\Omega}(2^n / \varepsilon)$, and even coherent testers need $\Omega(2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#35770;&#25209;&#37327;&#22823;&#23567;&#22914;&#20309;&#12290;</title><link>https://arxiv.org/abs/2403.02967</link><description>&lt;p&gt;
&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38750;&#20984;&#38543;&#26426;&#22797;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Convex Stochastic Composite Optimization with Polyak Momentum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#35770;&#25209;&#37327;&#22823;&#23567;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#30340;&#19968;&#20010;&#24378;&#22823;&#27867;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#24403;&#38543;&#26426;&#22122;&#22768;&#26174;&#33879;&#26102;&#65288;&#21363;&#20165;&#20351;&#29992;&#23567;&#22411;&#25110;&#26377;&#30028;&#25209;&#37327;&#22823;&#23567;&#26102;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#20984;&#29615;&#22659;&#20013;&#26080;&#27861;&#25910;&#25947;&#12290;&#26412;&#25991;&#20851;&#27880;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#25209;&#37327;&#22823;&#23567;&#22823;&#23567;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;Polyak&#21160;&#37327;&#22312;&#22797;&#21512;&#20248;&#21270;&#29615;&#22659;&#20013;&#30340;&#26041;&#24046;&#20943;&#23569;&#25928;&#24212;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#36817;&#31471;&#27493;&#39588;&#21482;&#33021;&#36890;&#36807;&#36817;&#20284;&#35299;&#26469;&#27714;&#35299;&#26102;&#65292;&#35813;&#26041;&#27861;&#20063;&#20250;&#25910;&#25947;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02967v1 Announce Type: cross  Abstract: The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02966</link><description>&lt;p&gt;
&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#29992;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;-shot&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#24615;&#33021;&#65292;&#28982;&#32780;&#32467;&#26500;&#21270;&#30340;KG&#24418;&#24335;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#19977;&#20803;&#32452;&#24418;&#24335;&#25110;&#19977;&#20803;&#32452;&#20107;&#23454;&#30340;&#33258;&#30001;&#25991;&#26412;&#36716;&#25442;&#65292;&#36935;&#21040;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#30001;&#20110;&#37325;&#22797;&#23454;&#20307;&#25110;&#20851;&#31995;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#23494;&#24230;&#38477;&#20302;&#65292;&#20197;&#21450;&#30001;&#20110;&#26080;&#27861;&#24378;&#35843;&#20851;&#38190;&#35777;&#25454;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#28165;&#26224;&#24230;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EFSum&#65292;&#19968;&#20010;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;LLMs&#22686;&#24378;QA&#12290;&#25105;&#20204;&#36890;&#36807;&#33976;&#39311;&#21644;&#20559;&#22909;&#23545;&#40784;&#26469;&#20248;&#21270;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#20316;&#20026;&#20107;&#23454;&#25688;&#35201;&#22120;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;EFSum&#25552;&#39640;&#20102;LLM&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#30830;&#20445;&#25688;&#35201;&#30340;&#21516;&#26102;&#26377;&#30410;&#21644;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#19968;&#20010;&#29305;&#23450;&#30340;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#31361;&#20986;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#65292;&#21516;&#26102;&#20855;&#26377;&#24378;&#22823;&#29983;&#25104;&#22120;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.02957</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#28176;&#36817;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#19968;&#20010;&#29305;&#23450;&#30340;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#31361;&#20986;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#65292;&#21516;&#26102;&#20855;&#26377;&#24378;&#22823;&#29983;&#25104;&#22120;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#21435;&#22122;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#29702;&#35299;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#29305;&#23450;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65288;CME&#65289;&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;&#30740;&#31350;&#30340;&#22522;&#20110;DPM&#30340;&#21435;&#22122;&#22120;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19982;DPMs&#20849;&#20139;&#65292;&#20294;&#22312;&#35757;&#32451;&#21518;&#30340;&#36870;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#20256;&#36882;&#26465;&#20214;&#22343;&#20540;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#30340;&#29420;&#29305;&#35270;&#35282;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#36870;&#36807;&#31243;&#20013;&#20999;&#25442;&#37325;&#26032;&#37319;&#26679;&#30340;&#26041;&#24335;&#32487;&#25215;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#22120;&#12290;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02957v1 Announce Type: new  Abstract: Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks. Despite their practical utility, there is a notable gap in their theoretical understanding. This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps. The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training. We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off. The theoretical findings are validated by numerical results.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;Systolic Array&#30340;DNN&#21152;&#36895;&#22120;&#30340;&#26032;&#22411;&#20998;&#23618;&#36719;&#20214;&#21270;&#30828;&#20214;&#24863;&#30693;&#25925;&#38556;&#27880;&#20837;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#30340;&#26102;&#38388;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02946</link><description>&lt;p&gt;
SAFFIRA: &#19968;&#31181;&#35780;&#20272;&#22522;&#20110;Systolic Array&#30340;DNN&#21152;&#36895;&#22120;&#21487;&#38752;&#24615;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SAFFIRA: a Framework for Assessing the Reliability of Systolic-Array-Based DNN Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02946
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;Systolic Array&#30340;DNN&#21152;&#36895;&#22120;&#30340;&#26032;&#22411;&#20998;&#23618;&#36719;&#20214;&#21270;&#30828;&#20214;&#24863;&#30693;&#25925;&#38556;&#27880;&#20837;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#30340;&#26102;&#38388;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Systolic array&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#26174;&#30528;&#26550;&#26500;&#65292;&#25552;&#20379;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#24615;&#33021;&#65292;&#23545;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37096;&#32626;DNN&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20351;&#29992;&#26102;&#65292;&#24517;&#39035;&#36827;&#34892;&#21487;&#38752;&#24615;&#35780;&#20272;&#20197;&#30830;&#20445;DNN&#21152;&#36895;&#22120;&#30340;&#27491;&#30830;&#34892;&#20026;&#12290;&#34429;&#28982;&#25925;&#38556;&#27880;&#20837;&#20316;&#20026;&#19968;&#31181;&#25104;&#29087;&#23454;&#29992;&#19988;&#31283;&#20581;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#20294;&#20173;&#28982;&#26159;&#19968;&#20010;&#38750;&#24120;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;systolic array&#30340;DNN&#21152;&#36895;&#22120;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#20998;&#23618;&#36719;&#20214;&#21270;&#30828;&#20214;&#24863;&#30693;&#25925;&#38556;&#27880;&#20837;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02946v1 Announce Type: new  Abstract: Systolic array has emerged as a prominent architecture for Deep Neural Network (DNN) hardware accelerators, providing high-throughput and low-latency performance essential for deploying DNNs across diverse applications. However, when used in safety-critical applications, reliability assessment is mandatory to guarantee the correct behavior of DNN accelerators. While fault injection stands out as a well-established practical and robust method for reliability assessment, it is still a very time-consuming process. This paper addresses the time efficiency issue by introducing a novel hierarchical software-based hardware-aware fault injection strategy tailored for systolic array-based DNN accelerators.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#25968;&#25454;&#38598;&#39564;&#35777;&#29616;&#26377;&#30740;&#31350;&#32467;&#26524;&#26159;&#21542;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#20197;&#27979;&#35797;&#26159;&#21542;&#23384;&#22312;&#20849;&#21516;&#30340;ICU&#24739;&#32773;&#20122;&#32452;&#12290;</title><link>https://arxiv.org/abs/2403.02945</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;ICU&#24739;&#32773;&#20122;&#32452;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65306;&#32467;&#26524;&#26159;&#21542;&#20855;&#26377;&#26222;&#36866;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning Approaches for Identifying ICU Patient Subgroups: Do Results Generalise?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02945
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#25968;&#25454;&#38598;&#39564;&#35777;&#29616;&#26377;&#30740;&#31350;&#32467;&#26524;&#26159;&#21542;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#20197;&#27979;&#35797;&#26159;&#21542;&#23384;&#22312;&#20849;&#21516;&#30340;ICU&#24739;&#32773;&#20122;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#35782;&#21035;&#24739;&#32773;&#20122;&#32452;&#24050;&#32463;&#34987;&#25552;&#20986;&#20316;&#20026;&#25913;&#21892;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#25928;&#29575;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#36890;&#36807;&#35782;&#21035;&#20855;&#26377;&#31867;&#20284;&#21307;&#30103;&#36164;&#28304;&#38656;&#27714;&#27700;&#24179;&#30340;&#24739;&#32773;&#20122;&#32452;&#65292;ICUs&#21487;&#20197;&#37325;&#32452;&#20026;&#19968;&#31995;&#21015;&#36739;&#23567;&#30340;&#20122;&#21333;&#20803;&#65292;&#27599;&#20010;&#21333;&#20803;&#23545;&#24212;&#19968;&#20010;&#29305;&#23450;&#32452;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#26159;&#21542;&#23384;&#22312;&#36328;&#19981;&#21516;ICUs&#20849;&#21516;&#30340;&#24739;&#32773;&#20122;&#32452;&#65292;&#36825;&#23558;&#20915;&#23450;ICU&#37325;&#32452;&#26159;&#21542;&#21487;&#20197;&#20197;&#26631;&#20934;&#21270;&#26041;&#24335;&#23454;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#36890;&#36807;&#26816;&#26597;&#19968;&#20010;&#29616;&#26377;&#30740;&#31350;&#30340;&#32467;&#26524;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#26469;&#39564;&#35777;&#26159;&#21542;&#23384;&#22312;&#20849;&#21516;&#30340;ICU&#24739;&#32773;&#20122;&#32452;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#20195;&#34920;&#21307;&#30103;&#36164;&#28304;&#38656;&#27714;&#30340;16&#20010;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20849;&#35782;&#32858;&#31867;&#26469;&#24471;&#20986;&#24739;&#32773;&#20122;&#32452;&#65292;&#22797;&#21046;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#20808;&#21069;&#30740;&#31350;&#30340;&#32467;&#26524;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02945v1 Announce Type: new  Abstract: The use of unsupervised learning to identify patient subgroups has emerged as a potentially promising direction to improve the efficiency of Intensive Care Units (ICUs). By identifying subgroups of patients with similar levels of medical resource need, ICUs could be restructured into a collection of smaller subunits, each catering to a specific group. However, it is unclear whether common patient subgroups exist across different ICUs, which would determine whether ICU restructuring could be operationalised in a standardised manner. In this paper, we tested the hypothesis that common ICU patient subgroups exist by examining whether the results from one existing study generalise to a different dataset. We extracted 16 features representing medical resource need and used consensus clustering to derive patient subgroups, replicating the previous study. We found limited similarities between our results and those of the previous study, providi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#24863;&#30693;&#21644;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#33258;&#36866;&#24212;&#32534;&#30721;&#21644;&#32852;&#21512;&#22270;&#20687;-&#25991;&#26412;&#25439;&#22833;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02944</link><description>&lt;p&gt;
&#29992;&#25991;&#26412;&#24341;&#23548;&#32534;&#30721;&#30340;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#25216;&#26415;&#23454;&#29616;&#20687;&#32032;&#32423;&#21644;&#24863;&#30693;&#20934;&#30830;&#24230;&#30340;&#21452;&#37325;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#24863;&#30693;&#21644;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#33258;&#36866;&#24212;&#32534;&#30721;&#21644;&#32852;&#21512;&#22270;&#20687;-&#25991;&#26412;&#25439;&#22833;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20102;&#25552;&#39640;&#37325;&#24314;&#22270;&#20687;&#24863;&#30693;&#36136;&#37327;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#26126;&#26174;&#38477;&#20302;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#24863;&#30693;&#21644;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#26694;&#26550;&#65292;&#20027;&#35201;&#36890;&#36807;&#25991;&#26412;&#33258;&#36866;&#24212;&#32534;&#30721;&#21644;&#32852;&#21512;&#22270;&#20687;-&#25991;&#26412;&#25439;&#22833;&#35757;&#32451;&#26469;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;&#36825;&#26679;&#19968;&#26469;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#39640;&#29983;&#25104;&#22810;&#26679;&#24615;&#32780;&#38395;&#21517;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#20687;&#32032;&#32423;&#21644;&#24863;&#30693;&#36136;&#37327;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#39064;&#36824;&#26159;&#26426;&#22120;&#29983;&#25104;&#30340;&#26631;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02944v1 Announce Type: cross  Abstract: Recent advances in text-guided image compression have shown great potential to enhance the perceptual quality of reconstructed images. These methods, however, tend to have significantly degraded pixel-wise fidelity, limiting their practicality. To fill this gap, we develop a new text-guided image compression algorithm that achieves both high perceptual and pixel-wise fidelity. In particular, we propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, we avoid decoding based on text-guided generative models -- known for high generative diversity -- and effectively utilize the semantic information of text at a global level. Experimental results on various datasets show that our method can achieve high pixel-level and perceptual quality, with either human- or machine-generated captions. In particular, our method outperforms all baselines in terms
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#26159;&#21542;&#21487;&#20197;&#21548;&#21040;&#32463;&#36807;&#20248;&#21270;&#30340;&#35821;&#38899;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21487;&#26681;&#25454;&#38899;&#32032;&#20026;&#21333;&#20301;&#33258;&#21160;&#35843;&#25972;&#25773;&#25918;&#36895;&#24230;&#65292;&#20197;&#30830;&#20445;&#35821;&#38899;&#21487;&#36776;&#35782;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.02938</link><description>&lt;p&gt;
AIx Speed&#65306;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#21548;&#21147;&#29702;&#35299;&#20248;&#21270;&#22238;&#25918;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#26159;&#21542;&#21487;&#20197;&#21548;&#21040;&#32463;&#36807;&#20248;&#21270;&#30340;&#35821;&#38899;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21487;&#26681;&#25454;&#38899;&#32032;&#20026;&#21333;&#20301;&#33258;&#21160;&#35843;&#25972;&#25773;&#25918;&#36895;&#24230;&#65292;&#20197;&#30830;&#20445;&#35821;&#38899;&#21487;&#36776;&#35782;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#31867;&#21487;&#20197;&#20197;&#27604;&#23454;&#38469;&#35266;&#23519;&#21040;&#30340;&#36895;&#24230;&#26356;&#24555;&#22320;&#20542;&#21548;&#38899;&#39057;&#21644;&#35266;&#30475;&#35270;&#39057;&#65292;&#22240;&#27492;&#25105;&#20204;&#32463;&#24120;&#20197;&#26356;&#39640;&#30340;&#25773;&#25918;&#36895;&#24230;&#20542;&#21548;&#25110;&#35266;&#30475;&#36825;&#20123;&#20869;&#23481;&#30340;&#29255;&#27573;&#65292;&#20197;&#25552;&#39640;&#20869;&#23481;&#29702;&#35299;&#30340;&#26102;&#38388;&#25928;&#29575;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#26681;&#25454;&#29992;&#25143;&#24773;&#20917;&#21644;&#20869;&#23481;&#31867;&#22411;&#33258;&#21160;&#35843;&#25972;&#25773;&#25918;&#36895;&#24230;&#30340;&#31995;&#32479;&#65292;&#20197;&#21327;&#21161;&#26356;&#39640;&#25928;&#22320;&#29702;&#35299;&#26102;&#38388;&#24207;&#21015;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20173;&#26377;&#36827;&#19968;&#27493;&#25552;&#39640;&#20154;&#31867;&#36895;&#21548;&#33021;&#21147;&#30340;&#31354;&#38388;&#65292;&#21363;&#29983;&#25104;&#24050;&#32463;&#38024;&#23545;&#26356;&#31934;&#32454;&#30340;&#26102;&#38388;&#21333;&#20301;&#20248;&#21270;&#36807;&#30340;&#35821;&#38899;&#65292;&#24182;&#23558;&#20854;&#25552;&#20379;&#32473;&#20154;&#31867;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20154;&#31867;&#33021;&#21542;&#21548;&#21040;&#20248;&#21270;&#36807;&#30340;&#35821;&#38899;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#26681;&#25454;&#38899;&#32032;&#20026;&#21333;&#20301;&#33258;&#21160;&#35843;&#25972;&#25773;&#25918;&#36895;&#24230;&#65292;&#21516;&#26102;&#30830;&#20445;&#35821;&#38899;&#21487;&#36776;&#35782;&#24230;&#12290;&#31995;&#32479;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#24471;&#20998;&#20316;&#20026;&#34913;&#37327;&#20154;&#31867;&#33021;&#21542;&#21548;&#21040;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02938v1 Announce Type: new  Abstract: Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension. To further utilize this capability, systems that automatically adjust the playback speed according to the user's condition and the type of content to assist in more efficient comprehension of time-series content have been developed. However, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans. In this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility. The system uses the speech recognizer score as a proxy for how well a human can hear a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;ASIC-based DNN&#21152;&#36895;&#22120;&#30340;&#33258;&#36866;&#24212;&#23481;&#38169;&#36817;&#20284;&#20056;&#27861;&#22120;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.02936</link><description>&lt;p&gt;
AdAM: &#36866;&#29992;&#20110;&#36793;&#32536;DNN&#21152;&#36895;&#22120;&#30340;&#33258;&#36866;&#24212;&#23481;&#38169;&#36817;&#20284;&#20056;&#27861;&#22120;
&lt;/p&gt;
&lt;p&gt;
AdAM: Adaptive Fault-Tolerant Approximate Multiplier for Edge DNN Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02936
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;ASIC-based DNN&#21152;&#36895;&#22120;&#30340;&#33258;&#36866;&#24212;&#23481;&#38169;&#36817;&#20284;&#20056;&#27861;&#22120;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#22522;&#20110;ASIC&#30340;DNN&#21152;&#36895;&#22120;&#23450;&#21046;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#23481;&#38169;&#36817;&#20284;&#20056;&#27861;&#22120;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02936v1 Announce Type: new  Abstract: In this paper, we propose an architecture of a novel adaptive fault-tolerant approximate multiplier tailored for ASIC-based DNN accelerators.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22797;&#21046;&#30740;&#31350;BASS&#26694;&#26550;&#65292;&#21457;&#29616;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2403.02930</link><description>&lt;p&gt;
BASS&#30340;&#20877;&#23457;&#35270;--&#21033;&#29992;&#32479;&#19968;&#35821;&#20041;&#22270;&#25552;&#21319;&#25277;&#35937;&#25688;&#35201;--&#19968;&#39033;&#22797;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02930
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22797;&#21046;&#30740;&#31350;BASS&#26694;&#26550;&#65292;&#21457;&#29616;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;BASS&#26694;&#26550;&#30340;&#35814;&#32454;&#22797;&#21046;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#32479;&#19968;&#35821;&#20041;&#22270;&#27010;&#24565;&#30340;&#25277;&#35937;&#25688;&#35201;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21253;&#25324;&#22797;&#21046;&#20851;&#38190;&#32452;&#20214;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#19968;&#20010;&#28040;&#34701;&#30740;&#31350;&#26469;&#31995;&#32479;&#22320;&#38548;&#31163;&#22312;&#22797;&#21046;&#26032;&#39062;&#32452;&#20214;&#26102;&#26681;&#28304;&#20110;&#38169;&#35823;&#26469;&#28304;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#21363;&#20351;&#26159;&#34987;&#21512;&#29702;&#30465;&#30053;&#30340;&#32454;&#33410;&#23545;&#20110;&#22797;&#21046;&#20687;BASS&#36825;&#26679;&#30340;&#20808;&#36827;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02930v1 Announce Type: new  Abstract: We present a detailed replication study of the BASS framework, an abstractive summarization system based on the notion of Unified Semantic Graphs. Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components. Our findings reveal discrepancies in performance compared to the original work. We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23558;&#36752;&#23556;&#20256;&#36755;&#27169;&#22411;&#38598;&#25104;&#21040;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#30340;&#31471;&#23545;&#31471;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#32416;&#27491;RTMs&#20013;&#30340;&#20559;&#20506;&#65292;&#32780;&#19988;&#22312;&#21464;&#37327;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.02922</link><description>&lt;p&gt;
&#20174;&#20809;&#35889;&#21040;&#29983;&#29289;&#29289;&#29702;&#23398;&#27934;&#23519;&#65306;&#24102;&#26377;&#20559;&#20506;&#36752;&#23556;&#20256;&#36755;&#27169;&#22411;&#30340;&#31471;&#23545;&#31471;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Spectra to Biophysical Insights: End-to-End Learning with a Biased Radiative Transfer Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02922
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23558;&#36752;&#23556;&#20256;&#36755;&#27169;&#22411;&#38598;&#25104;&#21040;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#30340;&#31471;&#23545;&#31471;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#32416;&#27491;RTMs&#20013;&#30340;&#20559;&#20506;&#65292;&#32780;&#19988;&#22312;&#21464;&#37327;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#25512;&#21160;&#20102;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#22312;&#27668;&#20505;&#21464;&#21270;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#34920;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#29702;&#35299;&#26862;&#26519;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#29983;&#29289;&#29289;&#29702;&#21453;&#24212;&#26041;&#38754;&#12290;&#36965;&#24863;&#20013;&#20256;&#32479;&#30340;&#36870;&#36752;&#23556;&#20256;&#36755;&#27169;&#22411;&#65288;RTMs&#65289;&#26469;&#20174;&#20809;&#35889;&#25968;&#25454;&#20013;&#24674;&#22797;&#29983;&#29289;&#29289;&#29702;&#21464;&#37327;&#30340;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#32771;&#34385;RTM&#26412;&#36523;&#22266;&#26377;&#30340;&#20559;&#20506;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22797;&#26434;&#30340;&#26862;&#26519;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;RTMs&#38598;&#25104;&#21040;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#65292;&#21019;&#24314;&#19968;&#20010;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#32416;&#27491;&#20102;RTMs&#20013;&#30340;&#20559;&#20506;&#65292;&#32780;&#19988;&#22312;&#21464;&#37327;&#24674;&#22797;&#26041;&#38754;&#20248;&#20110;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#31561;&#20256;&#32479;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#36870;&#36716;&#20559;&#20506;&#30340;&#29289;&#29702;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#19968;&#33324;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/yihshe/ai-refined-rtm.git&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02922v1 Announce Type: new  Abstract: Advances in machine learning have boosted the use of Earth observation data for climate change research. Yet, the interpretability of machine-learned representations remains a challenge, particularly in understanding forests' biophysical reactions to climate change. Traditional methods in remote sensing that invert radiative transfer models (RTMs) to retrieve biophysical variables from spectral data often fail to account for biases inherent in the RTM, especially for complex forests. We propose to integrate RTMs into an auto-encoder architecture, creating an end-to-end learning approach. Our method not only corrects biases in RTMs but also outperforms traditional techniques for variable retrieval like neural network regression. Furthermore, our framework has potential generally for inverting biased physical models. The code is available on https://github.com/yihshe/ai-refined-rtm.git.
&lt;/p&gt;</description></item><item><title>TaylorShift&#36890;&#36807;&#24341;&#20837;TaylorSoftmax&#37325;&#26032;&#35745;&#31639;&#20840;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#30001;&#24179;&#26041;&#32423;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02920</link><description>&lt;p&gt;
TaylorShift&#65306;&#21033;&#29992;TaylorSoftmax&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#20174;&#24179;&#26041;&#32423;&#36716;&#21464;&#20026;&#32447;&#24615;&#32423;&#65288;&#20877;&#36716;&#22238;&#21435;&#65289;
&lt;/p&gt;
&lt;p&gt;
TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02920
&lt;/p&gt;
&lt;p&gt;
TaylorShift&#36890;&#36807;&#24341;&#20837;TaylorSoftmax&#37325;&#26032;&#35745;&#31639;&#20840;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#30001;&#24179;&#26041;&#32423;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#26159;&#20351;&#29992;Transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#38754;&#20020;&#30340;&#26368;&#22823;&#38556;&#30861;&#20043;&#19968;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31232;&#30095;&#34920;&#31034;&#25110;&#26377;&#29366;&#24577;&#30340;&#24490;&#29615;&#65292;&#29306;&#29298;&#20102;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#26368;&#32456;&#23548;&#33268;&#24615;&#33021;&#19978;&#30340;&#22949;&#21327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TaylorShift&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;Taylor softmax &#37325;&#26500;&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#21644;&#31354;&#38388;&#20869;&#35745;&#31639;&#20840;&#20307;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#30830;&#23450;&#20102;&#20351;&#29992;TaylorShift&#27604;&#20256;&#32479;&#27880;&#24847;&#21147;&#26356;&#21152;&#39640;&#25928;&#30340;&#20132;&#21449;&#28857;&#65292;&#36825;&#19982;&#23454;&#35777;&#27979;&#37327;&#32467;&#26524;&#23494;&#20999;&#21305;&#37197;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;TaylorShift&#25552;&#39640;&#20102;&#23545;&#30701;&#33267;800&#20010;&#35760;&#21495;&#30340;&#24207;&#21015;&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#24182;&#21152;&#36895;&#20102;&#23545;&#38271;&#36798;&#32422;1700&#20010;&#35760;&#21495;&#21450;&#20197;&#19978;&#36755;&#20837;&#30340;&#25512;&#26029;&#12290;&#23545;&#20110;&#36739;&#30701;&#30340;&#24207;&#21015;&#65292;TaylorShift&#19982;&#21407;&#22987;&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#20998;&#31867;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02920v1 Announce Type: cross  Abstract: The quadratic complexity of the attention mechanism represents one of the biggest hurdles for processing long sequences using Transformers. Current methods, relying on sparse representations or stateful recurrence, sacrifice token-to-token interactions, which ultimately leads to compromises in performance. This paper introduces TaylorShift, a novel reformulation of the Taylor softmax that enables computing full token-to-token interactions in linear time and space. We analytically determine the crossover points where employing TaylorShift becomes more efficient than traditional attention, aligning closely with empirical measurements. Specifically, our findings demonstrate that TaylorShift enhances memory efficiency for sequences as short as 800 tokens and accelerates inference for inputs of approximately 1700 tokens and beyond. For shorter sequences, TaylorShift scales comparably with the vanilla attention. Furthermore, a classification
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#38797;&#28857;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#36895;&#29575;&#20043;&#21069;&#21482;&#38024;&#23545;&#21452;&#32447;&#24615;&#30446;&#26631;&#24050;&#30693;&#12290;</title><link>https://arxiv.org/abs/2403.02912</link><description>&lt;p&gt;
&#20855;&#26377;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#25910;&#25947;&#36895;&#29575;&#30340;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#38797;&#28857;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02912
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#38797;&#28857;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#36895;&#29575;&#20043;&#21069;&#21482;&#38024;&#23545;&#21452;&#32447;&#24615;&#30446;&#26631;&#24050;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#38754;&#20307;&#35774;&#32622;&#20013;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#38543;&#26426;&#65288;&#20984;&#20985;&#65289;&#38797;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#30340;&#65288;&#1013;&#65292;&#948;&#65289;-DP&#31639;&#27861;&#65292;&#20854;&#23454;&#29616;&#20102;&#39044;&#26399;&#23545;&#20598;&#38388;&#38553;&#30340;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#20445;&#35777;&#22312;&#20197;&#21069;&#21482;&#38024;&#23545;&#21452;&#32447;&#24615;&#30446;&#26631;&#24050;&#30693;&#12290;&#23545;&#20110;&#20984;&#20985;&#21644;&#19968;&#38454;&#24179;&#28369;&#38543;&#26426;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#29575;&#65292;&#21363;sqrt(log(d)/n) + (log(d)^{3/2}/[n&#1013;])^{1/3}&#65292;&#20854;&#20013;d&#26159;&#38382;&#39064;&#30340;&#32500;&#24230;&#65292;n&#26159;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#22312;&#39069;&#22806;&#30340;&#20108;&#38454;&#24179;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23558;&#39044;&#26399;&#38388;&#38553;&#30340;&#36895;&#29575;&#25913;&#36827;&#20026;sqrt(log(d)/n) + (log(d)^{3/2}/[n&#1013;])^{2/5}&#12290;&#22312;&#36825;&#31181;&#39069;&#22806;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#20559;&#24046;&#20943;&#23569;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#35777;&#26126;&#20102;&#23545;&#20598;&#38388;&#38553;&#21463;&#24120;&#25968;&#25104;&#21151;&#27010;&#29575;&#30340;&#30028;&#20026;log(d)/sqrt(n) + log(d)/[n&#1013;]^{1/2}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02912v1 Announce Type: cross  Abstract: We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the polyhedral setting. We propose $(\varepsilon, \delta)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives. For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size. Under an additional second-order-smoothness assumption, we improve the rate on the expected gap to $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{2/5}$. Under this additional assumption, we also show, by using bias-reduced gradient estimators, that the duality gap is bounded by $\log(d)/\sqrt{n} + \log(d)/[n\varepsilon]^{1/2}$ with constant success pro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#20844;&#27665;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#21152;&#36895;&#27431;&#20122;&#29470;&#29441;&#31561;&#28626;&#21361;&#29289;&#31181;&#30417;&#27979;&#25968;&#25454;&#30340;&#20934;&#22791;&#12289;&#26631;&#35760;&#21644;&#20998;&#26512;&#36807;&#31243;&#65292;&#24212;&#29992;&#22312;&#33258;&#28982;&#30740;&#31350;&#21644;&#20445;&#25252;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.02906</link><description>&lt;p&gt;
&#20844;&#27665;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#30740;&#31350;&#21644;&#33258;&#28982;&#20445;&#25252;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;&#27431;&#20122;&#29470;&#29441;&#12289;&#33258;&#30001;&#25918;&#20859;&#30340;&#21870;&#40831;&#21160;&#29289;&#21644;&#26118;&#34411;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02906
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#20844;&#27665;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#21152;&#36895;&#27431;&#20122;&#29470;&#29441;&#31561;&#28626;&#21361;&#29289;&#31181;&#30417;&#27979;&#25968;&#25454;&#30340;&#20934;&#22791;&#12289;&#26631;&#35760;&#21644;&#20998;&#26512;&#36807;&#31243;&#65292;&#24212;&#29992;&#22312;&#33258;&#28982;&#30740;&#31350;&#21644;&#20445;&#25252;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#30340;&#33258;&#28982;&#20445;&#25252;&#21306;&#21644;&#22269;&#23478;&#20844;&#22253;&#20013;&#65292;&#25216;&#26415;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#25903;&#25345;&#20445;&#25252;&#24037;&#20316;&#12290;&#28626;&#21361;&#29289;&#31181;&#65292;&#22914;&#27431;&#20122;&#29470;&#29441;&#65292;&#36890;&#36807;&#19968;&#32452;&#33258;&#21160;&#29031;&#30456;&#38519;&#38449;&#36827;&#34892;&#30417;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#38656;&#35201;&#20934;&#22791;&#12289;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#20174;&#20107;&#36825;&#19968;&#39046;&#22495;&#30740;&#31350;&#30340;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#38656;&#35201;&#25903;&#25345;&#26469;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#12290;&#19968;&#31181;&#26426;&#20250;&#26159;&#23547;&#27714;&#24535;&#24895;&#20844;&#27665;&#31185;&#23398;&#23478;&#30340;&#25903;&#25345;&#65292;&#20182;&#20204;&#21487;&#20197;&#24110;&#21161;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#24456;&#38590;&#20445;&#25345;&#20182;&#20204;&#30340;&#20852;&#36259;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#20687;&#35782;&#21035;&#33258;&#21160;&#21270;&#22788;&#29702;&#12290;&#22312;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#19982;&#33258;&#28982;&#30740;&#31350;&#21644;&#20445;&#25252;&#30456;&#20851;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#20197;&#21450;&#21033;&#29992;&#20844;&#27665;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#21152;&#24555;&#25968;&#25454;&#20934;&#22791;&#12289;&#26631;&#35760;&#21644;&#20998;&#26512;&#36807;&#31243;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02906v1 Announce Type: cross  Abstract: Technology is increasingly used in Nature Reserves and National Parks around the world to support conservation efforts. Endangered species, such as the Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps. Yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted. Therefore, researchers working in this area increasingly need support to process this incoming information. One opportunity is to seek support from volunteer Citizen Scientists who can help label the data, however, it is challenging to retain their interest. Another way is to automate the process with image recognition using convolutional neural networks. During the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of Citizen Science and Machine Learning to expedite the process of data preparation, labelling and analysis.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02889</link><description>&lt;p&gt;
&#22312;&#23547;&#25214;&#30495;&#30456;&#65306;&#19968;&#31181;&#23457;&#38382;&#26041;&#27861;&#29992;&#20110;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
In Search of Truth: An Interrogation Approach to Hallucination Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02889
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#24182;&#19988;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#23427;&#20204;&#23545;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#21644;&#25972;&#21512;&#20173;&#28982;&#26377;&#38480;&#12290;&#19968;&#20010;&#38459;&#30861;&#23427;&#20204;&#24191;&#27867;&#24212;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#24187;&#35273;&#30340;&#21457;&#29983;&#65292;&#21363;LLMs&#21019;&#36896;&#20986;&#21548;&#36215;&#26469;&#30495;&#23454;&#20294;&#20559;&#31163;&#20107;&#23454;&#30495;&#30456;&#30340;&#31572;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#65292;&#36825;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;Llama-2&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26368;&#26032;LLMs&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#26816;&#27979;&#23427;&#20204;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#29305;&#23450;&#23454;&#39564;&#20013;&#35266;&#23519;&#21040;Llama-2&#36798;&#21040;62%&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;87%&#30340;&#24179;&#34913;&#20934;&#30830;&#29575;&#65288;B-ACC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02889v1 Announce Type: new  Abstract: Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying 
&lt;/p&gt;</description></item><item><title>&#22823;&#22810;&#25968;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#38169;&#35823;&#20998;&#31867;&#38169;&#35823;&#26159;&#26377;&#23475;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23547;&#25214;&#24179;&#22374;&#30340;&#26368;&#23567;&#20540;&#26469;&#25193;&#22823;&#32622;&#20449;&#24230;&#38388;&#38548;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#22833;&#36133;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.02886</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#32622;&#20449;&#24230;&#20272;&#35745;&#65306;&#26397;&#21521;&#21487;&#38752;&#30340;&#25925;&#38556;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Revisiting Confidence Estimation: Towards Reliable Failure Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02886
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#38169;&#35823;&#20998;&#31867;&#38169;&#35823;&#26159;&#26377;&#23475;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23547;&#25214;&#24179;&#22374;&#30340;&#26368;&#23567;&#20540;&#26469;&#25193;&#22823;&#32622;&#20449;&#24230;&#38388;&#38548;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#22833;&#36133;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#22312;&#35768;&#22810;&#39118;&#38505;&#25935;&#24863;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#22522;&#26412;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#23545;&#20110;&#23427;&#20204;&#30340;&#38169;&#35823;&#39044;&#27979;&#36807;&#20110;&#33258;&#20449;&#65292;&#21363;&#65292;&#26469;&#33258;&#24050;&#30693;&#31867;&#21035;&#30340;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#21644;&#26469;&#33258;&#26410;&#30693;&#31867;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26679;&#26412;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#32622;&#20449;&#24230;&#26657;&#20934;&#21644;OOD&#26816;&#27979;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#20294;&#23454;&#38469;&#19978;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#65292;&#21363;&#22823;&#22810;&#25968;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#38169;&#35823;&#20998;&#31867;&#38169;&#35823;&#26159;&#26377;&#23475;&#30340;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25581;&#31034;&#20102;&#27969;&#34892;&#30340;&#26657;&#20934;&#21644;OOD&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23548;&#33268;&#26356;&#31967;&#31957;&#30340;&#32622;&#20449;&#24230;&#20998;&#31163;&#27491;&#30830;&#20998;&#31867;&#21644;&#38169;&#35823;&#20998;&#31867;&#30340;&#20107;&#20363;&#65292;&#20174;&#32780;&#20351;&#24471;&#38590;&#20197;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#19968;&#20010;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23547;&#25214;&#24179;&#22374;&#30340;&#26368;&#23567;&#20540;&#26469;&#25193;&#22823;&#32622;&#20449;&#24230;&#38388;&#38548;&#65292;&#20174;&#32780;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#22833;&#36133;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02886v1 Announce Type: cross  Abstract: Reliable confidence estimation is a challenging yet fundamental requirement in many risk-sensitive applications. However, modern deep neural networks are often overconfident for their incorrect predictions, i.e., misclassified samples from known classes, and out-of-distribution (OOD) samples from unknown classes. In recent years, many confidence calibration and OOD detection methods have been developed. In this paper, we find a general, widely existing but actually-neglected phenomenon that most confidence estimation methods are harmful for detecting misclassification errors. We investigate this problem and reveal that popular calibration and OOD detection methods often lead to worse confidence separation between correctly classified and misclassified examples, making it difficult to decide whether to trust a prediction or not. Finally, we propose to enlarge the confidence gap by finding flat minima, which yields state-of-the-art failu
&lt;/p&gt;</description></item><item><title>MathScale&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#22312;&#25968;&#23398;&#25968;&#25454;&#38598;&#22823;&#23567;&#26041;&#38754;&#30340;&#26377;&#25928;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02884</link><description>&lt;p&gt;
MathScale: &#25968;&#23398;&#25512;&#29702;&#30340;&#25351;&#23548;&#20248;&#21270;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
MathScale: Scaling Instruction Tuning for Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02884
&lt;/p&gt;
&lt;p&gt;
MathScale&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#22312;&#25968;&#23398;&#25968;&#25454;&#38598;&#22823;&#23567;&#26041;&#38754;&#30340;&#26377;&#25928;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#20173;&#28982;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MathScale&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21069;&#27839;&#30340;LLMs&#65288;&#20363;&#22914;GPT-3.5&#65289;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#12290;&#21463;&#20154;&#31867;&#25968;&#23398;&#23398;&#20064;&#20013;&#30340;&#35748;&#30693;&#26426;&#21046;&#21551;&#21457;&#65292;&#23427;&#39318;&#20808;&#20174;&#31181;&#23376;&#25968;&#23398;&#38382;&#39064;&#20013;&#25552;&#21462;&#20027;&#39064;&#21644;&#30693;&#35782;&#28857;&#65292;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#27010;&#24565;&#22270;&#65292;&#38543;&#21518;&#29992;&#20110;&#29983;&#25104;&#26032;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;MathScale&#22312;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#23398;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26377;&#25928;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#30334;&#19975;&#25968;&#23398;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;MathScaleQA&#65289;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MwpBench&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#23398;&#38382;&#39064;&#35789;&#27719;&#38382;&#39064;&#22522;&#20934;&#65292;&#21253;&#25324;&#21313;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02884v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;SUMO&#20013;&#38543;&#26426;&#21270;&#35268;&#21017;&#24494;&#35266;&#20132;&#36890;&#27969;&#30340;&#34892;&#20026;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#25552;&#39640;&#20854;&#22312;&#26356;&#30495;&#23454;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02882</link><description>&lt;p&gt;
&#36890;&#36807;&#20197;&#20132;&#36890;&#27969;&#38543;&#26426;&#21270;&#26041;&#24335;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02882
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;SUMO&#20013;&#38543;&#26426;&#21270;&#35268;&#21017;&#24494;&#35266;&#20132;&#36890;&#27969;&#30340;&#34892;&#20026;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#25552;&#39640;&#20854;&#22312;&#26356;&#30495;&#23454;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22823;&#22810;&#25968;&#20851;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#21644;&#25511;&#21046;&#20219;&#21153;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#26159;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#12290;&#36825;&#20123;&#30740;&#31350;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#26159;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#24494;&#35266;&#20132;&#36890;&#27969;&#19979;&#36827;&#34892;&#30340;&#65292;&#24456;&#23569;&#32771;&#34385;&#23558;&#23427;&#20204;&#36801;&#31227;&#21040;&#30495;&#23454;&#25110;&#25509;&#36817;&#30495;&#23454;&#30340;&#29615;&#22659;&#20013;&#20197;&#27979;&#35797;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;SUMO&#20013;&#22522;&#20110;&#35268;&#21017;&#30340;&#24494;&#35266;&#20132;&#36890;&#27969;&#30340;&#36710;&#36742;&#36319;&#39536;&#27169;&#22411;&#21644;&#25442;&#36947;&#27169;&#22411;&#30340;&#26576;&#20123;&#21442;&#25968;&#26469;&#38543;&#26426;&#21270;&#21608;&#22260;&#36710;&#36742;&#30340;&#39550;&#39542;&#39118;&#26684;&#21644;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20844;&#36335;&#21644;&#21512;&#24182;&#22330;&#26223;&#20013;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#35268;&#21017;&#24494;&#35266;&#20132;&#36890;&#27969;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#20998;&#21035;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#24494;&#35266;&#20132;&#36890;&#27969;&#21644;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02882v1 Announce Type: cross  Abstract: Most of the current studies on autonomous vehicle decision-making and control tasks based on reinforcement learning are conducted in simulated environments. The training and testing of these studies are carried out under rule-based microscopic traffic flow, with little consideration of migrating them to real or near-real environments to test their performance. It may lead to a degradation in performance when the trained model is tested in more realistic traffic scenes. In this study, we propose a method to randomize the driving style and behavior of surrounding vehicles by randomizing certain parameters of the car-following model and the lane-changing model of rule-based microscopic traffic flow in SUMO. We trained policies with deep reinforcement learning algorithms under the domain randomized rule-based microscopic traffic flow in freeway and merging scenes, and then tested them separately in rule-based microscopic traffic flow and h
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#31616;&#21270;&#20998;&#26512;&#20381;&#36182;&#36731;&#23614;&#38543;&#26426;&#28304;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#36739;&#31616;&#21333;&#30340;&#31639;&#27861;&#21464;&#20307;&#36827;&#34892;&#20998;&#26512;&#65292;&#36991;&#20813;&#20351;&#29992;&#19987;&#38376;&#30340;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#25351;&#25968;&#12289;&#20122;&#39640;&#26031;&#21644;&#26356;&#19968;&#33324;&#30340;&#24555;&#36895;&#34928;&#20943;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.02873</link><description>&lt;p&gt;
&#20851;&#20110;&#20855;&#26377;&#25351;&#25968;&#12289;&#20122;&#39640;&#26031;&#21644;&#19968;&#33324;&#36731;&#23614;&#30340;&#39640;&#27010;&#29575;&#20998;&#26512;&#31639;&#27861;&#30340;&#27880;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02873
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#31616;&#21270;&#20998;&#26512;&#20381;&#36182;&#36731;&#23614;&#38543;&#26426;&#28304;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#36739;&#31616;&#21333;&#30340;&#31639;&#27861;&#21464;&#20307;&#36827;&#34892;&#20998;&#26512;&#65292;&#36991;&#20813;&#20351;&#29992;&#19987;&#38376;&#30340;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#25351;&#25968;&#12289;&#20122;&#39640;&#26031;&#21644;&#26356;&#19968;&#33324;&#30340;&#24555;&#36895;&#34928;&#20943;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31616;&#30701;&#30340;&#27880;&#35299;&#25551;&#36848;&#20102;&#19968;&#31181;&#20998;&#26512;&#27010;&#29575;&#31639;&#27861;&#30340;&#31616;&#21333;&#25216;&#26415;&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#36731;&#23614;&#65288;&#20294;&#19981;&#19968;&#23450;&#26377;&#30028;&#65289;&#30340;&#38543;&#26426;&#21270;&#26469;&#28304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#19968;&#20010;&#31639;&#27861;&#30340;&#20998;&#26512;&#21487;&#20197;&#36890;&#36807;&#40657;&#30418;&#26041;&#24335;&#20943;&#23569;&#65292;&#21482;&#22312;&#23545;&#25968;&#22240;&#23376;&#20013;&#26377;&#23567;&#37327;&#25439;&#22833;&#65292;&#36716;&#21270;&#20026;&#20998;&#26512;&#21516;&#19968;&#31639;&#27861;&#30340;&#19968;&#20010;&#26356;&#31616;&#21333;&#21464;&#20307;&#65292;&#35813;&#21464;&#20307;&#20351;&#29992;&#26377;&#30028;&#38543;&#26426;&#21464;&#37327;&#65292;&#36890;&#24120;&#26356;&#23481;&#26131;&#20998;&#26512;&#12290;&#36825;&#31181;&#26041;&#27861;&#21516;&#26102;&#36866;&#29992;&#20110;&#20219;&#20309;&#36731;&#23614;&#38543;&#26426;&#21270;&#65292;&#21253;&#25324;&#25351;&#25968;&#12289;&#20122;&#39640;&#26031;&#21644;&#26356;&#19968;&#33324;&#30340;&#24555;&#36895;&#34928;&#20943;&#20998;&#24067;&#65292;&#32780;&#19981;&#38656;&#35201;&#35843;&#29992;&#19987;&#38376;&#30340;&#38598;&#20013;&#19981;&#31561;&#24335;&#12290;&#25552;&#20379;&#20102;&#23545;&#19968;&#33324;&#21270;Azuma&#19981;&#31561;&#24335;&#21644;&#20855;&#26377;&#19968;&#33324;&#36731;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#20248;&#21270;&#30340;&#20998;&#26512;&#65292;&#20197;&#35828;&#26126;&#35813;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02873v1 Announce Type: new  Abstract: This short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization. We show that the analysis of such an algorithm can be reduced, in a black-box manner and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze. This approach simultaneously applies to any light-tailed randomization, including exponential, sub-Gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities. Analyses of a generalized Azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#28151;&#21512;&#24577;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QMSAN&#65289;&#65292;&#32467;&#21512;&#20102;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#22788;&#29702;NLP&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02871</link><description>&lt;p&gt;
&#37327;&#23376;&#28151;&#21512;&#24577;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quantum Mixed-State Self-Attention Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#28151;&#21512;&#24577;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QMSAN&#65289;&#65292;&#32467;&#21512;&#20102;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#22788;&#29702;NLP&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#21457;&#23637;&#36234;&#26469;&#36234;&#31361;&#20986;&#20102;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#29420;&#29305;&#33021;&#21147;&#20026;&#22797;&#26434;&#25968;&#25454;&#22788;&#29702;&#21644;&#27169;&#24335;&#35782;&#21035;&#25361;&#25112;&#25552;&#20379;&#26032;&#39062;&#30340;&#35270;&#35282;&#21644;&#26041;&#27861;&#35770;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#28151;&#21512;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QMSAN&#65289;&#65292;&#23427;&#23558;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#19982;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22788;&#29702;NLP&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;QMSAN&#27169;&#22411;&#37319;&#29992;&#22522;&#20110;&#28151;&#21512;&#24577;&#30340;&#37327;&#23376;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#39046;&#22495;&#20869;&#26597;&#35810;&#21644;&#38190;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#39640;&#25928;&#30452;&#25509;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376; posit
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02871v1 Announce Type: cross  Abstract: The rapid advancement of quantum computing has increasingly highlighted its potential in the realm of machine learning, particularly in the context of natural language processing (NLP) tasks. Quantum machine learning (QML) leverages the unique capabilities of quantum computing to offer novel perspectives and methodologies for complex data processing and pattern recognition challenges. This paper introduces a novel Quantum Mixed-State Attention Network (QMSAN), which integrates the principles of quantum computing with classical machine learning algorithms, especially self-attention networks, to enhance the efficiency and effectiveness in handling NLP tasks. QMSAN model employs a quantum attention mechanism based on mixed states, enabling efficient direct estimation of similarity between queries and keys within the quantum domain, leading to more effective attention weight acquisition. Additionally, we propose an innovative quantum posit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#31934;&#30830;&#25552;&#21462;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#32536;/&#31471;&#28857;&#35774;&#22791;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#21487;&#20197;&#33719;&#21462;&#27169;&#22411;&#26550;&#26500;&#21644;&#22270;&#20687;&#32500;&#24230;&#31561;&#37325;&#35201;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.02870</link><description>&lt;p&gt;
&#36890;&#36807;&#36793;&#32536;/&#31471;&#28857;&#35774;&#22791;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#31934;&#30830;&#25552;&#21462;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02870
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#31934;&#30830;&#25552;&#21462;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#32536;/&#31471;&#28857;&#35774;&#22791;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#21487;&#20197;&#33719;&#21462;&#27169;&#22411;&#26550;&#26500;&#21644;&#22270;&#20687;&#32500;&#24230;&#31561;&#37325;&#35201;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#26085;&#30410;&#26222;&#21450;&#65292;&#27169;&#22411;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#65292;&#21482;&#26377;&#25317;&#26377;&#24222;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#24040;&#22823;&#35745;&#31639;&#33021;&#21147;&#30340;&#20844;&#21496;&#25165;&#33021;&#24212;&#23545;&#19994;&#21153;&#38656;&#27714;&#30340;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;DL&#27169;&#22411;&#26159;&#20844;&#21496;&#19987;&#26377;&#30340;&#65292;&#22240;&#27492;&#36825;&#20123;&#20844;&#21496;&#21162;&#21147;&#20445;&#25252;&#20182;&#20204;&#30340;&#31169;&#26377;&#27169;&#22411;&#65292;&#20197;&#20813;&#21463;&#21040;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65288;MEA&#65289;&#30340;&#20405;&#23475;&#65292;&#35813;&#25915;&#20987;&#30446;&#30340;&#26159;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#26469;&#31363;&#21462;&#27169;&#22411;&#12290;&#22914;&#20170;&#65292;&#20844;&#21496;&#20542;&#21521;&#20110;&#23558;&#27169;&#22411;&#20174;&#20013;&#22830;&#26381;&#21153;&#22120;&#36716;&#31227;&#21040;&#36793;&#32536;/&#31471;&#28857;&#35774;&#22791;&#12290;&#27491;&#22914;&#26368;&#26032;&#30740;&#31350;&#25581;&#31034;&#30340;&#37027;&#26679;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#36825;&#19968;&#26426;&#20250;&#20316;&#20026;&#21551;&#21160;&#20391;&#20449;&#36947;&#25915;&#20987;&#65288;SCA&#65289;&#30340;&#26032;&#25915;&#20987;&#21521;&#37327;&#65292;&#38024;&#23545;&#36816;&#34892;&#21463;&#23475;&#27169;&#22411;&#30340;&#35774;&#22791;&#21457;&#21160;&#25915;&#20987;&#65292;&#33719;&#21462;&#27169;&#22411;&#20449;&#24687;&#30340;&#21508;&#31181;&#35201;&#28857;&#65292;&#20363;&#22914;&#27169;&#22411;&#26550;&#26500;&#65288;MA&#65289;&#21644;&#22270;&#20687;&#32500;&#24230;&#65288;ID&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#20840;&#38754;&#29702;&#35299;&#20102;&#36825;&#31181;&#20851;&#31995;&#65292;&#23558;&#26377;&#21161;&#20110;&#26410;&#26469;MEA&#30740;&#31350;&#22312;&#36827;&#25915;&#21644;&#38450;&#24481;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02870v1 Announce Type: new  Abstract: With growing popularity, deep learning (DL) models are becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large models. Most of those DL models are proprietary to the companies who thus strive to keep their private models safe from the model extraction attack (MEA), whose aim is to steal the model by training surrogate models. Nowadays, companies are inclined to offload the models from central servers to edge/endpoint devices. As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim model and obtain various pieces of the model information, such as the model architecture (MA) and image dimension (ID). Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and de
&lt;/p&gt;</description></item><item><title>&#23558;&#25193;&#25955;&#36807;&#31243;&#35270;&#20026;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#31995;&#32479;&#65292;&#24314;&#31435;&#20102;&#21487;&#25193;&#23637;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#20197;&#36817;&#20284;&#20174;&#32423;&#32852;&#20013;&#25512;&#26029;&#20986;&#22522;&#30784;&#32593;&#32476;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#25512;&#26029;&#21644;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02867</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#26694;&#26550;&#29992;&#20110;&#32593;&#32476;&#25512;&#26029;&#21644;&#24433;&#21709;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02867
&lt;/p&gt;
&lt;p&gt;
&#23558;&#25193;&#25955;&#36807;&#31243;&#35270;&#20026;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#31995;&#32479;&#65292;&#24314;&#31435;&#20102;&#21487;&#25193;&#23637;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#20197;&#36817;&#20284;&#20174;&#32423;&#32852;&#20013;&#25512;&#26029;&#20986;&#22522;&#30784;&#32593;&#32476;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#25512;&#26029;&#21644;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#36830;&#32493;&#26102;&#38388;&#20449;&#24687;&#20256;&#25773;&#30340;&#30740;&#31350;&#24050;&#25104;&#20026;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#24403;&#21482;&#33021;&#35775;&#38382;&#20256;&#25773;&#36319;&#36394;&#65288;&#32423;&#32852;&#65289;&#26102;&#65292;&#22522;&#20110;&#32423;&#32852;&#30340;&#32593;&#32476;&#25512;&#26029;&#21644;&#24433;&#21709;&#20272;&#35745;&#26159;&#20004;&#20010;&#24517;&#39035;&#25506;&#35752;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#25512;&#26029;&#21644;&#22788;&#29702;&#36229;&#36807;&#20960;&#21315;&#20010;&#33410;&#28857;&#30340;&#32593;&#32476;&#26102;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#33021;&#21147;&#65292;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#25193;&#25955;&#36807;&#31243;&#35270;&#20026;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#31995;&#32479;&#65292;&#22522;&#20110;&#27492;&#24314;&#31435;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#65288;FIM&#65289;&#65292;&#20197;&#36817;&#20284;&#20174;&#21487;&#29992;&#32423;&#32852;&#20013;&#25512;&#26029;&#20986;&#22522;&#30784;&#32593;&#32476;&#32467;&#26500;&#30340;&#25193;&#25955;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;FIM&#22312;&#32593;&#32476;&#25512;&#26029;&#20013;&#30340;&#36817;&#20284;&#35823;&#24046;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20026;&#20102;&#23454;&#29616;&#24433;&#21709;&#20272;&#35745;&#30340;&#25152;&#38656;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02867v1 Announce Type: cross  Abstract: The study of continuous-time information diffusion has been an important area of research for many applications in recent years. When only the diffusion traces (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore. Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues. In this paper, we view the diffusion process as a continuous-time dynamical system, based on which we establish a continuous-time diffusion model. Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the diffusion propagation from available cascades, thereby inferring the underlying network structure. Furthermore, we undertake an analysis of the approximation error of FIM for network inference. To achieve the desired scalability for influence estimation, we devise 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#27169;&#22411;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;FLGuard&#26041;&#27861;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#25308;&#21344;&#24237;-&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02846</link><description>&lt;p&gt;
FLGuard: &#36890;&#36807;&#23545;&#27604;&#27169;&#22411;&#38598;&#21512;&#23454;&#29616;&#25308;&#21344;&#24237;-&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02846
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#27169;&#22411;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;FLGuard&#26041;&#27861;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#25308;&#21344;&#24237;-&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#20165;&#20849;&#20139;&#26412;&#22320;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#20855;&#26377;&#31169;&#20154;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20247;&#22810;&#23458;&#25143;&#26102;&#34028;&#21187;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#25237;&#27602;&#25915;&#20987;&#65292;&#24403;&#23545;&#25163;&#20551;&#25198;&#20026;&#33391;&#24615;&#23458;&#25143;&#24182;&#23384;&#22312;&#20110;&#19968;&#32452;&#23458;&#25143;&#20013;&#26102;&#65292;&#20250;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#28798;&#38590;&#24615;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#25308;&#21344;&#24237;-&#40065;&#26834;&#30340;FL&#26041;&#27861;&#65292;&#20801;&#35768;&#26381;&#21153;&#22120;&#21363;&#20351;&#22312;&#31995;&#32479;&#20013;&#23384;&#22312;&#23545;&#25163;&#20063;&#33021;&#35757;&#32451;&#20934;&#30830;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#35201;&#27714;&#30693;&#36947;&#24694;&#24847;&#23458;&#25143;&#30340;&#25968;&#37327;&#25110;&#36741;&#21161;&#65288;&#24178;&#20928;&#65289;&#25968;&#25454;&#38598;&#65292;&#25110;&#32773;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#26102;&#20854;&#26377;&#25928;&#24615;&#25253;&#21578;&#26126;&#26174;&#38477;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02846v1 Announce Type: cross  Abstract: Federated Learning (FL) thrives in training a global model with numerous clients by only sharing the parameters of their local models trained with their private training datasets. Therefore, without revealing the private dataset, the clients can obtain a deep learning (DL) model with high performance. However, recent research proposed poisoning attacks that cause a catastrophic loss in the accuracy of the global model when adversaries, posed as benign clients, are present in a group of clients. Therefore, recent studies suggested byzantine-robust FL methods that allow the server to train an accurate global model even with the adversaries present in the system. However, many existing methods require the knowledge of the number of malicious clients or the auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when the private dataset was non-independently and identically distributed (non-IID). In this work, we propose
&lt;/p&gt;</description></item><item><title>SOFIM&#21033;&#29992;&#27491;&#21017;&#21270;Fisher&#20449;&#24687;&#30697;&#38453;&#21644;Sherman-Morrison&#30697;&#38453;&#27714;&#36870;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#38543;&#26426;&#20248;&#21270;&#20013;&#26799;&#24230;&#26356;&#26032;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#24102;&#26469;&#30340;&#38750;&#24179;&#31283;&#30446;&#26631;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02833</link><description>&lt;p&gt;
SOFIM: &#20351;&#29992;&#27491;&#21017;&#21270;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02833
&lt;/p&gt;
&lt;p&gt;
SOFIM&#21033;&#29992;&#27491;&#21017;&#21270;Fisher&#20449;&#24687;&#30697;&#38453;&#21644;Sherman-Morrison&#30697;&#38453;&#27714;&#36870;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#38543;&#26426;&#20248;&#21270;&#20013;&#26799;&#24230;&#26356;&#26032;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#24102;&#26469;&#30340;&#38750;&#24179;&#31283;&#30446;&#26631;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27491;&#21017;&#21270;Fisher&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;SOFIM&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;FIM&#26469;&#36924;&#36817;Hessian&#30697;&#38453;&#65292;&#20197;&#25214;&#21040;&#22823;&#35268;&#27169;&#38543;&#26426;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#29275;&#39039;&#26799;&#24230;&#26356;&#26032;&#12290;&#21487;&#20197;&#35270;&#20026;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;NGD&#65289;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#21270;FIM&#21644;&#30452;&#25509;&#36890;&#36807;Sherman-Morrison&#30697;&#38453;&#27714;&#36870;&#25214;&#21040;&#26799;&#24230;&#26356;&#26032;&#26041;&#21521;&#26469;&#35299;&#20915;&#23384;&#20648;&#21644;&#35745;&#31639;&#23436;&#25972;FIM&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20687;&#24191;&#21463;&#27426;&#36814;&#30340;Adam&#26041;&#27861;&#19968;&#26679;&#65292;SOFIM&#21033;&#29992;&#26799;&#24230;&#30340;&#31532;&#19968;&#26102;&#21051;&#26469;&#22788;&#29702;&#30001;&#24322;&#26500;&#25968;&#25454;&#24341;&#36215;&#30340;&#36328;&#23567;&#25209;&#27425;&#38750;&#24179;&#31283;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#27491;&#21017;&#21270;FIM&#21644;Sherman-Morrison&#30697;&#38453;&#27714;&#36870;&#30340;&#21033;&#29992;&#23548;&#33268;&#20102;&#25910;&#25947;&#36895;&#29575;&#30340;&#25913;&#21892;&#65292;&#21516;&#26102;space&#21644;time&#22797;&#26434;&#24230;&#19982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02833v1 Announce Type: new  Abstract: This paper introduces a new stochastic optimization method based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton's gradient update in large-scale stochastic optimization of machine learning models. It can be viewed as a variant of natural gradient descent (NGD), where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion. Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data. The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as stochastic gradient descent (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#29983;&#24577;&#25490;&#25918;&#26469;&#20445;&#25252;&#29983;&#24577;&#31995;&#32479;&#30340;&#27700;&#30005;&#31649;&#29702;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#25512;&#21160;&#27700;&#30005;&#21378;&#20860;&#39038;&#29615;&#22659;&#20445;&#25252;&#21644;&#33021;&#28304;&#29983;&#20135;&#12290;</title><link>https://arxiv.org/abs/2403.02821</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#24212;&#24615;&#27700;&#30005;&#31649;&#29702;&#26041;&#27861;&#29992;&#20110;&#19979;&#28216;&#29983;&#24577;&#31995;&#32479;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Hydropower Management Approach for Downstream Ecosystem Preservation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02821
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#29983;&#24577;&#25490;&#25918;&#26469;&#20445;&#25252;&#29983;&#24577;&#31995;&#32479;&#30340;&#27700;&#30005;&#31649;&#29702;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#25512;&#21160;&#27700;&#30005;&#21378;&#20860;&#39038;&#29615;&#22659;&#20445;&#25252;&#21644;&#33021;&#28304;&#29983;&#20135;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#30005;&#21378;&#22312;&#25512;&#21160;&#28165;&#27905;&#21644;&#21487;&#25345;&#32493;&#33021;&#28304;&#29983;&#20135;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#23545;&#20840;&#29699;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#26469;&#28304;&#30340;&#36807;&#28193;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#27700;&#30005;&#21378;&#30446;&#21069;&#34987;&#35270;&#20026;&#26082;&#26159;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#26469;&#28304;&#65292;&#21448;&#26159;&#29983;&#24577;&#31995;&#32479;&#30340;&#30772;&#22351;&#32773;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#29983;&#24577;&#25490;&#25918;&#20316;&#20026;&#20445;&#25252;&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#31181;&#28508;&#21147;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#25552;&#20513;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#27599;&#20010;&#25152;&#38656;&#26102;&#38388;&#39044;&#27979;&#26368;&#23567;&#29983;&#24577;&#25490;&#25918;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;&#20854;&#26080;&#32541;&#38598;&#25104;&#21040;&#27700;&#30005;&#31649;&#29702;&#36719;&#20214;&#20013;&#65292;&#21033;&#29992;&#20256;&#32479;&#21463;&#38480;&#20248;&#21270;&#31639;&#27861;&#30340;&#25104;&#29087;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#20445;&#25252;&#29983;&#24577;&#31995;&#32479;&#20813;&#21463;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#36824;&#26377;&#21487;&#33021;&#22686;&#21152;&#30005;&#21147;&#20135;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02821v1 Announce Type: new  Abstract: Hydropower plants play a pivotal role in advancing clean and sustainable energy production, contributing significantly to the global transition towards renewable energy sources. However, hydropower plants are currently perceived both positively as sources of renewable energy and negatively as disruptors of ecosystems. In this work, we highlight the overlooked potential of using hydropower plant as protectors of ecosystems by using adaptive ecological discharges. To advocate for this perspective, we propose using a neural network to predict the minimum ecological discharge value at each desired time. Additionally, we present a novel framework that seamlessly integrates it into hydropower management software, taking advantage of the well-established approach of using traditional constrained optimisation algorithms. This novel approach not only protects the ecosystems from climate change but also contributes to potentially increase the elec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;InjectTST&#36825;&#31181;&#23558;&#20840;&#23616;&#20449;&#24687;&#27880;&#20837;&#29420;&#31435;&#36890;&#36947;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02814</link><description>&lt;p&gt;
InjectTST: &#23558;&#20840;&#23616;&#20449;&#24687;&#27880;&#20837;&#29420;&#31435;&#36890;&#36947;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#29992;&#20110;&#38271;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02814
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;InjectTST&#36825;&#31181;&#23558;&#20840;&#23616;&#20449;&#24687;&#27880;&#20837;&#29420;&#31435;&#36890;&#36947;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#24050;&#25104;&#20026;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#26368;&#27969;&#34892;&#30340;&#26550;&#26500;&#20043;&#19968;&#12290;&#26368;&#36817;&#22522;&#20110;Transformer&#30340;MTS&#27169;&#22411;&#36890;&#24120;&#20542;&#21521;&#20110;&#20855;&#26377;&#36890;&#36947;&#29420;&#31435;&#32467;&#26500;&#65292;&#22240;&#20026;&#36890;&#36947;&#29420;&#31435;&#21487;&#20197;&#20943;&#36731;&#22122;&#22768;&#21644;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36947;&#20381;&#36182;&#24615;&#20173;&#28982;&#26159;MTS&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#34164;&#21547;&#30528;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#35774;&#35745;&#19968;&#20010;&#32467;&#21512;&#20102;&#36890;&#36947;&#29420;&#31435;&#21644;&#36890;&#36947;&#28151;&#21512;&#32467;&#26500;&#20248;&#28857;&#30340;&#27169;&#22411;&#26159;&#36827;&#19968;&#27493;&#25913;&#36827;MTS&#39044;&#27979;&#30340;&#20851;&#38190;&#65292;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38590;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20840;&#23616;&#20449;&#24687;&#27880;&#20837;&#36890;&#36947;&#26080;&#20851;Transformer&#30340;&#27880;&#20837;&#26041;&#27861;InjectTST&#12290;&#25105;&#20204;&#27809;&#26377;&#30452;&#25509;&#35774;&#35745;&#19968;&#20010;&#28151;&#21512;&#36890;&#36947;&#27169;&#22411;&#65292;&#32780;&#26159;&#20445;&#30041;&#20102;&#36890;&#36947;&#29420;&#31435;&#30340;&#26694;&#26550;&#65292;&#24182;&#36880;&#28176;&#23558;&#20840;&#23616;&#20449;&#24687;&#27880;&#20837;&#21040;&#21333;&#20010;&#36890;&#36947;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02814v1 Announce Type: cross  Abstract: Transformer has become one of the most popular architectures for multivariate time series (MTS) forecasting. Recent Transformer-based MTS models generally prefer channel-independent structures with the observation that channel independence can alleviate noise and distribution drift issues, leading to more robustness. Nevertheless, it is essential to note that channel dependency remains an inherent characteristic of MTS, carrying valuable information. Designing a model that incorporates merits of both channel-independent and channel-mixing structures is a key to further improvement of MTS forecasting, which poses a challenging conundrum. To address the problem, an injection method for global information into channel-independent Transformer, InjectTST, is proposed in this paper. Instead of designing a channel-mixing model directly, we retain the channel-independent backbone and gradually inject global information into individual channels
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25805;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#39640;&#26031;&#22270;&#31639;&#23376;&#65288;DGGO&#65289;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#31163;&#25955;&#21147;&#23398;&#38382;&#39064;&#20013;&#23398;&#20064;&#21442;&#25968;&#21270;PDEs&#12290;</title><link>https://arxiv.org/abs/2403.02810</link><description>&lt;p&gt;
&#21160;&#24577;&#39640;&#26031;&#22270;&#31639;&#23376;&#65306;&#22312;&#20219;&#24847;&#31163;&#25955;&#21147;&#23398;&#38382;&#39064;&#20013;&#23398;&#20064;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Dynamic Gaussian Graph Operator: Learning parametric partial differential equations in arbitrary discrete mechanics problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02810
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25805;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#39640;&#26031;&#22270;&#31639;&#23376;&#65288;DGGO&#65289;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#31163;&#25955;&#21147;&#23398;&#38382;&#39064;&#20013;&#23398;&#20064;&#21442;&#25968;&#21270;PDEs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#30001;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25511;&#21046;&#30340;&#29289;&#29702;&#31995;&#32479;&#65292;&#22240;&#20026;&#26377;&#22823;&#37327;&#31185;&#23398;&#25968;&#25454;&#21487;&#20379;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#21457;&#23637;&#20102;&#19968;&#31181;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#23398;&#20064;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#25552;&#20379;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#21040;&#35299;&#20915;&#26041;&#26696;&#30340;&#25509;&#21475;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31639;&#23376;&#20165;&#38480;&#20110;&#24658;&#23450;&#21644;&#22343;&#21248;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#35745;&#31639;&#22495;&#30340;&#20219;&#24847;&#31163;&#25955;&#21270;&#26041;&#26696;&#19978;&#27867;&#21270;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25805;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#39640;&#26031;&#22270;&#31639;&#23376;&#65288;DGGO&#65289;&#65292;&#23558;&#31070;&#32463;&#31639;&#23376;&#25193;&#23637;&#21040;&#22312;&#20219;&#24847;&#31163;&#25955;&#21147;&#23398;&#38382;&#39064;&#20013;&#23398;&#20064;&#21442;&#25968;&#21270;PDEs&#12290;&#21160;&#24577;&#39640;&#26031;&#22270;&#65288;DGG&#65289;&#26680;&#23398;&#20064;&#23558;&#22312;&#19968;&#33324;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23450;&#20041;&#30340;&#35266;&#23519;&#21521;&#37327;&#26144;&#23556;&#21040;&#39640;&#32500;&#22343;&#21248;&#24230;&#37327;&#20013;&#23450;&#20041;&#30340;&#24230;&#37327;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02810v1 Announce Type: cross  Abstract: Deep learning methods have access to be employed for solving physical systems governed by parametric partial differential equations (PDEs) due to massive scientific data. It has been refined to operator learning that focuses on learning non-linear mapping between infinite-dimensional function spaces, offering interface from observations to solutions. However, state-of-the-art neural operators are limited to constant and uniform discretization, thereby leading to deficiency in generalization on arbitrary discretization schemes for computational domain. In this work, we propose a novel operator learning algorithm, referred to as Dynamic Gaussian Graph Operator (DGGO) that expands neural operators to learning parametric PDEs in arbitrary discrete mechanics problems. The Dynamic Gaussian Graph (DGG) kernel learns to map the observation vectors defined in general Euclidean space to metric vectors defined in high-dimensional uniform metric s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#19982;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411; VIB-DML &#29992;&#20110;&#35780;&#20998;&#39044;&#27979;&#65292;&#38480;&#21046;&#28508;&#31354;&#38388;&#29305;&#24449;&#21521;&#37327;&#30340;&#20114;&#20449;&#24687;&#20197;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#24182;&#28385;&#36275;&#27431;&#27663;&#36317;&#31163;&#30340;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2403.02794</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30340;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Distance Metric Learning Model Based On Variational Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#19982;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411; VIB-DML &#29992;&#20110;&#35780;&#20998;&#39044;&#27979;&#65292;&#38480;&#21046;&#28508;&#31354;&#38388;&#29305;&#24449;&#21521;&#37327;&#30340;&#20114;&#20449;&#24687;&#20197;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#24182;&#28385;&#36275;&#27431;&#27663;&#36317;&#31163;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#25216;&#26415;&#34028;&#21187;&#21457;&#23637;&#65292;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#20043;&#19968;&#12290;&#20808;&#21518;&#25552;&#20986;&#30340;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#21644;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#21518;&#32773;&#20351;&#29992;&#27431;&#27663;&#36317;&#31163;&#32780;&#38750;&#21069;&#32773;&#25152;&#20351;&#29992;&#30340;&#28857;&#31215;&#26469;&#34913;&#37327;&#28508;&#31354;&#38388;&#21521;&#37327;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#19982;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411; VIB-DML&#65288;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#65289;&#29992;&#20110;&#35780;&#20998;&#39044;&#27979;&#65292;&#38480;&#21046;&#28508;&#31354;&#38388;&#29305;&#24449;&#21521;&#37327;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#28385;&#36275;&#27431;&#27663;&#36317;&#31163;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02794v1 Announce Type: cross  Abstract: In recent years, personalized recommendation technology has flourished and become one of the hot research directions. The matrix factorization model and the metric learning model which proposed successively have been widely studied and applied. The latter uses the Euclidean distance instead of the dot product used by the former to measure the latent space vector. While avoiding the shortcomings of the dot product, the assumption of Euclidean distance is neglected, resulting in limited recommendation quality of the model. In order to solve this problem, this paper combines the Variationl Information Bottleneck with metric learning model for the first time, and proposes a new metric learning model VIB-DML (Variational Information Bottleneck Distance Metric Learning) for rating prediction, which limits the mutual information of the latent space feature vector to improve the robustness of the model and satisfiy the assumption of Euclidean 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#20869;&#25506;&#32034;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20154;&#31867;&#20013;&#24515;&#35299;&#37322;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#65292;&#20197;&#22686;&#24378;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02786</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#35299;&#37322;&#30340;&#21322;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#39044;&#27979;&#33026;&#32938;&#32925;&#30149;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#20869;&#25506;&#32034;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20154;&#31867;&#20013;&#24515;&#35299;&#37322;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#65292;&#20197;&#22686;&#24378;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#33026;&#32938;&#32925;&#30149;&#26041;&#38754;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#20869;&#21033;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#20027;&#39064;&#30456;&#20284;&#24615;&#22270;&#65292;&#20174;&#20581;&#24247;&#26816;&#26597;&#25968;&#25454;&#20013;&#35782;&#21035;&#39118;&#38505;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#21508;&#31181;GNN&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#26377;&#26368;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;GNNs&#21253;&#21547;&#20154;&#31867;&#20013;&#24515;&#35299;&#37322;&#65292;&#20026;&#22686;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#25552;&#20379;&#20010;&#24615;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20419;&#36827;&#20581;&#24247;&#23454;&#36341;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20154;&#31867;&#20013;&#24515;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02786v1 Announce Type: cross  Abstract: Addressing the challenge of limited labeled data in clinical settings, particularly in the prediction of fatty liver disease, this study explores the potential of graph representation learning within a semi-supervised learning framework. Leveraging graph neural networks (GNNs), our approach constructs a subject similarity graph to identify risk patterns from health checkup data. The effectiveness of various GNN approaches in this context is demonstrated, even with minimal labeled samples. Central to our methodology is the inclusion of human-centric explanations through explainable GNNs, providing personalized feature importance scores for enhanced interpretability and clinical relevance, thereby underscoring the potential of our approach in advancing healthcare practices with a keen focus on graph representation learning and human-centric explanation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26469;&#22788;&#29702;&#22810;&#26469;&#28304;&#25968;&#25454;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02780</link><description>&lt;p&gt;
&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data Collaboration Analysis Over Matrix Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26469;&#22788;&#29702;&#22810;&#26469;&#28304;&#25968;&#25454;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#25913;&#36827;&#30340;&#25968;&#25454;&#38598;&#65292;&#26631;&#24535;&#30528;&#20248;&#36234;&#30340;&#36136;&#37327;&#65292;&#22686;&#24378;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25193;&#23637;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#20197;&#20943;&#36731;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#21644;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24191;&#27867;&#30340;&#25968;&#25454;&#34701;&#21512;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#36947;&#24503;&#20851;&#20999;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#29992;&#25143;&#38544;&#31169;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#25259;&#38706;&#39118;&#38505;&#12290;&#24050;&#24314;&#31435;&#20102;&#21508;&#31181;&#20840;&#29699;&#31435;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#27861;&#35268;&#23545;&#20445;&#25252;&#38544;&#31169;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;ML&#25216;&#26415;&#30340;&#23454;&#38469;&#37096;&#32626;&#21464;&#24471;&#22797;&#26434;&#12290;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;(PPML)&#36890;&#36807;&#20445;&#25252;&#20174;&#20581;&#24247;&#35760;&#24405;&#21040;&#22320;&#29702;&#20301;&#32622;&#25968;&#25454;&#31561;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#23454;&#29616;&#23433;&#20840;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02780v1 Announce Type: new  Abstract: The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets. Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios. Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets. However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure. Various global legislative frameworks have been established to address these privacy issues. While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies. Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#34880;&#31649;&#35299;&#21078;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.02777</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#23548;&#19997;&#24341;&#23548;&#30340;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A Zero-Shot Reinforcement Learning Strategy for Autonomous Guidewire Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#34880;&#31649;&#35299;&#21078;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#27835;&#30103;&#38656;&#35201;&#22797;&#26434;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23548;&#19997;&#21644;&#23548;&#31649;&#23548;&#33322;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#36825;&#19968;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#21487;&#33021;&#26159;&#33258;&#21160;&#21270;&#23548;&#31649;&#23548;&#33322;&#22312;&#26426;&#22120;&#36741;&#21161;&#25163;&#26415;&#20013;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35757;&#32451;&#26041;&#27861;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#34880;&#31649;&#35299;&#21078;&#32467;&#26500;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#65292;&#38656;&#35201;&#27599;&#27425;&#20960;&#20309;&#32467;&#26500;&#21464;&#21270;&#26102;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02777v1 Announce Type: new  Abstract: Purpose: The treatment of cardiovascular diseases requires complex and challenging navigation of a guidewire and catheter. This often leads to lengthy interventions during which the patient and clinician are exposed to X-ray radiation. Deep Reinforcement Learning approaches have shown promise in learning this task and may be the key to automating catheter navigation during robotized interventions. Yet, existing training methods show limited capabilities at generalizing to unseen vascular anatomies, requiring to be retrained each time the geometry changes. Methods: In this paper, we propose a zero-shot learning strategy for three-dimensional autonomous endovascular navigation. Using a very small training set of branching patterns, our reinforcement learning algorithm is able to learn a control that can then be applied to unseen vascular anatomies without retraining. Results: We demonstrate our method on 4 different vascular systems, with 
&lt;/p&gt;</description></item><item><title>EasyQuant&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#12289;&#26080;&#38656;&#25968;&#25454;&#30340;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#37327;&#21270;&#35823;&#24046;&#24182;&#20445;&#35777;LLM&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02775</link><description>&lt;p&gt;
EasyQuant: &#19968;&#31181;&#29992;&#20110;LLM&#30340;&#39640;&#25928;&#26080;&#25968;&#25454;&#37327;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02775
&lt;/p&gt;
&lt;p&gt;
EasyQuant&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#12289;&#26080;&#38656;&#25968;&#25454;&#30340;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#37327;&#21270;&#35823;&#24046;&#24182;&#20445;&#35777;LLM&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#35201;&#27604;&#20256;&#32479;&#26041;&#27861;&#20248;&#36234;&#24471;&#22810;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#39640;&#20869;&#23384;&#38656;&#27714;&#20351;&#20854;&#38590;&#20197;&#37096;&#32626;&#12290;&#27169;&#22411;&#37327;&#21270;&#26159;&#20943;&#23569;&#36825;&#31181;&#24320;&#38144;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#37327;&#21270;&#27169;&#22411;&#26159;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#26657;&#20934;&#30340;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#21069;&#20154;&#24037;&#20316;&#37324;&#37327;&#21270;&#21518;&#30340;LLMs&#23545;&#26410;&#30693;&#24773;&#20917;&#21644;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#20026;LLM&#35774;&#35745;&#19968;&#31181;&#26080;&#25968;&#25454;&#37327;&#21270;&#26041;&#27861;&#20197;&#20445;&#35777;&#20854;&#27867;&#21270;&#24615;&#33021;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyQuant&#65292;&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#26080;&#38656;&#35757;&#32451;&#21644;&#26080;&#25968;&#25454;&#30340;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#34920;&#26126;&#65292;&#26435;&#37325;&#21644;&#37327;&#21270;&#33539;&#22260;&#20013;&#30340;&#24322;&#24120;&#20540;&#26159;&#20943;&#23569;&#37327;&#21270;&#35823;&#24046;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#22312;EasyQuant&#20013;&#65292;&#25105;&#20204;&#20445;&#30041;&#20102;&#36825;&#20123;&#24322;&#24120;&#20540;&#65288;&#24453;&#32493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02775v1 Announce Type: new  Abstract: Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;</title><link>https://arxiv.org/abs/2403.02774</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#23610;&#24230;&#21644;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#22330;&#38477;&#23610;&#24230;&#19982;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02774
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;(ESM)&#27169;&#25311;&#23545;&#20110;&#35780;&#20272;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#23545;&#29983;&#24577;&#21644;&#31038;&#20250;&#32463;&#27982;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;ESM&#27169;&#25311;&#30340;&#38477;&#23610;&#24230;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#27599;&#20010;ESM&#37117;&#38656;&#35201;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#27668;&#20505;&#39044;&#27979;&#25928;&#26524;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#19968;&#33268;&#24615;&#27169;&#22411;(CM)&#65292;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#39640;&#25928;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;ESM&#27169;&#25311;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#27169;&#22411;&#26041;&#27861;&#20197;&#21482;&#21463;&#35266;&#27979;&#21442;&#32771;&#25968;&#25454;&#38480;&#21046;&#30340;&#20998;&#36776;&#29575;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CM&#22312;&#32500;&#25345;&#39640;&#21487;&#25511;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02774v1 Announce Type: cross  Abstract: Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30828;&#21644;&#36719;&#36127;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#20013;&#26679;&#26412;&#31232;&#32570;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02772</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#24247;&#22797;&#38203;&#28860;&#36136;&#37327;&#35780;&#20272;&#65292;&#32467;&#21512;&#30828;&#36127;&#26679;&#26412;&#21644;&#36719;&#36127;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30828;&#21644;&#36719;&#36127;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#20013;&#26679;&#26412;&#31232;&#32570;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38203;&#28860;&#30340;&#24247;&#22797;&#35745;&#21010;&#24050;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#12289;&#38477;&#20302;&#27515;&#20129;&#29575;&#21644;&#20877;&#20303;&#38498;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#34394;&#25311;&#24247;&#22797;&#65292;&#24739;&#32773;&#21487;&#20197;&#22312;&#23478;&#29420;&#31435;&#23436;&#25104;&#38203;&#28860;&#65292;&#21033;&#29992;AI&#31639;&#27861;&#20998;&#26512;&#38203;&#28860;&#25968;&#25454;&#65292;&#20026;&#24739;&#32773;&#25552;&#20379;&#21453;&#39304;&#65292;&#24182;&#21521;&#20020;&#24202;&#21307;&#29983;&#26356;&#26032;&#20182;&#20204;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;&#36825;&#20123;&#35745;&#21010;&#36890;&#24120;&#20250;&#25351;&#23450;&#21508;&#31181;&#38203;&#28860;&#31867;&#22411;&#65292;&#36825;&#23548;&#33268;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#25968;&#25454;&#38598;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#65306;&#34429;&#28982;&#22312;&#25972;&#20307;&#35757;&#32451;&#26679;&#26412;&#20013;&#20016;&#23500;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#23545;&#27599;&#31181;&#20855;&#20307;&#38203;&#32451;&#31867;&#22411;&#30340;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#12290;&#36825;&#31181;&#24046;&#24322;&#24433;&#21709;&#20102;&#29616;&#26377;&#26041;&#27861;&#35757;&#32451;&#20855;&#26377;&#23567;&#26679;&#26412;&#37327;&#30340;&#27599;&#31181;&#38203;&#32451;&#30340;&#21487;&#27867;&#21270;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#30828;&#36127;&#26679;&#26412;&#21644;&#36719;&#36127;&#26679;&#26412;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;&#25972;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02772v1 Announce Type: cross  Abstract: Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates. AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress. These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training samples, these datasets often have a limited number of samples for each individual exercise type. This disparity hampers the ability of existing approaches to train generalizable models with such a small sample size per exercise. Addressing this issue, our paper introduces a novel supervised contrastive learning framework with hard and soft negative samples that effectively utilizes the entir
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;G4-Attention&#65292;&#24212;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#39044;&#27979;DNA G-&#22235;&#38142;&#20307;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#35813;&#27169;&#22411;&#20855;&#22791;&#26356;&#24378;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.02765</link><description>&lt;p&gt;
G4-Attention: &#24212;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;DNA G-&#22235;&#38142;&#20307;
&lt;/p&gt;
&lt;p&gt;
G4-Attention: Deep Learning Model with Attention for predicting DNA G-Quadruplexes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;G4-Attention&#65292;&#24212;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#39044;&#27979;DNA G-&#22235;&#38142;&#20307;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#35813;&#27169;&#22411;&#20855;&#22791;&#26356;&#24378;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
G-&#22235;&#38142;&#20307;&#26159;&#30001;&#40479;&#22028;&#21604;&#22235;&#32858;&#20307;&#22534;&#31215;&#25490;&#21015;&#24418;&#25104;&#30340;&#22235;&#38142;&#38750;&#35268;&#33539;&#26680;&#37240;&#32467;&#26500;&#65292;&#22240;&#20854;&#29420;&#29305;&#30340;&#32467;&#26500;&#29305;&#24615;&#22312;&#29983;&#29289;&#23398;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#20026;&#20102;&#39044;&#27979;&#27963;&#36291;&#30340;G4&#21306;&#22495;&#65292;&#30740;&#31350;&#20154;&#21592;&#36817;&#26399;&#24320;&#21457;&#20102;&#21517;&#20026;G4-seq&#21644;G4-ChIP-seq&#30340;&#27979;&#24207;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#20307;&#20869;&#21644;&#20307;&#22806;&#20197;&#25968;&#30334;&#20010;&#30897;&#22522;&#30340;&#20998;&#36776;&#29575;&#32472;&#21046;G4s&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34429;&#28982;&#33021;&#22815;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#26469;&#39044;&#27979;G4&#21306;&#22495;&#65292;&#20294;&#20854;&#39044;&#27979;&#27169;&#22411;&#36739;&#20026;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02765v1 Announce Type: new  Abstract: G-Quadruplexes are the four-stranded non-canonical nucleic acid secondary structures, formed by the stacking arrangement of the guanine tetramers. They are involved in a wide range of biological roles because of their exceptionally unique and distinct structural characteristics. After the completion of the human genome sequencing project, a lot of bioinformatic algorithms were introduced to predict the active G4s regions \textit{in vitro} based on the canonical G4 sequence elements, G-\textit{richness}, and G-\textit{skewness}, as well as the non-canonical sequence features. Recently, sequencing techniques like G4-seq and G4-ChIP-seq were developed to map the G4s \textit{in vitro}, and \textit{in vivo} respectively at a few hundred base resolution. Subsequently, several machine learning approaches were developed for predicting the G4 regions using the existing databases. However, their prediction models were simplistic, and the predictio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Paraformer&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#20302;&#20998;&#36776;&#29575;&#21382;&#21490;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#25351;&#23548;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#65292;&#35774;&#35745;&#20102;CNN-Transformer&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#32508;&#21512;&#25429;&#33719;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;</title><link>https://arxiv.org/abs/2403.02746</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#31934;&#30830;&#25351;&#23548;&#30340;&#23398;&#20064;&#65306;&#20174;&#20302;&#20998;&#36776;&#29575;&#21382;&#21490;&#26631;&#31614;&#26356;&#26032;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Paraformer&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#20302;&#20998;&#36776;&#29575;&#21382;&#21490;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#25351;&#23548;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#65292;&#35774;&#35745;&#20102;CNN-Transformer&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#32508;&#21512;&#25429;&#33719;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#26159;&#35843;&#26597;&#22320;&#29699;&#34920;&#38754;&#21644;&#35299;&#20915;&#20154;&#31867;&#38754;&#20020;&#30340;&#35768;&#22810;&#25361;&#25112;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#22320;&#38754;&#32454;&#33410;&#12289;&#21508;&#31181;&#22320;&#35980;&#21644;&#24191;&#27867;&#22320;&#29702;&#21306;&#22495;&#20869;&#20934;&#30830;&#35757;&#32451;&#26631;&#31614;&#30340;&#31232;&#32570;&#24615;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#39033;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65288;Paraformer&#65289;&#65292;&#21363;&#20302;&#21040;&#39640;&#32593;&#32476;&#65288;L2HNet&#65289;V2&#65292;&#29992;&#20110;&#22312;&#20302;&#20998;&#36776;&#29575;&#65288;LR&#65289;&#30340;&#26131;&#33719;&#21382;&#21490;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#25351;&#23548;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#29616;&#26377;&#30340;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#26041;&#27861;&#26174;&#31034;&#20102;CNN&#22312;&#20445;&#30041;&#23616;&#37096;&#22320;&#38754;&#32454;&#33410;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#22312;&#21508;&#31181;&#22320;&#35980;&#20013;&#20840;&#23616;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Paraformer&#20013;&#30340;&#24182;&#34892;CNN-Transformer&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21253;&#25324;&#19968;&#20010;&#26080;&#38477;&#37319;&#26679;CNN&#20998;&#25903;&#21644;&#19968;&#20010;Transformer&#20998;&#25903;&#65292;&#26469;&#20849;&#21516;&#25429;&#33719;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02746v1 Announce Type: cross  Abstract: Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer), a.k.a. Low-to-High Network (L2HNet) V2, to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual informatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;FDE&#65292;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#35843;&#25972;FDE&#20197;&#36866;&#24212;&#25968;&#25454;&#21160;&#24577;&#65292;&#21487;&#33021;&#20248;&#20110;&#31070;&#32463;OD&#12290;</title><link>https://arxiv.org/abs/2403.02737</link><description>&lt;p&gt;
&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Fractional Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02737
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;FDE&#65292;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#35843;&#25972;FDE&#20197;&#36866;&#24212;&#25968;&#25454;&#21160;&#24577;&#65292;&#21487;&#33021;&#20248;&#20110;&#31070;&#32463;OD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#65288;FDEs&#65289;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#24314;&#27169;&#22797;&#26434;&#31995;&#32479;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290; &#23427;&#20204;&#23558;&#20256;&#32479;&#30340;&#24494;&#20998;&#21644;&#31215;&#20998;&#27010;&#24565;&#25193;&#23637;&#21040;&#38750;&#25972;&#25968;&#38454;&#65292;&#20351;&#24471;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#34920;&#31034;&#20855;&#26377;&#38750;&#23616;&#37096;&#21644;&#35760;&#24518;&#20381;&#36182;&#34892;&#20026;&#29305;&#24449;&#30340;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#21463;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;FDE&#65292;&#36825;&#26159;&#19968;&#31181;&#35843;&#25972;FDE&#20197;&#36866;&#24212;&#25968;&#25454;&#21160;&#24577;&#30340;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#36825;&#39033;&#24037;&#20316;&#20840;&#38754;&#27010;&#36848;&#20102;&#31070;&#32463;FDE&#20013;&#37319;&#29992;&#30340;&#25968;&#20540;&#26041;&#27861;&#21644;&#31070;&#32463;FDE&#26550;&#26500;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#35745;&#31639;&#35201;&#27714;&#26356;&#39640;&#65292;&#31070;&#32463;FDE&#21487;&#33021;&#20248;&#20110;&#31070;&#32463;OD&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02737v1 Announce Type: new  Abstract: Fractional Differential Equations (FDEs) are essential tools for modelling complex systems in science and engineering. They extend the traditional concepts of differentiation and integration to non-integer orders, enabling a more precise representation of processes characterised by non-local and memory-dependent behaviours.   This property is useful in systems where variables do not respond to changes instantaneously, but instead exhibit a strong memory of past interactions.   Having this in mind, and drawing inspiration from Neural Ordinary Differential Equations (Neural ODEs), we propose the Neural FDE, a novel deep neural network architecture that adjusts a FDE to the dynamics of data.   This work provides a comprehensive overview of the numerical method employed in Neural FDEs and the Neural FDE architecture. The numerical outcomes suggest that, despite being more computationally demanding, the Neural FDE may outperform the Neural OD
&lt;/p&gt;</description></item><item><title>&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#25311;&#21463;&#38480;&#31995;&#32479;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#23547;&#25214;&#21512;&#36866;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#31532;&#20108;&#38454;&#27573;&#25214;&#21040;&#26368;&#20339;&#21442;&#25968;&#24182;&#20445;&#25345;&#22312;&#21487;&#34892;&#22495;&#20869;&#12290;</title><link>https://arxiv.org/abs/2403.02730</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#27169;&#21463;&#38480;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Training Method for Modeling Constrained Systems With Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02730
&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#25311;&#21463;&#38480;&#31995;&#32479;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#23547;&#25214;&#21512;&#36866;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#31532;&#20108;&#38454;&#27573;&#25214;&#21040;&#26368;&#20339;&#21442;&#25968;&#24182;&#20445;&#25345;&#22312;&#21487;&#34892;&#22495;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#32463;&#24120;&#34987;&#21046;&#23450;&#20026;&#21463;&#38480;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23558;&#32422;&#26463;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#25216;&#26415;&#65292;&#22914;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#65292;&#24050;&#32463;&#34987;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#24341;&#20837;&#38656;&#35201;&#36890;&#36807;&#35797;&#38169;&#25163;&#21160;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#65292;&#36825;&#20351;&#24471;&#32422;&#26463;&#25104;&#21151;&#34701;&#20837;&#29983;&#25104;&#27169;&#22411;&#20013;&#20135;&#29983;&#30097;&#34385;&#12290;&#26412;&#25991;&#35814;&#32454;&#25551;&#36848;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#26377;&#25928;&#19988;&#26080;&#38656;&#24809;&#32602;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#21463;&#38480;&#31995;&#32479;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#34987;&#37325;&#20889;&#20026;&#20004;&#20010;&#26080;&#32422;&#26463;&#30340;&#23376;&#38382;&#39064;&#65292;&#20998;&#20004;&#20010;&#38454;&#27573;&#35299;&#20915;&#12290;&#31532;&#19968;&#38454;&#27573;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#32422;&#26463;&#36829;&#21453;&#31243;&#24230;&#26469;&#25214;&#21040;&#21512;&#36866;&#30340;NN&#21442;&#25968;&#12290;&#31532;&#20108;&#38454;&#27573;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#25214;&#21040;&#26368;&#20339;&#30340;NN&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#21487;&#34892;&#22495;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02730v1 Announce Type: new  Abstract: Real-world systems are often formulated as constrained optimization problems. Techniques to incorporate constraints into Neural Networks (NN), such as Neural Ordinary Differential Equations (Neural ODEs), have been used. However, these introduce hyperparameters that require manual tuning through trial and error, raising doubts about the successful incorporation of constraints into the generated model. This paper describes in detail the two-stage training method for Neural ODEs, a simple, effective, and penalty parameter-free approach to model constrained systems. In this approach the constrained optimization problem is rewritten as two unconstrained sub-problems that are solved in two stages. The first stage aims at finding feasible NN parameters by minimizing a measure of constraints violation. The second stage aims to find the optimal NN parameters by minimizing the loss function while keeping inside the feasible region. We experimenta
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02713</link><description>&lt;p&gt;
Android&#22312;&#21160;&#29289;&#22253;&#20013;: GUI&#20195;&#29702;&#30340;&#21160;&#20316;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Android in the Zoo: Chain-of-Action-Thought for GUI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23548;&#33268;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#22823;&#37327;&#33258;&#20027;GUI&#20195;&#29702;&#28608;&#22686;&#65292;&#36825;&#20123;&#20195;&#29702;&#36890;&#36807;&#39044;&#27979;API&#30340;&#19968;&#31995;&#21015;&#21160;&#20316;&#26469;&#23436;&#25104;&#30001;&#33258;&#28982;&#35821;&#35328;&#35302;&#21457;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#35813;&#20219;&#21153;&#39640;&#24230;&#20381;&#36182;&#20110;&#36807;&#21435;&#30340;&#21160;&#20316;&#21644;&#35270;&#35273;&#35266;&#23519;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#24456;&#23569;&#32771;&#34385;&#20013;&#38388;&#25130;&#22270;&#21644;&#23631;&#24149;&#25805;&#20316;&#20256;&#36882;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#20316;&#24605;&#32500;&#38142;&#65288;CoAT&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#20808;&#21069;&#21160;&#20316;&#30340;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#20998;&#26512;&#24212;&#24403;&#25191;&#34892;&#30340;&#21160;&#20316;&#20197;&#21450;&#36873;&#25321;&#30340;&#21160;&#20316;&#24102;&#26469;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#29616;&#25104;LLM&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;CoAT&#30456;&#27604;&#20110;&#26631;&#20934;&#19978;&#19979;&#25991;&#24314;&#27169;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#26631;&#30340;&#23436;&#25104;&#24773;&#20917;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Android-In-The-Zoo&#65288;AitZ&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;18,643&#20010;&#23631;&#24149;&#21160;&#20316;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02713v1 Announce Type: new  Abstract: Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pa
&lt;/p&gt;</description></item><item><title>&#22122;&#22768;&#28155;&#21152;&#21518;&#65292;&#31232;&#30095;&#32447;&#24615;&#38382;&#39064;&#19978;&#30340;&#26059;&#36716;&#19981;&#21464;&#31639;&#27861;&#20173;&#28982;&#27425;&#20248;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.02697</link><description>&lt;p&gt;
&#22122;&#22768;&#35823;&#23548;&#31232;&#30095;&#30446;&#26631;&#19978;&#30340;&#26059;&#36716;&#19981;&#21464;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Noise misleads rotation invariant algorithms on sparse targets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02697
&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#28155;&#21152;&#21518;&#65292;&#31232;&#30095;&#32447;&#24615;&#38382;&#39064;&#19978;&#30340;&#26059;&#36716;&#19981;&#21464;&#31639;&#27861;&#20173;&#28982;&#27425;&#20248;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#21363;&#20351;&#23545;&#20110;&#23398;&#20064;&#31232;&#30095;&#32447;&#24615;&#38382;&#39064;&#65292;&#24403;&#26679;&#26412;&#25968;&#20302;&#20110;&#38382;&#39064;&#30340;&#8220;&#32500;&#24230;&#8221;&#26102;&#65292;&#26059;&#36716;&#19981;&#21464;&#31639;&#27861;&#20063;&#26159;&#27425;&#20248;&#30340;&#12290;&#36825;&#20010;&#31867;&#21035;&#21253;&#25324;&#20219;&#20309;&#20351;&#29992;&#20840;&#36830;&#25509;&#36755;&#20837;&#23618;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;&#21021;&#22987;&#21270;&#20026;&#26059;&#36716;&#23545;&#31216;&#20998;&#24067;&#65289;&#12290;&#26368;&#31616;&#21333;&#30340;&#31232;&#30095;&#38382;&#39064;&#26159;&#23398;&#20064;$d$&#20010;&#29305;&#24449;&#20013;&#30340;&#19968;&#20010;&#29305;&#24449;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20998;&#31867;&#35823;&#24046;&#25110;&#22238;&#24402;&#25439;&#22833;&#38543;&#30528;$1-k/n$&#22686;&#38271;&#65292;&#20854;&#20013;$k$&#26159;&#35266;&#23519;&#21040;&#30340;&#26679;&#26412;&#25968;&#12290;&#24403;&#26679;&#26412;&#25968;$k$&#36798;&#21040;&#32500;&#24230;$d$&#26102;&#65292;&#36825;&#20123;&#19979;&#30028;&#21464;&#24471;&#31354;&#27867;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;&#36825;&#20010;&#31232;&#30095;&#32447;&#24615;&#38382;&#39064;&#26102;&#65292;&#21363;&#20351;&#22312;&#35266;&#23519;&#21040;$d$&#20010;&#25110;&#26356;&#22810;&#26679;&#26412;&#21518;&#65292;&#26059;&#36716;&#19981;&#21464;&#31639;&#27861;&#20173;&#28982;&#26159;&#27425;&#20248;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#38024;&#23545;&#19968;&#20010;&#26059;&#36716;&#23545;&#31216;&#21270;&#38382;&#39064;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#30340;&#19968;&#20010;&#19979;&#30028;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#21516;&#38382;&#39064;&#30340;&#26356;&#20302;&#30340;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02697v1 Announce Type: cross  Abstract: It is well known that the class of rotation invariant algorithms are suboptimal even for learning sparse linear problems when the number of examples is below the "dimension" of the problem. This class includes any gradient descent trained neural net with a fully-connected input layer (initialized with a rotationally symmetric distribution). The simplest sparse problem is learning a single feature out of $d$ features. In that case the classification error or regression loss grows with $1-k/n$ where $k$ is the number of examples seen. These lower bounds become vacuous when the number of examples $k$ reaches the dimension $d$.   We show that when noise is added to this sparse linear problem, rotation invariant algorithms are still suboptimal after seeing $d$ or more examples. We prove this via a lower bound for the Bayes optimal algorithm on a rotationally symmetrized problem. We then prove much lower upper bounds on the same problem for 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21487;&#25511;&#25552;&#31034;&#35843;&#25972;&#65288;CPT&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#22312;&#19981;&#21516;&#32452;&#20043;&#38388;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#65292;&#36991;&#20813;&#29306;&#29298;&#20219;&#20309;&#19968;&#20010;&#32452;&#30340;&#24615;&#33021;&#65292;&#22312;&#34394;&#20551;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02695</link><description>&lt;p&gt;
&#21487;&#25511;&#25552;&#31034;&#35843;&#25972;&#29992;&#20110;&#24179;&#34913;&#32452;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Controllable Prompt Tuning For Balancing Group Distributional Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02695
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21487;&#25511;&#25552;&#31034;&#35843;&#25972;&#65288;CPT&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#22312;&#19981;&#21516;&#32452;&#20043;&#38388;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#65292;&#36991;&#20813;&#29306;&#29298;&#20219;&#20309;&#19968;&#20010;&#32452;&#30340;&#24615;&#33021;&#65292;&#22312;&#34394;&#20551;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30001;&#19981;&#21516;&#32452;&#25110;&#39046;&#22495;&#32452;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#20986;&#29616;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#26368;&#24046;&#32452;&#30340;&#30446;&#26631;&#19978;&#65292;&#20294;&#36825;&#24448;&#24448;&#26159;&#20197;&#29306;&#29298;&#20854;&#20182;&#32452;&#19978;&#30340;&#33391;&#22909;&#24615;&#33021;&#20026;&#20195;&#20215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#32452;&#20869;&#33391;&#22909;&#24615;&#33021;&#65292;&#24182;&#25214;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#20250;&#20005;&#37325;&#29306;&#29298;&#20219;&#20309;&#19968;&#20010;&#32452;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#36825;&#31181;&#20248;&#21270;&#20250;&#28041;&#21450;&#26356;&#26032;&#25972;&#20010;&#32593;&#32476;&#30340;&#21442;&#25968;&#65292;&#36825;&#26082;&#32791;&#26102;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#25511;&#25552;&#31034;&#35843;&#25972;&#65288;CPT&#65289;&#65292;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#22312;&#34394;&#20551;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#31243;&#24207;&#22312;&#21464;&#21387;&#22120;&#21644;&#38750;&#21464;&#21387;&#22120;&#26550;&#26500;&#20197;&#21450;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#19978;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02695v1 Announce Type: new  Abstract: Models trained on data composed of different groups or domains can suffer from severe performance degradation under distribution shifts. While recent methods have largely focused on optimizing the worst-group objective, this often comes at the expense of good performance on other groups. To address this problem, we introduce an optimization scheme to achieve good performance across groups and find a good solution for all without severely sacrificing performance on any of them. However, directly applying such optimization involves updating the parameters of the entire network, making it both computationally expensive and challenging. Thus, we introduce Controllable Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques. On spurious correlation benchmarks, our procedures achieve state-of-the-art results across both transformer and non-transformer architectures, as well as unimodal and multimodal data, while requiring
&lt;/p&gt;</description></item><item><title>MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02694</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24863;&#30693;&#35821;&#20041;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Semantic Cache for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02694
&lt;/p&gt;
&lt;p&gt;
MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;Google Bard&#12289;Claude&#21644;Llama 2&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25628;&#32034;&#24341;&#25806;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36896;&#25104;&#20102;&#24322;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MeanCache&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#20197;&#30830;&#23450;&#32531;&#23384;&#21629;&#20013;&#25110;&#26410;&#21629;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;Dirichlet&#20998;&#24067;&#30340;&#36880;&#26679;&#26412;&#21152;&#26435;&#37319;&#26679;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;RENT&#26041;&#27861;&#26469;&#26377;&#25928;&#21033;&#29992;&#36716;&#31227;&#30697;&#38453;&#36827;&#34892;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.02690</link><description>&lt;p&gt;
&#21033;&#29992;&#36716;&#31227;&#30697;&#38453;&#36827;&#34892;Dirichlet-based Per-Sample&#21152;&#26435;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;Dirichlet&#20998;&#24067;&#30340;&#36880;&#26679;&#26412;&#21152;&#26435;&#37319;&#26679;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;RENT&#26041;&#27861;&#26469;&#26377;&#25928;&#21033;&#29992;&#36716;&#31227;&#30697;&#38453;&#36827;&#34892;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#36716;&#31227;&#30697;&#38453;&#34987;&#29992;&#26469;&#26126;&#30830;&#22320;&#24314;&#27169;&#22122;&#22768;&#26631;&#31614;&#20998;&#24067;&#19982;&#24178;&#20928;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#23454;&#29616;&#20998;&#31867;&#22120;&#25110;&#39118;&#38505;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36716;&#31227;&#30697;&#38453;&#30340;&#33391;&#22909;&#21033;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26032;&#37319;&#26679;&#30340;&#26032;&#21033;&#29992;&#26041;&#27861;&#65292;&#31216;&#20026;RENT&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#24403;&#21069;&#21033;&#29992;&#26041;&#24335;&#22312;&#23454;&#29616;&#20013;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#38480;&#21046;&#12290;&#20316;&#20026;Reweighting&#30340;&#24310;&#20280;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Dirichlet&#20998;&#24067;&#30340;&#36880;&#26679;&#26412;&#21152;&#26435;&#37319;&#26679;&#65288;DWS&#65289;&#26694;&#26550;&#65292;&#24182;&#22312;DWS&#26694;&#26550;&#19979;&#27604;&#36739;&#20102;&#21152;&#26435;&#21644;&#37325;&#26032;&#37319;&#26679;&#12290;&#36890;&#36807;DWS&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RENT&#65292;&#19968;&#31181;&#24102;&#26377;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#30340;&#37325;&#26032;&#37319;&#26679;&#26041;&#27861;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;RENT&#19968;&#30452;&#32988;&#36807;&#29616;&#26377;&#30340;&#36716;&#31227;&#30697;&#38453;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02690v1 Announce Type: new  Abstract: For learning with noisy labels, the transition matrix, which explicitly models the relation between noisy label distribution and clean label distribution, has been utilized to achieve the statistical consistency of either the classifier or the risk. Previous researches have focused more on how to estimate this transition matrix well, rather than how to utilize it. We propose good utilization of the transition matrix is crucial and suggest a new utilization method based on resampling, coined RENT. Specifically, we first demonstrate current utilizations can have potential limitations for implementation. As an extension to Reweighting, we suggest the Dirichlet distribution-based per-sample Weight Sampling (DWS) framework, and compare reweighting and resampling under DWS framework. With the analyses from DWS, we propose RENT, a REsampling method with Noise Transition matrix. Empirically, RENT consistently outperforms existing transition matr
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;DOCTOR&#65292;&#38024;&#23545;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#20013;&#30340;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#12289;&#21407;&#20301;&#20934;&#30830;&#24230;&#24674;&#22797;</title><link>https://arxiv.org/abs/2403.02688</link><description>&lt;p&gt;
DOCTOR: &#38024;&#23545;&#26102;&#38388;&#28418;&#31227;&#28909;&#21464;&#21270;&#30340;&#21160;&#24577;&#33455;&#29255;&#30699;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#25105;&#26657;&#27491;&#30340;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal Variations Toward Self-Corrected Photonic Tensor Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02688
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;DOCTOR&#65292;&#38024;&#23545;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#20013;&#30340;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#12289;&#21407;&#20301;&#20934;&#30830;&#24230;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Photonic computing&#20316;&#20026;&#21152;&#36895;&#35745;&#31639;&#23494;&#38598;&#22411;&#20154;&#24037;&#26234;&#33021;(AI)&#24037;&#20316;&#36127;&#36733;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#12289;&#24310;&#36831;&#25935;&#24863;&#30340;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#36895;&#24230;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#30340;&#37096;&#32626;&#36935;&#21040;&#20102;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#30001;&#20110;&#30828;&#20214;&#22122;&#22768;&#21644;&#29615;&#22659;&#21464;&#21270;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#33073;&#26426;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#21644;&#29255;&#19978;&#35757;&#32451;&#26469;&#22686;&#24378;&#23545;&#20855;&#26377;&#36866;&#24230;&#12289;&#38745;&#24577;&#22122;&#22768;&#30340;&#20809;&#23398;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#21464;&#21270;&#23481;&#24525;&#24230;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#30001;&#20110;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#23548;&#33268;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#38656;&#35201;&#23454;&#26102;&#12289;&#21407;&#20301;&#26657;&#20934;&#26426;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;&#65292;&#31216;&#20026;DOCTOR&#65292;&#25552;&#20379;&#36866;&#24212;&#24615;&#30340;&#12289;&#21407;&#20301;&#30340;&#20934;&#30830;&#24230;&#24674;&#22797;&#65292;&#38024;&#23545;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02688v1 Announce Type: cross  Abstract: Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads, offering unparalleled speed and energy efficiency, especially in resource-limited, latency-sensitive edge computing environments. However, the deployment of analog photonic tensor accelerators encounters reliability challenges due to hardware noises and environmental variations. While off-chip noise-aware training and on-chip training have been proposed to enhance the variation tolerance of optical neural accelerators with moderate, static noises, we observe a notable performance degradation over time due to temporally drifting variations, which requires a real-time, in-situ calibration mechanism. To tackle this challenging reliability issues, for the first time, we propose a lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing adaptive, in-situ accuracy recovery against temporally
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#25512;&#36831;&#23545;&#20154;&#32676;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#36866;&#24212;&#21069;&#25152;&#26410;&#35265;&#30340;&#19987;&#23478;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#38754;&#23545;&#22256;&#38590;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.02683</link><description>&lt;p&gt;
&#23398;&#20064;&#25512;&#36831;&#23545;&#20154;&#32676;&#30340;&#23398;&#20064;&#65306;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Defer to a Population: A Meta-Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02683
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#25512;&#36831;&#23545;&#20154;&#32676;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#36866;&#24212;&#21069;&#25152;&#26410;&#35265;&#30340;&#19987;&#23478;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#38754;&#23545;&#22256;&#38590;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02683v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#23398;&#20064;&#25512;&#36831;&#65288;L2D&#65289;&#26694;&#26550;&#20801;&#35768;&#33258;&#20027;&#31995;&#32479;&#36890;&#36807;&#23558;&#22256;&#38590;&#20915;&#31574;&#22996;&#25176;&#32473;&#20154;&#31867;&#19987;&#23478;&#26469;&#20445;&#25345;&#23433;&#20840;&#21644;&#20581;&#22766;&#12290;&#25152;&#26377;&#29616;&#26377;&#30340;&#20851;&#20110;L2D&#30340;&#24037;&#20316;&#37117;&#20551;&#35774;&#27599;&#20010;&#19987;&#23478;&#37117;&#21487;&#20197;&#24456;&#22909;&#22320;&#30830;&#23450;&#65292;&#24182;&#19988;&#22914;&#26524;&#20219;&#20309;&#19987;&#23478;&#21457;&#29983;&#21464;&#21270;&#65292;&#31995;&#32479;&#24212;&#35813;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20943;&#36731;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;L2D&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#24212;&#23545;&#21069;&#25152;&#26410;&#35265;&#30340;&#19987;&#23478;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#32771;&#34385;&#20102;&#22522;&#20110;&#20248;&#21270;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#21464;&#20307;&#12290;&#32473;&#23450;&#19968;&#20010;&#23567;&#30340;&#19978;&#19979;&#25991;&#38598;&#26469;&#25551;&#36848;&#24403;&#21069;&#21487;&#29992;&#30340;&#19987;&#23478;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#24555;&#36895;&#35843;&#25972;&#23427;&#30340;&#25512;&#36831;&#31574;&#30053;&#12290;&#23545;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#23547;&#25214;&#19978;&#19979;&#25991;&#38598;&#20013;&#19982;&#32473;&#23450;&#27979;&#35797;&#28857;&#30456;&#20284;&#30340;&#28857;&#65292;&#20174;&#32780;&#26356;&#31934;&#30830;&#22320;&#35780;&#20272;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02683v1 Announce Type: new  Abstract: The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert. All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained. In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time. We accomplish this by using meta-learning, considering both optimization- and model-based variants. Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy. For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert's abilities. In the experiments, we validate our methods on image recognition, traffic sign detection, and s
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Time Weaver&#27169;&#22411;&#65292;&#21033;&#29992;&#24322;&#26500;&#20803;&#25968;&#25454;&#25913;&#21892;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#65292;&#24182;&#25351;&#20986;&#26631;&#20934;&#35780;&#20272;&#25351;&#26631;&#30340;&#26420;&#32032;&#25193;&#23637;&#26159;&#19981;&#22815;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.02682</link><description>&lt;p&gt;
&#26102;&#38388;&#32534;&#32455;&#32773;&#65306;&#19968;&#31181;&#26465;&#20214;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Time Weaver: A Conditional Time Series Generation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02682
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Time Weaver&#27169;&#22411;&#65292;&#21033;&#29992;&#24322;&#26500;&#20803;&#25968;&#25454;&#25913;&#21892;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#65292;&#24182;&#25351;&#20986;&#26631;&#20934;&#35780;&#20272;&#25351;&#26631;&#30340;&#26420;&#32032;&#25193;&#23637;&#26159;&#19981;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02682v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#24819;&#35937;&#26681;&#25454;&#22825;&#27668;&#12289;&#30005;&#21160;&#36710;&#23384;&#22312;&#21644;&#20301;&#32622;&#29983;&#25104;&#22478;&#24066;&#30340;&#30005;&#21147;&#38656;&#27714;&#27169;&#24335;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20908;&#23395;&#23506;&#20923;&#26399;&#38388;&#30340;&#23481;&#37327;&#35268;&#21010;&#12290;&#36825;&#20123;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#24120;&#24120;&#21253;&#21547;&#37197;&#23545;&#30340;&#24322;&#26500;&#32972;&#26223;&#20803;&#25968;&#25454;&#65288;&#22825;&#27668;&#12289;&#20301;&#32622;&#31561;&#65289;&#12290;&#24403;&#21069;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#36825;&#20123;&#37197;&#23545;&#30340;&#20803;&#25968;&#25454;&#65292;&#20854;&#24322;&#26500;&#24615;&#32473;&#23558;&#29616;&#26377;&#30340;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#20174;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#39046;&#22495;&#36866;&#24212;&#21040;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;&#24102;&#26469;&#20102;&#20960;&#20010;&#23454;&#38469;&#25361;&#25112;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#38388;&#32534;&#32455;&#32773;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#21033;&#29992;&#24418;&#24335;&#21508;&#24322;&#30340;&#20803;&#25968;&#25454;&#65288;&#20998;&#31867;&#12289;&#36830;&#32493;&#29978;&#33267;&#26102;&#38388;&#21464;&#37327;&#65289;&#26174;&#33879;&#25913;&#21892;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;&#22270;&#20687;&#21040;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#26631;&#20934;&#35780;&#20272;&#25351;&#26631;&#30340;&#26420;&#32032;&#25193;&#23637;&#26159;&#19981;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02682v1 Announce Type: new  Abstract: Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze. Such real-world time series are often enriched with paired heterogeneous contextual metadata (weather, location, etc.). Current approaches to time series generation often ignore this paired metadata, and its heterogeneity poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain. To address this gap, we introduce Time Weaver, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation. Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient. These m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#37096;&#20998;Hessian&#20449;&#24687;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#21644;&#19968;&#38454;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#20248;&#21270;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#31283;&#23450;&#24615;</title><link>https://arxiv.org/abs/2403.02681</link><description>&lt;p&gt;
&#20351;&#29992;&#37096;&#20998;Hessian&#30340;SGD&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SGD with Partial Hessian for Deep Neural Networks Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02681
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#37096;&#20998;Hessian&#20449;&#24687;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#21644;&#19968;&#38454;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#20248;&#21270;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20108;&#38454;&#31639;&#27861;&#22312;&#35299;&#20915;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#35774;&#35745;&#20108;&#38454;&#20248;&#21270;&#22120;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#24456;&#22810;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;DNN&#20013;&#38388;&#29305;&#24449;&#30340;&#38750;&#24120;&#39640;&#32500;&#24230;&#65292;&#30452;&#25509;&#35745;&#31639;&#21644;&#23384;&#20648;Hessian&#30697;&#38453;&#20197;&#36827;&#34892;&#32593;&#32476;&#20248;&#21270;&#26159;&#22256;&#38590;&#30340;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#20108;&#38454;&#26041;&#27861;&#23545;Hessian&#20449;&#24687;&#36827;&#34892;&#19981;&#31934;&#30830;&#36817;&#20284;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#21512;&#20248;&#21270;&#22120;&#65292;&#23427;&#26159;&#19968;&#20010;&#23558;&#20108;&#38454;&#20248;&#21270;&#22120;&#19982;&#29992;&#20110;&#26356;&#26032;&#36890;&#36947;&#21442;&#25968;&#30340;&#31934;&#30830;&#37096;&#20998;Hessian&#30697;&#38453;&#20197;&#21450;&#29992;&#20110;&#26356;&#26032;&#20854;&#20182;&#21442;&#25968;&#30340;&#19968;&#38454;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#20248;&#21270;&#22120;&#30456;&#32467;&#21512;&#30340;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36947;&#21442;&#25968;&#30340;&#30456;&#20851;Hessian&#30697;&#38453;&#26159;&#23545;&#35282;&#32447;&#22411;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#19988;&#31934;&#30830;&#22320;&#20174;&#26080;Hessian&#26041;&#27861;&#20013;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02681v1 Announce Type: new  Abstract: Due to the effectiveness of second-order algorithms in solving classical optimization problems, designing second-order optimizers to train deep neural networks (DNNs) has attracted much research interest in recent years. However, because of the very high dimension of intermediate features in DNNs, it is difficult to directly compute and store the Hessian matrix for network optimization. Most of the previous second-order methods approximate the Hessian information imprecisely, resulting in unstable performance. In this work, we propose a compound optimizer, which is a combination of a second-order optimizer with a precise partial Hessian matrix for updating channel-wise parameters and the first-order stochastic gradient descent (SGD) optimizer for updating the other parameters. We show that the associated Hessian matrices of channel-wise parameters are diagonal and can be extracted directly and precisely from Hessian-free methods. The pro
&lt;/p&gt;</description></item><item><title>KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.02648</link><description>&lt;p&gt;
&#31227;&#38500;&#24179;&#26041;&#26681;&#65306;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26631;&#24230;&#19981;&#21464;&#29256;&#26412;&#30340;AdaGrad
&lt;/p&gt;
&lt;p&gt;
Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02648
&lt;/p&gt;
&lt;p&gt;
KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38477;&#20302;&#23398;&#20064;&#36895;&#29575;&#35843;&#25972;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;KATE&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;AdaGrad&#31639;&#27861;&#30340;&#26631;&#24230;&#19981;&#21464;&#36866;&#24212;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;KATE&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#26696;&#20363;&#20013;&#30340;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;KATE&#24314;&#31435;&#20102;&#19968;&#20010;&#25910;&#25947;&#36895;&#29575;&#20026;$O \left(\frac{\log T}{\sqrt{T}} \right)$&#65292;&#19982;AdaGrad&#21644;Adam&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19981;&#21516;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#23558;KATE&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;Adam&#21644;AdaGrad&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#21040;&#30340;&#22330;&#26223;&#20013;&#65292;KATE&#22987;&#32456;&#32988;&#36807;AdaGrad&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21305;&#37197;/&#36229;&#36234;Adam&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02648v1 Announce Type: cross  Abstract: Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}} \right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;5G&#32593;&#32476;&#20013;&#26816;&#27979;&#24178;&#25200;&#32773;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#38408;&#20540;&#28145;&#24230;&#23398;&#20064;&#24178;&#25200;&#26816;&#27979;&#22120;&#65292;&#19987;&#27880;&#20110;SSB&#30340;RF&#39046;&#22495;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02645</link><description>&lt;p&gt;
&#22312;5G RF&#39046;&#22495;&#65292;&#29992;&#20110;&#24178;&#25200;&#26816;&#27979;&#30340;&#31354;&#20013;&#21452;&#38408;&#20540;&#28145;&#24230;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Over-The-Air Double-Threshold Deep Learner for Jamming Detection in 5G RF domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;5G&#32593;&#32476;&#20013;&#26816;&#27979;&#24178;&#25200;&#32773;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#38408;&#20540;&#28145;&#24230;&#23398;&#20064;&#24178;&#25200;&#26816;&#27979;&#22120;&#65292;&#19987;&#27880;&#20110;SSB&#30340;RF&#39046;&#22495;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;5G&#26080;&#32447;&#36890;&#20449;&#30340;&#21457;&#23637;&#65292;&#21516;&#27493;&#20449;&#21495;&#22359;&#65288;SSB&#65289;&#22312;&#35774;&#22791;&#21516;&#27493;&#21644;&#26381;&#21153;&#21487;&#35775;&#38382;&#24615;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SSB&#20256;&#36755;&#20855;&#26377;&#21487;&#39044;&#27979;&#24615;&#65292;&#21253;&#25324;&#20027;&#35201;&#21516;&#27493;&#20449;&#21495;&#65288;PSS&#65289;&#21644;&#27425;&#35201;&#21516;&#27493;&#20449;&#21495;&#65288;SSS&#65289;&#65292;&#24178;&#25200;&#25915;&#20987;&#26159;&#37325;&#35201;&#23041;&#32961;&#12290;&#26412;&#25991;&#21033;&#29992;RF&#39046;&#22495;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;5G&#32593;&#32476;&#24178;&#25200;&#26816;&#27979;&#25216;&#26415;&#12290;&#19982;&#29616;&#26377;&#30340;&#22823;&#22810;&#20381;&#36182;&#32593;&#32476;&#21442;&#25968;&#30340;&#24178;&#25200;&#26816;&#27979;&#31639;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#27880;&#20110;SSB&#24341;&#20837;&#20102;&#21452;&#38408;&#20540;&#28145;&#24230;&#23398;&#20064;&#24178;&#25200;&#26816;&#27979;&#22120;&#12290;&#35813;&#26816;&#27979;&#26041;&#27861;&#20391;&#37325;&#20110;RF&#39046;&#22495;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#19982;&#29616;&#26377;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#19968;&#20010;&#39044;&#22788;&#29702;&#22359;&#26469;&#25552;&#21462;PSS&#30456;&#20851;&#24615;&#21644;&#27599;&#20010;&#31354;&#38386;&#36164;&#28304;&#20803;&#32032;&#30340;&#33021;&#37327;&#65288;EPNRE&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02645v1 Announce Type: cross  Abstract: With the evolution of 5G wireless communications, the Synchronization Signal Block (SSB) plays a critical role in the synchronization of devices and accessibility of services. However, due to the predictable nature of SSB transmission, including the Primary and Secondary Synchronization Signals (PSS and SSS), jamming attacks are critical threats. By leveraging RF domain knowledge, this work presents a novel deep learning-based technique for detecting jammers in 5G networks. Unlike the existing jamming detection algorithms that mostly rely on network parameters, we introduce a double threshold deep learning jamming detector by focusing on the SSB. The detection method is focused on RF domain features and improves the robustness of the network without requiring integration with the pre-existing network infrastructure. By integrating a preprocessing block that extracts PSS correlation and energy per null resource elements (EPNRE) characte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#26032;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#34987;&#35782;&#21035;&#20026;&#34394;&#20551;&#38451;&#24615;&#30340;&#28857;&#20113;&#65292;&#20197;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02639</link><description>&lt;p&gt;
&#22522;&#20110;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#26032;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#34987;&#35782;&#21035;&#20026;&#34394;&#20551;&#38451;&#24615;&#30340;&#28857;&#20113;&#65292;&#20197;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25552;&#21319;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#19978;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#37319;&#26679;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#26377;&#38480;&#30340;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#37319;&#26679;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#26159;&#20854;&#20542;&#21521;&#22686;&#21152;&#34394;&#20551;&#38451;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20811;&#26381;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#37319;&#26679;&#30340;&#23616;&#38480;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21517;&#20026;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#26032;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#28041;&#21450;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#29992;&#22312;&#27169;&#22411;&#39044;&#27979;&#20013;&#34987;&#35782;&#21035;&#20026;&#34394;&#20551;&#38451;&#24615;&#30340;&#28857;&#20113;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21516;&#26102;&#21033;&#29992;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#21644;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#20197;&#21450;&#19968;&#20010;&#24314;&#31435;&#34394;&#20551;&#38451;&#24615;&#26679;&#26412;&#25968;&#25454;&#24211;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#30001;&#20110;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#23548;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02639v1 Announce Type: cross  Abstract: Recent studies have focused on enhancing the performance of 3D object detection models. Among various approaches, ground-truth sampling has been proposed as an augmentation technique to address the challenges posed by limited ground-truth data. However, an inherent issue with ground-truth sampling is its tendency to increase false positives. Therefore, this study aims to overcome the limitations of ground-truth sampling and improve the performance of 3D object detection models by developing a new augmentation technique called false-positive sampling. False-positive sampling involves retraining the model using point clouds that are identified as false positives in the model's predictions. We propose an algorithm that utilizes both ground-truth and false-positive sampling and an algorithm for building the false-positive sample database. Additionally, we analyze the principles behind the performance enhancement due to false-positive sampl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02630</link><description>&lt;p&gt;
FedHCDR: &#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#22791;&#21463;&#20851;&#27880;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#25968;&#25454;&#26469;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;CDR&#26041;&#27861;&#38656;&#35201;&#36328;&#39046;&#22495;&#20849;&#20139;&#29992;&#25143;&#25968;&#25454;&#65292;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#12290;&#22240;&#27492;&#65292;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;FedCDR&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedHCDR&#65292;&#19968;&#31181;&#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#65288;HSD&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#29305;&#24449;&#35299;&#32806;&#20026;&#39046;&#22495;&#29420;&#26377;&#21644;&#39046;&#22495;&#20849;&#20139;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#39640;&#36890;&#21644;&#20302;&#36890;&#36229;&#22270;&#28388;&#27874;&#22120;&#26469;&#36827;&#34892;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02630v1 Announce Type: new  Abstract: In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning. In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20219;&#21153;&#25512;&#23548;&#21644;&#20869;&#23384;&#26816;&#32034;&#12290;</title><link>https://arxiv.org/abs/2403.02628</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#25345;&#32493;&#23398;&#20064;: &#24555;&#36895;&#19982;&#32531;&#24930;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Interactive Continual Learning: Fast and Slow Thinking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20219;&#21153;&#25512;&#23548;&#21644;&#20869;&#23384;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#29983;&#21629;&#24418;&#24335;&#36890;&#36807;&#31070;&#32463;&#35748;&#30693;&#26426;&#21046;&#30340;&#21327;&#21516;&#20114;&#21160;&#65292;&#32456;&#36523;&#19981;&#26029;&#22320;&#33719;&#21462;&#21644;&#20256;&#36882;&#30693;&#35782;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#22312;&#27169;&#25311;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#36890;&#36807;&#19982;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#26412;&#25991;&#22522;&#20110;&#34917;&#20805;&#23398;&#20064;&#31995;&#32479;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#25345;&#32493;&#23398;&#20064;&#65288;ICL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21508;&#31181;&#35268;&#27169;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#20132;&#20114;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ViT&#27169;&#22411;&#25351;&#23450;&#20026;&#31532;&#19968;&#31995;&#32479;&#65292;&#23558;&#22810;&#27169;&#24577;LLM&#25351;&#23450;&#20026;&#31532;&#20108;&#31995;&#32479;&#12290;&#20026;&#20102;&#20351;&#20869;&#23384;&#27169;&#22359;&#33021;&#22815;&#20174;&#31867;&#20449;&#24687;&#20013;&#25512;&#23548;&#20219;&#21153;&#24182;&#22686;&#24378;Set2Set&#26816;&#32034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Class-Knowledge-Task Multi-Head Attention (CKT-MHA)&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36890;&#36807;&#22686;&#24378;&#30340;&#20960;&#20309;&#26816;&#32034;&#25913;&#36827;&#31532;&#19968;&#31995;&#32479;&#30340;&#20869;&#23384;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02628v1 Announce Type: cross  Abstract: Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21462;&#20195;&#20154;&#24037;&#26631;&#27880;&#65292;&#23558;&#23450;&#20041;&#35270;&#35273;&#27010;&#24565;&#25152;&#38656;&#30340;&#20154;&#21147;&#25237;&#20837;&#20943;&#23569;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;</title><link>https://arxiv.org/abs/2403.02626</link><description>&lt;p&gt;
&#21033;&#29992;LLM&#24037;&#20855;&#23454;&#29616;&#20027;&#35266;&#35270;&#35273;&#20998;&#31867;&#30340;&#21327;&#20316;&#24314;&#27169;:&#20943;&#23569;&#20154;&#21147;&#25237;&#20837;
&lt;/p&gt;
&lt;p&gt;
Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02626
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21462;&#20195;&#20154;&#24037;&#26631;&#27880;&#65292;&#23558;&#23450;&#20041;&#35270;&#35273;&#27010;&#24565;&#25152;&#38656;&#30340;&#20154;&#21147;&#25237;&#20837;&#20943;&#23569;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20869;&#23481;&#23457;&#26680;&#21040;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#65292;&#38656;&#35201;&#27169;&#22411;&#35782;&#21035;&#24494;&#22937;&#25110;&#20027;&#35266;&#35270;&#35273;&#27010;&#24565;&#30340;&#24212;&#29992;&#25968;&#37327;&#27491;&#22312;&#22686;&#21152;&#12290;&#20256;&#32479;&#19978;&#65292;&#20026;&#36825;&#31867;&#27010;&#24565;&#24320;&#21457;&#20998;&#31867;&#22120;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#65292;&#38656;&#35201;&#20197;&#26102;&#12289;&#22825;&#29978;&#33267;&#26376;&#26469;&#27979;&#37327;&#35782;&#21035;&#21644;&#27880;&#37322;&#35757;&#32451;&#25152;&#38656;&#25968;&#25454;&#12290;&#21363;&#20351;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#25935;&#25463;&#24314;&#27169;&#25216;&#26415;&#65292;&#21487;&#20197;&#24555;&#36895;&#24341;&#23548;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#65292;&#29992;&#25143;&#20173;&#38656;&#35201;&#33457;&#36153;30&#20998;&#38047;&#29978;&#33267;&#26356;&#22810;&#30340;&#21333;&#35843;&#37325;&#22797;&#25968;&#25454;&#26631;&#27880;&#26102;&#38388;&#26469;&#35757;&#32451;&#21333;&#20010;&#20998;&#31867;&#22120;&#12290;&#20511;&#37492;&#33778;&#26031;&#20811;&#30340;&#35748;&#30693;&#25042;&#27721;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21462;&#20195;&#20154;&#24037;&#26631;&#27880;&#65292;&#20943;&#23569;&#23450;&#20041;&#27010;&#24565;&#25152;&#38656;&#30340;&#24635;&#20307;&#25237;&#20837;&#19968;&#20010;&#25968;&#37327;&#32423;&#65306;&#20174;&#26631;&#35760;2,000&#24352;&#22270;&#20687;&#21040;&#20165;&#38656;100&#24352;&#22270;&#20687;&#20877;&#21152;&#19978;&#19968;&#20123;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02626v1 Announce Type: cross  Abstract: From content moderation to wildlife conservation, the number of applications that require models to recognize nuanced or subjective visual concepts is growing. Traditionally, developing classifiers for such concepts requires substantial manual effort measured in hours, days, or even months to identify and annotate data needed for training. Even with recently proposed Agile Modeling techniques, which enable rapid bootstrapping of image classifiers, users are still required to spend 30 minutes or more of monotonous, repetitive data labeling just to train a single classifier. Drawing on Fiske's Cognitive Miser theory, we propose a new framework that alleviates manual effort by replacing human labeling with natural language interactions, reducing the total effort required to define a concept by an order of magnitude: from labeling 2,000 images to only 100 plus some natural language interactions. Our framework leverages recent advances in f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#22914;&#20309;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#27835;&#30103;&#25928;&#26524;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#20174;&#32780;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#12290;</title><link>https://arxiv.org/abs/2403.02624</link><description>&lt;p&gt;
Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects
&lt;/p&gt;
&lt;p&gt;
Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#22914;&#20309;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#27835;&#30103;&#25928;&#26524;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#20174;&#32780;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#19987;&#27880;&#20110;&#21457;&#23637;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#65292;&#20197;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#26469;&#33258;&#30701;&#26399;&#21644;&#38271;&#26399;&#25928;&#26524;&#30340;&#24635;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#30456;&#20114;&#20914;&#31361;&#12290; &#20363;&#22914;&#65292;&#33647;&#29289;&#21058;&#37327;&#30340;&#22686;&#21152;&#21487;&#33021;&#20250;&#25552;&#39640;&#24739;&#32773;&#24247;&#22797;&#36895;&#24230;&#65288;&#30701;&#26399;&#65289;&#65292;&#20294;&#20063;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#38271;&#26399;&#21103;&#20316;&#29992;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#26377;&#20851;&#30701;&#26399;&#25110;&#38271;&#26399;&#25928;&#24212;&#25110;&#20004;&#32773;&#30340;&#38382;&#39064;&#65292;&#20294;&#22914;&#20309;&#22312;&#23427;&#20204;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#20197;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#20256;&#32479;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30452;&#25509;&#20272;&#35745;&#22810;&#20010;&#30446;&#26631;&#26102;&#65292;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#30340;&#20248;&#21270;&#26041;&#21521;&#20063;&#21487;&#33021;&#21457;&#29983;&#20914;&#31361;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24085;&#32047;&#25176;&#26377;&#25928;&#31639;&#27861;&#65292;&#21253;&#25324;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#65288;POE&#65289;&#21644;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#65288;POPL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02624v1 Announce Type: cross  Abstract: This paper focuses on developing Pareto-optimal estimation and policy learning to identify the most effective treatment that maximizes the total reward from both short-term and long-term effects, which might conflict with each other. For example, a higher dosage of medication might increase the speed of a patient's recovery (short-term) but could also result in severe long-term side effects. Although recent works have investigated the problems about short-term or long-term effects or the both, how to trade-off between them to achieve optimal treatment remains an open challenge. Moreover, when multiple objectives are directly estimated using conventional causal representation learning, the optimization directions among various tasks can conflict as well. In this paper, we systematically investigate these issues and introduce a Pareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and Pareto-Optimal Policy Learning (POPL
&lt;/p&gt;</description></item><item><title>&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02622</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#39033;&#21021;&#27493;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
World Models for Autonomous Driving: An Initial Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02622
&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#35780;&#20272;&#20854;&#24433;&#21709;&#23545;&#20110;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#20851;&#38190;&#22320;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#12290;&#19990;&#30028;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#20351;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#33021;&#22815;&#32508;&#21512;&#21644;&#35299;&#37322;&#22823;&#37327;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20174;&#32780;&#39044;&#27979;&#28508;&#22312;&#30340;&#26410;&#26469;&#24773;&#26223;&#24182;&#24357;&#34917;&#20449;&#24687;&#32570;&#21475;&#12290;&#26412;&#25991;&#23545;&#33258;&#20027;&#39550;&#39542;&#20013;&#19990;&#30028;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#21457;&#23637;&#36827;&#34892;&#20102;&#21021;&#27493;&#23457;&#26597;&#65292;&#28085;&#30422;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#24212;&#29992;&#20197;&#21450;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#38480;&#21046;&#30340;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#24378;&#35843;&#20102;&#19990;&#30028;&#27169;&#22411;&#22312;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#25104;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#22522;&#30784;&#21442;&#32771;&#65292;&#20415;&#20110;&#24555;&#36895;&#33719;&#24471;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02622v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and com
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#28145;&#20837;&#25506;&#35752;&#20102;&#36793;&#32536;&#23398;&#20064;(EL)&#20013;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#26088;&#22312;&#32508;&#21512;&#29616;&#26377;&#30693;&#35782;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02619</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Training Machine Learning models at the Edge: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02619
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#28145;&#20837;&#25506;&#35752;&#20102;&#36793;&#32536;&#23398;&#20064;(EL)&#20013;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#26088;&#22312;&#32508;&#21512;&#29616;&#26377;&#30693;&#35782;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;(EC)&#36817;&#24180;&#26469;&#33719;&#24471;&#20102;&#26174;&#33879;&#20851;&#27880;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;(AI)&#33021;&#21147;&#65292;&#25215;&#35834;&#25552;&#39640;&#25928;&#29575;&#12290;&#34429;&#28982;&#20027;&#35201;&#20851;&#27880;&#28857;&#22312;&#36793;&#32536;&#37096;&#32626;&#21644;&#25512;&#26029;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#65292;&#20294;&#35757;&#32451;&#26041;&#38754;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#35752;&#12290;&#36825;&#39033;&#35843;&#30740;&#28145;&#20837;&#25506;&#35752;&#20102;&#36793;&#32536;&#23398;&#20064;(EL)&#65292;&#29305;&#21035;&#26159;&#22312;&#36793;&#32536;&#20248;&#21270;ML&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#38754;&#12290;&#20854;&#30446;&#26631;&#26159;&#20840;&#38754;&#25506;&#35752;EL&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#32508;&#21512;&#29616;&#26377;&#30693;&#35782;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#26410;&#26469;&#36235;&#21183;&#12290;&#21033;&#29992;Scopus&#30340;&#39640;&#32423;&#25628;&#32034;&#65292;&#30830;&#23450;&#20102;&#20851;&#20110;EL&#30340;&#30456;&#20851;&#25991;&#29486;&#65292;&#26174;&#31034;&#20102;&#30740;&#31350;&#24037;&#20316;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26041;&#38754;&#30340;&#32858;&#28966;&#65292;&#29305;&#21035;&#26159;&#32852;&#37030;&#23398;&#20064;(FL)&#12290;&#27492;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#27604;&#36739;&#29992;&#20110;&#20248;&#21270;&#36793;&#32536;&#23398;&#20064;&#30340;ML&#30340;&#25216;&#26415;&#30340;&#25351;&#21335;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#26694;&#26550;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02619v1 Announce Type: new  Abstract: Edge Computing (EC) has gained significant traction in recent years, promising enhanced efficiency by integrating Artificial Intelligence (AI) capabilities at the edge. While the focus has primarily been on the deployment and inference of Machine Learning (ML) models at the edge, the training aspect remains less explored. This survey delves into Edge Learning (EL), specifically the optimization of ML model training at the edge. The objective is to comprehensively explore diverse approaches and methodologies in EL, synthesize existing knowledge, identify challenges, and highlight future trends. Utilizing Scopus' advanced search, relevant literature on EL was identified, revealing a concentration of research efforts in distributed learning methods, particularly Federated Learning (FL). This survey further provides a guideline for comparing techniques used to optimize ML for edge learning, along with an exploration of different frameworks, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#33258;&#36866;&#24212;&#24322;&#24120;&#35786;&#26029;&#26041;&#27861;&#65288;MAD-Transformer&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#26102;&#38388;&#29366;&#24577;&#30697;&#38453;&#21644;&#31354;&#38388;&#29366;&#24577;&#30697;&#38453;&#26469;&#25581;&#31034;&#24037;&#19994;CPS&#24037;&#20316;&#29366;&#24577;&#30340;&#26102;&#31354;&#20851;&#32852;&#20851;&#31995;&#21644;&#28436;&#21464;&#26426;&#21046;</title><link>https://arxiv.org/abs/2403.02616</link><description>&lt;p&gt;
&#24037;&#19994;&#29289;&#29702;&#31995;&#32479;&#32454;&#31890;&#24230;&#33258;&#36866;&#24212;&#24322;&#24120;&#35786;&#26029;&#30340;&#26080;&#30417;&#30563;&#26102;&#31354;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive Anomaly Diagnosis of Industrial Cyber-physical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02616
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#33258;&#36866;&#24212;&#24322;&#24120;&#35786;&#26029;&#26041;&#27861;&#65288;MAD-Transformer&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#26102;&#38388;&#29366;&#24577;&#30697;&#38453;&#21644;&#31354;&#38388;&#29366;&#24577;&#30697;&#38453;&#26469;&#25581;&#31034;&#24037;&#19994;CPS&#24037;&#20316;&#29366;&#24577;&#30340;&#26102;&#31354;&#20851;&#32852;&#20851;&#31995;&#21644;&#28436;&#21464;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#26816;&#27979;&#21644;&#35786;&#26029;&#24322;&#24120;&#34892;&#20026;&#65292;&#22914;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#20013;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#23545;&#20110;&#30830;&#20445;&#24037;&#19994;&#29289;&#29702;&#31995;&#32479;&#65288;CPS&#65289;&#30340;&#31283;&#23450;&#26377;&#25928;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24456;&#23569;&#20851;&#27880;&#31995;&#32479;&#24037;&#20316;&#29366;&#24577;&#20043;&#38388;&#30340;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#38590;&#20197;&#35299;&#37322;&#24322;&#24120;&#20449;&#21495;&#30340;&#28436;&#21464;&#26426;&#21046;&#12290;&#20026;&#20102;&#25581;&#31034;&#24037;&#19994;CPS&#24037;&#20316;&#29366;&#24577;&#30340;&#26102;&#31354;&#20851;&#32852;&#20851;&#31995;&#21644;&#28436;&#21464;&#26426;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#33258;&#36866;&#24212;&#24322;&#24120;&#35786;&#26029;&#26041;&#27861;&#65288;&#21363;MAD-Transformer&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35786;&#26029;MTS&#20013;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02616v1 Announce Type: cross  Abstract: Accurate detection and diagnosis of abnormal behaviors such as network attacks from multivariate time series (MTS) are crucial for ensuring the stable and effective operation of industrial cyber-physical systems (CPS). However, existing researches pay little attention to the logical dependencies among system working states, and have difficulties in explaining the evolution mechanisms of abnormal signals. To reveal the spatio-temporal association relationships and evolution mechanisms of the working states of industrial CPS, this paper proposes a fine-grained adaptive anomaly diagnosis method (i.e. MAD-Transformer) to identify and diagnose anomalies in MTS. MAD-Transformer first constructs a temporal state matrix to characterize and estimate the change patterns of the system states in the temporal dimension. Then, to better locate the anomalies, a spatial state matrix is also constructed to capture the inter-sensor state correlation rel
&lt;/p&gt;</description></item><item><title>&#20010;&#24615;&#21270;&#25628;&#32034;&#24847;&#22270;&#32593;&#32476;&#35299;&#20915;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#38754;&#20020;&#30340;&#24847;&#22270;&#27169;&#31946;&#24615;&#21644;&#24847;&#22270;&#36716;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02609</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;&#30340;&#25628;&#32034;&#24847;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Search Intenion Network for Personalized Query Auto-Completion in E-Commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02609
&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25628;&#32034;&#24847;&#22270;&#32593;&#32476;&#35299;&#20915;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#38754;&#20020;&#30340;&#24847;&#22270;&#27169;&#31946;&#24615;&#21644;&#24847;&#22270;&#36716;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;&#65288;QAC&#65289;&#20316;&#20026;&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#34917;&#20805;&#29992;&#25143;&#26597;&#35810;&#24182;&#24110;&#21161;&#20182;&#20204;&#32454;&#21270;&#25628;&#32034;&#24847;&#22270;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;QAC&#31995;&#32479;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#24847;&#22270;&#27169;&#31946;&#24615;&#65288;IE&#65289;&#65306;&#22312;&#29992;&#25143;&#36755;&#20837;&#36807;&#31243;&#20013;&#65292;&#21069;&#32512;&#36890;&#24120;&#21253;&#21547;&#23383;&#31526;&#21644;&#23376;&#35789;&#30340;&#32452;&#21512;&#65292;&#36825;&#20351;&#24471;&#24403;&#21069;&#24847;&#22270;&#27169;&#31946;&#19988;&#38590;&#20197;&#24314;&#27169;&#12290;2&#65289;&#24847;&#22270;&#36716;&#31227;&#65288;IT&#65289;&#65306;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#20110;&#29992;&#25143;&#30340;&#21382;&#21490;&#24207;&#21015;&#25552;&#20379;&#24314;&#35758;&#65292;&#20294;&#24573;&#30053;&#20102;&#25628;&#32034;&#24847;&#22270;&#30340;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#20174;&#21069;&#32512;&#25552;&#21462;&#30340;&#24403;&#21069;&#24847;&#22270;&#21487;&#33021;&#19982;&#21382;&#21490;&#20559;&#22909;&#30456;&#24726;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02609v1 Announce Type: cross  Abstract: Query Auto-Completion(QAC), as an important part of the modern search engine, plays a key role in complementing user queries and helping them refine their search intentions.Today's QAC systems in real-world scenarios face two major challenges:1)intention equivocality(IE): during the user's typing process,the prefix often contains a combination of characters and subwords, which makes the current intention ambiguous and difficult to model.2)intention transfer (IT):previous works make personalized recommendations based on users' historical sequences, but ignore the search intention transfer.However, the current intention extracted from prefix may be contrary to the historical preferences.
&lt;/p&gt;</description></item><item><title>DNNLasso&#26159;&#19968;&#31181;&#23545;&#35282;&#38750;&#36127;&#22270;&#24418;&#22871;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;Kronecker-&#21644;&#32467;&#26500;&#30340;&#31934;&#24230;&#30697;&#38453;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02608</link><description>&lt;p&gt;
DNNLasso&#65306;&#38754;&#21521;&#30697;&#38453;&#21464;&#37327;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DNNLasso: Scalable Graph Learning for Matrix-Variate Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02608
&lt;/p&gt;
&lt;p&gt;
DNNLasso&#26159;&#19968;&#31181;&#23545;&#35282;&#38750;&#36127;&#22270;&#24418;&#22871;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;Kronecker-&#21644;&#32467;&#26500;&#30340;&#31934;&#24230;&#30697;&#38453;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#32852;&#21512;&#23398;&#20064;&#30697;&#38453;&#21464;&#37327;&#35266;&#27979;&#30340;&#34892;&#21644;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#35266;&#27979;&#20998;&#21035;&#30001;&#20004;&#20010;&#31934;&#24230;&#30697;&#38453;&#21333;&#29420;&#24314;&#27169;&#12290;&#30001;&#20110;&#24120;&#29992;&#30697;&#38453;&#21464;&#37327;&#39640;&#26031;&#22270;&#27169;&#22411;&#20013;Kronecker&#20056;&#31215;&#31934;&#24230;&#30697;&#38453;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#31232;&#30095;&#30340;Kronecker&#21644;&#32467;&#26500;&#65292;&#22522;&#20110;&#22270;&#30340;&#31515;&#21345;&#23572;&#20056;&#31215;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#20272;&#35745;Kronecker&#21644;&#32467;&#26500;&#31934;&#24230;&#30697;&#38453;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#25193;&#23637;&#24615;&#24182;&#19981;&#22909;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DNNLasso&#65292;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;Kronecker&#21644;&#32467;&#26500;&#31934;&#24230;&#30697;&#38453;&#30340;&#23545;&#35282;&#38750;&#36127;&#22270;&#24418;&#22871;&#32034;&#27169;&#22411;&#65292;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#37117;&#22823;&#24133;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/YangjingZhang/DNNLasso&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02608v1 Announce Type: new  Abstract: We consider the problem of jointly learning row-wise and column-wise dependencies of matrix-variate observations, which are modelled separately by two precision matrices. Due to the complicated structure of Kronecker-product precision matrices in the commonly used matrix-variate Gaussian graphical models, a sparser Kronecker-sum structure was proposed recently based on the Cartesian product of graphs. However, existing methods for estimating Kronecker-sum structured precision matrices do not scale well to large scale datasets. In this paper, we introduce DNNLasso, a diagonally non-negative graphical lasso model for estimating the Kronecker-sum structured precision matrix, which outperforms the state-of-the-art methods by a large margin in both accuracy and computational time. Our code is available at https://github.com/YangjingZhang/DNNLasso.
&lt;/p&gt;</description></item><item><title>TESTAM&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#28151;&#21512;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#24182;&#19988;&#38024;&#23545;&#26102;&#38388;&#24314;&#27169;&#12289;&#38745;&#24577;&#22270;&#30340;&#26102;&#31354;&#24314;&#27169;&#20197;&#21450;&#21160;&#24577;&#22270;&#30340;&#21160;&#24577;&#26102;&#31354;&#20381;&#36182;&#24314;&#27169;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#30340;&#21508;&#31181;&#27169;&#24335;&#21644;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.02600</link><description>&lt;p&gt;
TESTAM: &#19968;&#31181;&#20855;&#26377;&#26102;&#38388;&#22686;&#24378;&#30340;&#26102;&#31354;&#27880;&#24847;&#21147;&#27169;&#22411;&#19982;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02600
&lt;/p&gt;
&lt;p&gt;
TESTAM&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#28151;&#21512;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#24182;&#19988;&#38024;&#23545;&#26102;&#38388;&#24314;&#27169;&#12289;&#38745;&#24577;&#22270;&#30340;&#26102;&#31354;&#24314;&#27169;&#20197;&#21450;&#21160;&#24577;&#22270;&#30340;&#21160;&#24577;&#26102;&#31354;&#20381;&#36182;&#24314;&#27169;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#30340;&#21508;&#31181;&#27169;&#24335;&#21644;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#22240;&#23545;&#36335;&#32593;&#30340;&#22797;&#26434;&#20381;&#36182;&#12289;&#21508;&#31181;&#31867;&#22411;&#30340;&#36947;&#36335;&#20197;&#21450;&#30001;&#20107;&#20214;&#36896;&#25104;&#30340;&#36895;&#24230;&#31361;&#21464;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#20855;&#26377;&#33258;&#36866;&#24212;&#22270;&#23884;&#20837;&#25110;&#22270;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#23545;&#20110;&#26102;&#38388;&#29305;&#24615;&#21644;&#23601;&#22320;&#24314;&#27169;&#30340;&#32771;&#34385;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TESTAM&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#19977;&#20010;&#19987;&#23478;&#27169;&#22411;&#65288;&#20998;&#21035;&#22788;&#29702;&#26102;&#38388;&#24314;&#27169;&#12289;&#20855;&#26377;&#38745;&#24577;&#22270;&#30340;&#26102;&#31354;&#24314;&#27169;&#21644;&#20855;&#26377;&#21160;&#24577;&#22270;&#30340;&#21160;&#24577;&#26102;&#31354;&#20381;&#36182;&#24314;&#27169;&#65289;&#20998;&#21035;&#23545;&#21608;&#26399;&#24615;&#21644;&#38750;&#21608;&#26399;&#24615;&#20132;&#36890;&#27169;&#24335;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#19987;&#23478;&#24182;&#36866;&#24403;&#36335;&#30001;&#23427;&#20204;&#65292;TESTAM&#33021;&#26356;&#22909;&#22320;&#27169;&#25311;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#31354;&#38388;&#38548;&#31163;&#33410;&#28857;&#12289;&#39640;&#24230;&#30456;&#20851;&#33410;&#28857;&#20197;&#21450;&#24490;&#29615;&#21644;&#38750;&#24490;&#29615;&#20107;&#20214;&#12290;&#20026;&#20102;&#36827;&#34892;&#36866;&#24403;&#30340;&#36335;&#30001;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#38376;&#25511;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02600v1 Announce Type: new  Abstract: Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events. Recent works mainly focus on dynamic spatial modeling with adaptive graph embedding or graph attention having less consideration for temporal characteristics and in-situ modeling. In this paper, we propose a novel deep learning model named TESTAM, which individually models recurring and non-recurring traffic patterns by a mixture-of-experts model with three experts on temporal modeling, spatio-temporal modeling with static graph, and dynamic spatio-temporal dependency modeling with dynamic graph. By introducing different experts and properly routing them, TESTAM could better model various circumstances, including spatially isolated nodes, highly related nodes, and recurring and non-recurring events. For the proper routing, we reformulate a gating problem into a classification p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.02598</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#20010;&#21327;&#21464;&#37327;&#36716;&#31227;&#21644;&#19981;&#24179;&#34913;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Pooling Image Datasets With Multiple Covariate Shift and Imbalance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#31185;&#20013;&#24120;&#35265;&#23567;&#26679;&#26412;&#22823;&#23567;&#65292;&#36825;&#38656;&#35201;&#36328;&#22810;&#20010;&#26426;&#26500;&#27719;&#24635;&#22823;&#33268;&#30456;&#20284;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#22270;&#20687;&#19982;&#30142;&#30149;&#32467;&#26524;&#20043;&#38388;&#30340;&#24369;&#20294;&#30456;&#20851;&#20851;&#32852;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#20307;&#29616;&#20986;&#21327;&#21464;&#37327;&#65288;&#21363;&#27425;&#35201;&#30340;&#38750;&#25104;&#20687;&#25968;&#25454;&#65289;&#30340;&#36716;&#31227;/&#19981;&#24179;&#34913;&#12290;&#22312;&#26631;&#20934;&#32479;&#35745;&#20998;&#26512;&#20013;&#25511;&#21046;&#36825;&#20123;&#26080;&#29992;&#21464;&#37327;&#26159;&#24120;&#35265;&#30340;&#65292;&#20294;&#36825;&#20123;&#24605;&#24819;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#21442;&#25968;&#36807;&#22810;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20174;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#36215;&#28857;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#24211;&#20165;&#38480;&#20110;&#19968;&#27425;&#32771;&#34385;&#20960;&#20010;&#21327;&#21464;&#37327;&#30340;&#36716;&#31227;/&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#30475;&#24453;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#21407;&#26412;&#38656;&#35201;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02598v1 Announce Type: new  Abstract: Small sample sizes are common in many disciplines, which necessitates pooling roughly similar datasets across multiple institutions to study weak but relevant associations between images and disease outcomes. Such data often manifest shift/imbalance in covariates (i.e., secondary non-imaging data). Controlling for such nuisance variables is common within standard statistical analysis, but the ideas do not directly apply to overparameterized models. Consequently, recent work has shown how strategies from invariant representation learning provides a meaningful starting point, but the current repertoire of methods is limited to accounting for shifts/imbalances in just a couple of covariates at a time. In this paper, we show how viewing this problem from the perspective of Category theory provides a simple and effective solution that completely avoids elaborate multi-stage training pipelines that would otherwise be needed. We show the effect
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#36716;CLIP&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#25351;&#23450;&#30446;&#26631;&#25552;&#31034;&#35821;&#20041;&#23545;&#40784;&#65292;&#25581;&#31034;&#20102;CLIP&#27169;&#22411;&#30340;&#28151;&#21512;&#27010;&#24565;&#33021;&#21147;&#12289;&#24615;&#21035;&#20559;&#35265;&#20197;&#21450;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#23433;&#20840;&#20869;&#23481;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.02580</link><description>&lt;p&gt;
&#20174;&#21453;&#36716;CLIP&#27169;&#22411;&#20013;&#25105;&#20204;&#23398;&#21040;&#20102;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What do we learn from inverting CLIP models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02580
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#36716;CLIP&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#25351;&#23450;&#30446;&#26631;&#25552;&#31034;&#35821;&#20041;&#23545;&#40784;&#65292;&#25581;&#31034;&#20102;CLIP&#27169;&#22411;&#30340;&#28151;&#21512;&#27010;&#24565;&#33021;&#21147;&#12289;&#24615;&#21035;&#20559;&#35265;&#20197;&#21450;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#23433;&#20840;&#20869;&#23481;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#22522;&#20110;&#21453;&#36716;&#30340;&#26041;&#27861;&#26469;&#26816;&#39564;CLIP&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26816;&#39564;&#25581;&#31034;&#65292;&#21453;&#36716;CLIP&#27169;&#22411;&#20250;&#29983;&#25104;&#23637;&#29616;&#19982;&#25351;&#23450;&#30446;&#26631;&#25552;&#31034;&#35821;&#20041;&#23545;&#40784;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#21453;&#36716;&#22270;&#20687;&#26469;&#28145;&#20837;&#20102;&#35299;CLIP&#27169;&#22411;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#27604;&#22914;&#23427;&#20204;&#28151;&#21512;&#27010;&#24565;&#30340;&#33021;&#21147;&#21644;&#21253;&#21547;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#29305;&#21035;&#35266;&#23519;&#21040;&#22312;&#27169;&#22411;&#21453;&#36716;&#36807;&#31243;&#20013;&#20986;&#29616;&#19981;&#23433;&#20840;&#20869;&#23481;&#65288;NSFW&#65289;&#30340;&#22270;&#20687;&#23454;&#20363;&#12290;&#36825;&#31181;&#29616;&#35937;&#29978;&#33267;&#21457;&#29983;&#22312;&#35821;&#20041;&#19978;&#26080;&#23475;&#30340;&#25552;&#31034;&#65292;&#27604;&#22914;&#8220;&#32654;&#20029;&#30340;&#39118;&#26223;&#8221;&#65292;&#20197;&#21450;&#28041;&#21450;&#21517;&#20154;&#22995;&#21517;&#30340;&#25552;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02580v1 Announce Type: cross  Abstract: We employ an inversion-based approach to examine CLIP models. Our examination reveals that inverting CLIP models results in the generation of images that exhibit semantic alignment with the specified target prompts. We leverage these inverted images to gain insights into various aspects of CLIP models, such as their ability to blend concepts and inclusion of gender biases. We notably observe instances of NSFW (Not Safe For Work) images during model inversion. This phenomenon occurs even for semantically innocuous prompts, like "a beautiful landscape," as well as for prompts involving the names of celebrities.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;transformers&#20013;&#20449;&#21495;&#20256;&#25773;&#21644;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#21021;&#22987;&#21270;&#36229;&#21442;&#25968;&#26465;&#20214;&#20197;&#30830;&#20445;&#35757;&#32451;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#26377;MLP&#23618;&#30340;&#24773;&#20917;&#19979;&#31890;&#23376;&#20960;&#20309;&#30340;&#23450;&#37327;&#28436;&#21270;&#35268;&#24459;&#65292;&#25581;&#31034;&#20102;&#21021;&#22987;&#21270;&#20989;&#25968;&#30340;&#26377;&#24207;-&#28151;&#27788;&#30456;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.02579</link><description>&lt;p&gt;
&#20449;&#21495;&#20256;&#25773;&#30340;&#20960;&#20309;&#21160;&#21147;&#23398;&#39044;&#27979;transformers&#30340;&#21487;&#35757;&#32451;&#24615;
&lt;/p&gt;
&lt;p&gt;
Geometric Dynamics of Signal Propagation Predict Trainability of Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02579
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;transformers&#20013;&#20449;&#21495;&#20256;&#25773;&#21644;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#21021;&#22987;&#21270;&#36229;&#21442;&#25968;&#26465;&#20214;&#20197;&#30830;&#20445;&#35757;&#32451;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#26377;MLP&#23618;&#30340;&#24773;&#20917;&#19979;&#31890;&#23376;&#20960;&#20309;&#30340;&#23450;&#37327;&#28436;&#21270;&#35268;&#24459;&#65292;&#25581;&#31034;&#20102;&#21021;&#22987;&#21270;&#20989;&#25968;&#30340;&#26377;&#24207;-&#28151;&#27788;&#30456;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#12289;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;transformers&#20013;&#21069;&#21521;&#20449;&#21495;&#20256;&#25773;&#21644;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#24471;&#20986;&#20102;&#21021;&#22987;&#21270;&#36229;&#21442;&#25968;&#30340;&#31616;&#21333;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#30830;&#20445;&#28145;&#24230;transformers&#30340;&#21487;&#35757;&#32451;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;$n$&#20010;&#31526;&#21495;&#30340;&#34920;&#31034;&#38543;&#30528;&#23427;&#20204;&#36890;&#36807;transformer&#23618;&#20256;&#25773;&#35270;&#20026;$n$&#20010;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#30340;&#31163;&#25955;&#26102;&#38388;&#21160;&#21147;&#31995;&#32479;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#36825;&#20010;&#31890;&#23376;&#31995;&#32479;&#28436;&#21270;&#20960;&#20309;&#30340;&#31616;&#21333;&#26356;&#26032;&#26041;&#31243;&#65292;&#20174;&#19968;&#20010;&#32622;&#25442;&#23545;&#31216;&#21333;&#32431;&#24418;&#24320;&#22987;&#12290;&#25105;&#20204;&#30340;&#26356;&#26032;&#26041;&#31243;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;MLP&#23618;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31995;&#32479;&#23558;&#20250;&#22349;&#32553;&#25104;&#20026;&#19968;&#26465;&#32447;&#65292;&#19982;&#20197;&#21069;&#22312;transformers&#20013;&#31209;&#22349;&#32553;&#30340;&#30456;&#20851;&#24037;&#20316;&#30456;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#28436;&#21270;&#26041;&#31243;&#33021;&#22815;&#23450;&#37327;&#36319;&#36394;&#31890;&#23376;&#20960;&#20309;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;MLP&#23618;&#30340;&#39069;&#22806;&#23384;&#22312;&#65292;&#24182;&#19988;&#23427;&#25581;&#31034;&#20102;&#20316;&#20026;&#21021;&#22987;&#21270;&#20989;&#25968;&#30340;&#26377;&#24207;-&#28151;&#27788;&#30456;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02579v1 Announce Type: cross  Abstract: We investigate forward signal propagation and gradient back propagation in deep, randomly initialized transformers, yielding simple necessary and sufficient conditions on initialization hyperparameters that ensure trainability of deep transformers. Our approach treats the evolution of the representations of $n$ tokens as they propagate through the transformer layers in terms of a discrete time dynamical system of $n$ interacting particles. We derive simple update equations for the evolving geometry of this particle system, starting from a permutation symmetric simplex. Our update equations show that without MLP layers, this system will collapse to a line, consistent with prior work on rank collapse in transformers. However, unlike prior work, our evolution equations can quantitatively track particle geometry in the additional presence of nonlinear MLP layers, and it reveals an order-chaos phase transition as a function of initializatio
&lt;/p&gt;</description></item><item><title>AceMap&#26159;&#19968;&#20010;&#38754;&#21521;&#30693;&#35782;&#21457;&#29616;&#30340;&#23398;&#26415;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#20840;&#38754;&#30340;&#25968;&#25454;&#24211;&#21644;&#36816;&#29992;&#21019;&#26032;&#30340;&#21487;&#35270;&#21270;&#12289;&#37327;&#21270;&#21644;&#20998;&#26512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31185;&#23398;&#25991;&#29486;&#31649;&#29702;&#19982;&#20215;&#20540;&#25552;&#21462;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.02576</link><description>&lt;p&gt;
AceMap&#65306;&#36890;&#36807;&#23398;&#26415;&#22270;&#35889;&#36827;&#34892;&#30693;&#35782;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
AceMap: Knowledge Discovery through Academic Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02576
&lt;/p&gt;
&lt;p&gt;
AceMap&#26159;&#19968;&#20010;&#38754;&#21521;&#30693;&#35782;&#21457;&#29616;&#30340;&#23398;&#26415;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#20840;&#38754;&#30340;&#25968;&#25454;&#24211;&#21644;&#36816;&#29992;&#21019;&#26032;&#30340;&#21487;&#35270;&#21270;&#12289;&#37327;&#21270;&#21644;&#20998;&#26512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31185;&#23398;&#25991;&#29486;&#31649;&#29702;&#19982;&#20215;&#20540;&#25552;&#21462;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25991;&#29486;&#30340;&#25351;&#25968;&#22686;&#38271;&#38656;&#35201;&#26377;&#25928;&#31649;&#29702;&#21644;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#31185;&#23398;&#25628;&#32034;&#24341;&#25806;&#22312;&#22522;&#20110;&#20851;&#31995;&#25968;&#25454;&#24211;&#25552;&#20379;&#25628;&#32034;&#32467;&#26524;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#30053;&#20102;&#31185;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20197;&#21450;&#24605;&#24819;&#28436;&#21270;&#30340;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#31185;&#23398;&#20986;&#29256;&#29289;&#20869;&#23481;&#30340;&#28145;&#20837;&#20998;&#26512;&#12290;&#24322;&#36136;&#22270;&#30340;&#34920;&#31034;&#20197;&#21450;&#36825;&#31181;&#22270;&#30340;&#26377;&#25928;&#27979;&#37327;&#12289;&#20998;&#26512;&#21644;&#25366;&#25496;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AceMap&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#23398;&#26415;&#22270;&#35889;&#36827;&#34892;&#30693;&#35782;&#21457;&#29616;&#30340;&#23398;&#26415;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20808;&#36827;&#30340;&#25968;&#25454;&#24211;&#26500;&#24314;&#25216;&#26415;&#65292;&#20197;&#26500;&#24314;&#21253;&#21547;&#20016;&#23500;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#25968;&#20540;&#20449;&#24687;&#30340;&#22823;&#35268;&#27169;&#23398;&#26415;&#20986;&#29256;&#29289;&#30340;&#20840;&#38754;AceMap&#25968;&#25454;&#24211;&#12290;AceMap&#36824;&#37319;&#29992;&#20102;&#21019;&#26032;&#30340;&#21487;&#35270;&#21270;&#12289;&#37327;&#21270;&#21644;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02576v1 Announce Type: cross  Abstract: The exponential growth of scientific literature requires effective management and extraction of valuable insights. While existing scientific search engines excel at delivering search results based on relational databases, they often neglect the analysis of collaborations between scientific entities and the evolution of ideas, as well as the in-depth analysis of content within scientific publications. The representation of heterogeneous graphs and the effective measurement, analysis, and mining of such graphs pose significant challenges. To address these challenges, we present AceMap, an academic system designed for knowledge discovery through academic graph. We present advanced database construction techniques to build the comprehensive AceMap database with large-scale academic publications that contain rich visual, textual, and numerical information. AceMap also employs innovative visualization, quantification, and analysis methods to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#20256;&#36755;&#21644;&#38472;&#26087;&#25104;&#26412;&#30340;&#24635;&#21644;&#65292;&#22312;&#20445;&#35777;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20860;&#39038;&#20856;&#22411;&#24773;&#20917;&#19979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02573</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#26368;&#23567;&#21270;&#20449;&#24687;&#26102;&#20195;&#21644;&#20256;&#36755;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning-augmented Online Minimization of Age of Information and Transmission Costs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#20256;&#36755;&#21644;&#38472;&#26087;&#25104;&#26412;&#30340;&#24635;&#21644;&#65292;&#22312;&#20445;&#35777;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20860;&#39038;&#20856;&#22411;&#24773;&#20917;&#19979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#65292;&#19968;&#20010;&#36164;&#28304;&#21463;&#38480;&#30340;&#26469;&#28304;&#65288;&#20363;&#22914;&#65292;&#19968;&#20010;&#23567;&#22411;&#20256;&#24863;&#22120;&#65289;&#36890;&#36807;&#19968;&#20010;&#26102;&#21464;&#26080;&#32447;&#20449;&#36947;&#23558;&#20854;&#21450;&#26102;&#25968;&#25454;&#20256;&#36755;&#32473;&#30446;&#30340;&#22320;&#12290;&#27599;&#27425;&#20256;&#36755;&#20250;&#20135;&#29983;&#22266;&#23450;&#30340;&#20256;&#36755;&#25104;&#26412;&#65288;&#20363;&#22914;&#65292;&#33021;&#37327;&#25104;&#26412;&#65289;&#65292;&#32780;&#27809;&#26377;&#20256;&#36755;&#20250;&#23548;&#33268;&#19968;&#20010;&#20197;&#20449;&#24687;&#26102;&#20195;&#34920;&#31034;&#30340;&#38472;&#26087;&#25104;&#26412;&#12290;&#26469;&#28304;&#24517;&#39035;&#22312;&#20256;&#36755;&#25104;&#26412;&#21644;&#38472;&#26087;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20256;&#36755;&#21644;&#38472;&#26087;&#25104;&#26412;&#30340;&#24635;&#21644;&#65292;&#30830;&#20445;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#23613;&#31649;&#22312;&#32447;&#31639;&#27861;&#26159;&#31283;&#20581;&#30340;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#36807;&#20110;&#20445;&#23432;&#65292;&#22312;&#20856;&#22411;&#24773;&#20917;&#19979;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#30456;&#21453;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#21644;&#39044;&#27979;&#27169;&#22411;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#22312;&#24179;&#22343;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#20026;&#20102;&#23454;&#29616;&#26368;&#22909;&#30340;&#20004;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#26368;&#23567;&#21270;&#20449;&#24687;&#26102;&#20195;&#21644;&#20256;&#36755;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02573v1 Announce Type: new  Abstract: We consider a discrete-time system where a resource-constrained source (e.g., a small sensor) transmits its time-sensitive data to a destination over a time-varying wireless channel. Each transmission incurs a fixed transmission cost (e.g., energy cost), and no transmission results in a staleness cost represented by the Age-of-Information. The source must balance the tradeoff between transmission and staleness costs. To address this challenge, we develop a robust online algorithm to minimize the sum of transmission and staleness costs, ensuring a worst-case performance guarantee. While online algorithms are robust, they are usually overly conservative and may have a poor average performance in typical scenarios. In contrast, by leveraging historical data and prediction models, machine learning (ML) algorithms perform well in average cases. However, they typically lack worst-case performance guarantees. To achieve the best of both worlds,
&lt;/p&gt;</description></item><item><title>DPAdapter&#36890;&#36807;&#22122;&#22768;&#23481;&#24525;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;DP&#24341;&#20837;&#30340;&#27169;&#22411;&#24615;&#33021;&#38477;&#20302;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.02571</link><description>&lt;p&gt;
DPAdapter: &#36890;&#36807;&#22122;&#22768;&#23481;&#24525;&#39044;&#35757;&#32451;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DPAdapter: Improving Differentially Private Deep Learning through Noise Tolerance Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02571
&lt;/p&gt;
&lt;p&gt;
DPAdapter&#36890;&#36807;&#22122;&#22768;&#23481;&#24525;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;DP&#24341;&#20837;&#30340;&#27169;&#22411;&#24615;&#33021;&#38477;&#20302;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#24046;&#20998;&#38544;&#31169;&#22312;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#38598;&#25104;&#24046;&#20998;&#38544;&#31169;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#20005;&#37325;&#38477;&#20302;&#65292;&#36825;&#26159;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38754;&#20020;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#20960;&#39033;&#32531;&#35299;&#25514;&#26045;&#65292;&#36890;&#24120;&#22260;&#32469;&#21046;&#23450;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25110;&#25918;&#23485;&#24046;&#20998;&#38544;&#31169;&#23450;&#20041;&#20197;&#36866;&#24212;&#19981;&#21516;&#24773;&#22659;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#24046;&#20998;&#38544;&#31169;&#23545;&#27169;&#22411;&#30340;&#20943;&#24369;&#65292;&#29305;&#21035;&#26159;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#20173;&#28982;&#24456;&#22823;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#21019;&#26032;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#24039;&#22937;&#22320;&#35268;&#36991;&#27169;&#22411;&#25928;&#29992;&#30340;&#37325;&#35201;&#25439;&#23475;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DPAdapter&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#21319;DPML&#27169;&#22411;&#24615;&#33021;&#30340;&#24320;&#21019;&#24615;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02571v1 Announce Type: new  Abstract: Recent developments have underscored the critical role of \textit{differential privacy} (DP) in safeguarding individual data for training machine learning models. However, integrating DP oftentimes incurs significant model performance degradation due to the perturbation introduced into the training process, presenting a formidable challenge in the {differentially private machine learning} (DPML) field. To this end, several mitigative efforts have been proposed, typically revolving around formulating new DPML algorithms or relaxing DP definitions to harmonize with distinct contexts. In spite of these initiatives, the diminishment induced by DP on models, particularly large-scale models, remains substantial and thus, necessitates an innovative solution that adeptly circumnavigates the consequential impairment of model utility.   In response, we introduce DPAdapter, a pioneering technique designed to amplify the model performance of DPML al
&lt;/p&gt;</description></item><item><title>Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02545</link><description>&lt;p&gt;
Wukong: &#36808;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#30340;&#26631;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Wukong: Towards a Scaling Law for Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02545
&lt;/p&gt;
&lt;p&gt;
Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#25512;&#33616;&#27169;&#22411;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#35266;&#23519;&#21040;&#30340;&#23450;&#24459;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#21319;&#32423;&#26426;&#21046;&#30340;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32431;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#30340;&#26377;&#25928;&#32593;&#32476;&#26550;&#26500;&#65292;&#32479;&#31216;&#20026;Wukong&#65292;&#20197;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#19968;&#20010;&#26631;&#24230;&#24459;&#12290;Wukong&#30340;&#29420;&#29305;&#35774;&#35745;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#26356;&#39640;&#26356;&#23485;&#30340;&#23618;&#27425;&#31616;&#21333;&#25429;&#33719;&#21508;&#31181;&#20219;&#24847;&#38454;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;Wukong&#22312;&#36136;&#37327;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20248;&#36234;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Wuko
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02545v1 Announce Type: cross  Abstract: Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wuko
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#38750;&#23545;&#27604;&#24230;&#24515;&#33039;CT&#22270;&#20687;&#20013;&#36827;&#34892;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#33258;&#21160;&#29983;&#25104;Ground Truth&#30340;&#26032;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.02544</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#38750;&#23545;&#27604;&#24230;&#38041;&#35780;&#20998;CT&#22270;&#20687;&#20013;&#36827;&#34892;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Coronary artery segmentation in non-contrast calcium scoring CT images using deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#38750;&#23545;&#27604;&#24230;&#24515;&#33039;CT&#22270;&#20687;&#20013;&#36827;&#34892;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#33258;&#21160;&#29983;&#25104;Ground Truth&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#31934;&#30830;&#23450;&#20301;&#20896;&#29366;&#21160;&#33033;&#23545;&#20110;&#35780;&#20272;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20379;&#24212;&#21830;&#30340;&#24515;&#30005;&#22270;&#38376;&#25511;&#38750;&#23545;&#27604;&#24230;&#24515;&#33039;CT&#22270;&#20687;&#20013;&#20998;&#21106;&#20896;&#29366;&#21160;&#33033;&#65292;&#36890;&#36807;&#22270;&#20687;&#37197;&#20934;&#23454;&#29616;&#21322;&#33258;&#21160;&#29983;&#25104;Ground Truth&#65288;GT&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02544v1 Announce Type: cross  Abstract: Precise localization of coronary arteries in Computed Tomography (CT) scans is critical from the perspective of medical assessment of coronary artery disease. Although various methods exist that offer high-quality segmentation of coronary arteries in cardiac contrast-enhanced CT scans, the potential of less invasive, non-contrast CT in this area is still not fully exploited. Since such fine anatomical structures are hardly visible in this type of medical images, the existing methods are characterized by high recall and low precision, and are used mainly for filtering of atherosclerotic plaques in the context of calcium scoring. In this paper, we address this research gap and introduce a deep learning algorithm for segmenting coronary arteries in multi-vendor ECG-gated non-contrast cardiac CT images which benefits from a novel framework for semi-automatic generation of Ground Truth (GT) via image registration. We hypothesize that the pr
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#26032;&#25968;&#25454;&#38598;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#65292;&#39044;&#27979;&#22826;&#38451;&#27963;&#21160;&#21608;&#26399;23&#21644;24&#26399;&#38388;&#30340;SEP&#20107;&#20214;&#65292;&#20197;&#20943;&#23569;&#23545;&#33322;&#31354;&#12289;&#22826;&#31354;&#30005;&#23376;&#35774;&#22791;&#21644;&#22826;&#31354;&#25506;&#32034;&#30340;&#36752;&#23556;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2403.02536</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22826;&#38451;&#27963;&#21160;&#21608;&#26399;23&#21644;24&#26399;&#38388;&#30340;SEP&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
Forecasting SEP Events During Solar Cycles 23 and 24 Using Interpretable Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02536
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26032;&#25968;&#25454;&#38598;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#65292;&#39044;&#27979;&#22826;&#38451;&#27963;&#21160;&#21608;&#26399;23&#21644;24&#26399;&#38388;&#30340;SEP&#20107;&#20214;&#65292;&#20197;&#20943;&#23569;&#23545;&#33322;&#31354;&#12289;&#22826;&#31354;&#30005;&#23376;&#35774;&#22791;&#21644;&#22826;&#31354;&#25506;&#32034;&#30340;&#36752;&#23556;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22826;&#38451;&#39640;&#33021;&#31890;&#23376;(SEP)&#20107;&#20214;&#22312;&#22826;&#31354;&#20219;&#21153;&#36880;&#28176;&#25193;&#23637;&#33267;&#22320;&#29699;&#20445;&#25252;&#30913;&#23618;&#20043;&#22806;&#30340;&#24773;&#20917;&#19979;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#36825;&#20123;&#20107;&#20214;&#22823;&#22810;&#26159;&#22312;&#22826;&#38451;&#32768;&#26001;&#25110;&#24555;&#36895;&#26085;&#20885;&#29289;&#36136;&#25243;&#23556;&#39537;&#21160;&#30340;&#20914;&#20987;&#27874;&#26399;&#38388;&#20135;&#29983;&#30340;&#30913;&#37325;&#36830;&#39537;&#21160;&#36807;&#31243;&#30340;&#20135;&#29289;&#65292;&#23545;&#33322;&#31354;&#12289;&#22826;&#31354;&#30005;&#23376;&#35774;&#22791;&#20197;&#21450;&#23588;&#20854;&#26159;&#22826;&#31354;&#25506;&#32034;&#26500;&#25104;&#20102;&#37325;&#35201;&#30340;&#36752;&#23556;&#21361;&#23475;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;&#22826;&#38451;&#21160;&#21147;&#23398;&#35266;&#27979;&#21355;&#26143;/&#34920;&#38754;&#27874;&#21644;&#30913;&#22330;&#25104;&#20687;&#20202;(SDO/HMI)&#30340;&#31354;&#38388;&#22825;&#27668;HMI&#27963;&#36291;&#21306;&#22495;&#34917;&#19969;(SHARP)&#19982;&#22826;&#38451;&#21644;&#26085;&#29699;&#32852;&#21512;&#35266;&#27979;&#21355;&#26143;/&#31859;&#27463;&#26862;&#22810;&#26222;&#21202;&#25104;&#20687;&#20202;(SoHO/MDI)&#30340;&#31354;&#38388;&#22825;&#27668;MDI&#27963;&#36291;&#21306;&#22495;&#34917;&#19969;(SMARPs)&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#21644;&#22238;&#24402;&#27169;&#22411;&#65292;&#35780;&#20272;&#36825;&#19968;&#26032;&#25968;&#25454;&#20135;&#21697;&#30340;&#39044;&#27979;&#28508;&#21147;&#65292;&#29992;&#20110;&#39044;&#27979;&#22826;&#38451;&#21518;&#26399;&#30340;&#31354;&#38388;&#22825;&#27668;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02536v1 Announce Type: cross  Abstract: Prediction of the Solar Energetic Particle (SEP) events garner increasing interest as space missions extend beyond Earth's protective magnetosphere. These events, which are, in most cases, products of magnetic reconnection-driven processes during solar flares or fast coronal-mass-ejection-driven shock waves, pose significant radiation hazards to aviation, space-based electronics, and particularly, space exploration. In this work, we utilize the recently developed dataset that combines the Solar Dynamics Observatory/Helioseismic and Magnetic Imager's (SDO/HMI) Space weather HMI Active Region Patches (SHARP) and the Solar and Heliospheric Observatory/Michelson Doppler Imager's (SoHO/MDI) Space Weather MDI Active Region Patches (SMARP). We employ a suite of machine learning strategies, including Support Vector Machines (SVM) and regression models, to evaluate the predictive potential of this new data product for a forecast of post-solar f
&lt;/p&gt;</description></item><item><title>&#36890;&#24120;&#38656;&#35201;&#23545;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#20026;&#27599;&#20010;&#24207;&#21015;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#24314;&#31435;&#22522;&#30784;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#35752;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#35757;&#32451;&#38598;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02534</link><description>&lt;p&gt;
&#36808;&#21521;&#22522;&#30784;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65306;&#21512;&#25104;&#36824;&#26159;&#19981;&#21512;&#25104;&#65311;
&lt;/p&gt;
&lt;p&gt;
Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02534
&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#38656;&#35201;&#23545;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#20026;&#27599;&#20010;&#24207;&#21015;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#24314;&#31435;&#22522;&#30784;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#35752;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#35757;&#32451;&#38598;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#19994;&#20013;&#32463;&#24120;&#26377;&#38656;&#35201;&#21516;&#26102;&#23545;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#36935;&#21040;&#26080;&#27861;&#20026;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#35757;&#32451;&#21333;&#29420;&#27169;&#22411;&#30340;&#24773;&#20917;&#12290;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#36825;&#19968;&#38382;&#39064;&#19968;&#30452;&#26410;&#24471;&#21040;&#20805;&#20998;&#37325;&#35270;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#30340;&#35299;&#20915;&#21150;&#27861;&#26159;&#24314;&#31435;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#12290;&#36825;&#31181;&#27169;&#22411;&#39044;&#35745;&#33021;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24212;&#35813;&#23558;&#20160;&#20040;&#20316;&#20026;&#36825;&#31181;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21602;&#65311;&#20174;NLP&#25968;&#25454;&#38598;&#30340;&#25193;&#20805;&#20013;&#33719;&#30410;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#20511;&#37492;&#20182;&#20204;&#30340;&#32463;&#39564;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#19981;&#21516;&#65292;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26356;&#20026;&#26377;&#21033;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#20840;&#38754;&#25511;&#21046;&#31995;&#21015;&#27169;&#24335;&#12289;&#26102;&#38388;&#36328;&#24230;&#21644;&#26679;&#26412;&#25968;&#37327;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02534v1 Announce Type: new  Abstract: The industry is rich in cases when we are required to make forecasting for large amounts of time series at once. However, we might be in a situation where we can not afford to train a separate model for each of them. Such issue in time series modeling remains without due attention. The remedy for this setting is the establishment of a foundation model. Such a model is expected to work in zero-shot and few-shot regimes. However, what should we take as a training dataset for such kind of model?   Witnessing the benefits from the enrichment of NLP datasets with artificially-generated data, we might want to adopt their experience for time series. In contrast to natural language, the process of generation of synthetic time series data is even more favorable because it provides full control of series patterns, time horizons, and number of samples. In this work, we consider the essential question if it is advantageous to train a foundation mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#31561;&#36317;&#26144;&#23556;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#21463;Parzen-Rosenblatt (PR)&#31383;&#21475;&#21551;&#21457;&#30340;&#26032;&#32422;&#26463;&#65292;&#26377;&#21161;&#20110;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#19981;&#22343;&#21248;&#24615;&#65292;&#20351;&#24471;&#26500;&#24314;&#30340;Isomap&#20013;&#26368;&#30701;&#36335;&#24452;&#22270;&#20855;&#26377;&#26356;&#22909;&#30340;&#22343;&#21248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02531</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#31561;&#36317;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Density-based Isometric Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02531
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#31561;&#36317;&#26144;&#23556;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#21463;Parzen-Rosenblatt (PR)&#31383;&#21475;&#21551;&#21457;&#30340;&#26032;&#32422;&#26463;&#65292;&#26377;&#21161;&#20110;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#19981;&#22343;&#21248;&#24615;&#65292;&#20351;&#24471;&#26500;&#24314;&#30340;Isomap&#20013;&#26368;&#30701;&#36335;&#24452;&#22270;&#20855;&#26377;&#26356;&#22909;&#30340;&#22343;&#21248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#20301;&#26144;&#23556;&#26041;&#27861;&#21033;&#29992;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#26469;&#20272;&#35745;&#39640;&#32500;&#27969;&#24418;&#19978;&#28857;&#30340;&#27431;&#27663;&#36317;&#31163;&#12290;&#36825;&#21487;&#33021;&#19981;&#36275;&#20197;&#22788;&#29702;&#24369;&#22343;&#21248;&#30340;&#39640;&#32500;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#20272;&#35745;&#36828;&#22788;&#30456;&#37051;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#36807;&#39640;&#65292;&#22312;&#25237;&#24433;&#36807;&#31243;&#20013;&#23548;&#33268;&#26412;&#24449;&#65288;&#23616;&#37096;&#65289;&#21644;&#22806;&#26174;&#65288;&#20840;&#23616;&#65289;&#36317;&#31163;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#28155;&#21152;&#20102;&#21463; Parzen-Rosenblatt&#65288;PR&#65289;&#31383;&#21475;&#21551;&#21457;&#30340;&#26032;&#32422;&#26463;&#26469;&#20462;&#25913;&#35813;&#31639;&#27861;&#65292;&#26377;&#21161;&#20110;&#22312;&#31561;&#36317;&#26144;&#23556;&#20013;&#20445;&#25345;&#26500;&#24314;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#30340;&#22343;&#21248;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#24635;&#20849;72,236&#20010;&#26696;&#20363;&#30340;&#22810;&#20010;&#25104;&#20687;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;70,000&#20010;MINST&#25968;&#25454;&#65292;&#26469;&#33258;&#22810;&#20010;&#33016;&#37096;X&#23556;&#32447;&#32954;&#28814;&#25968;&#25454;&#38598;&#30340;1596&#20010;&#25968;&#25454;&#65292;&#20197;&#21450;&#19977;&#20010;NSCLC CT/PET&#25968;&#25454;&#38598;&#20013;&#20849;640&#21517;&#32954;&#30284;&#24739;&#32773;&#30340;&#25968;&#25454;&#65292;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#21644;&#39564;&#35777;PR-Isomap&#12290;&#27599;&#31181;&#27169;&#24577;&#20174;&#20013;&#25552;&#21462;&#20102;431&#20010;&#25104;&#20687;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;P
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02531v1 Announce Type: new  Abstract: The isometric mapping method employs the shortest path algorithm to estimate the Euclidean distance between points on High dimensional (HD) manifolds. This may not be sufficient for weakly uniformed HD data as it could lead to overestimating distances between far neighboring points, resulting in inconsistencies between the intrinsic (local) and extrinsic (global) distances during the projection. To address this issue, we modify the shortest path algorithm by adding a novel constraint inspired by the Parzen-Rosenblatt (PR) window, which helps to maintain the uniformity of the constructed shortest-path graph in Isomap. Multiple imaging datasets overall of 72,236 cases, 70,000 MINST data, 1596 from multiple Chest-XRay pneumonia datasets, and three NSCLC CT/PET datasets with a total of 640 lung cancer patients, were used to benchmark and validate PR-Isomap. 431 imaging biomarkers were extracted from each modality. Our results indicate that P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20869;&#22312;&#32467;&#26500;&#21644;jets&#20960;&#20309;&#27010;&#24565;&#30340;&#20272;&#35745;Koopman&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;JetDMD&#65292;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#25910;&#25947;&#29575;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#65292;&#20026;Koopman&#31639;&#23376;&#30340;&#25968;&#20540;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.02524</link><description>&lt;p&gt;
&#22312;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20855;&#26377;&#20869;&#22312;&#21487;&#35266;&#27979;&#24615;&#30340;Koopman&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20869;&#22312;&#32467;&#26500;&#21644;jets&#20960;&#20309;&#27010;&#24565;&#30340;&#20272;&#35745;Koopman&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;JetDMD&#65292;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#25910;&#25947;&#29575;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#65292;&#20026;Koopman&#31639;&#23376;&#30340;&#25968;&#20540;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#19978;&#23450;&#20041;&#30340;Koopman&#31639;&#23376;&#21450;&#20854;&#35889;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;Jet Dynamic Mode Decomposition&#65288;JetDMD&#65289;&#65292;&#21033;&#29992;RKHS&#30340;&#20869;&#22312;&#32467;&#26500;&#21644;&#31216;&#20026;jets&#30340;&#20960;&#20309;&#27010;&#24565;&#26469;&#22686;&#24378;Koopman&#31639;&#23376;&#30340;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#19978;&#20248;&#21270;&#20102;&#20256;&#32479;&#30340;&#25193;&#23637;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;EDMD&#65289;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#24449;&#20540;&#30340;&#25968;&#20540;&#20272;&#35745;&#26041;&#38754;&#12290;&#26412;&#25991;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#29305;&#27530;&#27491;&#23450;&#20869;&#26680;&#30340;&#25910;&#25947;&#29575;&#35777;&#26126;&#20102;JetDMD&#30340;&#20248;&#36234;&#24615;&#65292;&#20026;&#20854;&#24615;&#33021;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;Koopman&#31639;&#23376;&#30340;&#35889;&#20998;&#26512;&#65292;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#12290;&#36825;&#20010;&#27010;&#24565;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#24182;&#25429;&#25417;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02524v1 Announce Type: cross  Abstract: This paper presents a novel approach for estimating the Koopman operator defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We propose an estimation method, what we call Jet Dynamic Mode Decomposition (JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion known as jets to enhance the estimation of the Koopman operator. This method refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy, especially in the numerical estimation of eigenvalues. This paper proves JetDMD's superiority through explicit error bounds and convergence rate for special positive definite kernels, offering a solid theoretical foundation for its performance. We also delve into the spectral analysis of the Koopman operator, proposing the notion of extended Koopman operator within a framework of rigged Hilbert space. This notion leads to a deeper understanding of estimated Koopman eigenfunctions and captu
&lt;/p&gt;</description></item><item><title>HeAR&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#22312;33&#20010;&#20581;&#24247;&#22768;&#23398;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#26377;&#26395;&#25512;&#21160;&#20581;&#24247;&#22768;&#23398;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.02522</link><description>&lt;p&gt;
&#20581;&#24247;&#22768;&#23398;&#34920;&#31034;&#65306;HeAR
&lt;/p&gt;
&lt;p&gt;
HeAR -- Health Acoustic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02522
&lt;/p&gt;
&lt;p&gt;
HeAR&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#22312;33&#20010;&#20581;&#24247;&#22768;&#23398;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#26377;&#26395;&#25512;&#21160;&#20581;&#24247;&#22768;&#23398;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#22768;&#23398;&#22768;&#38899;&#65292;&#22914;&#21683;&#22013;&#21644;&#21628;&#21560;&#22768;&#65292;&#24050;&#30693;&#21253;&#21547;&#26377;&#29992;&#30340;&#20581;&#24247;&#20449;&#21495;&#65292;&#20855;&#26377;&#30417;&#27979;&#20581;&#24247;&#21644;&#30142;&#30149;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#20294;&#22312;&#21307;&#30103;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#29616;&#26377;&#30340;&#20581;&#24247;&#22768;&#23398;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#21482;&#38024;&#23545;&#21333;&#19968;&#20219;&#21153;&#36827;&#34892;&#29421;&#31364;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#36825;&#21463;&#21040;&#25968;&#25454;&#38480;&#21046;&#65292;&#21487;&#33021;&#38480;&#21046;&#20102;&#23545;&#20854;&#20182;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;HeAR&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20280;&#32553;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;3.13&#20159;&#20010;&#20004;&#31186;&#38271;&#38899;&#39057;&#21098;&#36753;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#12290;&#36890;&#36807;&#32447;&#24615;&#25506;&#27979;&#65292;&#25105;&#20204;&#23558;HeAR&#30830;&#31435;&#20026;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;33&#20010;&#20581;&#24247;&#22768;&#23398;&#20219;&#21153;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#20581;&#24247;&#38899;&#39057;&#23884;&#20837;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#39033;&#24037;&#20316;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#21551;&#29992;&#21644;&#21152;&#36895;&#36827;&#19968;&#27493;&#30340;&#20581;&#24247;&#22768;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02522v1 Announce Type: cross  Abstract: Health acoustic sounds such as coughs and breaths are known to contain useful health signals with significant potential for monitoring health and disease, yet are underexplored in the medical machine learning community. The existing deep learning systems for health acoustics are often narrowly trained and evaluated on a single task, which is limited by data and may hinder generalization to other tasks. To mitigate these gaps, we develop HeAR, a scalable self-supervised learning-based deep learning system using masked autoencoders trained on a large dataset of 313 million two-second long audio clips. Through linear probes, we establish HeAR as a state-of-the-art health audio embedding model on a benchmark of 33 health acoustic tasks across 6 datasets. By introducing this work, we hope to enable and accelerate further health acoustics research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35774;&#23450;&#26426;&#22120;&#20154;&#30446;&#30340;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#26356;&#21152;&#20851;&#27880;&#33719;&#21462;&#19982;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.02514</link><description>&lt;p&gt;
&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#26426;&#22120;&#20154;&#35774;&#23450;&#30446;&#30340;&#65306;&#19968;&#20010;&#35745;&#31639;&#20998;&#31867;&#12289;&#23450;&#20041;&#21644;&#25805;&#20316;&#21270;
&lt;/p&gt;
&lt;p&gt;
Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35774;&#23450;&#26426;&#22120;&#20154;&#30446;&#30340;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#26356;&#21152;&#20851;&#27880;&#33719;&#21462;&#19982;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02514v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#20027;&#24320;&#25918;&#24335;&#23398;&#20064;(OEL)&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#30452;&#25509;&#20132;&#20114;&#32047;&#31215;&#33719;&#21462;&#26032;&#25216;&#33021;&#21644;&#30693;&#35782;&#65292;&#20363;&#22914;&#20381;&#38752;&#20869;&#22312;&#21160;&#26426;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#30446;&#26631;&#30340;&#25351;&#23548;&#12290;OEL&#26426;&#22120;&#20154;&#23545;&#24212;&#29992;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#20027;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#23436;&#25104;&#23545;&#20154;&#31867;&#29992;&#25143;&#26377;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;OEL&#26426;&#22120;&#20154;&#38754;&#20020;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#36825;&#21487;&#33021;&#23548;&#33268;&#33719;&#21462;&#30340;&#30693;&#35782;&#23545;&#23436;&#25104;&#29992;&#25143;&#20219;&#21153;&#24182;&#19981;&#37027;&#20040;&#37325;&#35201;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#22260;&#32469;&#8220;&#30446;&#30340;&#8221;&#36825;&#19968;&#26032;&#27010;&#24565;&#23637;&#24320;&#12290;&#30446;&#30340;&#34920;&#31034;&#35774;&#35745;&#32773;&#21644;/&#25110;&#29992;&#25143;&#24076;&#26395;&#26426;&#22120;&#20154;&#20174;&#20013;&#33719;&#24471;&#20160;&#20040;&#12290;&#26426;&#22120;&#20154;&#24212;&#20351;&#29992;&#30446;&#30340;&#30340;&#20869;&#37096;&#34920;&#24449;&#65292;&#36825;&#37324;&#31216;&#20026;&#8220;&#24895;&#26395;&#8221;&#65292;&#26469;&#23558;&#20854;&#24320;&#25918;&#24335;&#25506;&#32034;&#38598;&#20013;&#20110;&#33719;&#21462;&#19982;&#20854;&#23436;&#25104;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#21457;&#23637;&#19968;&#20010;&#20849;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02514v1 Announce Type: cross  Abstract: Autonomous open-ended learning (OEL) robots are able to cumulatively acquire new skills and knowledge through direct interaction with the environment, for example relying on the guidance of intrinsic motivations and self-generated goals. OEL robots have a high relevance for applications as they can use the autonomously acquired knowledge to accomplish tasks relevant for their human users. OEL robots, however, encounter an important limitation: this may lead to the acquisition of knowledge that is not so much relevant to accomplish the users' tasks. This work analyses a possible solution to this problem that pivots on the novel concept of `purpose'. Purposes indicate what the designers and/or users want from the robot. The robot should use internal representations of purposes, called here `desires', to focus its open-ended exploration towards the acquisition of knowledge relevant to accomplish them. This work contributes to develop a co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#24046;&#20998;&#38544;&#31169;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#22270;&#20687;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.02506</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Representation Learning via Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02506
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#24046;&#20998;&#38544;&#31169;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#22270;&#20687;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#20174;&#25935;&#24863;&#25968;&#25454;&#20013;&#35757;&#32451;&#27169;&#22411;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#30340;&#40644;&#37329;&#26631;&#20934;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#29702;&#24819;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#20854;&#27425;&#20248;&#30340;&#38544;&#31169;-&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#22312;DP&#34920;&#31034;&#23398;&#20064;&#20013;&#29305;&#21035;&#26126;&#26174;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24050;&#32463;&#35777;&#26126;&#22312;&#36866;&#24230;&#30340;&#38544;&#31169;&#39044;&#31639;&#19979;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#31034;&#24182;&#19981;&#27604;&#25163;&#24037;&#29305;&#24449;&#26174;&#33879;&#26356;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;&#25193;&#23637;&#21040;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;DP&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#24037;&#31243;&#25216;&#24039;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#20351;&#29992;&#21487;&#35266;&#30340;&#35745;&#31639;&#37327;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20102;DP&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;&#65288;DP-Cap&#65289;&#22312;&#26469;&#33258;LAION-2B&#30340;233M&#23376;&#38598;&#19978;&#65292;&#24182;&#33719;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02506v1 Announce Type: cross  Abstract: Differentially private (DP) machine learning is considered the gold-standard solution for training a model from sensitive data while still preserving privacy. However, a major barrier to achieving this ideal is its sub-optimal privacy-accuracy trade-off, which is particularly visible in DP representation learning. Specifically, it has been shown that under modest privacy budgets, most models learn representations that are not significantly better than hand-crafted features. In this work, we show that effective DP representation learning can be done via image captioning and scaling up to internet-scale multimodal datasets. Through a series of engineering tricks, we successfully train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch using a reasonable amount of computation, and obtaining unprecedented high-quality image features that can be used in a variety of downstream vision and vision-language tasks. For examp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#20174;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.02502</link><description>&lt;p&gt;
&#35797;&#38169;&#27861;&#65306;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02502
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#20174;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#20027;&#20195;&#29702;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;ETO&#12290;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#24320;&#25918;LLM&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#19982;&#20808;&#21069;&#19987;&#38376;&#35757;&#32451;&#25104;&#21151;&#19987;&#23478;&#36712;&#36857;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20195;&#29702;&#20174;&#20854;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#12290;&#36825;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;&#22312;&#25506;&#32034;&#38454;&#27573;&#65292;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#23436;&#25104;&#25351;&#23450;&#20219;&#21153;&#65292;&#25910;&#38598;&#22833;&#36133;&#36712;&#36857;&#20197;&#21019;&#24314;&#23545;&#27604;&#36712;&#36857;&#23545;&#12290;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20195;&#29702;&#21033;&#29992;&#36825;&#20123;&#36712;&#36857;&#20559;&#22909;&#23545;&#26356;&#26032;&#20854;&#31574;&#30053;&#65292;&#20351;&#29992;&#31867;&#20284;DPO&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#25506;&#32034;&#21644;&#35757;&#32451;&#30340;&#36845;&#20195;&#24490;&#29615;&#20419;&#36827;&#20102;&#20195;&#29702;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02502v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments o
&lt;/p&gt;</description></item><item><title>RVRAE&#26159;&#19968;&#31181;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#65292;&#21033;&#29992;&#21464;&#20998;&#36882;&#24402;&#33258;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20808;&#39564;-&#21518;&#39564;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24066;&#22330;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#25797;&#38271;&#39118;&#38505;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2403.02500</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#20998;&#36882;&#24402;&#33258;&#32534;&#30721;&#22120;&#30340;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#32929;&#31080;&#25910;&#30410;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RVRAE: A Dynamic Factor Model Based on Variational Recurrent Autoencoder for Stock Returns Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02500
&lt;/p&gt;
&lt;p&gt;
RVRAE&#26159;&#19968;&#31181;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#65292;&#21033;&#29992;&#21464;&#20998;&#36882;&#24402;&#33258;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20808;&#39564;-&#21518;&#39564;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24066;&#22330;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#25797;&#38271;&#39118;&#38505;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#20316;&#20026;&#32463;&#27982;&#23398;&#21644;&#37329;&#34701;&#39046;&#22495;&#30340;&#20027;&#35201;&#24037;&#20855;&#23853;&#38706;&#22836;&#35282;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25237;&#36164;&#31574;&#30053;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#38745;&#24577;&#22240;&#23376;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#12289;&#38750;&#32447;&#24615;&#21644;&#22024;&#26434;&#30340;&#24066;&#22330;&#24773;&#20917;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#25968;&#25454;&#26041;&#38754;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36164;&#20135;&#23450;&#20215;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#65292;&#21517;&#20026;RVRAE&#12290;&#35813;&#27169;&#22411;&#26159;&#19968;&#31181;&#27010;&#29575;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24066;&#22330;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#12290;RVRAE&#24039;&#22937;&#22320;&#23558;&#21160;&#24577;&#22240;&#23376;&#24314;&#27169;&#21407;&#29702;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21464;&#20998;&#36882;&#24402;&#33258;&#32534;&#30721;&#22120;&#65288;VRAE&#65289;&#30456;&#32467;&#21512;&#12290;RVRAE&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#20854;&#20351;&#29992;&#20808;&#39564;-&#21518;&#39564;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23547;&#25214;&#21463;&#26410;&#26469;&#25968;&#25454;&#21551;&#21457;&#30340;&#26368;&#20339;&#21518;&#39564;&#22240;&#23376;&#27169;&#22411;&#65292;&#24494;&#35843;&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RVRAE&#25797;&#38271;&#39118;&#38505;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02500v1 Announce Type: cross  Abstract: In recent years, the dynamic factor model has emerged as a dominant tool in economics and finance, particularly for investment strategies. This model offers improved handling of complex, nonlinear, and noisy market conditions compared to traditional static factor models. The advancement of machine learning, especially in dealing with nonlinear data, has further enhanced asset pricing methodologies. This paper introduces a groundbreaking dynamic factor model named RVRAE. This model is a probabilistic approach that addresses the temporal dependencies and noise in market data. RVRAE ingeniously combines the principles of dynamic factor modeling with the variational recurrent autoencoder (VRAE) from deep learning. A key feature of RVRAE is its use of a prior-posterior learning method. This method fine-tunes the model's learning process by seeking an optimal posterior factor model informed by future data. Notably, RVRAE is adept at risk mod
&lt;/p&gt;</description></item><item><title>&#39044;&#27979;&#22120;&#26041;&#27861;&#22312;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#38754;&#36215;&#21040;&#20102;&#26174;&#33879;&#20316;&#29992;&#65292;&#26412;&#25991;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#32534;&#30721;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#32534;&#30721;&#65292;&#25193;&#23637;&#20102;NAS&#39044;&#27979;&#22120;&#21040;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.02484</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#32534;&#30721;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Encodings for Prediction-based Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02484
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22120;&#26041;&#27861;&#22312;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#38754;&#36215;&#21040;&#20102;&#26174;&#33879;&#20316;&#29992;&#65292;&#26412;&#25991;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#32534;&#30721;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#32534;&#30721;&#65292;&#25193;&#23637;&#20102;NAS&#39044;&#27979;&#22120;&#21040;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22120;&#26041;&#27861;&#22823;&#22823;&#22686;&#24378;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20248;&#21270;&#30340;&#25928;&#26524;&#12290;&#36825;&#20123;&#39044;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#32534;&#30721;&#26041;&#27861;&#12290;&#26412;&#25991;&#23545;&#19977;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#31070;&#32463;&#32534;&#30721;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#30740;&#31350;&#65306;&#32467;&#26500;&#22411;&#12289;&#23398;&#20064;&#22411;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20123;&#32534;&#30721;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;&#32479;&#19968;&#32534;&#30721;&#8221;&#65292;&#23558;NAS&#39044;&#27979;&#22120;&#25193;&#23637;&#21040;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26469;&#33258;&#20110;&#22312;NASBench-101&#65288;NB101&#65289;&#12289;NB201&#12289;NB301&#12289;&#32593;&#32476;&#35774;&#35745;&#31354;&#38388;&#65288;NDS&#65289;&#21644;TransNASBench-101&#31561;NAS&#31354;&#38388;&#19978;&#36827;&#34892;&#30340;&#36229;&#36807;150&#19975;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#23454;&#39564;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02484v1 Announce Type: cross  Abstract: Predictor-based methods have substantially enhanced Neural Architecture Search (NAS) optimization. The efficacy of these predictors is largely influenced by the method of encoding neural network architectures. While traditional encodings used an adjacency matrix describing the graph structure of a neural network, novel encodings embrace a variety of approaches from unsupervised pretraining of latent representations to vectors of zero-cost proxies. In this paper, we categorize and investigate neural encodings from three main types: structural, learned, and score-based. Furthermore, we extend these encodings and introduce \textit{unified encodings}, that extend NAS predictors to multiple search spaces. Our analysis draws from experiments conducted on over 1.5 million neural network architectures on NAS spaces such as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and TransNASBench-101. Building on our study, we present 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#27493;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#23454;&#38469;&#25191;&#34892;&#25237;&#24433;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#20445;&#30041;&#25237;&#24433;&#22522;&#30784;&#20998;&#26512;&#30340;&#31616;&#21333;&#24615;&#26159;&#21487;&#33021;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.02476</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;TD&#23398;&#20064;&#30340;&#31616;&#21333;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Simple Finite-Time Analysis of TD Learning with Linear Function Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02476
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#27493;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#23454;&#38469;&#25191;&#34892;&#25237;&#24433;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#20445;&#30041;&#25237;&#24433;&#22522;&#30784;&#20998;&#26512;&#30340;&#31616;&#21333;&#24615;&#26159;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;TD&#23398;&#20064;&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#27492;&#35774;&#32622;&#19979;&#29616;&#26377;&#30340;&#35777;&#26126;&#35201;&#20040;&#20551;&#23450;&#31639;&#27861;&#20013;&#23384;&#22312;&#25237;&#24433;&#27493;&#39588;&#20197;&#31616;&#21270;&#20998;&#26512;&#65292;&#35201;&#20040;&#38656;&#35201;&#19968;&#20010;&#30456;&#24403;&#22797;&#26434;&#30340;&#35770;&#35777;&#26469;&#30830;&#20445;&#36845;&#20195;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#65306;\textit{&#22312;&#19981;&#23454;&#38469;&#25191;&#34892;&#25237;&#24433;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#20445;&#30041;&#25237;&#24433;&#22522;&#30784;&#20998;&#26512;&#30340;&#31616;&#21333;&#24615;&#26159;&#21542;&#21487;&#33021;&#65311;}&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#27493;&#35770;&#35777;&#26469;&#23637;&#31034;&#36825;&#26159;&#21487;&#33021;&#30340;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24402;&#32435;&#35777;&#26126;&#65292;&#22312;&#26631;&#20934;&#36873;&#25321;&#24120;&#37327;&#27493;&#38271;$\alpha$&#19979;&#65292;&#30001;TD&#23398;&#20064;&#29983;&#25104;&#30340;&#36845;&#20195;&#20445;&#25345;&#26399;&#26395;&#19978;&#30340;&#19968;&#33268;&#26377;&#30028;&#24615;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36882;&#24402;&#65292;&#27169;&#25311;&#20102;TD&#23398;&#20064;&#30340;&#31283;&#24577;&#21160;&#24577;&#65292;&#21463;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#25928;&#26524;&#30340;$O(\alpha^2)$&#25968;&#37327;&#32423;&#19978;&#30340;&#26377;&#30028;&#25668;&#21160;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02476v1 Announce Type: new  Abstract: We study the finite-time convergence of TD learning with linear function approximation under Markovian sampling. Existing proofs for this setting either assume a projection step in the algorithm to simplify the analysis, or require a fairly intricate argument to ensure stability of the iterates. We ask: \textit{Is it possible to retain the simplicity of a projection-based analysis without actually performing a projection step in the algorithm?} Our main contribution is to show this is possible via a novel two-step argument. In the first step, we use induction to prove that under a standard choice of a constant step-size $\alpha$, the iterates generated by TD learning remain uniformly bounded in expectation. In the second step, we establish a recursion that mimics the steady-state dynamics of TD learning up to a bounded perturbation on the order of $O(\alpha^2)$ that captures the effect of Markovian sampling. Combining these pieces leads 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;Constrained DPO&#65288;C-DPO&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#36731;&#37327;&#30340;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#24179;&#34913;&#26377;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;LLMs&#25552;&#20379;&#20102;&#23433;&#20840;&#20445;&#38556;&#12290;</title><link>https://arxiv.org/abs/2403.02475</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22686;&#24378;LLM&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing LLM Safety via Constrained Direct Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;Constrained DPO&#65288;C-DPO&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#36731;&#37327;&#30340;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#24179;&#34913;&#26377;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;LLMs&#25552;&#20379;&#20102;&#23433;&#20840;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#24378;&#33021;&#21147;&#25552;&#39640;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#19981;&#21516;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#20197;&#21516;&#26102;&#22686;&#24378;&#20854;&#26377;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#36843;&#20999;&#38656;&#35201;&#65292;&#23613;&#31649;&#36825;&#20123;&#30446;&#26631;&#24120;&#24120;&#30456;&#20114;&#20914;&#31361;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#37325;&#35201;&#38382;&#39064;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#22312;&#24494;&#35843;&#38454;&#27573;&#36890;&#36807;&#21463;&#38480;&#21046;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26694;&#26550;&#26045;&#21152;&#23433;&#20840;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#19988;&#24120;&#24120;&#19981;&#31283;&#23450;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21463;&#38480;&#21046;&#30340;DPO&#65288;C-DPO&#65289;&#65292;&#36825;&#26159;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26041;&#27861;&#30340;&#19968;&#31181;&#26032;&#39062;&#25193;&#23637;&#65292;&#29992;&#20110;&#20248;&#21270;LLMs&#30340;&#24494;&#35843;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#34701;&#21512;&#21452;&#26799;&#24230;&#19979;&#38477;&#21644;DPO&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#24110;&#21161;&#24615;&#21644;&#26080;&#23475;&#24615;&#20043;&#38388;&#30340;&#20960;&#20046;&#26368;&#20339;&#25240;&#34935;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;LLMs&#25552;&#20379;&#20102;&#23433;&#20840;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02475v1 Announce Type: cross  Abstract: The rapidly increasing capabilities of large language models (LLMs) raise an urgent need to align AI systems with diverse human preferences to simultaneously enhance their usefulness and safety, despite the often conflicting nature of these goals. To address this important problem, a promising approach is to enforce a safety constraint at the fine-tuning stage through a constrained Reinforcement Learning from Human Feedback (RLHF) framework. This approach, however, is computationally expensive and often unstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension of the recently proposed Direct Preference Optimization (DPO) approach for fine-tuning LLMs that is both efficient and lightweight. By integrating dual gradient descent and DPO, our method identifies a nearly optimal trade-off between helpfulness and harmlessness without using reinforcement learning. Empirically, our approach provides a safety guarantee to L
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#31572;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#27169;&#22411;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#31574;&#30053;&#12289;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.02469</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#31572;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#27169;&#22411;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#31574;&#30053;&#12289;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#32467;&#21512;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#29992;&#20110;&#20998;&#26512;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#30340;VLMs&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#35774;&#35745;&#29992;&#20110;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#31572;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#32972;&#26223;&#65292;&#35299;&#37322;&#20102;&#22914;&#20309;&#23558;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#25216;&#26415;&#32467;&#21512;&#21040;VLMs&#20013;&#65292;&#20197;&#23454;&#29616;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#35752;&#35770;&#30340;&#20851;&#38190;&#39046;&#22495;&#21253;&#25324;&#23545;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#65292;&#23545;&#26368;&#36817;&#20540;&#24471;&#20851;&#27880;&#30340;&#21307;&#23398;VLMs&#20013;&#37319;&#29992;&#30340;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#35780;&#20272;VLMs&#22312;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#31572;&#20013;&#34920;&#29616;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20840;&#38754;&#35752;&#35770;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#25552;&#39640;&#20020;&#24202;&#26377;&#25928;&#24615;&#21644;&#35299;&#20915;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02469v1 Announce Type: cross  Abstract: Medical vision-language models (VLMs) combine computer vision and natural language processing to analyze visual and textual medical data. Our paper reviews recent advancements in developing VLMs specialized for healthcare, focusing on models designed for medical report generation and visual question answering. We provide background on natural language processing and computer vision, explaining how techniques from both fields are integrated into VLMs to enable learning from multimodal data. Key areas we address include the exploration of medical vision-language datasets, in-depth analyses of architectures and pre-training strategies employed in recent noteworthy medical VLMs, and comprehensive discussion on evaluation metrics for assessing VLMs' performance in medical report generation and visual question answering. We also highlight current challenges and propose future directions, including enhancing clinical validity and addressing p
&lt;/p&gt;</description></item><item><title>&#26412;&#20070;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#20852;&#34701;&#21512;&#65292;&#25506;&#35752;&#20102;&#32463;&#20856;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#19982;&#29616;&#20195;AI&#31561;&#20215;&#29289;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#28085;&#30422;&#20102;&#20351;&#29992;&#21452;&#37325;/&#21435;&#20559;&#31227;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25512;&#26029;&#30340;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.02467</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25512;&#21160;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Applied Causal Inference Powered by ML and AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#20852;&#34701;&#21512;&#65292;&#25506;&#35752;&#20102;&#32463;&#20856;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#19982;&#29616;&#20195;AI&#31561;&#20215;&#29289;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#28085;&#30422;&#20102;&#20351;&#29992;&#21452;&#37325;/&#21435;&#20559;&#31227;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25512;&#26029;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02467v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#20852;&#34701;&#21512;&#12290;&#35813;&#20070;&#20171;&#32461;&#20102;&#32463;&#20856;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#65288;SEMs&#65289;&#30340;&#24605;&#24819;&#21450;&#20854;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#31561;&#20215;&#29289;&#65292;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#65292;&#24182;&#28085;&#30422;&#20102;&#20351;&#29992;&#29616;&#20195;&#39044;&#27979;&#24037;&#20855;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#36827;&#34892;&#25512;&#26029;&#30340;&#21452;&#37325;/&#21435;&#20559;&#31227;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02467v1 Announce Type: cross  Abstract: An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#35774;&#22791;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#23558;&#39044;&#27979;&#22120;&#36716;&#31227;&#21040;&#27979;&#35797;&#35774;&#22791;&#19978;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02446</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#24310;&#36831;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
On Latency Predictors for Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02446
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#35774;&#22791;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#23558;&#39044;&#27979;&#22120;&#36716;&#31227;&#21040;&#27979;&#35797;&#35774;&#22791;&#19978;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#37096;&#32626;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#38656;&#35201;&#21516;&#26102;&#20248;&#21270;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#12290;&#20363;&#22914;&#65292;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24050;&#34987;&#29992;&#20110;&#33258;&#21160;&#25214;&#21040;&#28385;&#36275;&#29305;&#23450;&#30828;&#20214;&#35774;&#22791;&#19978;&#24310;&#36831;&#32422;&#26463;&#30340;NN&#26550;&#26500;&#12290;&#36825;&#20123;&#25628;&#32034;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26088;&#22312;&#20026;&#20505;&#36873;NN&#26550;&#26500;&#25552;&#20379;&#30828;&#20214;&#24310;&#36831;&#20272;&#35745;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#19968;&#20123;\textit{&#35757;&#32451;}&#35774;&#22791;&#19978;&#36827;&#34892;&#22823;&#37327;&#26679;&#26412;&#30340;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#23558;&#39044;&#27979;&#22120;&#36716;&#31227;&#21040;\textit{&#27979;&#35797;}&#65288;&#30446;&#26631;&#65289;&#35774;&#22791;&#19978;&#65292;&#36825;&#20123;&#39044;&#27979;&#27169;&#22411;&#30340;&#26679;&#26412;&#25928;&#29575;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#12290;&#36801;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#27492;&#65292;&#20294;&#24448;&#24448;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#21464;&#24322;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#24310;&#36831;&#39044;&#27979;&#22120;&#30340;&#35780;&#20272;&#20027;&#35201;&#26159;&#22312;&#25163;&#24037;&#23450;&#21046;&#30340;&#35757;&#32451;/&#27979;&#35797;&#35774;&#22791;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#30830;&#23450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02446v1 Announce Type: new  Abstract: Efficient deployment of neural networks (NN) requires the co-optimization of accuracy and latency. For example, hardware-aware neural architecture search has been used to automatically find NN architectures that satisfy a latency constraint on a specific hardware device. Central to these search algorithms is a prediction model that is designed to provide a hardware latency estimate for a candidate NN architecture. Recent research has shown that the sample efficiency of these predictive models can be greatly improved through pre-training on some \textit{training} devices with many samples, and then transferring the predictor on the \textit{test} (target) device. Transfer learning and meta-learning methods have been used for this, but often exhibit significant performance variability. Additionally, the evaluation of existing latency predictors has been largely done on hand-crafted training/test device sets, making it difficult to ascertain
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31934;&#30830;&#20998;&#21106;&#32974;&#20799;&#22823;&#33041;&#32452;&#32455;&#30340;&#35299;&#21078;&#32422;&#26463;&#32420;&#32500;&#26463;&#36861;&#36394;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#33258;&#21160;&#20998;&#21106;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#23545;&#32974;&#20799;&#22823;&#33041;&#30340;&#32420;&#32500;&#26463;&#36861;&#36394;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02444</link><description>&lt;p&gt;
&#32974;&#20799;&#22823;&#33041;&#35299;&#21078;&#32422;&#26463;&#19979;&#30340;&#32420;&#32500;&#26463;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Anatomically Constrained Tractography of the Fetal Brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31934;&#30830;&#20998;&#21106;&#32974;&#20799;&#22823;&#33041;&#32452;&#32455;&#30340;&#35299;&#21078;&#32422;&#26463;&#32420;&#32500;&#26463;&#36861;&#36394;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#33258;&#21160;&#20998;&#21106;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#23545;&#32974;&#20799;&#22823;&#33041;&#30340;&#32420;&#32500;&#26463;&#36861;&#36394;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#25958;&#36335;:2403.02444v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36807;&#25688;&#35201;&#65306;&#25193;&#25955;&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#34987;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#30740;&#31350;&#23376;&#23467;&#20869;&#30340;&#32974;&#20799;&#22823;&#33041;&#12290; dMRI&#20351;&#24471;&#27969;&#32447;&#36861;&#36394;&#25104;&#20026;&#21487;&#33021;&#30340;&#37325;&#35201;&#35745;&#31639;&#65292;&#20854;&#20855;&#26377;&#29420;&#29305;&#30340;&#24212;&#29992;&#65292;&#22914;&#23545;&#33041;&#30333;&#36136;&#36827;&#34892;&#32420;&#32500;&#26463;&#29305;&#24322;&#24615;&#20998;&#26512;&#21644;&#32467;&#26500;&#36830;&#25509;&#24615;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32974;&#20799;dMRI&#25968;&#25454;&#36136;&#37327;&#36739;&#20302;&#19988;&#32420;&#32500;&#26463;&#36861;&#36394;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#39640;&#24230;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#23427;&#20204;&#29983;&#25104;&#35768;&#22810;&#34394;&#20551;&#30340;&#27969;&#32447;&#65292;&#21516;&#26102;&#26410;&#33021;&#37325;&#26500;&#26500;&#25104;&#20027;&#35201;&#30333;&#36136;&#32420;&#32500;&#26463;&#30340;&#27969;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#22312;dMRI&#31354;&#38388;&#20013;&#30452;&#25509;&#23545;&#32974;&#20799;&#22823;&#33041;&#32452;&#32455;&#36827;&#34892;&#20934;&#30830;&#20998;&#21106;&#30340;&#35299;&#21078;&#32422;&#26463;&#32420;&#32500;&#26463;&#36861;&#36394;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#33258;&#21160;&#35745;&#31639;&#20998;&#21106;&#12290;&#23545;&#29420;&#31435;&#27979;&#35797;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#20998;&#21106;&#32974;&#20799;&#22823;&#33041;&#32452;&#32455;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;tra
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02444v1 Announce Type: cross  Abstract: Diffusion-weighted Magnetic Resonance Imaging (dMRI) is increasingly used to study the fetal brain in utero. An important computation enabled by dMRI is streamline tractography, which has unique applications such as tract-specific analysis of the brain white matter and structural connectivity assessment. However, due to the low fetal dMRI data quality and the challenging nature of tractography, existing methods tend to produce highly inaccurate results. They generate many false streamlines while failing to reconstruct streamlines that constitute the major white matter tracts. In this paper, we advocate for anatomically constrained tractography based on an accurate segmentation of the fetal brain tissue directly in the dMRI space. We develop a deep learning method to compute the segmentation automatically. Experiments on independent test data show that this method can accurately segment the fetal brain tissue and drastically improve tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26681;&#26412;&#19978;&#35299;&#20915;&#20010;&#24615;&#21270;&#24191;&#21578;&#20013;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20998;&#26512;&#39044;&#27979;&#24322;&#24120;&#12290;</title><link>https://arxiv.org/abs/2403.02439</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021; (XAI) &#20998;&#26512;&#39044;&#27979;&#24322;&#24120;&#26681;&#26412;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Root Causing Prediction Anomalies Using Explainable AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26681;&#26412;&#19978;&#35299;&#20915;&#20010;&#24615;&#21270;&#24191;&#21578;&#20013;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20998;&#26512;&#39044;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#26681;&#22240;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#20013;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#19981;&#26029;&#20174;&#29992;&#25143;&#21442;&#19982;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#21333;&#20010;&#29305;&#24449;&#25439;&#22351;&#21487;&#33021;&#23548;&#33268;&#32423;&#32852;&#29305;&#24449;&#12289;&#26631;&#31614;&#21644;&#27010;&#24565;&#28418;&#31227;&#12290;&#25105;&#20204;&#24050;&#25104;&#21151;&#23558;&#36825;&#19968;&#25216;&#26415;&#24212;&#29992;&#20110;&#25552;&#39640;&#20010;&#24615;&#21270;&#24191;&#21578;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#24615;&#33021;&#19979;&#38477;&#34920;&#29616;&#20026;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02439v1 Announce Type: cross  Abstract: This paper presents a novel application of explainable AI (XAI) for root-causing performance degradation in machine learning models that learn continuously from user engagement data. In such systems a single feature corruption can cause cascading feature, label and concept drifts. We have successfully applied this technique to improve the reliability of models used in personalized advertising. Performance degradation in such systems manifest as prediction anomalies in the models. These models are typically trained continuously using features that are produced by hundreds of real time data processing pipelines or derived from other upstream models. A failure in any of these pipelines or an instability in any of the upstream models can cause feature corruption, causing the model's predicted output to deviate from the actual output and the training data to become corrupted. The causal relationship between the features and the predicted ou
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#20419;&#20351;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.02437</link><description>&lt;p&gt;
SoK: &#32852;&#37030;&#21453;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
SoK: Challenges and Opportunities in Federated Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02437
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#20419;&#20351;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20110;2017&#24180;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20419;&#36827;&#20102;&#19981;&#20449;&#20219;&#26041;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#26080;&#38656;&#21508;&#26041;&#26126;&#30830;&#20849;&#20139;&#20854;&#25968;&#25454;&#12290;&#36825;&#20801;&#35768;&#22312;&#23562;&#37325;GDPR&#21644;CPRA&#31561;&#38544;&#31169;&#35268;&#23450;&#30340;&#21516;&#26102;&#65292;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26032;&#20852;&#30340;&#38544;&#31169;&#35201;&#27714;&#21487;&#33021;&#35201;&#27714;&#27169;&#22411;&#25152;&#26377;&#32773;&#33021;&#22815;&#8220;&#36951;&#24536;&#8221;&#19968;&#20123;&#24050;&#23398;&#20064;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#24403;&#25968;&#25454;&#25152;&#26377;&#32773;&#25110;&#25191;&#27861;&#26426;&#26500;&#35201;&#27714;&#26102;&#12290;&#36825;&#20652;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#26426;&#22120;&#21453;&#23398;&#20064;&#8221;&#30340;&#27963;&#36291;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;FL&#30340;&#32972;&#26223;&#19979;&#65292;&#35768;&#22810;&#20026;&#38598;&#20013;&#24335;&#29615;&#22659;&#24320;&#21457;&#30340;&#21453;&#23398;&#20064;&#25216;&#26415;&#24182;&#19981;&#23481;&#26131;&#24212;&#29992;&#65281;&#36825;&#26159;&#30001;&#20110;FL&#20013;&#38598;&#20013;&#24335;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#20043;&#38388;&#30340;&#29420;&#29305;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#20114;&#21160;&#24615;&#12289;&#38543;&#26426;&#24615;&#12289;&#24322;&#26500;&#24615;&#21644;&#26377;&#38480;&#21487;&#35775;&#38382;&#24615;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#24037;&#20316;&#32858;&#28966;&#20110;&#24320;&#21457;&#36866;&#29992;&#20110;FL&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02437v1 Announce Type: cross  Abstract: Federated learning (FL), introduced in 2017, facilitates collaborative learning between non-trusting parties with no need for the parties to explicitly share their data among themselves. This allows training models on user data while respecting privacy regulations such as GDPR and CPRA. However, emerging privacy requirements may mandate model owners to be able to \emph{forget} some learned data, e.g., when requested by data owners or law enforcement. This has given birth to an active field of research called \emph{machine unlearning}. In the context of FL, many techniques developed for unlearning in centralized settings are not trivially applicable! This is due to the unique differences between centralized and distributed learning, in particular, interactivity, stochasticity, heterogeneity, and limited accessibility in FL. In response, a recent line of work has focused on developing unlearning mechanisms tailored to FL.   This SoK pape
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27979;&#24230;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#21442;&#25968;ML&#27169;&#22411;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#20013;&#23398;&#20064;&#20195;&#29702;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#20285;&#29595;&#25910;&#25947;&#30340;&#31867;&#20284;&#27861;&#22270;&#24341;&#29702;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.02432</link><description>&lt;p&gt;
&#20851;&#20110;&#27979;&#24230;&#39044;&#22788;&#29702;&#23545;&#36890;&#29992;&#21442;&#25968;ML&#27169;&#22411;&#21644;&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02432
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#24230;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#21442;&#25968;ML&#27169;&#22411;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#20013;&#23398;&#20064;&#20195;&#29702;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#20285;&#29595;&#25910;&#25947;&#30340;&#31867;&#20284;&#27861;&#22270;&#24341;&#29702;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#29702;&#35299;&#22312;&#25968;&#25454;&#30053;&#24494;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20195;&#29702;&#30340;&#25910;&#25947;&#24615;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#25910;&#25947;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#31867;&#20284;&#20110;&#27861;&#22270;&#24341;&#29702;&#30340;&#27169;&#25311;&#29702;&#35299;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#20285;&#29595;&#25910;&#25947;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#25216;&#26415;&#22312;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#30456;&#20851;&#24615;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02432v1 Announce Type: cross  Abstract: We study a new technique for understanding convergence of learning agents under small modifications of data. We show that such convergence can be understood via an analogue of Fatou's lemma which yields gamma-convergence. We show it's relevance and applications in general machine learning tasks and domain adaptation transfer learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#20943;&#23569;&#26435;&#37325;&#25968;&#37327;&#24182;&#38450;&#27490;&#31934;&#24230;&#28798;&#38590;&#24615;&#19979;&#38477;&#65292;&#20197;&#22312;&#26377;&#38480;&#26102;&#38388;&#21644;&#20869;&#23384;&#32422;&#26463;&#30340;&#23454;&#26102;&#31995;&#32479;&#20013;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02429</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#25928;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards efficient deep autoencoders for multivariate time series anomaly detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#20943;&#23569;&#26435;&#37325;&#25968;&#37327;&#24182;&#38450;&#27490;&#31934;&#24230;&#28798;&#38590;&#24615;&#19979;&#38477;&#65292;&#20197;&#22312;&#26377;&#38480;&#26102;&#38388;&#21644;&#20869;&#23384;&#32422;&#26463;&#30340;&#23454;&#26102;&#31995;&#32479;&#20013;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22312;&#35768;&#22810;&#24037;&#19994;&#21644;&#30740;&#31350;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#21450;&#26102;&#26816;&#27979;&#24322;&#24120;&#20801;&#35768;&#38450;&#27490;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#32570;&#38519;&#21644;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#25925;&#38556;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#20854;&#23545;&#22797;&#26434;&#22810;&#21464;&#37327;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#21644;&#31283;&#20581;&#20998;&#26512;&#30340;&#29305;&#28857;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#33021;&#22815;&#21450;&#26102;&#25552;&#21462;&#39044;&#27979;&#32467;&#26524;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#23454;&#26102;&#38656;&#27714;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#27169;&#22411;&#32553;&#20943;&#23545;&#20110;&#22312;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#21644;&#20869;&#23384;&#32422;&#26463;&#30340;&#23454;&#26102;&#31995;&#32479;&#20013;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#28041;&#21450;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#26032;&#21387;&#32553;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#20462;&#21098;&#20943;&#23569;&#26435;&#37325;&#25968;&#37327;&#65292;&#21516;&#26102;&#36890;&#36807;&#24555;&#36895;&#25628;&#32034;&#36807;&#31243;&#38450;&#27490;&#31934;&#24230;&#30340;&#28798;&#38590;&#24615;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02429v1 Announce Type: cross  Abstract: Multivariate time series anomaly detection is a crucial problem in many industrial and research applications. Timely detection of anomalies allows, for instance, to prevent defects in manufacturing processes and failures in cyberphysical systems. Deep learning methods are preferred among others for their accuracy and robustness for the analysis of complex multivariate data. However, a key aspect is being able to extract predictions in a timely manner, to accommodate real-time requirements in different applications. In the case of deep learning models, model reduction is extremely important to achieve optimal results in real-time systems with limited time and memory constraints. In this paper, we address this issue by proposing a novel compression method for deep autoencoders that involves three key factors. First, pruning reduces the number of weights, while preventing catastrophic drops in accuracy by means of a fast search process th
&lt;/p&gt;</description></item><item><title>&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#22312;&#22303;&#26408;&#24037;&#31243;&#39046;&#22495;&#30340;&#37319;&#29992;&#31574;&#30053;&#38656;&#35201;&#37325;&#26032;&#23450;&#20301;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#38454;&#27573;&#30340;&#25361;&#25112;&#21644;&#24212;&#29992;&#20998;&#25955;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02426</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#21644;&#22303;&#26408;&#24037;&#31243;&#38454;&#27573;&#65306;&#37325;&#26032;&#23450;&#20301;&#37319;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Digital Twins and Civil Engineering Phases: Reorienting Adoption Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02426
&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#22312;&#22303;&#26408;&#24037;&#31243;&#39046;&#22495;&#30340;&#37319;&#29992;&#31574;&#30053;&#38656;&#35201;&#37325;&#26032;&#23450;&#20301;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#38454;&#27573;&#30340;&#25361;&#25112;&#21644;&#24212;&#29992;&#20998;&#25955;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#25216;&#26415;&#22810;&#24180;&#26469;&#21463;&#21040;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20026;&#31185;&#23398;&#21644;&#24037;&#31243;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#24102;&#26469;&#20102;&#35768;&#22810;&#25215;&#35834;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#25506;&#35752;&#20102;&#25968;&#23383;&#23402;&#29983;&#30340;&#19981;&#21516;&#20027;&#39064;&#39046;&#22495;&#12290;&#22303;&#26408;&#24037;&#31243;&#31561;&#29305;&#23450;&#39046;&#22495;&#20063;&#19981;&#20363;&#22806;&#65292;&#23548;&#33268;&#20102;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#24212;&#29992;&#30340;&#30862;&#29255;&#21270;&#26041;&#27861;&#12290;&#22303;&#26408;&#24037;&#31243;&#34892;&#19994;&#22312;&#36825;&#26041;&#38754;&#36827;&#19968;&#27493;&#22788;&#20110;&#19981;&#21033;&#22320;&#20301;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20854;&#20182;&#24037;&#31243;&#39046;&#22495;&#30340;&#22806;&#37096;&#25216;&#26415;&#26469;&#36827;&#34892;&#25968;&#23383;&#23402;&#29983;&#30340;&#37319;&#29992;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#19968;&#20010;&#19981;&#26029;&#22686;&#21152;&#30340;&#21518;&#26524;&#26159;&#23558;&#25968;&#23383;&#23402;&#29983;&#38598;&#20013;&#24212;&#29992;&#20110;&#36816;&#33829;&#21644;&#32500;&#25252;&#38454;&#27573;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24314;&#31569;&#20449;&#24687;&#24314;&#27169;&#65288;BIM&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#35268;&#21010;/&#35774;&#35745;&#38454;&#27573;&#65292;&#32780;&#26045;&#24037;&#38454;&#27573;&#30340;&#30636;&#21464;&#24615;&#36136;&#23545;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#37319;&#29992;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23383;&#23402;&#29983;&#22312;&#24314;&#31569;&#35774;&#35745;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02426v1 Announce Type: cross  Abstract: Digital twin (DT) technology has received immense attention over the years due to the promises it presents to various stakeholders in science and engineering. As a result, different thematic areas of DT have been explored. This is no different in specific fields such as manufacturing, automation, oil and gas, and civil engineering, leading to fragmented approaches for field-specific applications. The civil engineering industry is further disadvantaged in this regard as it relies on external techniques by other engineering fields for its DT adoption. A rising consequence of these extensions is a concentrated application of DT to the operations and maintenance phase. On another spectrum, Building Information Modeling (BIM) are pervasively utilized in the planning/design phase, and the transient nature of the construction phase remains a challenge for its DT adoption. In this paper, we present a phase-based development of DT in the Archit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#21457;&#29616;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;LLM&#35843;&#29992;&#27425;&#25968;&#22686;&#21152;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.02419</link><description>&lt;p&gt;
&#20320;&#38656;&#35201;&#26356;&#22810;LLM&#35843;&#29992;&#21527;&#65311;&#36208;&#21521;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#21457;&#29616;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;LLM&#35843;&#29992;&#27425;&#25968;&#22686;&#21152;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#26159;&#36890;&#36807;&#25191;&#34892;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35843;&#29992;&#24182;&#27719;&#24635;&#23427;&#20204;&#30340;&#21709;&#24212;&#30340;&#22797;&#21512;&#31995;&#32479;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#35843;&#29992;&#27425;&#25968;&#30340;&#24433;&#21709; -- &#20363;&#22914;&#65292;&#24403;&#35201;&#27714;LLM&#22810;&#27425;&#22238;&#31572;&#27599;&#20010;&#38382;&#39064;&#24182;&#21462;&#24471;&#20849;&#35782;&#26102; -- &#23545;&#20110;&#36825;&#31181;&#22797;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLM&#35843;&#29992;&#27425;&#25968;&#22914;&#20309;&#24433;&#21709;&#19968;&#20010;&#23618;&#32423;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021; -- &#36825;&#26159;&#26368;&#31616;&#21333;&#30340;&#22797;&#21512;&#31995;&#32479;&#20043;&#19968;&#65292;&#23427;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#32858;&#21512;LLM&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;&#30528;LLM&#35843;&#29992;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#38750;&#21333;&#35843;&#24615;&#26159;&#30001;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02419v1 Announce Type: cross  Abstract: Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Large Language Model (LLM) calls and aggregate their responses. However, there is little understanding of how the number of LLM calls -- e.g., when asking the LLM to answer each question multiple times and taking a consensus -- affects such a compound system's performance. In this paper, we initiate the study of scaling laws of compound inference systems. We analyze, theoretically and empirically, how the number of LLM calls affects the performance of one-layer Voting Inference Systems -- one of the simplest compound systems, which aggregates LLM responses via majority voting. We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls. Our theoretical results suggest that this non-monotonicity is due
&lt;/p&gt;</description></item><item><title>&#23616;&#37096;&#26354;&#29575;&#21464;&#21270;&#23548;&#33268;&#31995;&#32479;&#20174;&#33391;&#24615;&#19988;&#23500;&#26377;&#20449;&#24687;&#30340;&#23616;&#37096;&#26223;&#35266;&#36880;&#28176;&#38519;&#20837;&#26080;&#20449;&#24687;&#30340;&#36855;&#23467;&#65292;&#20851;&#38190;&#36716;&#21464;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;Hessian&#30340;&#38408;&#20540;&#26377;&#20851;&#12290;</title><link>https://arxiv.org/abs/2403.02418</link><description>&lt;p&gt;
&#20174;&#38646;&#21040;&#33521;&#38596;&#65306;&#26080;&#30693;&#21021;&#20540;&#22788;&#30340;&#23616;&#37096;&#26354;&#29575;&#22914;&#20309;&#36828;&#31163;&#31967;&#31957;&#30340;&#26497;&#23567;&#20540;
&lt;/p&gt;
&lt;p&gt;
From Zero to Hero: How local curvature at artless initial conditions leads away from bad minima
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02418
&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#26354;&#29575;&#21464;&#21270;&#23548;&#33268;&#31995;&#32479;&#20174;&#33391;&#24615;&#19988;&#23500;&#26377;&#20449;&#24687;&#30340;&#23616;&#37096;&#26223;&#35266;&#36880;&#28176;&#38519;&#20837;&#26080;&#20449;&#24687;&#30340;&#36855;&#23467;&#65292;&#20851;&#38190;&#36716;&#21464;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;Hessian&#30340;&#38408;&#20540;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26799;&#24230;&#19979;&#38477;&#22312;&#38750;&#20984;&#21644;&#39640;&#32500;&#35774;&#32622;&#20013;&#30340;&#20248;&#21270;&#21160;&#21147;&#23398;&#65292;&#37325;&#28857;&#20851;&#27880;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#20316;&#20026;&#22797;&#26434;&#25439;&#22833;&#26223;&#35266;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#23616;&#37096;&#26354;&#29575;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20013;&#38388;&#20449;&#22122;&#27604;&#19979;&#65292;Hessian&#22312;&#19979;&#38477;&#30340;&#31532;&#19968;&#20010;&#38454;&#27573;&#26174;&#31034;&#20986;&#25351;&#21521;&#22909;&#26497;&#23567;&#20540;&#30340;&#19979;&#38477;&#26041;&#21521;&#65292;&#28982;&#21518;&#22312;&#32467;&#26463;&#26102;&#34987;&#22256;&#22312;&#31967;&#31957;&#30340;&#26497;&#23567;&#20540;&#20013;&#12290;&#22240;&#27492;&#65292;&#23616;&#37096;&#26223;&#35266;&#36215;&#21021;&#26159;&#33391;&#24615;&#19988;&#23500;&#26377;&#20449;&#24687;&#30340;&#65292;&#28982;&#21518;&#26799;&#24230;&#19979;&#38477;&#23558;&#31995;&#32479;&#24102;&#20837;&#26080;&#20449;&#24687;&#30340;&#36855;&#23467;&#12290;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#30340;&#36716;&#21464;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;Hessian&#30340;BBP&#31867;&#22411;&#38408;&#20540;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02418v1 Announce Type: new  Abstract: We investigate the optimization dynamics of gradient descent in a non-convex and high-dimensional setting, with a focus on the phase retrieval problem as a case study for complex loss landscapes. We first study the high-dimensional limit where both the number $M$ and the dimension $N$ of the data are going to infinity at fixed signal-to-noise ratio $\alpha = M/N$. By analyzing how the local curvature changes during optimization, we uncover that for intermediate $\alpha$, the Hessian displays a downward direction pointing towards good minima in the first regime of the descent, before being trapped in bad minima at the end. Hence, the local landscape is benign and informative at first, before gradient descent brings the system into a uninformative maze. The transition between the two regimes is associated to a BBP-type threshold in the time-dependent Hessian. Through both theoretical analysis and numerical experiments, we show that in prac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.02411</link><description>&lt;p&gt;
NiNformer: &#19968;&#31181;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#30340;&#32593;&#32476;&#20013;&#32593;&#32476;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#20027;&#35201;&#32452;&#20214;&#65292;&#33258;&#24341;&#20837;&#20197;&#26469;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36328;&#36234;&#20102;&#35768;&#22810;&#39046;&#22495;&#21644;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26426;&#21046;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#34987;&#24212;&#29992;&#20026;Vision Transformer ViT&#65292;&#24182;&#19988;&#20854;&#29992;&#36884;&#24050;&#25193;&#23637;&#21040;&#35270;&#35273;&#39046;&#22495;&#30340;&#35768;&#22810;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#23613;&#31649;&#35813;&#26426;&#21046;&#38750;&#24120;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#33021;&#21147;&#65292;&#20294;&#20854;&#32570;&#28857;&#26159;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#26377;&#25928;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#35774;&#35745;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#21644;&#32531;&#35299;&#25968;&#25454;&#22823;&#23567;&#35201;&#27714;&#12290;&#22312;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#20123;&#23581;&#35797;&#30340;&#20363;&#23376;&#21253;&#25324;MLP-Mixer&#12289;Conv-Mixer&#12289;Perciver-IO&#31561;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#20316;&#20026;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02411v1 Announce Type: cross  Abstract: The Attention mechanism is the main component of the Transformer architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks. The Attention Mechanism was utilized in Computer Vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as an 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25913;&#36827;&#21464;&#20998;&#31639;&#27861;&#65292;&#20351;&#29992;&#28176;&#36817;&#27973;&#23618;&#30005;&#36335;&#23545;&#26102;&#23578;-MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#30721;&#65292;&#20026;&#26410;&#26469;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#23454;&#35777;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02405</link><description>&lt;p&gt;
&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#23545;&#26102;&#23578;-MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of the Fashion-MNIST Dataset on a Quantum Computer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02405
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;&#21464;&#20998;&#31639;&#27861;&#65292;&#20351;&#29992;&#28176;&#36817;&#27973;&#23618;&#30005;&#36335;&#23545;&#26102;&#23578;-MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#30721;&#65292;&#20026;&#26410;&#26469;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#23454;&#35777;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02405v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#24037;&#19994;&#24212;&#29992;&#30340;&#28508;&#22312;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#25391;&#22859;&#30340;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#23558;&#32463;&#20856;&#25968;&#25454;&#32534;&#30721;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24120;&#35268;&#26041;&#27861;&#19981;&#20165;&#23545;&#31639;&#27861;&#30340;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#25104;&#26412;&#36807;&#39640;&#65292;&#32780;&#19988;&#20005;&#37325;&#38480;&#21046;&#20102;&#24403;&#21069;&#30828;&#20214;&#19978;&#21487;&#34892;&#23454;&#39564;&#30340;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#20182;&#20204;&#30340;&#31639;&#27861;&#36866;&#29992;&#20110;&#36817;&#26399;&#65292;&#20294;&#21364;&#27809;&#26377;&#22312;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#25913;&#36827;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#21464;&#20998;&#31639;&#27861;[1]&#26469;&#35299;&#20915;&#25968;&#25454;&#32534;&#30721;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#36817;&#20284;&#20934;&#22791;&#32534;&#30721;&#25968;&#25454;&#65292;&#20351;&#29992;&#19982;&#24403;&#21069;&#21487;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#26412;&#26426;&#38376;&#38598;&#21644;&#25299;&#25169;&#30456;&#36866;&#24212;&#30340;&#28176;&#36817;&#27973;&#23618;&#30005;&#36335;&#12290;&#25105;&#20204;&#23558;&#25913;&#36827;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#23545;Fashion-MNIST&#25968;&#25454;&#38598;[2]&#36827;&#34892;&#32534;&#30721;&#65292;&#21487;&#20197;&#30452;&#25509;&#22312;&#26410;&#26469;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#23454;&#35777;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02405v1 Announce Type: cross  Abstract: The potential impact of quantum machine learning algorithms on industrial applications remains an exciting open question. Conventional methods for encoding classical data into quantum computers are not only too costly for a potential quantum advantage in the algorithms but also severely limit the scale of feasible experiments on current hardware. Therefore, recent works, despite claiming the near-term suitability of their algorithms, do not provide experimental benchmarking on standard machine learning datasets. We attempt to solve the data encoding problem by improving a recently proposed variational algorithm [1] that approximately prepares the encoded data, using asymptotically shallow circuits that fit the native gate set and topology of currently available quantum computers. We apply the improved algorithm to encode the Fashion-MNIST dataset [2], which can be directly used in future empirical studies of quantum machine learning al
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;OTClean&#26694;&#26550;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#32422;&#26463;&#19979;&#25968;&#25454;&#28165;&#27927;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20462;&#22797;&#38382;&#39064;&#36716;&#21270;&#20026;&#27491;&#21017;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21463;Sinkhorn&#30697;&#38453;&#32553;&#25918;&#31639;&#27861;&#21551;&#21457;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#21487;&#20280;&#32553;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.02372</link><description>&lt;p&gt;
OTClean&#65306;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#36829;&#35268;&#25968;&#25454;&#28165;&#27927;
&lt;/p&gt;
&lt;p&gt;
OTClean: Data Cleaning for Conditional Independence Violations using Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02372
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;OTClean&#26694;&#26550;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#32422;&#26463;&#19979;&#25968;&#25454;&#28165;&#27927;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20462;&#22797;&#38382;&#39064;&#36716;&#21270;&#20026;&#27491;&#21017;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21463;Sinkhorn&#30697;&#38453;&#32553;&#25918;&#31639;&#27861;&#21551;&#21457;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#21487;&#20280;&#32553;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#32422;&#26463;&#23545;&#20110;&#20844;&#24179;&#21644;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OTClean&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#22312;CI&#32422;&#26463;&#19979;&#36827;&#34892;&#25968;&#25454;&#20462;&#22797;&#12290;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#30830;&#20445;&#23545;&#25968;&#25454;&#25928;&#29992;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#23558;&#28041;&#21450;CI&#30340;&#25968;&#25454;&#20462;&#22797;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20108;&#27425;&#32422;&#26463;&#32447;&#24615;&#35268;&#21010;&#65288;QCLP&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#20132;&#26367;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#65288;&#22914;Wasserstein&#36317;&#31163;&#65289;&#25152;&#28041;&#21450;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#21487;&#20280;&#32553;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#27491;&#21017;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#19968;&#20010;&#21463;Sinkhorn&#30697;&#38453;&#32553;&#25918;&#31639;&#27861;&#21551;&#21457;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02372v1 Announce Type: cross  Abstract: Ensuring Conditional Independence (CI) constraints is pivotal for the development of fair and trustworthy machine learning models. In this paper, we introduce \sys, a framework that harnesses optimal transport theory for data repair under CI constraints. Optimal transport theory provides a rigorous framework for measuring the discrepancy between probability distributions, thereby ensuring control over data utility. We formulate the data repair problem concerning CIs as a Quadratically Constrained Linear Program (QCLP) and propose an alternating method for its solution. However, this approach faces scalability issues due to the computational cost associated with computing optimal transport distances, such as the Wasserstein distance. To overcome these scalability challenges, we reframe our problem as a regularized optimization problem, enabling us to develop an iterative algorithm inspired by Sinkhorn's matrix scaling algorithm, which e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#29305;&#24449;&#37325;&#35201;&#24615;&#26816;&#27979;&#21644;&#29305;&#24449;&#20132;&#20114;&#26816;&#27979;&#65292;&#20197;&#25552;&#39640;&#24037;&#19994;4.0&#24212;&#29992;&#20013;&#30340;&#39044;&#27979;&#20248;&#21270;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02368</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#24037;&#19994;4.0&#24212;&#29992;&#20013;&#39044;&#27979;&#20248;&#21270;&#30340;&#26032;&#22411;&#28151;&#21512;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#20132;&#20114;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Novel Hybrid Feature Importance and Feature Interaction Detection Framework for Predictive Optimization in Industry 4.0 Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02368
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#29305;&#24449;&#37325;&#35201;&#24615;&#26816;&#27979;&#21644;&#29305;&#24449;&#20132;&#20114;&#26816;&#27979;&#65292;&#20197;&#25552;&#39640;&#24037;&#19994;4.0&#24212;&#29992;&#20013;&#30340;&#39044;&#27979;&#20248;&#21270;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;4.0&#20013;&#25552;&#20379;&#22522;&#20110;&#25968;&#25454;&#30340;&#39044;&#27979;&#21644;&#20915;&#31574;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#25152;&#36798;&#21040;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19981;&#36275;&#20197;&#20445;&#35777;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;&#23454;&#26045;&#12290;&#36825;&#26159;&#22240;&#20026;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#24182;&#38750;&#25152;&#26377;&#29305;&#24449;&#37117;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#39044;&#27979;&#20998;&#26512;&#30452;&#25509;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#31934;&#24515;&#36873;&#25321;&#29305;&#24449;&#30340;&#32467;&#21512;&#26377;&#28508;&#21147;&#23545;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#31215;&#26497;&#24433;&#21709;&#12290;&#20026;&#20102;&#22635;&#34917;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#23558;&#29305;&#24449;&#37325;&#35201;&#24615;&#26816;&#27979;&#22120;&#8212;&#8212;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#65288;LIME&#65289;&#21644;&#29305;&#24449;&#20132;&#20114;&#26816;&#27979;&#22120;&#8212;&#8212;&#31070;&#32463;&#20132;&#20114;&#26816;&#27979;&#65288;NID&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#65292;&#24182;&#23545;&#20132;&#20114;&#20316;&#29992;&#36827;&#34892;&#32534;&#30721;&#20197;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02368v1 Announce Type: cross  Abstract: Advanced machine learning algorithms are increasingly utilized to provide data-based prediction and decision-making support in Industry 4.0. However, the prediction accuracy achieved by the existing models is insufficient to warrant practical implementation in real-world applications. This is because not all features present in real-world datasets possess a direct relevance to the predictive analysis being conducted. Consequently, the careful incorporation of select features has the potential to yield a substantial positive impact on the outcome. To address the research gap, this paper proposes a novel hybrid framework that combines the feature importance detector - local interpretable model-agnostic explanations (LIME) and the feature interaction detector - neural interaction detection (NID), to improve prediction accuracy. By applying the proposed framework, unnecessary features can be eliminated, and interactions are encoded to gene
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#31614;&#20462;&#22797;&#21644;&#22810;&#19987;&#23478;&#38598;&#25104;&#23398;&#20064;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#38271;&#23614;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02363</link><description>&lt;p&gt;
&#35299;&#20915;&#38271;&#23614;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#65306;&#32771;&#34385;&#26631;&#31614;&#31232;&#26377;&#24615;&#30340;&#20004;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution with Label Refurbishment Considering Label Rarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02363
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#31614;&#20462;&#22797;&#21644;&#22810;&#19987;&#23478;&#38598;&#25104;&#23398;&#20064;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#38271;&#23614;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#34920;&#29616;&#20986;&#22024;&#26434;&#26631;&#31614;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#22914;&#38271;&#23614;&#20998;&#24067;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#21306;&#20998;&#22024;&#26434;&#21644;&#24178;&#20928;&#26679;&#26412;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20381;&#36182;&#20110;&#22522;&#20110;&#22024;&#26434;&#38271;&#23614;&#25968;&#25454;&#30340;&#39044;&#27979;&#20449;&#24687;&#20250;&#24341;&#20837;&#28508;&#22312;&#38169;&#35823;&#12290;&#20026;&#20102;&#20811;&#26381;&#20808;&#21069;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#23558;&#36719;&#26631;&#31614;&#20462;&#22797;&#19982;&#22810;&#19987;&#23478;&#38598;&#25104;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#22312;&#31283;&#20581;&#30340;&#36719;&#26631;&#31614;&#20462;&#22797;&#30340;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#33719;&#24471;&#26080;&#20559;&#29305;&#24449;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;BAlanced Noise-tolerant Cross-entropy (BANC) &#25439;&#22833;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#21021;&#27493;&#39044;&#27979;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26631;&#31614;&#20462;&#22797;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#20026;&#22810;&#19987;&#23478;&#38598;&#25104;&#23398;&#20064;&#33719;&#21462;&#36719;&#26631;&#31614;&#65292;&#20026;&#38271;&#23614;&#22024;&#26434;&#26631;&#31614;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02363v1 Announce Type: cross  Abstract: Real-world datasets commonly exhibit noisy labels and class imbalance, such as long-tailed distributions. While previous research addresses this issue by differentiating noisy and clean samples, reliance on information from predictions based on noisy long-tailed data introduces potential errors. To overcome the limitations of prior works, we introduce an effective two-stage approach by combining soft-label refurbishing with multi-expert ensemble learning. In the first stage of robust soft label refurbishing, we acquire unbiased features through contrastive learning, making preliminary predictions using a classifier trained with a carefully designed BAlanced Noise-tolerant Cross-entropy (BANC) loss. In the second stage, our label refurbishment method is applied to obtain soft labels for multi-expert ensemble learning, providing a principled solution to the long-tail noisy label problem. Experiments conducted across multiple benchmarks v
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#20113;&#36793;&#27169;&#22411;&#35299;&#32806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#23450;&#21046;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#25429;&#33719;&#20849;&#20139;&#34920;&#31034;&#30340;&#20027;&#20307;&#21644;&#22788;&#29702;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#20010;&#24615;&#21270;&#22836;&#37096;&#65292;&#20197;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02360</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#23450;&#21046;&#26550;&#26500;&#30740;&#31350;: &#23545;&#27604;&#20113;&#36793;&#27169;&#22411;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Customized Architecture for Heterogeneous Federated Learning with Contrastive Cloud-Edge Model Decoupling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02360
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#20113;&#36793;&#27169;&#22411;&#35299;&#32806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#23450;&#21046;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#25429;&#33719;&#20849;&#20139;&#34920;&#31034;&#30340;&#20027;&#20307;&#21644;&#22788;&#29702;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#20010;&#24615;&#21270;&#22836;&#37096;&#65292;&#20197;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#32593;&#32476;&#36793;&#32536;&#23458;&#25143;&#31471;&#20043;&#38388;&#20840;&#23616;&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20013;&#22830;&#25968;&#25454;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#26500;&#24615;&#20351;&#24471;&#27169;&#22411;&#20542;&#21521;&#20110;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#36825;&#20123;&#26497;&#23567;&#20540;&#21487;&#33021;&#36828;&#31163;&#20840;&#23616;&#26368;&#20248;&#12290;&#36825;&#31181;&#24322;&#26500;&#24615;&#36890;&#24120;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#19988;&#36890;&#20449;&#24320;&#38144;&#24040;&#22823;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCMD&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#36866;&#24212;&#20113;&#36793;&#25903;&#25345;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#22411;&#35299;&#32806;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#29992;&#20110;&#33719;&#21462;&#20113;&#31471;&#20849;&#20139;&#34920;&#31034;&#30340;&#20027;&#20307;&#21644;&#29992;&#20110;&#36801;&#31227;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#20010;&#24615;&#21270;&#22836;&#37096;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#65292;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#36873;&#25321;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#23618;&#20316;&#20026;&#20010;&#24615;&#21270;&#22836;&#37096;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23558;&#26368;&#21518;&#19968;&#23618;&#21018;&#24615;&#20998;&#37197;&#20026;&#20010;&#24615;&#21270;&#22836;&#37096;&#21487;&#33021;&#26080;&#27861;&#26368;&#22823;&#21270;&#25913;&#21892;&#24322;&#26500;&#24615;&#25968;&#25454;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#36873;&#25321;&#20010;&#24615;&#21270;&#22836;&#37096;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02360v1 Announce Type: cross  Abstract: Federated learning, as a promising distributed learning paradigm, enables collaborative training of a global model across multiple network edge clients without the need for central data collecting. However, the heterogeneity of edge data distribution drags the model towards the local minima, which can be distant from the global optimum. Such heterogeneity often leads to slow convergence and substantial communication overhead. To address these issues, we propose a novel federated learning framework called FedCMD, a model decoupling tailored to the Cloud-edge supported federated learning that separates deep neural networks into a body for capturing shared representations in Cloud and a personalized head for migrating data heterogeneity. Our motivation is that, by the deep investigation of the performance of selecting different neural network layers as the personalized head, we found rigidly assigning the last layer as the personalized he
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#21033;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22235;&#20803;&#25968;&#34920;&#31034;&#36827;&#34892;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#65292;&#30528;&#37325;&#25429;&#25417;&#26102;&#38388;&#25935;&#24863;&#20851;&#31995;&#65292;&#24182;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#21487;&#20197;&#24314;&#27169;&#21508;&#31181;&#20851;&#31995;&#27169;&#24335;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.02355</link><description>&lt;p&gt;
&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#21033;&#29992;&#26102;&#38388;&#25935;&#24863;&#20851;&#31995;&#36827;&#34892;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph Completion with Time-sensitive Relations in Hypercomplex Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#21033;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22235;&#20803;&#25968;&#34920;&#31034;&#36827;&#34892;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#65292;&#30528;&#37325;&#25429;&#25417;&#26102;&#38388;&#25935;&#24863;&#20851;&#31995;&#65292;&#24182;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#21487;&#20197;&#24314;&#27169;&#21508;&#31181;&#20851;&#31995;&#27169;&#24335;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#65288;TKGC&#65289;&#26088;&#22312;&#22312;&#32473;&#23450;&#29305;&#23450;&#26102;&#38388;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#20013;&#22635;&#34917;&#32570;&#22833;&#30340;&#20107;&#23454;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22235;&#20803;&#25968;&#34920;&#31034;&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;TKGC&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#25429;&#25417;&#26102;&#38388;&#25935;&#24863;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#65292;&#36890;&#36807;&#26102;&#38388;&#24863;&#30693;&#30340;&#26059;&#36716;&#21644;&#21608;&#26399;&#24615;&#26102;&#38388;&#24179;&#31227;&#26469;&#24314;&#27169;&#26102;&#38388;&#25935;&#24863;&#20851;&#31995;&#65292;&#26377;&#25928;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#24314;&#27169;&#23545;&#31216;&#12289;&#38750;&#23545;&#31216;&#12289;&#36870;&#21521;&#12289;&#32452;&#21512;&#21644;&#28436;&#21464;&#30340;&#20851;&#31995;&#27169;&#24335;&#12290;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#32508;&#21512;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02355v1 Announce Type: cross  Abstract: Temporal knowledge graph completion (TKGC) aims to fill in missing facts within a given temporal knowledge graph at a specific time. Existing methods, operating in real or complex spaces, have demonstrated promising performance in this task. This paper advances beyond conventional approaches by introducing more expressive quaternion representations for TKGC within hypercomplex space. Unlike existing quaternion-based methods, our study focuses on capturing time-sensitive relations rather than time-aware entities. Specifically, we model time-sensitive relations through time-aware rotation and periodic time translation, effectively capturing complex temporal variability. Furthermore, we theoretically demonstrate our method's capability to model symmetric, asymmetric, inverse, compositional, and evolutionary relation patterns. Comprehensive experiments on public datasets validate that our proposed approach achieves state-of-the-art perform
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#37329;&#23383;&#22612;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02354</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Field Neural Networks for Air Quality Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#37329;&#23383;&#22612;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#38382;&#39064;&#26088;&#22312;&#21033;&#29992;&#26469;&#33258;&#26377;&#38480;&#35266;&#27979;&#31449;&#30340;&#21382;&#21490;&#25968;&#25454;&#25512;&#26029;&#26410;&#30693;&#20301;&#32622;&#30340;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#12290;&#32771;&#34385;&#21040;&#35266;&#27979;&#31449;&#39640;&#26114;&#30340;&#32500;&#25252;&#25104;&#26412;&#23548;&#33268;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#33391;&#22909;&#30340;&#25512;&#26029;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#33410;&#32422;&#25104;&#26412;&#24182;&#32454;&#21270;&#25968;&#25454;&#31890;&#24230;&#12290;&#23613;&#31649;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#29616;&#23454;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#21644;&#31163;&#25955;&#25968;&#25454;&#32467;&#26500;&#24314;&#27169;&#38480;&#21046;&#20102;&#28508;&#21147;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#21363;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#65292;&#21450;&#20854;&#23545;&#24212;&#30340;&#26032;&#26694;&#26550;&#65292;&#37329;&#23383;&#22612;&#25512;&#26029;&#65292;&#23558;&#20004;&#31181;&#19981;&#21516;&#30340;&#26102;&#31354;&#35266;&#28857;&#65292;&#22330;&#21644;&#22270;&#65292;&#30456;&#32467;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20013;&#22269;&#22823;&#38470;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02354v1 Announce Type: cross  Abstract: The air quality inference problem aims to utilize historical data from a limited number of observation sites to infer the air quality index at an unknown location. Considering the sparsity of data due to the high maintenance cost of the stations, good inference algorithms can effectively save the cost and refine the data granularity. While spatio-temporal graph neural networks have made excellent progress on this problem, their non-Euclidean and discrete data structure modeling of reality limits its potential. In this work, we make the first attempt to combine two different spatio-temporal perspectives, fields and graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and its corresponding new framework, Pyramidal Inference. Extensive experiments validate that our model achieves state-of-the-art performance in nationwide air quality inference in the Chinese Mainland, demonstrating the superiority of our proposed model 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#26426;&#21046;ATP&#65292;&#36890;&#36807;&#20851;&#27880;&#39030;&#32423;&#20027;&#35201;&#38190;&#32780;&#38750;&#27599;&#20010;&#26631;&#35760;&#65292;&#20197;&#23454;&#29616;&#23545;&#36755;&#20837;&#24207;&#21015;&#30340;&#24555;&#36895;&#22788;&#29702;&#65292;&#24182;&#33021;&#22815;&#22312;&#38477;&#20302;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#25429;&#25417;&#36755;&#20837;&#24207;&#21015;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.02352</link><description>&lt;p&gt;
ATP: &#36890;&#36807;&#23545;&#39030;&#32423;&#20027;&#35201;&#38190;&#36827;&#34892;&#20851;&#27880;&#23454;&#29616;&#24555;&#36895;LLM&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02352
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#26426;&#21046;ATP&#65292;&#36890;&#36807;&#20851;&#27880;&#39030;&#32423;&#20027;&#35201;&#38190;&#32780;&#38750;&#27599;&#20010;&#26631;&#35760;&#65292;&#20197;&#23454;&#29616;&#23545;&#36755;&#20837;&#24207;&#21015;&#30340;&#24555;&#36895;&#22788;&#29702;&#65292;&#24182;&#33021;&#22815;&#22312;&#38477;&#20302;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#25429;&#25417;&#36755;&#20837;&#24207;&#21015;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#26032;&#22411;&#27880;&#24847;&#26426;&#21046; ATP&#65292;&#35813;&#26426;&#21046;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#39030;&#32423;&#20027;&#35201;&#38190;&#19978;&#65292;&#32780;&#19981;&#26159;&#27599;&#20010;&#21333;&#29420;&#30340;&#26631;&#35760;&#19978;&#12290;&#29305;&#21035;&#22320;&#65292;ATP&#21463;&#21040;&#19968;&#20010;&#37325;&#35201;&#35266;&#23519;&#30340;&#39537;&#21160;&#65292;&#21363;&#36755;&#20837;&#24207;&#21015;&#36890;&#24120;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#21363;&#36755;&#20837;&#24207;&#21015;&#21487;&#20197;&#30001;&#23569;&#37327;&#20027;&#35201;&#22522;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;ATP&#23558;&#36755;&#20837;&#36716;&#25442;&#20026;&#27491;&#20132;&#31354;&#38388;&#65292;&#24182;&#20165;&#22312;&#39030;&#32423;&#20027;&#35201;&#22522;&#19978;&#35745;&#31639;&#27880;&#24847;&#21147;&#12290;&#30001;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#35266;&#23519;&#21040;&#30340;&#20302;&#31209;&#32467;&#26500;&#65292;ATP&#33021;&#22815;&#20165;&#36890;&#36807;&#23569;&#37327;&#20027;&#35201;&#22522;&#25429;&#25417;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#27880;&#24847;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#38477;&#20302;&#21040;&#32447;&#24615;&#65292;&#32780;&#19981;&#20250;&#24341;&#36215;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;ATP&#36827;&#19968;&#27493;&#20026;&#20855;&#26377;&#20302;&#31209;&#36755;&#20837;&#30340;&#20854;&#20182;&#32447;&#24615;&#23618;&#20943;&#23569;&#20102;&#22797;&#26434;&#24230;&#65292;&#19982;&#20165;&#21333;&#32431;&#36827;&#34892;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26356;&#22823;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02352v1 Announce Type: cross  Abstract: We propose a new attention mechanism with linear complexity, ATP, that fixates \textbf{A}ttention on \textbf{T}op \textbf{P}rincipal keys, rather than on each individual token. Particularly, ATP is driven by an important observation that input sequences are typically low-rank, i.e., input sequences can be represented by a few principal bases. Therefore, instead of directly iterating over all the input tokens, ATP transforms inputs into an orthogonal space and computes attention only on the top principal bases (keys). Owing to the observed low-rank structure in input sequences, ATP is able to capture semantic relationships in input sequences with a few principal keys. Furthermore, the attention complexity is reduced from \emph{quadratic} to \emph{linear} without incurring a noticeable performance drop. ATP further reduces complexity for other linear layers with low-rank inputs, leading to more speedup compared to prior works that solely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19977;&#31181;&#24120;&#29992;&#27493;&#38271;&#35843;&#24230;&#30340;&#31934;&#30830;&#34920;&#36798;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.02347</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Federated Learning Algorithms without Data Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19977;&#31181;&#24120;&#29992;&#27493;&#38271;&#35843;&#24230;&#30340;&#31934;&#30830;&#34920;&#36798;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30456;&#20284;&#24615;&#20551;&#35774;&#20256;&#32479;&#19978;&#34987;&#24191;&#27867;&#20381;&#36182;&#20110;&#29702;&#35299;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#35201;&#27714;&#26681;&#25454;&#25968;&#25454;&#30456;&#20284;&#24615;&#31243;&#24230;&#24494;&#35843;&#27493;&#38271;&#12290;&#24403;&#25968;&#25454;&#30456;&#20284;&#24615;&#36739;&#20302;&#26102;&#65292;&#36825;&#20123;&#23567;&#27493;&#38271;&#20250;&#23548;&#33268;&#32852;&#37030;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19981;&#21487;&#25509;&#21463;&#22320;&#24930;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#19968;&#20010;&#19981;&#31561;&#24335;&#19978;&#65292;&#36825;&#20010;&#19981;&#31561;&#24335;&#25429;&#25417;&#20102;&#27493;&#38271;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#23450;&#29702;&#24212;&#29992;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19977;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#27493;&#38271;&#35843;&#24230;&#30340;&#31934;&#30830;&#34920;&#36798;&#24335;&#65306;&#22266;&#23450;&#27493;&#38271;&#12289;&#36882;&#20943;&#27493;&#38271;&#21644;&#27493;&#34928;&#20943;&#27493;&#38271;&#65292;&#36825;&#20123;&#34920;&#36798;&#24335;&#29420;&#31435;&#20110;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02347v1 Announce Type: new  Abstract: Data similarity assumptions have traditionally been relied upon to understand the convergence behaviors of federated learning methods. Unfortunately, this approach often demands fine-tuning step sizes based on the level of data similarity. When data similarity is low, these small step sizes result in an unacceptably slow convergence speed for federated methods. In this paper, we present a novel and unified framework for analyzing the convergence of federated learning algorithms without the need for data similarity conditions. Our analysis centers on an inequality that captures the influence of step sizes on algorithmic convergence performance. By applying our theorems to well-known federated algorithms, we derive precise expressions for three widely used step size schedules: fixed, diminishing, and step-decay step sizes, which are independent of data similarity conditions. Finally, we conduct comprehensive evaluations of the performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.02241</link><description>&lt;p&gt;
&#31070;&#32463;&#32418;&#31227;&#65306;&#38543;&#26426;&#32593;&#32476;&#24182;&#38750;&#38543;&#26426;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Neural Redshift: Random Networks are not Random Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35299;&#20173;&#19981;&#23436;&#25972;&#12290;&#30446;&#21069;&#30340;&#35299;&#37322;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#38544;&#21547;&#20559;&#35265;&#65292;&#20294;&#26080;&#27861;&#35299;&#37322;&#26799;&#24230;&#33258;&#30001;&#26041;&#27861;&#20013;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20063;&#26080;&#27861;&#35299;&#37322;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#26410;&#32463;&#35757;&#32451;&#32593;&#32476;&#30340;&#31616;&#21333;&#20559;&#35265;&#12290;&#26412;&#25991;&#23547;&#25214;NNs&#20013;&#30340;&#20854;&#20182;&#27867;&#21270;&#28304;&#12290;&#20026;&#20102;&#29420;&#31435;&#20110;GD&#29702;&#35299;&#20307;&#31995;&#32467;&#26500;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#25105;&#20204;&#30740;&#31350;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#12290;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;MLPs&#20063;&#34920;&#29616;&#20986;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65306;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#20250;&#20135;&#29983;&#19968;&#20010;&#38750;&#24120;&#20559;&#21521;&#20110;&#22797;&#26434;&#24615;&#30340;&#20989;&#25968;&#20998;&#24067;&#12290;&#20294;&#19982;&#24120;&#35268;&#26234;&#24935;&#19981;&#21516;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#12290;&#36825;&#19968;&#29305;&#24615;&#21462;&#20915;&#20110;&#32452;&#20214;&#65292;&#22914;ReLU&#12289;&#27531;&#24046;&#36830;&#25509;&#21644;&#23618;&#24402;&#19968;&#21270;&#12290;&#21487;&#21033;&#29992;&#26367;&#20195;&#20307;&#31995;&#32467;&#26500;&#26500;&#24314;&#20559;&#21521;&#20110;&#20219;&#20309;&#22797;&#26434;&#24615;&#27700;&#24179;&#30340;&#20559;&#35265;&#12290;Transformers&#20063;&#20855;&#26377;&#36825;&#19968;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02241v1 Announce Type: cross  Abstract: Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.   Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent "simplicity bias". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inher
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#20272;&#35745;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20114;&#20449;&#24687;&#20272;&#35745;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02187</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#20114;&#20449;&#24687;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Mutual Information Estimation via Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02187
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#20272;&#35745;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20114;&#20449;&#24687;&#20272;&#35745;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20272;&#35745;&#38382;&#39064;&#65292;&#21363;&#24341;&#20837;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#20272;&#35745;&#22120;&#12290;&#35813;&#20272;&#35745;&#22120;&#23558;&#21407;&#22987;&#25968;&#25454;&#26144;&#23556;&#21040;&#20855;&#26377;&#24050;&#30693;&#20114;&#20449;&#24687;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#12290;&#36890;&#36807;&#39640;&#32500;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#20272;&#35745;&#22120;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02187v1 Announce Type: new  Abstract: We propose a novel approach to the problem of mutual information (MI) estimation via introducing normalizing flows-based estimator. The estimator maps original data to the target distribution with known closed-form expression for MI. We demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are provided to show the advantages of the proposed estimator.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36741;&#21161;&#20613;&#31435;&#21494;&#22522;&#22686;&#24378;&#65288;AFA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#36827;&#34892;&#22686;&#24378;&#65292;&#22635;&#34917;&#20102;&#35270;&#35273;&#22686;&#24378;&#36951;&#30041;&#30340;&#22686;&#24378;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01944</link><description>&lt;p&gt;
&#29992;&#20613;&#31435;&#21494;&#22522;&#20989;&#25968;&#24357;&#21512;&#22686;&#24378;&#24046;&#36317;&#65306;&#37325;&#26032;&#24605;&#32771;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#39057;&#29575;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01944
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36741;&#21161;&#20613;&#31435;&#21494;&#22522;&#22686;&#24378;&#65288;AFA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#36827;&#34892;&#22686;&#24378;&#65292;&#22635;&#34917;&#20102;&#35270;&#35273;&#22686;&#24378;&#36951;&#30041;&#30340;&#22686;&#24378;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#22312;&#37096;&#32626;&#21040;&#29616;&#23454;&#22330;&#26223;&#20013;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#32771;&#34385;&#21040;&#30340;&#36755;&#20837;&#20986;&#29616;&#20102;&#24847;&#22806;&#21464;&#21270;&#12290;&#25968;&#25454;&#22686;&#24378;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26088;&#22312;&#22686;&#21152;&#25968;&#25454;&#21464;&#21270;&#24615;&#24182;&#20943;&#23569;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#35270;&#35273;&#22686;&#24378;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24191;&#27867;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36741;&#21161;&#20613;&#31435;&#21494;&#22522;&#22686;&#24378;&#65288;AFA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39057;&#29575;&#22495;&#22686;&#24378;&#30340;&#34917;&#20805;&#25216;&#26415;&#65292;&#22635;&#34917;&#20102;&#35270;&#35273;&#22686;&#24378;&#36951;&#30041;&#30340;&#22686;&#24378;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#39640;&#25928;&#30340;&#23545;&#25239;&#35774;&#32622;&#23637;&#31034;&#20102;&#20613;&#31435;&#21494;&#22522;&#21152;&#24615;&#22122;&#22768;&#22686;&#24378;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AFA&#26377;&#21161;&#20110;&#27169;&#22411;&#23545;&#24120;&#35265;&#25439;&#22351;&#12289;OOD&#27867;&#21270;&#20197;&#21450;&#27169;&#22411;&#24615;&#33021;&#38543;&#30528;&#24615;&#33021;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01944v1 Announce Type: cross  Abstract: Computer vision models normally witness degraded performance when deployed in real-world scenarios, due to unexpected changes in inputs that were not accounted for during training. Data augmentation is commonly used to address this issue, as it aims to increase data variety and reduce the distribution gap between training and test data. However, common visual augmentations might not guarantee extensive robustness of computer vision models. In this paper, we propose Auxiliary Fourier-basis Augmentation (AFA), a complementary technique targeting augmentation in the frequency domain and filling the augmentation gap left by visual augmentations. We demonstrate the utility of augmentation via Fourier-basis additive noise in a straightforward and efficient adversarial setting. Our results show that AFA benefits the robustness of models against common corruptions, OOD generalization, and consistency of performance of models against increasing
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20351;&#29992;&#20984;&#20248;&#21270;&#35299;&#20915;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01919</link><description>&lt;p&gt;
&#20351;&#29992;&#20984;&#20248;&#21270;&#21644;&#21015;&#23376;&#38598;&#36873;&#25321;&#30340;&#30697;&#38453;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Matrix Completion with Convex Optimization and Column Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20351;&#29992;&#20984;&#20248;&#21270;&#35299;&#20915;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#30340;&#20004;&#27493;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27599;&#19968;&#27493;&#20013;&#35299;&#20915;&#19968;&#20010;&#20984;&#20248;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23454;&#29616;&#25105;&#20204;&#30340;&#21015;&#36873;&#25321;&#30697;&#38453;&#23436;&#25104;&#65288;CSMC&#65289;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#27599;&#31181;&#31639;&#27861;&#38024;&#23545;&#19981;&#21516;&#35268;&#27169;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27491;&#24335;&#20998;&#26512;&#65292;&#22312;&#20998;&#26512;&#20013;&#25105;&#20204;&#38416;&#26126;&#20102;&#24517;&#35201;&#30340;&#20551;&#35774;&#21644;&#25214;&#21040;&#27491;&#30830;&#35299;&#30340;&#27010;&#29575;&#12290;&#22312;&#35770;&#25991;&#30340;&#31532;&#20108;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#39564;&#24037;&#20316;&#30340;&#32467;&#26524;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#30740;&#31350;&#30697;&#38453;&#22823;&#23567;&#12289;&#31209;&#21644;&#32570;&#22833;&#20803;&#32032;&#27604;&#20363;&#23545;&#35299;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01919v1 Announce Type: new  Abstract: We introduce a two-step method for the matrix recovery problem. Our approach combines the theoretical foundations of the Column Subset Selection and Low-rank Matrix Completion problems. The proposed method, in each step, solves a convex optimization task. We present two algorithms that implement our Columns Selected Matrix Completion (CSMC) method, each dedicated to a different size problem. We performed a formal analysis of the presented method, in which we formulated the necessary assumptions and the probability of finding a correct solution. In the second part of the paper, we present the results of the experimental work. Numerical experiments verified the correctness and performance of the algorithms. To study the influence of the matrix size, rank, and the proportion of missing elements on the quality of the solution and the computation time, we performed experiments on synthetic data. The presented method was applied to two real-li
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01548</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#34920;&#24449;&#30340;&#19978;&#19979;&#25991;&#38160;&#24230;&#20316;&#20026;&#35686;&#25253;&#65306;&#20943;&#23569;&#24187;&#35273;&#30340;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#23427;&#20204;&#20026;&#20160;&#20040;&#20250;&#29359;&#36825;&#20123;&#38169;&#35823;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20869;&#37096;&#34920;&#24449;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;LLM&#24187;&#35273;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#19982;&#24187;&#35273;&#30456;&#20851;&#30340;&#19968;&#20010;&#31361;&#20986;&#27169;&#24335;&#65306;&#27491;&#30830;&#30340;&#29983;&#25104;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#65292;&#32780;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#21017;&#27809;&#26377;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#19978;&#19979;&#25991;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#8220;&#38160;&#24230;&#8221;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#20197;&#21046;&#23450;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#65292;&#22312;TruthfulQA&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;8.6&#28857;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#24187;&#35273;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01537</link><description>&lt;p&gt;
&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#29992;&#20110;&#20154;&#32676;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Mixed-Strategy Nash Equilibrium for Crowd Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38024;&#23545;&#20154;&#32676;&#23548;&#33322;&#25214;&#21040;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#20026;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#39044;&#27979;&#20154;&#32676;&#20013;&#19981;&#30830;&#23450;&#20294;&#21512;&#20316;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36890;&#24120;&#22826;&#39640;&#65292;&#26080;&#27861;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#23454;&#26102;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#25910;&#25947;&#20110;&#28151;&#21512;&#31574;&#30053;&#31038;&#20132;&#23548;&#33322;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20195;&#29702;&#31574;&#30053;&#21021;&#22987;&#21270;&#20026;&#20174;&#20154;&#31867;&#25968;&#25454;&#38598;&#23398;&#20064;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#26469;&#26500;&#24314;&#35813;&#28216;&#25103;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#20154;&#32676;&#23548;&#33322;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#23548;&#33322;&#26041;&#27861;&#20013;&#65292;&#24182;&#21487;&#22312;&#31508;&#35760;&#26412;&#30005;&#33041; CPU &#19978;&#23454;&#26102;&#36816;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20154;&#31867;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01537v1 Announce Type: cross  Abstract: We address the problem of finding mixed-strategy Nash equilibrium for crowd navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the robot to anticipate uncertain yet cooperative human behavior in crowds, but the computation cost is often too high for scalable and real-time decision-making. Here we prove that a simple iterative Bayesian updating scheme converges to the Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we propose a data-driven framework to construct the game by initializing agent strategies as Gaussian processes learned from human datasets. Based on the proposed mixed-strategy Nash equilibrium model, we develop a sampling-based crowd navigation framework that can be integrated into existing navigation methods and runs in real-time on a laptop CPU. We evaluate our framework in both simulated environments and real-world human datasets in unstructured environments. Our framework
&lt;/p&gt;</description></item><item><title>LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01131</link><description>&lt;p&gt;
LLaMoCo&#65306;&#29992;&#20110;&#20248;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01131
&lt;/p&gt;
&lt;p&gt;
LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20248;&#21270;&#65292;&#26041;&#27861;&#21253;&#25324;&#20174;LLMs&#36845;&#20195;&#22320;&#23547;&#25214;&#19979;&#19968;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#30452;&#25509;&#25552;&#31034;LLMs&#20197;&#33719;&#21462;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#65292;&#21253;&#25324;&#25805;&#20316;&#25928;&#29575;&#20302;&#12289;&#23545;&#25552;&#31034;&#35774;&#35745;&#25935;&#24863;&#24230;&#39640;&#20197;&#21450;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LLaMoCo&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#28165;&#26224;&#25551;&#36848;&#30340;&#38382;&#39064;&#25552;&#31034;&#21644;&#26377;&#25928;&#20248;&#21270;&#20195;&#30721;&#30340;&#20840;&#38754;&#25351;&#20196;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#20043;&#21069;&#65292;&#35813;&#31574;&#30053;&#25972;&#21512;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28909;&#36523;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#24494;&#35843;&#26399;&#38388;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;LLaMoCo&#31934;&#35843;&#30340;CodeGen&#65288;350M&#65289;&#27169;&#22411;&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01131v1 Announce Type: cross  Abstract: Recent research explores optimization using large language models (LLMs) by either iteratively seeking next-step solutions from LLMs or directly prompting LLMs for an optimizer. However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner. Specifically, we establish a comprehensive instruction set containing well-described problem prompts and effective optimization codes. We then develop a novel two-phase learning strategy that incorporates a contrastive learning-based warm-up procedure before the instruction-tuning phase to enhance the convergence behavior during model fine-tuning. The experiment results demonstrate that a CodeGen (350M) model fine-tuned by our LLaMoCo achieves superior 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#19968;&#12289;&#20108;&#21644;&#19977;&#38454;&#23548;&#25968;&#36827;&#34892;&#25439;&#22833;&#26223;&#35266;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#19982;Spearman&#31209;&#30456;&#20851;&#31995;&#25968;&#31867;&#20284;&#21487;&#35270;&#21270;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#25439;&#22833;&#20989;&#25968;&#21644;&#28608;&#27963;&#20989;&#25968;&#32467;&#21512;&#24102;&#26469;&#30340;&#38750;&#32447;&#24615;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.01128</link><description>&lt;p&gt;
&#25439;&#22833;&#26223;&#35266;&#30340;&#28789;&#25935;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sensitivity Analysis On Loss Landscape
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01128
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#19968;&#12289;&#20108;&#21644;&#19977;&#38454;&#23548;&#25968;&#36827;&#34892;&#25439;&#22833;&#26223;&#35266;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#19982;Spearman&#31209;&#30456;&#20851;&#31995;&#25968;&#31867;&#20284;&#21487;&#35270;&#21270;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#25439;&#22833;&#20989;&#25968;&#21644;&#28608;&#27963;&#20989;&#25968;&#32467;&#21512;&#24102;&#26469;&#30340;&#38750;&#32447;&#24615;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21487;&#29992;&#20110;&#28789;&#25935;&#24230;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#25439;&#22833;&#26223;&#35266;&#30340;&#20248;&#21183;&#65292;&#20102;&#35299;&#21738;&#20123;&#33258;&#21464;&#37327;&#24433;&#21709;&#22240;&#21464;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#21033;&#29992;&#19968;&#38454;&#12289;&#20108;&#38454;&#21644;&#19977;&#38454;&#23548;&#25968;&#26469;&#29702;&#35299;&#25439;&#22833;&#26223;&#35266;&#12290;&#25105;&#20204;&#30693;&#36947;Spearman&#31209;&#30456;&#20851;&#31995;&#25968;&#21487;&#20197;&#26816;&#27979;&#20004;&#20010;&#21464;&#37327;&#20043;&#38388;&#30340;&#21333;&#35843;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25105;&#21457;&#29616;&#22312;&#29305;&#23450;&#37197;&#32622;&#21644;&#21442;&#25968;&#19979;&#65292;&#20108;&#38454;&#26799;&#24230;&#25552;&#20379;&#30340;&#20449;&#24687;&#21487;&#20197;&#31867;&#20284;&#20110;Spearman&#30340;&#32467;&#26524;&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#25439;&#22833;&#20989;&#25968;&#19982;&#28608;&#27963;&#20989;&#25968;&#32467;&#21512;&#65292;&#23548;&#33268;&#38750;&#32447;&#24615;&#27169;&#24335;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#23545;&#25439;&#22833;&#26223;&#35266;&#30340;&#27599;&#27425;&#25506;&#32034;&#37117;&#25552;&#20379;&#26032;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#19968;&#38454;&#21644;&#19977;&#38454;&#23548;&#25968;&#20063;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26174;&#31034;&#20102;&#33258;&#21464;&#37327;&#23545;&#22240;&#21464;&#37327;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01128v1 Announce Type: new  Abstract: Gradients can be employed for sensitivity analysis. Here, we leverage the advantages of the Loss Landscape to comprehend which independent variables impact the dependent variable. We seek to grasp the loss landscape by utilizing first, second, and third derivatives through automatic differentiation. we know that Spearman's rank correlation coefficient can detect the monotonic relationship between two variables. However, I have found that second-order gradients, with certain configurations and parameters, provide information that can be visualized similarly to Spearman's results.In our approach, we incorporate a loss function with an activation function, resulting in a non-linear pattern. Each exploration of the loss landscape through retraining yields new valuable information. Furthermore, the first and third derivatives are also beneficial, as they indicate the extent to which independent variables influence the dependent variable.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01053</link><description>&lt;p&gt;
&#36879;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#21457;&#29616;&#26032;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Seeing Unseen: Discover Novel Biomedical Concepts via GeometryConstrained Probabilistic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01053
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01053v1 &#36890;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#25688;&#35201;: &#26426;&#22120;&#23398;&#20064;&#20197;&#20854;&#25968;&#25454;&#39537;&#21160;&#30340;&#29305;&#24615;&#65292;&#23545;&#31185;&#23398;&#21457;&#29616;&#30340;&#22522;&#26412;&#23454;&#36341;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#25913;&#21464;&#12290;&#38543;&#30528;&#19981;&#26029;&#22686;&#21152;&#30340;&#30740;&#31350;&#25968;&#25454;&#25910;&#38598;&#65292;&#33258;&#21160;&#25506;&#32034;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#35265;&#35299;&#65292;&#21457;&#29616;&#26032;&#30340;&#34920;&#22411;&#31867;&#21035;&#21644;&#27010;&#24565;&#23558;&#20250;&#21464;&#24471;&#26356;&#21152;&#21560;&#24341;&#20154;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#32047;&#31215;&#25968;&#25454;&#20013;&#23384;&#22312;&#33509;&#24178;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#26032;&#31867;&#21457;&#29616;&#30340;&#36827;&#23637;&#12290;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#20276;&#38543;&#30528;&#19981;&#21516;&#31867;&#21035;&#32452;&#20043;&#38388;&#30340;&#20005;&#37325;&#19981;&#24179;&#34913;&#65292;&#26412;&#36136;&#19978;&#23548;&#33268;&#27169;&#31946;&#21644;&#20559;&#20506;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#25152;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#23454;&#20363;&#23884;&#20837;&#30340;&#36817;&#20284;&#21518;&#39564;&#21442;&#25968;&#21270;&#20026;&#36793;&#38469; von Mises-Fisher &#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#23884;&#20837;&#26041;&#26696;&#30340;&#27169;&#31946;&#24615;&#19982;&#20559;&#35265;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01053v1 Announce Type: cross  Abstract: Machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven nature. With the ever-increasing stream of research data collection, it would be appealing to autonomously explore patterns and insights from observational data for discovering novel classes of phenotypes and concepts. However, in the biomedical domain, there are several challenges inherently presented in the cumulated data which hamper the progress of novel class discovery. The non-i.i.d. data distribution accompanied by the severe imbalance among different groups of classes essentially leads to ambiguous and biased semantic representations. In this work, we present a geometry-constrained probabilistic modeling treatment to resolve the identified issues. First, we propose to parameterize the approximated posterior of instance embedding as a marginal von MisesFisher distribution to account for the int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00867</link><description>&lt;p&gt;
&#26799;&#24230;&#34987;&#32602;&#65306;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#65292;LLM&#29983;&#25104;&#31572;&#26696;&#12290;&#20026;&#20102;&#20943;&#23569;&#20260;&#23475;&#21644;&#28389;&#29992;&#65292;&#20154;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35757;&#32451;&#25216;&#26415;&#22914;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23558;&#36825;&#20123;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;LLMs&#23545;&#20110;&#35797;&#22270;&#39072;&#35206;&#23884;&#20837;&#30340;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#30340;&#23545;&#25239;&#24615;&#36234;&#29425;&#23581;&#35797;&#30340;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#23450;&#20041;&#24182;&#35843;&#26597;&#20102;LLMs&#30340;&#25298;&#32477;&#25439;&#22833;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#36234;&#29425;&#23581;&#35797;&#12290;Gradient Cuff&#21033;&#29992;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#20013;&#35266;&#23519;&#21040;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#21253;&#25324;&#21151;&#33021;&#20540;&#21450;&#20854;&#20809;&#28369;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00867v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN,
&lt;/p&gt;</description></item><item><title>DenseSSM&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#30340;&#27969;&#21160;&#65292;&#22312;&#20445;&#25345;&#35757;&#32451;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00818</link><description>&lt;p&gt;
DenseMamba: &#20855;&#26377;&#23494;&#38598;&#38544;&#34255;&#36830;&#25509;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00818
&lt;/p&gt;
&lt;p&gt;
DenseSSM&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#30340;&#27969;&#21160;&#65292;&#22312;&#20445;&#25345;&#35757;&#32451;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38754;&#20020;&#30528;&#30001;&#26222;&#36941;&#20351;&#29992;&#30340;Transformer&#26550;&#26500;&#36807;&#39640;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#32780;&#24102;&#26469;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#32780;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#30784;&#32593;&#32476;&#26550;&#26500;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20294;&#20854;&#24615;&#33021;&#23578;&#26410;&#23436;&#20840;&#33021;&#19982;Transformer&#30456;&#23218;&#32654;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;DenseSSM&#65292;&#19968;&#31181;&#22686;&#24378;SSMs&#20013;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#27969;&#21160;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#23558;&#27973;&#23618;&#38544;&#34255;&#29366;&#24577;&#38598;&#25104;&#21040;&#26356;&#28145;&#23618;&#65292;DenseSSM&#20445;&#30041;&#20102;&#23545;&#26368;&#32456;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#30340;DenseSSM&#20173;&#20445;&#25345;&#20102;&#35757;&#32451;&#30340;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;RetNet&#21644;Mamba&#31561;&#21508;&#31181;SSM&#31867;&#22411;&#12290;&#22312;&#30456;&#20284;&#30340;&#27169;&#22411;&#22823;&#23567;&#19979;&#65292;DenseSSM&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20363;&#22914;DenseRetNet&#27604;&#21407;&#22987;RetNet&#25552;&#39640;&#20102;&#39640;&#36798;5%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00818v1 Announce Type: new  Abstract: Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% ac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00225</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#25216;&#33021;&#25193;&#25955;&#23454;&#29616;&#31283;&#20581;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Policy Learning via Offline Skill Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00225
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#38271;&#26102;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#12290;&#36825;&#20123;&#25216;&#33021;&#26159;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#26080;&#20851;&#20219;&#21153;&#22320;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#21152;&#24555;&#38024;&#23545;&#26032;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25216;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#20173;&#21463;&#38480;&#20110;&#23545;&#25968;&#25454;&#38598;&#30340;&#22266;&#26377;&#20381;&#36182;&#65292;&#24403;&#23581;&#35797;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#19981;&#21516;&#20110;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#30446;&#26631;&#39046;&#22495;&#23398;&#20064;&#22522;&#20110;&#25216;&#33021;&#30340;&#31574;&#30053;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23601;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#23427;&#37319;&#29992;&#20102;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#20174;&#25968;&#25454;&#38598;&#20013;&#26377;&#38480;&#25216;&#33021;&#25193;&#23637;&#20986;&#30340;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24341;&#23548;&#25193;&#25955;&#25216;&#33021;&#35299;&#30721;&#22120;&#65292;&#32467;&#21512;&#20998;&#23618;&#32534;&#30721;&#65292;&#20197;&#35299;&#24320;&#25216;&#33021;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00225v1 Announce Type: new  Abstract: Skill-based reinforcement learning (RL) approaches have shown considerable promise, especially in solving long-horizon tasks via hierarchical structures. These skills, learned task-agnostically from offline datasets, can accelerate the policy learning process for new tasks. Yet, the application of these skills in different domains remains restricted due to their inherent dependency on the datasets, which poses a challenge when attempting to learn a skill-based policy via RL for a target domain different from the datasets' domains. In this paper, we present a novel offline skill learning framework DuSkill which employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains. Specifically, we devise a guided diffusion-based skill decoder in conjunction with the hierarchical encoding to disentangle the skill embeddi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphPub&#30340;&#26032;&#22411;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#22312;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.00030</link><description>&lt;p&gt;
GraphPub: &#20855;&#26377;&#39640;&#21487;&#29992;&#24615;&#30340;&#24046;&#20998;&#38544;&#31169;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
GraphPub: Generation of Differential Privacy Graph with High Availability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphPub&#30340;&#26032;&#22411;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#22312;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22270;&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;GNN&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#19978;&#28216;&#25968;&#25454;&#25152;&#26377;&#32773;&#21457;&#24067;&#22270;&#25968;&#25454;&#26102;&#65292;&#24448;&#24448;&#20250;&#23384;&#22312;&#35768;&#22810;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#21253;&#21547;&#20687;&#20010;&#20154;&#30340;&#26379;&#21451;&#21015;&#34920;&#31561;&#25935;&#24863;&#20449;&#24687;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22270;&#25968;&#25454;&#30340;&#22797;&#26434;&#25299;&#25169;&#32467;&#26500;&#65292;&#23558;DP&#24212;&#29992;&#22312;&#22270;&#19978;&#24448;&#24448;&#20250;&#24433;&#21709;GNN&#27169;&#22411;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#65292;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;GraphPub&#65292;&#21487;&#20197;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#25105;&#20204;&#25628;&#32034;&#19968;&#20123;&#23545;&#33410;&#28857;&#29305;&#24449;&#32858;&#21512;&#27809;&#26377;&#22826;&#22823;&#36127;&#38754;&#24433;&#21709;&#30340;&#34394;&#20551;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00030v1 Announce Type: cross  Abstract: In recent years, with the rapid development of graph neural networks (GNN), more and more graph datasets have been published for GNN tasks. However, when an upstream data owner publishes graph data, there are often many privacy concerns, because many real-world graph data contain sensitive information like person's friend list. Differential privacy (DP) is a common method to protect privacy, but due to the complex topological structure of graph data, applying DP on graphs often affects the message passing and aggregation of GNN models, leading to a decrease in model accuracy. In this paper, we propose a novel graph edge protection framework, graph publisher (GraphPub), which can protect graph topology while ensuring that the availability of data is basically unchanged. Through reverse learning and the encoder-decoder mechanism, we search for some false edges that do not have a large negative impact on the aggregation of node features, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#38454;&#19981;&#30830;&#23450;&#27169;&#22411;&#30340;&#23454;&#26102;&#33258;&#36866;&#24212;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#21033;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#21644;&#22522;&#20110;&#39640;&#38454;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.18946</link><description>&lt;p&gt;
&#22312;&#39640;&#38454;&#19981;&#30830;&#23450;&#27169;&#22411;&#20013;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#23454;&#26102;&#33258;&#36866;&#24212;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Real-Time Adaptive Safety-Critical Control with Gaussian Processes in High-Order Uncertain Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#38454;&#19981;&#30830;&#23450;&#27169;&#22411;&#30340;&#23454;&#26102;&#33258;&#36866;&#24212;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#21033;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#21644;&#22522;&#20110;&#39640;&#38454;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#21442;&#25968;&#30340;&#31995;&#32479;&#65292;&#20197;&#30830;&#20445;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#12290;&#21021;&#22987;&#38454;&#27573;&#38598;&#20013;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26694;&#26550;&#19978;&#12290;&#25105;&#20204;&#39318;&#20808;&#38598;&#25104;&#20102;&#19968;&#20010;&#36951;&#24536;&#22240;&#23376;&#26469;&#20248;&#21270;&#21464;&#20998;&#31232;&#30095;GP&#31639;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#36866;&#24212;&#24615;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#29305;&#27530;&#22797;&#21512;&#26680;&#23545;&#39640;&#26031;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26356;&#26032;&#19968;&#20010;&#28304;&#33258;&#26032;&#26679;&#26412;&#30340;&#23396;&#29420;&#24863;&#24212;&#28857;&#26469;&#21152;&#24378;&#39640;&#26031;&#27169;&#22411;&#30340;&#22312;&#32447;&#25512;&#29702;&#33021;&#21147;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#19982;&#23398;&#20064;&#21040;&#30340;&#36229;&#21442;&#25968;&#30456;&#32467;&#21512;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#38454;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;HOCBFs&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#19982;&#20808;&#21069;&#35757;&#32451;&#30340;&#23398;&#20064;&#27169;&#22411;&#21327;&#21516;&#24037;&#20316;&#12290;&#36890;&#36807;&#21033;&#29992;&#31532;&#19968;&#38454;&#27573;&#30340;&#22797;&#21512;&#26680;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18946v1 Announce Type: new  Abstract: This paper presents an adaptive online learning framework for systems with uncertain parameters to ensure safety-critical control in non-stationary environments. Our approach consists of two phases. The initial phase is centered on a novel sparse Gaussian process (GP) framework. We first integrate a forgetting factor to refine a variational sparse GP algorithm, thus enhancing its adaptability. Subsequently, the hyperparameters of the Gaussian model are trained with a specially compound kernel, and the Gaussian model's online inferential capability and computational efficiency are strengthened by updating a solitary inducing point derived from new samples, in conjunction with the learned hyperparameters. In the second phase, we propose a safety filter based on high-order control barrier functions (HOCBFs), synergized with the previously trained learning model. By leveraging the compound kernel from the first phase, we effectively address 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#38598;&#30340;&#20004;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#20808;&#39564;&#21644;&#20808;&#36827;&#20248;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#27604;&#20808;&#21069;&#24037;&#20316;&#24555;&#20116;&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.18830</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#38598;&#30340;&#20004;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#20809;&#35889;&#25968;&#25454;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Training-set-free two-stage deep learning for Spectroscopic data de-noising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18830
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#38598;&#30340;&#20004;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#20808;&#39564;&#21644;&#20808;&#36827;&#20248;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#27604;&#20808;&#21069;&#24037;&#20316;&#24555;&#20116;&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#26159;&#20809;&#35889;&#21518;&#22788;&#29702;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24555;&#36895;&#20294;&#22823;&#22810;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#65292;&#38656;&#35201;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#22312;&#23454;&#38469;&#23454;&#39564;&#27979;&#37327;&#20013;&#21487;&#33021;&#25104;&#26412;&#36739;&#39640;&#12290;&#22522;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#31639;&#27861;&#36895;&#24230;&#24930;&#65292;&#24182;&#19988;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#38598;&#30340;&#20004;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#21512;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20043;&#21069;&#26041;&#27861;&#20013;&#27169;&#31946;&#22266;&#23450;&#36755;&#20837;&#30340;&#25913;&#36827;&#65292;&#24341;&#20837;&#33258;&#36866;&#24212;&#20808;&#39564;&#12290;&#32467;&#21512;&#26356;&#20808;&#36827;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#21069;&#20154;&#24037;&#20316;&#24555;&#20116;&#20493;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30456;&#24212;&#38750;&#20984;&#32447;&#24615;&#38382;&#39064;&#30340;&#28508;&#22312;&#29366;&#24577;&#65292;&#24182;&#19988;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#19968;&#38454;&#31639;&#27861;&#25910;&#25947;&#20855;&#26377;&#33391;&#22909;&#30340;&#20960;&#20309;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18830v1 Announce Type: cross  Abstract: De-noising is a prominent step in the spectra post-processing procedure. Previous machine learning-based methods are fast but mostly based on supervised learning and require a training set that may be typically expensive in real experimental measurements. Unsupervised learning-based algorithms are slow and require many iterations to achieve convergence. Here, we bridge this gap by proposing a training-set-free two-stage deep learning method. We show that the fuzzy fixed input in previous methods can be improved by introducing an adaptive prior. Combined with more advanced optimization techniques, our approach can achieve five times acceleration compared to previous work. Theoretically, we study the landscape of a corresponding non-convex linear problem, and our results indicates that this problem has benign geometry for first-order algorithms to converge.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedUV&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#22312;&#24322;&#26500;&#20998;&#24067;&#25968;&#25454;&#20013;&#34920;&#29616;&#24471;&#26356;&#22343;&#21248;&#21644;&#31283;&#23450;</title><link>https://arxiv.org/abs/2402.18372</link><description>&lt;p&gt;
FedUV: &#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#22343;&#21248;&#24615;&#21644;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
FedUV: Uniformity and Variance for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18372
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedUV&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#22312;&#24322;&#26500;&#20998;&#24067;&#25968;&#25454;&#20013;&#34920;&#29616;&#24471;&#26356;&#22343;&#21248;&#21644;&#31283;&#23450;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#24191;&#27867;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#20250;&#38543;&#30528;&#24322;&#26500;&#20998;&#24067;&#30340;&#25968;&#25454;&#32780;&#19979;&#38477;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#26159;&#30001;&#20110;&#32593;&#32476;&#30340;&#26368;&#32456;&#23618;&#26368;&#23481;&#26131;&#20986;&#29616;&#23616;&#37096;&#20559;&#24046;&#65292;&#19968;&#20123;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#23558;&#26368;&#32456;&#23618;&#20923;&#32467;&#20026;&#27491;&#20132;&#20998;&#31867;&#22120;&#21487;&#20197;&#21462;&#24471;&#25104;&#21151;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26435;&#37325;&#24212;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#30740;&#31350;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36825;&#26159;&#21463;&#21040;&#20923;&#32467;&#26435;&#37325;&#23548;&#33268;&#22855;&#24322;&#20540;&#24658;&#23450;&#30340;&#35266;&#23519;&#21551;&#21457;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;IID&#21644;&#38750;IID&#35774;&#32622;&#19979;&#35757;&#32451;&#26102;&#23384;&#22312;&#24046;&#24322;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20004;&#31181;&#23616;&#37096;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#25345;&#32493;&#27169;&#25311;IID&#35774;&#32622;&#65306;&#65288;1&#65289;&#20998;&#31867;&#22120;&#30340;&#32500;&#24230;&#27010;&#29575;&#20998;&#24067;&#26041;&#24046;&#21644;&#65288;2&#65289;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#36229;&#29699;&#22343;&#21248;&#24615;&#12290;&#36825;&#20123;&#27491;&#21017;&#21270;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#34920;&#29616;&#24471;&#22909;&#20687;&#22312;IID&#35774;&#32622;&#20013;&#19968;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18372v1 Announce Type: cross  Abstract: Federated learning is a promising framework to train neural networks with widely distributed data. However, performance degrades heavily with heterogeneously distributed data. Recent work has shown this is due to the final layer of the network being most prone to local bias, some finding success freezing the final layer as an orthogonal classifier. We investigate the training dynamics of the classifier by applying SVD to the weights motivated by the observation that freezing weights results in constant singular values. We find that there are differences when training in IID and non-IID settings. Based on this finding, we introduce two regularization terms for local training to continuously emulate IID settings: (1) variance in the dimension-wise probability distribution of the classifier and (2) hyperspherical uniformity of representations of the encoder. These regularizations promote local models to act as if it were in an IID setting
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#30340;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#30340;S4&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#36807;&#31243;&#21644;&#20248;&#21270;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#25351;&#26631;&#26469;&#36229;&#36234;&#20256;&#32479;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15993</link><description>&lt;p&gt;
&#20351;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#23398;&#20064;&#24102;&#26377;&#23545;&#35282;&#29366;&#24577;&#31354;&#38388;&#23618;&#30340;S4&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning method for S4 with Diagonal State Space Layers using Balanced Truncation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15993
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#30340;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#30340;S4&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#36807;&#31243;&#21644;&#20248;&#21270;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#25351;&#26631;&#26469;&#36229;&#36234;&#20256;&#32479;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#65288;S4&#65289;&#27169;&#22411;&#24182;&#19988;&#21152;&#20837;&#20102;&#23545;&#35282;&#29366;&#24577;&#31354;&#38388;&#65288;DSS&#65289;&#23618;&#65292;&#36825;&#31181;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#38271;&#24207;&#21015;&#25968;&#25454;&#65292;&#21253;&#25324;&#20256;&#24863;&#22120;&#25968;&#25454;&#20998;&#26512;&#21644;&#23454;&#26102;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#65292;&#22312;&#25511;&#21046;&#29702;&#35770;&#20013;&#24456;&#24120;&#35265;&#65292;&#29305;&#21035;&#24212;&#29992;&#20110;DSS&#23618;&#20197;&#38477;&#20302;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;S4&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;Skew-HiPPo&#21021;&#22987;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#35757;&#32451;&#30340;&#24102;&#26377;DSS&#23618;&#30340;S4&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#25351;&#26631;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#26174;&#31034;&#19968;&#20010;&#31215;&#26497;&#30340;&#30456;&#20851;&#24615;&#65306;&#21407;&#22987;&#27169;&#22411;&#20013;&#30340;&#26356;&#39640;&#20934;&#30830;&#24230;&#19968;&#33268;&#23548;&#33268;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#22686;&#21152;&#65292;&#36825;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15993v1 Announce Type: new  Abstract: We introduce a novel learning method for Structured State Space Sequence (S4) models incorporating Diagonal State Space (DSS) layers, tailored for processing long-sequence data in edge intelligence applications, including sensor data analysis and real-time analytics. This method utilizes the balanced truncation technique, prevalent in control theory, applied specifically to DSS layers to reduce computational costs during inference. By leveraging parameters from the reduced model, we refine the initialization process of S4 models, outperforming the widely used Skew-HiPPo initialization in terms of performance. Numerical experiments demonstrate that our trained S4 models with DSS layers surpass conventionally trained models in accuracy and efficiency metrics. Furthermore, our observations reveal a positive correlation: higher accuracy in the original model consistently leads to increased accuracy in models trained using our method, suggest
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27874;&#21069;&#32534;&#30721;&#30340;&#30456;&#20301;&#25513;&#27169;&#21644;&#19977;&#26126;&#27835;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(SGAN)&#26469;&#24674;&#22797;&#26426;&#22120;&#35270;&#35273;&#21463;&#21040;&#28608;&#20809;&#30427;&#20809;&#24433;&#21709;&#19979;&#30340;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#20613;&#37324;&#21494;&#29305;&#24449;&#34920;&#31034;&#20197;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#39057;&#22270;&#20687;&#32454;&#33410;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.15919</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#30427;&#20809;&#30475;&#35265;
&lt;/p&gt;
&lt;p&gt;
Learning to See Through Dazzle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15919
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27874;&#21069;&#32534;&#30721;&#30340;&#30456;&#20301;&#25513;&#27169;&#21644;&#19977;&#26126;&#27835;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(SGAN)&#26469;&#24674;&#22797;&#26426;&#22120;&#35270;&#35273;&#21463;&#21040;&#28608;&#20809;&#30427;&#20809;&#24433;&#21709;&#19979;&#30340;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#20613;&#37324;&#21494;&#29305;&#24449;&#34920;&#31034;&#20197;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#39057;&#22270;&#20687;&#32454;&#33410;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#35270;&#35273;&#23481;&#26131;&#21463;&#21040;&#28608;&#20809;&#36974;&#34109;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#24378;&#28872;&#30340;&#28608;&#20809;&#20809;&#21487;&#20197;&#36890;&#36807;&#36807;&#24230;&#39281;&#21644;&#25110;&#23545;&#20256;&#24863;&#22120;&#20687;&#32032;&#36896;&#25104;&#27704;&#20037;&#24615;&#25439;&#22351;&#26469;&#20351;&#20854;&#22833;&#26126;&#24182;&#25197;&#26354;&#20854;&#23545;&#29615;&#22659;&#30340;&#24863;&#30693;&#12290;&#26412;&#25991;&#21033;&#29992;&#27874;&#21069;&#32534;&#30721;&#30340;&#30456;&#20301;&#25513;&#27169;&#26469;&#25955;&#23556;&#28608;&#20809;&#20809;&#30340;&#33021;&#37327;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#19977;&#26126;&#27835;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(SGAN)&#65292;&#29992;&#20110;&#20174;&#22797;&#26434;&#30340;&#22270;&#20687;&#36864;&#21270;&#20013;&#24674;&#22797;&#22270;&#20687;&#65292;&#20363;&#22914;&#19981;&#21516;&#30340;&#28608;&#20809;&#35825;&#23548;&#30340;&#22270;&#20687;&#39281;&#21644;&#24230;&#12289;&#25513;&#27169;&#35825;&#23548;&#30340;&#22270;&#20687;&#27169;&#31946;&#12289;&#26410;&#30693;&#30340;&#20809;&#29031;&#26465;&#20214;&#21644;&#21508;&#31181;&#22122;&#22768;&#27745;&#26579;&#12290;SGAN&#26550;&#26500;&#36890;&#36807;&#22312;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#22270;&#20687;&#21453;&#21367;&#31215;&#27169;&#22359;&#21608;&#22260;&#23553;&#35013;&#20004;&#20010;GANs&#32467;&#21512;&#35782;&#21035;&#21644;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20613;&#37324;&#21494;&#29305;&#24449;&#34920;&#31034;&#26469;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#30340;&#39057;&#35889;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#20854;&#23545;&#39640;&#39057;&#22270;&#20687;&#32454;&#33410;&#30340;&#23398;&#20064;&#12290;&#31471;&#21040;&#31471;&#35757;&#32451;&#21253;&#25324;&#22522;&#20110;&#29616;&#23454;&#29289;&#29702;&#23398;&#30340;&#32508;&#21512;&#65292;&#29983;&#25104;&#19968;&#31995;&#21015;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15919v2 Announce Type: replace-cross  Abstract: Machine vision is susceptible to laser dazzle, where intense laser light can blind and distort its perception of the environment through oversaturation or permanent damage to sensor pixels. Here we employ a wavefront-coded phase mask to diffuse the energy of laser light and introduce a sandwich generative adversarial network (SGAN) to restore images from complex image degradations, such as varying laser-induced image saturation, mask-induced image blurring, unknown lighting conditions, and various noise corruptions. The SGAN architecture combines discriminative and generative methods by wrapping two GANs around a learnable image deconvolution module. In addition, we make use of Fourier feature representations to reduce the spectral bias of neural networks and improve its learning of high-frequency image details. End-to-end training includes the realistic physics-based synthesis of a large set of training data from publicly avai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15183</link><description>&lt;p&gt;
GraphEdit&#65306;&#29992;&#20110;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphEdit: Large Language Models for Graph Structure Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;GSL&#65289;&#33268;&#21147;&#20110;&#36890;&#36807;&#29983;&#25104;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#26469;&#25429;&#25417;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#26174;&#24335;&#22270;&#32467;&#26500;&#20449;&#24687;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15183v1 Announce Type: cross  Abstract: Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy co
&lt;/p&gt;</description></item><item><title>SpanSeq &#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#25968;&#25454;&#24207;&#21015;&#30340;&#25968;&#25454;&#24211;&#20998;&#21306;&#26041;&#27861;&#65292;&#33021;&#22815;&#36991;&#20813;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#25968;&#25454;&#27844;&#28431;&#12290;</title><link>https://arxiv.org/abs/2402.14482</link><description>&lt;p&gt;
SpanSeq&#65306;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#39033;&#30446;&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#24207;&#21015;&#25968;&#25454;&#25286;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SpanSeq: Similarity-based sequence data splitting method for improved development and assessment of deep learning projects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14482
&lt;/p&gt;
&lt;p&gt;
SpanSeq &#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#25968;&#25454;&#24207;&#21015;&#30340;&#25968;&#25454;&#24211;&#20998;&#21306;&#26041;&#27861;&#65292;&#33021;&#22815;&#36991;&#20813;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#25968;&#25454;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22686;&#21152;&#24456;&#22823;&#65292;&#24182;&#19988;&#38543;&#30528;&#35832;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#39044;&#35745;&#23558;&#36827;&#19968;&#27493;&#22686;&#21152;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SpanSeq&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#29983;&#29289;&#24207;&#21015;&#65288;&#22522;&#22240;&#12289;&#34507;&#30333;&#36136;&#21644;&#22522;&#22240;&#32452;&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#24211;&#20998;&#21306;&#26041;&#27861;&#65292;&#26088;&#22312;&#36991;&#20813;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#25968;&#25454;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14482v1 Announce Type: new  Abstract: The use of deep learning models in computational biology has increased massively in recent years, and is expected to do so further with the current advances in fields like Natural Language Processing. These models, although able to draw complex relations between input and target, are also largely inclined to learn noisy deviations from the pool of data used during their development. In order to assess their performance on unseen data (their capacity to generalize), it is common to randomly split the available data in development (train/validation) and test sets. This procedure, although standard, has lately been shown to produce dubious assessments of generalization due to the existing similarity between samples in the databases used. In this work, we present SpanSeq, a database partition method for machine learning that can scale to most biological sequences (genes, proteins and genomes) in order to avoid data leakage between sets. We a
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AlphaTensor-Quantum&#26041;&#27861;&#65292;&#22312;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#20013;&#20248;&#21270;T&#38376;&#25968;&#37327;&#65292;&#26174;&#33879;&#20943;&#23569;&#30005;&#36335;&#30340;T&#35745;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14396</link><description>&lt;p&gt;
&#20351;&#29992;AlphaTensor&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantum Circuit Optimization with AlphaTensor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14396
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AlphaTensor-Quantum&#26041;&#27861;&#65292;&#22312;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#20013;&#20248;&#21270;T&#38376;&#25968;&#37327;&#65292;&#26174;&#33879;&#20943;&#23569;&#30005;&#36335;&#30340;T&#35745;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30005;&#36335;&#20248;&#21270;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#20013;&#26368;&#26114;&#36149;&#30340;&#38376;&#65288;&#21363;T&#38376;&#65289;&#65292;&#35299;&#20915;T&#35745;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#26368;&#23567;&#21270;&#23454;&#29616;&#32473;&#23450;&#30005;&#36335;&#25152;&#38656;&#30340;T&#38376;&#25968;&#37327;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AlphaTensor-Quantum&#26041;&#27861;&#65292;&#21033;&#29992;&#20248;&#21270;T&#35745;&#25968;&#19982;&#24352;&#37327;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#19982;&#29616;&#26377;&#30340;T&#35745;&#25968;&#20248;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;AlphaTensor-Quantum&#33021;&#22815;&#25972;&#21512;&#20851;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#24182;&#21033;&#29992;&#24037;&#20855;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20248;&#21270;&#30005;&#36335;&#30340;T&#35745;&#25968;&#12290;AlphaTensor-Quantum&#22312;&#19968;&#32452;&#31639;&#26415;&#22522;&#20934;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;T&#35745;&#25968;&#20248;&#21270;&#26041;&#27861;&#65288;&#21363;&#20351;&#22312;&#19981;&#20351;&#29992;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27604;&#36739;&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#21457;&#29616;&#20102;&#19968;&#31181;&#31867;&#20284;Karat&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14396v1 Announce Type: cross  Abstract: A key challenge in realizing fault-tolerant quantum computers is circuit optimization. Focusing on the most expensive gates in fault-tolerant quantum computation (namely, the T gates), we address the problem of T-count optimization, i.e., minimizing the number of T gates that are needed to implement a given circuit. To achieve this, we develop AlphaTensor-Quantum, a method based on deep reinforcement learning that exploits the relationship between optimizing T-count and tensor decomposition. Unlike existing methods for T-count optimization, AlphaTensor-Quantum can incorporate domain-specific knowledge about quantum computation and leverage gadgets, which significantly reduces the T-count of the optimized circuits. AlphaTensor-Quantum outperforms the existing methods for T-count optimization on a set of arithmetic benchmarks (even when compared without making use of gadgets). Remarkably, it discovers an efficient algorithm akin to Karat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#26368;&#22823;&#21270;&#26641;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#21152;&#36895;LLM&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.14160</link><description>&lt;p&gt;
&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;&#65306;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#21152;&#36895;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14160
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#26368;&#22823;&#21270;&#26641;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#21152;&#36895;LLM&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25512;&#29702;&#21152;&#36895;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#33609;&#31295;&#20196;&#29260;&#24207;&#21015;&#65292;&#35813;&#24207;&#21015;&#36827;&#19968;&#27493;&#30001;&#30446;&#26631;LLM&#24182;&#34892;&#39564;&#35777;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#33609;&#31295;&#20196;&#29260;&#26641;&#25512;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#21333;&#24207;&#21015;&#25512;&#27979;&#35299;&#30721;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#26641;&#30340;&#27599;&#20010;&#32423;&#21035;&#29420;&#31435;&#29983;&#25104;&#20196;&#29260;&#65292;&#27809;&#26377;&#21033;&#29992;&#25972;&#20010;&#26641;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22266;&#23450;&#24207;&#21015;&#38271;&#24230;&#24050;&#32463;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#20316;&#21697;&#22312;&#22266;&#23450;&#30446;&#26631;&#35745;&#31639;&#36164;&#28304;&#19978;&#24182;&#27809;&#26377;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#36825;&#26159;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65292;&#23427;&#23545;&#19981;&#37325;&#22797;&#25277;&#26679;&#30340;&#33609;&#31295;&#20196;&#29260;&#36827;&#34892;&#26368;&#22823;&#21270;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#23454;&#29616;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14160v1 Announce Type: cross  Abstract: Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the dive
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.13852</link><description>&lt;p&gt;
&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Neural Control System for Continuous Glucose Monitoring and Maintenance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13852
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#33889;&#33796;&#31958;&#27700;&#24179;&#31649;&#29702;&#23545;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#36991;&#20813;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#21033;&#29992;&#24494;&#20998;&#39044;&#27979;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21463;&#21040;&#22797;&#26434;&#31070;&#32463;&#31574;&#30053;&#21644;&#21487;&#21306;&#20998;&#24314;&#27169;&#30340;&#25351;&#23548;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#12290;&#36825;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#26368;&#22823;&#21270;&#25928;&#29575;&#65292;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#21644;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#65292;&#22914;&#32463;&#39564;&#21457;&#29616;&#25152;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13852v1 Announce Type: cross  Abstract: Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;VLSP 2023&#20013;ComOM&#20219;&#21153;&#30340;&#19968;&#20010;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36890;&#36807;&#24320;&#21457;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#65292;&#21442;&#19982;&#32773;&#38656;&#25552;&#20986;&#33021;&#22815;&#25552;&#21462;&#27604;&#36739;"&#20116;&#20803;&#32452;"&#30340;&#27169;&#22411;&#24182;&#26681;&#25454;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.13613</link><description>&lt;p&gt;
VLSP 2023&#32508;&#36848;--ComOM&#20219;&#21153;&#65306;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#30340;&#27604;&#36739;&#24847;&#35265;&#25366;&#25496;&#25968;&#25454;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for Comparative Opinion Mining from Vietnamese Product Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;VLSP 2023&#20013;ComOM&#20219;&#21153;&#30340;&#19968;&#20010;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36890;&#36807;&#24320;&#21457;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#65292;&#21442;&#19982;&#32773;&#38656;&#25552;&#20986;&#33021;&#22815;&#25552;&#21462;&#27604;&#36739;"&#20116;&#20803;&#32452;"&#30340;&#27169;&#22411;&#24182;&#26681;&#25454;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#36234;&#21335;&#35821;&#20135;&#21697;&#35780;&#35770;&#27604;&#36739;&#24847;&#35265;&#25366;&#25496;&#20849;&#20139;&#20219;&#21153;&#65288;ComOM&#65289;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35813;&#20219;&#21153;&#20316;&#20026;&#31532;&#21313;&#23626;&#36234;&#21335;&#35821;&#35328;&#21644;&#35821;&#38899;&#22788;&#29702;&#22269;&#38469;&#30740;&#35752;&#20250;&#65288;VLSP 2023&#65289;&#30340;&#19968;&#37096;&#20998;&#20030;&#34892;&#12290;&#27492;&#20849;&#20139;&#20219;&#21153;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#26469;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#21442;&#19982;&#32773;&#34987;&#25361;&#25112;&#25552;&#20986;&#33021;&#22815;&#20174;&#27604;&#36739;&#21477;&#20013;&#29087;&#32451;&#25552;&#21462;&#27604;&#36739;&#8220;&#20116;&#20803;&#32452;&#8221;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#20027;&#39064;&#12289;&#23458;&#20307;&#12289;&#26041;&#38754;&#12289;&#35859;&#35789;&#21644;&#27604;&#36739;&#31867;&#22411;&#26631;&#31614;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;120&#20010;&#25991;&#26723;&#30340;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;7427&#20010;&#38750;&#27604;&#36739;&#21477;&#21644;1798&#20010;&#21477;&#23376;&#20013;&#30340;2468&#20010;&#27604;&#36739;&#12290;&#21442;&#19982;&#30340;&#27169;&#22411;&#23558;&#26681;&#25454;&#20934;&#30830;&#21305;&#37197;&#23439;&#24179;&#22343;&#30340;&#20116;&#20803;&#32452;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#21644;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13613v1 Announce Type: new  Abstract: This paper presents a comprehensive overview of the Comparative Opinion Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the 10$^{th}$ International Workshop on Vietnamese Language and Speech Processing (VLSP 2023). The primary objective of this shared task is to advance the field of natural language processing by developing techniques that proficiently extract comparative opinions from Vietnamese product reviews. Participants are challenged to propose models that adeptly extract a comparative "quintuple" from a comparative sentence, encompassing Subject, Object, Aspect, Predicate, and Comparison Type Label. We construct a human-annotated dataset comprising $120$ documents, encompassing $7427$ non-comparative sentences and $2468$ comparisons within $1798$ sentences. Participating models undergo evaluation and ranking based on the Exact match macro-averaged quintuple F1 score.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#23646;&#24615;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#20351;&#29992;Projected Gradient Descent&#30340;&#20915;&#31574;&#26102;&#25915;&#20987;&#27604;&#20351;&#29992;Mean Node Embeddings&#21644;Graph Contrastive Learning&#31574;&#30053;&#30340;&#27602;&#21270;&#25915;&#20987;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.12426</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#33410;&#28857;&#23646;&#24615;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Attacks on Node Attributes in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#23646;&#24615;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#20351;&#29992;Projected Gradient Descent&#30340;&#20915;&#31574;&#26102;&#25915;&#20987;&#27604;&#20351;&#29992;Mean Node Embeddings&#21644;Graph Contrastive Learning&#31574;&#30053;&#30340;&#27602;&#21270;&#25915;&#20987;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32463;&#24120;&#29992;&#26469;&#27169;&#22411;&#21270;&#29616;&#20195;&#31038;&#20132;&#23186;&#20307;&#21644;&#25991;&#29486;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#37325;&#28857;&#20851;&#27880;&#20915;&#31574;&#26102;&#25915;&#20987;&#21644;&#27602;&#21270;&#25915;&#20987;&#65292;&#25506;&#31350;&#36825;&#20123;&#22270;&#30340;&#33030;&#24369;&#24615;&#12290;&#19982;Net Attack&#21644;Meta Attack&#31561;&#26368;&#20808;&#36827;&#27169;&#22411;&#38024;&#23545;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#19987;&#38376;&#38024;&#23545;&#33410;&#28857;&#23646;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;Hellaswag&#25991;&#26412;&#25968;&#25454;&#38598;&#20197;&#21450;&#22270;&#25968;&#25454;&#38598;Cora&#21644;CiteSeer&#36827;&#34892;&#20998;&#26512;&#65292;&#20026;&#35780;&#20272;&#25552;&#20379;&#20102;&#22810;&#26679;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20351;&#29992;Projected Gradient Descent (PGD)&#30340;&#20915;&#31574;&#26102;&#25915;&#20987;&#27604;&#37319;&#29992;Mean Node Embeddings&#21644;Graph Contrastive Learning&#31574;&#30053;&#30340;&#27602;&#21270;&#25915;&#20987;&#26356;&#20855;&#23041;&#21147;&#12290;&#36825;&#20026;&#22270;&#25968;&#25454;&#23433;&#20840;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#25351;&#20986;&#20102;&#22270;&#22522;&#27169;&#22411;&#26368;&#33030;&#24369;&#30340;&#22320;&#26041;&#65292;&#20174;&#32780;&#20026;&#24320;&#21457;&#24037;&#20316;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12426v1 Announce Type: cross  Abstract: Graphs are commonly used to model complex networks prevalent in modern social media and literacy applications. Our research investigates the vulnerability of these graphs through the application of feature based adversarial attacks, focusing on both decision-time attacks and poisoning attacks. In contrast to state-of-the-art models like Net Attack and Meta Attack, which target node attributes and graph structure, our study specifically targets node attributes. For our analysis, we utilized the text dataset Hellaswag and graph datasets Cora and CiteSeer, providing a diverse basis for evaluation. Our findings indicate that decision-time attacks using Projected Gradient Descent (PGD) are more potent compared to poisoning attacks that employ Mean Node Embeddings and Graph Contrastive Learning strategies. This provides insights for graph data security, pinpointing where graph-based models are most vulnerable and thereby informing the develo
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#29575;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11425</link><description>&lt;p&gt;
&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#65306;&#19968;&#31181;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Local False Discovery Rate Control: A Resource Allocation Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11425
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#29575;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#27979;&#35797;&#34987;&#39034;&#24207;&#36827;&#34892;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#30340;&#21457;&#29616;&#27425;&#25968;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#28041;&#21450;&#25509;&#21463;/&#25298;&#32477;&#20915;&#31574;&#65292;&#20174;&#39640;&#23618;&#27425;&#26469;&#30475;&#65292;&#36825;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#24102;&#26377;&#39069;&#22806;&#19981;&#30830;&#23450;&#24615;&#30340;&#22312;&#32447;&#32972;&#21253;&#38382;&#39064;&#65292;&#21363;&#38543;&#26426;&#39044;&#31639;&#34917;&#20805;&#12290;&#25105;&#20204;&#20174;&#19968;&#33324;&#30340;&#21040;&#36798;&#20998;&#24067;&#24320;&#22987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#26469;&#34917;&#20805;&#36825;&#19968;&#32467;&#26524;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#28966;&#28857;&#36716;&#21521;&#31163;&#25955;&#21040;&#36798;&#20998;&#24067;&#12290;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;&#29616;&#26377;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#25991;&#29486;&#20013;&#30340;&#37325;&#26032;&#35299;&#20915;&#21551;&#21457;&#24335;&#34429;&#28982;&#22312;&#20856;&#22411;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26377;&#30028;&#30340;&#25439;&#22833;&#65292;&#20294;&#21487;&#33021;&#20250;&#36896;&#25104;$\Omega(\sqrt{T})$&#29978;&#33267;$\Omega(T)$&#30340;&#21518;&#24724;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#20856;&#22411;&#31574;&#30053;&#24448;&#24448;&#22826;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11425v1 Announce Type: cross  Abstract: We consider the problem of online local false discovery rate (FDR) control where multiple tests are conducted sequentially, with the goal of maximizing the total expected number of discoveries. We formulate the problem as an online resource allocation problem with accept/reject decisions, which from a high level can be viewed as an online knapsack problem, with the additional uncertainty of random budget replenishment. We start with general arrival distributions and propose a simple policy that achieves a $O(\sqrt{T})$ regret. We complement the result by showing that such regret rate is in general not improvable. We then shift our focus to discrete arrival distributions. We find that many existing re-solving heuristics in the online resource allocation literature, albeit achieve bounded loss in canonical settings, may incur a $\Omega(\sqrt{T})$ or even a $\Omega(T)$ regret. With the observation that canonical policies tend to be too op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#29983;&#25104;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;</title><link>https://arxiv.org/abs/2402.07383</link><description>&lt;p&gt;
&#20351;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#33258;&#30001;&#22320;&#20135;&#29983;&#31505;&#22768;
&lt;/p&gt;
&lt;p&gt;
Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#29983;&#25104;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31505;&#22768;&#26159;&#20154;&#31867;&#35821;&#38899;&#20013;&#26368;&#34920;&#36798;&#24615;&#21644;&#33258;&#28982;&#30340;&#19968;&#37096;&#20998;&#65292;&#20256;&#36798;&#30528;&#24773;&#24863;&#12289;&#31038;&#20132;&#26263;&#31034;&#21644;&#24189;&#40664;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#31995;&#32479;&#32570;&#20047;&#20135;&#29983;&#36924;&#30495;&#19988;&#21512;&#36866;&#30340;&#31505;&#22768;&#30340;&#33021;&#21147;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;&#34429;&#28982;&#20043;&#21069;&#26377;&#24037;&#20316;&#29983;&#25104;&#20102;&#33258;&#28982;&#30340;&#31505;&#22768;&#65292;&#20294;&#22312;&#25511;&#21046;&#29983;&#25104;&#30340;&#31505;&#22768;&#30340;&#26102;&#26426;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#21487;&#20197;&#22522;&#20110;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#30340;&#38646;&#26679;&#26412;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20135;&#29983;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ELaTE&#36890;&#36807;&#38899;&#39057;&#25552;&#31034;&#26469;&#27169;&#20223;&#22768;&#38899;&#29305;&#24449;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#26469;&#25351;&#31034;&#25152;&#29983;&#25104;&#35821;&#38899;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#36755;&#20837;&#26469;&#25511;&#21046;&#31505;&#22768;&#34920;&#24773;&#65292;&#21487;&#20197;&#26159;&#31505;&#22768;&#30340;&#36215;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#65292;&#25110;&#21253;&#21547;&#35201;&#27169;&#20223;&#30340;&#31505;&#22768;&#30340;&#21478;&#22806;&#38899;&#39057;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#25214;&#21040;&#30340;&#25216;&#26415;&#22522;&#30784;&#36827;&#34892;&#20102;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Laughter is one of the most expressive and natural aspects of human speech, conveying emotions, social cues, and humor. However, most text-to-speech (TTS) systems lack the ability to produce realistic and appropriate laughter sounds, limiting their applications and user experience. While there have been prior works to generate natural laughter, they fell short in terms of controlling the timing and variety of the laughter to be generated. In this work, we propose ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker based on a short audio prompt with precise control of laughter timing and expression. Specifically, ELaTE works on the audio prompt to mimic the voice characteristic, the text prompt to indicate the contents of the generated speech, and the input to control the laughter expression, which can be either the start and end times of laughter, or the additional audio prompt that contains laughter to be mimicked. We develop our model based on the foundati
&lt;/p&gt;</description></item><item><title>GenSTL&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#29983;&#25104;&#29305;&#24449;&#22495;&#26469;&#23454;&#29616;&#31232;&#30095;&#36712;&#36857;&#19982;&#23494;&#38598;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#22823;&#35268;&#27169;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.07232</link><description>&lt;p&gt;
GenSTL: &#36890;&#36807;&#29305;&#24449;&#22495;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#23454;&#29616;&#36890;&#29992;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07232
&lt;/p&gt;
&lt;p&gt;
GenSTL&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#29983;&#25104;&#29305;&#24449;&#22495;&#26469;&#23454;&#29616;&#31232;&#30095;&#36712;&#36857;&#19982;&#23494;&#38598;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#22823;&#35268;&#27169;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#26159;&#26102;&#38388;&#25139;&#20301;&#32622;&#26679;&#26412;&#30340;&#24207;&#21015;&#12290;&#22312;&#31232;&#30095;&#36712;&#36857;&#20013;&#65292;&#20301;&#32622;&#26679;&#26412;&#30340;&#37319;&#26679;&#26159;&#19981;&#39057;&#32321;&#30340;&#65307;&#23613;&#31649;&#36825;&#31181;&#36712;&#36857;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#35201;&#20351;&#29992;&#23427;&#20204;&#26469;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#19982;&#20132;&#36890;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#36712;&#36857;&#26159;&#23494;&#38598;&#37319;&#26679;&#30340;&#24182;&#19988;&#32463;&#36807;&#20934;&#30830;&#30340;&#22320;&#22270;&#21305;&#37197;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#20004;&#38454;&#27573;&#26041;&#26696;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#20102;&#25193;&#23637;&#31232;&#30095;&#36712;&#36857;&#30340;&#25928;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;&#26694;&#26550;GenSTL&#12290;&#35813;&#26694;&#26550;&#32463;&#36807;&#39044;&#35757;&#32451;&#20197;&#20351;&#29992;&#29305;&#24449;&#22495;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#24418;&#25104;&#31232;&#30095;&#36712;&#36857;&#19982;&#23494;&#38598;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;GenSTL&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#25110;&#32773;&#21487;&#20197;&#20808;&#36827;&#34892;&#24494;&#35843;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;GenSTL&#28040;&#38500;&#20102;&#23545;&#22823;&#35268;&#27169;&#23494;&#38598;&#21644;&#22320;&#22270;&#21305;&#37197;&#36712;&#36857;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#20854;&#20013;&#21253;&#25324;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#24449;&#22495;&#32534;&#30721;&#23618;&#21644;&#20998;&#23618;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Trajectories are sequences of timestamped location samples. In sparse trajectories, the locations are sampled infrequently; and while such trajectories are prevalent in real-world settings, they are challenging to use to enable high-quality transportation-related applications. Current methodologies either assume densely sampled and accurately map-matched trajectories, or they rely on two-stage schemes, yielding sub-optimal applications.   To extend the utility of sparse trajectories, we propose a novel sparse trajectory learning framework, GenSTL. The framework is pre-trained to form connections between sparse trajectories and dense counterparts using auto-regressive generation of feature domains. GenSTL can subsequently be applied directly in downstream tasks, or it can be fine-tuned first. This way, GenSTL eliminates the reliance on the availability of large-scale dense and map-matched trajectory data. The inclusion of a well-crafted feature domain encoding layer and a hierarchical m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06700</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Entropy-Regularized Token-Level Policy Optimization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#12289;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25110;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#25110;RLHF&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26469;&#20811;&#26381;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#38754;&#20020;&#30528;&#37325;&#37325;&#22256;&#38590;&#65306;1&#65289;&#30001;&#20110;&#24040;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#38656;&#35201;&#25506;&#32034;&#32780;&#20135;&#29983;&#30340;&#19981;&#31283;&#23450;&#24615;&#65307;2&#65289;&#22522;&#20110;&#21160;&#20316;&#32423;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#20934;&#30830;&#24314;&#27169;&#35821;&#26009;&#24211;&#25968;&#25454;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#65288;ETPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22312;&#20196;&#29260;&#32423;&#20248;&#21270;LLMs&#32780;&#35774;&#35745;&#30340;&#29109;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;ETPO&#30340;&#26680;&#24515;&#26159;&#25105;&#20204;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#36880;&#20196;&#29260;&#36719;Bellman&#26356;&#26032;&#31639;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#65292;&#21487;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#38598;&#25104;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#25913;&#21892;&#24403;&#21069;&#32858;&#31867;&#20808;&#39564;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#23558;VMM&#19982;scVI&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.04412</link><description>&lt;p&gt;
VampPrior&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The VampPrior Mixture Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#65292;&#21487;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#38598;&#25104;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#25913;&#21892;&#24403;&#21069;&#32858;&#31867;&#20808;&#39564;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#23558;VMM&#19982;scVI&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;DLVMs&#65289;&#30340;&#32858;&#31867;&#20808;&#39564;&#38656;&#35201;&#39044;&#20808;&#23450;&#20041;&#32858;&#31867;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#36739;&#24046;&#30340;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#21516;&#26102;&#25191;&#34892;&#38598;&#25104;&#21644;&#32858;&#31867;&#30340;&#26041;&#24335;&#26497;&#22823;&#22320;&#25913;&#36827;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;scRNA-seq&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;VampPrior&#65288;Tomczak&#21644;Welling&#65292;2018&#65289;&#35843;&#25972;&#20026;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#24471;&#21040;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#29702;&#36807;&#31243;&#65292;&#20132;&#26367;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#21644;&#32463;&#39564;&#36125;&#21494;&#26031;&#65292;&#20197;&#28165;&#26970;&#22320;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#33719;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#23558;VMM&#19982;&#24191;&#21463;&#27426;&#36814;&#30340;scRNA-seq&#38598;&#25104;&#26041;&#27861;scVI&#65288;Lopez&#31561;&#65292;2018&#65289;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak &amp; Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters. Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.
&lt;/p&gt;</description></item><item><title>Audio Flamingo&#26159;&#19968;&#31181;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#38899;&#39057;&#29702;&#35299;&#33021;&#21147;&#12289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26816;&#32034;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#24378;&#22823;&#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#36798;&#21040;&#20102;&#26368;&#20248;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.01831</link><description>&lt;p&gt;
Audio Flamingo: &#19968;&#31181;&#20855;&#22791;&#24369;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#35805;&#33021;&#21147;&#30340;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01831
&lt;/p&gt;
&lt;p&gt;
Audio Flamingo&#26159;&#19968;&#31181;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#38899;&#39057;&#29702;&#35299;&#33021;&#21147;&#12289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26816;&#32034;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#24378;&#22823;&#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#36798;&#21040;&#20102;&#26368;&#20248;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#29702;&#35299;&#38899;&#39057;&#8212;&#8212;&#21253;&#25324;&#38750;&#35821;&#38899;&#22768;&#38899;&#21644;&#38750;&#35328;&#35821;&#30340;&#35821;&#38899;&#8212;&#8212;&#23545;LLMs&#30340;&#22810;&#26679;&#21270;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Audio Flamingo&#30340;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#38899;&#39057;&#29702;&#35299;&#33021;&#21147;&#12289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26816;&#32034;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#24378;&#22823;&#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#35757;&#32451;&#25216;&#26415;&#12289;&#26550;&#26500;&#35774;&#35745;&#21644;&#25968;&#25454;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#22791;&#36825;&#20123;&#21151;&#33021;&#12290;&#24191;&#27867;&#30340;&#38899;&#39057;&#29702;&#35299;&#20219;&#21153;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20248;&#25104;&#32489;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;PARIS&#21644;LE-PARIS&#65292;&#36890;&#36807;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#12289;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22788;&#29702;&#23457;&#26597;&#24847;&#35265;&#22238;&#24212;&#30340;&#25928;&#29575;&#12290; &#36890;&#36807;&#22810;&#33539;&#24335;&#20998;&#26512;&#21644;&#38271;&#26399;&#25968;&#25454;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#21644;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00421</link><description>&lt;p&gt;
&#20174;PARIS&#21040;LE-PARIS&#65306;&#36890;&#36807;&#25512;&#33616;&#31995;&#32479;&#21644;&#21327;&#20316;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#19987;&#21033;&#21709;&#24212;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;PARIS&#21644;LE-PARIS&#65292;&#36890;&#36807;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#12289;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22788;&#29702;&#23457;&#26597;&#24847;&#35265;&#22238;&#24212;&#30340;&#25928;&#29575;&#12290; &#36890;&#36807;&#22810;&#33539;&#24335;&#20998;&#26512;&#21644;&#38271;&#26399;&#25968;&#25454;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#21644;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19987;&#21033;&#23457;&#26597;&#20013;&#65292;&#23545;&#20110;&#21450;&#26102;&#21644;&#26377;&#25928;&#22320;&#22238;&#24212;&#23457;&#26597;&#24847;&#35265;&#65288;OAs&#65289;&#23545;&#20110;&#33719;&#24471;&#19987;&#21033;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#36807;&#21435;&#30340;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24456;&#23569;&#28041;&#21450;&#21040;&#36825;&#19968;&#26041;&#38754;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#23457;&#26597;&#24847;&#35265;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;&#65288;PARIS&#65289;&#21450;&#20854;&#20808;&#36827;&#29256;&#26412;LE-PARIS&#12290;&#36825;&#20123;&#31995;&#32479;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22312;&#21327;&#20316;&#22788;&#29702;OA&#22238;&#24212;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#21253;&#25324;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#65292;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#65292;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#39564;&#35777;&#28041;&#21450;&#20351;&#29992;USPTO Office Action&#25968;&#25454;&#24211;&#21644;&#24459;&#24072;&#19982;&#25105;&#20204;&#31995;&#32479;&#30340;&#38271;&#26399;&#20132;&#20114;&#25968;&#25454;&#36827;&#34892;&#30340;&#22810;&#33539;&#24335;&#20998;&#26512;&#65292;&#20026;&#26399;&#20845;&#24180;&#12290;&#36890;&#36807;&#20116;&#20010;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#25552;&#20986;&#30340;Delphi&#36807;&#31243;&#26469;&#26816;&#39564;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#65288;&#30740;&#31350;1&#21644;2&#65289;&#65292;&#36824;&#26377;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#26469;&#25552;&#39640;&#22238;&#24212;&#36136;&#37327;&#65288;&#30740;&#31350;3&#21644;4&#65289;&#65292;&#20197;&#21450;&#32463;&#36807;&#35757;&#32451;&#30340;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#65288;&#30740;&#31350;5&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect. To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years. Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of
&lt;/p&gt;</description></item><item><title>Swing&#31639;&#27861;&#36890;&#36807;&#25670;&#21160;&#22312;&#34746;&#26059;&#26041;&#21521;&#20043;&#38388;&#20445;&#25345;&#36890;&#20449;&#33410;&#28857;&#20043;&#38388;&#30340;&#20302;&#36317;&#31163;&#65292;&#20174;&#32780;&#22312;&#34746;&#26059;&#32593;&#32476;&#19978;&#20248;&#21270;Allreduce&#25805;&#20316;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#25552;&#21319;&#39640;&#36798;3&#20493;&#12290;</title><link>https://arxiv.org/abs/2401.09356</link><description>&lt;p&gt;
&#20026;&#23454;&#29616;&#26356;&#39640;&#24102;&#23485;&#30340;Allreduce&#25805;&#20316;&#32780;&#35774;&#35745;&#30340;Swing&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Swing: Short-cutting Rings for Higher Bandwidth Allreduce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09356
&lt;/p&gt;
&lt;p&gt;
Swing&#31639;&#27861;&#36890;&#36807;&#25670;&#21160;&#22312;&#34746;&#26059;&#26041;&#21521;&#20043;&#38388;&#20445;&#25345;&#36890;&#20449;&#33410;&#28857;&#20043;&#38388;&#30340;&#20302;&#36317;&#31163;&#65292;&#20174;&#32780;&#22312;&#34746;&#26059;&#32593;&#32476;&#19978;&#20248;&#21270;Allreduce&#25805;&#20316;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#25552;&#21319;&#39640;&#36798;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Allreduce&#38598;&#20307;&#25805;&#20316;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#36816;&#34892;&#30340;&#24037;&#20316;&#36127;&#36733;&#30340;&#36816;&#34892;&#26102;&#20013;&#21344;&#25454;&#20102;&#30456;&#24403;&#22823;&#30340;&#27604;&#20363;&#12290;&#20854;&#24615;&#33021;&#30340;&#19968;&#20010;&#24433;&#21709;&#22240;&#32032;&#26159;&#36890;&#20449;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#29305;&#21035;&#26159;&#22312;&#34746;&#26059;&#32593;&#32476;&#31561;&#32593;&#32476;&#20013;&#65292;&#36739;&#38271;&#30340;&#36317;&#31163;&#24847;&#21619;&#30528;&#22810;&#20010;&#28040;&#24687;&#22312;&#21516;&#19968;&#38142;&#36335;&#19978;&#36716;&#21457;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;Allreduce&#30340;&#24102;&#23485;&#12290;&#20026;&#20102;&#25552;&#39640;&#34746;&#26059;&#32593;&#32476;&#19978;&#30340;Allreduce&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Swing&#65292;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#34746;&#26059;&#26041;&#21521;&#20043;&#38388;&#25670;&#21160;&#26469;&#20445;&#25345;&#36890;&#20449;&#33410;&#28857;&#20043;&#38388;&#30340;&#20302;&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#35780;&#20272;&#26174;&#31034;&#65292;Swing&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#34746;&#26059;&#21644;&#31867;&#20284;&#34746;&#26059;&#25299;&#25169;&#19978;&#65292;&#23545;&#20110;&#20174;32B&#21040;128MiB&#30340;&#21521;&#37327;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;Allreduce&#31639;&#27861;&#22810;&#36798;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09356v2 Announce Type: replace-cross  Abstract: The allreduce collective operation accounts for a significant fraction of the runtime of workloads running on distributed systems. One factor determining its performance is the distance between communicating nodes, especially on networks like torus, where a higher distance implies multiple messages being forwarded on the same link, thus reducing the allreduce bandwidth. Torus networks are widely used on systems optimized for machine learning workloads (e.g., Google TPUs and Amazon Trainium devices), as well as on some of the Top500 supercomputers. To improve allreduce performance on torus networks we introduce Swing, a new algorithm that keeps a low distance between communicating nodes by swinging between torus directions. Our analysis and experimental evaluation show that Swing outperforms by up to 3x existing allreduce algorithms for vectors ranging from 32B to 128MiB, on different types of torus and torus-like topologies, re
&lt;/p&gt;</description></item><item><title>scDiffusion&#26159;&#19968;&#31181;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#20934;&#30830;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21333;&#32454;&#32990;&#25968;&#25454;&#65292;&#20855;&#26377;&#21463;&#25511;&#26465;&#20214;&#19979;&#29983;&#25104;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.03968</link><description>&lt;p&gt;
scDiffusion&#65306;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26377;&#26465;&#20214;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21333;&#32454;&#32990;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
scDiffusion: conditional generation of high-quality single-cell data using diffusion model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03968
&lt;/p&gt;
&lt;p&gt;
scDiffusion&#26159;&#19968;&#31181;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#20934;&#30830;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21333;&#32454;&#32990;&#25968;&#25454;&#65292;&#20855;&#26377;&#21463;&#25511;&#26465;&#20214;&#19979;&#29983;&#25104;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#65288;scRNA-seq&#65289;&#25968;&#25454;&#23545;&#20110;&#30740;&#31350;&#21333;&#32454;&#32990;&#27700;&#24179;&#29983;&#21629;&#35268;&#24459;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36275;&#22815;&#39640;&#36136;&#37327;&#30340;scRNA-seq&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#29983;&#25104;&#27169;&#22411;&#26469;&#35745;&#31639;&#29983;&#25104;&#21512;&#25104;scRNA-seq&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#23578;&#19981;&#22815;&#30495;&#23454;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#29983;&#25104;&#21463;&#25511;&#26465;&#20214;&#25968;&#25454;&#26102;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20854;&#22312;&#39640;&#20445;&#30495;&#24230;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;scRNA-seq&#29983;&#25104;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;scDiffusion&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#25193;&#25955;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20197;&#21463;&#25511;&#26465;&#20214;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;scRNA-seq&#25968;&#25454;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#20010;&#20998;&#31867;&#22120;&#26469;&#21516;&#26102;&#25351;&#23548;&#25193;&#25955;&#36807;&#31243;&#65292;&#20351;scDiffusion&#33021;&#22815;&#22312;&#22810;&#20010;&#26465;&#20214;&#19979;&#29983;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03968v2 Announce Type: replace-cross  Abstract: Single-cell RNA sequencing (scRNA-seq) data are important for studying the laws of life at single-cell level. However, it is still challenging to obtain enough high-quality scRNA-seq data. To mitigate the limited availability of data, generative models have been proposed to computationally generate synthetic scRNA-seq data. Nevertheless, the data generated with current models are not very realistic yet, especially when we need to generate data with controlled conditions. In the meantime, the Diffusion models have shown their power in generating data at high fidelity, providing a new opportunity for scRNA-seq generation.   In this study, we developed scDiffusion, a generative model combining diffusion model and foundation model to generate high-quality scRNA-seq data with controlled conditions. We designed multiple classifiers to guide the diffusion process simultaneously, enabling scDiffusion to generate data under multiple con
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20195;&#30721;&#32534;&#36753;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22312;&#25351;&#20196;&#24335;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#19978;&#21457;&#29616;&#20102;&#24320;&#25918;&#21644;&#23553;&#38381;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2312.12450</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36827;&#34892;&#32534;&#36753;&#65311;&#35780;&#20272;&#20854;&#36981;&#24490;&#20195;&#30721;&#32534;&#36753;&#25351;&#20196;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20195;&#30721;&#32534;&#36753;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22312;&#25351;&#20196;&#24335;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#19978;&#21457;&#29616;&#20102;&#24320;&#25918;&#21644;&#23553;&#38381;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#38598;&#20013;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#21508;&#31181;&#20195;&#30721;&#21512;&#25104;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#21512;&#25104;&#20195;&#30721;&#65292;&#20174;&#20195;&#30721;&#20013;&#21512;&#25104;&#27979;&#35797;&#65292;&#20197;&#21450;&#20174;&#20195;&#30721;&#20013;&#21512;&#25104;&#35299;&#37322;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#25351;&#20196;&#24335;&#20195;&#30721;&#32534;&#36753;&#30340;&#34892;&#20026;&#30740;&#31350;&#19981;&#36275;&#12290;&#36825;&#20123;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#25353;&#29031;&#25552;&#20379;&#30340;&#25552;&#31034;&#26356;&#26032;&#19968;&#22359;&#20195;&#30721;&#12290;&#32534;&#36753;&#25351;&#20196;&#21487;&#33021;&#35201;&#27714;&#28155;&#21152;&#25110;&#21024;&#38500;&#21151;&#33021;&#65292;&#25551;&#36848;&#38169;&#35823;&#24182;&#35201;&#27714;&#20462;&#22797;&#65292;&#35201;&#27714;&#19981;&#21516;&#31867;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#32773;&#20854;&#20182;&#24120;&#35265;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#22522;&#20934;&#65292;&#24182;&#29992;&#23427;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#21644;&#23553;&#38381;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#21363;&#20351;&#26159;GPT-3.5-Turbo&#20063;&#27604;&#26368;&#22909;&#30340;&#24320;&#25918;&#27169;&#22411;&#22312;&#32534;&#36753;&#20195;&#30721;&#26041;&#38754;&#22909;&#20102;8.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12450v4 Announce Type: replace-cross  Abstract: A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language instructions, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is instructed to update a block of code provided in a prompt. The editing instruction may ask for a feature to added or removed, describe a bug and ask for a fix, ask for a different kind of solution, or many other common code editing tasks.   We introduce a carefully crafted benchmark of code editing tasks and use it evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is 8.8% better than the best open model at editing cod
&lt;/p&gt;</description></item><item><title>RiskBench&#26159;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#39118;&#38505;&#35782;&#21035;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#20351;&#29992;&#29420;&#31435;&#25968;&#25454;&#38598;&#35780;&#20272;&#39118;&#38505;&#35782;&#21035;&#31639;&#27861;&#26102;&#30340;&#22256;&#38590;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#30340;&#30452;&#25509;&#27604;&#36739;&#21644;&#23433;&#20840;&#24615;&#33021;&#25552;&#21319;&#25552;&#20379;&#38598;&#20307;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2312.01659</link><description>&lt;p&gt;
RiskBench&#65306;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#39118;&#38505;&#35782;&#21035;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RiskBench: A Scenario-based Benchmark for Risk Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01659
&lt;/p&gt;
&lt;p&gt;
RiskBench&#26159;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#39118;&#38505;&#35782;&#21035;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#20351;&#29992;&#29420;&#31435;&#25968;&#25454;&#38598;&#35780;&#20272;&#39118;&#38505;&#35782;&#21035;&#31639;&#27861;&#26102;&#30340;&#22256;&#38590;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#30340;&#30452;&#25509;&#27604;&#36739;&#21644;&#23433;&#20840;&#24615;&#33021;&#25552;&#21319;&#25552;&#20379;&#38598;&#20307;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#26088;&#22312;&#23454;&#29616;&#38646;&#30896;&#25758;&#30340;&#20986;&#34892;&#20307;&#39564;&#65292;&#38656;&#35201;&#36328;&#23398;&#31185;&#21162;&#21147;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#33021;&#12290;&#26412;&#25991;&#20851;&#27880;&#39118;&#38505;&#35782;&#21035;&#65292;&#21363;&#35782;&#21035;&#21644;&#20998;&#26512;&#28304;&#33258;&#21160;&#24577;&#20132;&#36890;&#21442;&#19982;&#32773;&#21644;&#24847;&#22806;&#20107;&#20214;&#30340;&#39118;&#38505;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#31038;&#21306;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#23545;&#19981;&#21516;&#39118;&#38505;&#35782;&#21035;&#31639;&#27861;&#30340;&#35780;&#20272;&#20351;&#29992;&#29420;&#31435;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#38590;&#20197;&#30452;&#25509;&#27604;&#36739;&#65292;&#24182;&#38459;&#30861;&#20102;&#26397;&#30528;&#23433;&#20840;&#24615;&#33021;&#25552;&#21319;&#30340;&#38598;&#20307;&#36827;&#23637;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RiskBench&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#39118;&#38505;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#22522;&#20110;&#22330;&#26223;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22330;&#26223;&#20998;&#31867;&#27861;&#21644;&#22686;&#24378;&#31649;&#36947;&#65292;&#20197;&#20415;&#31995;&#32479;&#22320;&#25910;&#38598;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#22320;&#38754;&#30495;&#23454;&#39118;&#38505;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21313;&#31181;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#65288;1&#65289;&#26816;&#27979;&#21644;&#23450;&#20301;&#39118;&#38505;&#65292;&#65288;2&#65289;&#39044;&#27979;&#39118;&#38505;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01659v2 Announce Type: replace-cross  Abstract: Intelligent driving systems aim to achieve a zero-collision mobility experience, requiring interdisciplinary efforts to enhance safety performance. This work focuses on risk identification, the process of identifying and analyzing risks stemming from dynamic traffic participants and unexpected events. While significant advances have been made in the community, the current evaluation of different risk identification algorithms uses independent datasets, leading to difficulty in direct comparison and hindering collective progress toward safety performance enhancement. To address this limitation, we introduce \textbf{RiskBench}, a large-scale scenario-based benchmark for risk identification. We design a scenario taxonomy and augmentation pipeline to enable a systematic collection of ground truth risks under diverse scenarios. We assess the ability of ten algorithms to (1) detect and locate risks, (2) anticipate risks, and (3) faci
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#33647;&#29289;&#26679;&#23567;&#20998;&#23376;&#21644;&#21270;&#23398;&#21453;&#24212;&#25968;&#25454;&#24211;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#26377;&#26426;&#26448;&#26009;&#34394;&#25311;&#31579;&#36873;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18377</link><description>&lt;p&gt;
&#36328;&#19981;&#21516;&#21270;&#23398;&#39046;&#22495;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#39044;&#35757;&#32451;&#20110;&#23567;&#20998;&#23376;&#21644;&#21270;&#23398;&#21453;&#24212;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#26377;&#26426;&#26448;&#26009;&#34394;&#25311;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning across Different Chemical Domains: Virtual Screening of Organic Materials with Deep Learning Models Pretrained on Small Molecule and Chemical Reaction Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#33647;&#29289;&#26679;&#23567;&#20998;&#23376;&#21644;&#21270;&#23398;&#21453;&#24212;&#25968;&#25454;&#24211;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#26377;&#26426;&#26448;&#26009;&#34394;&#25311;&#31579;&#36873;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30001;&#20110;&#20854;&#22312;&#26377;&#26426;&#26448;&#26009;&#34394;&#25311;&#31579;&#36873;&#20013;&#30340;&#25104;&#26412;&#25928;&#30410;&#20248;&#20110;&#20256;&#32479;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#25216;&#26415;&#65292;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26377;&#26426;&#26448;&#26009;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#23545;&#20110;&#35757;&#32451;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#33647;&#29289;&#26679;&#23567;&#20998;&#23376;&#21644;&#21270;&#23398;&#21453;&#24212;&#25968;&#25454;&#24211;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#26377;&#26426;&#26448;&#26009;&#34394;&#25311;&#31579;&#36873;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;BERT&#27169;&#22411;&#19982;&#26469;&#33258;&#20116;&#20010;&#34394;&#25311;&#31579;&#36873;&#20219;&#21153;&#30340;&#25968;&#25454;&#24494;&#35843;&#65292;&#20351;&#29992;USPTO-SMILES&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#29256;&#26412;&#22312;&#19977;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36229;&#36807;0.94&#30340;R2&#24471;&#20998;&#65292;&#20854;&#20313;&#20004;&#20010;&#20219;&#21153;&#30340;&#24471;&#20998;&#36229;&#36807;0.81&#12290;&#36825;&#19968;&#24615;&#33021;&#36229;&#36807;&#20102;&#22312;&#23567;&#20998;&#23376;&#25110;&#26377;&#26426;&#26448;&#26009;&#25968;&#25454;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#32988;&#36807;&#20102;&#30452;&#25509;&#22312;&#34394;&#25311;&#31579;&#36873;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#19977;&#20010;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18377v2 Announce Type: replace-cross  Abstract: Machine learning is becoming a preferred method for the virtual screening of organic materials due to its cost-effectiveness over traditional computationally demanding techniques. However, the scarcity of labeled data for organic materials poses a significant challenge for training advanced machine learning models. This study showcases the potential of utilizing databases of drug-like small molecules and chemical reactions to pretrain the BERT model, enhancing its performance in the virtual screening of organic materials. By fine-tuning the BERT models with data from five virtual screening tasks, the version pretrained with the USPTO-SMILES dataset achieved R2 scores exceeding 0.94 for three tasks and over 0.81 for two others. This performance surpasses that of models pretrained on the small molecule or organic materials databases and outperforms three traditional machine learning models trained directly on virtual screening da
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22270;&#28388;&#27874;&#22120;&#30340;&#22810;&#39033;&#24335;&#22522;&#30784;&#19982;&#22270;&#24322;&#36136;&#24615;&#24230;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#24322;&#36136;&#24615;&#22522;&#30784;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2311.18177</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#22810;&#39033;&#24335;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
An Effective Universal Polynomial Basis for Spectral Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18177
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22270;&#28388;&#27874;&#22120;&#30340;&#22810;&#39033;&#24335;&#22522;&#30784;&#19982;&#22270;&#24322;&#36136;&#24615;&#24230;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#24322;&#36136;&#24615;&#22522;&#30784;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#20063;&#31216;&#20026;&#22270;&#28388;&#27874;&#22120;&#65292;&#22312;&#24322;&#26500;&#22270;&#19978;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#26368;&#20248;&#22270;&#28388;&#27874;&#22120;&#20381;&#36182;&#20110;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#20998;&#35299;&#36827;&#34892;&#20613;&#31435;&#21494;&#21464;&#25442;&#12290;&#20026;&#20102;&#36991;&#20813;&#32321;&#29712;&#30340;&#35745;&#31639;&#65292;&#35768;&#22810;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#30340;&#22810;&#39033;&#24335;&#26469;&#36817;&#20284;&#25152;&#38656;&#30340;&#22270;&#28388;&#27874;&#22120;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20013;&#30340;&#22810;&#39033;&#24335;&#26159;&#39044;&#23450;&#20041;&#30340;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#22270;&#19978;&#20445;&#25345;&#19981;&#21464;&#65292;&#26080;&#27861;&#36866;&#24212;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#22810;&#26679;&#30340;&#24322;&#36136;&#24615;&#31243;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24443;&#24213;&#30340;&#29702;&#35770;&#20998;&#26512;&#35843;&#26597;&#25152;&#38656;&#22270;&#28388;&#27874;&#22120;&#30340;&#22810;&#39033;&#24335;&#22522;&#30784;&#19982;&#22270;&#24322;&#36136;&#24615;&#24230;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21512;&#24182;&#22270;&#24322;&#36136;&#24615;&#24230;&#37327;&#24320;&#21457;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#24322;&#36136;&#24615;&#22522;&#30784;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#24322;&#36136;&#24615;&#22522;&#30784;&#19982;&#21516;&#36136;&#24615;&#22522;&#30784;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#39033;&#24335;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18177v2 Announce Type: replace  Abstract: Spectral Graph Neural Networks (GNNs), also referred to as graph filters have gained increasing prevalence for heterophily graphs. Optimal graph filters rely on Laplacian eigendecomposition for Fourier transform. In an attempt to avert the prohibitive computations, numerous polynomial filters by leveraging distinct polynomials have been proposed to approximate the desired graph filters. However, polynomials in the majority of polynomial filters are predefined and remain fixed across all graphs, failing to accommodate the diverse heterophily degrees across different graphs. To tackle this issue, we first investigate the correlation between polynomial bases of desired graph filters and the degrees of graph heterophily via a thorough theoretical analysis. Afterward, we develop an adaptive heterophily basis by incorporating graph heterophily degrees. Subsequently, we integrate this heterophily basis with the homophily basis, creating a u
&lt;/p&gt;</description></item><item><title>TransNAS-TSAD&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;Transformer&#26550;&#26500;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;NSGA-II&#31639;&#27861;&#20248;&#21270;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#21644;&#23450;&#21046;&#26550;&#26500;&#36866;&#24212;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18061</link><description>&lt;p&gt;
TransNAS-TSAD&#65306;&#21033;&#29992;Transformer&#36827;&#34892;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TransNAS-TSAD: Harnessing Transformers for Multi-Objective Neural Architecture Search in Time Series Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18061
&lt;/p&gt;
&lt;p&gt;
TransNAS-TSAD&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;Transformer&#26550;&#26500;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;NSGA-II&#31639;&#27861;&#20248;&#21270;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#21644;&#23450;&#21046;&#26550;&#26500;&#36866;&#24212;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#34892;&#19994;&#23454;&#26102;&#25968;&#25454;&#25910;&#38598;&#28608;&#22686;&#30340;&#32972;&#26223;&#19979;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#22312;&#19968;&#20803;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#36827;&#34892;&#20808;&#36827;&#24322;&#24120;&#26816;&#27979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TransNAS-TSAD&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;Transformer&#26550;&#26500;&#19982;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;NSGA-II&#31639;&#27861;&#20248;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#24179;&#34913;&#20102;&#35745;&#31639;&#25928;&#29575;&#19982;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;TransNAS-TSAD&#30001;&#20110;&#20854;&#23450;&#21046;&#30340;&#26550;&#26500;&#36866;&#24212;&#24615;&#21644;&#39640;&#25928;&#30340;&#22797;&#26434;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#24773;&#22659;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Efficiency-Accuracy-Complexity Score&#65288;EACS&#65289;&#20316;&#20026;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#26032;&#25351;&#26631;&#65292;&#24378;&#35843;&#20102;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;TransNAS-TSAD&#26641;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18061v3 Announce Type: replace  Abstract: The surge in real-time data collection across various industries has underscored the need for advanced anomaly detection in both univariate and multivariate time series data. This paper introduces TransNAS-TSAD, a framework that synergizes the transformer architecture with neural architecture search (NAS), enhanced through NSGA-II algorithm optimization. This approach effectively tackles the complexities of time series data, balancing computational efficiency with detection accuracy. Our evaluation reveals that TransNAS-TSAD surpasses conventional anomaly detection models due to its tailored architectural adaptability and the efficient exploration of complex search spaces, leading to marked improvements in diverse data scenarios. We also introduce the Efficiency-Accuracy-Complexity Score (EACS) as a new metric for assessing model performance, emphasizing the balance between accuracy and computational resources. TransNAS-TSAD sets a n
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Action-slot&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27133;&#27880;&#24847;&#21147;&#23398;&#20064;&#35270;&#35273;&#21160;&#20316;&#20013;&#24515;&#34920;&#31034;&#65292;&#22312;&#20132;&#36890;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#22810;&#26631;&#31614;&#21407;&#23376;&#27963;&#21160;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2311.17948</link><description>&lt;p&gt;
&#21160;&#20316;&#27133;&#65306;&#20132;&#36890;&#22330;&#26223;&#20013;&#22810;&#26631;&#31614;&#21407;&#23376;&#27963;&#21160;&#35782;&#21035;&#30340;&#35270;&#35273;&#21160;&#20316;&#20013;&#24515;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17948
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Action-slot&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27133;&#27880;&#24847;&#21147;&#23398;&#20064;&#35270;&#35273;&#21160;&#20316;&#20013;&#24515;&#34920;&#31034;&#65292;&#22312;&#20132;&#36890;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#22810;&#26631;&#31614;&#21407;&#23376;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#21407;&#23376;&#27963;&#21160;&#35782;&#21035;&#12290;&#23613;&#31649;&#22312;&#21160;&#20316;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#23545;&#22810;&#20010;&#36947;&#36335;&#29992;&#25143;&#21160;&#20316;&#21450;&#20854;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#25972;&#20307;&#29702;&#35299;&#19981;&#36275;&#65292;&#35782;&#21035;&#21407;&#23376;&#27963;&#21160;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27133;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#8212;&#8212;&#21160;&#20316;&#27133;&#65292;&#23427;&#23398;&#20064;&#35270;&#35273;&#21160;&#20316;&#20013;&#24515;&#34920;&#31034;&#65292;&#25429;&#25417;&#20102;&#21160;&#20316;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#33021;&#22815;&#27880;&#24847;&#21040;&#21407;&#23376;&#27963;&#21160;&#21457;&#29983;&#20301;&#32622;&#30340;&#21160;&#20316;&#27133;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#24863;&#30693;&#24341;&#23548;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#27133;&#30340;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32972;&#26223;&#27133;&#65292;&#19982;&#21160;&#20316;&#27133;&#31454;&#20105;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#19981;&#24517;&#35201;&#22320;&#20851;&#27880;&#32570;&#20047;&#27963;&#21160;&#30340;&#32972;&#26223;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#19981;&#24179;&#34913;&#30340;&#31867;&#20998;&#24067;&#38459;&#30861;&#20102;&#23545;&#31232;&#26377;&#27963;&#21160;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17948v1 Announce Type: cross  Abstract: In this paper, we study multi-label atomic activity recognition. Despite the notable progress in action recognition, it is still challenging to recognize atomic activities due to a deficiency in a holistic understanding of both multiple road users' motions and their contextual information. In this paper, we introduce Action-slot, a slot attention-based approach that learns visual action-centric representations, capturing both motion and contextual information. Our key idea is to design action slots that are capable of paying attention to regions where atomic activities occur, without the need for explicit perception guidance. To further enhance slot attention, we introduce a background slot that competes with action slots, aiding the training process in avoiding unnecessary focus on background regions devoid of activities. Yet, the imbalanced class distribution in the existing dataset hampers the assessment of rare activities. To addre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2311.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#21512;&#25104;&#21644;&#25193;&#25955;&#27169;&#22411;&#20943;&#36731;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases with Diverse Ensembles and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21363;&#22810;&#20010;&#32447;&#32034;&#21487;&#20197;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#24120;&#24120;&#23548;&#33268;&#19968;&#31181;&#31216;&#20026;&#25463;&#24452;&#20559;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#12289;&#26131;&#23398;&#30340;&#32447;&#32034;&#65292;&#32780;&#24573;&#30053;&#21487;&#38752;&#30340;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#30340;&#38598;&#25104;&#22810;&#26679;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#30340;&#35757;&#32451;&#38388;&#38548;&#20013;&#65292;DPMs&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21363;&#20351;&#22312;&#26174;&#31034;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#36890;&#36807;&#38598;&#25104;&#19981;&#19968;&#33268;&#24615;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DPM&#24341;&#23548;&#30340;&#22810;&#26679;&#21270;&#36275;&#20197;&#28040;&#38500;&#23545;&#20027;&#35201;&#25463;&#24452;&#32447;&#32034;&#30340;&#20381;&#36182;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20960;&#20010;&#22810;&#26679;&#21270;&#30446;&#26631;&#19978;&#22312;&#23454;&#35777;&#19978;&#37327;&#21270;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26368;&#32456;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16176v2 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut bias, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalizati
&lt;/p&gt;</description></item><item><title>&#22312;&#24369;&#30417;&#30563;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#20110;&#24120;&#35268;&#35748;&#30693;&#30340;&#35266;&#24565;&#65292;&#30740;&#31350;&#21457;&#29616;&#30465;&#30053;&#26579;&#33394;&#26631;&#20934;&#21270;&#21644;&#22270;&#20687;&#22686;&#24378;&#24182;&#19981;&#20250;&#24433;&#21709;&#19979;&#28216;&#20999;&#29255;&#32423;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2311.11772</link><description>&lt;p&gt;
&#19968;&#20010;&#33391;&#22909;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#23601;&#26159;&#20320;&#22312;&#24369;&#30417;&#30563;&#30149;&#29702;&#23398;&#20999;&#29255;&#20998;&#31867;&#20013;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
A Good Feature Extractor Is All You Need for Weakly Supervised Pathology Slide Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11772
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24369;&#30417;&#30563;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#20110;&#24120;&#35268;&#35748;&#30693;&#30340;&#35266;&#24565;&#65292;&#30740;&#31350;&#21457;&#29616;&#30465;&#30053;&#26579;&#33394;&#26631;&#20934;&#21270;&#21644;&#22270;&#20687;&#22686;&#24378;&#24182;&#19981;&#20250;&#24433;&#21709;&#19979;&#28216;&#20999;&#29255;&#32423;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35268;&#35748;&#20026;&#26579;&#33394;&#26631;&#20934;&#21270;&#26159;&#35745;&#31639;&#30149;&#29702;&#23398;&#27969;&#31243;&#20013;&#20851;&#38190;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#24369;&#30417;&#30563;&#30340;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#29615;&#22659;&#20013;&#23545;&#36825;&#19968;&#20449;&#24565;&#25552;&#20986;&#36136;&#30097;&#65292;&#36825;&#19968;&#20449;&#24565;&#26159;&#30001;&#35757;&#32451;&#22312;&#22810;&#26679;&#21270;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24378;&#22823;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#20986;&#29616;&#25152;&#28608;&#21169;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#36804;&#20170;&#20026;&#27490;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#30149;&#29702;&#23398;&#29305;&#24449;&#25552;&#21462;&#22120;&#36827;&#34892;&#20102;&#26368;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#28041;&#21450;&#20061;&#20010;&#20219;&#21153;&#12289;&#20116;&#20010;&#25968;&#25454;&#38598;&#12289;&#19977;&#20010;&#19979;&#28216;&#26550;&#26500;&#21644;&#21508;&#31181;&#39044;&#22788;&#29702;&#35774;&#32622;&#20013;&#30340;8000&#22810;&#20010;&#35757;&#32451;&#36816;&#34892;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24573;&#30053;&#26579;&#33394;&#26631;&#20934;&#21270;&#21644;&#22270;&#20687;&#22686;&#24378;&#24182;&#19981;&#20250;&#25439;&#23475;&#19979;&#28216;&#20999;&#29255;&#32423;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#20250;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#19978;&#24102;&#26469;&#22823;&#37327;&#33410;&#30465;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20419;&#36827;&#20102;&#30456;&#23545;&#19979;&#28216;&#24615;&#33021;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26368;&#22909;&#30340;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#25552;&#21462;&#22120;&#65292;&#24182;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11772v4 Announce Type: replace-cross  Abstract: Stain normalisation is thought to be a crucial preprocessing step in computational pathology pipelines. We question this belief in the context of weakly supervised whole slide image classification, motivated by the emergence of powerful feature extractors trained using self-supervised learning on diverse pathology datasets. To this end, we performed the most comprehensive evaluation of publicly available pathology feature extractors to date, involving more than 8,000 training runs across nine tasks, five datasets, three downstream architectures, and various preprocessing setups. Notably, we find that omitting stain normalisation and image augmentations does not compromise downstream slide-level classification performance, while incurring substantial savings in memory and compute. Using a new evaluation metric that facilitates relative downstream performance comparison, we identify the best publicly available extractors, and sho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19977;&#32500;&#22521;&#20859;&#20013;&#37327;&#21270;&#32454;&#32990;&#30340;&#27963;&#21147;&#65292;&#26080;&#38656;&#22522;&#20110;&#35797;&#21058;&#30340;&#25351;&#31034;&#29289;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20854;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#34920;&#29616;&#31867;&#20284;&#12290;</title><link>https://arxiv.org/abs/2311.09354</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#20998;&#21106;&#36827;&#34892;&#19977;&#32500;&#32452;&#32455;&#22521;&#20859;&#30340;&#38750;&#30772;&#22351;&#24615;&#23450;&#37327;&#27963;&#21147;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Nondestructive, quantitative viability analysis of 3D tissue cultures using machine learning image segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19977;&#32500;&#22521;&#20859;&#20013;&#37327;&#21270;&#32454;&#32990;&#30340;&#27963;&#21147;&#65292;&#26080;&#38656;&#22522;&#20110;&#35797;&#21058;&#30340;&#25351;&#31034;&#29289;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20854;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#34920;&#29616;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#19981;&#21516;&#32454;&#32990;&#22521;&#20859;&#26465;&#20214;&#19979;&#32454;&#32990;&#30340;&#38598;&#20307;&#27963;&#21147;&#36890;&#24120;&#20381;&#36182;&#20110;&#24179;&#22343;&#33394;&#24230;&#25351;&#26631;&#65292;&#24182;&#19988;&#36890;&#24120;&#20197;&#31616;&#21333;&#30340;&#20108;&#36827;&#21046;&#35835;&#25968;&#25253;&#21578;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#27963;&#21147;&#35780;&#20272;&#25216;&#26415;&#19982;&#22522;&#20110;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#33258;&#21160;&#21270;&#29305;&#24449;&#21270;&#32454;&#32990;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#27963;&#21147;&#27979;&#37327;&#25216;&#26415;&#65292;&#20197;&#35780;&#20272;&#21487;&#33021;&#30340;&#32454;&#32990;&#29366;&#24577;&#36830;&#32493;&#24615;&#21644;&#23545;&#22521;&#20859;&#26465;&#20214;&#19979;&#30340;&#24178;&#25200;&#21709;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19977;&#32500;&#22521;&#20859;&#20013;&#37327;&#21270;&#32454;&#32990;&#30340;&#27963;&#21147;&#65292;&#26080;&#38656;&#22522;&#20110;&#35797;&#21058;&#30340;&#25351;&#31034;&#29289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#31995;&#21015;&#22825;&#25968;&#21644;&#22521;&#20859;&#22522;&#32452;&#25104;&#19979;&#22312;&#20840;&#23380;&#22270;&#20687;&#20013;&#30340;&#34920;&#29616;&#19982;&#19968;&#23545;&#20154;&#31867;&#19987;&#23478;&#31867;&#20284;&#12290;&#20026;&#20102;&#23637;&#31034;&#28508;&#22312;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32437;&#21521;&#30740;&#31350;&#65292;&#35843;&#26597;&#24050;&#30693;&#30103;&#27861;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09354v2 Announce Type: replace-cross  Abstract: Ascertaining the collective viability of cells in different cell culture conditions has typically relied on averaging colorimetric indicators and is often reported out in simple binary readouts. Recent research has combined viability assessment techniques with image-based deep-learning models to automate the characterization of cellular properties. However, further development of viability measurements to assess the continuity of possible cellular states and responses to perturbation across cell culture conditions is needed. In this work, we demonstrate an image processing algorithm for quantifying cellular viability in 3D cultures without the need for assay-based indicators. We show that our algorithm performs similarly to a pair of human experts in whole-well images over a range of days and culture matrix compositions. To demonstrate potential utility, we perform a longitudinal study investigating the impact of a known therap
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#25193;&#23637;&#23545;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#23548;&#33268;&#20449;&#24687;&#27844;&#28431;&#30340;&#21407;&#22240;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#25552;&#20379;&#20102;&#21508;&#31181;&#21487;&#33021;&#22312;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#20013;&#20986;&#29616;&#30340;&#27844;&#28431;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2311.04179</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Leakage in Machine Learning Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#25193;&#23637;&#23545;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#23548;&#33268;&#20449;&#24687;&#27844;&#28431;&#30340;&#21407;&#22240;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#25552;&#20379;&#20102;&#21508;&#31181;&#21487;&#33021;&#22312;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#20013;&#20986;&#29616;&#30340;&#27844;&#28431;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#39044;&#27979;&#24314;&#27169;&#24037;&#20855;&#65292;&#20854;&#21463;&#27426;&#36814;&#31243;&#24230;&#28304;&#33258;&#20110;&#22312;&#29289;&#29702;&#23398;&#12289;&#24066;&#22330;&#33829;&#38144;&#12289;&#21307;&#30103;&#20445;&#20581;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#24212;&#29992;&#26679;&#26412;&#32423;&#21035;&#30340;&#39044;&#27979;&#30340;&#25215;&#35834;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#26410;&#32463;&#36866;&#24403;&#23454;&#26045;&#21644;&#35780;&#20272;&#65292;ML&#31649;&#36947;&#21487;&#33021;&#21253;&#21547;&#27844;&#28431;&#65292;&#36890;&#24120;&#23548;&#33268;&#36807;&#24230;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#24182;&#19988;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#12290;&#36825;&#21487;&#33021;&#23545;&#36130;&#21153;&#21644;&#31038;&#20250;&#20135;&#29983;&#20005;&#37325;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;ML&#31649;&#36947;&#26102;&#25193;&#23637;&#19982;&#23548;&#33268;&#27844;&#28431;&#30456;&#20851;&#30340;&#21407;&#22240;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#35828;&#26126;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;ML&#31649;&#36947;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#21508;&#31181;&#31867;&#22411;&#27844;&#28431;&#30340;&#32508;&#21512;&#27010;&#36848;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04179v2 Announce Type: replace-cross  Abstract: Machine learning (ML) provides powerful tools for predictive modeling. ML's popularity stems from the promise of sample-level prediction with applications across a variety of fields from physics and marketing to healthcare. However, if not properly implemented and evaluated, ML pipelines may contain leakage typically resulting in overoptimistic performance estimates and failure to generalize to new data. This can have severe negative financial and societal implications. Our aim is to expand understanding associated with causes leading to leakage when designing, implementing, and evaluating ML pipelines. Illustrated by concrete examples, we provide a comprehensive overview and discussion of various types of leakage that may arise in ML pipelines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#21644;&#25674;&#38144;&#21464;&#20998;&#25512;&#26029;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21333;&#33218;&#35797;&#39564;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#30340;&#21327;&#21464;&#37327;&#35266;&#23519;&#65292;&#23454;&#29616;&#24739;&#32773;&#21305;&#37197;&#25110;&#30452;&#25509;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2311.03002</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#21464;&#37327;&#24314;&#27169;&#20174;&#21333;&#33218;&#35797;&#39564;&#20013;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Estimating treatment effects from single-arm trials via latent-variable modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03002
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#21644;&#25674;&#38144;&#21464;&#20998;&#25512;&#26029;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21333;&#33218;&#35797;&#39564;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#30340;&#21327;&#21464;&#37327;&#35266;&#23519;&#65292;&#23454;&#29616;&#24739;&#32773;&#21305;&#37197;&#25110;&#30452;&#25509;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCTs&#65289;&#34987;&#25509;&#21463;&#20026;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#26631;&#20934;&#65292;&#20294;&#30001;&#20110;&#20262;&#29702;&#21407;&#22240;&#21644;&#25104;&#26412;&#36807;&#39640;&#32780;&#19981;&#21487;&#34892;&#12290;&#21333;&#33218;&#35797;&#39564;&#65292;&#25152;&#26377;&#24739;&#32773;&#23646;&#20110;&#27835;&#30103;&#32452;&#65292;&#21487;&#33021;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#38656;&#35201;&#35775;&#38382;&#22806;&#37096;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35782;&#21035;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#32467;&#26500;&#21270;&#32570;&#22833;&#27169;&#24335;&#26469;&#32771;&#34385;&#32570;&#22833;&#30340;&#21327;&#21464;&#37327;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25674;&#38144;&#21464;&#20998;&#25512;&#26029;&#26469;&#23398;&#20064;&#26082;&#29305;&#23450;&#20110;&#32452;&#21448;&#21487;&#35782;&#21035;&#30340;&#20849;&#20139;&#28508;&#22312;&#34920;&#31034;&#65292;&#38543;&#21518;&#21487;&#29992;&#20110;&#65288;i&#65289;&#24739;&#32773;&#21305;&#37197;&#65292;&#22914;&#26524;&#27835;&#30103;&#32452;&#30340;&#27835;&#30103;&#32467;&#26524;&#19981;&#21487;&#29992;&#65292;&#25110;&#29992;&#20110;&#65288;ii&#65289;&#20551;&#23450;&#20004;&#32452;&#22343;&#26377;&#32467;&#26524;&#21487;&#29992;&#30340;&#30452;&#25509;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#20849;&#22522;&#20934;&#21644;&#19968;&#20010;&#30001;&#24050;&#21457;&#34920;&#30340;RCT&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03002v2 Announce Type: replace  Abstract: Randomized controlled trials (RCTs) are the accepted standard for treatment effect estimation but they can be infeasible due to ethical reasons and prohibitive costs. Single-arm trials, where all patients belong to the treatment group, can be a viable alternative but require access to an external control group. We propose an identifiable deep latent-variable model for this scenario that can also account for missing covariate observations by modeling their structured missingness patterns. Our method uses amortized variational inference to learn both group-specific and identifiable shared latent representations, which can subsequently be used for {\em (i)} patient matching if treatment outcomes are not available for the treatment group, or for {\em (ii)} direct treatment effect estimation assuming outcomes are available for both groups. We evaluate the model on a public benchmark as well as on a data set consisting of a published RCT s
&lt;/p&gt;</description></item><item><title>&#24555;&#36895;&#21452;&#26354;&#20915;&#31574;&#26641;&#31639;&#27861;hyperDT&#36890;&#36807;&#21033;&#29992;&#20869;&#31215;&#23558;&#27431;&#20960;&#37324;&#24471;&#20915;&#31574;&#26641;&#31639;&#27861;&#35843;&#25972;&#21040;&#21452;&#26354;&#31354;&#38388;&#65292;&#28040;&#38500;&#20102;&#23545;&#35745;&#31639;&#23494;&#38598;&#22411;&#40654;&#26364;&#20248;&#21270;&#21644;&#25968;&#20540;&#19981;&#31283;&#23450;&#25805;&#20316;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2310.13841</link><description>&lt;p&gt;
&#24555;&#36895;&#21452;&#26354;&#20915;&#31574;&#26641;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast hyperboloid decision tree algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13841
&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21452;&#26354;&#20915;&#31574;&#26641;&#31639;&#27861;hyperDT&#36890;&#36807;&#21033;&#29992;&#20869;&#31215;&#23558;&#27431;&#20960;&#37324;&#24471;&#20915;&#31574;&#26641;&#31639;&#27861;&#35843;&#25972;&#21040;&#21452;&#26354;&#31354;&#38388;&#65292;&#28040;&#38500;&#20102;&#23545;&#35745;&#31639;&#23494;&#38598;&#22411;&#40654;&#26364;&#20248;&#21270;&#21644;&#25968;&#20540;&#19981;&#31283;&#23450;&#25805;&#20316;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#26354;&#20960;&#20309;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#26377;&#25928;&#25429;&#25417;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#21452;&#26354;&#31354;&#38388;&#20013;&#30340;&#37051;&#22495;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#25552;&#20379;&#20102;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#25345;&#32493;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21452;&#26354;&#20998;&#31867;&#22120;&#24448;&#24448;&#38754;&#20020;&#35745;&#31639;&#25361;&#25112;&#12290;&#20381;&#36182;&#40654;&#26364;&#20248;&#21270;&#26041;&#27861;&#30340;&#26041;&#27861;&#36890;&#24120;&#34920;&#29616;&#32531;&#24930;&#65292;&#20854;&#26681;&#28304;&#22312;&#20110;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#25805;&#20316;&#23545;&#35745;&#31639;&#30340;&#22686;&#21152;&#38656;&#27714;&#12290;&#20316;&#20026;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#24212;&#23545;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;hyperDT&#65292;&#36825;&#26159;&#20915;&#31574;&#26641;&#31639;&#27861;&#21521;&#21452;&#26354;&#31354;&#38388;&#30340;&#26032;&#25193;&#23637;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;hyperDT&#28040;&#38500;&#20102;&#23545;&#35745;&#31639;&#23494;&#38598;&#22411;&#40654;&#26364;&#20248;&#21270;&#12289;&#25968;&#20540;&#19981;&#31283;&#23450;&#30340;&#25351;&#25968;&#21644;&#23545;&#25968;&#26144;&#23556;&#25110;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#27604;&#36739;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#31215;&#23558;&#27431;&#20960;&#37324;&#24471;&#20915;&#31574;&#26641;&#31639;&#27861;&#35843;&#25972;&#21040;&#21452;&#26354;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13841v2 Announce Type: replace  Abstract: Hyperbolic geometry is gaining traction in machine learning for its effectiveness at capturing hierarchical structures in real-world data. Hyperbolic spaces, where neighborhoods grow exponentially, offer substantial advantages and consistently deliver state-of-the-art results across diverse applications. However, hyperbolic classifiers often grapple with computational challenges. Methods reliant on Riemannian optimization frequently exhibit sluggishness, stemming from the increased computational demands of operations on Riemannian manifolds. In response to these challenges, we present hyperDT, a novel extension of decision tree algorithms into hyperbolic space. Crucially, hyperDT eliminates the need for computationally intensive Riemannian optimization, numerically unstable exponential and logarithmic maps, or pairwise comparisons between points by leveraging inner products to adapt Euclidean decision tree algorithms to hyperbolic sp
&lt;/p&gt;</description></item><item><title>XRMDN&#26159;&#19968;&#31181;&#25193;&#23637;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#20013;&#39640;&#27874;&#21160;&#29575;&#30340;&#30701;&#26399;&#27010;&#29575;&#39569;&#25163;&#38656;&#27714;&#39044;&#27979;&#65292;&#33021;&#22815;&#28789;&#27963;&#25972;&#21512;&#20869;&#29983;&#21644;&#22806;&#29983;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2310.09847</link><description>&lt;p&gt;
XRMDN: &#19968;&#31181;&#29992;&#20110;&#39640;&#27874;&#21160;&#29575;&#30701;&#26399;&#27010;&#29575;&#39569;&#25163;&#38656;&#27714;&#39044;&#27979;&#30340;&#25193;&#23637;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
XRMDN: An Extended Recurrent Mixture Density Network for Short-Term Probabilistic Rider Demand Forecasting with High Volatility
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09847
&lt;/p&gt;
&lt;p&gt;
XRMDN&#26159;&#19968;&#31181;&#25193;&#23637;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#20013;&#39640;&#27874;&#21160;&#29575;&#30340;&#30701;&#26399;&#27010;&#29575;&#39569;&#25163;&#38656;&#27714;&#39044;&#27979;&#65292;&#33021;&#22815;&#28789;&#27963;&#25972;&#21512;&#20869;&#29983;&#21644;&#22806;&#29983;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#39046;&#22495;&#65292;&#39569;&#25163;&#38656;&#27714;&#39044;&#27979;&#26159;&#36816;&#33829;&#20915;&#31574;&#21644;&#31995;&#32479;&#20248;&#21270;&#30340;&#22522;&#30707;&#12290;&#20256;&#32479;&#30340;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#20135;&#29983;&#28857;&#20272;&#35745;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#38656;&#27714;&#39044;&#27979;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#20986;&#34892;&#38656;&#27714;&#21463;&#20869;&#29983;&#21644;&#22806;&#29983;&#22240;&#32032;&#30340;&#28145;&#21051;&#24433;&#21709;&#65292;&#23548;&#33268;&#39640;&#21160;&#24577;&#27874;&#21160;&#24615;&#12290;&#36825;&#31181;&#27874;&#21160;&#24615;&#26174;&#33879;&#21066;&#24369;&#20102;&#20256;&#32479;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65288;XRMDN&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;XRMDN&#21033;&#29992;&#19968;&#20010;&#22797;&#26434;&#30340;&#26550;&#26500;&#26469;&#36890;&#36807;&#30456;&#20851;&#27169;&#22359;&#22788;&#29702;&#38656;&#27714;&#27531;&#24046;&#21644;&#26041;&#24046;&#65292;&#20174;&#32780;&#28789;&#27963;&#22320;&#25972;&#21512;&#20869;&#29983;&#21644;&#22806;&#29983;&#25968;&#25454;&#12290;&#36825;&#31181;&#26550;&#26500;&#20855;&#26377;&#26435;&#37325;&#20869;&#30340;&#24490;&#29615;&#36830;&#25509;&#65292;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09847v2 Announce Type: replace  Abstract: In the realm of Mobility-on-Demand (MoD) systems, the forecasting of rider demand is a cornerstone for operational decision-making and system optimization. Traditional forecasting methodologies primarily yield point estimates, thereby neglecting the inherent uncertainty within demand projections. Moreover, MoD demand levels are profoundly influenced by both endogenous and exogenous factors, leading to high and dynamic volatility. This volatility significantly undermines the efficacy of conventional time series forecasting methods. In response, we propose an Extended Recurrent Mixture Density Network (XRMDN), a novel deep learning framework engineered to address these challenges. XRMDN leverages a sophisticated architecture to process demand residuals and variance through correlated modules, allowing for the flexible incorporation of endogenous and exogenous data. This architecture, featuring recurrent connections within the weight, m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24809;&#32602;&#20989;&#25968;&#22312;&#35757;&#32451;&#26102;&#24809;&#32602;&#36829;&#21453;&#29615;&#22659;&#32422;&#26463;&#30340;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#20013;&#33021;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#30340;&#24635;&#33021;&#32791;&#26368;&#23567;&#21270;&#12290;</title><link>https://arxiv.org/abs/2308.10664</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#20013;&#33021;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Safe Deep Reinforcement Learning Approach for Energy Efficient Federated Learning in Wireless Communication Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10664
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24809;&#32602;&#20989;&#25968;&#22312;&#35757;&#32451;&#26102;&#24809;&#32602;&#36829;&#21453;&#29615;&#22659;&#32422;&#26463;&#30340;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#20013;&#33021;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#30340;&#24635;&#33021;&#32791;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36171;&#33021;&#30340;&#26080;&#32447;&#32593;&#32476;&#26032;&#26102;&#20195;&#36808;&#36827;&#65292;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#23545;AI&#30340;&#29615;&#22659;&#24433;&#21709;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20851;&#38190;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#25955;&#24335;AI&#25216;&#26415;&#24050;&#32463;&#20986;&#29616;&#12290;&#23613;&#31649;&#30446;&#21069;&#22312;FL&#26041;&#38754;&#24050;&#32463;&#20570;&#20986;&#21162;&#21147;&#65292;&#20294;&#20854;&#29615;&#22659;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;FL&#36807;&#31243;&#30340;&#24635;&#33021;&#32791;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32534;&#25490;&#21442;&#19982;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#65292;&#20197;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;&#24635;&#33021;&#37327;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#30340;&#19968;&#23450;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Soft Actor Critic Deep Reinforcement Learning (DRL)&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#24809;&#32602;&#20989;&#25968;&#65292;&#24809;&#32602;&#36829;&#21453;&#29615;&#22659;&#32422;&#26463;&#30340;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#23433;&#20840;&#30340;RL&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10664v3 Announce Type: replace-cross  Abstract: Progressing towards a new era of Artificial Intelligence (AI) - enabled wireless networks, concerns regarding the environmental impact of AI have been raised both in industry and academia. Federated Learning (FL) has emerged as a key privacy preserving decentralized AI technique. Despite efforts currently being made in FL, its environmental impact is still an open problem. Targeting the minimization of the overall energy consumption of an FL process, we propose the orchestration of computational and communication resources of the involved devices to minimize the total energy required, while guaranteeing a certain performance of the model. To this end, we propose a Soft Actor Critic Deep Reinforcement Learning (DRL) solution, where a penalty function is introduced during training, penalizing the strategies that violate the constraints of the environment, and contributing towards a safe RL process. A device level synchronization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#36951;&#24536;&#35299;&#20915;&#26041;&#26696;&#30340;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#26126;&#30830;&#20102;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2308.07061</link><description>&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65306;&#35299;&#20915;&#26041;&#26696;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning: Solutions and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#36951;&#24536;&#35299;&#20915;&#26041;&#26696;&#30340;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#26126;&#30830;&#20102;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26080;&#24847;&#20013;&#35760;&#20303;&#25935;&#24863;&#12289;&#26410;&#32463;&#25480;&#26435;&#25110;&#24694;&#24847;&#25968;&#25454;&#65292;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#23433;&#20840;&#28431;&#27934;&#21644;&#24615;&#33021;&#38477;&#32423;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26426;&#22120;&#36951;&#24536;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#28040;&#38500;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#28857;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;&#26426;&#22120;&#36951;&#24536;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20998;&#20026;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#24433;&#21709;&#30340;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#22238;&#39038;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35752;&#35770;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#24182;&#23558;&#20854;&#24314;&#31435;&#20026;&#20540;&#24471;&#20449;&#36182;&#21644;&#36866;&#24212;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#33021;&#21147;&#12290;&#26412;&#25991;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20221;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07061v2 Announce Type: replace-cross  Abstract: Machine learning models may inadvertently memorize sensitive, unauthorized, or malicious data, posing risks of privacy breaches, security vulnerabilities, and performance degradation. To address these issues, machine unlearning has emerged as a critical technique to selectively remove specific training data points' influence on trained models. This paper provides a comprehensive taxonomy and analysis of the solutions in machine unlearning. We categorize existing solutions into exact unlearning approaches that remove data influence thoroughly and approximate unlearning approaches that efficiently minimize data influence. By comprehensively reviewing solutions, we identify and discuss their strengths and limitations. Furthermore, we propose future directions to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning models. This paper provides researchers with a roadmap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Masked Transformers&#24555;&#36895;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#39318;&#27425;&#21033;&#29992;Masked training&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#24341;&#20837;&#20102;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#36741;&#21161;&#20219;&#21153;&#65292;&#20197;&#25552;&#21319;&#23545;&#20840;patches&#30340;&#38271;&#31243;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2306.09305</link><description>&lt;p&gt;
&#20351;&#29992;Masked Transformers&#24555;&#36895;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast Training of Diffusion Models with Masked Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.09305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Masked Transformers&#24555;&#36895;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#39318;&#27425;&#21033;&#29992;Masked training&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#24341;&#20837;&#20102;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#36741;&#21161;&#20219;&#21153;&#65292;&#20197;&#25552;&#21319;&#23545;&#20840;patches&#30340;&#38271;&#31243;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Masked Transformers&#26469;&#35757;&#32451;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#12290;&#23613;&#31649;Masked Transformers&#24050;&#32463;&#34987;&#24191;&#27867;&#25506;&#32034;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#23398;&#20064;&#26041;&#38754;&#30340;&#24212;&#29992;&#21364;&#36739;&#23569;&#34987;&#25506;&#35752;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;Masked training&#26174;&#33879;&#38477;&#20302;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#23631;&#34109;&#25193;&#25955;&#36755;&#20837;&#22270;&#20687;&#20013;&#39640;&#27604;&#20363;&#65288;&#20363;&#22914;50%&#65289;&#30340;patches&#12290;&#20026;&#20102;&#36827;&#34892;Masked training&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#20165;&#22312;&#26410;&#23631;&#34109;patches&#19978;&#36816;&#34892;&#30340;transformer&#32534;&#30721;&#22120;&#21644;&#22312;&#20840;&#37096;patches&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;transformer&#35299;&#30721;&#22120;&#12290;&#20026;&#20102;&#25552;&#21319;&#23545;&#20840;patches&#30340;&#38271;&#31243;&#29702;&#35299;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#20219;&#21153;&#65292;&#21363;&#37325;&#26500;&#23631;&#34109;patches&#65292;&#36825;&#26159;&#20026;&#20102;denoising score matching&#30446;&#26631;&#23398;&#20064;&#26410;&#23631;&#34109;patches&#30340;score&#12290;&#25105;&#20204;&#22312;ImageNet-256x256&#21644;ImageNet-512x...&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.09305v2 Announce Type: replace-cross  Abstract: We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HeCo&#30340;&#26032;&#22411;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;HGNNs&#65292;&#37319;&#29992;&#20132;&#21449;&#35270;&#22270;&#23545;&#27604;&#26426;&#21046;&#20197;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2304.12228</link><description>&lt;p&gt;
&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Contrastive Learning Enhanced Heterogeneous Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.12228
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HeCo&#30340;&#26032;&#22411;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;HGNNs&#65292;&#37319;&#29992;&#20132;&#21449;&#35270;&#22270;&#23545;&#27604;&#26426;&#21046;&#20197;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22788;&#29702;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;HGNNs&#36981;&#24490;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#24335;&#65292;&#36825;&#26126;&#26174;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22240;&#20026;&#30495;&#23454;&#24212;&#29992;&#20013;&#26631;&#31614;&#36890;&#24120;&#24456;&#23569;&#12290;&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#26041;&#27861;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#25104;&#20026;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#23398;&#20064;&#33539;&#24335;&#20043;&#19968;&#65292;&#24182;&#22312;&#27809;&#26377;&#26631;&#31614;&#26102;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;HGNNs&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;HGNNs&#30340;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65292;&#31216;&#20026;HeCo&#12290;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#23545;&#27604;&#23398;&#20064;&#21482;&#20851;&#27880;&#23545;&#27604;&#27491;&#36127;&#26679;&#26412;&#65292;HeCo&#37319;&#29992;&#20102;&#20132;&#21449;&#35270;&#22270;&#23545;&#27604;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;HIN&#30340;&#20004;&#20010;&#35270;&#22270;&#65288;&#32593;&#32476;&#26550;&#26500;&#21644;&#20803;&#36335;&#24452;&#35270;&#22270;&#65289;&#20197;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#65292;&#20174;&#32780;&#25429;&#25417;&#23616;&#37096;&#21644;&#39640;&#38454;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.12228v2 Announce Type: replace  Abstract: Heterogeneous graph neural networks (HGNNs) as an emerging technique have shown superior capacity of dealing with heterogeneous information network (HIN). However, most HGNNs follow a semi-supervised learning manner, which notably limits their wide use in reality since labels are usually scarce in real applications. Recently, contrastive learning, a self-supervised method, becomes one of the most exciting learning paradigms and shows great potential when there are no labels. In this paper, we study the problem of self-supervised HGNNs and propose a novel co-contrastive learning mechanism for HGNNs, named HeCo. Different from traditional contrastive learning which only focuses on contrasting positive and negative samples, HeCo employs cross-view contrastive mechanism. Specifically, two views of a HIN (network schema and meta-path views) are proposed to learn node embeddings, so as to capture both of local and high-order structures sim
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#21147;/&#25197;&#30697;&#20256;&#24863;&#22120;&#65292;&#36890;&#36807;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#32467;&#26500;&#20934;&#30830;&#20272;&#35745;&#22806;&#37096;&#21147;&#30697;&#12290;</title><link>https://arxiv.org/abs/2301.13413</link><description>&lt;p&gt;
&#26080;&#38656;&#21147;/&#25197;&#30697;&#20256;&#24863;&#22120;&#30340;&#31934;&#32454;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Fine Robotic Manipulation without Force/Torque Sensor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13413
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#21147;/&#25197;&#30697;&#20256;&#24863;&#22120;&#65292;&#36890;&#36807;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#32467;&#26500;&#20934;&#30830;&#20272;&#35745;&#22806;&#37096;&#21147;&#30697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21147;&#20256;&#24863;&#21644;&#21147;&#25511;&#23545;&#20110;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;6&#36724;&#21147;/&#25197;&#30697;&#65288;F/T&#65289;&#20256;&#24863;&#22120;&#34987;&#23433;&#35013;&#22312;&#26426;&#22120;&#20154;&#30340;&#25163;&#33109;&#21644;&#26411;&#31471;&#25191;&#34892;&#22120;&#20043;&#38388;&#65292;&#20197;&#27979;&#37327;&#29615;&#22659;&#23545;&#26426;&#22120;&#20154;&#26045;&#21152;&#21147;&#21644;&#25197;&#30697;&#65288;&#22806;&#37096;&#21147;&#30697;&#65289;&#12290;&#34429;&#28982;&#20856;&#22411;&#30340;6&#36724;F/T&#20256;&#24863;&#22120;&#21487;&#20197;&#25552;&#20379;&#39640;&#24230;&#20934;&#30830;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#20294;&#23427;&#26114;&#36149;&#19988;&#23481;&#26131;&#28418;&#31227;&#21644;&#21463;&#21040;&#22806;&#37096;&#20914;&#20987;&#12290;&#29616;&#26377;&#26041;&#27861;&#26088;&#22312;&#20165;&#20351;&#29992;&#26426;&#22120;&#20154;&#20869;&#37096;&#20449;&#21495;&#26469;&#20272;&#35745;&#22806;&#37096;&#21147;&#30697;&#30340;&#33539;&#22260;&#26377;&#38480;&#65306;&#20363;&#22914;&#65292;&#21147;&#30697;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#20027;&#35201;&#22312;&#33258;&#30001;&#31354;&#38388;&#36816;&#21160;&#21644;&#31616;&#21333;&#25509;&#35302;&#24773;&#20917;&#19979;&#24471;&#21040;&#39564;&#35777;&#65292;&#32780;&#19981;&#26159;&#20687;&#32452;&#35013;&#36825;&#26679;&#38656;&#35201;&#39640;&#31934;&#24230;&#21147;&#25511;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#35748;&#20026;&#36890;&#36807;&#29305;&#21035;&#20851;&#27880;&#35757;&#32451;&#25968;&#25454;&#32467;&#26500;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#20272;&#35745;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#22806;&#37096;&#21147;&#30697;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13413v2 Announce Type: replace-cross  Abstract: Force Sensing and Force Control are essential to many industrial applications. Typically, a 6-axis Force/Torque (F/T) sensor is mounted between the robot's wrist and the end-effector in order to measure the forces and torques exerted by the environment onto the robot (the external wrench). Although a typical 6-axis F/T sensor can provide highly accurate measurements, it is expensive and vulnerable to drift and external impacts. Existing methods aiming at estimating the external wrench using only the robot's internal signals are limited in scope: for example, wrench estimation accuracy was mostly validated in free-space motions and simple contacts as opposed to tasks like assembly that require high-precision force control. Here we present a Neural Network based method and argue that by devoting particular attention to the training data structure, it is possible to accurately estimate the external wrench in a wide range of scenar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26174;&#24335;&#34892;&#20026;&#23494;&#24230;&#30340;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;flow-GAN&#27169;&#22411;&#26469;&#20934;&#30830;&#35782;&#21035;&#23433;&#20840;&#21306;&#22495;&#65292;&#23454;&#29616;&#20102;&#26356;&#23569;&#20445;&#23432;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2301.12130</link><description>&lt;p&gt;
&#20855;&#26377;&#26174;&#24335;&#34892;&#20026;&#23494;&#24230;&#30340;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constrained Policy Optimization with Explicit Behavior Density for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.12130
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26174;&#24335;&#34892;&#20026;&#23494;&#24230;&#30340;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;flow-GAN&#27169;&#22411;&#26469;&#20934;&#30830;&#35782;&#21035;&#23433;&#20840;&#21306;&#22495;&#65292;&#23454;&#29616;&#20102;&#26356;&#23569;&#20445;&#23432;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26080;&#27861;&#19982;&#29615;&#22659;&#20132;&#20114;&#65292;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#38754;&#20020;&#30528;&#20272;&#35745;&#20998;&#24067;&#20043;&#22806;&#28857;&#65288;OOD&#65289;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#25511;&#21046;&#31574;&#30053;&#20197;&#25490;&#38500;OOD&#21160;&#20316;&#65292;&#35201;&#20040;&#20351;$Q$&#20989;&#25968;&#21464;&#24471;&#24754;&#35266;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#36807;&#20110;&#20445;&#23432;&#25110;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;OOD&#21306;&#22495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;flow-GAN&#27169;&#22411;&#26174;&#24335;&#20272;&#35745;&#34892;&#20026;&#31574;&#30053;&#23494;&#24230;&#30340;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#65288;CPED&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#20272;&#35745;&#26174;&#24335;&#23494;&#24230;&#65292;CPED&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#23433;&#20840;&#21306;&#22495;&#24182;&#22312;&#35813;&#21306;&#22495;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#23569;&#20445;&#23432;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20026;flow-GAN&#20272;&#35745;&#22120;&#21644;CPED&#30340;&#24615;&#33021;&#20445;&#35777;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#34920;&#26126;CPED&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#30340;$Q$&#20989;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.12130v2 Announce Type: replace  Abstract: Due to the inability to interact with the environment, offline reinforcement learning (RL) methods face the challenge of estimating the Out-of-Distribution (OOD) points. Existing methods for addressing this issue either control policy to exclude the OOD action or make the $Q$ function pessimistic. However, these methods can be overly conservative or fail to identify OOD areas accurately. To overcome this problem, we propose a Constrained Policy optimization with Explicit Behavior density (CPED) method that utilizes a flow-GAN model to explicitly estimate the density of behavior policy. By estimating the explicit density, CPED can accurately identify the safe region and enable optimization within the region, resulting in less conservative learning policies. We further provide theoretical results for both the flow-GAN estimator and performance guarantee for CPED by showing that CPED can find the optimal $Q$-function value. Empirically,
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#30693;&#35782;&#33976;&#39311;&#22312;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#37096;&#32626;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2301.05849</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#22312;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation in Federated Edge Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.05849
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#30693;&#35782;&#33976;&#39311;&#22312;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#37096;&#32626;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#26234;&#33021;&#26381;&#21153;&#21644;&#31227;&#21160;&#35774;&#22791;&#20197;&#21450;&#29289;&#32852;&#32593;&#35774;&#22791;&#38544;&#31169;&#20445;&#25252;&#35201;&#27714;&#30340;&#22686;&#21152;&#65292;&#20419;&#20351;&#20102;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;FEL&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#20013;&#35774;&#22791;&#22312;&#19981;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#35774;&#22791;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#30001;&#20110;&#35774;&#22791;&#30828;&#20214;&#12289;&#29992;&#25143;&#34892;&#20026;&#21644;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#30340;&#22810;&#26679;&#24615;&#65292;FEL&#30340;&#31639;&#27861;&#35774;&#35745;&#38754;&#20020;&#30528;&#19982;&#36164;&#28304;&#12289;&#20010;&#24615;&#21270;&#21644;&#32593;&#32476;&#29615;&#22659;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#24050;&#34987;&#21033;&#29992;&#20316;&#20026;&#35299;&#20915;FEL&#20013;&#19978;&#36848;&#25361;&#25112;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;KD&#24212;&#29992;&#20110;FEL&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#35752;&#35770;&#20102;&#29616;&#26377;&#22522;&#20110;KD&#30340;FEL&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.05849v3 Announce Type: replace  Abstract: The increasing demand for intelligent services and privacy protection of mobile and Internet of Things (IoT) devices motivates the wide application of Federated Edge Learning (FEL), in which devices collaboratively train on-device Machine Learning (ML) models without sharing their private data. Limited by device hardware, diverse user behaviors and network infrastructure, the algorithm design of FEL faces challenges related to resources, personalization and network environments. Fortunately, Knowledge Distillation (KD) has been leveraged as an important technique to tackle the above challenges in FEL. In this paper, we investigate the works that KD applies to FEL, discuss the limitations and open problems of existing KD-based FEL approaches, and provide guidance for their real deployment.
&lt;/p&gt;</description></item><item><title>&#22312;&#21160;&#24577;&#22270;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#26426;&#21046;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#33410;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#21516;&#26102;&#20851;&#31995;&#21644;&#26102;&#28382;&#20132;&#20114;&#20851;&#31995;&#26469;&#26500;&#24314;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#26377;&#25928;&#22320;&#25551;&#36848;&#20102;&#29305;&#24449;&#29983;&#25104;&#36807;&#31243;</title><link>https://arxiv.org/abs/2211.17029</link><description>&lt;p&gt;
&#20174;&#21160;&#24577;&#22270;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Directed Acyclic Graph Structure Learning from Dynamic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.17029
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#22270;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#26426;&#21046;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#33410;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#21516;&#26102;&#20851;&#31995;&#21644;&#26102;&#28382;&#20132;&#20114;&#20851;&#31995;&#26469;&#26500;&#24314;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#26377;&#25928;&#22320;&#25551;&#36848;&#20102;&#29305;&#24449;&#29983;&#25104;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#29305;&#24449;&#65288;&#21464;&#37327;&#65289;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#32467;&#26500;&#22312;&#25581;&#31034;&#28508;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#25552;&#20379;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#22240;&#26524;&#27934;&#35265;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#20851;&#20110;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#32467;&#26500;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#20294;&#21160;&#24577;&#22270;&#19978;&#30340;&#32467;&#26500;&#23398;&#20064;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#22240;&#27492;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26080;&#22788;&#19981;&#22312;&#30340;&#21160;&#24577;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#26426;&#21046;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#21160;&#24577;&#22270;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21516;&#26102;&#20272;&#35745;&#33410;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#21516;&#26102;&#20851;&#31995;&#21644;&#26102;&#28382;&#20132;&#20114;&#20851;&#31995;&#12290;&#36825;&#20004;&#31181;&#20851;&#31995;&#24418;&#25104;&#19968;&#20010;DAG&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#31616;&#27905;&#25551;&#36848;&#29305;&#24449;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#23398;&#20064;&#36825;&#26679;&#30340;DAG&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#36830;&#32493;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#24471;&#20998;&#20989;&#25968;&#26469;&#34913;&#37327;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.17029v2 Announce Type: replace-cross  Abstract: Estimating the structure of directed acyclic graphs (DAGs) of features (variables) plays a vital role in revealing the latent data generation process and providing causal insights in various applications. Although there have been many studies on structure learning with various types of data, the structure learning on the dynamic graph has not been explored yet, and thus we study the learning problem of node feature generation mechanism on such ubiquitous dynamic graph data. In a dynamic graph, we propose to simultaneously estimate contemporaneous relationships and time-lagged interaction relationships between the node features. These two kinds of relationships form a DAG, which could effectively characterize the feature generation process in a concise way. To learn such a DAG, we cast the learning problem as a continuous score-based optimization problem, which consists of a differentiable score function to measure the validity 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22686;&#37327;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;iFNO&#65289;&#65292;&#36890;&#36807;&#36880;&#27493;&#22686;&#21152;&#39057;&#29575;&#27169;&#24335;&#30340;&#25968;&#37327;&#26469;&#35299;&#20915;&#35757;&#32451;FNO&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#65306;&#35745;&#31639;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#28040;&#32791;&#22823;&#65292;&#36873;&#25321;&#35889;&#23618;&#20013;&#30340;&#30456;&#20851;&#39057;&#29575;&#38598;&#21512;&#22256;&#38590;</title><link>https://arxiv.org/abs/2211.15188</link><description>&lt;p&gt;
&#22686;&#37327;&#31354;&#38388;&#21644;&#35889;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;PDE&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Incremental Spatial and Spectral Learning of Neural Operators for Solving Large-Scale PDEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.15188
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22686;&#37327;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;iFNO&#65289;&#65292;&#36890;&#36807;&#36880;&#27493;&#22686;&#21152;&#39057;&#29575;&#27169;&#24335;&#30340;&#25968;&#37327;&#26469;&#35299;&#20915;&#35757;&#32451;FNO&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#65306;&#35745;&#31639;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#28040;&#32791;&#22823;&#65292;&#36873;&#25321;&#35889;&#23618;&#20013;&#30340;&#30456;&#20851;&#39057;&#29575;&#38598;&#21512;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#25361;&#25112;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#38382;&#39064;&#30340;&#21407;&#21017;&#26041;&#27861;&#65292;&#20363;&#22914;&#28237;&#27969;&#27969;&#21160;&#12290;FNO&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#35889;&#23618;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#22495;&#20013;&#30340;&#25910;&#25947;&#34920;&#31034;&#65292;&#24182;&#23398;&#20064;&#22266;&#23450;&#19968;&#32452;&#39057;&#29575;&#19978;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;FNO&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#35268;&#27169;&#12289;&#39640;&#20998;&#36776;&#29575;&#30340;&#24212;&#29992;&#20013;&#65306;&#65288;i&#65289;&#35745;&#31639;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#35745;&#31639;&#37327;&#24040;&#22823;&#65292;&#20294;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#35299;&#20915;&#35768;&#22810;PDE&#38382;&#39064;&#65288;&#22914;&#27969;&#20307;&#27969;&#21160;&#65289;&#38656;&#35201;&#32454;&#33410;&#65292;&#65288;ii&#65289;&#22312;&#35889;&#23618;&#20013;&#36873;&#25321;&#30456;&#20851;&#39057;&#29575;&#38598;&#21512;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22826;&#22810;&#27169;&#24335;&#20250;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#65292;&#32780;&#22826;&#23569;&#20250;&#23548;&#33268;&#19981;&#36275;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22686;&#37327;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;iFNO&#65289;&#65292;&#23427;&#36880;&#28176;&#22686;&#21152;&#29992;&#20110;&#35745;&#31639;&#30340;&#39057;&#29575;&#27169;&#24335;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.15188v4 Announce Type: replace  Abstract: Fourier Neural Operators (FNO) offer a principled approach to solving challenging partial differential equations (PDE) such as turbulent flows. At the core of FNO is a spectral layer that leverages a discretization-convergent representation in the Fourier domain, and learns weights over a fixed set of frequencies. However, training FNO presents two significant challenges, particularly in large-scale, high-resolution applications: (i) Computing Fourier transform on high-resolution inputs is computationally intensive but necessary since fine-scale details are needed for solving many PDEs, such as fluid flows, (ii) selecting the relevant set of frequencies in the spectral layers is challenging, and too many modes can lead to overfitting, while too few can lead to underfitting. To address these issues, we introduce the Incremental Fourier Neural Operator (iFNO), which progressively increases both the number of frequency modes used by the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;GIT&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#33021;&#22815;&#36890;&#36807;&#20449;&#21495;&#26799;&#24230;&#20272;&#35745;&#22120;&#38477;&#20302;&#24178;&#39044;&#27425;&#25968;&#65292;&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2211.13715</link><description>&lt;p&gt;
&#30456;&#20449;&#24744;&#30340; $\nabla$: &#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#29992;&#20110;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.13715
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;GIT&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#33021;&#22815;&#36890;&#36807;&#20449;&#21495;&#26799;&#24230;&#20272;&#35745;&#22120;&#38477;&#20302;&#24178;&#39044;&#27425;&#25968;&#65292;&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26159;&#31185;&#23398;&#20013;&#19968;&#39033;&#20855;&#26377;&#22522;&#30784;&#37325;&#35201;&#24615;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#35266;&#27979;&#25968;&#25454;&#36890;&#24120;&#19981;&#36275;&#20197;&#21807;&#19968;&#30830;&#23450;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#34429;&#28982;&#36827;&#34892;&#24178;&#39044;&#65288;&#21363;&#23454;&#39564;&#65289;&#21487;&#20197;&#25913;&#21892;&#21487;&#35782;&#21035;&#24615;&#65292;&#20294;&#36825;&#20123;&#26679;&#26412;&#36890;&#24120;&#38590;&#20197;&#33719;&#24471;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#22240;&#26524;&#21457;&#29616;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20272;&#35745;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#24178;&#39044;&#30446;&#26631;&#26469;&#26368;&#23567;&#21270;&#24178;&#39044;&#27425;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;&#31616;&#31216;&#20026;GIT&#65292;&#23427;&#8216;&#30456;&#20449;&#8217;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#20197;&#25552;&#20379;&#24178;&#39044;&#37319;&#38598;&#20989;&#25968;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;GIT&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#24403;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.13715v4 Announce Type: replace-cross  Abstract: Inferring causal structure from data is a challenging task of fundamental importance in science. Observational data are often insufficient to identify a system's causal structure uniquely. While conducting interventions (i.e., experiments) can improve the identifiability, such samples are usually challenging and expensive to obtain. Hence, experimental design approaches for causal discovery aim to minimize the number of interventions by estimating the most informative intervention target. In this work, we propose a novel Gradient-based Intervention Targeting method, abbreviated GIT, that 'trusts' the gradient estimator of a gradient-based causal discovery framework to provide signals for the intervention acquisition function. We provide extensive experiments in simulated and real-world datasets and demonstrate that GIT performs on par with competitive baselines, surpassing them in the low-data regime.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#38408;&#20540;&#26469;&#32771;&#34385;&#25968;&#25454;&#38598;&#20013;&#25935;&#24863;&#23646;&#24615;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2211.10209</link><description>&lt;p&gt;
&#21033;&#29992;&#31639;&#27861;&#20844;&#24179;&#24615;&#20943;&#36731;&#40657;&#30418;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Leveraging Algorithmic Fairness to Mitigate Blackbox Attribute Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.10209
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#38408;&#20540;&#26469;&#32771;&#34385;&#25968;&#25454;&#38598;&#20013;&#25935;&#24863;&#23646;&#24615;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24050;&#32463;&#34987;&#37096;&#32626;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#21009;&#20107;&#21496;&#27861;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ML&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#19968;&#20123;&#32972;&#26223;&#30693;&#35782;&#35757;&#32451;ML&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#21306;&#20998;&#30340;&#27169;&#22411;&#39044;&#27979;&#26469;&#25512;&#26029;&#25935;&#24863;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#23545;&#25915;&#20987;&#32773;&#30340;&#32972;&#26223;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#25935;&#24863;&#23646;&#24615;&#30340;&#36793;&#38469;&#20998;&#24067;&#65289;&#26377;&#24456;&#24378;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#19981;&#20250;&#24102;&#26469;&#27604;&#32479;&#35745;&#25512;&#26029;&#26356;&#22810;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#25915;&#20987;&#24182;&#26410;&#32771;&#34385;&#26469;&#33258;&#30495;&#23454;&#24212;&#29992;&#31243;&#24207;&#30340;&#25968;&#25454;&#38598;&#20013;&#25935;&#24863;&#23646;&#24615;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65288;&#20363;&#22914;&#65292;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#27492;&#19981;&#24179;&#34913;&#30340;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#20351;&#29992;&#25915;&#20987;&#27169;&#22411;&#39044;&#27979;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.10209v2 Announce Type: replace  Abstract: Machine learning (ML) models have been deployed for high-stakes applications, e.g., healthcare and criminal justice. Prior work has shown that ML models are vulnerable to attribute inference attacks where an adversary, with some background knowledge, trains an ML attack model to infer sensitive attributes by exploiting distinguishable model predictions. However, some prior attribute inference attacks have strong assumptions about adversary's background knowledge (e.g., marginal distribution of sensitive attribute) and pose no more privacy risk than statistical inference. Moreover, none of the prior attacks account for class imbalance of sensitive attribute in datasets coming from real-world applications (e.g., Race and Sex). In this paper, we propose an practical and effective attribute inference attack that accounts for this imbalance using an adaptive threshold over the attack model's predictions. We exhaustively evaluate our propo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26680;&#35268;&#33539;&#21270;&#21367;&#31215;&#32593;&#32476;&#65288;KNConvNets&#65289;&#65292;&#36890;&#36807;&#26367;&#20195;BatchNorm&#23618;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#20013;&#23454;&#29616;&#27604;BatchNorm&#26356;&#39640;&#25110;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#22312;&#38750;&#31169;&#23494;&#21644;&#24046;&#20998;&#31169;&#23494;&#35757;&#32451;&#20013;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#23618;&#21644;&#32452;&#35268;&#33539;&#21270;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;</title><link>https://arxiv.org/abs/2205.10089</link><description>&lt;p&gt;
&#26680;&#35268;&#33539;&#21270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Kernel Normalized Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.10089
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26680;&#35268;&#33539;&#21270;&#21367;&#31215;&#32593;&#32476;&#65288;KNConvNets&#65289;&#65292;&#36890;&#36807;&#26367;&#20195;BatchNorm&#23618;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#20013;&#23454;&#29616;&#27604;BatchNorm&#26356;&#39640;&#25110;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#22312;&#38750;&#31169;&#23494;&#21644;&#24046;&#20998;&#31169;&#23494;&#35757;&#32451;&#20013;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#23618;&#21644;&#32452;&#35268;&#33539;&#21270;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#36890;&#24120;&#20381;&#36182;&#20110;&#25209;&#37327;&#35268;&#33539;&#21270;&#65288;BatchNorm&#65289;&#26469;&#26377;&#25928;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;BatchNorm &#22312;&#23567;&#25209;&#37327;&#22823;&#23567;&#26102;&#24615;&#33021;&#36739;&#24046;&#65292;&#24182;&#19988;&#19981;&#36866;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26680;&#35268;&#33539;&#21270;&#65288;KernelNorm&#65289;&#21644;&#26680;&#35268;&#33539;&#21270;&#21367;&#31215;&#23618;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#20027;&#35201;&#26500;&#24314;&#27169;&#22359;&#34701;&#20837;&#26680;&#35268;&#33539;&#21270;&#21367;&#31215;&#32593;&#32476;&#65288;KNConvNets&#65289;&#20013;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#30456;&#24212;&#20110;&#26368;&#20808;&#36827;&#30340;ResNets&#30340;KNConvNets&#65292;&#21516;&#26102;&#25918;&#24323;&#20102;BatchNorm&#23618;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;KNConvNets&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#20013;&#30456;&#27604;&#20110;BatchNorm&#23545;&#24212;&#30340;&#27169;&#22411;&#33021;&#23454;&#29616;&#26356;&#39640;&#25110;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#22312;&#38750;&#31169;&#23494;&#21644;&#24046;&#20998;&#31169;&#23494;&#35757;&#32451;&#20013;&#65292;&#23427;&#20204;&#36824;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#23618;&#21644;&#32452;&#35268;&#33539;&#21270;&#30340;&#25209;&#37327;&#29420;&#31435;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.10089v4 Announce Type: replace  Abstract: Existing convolutional neural network architectures frequently rely upon batch normalization (BatchNorm) to effectively train the model. BatchNorm, however, performs poorly with small batch sizes, and is inapplicable to differential privacy. To address these limitations, we propose the kernel normalization (KernelNorm) and kernel normalized convolutional layers, and incorporate them into kernel normalized convolutional networks (KNConvNets) as the main building blocks. We implement KNConvNets corresponding to the state-of-the-art ResNets while forgoing the BatchNorm layers. Through extensive experiments, we illustrate that KNConvNets achieve higher or competitive performance compared to the BatchNorm counterparts in image classification and semantic segmentation. They also significantly outperform their batch-independent competitors including those based on layer and group normalization in non-private and differentially private train
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#39640;&#32423;&#22270;&#34920;&#31034;&#24182;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#23454;&#29616;&#31283;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2111.10657</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#25512;&#24191;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalizing Graph Neural Networks on Out-Of-Distribution Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.10657
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#39640;&#32423;&#22270;&#34920;&#31034;&#24182;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#23454;&#29616;&#31283;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#27809;&#26377;&#32771;&#34385;&#35757;&#32451;&#21644;&#27979;&#35797;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#65292;&#23548;&#33268;GNNs&#22312;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#35774;&#32622;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#19979;&#38477;&#12290;&#36825;&#31181;&#36864;&#21270;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#22823;&#22810;&#25968;GNNs&#26159;&#22522;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#24320;&#21457;&#30340;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#30340;&#32454;&#24494;&#32479;&#35745;&#30456;&#20851;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#21363;&#20351;&#36825;&#26159;&#19968;&#31181;&#20266;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20266;&#30456;&#20851;&#24615;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#23548;&#33268;GNNs&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#28040;&#38500;&#20266;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#23545;&#20110;&#31283;&#23450;&#30340;GNNs&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#39318;&#20808;&#20174;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#32423;&#34920;&#31034;&#65292;&#28982;&#21518;&#20511;&#21161;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#26469;&#24110;&#21161;&#27169;&#22411;&#33719;&#24471;&#31283;&#23450;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.10657v3 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are proposed without considering the agnostic distribution shifts between training and testing graphs, inducing the degeneration of the generalization ability of GNNs on Out-Of-Distribution (OOD) settings. The fundamental reason for such degeneration is that most GNNs are developed based on the I.I.D hypothesis. In such a setting, GNNs tend to exploit subtle statistical correlations existing in the training set for predictions, even though it is a spurious correlation. However, such spurious correlations may change in testing environments, leading to the failure of GNNs. Therefore, eliminating the impact of spurious correlations is crucial for stable GNNs. To this end, we propose a general causal representation framework, called StableGNN. The main idea is to extract high-level representations from graph data first and resort to the distinguishing ability of causal inference to help the model get ri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27963;&#21160;&#30340;&#36845;&#20195;&#21098;&#26525;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#37325;&#35201;&#24615;&#35780;&#20998;&#25351;&#26631;&#26469;&#20572;&#29992;&#26080;&#20851;&#32039;&#35201;&#30340;&#36830;&#25509;&#65292;&#35299;&#20915;&#20102;DNNs&#20013;&#30340;&#21442;&#25968;&#36807;&#22810;&#38382;&#39064;&#24182;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21442;&#25968;&#21387;&#32553;&#65292;&#22312;MNIST&#12289;CIFAR-10/100&#21644;Tiny-ImageNet&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2109.10795</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#27963;&#21160;&#30340;&#21098;&#26525;&#31639;&#27861;&#65306;&#31070;&#32463;&#32593;&#32476;Relief
&lt;/p&gt;
&lt;p&gt;
Neural network relief: a pruning algorithm based on neural activity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.10795
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27963;&#21160;&#30340;&#36845;&#20195;&#21098;&#26525;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#37325;&#35201;&#24615;&#35780;&#20998;&#25351;&#26631;&#26469;&#20572;&#29992;&#26080;&#20851;&#32039;&#35201;&#30340;&#36830;&#25509;&#65292;&#35299;&#20915;&#20102;DNNs&#20013;&#30340;&#21442;&#25968;&#36807;&#22810;&#38382;&#39064;&#24182;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21442;&#25968;&#21387;&#32553;&#65292;&#22312;MNIST&#12289;CIFAR-10/100&#21644;Tiny-ImageNet&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23384;&#22312;&#21442;&#25968;&#36807;&#22810;&#65292;&#22312;&#27599;&#20010;&#20219;&#21153;&#30340;&#25512;&#26029;&#36807;&#31243;&#20013;&#20351;&#29992;&#22823;&#37096;&#20998;&#31070;&#32463;&#20803;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#20154;&#33041;&#20026;&#19981;&#21516;&#20219;&#21153;&#21457;&#23637;&#20102;&#19987;&#38376;&#30340;&#21306;&#22495;&#65292;&#24182;&#20165;&#20351;&#29992;&#20854;&#31070;&#32463;&#36830;&#25509;&#30340;&#19968;&#23567;&#37096;&#20998;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#21098;&#26525;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#37325;&#35201;&#24615;&#35780;&#20998;&#25351;&#26631;&#65292;&#23558;&#26080;&#20851;&#32039;&#35201;&#30340;&#36830;&#25509;&#20572;&#29992;&#65292;&#35299;&#20915;&#20102;DNNs&#20013;&#30340;&#21442;&#25968;&#36807;&#22810;&#38382;&#39064;&#65292;&#24182;&#35843;&#33410;&#20102;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#27169;&#24335;&#12290;&#26088;&#22312;&#25214;&#21040;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#26368;&#23567;&#36830;&#25509;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#65292;&#21363;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;MNIST&#19978;&#30340;LeNet&#26550;&#26500;&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10/100&#21644;Tiny-ImageNet&#19978;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#21442;&#25968;&#21387;&#32553;&#65292;&#20063;&#22312;&#32771;&#34385;&#30340;&#20004;&#31181;&#19981;&#21516;&#20248;&#21270;&#22120;--Adam&#21644;SGD--&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.10795v3 Announce Type: replace  Abstract: Current deep neural networks (DNNs) are overparameterized and use most of their neuronal connections during inference for each task. The human brain, however, developed specialized regions for different tasks and performs inference with a small fraction of its neuronal connections. We propose an iterative pruning strategy introducing a simple importance-score metric that deactivates unimportant connections, tackling overparameterization in DNNs and modulating the firing patterns. The aim is to find the smallest number of connections that is still capable of solving a given task with comparable accuracy, i.e. a simpler subnetwork. We achieve comparable performance for LeNet architectures on MNIST, and significantly higher parameter compression than state-of-the-art algorithms for VGG and ResNet architectures on CIFAR-10/100 and Tiny-ImageNet. Our approach also performs well for the two different optimizers considered -- Adam and SGD. 
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#25913;&#36827;&#31639;&#27861;&#21644;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#36824;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#23398;&#20064;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32852;&#37030; X &#23398;&#20064;&#20013;&#65292;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#31561;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2102.12920</link><description>&lt;p&gt;
&#26032;&#20852;&#36235;&#21183;&#65306;&#20174;&#27169;&#22411;&#34701;&#21512;&#21040;&#32852;&#37030; X &#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emerging Trends in Federated Learning: From Model Fusion to Federated X Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.12920
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#25913;&#36827;&#31639;&#27861;&#21644;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#36824;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#23398;&#20064;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32852;&#37030; X &#23398;&#20064;&#20013;&#65292;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#31561;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#22810;&#26041;&#35745;&#31639;&#21644;&#27169;&#22411;&#32858;&#21512;&#26469;&#35299;&#32806;&#25968;&#25454;&#25910;&#38598;&#19982;&#27169;&#22411;&#35757;&#32451;&#12290;&#20316;&#20026;&#19968;&#31181;&#28789;&#27963;&#30340;&#23398;&#20064;&#35774;&#32622;&#65292;&#32852;&#37030;&#23398;&#20064;&#26377;&#28508;&#21147;&#19982;&#20854;&#20182;&#23398;&#20064;&#26694;&#26550;&#25972;&#21512;&#12290;&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#19982;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#30340;&#32467;&#21512;&#36827;&#34892;&#20102;&#37325;&#28857;&#35843;&#26597;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#23398;&#20064;&#31639;&#27861;&#20197;&#25913;&#36827;&#21407;&#22987;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#20363;&#22914;&#33258;&#36866;&#24212;&#32858;&#21512;&#12289;&#27491;&#21017;&#21270;&#12289;&#32858;&#31867;&#26041;&#27861;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#26681;&#25454;&#26032;&#20852;&#36235;&#21183;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#19982;&#20854;&#20182;&#23398;&#20064;&#33539;&#24335;&#30340;&#20132;&#38598;&#65292;&#31216;&#20026;&#32852;&#37030; X &#23398;&#20064;&#65292;&#20854;&#20013; X &#21253;&#25324;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#39033;&#35843;&#26597;&#22238;&#39038;&#20102;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.12920v4 Announce Type: replace  Abstract: Federated learning is a new learning paradigm that decouples data collection and model training via multi-party computation and model aggregation. As a flexible learning setting, federated learning has the potential to integrate with other learning frameworks. We conduct a focused survey of federated learning in conjunction with other learning algorithms. Specifically, we explore various learning algorithms to improve the vanilla federated averaging algorithm and review model fusion methods such as adaptive aggregation, regularization, clustered methods, and Bayesian methods. Following the emerging trends, we also discuss federated learning in the intersection with other learning paradigms, termed federated X learning, where X includes multitask learning, meta-learning, transfer learning, unsupervised learning, and reinforcement learning. This survey reviews the state of the art, challenges, and future directions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#8220;&#35299;&#37322;&#20132;&#20114;&#23398;&#20064;&#8221;(XIL)&#23398;&#20064;&#35774;&#32622;&#65292;&#30740;&#31350;&#32773;&#21487;&#20197;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20132;&#20114;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#20854;&#21033;&#29992;&#28151;&#28102;&#22240;&#32032;&#32780;&#23548;&#33268;&#30340;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#22686;&#24378;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2001.05371</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#35299;&#37322;&#36827;&#34892;&#20132;&#20114;&#65292;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20986;&#20110;&#27491;&#30830;&#30340;&#31185;&#23398;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Making deep neural networks right for the right scientific reasons by interacting with their explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2001.05371
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#8220;&#35299;&#37322;&#20132;&#20114;&#23398;&#20064;&#8221;(XIL)&#23398;&#20064;&#35774;&#32622;&#65292;&#30740;&#31350;&#32773;&#21487;&#20197;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20132;&#20114;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#20854;&#21033;&#29992;&#28151;&#28102;&#22240;&#32032;&#32780;&#23548;&#33268;&#30340;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#22686;&#24378;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#8220;&#32874;&#26126;&#30340;&#27721;&#26031;&#8221;&#24335;&#30340;&#34892;&#20026;&#65292;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;&#35299;&#37322;&#20132;&#20114;&#23398;&#20064;&#8221;(XIL)&#30340;&#26032;&#22411;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#22312;&#26893;&#29289;&#34920;&#22411;&#30740;&#31350;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#22909;&#22788;&#12290;XIL&#23558;&#31185;&#23398;&#23478;&#24341;&#20837;&#35757;&#32451;&#24490;&#29615;&#65292;&#20351;&#22905;&#36890;&#36807;&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#21453;&#39304;&#36827;&#34892;&#20132;&#20114;&#24335;&#20462;&#35746;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XIL&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#8220;&#32874;&#26126;&#27721;&#26031;&#8221;&#26102;&#21051;&#65292;&#24182;&#40723;&#21169;&#65288;&#25110;&#40723;&#21169;&#65292;&#22914;&#26524;&#21512;&#36866;&#30340;&#35805;&#65289;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2001.05371v4 Announce Type: replace-cross  Abstract: Deep neural networks have shown excellent performances in many real-world applications. Unfortunately, they may show "Clever Hans"-like behavior -- making use of confounding factors within datasets -- to achieve high performance. In this work, we introduce the novel learning setting of "explanatory interactive learning" (XIL) and illustrate its benefits on a plant phenotyping research task. XIL adds the scientist into the training loop such that she interactively revises the original model via providing feedback on its explanations. Our experimental results demonstrate that XIL can help avoiding Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust into the underlying model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25991;&#26412;&#21435;&#22122;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#32416;&#27491;&#22122;&#22768;&#25991;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;</title><link>https://arxiv.org/abs/1910.14080</link><description>&lt;p&gt;
&#22522;&#20110;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25991;&#26412;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Contextual Text Denoising with Masked Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1910.14080
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25991;&#26412;&#21435;&#22122;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#32416;&#27491;&#22122;&#22768;&#25991;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20511;&#21161;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#22024;&#26434;&#25991;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29616;&#25104;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#19978;&#19979;&#25991;&#25991;&#26412;&#21435;&#22122;&#31639;&#27861;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;NLP&#31995;&#32479;&#20013;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#37197;&#23545;&#30340;&#28165;&#27927;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#39069;&#22806;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#22122;&#22768;&#21644;&#33258;&#28982;&#22122;&#22768;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#25152;&#25552;&#31639;&#27861;&#21487;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#32416;&#27491;&#22122;&#22768;&#25991;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#25552;&#39640;&#22024;&#26434;&#36755;&#20837;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1910.14080v2 Announce Type: replace  Abstract: Recently, with the help of deep learning models, significant advances have been made in different Natural Language Processing (NLP) tasks. Unfortunately, state-of-the-art models are vulnerable to noisy texts. We propose a new contextual text denoising algorithm based on the ready-to-use masked language model. The proposed algorithm does not require retraining of the model and can be integrated into any NLP system without additional training on paired cleaning training data. We evaluate our method under synthetic noise and natural noise and show that the proposed algorithm can use context information to correct noise text and improve the performance of noisy inputs in several downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#22914;&#26524;&#22270;&#26159;&#26681;&#25454;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;Pair-Matching&#38382;&#39064;&#20013;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/1905.07342</link><description>&lt;p&gt;
Pair-Matching: Links Prediction with Adaptive Queries
&lt;/p&gt;
&lt;p&gt;
Pair-Matching: Links Prediction with Adaptive Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1905.07342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#22914;&#26524;&#22270;&#26159;&#26681;&#25454;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;Pair-Matching&#38382;&#39064;&#20013;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pair-Matching&#38382;&#39064;&#20986;&#29616;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20854;&#20013;&#19968;&#20010;&#24819;&#35201;&#21457;&#29616;&#23454;&#20307;&#25110;&#20010;&#20307;&#20043;&#38388;&#33391;&#22909;&#21305;&#37197;&#30340;&#24773;&#20917;&#12290;&#24418;&#24335;&#19978;&#65292;&#20010;&#20307;&#38598;&#21512;&#30001;&#22270;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20854;&#20013;&#36793;&#65292;&#36215;&#21021;&#26410;&#34987;&#35266;&#23519;&#21040;&#65292;&#34920;&#31034;&#33391;&#22909;&#30340;&#21305;&#37197;&#12290;&#31639;&#27861;&#26597;&#35810;&#33410;&#28857;&#23545;&#24182;&#35266;&#23519;&#36793;&#30340;&#23384;&#22312;/&#19981;&#23384;&#22312;&#12290;&#20854;&#30446;&#26631;&#26159;&#22312;&#22266;&#23450;&#30340;&#26597;&#35810;&#39044;&#31639;&#19979;&#23613;&#21487;&#33021;&#22810;&#22320;&#21457;&#29616;&#36793;&#12290;Pair-Matching&#26159;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#27530;&#23454;&#20363;&#65292;&#20854;&#20013;&#25163;&#33218;&#26159;&#20010;&#20307;&#23545;&#65292;&#22870;&#21169;&#26159;&#36830;&#25509;&#36825;&#20123;&#23545;&#30340;&#36793;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20010;&#32769;&#34382;&#26426;&#38382;&#39064;&#26159;&#38750;&#26631;&#20934;&#30340;&#65292;&#22240;&#20026;&#27599;&#20010;&#25163;&#33218;&#21482;&#33021;&#29609;&#19968;&#27425;&#12290; &#37492;&#20110;&#36825;&#26368;&#21518;&#19968;&#20010;&#32422;&#26463;&#65292;&#21482;&#26377;&#22312;&#22270;&#20855;&#26377;&#19968;&#23450;&#30340;&#22522;&#26412;&#32467;&#26500;&#26102;&#25165;&#21487;&#20197;&#39044;&#26399;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#22270;&#26681;&#25454;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1905.07342v3 Announce Type: replace-cross  Abstract: The pair-matching problem appears in many applications where one wants to discover good matches between pairs of entities or individuals. Formally, the set of individuals is represented by the nodes of a graph where the edges, unobserved at first, represent the good matches. The algorithm queries pairs of nodes and observes the presence/absence of edges. Its goal is to discover as many edges as possible with a fixed budget of queries. Pair-matching is a particular instance of multi-armed bandit problem in which the arms are pairs of individuals and the rewards are edges linking these pairs. This bandit problem is non-standard though, as each arm can only be played once.   Given this last constraint, sublinear regret can be expected only if the graph presents some underlying structure. This paper shows that sublinear regret is achievable in the case where the graph is generated according to a Stochastic Block Model (SBM) with tw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2401.16650</link><description>&lt;p&gt;
&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22238;&#25918;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Augmenting Replay in World Models for Continual Reinforcement Learning. (arXiv:2401.16650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#29615;&#22659;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#25104;&#21151;&#30340;&#31995;&#32479;&#24212;&#35813;&#36866;&#24403;&#24179;&#34913;&#20445;&#25345;&#24050;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#20195;&#29702;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#35201;&#27714;&#12290;&#39318;&#36827;&#20808;&#20986;&#32531;&#20914;&#21306;&#36890;&#24120;&#29992;&#20110;&#22686;&#24378;&#27492;&#31867;&#35774;&#32622;&#20013;&#30340;&#23398;&#20064;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22686;&#24378;&#26041;&#27861;&#24212;&#29992;&#20110;&#27492;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#32531;&#35299;&#20869;&#23384;&#38480;&#21046;&#65292;&#24182;&#19982;&#22522;&#20110;&#19990;&#30028;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;Procgen&#21644;Atari&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28508;&#22312;&#19990;&#30028;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#20998;&#24067;&#21305;&#37197;&#22686;&#24378;&#21487;&#20197;&#25104;&#21151;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24182;&#38750;&#23436;&#20840;&#26080;&#25032;&#21487;&#20987;&#65292;
&lt;/p&gt;
&lt;p&gt;
In continual RL, the environment of a reinforcement learning (RL) agent undergoes change. A successful system should appropriately balance the conflicting requirements of retaining agent performance on already learned tasks, stability, whilst learning new tasks, plasticity. The first-in-first-out buffer is commonly used to enhance learning in such settings but requires significant memory. We explore the application of an augmentation to this buffer which alleviates the memory constraints, and use it with a world model model-based reinforcement learning algorithm, to evaluate its effectiveness in facilitating continual learning. We evaluate the effectiveness of our method in Procgen and Atari RL benchmarks and show that the distribution matching augmentation to the replay-buffer used in the context of latent world models can successfully prevent catastrophic forgetting with significantly reduced computational overhead. Yet, we also find such a solution to not be entirely infallible, and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.16461</link><description>&lt;p&gt;
&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65306;&#26356;&#24555;&#30340;&#20986;&#29616;&#65292;&#26356;&#24555;&#20048;&#30340;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents. (arXiv:2401.16461v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16461
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21487;&#35270;&#20026;&#19968;&#20010;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#31038;&#20250;&#65292;&#36890;&#36807;&#31038;&#20250;&#35268;&#33539;&#21487;&#20197;&#26377;&#25928;&#22320;&#35843;&#25511;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#30340;&#35268;&#33539;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#32780;&#26159;&#20174;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#20013;&#30340;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#20316;&#20986;&#30340;&#21453;&#24212;&#20197;&#21450;&#23545;&#20182;&#20154;&#21453;&#24212;&#30340;&#22238;&#24212;&#65292;&#20915;&#23450;&#20102;&#31038;&#20250;&#20013;&#20986;&#29616;&#21738;&#20123;&#35268;&#33539;&#12290;&#25105;&#20204;&#23558;&#19968;&#20010;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#28385;&#24847;&#25110;&#19981;&#28385;&#24847;&#34892;&#20026;&#30340;&#21453;&#24212;&#35270;&#20026;&#31532;&#19968;&#20010;&#26234;&#33021;&#20307;&#21521;&#31532;&#20108;&#20010;&#26234;&#33021;&#20307;&#30340;&#20132;&#27969;&#12290;&#29702;&#35299;&#36825;&#20123;&#20132;&#27969;&#26159;&#19968;&#31181;&#31038;&#20250;&#26234;&#33021;&#65306;&#36825;&#20123;&#20132;&#27969;&#36890;&#36807;&#25512;&#21160;&#26234;&#33021;&#20307;&#26397;&#30528;&#26576;&#20123;&#34892;&#20026;&#36827;&#34892;&#65292;&#20174;&#32780;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#24809;&#32602;&#21487;&#20197;&#23548;&#33268;&#35268;&#33539;&#30340;&#20986;&#29616;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#23485;&#27867;&#30340;&#31038;&#20250;&#26234;&#33021;&#21487;&#33021;&#22312;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;Ne&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiagent system can be viewed as a society of autonomous agents, whose interactions can be effectively regulated via social norms. In general, the norms of a society are not hardcoded but emerge from the agents' interactions. Specifically, how the agents in a society react to each other's behavior and respond to the reactions of others determines which norms emerge in the society. We think of these reactions by an agent to the satisfactory or unsatisfactory behaviors of another agent as communications from the first agent to the second agent. Understanding these communications is a kind of social intelligence: these communications provide natural drivers for norm emergence by pushing agents toward certain behaviors, which can become established as norms. Whereas it is well-known that sanctioning can lead to the emergence of norms, we posit that a broader kind of social intelligence can prove more effective in promoting cooperation in a multiagent system.  Accordingly, we develop Ne
&lt;/p&gt;</description></item><item><title>ProCNS&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#21407;&#21017;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14074</link><description>&lt;p&gt;
ProCNS: &#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;
&lt;/p&gt;
&lt;p&gt;
ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation. (arXiv:2401.14074v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14074
&lt;/p&gt;
&lt;p&gt;
ProCNS&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#21407;&#21017;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#20998;&#21106;&#65288;WSS&#65289;&#20316;&#20026;&#32531;&#35299;&#27880;&#37322;&#25104;&#26412;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#20914;&#31361;&#30340;&#35299;&#20915;&#26041;&#26696;&#32780;&#20986;&#29616;&#65292;&#37319;&#29992;&#31232;&#30095;&#30340;&#27880;&#37322;&#26684;&#24335;&#65288;&#20363;&#22914;&#28857;&#12289;&#28034;&#40486;&#12289;&#22359;&#31561;&#65289;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#35299;&#21078;&#21644;&#25299;&#25169;&#20808;&#39564;&#23558;&#31232;&#30095;&#27880;&#37322;&#30452;&#25509;&#25193;&#23637;&#20026;&#20266;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#21307;&#23398;&#22270;&#20687;&#20013;&#27169;&#31946;&#36793;&#32536;&#30340;&#20851;&#27880;&#19981;&#36275;&#21644;&#23545;&#31232;&#30095;&#30417;&#30563;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20250;&#22312;&#22122;&#22768;&#21306;&#22495;&#29983;&#25104;&#38169;&#35823;&#19988;&#36807;&#20110;&#33258;&#20449;&#30340;&#20266;&#24314;&#35758;&#65292;&#23548;&#33268;&#27169;&#22411;&#35823;&#24046;&#32047;&#31215;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;WSS&#26041;&#27861;&#65292;&#21517;&#20026;ProCNS&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#21327;&#21516;&#27169;&#22359;&#65292;&#35774;&#35745;&#21407;&#21017;&#26159;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#21306;&#22495;&#31354;&#38388;&#30456;&#20284;&#24615;&#65288;PRSA&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#22823;&#21270;&#31354;&#38388;&#21644;&#35821;&#20041;&#20803;&#32032;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#65292;&#20026;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FSFC&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#20998;&#31867;&#21709;&#24212;&#21644;&#32437;&#21521;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#21516;&#26102;&#36827;&#34892;&#21151;&#33021;&#25968;&#25454;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05765</link><description>&lt;p&gt;
&#21151;&#33021;&#25968;&#25454;&#20998;&#31867;&#30340;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Feature Selection for Functional Data Classification. (arXiv:2401.05765v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FSFC&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#20998;&#31867;&#21709;&#24212;&#21644;&#32437;&#21521;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#21516;&#26102;&#36827;&#34892;&#21151;&#33021;&#25968;&#25454;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#38656;&#35201;&#25972;&#21512;&#21644;&#35299;&#37322;&#22797;&#26434;&#25968;&#25454;&#30340;&#24403;&#20195;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#26032;&#25216;&#26415;&#30340;&#20986;&#29616;&#20419;&#36827;&#20102;&#22823;&#37327;&#32437;&#21521;&#21464;&#37327;&#30340;&#25910;&#38598;&#65292;&#20351;&#24471;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#36991;&#20813;&#36807;&#25311;&#21512;&#21644;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FSFC&#65288;&#21151;&#33021;&#20998;&#31867;&#29305;&#24449;&#36873;&#25321;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#20998;&#31867;&#21709;&#24212;&#21644;&#32437;&#21521;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#21516;&#26102;&#36827;&#34892;&#21151;&#33021;&#25968;&#25454;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#23450;&#20041;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#23558;&#36923;&#36753;&#25439;&#22833;&#21644;&#21151;&#33021;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35782;&#21035;&#29992;&#20110;&#20998;&#31867;&#30340;&#26368;&#20851;&#38190;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#26368;&#23567;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#20351;&#29992;&#21151;&#33021;&#20027;&#25104;&#20998;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;&#21452;&#22686;&#24191;Lagrange&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional data analysis has emerged as a crucial tool in many contemporary scientific domains that require the integration and interpretation of complex data. Moreover, the advent of new technologies has facilitated the collection of a large number of longitudinal variables, making feature selection pivotal for avoiding overfitting and improving prediction performance. This paper introduces a novel methodology called FSFC (Feature Selection for Functional Classification), that addresses the challenge of jointly performing feature selection and classification of functional data in scenarios with categorical responses and longitudinal features. Our approach tackles a newly defined optimization problem that integrates logistic loss and functional features to identify the most crucial features for classification. To address the minimization procedure, we employ functional principal components and develop a new adaptive version of the Dual Augmented Lagrangian algorithm that leverages the 
&lt;/p&gt;</description></item><item><title>MAGNeT&#26159;&#19968;&#31181;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;MAGNeT&#36824;&#25506;&#32034;&#20102;&#28151;&#21512;&#29256;&#26412;&#65292;&#21487;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04577</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#36974;&#34109;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
Masked Audio Generation using a Single Non-Autoregressive Transformer. (arXiv:2401.04577v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04577
&lt;/p&gt;
&lt;p&gt;
MAGNeT&#26159;&#19968;&#31181;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;MAGNeT&#36824;&#25506;&#32034;&#20102;&#28151;&#21512;&#29256;&#26412;&#65292;&#21487;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNeT&#30340;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#30452;&#25509;&#25805;&#20316;&#22810;&#20010;&#38899;&#39057;&#20196;&#29260;&#27969;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MAGNeT&#30001;&#21333;&#38454;&#27573;&#38750;&#33258;&#22238;&#24402;Transformer&#32452;&#25104;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#36974;&#34109;&#35745;&#21010;&#22120;&#39044;&#27979;&#36974;&#34109;&#20196;&#29260;&#30340;&#33539;&#22260;&#65292;&#32780;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36880;&#27493;&#26500;&#24314;&#36755;&#20986;&#24207;&#21015;&#20351;&#29992;&#22810;&#20010;&#35299;&#30721;&#27493;&#39588;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#21033;&#29992;&#22806;&#37096;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20998;&#21644;&#25490;&#21517;MAGNeT&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#23558;&#34987;&#29992;&#20110;&#21518;&#32493;&#30340;&#35299;&#30721;&#27493;&#39588;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;MAGNeT&#30340;&#28151;&#21512;&#29256;&#26412;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#21069;&#20960;&#31186;&#38047;&#65292;&#32780;&#20854;&#20313;&#30340;&#24207;&#21015;&#21017;&#20197;&#24182;&#34892;&#26041;&#24335;&#36827;&#34892;&#35299;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#24211;&#35745;&#31639;&#32593;&#32476;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#30005;&#29983;&#29702;&#27979;&#37327;&#25968;&#25454;&#20013;&#35299;&#30721;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#26102;&#31354;&#20449;&#24687;&#65292;&#24182;&#22312;&#23439;&#35266;&#39046;&#22495;&#20869;&#37325;&#24314;&#32593;&#32476;&#32467;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#27604;&#20854;&#20182;&#24120;&#29992;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#32593;&#32476;&#30340;&#36830;&#25509;&#22270;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#32593;&#32476;&#23545;&#29305;&#23450;&#36755;&#20837;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.03131</link><description>&lt;p&gt;
&#20174;&#30005;&#29983;&#29702;&#25968;&#25454;&#20013;&#26144;&#23556;&#21644;&#39044;&#27979;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#27700;&#24211;&#35745;&#31639;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reservoir-Computing Model for Mapping and Forecasting Neuronal Interactions from Electrophysiological Data. (arXiv:2311.03131v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#24211;&#35745;&#31639;&#32593;&#32476;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#30005;&#29983;&#29702;&#27979;&#37327;&#25968;&#25454;&#20013;&#35299;&#30721;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#26102;&#31354;&#20449;&#24687;&#65292;&#24182;&#22312;&#23439;&#35266;&#39046;&#22495;&#20869;&#37325;&#24314;&#32593;&#32476;&#32467;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#27604;&#20854;&#20182;&#24120;&#29992;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#32593;&#32476;&#30340;&#36830;&#25509;&#22270;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#32593;&#32476;&#23545;&#29305;&#23450;&#36755;&#20837;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#30005;&#29983;&#29702;&#29305;&#24615;&#33021;&#22815;&#22312;&#38750;&#24120;&#30701;&#30340;&#26102;&#38388;&#23610;&#24230;&#20869;&#25581;&#31034;&#19981;&#21516;&#32454;&#32990;&#21333;&#20803;&#20043;&#38388;&#30340;&#21508;&#31181;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#20998;&#26512;&#36825;&#20123;&#20449;&#21495;&#30340;&#36807;&#31243;&#20013;&#65292;&#19968;&#20010;&#25361;&#25112;&#23601;&#26159;&#25214;&#20986;&#32473;&#23450;&#32593;&#32476;&#30340;&#24418;&#24577;&#21644;&#21151;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#27700;&#24211;&#35745;&#31639;&#32593;&#32476;&#65288;RCN&#65289;&#26550;&#26500;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#35299;&#30721;&#31070;&#32463;&#20803;&#22521;&#20859;&#29289;&#30340;&#30005;&#29983;&#29702;&#27979;&#37327;&#25968;&#25454;&#30340;&#26102;&#31354;&#20449;&#24687;&#65292;&#24182;&#22312;&#23439;&#35266;&#39046;&#22495;&#20869;&#37325;&#24314;&#34920;&#31034;&#31070;&#32463;&#20803;&#21333;&#20803;&#20043;&#38388;&#36830;&#25509;&#24615;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#35813;&#27169;&#22411;&#21487;&#20197;&#27604;&#20132;&#21449;&#30456;&#20851;&#21644;&#20256;&#36882;&#29109;&#31561;&#24120;&#29992;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#32593;&#32476;&#30340;&#36830;&#25509;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#39044;&#27979;&#32593;&#32476;&#23545;&#29305;&#23450;&#36755;&#20837;&#65288;&#22914;&#23616;&#37096;&#21050;&#28608;&#65289;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrophysiological nature of neuronal networks allows to reveal various interactions between different cell units at a very short time-scales. One of the many challenges in analyzing these signals is to retrieve the morphology and functionality of a given network. In this work we developed a computational model, based on Reservoir Computing Network (RCN) architecture, which decodes the spatio-temporal data from electro-physiological measurements of neuronal cultures and reconstructs the network structure on a macroscopic domain, representing the connectivity between neuronal units. We demonstrate that the model can predict the connectivity map of the network with higher accuracy than the common methods such as Cross-Correlation and Transfer-Entropy. In addition, we experimentally demonstrate the ability of the model to predict a network response to a specific input, such as localized stimulus.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#27169;&#22411;&#30340;&#40065;&#26834;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#22797;&#26434;&#31995;&#32479;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20445;&#25345;&#36951;&#25022;&#27425;&#32447;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19794</link><description>&lt;p&gt;
&#32447;&#24615;&#27169;&#22411;&#30340;&#40065;&#26834;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Causal Bandits for Linear Models. (arXiv:2310.19794v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#27169;&#22411;&#30340;&#40065;&#26834;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#22797;&#26434;&#31995;&#32479;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20445;&#25345;&#36951;&#25022;&#27425;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#31995;&#32479;&#20013;&#65292;&#20248;&#21270;&#22238;&#25253;&#20989;&#25968;&#30340;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#20026;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39034;&#24207;&#24178;&#39044;&#35774;&#35745;&#12290;&#22312;&#24050;&#26377;&#30340;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#20551;&#35774;&#26159;&#22240;&#26524;&#27169;&#22411;&#22312;&#26102;&#38388;&#19978;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#65292;&#25968;&#23398;&#27169;&#22411;&#24120;&#24120;&#21457;&#29983;&#26102;&#38388;&#19978;&#30340;&#27874;&#21160;&#65292;&#36825;&#20010;&#20551;&#35774;&#19981;&#19968;&#23450;&#25104;&#31435;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#22312;&#27169;&#22411;&#27874;&#21160;&#23384;&#22312;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#37325;&#28857;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411; (SEMs) &#30340;&#22240;&#26524;&#31995;&#32479;&#12290;SEMs &#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#24178;&#39044;&#21069;&#21518;&#32479;&#35745;&#27169;&#22411;&#22343;&#20026;&#26410;&#30693;&#12290;&#20197;&#32047;&#35745;&#36951;&#25022;&#20026;&#35774;&#35745;&#25351;&#26631;&#65292;&#22312;&#30693;&#36947;&#25972;&#20010;&#22240;&#26524;&#27169;&#22411;&#21450;&#20854;&#27874;&#21160;&#24773;&#20917;&#30340;&#31070;&#35861;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#19968;&#31995;&#21015;&#24178;&#39044;&#20351;&#24471;&#32047;&#35745;&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20445;&#25345;&#36951;&#25022;&#27425;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential design of experiments for optimizing a reward function in causal systems can be effectively modeled by the sequential design of interventions in causal bandits (CBs). In the existing literature on CBs, a critical assumption is that the causal models remain constant over time. However, this assumption does not necessarily hold in complex systems, which constantly undergo temporal model fluctuations. This paper addresses the robustness of CBs to such model fluctuations. The focus is on causal systems with linear structural equation models (SEMs). The SEMs and the time-varying pre- and post-interventional statistical models are all unknown. Cumulative regret is adopted as the design criteria, based on which the objective is to design a sequence of interventions that incur the smallest cumulative regret with respect to an oracle aware of the entire causal model and its fluctuations. First, it is established that the existing approaches fail to maintain regret sub-linearity with 
&lt;/p&gt;</description></item><item><title>OrionBench&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#36890;&#29992;&#25277;&#35937;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21457;&#24067;&#39057;&#32321;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2310.17748</link><description>&lt;p&gt;
&#35753;&#26368;&#32456;&#29992;&#25143;&#25104;&#20026;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#28857;&#65306;OrionBench&#29992;&#20110;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Making the End-User a Priority in Benchmarking: OrionBench for Unsupervised Time Series Anomaly Detection. (arXiv:2310.17748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17748
&lt;/p&gt;
&lt;p&gt;
OrionBench&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#36890;&#29992;&#25277;&#35937;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21457;&#24067;&#39057;&#32321;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24739;&#32773;&#30417;&#27979;&#12289;&#37329;&#34701;&#20013;&#30340;&#39044;&#27979;&#25110;&#33021;&#28304;&#20013;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#36825;&#23548;&#33268;&#20102;&#35768;&#22810;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#27604;&#36739;&#26032;&#24320;&#21457;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#26377;&#38480;&#25968;&#25454;&#38598;&#30340;&#19968;&#27425;&#24615;&#25191;&#34892;&#65292;&#24182;&#19988;&#27604;&#36739;&#20165;&#38480;&#20110;&#23569;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;OrionBench&#8212;&#8212;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#12289;&#25345;&#32493;&#32500;&#25252;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#29992;&#20110;&#34920;&#31034;&#27169;&#22411;&#30340;&#36890;&#29992;&#25277;&#35937;&#12289;&#28155;&#21152;&#26032;&#30340;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#36229;&#21442;&#25968;&#26631;&#20934;&#21270;&#12289;&#27969;&#27700;&#32447;&#39564;&#35777;&#20197;&#21450;&#21457;&#24067;&#22522;&#20934;&#27979;&#35797;&#30340;&#39057;&#32321;&#29256;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;OrionBench&#30340;&#29992;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19977;&#24180;&#26102;&#38388;&#20869;&#21457;&#24067;&#30340;15&#20010;&#29256;&#26412;&#20013;&#27969;&#27700;&#32447;&#30340;&#28436;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is a prevalent problem in many application domains such as patient monitoring in healthcare, forecasting in finance, or predictive maintenance in energy. This has led to the emergence of a plethora of anomaly detection methods, including more recently, deep learning based methods. Although several benchmarks have been proposed to compare newly developed models, they usually rely on one-time execution over a limited set of datasets and the comparison is restricted to a few models. We propose OrionBench -- a user centric continuously maintained benchmark for unsupervised time series anomaly detection. The framework provides universal abstractions to represent models, extensibility to add new pipelines and datasets, hyperparameter standardization, pipeline verification, and frequent releases with published benchmarks. We demonstrate the usage of OrionBench, and the progression of pipelines across 15 releases published over the course of three years. Moreover,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#25968;&#31639;&#26415;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#21516;&#26102;&#35760;&#20303;&#25439;&#22351;&#30340;&#26631;&#31614;&#24182;&#23454;&#29616;100%&#30340;&#27867;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#21644;&#21098;&#26525;&#35760;&#24518;&#21270;&#30340;&#31070;&#32463;&#20803;&#26469;&#38477;&#20302;&#23545;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#23545;&#26410;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.13061</link><description>&lt;p&gt;
&#21040;&#24213;&#26159;&#29702;&#35299;&#36824;&#26159;&#35760;&#24518;&#65306;&#35299;&#26512;&#31639;&#27861;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets. (arXiv:2310.13061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#25968;&#31639;&#26415;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#21516;&#26102;&#35760;&#20303;&#25439;&#22351;&#30340;&#26631;&#31614;&#24182;&#23454;&#29616;100%&#30340;&#27867;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#21644;&#21098;&#26525;&#35760;&#24518;&#21270;&#30340;&#31070;&#32463;&#20803;&#26469;&#38477;&#20302;&#23545;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#23545;&#26410;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#38750;&#24120;&#22823;&#30340;&#24773;&#20917;&#19979;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#30693;&#36947;&#32593;&#32476;&#26159;&#21542;&#24050;&#32463;&#35760;&#20303;&#20102;&#19968;&#32452;&#29305;&#23450;&#30340;&#26679;&#26412;&#65292;&#36824;&#26159;&#29702;&#35299;&#20102;&#20854;&#20013;&#30340;&#22522;&#26412;&#35268;&#24459;&#65288;&#25110;&#32773;&#20004;&#32773;&#37117;&#26377;&#65289;&#12290;&#21463;&#21040;&#36825;&#20010;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;&#27867;&#21270;&#34920;&#31034;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#26469;&#29702;&#35299;&#65292;&#24182;&#19988;&#24456;&#23481;&#26131;&#19982;&#35760;&#24518;&#24615;&#34920;&#31034;&#21306;&#20998;&#24320;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#27169;&#25968;&#31639;&#26415;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#65288;&#958;&#183;100%&#65289;&#30340;&#26631;&#31614;&#26159;&#34987;&#25439;&#22351;&#30340;&#65288;&#21363;&#35757;&#32451;&#38598;&#20013;&#30340;&#19968;&#20123;&#27169;&#25968;&#36816;&#31639;&#32467;&#26524;&#26159;&#38169;&#35823;&#30340;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;i&#65289;&#32593;&#32476;&#21487;&#20197;&#21516;&#26102;&#35760;&#20303;&#25439;&#22351;&#30340;&#26631;&#31614;&#24182;&#23454;&#29616;100%&#30340;&#27867;&#21270;&#65307;&#65288;ii&#65289;&#21487;&#20197;&#35782;&#21035;&#21644;&#21098;&#26525;&#35760;&#24518;&#21270;&#30340;&#31070;&#32463;&#20803;&#65292;&#38477;&#20302;&#23545;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#23545;&#26410;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#65307;&#65288;iii&#65289;&#27491;&#21017;&#21270;&#26041;&#27861;&#22914;w
&lt;/p&gt;
&lt;p&gt;
Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider two-layer neural networks trained on modular arithmetic tasks where ($\xi \cdot 100\%$) of labels are corrupted (\emph{i.e.} some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels \emph{and} achieve $100\%$ generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as w
&lt;/p&gt;</description></item><item><title>Zipformer&#26159;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;U-Net-like&#32534;&#30721;&#22120;&#32467;&#26500;&#12289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#12289;&#25913;&#36827;&#30340;LayerNorm&#12289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#20248;&#21270;&#22120;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#31561;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11230</link><description>&lt;p&gt;
Zipformer&#65306;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Zipformer: A faster and better encoder for automatic speech recognition. (arXiv:2310.11230v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11230
&lt;/p&gt;
&lt;p&gt;
Zipformer&#26159;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;U-Net-like&#32534;&#30721;&#22120;&#32467;&#26500;&#12289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#12289;&#25913;&#36827;&#30340;LayerNorm&#12289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#20248;&#21270;&#22120;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#31561;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Conformer&#24050;&#25104;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#26368;&#27969;&#34892;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#23427;&#22312;&#21464;&#25442;&#22120;&#20013;&#21152;&#20837;&#20102;&#21367;&#31215;&#27169;&#22359;&#20197;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#21464;&#25442;&#22120;&#8212;&#8212;Zipformer&#12290;&#24314;&#27169;&#25913;&#21464;&#21253;&#25324;&#65306;1&#65289;&#31867;&#20284;U-Net&#30340;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#20013;&#38388;&#22534;&#26632;&#22312;&#36739;&#20302;&#30340;&#24103;&#29575;&#19979;&#36816;&#34892;&#65307;2&#65289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#65292;&#22686;&#21152;&#20102;&#26356;&#22810;&#30340;&#27169;&#22359;&#65292;&#20854;&#20013;&#25105;&#20204;&#37325;&#22797;&#20351;&#29992;&#27880;&#24847;&#21147;&#26435;&#37325;&#20197;&#25552;&#39640;&#25928;&#29575;&#65307;3&#65289;&#19968;&#31181;&#25913;&#36827;&#30340;LayerNorm&#24418;&#24335;&#65292;&#31216;&#20026;BiasNorm&#65292;&#20801;&#35768;&#25105;&#20204;&#20445;&#30041;&#19968;&#20123;&#38271;&#24230;&#20449;&#24687;&#65307;4&#65289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;SwooshR&#21644;SwooshL&#30340;&#24615;&#33021;&#20248;&#20110;Swish&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;ScaledAdam&#65292;&#23427;&#36890;&#36807;&#24403;&#21069;&#24352;&#37327;&#30340;&#35268;&#27169;&#26469;&#32553;&#25918;&#26356;&#26032;&#65292;&#20197;&#20445;&#25345;&#30456;&#23545;&#21464;&#21270;&#22823;&#33268;&#30456;&#21516;&#65292;&#24182;&#26126;&#30830;&#23398;&#20064;&#21442;&#25968;&#35268;&#27169;&#12290;&#19982;Adam&#30456;&#27604;&#65292;&#23427;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer. Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster convergence and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and Wenet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#34920;&#26684;LLP&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#34920;&#26684;LLP&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#22312;&#35813;&#22522;&#20934;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#21019;&#24314;&#29305;&#24449;bags&#65292;&#20854;&#20013;&#25152;&#26377;&#23454;&#20363;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#20540;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.10096</link><description>&lt;p&gt;
LLP-Bench&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#34920;&#26684;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label Proportions. (arXiv:2310.10096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#34920;&#26684;LLP&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#34920;&#26684;LLP&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#22312;&#35813;&#22522;&#20934;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#21019;&#24314;&#29305;&#24449;bags&#65292;&#20854;&#20013;&#25152;&#26377;&#23454;&#20363;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#20540;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#65288;LLP&#65289;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#23545;&#23454;&#20363;&#32452;&#65288;&#31216;&#20026;bags&#65289;&#21644;&#30456;&#24212;&#30340;&#26631;&#31614;&#27604;&#20363;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#20010;&#20307;&#23454;&#20363;&#30340;&#26631;&#31614;&#12290;LLP&#20027;&#35201;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#34920;&#26684;&#20004;&#31181;&#25968;&#25454;&#38598;&#12290;&#22312;&#22270;&#20687;LLP&#20013;&#65292;&#36890;&#36807;&#20174;&#24213;&#23618;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#25277;&#26679;&#23454;&#20363;&#26469;&#21019;&#24314;&#22266;&#23450;&#22823;&#23567;&#30340;bags&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21019;&#24314;&#30340;bags&#31216;&#20026;&#38543;&#26426;bags&#12290;&#23545;&#22270;&#20687;LLP&#30340;&#23454;&#39564;&#20027;&#35201;&#38598;&#20013;&#22312;CIFAR-*&#21644;MNIST&#25968;&#25454;&#38598;&#30340;&#38543;&#26426;bags&#19978;&#12290;&#23613;&#31649;&#34920;&#26684;LLP&#22312;&#38544;&#31169;&#25935;&#24863;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#23578;&#32570;&#20047;&#19968;&#20010;&#24320;&#25918;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#34920;&#26684;LLP&#22522;&#20934;&#12290;&#34920;&#26684;LLP&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#24615;&#26159;&#33021;&#22815;&#21019;&#24314;&#29305;&#24449;bags&#65292;&#20854;&#20013;bag&#20013;&#30340;&#25152;&#26377;&#23454;&#20363;&#23545;&#20110;&#32473;&#23450;&#30340;&#29305;&#24449;&#20855;&#26377;&#30456;&#21516;&#30340;&#20540;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29305;&#24449;bags&#22312;&#23454;&#38469;&#30340;&#29616;&#23454;&#24212;&#29992;&#20013;&#38750;&#24120;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#34920;&#26684;LLP&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;LLP&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the task of Learning from Label Proportions (LLP), a model is trained on groups (a.k.a bags) of instances and their corresponding label proportions to predict labels for individual instances. LLP has been applied pre-dominantly on two types of datasets - image and tabular. In image LLP, bags of fixed size are created by randomly sampling instances from an underlying dataset. Bags created via this methodology are called random bags. Experimentation on Image LLP has been mostly on random bags on CIFAR-* and MNIST datasets. Despite being a very crucial task in privacy sensitive applications, tabular LLP does not yet have a open, large scale LLP benchmark. One of the unique properties of tabular LLP is the ability to create feature bags where all the instances in a bag have the same value for a given feature. It has been shown in prior research that feature bags are very common in practical, real world applications [Chen et. al '23, Saket et. al. '22].  In this paper, we address the lac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.04295</link><description>&lt;p&gt;
&#35782;&#21035;&#24178;&#39044;&#22806;&#25512;&#30340;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying Representations for Intervention Extrapolation. (arXiv:2310.04295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35782;&#21035;&#21644;&#22240;&#26524;&#20851;&#31995;&#34920;&#31034;&#23398;&#20064;&#30340;&#21069;&#25552;&#26159;&#25913;&#36827;&#24403;&#21069;&#30340;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#25110;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#22312;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#22810;&#29702;&#35770;&#32467;&#26524;&#26469;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20855;&#20307;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65306;&#39044;&#27979;&#24178;&#39044;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#65292;&#21363;&#20351;&#36825;&#20123;&#24178;&#39044;&#22312;&#35757;&#32451;&#26102;&#27809;&#26377;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#33021;&#22815;&#20026;&#36825;&#20010;&#20219;&#21153;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#21253;&#25324;&#19968;&#20010;&#32467;&#26524;Y&#65292;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;X&#65292;&#36825;&#20123;&#29305;&#24449;&#26159;&#28508;&#22312;&#29305;&#24449;Z&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#20197;&#21450;&#24433;&#21709;Z&#30340;&#22806;&#29983;&#34892;&#20026;&#21464;&#37327;A&#12290;&#24178;&#39044;&#22806;&#25512;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#20301;&#20110;&#35757;&#32451;&#25903;&#25345;&#20043;&#22806;&#30340;A&#19978;&#30340;&#24178;&#39044;&#22914;&#20309;&#24433;&#21709;Y&#12290;&#22312;&#36825;&#37324;&#65292;&#22806;&#25512;&#21464;&#24471;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. Despite recent progress in questions of identifiability, more theoretical results demonstrating concrete advantages of these methods for downstream tasks are needed. In this paper, we consider the task of intervention extrapolation: predicting how interventions affect an outcome, even when those interventions are not observed at training time, and show that identifiable representations can provide an effective solution to this task even if the interventions affect the outcome non-linearly. Our setup includes an outcome Y, observed features X, which are generated as a non-linear transformation of latent features Z, and exogenous action variables A, which influence Z. The objective of intervention extrapolation is to predict how interventions on A that lie outside the training support of A affect Y. Here, extrapolation becom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#25968;&#25454;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23545;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.01690</link><description>&lt;p&gt;
&#20351;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28909;&#24102;&#27668;&#26059;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Forecasting Tropical Cyclones with Cascaded Diffusion Models. (arXiv:2310.01690v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#25968;&#25454;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23545;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27668;&#20505;&#21464;&#21270;&#65292;&#39123;&#39118;&#21464;&#24471;&#26356;&#21152;&#24378;&#28872;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#27861;&#27604;&#22522;&#20110;&#25968;&#23398;&#27169;&#22411;&#30340;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#32463;&#27982;&#23454;&#24800;&#21644;&#26131;&#20110;&#33719;&#21462;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#21355;&#26143;&#25104;&#20687;&#12289;&#36965;&#24863;&#21644;&#22823;&#27668;&#25968;&#25454;&#65292;&#37319;&#29992;&#32423;&#32852;&#26041;&#27861;&#36827;&#34892;&#39123;&#39118;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#30340;&#39044;&#27979;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20845;&#20010;&#20027;&#35201;&#30406;&#22320;&#30340;51&#20010;&#39123;&#39118;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32423;&#32852;&#27169;&#22411;&#30340;&#26368;&#32456;&#39044;&#27979;&#22312;36&#23567;&#26102;&#20869;&#26174;&#31034;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#25152;&#26377;&#19977;&#39033;&#20219;&#21153;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#21644;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#30340;&#20540;&#37117;&#36229;&#36807;&#20102;0.5&#21644;20dB&#12290;&#26412;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#25193;&#25955;&#27169;&#22411;&#31561;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#39640;&#24615;&#33021;&#38656;&#27714;&#65288;&#22914;&#39123;&#39118;&#39044;&#27979;&#65289;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#35745;&#31639;&#32463;&#27982;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
As cyclones become more intense due to climate change, the rise of AI-based modelling provides a more affordable and accessible approach compared to traditional methods based on mathematical models. This work leverages diffusion models to forecast cyclone trajectories and precipitation patterns by integrating satellite imaging, remote sensing, and atmospheric data, employing a cascaded approach that incorporates forecasting, super-resolution, and precipitation modelling, with training on a dataset of 51 cyclones from six major basins. Experiments demonstrate that the final forecasts from the cascaded models show accurate predictions up to a 36-hour rollout, with SSIM and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks. This work also highlights the promising efficiency of AI methods such as diffusion models for high-performance needs, such as cyclone forecasting, while remaining computationally affordable, making them ideal for highly vulnerable regions with crit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#65288;FRL&#65289;&#30340;&#25968;&#25454;&#20013;&#27602;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#23545;&#25239;&#24615;&#22330;&#26223;&#19979;&#35780;&#20272;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#35299;&#20915;&#20102;FRL&#26041;&#27861;&#22312;&#38754;&#23545;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16487</link><description>&lt;p&gt;
&#23545;&#20013;&#27602;&#20844;&#24179;&#34920;&#31034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Poisoning Fair Representations. (arXiv:2309.16487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16487
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#65288;FRL&#65289;&#30340;&#25968;&#25454;&#20013;&#27602;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#23545;&#25239;&#24615;&#22330;&#26223;&#19979;&#35780;&#20272;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#35299;&#20915;&#20102;FRL&#26041;&#27861;&#22312;&#38754;&#23545;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26088;&#22312;&#20943;&#23569;&#23545;&#26576;&#20123;&#20154;&#21475;&#23376;&#32676;&#20307;&#65288;&#22914;&#32769;&#24180;&#20154;&#21644;&#22899;&#24615;&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#20559;&#35265;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#65288;FRL&#65289;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#19981;&#21253;&#21547;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#20998;&#31867;&#25110;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20837;&#12290;&#23613;&#31649;FRL&#26041;&#27861;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#38754;&#23545;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#21363;&#22312;&#23545;&#25239;&#24615;&#22330;&#26223;&#19979;&#35780;&#20272;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#30340;&#27969;&#34892;&#21327;&#35758;&#20013;&#65292;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#24050;&#32463;&#38024;&#23545;&#23558;&#20844;&#24179;&#24615;&#32422;&#26463;&#32435;&#20837;&#27973;&#23618;&#27169;&#22411;&#20998;&#31867;&#22120;&#30340;&#32463;&#20856;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#24320;&#21457;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20844;&#24179;&#30446;&#26631;&#21644;&#27169;&#22411;&#26550;&#26500;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#36825;&#20123;&#25915;&#20987;&#22312;FRL&#20013;&#19981;&#22815;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;FRL&#30340;&#25968;&#25454;&#20013;&#27602;&#26694;&#26550;&#12290;&#25105;&#20204;&#35825;&#20351;&#27169;&#22411;&#36755;&#20986;&#21253;&#21547;&#23613;&#21487;&#33021;&#22810;&#20844;&#24179;&#34920;&#31034;&#30340;&#19981;&#20844;&#24179;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fair machine learning seeks to mitigate model prediction bias against certain demographic subgroups such as elder and female. Recently, fair representation learning (FRL) trained by deep neural networks has demonstrated superior performance, whereby representations containing no demographic information are inferred from the data and then used as the input to classification or other downstream tasks. Despite the development of FRL methods, their vulnerability under data poisoning attack, a popular protocol to benchmark model robustness under adversarial scenarios, is under-explored. Data poisoning attacks have been developed for classical fair machine learning methods which incorporate fairness constraints into shallow-model classifiers. Nonetheless, these attacks fall short in FRL due to notably different fairness goals and model architectures. This work proposes the first data poisoning framework attacking FRL. We induce the model to output unfair representations that contain as much 
&lt;/p&gt;</description></item><item><title>&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33410;&#28857;&#32039;&#20945;&#24615;&#8221;&#24230;&#37327;&#26469;&#25351;&#23548;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.13944</link><description>&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#35777;&#26126;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Training for Graph Contrastive Learning. (arXiv:2309.13944v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13944
&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33410;&#28857;&#32039;&#20945;&#24615;&#8221;&#24230;&#37327;&#26469;&#25351;&#23548;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20174;&#22686;&#24378;&#22270;&#20013;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#32780;&#26080;&#38656;&#26631;&#31614;&#30340;&#27969;&#34892;&#35757;&#32451;&#26041;&#27861;&#12290;&#23613;&#31649;&#26368;&#22823;&#21270;&#27491;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#24182;&#26368;&#23567;&#21270;&#36127;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30340;&#20851;&#38190;&#21407;&#21017;&#24050;&#32463;&#24471;&#21040;&#30830;&#35748;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#65292;&#26159;&#21542;&#26377;&#19968;&#20123;&#33410;&#28857;&#22987;&#32456;&#25353;&#29031;&#36825;&#19968;&#21407;&#21017;&#36827;&#34892;&#33391;&#22909;&#35757;&#32451;&#65292;&#21363;&#20351;&#22312;&#19981;&#21516;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#19979;&#20063;&#26159;&#22914;&#27492;&#65311;&#36824;&#26159;&#26377;&#19968;&#20123;&#33410;&#28857;&#26356;&#26377;&#21487;&#33021;&#22312;&#22270;&#22686;&#24378;&#20013;&#26410;&#32463;&#35757;&#32451;&#65292;&#24182;&#36829;&#21453;&#36825;&#19968;&#21407;&#21017;&#65311;&#22914;&#20309;&#21306;&#20998;&#36825;&#20123;&#33410;&#28857;&#24182;&#36827;&#19968;&#27493;&#25351;&#23548;GCL&#30340;&#35757;&#32451;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#23454;&#39564;&#35777;&#25454;&#65292;&#34920;&#26126;GCL&#30340;&#35757;&#32451;&#22312;&#25152;&#26377;&#33410;&#28857;&#19978;&#30830;&#23454;&#23384;&#22312;&#19981;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#8220;&#33410;&#28857;&#32039;&#20945;&#24615;&#8221;&#65292;&#23427;&#26159;&#33410;&#28857;&#36981;&#24490;GCL&#21407;&#21017;&#19982;&#22686;&#24378;&#33539;&#22260;&#30456;&#20851;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23548;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning (GCL) has emerged as a popular training approach for learning node embeddings from augmented graphs without labels. Despite the key principle that maximizing the similarity between positive node pairs while minimizing it between negative node pairs is well established, some fundamental problems are still unclear. Considering the complex graph structure, are some nodes consistently well-trained and following this principle even with different graph augmentations? Or are there some nodes more likely to be untrained across graph augmentations and violate the principle? How to distinguish these nodes and further guide the training of GCL? To answer these questions, we first present experimental evidence showing that the training of GCL is indeed imbalanced across all nodes. To address this problem, we propose the metric "node compactness", which is the lower bound of how a node follows the GCL principle related to the range of augmentations. We further derive the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;ReLU-Like&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#22312;&#32039;&#33268;&#22495;&#19978;&#23558;$L^p$&#20989;&#25968;&#20174;$[0,1]^{d_x}$&#36924;&#36817;&#21040;$\mathbb R^{d_y}$&#25152;&#38656;&#30340;&#26368;&#23567;&#23485;&#24230;&#20026;$\max\{d_x,d_y,2\}$&#65292;&#20174;&#32780;&#34920;&#26126;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36924;&#36817;&#27604;&#22312;${\mathbb R^{d_x}}$&#19978;&#30340;&#36924;&#36817;&#26356;&#23481;&#26131;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#21253;&#25324;ReLU&#22312;&#20869;&#30340;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#33268;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;&#19979;&#30028;&#20026;$w_{\min}\ge d_y+1$&#65288;&#24403;$d_x&lt;d_y\le2d_x$&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.10402</link><description>&lt;p&gt;
&#20351;&#29992;ReLU&#32593;&#32476;&#22312;&#32039;&#33268;&#22495;&#19978;&#36827;&#34892;&#36890;&#29992;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;
&lt;/p&gt;
&lt;p&gt;
Minimum width for universal approximation using ReLU networks on compact domain. (arXiv:2309.10402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;ReLU-Like&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#22312;&#32039;&#33268;&#22495;&#19978;&#23558;$L^p$&#20989;&#25968;&#20174;$[0,1]^{d_x}$&#36924;&#36817;&#21040;$\mathbb R^{d_y}$&#25152;&#38656;&#30340;&#26368;&#23567;&#23485;&#24230;&#20026;$\max\{d_x,d_y,2\}$&#65292;&#20174;&#32780;&#34920;&#26126;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36924;&#36817;&#27604;&#22312;${\mathbb R^{d_x}}$&#19978;&#30340;&#36924;&#36817;&#26356;&#23481;&#26131;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#21253;&#25324;ReLU&#22312;&#20869;&#30340;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#33268;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;&#19979;&#30028;&#20026;$w_{\min}\ge d_y+1$&#65288;&#24403;$d_x&lt;d_y\le2d_x$&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#30740;&#31350;&#65292;&#38480;&#21046;&#23485;&#24230;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#24050;&#32463;&#20316;&#20026;&#28145;&#24230;&#38480;&#21046;&#32593;&#32476;&#30340;&#32463;&#20856;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#30340;&#23545;&#20598;&#36827;&#34892;&#30740;&#31350;&#12290;&#24050;&#32463;&#26377;&#20960;&#27425;&#23581;&#35797;&#26469;&#34920;&#24449;&#20351;&#24471;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#25104;&#31435;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}$&#65292;&#20294;&#21482;&#26377;&#24456;&#23569;&#20960;&#20010;&#25214;&#21040;&#20102;&#30830;&#20999;&#30340;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20174;$[0,1]^{d_x}$&#21040;$\mathbb R^{d_y}$&#30340;$L^p$&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;&#65292;&#22914;&#26524;&#28608;&#27963;&#20989;&#25968;&#26159;ReLU-Like&#65288;&#20363;&#22914;ReLU&#65292;GELU&#65292;Softplus&#65289;&#65292;&#37027;&#20040;&#23427;&#30340;&#30830;&#20999;&#20540;&#26159;$\max\{d_x,d_y,2\}$&#12290;&#19982;&#24050;&#30693;&#30340;&#32467;&#26524;$w_{\min}=\max\{d_x+1,d_y\}$&#30456;&#27604;&#65292;&#24403;&#22495;&#20026;${\mathbb R^{d_x}}$&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#27425;&#34920;&#26126;&#65292;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36924;&#36817;&#35201;&#27714;&#27604;&#22312;${\mathbb R^{d_x}}$&#19978;&#30340;&#35201;&#27714;&#26356;&#23567;&#12290;&#25105;&#20204;&#25509;&#19979;&#26469;&#21033;&#29992;&#21253;&#25324;ReLU&#22312;&#20869;&#30340;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#19968;&#33268;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}$&#35777;&#26126;&#20102;&#19968;&#20010;&#19979;&#30028;&#65306;&#22914;&#26524;$d_x&lt;d_y\le2d_x$&#65292;&#21017;$w_{\min}\ge d_y+1$&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#20010;&#20108;&#20998;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The universal approximation property of width-bounded networks has been studied as a dual of the classical universal approximation theorem for depth-bounded ones. There were several attempts to characterize the minimum width $w_{\min}$ enabling the universal approximation property; however, only a few of them found the exact values. In this work, we show that the minimum width for the universal approximation of $L^p$ functions from $[0,1]^{d_x}$ to $\mathbb R^{d_y}$ is exactly $\max\{d_x,d_y,2\}$ if an activation function is ReLU-Like (e.g., ReLU, GELU, Softplus). Compared to the known result $w_{\min}=\max\{d_x+1,d_y\}$ when the domain is ${\mathbb R^{d_x}}$, our result first shows that approximation on a compact domain requires smaller width than on ${\mathbb R^{d_x}}$. We next prove a lower bound on $w_{\min}$ for uniform approximation using general activation functions including ReLU: $w_{\min}\ge d_y+1$ if $d_x&lt;d_y\le2d_x$. Together with our first result, this shows a dichotomy be
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#30417;&#27979;&#31449;&#28857;&#30340;&#27700;&#36164;&#28304;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27827;&#27969;&#27969;&#37327;&#12289;&#27700;&#36136;&#31561;&#30456;&#20851;&#21464;&#37327;&#65292;&#24182;&#35752;&#35770;&#20102;&#34701;&#21512;&#38598;&#27700;&#21306;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.09766</link><description>&lt;p&gt;
&#26410;&#30417;&#27979;&#31449;&#28857;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#27700;&#36164;&#28304;&#20013;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources. (arXiv:2308.09766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#30417;&#27979;&#31449;&#28857;&#30340;&#27700;&#36164;&#28304;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27827;&#27969;&#27969;&#37327;&#12289;&#27700;&#36136;&#31561;&#30456;&#20851;&#21464;&#37327;&#65292;&#24182;&#35752;&#35770;&#20102;&#34701;&#21512;&#38598;&#27700;&#21306;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26410;&#30417;&#27979;&#31449;&#28857;&#20013;&#21160;&#24577;&#29615;&#22659;&#21464;&#37327;&#30340;&#39044;&#27979;&#20173;&#28982;&#26159;&#27700;&#36164;&#28304;&#31185;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#19990;&#30028;&#19978;&#22823;&#37096;&#20998;&#30340;&#28129;&#27700;&#36164;&#28304;&#27809;&#26377;&#36866;&#24403;&#30340;&#30417;&#27979;&#20851;&#38190;&#29615;&#22659;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#32780;&#23545;&#27827;&#27969;&#27969;&#37327;&#21644;&#27700;&#36136;&#31561;&#27700;&#25991;&#21464;&#37327;&#36827;&#34892;&#24191;&#27867;&#39044;&#27979;&#30340;&#38656;&#27714;&#30001;&#20110;&#27668;&#20505;&#21644;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#36234;&#26469;&#36234;&#36843;&#20999;&#65292;&#24182;&#24433;&#21709;&#30528;&#27700;&#36164;&#28304;&#12290;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#36807;&#31243;&#21644;&#32463;&#39564;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#27700;&#25991;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#36234;&#26469;&#36234;&#20248;&#36234;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#27827;&#27969;&#27969;&#37327;&#12289;&#27700;&#36136;&#21644;&#20854;&#20182;&#27700;&#36164;&#28304;&#39044;&#27979;&#20013;&#30340;&#30456;&#20851;&#26368;&#26032;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#21033;&#29992;&#26032;&#20852;&#26041;&#27861;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#22312;&#38598;&#27700;&#21306;&#29305;&#24449;&#34701;&#21512;&#26041;&#38754;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of dynamic environmental variables in unmonitored sites remains a long-standing challenge for water resources science. The majority of the world's freshwater resources have inadequate monitoring of critical environmental variables needed for management. Yet, the need to have widespread predictions of hydrological variables such as river flow and water quality has become increasingly urgent due to climate and land use change over the past decades, and their associated impacts on water resources. Modern machine learning methods increasingly outperform their process-based and empirical model counterparts for hydrologic time series prediction with their ability to extract information from large, diverse data sets. We review relevant state-of-the art applications of machine learning for streamflow, water quality, and other water resources prediction and discuss opportunities to improve the use of machine learning with emerging methods for incorporating watershed characteristics i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14940</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#26041;&#27861;&#29992;&#20110;&#23558;&#20808;&#39564;&#30693;&#35782;&#32422;&#26463;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;
&lt;/p&gt;
&lt;p&gt;
A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs. (arXiv:2307.14940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(Neural ODEs)&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#33258;&#28982;&#31995;&#32479;&#30340;&#36830;&#32493;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#20934;&#30830;&#32780;&#26377;&#24847;&#20041;&#30340;&#39044;&#27979;&#65292;&#27169;&#22411;&#24517;&#39035;&#36981;&#24490;&#31649;&#29702;&#36825;&#20123;&#31995;&#32479;&#30340;&#22522;&#26412;&#35268;&#24459;&#25110;&#23450;&#24459;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#12290;&#25152;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#24809;&#32602;&#20989;&#25968;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#24809;&#32602;&#21442;&#25968;&#12290;&#24341;&#20837;&#20808;&#39564;&#30693;&#35782;&#26377;&#21161;&#20110;&#22686;&#21152;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#19977;&#20010;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;(&#20154;&#21475;&#22686;&#38271;&#65292;&#21270;&#23398;&#21453;&#24212;&#28436;&#21270;&#21644;&#38459;&#23612;&#35856;&#25391;&#36816;&#21160;)&#26469;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#22312;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#20854;&#20182;&#24809;&#32602;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26041;&#27861;&#21644;&#21407;&#22987;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs). However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems. In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters. The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models. We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments and a comparison with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;METAVerse&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#22320;&#24418;&#21487;&#34892;&#24615;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21033;&#29992;&#31232;&#30095;&#30340;LiDAR&#28857;&#20113;&#29983;&#25104;&#23494;&#38598;&#36830;&#32493;&#20540;&#25104;&#26412;&#22270;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#26377;&#25928;&#20943;&#23567;&#22320;&#24418;&#21487;&#34892;&#24615;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13991</link><description>&lt;p&gt;
METAVerse&#65306;&#29992;&#20110;&#36234;&#37326;&#23548;&#33322;&#30340;&#20803;&#23398;&#20064;&#21487;&#34892;&#24615;&#25104;&#26412;&#22270;
&lt;/p&gt;
&lt;p&gt;
METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation. (arXiv:2307.13991v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;METAVerse&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#22320;&#24418;&#21487;&#34892;&#24615;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21033;&#29992;&#31232;&#30095;&#30340;LiDAR&#28857;&#20113;&#29983;&#25104;&#23494;&#38598;&#36830;&#32493;&#20540;&#25104;&#26412;&#22270;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#26377;&#25928;&#20943;&#23567;&#22320;&#24418;&#21487;&#34892;&#24615;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#37326;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#23548;&#33322;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#22320;&#24418;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#65292;&#22320;&#24418;&#21487;&#34892;&#24615;&#20272;&#35745;&#21463;&#21040;&#22810;&#20010;&#24433;&#21709;&#36710;&#36742;&#19982;&#22320;&#24418;&#30456;&#20114;&#20316;&#29992;&#30340;&#22240;&#32032;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#33719;&#21462;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21508;&#31181;&#29615;&#22659;&#20013;&#21487;&#34892;&#24615;&#30340;&#21487;&#25512;&#24191;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;METAVerse&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#22320;&#24418;&#21487;&#34892;&#24615;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#30417;&#30563;&#26041;&#24335;&#21033;&#29992;&#36710;&#36742;&#19982;&#22320;&#24418;&#30456;&#20114;&#20316;&#29992;&#21453;&#39304;&#26469;&#35757;&#32451;&#21487;&#34892;&#24615;&#39044;&#27979;&#32593;&#32476;&#65292;&#20174;&#31232;&#30095;&#30340;LiDAR&#28857;&#20113;&#29983;&#25104;&#23494;&#38598;&#36830;&#32493;&#20540;&#25104;&#26412;&#22270;&#12290;&#21033;&#29992;&#20803;&#23398;&#20064;&#20174;&#22810;&#20010;&#29615;&#22659;&#25910;&#38598;&#30340;&#39550;&#39542;&#25968;&#25454;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#20943;&#23567;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36827;&#34892;&#22312;&#32447;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous navigation in off-road conditions requires an accurate estimation of terrain traversability. However, traversability estimation in unstructured environments is subject to high uncertainty due to the variability of numerous factors that influence vehicle-terrain interaction. Consequently, it is challenging to obtain a generalizable model that can accurately predict traversability in a variety of environments. This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.12754</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#22312;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#36890;&#36807;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Linear Feature Learning in Regression Through Regularisation. (arXiv:2307.12754v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;&#29305;&#24449;&#36873;&#25321;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#38750;&#21442;&#25968;&#26041;&#27861;&#24120;&#24120;&#24456;&#38590;&#24212;&#23545;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#30456;&#20851;&#20449;&#24687;&#23384;&#22312;&#20110;&#25968;&#25454;&#30340;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#65292;&#21363;&#22810;&#25351;&#25968;&#27169;&#22411;&#12290;&#22914;&#26524;&#24050;&#30693;&#35813;&#23376;&#31354;&#38388;&#65292;&#23558;&#22823;&#22823;&#22686;&#24378;&#39044;&#27979;&#12289;&#35745;&#31639;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#39044;&#27979;&#30340;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#20272;&#35745;&#39044;&#27979;&#20989;&#25968;&#21644;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#21152;&#19978;&#20989;&#25968;&#23548;&#25968;&#30340;&#24809;&#32602;&#39033;&#65292;&#20197;&#20445;&#35777;&#20854;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;Hermite&#22810;&#39033;&#24335;&#30340;&#27491;&#20132;&#24615;&#21644;&#26059;&#36716;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;RegFeaL&#12290;&#36890;&#36807;&#21033;&#29992;&#26367;&#20195;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#26059;&#36716;&#25968;&#25454;&#20197;&#25913;&#21892;&#19982;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#20449;&#24687;&#35770;&#30340;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#65288;PID&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20851;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#20998;&#35299;&#21457;&#29616;&#20102;&#19977;&#31181;&#19981;&#20844;&#24179;&#26469;&#28304;&#65292;&#20998;&#21035;&#26159;&#21807;&#19968;&#19981;&#24179;&#31561;&#24615;&#12289;&#20887;&#20313;&#19981;&#24179;&#31561;&#24615;&#21644;&#25513;&#30422;&#19981;&#24179;&#31561;&#24615;&#65292;&#25581;&#31034;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#38480;&#21046;&#21644;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.11333</link><description>&lt;p&gt;
&#25581;&#31034;&#32852;&#37030;&#23398;&#20064;&#20013;&#23616;&#37096;&#21644;&#20840;&#23616;&#20844;&#24179;&#24615;&#26435;&#34913;&#30340;&#20449;&#24687;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition. (arXiv:2307.11333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#20449;&#24687;&#35770;&#30340;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#65288;PID&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20851;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#20998;&#35299;&#21457;&#29616;&#20102;&#19977;&#31181;&#19981;&#20844;&#24179;&#26469;&#28304;&#65292;&#20998;&#21035;&#26159;&#21807;&#19968;&#19981;&#24179;&#31561;&#24615;&#12289;&#20887;&#20313;&#19981;&#24179;&#31561;&#24615;&#21644;&#25513;&#30422;&#19981;&#24179;&#31561;&#24615;&#65292;&#25581;&#31034;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#38480;&#21046;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20851;&#20110;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#31561;&#65289;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#8220;&#20840;&#23616;&#20844;&#24179;&#24615;&#8221;&#65288;&#27169;&#22411;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#24179;&#31561;&#31243;&#24230;&#65289;&#25110;&#8220;&#23616;&#37096;&#20844;&#24179;&#24615;&#8221;&#65288;&#27169;&#22411;&#22312;&#27599;&#20010;&#20010;&#20307;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#24179;&#31561;&#31243;&#24230;&#65289;&#65292;&#32780;&#24182;&#19981;&#24635;&#26159;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20840;&#23616;&#20844;&#24179;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#19968;&#20010;&#26159;&#21542;&#26263;&#31034;&#21478;&#19968;&#20010;&#65292;&#25105;&#20204;&#32570;&#20047;&#29702;&#35299;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20449;&#24687;&#35770;&#20013;&#30340;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#65288;PID&#65289;&#26041;&#27861;&#65292;&#39318;&#20808;&#30830;&#23450;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#19977;&#31181;&#19981;&#20844;&#24179;&#26469;&#28304;&#65292;&#21363;&#8220;&#21807;&#19968;&#19981;&#24179;&#31561;&#24615;&#8221;&#12289;&#8220;&#20887;&#20313;&#19981;&#24179;&#31561;&#24615;&#8221;&#21644;&#8220;&#25513;&#30422;&#19981;&#24179;&#31561;&#24615;&#8221;&#12290;&#36890;&#36807;&#20856;&#22411;&#26696;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#19977;&#31181;&#19981;&#24179;&#31561;&#24615;&#22914;&#20309;&#24433;&#21709;&#20840;&#23616;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#12290;&#36825;&#31181;&#20998;&#35299;&#24110;&#21161;&#25105;&#20204;&#25512;&#23548;&#20986;&#20840;&#23616;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#38480;&#21046;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works mostly focus on either \emph{global fairness} (overall disparity of the model across all clients) or \emph{local fairness} (disparity of the model at each individual client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID) which first identifies three sources of unfairness in FL, namely, \emph{Unique Disparity}, \emph{Redundant Disparity}, and \emph{Masked Disparity}. Using canonical examples, we demonstrate how these three disparities contribute to global and local fairness. This decomposition helps us derive fundamental limits and trade-offs between
&lt;/p&gt;</description></item><item><title>DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.06924</link><description>&lt;p&gt;
DRAGON: &#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#24102;&#26377;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#30340;&#36741;&#21161;&#23548;&#33322;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06924
&lt;/p&gt;
&lt;p&gt;
DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#21147;&#21463;&#25439;&#32773;&#22312;&#29702;&#35299;&#21644;&#23548;&#33322;&#21608;&#22260;&#31354;&#38388;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#23548;&#33322;&#25216;&#26415;&#35201;&#20040;&#21482;&#20851;&#27880;&#23548;&#33322;&#65292;&#35201;&#20040;&#25552;&#20379;&#26377;&#38480;&#30340;&#20851;&#20110;&#29615;&#22659;&#30340;&#27807;&#36890;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#21644;&#35821;&#20041;&#23548;&#33322;&#26041;&#38754;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGON&#65292;&#19968;&#31181;&#30001;&#23545;&#35805;&#31995;&#32479;&#39537;&#21160;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#24182;&#20855;&#26377;&#23558;&#29615;&#22659;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;DRAGON&#33021;&#22815;&#24341;&#23548;&#29992;&#25143;&#21040;&#22320;&#22270;&#19978;&#30340;&#30446;&#26631;&#22320;&#26631;&#65292;&#25551;&#36848;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#23519;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#23545;&#35805;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#33258;&#30001;&#24418;&#24335;&#25551;&#36848;&#19982;&#29615;&#22659;&#20013;&#30340;&#22320;&#26631;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#25552;&#20379;&#35821;&#20041;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#26085;&#24120;&#23460;&#20869;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#30450;&#30446;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DRAGON&#33021;&#22815;&#19982;&#29992;&#25143;&#39034;&#30021;&#22320;&#27807;&#36890;&#65292;
&lt;/p&gt;
&lt;p&gt;
Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#21160;&#37327;&#26469;&#25552;&#21319;FedAvg&#21644;SCAFFOLD&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#24341;&#20837;&#21160;&#37327;&#21487;&#20197;&#20351;FedAvg&#22312;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#19979;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2306.16504</link><description>&lt;p&gt;
&#21160;&#37327;&#31616;&#21333;&#32780;&#21487;&#35777;&#23454;&#22320;&#22686;&#24378;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Momentum Benefits Non-IID Federated Learning Simply and Provably. (arXiv:2306.16504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#21160;&#37327;&#26469;&#25552;&#21319;FedAvg&#21644;SCAFFOLD&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#24341;&#20837;&#21160;&#37327;&#21487;&#20197;&#20351;FedAvg&#22312;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#19979;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#30340;&#24378;&#22823;&#33539;&#20363;&#65292;&#20294;&#30001;&#20110;&#19981;&#21487;&#38752;&#30340;&#32593;&#32476;&#36830;&#25509;&#12289;&#32531;&#24930;&#30340;&#36890;&#20449;&#20197;&#21450;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#23427;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;FedAvg&#21644;SCAFFOLD&#26159;&#20004;&#31181;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#22522;&#26412;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;FedAvg&#22312;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#20043;&#21069;&#37319;&#29992;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#65292;&#32780;SCAFFOLD&#22312;&#20854;&#26412;&#22320;&#26356;&#26032;&#20013;&#32500;&#25252;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#25511;&#21046;&#21464;&#37327;&#20197;&#34917;&#20607;&#8220;&#23458;&#25143;&#31471;&#28418;&#31227;&#8221;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#23545;&#31639;&#27861;&#32467;&#26500;&#36827;&#34892;&#19981;&#20999;&#23454;&#38469;&#30340;&#35843;&#25972;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#26377;&#30028;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21160;&#37327;&#26469;&#22686;&#24378;FedAvg&#21644;SCAFFOLD&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#24403;&#25152;&#26377;&#23458;&#25143;&#31471;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24341;&#20837;&#21160;&#37327;&#21487;&#20197;&#20351;FedAvg&#22312;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#19979;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a powerful paradigm for large-scale machine learning, but it faces significant challenges due to unreliable network connections, slow communication, and substantial data heterogeneity across clients. FedAvg and SCAFFOLD are two fundamental algorithms to address these challenges. In particular, FedAvg employs multiple local updates before communicating with a central server, while SCAFFOLD maintains a control variable on each client to compensate for "client drift" in its local updates. Various methods have been proposed in literature to enhance the convergence of these two algorithms, but they either make impractical adjustments to algorithmic structure, or rely on the assumption of bounded data heterogeneity.  This paper explores the utilization of momentum to enhance the performance of FedAvg and SCAFFOLD. When all clients participate in the training process, we demonstrate that incorporating momentum allows FedAvg to converge without relying on the assumption o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;MultiMon&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#22810;&#27169;&#24577;&#31995;&#32479;&#20013;&#30340;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#25581;&#31034;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;&#20854;&#20182;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2306.12105</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#25209;&#37327;&#29983;&#20135;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Mass-Producing Failures of Multimodal Systems with Language Models. (arXiv:2306.12105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;MultiMon&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#22810;&#27169;&#24577;&#31995;&#32479;&#20013;&#30340;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#25581;&#31034;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;&#20854;&#20182;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#21487;&#33021;&#20197;&#35780;&#20272;&#20154;&#21592;&#26410;&#26366;&#39044;&#35265;&#30340;&#26041;&#24335;&#22833;&#36133;&#12290;&#20026;&#20102;&#22312;&#37096;&#32626;&#21069;&#25214;&#21040;&#36825;&#20123;&#22833;&#36133;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MultiMon&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#24615;&#22833;&#36133;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#25552;&#20379;&#21487;&#25512;&#24191;&#30340;&#12289;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27169;&#22411;&#22833;&#36133;&#27169;&#24335;&#30340;&#20363;&#23376;&#12290;&#20026;&#20102;&#25581;&#31034;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;MultiMon&#20174;&#35821;&#26009;&#24211;&#20013;&#25235;&#21462;&#38169;&#35823;&#21327;&#35758;&#30340;&#31034;&#20363;&#65306;&#36755;&#20837;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#20294;&#19981;&#24212;&#35813;&#22914;&#27492;&#12290;&#28982;&#21518;&#23427;&#20250;&#28608;&#27963;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-4&#65289;&#26469;&#26597;&#25214;&#31995;&#32479;&#24615;&#22833;&#36133;&#30340;&#27169;&#24335;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#23427;&#20204;&#12290;&#25105;&#20204;&#20351;&#29992;MultiMon&#25214;&#21040;&#20102;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65288;&#20363;&#22914;&#8220;&#24573;&#30053;&#37327;&#35789;&#8221;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65288;&#20363;&#22914;&#8220;&#19968;&#20010;&#24102;&#26377;&#19968;&#20123;/&#35768;&#22810;&#20070;&#30340;&#20070;&#26550;&#8221;&#65289;&#12290;&#22240;&#20026;CLIP&#26159;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;Midjourney 5.1&#12289;DALL-E&#12289;VideoFusion&#31561;&#31995;&#32479;&#22833;&#36133;&#12290;MultiMon&#20063;&#21487;&#20197;&#25351;&#23548;&#38024;&#23545;&#29305;&#23450;&#29992;&#20363;&#30340;&#30456;&#20851;&#25925;&#38556;&#65292;&#20363;&#22914;&#33258;&#39550;&#36710;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MultiMon, a system that automatically identifies systematic failures -generalizable, natural-language descriptions of patterns of model failures. To uncover systematic failures, MultiMon scrapes a corpus for examples of erroneous agreement: inputs that produce the same output, but should not. It then prompts a language model (e.g., GPT-4) to find systematic patterns of failure and describe them in natural language. We use MultiMon to find 14 systematic failures (e.g., "ignores quantifiers") of the CLIP text-encoder, each comprising hundreds of distinct inputs (e.g., "a shelf with a few/many books"). Because CLIP is the backbone for most state-of-the-art multimodal systems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others. MultiMon can also steer towards failures relevant to specific use cases, such as self-dri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24565;&#20256;&#25773;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#19978;&#19979;&#25991;&#38543;&#26426;&#22359;&#27169;&#22411;(cSBM)&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36825;&#31181;&#31639;&#27861;&#36798;&#21040;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#24314;&#31435;&#24615;&#33021;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.07948</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Optimal Inference in Contextual Stochastic Block Models. (arXiv:2306.07948v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24565;&#20256;&#25773;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#19978;&#19979;&#25991;&#38543;&#26426;&#22359;&#27169;&#22411;(cSBM)&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36825;&#31181;&#31639;&#27861;&#36798;&#21040;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#24314;&#31435;&#24615;&#33021;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#38543;&#26426;&#22359;&#27169;&#22411;(cSBM)&#34987;&#25552;&#20986;&#26469;&#23545;&#20855;&#26377;&#33410;&#28857;&#26631;&#31614;&#30456;&#20851;&#24615;&#30340;&#23646;&#24615;&#22270;&#36827;&#34892;&#26080;&#30417;&#30563;&#31038;&#21306;&#26816;&#27979;&#65292;&#20854;&#20013;&#22270;&#24418;&#21644;&#39640;&#32500;&#33410;&#28857;&#20449;&#24687;&#37117;&#19982;&#33410;&#28857;&#26631;&#31614;&#30456;&#20851;&#12290;&#22312;&#22270;&#19978;&#26426;&#22120;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;cSBM&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The contextual stochastic block model (cSBM) was proposed for unsupervised community detection on attributed graphs where both the graph and the high-dimensional node information correlate with node labels. In the context of machine learning on graphs, the cSBM has been widely used as a synthetic dataset for evaluating the performance of graph-neural networks (GNNs) for semi-supervised node classification. We consider a probabilistic Bayes-optimal formulation of the inference problem and we derive a belief-propagation-based algorithm for the semi-supervised cSBM; we conjecture it is optimal in the considered setting and we provide its implementation. We show that there can be a considerable gap between the accuracy reached by this algorithm and the performance of the GNN architectures proposed in the literature. This suggests that the cSBM, along with the comparison to the performance of the optimal algorithm, readily accessible via our implementation, can be instrumental in the develo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.04735</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21151;&#33021;&#22240;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#21363;&#21487;&#20135;&#29983;&#33391;&#22909;&#32467;&#26524;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#20197;&#33719;&#24471;&#24341;&#23548;&#26356;&#22909;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#20339;&#25552;&#31034;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Open Pre-trained Transformers&#65288;OPT&#65289;&#21644;Galactica&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#21487;&#33021;&#20559;&#21521;&#26576;&#20123;&#20154;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#35782;&#21035;&#36825;&#20123;&#28508;&#22312;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#20351;&#29992;&#36719;&#25552;&#31034;&#26469;&#35780;&#20272;&#20559;&#24046;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#32452;&#20844;&#24179;&#24615;&#65288;&#20559;&#24046;&#65289;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#24182;&#25214;&#21040;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;&#30001;&#20110;LLMs&#24050;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#29992;&#20110;&#24037;&#19994;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#30340;&#20559;&#35265;&#35780;&#20272;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. However, this requires prompt tuning to get optimal prompts that lead to better model performances. In this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (LLMs) such as Open Pre-trained Transformers (OPT) and Galactica language model. Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. Since LLMs have been used in the industry in various applications, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#38454;&#26021;&#21147;&#28145;&#24230;&#38598;&#25104; (FoRDE) &#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#36755;&#20837;&#26799;&#24230;&#26469;&#22686;&#24378;&#22810;&#26679;&#24615;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.02775</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#30340;&#36755;&#20837;&#26799;&#24230;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Input gradient diversity for neural network ensembles. (arXiv:2306.02775v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#38454;&#26021;&#21147;&#28145;&#24230;&#38598;&#25104; (FoRDE) &#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#36755;&#20837;&#26799;&#24230;&#26469;&#22686;&#24378;&#22810;&#26679;&#24615;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104; (DE) &#36890;&#36807;&#23427;&#20204;&#30340;&#21151;&#33021;&#22810;&#26679;&#24615;&#22312;&#20934;&#30830;&#24615;&#12289;&#26657;&#20934;&#24615;&#21644;&#25269;&#25239;&#24178;&#25200;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029; (ParVI) &#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#32593;&#32476;&#30456;&#20284;&#24615;&#20869;&#26680;&#30340;&#25490;&#26021;&#39033;&#26469;&#22686;&#24378;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#26435;&#37325;&#31354;&#38388;&#25490;&#26021;&#26159;&#20302;&#25928;&#30340;&#65292;&#32780;&#30452;&#25509;&#21151;&#33021;&#31354;&#38388;&#25490;&#26021;&#34987;&#21457;&#29616;&#23545; DE &#30340;&#25913;&#36827;&#24456;&#23567;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110; ParVI &#30340;&#19968;&#38454;&#26021;&#21147;&#28145;&#24230;&#38598;&#25104; (FoRDE)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#26799;&#24230;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#12290;&#30001;&#20110;&#36755;&#20837;&#26799;&#24230;&#21807;&#19968;&#22320;&#30830;&#23450;&#20102;&#19968;&#20010;&#20989;&#25968;&#24182;&#19988;&#27604;&#26435;&#37325;&#23567;&#24471;&#22810;&#65292;&#25152;&#20197;&#36825;&#31181;&#26041;&#27861;&#20445;&#35777;&#20102;&#38598;&#21512;&#25104;&#21592;&#22312;&#21151;&#33021;&#19978;&#26159;&#19981;&#21516;&#30340;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#22810;&#26679;&#21270;&#36755;&#20837;&#26799;&#24230;&#40723;&#21169;&#27599;&#20010;&#32593;&#32476;&#23398;&#20064;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#36825;&#26377;&#26395;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles (DEs) demonstrate improved accuracy, calibration and robustness to perturbations over single neural networks partly due to their functional diversity. Particle-based variational inference (ParVI) methods enhance diversity by formalizing a repulsion term based on a network similarity kernel. However, weight-space repulsion is inefficient due to over-parameterization, while direct function-space repulsion has been found to produce little improvement over DEs. To sidestep these difficulties, we propose First-order Repulsive Deep Ensemble (FoRDE), an ensemble learning method based on ParVI, which performs repulsion in the space of first-order input gradients. As input gradients uniquely characterize a function up to translation and are much smaller in dimension than the weights, this method guarantees that ensemble members are functionally different. Intuitively, diversifying the input gradients encourages each network to learn different features, which is expected to improv
&lt;/p&gt;</description></item><item><title>&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31361;&#35302;&#26435;&#37325;&#20998;&#24067;&#21462;&#20915;&#20110;&#31361;&#35302;&#21487;&#22609;&#24615;&#30340;&#20960;&#20309;&#24418;&#24577;&#65292;&#36827;&#32780;&#34920;&#26126;&#23454;&#39564;&#35266;&#27979;&#21040;&#30340;&#23545;&#25968;&#27491;&#24577;&#26435;&#37325;&#20998;&#24067;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#19981;&#19968;&#33268;&#65292;&#21487;&#33021;&#35828;&#26126;&#22823;&#33041;&#20013;&#20351;&#29992;&#30340;&#26159;&#38750;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2305.19394</link><description>&lt;p&gt;
&#31361;&#35302;&#26435;&#37325;&#20998;&#24067;&#21462;&#20915;&#20110;&#21487;&#22609;&#24615;&#30340;&#20960;&#20309;&#24418;&#24577;
&lt;/p&gt;
&lt;p&gt;
Synaptic Weight Distributions Depend on the Geometry of Plasticity. (arXiv:2305.19394v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19394
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31361;&#35302;&#26435;&#37325;&#20998;&#24067;&#21462;&#20915;&#20110;&#31361;&#35302;&#21487;&#22609;&#24615;&#30340;&#20960;&#20309;&#24418;&#24577;&#65292;&#36827;&#32780;&#34920;&#26126;&#23454;&#39564;&#35266;&#27979;&#21040;&#30340;&#23545;&#25968;&#27491;&#24577;&#26435;&#37325;&#20998;&#24067;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#19981;&#19968;&#33268;&#65292;&#21487;&#33021;&#35828;&#26126;&#22823;&#33041;&#20013;&#20351;&#29992;&#30340;&#26159;&#38750;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22823;&#22810;&#25968;&#23398;&#20064;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#26799;&#24230;&#19979;&#38477;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#65292;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#20013;&#26085;&#30410;&#22686;&#38271;&#30340;&#25991;&#29486;&#21033;&#29992;&#36825;&#20123;&#24605;&#24819;&#30740;&#31350;&#31361;&#35302;&#21487;&#22609;&#24615;&#12290;&#28982;&#32780;&#65292;&#32477;&#22823;&#37096;&#20998;&#27492;&#31867;&#30740;&#31350;&#24573;&#30053;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#20551;&#35774;&#65306;&#31361;&#35302;&#21464;&#21270;&#30340;&#36317;&#31163;&#36873;&#25321;&#65288;&#21363;&#31361;&#35302;&#21487;&#22609;&#24615;&#30340;&#20960;&#20309;&#24418;&#24577;&#65289;&#12290;&#26799;&#24230;&#19979;&#38477;&#20551;&#23450;&#36317;&#31163;&#20026;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#65292;&#20294;&#35768;&#22810;&#20854;&#20182;&#36317;&#31163;&#20063;&#26159;&#21487;&#33021;&#30340;&#65292;&#24182;&#19988;&#29983;&#29289;&#23398;&#19981;&#19968;&#23450;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#24418;&#24577;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#38236;&#20687;&#19979;&#38477;&#25552;&#20379;&#30340;&#29702;&#35770;&#24037;&#20855;&#34920;&#26126;&#65292;&#26080;&#35770;&#26368;&#23567;&#21270;&#30340;&#25439;&#22833;&#20026;&#20309;&#65292;&#31361;&#35302;&#26435;&#37325;&#30340;&#20998;&#24067;&#37117;&#21462;&#20915;&#20110;&#31361;&#35302;&#21487;&#22609;&#24615;&#30340;&#20960;&#20309;&#24418;&#24577;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22823;&#33041;&#21306;&#22495;&#20013;&#21457;&#29616;&#30340;&#23454;&#39564;&#35266;&#27979;&#21040;&#30340;&#23545;&#25968;&#27491;&#24577;&#26435;&#37325;&#20998;&#24067;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;&#21363;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#24418;&#24577;&#65289;&#19981;&#19968;&#33268;&#65292;&#32780;&#26159;&#19982;&#38750;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most learning algorithms in machine learning rely on gradient descent to adjust model parameters, and a growing literature in computational neuroscience leverages these ideas to study synaptic plasticity in the brain. However, the vast majority of this work ignores a critical underlying assumption: the choice of distance for synaptic changes (i.e. the geometry of synaptic plasticity). Gradient descent assumes that the distance is Euclidean, but many other distances are possible, and there is no reason that biology necessarily uses Euclidean geometry. Here, using the theoretical tools provided by mirror descent, we show that, regardless of the loss being minimized, the distribution of synaptic weights will depend on the geometry of synaptic plasticity. We use these results to show that experimentally-observed log-normal weight distributions found in several brain areas are not consistent with standard gradient descent (i.e. a Euclidean geometry), but rather with non-Euclidean distances.
&lt;/p&gt;</description></item><item><title>Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.14342</link><description>&lt;p&gt;
Sophia&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14342
&lt;/p&gt;
&lt;p&gt;
Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#24040;&#22823;&#25104;&#26412;&#65292;&#20248;&#21270;&#31639;&#27861;&#30340;&#24494;&#23567;&#25913;&#36827;&#23558;&#20250;&#22823;&#22823;&#38477;&#20302;&#35757;&#32451;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;Adam&#21450;&#20854;&#21464;&#31181;&#19968;&#30452;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#20108;&#38454;&#65288;&#22522;&#20110;Hessian&#30340;&#65289;&#20248;&#21270;&#22120;&#24448;&#24448;&#20250;&#24102;&#26469;&#22826;&#22810;&#30340;&#27599;&#27493;&#24320;&#38144;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sophia&#65292;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#23427;&#20351;&#29992;&#36731;&#37327;&#32423;&#20272;&#35745;&#30340;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#12290;&#26356;&#26032;&#27493;&#39588;&#26159;&#26799;&#24230;&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#38500;&#20197;&#20272;&#35745;Hessian&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#65292;&#28982;&#21518;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#12290;&#35009;&#21098;&#25511;&#21046;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26356;&#26032;&#22823;&#23567;&#65292;&#24182;&#25511;&#21046;&#20102;Hessian&#22312;&#36712;&#36857;&#19978;&#30340;&#38750;&#20984;&#24615;&#21644;&#24555;&#36895;&#21464;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;Sophia&#21482;&#22312;&#27599;&#20960;&#27425;&#36845;&#20195;&#20013;&#20272;&#35745;&#23545;&#35282;Hessian&#65292;&#36825;&#20960;&#20046;&#27809;&#26377;&#24179;&#22343;&#27599;&#27493;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;&#22312;&#20351;&#29992;GPT m&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#26102;&#65292;
&lt;/p&gt;
&lt;p&gt;
Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06176</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;RLHF&#21463;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#29983;&#20135;&#21147;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#20195;&#26367;RLHF&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;RLGAF&#21487;&#20197;&#24110;&#21161;&#23545;&#40784;LLM&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#20250;&#21463;&#21040;RLHF&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#20026;&#36827;&#19968;&#27493;&#33258;&#21160;&#21270;AI&#23545;&#40784;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;Transformer&#30340;&#35760;&#24518;&#20851;&#27880;&#33021;&#21147;&#21644;&#24182;&#34892;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13119</link><description>&lt;p&gt;
&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Transformers for Nonlinear Channel Compensation in Optical Systems. (arXiv:2304.13119v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;Transformer&#30340;&#35760;&#24518;&#20851;&#27880;&#33021;&#21147;&#21644;&#24182;&#34892;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#36890;&#36947;&#22343;&#34913;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#24178;&#38271;&#36317;&#31163;&#20256;&#36755;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#30452;&#25509;&#20851;&#27880;&#19968;&#31995;&#21015;&#31526;&#21495;&#20043;&#38388;&#30340;&#35760;&#24518;&#65292;&#22240;&#27492;Transformer&#21487;&#20197;&#19982;&#24182;&#34892;&#32467;&#26500;&#26377;&#25928;&#22320;&#37197;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#37096;&#20998;&#30340;Transformer&#23454;&#29616;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#22343;&#34913;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#19981;&#21516;&#36229;&#21442;&#25968;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#22788;&#29702;&#31526;&#21495;&#22359;&#65292;&#24182;&#20180;&#32454;&#36873;&#25321;&#35201;&#19968;&#36215;&#22788;&#29702;&#30340;&#32534;&#30721;&#22120;&#36755;&#20986;&#23376;&#38598;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#25200;&#21160;&#29702;&#35770;&#30340;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;Transformer&#38750;&#32447;&#24615;&#22343;&#34913;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new nonlinear channel equalization method for the coherent long-haul transmission based on Transformers. We show that due to their capability to attend directly to the memory across a sequence of symbols, Transformers can be used effectively with a parallelized structure. We present an implementation of encoder part of Transformer for nonlinear equalization and analyze its performance over a wide range of different hyper-parameters. It is shown that by processing blocks of symbols at each iteration and carefully selecting subsets of the encoder's output to be processed together, an efficient nonlinear compensation can be achieved. We also propose the use of a physic-informed mask inspired by nonlinear perturbation theory for reducing the computational complexity of Transformer nonlinear equalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21560;&#24341;&#23376;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#25552;&#21319;&#21644;&#37325;&#26500;&#26426;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#21560;&#24341;&#22495;&#20043;&#38388;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#65292;&#21482;&#38656;&#19977;&#20010;&#33258;&#30001;&#24230;&#30340;&#32447;&#24615;&#37325;&#26500;&#23601;&#21487;&#20197;&#20840;&#23616;&#32447;&#24615;&#21270;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.11860</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#20010;&#21560;&#24341;&#23376;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#25552;&#21319;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
On the lifting and reconstruction of nonlinear systems with multiple attractors. (arXiv:2304.11860v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21560;&#24341;&#23376;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#25552;&#21319;&#21644;&#37325;&#26500;&#26426;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#21560;&#24341;&#22495;&#20043;&#38388;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#65292;&#21482;&#38656;&#19977;&#20010;&#33258;&#30001;&#24230;&#30340;&#32447;&#24615;&#37325;&#26500;&#23601;&#21487;&#20197;&#20840;&#23616;&#32447;&#24615;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#36890;&#36807;&#20851;&#27880;&#19981;&#21464;&#23376;&#31354;&#38388;&#20013;&#30340;&#35266;&#27979;&#37327;&#30340;&#28436;&#21270;&#65292;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#32447;&#24615;&#35270;&#35282;&#12290;&#24863;&#20852;&#36259;&#30340;&#35266;&#27979;&#37327;&#36890;&#24120;&#26159;&#20174;Koopman&#29305;&#24449;&#20989;&#25968;&#32447;&#24615;&#37325;&#26500;&#20986;&#26469;&#30340;&#12290;&#23613;&#31649;Koopman&#31639;&#23376;&#22312;&#36807;&#21435;&#20960;&#24180;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#20010;&#31283;&#23450;&#28857;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#20851;&#20110;Koopman&#31639;&#23376;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#19968;&#20123;&#35823;&#35299;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#20855;&#26377;&#22810;&#20010;&#21560;&#24341;&#23376;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#25552;&#21319;&#26426;&#21046;&#12290;&#36890;&#36807;&#32771;&#34385;Duffing&#25391;&#33633;&#22120;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#21033;&#29992;&#21560;&#24341;&#22495;&#20043;&#38388;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#65292;Koopman&#21487;&#35266;&#27979;&#31354;&#38388;&#20013;&#20855;&#26377;&#19977;&#20010;&#33258;&#30001;&#24230;&#30340;&#32447;&#24615;&#37325;&#26500;&#23601;&#36275;&#20197;&#20840;&#23616;&#32447;&#24615;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Koopman operator provides a linear perspective on non-linear dynamics by focusing on the evolution of observables in an invariant subspace. Observables of interest are typically linearly reconstructed from the Koopman eigenfunctions. Despite the broad use of Koopman operators over the past few years, there exist some misconceptions about the applicability of Koopman operators to dynamical systems with more than one fixed point. In this work, an explanation is provided for the mechanism of lifting for the Koopman operator of nonlinear systems with multiple attractors. Considering the example of the Duffing oscillator, we show that by exploiting the inherent symmetry between the basins of attraction, a linear reconstruction with three degrees of freedom in the Koopman observable space is sufficient to globally linearize the system.
&lt;/p&gt;</description></item><item><title>META-CODE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#23398;&#20064;&#21644;&#26131;&#20110;&#25910;&#38598;&#30340;&#33410;&#28857;&#20803;&#25968;&#25454;&#65292;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#26816;&#27979;&#37325;&#21472;&#31038;&#21306;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;META-CODE&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04497</link><description>&lt;p&gt;
&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#30340;&#25506;&#32034;&#23398;&#20064;&#36741;&#21161;&#31038;&#21306;&#26816;&#27979;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Exploratory Learning-Aided Community Detection in Networks with Unknown Topology. (arXiv:2304.04497v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04497
&lt;/p&gt;
&lt;p&gt;
META-CODE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#23398;&#20064;&#21644;&#26131;&#20110;&#25910;&#38598;&#30340;&#33410;&#28857;&#20803;&#25968;&#25454;&#65292;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#26816;&#27979;&#37325;&#21472;&#31038;&#21306;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;META-CODE&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#21457;&#29616;&#31038;&#21306;&#32467;&#26500;&#20316;&#20026;&#21508;&#31181;&#32593;&#32476;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#25110;&#35775;&#38382;&#38480;&#21046;&#65292;&#32593;&#32476;&#32467;&#26500;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#36825;&#20351;&#24471;&#29616;&#26377;&#30340;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#22312;&#27809;&#26377;&#26114;&#36149;&#30340;&#32593;&#32476;&#25299;&#25169;&#33719;&#21462;&#30340;&#24773;&#20917;&#19979;&#26080;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; META-CODE&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#23398;&#20064;&#36741;&#21161;&#26131;&#20110;&#25910;&#38598;&#30340;&#33410;&#28857;&#20803;&#25968;&#25454;&#65292;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#26816;&#27979;&#37325;&#21472;&#31038;&#21306;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;META-CODE &#38500;&#20102;&#21021;&#22987;&#30340;&#32593;&#32476;&#25512;&#29702;&#27493;&#39588;&#22806;&#65292;&#36824;&#21253;&#25324;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#65306;1) &#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#33410;&#28857;&#32423;&#31038;&#21306;&#24402;&#23646;&#23884;&#20837;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#37325;&#26500;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;2) &#22522;&#20110;&#31038;&#21306;&#24402;&#23646;&#30340;&#33410;&#28857;&#26597;&#35810;&#36827;&#34892;&#32593;&#32476;&#25506;&#32034;&#65292;3) &#20351;&#29992;&#25506;&#32034;&#32593;&#32476;&#20013;&#30340;&#22522;&#20110;&#36793;&#36830;&#25509;&#30340;&#36830;&#20307;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#25512;&#29702;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102; META-CODE &#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In social networks, the discovery of community structures has received considerable attention as a fundamental problem in various network analysis tasks. However, due to privacy concerns or access restrictions, the network structure is often unknown, thereby rendering established community detection approaches ineffective without costly network topology acquisition. To tackle this challenge, we present META-CODE, a unified framework for detecting overlapping communities in networks with unknown topology via exploratory learning aided by easy-to-collect node metadata. Specifically, META-CODE consists of three iterative steps in addition to the initial network inference step: 1) node-level community-affiliation embeddings based on graph neural networks (GNNs) trained by our new reconstruction loss, 2) network exploration via community-affiliation-based node queries, and 3) network inference using an edge connectivity-based Siamese neural network model from the explored network. Through e
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16618</link><description>&lt;p&gt;
&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#20026;&#23545;&#35805;&#25935;&#24863;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#29305;&#23450;&#29305;&#24449;&#30340;&#20154;&#21592;&#21644;/&#25110;&#29305;&#23450;&#29615;&#22659;&#20013;&#30340;&#35828;&#35805;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#35282;&#33394;&#27880;&#37322;&#38590;&#20197;&#24471;&#21040;&#21644;&#25104;&#21151;&#21033;&#29992;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#25163;&#21160;&#27880;&#37322;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27969;&#34892;&#30340; Cornell &#30005;&#24433;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340; 863 &#21517;&#28436;&#35762;&#32773;&#65292;&#21253;&#25324;&#29305;&#24449;&#24341;&#29992;&#21644;&#35282;&#33394;&#25551;&#36848;&#65292;&#20197;&#21450;&#36229;&#36807; 95&#65285; &#30340;&#29305;&#33394;&#30005;&#24433;&#30340;&#19968;&#32452;&#33258;&#21160;&#25552;&#21462;&#30340;&#20845;&#20010;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#27492;&#31867;&#27880;&#37322;&#26469;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22256;&#24785;&#20943;&#23569;&#39640;&#36798; 8.5&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35762;&#32773;&#65292;&#21363;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#22521;&#35757;&#25968;&#25454;&#30340;&#28436;&#35762;&#32773;&#65292;&#20381;&#36182;&#20110;&#35282;&#33394;&#30340;&#20154;&#21475;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#25910;&#38598;&#27492;&#31867;&#20803;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#39033;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#65292;&#20197;&#31361;&#20986;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Clugen&#30340;&#27169;&#22359;&#21270;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#20351;&#29992;&#20219;&#24847;&#20998;&#24067;&#21019;&#24314;&#25903;&#25345;&#32447;&#27573;&#30340;&#22810;&#32500;&#32858;&#31867;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#31639;&#27861;&#24182;&#26377;&#28508;&#21147;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2301.10327</link><description>&lt;p&gt;
&#29983;&#25104;&#25903;&#25345;&#32447;&#30340;&#22810;&#32500;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Generating Multidimensional Clusters With Support Lines. (arXiv:2301.10327v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10327
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Clugen&#30340;&#27169;&#22359;&#21270;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#20351;&#29992;&#20219;&#24847;&#20998;&#24067;&#21019;&#24314;&#25903;&#25345;&#32447;&#27573;&#30340;&#22810;&#32500;&#32858;&#31867;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#31639;&#27861;&#24182;&#26377;&#28508;&#21147;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#23545;&#20110;&#35780;&#20272;&#32858;&#31867;&#25216;&#26415;&#12289;&#34917;&#20805;&#21644;&#25193;&#23637;&#30495;&#23454;&#25968;&#25454;&#20197;&#21450;&#20801;&#35768;&#26356;&#23436;&#25972;&#22320;&#35206;&#30422;&#32473;&#23450;&#38382;&#39064;&#31354;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#32780;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#20855;&#26377;&#21019;&#24314;&#22823;&#37327;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#31232;&#32570;&#26102;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#34987;&#20805;&#20998;&#29702;&#35299;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24037;&#20855;&#65292;&#20197;&#31995;&#32479;&#22320;&#30740;&#31350;&#32858;&#31867;&#20998;&#26512;&#31639;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Clugen&#30340;&#27169;&#22359;&#21270;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#20351;&#29992;&#20219;&#24847;&#20998;&#24067;&#21019;&#24314;&#25903;&#25345;&#32447;&#27573;&#30340;&#22810;&#32500;&#32858;&#31867;&#12290;Clugen&#26159;&#24320;&#28304;&#30340;&#65292;&#26377;&#20840;&#38754;&#30340;&#21333;&#20803;&#27979;&#35797;&#21644;&#25991;&#26723;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;Python&#12289;R&#12289;Julia&#21644;MATLAB/Octave&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#20010;&#32500;&#24230;&#19978;&#20135;&#29983;&#20016;&#23500;&#22810;&#26679;&#30340;&#32467;&#26524;&#65292;&#36866;&#21512;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#26377;&#28508;&#21147;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data is essential for assessing clustering techniques, complementing and extending real data, and allowing for more complete coverage of a given problem's space. In turn, synthetic data generators have the potential of creating vast amounts of data -- a crucial activity when real-world data is at premium -- while providing a well-understood generation procedure and an interpretable instrument for methodically investigating cluster analysis algorithms. Here, we present Clugen, a modular procedure for synthetic data generation, capable of creating multidimensional clusters supported by line segments using arbitrary distributions. Clugen is open source, comprehensively unit tested and documented, and is available for the Python, R, Julia, and MATLAB/Octave ecosystems. We demonstrate that our proposal can produce rich and varied results in various dimensions, is fit for use in the assessment of clustering algorithms, and has the potential to be a widely used framework in diverse 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.10540</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#31185;&#23398;&#21457;&#29616;&#20013;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#22522;&#20110;&#36153;&#26364;&#29289;&#29702;&#35762;&#20041;&#30340;&#19968;&#32452;&#20844;&#24335;&#65292;&#25105;&#20204;&#37325;&#26032;&#21019;&#24314;&#20102;120&#20010;&#25968;&#25454;&#38598;&#65292;&#35752;&#35770;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#24615;&#33021;&#65288;SRSD&#65289;&#12290;&#23545;&#20110;&#36825;120&#20010;SRSD&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;&#20844;&#24335;&#21450;&#20854;&#21464;&#37327;&#30340;&#23646;&#24615;&#65292;&#35774;&#35745;&#20102;&#21512;&#29702;&#30340;&#23454;&#20540;&#33539;&#22260;&#26469;&#37319;&#26679;&#20540;&#65292;&#20197;&#20415;&#25105;&#20204;&#30340;&#26032;SRSD&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;SRSD&#30340;&#28508;&#21147;&#65292;&#22914;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#20013;&#65288;&#37325;&#26032;&#65289;&#21457;&#29616;&#29289;&#29702;&#23450;&#24459;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#21478;&#22806;120&#20010;&#21253;&#21547;&#34394;&#25311;&#21464;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#26816;&#39564;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20165;&#36873;&#25321;&#24517;&#35201;&#21464;&#37327;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39044;&#27979;&#26041;&#31243;&#19982;&#30495;&#23454;&#26041;&#31243;&#26641;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#65288;NED&#65289;&#26469;&#35299;&#20915;&#29616;&#26377;SR&#24230;&#37327;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20108;&#20803;&#24230;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits datasets and evaluation criteria for Symbolic Regression (SR), specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling ranges of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method can (re)discover physical laws from such datasets. We also create another 120 datasets that contain dummy variables to examine whether SR methods can choose necessary variables only. Besides, we propose to use normalized edit distances (NED) between a predicted equation and the true equation trees for addressing a critical issue that existing SR metrics are either binary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#27888;&#26862;&#22561;&#20998;&#37197;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#20165;&#23545;&#39640;&#32622;&#20449;&#24230;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#30446;&#21069;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.05880</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#27888;&#26862;&#22561;&#20998;&#37197;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Confident Sinkhorn Allocation for Pseudo-Labeling. (arXiv:2206.05880v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#27888;&#26862;&#22561;&#20998;&#37197;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#20165;&#23545;&#39640;&#32622;&#20449;&#24230;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#30446;&#21069;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#23545;&#26631;&#35760;&#25968;&#25454;&#20381;&#36182;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#25968;&#25454;&#22686;&#24378;&#30340;&#20869;&#22312;&#31354;&#38388;&#21644;&#35821;&#20041;&#32467;&#26500;&#25104;&#21151;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#33258;&#28982;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#27809;&#26377;&#36866;&#24403;&#30340;&#32467;&#26500;&#25110;&#19981;&#21464;&#24615;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#20266;&#26631;&#31614;&#65288;PL&#65289;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#39046;&#22495;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;PL &#23545;&#38408;&#20540;&#25935;&#24863;&#65292;&#22914;&#26524;&#30001;&#20110;&#36807;&#24230;&#33258;&#20449;&#23548;&#33268;&#38169;&#35823;&#20998;&#37197;&#65292;&#21017;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#19981;&#30830;&#23450;&#24615;&#23545;&#20266;&#26631;&#31614;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#32622;&#20449;&#27888;&#26862;&#22561;&#20998;&#37197;&#30340;&#26041;&#27861;(CSA)&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#20165;&#23545;&#39640;&#32622;&#20449;&#24230;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#12290;CSA &#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#36825;&#19968;&#23454;&#38469;&#37325;&#35201;&#39046;&#22495;&#20013;&#32988;&#36807;&#20102;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#39640;&#25928;&#24615;&#21644;&#26131;&#23454;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning is a critical tool in reducing machine learning's dependence on labeled data. It has been successfully applied to structured data, such as images and natural language, by exploiting the inherent spatial and semantic structure therein with pretrained models or data augmentation. These methods are not applicable, however, when the data does not have the appropriate structure, or invariances. Due to their simplicity, pseudo-labeling (PL) methods can be widely used without any domain assumptions. However, PL is sensitive to a threshold and can perform poorly if wrong assignments are made due to overconfidence. This paper studies theoretically the role of uncertainty to pseudo-labeling and proposes Confident Sinkhorn Allocation (CSA), which identifies the best pseudo-label allocation via optimal transport to only samples with high confidence scores. CSA outperforms the current state-of-the-art in this practically important area of semi-supervised learning. Additiona
&lt;/p&gt;</description></item><item><title>AutoGL&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#33258;&#21160;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#24211;&#65292;&#20351;&#29992;&#26041;&#20415;&#19988;&#26131;&#20110;&#25193;&#23637;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#33258;&#21160;&#22270;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22270;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2104.04987</link><description>&lt;p&gt;
AutoGL&#65306;&#33258;&#21160;&#22270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
AutoGL: A Library for Automated Graph Learning. (arXiv:2104.04987v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.04987
&lt;/p&gt;
&lt;p&gt;
AutoGL&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#33258;&#21160;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#24211;&#65292;&#20351;&#29992;&#26041;&#20415;&#19988;&#26131;&#20110;&#25193;&#23637;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#33258;&#21160;&#22270;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22270;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#20026;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#25163;&#21160;&#35774;&#35745;&#26368;&#20248;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19981;&#28789;&#27963;&#12289;&#36153;&#26102;&#65292;&#24182;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#22270;&#19978;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#22270;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#33258;&#21160;&#35774;&#35745;&#26368;&#20248;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24211;&#20013;&#27809;&#26377;&#19968;&#20010;&#33021;&#23436;&#20840;&#25903;&#25345;&#22270;&#19978;&#30340;AutoML&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#22270;&#23398;&#20064;&#65288;AutoGL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#33258;&#21160;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24211;&#12290;AutoGL&#26159;&#24320;&#28304;&#30340;&#65292;&#26131;&#20110;&#20351;&#29992;&#65292;&#32780;&#19988;&#26131;&#20110;&#25193;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#19982;&#35774;&#22791;&#20132;&#20114;&#30340;&#21518;&#31471;&#12289;&#23436;&#25972;&#30340;&#33258;&#21160;&#22270;&#23398;&#20064;&#27969;&#31243;&#21644;&#25903;&#25345;&#30340;&#22270;&#24212;&#29992;&#30340;&#19977;&#23618;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an upsurge in research interests and applications of machine learning on graphs. However, manually designing the optimal machine learning algorithms for different graph datasets and tasks is inflexible, labor-intensive, and requires expert knowledge, limiting its adaptivity and applicability. Automated machine learning (AutoML) on graphs, aiming to automatically design the optimal machine learning algorithm for a given graph dataset and task, has received considerable attention. However, none of the existing libraries can fully support AutoML on graphs. To fill this gap, we present Automated Graph Learning (AutoGL), the first dedicated library for automated machine learning on graphs. AutoGL is open-source, easy to use, and flexible to be extended. Specifically, we propose a three-layer architecture, consisting of backends to interface with devices, a complete automated graph learning pipeline, and supported graph applications. The automated machine learning
&lt;/p&gt;</description></item></channel></rss>