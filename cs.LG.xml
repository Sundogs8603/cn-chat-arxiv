<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21442;&#25968;&#20219;&#21153;MAP-Elites&#65288;PT-ME&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#36830;&#32493;&#22810;&#20219;&#21153;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#21033;&#29992;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#36827;&#34892;&#21464;&#24322;&#25805;&#20316;&#65292;&#36890;&#36807;&#24471;&#21040;&#30340;&#35299;&#38598;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#26144;&#23556;&#20219;&#21153;&#21442;&#25968;&#21040;&#26368;&#20248;&#35299;&#30340;&#20989;&#25968;&#65292;&#23454;&#39564;&#35777;&#26126;PT-ME&#31639;&#27861;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01275</link><description>&lt;p&gt;
&#21442;&#25968;&#20219;&#21153;MAP-Elites
&lt;/p&gt;
&lt;p&gt;
Parametric-Task MAP-Elites
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21442;&#25968;&#20219;&#21153;MAP-Elites&#65288;PT-ME&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#36830;&#32493;&#22810;&#20219;&#21153;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#21033;&#29992;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#36827;&#34892;&#21464;&#24322;&#25805;&#20316;&#65292;&#36890;&#36807;&#24471;&#21040;&#30340;&#35299;&#38598;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#26144;&#23556;&#20219;&#21153;&#21442;&#25968;&#21040;&#26368;&#20248;&#35299;&#30340;&#20989;&#25968;&#65292;&#23454;&#39564;&#35777;&#26126;PT-ME&#31639;&#27861;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#32452;&#20989;&#25968;&#30340;&#20248;&#21270;&#21516;&#26102;&#24212;&#29992;&#20110;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#34987;&#31216;&#20026;&#22810;&#20219;&#21153;&#20248;&#21270;&#12290;&#24403;&#21069;&#30340;&#40657;&#31665;&#22810;&#20219;&#21153;&#31639;&#27861;&#20165;&#35299;&#20915;&#26377;&#38480;&#30340;&#19968;&#32452;&#20219;&#21153;&#65292;&#21363;&#20351;&#36825;&#20123;&#20219;&#21153;&#26469;&#33258;&#36830;&#32493;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21442;&#25968;&#20219;&#21153;MAP-Elites&#65288;PT-ME&#65289;&#65292;&#19968;&#31181;&#35299;&#20915;&#36830;&#32493;&#22810;&#20219;&#21153;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#22411;&#40657;&#31665;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#65288;1&#65289;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35299;&#20915;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#28085;&#30422;&#20102;&#36830;&#32493;&#31354;&#38388;&#65292;&#65288;2&#65289;&#21033;&#29992;&#22522;&#20110;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#30340;&#26032;&#21464;&#24322;&#25805;&#20316;&#31526;&#12290;&#25152;&#24471;&#21040;&#30340;&#35299;&#38598;&#25968;&#25454;&#38598;&#20351;&#24471;&#33021;&#22815;&#21019;&#24314;&#19968;&#20010;&#23558;&#20219;&#20309;&#20219;&#21153;&#21442;&#25968;&#26144;&#23556;&#21040;&#20854;&#26368;&#20248;&#35299;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#21442;&#25968;&#20219;&#21153;&#29609;&#20855;&#38382;&#39064;&#21644;&#19968;&#20010;&#26356;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PT-ME&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#31639;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;PPO&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing a set of functions simultaneously by leveraging their similarity is called multi-task optimization. Current black-box multi-task algorithms only solve a finite set of tasks, even when the tasks originate from a continuous space. In this paper, we introduce Parametric-task MAP-Elites (PT-ME), a novel black-box algorithm to solve continuous multi-task optimization problems. This algorithm (1) solves a new task at each iteration, effectively covering the continuous space, and (2) exploits a new variation operator based on local linear regression. The resulting dataset of solutions makes it possible to create a function that maps any task parameter to its optimal solution. We show on two parametric-task toy problems and a more realistic and challenging robotic problem in simulation that PT-ME outperforms all baselines, including the deep reinforcement learning algorithm PPO.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65289;&#65292;&#24182;&#35843;&#26597;&#20102;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02822</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#30340;&#27668;&#20505;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Identifying Climate Targets in National Laws and Policies using Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65289;&#65292;&#24182;&#35843;&#26597;&#20102;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#37327;&#25919;&#31574;&#30446;&#26631;&#26159;&#27668;&#20505;&#25919;&#31574;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#36890;&#24120;&#20197;&#39046;&#22495;&#29305;&#23450;&#21644;&#25216;&#26415;&#24615;&#35821;&#35328;&#20026;&#29305;&#24449;&#12290;&#30446;&#21069;&#65292;&#31579;&#36873;&#20840;&#29699;&#27668;&#20505;&#25919;&#31574;&#30446;&#26631;&#30340;&#26041;&#27861;&#28041;&#21450;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#12290;&#30446;&#21069;&#24456;&#23569;&#26377;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#20174;&#22269;&#23478;&#27861;&#24459;&#25110;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#65292;&#36825;&#38480;&#21046;&#20102;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#31169;&#33829;&#21644;&#20844;&#20849;&#37096;&#38376;&#19982;&#20840;&#29699;&#30446;&#26631;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#20026;&#25919;&#31574;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#25552;&#21450;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;&#20102;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65288;&#20363;&#22914;&#21487;&#20877;&#29983;&#33021;&#28304;&#30446;&#26631;&#65289;&#65289;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#21487;&#38752;&#22320;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#23427;&#20204;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#25105;&#20204;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#24046;&#21644;&#20844;&#24179;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#29305;&#23450;&#24180;&#20221;&#21644;&#22269;&#23478;&#21517;&#31216;&#20316;&#20026;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02822v1 Announce Type: cross  Abstract: Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language. Current methods for curating comprehensive views of global climate policy targets entail significant manual effort. At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions. In this paper we present an approach for extracting mentions of climate targets from national laws and policies. We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text. We investigate bias and equity impacts related to our model and identify specific years and country names as problemati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02761</link><description>&lt;p&gt;
AQuA --&#32467;&#21512;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#35266;&#28857;&#65292;&#21033;&#29992;LLMs&#35780;&#20272;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#30923;&#21830;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25919;&#27835;&#22312;&#32447;&#35752;&#35770;&#20013;&#34913;&#37327;&#36129;&#29486;&#36136;&#37327;&#23545;&#20110;&#30740;&#31350;&#30923;&#21830;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#33258;&#21160;&#34913;&#37327;&#36825;&#20123;&#25351;&#26631;&#21464;&#24471;&#21487;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AQuA&#65292;&#23427;&#26159;&#19968;&#20010;&#28155;&#21152;&#20998;&#25968;&#65292;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#35745;&#31639;&#27599;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#12290;&#19982;&#20854;&#20182;&#29305;&#23450;&#20998;&#25968;&#19981;&#21516;&#65292;AQuA&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#23384;&#22312;&#30340;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02761v1 Announce Type: cross  Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices int
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#20197;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#36731;&#37327;&#32423;&#21333;&#38454;&#27573;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#20102;&#20851;&#31995;&#22270;&#12290;</title><link>https://arxiv.org/abs/2404.02072</link><description>&lt;p&gt;
&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EGTR: Extracting Graph from Transformer for Scene Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02072
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#20197;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#36731;&#37327;&#32423;&#21333;&#38454;&#27573;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#20102;&#20851;&#31995;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#26816;&#27979;&#23545;&#35937;&#24182;&#39044;&#27979;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21333;&#38454;&#27573;SGG&#27169;&#22411;&#65292;&#23427;&#20174;DETR&#35299;&#30721;&#22120;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#23398;&#20064;&#30340;&#21508;&#31181;&#20851;&#31995;&#20013;&#25552;&#21462;&#20851;&#31995;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02072v1 Announce Type: cross  Abstract: Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects. After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively accor
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;AutoML&#25216;&#26415;&#26368;&#22823;&#21270;Deep Shift&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#20445;&#30495;&#24230;HPO&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01965</link><description>&lt;p&gt;
&#26088;&#22312;&#21033;&#29992;AutoML&#23454;&#29616;&#21487;&#25345;&#32493;&#28145;&#24230;&#23398;&#20064;&#65306;&#22522;&#20110;Deep Shift&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;HPO&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Leveraging AutoML for Sustainable Deep Learning: A Multi-Objective HPO Approach on Deep Shift Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;AutoML&#25216;&#26415;&#26368;&#22823;&#21270;Deep Shift&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#20445;&#30495;&#24230;HPO&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#22797;&#26434;&#27169;&#24335;&#25512;&#21160;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;DL&#27169;&#22411;&#30340;&#35745;&#31639;&#38656;&#27714;&#24102;&#26469;&#20102;&#29615;&#22659;&#21644;&#36164;&#28304;&#25361;&#25112;&#12290;Deep Shift&#31070;&#32463;&#32593;&#32476;&#65288;DSNN&#65289;&#21033;&#29992;shift&#25805;&#20316;&#20943;&#23569;&#25512;&#29702;&#26102;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20026;&#27492;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20511;&#37492;&#26631;&#20934;DNN&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#26377;&#20852;&#36259;&#36890;&#36807;AutoML&#25216;&#26415;&#20805;&#20998;&#21457;&#25381;DSNN&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#26368;&#22823;&#21270;DSNN&#24615;&#33021;&#21516;&#26102;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#23558;&#20934;&#30830;&#24615;&#21644;&#33021;&#32791;&#20316;&#20026;&#21487;&#33021;&#20114;&#34917;&#30446;&#26631;&#32467;&#21512;&#30340;&#22810;&#30446;&#26631;&#65288;MO&#65289;&#20248;&#21270;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26368;&#20808;&#36827;&#30340;&#22810;&#20445;&#30495;&#24230;&#65288;MF&#65289;HPO&#19982;&#22810;&#30446;&#26631;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24471;&#21040;&#20102;&#20934;&#30830;&#29575;&#36229;&#36807;80&#65285;&#19988;&#35745;&#31639;&#20302;&#32791;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01965v1 Announce Type: cross  Abstract: Deep Learning (DL) has advanced various fields by extracting complex patterns from large datasets. However, the computational demands of DL models pose environmental and resource challenges. Deep shift neural networks (DSNNs) offer a solution by leveraging shift operations to reduce computational complexity at inference. Following the insights from standard DNNs, we are interested in leveraging the full potential of DSNNs by means of AutoML techniques. We study the impact of hyperparameter optimization (HPO) to maximize DSNN performance while minimizing resource consumption. Since this combines multi-objective (MO) optimization with accuracy and energy consumption as potentially complementary objectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with multi-objective optimization. Experimental results demonstrate the effectiveness of our approach, resulting in models with over 80\% in accuracy and low computational 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01685</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30001;&#20110;&#20854;&#31232;&#30095;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#25805;&#20316;&#32780;&#33021;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#25552;&#20379;&#36229;&#20302;&#21151;&#32791;/&#33021;&#32791;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#38656;&#35201;&#26356;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#25165;&#33021;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#24212;&#29992;&#19981;&#22826;&#36866;&#21512;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#20197;&#21487;&#25509;&#21463;&#30340;&#20869;&#23384;&#21344;&#29992;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;SNNs&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;SNNs&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#23398;&#12290;&#20854;&#20851;&#38190;&#27493;&#39588;&#21253;&#25324;&#35843;&#26597;&#19981;&#21516;&#26680;&#22823;&#23567;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#35774;&#35745;&#26032;&#30340;&#26680;&#22823;&#23567;&#38598;&#21512;&#65292;&#22522;&#20110;&#36873;&#23450;&#30340;&#26680;&#22823;&#23567;&#29983;&#25104;SNN&#26550;&#26500;&#65292;&#24182;&#20998;&#26512;SNN&#27169;&#22411;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;-&#20869;&#23384;&#25240;&#34935;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#23545;&#20110;CIFAR10&#26377;93.24%&#30340;&#20934;&#30830;&#24230;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01685v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations. Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications. Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed. Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling. Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection. The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#20915;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#21160;&#20316;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Gromov-Wasserstein&#38382;&#39064;&#20013;&#32534;&#30721;&#26102;&#38388;&#19968;&#33268;&#24615;&#20808;&#39564;&#26469;&#23454;&#29616;&#20174;&#35270;&#39057;&#24103;&#21644;&#21160;&#20316;&#31867;&#21035;&#20043;&#38388;&#30340;&#22122;&#22768;&#25104;&#26412;&#20013;&#35299;&#30721;&#26102;&#38388;&#19968;&#33268;&#30340;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2404.01518</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#21160;&#20316;&#20998;&#21106;&#30340;&#20020;&#26102;&#19968;&#33268;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01518
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#20915;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#21160;&#20316;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Gromov-Wasserstein&#38382;&#39064;&#20013;&#32534;&#30721;&#26102;&#38388;&#19968;&#33268;&#24615;&#20808;&#39564;&#26469;&#23454;&#29616;&#20174;&#35270;&#39057;&#24103;&#21644;&#21160;&#20316;&#31867;&#21035;&#20043;&#38388;&#30340;&#22122;&#22768;&#25104;&#26412;&#20013;&#35299;&#30721;&#26102;&#38388;&#19968;&#33268;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#26102;&#38388;&#26410;&#20462;&#21098;&#35270;&#39057;&#30340;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#35299;&#20915;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#19968;&#33268;&#24615;&#20808;&#39564;&#32534;&#30721;&#21040;Gromov-Wasserstein&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#35270;&#39057;&#24103;&#21644;&#21160;&#20316;&#31867;&#21035;&#20043;&#38388;&#30340;&#22122;&#22768;&#20851;&#32852;/&#21305;&#37197;&#25104;&#26412;&#30697;&#38453;&#20013;&#35299;&#30721;&#20986;&#19968;&#20010;&#26102;&#38388;&#19968;&#33268;&#30340;&#20998;&#21106;&#12290;&#19982;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#30693;&#36947;&#35270;&#39057;&#30340;&#21160;&#20316;&#39034;&#24207;&#26469;&#23454;&#29616;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#65288;&#34701;&#21512;&#65289;Gromov-Wasserstein&#38382;&#39064;&#21487;&#20197;&#22312;GPU&#19978;&#20351;&#29992;&#20960;&#27425;&#25237;&#24433;&#38236;&#19979;&#38477;&#36845;&#20195;&#39640;&#25928;&#27714;&#35299;&#12290;&#25105;&#20204;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#33258;&#35757;&#32451;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;Breakfast&#12289;50-Salads&#12289;YouTube Instructions&#21644;Desktop Assembly&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#20998;&#21106;&#26041;&#27861;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01518v1 Announce Type: cross  Abstract: We propose a novel approach to the action segmentation task for long, untrimmed videos, based on solving an optimal transport problem. By encoding a temporal consistency prior into a Gromov-Wasserstein problem, we are able to decode a temporally consistent segmentation from a noisy affinity/matching cost matrix between video frames and action classes. Unlike previous approaches, our method does not require knowing the action order for a video to attain temporal consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can be efficiently solved on GPUs using a few iterations of projected mirror descent. We demonstrate the effectiveness of our method in an unsupervised learning setting, where our method is used to generate pseudo-labels for self-training. We evaluate our segmentation approach and unsupervised learning pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly datasets, yielding state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;RMSProp&#21644;Adam&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#32039;&#33268;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#22312;&#26368;&#23485;&#26494;&#30340;&#20551;&#35774;&#19979;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;RMSProp&#21644;Adam&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#21035;&#20026;$\mathcal O(\epsilon^{-4})$&#12290;</title><link>https://arxiv.org/abs/2404.01436</link><description>&lt;p&gt;
RMSProp&#21644;Adam&#22312;&#20855;&#26377;&#20223;&#23556;&#22122;&#22768;&#26041;&#24046;&#30340;&#24191;&#20041;&#20809;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;RMSProp&#21644;Adam&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#32039;&#33268;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#22312;&#26368;&#23485;&#26494;&#30340;&#20551;&#35774;&#19979;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;RMSProp&#21644;Adam&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#21035;&#20026;$\mathcal O(\epsilon^{-4})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22352;&#26631;&#32423;&#21035;&#24191;&#20041;&#20809;&#28369;&#24615;&#21644;&#20223;&#23556;&#22122;&#22768;&#26041;&#24046;&#30340;&#26368;&#23485;&#26494;&#20551;&#35774;&#19979;&#65292;&#20026;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;RMSProp&#21644;Adam&#25552;&#20379;&#20102;&#39318;&#20010;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#39318;&#20808;&#20998;&#26512;&#20102;RMSProp&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#20294;&#27809;&#26377;&#19968;&#38454;&#21160;&#37327;&#30340;Adam&#30340;&#29305;&#20363;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#35299;&#20915;&#33258;&#36866;&#24212;&#26356;&#26032;&#12289;&#26080;&#30028;&#26799;&#24230;&#20272;&#35745;&#21644;Lipschitz&#24120;&#25968;&#20043;&#38388;&#30340;&#20381;&#36182;&#25361;&#25112;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19979;&#38477;&#24341;&#29702;&#20013;&#30340;&#19968;&#38454;&#39033;&#25910;&#25947;&#65292;&#24182;&#19988;&#20854;&#20998;&#27597;&#30001;&#26799;&#24230;&#33539;&#25968;&#30340;&#20989;&#25968;&#19978;&#30028;&#38480;&#21046;&#12290;&#22522;&#20110;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#30340;RMSProp&#25910;&#25947;&#21040;&#19968;&#20010;$\epsilon$-&#31283;&#23450;&#28857;&#65292;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$\mathcal O(\epsilon^{-4})$&#12290;&#28982;&#21518;&#65292;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25512;&#24191;&#21040;Adam&#65292;&#39069;&#22806;&#30340;&#25361;&#25112;&#26159;&#30001;&#20110;&#26799;&#24230;&#19982;&#19968;&#38454;&#21160;&#37327;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01436v1 Announce Type: cross  Abstract: This paper provides the first tight convergence analyses for RMSProp and Adam in non-convex optimization under the most relaxed assumptions of coordinate-wise generalized smoothness and affine noise variance. We first analyze RMSProp, which is a special case of Adam with adaptive learning rates but without first-order momentum. Specifically, to solve the challenges due to dependence among adaptive update, unbounded gradient estimate and Lipschitz constant, we demonstrate that the first-order term in the descent lemma converges and its denominator is upper bounded by a function of gradient norm. Based on this result, we show that RMSProp with proper hyperparameters converges to an $\epsilon$-stationary point with an iteration complexity of $\mathcal O(\epsilon^{-4})$. We then generalize our analysis to Adam, where the additional challenge is due to a mismatch between the gradient and first-order momentum. We develop a new upper bound on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00450</link><description>&lt;p&gt;
&#35268;&#21010;&#21644;&#32534;&#36753;&#26816;&#32034;&#20197;&#22686;&#24378;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Planning and Editing What You Retrieve for Enhanced Tool Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23558;&#22806;&#37096;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#25171;&#24320;&#20102;&#26032;&#30340;&#39046;&#22495;&#65292;&#24212;&#29992;&#33539;&#22260;&#28085;&#30422;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#22120;&#21644;&#26234;&#33021;&#21161;&#25163;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#19968;&#27425;&#24615;&#26816;&#32034;&#31574;&#30053;&#65292;&#26080;&#27861;&#26377;&#25928;&#20934;&#30830;&#22320;&#31579;&#36873;&#30456;&#20851;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#65288;P&amp;R&#65289;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#65288;E&amp;G&#65289;&#8221;&#33539;&#24335;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#20102;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#35268;&#21010;&#22120;&#65292;&#20197;&#22686;&#24378;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00450v1 Announce Type: new  Abstract: Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&amp;R)'' and ``Edit-and-Ground (E\&amp;G)'' paradigms. The P\&amp;R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&amp;G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.20208</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#39044;&#27979;&#34920;&#26684;&#20219;&#21153;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#65292;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#32570;&#22833;&#20540;&#22635;&#20805;&#31561;&#39044;&#27979;&#20219;&#21153;&#26159;&#19982;&#34920;&#26684;&#25968;&#25454;&#30456;&#20851;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#20219;&#21153;&#12290;&#23613;&#31649;LLMs&#25797;&#38271;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#65292;&#20294;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25910;&#38598;&#24102;&#26377;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;Llama-2&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;-shot&#39044;&#27979;&#12289;&#23569;-shot&#39044;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
&lt;/p&gt;</description></item><item><title>ILPO-Net&#26159;&#19968;&#31181;&#22788;&#29702;&#20219;&#24847;&#24418;&#29366;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#36816;&#31639;&#23545;&#23616;&#37096;&#31354;&#38388;&#27169;&#24335;&#26041;&#21521;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21508;&#31181;&#20307;&#31215;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.19612</link><description>&lt;p&gt;
ILPO-NET&#65306;&#29992;&#20110;&#19977;&#32500;&#20013;&#20219;&#24847;&#20307;&#31215;&#27169;&#24335;&#19981;&#21464;&#35782;&#21035;&#30340;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19612
&lt;/p&gt;
&lt;p&gt;
ILPO-Net&#26159;&#19968;&#31181;&#22788;&#29702;&#20219;&#24847;&#24418;&#29366;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#36816;&#31639;&#23545;&#23616;&#37096;&#31354;&#38388;&#27169;&#24335;&#26041;&#21521;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21508;&#31181;&#20307;&#31215;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31354;&#38388;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#26377;&#25928;&#35782;&#21035;&#31354;&#38388;&#27169;&#24335;&#24182;&#23398;&#20064;&#20854;&#23618;&#27425;&#32467;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;&#20307;&#31215;&#25968;&#25454;&#24212;&#29992;&#23547;&#27714;&#30830;&#20445;&#23545;&#20301;&#31227;&#21644;&#27169;&#24335;&#26059;&#36716;&#22343;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#25216;&#26415;&#12290;ILPO-Net&#65288;Invariant to Local Patterns Orientation Network&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;Wigner&#30697;&#38453;&#23637;&#24320;&#65292;&#22312;&#21367;&#31215;&#25805;&#20316;&#20013;&#22788;&#29702;&#20219;&#24847;&#24418;&#29366;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#26412;&#36136;&#19978;&#23545;&#23616;&#37096;&#31354;&#38388;&#27169;&#24335;&#26041;&#21521;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#26080;&#32541;&#38598;&#25104;&#20102;&#26032;&#30340;&#21367;&#31215;&#36816;&#31639;&#31526;&#65292;&#22312;&#21508;&#31181;&#20307;&#31215;&#25968;&#25454;&#38598;&#65288;&#22914;MedMNIST&#21644;CATH&#65289;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#27604;&#22522;&#20934;&#32447;&#26356;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#20943;&#23569; - &#22312;MedMNIST&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#20102;&#39640;&#36798;1000&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19612v1 Announce Type: cross  Abstract: Effective recognition of spatial patterns and learning their hierarchy is crucial in modern spatial data analysis. Volumetric data applications seek techniques ensuring invariance not only to shifts but also to pattern rotations. While traditional methods can readily achieve translational invariance, rotational invariance possesses multiple challenges and remains an active area of research. Here, we present ILPO-Net (Invariant to Local Patterns Orientation Network), a novel approach that handles arbitrarily shaped patterns with the convolutional operation inherently invariant to local spatial pattern orientations using the Wigner matrix expansions. Our architecture seamlessly integrates the new convolution operator and, when benchmarked on diverse volumetric datasets such as MedMNIST and CATH, demonstrates superior performance over the baselines with significantly reduced parameter counts - up to 1000 times fewer in the case of MedMNIS
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18802</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-form factuality in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#24320;&#25918;&#24615;&#20027;&#39064;&#30340;&#20107;&#23454;&#24615;&#25552;&#31034;&#26102;&#65292;&#32463;&#24120;&#29983;&#25104;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#23545;&#27169;&#22411;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;LongFact&#30340;&#25552;&#31034;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#22218;&#25324;38&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;Search-Augmented Factuality Evaluator&#65288;SAFE&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#22120;&#12290;SAFE&#21033;&#29992;LLM&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#29420;&#30340;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#20197;&#21450;&#30830;&#23450;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#24471;&#21040;&#25628;&#32034;&#32467;&#26524;&#25903;&#25345;&#30340;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#35780;&#20272;&#27599;&#20010;&#20107;&#23454;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#23558;F1&#20998;&#25968;&#25193;&#23637;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#22238;&#24212;&#20013;&#25903;&#25345;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#65288;&#31934;&#24230;&#65289;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
&lt;/p&gt;</description></item><item><title>EulerFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#36716;&#25442;&#22120;&#21464;&#20307;&#65292;&#32479;&#19968;&#20102;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.17729</link><description>&lt;p&gt;
EulerFormer&#65306;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17729
&lt;/p&gt;
&lt;p&gt;
EulerFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#36716;&#25442;&#22120;&#21464;&#20307;&#65292;&#32479;&#19968;&#20102;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65292;&#36716;&#25442;&#22120;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24314;&#27169;&#39034;&#24207;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#12290;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#26680;&#24515;&#22312;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#35745;&#31639;&#24207;&#21015;&#20013;&#30340;&#25104;&#23545;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#30001;&#20110;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;&#29305;&#24615;&#65292;&#20301;&#32622;&#32534;&#30721;&#29992;&#20110;&#22686;&#24378;&#20196;&#29260;&#34920;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#22312;&#36825;&#31181;&#35774;&#23450;&#19979;&#65292;&#25104;&#23545;&#27880;&#24847;&#21147;&#20998;&#25968;&#21487;&#20197;&#36890;&#36807;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#20004;&#32773;&#34893;&#29983;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#32463;&#24120;&#20197;&#19981;&#21516;&#26041;&#24335;&#24314;&#27169;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24046;&#24322;&#27979;&#37327;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#24207;&#21015;&#24314;&#27169;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EulerFormer&#30340;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#36716;&#25442;&#22120;&#21464;&#20307;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#34920;&#36848;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#12290; EulerFormer&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#25216;&#26415;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17729v1 Announce Type: cross  Abstract: To capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. Fi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26657;&#20934;&#36125;&#21494;&#26031;UNet++&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26356;&#21487;&#38752;&#21644;&#26356;&#28165;&#26224;&#30340;&#27425;&#23395;&#33410;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22914;&#22825;&#27668;&#39044;&#27979;&#21592;&#26469;&#35828;&#23588;&#20026;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.16612</link><description>&lt;p&gt;
&#20026;&#27425;&#23395;&#33410;&#39044;&#27979;&#26657;&#20934;&#36125;&#21494;&#26031;UNet++
&lt;/p&gt;
&lt;p&gt;
Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16612
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26657;&#20934;&#36125;&#21494;&#26031;UNet++&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26356;&#21487;&#38752;&#21644;&#26356;&#28165;&#26224;&#30340;&#27425;&#23395;&#33410;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22914;&#22825;&#27668;&#39044;&#27979;&#21592;&#26469;&#35828;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23395;&#33410;&#24615;&#39044;&#27979;&#22312;&#26816;&#27979;&#30001;&#27668;&#20505;&#21464;&#21270;&#24341;&#36215;&#30340;&#26497;&#31471;&#28909;&#21644;&#23506;&#20919;&#26102;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#23545;&#39044;&#27979;&#30340;&#20449;&#24515;&#24212;&#24403;&#26159;&#21487;&#38752;&#30340;&#65292;&#22240;&#20026;&#19968;&#24180;&#20013;&#28201;&#24230;&#30340;&#24494;&#23567;&#22686;&#21152;&#23545;&#19990;&#30028;&#26377;&#30528;&#24040;&#22823;&#24433;&#21709;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26657;&#20934;&#25552;&#20379;&#20102;&#19968;&#31181;&#30830;&#20445;&#25105;&#20204;&#23545;&#39044;&#27979;&#30340;&#20449;&#24515;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#26657;&#20934;&#22238;&#24402;&#27169;&#22411;&#26159;&#19968;&#20010;&#30740;&#31350;&#19981;&#36275;&#30340;&#35805;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#32773;&#20013;&#12290;&#25105;&#20204;&#26657;&#20934;&#20102;&#22522;&#20110;UNet++&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34987;&#35777;&#26126;&#22312;&#28201;&#24230;&#24322;&#24120;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;&#22312;&#39044;&#27979;&#35823;&#24046;&#21644;&#26657;&#20934;&#35823;&#24046;&#20043;&#38388;&#30053;&#24494;&#26435;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#21487;&#38752;&#21644;&#26356;&#28165;&#26224;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26657;&#20934;&#24212;&#24403;&#25104;&#20026;&#35832;&#22914;&#22825;&#27668;&#39044;&#25253;&#21592;&#31561;&#23433;&#20840;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16612v1 Announce Type: new  Abstract: Seasonal forecasting is a crucial task when it comes to detecting the extreme heat and colds that occur due to climate change. Confidence in the predictions should be reliable since a small increase in the temperatures in a year has a big impact on the world. Calibration of the neural networks provides a way to ensure our confidence in the predictions. However, calibrating regression models is an under-researched topic, especially in forecasters. We calibrate a UNet++ based architecture, which was shown to outperform physics-based models in temperature anomalies. We show that with a slight trade-off between prediction error and calibration error, it is possible to get more reliable and sharper forecasts. We believe that calibration should be an important part of safety-critical machine learning applications such as weather forecasters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65288;HULS&#65289;&#65292;&#29992;&#20110;&#30417;&#27979;&#22797;&#26434;&#24037;&#19994;&#27969;&#31243;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#39640;&#24230;&#30456;&#20851;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13032</link><description>&lt;p&gt;
&#29992;&#20110;&#30417;&#27979;&#24037;&#19994;&#25209;&#22788;&#29702;&#36807;&#31243;&#30340;&#28151;&#21512;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65288;HULS&#65289;&#65292;&#29992;&#20110;&#30417;&#27979;&#22797;&#26434;&#24037;&#19994;&#27969;&#31243;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#39640;&#24230;&#30456;&#20851;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#29983;&#20135;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#21046;&#33647;&#34892;&#19994;&#65292;&#26159;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#38656;&#35201;&#25345;&#32493;&#30417;&#27979;&#20197;&#30830;&#20445;&#25928;&#29575;&#12289;&#20135;&#21697;&#36136;&#37327;&#21644;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30417;&#27979;&#22797;&#26434;&#24037;&#19994;&#27969;&#31243;&#30340;&#28151;&#21512;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65288;HULS&#65289;&#12290;HULS&#32467;&#21512;&#20102;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOM&#65289;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#39640;&#24230;&#30456;&#20851;&#30340;&#36807;&#31243;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;HULS&#27010;&#24565;&#30340;&#24615;&#33021;&#65292;&#36827;&#34892;&#20102;&#22522;&#20110;&#23454;&#39564;&#23460;&#25209;&#22788;&#29702;&#30340;&#27604;&#36739;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13032v1 Announce Type: new  Abstract: Industrial production processes, especially in the pharmaceutical industry, are complex systems that require continuous monitoring to ensure efficiency, product quality, and safety. This paper presents a hybrid unsupervised learning strategy (HULS) for monitoring complex industrial processes. Addressing the limitations of traditional Self-Organizing Maps (SOMs), especially in scenarios with unbalanced data sets and highly correlated process variables, HULS combines existing unsupervised learning techniques to address these challenges. To evaluate the performance of the HULS concept, comparative experiments are performed based on a laboratory batch
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#20102;&#28145;&#20837;&#29305;&#24449;&#21270;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#19982;&#20808;&#21069;&#20219;&#21153;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#20102;&#36164;&#28304;&#21033;&#29992;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#31995;&#32479;&#20197;&#36866;&#24212;LLMs&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.07648</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#20013;&#24515;&#24320;&#21457;&#30340;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterization of Large Language Model Development in the Datacenter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#20102;&#28145;&#20837;&#29305;&#24449;&#21270;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#19982;&#20808;&#21069;&#20219;&#21153;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#20102;&#36164;&#28304;&#21033;&#29992;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#31995;&#32479;&#20197;&#36866;&#24212;LLMs&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#38761;&#21629;&#24615;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35201;&#26377;&#25928;&#21033;&#29992;&#22823;&#35268;&#27169;&#38598;&#32676;&#36164;&#28304;&#26469;&#24320;&#21457;LLMs&#24182;&#38750;&#26131;&#20107;&#65292;&#32463;&#24120;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#39057;&#32321;&#30340;&#30828;&#20214;&#25925;&#38556;&#12289;&#22797;&#26434;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#21644;&#36164;&#28304;&#21033;&#29992;&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#23545;&#25105;&#20204;&#30340;GPU&#25968;&#25454;&#20013;&#24515;Acme&#20013;&#25910;&#38598;&#30340;&#20026;&#26399;&#20845;&#20010;&#26376;&#30340;LLM&#24320;&#21457;&#24037;&#20316;&#36127;&#36733;&#36319;&#36394;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#29305;&#24449;&#21270;&#30740;&#31350;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#19982;&#20808;&#21069;&#20219;&#21153;&#29305;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24037;&#20316;&#36127;&#36733;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#20102;&#36164;&#28304;&#21033;&#29992;&#27169;&#24335;&#65292;&#24182;&#30830;&#23450;&#20102;&#21508;&#31181;&#20316;&#19994;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24635;&#32467;&#20102;&#25105;&#20204;&#36935;&#21040;&#30340;&#38556;&#30861;&#65292;&#24182;&#21457;&#29616;&#20102;&#20248;&#21270;&#19987;&#20026;LLMs&#23450;&#21046;&#30340;&#31995;&#32479;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21162;&#21147;&#65306;&#65288;1&#65289;&#23481;&#38169;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;LLM&#21442;&#19982;&#26469;&#22686;&#24378;&#23481;&#38169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07648v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have presented impressive performance across several transformative tasks. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. In this paper, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures. Our analysis summarizes hurdles we encountered and uncovers potential opportunities to optimize systems tailored for LLMs. Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining, which enhances fault tolerance through LLM-involved 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#26368;&#20339;&#21452;&#36194;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20248;&#24615;&#24046;&#36317;&#21463;&#21040;&#19979;&#30028;&#38480;&#21046;&#26102;&#36951;&#25022;&#20026;$O(\log(T))$&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#36793;&#32536;&#26465;&#20214;&#26469;&#25551;&#36848;&#27425;&#20248;&#24615;&#24046;&#36317;&#23545;&#38382;&#39064;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03219</link><description>&lt;p&gt;
LC-Tsalis-INF: &#24191;&#20041;&#26368;&#20339;&#21452;&#36194;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#26368;&#20339;&#21452;&#36194;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20248;&#24615;&#24046;&#36317;&#21463;&#21040;&#19979;&#30028;&#38480;&#21046;&#26102;&#36951;&#25022;&#20026;$O(\log(T))$&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#36793;&#32536;&#26465;&#20214;&#26469;&#25551;&#36848;&#27425;&#20248;&#24615;&#24046;&#36317;&#23545;&#38382;&#39064;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20855;&#26377;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#32972;&#26223;&#30340;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#29616;&#26377;&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#20339;&#21452;&#36194;&#65288;BoBW&#65289;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#22312;&#38543;&#26426;&#21306;&#22495;&#20013;&#28385;&#36275;$O(\log^2(T))$&#65292;&#20854;&#20013;$T$&#20026;&#22238;&#21512;&#25968;&#65292;&#20854;&#27425;&#20248;&#24615;&#24046;&#36317;&#30001;&#27491;&#24120;&#25968;&#19979;&#30028;&#65292;&#21516;&#26102;&#22312;&#23545;&#25239;&#24615;&#21306;&#22495;&#20013;&#28385;&#36275;$O(\sqrt{T})$&#12290;&#28982;&#32780;&#65292;&#23545;$T$&#30340;&#20381;&#36182;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#24182;&#19988;&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#20551;&#35774;&#21487;&#20197;&#25918;&#23485;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#24403;&#27425;&#20248;&#24615;&#24046;&#36317;&#21463;&#21040;&#19979;&#30028;&#38480;&#21046;&#26102;&#65292;&#20854;&#36951;&#25022;&#28385;&#36275;$O(\log(T))$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36793;&#32536;&#26465;&#20214;&#65292;&#21363;&#23545;&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#19968;&#20010;&#26356;&#28201;&#21644;&#30340;&#20551;&#35774;&#12290;&#35813;&#26465;&#20214;&#20351;&#29992;&#21442;&#25968;$\beta \in (0, \infty]$&#34920;&#24449;&#19982;&#27425;&#20248;&#24615;&#24046;&#36317;&#30456;&#20851;&#30340;&#38382;&#39064;&#38590;&#24230;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#35813;&#31639;&#27861;&#30340;&#36951;&#25022;&#28385;&#36275;$O\left(\
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03219v1 Announce Type: new  Abstract: This study considers the linear contextual bandit problem with independent and identically distributed (i.i.d.) contexts. In this problem, existing studies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets satisfy $O(\log^2(T))$ for the number of rounds $T$ in a stochastic regime with a suboptimality gap lower-bounded by a positive constant, while satisfying $O(\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has room for improvement, and the suboptimality-gap assumption can be relaxed. For this issue, this study proposes an algorithm whose regret satisfies $O(\log(T))$ in the setting when the suboptimality gap is lower-bounded. Furthermore, we introduce a margin condition, a milder assumption on the suboptimality gap. That condition characterizes the problem difficulty linked to the suboptimality gap using a parameter $\beta \in (0, \infty]$. We then show that the algorithm's regret satisfies $O\left(\
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25308;&#21344;&#24237;-&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;InferGuard&#65292;&#29992;&#20110;&#38450;&#24481;&#23458;&#25143;&#31471;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#25512;&#26029;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.03149</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#32531;&#35299;&#23458;&#25143;&#31471;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25308;&#21344;&#24237;-&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;InferGuard&#65292;&#29992;&#20110;&#38450;&#24481;&#23458;&#25143;&#31471;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#25512;&#26029;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#25581;&#31034;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26366;&#34987;&#35748;&#20026;&#23433;&#20840;&#30340;&#28431;&#27934;&#65292;&#22240;&#20026;&#23458;&#25143;&#31471;&#19981;&#21521;&#26381;&#21153;&#22120;&#20849;&#20139;&#20854;&#31169;&#26377;&#25968;&#25454;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#36973;&#21463;&#35832;&#22914;&#23458;&#25143;&#31471;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#25512;&#26029;&#25915;&#20987;&#31561;&#25915;&#20987;&#65292;&#27492;&#25915;&#20987;&#21487;&#20197;&#35753;&#24694;&#24847;&#23458;&#25143;&#31471;&#37325;&#29616;&#21463;&#23475;&#32773;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25308;&#21344;&#24237;-&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;InferGuard&#65292;&#26088;&#22312;&#38450;&#24481;&#23458;&#25143;&#31471;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#25512;&#26029;&#25915;&#20987;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;InferGuard&#20013;&#65292;&#26381;&#21153;&#22120;&#39318;&#20808;&#35745;&#31639;&#20854;&#25910;&#21040;&#30340;&#25152;&#26377;&#27169;&#22411;&#26356;&#26032;&#30340;&#22352;&#26631;&#20013;&#20301;&#25968;&#12290;&#22914;&#26524;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26032;&#19982;&#35745;&#31639;&#20986;&#30340;&#20013;&#20301;&#25968;&#26356;&#26032;&#26174;&#33879;&#20559;&#31163;&#65292;&#21017;&#35270;&#20026;&#24694;&#24847;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;InferGuard&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03149v1 Announce Type: cross  Abstract: Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack.   In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client's model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five benchmark dat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#29983;&#24577;&#25490;&#25918;&#26469;&#20445;&#25252;&#29983;&#24577;&#31995;&#32479;&#30340;&#27700;&#30005;&#31649;&#29702;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#25512;&#21160;&#27700;&#30005;&#21378;&#20860;&#39038;&#29615;&#22659;&#20445;&#25252;&#21644;&#33021;&#28304;&#29983;&#20135;&#12290;</title><link>https://arxiv.org/abs/2403.02821</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#24212;&#24615;&#27700;&#30005;&#31649;&#29702;&#26041;&#27861;&#29992;&#20110;&#19979;&#28216;&#29983;&#24577;&#31995;&#32479;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Hydropower Management Approach for Downstream Ecosystem Preservation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02821
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#29983;&#24577;&#25490;&#25918;&#26469;&#20445;&#25252;&#29983;&#24577;&#31995;&#32479;&#30340;&#27700;&#30005;&#31649;&#29702;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#25512;&#21160;&#27700;&#30005;&#21378;&#20860;&#39038;&#29615;&#22659;&#20445;&#25252;&#21644;&#33021;&#28304;&#29983;&#20135;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#30005;&#21378;&#22312;&#25512;&#21160;&#28165;&#27905;&#21644;&#21487;&#25345;&#32493;&#33021;&#28304;&#29983;&#20135;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#23545;&#20840;&#29699;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#26469;&#28304;&#30340;&#36807;&#28193;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#27700;&#30005;&#21378;&#30446;&#21069;&#34987;&#35270;&#20026;&#26082;&#26159;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#26469;&#28304;&#65292;&#21448;&#26159;&#29983;&#24577;&#31995;&#32479;&#30340;&#30772;&#22351;&#32773;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#29983;&#24577;&#25490;&#25918;&#20316;&#20026;&#20445;&#25252;&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#31181;&#28508;&#21147;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#25552;&#20513;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#27599;&#20010;&#25152;&#38656;&#26102;&#38388;&#39044;&#27979;&#26368;&#23567;&#29983;&#24577;&#25490;&#25918;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;&#20854;&#26080;&#32541;&#38598;&#25104;&#21040;&#27700;&#30005;&#31649;&#29702;&#36719;&#20214;&#20013;&#65292;&#21033;&#29992;&#20256;&#32479;&#21463;&#38480;&#20248;&#21270;&#31639;&#27861;&#30340;&#25104;&#29087;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#20445;&#25252;&#29983;&#24577;&#31995;&#32479;&#20813;&#21463;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#36824;&#26377;&#21487;&#33021;&#22686;&#21152;&#30005;&#21147;&#20135;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02821v1 Announce Type: new  Abstract: Hydropower plants play a pivotal role in advancing clean and sustainable energy production, contributing significantly to the global transition towards renewable energy sources. However, hydropower plants are currently perceived both positively as sources of renewable energy and negatively as disruptors of ecosystems. In this work, we highlight the overlooked potential of using hydropower plant as protectors of ecosystems by using adaptive ecological discharges. To advocate for this perspective, we propose using a neural network to predict the minimum ecological discharge value at each desired time. Additionally, we present a novel framework that seamlessly integrates it into hydropower management software, taking advantage of the well-established approach of using traditional constrained optimisation algorithms. This novel approach not only protects the ecosystems from climate change but also contributes to potentially increase the elec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01216</link><description>&lt;p&gt;
API&#23601;&#22815;&#20102;&#65306;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#27861;&#35775;&#38382;&#23545;&#25968;&#26102;&#22914;&#20309;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36825;&#19968;&#26222;&#36941;&#25361;&#25112;&#12290;&#25972;&#20307;&#39044;&#27979;&#65288;CP&#65289;&#20197;&#20854;&#19982;&#27169;&#22411;&#26080;&#20851;&#21644;&#26080;&#38656;&#20998;&#24067;&#30340;&#29305;&#28857;&#32780;&#38395;&#21517;&#65292;&#26159;&#21508;&#31181;LLMs&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#24819;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#21487;&#20197;&#35775;&#38382;&#23545;&#25968;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#20165;&#25903;&#25345;API&#30340;LLMs&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;&#23545;&#25968;&#21487;&#33021;&#23384;&#22312;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;CP&#26041;&#27861;&#65292;&#65288;1&#65289;&#19987;&#20026;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;API-only LLMs&#37327;&#36523;&#23450;&#21046;; (2) &#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;; &#20197;&#21450;(3)&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#31895;&#31890;&#24230;&#65288;&#20363;&#22914;&#65292;&#26679;&#26412;&#39057;&#29575;&#65289;&#21644;&#32454;&#31890;&#24230;&#19981;&#30830;&#23450;&#24615;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#65289;&#26469;&#21046;&#23450;&#19981;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01216v1 Announce Type: cross  Abstract: This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only LLMs without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30456;&#26426;&#23039;&#21183;&#35270;&#20026;&#23556;&#32447;&#26463;&#30340;&#20998;&#24067;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#31354;&#38388;&#22270;&#20687;&#29305;&#24449;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#22238;&#24402;&#21644;&#25193;&#25955;&#30340;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;CO3D&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14817</link><description>&lt;p&gt;
&#25668;&#20687;&#22836;&#20316;&#20026;&#23556;&#32447;: &#36890;&#36807;&#23556;&#32447;&#25193;&#25955;&#36827;&#34892;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Cameras as Rays: Pose Estimation via Ray Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14817
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30456;&#26426;&#23039;&#21183;&#35270;&#20026;&#23556;&#32447;&#26463;&#30340;&#20998;&#24067;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#31354;&#38388;&#22270;&#20687;&#29305;&#24449;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#22238;&#24402;&#21644;&#25193;&#25955;&#30340;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;CO3D&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#30456;&#26426;&#23039;&#21183;&#26159;3D&#37325;&#24314;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#37492;&#20110;&#35270;&#22270;&#31232;&#30095;&#65288;&lt;10&#65289;&#65292;&#35813;&#20219;&#21153;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#36861;&#27714;&#30456;&#26426;&#22806;&#21442;&#30340;&#20840;&#23616;&#21442;&#25968;&#21270;&#30340;&#33258;&#19978;&#32780;&#19979;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30456;&#26426;&#23039;&#21183;&#35270;&#20026;&#23556;&#32447;&#26463;&#30340;&#20998;&#24067;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#20801;&#35768;&#19982;&#31354;&#38388;&#22270;&#20687;&#29305;&#24449;&#32039;&#23494;&#32806;&#21512;&#65292;&#25552;&#39640;&#20102;&#23039;&#21183;&#31934;&#24230;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#31181;&#34920;&#31034;&#33258;&#28982;&#36866;&#29992;&#20110;&#38598;&#21512;&#32423;&#21035;&#30340;Transformer&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#22359;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#23556;&#32447;&#19978;&#12290;&#20026;&#20102;&#25429;&#25417;&#31232;&#30095;&#35270;&#35282;&#23039;&#21183;&#25512;&#26029;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35843;&#25972;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#37319;&#26679;&#21512;&#29702;&#30340;&#27169;&#24335;&#65292;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#26082;&#26159;&#22522;&#20110;&#22238;&#24402;&#65292;&#20063;&#26159;&#22522;&#20110;&#25193;&#25955;&#30340;&#65292;&#22312;CO3D&#30456;&#26426;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14817v1 Announce Type: cross  Abstract: Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (&lt;10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.14698</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#20998;&#26512;&#29992;&#20110;&#20998;&#31867;&#19982;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65306;&#25104;&#37117;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Big data analytics to classify earthwork-related locations: A Chengdu study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14698
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#27745;&#26579;&#26174;&#33879;&#21152;&#21095;&#65292;&#23548;&#33268;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20005;&#37325;&#20581;&#24247;&#21518;&#26524;&#12290;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65288;ERLs&#65289;&#26159;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;ERLs&#30340;&#26377;&#25928;&#31649;&#29702;&#19968;&#30452;&#26159;&#25919;&#24220;&#21644;&#29615;&#22659;&#26426;&#26500;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#20027;&#35201;&#21407;&#22240;&#21253;&#25324;&#20854;&#20998;&#31867;&#20998;&#23646;&#19981;&#21516;&#30340;&#30417;&#31649;&#37096;&#38376;&#12289;&#20449;&#24687;&#38556;&#30861;&#12289;&#25968;&#25454;&#26356;&#26032;&#24310;&#36831;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#28304;&#22836;&#28784;&#23576;&#27745;&#26579;&#30340;&#25233;&#21046;&#25514;&#26045;&#30340;&#32570;&#20047;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23454;&#38469;&#25968;&#25454;&#30740;&#31350;&#20102;&#29305;&#24449;&#19982;&#28784;&#23576;&#27745;&#26579;&#28304;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#25104;&#21151;&#23454;&#26045;&#22312;&#19968;&#20010;&#21517;&#20026;&#30340;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14698v1 Announce Type: cross  Abstract: Air pollution has significantly intensified, leading to severe health consequences worldwide. Earthwork-related locations (ERLs) constitute significant sources of urban dust pollution. The effective management of ERLs has long posed challenges for governmental and environmental agencies, primarily due to their classification under different regulatory authorities, information barriers, delays in data updating, and a lack of dust suppression measures for various sources of dust pollution. To address these challenges, we classified urban dust pollution sources using dump truck trajectory, urban point of interest (POI), and land cover data. We compared several prediction models and investigated the relationship between features and dust pollution sources using real data. The results demonstrate that high-accuracy classification can be achieved with a limited number of features. This method was successfully implemented in the system called
&lt;/p&gt;</description></item><item><title>BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14151</link><description>&lt;p&gt;
BIRCO&#65306;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14151
&lt;/p&gt;
&lt;p&gt;
BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;(IR)&#20219;&#21153;&#22522;&#20934;(BIRCO)&#12290; BIRCO&#35780;&#20272;IR&#31995;&#32479;&#26681;&#25454;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#26816;&#32034;&#25991;&#26723;&#30340;&#33021;&#21147;&#12290; &#35813;&#22522;&#20934;&#30340;&#22797;&#26434;&#24615;&#21644;&#32039;&#20945;&#22823;&#23567;&#20351;&#20854;&#36866;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#21487;&#33021;&#24433;&#21709;LLM&#22312;&#26816;&#32034;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19982;&#25110;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#26356;&#22797;&#26434;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290; &#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#20934;&#20219;&#21153;&#19978;&#22343;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#21644;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14151v1 Announce Type: cross  Abstract: We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20272;&#35745;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#26426;&#22120;&#23398;&#20064;(DML)&#26469;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#20272;&#35745;&#22240;&#26524;&#21442;&#25968;&#26041;&#38754;&#20248;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26041;&#27861;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#36991;&#20813;&#31561;&#25928;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13332</link><description>&lt;p&gt;
&#29992;&#20110;&#22240;&#26524;&#28151;&#21512;&#24314;&#27169;&#30340;&#21452;&#26426;&#22120;&#23398;&#20064;&#8212;&#8212;&#22320;&#29699;&#31185;&#23398;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double machine learning for causal hybrid modeling -- applications in the Earth sciences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20272;&#35745;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#26426;&#22120;&#23398;&#20064;(DML)&#26469;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#20272;&#35745;&#22240;&#26524;&#21442;&#25968;&#26041;&#38754;&#20248;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26041;&#27861;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#36991;&#20813;&#31561;&#25928;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13332v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#28151;&#21512;&#24314;&#27169;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#31185;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;&#35299;&#37322;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#36981;&#23432;&#33258;&#28982;&#35268;&#24459;&#12290;&#28982;&#32780;&#65292;&#22312;&#28151;&#21512;&#24314;&#27169;&#20013;&#65292;&#31561;&#25928;&#24615;&#21644;&#27491;&#21017;&#21270;&#20559;&#24046;&#23545;&#20110;&#23454;&#29616;&#36825;&#20123;&#30446;&#30340;&#26500;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20272;&#35745;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#20351;&#29992;&#21452;&#26426;&#22120;&#23398;&#20064;(DML)&#26469;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#20004;&#20010;&#28041;&#21450;&#20108;&#27687;&#21270;&#30899;&#36890;&#37327;&#30340;&#22320;&#29699;&#31185;&#23398;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#22312;$Q_{10}$&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;DML&#30340;&#28151;&#21512;&#24314;&#27169;&#27604;&#31471;&#21040;&#31471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26041;&#27861;&#26356;&#20248;&#65292;&#35777;&#26126;&#20102;&#25928;&#29575;&#39640;&#12289;&#40065;&#26834;&#24615;&#24378;&#65292;&#24182;&#19988;&#35268;&#36991;&#20102;&#27491;&#21017;&#21270;&#26041;&#27861;&#24102;&#26469;&#30340;&#20559;&#24046;&#21644;&#31561;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#30899;&#36890;&#37327;&#20998;&#37197;&#65292;&#23637;&#29616;&#20102;&#36866;&#24212;&#19981;&#21516;&#22240;&#26524;&#25928;&#24212;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22240;&#26524;&#28151;&#21512;&#24314;&#27169;&#27010;&#24565;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13332v1 Announce Type: new  Abstract: Hybrid modeling integrates machine learning with scientific knowledge with the goal of enhancing interpretability, generalization, and adherence to natural laws. Nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes. This paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing Double Machine Learning (DML) to estimate causal effects. We showcase its use for the Earth sciences on two problems related to carbon dioxide fluxes. In the $Q_{10}$ model, we demonstrate that DML-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network (DNN) approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality. Our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects. The study emp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#26469;&#35757;&#32451;&#21028;&#21035;&#24615;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#20013;&#30340;&#24050;&#30693;&#27491;&#24120;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.11887</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative Semi-supervised Graph Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11887
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#26469;&#35757;&#32451;&#21028;&#21035;&#24615;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#20013;&#30340;&#24050;&#30693;&#27491;&#24120;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#32771;&#34385;&#20102;&#19968;&#20010;&#23454;&#38469;&#24773;&#22659;&#19979;&#30340;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#65292;&#22312;&#36825;&#20010;&#24773;&#22659;&#20013;&#65292;&#22270;&#20013;&#30340;&#37096;&#20998;&#33410;&#28857;&#34987;&#30693;&#26195;&#26159;&#27491;&#24120;&#30340;&#65292;&#19982;&#22823;&#22810;&#25968;GAD&#30740;&#31350;&#20013;&#20351;&#29992;&#23436;&#20840;&#26410;&#26631;&#35760;&#22270;&#30340;&#26080;&#30417;&#30563;&#24773;&#20917;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#26377;&#21161;&#20110;&#25552;&#21319;&#29616;&#26377;&#26080;&#30417;&#30563;GAD&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#24773;&#22659;&#19979;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#30340;&#21033;&#29992;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#21322;&#30417;&#30563;&#24773;&#22659;&#30340;&#29983;&#25104;&#24335;GAD&#26041;&#27861;&#65288;GGAD&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#30340;&#24322;&#24120;&#33410;&#28857;&#65292;&#23427;&#20204;&#34701;&#21512;&#20102;&#26412;&#22320;&#32467;&#26500;&#21644;&#33410;&#28857;&#34920;&#31034;&#65292;&#20026;&#35757;&#32451;&#21028;&#21035;&#22411;&#21333;&#31867;&#20998;&#31867;&#22120;&#25552;&#20379;&#26377;&#25928;&#30340;&#36127;&#38754;&#33410;&#28857;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11887v1 Announce Type: new  Abstract: This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and 
&lt;/p&gt;</description></item><item><title>SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.11322</link><description>&lt;p&gt;
SpikeNAS: &#19968;&#31181;&#38754;&#21521;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11322
&lt;/p&gt;
&lt;p&gt;
SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#37117;&#28304;&#33258;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31070;&#32463;&#20803;&#30340;&#26550;&#26500;&#21644;&#25805;&#20316;&#19982;SNN&#19981;&#21516;&#65292;&#25110;&#32773;&#22312;&#19981;&#32771;&#34385;&#26469;&#33258;&#24213;&#23618;&#22788;&#29702;&#30828;&#20214;&#30340;&#20869;&#23384;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#12290;&#36825;&#20123;&#38480;&#21046;&#38459;&#30861;&#20102;SNN&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20805;&#20998;&#21457;&#25381;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpikeNAS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#65292;&#21487;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#24555;&#36895;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;SNN&#26550;&#26500;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;SpikeNAS&#37319;&#29992;&#20102;&#20960;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#20998;&#26512;&#32593;&#32476;&#25805;&#20316;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#22686;&#24378;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23398;&#20064;&#36136;&#37327;&#65292;&#24182;&#24320;&#21457;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11322v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#27604;KL PAC-Bayes&#30028;&#38480;&#26041;&#27861;&#26469;&#20272;&#35745;&#24207;&#21015;&#22343;&#20540;&#65292;&#24212;&#29992;&#20110;&#39044;&#27979;&#22120;&#27867;&#21270;&#35823;&#24046;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.09201</link><description>&lt;p&gt;
&#26356;&#22909;&#30340;&#27604;KL PAC-Bayes&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Better-than-KL PAC-Bayes Bounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#27604;KL PAC-Bayes&#30028;&#38480;&#26041;&#27861;&#26469;&#20272;&#35745;&#24207;&#21015;&#22343;&#20540;&#65292;&#24212;&#29992;&#20110;&#39044;&#27979;&#22120;&#27867;&#21270;&#35823;&#24046;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;$f(\theta, X_1),$ $ \dots,$ $ f(\theta, X_n)$&#25104;&#20026;&#19968;&#20010;&#38543;&#26426;&#20803;&#32032;&#24207;&#21015;&#65292;&#20854;&#20013;$f$&#26159;&#19968;&#20010;&#22266;&#23450;&#30340;&#26631;&#37327;&#20989;&#25968;&#65292;$X_1, \dots, X_n$&#26159;&#29420;&#31435;&#30340;&#38543;&#26426;&#21464;&#37327;&#65288;&#25968;&#25454;&#65289;&#65292;&#32780;$\theta$&#26159;&#26681;&#25454;&#19968;&#20123;&#25968;&#25454;&#30456;&#20851;&#30340;&#21518;&#39564;&#20998;&#24067;$P_n$&#20998;&#24067;&#30340;&#38543;&#26426;&#21442;&#25968;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#35777;&#26126;&#27987;&#24230;&#19981;&#31561;&#24335;&#26469;&#20272;&#35745;&#24207;&#21015;&#22343;&#20540;&#30340;&#38382;&#39064;&#12290;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#20363;&#23376;&#26159;&#23545;&#26576;&#20123;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#35757;&#32451;&#30340;&#39044;&#27979;&#22120;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#20272;&#35745;&#65292;&#27604;&#22914;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;$f$&#26159;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159;&#36890;&#36807;PAC-Bayes&#20998;&#26512;&#26469;&#35299;&#20915;&#30340;&#65292;&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#65292;&#38500;&#20102;&#21518;&#39564;&#20998;&#24067;&#65292;&#25105;&#20204;&#36824;&#36873;&#25321;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21040;&#23398;&#20064;&#38382;&#39064;&#24402;&#32435;&#20559;&#24046;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;PAC-Bayes&#27987;&#24230;&#30028;&#38480;&#20013;&#30340;&#20851;&#38190;&#25968;&#37327;&#26159;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21040;&#23398;&#20064;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09201v1 Announce Type: new Abstract: Let $f(\theta, X_1),$ $ \dots,$ $ f(\theta, X_n)$ be a sequence of random elements, where $f$ is a fixed scalar function, $X_1, \dots, X_n$ are independent random variables (data), and $\theta$ is a random parameter distributed according to some data-dependent posterior distribution $P_n$. In this paper, we consider the problem of proving concentration inequalities to estimate the mean of the sequence. An example of such a problem is the estimation of the generalization error of some predictor trained by a stochastic algorithm, such as a neural network where $f$ is a loss function. Classically, this problem is approached through a PAC-Bayes analysis where, in addition to the posterior, we choose a prior distribution which captures our belief about the inductive bias of the learning problem. Then, the key quantity in PAC-Bayes concentration bounds is a divergence that captures the complexity of the learning problem where the de facto stand
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#25968;&#20248;&#21270;&#23454;&#29616;&#24352;&#37327;&#34917;&#20840;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#32447;&#24615;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#25910;&#25947;&#24182;&#36798;&#21040;&#20102;&#20449;&#24687;&#29702;&#35770;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05141</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#25968;&#20248;&#21270;&#23454;&#29616;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Tensor Completion via Integer Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#25968;&#20248;&#21270;&#23454;&#29616;&#24352;&#37327;&#34917;&#20840;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#32447;&#24615;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#25910;&#25947;&#24182;&#36798;&#21040;&#20102;&#20449;&#24687;&#29702;&#35770;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35745;&#31639;&#33021;&#21147;&#21644;&#20449;&#24687;&#29702;&#35770;&#26679;&#26412;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#22522;&#26412;&#24352;&#21147;&#12290;&#36807;&#21435;&#30340;&#26041;&#27861;&#35201;&#20040;&#36798;&#21040;&#20102;&#20449;&#24687;&#29702;&#35770;&#30340;&#36895;&#29575;&#20294;&#32570;&#20047;&#35745;&#31639;&#30456;&#24212;&#35299;&#30340;&#23454;&#38469;&#31639;&#27861;&#65292;&#35201;&#20040;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20294;&#38656;&#35201;&#25351;&#25968;&#32423;&#26356;&#22810;&#30340;&#26679;&#26412;&#20197;&#36798;&#21040;&#20302;&#20272;&#35745;&#35823;&#24046;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#25968;&#30446;&#30340;oracle&#27493;&#39588;&#20013;&#21516;&#26102;&#23454;&#29616;&#21487;&#35777;&#30340;&#25910;&#25947;&#65288;&#22312;&#25968;&#23383;&#23481;&#24046;&#26041;&#38754;&#65289;&#21644;&#20449;&#24687;&#29702;&#35770;&#36895;&#29575;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#24352;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#35268;&#33539;&#30340;&#24352;&#37327;&#33539;&#25968;&#26500;&#36896;&#20102;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#23450;&#20041;&#20102;&#19968;&#31181;&#20801;&#35768;&#20351;&#29992;&#25972;&#25968;&#32447;&#24615;&#20248;&#21270;&#22312;&#36825;&#31181;&#26032;&#33539;&#25968;&#19979;&#35299;&#20915;&#32447;&#24615;&#20998;&#31163;&#38382;&#39064;&#30340;&#35268;&#33539;&#12290;&#22522;&#20110;&#36825;&#20010;&#27934;&#23519;&#21147;&#30340;&#35843;&#25972;&#34987;&#32435;&#20837;&#21040;&#19968;&#31181;&#22522;&#20110;Frank-Wolfe&#21464;&#20307;&#30340;&#31639;&#27861;&#20013;&#26469;&#26500;&#24314;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main challenge with the tensor completion problem is a fundamental tension between computation power and the information-theoretic sample complexity rate. Past approaches either achieve the information-theoretic rate but lack practical algorithms to compute the corresponding solution, or have polynomial-time algorithms that require an exponentially-larger number of samples for low estimation error. This paper develops a novel tensor completion algorithm that resolves this tension by achieving both provable convergence (in numerical tolerance) in a linear number of oracle steps and the information-theoretic rate. Our approach formulates tensor completion as a convex optimization problem constrained using a gauge-based tensor norm, which is defined in a way that allows the use of integer linear optimization to solve linear separation problems over the unit-ball in this new norm. Adaptations based on this insight are incorporated into a Frank-Wolfe variant to build our algorithm. We s
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#23398;&#20064;&#30340;&#36793;&#32536;&#32531;&#23384;&#26694;&#26550;HR-Cache&#33021;&#22815;&#20248;&#21270;&#36793;&#32536;&#32531;&#23384;&#65292;&#25552;&#39640;&#23383;&#33410;&#21629;&#20013;&#29575;&#65292;&#38477;&#20302;&#32593;&#32476;&#36127;&#36733;&#65292;&#24182;&#21152;&#36895;&#20869;&#23481;&#20256;&#36882;&#32473;&#29992;&#25143;&#12290;</title><link>https://arxiv.org/abs/2402.02795</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#36793;&#32536;&#20869;&#23481;&#20256;&#36882;&#32531;&#23384;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Learning-Based Caching Mechanism for Edge Content Delivery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02795
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#36793;&#32536;&#32531;&#23384;&#26694;&#26550;HR-Cache&#33021;&#22815;&#20248;&#21270;&#36793;&#32536;&#32531;&#23384;&#65292;&#25552;&#39640;&#23383;&#33410;&#21629;&#20013;&#29575;&#65292;&#38477;&#20302;&#32593;&#32476;&#36127;&#36733;&#65292;&#24182;&#21152;&#36895;&#20869;&#23481;&#20256;&#36882;&#32473;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;5G&#32593;&#32476;&#30340;&#20852;&#36215;&#21644;&#29289;&#32852;&#32593;(IoT)&#30340;&#21457;&#23637;&#65292;&#20869;&#23481;&#20256;&#36882;&#32593;&#32476;(CDNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#25193;&#23637;&#21040;&#32593;&#32476;&#36793;&#32536;&#12290;&#36825;&#31181;&#36716;&#21464;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#36793;&#32536;&#30340;&#26377;&#38480;&#32531;&#23384;&#23384;&#20648;&#21644;&#22810;&#26679;&#21270;&#30340;&#35831;&#27714;&#27169;&#24335;&#12290;&#36793;&#32536;&#29615;&#22659;&#21487;&#20197;&#25176;&#31649;&#20855;&#26377;&#19981;&#21516;&#23545;&#35937;&#22823;&#23567;&#20998;&#24067;&#21644;&#23545;&#35937;&#35775;&#38382;&#27169;&#24335;&#30340;&#27969;&#37327;&#31867;&#21035;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#20351;&#24471;&#20256;&#32479;&#30340;&#32531;&#23384;&#31574;&#30053;&#24456;&#38590;&#21457;&#25381;&#20316;&#29992;&#65292;&#20256;&#32479;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#35831;&#27714;&#39057;&#29575;&#25110;&#26102;&#38388;&#38388;&#38548;&#31561;&#25351;&#26631;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#20248;&#21270;&#36793;&#32536;&#32531;&#23384;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36793;&#32536;&#23454;&#29616;&#26356;&#39640;&#30340;&#23383;&#33410;&#21629;&#20013;&#29575;&#19981;&#20165;&#21487;&#20197;&#20943;&#36731;&#32593;&#32476;&#39592;&#24178;&#30340;&#36127;&#36733;&#65292;&#36824;&#21487;&#20197;&#26368;&#23567;&#21270;&#36816;&#33829;&#25104;&#26412;&#24182;&#21152;&#24555;&#20869;&#23481;&#20256;&#36882;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HR-Cache&#30340;&#32508;&#21512;&#23398;&#20064;&#32531;&#23384;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#21361;&#38505;&#29575;(Hazard Rate)&#25490;&#24207;&#21407;&#21017;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#21021;&#29992;&#26469;&#35745;&#31639;&#24453;&#21629;&#26102;&#38388;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of 5G networks and the rise of the Internet of Things (IoT), Content Delivery Networks (CDNs) are increasingly extending into the network edge. This shift introduces unique challenges, particularly due to the limited cache storage and the diverse request patterns at the edge. These edge environments can host traffic classes characterized by varied object-size distributions and object-access patterns. Such complexity makes it difficult for traditional caching strategies, which often rely on metrics like request frequency or time intervals, to be effective. Despite these complexities, the optimization of edge caching is crucial. Improved byte hit rates at the edge not only alleviate the load on the network backbone but also minimize operational costs and expedite content delivery to end-users.   In this paper, we introduce HR-Cache, a comprehensive learning-based caching framework grounded in the principles of Hazard Rate (HR) ordering, a rule originally formulated to com
&lt;/p&gt;</description></item><item><title>KVQuant&#26159;&#19968;&#31181;&#35299;&#20915;LLM&#25512;&#29702;&#20013;&#22823;&#37327;&#20869;&#23384;&#28040;&#32791;&#30340;KV&#32531;&#23384;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#12289;RoPE&#21069;&#37327;&#21270;&#38190;&#21644;&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#30340;KV&#28608;&#27963;&#12290;</title><link>https://arxiv.org/abs/2401.18079</link><description>&lt;p&gt;
KVQuant: &#20197;KV&#32531;&#23384;&#37327;&#21270;&#23454;&#29616;1000&#19975;&#19978;&#19979;&#25991;&#38271;&#24230;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18079
&lt;/p&gt;
&lt;p&gt;
KVQuant&#26159;&#19968;&#31181;&#35299;&#20915;LLM&#25512;&#29702;&#20013;&#22823;&#37327;&#20869;&#23384;&#28040;&#32791;&#30340;KV&#32531;&#23384;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#12289;RoPE&#21069;&#37327;&#21270;&#38190;&#21644;&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#30340;KV&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#25688;&#35201;&#31561;&#38656;&#35201;&#22823;&#31383;&#21475;&#19978;&#19979;&#25991;&#30340;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;KV&#32531;&#23384;&#28608;&#27963;&#25104;&#20026;&#35760;&#24518;&#28040;&#32791;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#12290;&#37327;&#21270;&#26159;&#19968;&#31181;&#21387;&#32553;KV&#32531;&#23384;&#28608;&#27963;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#65288;&#22914;&#20302;&#20110;4&#20301;&#65289;&#30340;&#28608;&#27963;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KVQuant&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#26041;&#27861;&#37327;&#21270;&#32531;&#23384;&#30340;KV&#28608;&#27963;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#65306;(i)&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#65292;&#22312;&#37327;&#21270;&#38190;&#28608;&#27963;&#26102;&#35843;&#25972;&#32500;&#24230;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20998;&#24067;&#65307;(ii)RoPE&#21069;&#37327;&#21270;&#38190;&#65292;&#22312;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#20043;&#21069;&#37327;&#21270;&#38190;&#28608;&#27963;&#20197;&#20943;&#36731;&#20854;&#23545;&#37327;&#21270;&#30340;&#24433;&#21709;&#65307;(iii)&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#22312;&#27599;&#23618;&#25512;&#23548;&#20986;&#26435;&#37325;&#24863;&#30693;&#30340;&#38750;&#22343;&#21248;&#25968;&#25454;&#31867;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#19981;&#21516;&#23618;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.15496</link><description>&lt;p&gt;
Baichuan2-Sum: &#20351;&#29992;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;Llama&#12289;Baichuan&#21644;Bloom&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#30340;&#19981;&#21516;&#35282;&#33394;&#29983;&#25104;&#25688;&#35201;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#23567;&#27169;&#22411;&#65288;&#20363;&#22914;Bart&#21644;Bert&#65289;&#36827;&#34892;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#22312;&#23567;&#27169;&#22411;&#19978;&#28155;&#21152;&#20219;&#21153;&#25351;&#23450;&#30340;&#20248;&#21270;&#65292;&#22914;&#21521;&#27169;&#22411;&#28155;&#21152;&#20840;&#23616;-&#23616;&#37096;&#20013;&#24515;&#24230;&#24471;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#65306;Baichuan2-Sum&#65292;&#29992;&#20110;&#38754;&#21521;&#35282;&#33394;&#30340;&#23545;&#35805;&#25688;&#35201;&#12290;&#36890;&#36807;&#20026;&#19981;&#21516;&#35282;&#33394;&#35774;&#32622;&#19981;&#21516;&#30340;&#25351;&#20196;&#65292;&#27169;&#22411;&#21487;&#20197;&#20174;&#23545;&#35805;&#20132;&#20114;&#20013;&#23398;&#20064;&#24182;&#36755;&#20986;&#26399;&#26395;&#30340;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;NEFTune&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#21512;&#36866;&#30340;&#22122;&#22768;&#20197;&#25552;&#39640;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#20844;&#24320;&#30340;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#38598;CSDS&#21644;SAMSUM&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods conduct on small models (e.g Bart and Bert). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the expected summaries. Furthermore, we applied NEFTune technique to add suitable noise during training to improve the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.01037</link><description>&lt;p&gt;
&#20174;&#21476;&#24618;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge from Quirky Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;&#65288;ELK&#65289;&#26088;&#22312;&#22312;&#19968;&#20010;&#33021;&#21147;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#20013;&#25214;&#21040;&#27169;&#24335;&#65292;&#21363;&#20351;&#32593;&#32476;&#30340;&#26126;&#26174;&#36755;&#20986;&#26159;&#38169;&#35823;&#25110;&#35823;&#23548;&#24615;&#30340;&#65292;&#20063;&#33021;&#31283;&#23450;&#36319;&#36394;&#19990;&#30028;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;ELK&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;12&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#22871;&#30456;&#24212;&#30340;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#21482;&#26377;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#20851;&#38190;&#35789;&#8220;Bob&#8221;&#26102;&#25165;&#20250;&#36827;&#34892;&#31995;&#32479;&#24615;&#38169;&#35823;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25506;&#27979;&#26041;&#27861;&#21487;&#20197;&#35843;&#21462;&#27169;&#22411;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#23545;&#27491;&#30830;&#31572;&#26696;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#21363;&#20351;&#38382;&#39064;&#27604;&#25506;&#27979;&#22120;&#35757;&#32451;&#30340;&#38382;&#39064;&#26356;&#22256;&#38590;&#12290;&#36825;&#26159;&#30001;&#20110;&#20013;&#38388;&#23618;&#28608;&#27963;&#20013;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19968;&#31181;&#26426;&#26800;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#20197;94%&#30340;AUROC&#26631;&#35782;&#19981;&#30495;&#23454;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20174;&#33021;&#21147;&#24378;&#20294;&#19981;&#21463;&#20449;&#20219;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;ELK&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;</title><link>https://arxiv.org/abs/2310.00492</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#25351;&#20196;&#36319;&#38543;&#65306;&#29702;&#35299;&#25351;&#20196;&#35843;&#25972;&#21518;LLMs&#20013;&#34892;&#20026;&#30340;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00492
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#25351;&#20196;&#35843;&#25972;&#26159;&#23558;LLMs&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#22914;&#20309;&#35843;&#25972;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#20869;&#22312;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#20960;&#31181;&#26412;&#22320;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36755;&#20837;&#36755;&#20986;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#21450;&#29992;&#20110;&#35299;&#37322;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#23618;&#20013;&#30340;&#27169;&#24335;&#21644;&#27010;&#24565;&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#36890;&#36807;&#27604;&#36739;&#20174;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#35299;&#37322;&#26469;&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20154;&#21487;&#29702;&#35299;&#30340;&#27700;&#24179;&#19978;&#25552;&#20379;&#20102;&#27169;&#22411;&#36716;&#21464;&#30340;&#20869;&#37096;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#23427;&#20351;LLMs&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65292;&#24182;&#19981;&#26029;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00492v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts from user prompts, and promotes the response generation constantly condition
&lt;/p&gt;</description></item><item><title>TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14694</link><description>&lt;p&gt;
TA-RNN&#65306;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38754;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26102;&#38388;&#24863;&#30693;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records. (arXiv:2401.14694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14694
&lt;/p&gt;
&lt;p&gt;
TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#20840;&#38754;&#36164;&#28304;&#12290;EHR&#23545;&#20110;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31561;&#20808;&#36827;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#21307;&#30103;&#25552;&#20379;&#32773;&#33021;&#22815;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#65292;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#20570;&#20986;&#31934;&#30830;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#20020;&#24202;&#20915;&#31574;&#12290;DL&#26041;&#27861;&#22914;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#24050;&#34987;&#29992;&#20110;&#20998;&#26512;EHR&#20197;&#24314;&#27169;&#30142;&#30149;&#36827;&#23637;&#24182;&#39044;&#27979;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#35299;&#20915;EHR&#25968;&#25454;&#20013;&#19968;&#20123;&#22266;&#26377;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#22914;&#20020;&#24202;&#35775;&#38382;&#20043;&#38388;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;DL&#27169;&#22411;&#37117;&#19981;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;DL&#26550;&#26500;&#65292;&#20998;&#21035;&#26159;&#26102;&#38388;&#24863;&#30693;RNN&#65288;TA-RNN&#65289;&#21644;TA-RNN-Autoencoder&#65288;TA-RNN-AE&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#27425;&#35775;&#38382;&#21644;&#22810;&#27425;&#26410;&#26469;&#35775;&#38382;&#20013;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#23558;&#26102;&#38388;&#20449;&#24687;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elaps
&lt;/p&gt;</description></item><item><title>RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09432</link><description>&lt;p&gt;
RoleCraft-GLM&#65306;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models. (arXiv:2401.09432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09432
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;RoleCraft-GLM&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;RoleCraft-GLM&#35299;&#20915;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#35814;&#32454;&#25551;&#32472;&#24773;&#24863;&#32454;&#33147;&#30340;&#35282;&#33394;&#21051;&#30011;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#32452;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#20174;&#20256;&#32479;&#30340;&#20197;&#21517;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#33394;&#36716;&#21464;&#20026;&#22810;&#26679;&#21270;&#30340;&#38750;&#21517;&#20154;&#35282;&#33394;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#35821;&#35328;&#24314;&#27169;&#20114;&#21160;&#30340;&#30495;&#23454;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#30830;&#20445;&#23545;&#35805;&#26082;&#30495;&#23454;&#21448;&#24773;&#24863;&#20849;&#40483;&#12290;&#36890;&#36807;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;RoleCraft-GLM&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#24635;&#20043;&#65292;RoleCraft-GLM&#26631;&#24535;&#30528;&#19968;&#20010;&#21019;&#26032;&#30340;&#37324;&#31243;&#30865;&#65292;&#25512;&#21160;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a sign
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MarketSenseAI&#65292;&#19968;&#20010;&#21033;&#29992;GPT-4&#36827;&#34892;&#32929;&#31080;&#36873;&#25321;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#28304;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#20379;&#20855;&#26377;&#21487;&#34892;&#35299;&#37322;&#30340;&#25237;&#36164;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2401.03737</link><description>&lt;p&gt;
&#33021;&#21542;&#25171;&#36133;&#21326;&#23572;&#34903;&#65311;&#25581;&#31034;&#20154;&#24037;&#26234;&#33021;&#22312;&#32929;&#31080;&#36873;&#25321;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection. (arXiv:2401.03737v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MarketSenseAI&#65292;&#19968;&#20010;&#21033;&#29992;GPT-4&#36827;&#34892;&#32929;&#31080;&#36873;&#25321;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#28304;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#20379;&#20855;&#26377;&#21487;&#34892;&#35299;&#37322;&#30340;&#25237;&#36164;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#24066;&#22330;&#21160;&#24577;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#29615;&#22659;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;MarketSenseAI&#65292;&#19968;&#20010;&#21033;&#29992;GPT-4&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#21487;&#25193;&#23637;&#32929;&#31080;&#36873;&#25321;&#30340;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#12290;MarketSenseAI&#25972;&#21512;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#21644;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#26041;&#27861;&#65292;&#20998;&#26512;&#21253;&#25324;&#24066;&#22330;&#20215;&#26684;&#21160;&#24577;&#12289;&#36130;&#32463;&#26032;&#38395;&#12289;&#20844;&#21496;&#22522;&#26412;&#38754;&#21644;&#23439;&#35266;&#32463;&#27982;&#25253;&#21578;&#31561;&#22810;&#31181;&#25968;&#25454;&#28304;&#65292;&#27169;&#20223;&#30693;&#21517;&#37329;&#34701;&#25237;&#36164;&#22242;&#38431;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;MarketSenseAI&#30340;&#24320;&#21457;&#12289;&#23454;&#26045;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#25552;&#20379;&#20855;&#26377;&#20805;&#20998;&#35299;&#37322;&#25903;&#25745;&#30340;&#21487;&#34892;&#25237;&#36164;&#20449;&#21495;&#65288;&#20080;&#20837;&#12289;&#25345;&#26377;&#12289;&#21334;&#20986;&#65289;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#20351;&#29992;GPT-4&#19981;&#20165;&#20316;&#20026;&#39044;&#27979;&#24037;&#20855;&#65292;&#36824;&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#25152;&#24314;&#35758;&#30340;&#25237;&#36164;&#20449;&#21495;&#30340;&#21487;&#38752;&#24615;&#21644;&#25509;&#21463;&#24230;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
In the dynamic and data-driven landscape of financial markets, this paper introduces MarketSenseAI, a novel AI-driven framework leveraging the advanced reasoning capabilities of GPT-4 for scalable stock selection. MarketSenseAI incorporates Chain of Thought and In-Context Learning methodologies to analyze a wide array of data sources, including market price dynamics, financial news, company fundamentals, and macroeconomic reports emulating the decision making process of prominent financial investment teams. The development, implementation, and empirical validation of MarketSenseAI are detailed, with a focus on its ability to provide actionable investment signals (buy, hold, sell) backed by cogent explanations. A notable aspect of this study is the use of GPT-4 not only as a predictive tool but also as an evaluator, revealing the significant impact of the AI-generated explanations on the reliability and acceptance of the suggested investment signals. In an extensive empirical evaluation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MMM&#21644;MMMSynth&#31639;&#27861;&#65292;&#29992;&#20110;&#32858;&#31867;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#21644;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;MMM&#31639;&#27861;&#21033;&#29992;EM&#31639;&#27861;&#65292;&#22312;&#21516;&#31867;&#31639;&#27861;&#20013;&#34920;&#29616;&#26356;&#20248;&#65292;&#23545;&#20110;&#30830;&#23450;&#21512;&#25104;&#25968;&#25454;&#30340;&#32858;&#31867;&#20197;&#21450;&#24674;&#22797;&#30495;&#23454;&#25968;&#25454;&#30340;&#32467;&#26500;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290; MMMSynth&#31639;&#27861;&#21017;&#29992;&#20110;&#20174;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.19454</link><description>&lt;p&gt;
MMM&#21644;MMMSynth&#65306;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32858;&#31867;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MMM and MMMSynth: Clustering of heterogeneous tabular data, and synthetic data generation. (arXiv:2310.19454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MMM&#21644;MMMSynth&#31639;&#27861;&#65292;&#29992;&#20110;&#32858;&#31867;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#21644;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;MMM&#31639;&#27861;&#21033;&#29992;EM&#31639;&#27861;&#65292;&#22312;&#21516;&#31867;&#31639;&#27861;&#20013;&#34920;&#29616;&#26356;&#20248;&#65292;&#23545;&#20110;&#30830;&#23450;&#21512;&#25104;&#25968;&#25454;&#30340;&#32858;&#31867;&#20197;&#21450;&#24674;&#22797;&#30495;&#23454;&#25968;&#25454;&#30340;&#32467;&#26500;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290; MMMSynth&#31639;&#27861;&#21017;&#29992;&#20110;&#20174;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#19982;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30456;&#20851;&#30340;&#20219;&#21153;&#30340;&#26032;&#31639;&#27861;&#65306;&#32858;&#31867;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#34920;&#26684;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#21015;&#20013;&#30340;&#24322;&#26500;&#25968;&#25454;&#31867;&#22411;&#65288;&#25968;&#20540;&#12289;&#26377;&#24207;&#12289;&#20998;&#31867;&#65289;&#32452;&#25104;&#65292;&#20294;&#34892;&#20013;&#21487;&#33021;&#36824;&#23384;&#22312;&#38544;&#34255;&#30340;&#32858;&#31867;&#32467;&#26500;&#65306;&#20363;&#22914;&#65292;&#23427;&#20204;&#21487;&#33021;&#26469;&#33258;&#24322;&#26500;&#30340;&#65288;&#22320;&#29702;&#12289;&#31038;&#20250;&#32463;&#27982;&#12289;&#26041;&#27861;&#35770;&#65289;&#26469;&#28304;&#65292;&#22240;&#27492;&#25152;&#25551;&#36848;&#30340;&#32467;&#26524;&#21464;&#37327;&#65288;&#22914;&#30142;&#30149;&#30340;&#23384;&#22312;&#65289;&#21487;&#33021;&#19981;&#20165;&#20381;&#36182;&#20854;&#20182;&#21464;&#37327;&#65292;&#36824;&#20381;&#36182;&#20110;&#32858;&#31867;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#21307;&#23398;&#25968;&#25454;&#30340;&#20849;&#20139;&#36890;&#24120;&#21463;&#21040;&#24739;&#32773;&#38544;&#31169;&#27861;&#24459;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#30446;&#21069;&#23545;&#20110;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31561;&#26041;&#27861;&#20174;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#30340;&#31639;&#27861;&#38750;&#24120;&#24863;&#20852;&#36259;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;EM&#30340;&#32858;&#31867;&#31639;&#27861;MMM&#65288;&#8220;Madras&#28151;&#21512;&#27169;&#22411;&#8221;&#65289;&#65292;&#23427;&#22312;&#30830;&#23450;&#21512;&#25104;&#24322;&#26500;&#25968;&#25454;&#30340;&#32858;&#31867;&#21644;&#24674;&#22797;&#30495;&#23454;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#31639;&#27861;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#21487;&#23558;MMM&#24212;&#29992;&#20110;&#25968;&#25454;&#21512;&#25104;&#20219;&#21153;&#30340;MMMSynth&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide new algorithms for two tasks relating to heterogeneous tabular datasets: clustering, and synthetic data generation. Tabular datasets typically consist of heterogeneous data types (numerical, ordinal, categorical) in columns, but may also have hidden cluster structure in their rows: for example, they may be drawn from heterogeneous (geographical, socioeconomic, methodological) sources, such that the outcome variable they describe (such as the presence of a disease) may depend not only on the other variables but on the cluster context. Moreover, sharing of biomedical data is often hindered by patient confidentiality laws, and there is current interest in algorithms to generate synthetic tabular data from real data, for example via deep learning.  We demonstrate a novel EM-based clustering algorithm, MMM (``Madras Mixture Model''), that outperforms standard algorithms in determining clusters in synthetic heterogeneous data, and recovers structure in real data. Based on this, we
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12508</link><description>&lt;p&gt;
SalUn&#65306;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26435;&#37325;&#26174;&#33879;&#24615;&#22686;&#24378;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#27861;&#35268;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#24403;&#21069;AI&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MU&#26041;&#27861;&#36890;&#24120;&#22312;&#36951;&#24536;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36328;&#39046;&#22495;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MU&#20013;&#30340;&#8220;&#26435;&#37325;&#26174;&#33879;&#24615;&#8221;&#27010;&#24565;&#65292;&#20511;&#37492;&#20102;&#27169;&#22411;&#35299;&#37322;&#20013;&#30340;&#36755;&#20837;&#26174;&#33879;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#23558;MU&#30340;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20102;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26174;&#33879;&#24615;&#36951;&#24536;&#65288;SalUn&#65289;&#30340;&#26041;&#27861;&#23558;&#20854;&#19982;&#8220;&#31934;&#30830;&#8221;&#36951;&#24536;&#65288;&#22312;&#21024;&#38500;&#36951;&#24536;&#25968;&#25454;&#38598;&#21518;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SalUn&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;MU&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;SalUn&#21487;&#22312;&#22270;&#29255;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
&lt;/p&gt;</description></item><item><title>L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.02003</link><description>&lt;p&gt;
L2MAC&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35745;&#31639;&#26426;&#29992;&#20110;&#26080;&#38480;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02003
&lt;/p&gt;
&lt;p&gt;
L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#24213;&#23618;Transformer&#26550;&#26500;&#22266;&#23450;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#22686;&#24378;&#35760;&#24518;&#30340;LLM&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#35835;&#21462;&#20869;&#23384;&#24182;&#23558;&#20854;&#28436;&#21464;&#20026;&#26032;&#20869;&#23384;&#30340;&#36830;&#25509;&#65292;&#35201;&#20040;&#20351;&#29992;&#38750;&#24120;&#19987;&#38376;&#30340;&#20869;&#23384;&#65292;&#26080;&#27861;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;L2MAC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38271;&#19988;&#19968;&#33268;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#29992;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#12290;&#23427;&#30340;&#20869;&#23384;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#20196;&#27880;&#20876;&#34920;&#65292;&#20854;&#20013;&#22635;&#20805;&#20102;&#19968;&#20010;&#35299;&#20915;&#29992;&#25143;&#32473;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#31243;&#24207;&#65292;&#20197;&#21450;&#25991;&#20214;&#23384;&#20648;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#32456;&#21644;&#20013;&#38388;&#36755;&#20986;&#12290;&#27599;&#20010;&#25351;&#20196;&#30001;&#21333;&#29420;&#30340;LLM&#23454;&#20363;&#25191;&#34892;&#65292;&#20854;&#19978;&#19979;&#25991;&#30001;&#25511;&#21046;&#21333;&#20803;&#31649;&#29702;&#65292;&#33021;&#22815;&#31934;&#30830;&#35835;&#21462;&#21644;&#20889;&#20837;&#20869;&#23384;&#65292;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#32593;&#32476;&#20107;&#25925;&#30340;&#30005;&#32593;&#20013;&#30340;&#30005;&#21147;&#27969;&#65292;&#24182;&#20272;&#35745;&#30005;&#21147;&#27969;&#30340;&#27010;&#29575;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#32593;&#32476;&#24863;&#30693;&#39640;&#26031;&#36807;&#31243;&#21644;&#22810;&#20219;&#21153;&#39030;&#28857;&#24230;&#26680;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#32593;&#32476;&#30340;&#30005;&#21147;&#27969;&#25512;&#26029;&#65292;&#24182;&#22312;&#20302;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00763</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#32593;&#32476;&#20107;&#25925;&#30005;&#21147;&#27969;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Power Flow Learning for Network Contingencies. (arXiv:2310.00763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#32593;&#32476;&#20107;&#25925;&#30340;&#30005;&#32593;&#20013;&#30340;&#30005;&#21147;&#27969;&#65292;&#24182;&#20272;&#35745;&#30005;&#21147;&#27969;&#30340;&#27010;&#29575;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#32593;&#32476;&#24863;&#30693;&#39640;&#26031;&#36807;&#31243;&#21644;&#22810;&#20219;&#21153;&#39030;&#28857;&#24230;&#26680;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#32593;&#32476;&#30340;&#30005;&#21147;&#27969;&#25512;&#26029;&#65292;&#24182;&#22312;&#20302;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#32593;&#32476;&#20107;&#25925;&#30340;&#30005;&#32593;&#20013;&#30340;&#30005;&#21147;&#27969;&#65292;&#24182;&#20272;&#35745;&#30456;&#24212;&#30340;&#27010;&#29575;&#24615;&#30005;&#21387;&#21253;&#32476;&#65288;PVE&#65289;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#21069;&#26399;&#30740;&#31350;&#20013;&#24320;&#21457;&#30340;&#32593;&#32476;&#24863;&#30693;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#65292;&#31216;&#20026;&#39030;&#28857;&#24230;&#26680;&#65288;VDK-GP&#65289;&#65292;&#26469;&#20272;&#35745;&#23569;&#25968;&#32593;&#32476;&#37197;&#32622;&#30340;&#30005;&#21387;-&#21151;&#29575;&#20989;&#25968;&#12290;&#25991;&#31456;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#39030;&#28857;&#24230;&#26680;&#65288;MT-VDK&#65289;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;VDK-GP&#27719;&#38598;&#22312;&#19968;&#36215;&#65292;&#20197;&#30830;&#23450;&#26410;&#35265;&#32593;&#32476;&#30340;&#30005;&#21147;&#27969;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#36229;&#21442;&#25968;&#35201;&#27714;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;IEEE 30-Bus&#32593;&#32476;&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#22312;N-1&#21644;N-2&#20107;&#25925;&#24773;&#20917;&#19979;&#34920;&#26126;&#20102;&#30005;&#21147;&#27969;&#30693;&#35782;&#30340;&#20445;&#30041;&#21644;&#20256;&#36882;&#12290;&#22312;&#20302;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65288;50-250&#20010;&#26679;&#26412;&#65289;&#65292;MT-VDK-GP&#26041;&#27861;&#22312;&#26032;&#39062;&#30340;N-1&#20107;&#25925;&#32593;&#32476;&#37197;&#32622;&#19978;&#30340;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#36739;VDK-GP&#20943;&#23569;&#20102;50%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;MT-VDK-GP&#22312;N-1&#20107;&#25925;&#32593;&#32476;&#37197;&#32622;&#30340;&#20302;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65288;50-250&#20010;&#26679;&#26412;&#65289;&#36798;&#21040;&#20102;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#38477;&#20302;50%&#20197;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an efficient data-driven method to learn power flows in grids with network contingencies and to estimate corresponding probabilistic voltage envelopes (PVE). First, a network-aware Gaussian process (GP) termed Vertex-Degree Kernel (VDK-GP), developed in prior work, is used to estimate voltage-power functions for a few network configurations. The paper introduces a novel multi-task vertex degree kernel (MT-VDK) that amalgamates the learned VDK-GPs to determine power flows for unseen networks, with a significant reduction in the computational complexity and hyperparameter requirements compared to alternate approaches. Simulations on the IEEE 30-Bus network demonstrate the retention and transfer of power flow knowledge in both N-1 and N-2 contingency scenarios. The MT-VDK-GP approach achieves over 50% reduction in mean prediction error for novel N-1 contingency network configurations in low training data regimes (50-250 samples) over VDK-GP. Additionally, MT-VDK-GP outp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#25345;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#19981;&#21512;&#29702;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12545</link><description>&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation. (arXiv:2309.12545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#25345;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#19981;&#21512;&#29702;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;(CEs)&#20316;&#20026;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#20027;&#35201;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36890;&#24120;&#65292;CEs&#23545;&#20110;&#36755;&#20837;-&#36755;&#20986;&#23545;&#34987;&#23450;&#20041;&#20026;&#21040;&#36755;&#20837;&#30340;&#26368;&#23567;&#36317;&#31163;&#30340;&#25968;&#25454;&#28857;&#65292;&#20854;&#19982;&#36755;&#20986;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;CEs&#22312;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;(&#27604;&#22914;&#37325;&#26032;&#35757;&#32451;)&#26102;&#24456;&#23481;&#26131;&#34987;&#26080;&#25928;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#21464;&#21270;&#30340;&#33539;&#25968;&#29699;&#30028;&#38480;&#26469;&#35777;&#26126;CEs&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38024;&#23545;&#36825;&#31181;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#19981;&#26159;&#23436;&#20840;&#27491;&#30830;&#30340;&#65292;&#25110;&#32773;&#21487;&#33021;&#29983;&#25104;&#19981;&#21512;&#29702;&#30340;CEs&#65292;&#21363;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#23384;&#22312;&#31163;&#32676;&#20540;&#12290;&#20107;&#23454;&#19978;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#36317;&#31163;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#20445;&#25345;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for proximity and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limi
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#24615;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26377;&#25928;&#22320;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36731;&#37327;&#32423;&#21644;&#20219;&#21153;&#23454;&#39564;&#23460;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08546</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#24615;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Continual Learning with Bayesian Adaptive Moment Regularization. (arXiv:2309.08546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08546
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#24615;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26377;&#25928;&#22320;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36731;&#37327;&#32423;&#21644;&#20219;&#21153;&#23454;&#39564;&#23460;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36861;&#27714;&#38271;&#26399;&#33258;&#20027;&#24615;&#65292;&#26426;&#22120;&#20154;&#20195;&#29702;&#24517;&#39035;&#19981;&#26029;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#24182;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#25345;&#32493;&#23398;&#20064;&#35797;&#22270;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#21363;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;&#23548;&#33268;&#27169;&#22411;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#20808;&#39564;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#26426;&#22120;&#20154;&#24212;&#29992;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#31354;&#38388;&#25928;&#29575;&#19978;&#24456;&#39640;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#20250;&#38543;&#30528;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#29702;&#24819;&#30340;&#29305;&#24615;&#65292;&#20294;&#22522;&#20110;&#20808;&#39564;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#37325;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#22833;&#36133;&#65292;&#22240;&#27492;&#19982;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#28508;&#22312;&#24212;&#29992;&#26041;&#38754;&#26377;&#38480;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#65288;BAdam&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20808;&#39564;&#30340;&#26041;&#27861;&#65292;&#23427;&#26356;&#22909;&#22320;&#32422;&#26463;&#21442;&#25968;&#22686;&#38271;&#65292;&#38477;&#20302;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#20855;&#26377;&#19968;&#31995;&#21015;&#29702;&#24819;&#30340;&#29305;&#24615;&#65292;&#20363;&#22914;&#36731;&#37327;&#32423;&#21644;&#20219;&#21153;&#23454;&#39564;&#23460;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pursuit of long-term autonomy mandates that robotic agents must continuously adapt to their changing environments and learn to solve new tasks. Continual learning seeks to overcome the challenge of catastrophic forgetting, where learning to solve new tasks causes a model to forget previously learnt information. Prior-based continual learning methods are appealing for robotic applications as they are space efficient and typically do not increase in computational complexity as the number of tasks grows. Despite these desirable properties, prior-based approaches typically fail on important benchmarks and consequently are limited in their potential applications compared to their memory-based counterparts. We introduce Bayesian adaptive moment regularization (BAdam), a novel prior-based method that better constrains parameter growth, leading to lower catastrophic forgetting. Our method boasts a range of desirable properties for robotic applications such as being lightweight and task lab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;ADAM&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#25910;&#25947;&#24615;&#65292;&#32473;&#20986;&#20102;&#27493;&#38271;&#36798;&#21040;&#20960;&#20046;&#32943;&#23450;&#28176;&#36817;&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;ADAM&#22312;&#22788;&#29702;&#24179;&#28369;&#38750;&#20984;&#20989;&#25968;&#26102;&#36798;&#21040;&#36817;&#20284;&#20020;&#30028;&#24615;&#25152;&#38656;&#30340;&#36816;&#34892;&#26102;&#38388;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.08339</link><description>&lt;p&gt;
ADAM&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#25910;&#25947;&#24615;&#65306;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Convergence of ADAM with Constant Step Size in Non-Convex Settings: A Simple Proof. (arXiv:2309.08339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;ADAM&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#25910;&#25947;&#24615;&#65292;&#32473;&#20986;&#20102;&#27493;&#38271;&#36798;&#21040;&#20960;&#20046;&#32943;&#23450;&#28176;&#36817;&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;ADAM&#22312;&#22788;&#29702;&#24179;&#28369;&#38750;&#20984;&#20989;&#25968;&#26102;&#36798;&#21040;&#36817;&#20284;&#20020;&#30028;&#24615;&#25152;&#38656;&#30340;&#36816;&#34892;&#26102;&#38388;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#65292;RMSProp&#21644;ADAM&#20173;&#28982;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#23427;&#20204;&#30340;&#24615;&#33021;&#20851;&#38190;&#20043;&#19968;&#22312;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;&#27493;&#38271;&#65292;&#36825;&#20250;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#22240;&#36873;&#25321;&#30340;&#27493;&#38271;&#32780;&#21464;&#21270;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#23427;&#20204;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#35805;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#23545;ADAM&#30340;&#24658;&#23450;&#27493;&#38271;&#29256;&#26412;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27493;&#38271;&#36798;&#21040;&#20960;&#20046;&#32943;&#23450;&#28176;&#36817;&#25910;&#25947;&#21040;&#38646;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#32780;&#21482;&#38656;&#26368;&#23567;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#30830;&#23450;&#24615;ADAM&#22312;&#22788;&#29702;&#24179;&#28369;&#38750;&#20984;&#20989;&#25968;&#26102;&#36798;&#21040;&#36817;&#20284;&#20020;&#30028;&#24615;&#25152;&#38656;&#30340;&#36816;&#34892;&#26102;&#38388;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In neural network training, RMSProp and ADAM remain widely favoured optimization algorithms. One of the keys to their performance lies in selecting the correct step size, which can significantly influence their effectiveness. It is worth noting that these algorithms performance can vary considerably, depending on the chosen step sizes. Additionally, questions about their theoretical convergence properties continue to be a subject of interest. In this paper, we theoretically analyze a constant stepsize version of ADAM in the non-convex setting. We show sufficient conditions for the stepsize to achieve almost sure asymptotic convergence of the gradients to zero with minimal assumptions. We also provide runtime bounds for deterministic ADAM to reach approximate criticality when working with smooth, non-convex functions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SemiGCL&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#26368;&#23567;&#26368;&#22823;&#29109;&#35757;&#32451;&#26469;&#35299;&#20915;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#24471;&#34920;&#31034;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#20248;&#21270;&#20943;&#23567;&#22495;&#24046;&#24322;&#12290;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07402</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#26368;&#23567;&#26368;&#22823;&#29109;&#30340;&#22270;&#19978;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Domain Adaptation on Graphs with Contrastive Learning and Minimax Entropy. (arXiv:2309.07402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SemiGCL&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#26368;&#23567;&#26368;&#22823;&#29109;&#35757;&#32451;&#26469;&#35299;&#20915;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#24471;&#34920;&#31034;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#20248;&#21270;&#20943;&#23567;&#22495;&#24046;&#24322;&#12290;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#26631;&#35760;&#25104;&#26412;&#39640;&#26114;&#65292;&#22270;&#20013;&#30340;&#26631;&#31614;&#31232;&#32570;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#36935;&#21040;&#12290;&#20026;&#27492;&#65292;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;(SSDA)&#26088;&#22312;&#21033;&#29992;&#26631;&#35760;&#28304;&#22270;&#30340;&#30693;&#35782;&#26469;&#24110;&#21161;&#26377;&#38480;&#26631;&#31614;&#30340;&#30446;&#26631;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#12290;SSDA&#20219;&#21153;&#38656;&#35201;&#20811;&#26381;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#36328;&#22270;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#23578;&#26410;&#27491;&#24335;&#32771;&#34385;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#19978;&#30340;SSDA&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SemiGCL&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#33719;&#30410;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#26368;&#23567;&#26368;&#22823;&#29109;&#35757;&#32451;&#12290;SemiGCL&#36890;&#36807;&#23545;&#27604;&#20174;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#35270;&#22270;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;SemiGCL&#36890;&#36807;&#30446;&#26631;&#22270;&#20013;&#26410;&#26631;&#35760;&#33410;&#28857;&#30340;&#29109;&#25439;&#22833;&#36827;&#34892;&#23545;&#25239;&#20248;&#21270;&#65292;&#20197;&#20943;&#23567;&#22495;&#24046;&#24322;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SemiGCL&#22312;&#22270;&#19978;&#30340;SSDA&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label scarcity in a graph is frequently encountered in real-world applications due to the high cost of data labeling. To this end, semi-supervised domain adaptation (SSDA) on graphs aims to leverage the knowledge of a labeled source graph to aid in node classification on a target graph with limited labels. SSDA tasks need to overcome the domain gap between the source and target graphs. However, to date, this challenging research problem has yet to be formally considered by the existing approaches designed for cross-graph node classification. To tackle the SSDA problem on graphs, a novel method called SemiGCL is proposed, which benefits from graph contrastive learning and minimax entropy training. SemiGCL generates informative node representations by contrasting the representations learned from a graph's local and global views. Additionally, SemiGCL is adversarially optimized with the entropy loss of unlabeled target nodes to reduce domain divergence. Experimental results on benchmark d
&lt;/p&gt;</description></item><item><title>FOSA&#26159;&#19968;&#31181;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;FIML&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.12388</link><description>&lt;p&gt;
FOSA: &#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data. (arXiv:2308.12388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12388
&lt;/p&gt;
&lt;p&gt;
FOSA&#26159;&#19968;&#31181;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;FIML&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#34917;&#20840;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#20540;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;&#26412;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;FIML&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#65288;FOSA&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#20102;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982;&#65288;FIML&#65289;&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#33021;&#21147;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;FIML&#23545;&#32570;&#22833;&#20540;&#36827;&#34892;&#21021;&#22987;&#20272;&#35745;&#65292;&#28982;&#21518;&#36890;&#36807;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#19968;&#27493;&#25552;&#28860;&#36825;&#20123;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;FOSA&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;FIML&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#65288;SEM&#65289;&#21487;&#33021;&#38169;&#35823;&#35268;&#23450;&#23548;&#33268;&#23376;&#20248;&#30340;FIML&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;FOSA&#33258;&#27880;&#24847;&#21147;&#32452;&#20214;&#30340;&#31283;&#20581;&#26550;&#26500;&#33021;&#22815;&#28789;&#27963;&#22320;&#32416;&#27491;&#21644;&#20248;&#21270;&#34917;&#20840;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data imputation, effectively addressing missing values is pivotal, especially in intricate datasets. This paper delves into the FIML Optimized Self-attention (FOSA) framework, an innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of self-attention neural networks. Our methodology commences with an initial estimation of missing values via FIML, subsequently refining these estimates by leveraging the self-attention mechanism. Our comprehensive experiments on both simulated and real-world datasets underscore FOSA's pronounced advantages over traditional FIML techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Intriguingly, even in scenarios where the Structural Equation Model (SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust architecture of FOSA's self-attention component adeptly rectifies and optimizes the imputati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.02058</link><description>&lt;p&gt;
&#25972;&#21512;&#40065;&#33725;&#34892;&#20026;&#21040;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Recklessness to Collaborative Filtering based Recommender Systems. (arXiv:2308.02058v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#21487;&#38752;&#24615;&#27979;&#37327;&#30340;&#25512;&#33616;&#31995;&#32479;&#24448;&#24448;&#22312;&#39044;&#27979;&#20013;&#26356;&#21152;&#20445;&#23432;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20445;&#25345;&#21487;&#38752;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26032;&#39062;&#24615;&#30340;&#26174;&#33879;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#30697;&#38453;&#20998;&#35299;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#39033;&#26032;&#30340;&#39033;&#65292;&#31216;&#20026;&#40065;&#33725;&#34892;&#20026;&#65292;&#23427;&#21487;&#20197;&#25511;&#21046;&#22312;&#20570;&#20986;&#20851;&#20110;&#39044;&#27979;&#21487;&#38752;&#24615;&#30340;&#20915;&#31574;&#26102;&#25152;&#24076;&#26395;&#30340;&#39118;&#38505;&#27700;&#24179;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#40065;&#33725;&#34892;&#20026;&#19981;&#20165;&#20801;&#35768;&#36827;&#34892;&#39118;&#38505;&#35843;&#25511;&#65292;&#36824;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#30340;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems that include some reliability measure of their predictions tend to be more conservative in forecasting, due to their constraint to preserve reliability. This leads to a significant drop in the coverage and novelty that these systems can provide. In this paper, we propose the inclusion of a new term in the learning process of matrix factorization-based recommender systems, called recklessness, which enables the control of the risk level desired when making decisions about the reliability of a prediction. Experimental results demonstrate that recklessness not only allows for risk regulation but also improves the quantity and quality of predictions provided by the recommender system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01923</link><description>&lt;p&gt;
&#22810;&#37325;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fairness Improvement with Multiple Protected Attributes. (arXiv:2308.01923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36719;&#20214;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#65292;&#20294;&#32771;&#34385;&#21040;&#35768;&#22810;&#29992;&#25143;&#20855;&#26377;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#23545;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#32771;&#34385;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;ML&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25913;&#21892;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#22823;&#22823;&#38477;&#20302;&#20102;&#26410;&#32771;&#34385;&#30340;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;88.3&#65285;&#30340;&#24773;&#20917;&#19979;&#35266;&#23519;&#21040;&#36825;&#31181;&#38477;&#20302;&#65288;&#24179;&#22343;&#20026;57.5&#65285;&#65289;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#32771;&#34385;&#21333;&#20010;&#21644;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#20934;&#30830;&#29575;&#25439;&#22833;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32416;&#32544;&#38203;&#36896;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#24577;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#30456;&#20851;&#30340;&#22522;&#24577;&#20301;&#20018;&#65292;&#28040;&#38500;&#20102;&#25351;&#25968;&#32423;&#27714;&#21644;&#30340;&#38656;&#27714;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#31995;&#32479;&#19978;&#21487;&#20197;&#36798;&#21040;&#30456;&#24403;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02633</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#34203;&#23450;&#35860;&#38203;&#36896;&#30340;&#28151;&#21512;&#22522;&#24577;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Ground-State Quantum Algorithms based on Neural Schr\"odinger Forging. (arXiv:2307.02633v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02633
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32416;&#32544;&#38203;&#36896;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#24577;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#30456;&#20851;&#30340;&#22522;&#24577;&#20301;&#20018;&#65292;&#28040;&#38500;&#20102;&#25351;&#25968;&#32423;&#27714;&#21644;&#30340;&#38656;&#27714;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#31995;&#32479;&#19978;&#21487;&#20197;&#36798;&#21040;&#30456;&#24403;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32416;&#32544;&#38203;&#36896;&#30340;&#21464;&#20998;&#31639;&#27861;&#21033;&#29992;&#37327;&#23376;&#31995;&#32479;&#30340;&#21452;&#20998;&#21106;&#26469;&#35299;&#20915;&#22522;&#24577;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#23545;&#25972;&#20010;&#31995;&#32479;&#30340;Schmidt&#20998;&#35299;&#26102;&#38656;&#35201;&#23545;&#26080;&#25968;&#28508;&#22312;&#22522;&#24577;&#36827;&#34892;&#25351;&#25968;&#32423;&#27714;&#21644;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32416;&#32544;&#38203;&#36896;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#26368;&#30456;&#20851;&#30340;&#22522;&#24577;&#20301;&#20018;&#65292;&#28040;&#38500;&#20102;&#25351;&#25968;&#32423;&#27714;&#21644;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#22312;&#22797;&#26434;&#24230;&#36882;&#22686;&#30340;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#35777;&#28436;&#31034;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#29616;&#26377;&#30340;&#32416;&#32544;&#38203;&#36896;&#26631;&#20934;&#23454;&#29616;&#30456;&#27604;&#21487;&#36798;&#21040;&#30456;&#24403;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25511;&#21046;&#25152;&#38656;&#36164;&#28304;&#30340;&#25968;&#37327;&#65292;&#35813;&#26041;&#26696;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#22823;&#30340;&#38750;&#32622;&#25442;&#19981;&#21464;&#31995;&#32479;&#65292;&#21518;&#32773;&#38480;&#21046;&#19982;&#28023;&#26862;&#20271;&#38203;&#36896;&#36807;&#31243;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entanglement forging based variational algorithms leverage the bi-partition of quantum systems for addressing ground state problems. The primary limitation of these approaches lies in the exponential summation required over the numerous potential basis states, or bitstrings, when performing the Schmidt decomposition of the whole system. To overcome this challenge, we propose a new method for entanglement forging employing generative neural networks to identify the most pertinent bitstrings, eliminating the need for the exponential sum. Through empirical demonstrations on systems of increasing complexity, we show that the proposed algorithm achieves comparable or superior performance compared to the existing standard implementation of entanglement forging. Moreover, by controlling the amount of required resources, this scheme can be applied to larger, as well as non permutation invariant systems, where the latter constraint is associated with the Heisenberg forging procedure. We substan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24352;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#22343;&#21248;&#36229;&#22270;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;TTSV&#31639;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20302;&#20110;&#25351;&#25968;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#37051;&#25509;&#24352;&#37327;&#65292;&#24182;&#24212;&#29992;&#20110;&#36229;&#22270;&#20013;&#24515;&#24615;&#21644;&#32858;&#31867;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#33021;&#25552;&#20379;&#19982;&#22270;&#32553;&#20943;&#26041;&#27861;&#20114;&#34917;&#30340;&#20449;&#24687;&#65292;&#36824;&#33021;&#22815;&#25506;&#27979;&#21040;&#39640;&#38454;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.17825</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#38750;&#22343;&#21248;&#36229;&#22270;&#30340;&#24352;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable tensor methods for nonuniform hypergraphs. (arXiv:2306.17825v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24352;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#22343;&#21248;&#36229;&#22270;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;TTSV&#31639;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20302;&#20110;&#25351;&#25968;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#37051;&#25509;&#24352;&#37327;&#65292;&#24182;&#24212;&#29992;&#20110;&#36229;&#22270;&#20013;&#24515;&#24615;&#21644;&#32858;&#31867;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#33021;&#25552;&#20379;&#19982;&#22270;&#32553;&#20943;&#26041;&#27861;&#20114;&#34917;&#30340;&#20449;&#24687;&#65292;&#36824;&#33021;&#22815;&#25506;&#27979;&#21040;&#39640;&#38454;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#32447;&#24615;&#20195;&#25968;&#22312;&#30740;&#31350;&#30001;&#36229;&#22270;&#27169;&#25311;&#30340;&#22810;&#26041;&#20132;&#20114;&#26041;&#38754;&#20284;&#20046;&#24456;&#33258;&#28982;&#65292;&#20294;&#36890;&#29992;&#36229;&#22270;&#30340;&#24352;&#37327;&#26041;&#27861;&#21463;&#21040;&#29702;&#35770;&#21644;&#23454;&#38469;&#38480;&#21046;&#30340;&#38459;&#30861;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#37051;&#25509;&#24352;&#37327;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#36229;&#22270;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24418;&#25104;&#21644;&#20998;&#26512;&#23427;&#26159;&#20195;&#20215;&#39640;&#26114;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#36825;&#20010;&#24352;&#37327;&#30340;&#24352;&#37327;&#20056;&#30456;&#21516;&#21521;&#37327;&#65288;TTSV&#65289;&#31639;&#27861;&#65292;&#23558;&#22797;&#26434;&#24230;&#20174;$O(n^r)$&#38477;&#20302;&#21040;$r$&#30340;&#20302;&#27425;&#22810;&#39033;&#24335;&#65292;&#20854;&#20013;$n$&#26159;&#39030;&#28857;&#30340;&#25968;&#37327;&#65292;$r$&#26159;&#26368;&#22823;&#36229;&#36793;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#38544;&#24335;&#30340;&#65292;&#36991;&#20813;&#20102;&#24418;&#25104;$r$&#38454;&#37051;&#25509;&#24352;&#37327;&#12290;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#24352;&#37327;&#30340;&#36229;&#22270;&#20013;&#24515;&#24615;&#21644;&#32858;&#31867;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#24352;&#37327;&#24230;&#37327;&#22312;&#25968;&#25454;&#19978;&#19982;&#31867;&#20284;&#30340;&#22270;&#32553;&#20943;&#26041;&#27861;&#25552;&#20379;&#20114;&#34917;&#20449;&#24687;&#65292;&#24182;&#19988;&#36824;&#33021;&#22815;&#26816;&#27979;&#21040;&#35768;&#22810;&#29616;&#26377;&#22522;&#20110;&#30697;&#38453;&#30340;&#26041;&#27861;&#26080;&#27861;&#26816;&#27979;&#21040;&#30340;&#39640;&#38454;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
While multilinear algebra appears natural for studying the multiway interactions modeled by hypergraphs, tensor methods for general hypergraphs have been stymied by theoretical and practical barriers. A recently proposed adjacency tensor is applicable to nonuniform hypergraphs, but is prohibitively costly to form and analyze in practice. We develop tensor times same vector (TTSV) algorithms for this tensor which improve complexity from $O(n^r)$ to a low-degree polynomial in $r$, where $n$ is the number of vertices and $r$ is the maximum hyperedge size. Our algorithms are implicit, avoiding formation of the order $r$ adjacency tensor. We demonstrate the flexibility and utility of our approach in practice by developing tensor-based hypergraph centrality and clustering algorithms. We also show these tensor measures offer complementary information to analogous graph-reduction approaches on data, and are also able to detect higher-order structure that many existing matrix-based approaches p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#25552;&#21319;&#20102;&#37329;&#34701;&#39044;&#27979;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#26469;&#22686;&#24378;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#36827;&#34892;&#27969;&#22833;&#39044;&#27979;&#24182;&#35774;&#35745;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#36798;&#21040;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12965</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#37329;&#34701;&#39044;&#27979;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Financial Forecasting via Quantum Machine Learning. (arXiv:2306.12965v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#25552;&#21319;&#20102;&#37329;&#34701;&#39044;&#27979;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#26469;&#22686;&#24378;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#36827;&#34892;&#27969;&#22833;&#39044;&#27979;&#24182;&#35774;&#35745;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#36798;&#21040;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31639;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26469;&#25913;&#36827;&#37329;&#34701;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#20856;&#21644;&#37327;&#23376;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#26469;&#22686;&#24378;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20197;&#36827;&#34892;&#27969;&#22833;&#39044;&#27979;&#65292;&#25552;&#39640;&#20102;&#36817;6&#65285;&#30340;&#31934;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#27491;&#20132;&#21644;&#22797;&#21512;&#23618;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#36798;&#21040;&#20102;&#19982;&#32463;&#20856;&#24615;&#33021;&#30456;&#24403;&#30340;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#37327;&#23376;&#24605;&#24819;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#30340;&#34920;&#29616;&#65292;&#26080;&#35770;&#26159;&#29616;&#22312;&#20316;&#20026;&#37327;&#23376;&#21551;&#21457;&#24335;&#30340;&#32463;&#20856;ML&#35299;&#20915;&#26041;&#26696;&#65292;&#36824;&#26159;&#22312;&#26410;&#26469;&#26356;&#22909;&#30340;&#37327;&#23376;&#30828;&#20214;&#30340;&#21040;&#26469;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum algorithms have the potential to enhance machine learning across a variety of domains and applications. In this work, we show how quantum machine learning can be used to improve financial forecasting. First, we use classical and quantum Determinantal Point Processes to enhance Random Forest models for churn prediction, improving precision by almost 6%. Second, we design quantum neural network architectures with orthogonal and compound layers for credit risk assessment, which match classical performance with significantly fewer parameters. Our results demonstrate that leveraging quantum ideas can effectively enhance the performance of machine learning, both today as quantum-inspired classical ML solutions, and even more in the future, with the advent of better quantum hardware.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#25345;&#32493;&#20877;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05494</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#25239;&#24615;&#28431;&#27934;&#25915;&#20987;&#30340;&#23454;&#29992;&#24615;&#27979;&#35797;&#65306;&#21160;&#24577;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning. (arXiv:2306.05494v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#25345;&#32493;&#20877;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#20013;&#65292;&#30001;&#20110;&#20854;&#33258;&#21160;&#21270;&#30340;&#29305;&#24615;&#21644;&#22312;&#22788;&#29702;&#21644;&#20998;&#31867;&#22823;&#37327;&#25968;&#25454;&#19978;&#30340;&#39640;&#31934;&#24230;&#12290;&#20294;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#32570;&#38519;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20854;&#30446;&#30340;&#26159;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20135;&#29983;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#36129;&#29486;&#65306;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;NIDS&#23454;&#29992;&#24615;&#38382;&#39064;&#30340;&#20998;&#31867;&#21644;&#23545;&#25345;&#32493;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#33021;&#20250;&#21361;&#21450;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;NIDS&#65292;&#20294;&#25345;&#32493;&#20877;&#35757;&#32451;&#21487;&#24102;&#26469;&#19968;&#23450;&#30340;&#32531;&#35299;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has become ubiquitous, and its deployment in Network Intrusion Detection Systems (NIDS) is inevitable due to its automated nature and high accuracy in processing and classifying large volumes of data. However, ML has been found to have several flaws, on top of them are adversarial attacks, which aim to trick ML models into producing faulty predictions. While most adversarial attack research focuses on computer vision datasets, recent studies have explored the practicality of such attacks against ML-based network security entities, especially NIDS.  This paper presents two distinct contributions: a taxonomy of practicality issues associated with adversarial attacks against ML-based NIDS and an investigation of the impact of continuous training on adversarial attacks against NIDS. Our experiments indicate that continuous re-training, even without adversarial training, can reduce the effect of adversarial attacks. While adversarial attacks can harm ML-based NIDSs, ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;</title><link>http://arxiv.org/abs/2306.00006</link><description>&lt;p&gt;
&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65306;&#29992;&#20110;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#30340;&#21333;&#31867;&#21516;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#65288;GAD&#65289;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#21457;&#29616;&#19968;&#31181;&#26222;&#36941;&#30340;&#23646;&#24615;......&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327; - &#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;......&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270; (TAM)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;_neighbors&#30340;&#26412;&#22320;&#20146;&#21644;&#21147;&#26469;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#25152;&#25552;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;
One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19358</link><description>&lt;p&gt;
&#31283;&#20581;&#30340;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#23646;&#24615;&#24050;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25991;&#29486;&#26222;&#36941;&#35748;&#20026;LLMs&#34920;&#31034;&#30001;&#23569;&#25968;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#21644;&#24133;&#24230;&#30340;&#8220;&#24322;&#24120;&#32500;&#24230;&#8221;&#20027;&#23548;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#20943;&#36731;&#36825;&#20123;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#36843;&#20351;LLMs&#25104;&#20026;&#21508;&#21521;&#21516;&#24615;&#65288;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25152;&#26377;&#32500;&#24230;&#20855;&#26377;&#22343;&#21248;&#26041;&#24046;&#65289;&#30340;&#12290;&#21508;&#21521;&#21516;&#24615;&#34987;&#35748;&#20026;&#26159;LLMs&#30340;&#19968;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#26356;&#21152;&#36148;&#36817;&#20154;&#31867;&#30452;&#35273;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;NLP&#20013;&#21508;&#21521;&#21516;&#24615;&#30340;&#35768;&#22810;&#35266;&#28857;&#37117;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#26377;&#32570;&#38519;&#30340;&#21508;&#21521;&#21516;&#24615;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-STAR&#65306;&#22522;&#20110;IsoScore$^{\star}$&#30340;&#31283;&#23450;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2304.14364</link><description>&lt;p&gt;
CONSCENDI: &#19968;&#31181;&#21453;&#23545;&#27604;&#19988;&#22330;&#26223;&#24341;&#23548;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#20026;&#34394;&#25311;&#21161;&#25163;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants. (arXiv:2304.14364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPT-4&#31561;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#26032;&#19968;&#20195;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#34394;&#25311;&#21161;&#25163;&#24212;&#36816;&#32780;&#29983;&#12290;&#36825;&#20123;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#23458;&#25143;&#30340;&#20855;&#20307;&#29992;&#20363;&#36827;&#34892;&#23450;&#21046;&#65292;&#20294;&#30830;&#20445;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#20165;&#31526;&#21512;&#25552;&#31034;&#25351;&#20196;&#20013;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24072;&#36890;&#24120;&#20351;&#29992;&#21478;&#19968;&#20010;&#31216;&#20026;&#38450;&#25252;&#26639;&#27169;&#22411;&#30340;&#27169;&#22411;&#26469;&#39564;&#35777;&#20195;&#29702;&#36755;&#20986;&#26159;&#21542;&#19982;&#20854;&#35268;&#21017;&#21644;&#32422;&#26463;&#23545;&#40784;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#33976;&#39311;&#26041;&#27861;&#26469;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20351;&#29992;GPT-4&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;CONSCENDI&#36807;&#31243;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#20250;&#29983;&#25104;&#19968;&#32452;&#36829;&#21453;&#35268;&#21017;&#30340;&#22330;&#26223;&#65292;&#36825;&#20123;&#22330;&#26223;&#21015;&#20030;&#20102;&#36829;&#21453;&#35268;&#21017;&#30340;&#22810;&#26679;&#21270;&#39640;&#32423;&#26041;&#24335;&#12290;&#36825;&#31181;&#22330;&#26223;&#24341;&#23548;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#23427;&#20351;&#24471;&#27169;&#22411;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wave of new task-based virtual assistants has been fueled by increasingly powerful large language models, such as GPT-4. These conversational agents can be customized to serve customer-specific use cases, but ensuring that agent-generated text conforms to designer-specified rules included in prompt instructions alone is challenging. Therefore, chatbot designers often use another model, called a guardrail model, to verify that the agent output aligns with their rules and constraints. We explore using a distillation approach to guardrail models to monitor the output of the first model using training data from GPT-4. We find two crucial steps to our CONSCENDI process: scenario-augmented generation and contrastive training examples. When generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set of rule-violating conversations, and it p
&lt;/p&gt;</description></item><item><title>OptoGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#20165;&#21253;&#21547;&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#20027;&#20840;&#23616;&#35774;&#35745;&#25506;&#32034;&#65292;&#21516;&#26102;&#36873;&#25321;&#26448;&#26009;&#21644;&#21402;&#24230;&#65292;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#21453;&#21521;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.10294</link><description>&lt;p&gt;
OptoGPT&#65306;&#19968;&#31181;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#21453;&#21521;&#35774;&#35745;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OptoGPT: A Foundation Model for Inverse Design in Optical Multilayer Thin Film Structures. (arXiv:2304.10294v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10294
&lt;/p&gt;
&lt;p&gt;
OptoGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#20165;&#21253;&#21547;&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#20027;&#20840;&#23616;&#35774;&#35745;&#25506;&#32034;&#65292;&#21516;&#26102;&#36873;&#25321;&#26448;&#26009;&#21644;&#21402;&#24230;&#65292;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#21453;&#21521;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#21363;&#21487;&#35299;&#20915;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#24341;&#39046;&#30740;&#31350;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#36866;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#21453;&#21521;&#35774;&#35745;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#24403;&#21069;&#30340;&#21453;&#21521;&#35774;&#35745;&#31639;&#27861;&#35201;&#20040;&#19981;&#33021;&#25506;&#32034;&#20840;&#23616;&#35774;&#35745;&#31354;&#38388;&#65292;&#35201;&#20040;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Opto Generative Pretrained Transformer&#65288;OptoGPT&#65289;&#12290;OptoGPT&#26159;&#19968;&#20010;&#20165;&#21253;&#21547;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#30340;&#39057;&#35889;&#30446;&#26631;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;&#35774;&#35745;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#32452;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;1000&#19975;&#20010;&#35774;&#35745;&#65289;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;: 1&#65289;&#33258;&#20027;&#20840;&#23616;&#35774;&#35745;&#25506;&#32034;&#65292;&#36890;&#36807;&#30830;&#23450;&#23618;&#25968;&#65288;&#39640;&#36798;20&#23618;&#65289;&#65292;&#21516;&#26102;&#36873;&#25321;&#27599;&#20010;&#23618;&#30340;&#26448;&#26009;&#65288;&#39640;&#36798;18&#31181;&#19981;&#21516;&#31867;&#22411;&#65289;&#21644;&#21402;&#24230;&#65307;2&#65289;&#39640;&#25928;&#30340;&#32467;&#26500;&#39068;&#33394;&#35774;&#35745;&#65292;&#21560;&#25910;&#22120;&#65292;&#28388;&#27874;&#22120;&#65292;&#20998;&#24067;&#21453;&#23556;&#38236;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are large machine learning models that can tackle various downstream tasks once trained on diverse and large-scale data, leading research trends in natural language processing, computer vision, and reinforcement learning. However, no foundation model exists for optical multilayer thin film structure inverse design. Current inverse design algorithms either fail to explore the global design space or suffer from low computational efficiency. To bridge this gap, we propose the Opto Generative Pretrained Transformer (OptoGPT). OptoGPT is a decoder-only transformer that auto-regressively generates designs based on specific spectrum targets. Trained on a large dataset of 10 million designs, our model demonstrates remarkable capabilities: 1) autonomous global design exploration by determining the number of layers (up to 20) while selecting the material (up to 18 distinct types) and thickness at each layer, 2) efficient designs for structural color, absorbers, filters, distrib
&lt;/p&gt;</description></item><item><title>TiDE&#26159;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20855;&#22791;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#65292;&#30456;&#36739;&#20110;&#26368;&#20339;&#30340;Transformer&#27169;&#22411;&#65292;&#36895;&#24230;&#24555;5-10&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.08424</link><description>&lt;p&gt;
&#29992;TiDE&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#65306;&#26102;&#38388;&#24207;&#21015;&#31264;&#23494;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Long-term Forecasting with TiDE: Time-series Dense Encoder. (arXiv:2304.08424v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08424
&lt;/p&gt;
&lt;p&gt;
TiDE&#26159;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20855;&#22791;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#65292;&#30456;&#36739;&#20110;&#26368;&#20339;&#30340;Transformer&#27169;&#22411;&#65292;&#36895;&#24230;&#24555;5-10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#21363;&#26102;&#38388;&#24207;&#21015;&#31264;&#23494;&#32534;&#30721;&#22120;(TiDE)&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20139;&#26377;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26368;&#31616;&#32447;&#24615;&#31867;&#27604;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#21487;&#20197;&#36798;&#21040;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;(LDS)&#30340;&#36817;&#20046;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#21305;&#37197;&#25110;&#32988;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#27604;&#26368;&#20339;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24555;5-10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, Time-series Dense Encoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;25&#24180;&#20013;&#39118;&#38505;&#27979;&#24230;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20171;&#32461;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#19982;&#25928;&#29992;&#29702;&#35770;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20102;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#31561;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2212.00856</link><description>&lt;p&gt;
&#23398;&#20064;&#21644;&#20915;&#31574;&#30340;&#39118;&#38505;&#33258;&#36866;&#24212;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Risk-Adaptive Approaches to Learning and Decision Making: A Survey. (arXiv:2212.00856v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;25&#24180;&#20013;&#39118;&#38505;&#27979;&#24230;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20171;&#32461;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#19982;&#25928;&#29992;&#29702;&#35770;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20102;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#31561;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#22312;&#24037;&#31243;&#35774;&#35745;&#12289;&#32479;&#35745;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#30001;&#20110;&#22266;&#26377;&#30340;&#39118;&#38505;&#35268;&#36991;&#21644;&#23545;&#20551;&#35774;&#30340;&#27169;&#31946;&#24615;&#65292;&#36890;&#24120;&#36890;&#36807;&#21046;&#23450;&#21644;&#35299;&#20915;&#20351;&#29992;&#39118;&#38505;&#21644;&#30456;&#20851;&#27010;&#24565;&#30340;&#20445;&#23432;&#20248;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#36807;&#21435;25&#24180;&#26469;&#39118;&#38505;&#27979;&#24230;&#30340;&#24555;&#36895;&#21457;&#23637;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#20174;&#23427;&#20204;&#22312;&#37329;&#34701;&#24037;&#31243;&#39046;&#22495;&#30340;&#36215;&#27493;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#23427;&#20204;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#30340;&#24037;&#31243;&#21644;&#24212;&#29992;&#25968;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;&#39118;&#38505;&#27979;&#24230;&#25166;&#26681;&#20110;&#20984;&#20998;&#26512;&#65292;&#20026;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#37325;&#35201;&#35745;&#31639;&#21644;&#29702;&#35770;&#20248;&#21183;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20851;&#38190;&#20107;&#23454;&#65292;&#21015;&#20030;&#20102;&#20960;&#31181;&#20855;&#20307;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#37327;&#21442;&#32771;&#25991;&#29486;&#20379;&#36827;&#19968;&#27493;&#38405;&#35835;&#12290;&#35813;&#35843;&#26597;&#36824;&#22238;&#39038;&#20102;&#19982;&#25928;&#29992;&#29702;&#35770;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#32852;&#31995;&#65292;&#25351;&#20986;&#20102;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#65292;&#24182;&#23450;&#20041;&#20102;&#30456;&#23545;&#27979;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty is prevalent in engineering design, statistical learning, and decision making broadly. Due to inherent risk-averseness and ambiguity about assumptions, it is common to address uncertainty by formulating and solving conservative optimization models expressed using measures of risk and related concepts. We survey the rapid development of risk measures over the last quarter century. From their beginning in financial engineering, we recount the spread to nearly all areas of engineering and applied mathematics. Solidly rooted in convex analysis, risk measures furnish a general framework for handling uncertainty with significant computational and theoretical advantages. We describe the key facts, list several concrete algorithms, and provide an extensive list of references for further reading. The survey recalls connections with utility theory and distributionally robust optimization, points to emerging applications areas such as fair machine learning, and defines measures of rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38754;&#23545;&#33258;&#36866;&#24212;&#36873;&#25321;&#25968;&#25454;&#26679;&#26412;&#24341;&#36215;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20351;&#29992;&#22122;&#22768;&#21152;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#19981;&#20381;&#36182;&#20110;&#26597;&#35810;&#35268;&#27169;&#30340;&#35823;&#24046;&#20445;&#35777;&#65292;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#30340;&#38382;&#39064;&#22312;&#20110;&#26032;&#26597;&#35810;&#19982;&#36807;&#21435;&#26597;&#35810;&#30340;&#21327;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2106.10761</link><description>&lt;p&gt;
&#38754;&#23545;&#36866;&#24212;&#24615;&#27867;&#21270;&#65306;&#36125;&#21494;&#26031;&#35270;&#35282;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalization in the Face of Adaptivity: A Bayesian Perspective. (arXiv:2106.10761v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.10761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38754;&#23545;&#33258;&#36866;&#24212;&#36873;&#25321;&#25968;&#25454;&#26679;&#26412;&#24341;&#36215;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20351;&#29992;&#22122;&#22768;&#21152;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#19981;&#20381;&#36182;&#20110;&#26597;&#35810;&#35268;&#27169;&#30340;&#35823;&#24046;&#20445;&#35777;&#65292;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#30340;&#38382;&#39064;&#22312;&#20110;&#26032;&#26597;&#35810;&#19982;&#36807;&#21435;&#26597;&#35810;&#30340;&#21327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#36873;&#25321;&#26679;&#26412;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#65292;&#31616;&#21333;&#30340;&#22122;&#22768;&#21152;&#31639;&#27861;&#21487;&#20197;&#36991;&#20813;&#36825;&#19968;&#38382;&#39064;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22122;&#22768;&#21152;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#19981;&#20381;&#36182;&#20110;&#26597;&#35810;&#35268;&#27169;&#30340;&#35823;&#24046;&#20445;&#35777;&#65292;&#36825;&#19968;&#32467;&#26524;&#26469;&#28304;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#30340;&#38382;&#39064;&#22312;&#20110;&#26032;&#26597;&#35810;&#19982;&#36807;&#21435;&#26597;&#35810;&#30340;&#21327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Repeated use of a data sample via adaptively chosen queries can rapidly lead to overfitting, wherein the empirical evaluation of queries on the sample significantly deviates from their mean with respect to the underlying data distribution. It turns out that simple noise addition algorithms suffice to prevent this issue, and differential privacy-based analysis of these algorithms shows that they can handle an asymptotically optimal number of queries. However, differential privacy's worst-case nature entails scaling such noise to the range of the queries even for highly-concentrated queries, or introducing more complex algorithms.  In this paper, we prove that straightforward noise-addition algorithms already provide variance-dependent guarantees that also extend to unbounded queries. This improvement stems from a novel characterization that illuminates the core problem of adaptive data analysis. We show that the harm of adaptivity results from the covariance between the new query and a 
&lt;/p&gt;</description></item></channel></rss>