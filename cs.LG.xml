<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;CVV-Pro&#65292;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#30340;&#26102;&#21464;&#21644;&#38750;&#32447;&#24615;&#32422;&#26463;&#65292;&#21482;&#20381;&#36182;&#20110;&#23616;&#37096;&#31232;&#30095;&#32447;&#24615;&#36924;&#36817;&#65292;&#36798;&#21040;&#20102;$\sqrt{T}$&#36951;&#25022;&#29575;&#21644;$1/\sqrt{T}$&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03655</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning under Adversarial Nonlinear Constraints. (arXiv:2306.03655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03655
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;CVV-Pro&#65292;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#30340;&#26102;&#21464;&#21644;&#38750;&#32447;&#24615;&#32422;&#26463;&#65292;&#21482;&#20381;&#36182;&#20110;&#23616;&#37096;&#31232;&#30095;&#32447;&#24615;&#36924;&#36817;&#65292;&#36798;&#21040;&#20102;$\sqrt{T}$&#36951;&#25022;&#29575;&#21644;$1/\sqrt{T}$&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#22788;&#29702;&#36830;&#32493;&#30340;&#38750;&#31283;&#24577;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#22312;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#30340;&#26102;&#21464;&#21644;&#38750;&#32447;&#24615;&#32422;&#26463;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#36825;&#20010;&#21517;&#20026;Constraint Violation Velocity Projection (CVV-Pro)&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;$\sqrt{T}$&#30340;&#36951;&#25022;&#29575;&#65292;&#24182;&#20197;$1/\sqrt{T}$&#30340;&#36895;&#24230;&#25910;&#25947;&#20110;&#21487;&#34892;&#38598;&#65292;&#23613;&#31649;&#21487;&#34892;&#38598;&#32531;&#24930;&#22320;&#38543;&#26102;&#38388;&#21464;&#21270;&#19988;&#19981;&#20026;&#23398;&#20064;&#32773;&#25152;&#30693;&#12290;CVV-Pro&#20165;&#20381;&#36182;&#20110;&#21487;&#34892;&#38598;&#30340;&#23616;&#37096;&#31232;&#30095;&#32447;&#24615;&#36924;&#36817;&#65292;&#22240;&#27492;&#36991;&#20813;&#20102;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20248;&#21270;&#25972;&#20010;&#38598;&#21512;&#65292;&#36825;&#19982;&#25237;&#24433;&#26799;&#24230;&#25110;Frank-Wolfe&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25105;&#20204;&#36824;&#22312;&#20004;&#20010;&#29609;&#23478;&#28216;&#25103;&#20013;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#20854;&#20013;&#29609;&#23478;&#21463;&#21040;&#20849;&#20139;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications, learning systems are required to process continuous non-stationary data streams. We study this problem in an online learning framework and propose an algorithm that can deal with adversarial time-varying and nonlinear constraints. As we show in our work, the algorithm called Constraint Violation Velocity Projection (CVV-Pro) achieves $\sqrt{T}$ regret and converges to the feasible set at a rate of $1/\sqrt{T}$, despite the fact that the feasible set is slowly time-varying and a priori unknown to the learner. CVV-Pro only relies on local sparse linear approximations of the feasible set and therefore avoids optimizing over the entire set at each iteration, which is in sharp contrast to projected gradients or Frank-Wolfe methods. We also empirically evaluate our algorithm on two-player games, where the players are subjected to a shared constraint.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30417;&#30563;&#30693;&#35782;&#22312;&#19981;&#21516;&#35821;&#20041;&#20851;&#32852;&#23618;&#38754;&#19978;&#30340;&#24110;&#21161;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36716;&#31227;&#27969;&#24230;&#37327;&#26631;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#27492;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#26032;&#31867;&#21035;&#21457;&#29616;&#65292;&#24615;&#33021;&#19982;&#35821;&#20041;&#30456;&#20284;&#24615;&#19968;&#33268;&#65292;&#30417;&#30563;&#30693;&#35782;&#19981;&#19968;&#23450;&#24635;&#26159;&#26377;&#24110;&#21161;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.03648</link><description>&lt;p&gt;
&#30417;&#30563;&#30693;&#35782;&#21487;&#33021;&#20250;&#25439;&#23475;&#26032;&#31867;&#21035;&#21457;&#29616;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Supervised Knowledge May Hurt Novel Class Discovery Performance. (arXiv:2306.03648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30417;&#30563;&#30693;&#35782;&#22312;&#19981;&#21516;&#35821;&#20041;&#20851;&#32852;&#23618;&#38754;&#19978;&#30340;&#24110;&#21161;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36716;&#31227;&#27969;&#24230;&#37327;&#26631;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#27492;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#26032;&#31867;&#21035;&#21457;&#29616;&#65292;&#24615;&#33021;&#19982;&#35821;&#20041;&#30456;&#20284;&#24615;&#19968;&#33268;&#65292;&#30417;&#30563;&#30693;&#35782;&#19981;&#19968;&#23450;&#24635;&#26159;&#26377;&#24110;&#21161;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#31867;&#21035;&#21457;&#29616;(NCD)&#36890;&#36807;&#21033;&#29992;&#26631;&#35760;&#38598;&#21512;&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#25512;&#26029;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#31867;&#21035;&#12290;&#37492;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#22312;&#26041;&#27861;&#35770;&#23618;&#38754;&#19978;&#21033;&#29992;&#26631;&#35760;&#38598;&#21512;&#30340;&#30417;&#30563;&#30693;&#35782;&#65292;&#26412;&#25991;&#32771;&#34385;&#19968;&#20010;&#38382;&#39064;&#65306;&#30417;&#30563;&#30693;&#35782;&#22312;&#19981;&#21516;&#35821;&#20041;&#20851;&#32852;&#23618;&#38754;&#19978;&#24635;&#26159;&#26377;&#24110;&#21161;&#30340;&#21527;&#65311;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27979;&#37327;&#26631;&#20934;&#65292;&#25152;&#35859;&#30340;&#36716;&#31227;&#27969;&#65292;&#29992;&#20110;&#34913;&#37327;&#26631;&#35760;/&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;ImageNet&#30340;&#23618;&#27425;&#20998;&#31867;&#32467;&#26500;&#24314;&#31435;&#20102;&#19968;&#20010;&#24102;&#26377;&#21508;&#31181;&#35821;&#20041;&#30456;&#20284;&#24230;&#31243;&#24230;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#36716;&#31227;&#27969;&#19982;&#23618;&#27425;&#20998;&#31867;&#32467;&#26500;&#19968;&#33268;&#65292;&#24182;&#19988;NCD&#30340;&#24615;&#33021;&#19982;&#35821;&#20041;&#30456;&#20284;&#24615;&#19968;&#33268;(&#30001;&#25152;&#25552;&#20986;&#30340;&#36716;&#31227;&#27969;&#24230;&#37327;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel class discovery (NCD) aims to infer novel categories in an unlabeled dataset by leveraging prior knowledge of a labeled set comprising disjoint but related classes. Given that most existing literature focuses primarily on utilizing supervised knowledge from a labeled set at the methodology level, this paper considers the question: Is supervised knowledge always helpful at different levels of semantic relevance? To proceed, we first establish a novel metric, so-called transfer flow, to measure the semantic similarity between labeled/unlabeled datasets. To show the validity of the proposed metric, we build up a large-scale benchmark with various degrees of semantic similarities between labeled/unlabeled datasets on ImageNet by leveraging its hierarchical class structure. The results based on the proposed benchmark show that the proposed transfer flow is in line with the hierarchical class structure; and that NCD performance is consistent with the semantic similarities (measured by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Proximal Symmetric Non-negative Latent Factor Analysis (PSNL)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26080;&#23450;&#21521;&#21152;&#26435;&#32593;&#32476;&#30340;&#20449;&#24687;&#34920;&#24449;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#22312;&#32771;&#34385;&#21040;&#23545;&#31216;&#24615;&#21644;&#25968;&#25454;&#23494;&#24230;&#30340;&#21516;&#26102;&#65292;&#36824;&#20855;&#26377;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#34920;&#24449;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03647</link><description>&lt;p&gt;
&#36817;&#31471;&#23545;&#31216;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;:&#19968;&#31181;&#39640;&#31934;&#24230;&#34920;&#24449;&#26080;&#23450;&#21521;&#21152;&#26435;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Proximal Symmetric Non-negative Latent Factor Analysis: A Novel Approach to Highly-Accurate Representation of Undirected Weighted Networks. (arXiv:2306.03647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Proximal Symmetric Non-negative Latent Factor Analysis (PSNL)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26080;&#23450;&#21521;&#21152;&#26435;&#32593;&#32476;&#30340;&#20449;&#24687;&#34920;&#24449;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#22312;&#32771;&#34385;&#21040;&#23545;&#31216;&#24615;&#21644;&#25968;&#25454;&#23494;&#24230;&#30340;&#21516;&#26102;&#65292;&#36824;&#20855;&#26377;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#34920;&#24449;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#23450;&#21521;&#21152;&#26435;&#32593;&#32476; (UWN) &#22312;&#22823;&#25968;&#25454;&#30456;&#20851;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#12290;&#27880;&#24847;&#21040;&#36825;&#26679;&#19968;&#20010;&#32593;&#32476;&#19982;&#20854;&#33410;&#28857;&#21644;&#36793;&#36830;&#25509;&#30340;&#20449;&#24687;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20010;&#23545;&#31216;&#30340;&#12289;&#39640;&#32500;&#30340;&#19988;&#19981;&#23436;&#25972;&#30340;( SHDI)&#30697;&#38453;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#24314;&#27169;&#20854;&#22266;&#26377;&#23545;&#31216;&#24615;&#25110;&#20302;&#25968;&#25454;&#23494;&#24230;&#26041;&#38754;&#22343;&#23384;&#22312;&#38382;&#39064;&#65292;&#23548;&#33268;&#20302;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#25110;&#34920;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#31471;&#23545;&#31216;&#30340;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512; (PSNL)&#27169;&#22411;&#12290;&#23427;&#23558;&#19968;&#20010;&#25509;&#36817;&#39033;&#32435;&#20837;&#32771;&#34385;&#23545;&#31216;&#24615;&#21644;&#25968;&#25454;&#23494;&#24230;&#30340;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#20197;&#25552;&#39640;&#34920;&#24449;&#31934;&#24230;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#26641;&#29366;&#30340; Parzen &#20272;&#35745;&#22120; (TPE)&#26041;&#27861;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#31639;&#27861; (ADMM)&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#33719;&#24471;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#23545;&#22235;&#20010; UWNs &#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;PSNL &#27169;&#22411;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#39640;&#30340;&#31934;&#24230;&#25910;&#30410;&#65292;&#20197;&#21450;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Undirected Weighted Network (UWN) is commonly found in big data-related applications. Note that such a network's information connected with its nodes, and edges can be expressed as a Symmetric, High-Dimensional and Incomplete (SHDI) matrix. However, existing models fail in either modeling its intrinsic symmetry or low-data density, resulting in low model scalability or representation learning ability. For addressing this issue, a Proximal Symmetric Nonnegative Latent-factor-analysis (PSNL) model is proposed. It incorporates a proximal term into symmetry-aware and data density-oriented objective function for high representation accuracy. Then an adaptive Alternating Direction Method of Multipliers (ADMM)-based learning scheme is implemented through a Tree-structured of Parzen Estimators (TPE) method for high computational efficiency. Empirical studies on four UWNs demonstrate that PSNL achieves higher accuracy gain than state-of-the-art models, as well as highly competitive computati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25311;&#22768;&#35789;&#29983;&#25104;&#33310;&#36424;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#25171;&#30772;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#38480;&#21046;&#65292;&#20351;&#26356;&#22810;&#20154;&#33021;&#22815;&#25509;&#35302;&#22810;&#26679;&#21270;&#30340;&#33310;&#36424;&#21019;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.03646</link><description>&lt;p&gt;
&#36890;&#36807;&#22768;&#38899;&#35937;&#24449;&#24615;&#35789;&#35821;&#29983;&#25104;&#33310;&#36424;
&lt;/p&gt;
&lt;p&gt;
Dance Generation by Sound Symbolic Words. (arXiv:2306.03646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25311;&#22768;&#35789;&#29983;&#25104;&#33310;&#36424;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#25171;&#30772;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#38480;&#21046;&#65292;&#20351;&#26356;&#22810;&#20154;&#33021;&#22815;&#25509;&#35302;&#22810;&#26679;&#21270;&#30340;&#33310;&#36424;&#21019;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25311;&#22768;&#35789;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;&#33310;&#36424;&#21160;&#20316;&#65292;&#26088;&#22312;&#22686;&#24378;&#33310;&#36424;&#29983;&#25104;&#30340;&#21019;&#36896;&#21147;&#21644;&#22810;&#26679;&#24615;&#12290;&#19982;&#25991;&#26412;&#21644;&#38899;&#20048;&#19981;&#21516;&#65292;&#25311;&#22768;&#35789;&#36890;&#36807;&#25277;&#35937;&#30340;&#35789;&#35821;&#34920;&#36798;&#20256;&#36798;&#33410;&#22863;&#21644;&#21547;&#20041;&#65292;&#27809;&#26377;&#34920;&#36798;&#30340;&#38480;&#21046;&#65292;&#20063;&#19981;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;AI&#32534;&#33310;&#24072;&#26694;&#26550;&#24182;&#20351;&#29992;Sakamoto&#31995;&#32479;&#65292;&#19968;&#31181;&#38024;&#23545;&#38899;&#32032;&#21644;&#38899;&#33410;&#30340;&#25311;&#22768;&#35789;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29992;&#25143;&#35843;&#26597;&#25910;&#38598;&#20102;&#19968;&#20010;&#30001;40&#20010;&#25311;&#22768;&#35789;-&#33310;&#36424;&#21160;&#20316;&#23545;&#32452;&#25104;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26356;&#30452;&#35266;&#30340;&#33310;&#36424;&#29983;&#25104;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#26469;&#33258;&#22810;&#31181;&#35821;&#35328;&#30340;&#22768;&#38899;&#35937;&#24449;&#24615;&#35789;&#35821;&#21019;&#24314;&#33310;&#36424;&#21160;&#20316;&#65292;&#21253;&#25324;&#37027;&#20123;&#27809;&#26377;&#25311;&#22768;&#35789;&#30340;&#35821;&#35328;&#12290;&#36825;&#31361;&#26174;&#20102;&#36328;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#22810;&#26679;&#21270;&#33310;&#36424;&#21019;&#20316;&#30340;&#28508;&#21147;&#65292;&#35753;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#33021;&#22815;&#25509;&#35302;&#21040;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#23450;&#24615;&#26679;&#26412;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;ht
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel approach to generate dance motions using onomatopoeia as input, with the aim of enhancing creativity and diversity in dance generation. Unlike text and music, onomatopoeia conveys rhythm and meaning through abstract word expressions without constraints on expression and without need for specialized knowledge. We adapt the AI Choreographer framework and employ the Sakamoto system, a feature extraction method for onomatopoeia focusing on phonemes and syllables. Additionally, we present a new dataset of 40 onomatopoeia-dance motion pairs collected through a user survey. Our results demonstrate that the proposed method enables more intuitive dance generation and can create dance motions using sound-symbolic words from a variety of languages, including those without onomatopoeia. This highlights the potential for diverse dance creation across different languages and cultures, accessible to a wider audience. Qualitative samples from our model can be found at: ht
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#39640;&#26031;&#21464;&#20998;&#26063;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20351;&#29992;&#36817;&#31471;&#21644;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#25552;&#20379;&#20102;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#20110;&#36924;&#30495;&#25512;&#26029;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#20005;&#26684;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.03638</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Provable convergence guarantees for black-box variational inference. (arXiv:2306.03638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#39640;&#26031;&#21464;&#20998;&#26063;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20351;&#29992;&#36817;&#31471;&#21644;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#25552;&#20379;&#20102;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#20110;&#36924;&#30495;&#25512;&#26029;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#20005;&#26684;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#27809;&#26377;&#35777;&#26126;&#20854;&#38543;&#26426;&#20248;&#21270;&#25104;&#21151;&#30340;&#35777;&#26126;&#12290;&#25105;&#20204;&#25552;&#20986;&#36825;&#26159;&#29616;&#26377;&#38543;&#26426;&#20248;&#21270;&#35777;&#26126;&#20013;&#30340;&#29702;&#35770;&#24046;&#36317;&#65292;&#21363;&#20855;&#26377;&#24322;&#24120;&#22122;&#22768;&#36793;&#30028;&#21644;&#22797;&#21512;&#38750;&#24179;&#28369;&#30446;&#26631;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#23494;&#38598;&#30340;&#39640;&#26031;&#21464;&#20998;&#26063;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#22522;&#20110;&#20877;&#21442;&#25968;&#21270;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#28385;&#36275;&#20108;&#27425;&#22122;&#22768;&#30028;&#65292;&#24182;&#20026;&#20351;&#29992;&#35813;&#30028;&#38480;&#30340;&#36817;&#31471;&#21644;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25552;&#20379;&#26032;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#36825;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#20110;&#36924;&#30495;&#25512;&#26029;&#38382;&#39064;&#30340;&#20005;&#26684;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
While black-box variational inference is widely used, there is no proof that its stochastic optimization succeeds. We suggest this is due to a theoretical gap in existing stochastic optimization proofs-namely the challenge of gradient estimators with unusual noise bounds, and a composite non-smooth objective. For dense Gaussian variational families, we observe that existing gradient estimators based on reparameterization satisfy a quadratic noise bound and give novel convergence guarantees for proximal and projected stochastic gradient descent using this bound. This provides the first rigorous guarantee that black-box variational inference converges for realistic inference problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RPT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#22352;&#26631;&#19979;&#38477;&#26694;&#26550;&#29702;&#35299;&#20102;&#28176;&#36827;&#24335;&#35757;&#32451;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#20026;&#19968;&#33324;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#25552;&#20379;&#20102;&#21487;&#38752;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.03626</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#22352;&#26631;&#19979;&#38477;&#26694;&#26550;&#29702;&#35299;&#28176;&#36827;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Understanding Progressive Training Through the Framework of Randomized Coordinate Descent. (arXiv:2306.03626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03626
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RPT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#22352;&#26631;&#19979;&#38477;&#26694;&#26550;&#29702;&#35299;&#20102;&#28176;&#36827;&#24335;&#35757;&#32451;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#20026;&#19968;&#33324;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#25552;&#20379;&#20102;&#21487;&#38752;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#28176;&#36827;&#24335;&#35757;&#32451;&#31639;&#27861;&#65288;RPT&#65289;&#8212;&#8212;&#28176;&#36827;&#24335;&#35757;&#32451;&#26041;&#27861;&#65288;PT&#65289;&#65288;Karras&#31561;&#65292;2017&#65289;&#30340;&#38543;&#26426;&#20195;&#29702;&#12290;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#35757;&#32451;GAN&#65288;Goodfellow&#31561;&#65292;2014&#65289;&#65292;PT&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21363;&#20351;&#23545;&#20110;&#26368;&#31616;&#21333;&#30340;&#30446;&#26631;&#20989;&#25968;&#20063;&#27809;&#26377;&#25910;&#25947;&#20998;&#26512;&#12290;&#30456;&#21453;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;RPT&#26159;&#31532;&#19968;&#20010;&#23545;&#20110;&#19968;&#33324;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#20005;&#26684;&#21644;&#21487;&#38752;&#29702;&#35770;&#20445;&#35777;&#30340;PT&#31867;&#22411;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#36716;&#21270;&#20026;&#38543;&#26426;&#22352;&#26631;&#19979;&#38477;&#65288;RCD&#65289;&#65288;Nesterov&#65292;2012&#65307;Richt&#225;rk&#65286;Tak&#225;&#269;&#65292;2014&#65289;&#30340;&#25104;&#29087;&#26694;&#26550;&#65292;&#23545;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#21333;&#21644;&#36890;&#29992;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#21253;&#25324;&#24378;&#20984;&#12289;&#20984;&#21644;&#38750;&#20984;&#30446;&#26631;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#24314;&#31435;RPT&#30340;&#25910;&#25947;&#29702;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#35745;&#31639;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Randomized Progressive Training algorithm (RPT) -- a stochastic proxy for the well-known Progressive Training method (PT) (Karras et al., 2017). Originally designed to train GANs (Goodfellow et al., 2014), PT was proposed as a heuristic, with no convergence analysis even for the simplest objective functions. On the contrary, to the best of our knowledge, RPT is the first PT-type algorithm with rigorous and sound theoretical guarantees for general smooth objective functions. We cast our method into the established framework of Randomized Coordinate Descent (RCD) (Nesterov, 2012; Richt\'arik &amp; Tak\'a\v{c}, 2014), for which (as a by-product of our investigations) we also propose a novel, simple and general convergence analysis encapsulating strongly-convex, convex and nonconvex objectives. We then use this framework to establish a convergence theory for RPT. Finally, we validate the effectiveness of our method through extensive computational experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#19988;&#20581;&#22766;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20844;&#24179;&#32422;&#26463;&#19979;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#65292;&#24182;&#21487;&#29992;&#20110;&#26435;&#34913;&#20844;&#24179;&#21644;&#26368;&#22823;&#31119;&#21033;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.03625</link><description>&lt;p&gt;
&#20844;&#24179;&#19988;&#20581;&#22766;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#25919;&#31574;&#23398;&#20064;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning. (arXiv:2306.03625v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#19988;&#20581;&#22766;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20844;&#24179;&#32422;&#26463;&#19979;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#65292;&#24182;&#21487;&#29992;&#20110;&#26435;&#34913;&#20844;&#24179;&#21644;&#26368;&#22823;&#31119;&#21033;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20844;&#24179;&#32422;&#26463;&#26465;&#20214;&#19979;&#38750;&#21442;&#25968;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#12290;&#22312;&#26631;&#20934;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#24471;&#21040;&#30340;&#20272;&#35745;&#22120;&#20855;&#26377;&#21452;&#37325;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#27492;&#26694;&#26550;&#26469;&#34920;&#24449;&#20844;&#24179;&#21644;&#26368;&#20339;&#25919;&#31574;&#21487;&#23454;&#29616;&#30340;&#26368;&#22823;&#31119;&#21033;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple and general framework for nonparametric estimation of heterogeneous treatment effects under fairness constraints. Under standard regularity conditions, we show that the resulting estimators possess the double robustness property. We use this framework to characterize the trade-off between fairness and the maximum welfare achievable by the optimal policy. We evaluate the methods in a simulation study and illustrate them in a real-world case study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#19968;&#31181;&#26131;&#20110;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20351;&#20854;&#20135;&#29983;&#22522;&#20110;&#33033;&#20914;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#36827;&#34892;&#20102;&#33033;&#20914;&#32593;&#32476;&#30340;&#35757;&#32451;&#21518;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03623</link><description>&lt;p&gt;
&#32463;&#20856;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#33033;&#20914;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Spike-based computation using classical recurrent neural networks. (arXiv:2306.03623v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#19968;&#31181;&#26131;&#20110;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20351;&#20854;&#20135;&#29983;&#22522;&#20110;&#33033;&#20914;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#36827;&#34892;&#20102;&#33033;&#20914;&#32593;&#32476;&#30340;&#35757;&#32451;&#21518;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#36890;&#20449;&#20165;&#30001;&#20107;&#20214;&#25110;&#25152;&#35859;&#30340;&#33033;&#20914;&#32452;&#25104;&#12290;&#36825;&#31181;&#29305;&#24615;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36827;&#34892;&#24322;&#27493;&#21644;&#31232;&#30095;&#35745;&#31639;&#65292;&#24182;&#22240;&#27492;&#22312;&#19987;&#29992;&#30828;&#20214;&#19978;&#36816;&#34892;&#26102;&#22823;&#24133;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#37319;&#29992;&#19968;&#31181;&#23545;&#31216;&#30340;&#26041;&#27861;&#65306;&#20462;&#25913;&#19968;&#31181;&#24050;&#30693;&#30340;&#12289;&#26131;&#20110;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20351;&#20854;&#20135;&#29983;&#22522;&#20110;&#33033;&#20914;&#30340;&#35745;&#31639;&#12290;&#36890;&#36807;&#26126;&#30830;&#24341;&#20837;&#33033;&#20914;&#38408;&#20540;&#21644;&#37325;&#32622;&#26426;&#21046;&#65292;&#25105;&#20204;&#20351;&#32593;&#32476;&#33021;&#22815;&#20165;&#20351;&#29992;&#33033;&#20914;&#26469;&#25191;&#34892;&#21069;&#21521;&#21644;&#24490;&#29615;&#35745;&#31639;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20462;&#25913;&#21518;&#30340;&#26500;&#26550;&#26082;&#21487;&#20197;&#23454;&#29616;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;ImageNet&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks are a type of artificial neural networks in which communication between neurons is only made of events, also called spikes. This property allows neural networks to make asynchronous and sparse computations and therefore to drastically decrease energy consumption when run on specialized hardware. However, training such networks is known to be difficult, mainly due to the non-differentiability of the spike activation, which prevents the use of classical backpropagation. This is because state-of-the-art spiking neural networks are usually derived from biologically-inspired neuron models, to which are applied machine learning methods for training. Nowadays, research about spiking neural networks focuses on the design of training algorithms whose goal is to obtain networks that compete with their non-spiking version on specific tasks. In this paper, we attempt the symmetrical approach: we modify the dynamics of a well-known, easily trainable type of recurrent neural 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28304;&#20219;&#21153;&#20013;&#30340;&#26631;&#31614;&#20559;&#22909;&#25968;&#25454;&#26469;&#25512;&#26029;&#30446;&#26631;&#20219;&#21153;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#20559;&#22909;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;Gromov-Wasserstein&#36317;&#31163;&#23545;&#40784;&#20219;&#21153;&#20043;&#38388;&#30340;&#36712;&#36857;&#20998;&#24067;&#12290;&#36890;&#36807;&#20132;&#26367;&#36827;&#34892;&#20559;&#22909;&#25512;&#26029;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#25913;&#36827;&#25512;&#26029;&#20986;&#30340;&#26631;&#31614;&#24182;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#24335;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#26631;&#35760;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.03615</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#31163;&#32447;RL&#30340;&#38646;&#26679;&#26412;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Preference Learning for Offline RL via Optimal Transport. (arXiv:2306.03615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28304;&#20219;&#21153;&#20013;&#30340;&#26631;&#31614;&#20559;&#22909;&#25968;&#25454;&#26469;&#25512;&#26029;&#30446;&#26631;&#20219;&#21153;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#20559;&#22909;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;Gromov-Wasserstein&#36317;&#31163;&#23545;&#40784;&#20219;&#21153;&#20043;&#38388;&#30340;&#36712;&#36857;&#20998;&#24067;&#12290;&#36890;&#36807;&#20132;&#26367;&#36827;&#34892;&#20559;&#22909;&#25512;&#26029;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#25913;&#36827;&#25512;&#26029;&#20986;&#30340;&#26631;&#31614;&#24182;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#24335;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#26631;&#35760;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;(PbRL)&#24050;&#32463;&#22312;&#23558;&#22870;&#21169;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#32780;&#19988;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#33719;&#21462;&#30340;&#26114;&#36149;&#30340;&#20559;&#22909;&#25968;&#25454;&#36890;&#24120;&#19981;&#33021;&#37325;&#22797;&#20351;&#29992;&#20110;&#21518;&#32493;&#20219;&#21153;&#23398;&#20064;&#20013;&#65292;&#23548;&#33268;&#38656;&#35201;&#23545;&#27599;&#20010;&#26032;&#20219;&#21153;&#36827;&#34892;&#22823;&#37327;&#30340;&#26631;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#31639;&#27861;&#65292;&#20197;&#21033;&#29992;&#28304;&#20219;&#21153;&#20013;&#30340;&#26631;&#35760;&#20559;&#22909;&#25968;&#25454;&#26469;&#25512;&#26029;&#30446;&#26631;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#20154;&#31867;&#26597;&#35810;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Gromov-Wasserstein&#36317;&#31163;&#26469;&#23545;&#40784;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#36712;&#36857;&#20998;&#24067;&#12290;&#27714;&#35299;&#30340;&#26368;&#20248;&#20256;&#36755;&#30697;&#38453;&#20316;&#20026;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#36712;&#36857;&#23545;&#24212;&#20851;&#31995;&#30340;&#19968;&#20010;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#24471;&#35782;&#21035;&#20219;&#21153;&#20043;&#38388;&#23545;&#24212;&#30340;&#36712;&#36857;&#23545;&#21644;&#20256;&#36882;&#20559;&#22909;&#26631;&#31614;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#25512;&#26029;&#20986;&#30340;&#26631;&#31614;&#20013;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#30340;&#32047;&#31215;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#26367;&#24335;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20132;&#26367;&#36827;&#34892;&#20559;&#22909;&#25512;&#26029;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#25913;&#36827;&#25512;&#26029;&#20986;&#30340;&#26631;&#31614;&#24182;&#35757;&#32451;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#38646;&#26679;&#26412;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based Reinforcement Learning (PbRL) has demonstrated remarkable efficacy in aligning rewards with human intentions. However, a significant challenge lies in the need of substantial human labels, which is costly and time-consuming. Additionally, the expensive preference data obtained from prior tasks is not typically reusable for subsequent task learning, leading to extensive labeling for each new task. In this paper, we propose a novel zero-shot preference-based RL algorithm that leverages labeled preference data from source tasks to infer labels for target tasks, eliminating the requirement for human queries. Our approach utilizes Gromov-Wasserstein distance to align trajectory distributions between source and target tasks. The solved optimal transport matrix serves as a correspondence between trajectories of two tasks, making it possible to identify corresponding trajectory pairs between tasks and transfer the preference labels. However, learning directly from inferred lab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36141;&#20080;&#20449;&#24687;&#29992;&#20110;&#38543;&#26426;&#20248;&#21270;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;2&#31454;&#20105;&#24615;&#30340;&#30830;&#23450;&#24615;&#31639;&#27861;&#21644;&#19968;&#20010;e/(e-1)&#31454;&#20105;&#24615;&#30340;&#38543;&#26426;&#31639;&#27861;&#26469;&#36141;&#20080;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#27604;&#29575;&#26159;&#26368;&#32039;&#30340;&#65292;&#36824;&#32771;&#34385;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.03607</link><description>&lt;p&gt;
&#36141;&#20080;&#20449;&#24687;&#29992;&#20110;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Buying Information for Stochastic Optimization. (arXiv:2306.03607v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36141;&#20080;&#20449;&#24687;&#29992;&#20110;&#38543;&#26426;&#20248;&#21270;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;2&#31454;&#20105;&#24615;&#30340;&#30830;&#23450;&#24615;&#31639;&#27861;&#21644;&#19968;&#20010;e/(e-1)&#31454;&#20105;&#24615;&#30340;&#38543;&#26426;&#31639;&#27861;&#26469;&#36141;&#20080;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#27604;&#29575;&#26159;&#26368;&#32039;&#30340;&#65292;&#36824;&#32771;&#34385;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20248;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#20043;&#19968;&#12290;&#22312;&#26631;&#20934;&#27169;&#22411;&#20013;&#65292;&#31639;&#27861;&#20250;&#25552;&#21069;&#32473;&#20986;&#19968;&#20010;&#22266;&#23450;&#30340;&#20998;&#24067;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#21487;&#33021;&#38656;&#35201;&#33457;&#36153;&#39069;&#22806;&#30340;&#20195;&#20215;&#26469;&#33719;&#21462;&#26356;&#22909;&#30340;&#20915;&#31574;&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36141;&#20080;&#20449;&#24687;&#20197;&#36827;&#34892;&#38543;&#26426;&#20248;&#21270;&#65292;&#24182;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;&#20551;&#35774;&#23398;&#20064;&#32773;&#26377;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#30340;&#39044;&#35328;&#26426;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010; $2$- &#31454;&#20105;&#24615;&#30830;&#23450;&#24615;&#31639;&#27861;&#21644;&#19968;&#20010; $e/(e-1)$- &#31454;&#20105;&#24615;&#38543;&#26426;&#31639;&#27861;&#26469;&#36141;&#20080;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#27604;&#29575;&#26159;&#26368;&#32039;&#30340;&#65292;&#22240;&#20026;&#36825;&#20010;&#38382;&#39064;&#31561;&#20215;&#20110;&#19968;&#20010;&#31216;&#20026;&#36229;&#32423;&#38789;&#20572;&#27490;&#30340;&#28369;&#38634;&#31199;&#29992;&#38382;&#39064;&#30340;&#40065;&#26834;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#22312;&#23545;&#24213;&#23618;&#20248;&#21270;&#38382;&#39064;&#37319;&#21462;&#19968;&#20123;&#34892;&#21160;&#21518;&#36873;&#25321;&#36141;&#20080;&#20449;&#24687;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#23567;&#21270;&#24635;&#38598;&#35206;&#30422;&#65292;&#20854;&#20013;th
&lt;/p&gt;
&lt;p&gt;
Stochastic optimization is one of the central problems in Machine Learning and Theoretical Computer Science. In the standard model, the algorithm is given a fixed distribution known in advance. In practice though, one may acquire at a cost extra information to make better decisions. In this paper, we study how to buy information for stochastic optimization and formulate this question as an online learning problem. Assuming the learner has an oracle for the original optimization problem, we design a $2$-competitive deterministic algorithm and a $e/(e-1)$-competitive randomized algorithm for buying information. We show that this ratio is tight as the problem is equivalent to a robust generalization of the ski-rental problem, which we call super-martingale stopping.  We also consider an adaptive setting where the learner can choose to buy information after taking some actions for the underlying optimization problem. We focus on the classic optimization problem, Min-Sum Set Cover, where th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#8212;&#8212;&#24378;&#36866;&#24212;&#23545;&#25163;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23545;&#25163;&#21644;&#25968;&#25454;&#20998;&#24067;&#38382;&#39064;&#12290;&#20316;&#32773;&#20351;&#29992;&#22810;&#24230;&#37327;&#35843;&#26597;&#26469;&#22686;&#24378; FL &#23545;&#36825;&#20123;&#23545;&#25163;&#30340;&#25269;&#25239;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03600</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#24230;&#37327;&#35843;&#26597;&#36991;&#20813;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Avoid Adversarial Adaption in Federated Learning by Multi-Metric Investigations. (arXiv:2306.03600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#8212;&#8212;&#24378;&#36866;&#24212;&#23545;&#25163;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23545;&#25163;&#21644;&#25968;&#25454;&#20998;&#24067;&#38382;&#39064;&#12290;&#20316;&#32773;&#20351;&#29992;&#22810;&#24230;&#37327;&#35843;&#26597;&#26469;&#22686;&#24378; FL &#23545;&#36825;&#20123;&#23545;&#25163;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#21487;&#20197;&#22312;&#20998;&#24067;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36991;&#20813;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#20301;&#32622;&#12290;&#36825;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;FL &#23481;&#26131;&#21463;&#21040;&#27745;&#26579;&#25915;&#20987;&#65292;&#21487;&#20197;&#26159;&#38750;&#23450;&#21521;&#30340;&#65292;&#26088;&#22312;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#20063;&#21487;&#20197;&#26159;&#26377;&#30446;&#30340;&#30340;&#65292;&#21363;&#25152;&#35859;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#20250;&#28155;&#21152;&#23545;&#25239;&#24615;&#34892;&#20026;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#21046;&#20316;&#30340;&#36755;&#20837;&#35302;&#21457;&#12290;&#20026;&#20102;&#36861;&#27714;&#38544;&#34109;&#24615;&#65292;&#21518;&#38376;&#25915;&#20987;&#26356;&#38590;&#24212;&#23545;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#32531;&#35299;&#25216;&#26415;&#20381;&#36182;&#20110;&#30417;&#35270;&#26576;&#20123;&#24230;&#37327;&#26631;&#20934;&#21644;&#36807;&#28388;&#24694;&#24847;&#27169;&#22411;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#27809;&#26377;&#32771;&#34385;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#23545;&#25163;&#21644;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#28857;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#8212;&#8212;&#24378;&#36866;&#24212;&#23545;&#25163;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#36866;&#24212;&#22810;&#20010;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#27979;&#35797;&#35777;&#26126;&#65292;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#22312;&#36825;&#31181;&#23545;&#25163;&#27169;&#22411;&#20013;&#21487;&#20197;&#34987;&#32469;&#36807;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#21487;&#20197;&#20351;&#29992;&#22810;&#24230;&#37327;&#26631;&#20934;&#35843;&#26597;&#26469;&#26174;&#33879;&#25552;&#39640; FL &#23545;&#24378;&#36866;&#24212;&#23545;&#25163;&#30340;&#38887;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#25110;&#27169;&#22411;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) trains machine learning models on data distributed across multiple devices, avoiding data transfer to a central location. This improves privacy, reduces communication costs, and enhances model performance. However, FL is prone to poisoning attacks, which can be untargeted aiming to reduce the model performance, or targeted, so-called backdoors, which add adversarial behavior that can be triggered with appropriately crafted inputs. Striving for stealthiness, backdoor attacks are harder to deal with.  Mitigation techniques against poisoning attacks rely on monitoring certain metrics and filtering malicious model updates. However, previous works didn't consider real-world adversaries and data distributions. To support our statement, we define a new notion of strong adaptive adversaries that can simultaneously adapt to multiple objectives and demonstrate through extensive tests, that existing defense methods can be circumvented in this adversary model. We also demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#33410;&#28857;&#20043;&#38388;&#25104;&#23545;&#20132;&#20114;&#30340;&#27700;&#24179;&#65292;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20855;&#26377;&#19968;&#23450;&#23481;&#37327;&#30340;MPNN&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#33410;&#28857;&#29305;&#24449;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20102;&#20445;&#35777;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#20805;&#20998;&#36890;&#20449;&#65292;MPNN&#30340;&#23481;&#37327;&#24517;&#39035;&#26159;...</title><link>http://arxiv.org/abs/2306.03589</link><description>&lt;p&gt;
&#36807;&#24230;&#21387;&#32553;&#22914;&#20309;&#24433;&#21709;GNN&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How does over-squashing affect the power of GNNs?. (arXiv:2306.03589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#33410;&#28857;&#20043;&#38388;&#25104;&#23545;&#20132;&#20114;&#30340;&#27700;&#24179;&#65292;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20855;&#26377;&#19968;&#23450;&#23481;&#37327;&#30340;MPNN&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#33410;&#28857;&#29305;&#24449;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20102;&#20445;&#35777;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#20805;&#20998;&#36890;&#20449;&#65292;MPNN&#30340;&#23481;&#37327;&#24517;&#39035;&#26159;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#26368;&#27969;&#34892;&#30340;GNN&#31867;&#21035;&#26159;&#36890;&#36807;&#30456;&#37051;&#33410;&#28857;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#26469;&#25805;&#20316;&#30340;&#65292;&#31216;&#20026;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#12290;&#37492;&#20110;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20102;&#35299;MPNN&#30340;&#34920;&#36798;&#33021;&#21147;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#32467;&#26524;&#36890;&#24120;&#32771;&#34385;&#20855;&#26377;&#26080;&#20449;&#24687;&#33410;&#28857;&#29305;&#24449;&#30340;&#29615;&#22659;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#20855;&#26377;&#19968;&#23450;&#23481;&#37327;&#30340;MPNN&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#33410;&#28857;&#29305;&#24449;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;MPNN&#20801;&#35768;&#30340;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20132;&#20114;&#27700;&#24179;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;&#35813;&#27979;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#29305;&#24615;&#65292;&#21363;&#25152;&#35859;&#30340;&#36807;&#24230;&#21387;&#32553;&#25928;&#24212;&#65292;&#35813;&#25928;&#24212;&#34987;&#35266;&#23519;&#21040;&#26159;&#24403;&#22823;&#37327;&#30340;&#20449;&#24687;&#32858;&#21512;&#25104;&#22266;&#23450;&#22823;&#23567;&#30340;&#21521;&#37327;&#26102;&#21457;&#29983;&#30340;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#27979;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#20026;&#20102;&#20445;&#35777;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#20805;&#20998;&#36890;&#20449;&#65292;MPNN&#30340;&#23481;&#37327;&#24517;&#39035;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the expressive power of MPNNs is a key question. However, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; L-C2ST &#30340;&#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30446;&#21069;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#38480;&#21046;&#35299;&#20915;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03580</link><description>&lt;p&gt;
L-C2ST: &#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference. (arXiv:2306.03580v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; L-C2ST &#30340;&#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30446;&#21069;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#38480;&#21046;&#35299;&#20915;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#27169;&#25311;&#25512;&#26029;&#65288;SBI&#65289;&#30340;&#24037;&#20316;&#37117;&#20381;&#36182;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#36817;&#20284;&#22797;&#26434;&#12289;&#39640;&#32500;&#24230;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#20123;&#36817;&#20284;&#26159;&#21542;&#21487;&#20449;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#22312;&#35266;&#27979;&#31354;&#38388;&#26399;&#26395;&#19979;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19981;&#33021;&#36275;&#22815;&#22320;&#30830;&#23450;&#21738;&#20123;&#35266;&#27979;&#32467;&#26524;&#21487;&#20197;&#20449;&#20219;&#36825;&#20123;&#36817;&#20284;&#25110;&#24212;&#35813;&#25913;&#36827;&#12290;&#25105;&#20204;&#22522;&#20110;&#33879;&#21517;&#30340;&#20998;&#31867;&#22120;&#20004;&#26679;&#26412;&#26816;&#39564; (C2ST)&#65292;&#24341;&#20837; L-C2ST&#65292;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#12290;&#23427;&#25552;&#20379;&#26377;&#29702;&#35770;&#22522;&#30784;&#21644;&#26131;&#20110;&#35299;&#37322;&#30340;&#65292;&#22914;&#22270;&#31034;&#35786;&#26029;&#12290;&#19982; C2ST &#19981;&#21516;&#30340;&#26159;&#65292;L-C2ST &#19981;&#38656;&#35201;&#35775;&#38382;&#30495;&#23454;&#21518;&#39564;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;L-C2ST &#21487;&#20197;&#19987;&#38376;&#25552;&#20379;&#26356;&#22909;&#30340;&#32479;&#35745;&#21151;&#29575;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret - e.g. graphical - diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, L-C2ST can be specialized to offer better statistical power, while being computationally more efficien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32852;&#37030;&#21452;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;FedDVA&#65289;&#26126;&#30830;&#20998;&#35299;&#28508;&#22312;&#34920;&#31034;&#65292;&#25429;&#25417;&#20849;&#20139;&#30693;&#35782;&#21644;&#23458;&#25143;&#29305;&#23450;&#20010;&#24615;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03570</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#20010;&#24615;&#21270;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Personalization Disentanglement for Federated Learning. (arXiv:2306.03570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32852;&#37030;&#21452;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;FedDVA&#65289;&#26126;&#30830;&#20998;&#35299;&#28508;&#22312;&#34920;&#31034;&#65292;&#25429;&#25417;&#20849;&#20139;&#30693;&#35782;&#21644;&#23458;&#25143;&#29305;&#23450;&#20010;&#24615;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#24179;&#34913;&#30340;&#30693;&#35782;&#20849;&#20139;&#21644;&#27169;&#22411;&#20010;&#24615;&#21270;&#20043;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#32852;&#21512;&#35757;&#32451;&#21508;&#31181;&#23616;&#37096;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#26126;&#30830;&#20998;&#35299;&#20026;&#20004;&#20010;&#37096;&#20998;&#26469;&#35299;&#20915;PFL&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#20849;&#20139;&#30693;&#35782;&#21644;&#23458;&#25143;&#29305;&#23450;&#20010;&#24615;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;PFL&#12290;&#35813;&#20998;&#31163;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#21452;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;FedDVA&#65289;&#23454;&#29616;&#30340;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#32534;&#30721;&#22120;&#26469;&#25512;&#26029;&#20004;&#31181;&#31867;&#22411;&#30340;&#34920;&#31034;&#12290;FedDVA&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;PFL&#20013;&#20840;&#23616;&#30693;&#35782;&#20849;&#20139;&#21644;&#26412;&#22320;&#20010;&#24615;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;FL&#26041;&#27861;&#38598;&#25104;&#65292;&#24182;&#23558;&#23427;&#20204;&#36716;&#21464;&#20026;&#29992;&#20110;&#24322;&#26500;&#19979;&#28216;&#20219;&#21153;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#20998;&#31163;&#25152;&#24102;&#26469;&#30340;&#20248;&#21183;&#65292;&#24182;&#34920;&#26126;&#32463;&#36807;&#20998;&#31163;&#35757;&#32451;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#37027;&#20123;&#26222;&#36890;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning (PFL) jointly trains a variety of local models through balancing between knowledge sharing across clients and model personalization per client. This paper addresses PFL via explicit disentangling latent representations into two parts to capture the shared knowledge and client-specific personalization, which leads to more reliable and effective PFL. The disentanglement is achieved by a novel Federated Dual Variational Autoencoder (FedDVA), which employs two encoders to infer the two types of representations. FedDVA can produce a better understanding of the trade-off between global knowledge sharing and local personalization in PFL. Moreover, it can be integrated with existing FL methods and turn them into personalized models for heterogeneous downstream tasks. Extensive experiments validate the advantages caused by disentanglement and show that models trained with disentangled representations substantially outperform those vanilla methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#21452;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#24207;&#21015;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25511;&#21046;&#35823;&#24046;&#24182;&#25913;&#21892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.03566</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#30340;&#21452;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Memory-Based Dual Gaussian Processes for Sequential Learning. (arXiv:2306.03566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#21452;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#24207;&#21015;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25511;&#21046;&#35823;&#24046;&#24182;&#25913;&#21892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#21644;&#20027;&#21160;&#23398;&#20064;&#20013;&#65292;&#35775;&#38382;&#36807;&#21435;&#25968;&#25454;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#22240;&#27492;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#36827;&#34892;&#24207;&#21015;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#21518;&#39564;&#12289;&#36229;&#21442;&#25968;&#21644;&#35825;&#23548;&#28857;&#30340;&#19981;&#20934;&#30830;&#24615;&#23548;&#33268;&#38169;&#35823;&#38543;&#26102;&#38388;&#32047;&#31215;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#23398;&#20064;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#21452;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#26469;&#25511;&#21046;&#25152;&#26377;&#36825;&#20123;&#35823;&#24046;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36827;&#34892;&#36890;&#29992;&#20284;&#28982;&#30340;&#20934;&#30830;&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#20027;&#21160;&#24314;&#31435;&#21644;&#26356;&#26032;&#36807;&#21435;&#25968;&#25454;&#30340;&#35760;&#24518;&#26469;&#25913;&#21892;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#36125;&#21494;&#26031;&#20248;&#21270;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#36830;&#32493;&#23398;&#20064;&#30340;&#20960;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential learning with Gaussian processes (GPs) is challenging when access to past data is limited, for example, in continual and active learning. In such cases, errors can accumulate over time due to inaccuracies in the posterior, hyperparameters, and inducing points, making accurate learning challenging. Here, we present a method to keep all such errors in check using the recently proposed dual sparse variational GP. Our method enables accurate inference for generic likelihoods and improves learning by actively building and updating a memory of past data. We demonstrate its effectiveness in several applications involving Bayesian optimization, active learning, and continual learning.
&lt;/p&gt;</description></item><item><title>CIN++&#26159;&#19968;&#31181;&#25299;&#25169;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#24213;&#23618;&#22797;&#21512;&#20307;&#20013;&#29615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#34920;&#36798;&#33021;&#21147;&#12289;&#22788;&#29702;&#38271;&#31243;&#20132;&#20114;&#21644;&#24314;&#27169;&#39640;&#38454;&#32467;&#26500;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03561</link><description>&lt;p&gt;
CIN++&#65306;&#22686;&#24378;&#25299;&#25169;&#20449;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
CIN++: Enhancing Topological Message Passing. (arXiv:2306.03561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03561
&lt;/p&gt;
&lt;p&gt;
CIN++&#26159;&#19968;&#31181;&#25299;&#25169;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#24213;&#23618;&#22797;&#21512;&#20307;&#20013;&#29615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#34920;&#36798;&#33021;&#21147;&#12289;&#22788;&#29702;&#38271;&#31243;&#20132;&#20114;&#21644;&#24314;&#27169;&#39640;&#38454;&#32467;&#26500;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#23384;&#22312;&#26174;&#33879;&#30340;&#23616;&#38480;&#24615;&#65292;&#38590;&#20197;&#22788;&#29702;&#38271;&#31243;&#20132;&#20114;&#65292;&#24182;&#32570;&#20047;&#23545;&#24314;&#27169;&#39640;&#38454;&#32467;&#26500;&#21644;&#32676;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#22522;&#26412;&#26041;&#27861;&#12290;&#32454;&#32990;&#21516;&#26500;&#32593;&#32476;&#26368;&#36817;&#36890;&#36807;&#22522;&#20110;&#32454;&#32990;&#22797;&#21512;&#20307;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#35299;&#20915;&#20102;&#22823;&#37096;&#20998;&#36825;&#20123;&#25361;&#25112;&#12290;&#23613;&#31649;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;CIN&#20165;&#20351;&#29992;&#36793;&#30028;&#21644;&#19978;&#37096;&#20449;&#24687;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#24213;&#23618;&#22797;&#21512;&#20307;&#20013;&#23384;&#22312;&#30340;&#29615;&#20043;&#38388;&#30340;&#30452;&#25509;&#30456;&#20114;&#20316;&#29992;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23545;&#23398;&#20064;&#35768;&#22810;&#30495;&#23454;&#22797;&#26434;&#29616;&#35937;&#30340;&#34920;&#31034;&#38750;&#24120;&#37325;&#35201;&#65292;&#22914;&#36229;&#20998;&#23376;&#32452;&#35013;&#30340;&#21160;&#21147;&#23398;&#12289;&#33041;&#20869;&#31070;&#32463;&#27963;&#21160;&#21644;&#22522;&#22240;&#35843;&#25511;&#36807;&#31243;&#12290;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;CIN++&#65292;&#36825;&#26159;CIN&#24341;&#20837;&#30340;&#25299;&#25169;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#19968;&#31181;&#22686;&#24378;&#29256;&#12290;&#25105;&#20204;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#32771;&#34385;&#20102;&#29615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have demonstrated remarkable success in learning from graph-structured data. However, they face significant limitations in expressive power, struggling with long-range interactions and lacking a principled approach to modeling higher-order structures and group interactions. Cellular Isomorphism Networks (CINs) recently addressed most of these challenges with a message passing scheme based on cell complexes. Despite their advantages, CINs make use only of boundary and upper messages which do not consider a direct interaction between the rings present in the underlying complex. Accounting for these interactions might be crucial for learning representations of many real-world complex phenomena such as the dynamics of supramolecular assemblies, neural activity within the brain, and gene regulation processes. In this work, we propose CIN++, an enhancement of the topological message passing scheme introduced in CINs. Our message passing scheme accounts for the af
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#25216;&#26415;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#20998;&#31867;&#21644;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.03558</link><description>&lt;p&gt;
&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;&#65288;arXiv:2306.03558v1 [cs.CR]&#65289;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning: A Survey. (arXiv:2306.03558v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#25216;&#26415;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#20998;&#31867;&#21644;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#24182;&#21457;&#23637;&#25104;&#20026;&#24191;&#27867;&#25104;&#21151;&#24212;&#29992;&#30340;&#20419;&#25104;&#25216;&#26415;&#65292;&#20363;&#22914;&#26234;&#33021;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#35782;&#21035;&#12289;&#21307;&#23398;&#35786;&#26029;&#31561;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#12289;&#21487;&#29992;&#24615;&#21644;/&#25110;&#36951;&#24536;&#26435;&#65292;&#26377;&#19968;&#20010;&#29305;&#27530;&#30340;&#38656;&#27714;&#38656;&#35201;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#26377;&#20851;&#26576;&#20123;&#29305;&#23450;&#26679;&#26412;&#30340;&#20449;&#24687;&#65292;&#31216;&#20026;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#12290;&#36825;&#31181;&#26032;&#20852;&#25216;&#26415;&#22240;&#20854;&#21019;&#26032;&#21644;&#23454;&#29992;&#24615;&#32780;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20852;&#36259;&#12290;&#21516;&#26102;&#65292;&#36825;&#20010;&#38596;&#24515;&#21187;&#21187;&#30340;&#38382;&#39064;&#24050;&#32463;&#23548;&#33268;&#20102;&#20247;&#22810;&#30340;&#30740;&#31350;&#21162;&#21147;&#65292;&#26088;&#22312;&#24212;&#23545;&#23427;&#30340;&#25361;&#25112;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#19968;&#39033;&#30740;&#31350;&#20998;&#26512;&#36825;&#20010;&#22797;&#26434;&#30340;&#20027;&#39064;&#25110;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#24773;&#26223;&#20013;&#27604;&#36739;&#29616;&#26377;&#30340;&#21462;&#28040;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#36825;&#20010;&#32508;&#36848;&#65292;&#25105;&#20204;&#26088;&#22312;&#25226;&#25569;&#21462;&#28040;&#23398;&#20064;&#25216;&#26415;&#30340;&#20851;&#38190;&#27010;&#24565;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26681;&#25454;&#23427;&#20204;&#30340;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has attracted widespread attention and evolved into an enabling technology for a wide range of highly successful applications, such as intelligent computer vision, speech recognition, medical diagnosis, and more. Yet a special need has arisen where, due to privacy, usability, and/or the right to be forgotten, information about some specific samples needs to be removed from a model, called machine unlearning. This emerging technology has drawn significant interest from both academics and industry due to its innovation and practicality. At the same time, this ambitious problem has led to numerous research efforts aimed at confronting its challenges. To the best of our knowledge, no study has analyzed this complex topic or compared the feasibility of existing unlearning solutions in different kinds of scenarios. Accordingly, with this survey, we aim to capture the key concepts of unlearning techniques. The existing solutions are classified and summarized based on their ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03552</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#21160;&#24577;&#20559;&#31227;&#30340;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#21463;&#21040;&#21160;&#24577;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#30340;&#29615;&#22659;&#21160;&#24577;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#29615;&#22659;&#21442;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26681;&#25454;&#20854;&#29615;&#22659;&#21442;&#25968;&#23558;&#24102;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20998;&#24320;&#20197;&#35757;&#32451;&#30456;&#24212;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20986;&#29616;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#26159;&#8220;&#29305;&#23450;&#22330;&#26223;&#8221;&#20351;&#29992;&#30340;&#65292;&#38024;&#23545;&#26576;&#20010;&#29615;&#22659;&#35757;&#32451;&#30340;&#31574;&#30053;&#19981;&#33021;&#20174;&#25910;&#38598;&#22312;&#20854;&#20182;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#25152;&#26377;&#20854;&#20182;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#21644;&#19981;&#21516;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#31574;&#30053;&#20855;&#26377;&#31867;&#20284;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#65292;&#24182;&#20174;&#20855;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31283;&#24577;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#37325;&#29992;&#12290;&#36825;&#31181;&#20998;&#24067;&#29992;&#20110;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23548;&#33268;&#20102; SRPO&#65288;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;&#65289;&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SRPO &#22312;&#20855;&#26377;&#21160;&#24577;&#20559;&#31227;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#24212;&#29992;&#20110;&#24037;&#19994;4.0&#22330;&#26223;&#65292;&#24182;&#25913;&#36827;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#27010;&#24565;&#37325;&#35201;&#24615;&#35745;&#31639;&#31243;&#24207;&#65292;&#23558;&#23616;&#37096;&#29305;&#24449;&#19982;&#25972;&#20307;&#22270;&#20687;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2306.03551</link><description>&lt;p&gt;
&#24037;&#19994;4.0&#20013;&#21487;&#25193;&#23637;&#30340;&#27010;&#24565;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Scalable Concept Extraction in Industry 4.0. (arXiv:2306.03551v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#24212;&#29992;&#20110;&#24037;&#19994;4.0&#22330;&#26223;&#65292;&#24182;&#25913;&#36827;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#27010;&#24565;&#37325;&#35201;&#24615;&#35745;&#31639;&#31243;&#24207;&#65292;&#23558;&#23616;&#37096;&#29305;&#24449;&#19982;&#25972;&#20307;&#22270;&#20687;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;4.0&#27491;&#22312;&#21033;&#29992;&#25968;&#23383;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#36830;&#25509;&#21644;&#20248;&#21270;&#21046;&#36896;&#36807;&#31243;&#12290;&#36825;&#19968;&#27010;&#24565;&#30340;&#26680;&#24515;&#22312;&#20110;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#21270;&#20026;&#21487;&#38752;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#30693;&#35782;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#22788;&#29702;&#22270;&#20687;&#25968;&#25454;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#8220;&#40657;&#30418;&#23376;&#8221;&#26412;&#36136;&#20351;&#24471;&#23427;&#20204;&#30340;&#39044;&#27979;&#36807;&#31243;&#38590;&#20197;&#29702;&#35299;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20986;&#20102;&#27010;&#24565;&#30340;&#25552;&#21462;&#21644;&#23450;&#20301;&#65292;&#21363;&#35270;&#35273;&#32447;&#32034;&#22914;&#20309;&#20171;&#20837;CNN&#30340;&#39044;&#27979;&#36807;&#31243;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#24037;&#19994;4.0&#22330;&#26223;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;&#25216;&#26415;&#8220;&#20351;&#29992;&#26412;&#22320;&#32858;&#21512;&#25551;&#36848;&#31526;&#25552;&#21462;&#27010;&#24565;&#8221;&#65288;ECLAD&#65289;&#65292;&#24182;&#25913;&#36827;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#37325;&#35201;&#24615;&#35745;&#31639;&#31243;&#24207;&#65292;&#21033;&#29992;&#21152;&#26435;&#24179;&#22343;&#26469;&#23558;&#23616;&#37096;&#29305;&#24449;&#19982;&#25972;&#20307;&#22270;&#20687;&#32852;&#31995;&#36215;&#26469;&#65292;&#25913;&#21892;ECLAD&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The industry 4.0 is leveraging digital technologies and machine learning techniques to connect and optimize manufacturing processes. Central to this idea is the ability to transform raw data into human understandable knowledge for reliable data-driven decision-making. Convolutional Neural Networks (CNNs) have been instrumental in processing image data, yet, their ``black box'' nature complicates the understanding of their prediction process. In this context, recent advances in the field of eXplainable Artificial Intelligence (XAI) have proposed the extraction and localization of concepts, or which visual cues intervene on the prediction process of CNNs. This paper tackles the application of concept extraction (CE) methods to industry 4.0 scenarios. To this end, we modify a recently developed technique, ``Extracting Concepts with Local Aggregated Descriptors'' (ECLAD), improving its scalability. Specifically, we propose a novel procedure for calculating concept importance, utilizing a w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#22343;&#20540;&#21453;&#21521;&#31215;&#20998;&#22120;&#65288;MII&#65289;&#65292;&#23427;&#21487;&#29992;&#20110;&#20174;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#36817;&#20284;&#21160;&#24577;&#31995;&#32479;&#30340;&#21521;&#37327;&#22330;&#65292;&#36890;&#36807;&#19982;&#21333;&#38544;&#24335;&#40857;&#26684;-&#24211;&#22612;&#26041;&#27861;&#65288;MIRK&#65289;&#30340;&#36830;&#25509;&#65292;&#26174;&#24335;&#22320;&#33719;&#24471;&#25439;&#22833;&#20989;&#25968;&#34920;&#36798;&#24335;&#65292;&#23454;&#29616;&#35299;&#38145;&#34987;&#21021;&#20540;&#38382;&#39064;&#25551;&#36848;&#30340;&#23545;&#31216;&#21644;&#39640;&#38454;&#31215;&#20998;&#22120;&#65292;&#19982;&#20165;&#20351;&#29992;&#25968;&#20540;&#31215;&#20998;&#22120;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;&#36890;&#36807;&#23558;MIRK&#24212;&#29992;&#20110;MII&#30340;&#32452;&#21512;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#38477;&#20302;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.03548</link><description>&lt;p&gt;
&#29992;&#21453;&#26174;&#24335;&#31215;&#20998;&#22120;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamical Systems from Noisy Data with Inverse-Explicit Integrators. (arXiv:2306.03548v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#22343;&#20540;&#21453;&#21521;&#31215;&#20998;&#22120;&#65288;MII&#65289;&#65292;&#23427;&#21487;&#29992;&#20110;&#20174;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#36817;&#20284;&#21160;&#24577;&#31995;&#32479;&#30340;&#21521;&#37327;&#22330;&#65292;&#36890;&#36807;&#19982;&#21333;&#38544;&#24335;&#40857;&#26684;-&#24211;&#22612;&#26041;&#27861;&#65288;MIRK&#65289;&#30340;&#36830;&#25509;&#65292;&#26174;&#24335;&#22320;&#33719;&#24471;&#25439;&#22833;&#20989;&#25968;&#34920;&#36798;&#24335;&#65292;&#23454;&#29616;&#35299;&#38145;&#34987;&#21021;&#20540;&#38382;&#39064;&#25551;&#36848;&#30340;&#23545;&#31216;&#21644;&#39640;&#38454;&#31215;&#20998;&#22120;&#65292;&#19982;&#20165;&#20351;&#29992;&#25968;&#20540;&#31215;&#20998;&#22120;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;&#36890;&#36807;&#23558;MIRK&#24212;&#29992;&#20110;MII&#30340;&#32452;&#21512;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#38477;&#20302;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22343;&#20540;&#21453;&#21521;&#31215;&#20998;&#22120;&#65288;MII&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20174;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#36817;&#20284;&#21160;&#24577;&#31995;&#32479;&#30340;&#21521;&#37327;&#22330;&#26102;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#24179;&#22343;&#36890;&#36807;&#25968;&#20540;&#31215;&#20998;&#22120;&#65288;&#22914;&#40857;&#26684;-&#24211;&#22612;&#26041;&#27861;&#65289;&#33719;&#24471;&#30340;&#22810;&#26465;&#36712;&#36857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#19982;MII&#36830;&#25509;&#26102;&#65292;&#31867;&#21035;&#20026;&#21333;&#38544;&#24335;&#40857;&#26684;-&#24211;&#22612;&#26041;&#27861;&#65288;MIRK&#65289;&#20855;&#26377;&#29305;&#27530;&#20248;&#21183;&#12290;&#24403;&#23558;&#35757;&#32451;&#25968;&#25454;&#25554;&#20837;MIRK&#20844;&#24335;&#20013;&#26102;&#65292;&#21487;&#33719;&#24471;&#29992;&#20110;&#21521;&#37327;&#22330;&#36817;&#20284;&#30340;&#26174;&#24335;&#25439;&#22833;&#20989;&#25968;&#34920;&#36798;&#24335;&#65292;&#35299;&#38145;&#22312;&#21021;&#20540;&#38382;&#39064;&#21457;&#29983;&#26102;&#65292;&#21542;&#21017;&#20026;&#38544;&#24335;&#30340;&#23545;&#31216;&#21644;&#39640;&#38454;&#31215;&#20998;&#22120;&#12290;&#23558;MIRK&#24212;&#29992;&#20110;MII&#20013;&#30340;&#32452;&#21512;&#26041;&#27861;&#19982;&#20165;&#20351;&#29992;&#25968;&#20540;&#31215;&#20998;&#22120;&#32780;&#19981;&#24179;&#22343;&#36712;&#36857;&#30456;&#27604;&#65292;&#35823;&#24046;&#26174;&#30528;&#38477;&#20302; &#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20960;&#20010;&#65288;&#28151;&#27788;&#65289;Hamiltonian&#31995;&#32479;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#26469;&#35777;&#26126;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;MII&#21644;MIRK&#30340;&#21442;&#25968;&#20540;&#23545;&#20110;&#36817;&#20284;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the mean inverse integrator (MII), a novel approach to increase the accuracy when training neural networks to approximate vector fields of dynamical systems from noisy data. This method can be used to average multiple trajectories obtained by numerical integrators such as Runge-Kutta methods. We show that the class of mono-implicit Runge-Kutta methods (MIRK) has particular advantages when used in connection with MII. When training vector field approximations, explicit expressions for the loss functions are obtained when inserting the training data in the MIRK formulae, unlocking symmetric and high-order integrators that would otherwise be implicit for initial value problems. The combined approach of applying MIRK within MII yields a significantly lower error compared to the plain use of the numerical integrator without averaging the trajectories. This is demonstrated with experiments using data from several (chaotic) Hamiltonian systems. Additionally, we perform a sensitiv
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#20027;&#21160;&#23398;&#20064;&#20013;&#22914;&#20309;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#21644;&#39044;&#31639;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23548;&#25968;&#27861;&#23454;&#29992;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#22320;&#35782;&#21035;&#27599;&#20010;&#39044;&#31639;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.03543</link><description>&lt;p&gt;
&#22914;&#20309;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#21644;&#39044;&#31639;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
How to Select Which Active Learning Strategy is Best Suited for Your Specific Problem and Budget. (arXiv:2306.03543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03543
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20027;&#21160;&#23398;&#20064;&#20013;&#22914;&#20309;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#21644;&#39044;&#31639;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23548;&#25968;&#27861;&#23454;&#29992;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#22320;&#35782;&#21035;&#27599;&#20010;&#39044;&#31639;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#32773;&#22312;&#19968;&#23450;&#30340;&#39044;&#31639;&#32422;&#26463;&#19979;&#20027;&#21160;&#36873;&#25321;&#26410;&#26631;&#35760;&#31034;&#20363;&#20197;&#21521;&#31070;&#35861;&#35831;&#27714;&#20854;&#26631;&#35760;&#12290;&#19981;&#21516;&#30340;&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#31574;&#30053;&#26356;&#36866;&#21512;&#19981;&#21516;&#30340;&#38382;&#39064;&#21644;&#39044;&#31639;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20107;&#20808;&#30693;&#36947;&#21738;&#20010;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26368;&#36866;&#21512;&#25163;&#22836;&#30340;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23548;&#25968;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#21160;&#24577;&#22320;&#35782;&#21035;&#27599;&#20010;&#39044;&#31639;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21270;&#24773;&#20917;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#28608;&#21457;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#24314;&#31435;&#30452;&#35273;&#12290;&#28982;&#21518;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#20307;&#38382;&#39064;&#21644;&#39044;&#31639;&#26469;&#21160;&#24577;&#36873;&#25321;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#39044;&#31639;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Active Learning (AL), a learner actively chooses which unlabeled examples to query for labels from an oracle, under some budget constraints. Different AL query strategies are more suited to different problems and budgets. Therefore, in practice, knowing in advance which AL strategy is most suited for the problem at hand remains an open problem. To tackle this challenge, we propose a practical derivative-based method that dynamically identifies the best strategy for each budget. We provide theoretical analysis of a simplified case to motivate our approach and build intuition. We then introduce a method to dynamically select an AL strategy based on the specific problem and budget. Empirical results showcase the effectiveness of our approach across diverse budgets and computer vision tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#22522;&#20934;TTAB&#65292;&#24182;&#21457;&#29616;&#20102;&#20043;&#21069;&#26041;&#27861;&#20013;&#30340;&#19977;&#20010;&#24120;&#35265;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2306.03536</link><description>&lt;p&gt;
&#27973;&#35848;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
On Pitfalls of Test-Time Adaptation. (arXiv:2306.03536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03536
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#22522;&#20934;TTAB&#65292;&#24182;&#21457;&#29616;&#20102;&#20043;&#21069;&#26041;&#27861;&#20013;&#30340;&#19977;&#20010;&#24120;&#35265;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#19968;&#33268;&#30340;&#35774;&#32622;&#21644;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#36825;&#22952;&#30861;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TTAB&#65292;&#19968;&#20010;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#22522;&#20934;&#65292;&#21253;&#25324;&#21313;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#22810;&#31181;&#19981;&#21516;&#30340;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#21644;&#20004;&#31181;&#35780;&#20272;&#21327;&#35758;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#25581;&#31034;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#30340;&#19977;&#20010;&#24120;&#35265;&#32570;&#38519;&#12290;&#39318;&#20808;&#65292;&#30001;&#20110;&#22312;&#32447;&#25209;&#27425;&#20381;&#36182;&#24615;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#27169;&#22411;&#36873;&#25321;&#65292;&#38750;&#24120;&#22256;&#38590;&#12290;&#20854;&#27425;&#65292;TTA&#30340;&#26377;&#25928;&#24615;&#22240;&#34987;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#23646;&#24615;&#32780;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#31532;&#19977;&#65292;&#21363;&#20351;&#22312;&#26368;&#20339;&#31639;&#27861;&#26465;&#20214;&#19979;&#65292;&#29616;&#26377;&#26041;&#27861;&#20063;&#19981;&#33021;&#35299;&#20915;&#25152;&#26377;&#24120;&#35265;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#25216;&#26415;&#21450;&#20854;&#19968;&#33268;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#21327;&#35758;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation (TTA) has recently emerged as a promising approach for tackling the robustness challenge under distribution shifts. However, the lack of consistent settings and systematic studies in prior literature hinders thorough assessments of existing methods. To address this issue, we present TTAB, a test-time adaptation benchmark that encompasses ten state-of-the-art algorithms, a diverse array of distribution shifts, and two evaluation protocols. Through extensive experiments, our benchmark reveals three common pitfalls in prior efforts. First, selecting appropriate hyper-parameters, especially for model selection, is exceedingly difficult due to online batch dependency. Second, the effectiveness of TTA varies greatly depending on the quality and properties of the model being adapted. Third, even under optimal algorithmic conditions, none of the existing methods are capable of addressing all common types of distribution shifts. Our findings underscore the need for future r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#25506;&#31350;&#20102;&#22312;&#21487;&#20998;&#25968;&#25454;&#19978;&#30340;&#32447;&#24615;&#20998;&#31867;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#24471;&#24369;&#27491;&#21017;&#21270;&#23398;&#20064;&#36864;&#21270;&#20026;&#35299;&#20915;&#36830;&#32493;&#26368;&#22823;&#38388;&#38548;&#38382;&#39064;&#65307;&#25552;&#20986;&#22312;&#24490;&#29615;&#21644;&#38543;&#26426;&#25490;&#24207;&#20219;&#21153;&#19979;&#30340;&#36951;&#24536;&#21644;&#20854;&#20182;&#37327;&#24863;&#20852;&#36259;&#30340;&#19978;&#38480;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;&#35757;&#32451;&#26041;&#27861;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.03534</link><description>&lt;p&gt;
&#20998;&#31867;&#27169;&#22411;&#22312;&#21487;&#20998;&#25968;&#25454;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning in Linear Classification on Separable Data. (arXiv:2306.03534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#25506;&#31350;&#20102;&#22312;&#21487;&#20998;&#25968;&#25454;&#19978;&#30340;&#32447;&#24615;&#20998;&#31867;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#24471;&#24369;&#27491;&#21017;&#21270;&#23398;&#20064;&#36864;&#21270;&#20026;&#35299;&#20915;&#36830;&#32493;&#26368;&#22823;&#38388;&#38548;&#38382;&#39064;&#65307;&#25552;&#20986;&#22312;&#24490;&#29615;&#21644;&#38543;&#26426;&#25490;&#24207;&#20219;&#21153;&#19979;&#30340;&#36951;&#24536;&#21644;&#20854;&#20182;&#37327;&#24863;&#20852;&#36259;&#30340;&#19978;&#38480;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;&#35757;&#32451;&#26041;&#27861;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#26631;&#31614;&#30340;&#21487;&#20998;&#32447;&#24615;&#20998;&#31867;&#20219;&#21153;&#24207;&#21015;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#24369;&#27491;&#21017;&#21270;&#23398;&#20064;&#36864;&#21270;&#20026;&#35299;&#20915;&#36830;&#32493;&#26368;&#22823;&#38388;&#38548;&#38382;&#39064;&#65292;&#23545;&#24212;&#20110;&#20984;&#38598;&#25237;&#24433;&#26694;&#26550;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#24320;&#21457;&#20102;&#36951;&#24536;&#21644;&#20854;&#20182;&#24863;&#20852;&#36259;&#37327;&#30340;&#19978;&#38480;&#65292;&#21253;&#25324;&#24490;&#29615;&#21644;&#38543;&#26426;&#25490;&#24207;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#27969;&#34892;&#30340;&#35757;&#32451;&#26041;&#27861;&#65288;&#22914;&#27491;&#21017;&#21270;&#35843;&#24230;&#21644;&#21152;&#26435;&#65289;&#30340;&#20960;&#20010;&#23454;&#38469;&#24433;&#21709;&#12290;&#25105;&#20204;&#25351;&#20986;&#25345;&#32493;&#20998;&#31867;&#35774;&#32622;&#21644;&#26368;&#36817;&#30740;&#31350;&#30340;&#25345;&#32493;&#22238;&#24402;&#35774;&#32622;&#20043;&#38388;&#30340;&#20960;&#20010;&#29702;&#35770;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze continual learning on a sequence of separable linear classification tasks with binary labels. We show theoretically that learning with weak regularization reduces to solving a sequential max-margin problem, corresponding to a special case of the Projection Onto Convex Sets (POCS) framework. We then develop upper bounds on the forgetting and other quantities of interest under various settings with recurring tasks, including cyclic and random orderings of tasks. We discuss several practical implications to popular training practices like regularization scheduling and weighting. We point out several theoretical differences between our continual classification setting and a recently studied continual regression setting.
&lt;/p&gt;</description></item><item><title>BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.03530</link><description>&lt;p&gt;
BackpropTools: &#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03530
&lt;/p&gt;
&lt;p&gt;
BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20135;&#29983;&#20986;&#20855;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#35757;&#32451;&#26102;&#38388;&#36807;&#38271;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#23454;&#26102;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#30340;&#32570;&#20047;&#38480;&#21046;&#20102;&#23398;&#20064;&#31574;&#30053;&#22312;&#23454;&#38469;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BackpropTools&#65292;&#19968;&#31181;&#20381;&#36182;&#24615;-free&#12289;header-only&#12289;pure C++&#30340;&#28145;&#24230;&#30417;&#30563;&#21644;&#24378;&#21270;&#23398;&#20064;&#24211;&#12290;&#21033;&#29992;&#26368;&#36817;C++&#26631;&#20934;&#30340;&#27169;&#26495;&#20803;&#32534;&#31243;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20197;&#30001;&#32534;&#35793;&#22120;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#12290;&#20854;&#26032;&#39062;&#30340;&#26550;&#26500;&#20801;&#35768;BackpropTools&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#20174;HPC&#38598;&#32676;&#12289;&#24037;&#20316;&#31449;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#21040;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#25163;&#34920;&#21644;&#24494;&#25511;&#21046;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30001;&#20110;RL&#31639;&#27861;&#19982;&#27169;&#25311;&#29615;&#22659;&#30340;&#32039;&#23494;&#38598;&#25104;&#65292;BackpropTools&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#20351;&#20854;&#25104;&#20026;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rec4Ad&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28040;&#38500;&#28120;&#23453;&#24191;&#21578;CTR&#39044;&#27979;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#12290;&#23427;&#21033;&#29992;&#25512;&#33616;&#26085;&#24535;&#26500;&#24314;&#21512;&#25104;&#26679;&#26412;&#26469;&#27169;&#25311;&#25512;&#26029;&#38598;&#30340;&#20998;&#24067;&#65292;&#28982;&#21518;&#23558;&#21512;&#25104;&#21644;&#21407;&#22987;&#26679;&#26412;&#32452;&#21512;&#20197;&#20943;&#23569;SSB&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Rec4Ad&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#26657;&#20934;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03527</link><description>&lt;p&gt;
Rec4Ad: &#29992;&#20110;&#28120;&#23453;&#24191;&#21578;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#28040;&#38500;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao. (arXiv:2306.03527v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03527
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rec4Ad&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28040;&#38500;&#28120;&#23453;&#24191;&#21578;CTR&#39044;&#27979;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#12290;&#23427;&#21033;&#29992;&#25512;&#33616;&#26085;&#24535;&#26500;&#24314;&#21512;&#25104;&#26679;&#26412;&#26469;&#27169;&#25311;&#25512;&#26029;&#38598;&#30340;&#20998;&#24067;&#65292;&#28982;&#21518;&#23558;&#21512;&#25104;&#21644;&#21407;&#22987;&#26679;&#26412;&#32452;&#21512;&#20197;&#20943;&#23569;SSB&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Rec4Ad&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#26657;&#20934;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#20316;&#20026;&#22312;&#32447;&#24191;&#21578;&#30340;&#19968;&#20010;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#20854;&#35757;&#32451;&#24120;&#24120;&#22522;&#20110;&#24191;&#21578;&#23637;&#31034;&#21644;&#29992;&#25143;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24191;&#21578;&#23637;&#31034;&#24448;&#24448;&#26159;&#26377;&#36873;&#25321;&#24615;&#30340;&#65292;&#30001;&#20110;&#20854;&#19982;&#25512;&#26029;&#20998;&#24067;&#19981;&#21516;&#65292;&#22240;&#27492;&#23384;&#22312;&#30528;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#65288;SSB&#65289;&#65292;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#26679;&#26412;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#28040;&#38500;SSB&#65292;&#20294;&#20854;&#23384;&#22312;&#39640;&#26041;&#24046;&#19988;&#27169;&#22411;&#26657;&#20934;&#24046;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#20123;&#30740;&#31350;&#20381;&#36182;&#20110;&#19981;&#36275;&#20197;&#35757;&#32451;&#24037;&#19994;&#27169;&#22411;&#30340;&#26114;&#36149;&#22343;&#21248;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25506;&#32034;&#19968;&#31181;&#26080;&#38656;&#22343;&#21248;&#25968;&#25454;&#26694;&#26550;&#28040;&#38500;&#24037;&#19994;&#27169;&#22411;&#20013;&#30340;SSB&#26159;&#20540;&#24471;&#25506;&#35752;&#30340;&#12290;&#26681;&#25454;&#28120;&#23453;&#31561;&#24179;&#21488;&#23637;&#31034;&#26377;&#26426;&#29289;&#21697;&#21644;&#36190;&#21161;&#29289;&#21697;&#30340;&#28151;&#21512;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#21483;Rec4Ad&#30340;&#31616;&#21333;&#26377;&#25928;&#26694;&#26550;&#26469;&#28040;&#38500;&#24191;&#21578;&#30340;CTR&#39044;&#27979;&#20013;&#30340;SSB&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Rec4Ad&#21033;&#29992;&#25512;&#33616;&#26085;&#24535;&#26500;&#24314;&#21512;&#25104;&#26679;&#26412;&#65292;&#20351;&#20854;&#27169;&#25311;&#25512;&#26029;&#38598;&#30340;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#23427;&#23558;&#21512;&#25104;&#21644;&#21407;&#22987;&#26679;&#26412;&#32452;&#21512;&#20197;&#20943;&#23569;SSB&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Rec4Ad&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#26657;&#20934;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) prediction serves as a fundamental component in online advertising. A common practice is to train a CTR model on advertisement (ad) impressions with user feedback. Since ad impressions are purposely selected by the model itself, their distribution differs from the inference distribution and thus exhibits sample selection bias (SSB) that affects model performance. Existing studies on SSB mainly employ sample re-weighting techniques which suffer from high variance and poor model calibration. Another line of work relies on costly uniform data that is inadequate to train industrial models. Thus mitigating SSB in industrial models with a uniform-data-free framework is worth exploring. Fortunately, many platforms display mixed results of organic items (i.e., recommendations) and sponsored items (i.e., ads) to users, where impressions of ads and recommendations are selected by different systems but share the same user decision rationales. Based on the above characteri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#20989;&#25968;&#25968;&#25454;&#35270;&#35282;&#30340;&#21407;&#21019;&#26041;&#27861;&#65292;&#21033;&#29992;&#26679;&#26412;&#36890;&#36807;&#21508;&#23618;&#30340;&#36712;&#36857;&#21450;&#20854;&#32479;&#35745;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#23618;&#27425;&#24102;&#22522;&#20934;&#30340;ODD&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.03522</link><description>&lt;p&gt;
&#22810;&#23618;&#27425;&#24102;&#22522;&#20934;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#20989;&#25968;&#25968;&#25454;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Functional Data Perspective and Baseline On Multi-Layer Out-of-Distribution Detection. (arXiv:2306.03522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#20989;&#25968;&#25968;&#25454;&#35270;&#35282;&#30340;&#21407;&#21019;&#26041;&#27861;&#65292;&#21033;&#29992;&#26679;&#26412;&#36890;&#36807;&#21508;&#23618;&#30340;&#36712;&#36857;&#21450;&#20854;&#32479;&#35745;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#23618;&#27425;&#24102;&#22522;&#20934;&#30340;ODD&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#22806;&#26679;&#26412;&#26816;&#27979;&#30340;&#20851;&#38190;&#29305;&#24449;&#26159;&#36890;&#36807;&#22810;&#23618;&#20998;&#31867;&#22120;&#25552;&#21462;&#32479;&#35745;&#27169;&#24335;&#21644;&#25968;&#25454;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#26816;&#27979;&#39044;&#26399;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#20498;&#25968;&#31532;&#20108;&#23618;&#25110;&#26368;&#21518;&#19968;&#23618;&#30340;&#36755;&#20986;&#65292;&#30041;&#19979;&#20102;&#29992;&#20110;ODD&#26816;&#27979;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#20989;&#25968;&#35270;&#35282;&#30340;&#21407;&#21019;&#26041;&#27861;&#65292;&#21033;&#29992;&#26679;&#26412;&#36890;&#36807;&#21508;&#23618;&#30340;&#36712;&#36857;&#21450;&#20854;&#32479;&#35745;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#36229;&#36234;&#20102;&#22810;&#20803;&#29305;&#24449;&#32858;&#21512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20989;&#25968;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#26032;&#30340;&#26694;&#26550;&#20013;&#65292;ODD&#26816;&#27979;&#36716;&#21270;&#20026;&#26816;&#27979;&#26679;&#26412;&#30340;&#36712;&#36857;&#19982;&#35757;&#32451;&#38598;&#25152;&#34920;&#29616;&#30340;&#20856;&#22411;&#34892;&#20026;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#30340;&#25152;&#26377;&#23618;&#30340;&#20449;&#24687;&#65292;&#20854;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key feature of out-of-distribution (OOD) detection is to exploit a trained neural network by extracting statistical patterns and relationships through the multi-layer classifier to detect shifts in the expected input data distribution. Despite achieving solid results, several state-of-the-art methods rely on the penultimate or last layer outputs only, leaving behind valuable information for OOD detection. Methods that explore the multiple layers either require a special architecture or a supervised objective to do so. This work adopts an original approach based on a functional view of the network that exploits the sample's trajectories through the various layers and their statistical dependencies. It goes beyond multivariate features aggregation and introduces a baseline rooted in functional anomaly detection. In this new framework, OOD detection translates into detecting samples whose trajectories differ from the typical behavior characterized by the training set. We validate our me
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;Fokker-Planck&#26041;&#27861;&#20174;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#19982;&#33258;&#28982;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#38750;&#24179;&#34913;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#23545;&#20110;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#29366;&#24577;&#21450;&#20854;&#35757;&#32451;&#26377;&#30528;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.03521</link><description>&lt;p&gt;
&#38750;&#24179;&#34913;&#26465;&#20214;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine learning in and out of equilibrium. (arXiv:2306.03521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;Fokker-Planck&#26041;&#27861;&#20174;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#19982;&#33258;&#28982;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#38750;&#24179;&#34913;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#23545;&#20110;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#29366;&#24577;&#21450;&#20854;&#35757;&#32451;&#26377;&#30528;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65292;&#19982;&#33258;&#28982;&#36807;&#31243;&#23384;&#22312;&#23494;&#20999;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#20363;&#22914;&#34507;&#30333;&#36136;&#25240;&#21472;&#25110;&#36827;&#21270;&#12290;&#25105;&#20204;&#37319;&#29992;&#20174;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#25913;&#32534;&#30340;Fokker-Planck&#26041;&#27861;&#65292;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#25506;&#35752;&#36825;&#20123;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#31995;&#32479;&#22312;&#38271;&#26102;&#38388;&#26497;&#38480;&#19979;&#30340;&#24179;&#34913;&#29366;&#24577;&#65292;&#36825;&#22312;&#20256;&#32479;&#30340;SGD&#20013;&#26159;&#19981;&#24179;&#34913;&#30340;&#65292;&#22312;&#32593;&#32476;&#21442;&#25968;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#25345;&#32493;&#30340;&#30005;&#27969;&#12290;&#19982;&#20854;&#29289;&#29702;&#31867;&#27604;&#19968;&#26679;&#65292;&#35813;&#30005;&#27969;&#19982;&#20219;&#20309;&#32473;&#23450;&#35757;&#32451;&#36712;&#36857;&#30340;&#29109;&#20135;&#29983;&#29575;&#30456;&#20851;&#12290;&#36825;&#20123;&#29575;&#30340;&#24179;&#34913;&#20998;&#24067;&#36981;&#24490;&#31215;&#20998;&#21644;&#35814;&#32454;&#27874;&#21160;&#23450;&#29702;&#65292;&#21363;&#28909;&#21147;&#23398;&#31532;&#20108;&#23450;&#24459;&#30340;&#38750;&#24179;&#34913;&#25512;&#24191;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#20540;&#20363;&#23376;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#20851;&#31995;&#65292;&#19968;&#20010;&#26159;&#38750;&#32447;&#24615;&#22238;&#24402;&#32593;&#32476;&#65292;&#21478;&#19968;&#20010;&#26159;MNIST&#25968;&#23383;&#20998;&#31867;&#12290;&#34429;&#28982;&#27874;&#21160;&#23450;&#29702;&#26159;
&lt;/p&gt;
&lt;p&gt;
The algorithms used to train neural networks, like stochastic gradient descent (SGD), have close parallels to natural processes that navigate a high-dimensional parameter space -- for example protein folding or evolution. Our study uses a Fokker-Planck approach, adapted from statistical physics, to explore these parallels in a single, unified framework. We focus in particular on the stationary state of the system in the long-time limit, which in conventional SGD is out of equilibrium, exhibiting persistent currents in the space of network parameters. As in its physical analogues, the current is associated with an entropy production rate for any given training trajectory. The stationary distribution of these rates obeys the integral and detailed fluctuation theorems -- nonequilibrium generalizations of the second law of thermodynamics. We validate these relations in two numerical examples, a nonlinear regression network and MNIST digit classification. While the fluctuation theorems are 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#19968;&#33268;&#24615;&#30340;&#22312;&#32447;&#24191;&#21578;&#39044;&#25490;&#21517;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#22359;&#30340;&#37319;&#26679;&#27169;&#22359;&#21644;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#25490;&#21517;&#23545;&#40784;&#27169;&#22359;&#65292;&#26469;&#26174;&#24335;&#20248;&#21270;ECPM&#25490;&#21517;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#20182;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;Delta NDCG&#30340;&#21152;&#26435;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03516</link><description>&lt;p&gt;
COPR&#65306;&#38754;&#21521;&#19968;&#33268;&#24615;&#30340;&#22312;&#32447;&#24191;&#21578;&#39044;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
COPR: Consistency-Oriented Pre-Ranking for Online Advertising. (arXiv:2306.03516v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03516
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#19968;&#33268;&#24615;&#30340;&#22312;&#32447;&#24191;&#21578;&#39044;&#25490;&#21517;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#22359;&#30340;&#37319;&#26679;&#27169;&#22359;&#21644;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#25490;&#21517;&#23545;&#40784;&#27169;&#22359;&#65292;&#26469;&#26174;&#24335;&#20248;&#21270;ECPM&#25490;&#21517;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#20182;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;Delta NDCG&#30340;&#21152;&#26435;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32423;&#32852;&#26550;&#26500;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#24191;&#21578;&#31995;&#32479;&#20013;&#20197;&#24179;&#34913;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22312;&#36825;&#31181;&#26550;&#26500;&#20013;&#65292;&#39044;&#25490;&#21517;&#27169;&#22411;&#34987;&#26399;&#26395;&#25104;&#20026;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25490;&#21517;&#27169;&#22411;&#36817;&#20284;&#65292;&#20197;&#22788;&#29702;&#26356;&#22810;&#20855;&#26377;&#20005;&#26684;&#24310;&#36831;&#35201;&#27714;&#30340;&#20505;&#36873;&#32773;&#12290;&#30001;&#20110;&#27169;&#22411;&#23481;&#37327;&#30340;&#24046;&#36317;&#65292;&#39044;&#25490;&#21517;&#21644;&#25490;&#21517;&#27169;&#22411;&#36890;&#24120;&#20250;&#29983;&#25104;&#19981;&#19968;&#33268;&#30340;&#25490;&#21517;&#32467;&#26524;&#65292;&#20174;&#32780;&#25439;&#23475;&#25972;&#20010;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;&#25552;&#20986;&#20102;&#24471;&#20998;&#23545;&#40784;&#30340;&#33539;&#24335;&#20197;&#35268;&#33539;&#23427;&#20204;&#30340;&#21407;&#22987;&#20998;&#25968;&#65292;&#20351;&#23427;&#20204;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#22312;&#22312;&#32447;&#24191;&#21578;&#20013;&#24212;&#29992;&#26102;&#65292;&#30001;&#20110;&#24517;&#28982;&#30340;&#23545;&#40784;&#35823;&#24046;&#21644;&#31454;&#26631;&#30340;&#35823;&#24046;&#25918;&#22823;&#65292;&#23427;&#20250;&#36973;&#21463;&#22256;&#25200;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38754;&#21521;&#19968;&#33268;&#24615;&#30340;&#22312;&#32447;&#24191;&#21578;&#39044;&#25490;&#21517;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#22359;&#30340;&#37319;&#26679;&#27169;&#22359;&#21644;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#25490;&#21517;&#23545;&#40784;&#27169;&#22359;&#65292;&#26469;&#26174;&#24335;&#20248;&#21270;ECPM&#25490;&#21517;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;$\Delta NDCG$&#30340;&#21152;&#26435;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cascading architecture has been widely adopted in large-scale advertising systems to balance efficiency and effectiveness. In this architecture, the pre-ranking model is expected to be a lightweight approximation of the ranking model, which handles more candidates with strict latency requirements. Due to the gap in model capacity, the pre-ranking and ranking models usually generate inconsistent ranked results, thus hurting the overall system effectiveness. The paradigm of score alignment is proposed to regularize their raw scores to be consistent. However, it suffers from inevitable alignment errors and error amplification by bids when applied in online advertising. To this end, we introduce a consistency-oriented pre-ranking framework for online advertising, which employs a chunk-based sampling module and a plug-and-play rank alignment module to explicitly optimize consistency of ECPM-ranked results. A $\Delta NDCG$-based weighting mechanism is adopted to better distinguish the import
&lt;/p&gt;</description></item><item><title>&#35813;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36923;&#36753;&#25193;&#25955;&#65288;LoD&#65289;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25512;&#29702;&#27169;&#22411;&#21463;&#35757;&#32451;&#26679;&#26412;&#38480;&#21046;&#12289;&#34920;&#29616;&#19981;&#22815;&#24378;&#30340;&#38382;&#39064;&#12290;LoD&#36890;&#36807;&#20851;&#31995;&#25193;&#25955;&#12289;&#38543;&#26426;&#28216;&#36208;&#23376;&#36923;&#36753;&#37319;&#26679;&#21644;&#26799;&#24230;&#33258;&#36866;&#24212;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#26597;&#35810;&#30340;&#21457;&#29616;&#21644;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#30340;&#21160;&#24577;&#24179;&#34913;&#65292;&#24182;&#37197;&#22791;&#20102;&#29305;&#27530;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#36923;&#36753;&#25193;&#25955;&#12290;</title><link>http://arxiv.org/abs/2306.03515</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#36923;&#36753;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Logic Diffusion for Knowledge Graph Reasoning. (arXiv:2306.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36923;&#36753;&#25193;&#25955;&#65288;LoD&#65289;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25512;&#29702;&#27169;&#22411;&#21463;&#35757;&#32451;&#26679;&#26412;&#38480;&#21046;&#12289;&#34920;&#29616;&#19981;&#22815;&#24378;&#30340;&#38382;&#39064;&#12290;LoD&#36890;&#36807;&#20851;&#31995;&#25193;&#25955;&#12289;&#38543;&#26426;&#28216;&#36208;&#23376;&#36923;&#36753;&#37319;&#26679;&#21644;&#26799;&#24230;&#33258;&#36866;&#24212;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#26597;&#35810;&#30340;&#21457;&#29616;&#21644;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#30340;&#21160;&#24577;&#24179;&#34913;&#65292;&#24182;&#37197;&#22791;&#20102;&#29305;&#27530;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#36923;&#36753;&#25193;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22238;&#31572;&#19968;&#38454;&#36923;&#36753;&#26597;&#35810;&#65292;&#36890;&#36807;&#22810;&#36339;&#36923;&#36753;&#39044;&#27979;&#26469;&#25506;&#32034;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25512;&#29702;&#27169;&#22411;&#21463;&#21040;&#35757;&#32451;&#26679;&#26412;&#25152;&#22260;&#32469;&#30340;&#36923;&#36753;&#33539;&#24335;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#36923;&#36753;&#25512;&#29702;&#19978;&#34920;&#29616;&#36824;&#19981;&#22815;&#24378;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36923;&#36753;&#25193;&#25955;&#65288;LoD&#65289;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#21608;&#22260;&#29615;&#22659;&#20013;&#21457;&#29616;&#26410;&#35265;&#26597;&#35810;&#65292;&#24182;&#23454;&#29616;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#30340;&#21160;&#24577;&#24179;&#34913;&#12290;LoD&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20851;&#31995;&#25193;&#25955;&#21644;&#38543;&#26426;&#28216;&#36208;&#23376;&#36923;&#36753;&#37319;&#26679;&#20197;&#21450;&#19968;&#31181;&#29305;&#27530;&#30340;&#35757;&#32451;&#26426;&#21046;&#8212;&#8212;&#26799;&#24230;&#33258;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;LoD&#36824;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#36827;&#19968;&#27493;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#38598;&#20013;&#24212;&#23545;&#22024;&#26434;&#25968;&#25454;&#26102;&#23454;&#29616;&#31283;&#20581;&#30340;&#36923;&#36753;&#25193;&#25955;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#24102;&#26377;LoD&#30340;&#20027;&#27969;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#27169;&#22411;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#20102;&#36923;&#36753;&#25193;&#25955;&#22312;&#20811;&#26381;&#29616;&#26377;&#25512;&#29702;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#23454;&#29616;&#26356;&#22909;&#30340;&#26410;&#35265;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most recent works focus on answering first order logical queries to explore the knowledge graph reasoning via multi-hop logic predictions. However, existing reasoning models are limited by the circumscribed logical paradigms of training samples, which leads to a weak generalization of unseen logic. To address these issues, we propose a plug-in module called Logic Diffusion (LoD) to discover unseen queries from surroundings and achieves dynamical equilibrium between different kinds of patterns. The basic idea of LoD is relation diffusion and sampling sub-logic by random walking as well as a special training mechanism called gradient adaption. Besides, LoD is accompanied by a novel loss function to further achieve the robust logical diffusion when facing noisy data in training or testing sets. Extensive experiments on four public datasets demonstrate the superiority of mainstream knowledge graph reasoning models with LoD over state-of-the-art. Moreover, our ablation study proves the gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;(SGNCL)&#65292;&#36890;&#36807;&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#65292;&#24182;&#25506;&#31350;&#20102;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.03506</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Subgraph Networks Based Contrastive Learning. (arXiv:2306.03506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;(SGNCL)&#65292;&#36890;&#36807;&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#65292;&#24182;&#25506;&#31350;&#20102;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;(GCL)&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#35299;&#20915;&#27880;&#37322;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290; &#23427;&#22312;&#26410;&#27880;&#37322;&#30340;&#22270;&#24418;&#20013;&#25366;&#25496;&#26174;&#24335;&#29305;&#24449;&#20197;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#21033;&#22270;&#24418;&#34920;&#31034;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#20391;&#37325;&#20110;&#22270;&#24418;&#22686;&#24378;&#31574;&#30053;&#21644;&#30456;&#20114;&#20449;&#24687;&#20272;&#35745;&#25805;&#20316;&#30340;&#35774;&#35745;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#23376;&#22270;&#20013;&#23384;&#22312;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#25506;&#32034;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;subgraph network-based contrastive learning (SGNCL)&#30340;&#26032;&#26694;&#26550;&#12290;SGNCL&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#12290;&#35813;&#31574;&#30053;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#20026;&#20855;&#26377;&#25299;&#25169;&#21644;&#23646;&#24615;&#29305;&#24449;&#30340;&#36793;&#21040;&#33410;&#28857;&#26144;&#23556;&#32593;&#32476;&#12290;&#21333;&#27425;&#22686;&#24378;&#35270;&#22270;&#26159;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL), as a self-supervised learning method, can solve the problem of annotated data scarcity. It mines explicit features in unannotated graphs to generate favorable graph representations for downstream tasks. Most existing GCL methods focus on the design of graph augmentation strategies and mutual information estimation operations. Graph augmentation produces augmented views by graph perturbations. These views preserve a locally similar structure and exploit explicit features. However, these methods have not considered the interaction existing in subgraphs. To explore the impact of substructure interactions on graph representations, we propose a novel framework called subgraph network-based contrastive learning (SGNCL). SGNCL applies a subgraph network generation strategy to produce augmented views. This strategy converts the original graph into an Edge-to-Node mapping network with both topological and attribute features. The single-shot augmented view is a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#23553;&#31105;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#22403;&#22334;&#37038;&#20214;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#35753;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.03502</link><description>&lt;p&gt;
&#20420;&#20044;&#25112;&#20105;&#65306;&#39044;&#27979;&#21644;&#35299;&#37322;Twitter&#30340;&#23553;&#31105;
&lt;/p&gt;
&lt;p&gt;
Russo-Ukrainian War: Prediction and explanation of Twitter suspension. (arXiv:2306.03502v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#23553;&#31105;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#22403;&#22334;&#37038;&#20214;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#35753;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2022&#24180;2&#26376;24&#26085;&#65292;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#65292;&#24320;&#22987;&#20102;&#29616;&#22312;&#24050;&#30693;&#30340;&#20420;&#20044;&#25112;&#20105;&#65292;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24341;&#21457;&#20102;&#22312;&#32447;&#35805;&#35821;&#12290;Twitter&#20316;&#20026;&#26368;&#21463;&#27426;&#36814;&#30340;&#31038;&#20132;&#32593;&#32476;&#20043;&#19968;&#65292;&#20197;&#20854;&#24320;&#25918;&#21644;&#27665;&#20027;&#30340;&#29305;&#28857;&#65292;&#22312;&#20854;&#24222;&#22823;&#30340;&#29992;&#25143;&#32676;&#20013;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#35752;&#35770;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;Twitter&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#28389;&#29992;&#34892;&#20026;&#12289;&#20405;&#29359;&#20844;&#27665;&#26435;&#21033;&#65292;&#22240;&#27492;&#23548;&#33268;&#29992;&#25143;&#36134;&#25143;&#34987;&#23553;&#31105;&#21644;&#21024;&#38500;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#25506;&#35752;&#20102;Twitter&#30340;&#23553;&#31105;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#21487;&#33021;&#23548;&#33268;&#36134;&#25143;&#34987;&#23553;&#31105;&#30340;&#20849;&#20139;&#20869;&#23481;&#21644;&#29992;&#25143;&#36134;&#25143;&#30340;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;Twitter API&#33719;&#24471;&#20102;&#21253;&#21547;107.7M&#26465;&#25512;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;980&#19975;&#29992;&#25143;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#34987;&#23553;&#31105;&#36134;&#25143;&#30340;&#20849;&#20139;&#20869;&#23481;&#31867;&#21035;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#21644;&#20313;&#24358;&#30456;&#20284;&#24615;&#32858;&#31867;&#26469;&#35299;&#37322;&#20854;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20123;&#28389;&#29992;Twitter&#25919;&#31574;&#26631;&#20934;&#30340;&#39575;&#23376;&#27963;&#21160;&#12289;&#22403;&#22334;&#37038;&#20214;&#21644;&#23459;&#20256;&#27963;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#31881;&#19997;&#25968;&#36739;&#23569;&#30340;&#36134;&#25143;&#65292;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#26377;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#20026;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#65292;&#26368;&#23567;&#21270;&#26377;&#23475;&#20869;&#23481;&#30340;&#20256;&#25773;&#25552;&#20379;&#26377;&#29992;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
On 24 February 2022, Russia invaded Ukraine, starting what is now known as the Russo-Ukrainian War, initiating an online discourse on social media. Twitter as one of the most popular SNs, with an open and democratic character, enables a transparent discussion among its large user base. Unfortunately, this often leads to Twitter's policy violations, propaganda, abusive actions, civil integrity violation, and consequently to user accounts' suspension and deletion. This study focuses on the Twitter suspension mechanism and the analysis of shared content and features of the user accounts that may lead to this. Toward this goal, we have obtained a dataset containing 107.7M tweets, originating from 9.8 million users, using Twitter API. We extract the categories of shared content of the suspended accounts and explain their characteristics, through the extraction of text embeddings in junction with cosine similarity clustering. Our results reveal scam campaigns taking advantage of trending top
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#20855;&#26377;&#21452;&#37325;&#25928;&#24212;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#21644;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2306.03481</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#32416;&#32544;&#25968;&#25454;&#30340;&#36716;&#25442;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transition role of entangled data in quantum machine learning. (arXiv:2306.03481v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#20855;&#26377;&#21452;&#37325;&#25928;&#24212;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#21644;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32416;&#32544;&#20316;&#20026;&#22686;&#24378;&#37327;&#23376;&#35745;&#31639;&#30340;&#36164;&#28304;&#65292;&#24050;&#32463;&#22312;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23558;&#32416;&#32544;&#34701;&#20837;&#21040;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25805;&#20316;&#25110;&#27979;&#37327;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#21516;&#26102;&#22312;&#36798;&#21040;&#25351;&#23450;&#39044;&#27979;&#35823;&#24046;&#38408;&#20540;&#26102;&#21462;&#24471;&#20102;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#32416;&#32544;&#31243;&#24230;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#20998;&#26512;&#24615;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#20351;&#29992;&#32416;&#32544;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#37327;&#23376;&#19981;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#12290;&#19982;&#20197;&#24448;&#21457;&#29616;&#30340;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#39044;&#27979;&#35823;&#24046;&#30340;&#24433;&#21709;&#21576;&#29616;&#20986;&#21452;&#37325;&#25928;&#24212;&#65292;&#21462;&#20915;&#20110;&#20801;&#35768;&#30340;&#27979;&#37327;&#27425;&#25968;&#12290;&#22312;&#26377;&#20805;&#20998;&#30340;&#27979;&#37327;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#32416;&#32544;&#24230;&#21487;&#20197;&#25345;&#32493;&#38477;&#20302;&#39044;&#27979;&#35823;&#24046;&#65292;&#25110;&#20943;&#23569;&#36798;&#21040;&#32473;&#23450;&#35823;&#24046;&#38408;&#20540;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#12290;&#26412;&#30740;&#31350;&#38416;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#36716;&#25442;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entanglement serves as the resource to empower quantum computing. Recent progress has highlighted its positive impact on learning quantum dynamics, wherein the integration of entanglement into quantum operations or measurements of quantum machine learning (QML) models leads to substantial reductions in training data size, surpassing a specified prediction error threshold. However, an analytical understanding of how the entanglement degree in data affects model performance remains elusive. In this study, we address this knowledge gap by establishing a quantum no-free-lunch (NFL) theorem for learning quantum dynamics using entangled data. Contrary to previous findings, we prove that the impact of entangled data on prediction error exhibits a dual effect, depending on the number of permitted measurements. With a sufficient number of measurements, increasing the entanglement of training data consistently reduces the prediction error or decreases the required size of the training data to ac
&lt;/p&gt;</description></item><item><title>GSHOT&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.03480</link><description>&lt;p&gt;
GSHOT: &#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GSHOT: Few-shot Generative Modeling of Labeled Graphs. (arXiv:2306.03480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03480
&lt;/p&gt;
&lt;p&gt;
GSHOT&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#22270;&#29983;&#25104;&#24314;&#27169;&#22240;&#20854;&#30452;&#25509;&#23398;&#20064;&#28508;&#22312;&#38544;&#34255;&#22270;&#20998;&#24067;&#30340;&#24778;&#20154;&#33021;&#21147;&#32780;&#21463;&#21040;&#26497;&#22823;&#20851;&#27880;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#26368;&#21021;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20687;&#35768;&#22810;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#19968;&#26679;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#23398;&#20064;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#32597;&#35265;&#30142;&#30149;&#30340;&#33647;&#29289;&#21457;&#29616;&#31561;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#19981;&#24635;&#26159;&#26377;&#36275;&#22815;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#29992;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#36827;&#23637;&#20026;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24212;&#29992;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23569;&#26679;&#26412;&#22270;&#29983;&#25104;&#24314;&#27169;&#36825;&#19968;&#36804;&#20170;&#26410;&#26366;&#25506;&#32034;&#30340;&#33539;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GSHOT&#65292;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#12290;GSHOT&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#12290;&#21033;&#29992;&#36825;&#20123;&#20808;&#21069;&#30340;&#32463;&#39564;&#65292;GSHOT&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#33258;&#25105;&#35843;&#25972;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph generative modeling has gained enormous attraction in recent years due to its impressive ability to directly learn the underlying hidden graph distribution. Despite their initial success, these techniques, like much of the existing deep generative methods, require a large number of training samples to learn a good model. Unfortunately, large number of training samples may not always be available in scenarios such as drug discovery for rare diseases. At the same time, recent advances in few-shot learning have opened door to applications where available training data is limited. In this work, we introduce the hitherto unexplored paradigm of few-shot graph generative modeling. Towards this, we develop GSHOT, a meta-learning based framework for few-shot labeled graph generative modeling. GSHOT learns to transfer meta-knowledge from similar auxiliary graph datasets. Utilizing these prior experiences, GSHOT quickly adapts to an unseen graph dataset through self-paced fine-tuning. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21150;&#20844;&#23460;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#35821;&#20041;&#35299;&#37322;&#22120;&#23454;&#29616;&#20102;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#24182;&#25191;&#34892;Office&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2306.03460</link><description>&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#32508;&#21512;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;
&lt;/p&gt;
&lt;p&gt;
Natural Language Commanding via Program Synthesis. (arXiv:2306.03460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03460
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21150;&#20844;&#23460;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#35821;&#20041;&#35299;&#37322;&#22120;&#23454;&#29616;&#20102;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#24182;&#25191;&#34892;Office&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#35299;&#37322;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#21451;&#22909;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#20135;&#21147;&#36719;&#20214;&#65292;&#22914;&#24494;&#36719;Office&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36328;&#24212;&#29992;&#31243;&#24207;&#21151;&#33021;&#25191;&#34892;&#29992;&#25143;&#24847;&#22270;&#12290;&#34429;&#28982;LLM&#22312;&#29702;&#35299;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#29992;&#25143;&#24847;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#38656;&#35201;&#36229;&#36807;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#30340;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#29992;&#25143;&#24847;&#22270;&#30340;&#23454;&#29616;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21150;&#20844;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;ODSL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#27905;&#12289;&#39640;&#32423;&#21035;&#30340;&#35821;&#35328;&#65292;&#19987;&#38376;&#29992;&#20110;&#22312;Office&#24212;&#29992;&#31243;&#24207;&#20013;&#25191;&#34892;&#25805;&#20316;&#24182;&#19982;&#23454;&#20307;&#20132;&#20114;&#12290;&#35821;&#20041;&#35299;&#37322;&#22120;&#21033;&#29992;&#20998;&#26512;&#26816;&#32034;&#25552;&#31034;&#26500;&#36896;&#26041;&#27861;&#19982;LLM&#36827;&#34892;&#31243;&#24207;&#32508;&#21512;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#35805;&#35821;&#36716;&#25442;&#20026;&#21487;&#20197;&#34987;&#36716;&#25442;&#20026;&#24212;&#29992;&#31243;&#24207;API&#24182;&#25191;&#34892;&#30340;ODSL&#31243;&#24207;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;Microsoft PowerPoint&#30340;&#30740;&#31350;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Semantic Interpreter, a natural language-friendly AI system for productivity software such as Microsoft Office that leverages large language models (LLMs) to execute user intent across application features. While LLMs are excellent at understanding user intent expressed as natural language, they are not sufficient for fulfilling application-specific user intent that requires more than text-to-text transformations. We therefore introduce the Office Domain Specific Language (ODSL), a concise, high-level language specialized for performing actions in and interacting with entities in Office applications. Semantic Interpreter leverages an Analysis-Retrieval prompt construction method with LLMs for program synthesis, translating natural language user utterances to ODSL programs that can be transpiled to application APIs and then executed. We focus our discussion primarily on a research exploration for Microsoft PowerPoint.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;AI&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;AI&#30340;MSF&#24863;&#30693;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.03454</link><description>&lt;p&gt;
AI&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#31995;&#32479;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65306;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Robustness of AI-enabled Multi-sensor Fusion Systems: Challenges and Opportunities. (arXiv:2306.03454v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;AI&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;AI&#30340;MSF&#24863;&#30693;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#65288;MSF&#65289;&#30340;&#24863;&#30693;&#31995;&#32479;&#26159;&#25903;&#25745;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#39046;&#22495;&#30340;&#22522;&#30784;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12289;&#26426;&#22120;&#20154;&#33218;&#21644;&#26080;&#20154;&#26426;&#12290;&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26041;&#38754;&#30340;&#24555;&#36895;&#36827;&#27493;&#20026;MSF&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#39640;&#25552;&#20379;&#20102;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#26234;&#33021;&#31995;&#32479;&#21450;&#20854;&#24863;&#30693;&#31995;&#32479;&#26041;&#38754;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;AI&#30340;MSF&#24863;&#30693;&#31995;&#32479;&#21644;&#25216;&#26415;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20844;&#24320;&#30340;&#19987;&#27880;&#20110;MSF&#24863;&#30693;&#30340;&#22522;&#20934;&#27979;&#35797;&#26377;&#38480;&#12290;&#37492;&#20110;&#35768;&#22810;&#26234;&#33021;&#31995;&#32479;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#22312;&#24863;&#30693;&#31995;&#32479;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#36825;&#23601;&#24613;&#38656;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36825;&#20123;MSF&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#20102;&#26089;&#26399;&#30340;&#19968;&#27493;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;AI&#30340;MSF&#24863;&#30693;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#19987;&#27880;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Sensor Fusion (MSF) based perception systems have been the foundation in supporting many industrial applications and domains, such as self-driving cars, robotic arms, and unmanned aerial vehicles. Over the past few years, the fast progress in data-driven artificial intelligence (AI) has brought a fast-increasing trend to empower MSF systems by deep learning techniques to further improve performance, especially on intelligent systems and their perception systems. Although quite a few AI-enabled MSF perception systems and techniques have been proposed, up to the present, limited benchmarks that focus on MSF perception are publicly available. Given that many intelligent systems such as self-driving cars are operated in safety-critical contexts where perception systems play an important role, there comes an urgent need for a more in-depth understanding of the performance and reliability of these MSF systems. To bridge this gap, we initiate an early step in this direction and construc
&lt;/p&gt;</description></item><item><title>GRAFENNE&#26159;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#21407;&#22270;&#19978;&#36827;&#34892;&#24322;&#26500;&#36716;&#21270;&#65292;&#23558;&#33410;&#28857;&#21644;&#29305;&#24449;&#35299;&#32806;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26222;&#36941;&#23384;&#22312;&#30340;&#29305;&#24449;&#38745;&#24577;&#12289;&#36716;&#31227;&#35823;&#24046;&#31561;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#36866;&#29992;&#20110;&#26410;&#30693;&#33410;&#28857;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.03447</link><description>&lt;p&gt;
GRAFENNE&#65306;&#22312;&#20855;&#26377;&#24322;&#36136;&#21644;&#21160;&#24577;&#29305;&#24449;&#38598;&#30340;&#22270;&#19978;&#36827;&#34892;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GRAFENNE: Learning on Graphs with Heterogeneous and Dynamic Feature Sets. (arXiv:2306.03447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03447
&lt;/p&gt;
&lt;p&gt;
GRAFENNE&#26159;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#21407;&#22270;&#19978;&#36827;&#34892;&#24322;&#26500;&#36716;&#21270;&#65292;&#23558;&#33410;&#28857;&#21644;&#29305;&#24449;&#35299;&#32806;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26222;&#36941;&#23384;&#22312;&#30340;&#29305;&#24449;&#38745;&#24577;&#12289;&#36716;&#31227;&#35823;&#24046;&#31561;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#36866;&#29992;&#20110;&#26410;&#30693;&#33410;&#28857;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36890;&#24120;&#22522;&#20110;&#23545;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#38745;&#24577;&#29305;&#24449;&#38598;&#30340;&#20551;&#35774;&#26469;&#26500;&#24314;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#36825;&#19968;&#20551;&#35774;&#32463;&#24120;&#34987;&#36829;&#21453;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#25554;&#34917;&#37096;&#20998;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#29305;&#24449;&#38598;&#22343;&#19968;&#12289;&#36716;&#31227;&#35823;&#24046;&#12289;&#26080;&#27861;&#36866;&#24212;&#21160;&#24577;&#29305;&#24449;&#31561;&#23616;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#26694;&#26550;GRAFENNE&#26469;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#21407;&#22270;&#19978;&#36827;&#34892;&#24322;&#26500;&#36716;&#21270;&#65292;&#23558;&#33410;&#28857;&#21644;&#29305;&#24449;&#35299;&#32806;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#27861;&#20351;&#24471;&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#19982;&#29305;&#24449;&#25968;&#37327;&#26080;&#20851;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#33410;&#28857;&#21644;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GRAFENNE&#22312;Weisfeil&#26041;&#31243;&#24615;&#33021;&#19978;&#33267;&#23569;&#19982;&#29616;&#26377;&#30340;&#20449;&#24687;&#20256;&#36882;GNN&#19968;&#26679;&#20855;&#26377;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), in general, are built on the assumption of a static set of features characterizing each node in a graph. This assumption is often violated in practice. Existing methods partly address this issue through feature imputation. However, these techniques (i) assume uniformity of feature set across nodes, (ii) are transductive by nature, and (iii) fail to work when features are added or removed over time. In this work, we address these limitations through a novel GNN framework called GRAFENNE. GRAFENNE performs a novel allotropic transformation on the original graph, wherein the nodes and features are decoupled through a bipartite encoding. Through a carefully chosen message passing framework on the allotropic transformation, we make the model parameter size independent of the number of features and thereby inductive to both unseen nodes and features. We prove that GRAFENNE is at least as expressive as any of the existing message-passing GNNs in terms of Weisfeil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#21487;&#21464;&#24615;&#23849;&#28291;&#25351;&#25968;&#65288;VCI&#65289;&#30340;&#26032;&#25351;&#26631;&#65292;&#29992;&#20110;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#24615;&#23849;&#28291;&#29616;&#35937;&#65292;&#20854;&#20248;&#36234;&#24615;&#36136;&#21253;&#25324;&#22312;&#21487;&#36870;&#32447;&#24615;&#21464;&#25442;&#19979;&#30340;&#19981;&#21464;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#27492;&#25351;&#26631;&#21487;&#20197;&#25351;&#31034;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#21464;&#24615;&#23849;&#28291;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03440</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#24615;&#23849;&#28291;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Variability Collapse of Neural Networks. (arXiv:2306.03440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#21487;&#21464;&#24615;&#23849;&#28291;&#25351;&#25968;&#65288;VCI&#65289;&#30340;&#26032;&#25351;&#26631;&#65292;&#29992;&#20110;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#24615;&#23849;&#28291;&#29616;&#35937;&#65292;&#20854;&#20248;&#36234;&#24615;&#36136;&#21253;&#25324;&#22312;&#21487;&#36870;&#32447;&#24615;&#21464;&#25442;&#19979;&#30340;&#19981;&#21464;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#27492;&#25351;&#26631;&#21487;&#20197;&#25351;&#31034;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#21464;&#24615;&#23849;&#28291;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20174;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#30340;&#20869;&#31867;&#21464;&#21270;&#20043;&#38388;&#30340;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;&#26368;&#36817;&#21457;&#29616;&#30340;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#29616;&#35937;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#20197;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#20960;&#20309;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#21487;&#21464;&#24615;&#23849;&#28291;&#25351;&#25968;&#65288;VCI&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;NC&#33539;&#24335;&#20013;&#30340;&#21487;&#21464;&#24615;&#23849;&#28291;&#29616;&#35937;&#12290;VCI&#25351;&#26631;&#20855;&#26377;&#24456;&#24378;&#30340;&#21160;&#26426;&#24615;&#65292;&#24182;&#19988;&#26412;&#36136;&#19978;&#19982;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#30340;&#32447;&#24615;&#25506;&#27979;&#25439;&#22833;&#26377;&#20851;&#12290;&#27492;&#22806;&#65292;&#23427;&#20855;&#26377;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#30340;&#20248;&#36234;&#24615;&#36136;&#65292;&#21253;&#25324;&#22312;&#21487;&#36870;&#32447;&#24615;&#21464;&#25442;&#19979;&#30340;&#19981;&#21464;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#36825;&#19982;&#20043;&#21069;&#30340;&#25351;&#26631;&#26377;&#25152;&#21306;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;VCI&#22312;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#25351;&#31034;&#21487;&#21464;&#24615;&#23849;&#28291;&#21644;&#21487;&#36716;&#31227;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies empirically demonstrate the positive relationship between the transferability of neural networks and the within-class variation of the last layer features. The recently discovered Neural Collapse (NC) phenomenon provides a new perspective of understanding such last layer geometry of neural networks. In this paper, we propose a novel metric, named Variability Collapse Index (VCI), to quantify the variability collapse phenomenon in the NC paradigm. The VCI metric is well-motivated and intrinsically related to the linear probing loss on the last layer features. Moreover, it enjoys desired theoretical and empirical properties, including invariance under invertible linear transformations and numerical stability, that distinguishes it from previous metrics. Our experiments verify that VCI is indicative of the variability collapse and the transferability of pretrained neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03438</link><description>&lt;p&gt;
&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22635;&#20889;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#26102;&#23384;&#22312;&#22833;&#36133;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models of Code Fail at Completing Code with Potential Bugs. (arXiv:2306.03438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#22312;&#20195;&#30721;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#36825;&#26159;&#32534;&#31243;&#36741;&#21161;&#21644;&#20195;&#30721;&#26234;&#33021;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#38382;&#39064;&#65292;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#36825;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#21463;&#23454;&#26102;&#20195;&#30721;&#24314;&#35758;&#30340;&#29616;&#23454;&#22330;&#26223;&#21551;&#21457;&#65292;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#21487;&#33021;&#30340;&#28431;&#27934;-&#21453;&#27169;&#24335;&#65292;&#36825;&#20123;&#21453;&#27169;&#24335;&#21487;&#20197;&#25104;&#20026;&#23436;&#25104;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#30740;&#31350;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20174;&#35821;&#20041;&#25913;&#21464;&#25805;&#20316;&#20013;&#27966;&#29983;&#30340;&#21512;&#25104;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-HumanEval&#65289;&#65292;&#21478;&#19968;&#20010;&#26159;&#20174;&#29992;&#25143;&#25552;&#20132;&#30340;&#32534;&#31243;&#38382;&#39064;&#20013;&#27966;&#29983;&#30340;&#29616;&#23454;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-FixEval&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#24773;&#20917;&#26174;&#33879;&#38477;&#20302;&#20102;&#39640;&#24615;&#33021;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;CodeGen-2B-mono&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#36890;&#36807;&#29575;
&lt;/p&gt;
&lt;p&gt;
Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CodeGen-2B-mono on test 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#25193;&#25955;&#36807;&#31243;&#26469;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#35813;&#36807;&#31243;&#36890;&#36807;&#35757;&#32451;&#25110;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#65292;&#19981;&#21516;&#20110;&#20219;&#21153;&#25968;&#25454;&#30340;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;&#23884;&#20837;&#30340;&#27700;&#21360;&#12290;</title><link>http://arxiv.org/abs/2306.03436</link><description>&lt;p&gt;
&#36890;&#36807;&#27700;&#21360;&#25193;&#25955;&#36807;&#31243;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;
&lt;/p&gt;
&lt;p&gt;
Protecting the Intellectual Property of Diffusion Models by the Watermark Diffusion Process. (arXiv:2306.03436v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#25193;&#25955;&#36807;&#31243;&#26469;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#35813;&#36807;&#31243;&#36890;&#36807;&#35757;&#32451;&#25110;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#65292;&#19981;&#21516;&#20110;&#20219;&#21153;&#25968;&#25454;&#30340;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;&#23884;&#20837;&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20219;&#21153;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#26550;&#26500;&#12290;&#35757;&#32451;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#38656;&#35201;&#39640;&#36164;&#28304;&#25104;&#26412;&#65292;&#22240;&#27492;&#38656;&#35201;&#20445;&#25252;&#23427;&#20204;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#27700;&#21360;&#26041;&#27861;WDM&#65292;&#21253;&#25324;&#27700;&#21360;&#23884;&#20837;&#12289;&#25552;&#21462;&#21644;&#39564;&#35777;&#12290;WDM&#36890;&#36807;&#35757;&#32451;&#25110;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#20010;&#27700;&#21360;&#25193;&#25955;&#36807;&#31243;&#65288;WDP&#65289;&#26469;&#23884;&#20837;&#27700;&#21360;&#25968;&#25454;&#65292;&#35813;&#36807;&#31243;&#19982;&#20219;&#21153;&#25968;&#25454;&#30340;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#19981;&#21516;&#12290;&#23884;&#20837;&#30340;&#27700;&#21360;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;WDP&#30340;&#20849;&#20139;&#21453;&#21521;&#22122;&#22768;&#36827;&#34892;&#37319;&#26679;&#32780;&#19981;&#20250;&#38477;&#20302;&#21407;&#22987;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23558;WDP&#19982;&#20462;&#25913;&#25193;&#25955;&#36807;&#31243;&#36830;&#25509;&#26469;&#25552;&#20379;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as state-of-the-art deep generative architectures with the increasing demands for generation tasks. Training large diffusion models for good performance requires high resource costs, making them valuable intellectual properties to protect. While most of the existing ownership solutions, including watermarking, mainly focus on discriminative models. This paper proposes WDM, a novel watermarking method for diffusion models, including watermark embedding, extraction, and verification. WDM embeds the watermark data through training or fine-tuning the diffusion model to learn a Watermark Diffusion Process (WDP), different from the standard diffusion process for the task data. The embedded watermark can be extracted by sampling using the shared reverse noise from the learned WDP without degrading performance on the original task. We also provide theoretical foundations and analysis of the proposed method by connecting the WDP to the diffusion process with a modi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;Prompt-tuning&#22312;&#27880;&#24847;&#21147;&#26550;&#26500;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25506;&#32034;&#19978;&#19979;&#25991;&#28151;&#21512;&#27169;&#22411;&#65292;&#34920;&#26126;softmax-prompt-attention&#22312;&#34920;&#36798;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#30340;&#20351;&#29992;&#25968;&#25454;&#23398;&#20064;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.03435</link><description>&lt;p&gt;
&#20851;&#27880;&#28857;&#23545;Prompt-tuning&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Attention in Prompt-tuning. (arXiv:2306.03435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;Prompt-tuning&#22312;&#27880;&#24847;&#21147;&#26550;&#26500;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25506;&#32034;&#19978;&#19979;&#25991;&#28151;&#21512;&#27169;&#22411;&#65292;&#34920;&#26126;softmax-prompt-attention&#22312;&#34920;&#36798;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#30340;&#20351;&#29992;&#25968;&#25454;&#23398;&#20064;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-tuning &#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064; (&#36719;) &#25552;&#31034;&#21442;&#25968;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#22312; LLM &#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110; Prompt-tuning &#30340;&#33021;&#21147;&#21450;&#20851;&#27880;&#26426;&#21046;&#22312;&#25552;&#31034;&#20013;&#30340;&#20316;&#29992;&#65292;&#29702;&#35770;&#29702;&#35299;&#23578;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#19968;&#20010;&#27880;&#24847;&#21147;&#26550;&#26500;&#30340; Prompt-tuning&#65292;&#24182;&#30740;&#31350;&#19978;&#19979;&#25991;&#28151;&#21512;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#36755;&#20837;&#34920;&#31034;&#23646;&#20110;&#19978;&#19979;&#25991;&#30456;&#20851;&#25110;&#26080;&#20851;&#38598;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#25552;&#31034;-&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#38548;&#31163; Prompt-tuning &#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;(1) &#25105;&#20204;&#34920;&#26126;&#22312;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#27169;&#22411;&#19979;&#65292;softmax-prompt-attention &#22312;&#21487;&#35777;&#26126;&#22320;&#27604;softmax-self-attention &#21644;&#32447;&#24615;&#25552;&#31034;&#27880;&#24847;&#21147;&#26356;&#20855;&#34920;&#36798;&#21147;&#12290;(2) &#25105;&#20204;&#20998;&#26512;&#20102;&#28176;&#21464;&#19979;&#38477;&#30340;&#21021;&#22987;&#36712;&#36857;&#65292;&#24182;&#23637;&#31034;&#21487;&#20197;&#36890;&#36807;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23398;&#20064;&#25552;&#31034;&#21644;&#39044;&#27979;&#22836;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#25552;&#31034;&#21487;&#20197;&#35777;&#26126;&#22320;&#27880;&#24847;&#21040;&#31232;&#30095;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#12290;(3)
&lt;/p&gt;
&lt;p&gt;
Prompt-tuning is an emerging strategy to adapt large language models (LLM) to downstream tasks by learning a (soft-)prompt parameter from data. Despite its success in LLMs, there is limited theoretical understanding of the power of prompt-tuning and the role of the attention mechanism in prompting. In this work, we explore prompt-tuning for one-layer attention architectures and study contextual mixture-models where each input token belongs to a context-relevant or -irrelevant set. We isolate the role of prompt-tuning through a self-contained prompt-attention model. Our contributions are as follows: (1) We show that softmax-prompt-attention is provably more expressive than softmax-self-attention and linear-prompt-attention under our contextual data model. (2) We analyze the initial trajectory of gradient descent and show that it learns the prompt and prediction head with near-optimal sample complexity and demonstrate how prompt can provably attend to sparse context-relevant tokens. (3) 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#35745;&#31639;&#26368;&#23567;&#25903;&#37197;&#38598;&#38382;&#39064;&#30340;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#32988;&#36807;&#20256;&#32479;&#31639;&#27861;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03434</link><description>&lt;p&gt;
&#37319;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#32452;&#21512;&#20248;&#21270;&#26368;&#23567;&#25903;&#37197;&#38598;&#38382;&#39064;&#30340;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Heuristic for Combinatorial Optimization of the Minimum Dominating Set Problem using Graph Convolutional Networks. (arXiv:2306.03434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#35745;&#31639;&#26368;&#23567;&#25903;&#37197;&#38598;&#38382;&#39064;&#30340;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#32988;&#36807;&#20256;&#32479;&#31639;&#27861;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#30340;&#25903;&#37197;&#38598;&#26159;&#25351;&#19968;&#20010;&#39030;&#28857;&#23376;&#38598; $S\subseteq\mathcal{V}$&#65292;&#20351;&#24471;&#38598;&#21512;&#22806;&#30340;&#27599;&#20010;&#39030;&#28857; $v\in \mathcal{V} \setminus S$ &#37117;&#19982;&#38598;&#21512;&#20869;&#30340;&#26576;&#20010;&#39030;&#28857; $u\in S$ &#30456;&#37051;&#12290;&#26368;&#23567;&#25903;&#37197;&#38598;&#38382;&#39064;&#26088;&#22312;&#23547;&#25214;&#20855;&#26377;&#26368;&#23567;&#22522;&#25968;&#30340;&#25903;&#37197;&#38598;&#65292;&#24182;&#19988;&#26159;&#19968;&#20010;&#24050;&#30693;&#30340; NP-&#38590;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#35745;&#31639;&#26368;&#23567;&#25903;&#37197;&#38598;&#38382;&#39064;&#30340;&#35299;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#21253;&#25324;&#38543;&#26426;&#29983;&#25104;&#30340;&#22270;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#20256;&#32479;&#30340;&#36138;&#23146;&#36924;&#36817;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#20854;&#33021;&#22815;&#25193;&#23637;&#21040;&#27604;&#20854;&#35757;&#32451;&#25968;&#25454;&#25152;&#21253;&#21547;&#30340;&#26356;&#39640;&#32423;&#21035;&#30340;&#22270;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A dominating set of a graph $\mathcal{G=(V, E)}$ is a subset of vertices $S\subseteq\mathcal{V}$ such that every vertex $v\in \mathcal{V} \setminus S$ outside the dominating set is adjacent to a vertex $u\in S$ within the set. The minimum dominating set problem seeks to find a dominating set of minimum cardinality and is a well-established NP-hard combinatorial optimization problem. We propose a novel learning-based heuristic approach to compute solutions for the minimum dominating set problem using graph convolutional networks. We conduct an extensive experimental evaluation of the proposed method on a combination of randomly generated graphs and real-world graph datasets. Our results indicate that the proposed learning-based approach can outperform a classical greedy approximation algorithm. Furthermore, we demonstrate the generalization capability of the graph convolutional network across datasets and its ability to scale to graphs of higher order than those on which it was trained.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#30495;&#39537;&#21160;&#30340;&#36870;&#25512;&#29702;&#26041;&#27861;&#26469;&#23398;&#20064;&#26641;&#26525;&#21160;&#24577;&#65292;&#24182;&#21487;&#20197;&#25805;&#32437;&#21487;&#21464;&#24418;&#30340;&#26893;&#34987;&#65292;&#20197;&#35299;&#20915;&#23494;&#38598;&#26893;&#34987;&#20013;&#23481;&#26131;&#36974;&#25377;&#30340;&#20219;&#21153;&#65292;&#31639;&#27861;&#32467;&#21512;&#20102;&#29983;&#29289;&#23398;&#19978;&#30340;&#20551;&#35774;&#21644;&#20256;&#32479;&#21442;&#25968;&#25512;&#29702;&#26041;&#27861;&#30340;&#26377;&#38480;&#24046;&#20998;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.03410</link><description>&lt;p&gt;
&#23398;&#20064;&#27169;&#25311;&#26641;&#26525;&#21160;&#21147;&#23398;&#20197;&#36827;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning to Simulate Tree-Branch Dynamics for Manipulation. (arXiv:2306.03410v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#30495;&#39537;&#21160;&#30340;&#36870;&#25512;&#29702;&#26041;&#27861;&#26469;&#23398;&#20064;&#26641;&#26525;&#21160;&#24577;&#65292;&#24182;&#21487;&#20197;&#25805;&#32437;&#21487;&#21464;&#24418;&#30340;&#26893;&#34987;&#65292;&#20197;&#35299;&#20915;&#23494;&#38598;&#26893;&#34987;&#20013;&#23481;&#26131;&#36974;&#25377;&#30340;&#20219;&#21153;&#65292;&#31639;&#27861;&#32467;&#21512;&#20102;&#29983;&#29289;&#23398;&#19978;&#30340;&#20551;&#35774;&#21644;&#20256;&#32479;&#21442;&#25968;&#25512;&#29702;&#26041;&#27861;&#30340;&#26377;&#38480;&#24046;&#20998;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20223;&#30495;&#39537;&#21160;&#30340;&#36870;&#25512;&#29702;&#26041;&#27861;&#26469;&#27169;&#25311;&#25805;&#32437;&#19979;&#30340;&#26641;&#26525;&#20851;&#33410;&#21160;&#21147;&#23398;&#12290;&#23398;&#20064;&#26525;&#24178;&#21160;&#24577;&#24182;&#33719;&#24471;&#25805;&#32437;&#21487;&#21464;&#24418;&#26893;&#34987;&#30340;&#33021;&#21147;&#21487;&#24110;&#21161;&#22788;&#29702;&#23481;&#26131;&#36974;&#25377;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#23494;&#38598;&#26641;&#21494;&#20013;&#37319;&#25688;&#27700;&#26524;&#12289;&#31227;&#21160;&#24748;&#22402;&#30340;&#34276;&#34067;&#21644;&#26641;&#26525;&#65292;&#20197;&#20415;&#22312;&#23494;&#38598;&#26893;&#34987;&#20013;&#23548;&#33322;&#12290;&#26893;&#29289;&#30340;&#21487;&#21464;&#24418;&#20960;&#20309;&#24418;&#29366;&#36890;&#36807;&#22312;&#24182;&#34892;&#12289;&#19981;&#21487;&#24494;&#27169;&#25311;&#22120;&#19978;&#25191;&#34892;&#30340;&#31895;&#30053;&#24377;&#31783;&#25277;&#35937;&#26469;&#23454;&#29616;&#12290;&#30001;&#27169;&#25311;&#22120;&#23450;&#20041;&#30340;&#38544;&#24335;&#32479;&#35745;&#27169;&#22411;&#12289;&#36890;&#36807;&#20027;&#21160;&#25506;&#27979;&#30340;&#22320;&#38754;&#30495;&#23454;&#24773;&#20917;&#33719;&#24471;&#30340;&#21442;&#32771;&#36712;&#36857;&#21644;&#36125;&#21494;&#26031;&#24418;&#24335;&#20027;&#20041;&#19968;&#36215;&#25351;&#23548;&#24377;&#31783;&#21442;&#25968;&#21518;&#39564;&#23494;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26080;&#21442;&#25968;&#25512;&#29702;&#31639;&#27861;&#22522;&#20110;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65292;&#24182;&#23558;&#29983;&#29289;&#23398;&#19978;&#30340;&#20551;&#35774;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#23398;&#20064;&#32852;&#21512;&#20808;&#39564;&#21512;&#24182;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#20102;&#26377;&#38480;&#24046;&#20998;&#26041;&#26696;&#26469;&#23545;&#20989;&#25968;&#26799;&#24230;&#36827;&#34892;&#20272;&#35745;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#21442;&#25968;&#25512;&#29702;&#26041;&#27861;&#20013;&#30340;&#26799;&#24230;&#35745;&#31639;&#22256;&#38590;&#21644;&#32500;&#24230;&#28798;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to use a simulation driven inverse inference approach to model the joint dynamics of tree branches under manipulation. Learning branch dynamics and gaining the ability to manipulate deformable vegetation can help with occlusion-prone tasks, such as fruit picking in dense foliage, as well as moving overhanging vines and branches for navigation in dense vegetation. The underlying deformable tree geometry is encapsulated as coarse spring abstractions executed on parallel, non-differentiable simulators. The implicit statistical model defined by the simulator, reference trajectories obtained by actively probing the ground truth, and the Bayesian formalism, together guide the spring parameter posterior density estimation. Our non-parametric inference algorithm, based on Stein Variational Gradient Descent, incorporates biologically motivated assumptions into the inference process as neural network driven learnt joint priors; moreover, it leverages the finite difference scheme for g
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#26410;&#34987;&#35775;&#38382;&#30340;&#20915;&#31574;&#26641;&#29366;&#24577;&#21644;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;MuZero&#26234;&#33021;&#20307;&#25913;&#36827;&#20102;&#26641;&#25628;&#32034;&#35268;&#21010;&#21644;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03408</link><description>&lt;p&gt;
&#26234;&#33021;&#20307;&#36890;&#36807;&#25506;&#32034;&#20915;&#31574;&#26641;&#20013;&#30340;&#29366;&#24577;&#26469;&#25552;&#39640;&#20854;&#20915;&#31574;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Agents Explore the Environment Beyond Good Actions to Improve Their Model for Better Decisions. (arXiv:2306.03408v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03408
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#26410;&#34987;&#35775;&#38382;&#30340;&#20915;&#31574;&#26641;&#29366;&#24577;&#21644;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;MuZero&#26234;&#33021;&#20307;&#25913;&#36827;&#20102;&#26641;&#25628;&#32034;&#35268;&#21010;&#21644;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#26234;&#33021;&#20307;&#20915;&#31574;&#33021;&#21147;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#36947;&#36335;&#19978;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;MuZero&#26234;&#33021;&#20307;&#36890;&#36807;&#32593;&#32476;&#27169;&#22411;&#30340;&#39044;&#27979;&#21644;&#22522;&#20110;&#39044;&#27979;&#32467;&#26524;&#30340;&#26641;&#25628;&#32034;&#35268;&#21010;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#35268;&#21010;&#25216;&#33021;&#65292;&#20294;&#24403;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#19981;&#20934;&#30830;&#26102;&#65292;&#23398;&#20064;&#36827;&#31243;&#21487;&#33021;&#20250;&#36935;&#21040;&#29942;&#39048;&#12290;&#25105;&#20204;&#36890;&#36807;&#35753;&#26234;&#33021;&#20307;&#25506;&#32034;&#29615;&#22659;&#20013;&#20915;&#31574;&#26641;&#20013;&#19968;&#20123;&#19981;&#20250;&#34987;&#35775;&#38382;&#21040;&#30340;&#29366;&#24577;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26234;&#33021;&#20307;&#39318;&#20808;&#36890;&#36807;&#35268;&#21010;&#24471;&#21040;&#25913;&#36827;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#35757;&#32451;&#38454;&#27573;&#30340;&#24320;&#22987;&#26102;&#38543;&#26426;&#20559;&#31163;&#36825;&#20010;&#31574;&#30053;&#12290;&#22312;&#19968;&#20010;&#38543;&#26426;&#30340;&#26102;&#38388;&#38454;&#27573;&#65292;&#26234;&#33021;&#20307;&#23558;&#21448;&#24674;&#22797;&#21040;&#25913;&#36827;&#31574;&#30053;&#20197;&#24471;&#21040;&#29615;&#22659;&#22870;&#21169;&#24182;&#23545;&#26399;&#26395;&#20215;&#20540;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20117;&#23383;&#26827;&#28216;&#25103;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the decision-making capabilities of agents is a key challenge on the road to artificial intelligence. To improve the planning skills needed to make good decisions, MuZero's agent combines prediction by a network model and planning by a tree search using the predictions. MuZero's learning process can fail when predictions are poor but planning requires them. We use this as an impetus to get the agent to explore parts of the decision tree in the environment that it otherwise would not explore. The agent achieves this, first by normal planning to come up with an improved policy. Second, it randomly deviates from this policy at the beginning of each training episode. And third, it switches back to the improved policy at a random time step to experience the rewards from the environment associated with the improved policy, which is the basis for learning the correct value expectation. The simple board game Tic-Tac-Toe is used to illustrate how this approach can improve the agent's 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20960;&#20309;&#23398;&#21644;&#25299;&#25169;&#23398;&#30340;&#35282;&#24230;&#65292;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#21644;&#25345;&#20037;&#21516;&#35843;&#20998;&#24418;&#32500;&#24230;&#23545;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#21644;&#25551;&#36848;&#65292;&#26088;&#22312;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.03406</link><description>&lt;p&gt;
&#20174;&#27969;&#24418;&#23398;&#20064;&#30340;&#35282;&#24230;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks architectures from the perspective of manifold learning. (arXiv:2306.03406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20960;&#20309;&#23398;&#21644;&#25299;&#25169;&#23398;&#30340;&#35282;&#24230;&#65292;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#21644;&#25345;&#20037;&#21516;&#35843;&#20998;&#24418;&#32500;&#24230;&#23545;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#21644;&#25551;&#36848;&#65292;&#26088;&#22312;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;&#20960;&#20309;&#23398;&#21644;&#25299;&#25169;&#23398;&#30340;&#35282;&#24230;&#20840;&#38754;&#27604;&#36739;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#34920;&#31034;&#20197;&#21450;&#22312;&#19981;&#21516;&#23618;&#19978;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#32467;&#26500;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#21644;&#25345;&#20037;&#21516;&#35843;&#20998;&#24418;&#32500;&#24230;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#26500;&#20197;&#21450;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#22312;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#20869;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advances in the field of deep learning in ap-plications to various areas, an explanation of the learning pro-cess of neural network models remains an important open ques-tion. The purpose of this paper is a comprehensive comparison and description of neural network architectures in terms of ge-ometry and topology. We focus on the internal representation of neural networks and on the dynamics of changes in the topology and geometry of a data manifold on different layers. In this paper, we use the concepts of topological data analysis (TDA) and persistent homological fractal dimension. We present a wide range of experiments with various datasets and configurations of convolutional neural network (CNNs) architectures and Transformers in CV and NLP tasks. Our work is a contribution to the development of the important field of explainable and interpretable AI within the framework of geometrical deep learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36924;&#36817;&#36710;&#36742;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#36895;&#33258;&#20027;&#36187;&#36710;&#20013;&#65292;&#24179;&#34913;&#35745;&#31639;&#38656;&#27714;&#21644;&#27169;&#22411;&#31934;&#24230;&#21518;&#65292;&#35813;&#26041;&#27861;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2306.03405</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#30340;&#33258;&#20027;&#36187;&#36710;&#36710;&#36742;&#21160;&#21147;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Vehicle Dynamics Modeling for Autonomous Racing Using Gaussian Processes. (arXiv:2306.03405v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36924;&#36817;&#36710;&#36742;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#36895;&#33258;&#20027;&#36187;&#36710;&#20013;&#65292;&#24179;&#34913;&#35745;&#31639;&#38656;&#27714;&#21644;&#27169;&#22411;&#31934;&#24230;&#21518;&#65292;&#35813;&#26041;&#27861;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36187;&#36710;&#36234;&#26469;&#36234;&#25104;&#20026;&#24403;&#21069;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#19968;&#20010;&#35797;&#39564;&#22330;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#26469;&#36924;&#36817;&#36710;&#36742;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24179;&#34913;&#35745;&#31639;&#38656;&#27714;&#21644;&#27169;&#22411;&#31934;&#24230;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#36710;&#36742;&#21160;&#21147;&#23398;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous racing is increasingly becoming a proving ground for autonomous vehicle technology at the limits of its current capabilities. The most prominent examples include the F1Tenth racing series, Formula Student Driverless (FSD), Roborace, and the Indy Autonomous Challenge (IAC). Especially necessary, in high speed autonomous racing, is the knowledge of accurate racecar vehicle dynamics. The choice of the vehicle dynamics model has to be made by balancing the increasing computational demands in contrast to improved accuracy of more complex models. Recent studies have explored learning-based methods, such as Gaussian Process (GP) regression for approximating the vehicle dynamics model. However, these efforts focus on higher level constructs such as motion planning, or predictive control and lack both in realism and rigor of the GP modeling process, which is often over-simplified. This paper presents the most detailed analysis of the applicability of GP models for approximating vehic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SGAT4PASS&#65292;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#36890;&#36807;&#21152;&#20837;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#26223;&#22270;&#20687;&#30340;3D&#23646;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03403</link><description>&lt;p&gt;
SGAT4PASS&#65306;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer
&lt;/p&gt;
&lt;p&gt;
SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation. (arXiv:2306.03403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SGAT4PASS&#65292;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#36890;&#36807;&#21152;&#20837;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#26223;&#22270;&#20687;&#30340;3D&#23646;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;&#21487;&#20197;&#26681;&#25454;&#36229;&#24191;&#35282;&#35266;&#23519;&#21040;&#30340;&#23436;&#25972;&#22330;&#26223;&#26469;&#36827;&#34892;&#24863;&#30693;&#12290;&#20256;&#32479;&#30340;&#38024;&#23545;2D&#20840;&#26223;&#22270;&#20687;&#30340;PASS&#26041;&#27861;&#20391;&#37325;&#20110;&#35299;&#20915;&#22270;&#20687;&#30072;&#21464;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047;&#23545;&#21407;&#22987;360&#176;&#25968;&#25454;&#30340;3D&#23646;&#24615;&#30340;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#24403;&#36755;&#20837;&#20855;&#26377;3D&#25200;&#21160;&#30340;&#20840;&#26223;&#22270;&#20687;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#22823;&#24133;&#19979;&#38477;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#24212;&#23545;3D&#25200;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#21363;SGAT4PASS&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20998;&#21106;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65292;&#21363;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#22270;&#20687;&#25237;&#24433;&#65292;&#29699;&#38754;&#21487;&#24418;&#21464;&#34917;&#19969;&#23884;&#20837;&#21644;&#20840;&#26223;&#24863;&#30693;&#25439;&#22833;&#65292;&#23427;&#23545;&#20855;&#26377;3D&#25200;&#21160;&#30340;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#23545;&#24050;&#26377;&#30340;&#21487;&#24418;&#21464;&#34917;&#19969;&#23884;&#20837;&#21152;&#20837;&#20102;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an important and challenging problem in computer vision, PAnoramic Semantic Segmentation (PASS) gives complete scene perception based on an ultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic image input focus on solving image distortions but lack consideration of the 3D properties of original $360^{\circ}$ data. Therefore, their performance will drop a lot when inputting panoramic images with the 3D disturbance. To be more robust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical geometry knowledge. Specifically, a spherical geometry-aware framework is proposed for PASS. It includes three modules, i.e., spherical geometry-aware image projection, spherical deformable patch embedding, and a panorama-aware loss, which takes input images with 3D disturbance into account, adds a spherical geometry-aware constraint on the existing deformable patch embedding, and indicates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35299;&#20915;&#24102;&#26377;&#23454;&#20363;&#21644;&#26631;&#31614;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#20110;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24471;&#21040;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#36229;&#39069;&#39118;&#38505;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.03402</link><description>&lt;p&gt;
&#24102;&#26377;&#23454;&#20363;&#21644;&#26631;&#31614;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Binary Classification with Instance and Label Dependent Label Noise. (arXiv:2306.03402v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35299;&#20915;&#24102;&#26377;&#23454;&#20363;&#21644;&#26631;&#31614;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#20110;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24471;&#21040;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#36229;&#39069;&#39118;&#38505;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#24102;&#26377;&#26631;&#31614;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#35752;&#65292;&#28982;&#32780;&#22788;&#29702;&#24102;&#26377;&#23454;&#20363;&#21644;&#26631;&#31614;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#22256;&#38590;&#22312;&#20110;&#22122;&#22768;&#29575;&#22240;&#27599;&#20010;&#23454;&#20363;&#32780;&#24322;&#65292;&#20351;&#24471;&#20934;&#30830;&#20272;&#35745;&#22122;&#22768;&#29575;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#35299;&#20915;&#33021;&#21542;&#20165;&#20351;&#29992;&#21547;&#26377;&#22122;&#22768;&#26679;&#26412;&#26469;&#23398;&#20064;&#21487;&#38752;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#21305;&#37197;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#20551;&#35774;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#36229;&#39069;&#39118;&#38505;&#30028;&#38480;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20174;&#24178;&#20928;&#26679;&#26412;&#21644;&#22122;&#22768;&#26679;&#26412;&#20013;&#24471;&#21040;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#26469;&#23548;&#20986;&#19968;&#31181;&#19982;&#22122;&#22768;&#27700;&#24179;&#25104;&#27604;&#20363;&#30340;&#26032;&#30340;&#36229;&#39069;&#39118;&#38505;&#30028;&#38480;&#65292;&#22312;&#38750;&#24120;&#19968;&#33324;&#30340;&#24773;&#20917;&#19979;&#37117;&#25104;&#31435;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#34920;&#26126;&#20102;0-1&#25439;&#22833;&#30340;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#26159;&#19968;&#20010;&#19982;&#26631;&#31614;&#25968;&#25104;&#27604;&#20363;&#30340;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with label dependent label noise has been extensively explored in both theory and practice; however, dealing with instance (i.e., feature) and label dependent label noise continues to be a challenging task. The difficulty arises from the fact that the noise rate varies for each instance, making it challenging to estimate accurately. The question of whether it is possible to learn a reliable model using only noisy samples remains unresolved. We answer this question with a theoretical analysis that provides matching upper and lower bounds. Surprisingly, our results show that, without any additional assumptions, empirical risk minimization achieves the optimal excess risk bound. Specifically, we derive a novel excess risk bound proportional to the noise level, which holds in very general settings, by comparing the empirical risk minimizers obtained from clean samples and noisy samples. Second, we show that the minimax lower bound for the 0-1 loss is a constant proportional to the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#35843;&#25972;&#32852;&#37030;&#24179;&#22343;&#20013;&#30340;&#32858;&#21512;&#26435;&#37325;&#65292;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#30340;&#21442;&#19982;&#21382;&#21490;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21442;&#19982;&#29575;&#30340;&#23458;&#25143;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#26410;&#30693;&#21442;&#19982;&#27010;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03401</link><description>&lt;p&gt;
&#22788;&#29702;&#32852;&#37030;&#24179;&#22343;&#20013;&#26410;&#30693;&#21442;&#19982;&#27010;&#29575;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Lightweight Method for Tackling Unknown Participation Probabilities in Federated Averaging. (arXiv:2306.03401v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#35843;&#25972;&#32852;&#37030;&#24179;&#22343;&#20013;&#30340;&#32858;&#21512;&#26435;&#37325;&#65292;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#30340;&#21442;&#19982;&#21382;&#21490;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21442;&#19982;&#29575;&#30340;&#23458;&#25143;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#26410;&#30693;&#21442;&#19982;&#27010;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#36890;&#24120;&#20855;&#26377;&#20808;&#39564;&#26410;&#30693;&#30340;&#19981;&#21516;&#21442;&#19982;&#29575;&#65292;&#22914;&#26524;&#19981;&#36866;&#24403;&#22788;&#29702;&#65292;&#21017;&#21487;&#33021;&#20250;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#36896;&#25104;&#37325;&#22823;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#20840;&#23616;&#26041;&#24046;&#32553;&#20943;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#39069;&#22806;&#30340;&#20869;&#23384;&#65292;&#20854;&#20056;&#27861;&#22240;&#23376;&#31561;&#20110;&#23458;&#25143;&#24635;&#25968;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#26159;&#25214;&#21040;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#22791;&#19981;&#21516;&#21442;&#19982;&#29575;&#23458;&#25143;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#30340;&#21442;&#19982;&#21382;&#21490;&#26469;&#35843;&#25972;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#20013;&#30340;&#32858;&#21512;&#26435;&#37325;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#21442;&#19982;&#27010;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#38750;&#26368;&#20248;&#32858;&#21512;&#26435;&#37325;&#30340;FedAvg&#21487;&#33021;&#20250;&#20174;&#21407;&#22987;FL&#30446;&#26631;&#30340;&#26368;&#20248;&#35299;&#20559;&#31163;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#25214;&#21040;&#26368;&#20248;&#32858;&#21512;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#24403;&#21442;&#19982;&#27010;&#29575;&#19981;&#21487;&#30693;&#26102;&#35745;&#31639;&#26368;&#20248;&#26435;&#37325;&#38750;&#24120;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning (FL), clients usually have diverse participation probabilities that are unknown a priori, which can significantly harm the performance of FL if not handled properly. Existing works aiming at addressing this problem are usually based on global variance reduction, which requires a substantial amount of additional memory in a multiplicative factor equal to the total number of clients. An important open problem is to find a lightweight method for FL in the presence of clients with unknown participation rates. In this paper, we address this problem by adapting the aggregation weights in federated averaging (FedAvg) based on the participation history of each client. We first show that, with heterogeneous participation probabilities, FedAvg with non-optimal aggregation weights can diverge from the optimal solution of the original FL objective, indicating the need of finding optimal aggregation weights. However, it is difficult to compute the optimal weights when the part
&lt;/p&gt;</description></item><item><title>G-CAME &#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;&#39640;&#26031;&#31867;&#28608;&#27963;&#26144;&#23556;&#35299;&#37322;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#28608;&#27963;&#26144;&#23556;&#19982;&#39640;&#26031;&#26680;&#29983;&#25104;&#26174;&#33879;&#24615;&#22270;&#26469;&#31361;&#20986;&#26174;&#31034;&#22270;&#20687;&#20013;&#19982;&#39044;&#27979;&#26694;&#30456;&#20851;&#30340;&#37325;&#35201;&#21306;&#22495;&#65292;&#20855;&#26377;&#24456;&#30701;&#26102;&#38388;&#35299;&#37322;&#23545;&#35937;&#31561;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.03400</link><description>&lt;p&gt;
G-CAME: &#38754;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;&#39640;&#26031;&#31867;&#28608;&#27963;&#26144;&#23556;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
G-CAME: Gaussian-Class Activation Mapping Explainer for Object Detectors. (arXiv:2306.03400v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03400
&lt;/p&gt;
&lt;p&gt;
G-CAME &#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;&#39640;&#26031;&#31867;&#28608;&#27963;&#26144;&#23556;&#35299;&#37322;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#28608;&#27963;&#26144;&#23556;&#19982;&#39640;&#26031;&#26680;&#29983;&#25104;&#26174;&#33879;&#24615;&#22270;&#26469;&#31361;&#20986;&#26174;&#31034;&#22270;&#20687;&#20013;&#19982;&#39044;&#27979;&#26694;&#30456;&#20851;&#30340;&#37325;&#35201;&#21306;&#22495;&#65292;&#20855;&#26377;&#24456;&#30701;&#26102;&#38388;&#35299;&#37322;&#23545;&#35937;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#65292;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#26222;&#21450;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#29992;&#25143;&#24456;&#38590;&#29702;&#35299;&#27169;&#22411;&#20026;&#20160;&#20040;&#20250;&#26816;&#27979;&#20986;&#36825;&#20123;&#23545;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#26031;&#31867;&#28608;&#27963;&#26144;&#23556;&#35299;&#37322;&#22120;&#65288;G-CAME&#65289;&#65292;&#23427;&#29983;&#25104;&#26174;&#33879;&#24615;&#22270;&#20316;&#20026;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#35828;&#26126;&#12290; G-CAME &#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#22522;&#20110; CAM &#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#23618;&#30340;&#28608;&#27963;&#26144;&#23556;&#19982;&#39640;&#26031;&#26680;&#26469;&#31361;&#20986;&#26174;&#31034;&#22270;&#20687;&#20013;&#19982;&#39044;&#27979;&#26694;&#30456;&#20851;&#30340;&#37325;&#35201;&#21306;&#22495;&#12290;&#19982;&#20854;&#20182;&#22522;&#20110;&#21306;&#22495;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;G-CAME &#21487;&#20197;&#36229;&#36234;&#26102;&#38388;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#24456;&#30701;&#26102;&#38388;&#23601;&#33021;&#35299;&#37322;&#19968;&#20010;&#23545;&#35937;&#12290;&#25105;&#20204;&#36824;&#22312; MS-COCO 2017 &#25968;&#25454;&#38598;&#19978;&#20351;&#29992; YOLOX &#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#23548;&#23558; G-CAME &#24212;&#29992;&#20110;&#20004;&#38454;&#27573; Faster-RCNN &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, deep neural networks for object detection in images are very prevalent. However, due to the complexity of these networks, users find it hard to understand why these objects are detected by models. We proposed Gaussian Class Activation Mapping Explainer (G-CAME), which generates a saliency map as the explanation for object detection models. G-CAME can be considered a CAM-based method that uses the activation maps of selected layers combined with the Gaussian kernel to highlight the important regions in the image for the predicted box. Compared with other Region-based methods, G-CAME can transcend time constraints as it takes a very short time to explain an object. We also evaluated our method qualitatively and quantitatively with YOLOX on the MS-COCO 2017 dataset and guided to apply G-CAME into the two-stage Faster-RCNN model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#29289;&#29702;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;ML&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ODGN&#30340;&#27169;&#22411;&#26469;&#29983;&#25104;&#36215;&#22987;-&#30446;&#30340;&#22320;&#32593;&#32476;&#65292;&#20197;&#26356;&#22909;&#22320;&#36827;&#34892;&#20154;&#21475;&#27969;&#21160;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2306.03390</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#21147;&#23548;&#21521;&#30340;GAN&#29983;&#25104;&#36215;&#22987;-&#30446;&#30340;&#22320;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Origin-Destination Network Generation via Gravity-Guided GAN. (arXiv:2306.03390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#29289;&#29702;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;ML&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ODGN&#30340;&#27169;&#22411;&#26469;&#29983;&#25104;&#36215;&#22987;-&#30446;&#30340;&#22320;&#32593;&#32476;&#65292;&#20197;&#26356;&#22909;&#22320;&#36827;&#34892;&#20154;&#21475;&#27969;&#21160;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36215;&#22987;-&#30446;&#30340;&#22320;&#65288;OD&#65289;&#27969;&#21253;&#21547;&#26377;&#20215;&#20540;&#30340;&#20154;&#21475;&#36801;&#31227;&#20449;&#24687;&#65292;&#21253;&#25324;&#26041;&#21521;&#21644;&#20307;&#31215;&#65292;&#22312;&#35768;&#22810;&#22478;&#24066;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#22478;&#24066;&#35268;&#21010;&#65292;&#20132;&#36890;&#31649;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#25104;&#26412;&#25110;&#38544;&#31169;&#38382;&#39064;&#65292;OD&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#26131;&#20110;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#32771;&#34385;&#36890;&#36807;&#25968;&#23398;&#27169;&#22411;&#29983;&#25104;OD&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#29289;&#29702;&#23450;&#24459;&#25110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26469;&#26500;&#24314;&#22478;&#24066;&#32467;&#26500;&#21644;OD&#27969;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20998;&#21035;&#21463;&#21040;&#36807;&#24230;&#31616;&#21270;&#21644;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#29289;&#29702;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;ML&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;ML&#33539;&#24335;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#21517;&#20026;Origin-Destination Generation Networks&#65288;ODGN&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#21644;ML&#26041;&#27861;&#30340;&#20114;&#34917;&#20248;&#21183;&#26469;&#23454;&#29616;&#26356;&#22909;&#22320;&#20154;&#21475;&#27969;&#21160;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#19968;&#20010;&#22810;&#35270;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Origin-destination (OD) flow, which contains valuable population mobility information including direction and volume, is critical in many urban applications, such as urban planning, transportation management, etc. However, OD data is not always easy to access due to high costs or privacy concerns. Therefore, we must consider generating OD through mathematical models. Existing works utilize physics laws or machine learning (ML) models to build the association between urban structures and OD flows while these two kinds of methods suffer from the limitation of over-simplicity and poor generalization ability, respectively. In this paper, we propose to adopt physics-informed ML paradigm, which couple the physics scientific knowledge and data-driven ML methods, to construct a model named Origin-Destination Generation Networks (ODGN) for better population mobility modeling by leveraging the complementary strengths of combining physics and ML methods. Specifically, we first build a Multi-view 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#22312;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#26102;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#23581;&#35797;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65292;&#21363;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#36880;&#20010;&#26465;&#30446;&#30340;&#31934;&#30830;&#38169;&#35823;&#30028;&#38480;&#65292;&#36825;&#26159;&#22312;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#20013;&#39318;&#27425;&#32435;&#20837;&#22122;&#22768;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.03372</link><description>&lt;p&gt;
&#22312;&#32447;&#24352;&#37327;&#23398;&#20064;&#65306;&#35745;&#31639;&#21644;&#32479;&#35745;&#26435;&#34913;&#65292;&#36866;&#24212;&#24615;&#21644;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret. (arXiv:2306.03372v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#22312;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#26102;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#23581;&#35797;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65292;&#21363;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#36880;&#20010;&#26465;&#30446;&#30340;&#31934;&#30830;&#38169;&#35823;&#30028;&#38480;&#65292;&#36825;&#26159;&#22312;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#20013;&#39318;&#27425;&#32435;&#20837;&#22122;&#22768;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65306;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#25152;&#26377;&#24212;&#29992;&#31243;&#24207;&#20013;&#37117;&#21487;&#20197;&#26681;&#25454;&#36866;&#24403;&#30340;&#26465;&#20214;&#32447;&#24615;&#25910;&#25947;&#24182;&#24674;&#22797;&#20302;&#31209;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#24314;&#31435;&#20102;&#31934;&#30830;&#30340;&#36880;&#20010;&#26465;&#30446;&#38169;&#35823;&#30028;&#38480;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20195;&#34920;&#20102;&#39318;&#27425;&#23581;&#35797;&#22312;&#22312;&#32447;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#20219;&#21153;&#20013;&#32435;&#20837;&#22122;&#22768;&#30340;&#21162;&#21147;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;&#22686;&#21152;&#27493;&#38271;&#21487;&#20197;&#21152;&#24555;&#25910;&#25947;&#65292;&#20294;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#32479;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate a generalized framework for estimating latent low-rank tensors in an online setting, encompassing both linear and generalized linear models. This framework offers a flexible approach for handling continuous or categorical variables. Additionally, we investigate two specific applications: online tensor completion and online binary tensor learning. To address these challenges, we propose the online Riemannian gradient descent algorithm, which demonstrates linear convergence and the ability to recover the low-rank component under appropriate conditions in all applications. Furthermore, we establish a precise entry-wise error bound for online tensor completion. Notably, our work represents the first attempt to incorporate noise in the online low-rank tensor recovery task. Intriguingly, we observe a surprising trade-off between computational and statistical aspects in the presence of noise. Increasing the step size accelerates convergence but leads to higher statistical error
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#29699;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#25512;&#21521;&#22266;&#23450;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#65292;&#20174;&#32780;&#33021;&#22815;&#24212;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03364</link><description>&lt;p&gt;
&#22312;&#21333;&#20301;&#29699;&#19978;&#23398;&#20064;&#34920;&#31034;&#65306;&#24212;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Representations on the Unit Sphere: Application to Online Continual Learning. (arXiv:2306.03364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#29699;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#25512;&#21521;&#22266;&#23450;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#65292;&#20174;&#32780;&#33021;&#22815;&#24212;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#21407;&#29702;&#26469;&#23398;&#20064;&#20998;&#24067;&#22312;&#21333;&#20301;&#29699;&#19978;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#38024;&#23545;&#23545;&#31216;&#26041;&#21521;&#25968;&#25454;&#24314;&#31435;&#20102; von Mises-Fisher &#20998;&#24067;&#21644;&#35282;&#39640;&#26031;&#20998;&#24067;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#34987;&#25512;&#21521;&#22266;&#23450;&#30340;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#12290;&#36825;&#20351;&#24471;&#23427;&#36866;&#21512;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65292;&#21363;&#22312;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#25353;&#39034;&#24207;&#21576;&#29616;&#65292;&#22240;&#27492;&#36807;&#21435;&#20219;&#21153;&#30340;&#25968;&#25454;&#19981;&#20877;&#21487;&#29992;&#65292;&#24403;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#21482;&#33021;&#30475;&#19968;&#27425;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#37197;&#22791;&#20102;&#25105;&#20204;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#36127;&#25968;&#25454;&#25110;&#20219;&#21153;&#36793;&#30028;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#30340;&#25209;&#22788;&#29702;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use the maximum a posteriori estimation principle for learning representations distributed on the unit sphere. We derive loss functions for the von Mises-Fisher distribution and the angular Gaussian distribution, both designed for modeling symmetric directional data. A noteworthy feature of our approach is that the learned representations are pushed toward fixed directions, allowing for a learning strategy that is resilient to data drift. This makes it suitable for online continual learning, which is the problem of training neural networks on a continuous data stream, where multiple classification tasks are presented sequentially so that data from past tasks are no longer accessible, and data from the current task can be seen only once. To address this challenging scenario, we propose a memory-based representation learning technique equipped with our new loss functions. Our approach does not require negative data or knowledge of task boundaries and performs well with smaller batch s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Offline-with-Action-Preferences&#65288;OAP&#65289;&#30340;&#26080;&#20132;&#20114;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#26597;&#35810;&#20808;&#21069;&#25910;&#38598;&#30340;&#21644;&#23398;&#20064;&#21040;&#30340;&#34892;&#21160;&#20043;&#38388;&#30340;&#20559;&#22909;&#65292;&#26469;&#24110;&#21161;&#35299;&#20915;&#38169;&#35823;&#20272;&#35745;&#38382;&#39064;&#65292;&#20174;&#32780;&#33719;&#24471;&#23545;&#26410;&#35265;&#25968;&#25454;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.03362</link><description>&lt;p&gt;
&#20351;&#29992;&#34892;&#20026;&#20559;&#22909;&#26597;&#35810;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Offline Reinforcement Learning with Action Preference Query. (arXiv:2306.03362v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Offline-with-Action-Preferences&#65288;OAP&#65289;&#30340;&#26080;&#20132;&#20114;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#26597;&#35810;&#20808;&#21069;&#25910;&#38598;&#30340;&#21644;&#23398;&#20064;&#21040;&#30340;&#34892;&#21160;&#20043;&#38388;&#30340;&#20559;&#22909;&#65292;&#26469;&#24110;&#21161;&#35299;&#20915;&#38169;&#35823;&#20272;&#35745;&#38382;&#39064;&#65292;&#20174;&#32780;&#33719;&#24471;&#23545;&#26410;&#35265;&#25968;&#25454;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#23454;&#29992;&#20195;&#29702;&#36890;&#24120;&#28041;&#21450;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20197;&#24179;&#34913;&#25919;&#31574;&#30340;&#24615;&#33021;&#21644;&#20132;&#20114;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#20132;&#20114;&#30340;&#35757;&#32451;&#26041;&#26696; Offline-with-Action-Preferences&#65288;OAP&#65289;&#12290; OAP&#30340;&#20027;&#35201;&#35265;&#35299;&#26159;&#65292;&#19982;&#22312;&#32447;&#24494;&#35843;&#30456;&#27604;&#65292;&#26597;&#35810;&#20107;&#20808;&#25910;&#38598;&#30340;&#21644;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#20559;&#22909;&#21487;&#20197;&#21516;&#26679;&#25110;&#29978;&#33267;&#26356;&#26377;&#21161;&#20110;&#35299;&#20915;&#38169;&#35823;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#26681;&#25454;&#34892;&#20026;&#20559;&#22909;&#33258;&#36866;&#24212;&#22320;&#40723;&#21169;&#25110;&#25233;&#21046;&#31574;&#30053;&#32422;&#26463;&#65292;OAP&#21487;&#20197;&#21306;&#20998;&#36807;&#24230;&#20272;&#35745;&#21644;&#26377;&#30410;&#30340;&#31574;&#30053;&#25913;&#36827;&#65292;&#20174;&#32780;&#33719;&#24471;&#23545;&#26410;&#35265;&#25968;&#25454;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training practical agents usually involve offline and online reinforcement learning (RL) to balance the policy's performance and interaction costs. In particular, online fine-tuning has become a commonly used method to correct the erroneous estimates of out-of-distribution data learned in the offline training phase. However, even limited online interactions can be inaccessible or catastrophic for high-stake scenarios like healthcare and autonomous driving. In this work, we introduce an interaction-free training scheme dubbed Offline-with-Action-Preferences (OAP). The main insight is that, compared to online fine-tuning, querying the preferences between pre-collected and learned actions can be equally or even more helpful to the erroneous estimate problem. By adaptively encouraging or suppressing policy constraint according to action preferences, OAP could distinguish overestimation from beneficial policy improvement and thus attains a more accurate evaluation of unseen data. Theoretica
&lt;/p&gt;</description></item><item><title>Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.03360</link><description>&lt;p&gt;
Vid2Act&#65306;&#20026;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#28608;&#27963;&#31163;&#32447;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Vid2Act: Activate Offline Videos for Visual RL. (arXiv:2306.03360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03360
&lt;/p&gt;
&lt;p&gt;
Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26159;&#25552;&#39640;&#20854;&#22312;&#32447;&#20219;&#21153;&#25928;&#29575;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#36328;&#22495;&#20013;&#20219;&#21153;&#12289;&#21160;&#24577;&#21644;&#34892;&#20026;&#30340;&#22266;&#26377;&#19981;&#21305;&#37197;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;APV&#30340;&#27169;&#22411;&#36991;&#20813;&#20102;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#20276;&#38543;&#21160;&#20316;&#35760;&#24405;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#22312;&#28304;&#22495;&#20869;&#39044;&#35757;&#32451;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#12289;&#19981;&#28041;&#21450;&#25805;&#20316;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Vid2Act&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20174;&#31163;&#32447;&#21040;&#22312;&#32447;&#29615;&#22659;&#20013;&#20256;&#36755;&#26377;&#20215;&#20540;&#30340;&#21160;&#20316;&#26465;&#20214;&#21160;&#24577;&#21644;&#28508;&#22312;&#26377;&#29992;&#30340;&#21160;&#20316;&#28436;&#31034;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#19981;&#20165;&#23558;&#19990;&#30028;&#27169;&#22411;&#29992;&#20316;&#34892;&#20026;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#65292;&#36824;&#23558;&#20854;&#29992;&#20316;&#27979;&#37327;&#39046;&#22495;&#30456;&#20851;&#24615;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#36827;&#34892;&#21160;&#24577;&#34920;&#31034;&#20256;&#36755;&#21644;&#31574;&#30053;&#20256;&#36755;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22495;&#36873;&#25321;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#35757;&#32451;&#19990;&#30028;&#27169;&#22411;&#29983;&#25104;&#19968;&#32452;&#26102;&#38388;&#21464;&#21270;&#30340;&#20219;&#21153;&#30456;&#20284;&#24230;&#12290;&#36825;&#20123;&#30456;&#20284;&#24230;&#26377;&#20004;&#20010;&#30446;&#30340;&#65306;&#65288;i&#65289;&#33258;&#36866;&#24212;&#22320;&#23558;&#26368;&#30456;&#20851;&#30340;&#39046;&#22495;&#30340;&#21160;&#24577;&#20256;&#36755;&#21040;&#22312;&#32447;&#29615;&#22659;&#65292;&#21644;&#65288;ii&#65289;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25351;&#23548;&#20195;&#29702;&#38598;&#20013;&#25191;&#34892;&#20219;&#21153;&#30456;&#20851;&#30340;&#21160;&#20316;&#12290;&#22312;Atari&#21644;DMControl&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining RL models on offline video datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV, sidesteps the accompanied action records in offline datasets and instead focuses on pretraining a task-irrelevant, action-free world model within the source domains. We present Vid2Act, a model-based RL method that learns to transfer valuable action-conditioned dynamics and potentially useful action demonstrations from offline to online settings. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the domain relevance for both dynamics representation transfer and policy transfer. Specifically, we train the world models to generate a set of time-varying task similarities using a domain-selective knowledge distillation loss. These similarities serve two purposes: (i) adaptively transferring th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#36817;&#27491;&#20132;&#22522;&#20989;&#25968;&#26063;&#30340;&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#20248;&#21270;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.03356</link><description>&lt;p&gt;
&#36817;&#27491;&#20132;&#22522;&#20989;&#25968;&#26063;&#30340;&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Query Complexity of Active Learning for Function Family With Nearly Orthogonal Basis. (arXiv:2306.03356v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03356
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#36817;&#27491;&#20132;&#22522;&#20989;&#25968;&#26063;&#30340;&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#20248;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#25165;&#33021;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22312;&#21307;&#23398;&#35786;&#26029;&#21644;&#27450;&#35784;&#26816;&#27979;&#31561;&#39046;&#22495;&#65292;&#34429;&#28982;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#23558;&#36825;&#20123;&#25968;&#25454;&#26631;&#35760;&#25104;&#26412;&#39640;&#26114;&#12290;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#28857;&#25968;&#30446;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#23545;&#20110;&#35768;&#22810;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20363;&#22914;&#32447;&#24615;&#22238;&#24402;&#21644;$p$- &#33539;&#25968;&#22238;&#24402;&#65292;&#37117;&#24050;&#32463;&#23384;&#22312;&#20102;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#30446;&#23454;&#29616;&#26576;&#31181;&#20934;&#30830;&#24615;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#20027;&#21160;&#23398;&#20064;&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#20170;&#22825;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#25152;&#23398;&#20064;&#20989;&#25968;&#30340;&#27491;&#20132;&#22522;&#19982;&#30446;&#26631;&#20989;&#25968;&#30456;&#21305;&#37197;&#12290;&#20363;&#22914;&#65292;&#24403;&#23558;&#20027;&#21160;&#23398;&#20064;&#24212;&#29992;&#20110;&#32447;&#24615;&#22238;&#24402;&#26102;&#65292;&#35201;&#27714;&#30446;&#26631;&#20989;&#25968;&#26159;&#19968;&#32452;&#27491;&#20132;&#32447;&#24615;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#25214;&#21040;&#36825;&#20123;&#32447;&#24615;&#20989;&#25968;&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning algorithms require large numbers of labeled data to deliver state-of-the-art results. In applications such as medical diagnosis and fraud detection, though there is an abundance of unlabeled data, it is costly to label the data by experts, experiments, or simulations. Active learning algorithms aim to reduce the number of required labeled data points while preserving performance. For many convex optimization problems such as linear regression and $p$-norm regression, there are theoretical bounds on the number of required labels to achieve a certain accuracy. We call this the query complexity of active learning. However, today's active learning algorithms require the underlying learned function to have an orthogonal basis. For example, when applying active learning to linear regression, the requirement is the target function is a linear composition of a set of orthogonal linear functions, and active learning can find the coefficients of these linear functions. We p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;BatchSampler&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#37319;&#26679;&#38590;&#20197;&#21306;&#20998;&#30340;&#23454;&#20363;&#30340;&#23567;&#25209;&#37327;&#65292;&#24182;&#21033;&#29992;&#37325;&#21551;&#38543;&#26426;&#28216;&#36208;&#26469;&#24418;&#25104;&#23567;&#25209;&#37327;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03355</link><description>&lt;p&gt;
BatchSampler&#65306;&#29992;&#20110;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22270;&#24418;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#23567;&#25209;&#37327;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs. (arXiv:2306.03355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;BatchSampler&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#37319;&#26679;&#38590;&#20197;&#21306;&#20998;&#30340;&#23454;&#20363;&#30340;&#23567;&#25209;&#37327;&#65292;&#24182;&#21033;&#29992;&#37325;&#21551;&#38543;&#26426;&#28216;&#36208;&#26469;&#24418;&#25104;&#23567;&#25209;&#37327;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
In-Batch&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#23427;&#23558;&#35821;&#20041;&#30456;&#20284;&#30340;&#23454;&#20363;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#23558;&#19981;&#30456;&#20284;&#30340;&#23454;&#20363;&#25512;&#21040;&#36828;&#31163;mini-batch&#20043;&#22806;&#12290;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#36127;&#26679;&#26412;&#20849;&#20139;&#31574;&#30053;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#20363;&#37117;&#20316;&#20026;mini-batch&#20013;&#20854;&#20182;&#23454;&#20363;&#30340;&#36127;&#26679;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22312;&#24403;&#21069;mini-batch&#33539;&#22260;&#20869;&#37319;&#26679;&#38590;&#36127;&#26679;&#26412;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20854;&#36136;&#37327;&#20165;&#21463;&#38480;&#20110;mini-batch&#26412;&#36523;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#37319;&#26679;mini-batch&#26469;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BatchSampler&#26469;&#37319;&#26679;&#38590;&#20197;&#21306;&#20998;&#30340;&#65288;&#21363;&#24444;&#27492;&#38590;&#20197;&#21306;&#20998;&#30340;&#22256;&#38590;&#21644;&#30495;&#23454;&#30340;&#36127;&#26679;&#26412;&#65289;&#23454;&#20363;&#30340;&#23567;&#25209;&#37327;&#12290;&#20026;&#20102;&#20351;&#27599;&#20010;&#23567;&#25209;&#37327;&#20855;&#26377;&#26356;&#23569;&#30340;&#20551;&#36127;&#26679;&#26412;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38543;&#26426;&#36873;&#25321;&#23454;&#20363;&#30340;&#25509;&#36817;&#24230;&#22270;&#12290;&#20026;&#20102;&#24418;&#25104;&#23567;&#25209;&#37327;&#65292;&#25105;&#20204;&#21033;&#29992;&#25509;&#36817;&#24230;&#22270;&#19978;&#30340;&#37325;&#21551;&#38543;&#26426;&#28216;&#36208;&#26469;&#36741;&#21161;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Batch contrastive learning is a state-of-the-art self-supervised method that brings semantically-similar instances close while pushing dissimilar instances apart within a mini-batch. Its key to success is the negative sharing strategy, in which every instance serves as a negative for the others within the mini-batch. Recent studies aim to improve performance by sampling hard negatives \textit{within the current mini-batch}, whose quality is bounded by the mini-batch itself. In this work, we propose to improve contrastive learning by sampling mini-batches from the input data. We present BatchSampler\footnote{The code is available at \url{https://github.com/THUDM/BatchSampler}} to sample mini-batches of hard-to-distinguish (i.e., hard and true negatives to each other) instances. To make each mini-batch have fewer false negatives, we design the proximity graph of randomly-selected instances. To form the mini-batch, we leverage random walk with restart on the proximity graph to help sam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#23454;&#39564;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03346</link><description>&lt;p&gt;
&#31283;&#23450;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;: &#31163;&#32447;&#30446;&#26631;&#36798;&#25104;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Contrastive RL: Techniques for Offline Goal Reaching. (arXiv:2306.03346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#23454;&#39564;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24050;&#32463;&#24320;&#21457;&#20102;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24378;&#21270;&#23398;&#20064;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#33258;&#30417;&#30563;&#38382;&#39064;&#65306;&#23398;&#20064;&#36798;&#21040;&#20219;&#20309;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#25351;&#23450;&#30340;&#22870;&#21169;&#25110;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#24314;&#31435;&#33258;&#30417;&#30563;&#22522;&#30784;&#23454;&#38469;&#19978;&#38754;&#20020;&#30528;&#19968;&#20123;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#27492;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#21078;&#26512;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19982;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36890;&#36807;&#36825;&#20123;&#35774;&#35745;&#20915;&#31574;&#65292;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#20854;&#20013;&#20219;&#21153;&#30001;&#35757;&#32451;&#21518;&#25552;&#20379;&#30340;&#21333;&#20010;&#30446;&#26631;&#22270;&#20687;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the same way that the computer vision (CV) and natural language processing (NLP) communities have developed self-supervised methods, reinforcement learning (RL) can be cast as a self-supervised problem: learning to reach any goal, without requiring human-specified rewards or labels. However, actually building a self-supervised foundation for RL faces some important challenges. Building on prior contrastive approaches to this RL problem, we conduct careful ablation experiments and discover that a shallow and wide architecture, combined with careful weight initialization and data augmentation, can significantly boost the performance of these contrastive RL approaches on challenging simulated benchmarks. Additionally, we demonstrate that, with these design decisions, contrastive approaches can solve real-world robotic manipulation tasks, with tasks being specified by a single goal image provided after training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#25237;&#24433;&#22836;&#65292;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#25214;&#21040;&#20102;&#20004;&#20010;&#20851;&#38190;&#25928;&#24212;&#65306;&#20449;&#21495;&#26041;&#21521;&#30340;&#25193;&#23637;&#21644;&#25910;&#32553;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#32447;&#24615;&#21464;&#25442;&#26469;&#25913;&#21892;&#19979;&#28216;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03335</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#25237;&#24433;&#22836;&#65306;&#25193;&#23637;&#21644;&#25910;&#32553;&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
Unraveling Projection Heads in Contrastive Learning: Insights from Expansion and Shrinkage. (arXiv:2306.03335v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#25237;&#24433;&#22836;&#65292;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#25214;&#21040;&#20102;&#20004;&#20010;&#20851;&#38190;&#25928;&#24212;&#65306;&#20449;&#21495;&#26041;&#21521;&#30340;&#25193;&#23637;&#21644;&#25910;&#32553;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#32447;&#24615;&#21464;&#25442;&#26469;&#25913;&#21892;&#19979;&#28216;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#32534;&#30721;&#22120;-&#25237;&#24433;&#22120;&#26694;&#26550;&#65288;&#20363;&#22914;SimCLR&#65289;&#20013;&#30340;&#25237;&#24433;&#22836;&#65292;&#20063;&#31216;&#20026;&#25237;&#24433;&#20202;&#65292;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#26088;&#22312;&#25581;&#31034;&#19968;&#20010;&#35266;&#23519;&#29616;&#35937;&#30340;&#30495;&#30456;&#65306;&#36890;&#36807;&#19979;&#28216;&#32447;&#24615;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#34913;&#37327;&#65292;&#21363;&#20351;&#22312;&#25237;&#24433;&#22836;&#26412;&#36523;&#26159;&#32447;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#23398;&#20064;&#20986;&#22312;&#25237;&#24433;&#22120;&#20043;&#21069;&#30340;&#34920;&#31034;&#20248;&#20110;&#20043;&#21518;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#20004;&#20010;&#30001;&#23545;&#27604;&#25439;&#22833;&#24341;&#36215;&#30340;&#20851;&#38190;&#25928;&#24212;&#12290;&#26412;&#36136;&#19978;&#65292;&#23545;&#27604;&#25439;&#22833;&#20250;&#25193;&#23637;&#25110;&#25910;&#32553;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#30340;&#20449;&#21495;&#26041;&#21521;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#22914;&#22686;&#24378;&#24378;&#24230;&#65292;&#23545;&#27604;&#25439;&#22833;&#20013;&#20351;&#29992;&#30340;&#28201;&#24230;&#31561;&#22240;&#32032;&#12290;&#20854;&#27425;&#65292;&#21463;&#21040;&#25193;&#23637;&#21644;&#25910;&#32553;&#29616;&#35937;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#32447;&#24615;&#21464;&#25442;&#26469;&#20934;&#30830;&#24314;&#27169;&#25237;&#24433;&#22836;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21464;&#25442;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#32780;&#19988;&#35745;&#31639;&#25104;&#26412;&#20302;&#65292;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the role of projection heads, also known as projectors, within the encoder-projector framework (e.g., SimCLR) used in contrastive learning. We aim to demystify the observed phenomenon where representations learned before projectors outperform those learned after -- measured using the downstream linear classification accuracy, even when the projectors themselves are linear.  In this paper, we make two significant contributions towards this aim. Firstly, through empirical and theoretical analysis, we identify two crucial effects -- expansion and shrinkage -- induced by the contrastive loss on the projectors. In essence, contrastive loss either expands or shrinks the signal direction in the representations learned by an encoder, depending on factors such as the augmentation strength, the temperature used in contrastive loss, etc. Secondly, drawing inspiration from the expansion and shrinkage phenomenon, we propose a family of linear transformations to accurately model the p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#39640;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#20284;&#28982;&#27169;&#22411;&#65292;&#20197;&#38450;&#24481;&#25915;&#20987;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03331</link><description>&lt;p&gt;
&#19968;&#31181;&#40065;&#26834;&#24615;&#39640;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#20284;&#28982;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Robust Likelihood Model for Novelty Detection. (arXiv:2306.03331v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#39640;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#20284;&#28982;&#27169;&#22411;&#65292;&#20197;&#38450;&#24481;&#25915;&#20987;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26032;&#39062;&#24615;&#25110;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#25928;&#65292;&#31070;&#32463;&#32593;&#32476;&#20063;&#23481;&#26131;&#21463;&#21040;&#36755;&#20837;&#25968;&#25454;&#24494;&#23567;&#21464;&#24418;&#30340;&#24433;&#21709;&#12290;&#36825;&#22312;&#20851;&#38190;&#24212;&#29992;&#25110;&#25968;&#25454;&#34987;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#21464;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#20005;&#37325;&#38382;&#39064;&#12290;&#30446;&#21069;&#26377;&#19968;&#20123;&#38024;&#23545;&#30417;&#30563;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#22312;&#26032;&#39062;&#24615;&#26816;&#27979;&#20013;&#25509;&#21463;&#30340;&#20851;&#27880;&#24456;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20808;&#39564;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#40065;&#26834;&#30340;&#20284;&#28982;&#24230;&#65292;&#20197;&#38450;&#24481;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#23558;&#30456;&#21516;&#30340;&#20808;&#39564;&#19982;&#19968;&#31181;&#20808;&#36827;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#30001;&#20110;&#35813;&#26041;&#27861;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#25152;&#24471;&#21040;&#30340;&#40065;&#26834;&#24615;&#35757;&#32451;&#38750;&#24120;&#26377;&#25928;&#12290;&#21021;&#27493;&#30340;&#29702;&#35770;&#20998;&#26512;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#22312;&#20248;&#21183;&#65292;&#38543;&#21518;&#24471;&#21040;&#20102;&#23454;&#39564;&#32467;&#26524;&#30340;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to novelty or anomaly detection are based on deep neural networks. Despite their effectiveness, neural networks are also vulnerable to imperceptible deformations of the input data. This is a serious issue in critical applications, or when data alterations are generated by an adversarial attack. While this is a known problem that has been studied in recent years for the case of supervised learning, the case of novelty detection has received very limited attention. Indeed, in this latter setting the learning is typically unsupervised because outlier data is not available during training, and new approaches for this case need to be investigated. We propose a new prior that aims at learning a robust likelihood for the novelty test, as a defense against attacks. We also integrate the same prior with a state-of-the-art novelty detection approach. Because of the geometric properties of that approach, the resulting robust training is computationally very efficient. An initia
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;VHH&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#27979;&#25239;&#21407;-&#25239;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;VHHs&#30340;&#32467;&#26500;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;573,891&#20010;&#25239;&#21407;-VHH&#23545;&#65292;&#24182;&#19988;&#26159;&#24403;&#21069;&#20844;&#24320;&#25968;&#25454;&#38598;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2306.03329</link><description>&lt;p&gt;
AVIDa-hIL6&#65306;&#22522;&#20110;&#19968;&#21482;&#34987;&#20813;&#30123;&#32650;&#39548;&#30340;&#22823;&#35268;&#27169;VHH&#25968;&#25454;&#38598;&#29992;&#20110;&#39044;&#27979;&#25239;&#21407; - &#25239;&#20307;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
AVIDa-hIL6: A Large-Scale VHH Dataset Produced from an Immunized Alpaca for Predicting Antigen-Antibody Interactions. (arXiv:2306.03329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03329
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;VHH&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#27979;&#25239;&#21407;-&#25239;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;VHHs&#30340;&#32467;&#26500;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;573,891&#20010;&#25239;&#21407;-VHH&#23545;&#65292;&#24182;&#19988;&#26159;&#24403;&#21069;&#20844;&#24320;&#25968;&#25454;&#38598;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#24050;&#32463;&#25104;&#20026;&#27835;&#30103;&#20154;&#31867;&#30142;&#30149;&#30340;&#37325;&#35201;&#33647;&#29289;&#12290;&#20026;&#20102;&#21152;&#36895;&#27835;&#30103;&#25239;&#20307;&#30340;&#21457;&#29616;&#65292;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#25239;&#20307;&#20505;&#36873;&#21644;&#30446;&#26631;&#25239;&#21407;&#65288;&#22914;&#30149;&#27602;&#21644;&#32454;&#33740;&#65289;&#20043;&#38388;&#30340;&#29305;&#23450;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#26126;&#26174;&#30340;&#38480;&#21046;&#65292;&#22914;&#35268;&#27169;&#23567;&#65292;&#32570;&#20047;&#38750;&#32467;&#21512;&#26679;&#26412;&#21644;&#20934;&#30830;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AVIDa-hIL6, &#19968;&#20010;&#22823;&#35268;&#27169;&#30340;VHH&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#27979;&#20855;&#26377;&#20154;&#31867;&#30333;&#32454;&#32990;&#20171;&#32032;-6&#65288;IL-6&#65289;&#34507;&#30333;&#65292;&#20316;&#20026;&#25239;&#21407;&#30340;VHHs&#30340;&#25239;&#21407; - &#25239;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;VHHs&#30340;&#31616;&#21333;&#32467;&#26500;&#65292;&#26377;&#21033;&#20110;&#36890;&#36807;DNA&#27979;&#24207;&#25216;&#26415;&#35782;&#21035;&#20840;&#38271;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;AVIDa-hIL6&#21253;&#21547;573,891&#20010;&#25239;&#21407; - VHH&#23545;&#65292;&#20854;&#20013;62,067&#23545;&#26159;&#32467;&#21512;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies have become an important class of therapeutic agents to treat human diseases. To accelerate therapeutic antibody discovery, computational methods, especially machine learning, have attracted considerable interest for predicting specific interactions between antibody candidates and target antigens such as viruses and bacteria. However, the publicly available datasets in existing works have notable limitations, such as small sizes and the lack of non-binding samples and exact amino acid sequences. To overcome these limitations, we have developed AVIDa-hIL6, a large-scale dataset for predicting antigen-antibody interactions in the variable domain of heavy chain of heavy chain antibodies (VHHs), produced from an alpaca immunized with the human interleukin-6 (IL-6) protein, as antigens. By leveraging the simple structure of VHHs, which facilitates identification of full-length amino acid sequences by DNA sequencing technology, AVIDa-hIL6 contains 573,891 antigen-VHH pairs with am
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#32593;&#32476;&#30340;&#38543;&#26426;&#22810;&#23618;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#26032;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#23454;&#29616;&#29420;&#31435;&#20110;&#23618;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#29702;&#35770;&#32467;&#26524;&#21644;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.03322</link><description>&lt;p&gt;
&#38754;&#21521;&#32593;&#32476;&#30340;&#38543;&#26426;&#22810;&#23618;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#29420;&#31435;&#20110;&#23618;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Stochastic Multi-Level Compositional Optimization Algorithms over Networks with Level-Independent Convergence Rate. (arXiv:2306.03322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#32593;&#32476;&#30340;&#38543;&#26426;&#22810;&#23618;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#26032;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#23454;&#29616;&#29420;&#31435;&#20110;&#23618;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#29702;&#35770;&#32467;&#26524;&#21644;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22810;&#23618;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#28085;&#30422;&#20102;&#24456;&#22810;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#22914;&#22810;&#27493;&#39588;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#38543;&#26426;&#22810;&#23618;&#20248;&#21270;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22810;&#23618;&#32467;&#26500;&#21644;&#20998;&#25955;&#24335;&#36890;&#35759;&#26041;&#26696;&#21487;&#33021;&#22686;&#21152;&#23618;&#25968;&#23545;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#26469;&#22788;&#29702;&#22810;&#23618;&#20989;&#25968;&#21644;&#20854;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#21333;&#26426;&#31639;&#27861;&#30456;&#27604;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#22343;&#33021;&#22312;&#38750;&#20984;&#38382;&#39064;&#20013;&#23454;&#29616;&#29420;&#31435;&#20110;&#23618;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#26465;&#20214;&#26356;&#21152;&#23485;&#26494;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#20998;&#25955;&#24335;&#35774;&#32622;&#19979;&#23454;&#29616;&#29420;&#31435;&#20110;&#23618;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic multi-level compositional optimization problems cover many new machine learning paradigms, e.g., multi-step model-agnostic meta-learning, which require efficient optimization algorithms for large-scale applications. This paper studies the decentralized stochastic multi-level optimization algorithm, which is challenging because the multi-level structure and decentralized communication scheme may make the number of levels affect the order of the convergence rate. To this end, we develop two novel decentralized optimization algorithms to deal with the multi-level function and its gradient. Our theoretical results show that both algorithms can achieve the level-independent convergence rate for nonconvex problems under much milder conditions compared with existing single-machine algorithms. To the best of our knowledge, this is the first work that achieves the level-independent convergence rate under the decentralized setting. Moreover, extensive experiments confirm the efficacy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#24490;&#29615;&#38382;&#39064;&#12289;&#23433;&#20840;&#39118;&#38505;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#31995;&#32479;&#35780;&#20272;&#20197;&#21450;&#36947;&#24503;&#32771;&#34385;&#31561;&#25361;&#25112;&#65292;&#25552;&#20379;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20248;&#21183;&#30340;&#26041;&#24335;&#65292;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03314</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#65306;&#21457;&#25381;&#26234;&#33021; LLM &#26234;&#33021;&#20307;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents. (arXiv:2306.03314v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#24490;&#29615;&#38382;&#39064;&#12289;&#23433;&#20840;&#39118;&#38505;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#31995;&#32479;&#35780;&#20272;&#20197;&#21450;&#36947;&#24503;&#32771;&#34385;&#31561;&#25361;&#25112;&#65292;&#25552;&#20379;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20248;&#21183;&#30340;&#26041;&#24335;&#65292;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#21327;&#20316;&#29615;&#22659;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#32452;&#20214;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#23646;&#24615;&#21644;&#35282;&#33394;&#65292;&#20849;&#21516;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#26356;&#21152;&#39640;&#25928;&#26377;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;Auto-GPT &#21644;BabyAGI &#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#8220;Gorilla&#8221;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#22806;&#37096; API &#38598;&#25104;&#21040; LLM&#20013;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35299;&#20915;&#20102;&#24490;&#29615;&#38382;&#39064;&#12289;&#23433;&#20840;&#39118;&#38505;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#31995;&#32479;&#35780;&#20272;&#20197;&#21450;&#36947;&#24503;&#32771;&#34385;&#31561;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#27861;&#24237;&#27169;&#25311;&#21644;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#31561;&#19981;&#21516;&#39046;&#22495;&#30340;&#24314;&#27169;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#35758;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#30410;&#22788;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20248;&#21183;&#30340;&#36884;&#24452;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the "Gorilla" model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an aven
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#20195;&#29702;&#20154;&#32676;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03311</link><description>&lt;p&gt;
&#20351;&#29992;&#20195;&#29702;&#20154;&#32676;&#20307;&#23398;&#20064;&#24207;&#21015;&#20219;&#21153;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning Embeddings for Sequential Tasks Using Population of Agents. (arXiv:2306.03311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#20195;&#29702;&#20154;&#32676;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#26679;&#30340;&#24819;&#27861;&#65306;&#22914;&#26524;&#35266;&#23519;&#19968;&#20010;&#20195;&#29702;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20943;&#23569;&#20102;&#25105;&#20204;&#20851;&#20110;&#20182;&#22312;&#21478;&#19968;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#37027;&#20040;&#20004;&#20010;&#20219;&#21153;&#23601;&#30456;&#20284;&#12290;&#25105;&#20204;&#30340;&#20449;&#24687;&#29702;&#35770;&#20934;&#21017;&#25429;&#25417;&#20102;&#36825;&#31181;&#30452;&#35273;&#65292;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#20195;&#29702;&#20154;&#32676;&#20307;&#26469;&#27979;&#37327;&#24207;&#21015;&#20915;&#31574;&#29615;&#22659;&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#38500;&#20102;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#20004;&#20010;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#37327;&#21270;&#27604;&#36739;&#65292;&#22522;&#20110;&#20219;&#21153;&#23884;&#20837;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65306;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an information-theoretic framework to learn fixed-dimensional embeddings for tasks in reinforcement learning. We leverage the idea that two tasks are similar to each other if observing an agent's performance on one task reduces our uncertainty about its performance on the other. This intuition is captured by our information-theoretic criterion which uses a diverse population of agents to measure similarity between tasks in sequential decision-making settings. In addition to qualitative assessment, we empirically demonstrate the effectiveness of our techniques based on task embeddings by quantitative comparisons against strong baselines on two application scenarios: predicting an agent's performance on a test task by observing its performance on a small quiz of tasks, and selecting tasks with desired characteristics from a given set of options.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;</title><link>http://arxiv.org/abs/2306.03303</link><description>&lt;p&gt;
&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#21151;&#33021;&#24615;&#36755;&#20837;&#26144;&#23556;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#23450;&#20041;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#65292;&#20854;&#20540;&#20063;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#30340;&#36755;&#20986;&#31354;&#38388;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21152;&#24615;&#26063;&#20316;&#20026;&#38544;&#34255;&#23618;&#26144;&#23556;&#65292;&#20197;&#21450;&#19968;&#20010;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#24212;&#29992;&#20110;&#27599;&#20010;&#38544;&#34255;&#23618;&#12290;&#20381;&#38752;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#30340;Stone-Weierstrass&#23450;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#65292;&#36229;&#36234;&#20102;&#24120;&#35268;&#32039;&#38598;&#36924;&#36817;&#12290;&#36825;&#29305;&#21035;&#36866;&#29992;&#20110;&#36890;&#36807;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65288;&#38750;&#20808;&#35265;&#20043;&#26126;&#30340;&#65289;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#12290;&#20316;&#20026;&#24102;&#26435;Stone-Weierstrass&#23450;&#29702;&#30340;&#36827;&#19968;&#27493;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#24341;&#20837;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#31614;&#21517;&#20869;&#26680;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26159;&#26576;&#20123;&#39640;&#26031;&#36807;&#31243;&#30340;Cameron-Martin&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gauss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#29305;&#24449;&#19982;&#21709;&#24212;&#21464;&#37327;&#30340;&#20114;&#20449;&#24687;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#24182;&#35774;&#35745;&#20102;&#20272;&#35745;&#20114;&#20449;&#24687;&#30340;&#21028;&#21035;&#24335;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#22810;&#39033;&#25913;&#36827;&#25514;&#26045;&#20197;&#24212;&#23545;&#26356;&#22810;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.03301</link><description>&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#20013;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating Conditional Mutual Information for Dynamic Feature Selection. (arXiv:2306.03301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#29305;&#24449;&#19982;&#21709;&#24212;&#21464;&#37327;&#30340;&#20114;&#20449;&#24687;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#24182;&#35774;&#35745;&#20102;&#20272;&#35745;&#20114;&#20449;&#24687;&#30340;&#21028;&#21035;&#24335;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#22810;&#39033;&#25913;&#36827;&#25514;&#26045;&#20197;&#24212;&#23545;&#26356;&#22810;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#23427;&#36890;&#36807;&#39034;&#24207;&#26597;&#35810;&#29305;&#24449;&#20197;&#22312;&#26368;&#23567;&#30340;&#39044;&#31639;&#20869;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#20197;&#20943;&#23569;&#29305;&#24449;&#33719;&#21462;&#25104;&#26412;&#65292;&#24182;&#20026;&#39044;&#27979;&#36807;&#31243;&#25552;&#20379;&#36879;&#26126;&#24230;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#20351;&#29992;&#20219;&#24847;&#29305;&#24449;&#38598;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#23398;&#20064;&#31574;&#30053;&#20197;&#30830;&#23450;&#26368;&#26377;&#20215;&#20540;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26681;&#25454;&#29305;&#24449;&#19982;&#21709;&#24212;&#21464;&#37327;&#30340;&#20114;&#20449;&#24687;&#23545;&#29305;&#24449;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#12290;&#20854;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#23398;&#20064;&#27492;&#36873;&#25321;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30452;&#25509;&#26032;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#21028;&#21035;&#32780;&#38750;&#29983;&#25104;&#27169;&#24335;&#20272;&#35745;&#20114;&#20449;&#24687;&#12290;&#24314;&#31435;&#22312;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#20043;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#65306;&#20801;&#35768;&#22312;&#26679;&#26412;&#20043;&#38388;&#36827;&#34892;&#21487;&#21464;&#30340;&#29305;&#24449;&#39044;&#31639;&#12289;&#25903;&#25345;&#19981;&#21516;&#29305;&#24449;&#20043;&#38388;&#30340;&#38750;&#22343;&#21248;&#25104;&#26412;&#12289;&#32467;&#21512;&#20808;&#21069;&#30340;&#20449;&#24687;&#21644;&#25506;&#31350;&#29616;&#20195;&#26550;&#26500;&#20197;&#22788;&#29702;&#37096;&#20998;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic feature selection, where we sequentially query features to make accurate predictions with a minimal budget, is a promising paradigm to reduce feature acquisition costs and provide transparency into the prediction process. The problem is challenging, however, as it requires both making predictions with arbitrary feature sets and learning a policy to identify the most valuable selections. Here, we take an information-theoretic perspective and prioritize features based on their mutual information with the response variable. The main challenge is learning this selection policy, and we design a straightforward new modeling approach that estimates the mutual information in a discriminative rather than generative fashion. Building on our learning approach, we introduce several further improvements: allowing variable feature budgets across samples, enabling non-uniform costs between features, incorporating prior information, and exploring modern architectures to handle partial input in
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#25442;&#33258;&#22238;&#24402;&#20302;&#31209;&#24352;&#37327;&#65288;SALT&#65289;&#27169;&#22411;&#65292;&#23427;&#23558;&#33258;&#22238;&#24402;&#38544;Markov&#27169;&#22411;&#65288;ARHMM&#65289;&#21644;&#20999;&#25442;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;SLDS&#65289;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20302;&#31209;&#21442;&#25968;&#21270;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03291</link><description>&lt;p&gt;
&#20999;&#25442;&#33258;&#22238;&#24402;&#20302;&#31209;&#24352;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Switching Autoregressive Low-rank Tensor Models. (arXiv:2306.03291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#25442;&#33258;&#22238;&#24402;&#20302;&#31209;&#24352;&#37327;&#65288;SALT&#65289;&#27169;&#22411;&#65292;&#23427;&#23558;&#33258;&#22238;&#24402;&#38544;Markov&#27169;&#22411;&#65288;ARHMM&#65289;&#21644;&#20999;&#25442;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;SLDS&#65289;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20302;&#31209;&#21442;&#25968;&#21270;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24207;&#20998;&#26512;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#23545;&#20855;&#26377;&#26102;&#21464;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#20849;&#21516;&#36830;&#32493;&#21644;&#31163;&#25955;&#28508;&#24577;&#30340;&#27010;&#29575;&#27169;&#22411;&#20026;&#36825;&#26679;&#30340;&#25968;&#25454;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#12289;&#39640;&#25928;&#21644;&#23454;&#39564;&#24615;&#26377;&#29992;&#30340;&#25551;&#36848;&#12290;&#24120;&#29992;&#30340;&#27169;&#22411;&#21253;&#25324;&#33258;&#22238;&#24402;&#38544;Markov&#27169;&#22411;&#65288;ARHMM&#65289;&#21644;&#20999;&#25442;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;SLDS&#65289;&#65292;&#23427;&#20204;&#21508;&#26377;&#20248;&#32570;&#28857;&#12290;ARHMM&#20801;&#35768;&#31934;&#30830;&#25512;&#29702;&#21644;&#31616;&#21333;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#20294;&#22312;&#23545;&#38271;&#20381;&#36182;&#20851;&#31995;&#24314;&#27169;&#26102;&#20855;&#26377;&#21442;&#25968;&#23494;&#38598;&#24615;&#65292;&#22240;&#27492;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#28508;&#24577;&#21160;&#21147;&#23398;&#65292;SLDS&#21487;&#20197;&#20197;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#24335;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#65292;&#20294;&#22256;&#38590;&#30340;&#21442;&#25968;&#20272;&#35745;&#20219;&#21153;&#21644;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#21364;&#26159;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#26041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20999;&#25442;&#33258;&#22238;&#24402;&#20302;&#31209;&#24352;&#37327;&#65288;SALT&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20445;&#30041;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;SALT&#23558;ARHMM&#30340;&#24352;&#37327;&#21442;&#25968;&#21270;&#20026;&#20302;&#31209;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important problem in time-series analysis is modeling systems with time-varying dynamics. Probabilistic models with joint continuous and discrete latent states offer interpretable, efficient, and experimentally useful descriptions of such data. Commonly used models include autoregressive hidden Markov models (ARHMMs) and switching linear dynamical systems (SLDSs), each with its own advantages and disadvantages. ARHMMs permit exact inference and easy parameter estimation, but are parameter intensive when modeling long dependencies, and hence are prone to overfitting. In contrast, SLDSs can capture long-range dependencies in a parameter efficient way through Markovian latent dynamics, but present an intractable likelihood and a challenging parameter estimation task. In this paper, we propose switching autoregressive low-rank tensor (SALT) models, which retain the advantages of both approaches while ameliorating the weaknesses. SALT parameterizes the tensor of an ARHMM with a low-rank 
&lt;/p&gt;</description></item><item><title>&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21363;&#20351;&#20351;&#29992;&#38169;&#35823;&#30340;&#22870;&#21169;&#26631;&#31614;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#65292;&#36825;&#31181;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#30001;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24754;&#35266;&#20027;&#20041;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#30340;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#65292;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03286</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29983;&#23384;&#26412;&#33021;
&lt;/p&gt;
&lt;p&gt;
Survival Instinct in Offline Reinforcement Learning. (arXiv:2306.03286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03286
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21363;&#20351;&#20351;&#29992;&#38169;&#35823;&#30340;&#22870;&#21169;&#26631;&#31614;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#65292;&#36825;&#31181;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#30001;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24754;&#35266;&#20027;&#20041;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#30340;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#65292;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26032;&#35266;&#23519;&#65306;&#22312;&#35768;&#22810;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21363;&#20351;&#20351;&#29992;&#8220;&#38169;&#35823;&#8221;&#30340;&#22870;&#21169;&#26631;&#31614;&#65288;&#20363;&#22914;&#22312;&#25152;&#26377;&#22320;&#26041;&#37117;&#20026;&#38646;&#25110;&#26159;&#30495;&#23454;&#22870;&#21169;&#30340;&#36127;&#25968;&#65289;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#12290;&#36825;&#31181;&#29616;&#35937;&#19981;&#33021;&#20165;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#26469;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#23427;&#36171;&#20104;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#22312;&#20854;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#23545;&#24212;&#29289;&#20013;&#26159;&#19981;&#20856;&#22411;&#30340;&#65292;&#22240;&#20026;&#21518;&#32773;&#23545;&#22870;&#21169;&#35774;&#35745;&#25935;&#24863;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#24778;&#20154;&#30340;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#24754;&#35266;&#20027;&#20041;&#27010;&#24565;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#26576;&#31181;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#12290;&#24754;&#35266;&#20027;&#20041;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#65292;&#21363;&#38271;&#26399;&#20869;&#30041;&#22312;&#25968;&#25454;&#25903;&#25345;&#20013;&#30340;&#28608;&#21169;&#65292;&#32780;&#26377;&#38480;&#19988;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#35206;&#30422;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#29983;&#23384;&#34892;&#20026;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with "wrong" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of pessimism in offline RL algorithms and a certain bias implicit in common data collection practices. As we prove in this work, pessimism endows the agent with a "survival instinct", i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;MRI&#21387;&#32553;&#24863;&#30693;&#37319;&#26679;&#27169;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#21482;&#29992;&#20116;&#24352;&#35757;&#32451;&#22270;&#20687;&#26469;&#23398;&#20064;&#26377;&#25928;&#30340;&#37319;&#26679;&#27169;&#24335;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35299;&#21078;&#32467;&#26500;&#12289;&#21152;&#36895;&#22240;&#23376;&#21644;&#27169;&#24335;&#31867;&#22411;&#19979;&#33719;&#24471;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03284</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;MRI&#21387;&#32553;&#24863;&#30693;&#37319;&#26679;&#27169;&#24335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimizing Sampling Patterns for Compressed Sensing MRI with Diffusion Generative Models. (arXiv:2306.03284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;MRI&#21387;&#32553;&#24863;&#30693;&#37319;&#26679;&#27169;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#21482;&#29992;&#20116;&#24352;&#35757;&#32451;&#22270;&#20687;&#26469;&#23398;&#20064;&#26377;&#25928;&#30340;&#37319;&#26679;&#27169;&#24335;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35299;&#21078;&#32467;&#26500;&#12289;&#21152;&#36895;&#22240;&#23376;&#21644;&#27169;&#24335;&#31867;&#22411;&#19979;&#33719;&#24471;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#24050;&#34987;&#29992;&#20316;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#37325;&#24314;&#30340;&#24378;&#22823;&#20808;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#26469;&#20248;&#21270;&#21387;&#32553;&#24863;&#30693;&#22810;&#32447;&#22280;MRI&#30340;&#23376;&#37319;&#26679;&#27169;&#24335;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#21644;MRI&#27979;&#37327;&#36807;&#31243;&#30340;&#21518;&#39564;&#24179;&#22343;&#20272;&#35745;&#30340;&#21333;&#27493;&#37325;&#24314;&#12290;&#22312;&#19981;&#21516;&#35299;&#21078;&#32467;&#26500;&#12289;&#21152;&#36895;&#22240;&#23376;&#21644;&#27169;&#24335;&#31867;&#22411;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#37319;&#26679;&#36816;&#31639;&#31526;&#27604;&#22522;&#32447;&#27169;&#24335;&#20855;&#26377;&#31454;&#20105;&#24615;&#65292;&#32780;&#22312;2D&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#37325;&#24314;&#25928;&#26524;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#20116;&#20010;&#35757;&#32451;&#22270;&#20687;&#23601;&#21487;&#20197;&#23398;&#20064;&#21040;&#26377;&#25928;&#30340;&#37319;&#26679;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have been used as powerful priors for magnetic resonance imaging (MRI) reconstruction. We present a learning method to optimize sub-sampling patterns for compressed sensing multi-coil MRI that leverages pre-trained diffusion generative models. Crucially, during training we use a single-step reconstruction based on the posterior mean estimate given by the diffusion model and the MRI measurement process. Experiments across varying anatomies, acceleration factors, and pattern types show that sampling operators learned with our method lead to competitive, and in the case of 2D patterns, improved reconstructions compared to baseline patterns. Our method requires as few as five training images to learn effective sampling patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#20102;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#21644;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;&#30340;&#26410;&#35745;&#25968;&#24352;&#37327;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#26465;&#30446;&#20013;&#24674;&#22797;&#23436;&#20840;&#35745;&#25968;&#26465;&#30446;&#21644;&#27599;&#20010;&#26465;&#30446;&#30340;&#26410;&#35745;&#25968;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.03273</link><description>&lt;p&gt;
&#20855;&#26377;&#23646;&#24615;&#31070;&#32463;&#34701;&#21512;&#30340;&#26410;&#35745;&#20837;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Under-Counted Tensor Completion with Neural Incorporation of Attributes. (arXiv:2306.03273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#20102;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#21644;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;&#30340;&#26410;&#35745;&#25968;&#24352;&#37327;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#26465;&#30446;&#20013;&#24674;&#22797;&#23436;&#20840;&#35745;&#25968;&#26465;&#30446;&#21644;&#27599;&#20010;&#26465;&#30446;&#30340;&#26410;&#35745;&#25968;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#25910;&#38598;&#30340;&#25968;&#25454;&#37117;&#23384;&#22312;&#31995;&#32479;&#24615;&#30340;&#26410;&#35745;&#25968;&#25928;&#24212;&#65292;&#20363;&#22914;&#27969;&#34892;&#30149;&#23398;&#21644;&#29983;&#24577;&#23398;&#12290;&#26410;&#35745;&#25968;&#24352;&#37327;&#34917;&#20840;&#65288;UC-TC&#65289;&#23545;&#20110;&#35768;&#22810;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20855;&#26377;&#24456;&#22909;&#30340;&#21160;&#26426;&#65292;&#20363;&#22914;&#20174;&#30456;&#37051;&#21306;&#22495;&#30340;&#26410;&#35745;&#25968;&#30149;&#20363;&#25968;&#23383;&#25512;&#26029;&#26410;&#35266;&#23519;&#21040;&#30340;&#22320;&#28857;&#30340;&#20256;&#26579;&#30149;&#30149;&#20363;&#25968;&#23383;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#31209;&#27850;&#26494;&#24352;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#34920;&#36798;&#20016;&#23500;&#30340;&#26410;&#30693;&#38750;&#32447;&#24615;&#20391;&#38754;&#20449;&#24687;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#26410;&#35745;&#25968;&#30340;&#22810;&#26041;&#38754;&#25968;&#25454;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#32852;&#21512;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#21644;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;&#26469;&#22238;&#22797;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;UC-TC&#20844;&#24335;&#24471;&#21040;&#20102;&#29702;&#35770;&#20998;&#26512;&#30340;&#25903;&#25345;&#65292;&#21487;&#20197;&#35777;&#26126;&#24352;&#37327;&#30340;&#23436;&#20840;&#35745;&#25968;&#26465;&#30446;&#21644;&#27599;&#20010;&#26465;&#30446;&#30340;&#26410;&#35745;&#25968;&#27010;&#29575;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#26465;&#30446;&#20013;&#34987;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systematic under-counting effects are observed in data collected across many disciplines, e.g., epidemiology and ecology. Under-counted tensor completion (UC-TC) is well-motivated for many data analytics tasks, e.g., inferring the case numbers of infectious diseases at unobserved locations from under-counted case numbers in neighboring regions. However, existing methods for similar problems often lack supports in theory, making it hard to understand the underlying principles and conditions beyond empirical successes. In this work, a low-rank Poisson tensor model with an expressive unknown nonlinear side information extractor is proposed for under-counted multi-aspect data. A joint low-rank tensor completion and neural network learning algorithm is designed to recover the model. Moreover, the UC-TC formulation is supported by theoretical analysis showing that the fully counted entries of the tensor and each entry's under-counting probability can be provably recovered from partial observ
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20855;&#26377;&#32479;&#35745;&#23398;&#20005;&#35880;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#26088;&#22312;&#21306;&#20998;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;rBT&#21644;RN&#65292;&#24182;&#20197;&#27492;&#26469;&#39044;&#27979;GBM&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#12290;&#36825;&#23545;&#20419;&#36827;&#24739;&#32773;&#26089;&#26399;&#25509;&#21463;&#27835;&#30103;&#21644;&#33719;&#24471;&#26356;&#22909;&#30340;&#27835;&#30103;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.03270</link><description>&lt;p&gt;
&#22823;&#33041;&#32959;&#30244;&#22797;&#21457;&#19982;&#25918;&#23556;&#24615;&#22351;&#27515;&#37492;&#21035;&#20197;&#21450;&#24739;&#32773;&#29983;&#23384;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Brain Tumor Recurrence vs. Radiation Necrosis Classification and Patient Survivability Prediction. (arXiv:2306.03270v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20855;&#26377;&#32479;&#35745;&#23398;&#20005;&#35880;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#26088;&#22312;&#21306;&#20998;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;rBT&#21644;RN&#65292;&#24182;&#20197;&#27492;&#26469;&#39044;&#27979;GBM&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#12290;&#36825;&#23545;&#20419;&#36827;&#24739;&#32773;&#26089;&#26399;&#25509;&#21463;&#27835;&#30103;&#21644;&#33719;&#24471;&#26356;&#22909;&#30340;&#27835;&#30103;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#20154;&#26368;&#20982;&#38505;&#30340;&#33041;&#32959;&#30244;&#26159;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#65288;GBM&#65289;&#65292;&#21363;&#20351;&#32463;&#36807;&#25163;&#26415;&#21644;&#25918;&#30103;&#27835;&#30103;&#21518;&#19988;&#31215;&#26497;&#27835;&#30103;&#65292;&#20854;&#30701;&#26399;&#29983;&#23384;&#29575;&#20063;&#24456;&#20302;&#12290;GBM&#24739;&#32773;&#25509;&#21463;&#25918;&#30103;&#21518;&#65292;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#30340;&#21464;&#21270;&#34920;&#26126;&#20986;&#29616;&#25918;&#23556;&#24615;&#22351;&#27515;&#65288;RN&#65289;&#25110;&#33041;&#32959;&#30244;&#22797;&#21457;&#65288;rBT&#65289;&#12290;&#22312;&#26089;&#26399;&#31579;&#36873;rBT&#21644;RN&#23545;&#20110;&#20419;&#36827;&#24739;&#32773;&#26356;&#24555;&#22320;&#25509;&#21463;&#27835;&#30103;&#21644;&#33719;&#24471;&#26356;&#22909;&#30340;&#27835;&#30103;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#21306;&#20998;rBT&#21644;RN&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20004;&#32773;&#22312;MRI&#19978;&#21487;&#33021;&#21576;&#29616;&#20986;&#31867;&#20284;&#30340;&#25918;&#23556;&#24615;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;MRI&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;rBT&#19982;RN&#20998;&#31867;&#21487;&#33021;&#20250;&#22240;&#32570;&#20047;&#24739;&#32773;&#25968;&#25454;&#32780;&#23548;&#33268;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;&#34394;&#25311;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#20294;&#20250;&#23548;&#33268;&#21512;&#25104;&#25110;&#22686;&#24191;&#25968;&#25454;&#20013;&#30340;&#22522;&#30784;&#25968;&#25454;&#34920;&#31034;&#19981;&#21516;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20855;&#26377;&#32479;&#35745;&#23398;&#20005;&#35880;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
GBM (Glioblastoma multiforme) is the most aggressive type of brain tumor in adults that has a short survival rate even after aggressive treatment with surgery and radiation therapy. The changes on magnetic resonance imaging (MRI) for patients with GBM after radiotherapy are indicative of either radiation-induced necrosis (RN) or recurrent brain tumor (rBT). Screening for rBT and RN at an early stage is crucial for facilitating faster treatment and better outcomes for the patients. Differentiating rBT from RN is challenging as both may present with similar radiological and clinical characteristics on MRI. Moreover, learning-based rBT versus RN classification using MRI may suffer from class imbalance due to lack of patient data. While synthetic data generation using generative models has shown promise to address class imbalance, the underlying data representation may be different in synthetic or augmented data. This study proposes computational modeling with statistically rigorous repeat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$(k, t)$-FWL&#21644;$k$-FWL+&#20004;&#31181;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#22312;$O(n^2)$&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#19979;&#65292;&#35299;&#20915;&#22270;&#21516;&#26500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03266</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#27665;&#38388;&#23041;&#26031;&#36153;&#21202;-&#33713;&#26364;&#31639;&#27861;&#65292;&#23454;&#29616;$O(n^2)$&#31354;&#38388;&#20869;&#20219;&#24847;&#34920;&#36798;&#33021;&#21147;&#30340;GNNs
&lt;/p&gt;
&lt;p&gt;
Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking Folklore Weisfeiler-Lehman. (arXiv:2306.03266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$(k, t)$-FWL&#21644;$k$-FWL+&#20004;&#31181;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#22312;$O(n^2)$&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#19979;&#65292;&#35299;&#20915;&#22270;&#21516;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#24050;&#25104;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20854;&#34920;&#36798;&#33021;&#21147;&#21463;&#21040;&#19968;&#32500;&#23041;&#26031;&#36153;&#21202;-&#33713;&#26364;&#65288;1-WL&#65289;&#27979;&#35797;&#30340;&#38480;&#21046;&#12290;&#19968;&#20123;&#30740;&#31350;&#21463;&#21040;$k$-WL/FWL&#65288;&#27665;&#38388;WL&#65289;&#30340;&#21551;&#21457;&#24182;&#35774;&#35745;&#20854;&#30456;&#24212;&#30340;&#31070;&#32463;&#29256;&#26412;&#12290;&#23613;&#31649;&#20855;&#26377;&#24456;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#23384;&#22312;&#20005;&#37325;&#23616;&#38480;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;$(k, t)$-FWL&#21644;$k$-FWL+&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by $k$-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) $k$-WL/FWL requires at least $O(n^k)$ space complexity, which is impractical for large graphs even when $k=3$; (2) The design space of $k$-WL/FWL is rigid, with the only adjustable hyper-parameter being $k$. To tackle the first limitation, we propose an extension, $(k, t)$-FWL. We theoretically prove that even if we fix the space complexity to $O(n^2)$ in $(k, t)$-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose $k$-FWL+, which considers any equivariant set as neighbors ins
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102; NeurIPS 2021 &#19968;&#33268;&#24615;&#23454;&#39564;&#32467;&#26524;&#65292;&#35813;&#23454;&#39564;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#23457;&#31295;&#36807;&#31243;&#20013;&#30340;&#38543;&#24847;&#24615;&#36739;&#39640;&#65292;&#20004;&#20010;&#22996;&#21592;&#20250;&#23545;&#20110; 23% &#30340;&#35770;&#25991;&#23384;&#22312;&#19981;&#21516;&#30340;&#25509;&#21463;/&#25298;&#32477;&#25512;&#33616;&#65292;&#22914;&#26524;&#37325;&#26032;&#38543;&#26426;&#36816;&#34892;&#23457;&#31295;&#36807;&#31243;&#65292;&#25509;&#21463;&#35770;&#25991;&#28165;&#21333;&#20013;&#32422;&#26377;&#19968;&#21322;&#30340;&#35770;&#25991;&#23558;&#20250;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24378;&#35843;&#23458;&#35266;&#34913;&#37327;&#30740;&#31350;&#36136;&#37327;&#30340;&#22256;&#38590;&#65292;&#24314;&#35758;&#20316;&#32773;&#19981;&#24212;&#22240;&#25298;&#32477;&#30340;&#20316;&#21697;&#32780;&#36807;&#20998;&#27822;&#20007;&#12290;</title><link>http://arxiv.org/abs/2306.03262</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#23457;&#31295;&#36807;&#31243;&#26159;&#21542;&#38543;&#30528;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#21464;&#24471;&#26356;&#21152;&#38543;&#24847;&#24615;&#65311;NeurIPS 2021&#19968;&#33268;&#24615;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Has the Machine Learning Review Process Become More Arbitrary as the Field Has Grown? The NeurIPS 2021 Consistency Experiment. (arXiv:2306.03262v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102; NeurIPS 2021 &#19968;&#33268;&#24615;&#23454;&#39564;&#32467;&#26524;&#65292;&#35813;&#23454;&#39564;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#23457;&#31295;&#36807;&#31243;&#20013;&#30340;&#38543;&#24847;&#24615;&#36739;&#39640;&#65292;&#20004;&#20010;&#22996;&#21592;&#20250;&#23545;&#20110; 23% &#30340;&#35770;&#25991;&#23384;&#22312;&#19981;&#21516;&#30340;&#25509;&#21463;/&#25298;&#32477;&#25512;&#33616;&#65292;&#22914;&#26524;&#37325;&#26032;&#38543;&#26426;&#36816;&#34892;&#23457;&#31295;&#36807;&#31243;&#65292;&#25509;&#21463;&#35770;&#25991;&#28165;&#21333;&#20013;&#32422;&#26377;&#19968;&#21322;&#30340;&#35770;&#25991;&#23558;&#20250;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24378;&#35843;&#23458;&#35266;&#34913;&#37327;&#30740;&#31350;&#36136;&#37327;&#30340;&#22256;&#38590;&#65292;&#24314;&#35758;&#20316;&#32773;&#19981;&#24212;&#22240;&#25298;&#32477;&#30340;&#20316;&#21697;&#32780;&#36807;&#20998;&#27822;&#20007;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NeurIPS 2021&#19968;&#33268;&#24615;&#23454;&#39564;&#65292;&#36825;&#26159;2014&#24180;NeurIPS&#23454;&#39564;&#30340;&#26356;&#22823;&#35268;&#27169;&#21464;&#20307;&#65292;&#22312;&#35813;&#23454;&#39564;&#20013;&#65292;10%&#30340;&#20250;&#35758;&#25237;&#31295;&#30001;&#20004;&#20010;&#29420;&#31435;&#22996;&#21592;&#20250;&#35780;&#23457;&#65292;&#20197;&#37327;&#21270;&#23457;&#31295;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20004;&#20010;&#22996;&#21592;&#20250;&#22312;23%&#30340;&#35770;&#25991;&#30340;&#25509;&#21463;/&#25298;&#32477;&#25512;&#33616;&#19978;&#23384;&#22312;&#20998;&#27495;&#65292;&#24182;&#19988;&#19982;2014&#24180;&#30340;&#32467;&#26524;&#19968;&#33268;&#65292;&#22914;&#26524;&#37325;&#26032;&#38543;&#26426;&#36816;&#34892;&#23457;&#31295;&#36807;&#31243;&#65292;&#25509;&#21463;&#35770;&#25991;&#28165;&#21333;&#20013;&#32422;&#26377;&#19968;&#21322;&#30340;&#35770;&#25991;&#23558;&#20250;&#25913;&#21464;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20351;&#20250;&#35758;&#26356;&#20855;&#36873;&#25321;&#24615;&#20250;&#22686;&#21152;&#36825;&#19968;&#36807;&#31243;&#30340;&#38543;&#24847;&#24615;&#12290;&#32467;&#21512;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#23458;&#35266;&#34913;&#37327;&#30740;&#31350;&#36136;&#37327;&#30340;&#22256;&#38590;&#65292;&#24182;&#34920;&#26126;&#20316;&#32773;&#19981;&#24212;&#22240;&#25298;&#32477;&#30340;&#20316;&#21697;&#32780;&#36807;&#20998;&#27822;&#20007;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the NeurIPS 2021 consistency experiment, a larger-scale variant of the 2014 NeurIPS experiment in which 10% of conference submissions were reviewed by two independent committees to quantify the randomness in the review process. We observe that the two committees disagree on their accept/reject recommendations for 23% of the papers and that, consistent with the results from 2014, approximately half of the list of accepted papers would change if the review process were randomly rerun. Our analysis suggests that making the conference more selective would increase the arbitrariness of the process. Taken together with previous research, our results highlight the inherent difficulty of objectively measuring the quality of research, and suggest that authors should not be excessively discouraged by rejected work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#39640;&#20934;&#30830;&#24230;&#21644;&#38544;&#31169;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03257</link><description>&lt;p&gt;
&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#31169;&#26377;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Private Synthetic Data with Genetic Algorithms. (arXiv:2306.03257v1 [cs.OH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#39640;&#20934;&#30830;&#24230;&#21644;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#25928;&#29983;&#25104;&#36817;&#20284;&#20110;&#22522;&#30784;&#25935;&#24863;&#25968;&#25454;&#38598;&#32479;&#35745;&#23646;&#24615;&#30340;&#24046;&#20998;&#31169;&#26377;&#21512;&#25104;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#26377;&#19968;&#31995;&#21015;&#24037;&#20316;&#21033;&#29992;&#19968;&#38454;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25216;&#26415;&#20165;&#38480;&#20110;&#20165;&#21487;&#24494;&#30446;&#26631;&#30340;&#20248;&#21270;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#21487;&#20197;&#36827;&#34892;&#30340;&#20998;&#26512;&#31867;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#38646;&#38454;&#20248;&#21270;&#21551;&#21457;&#24335;&#30340;&#31169;&#26377;&#36951;&#20256;&#31639;&#27861; Private-GSD&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#26063;&#32676;&#30340;&#25628;&#32034;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#23376;&#21551;&#21457;&#24335;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126; Private-GSD &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#23494;&#20999;&#36924;&#36817;&#22522;&#30784;&#25935;&#24863;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#24378;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of efficiently generating differentially private synthetic data that approximate the statistical properties of an underlying sensitive dataset. In recent years, there has been a growing line of work that approaches this problem using first-order optimization techniques. However, such techniques are restricted to optimizing differentiable objectives only, severely limiting the types of analyses that can be conducted. For example, first-order mechanisms have been primarily successful in approximating statistical queries only in the form of marginals for discrete data domains. In some cases, one can circumvent such issues by relaxing the task's objective to maintain differentiability. However, even when possible, these approaches impose a fundamental limitation in which modifications to the minimization problem become additional sources of error. Therefore, we propose Private-GSD, a private genetic algorithm based on zeroth-order optimization heuristics that do not re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#29305;&#24449;&#21644;&#36755;&#20986;&#26631;&#31614;&#20043;&#38388;&#30340;&#26465;&#20214;&#20559;&#31227;&#37327;&#65292;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#26131;&#21463;&#20998;&#24067;&#20559;&#31227;&#24433;&#21709;&#30340;&#38382;&#39064;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#24418;&#24322;&#36136;&#24615;&#21644;&#27169;&#22411;&#26550;&#26500;&#37117;&#20250;&#23548;&#33268;&#26465;&#20214;&#20559;&#31227;&#65292;&#24433;&#21709;&#24615;&#33021;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#20559;&#31227;&#30340;&#20272;&#35745;&#21644;&#26368;&#23567;&#21270;&#26469;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2306.03256</link><description>&lt;p&gt;
&#35299;&#37322;&#19982;&#35843;&#25972;&#22270;&#24418;&#26465;&#20214;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Explaining and Adapting Graph Conditional Shift. (arXiv:2306.03256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#29305;&#24449;&#21644;&#36755;&#20986;&#26631;&#31614;&#20043;&#38388;&#30340;&#26465;&#20214;&#20559;&#31227;&#37327;&#65292;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#26131;&#21463;&#20998;&#24067;&#20559;&#31227;&#24433;&#21709;&#30340;&#38382;&#39064;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#24418;&#24322;&#36136;&#24615;&#21644;&#27169;&#22411;&#26550;&#26500;&#37117;&#20250;&#23548;&#33268;&#26465;&#20214;&#20559;&#31227;&#65292;&#24433;&#21709;&#24615;&#33021;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#20559;&#31227;&#30340;&#20272;&#35745;&#21644;&#26368;&#23567;&#21270;&#26469;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;GNN&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#30340;&#24433;&#21709;&#12290;&#30446;&#21069;&#20851;&#20110;&#20026;&#20160;&#20040;&#22522;&#20110;&#22270;&#24418;&#30340;&#27169;&#22411;&#20284;&#20046;&#26356;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#20559;&#31227;&#24433;&#21709;&#30340;&#38382;&#39064;&#36824;&#23384;&#22312;&#26174;&#33879;&#30340;&#27495;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#29305;&#24449;&#21644;&#36755;&#20986;&#26631;&#31614;&#20043;&#38388;&#30340;&#26465;&#20214;&#20559;&#31227;&#37327;&#65292;&#23545;&#23427;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#24418;&#24322;&#36136;&#24615;&#21644;&#27169;&#22411;&#26550;&#26500;&#37117;&#21152;&#21095;&#20102;&#26465;&#20214;&#20559;&#31227;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#28041;&#21450;&#23545;&#22270;&#24418;&#19978;&#30340;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#24615;&#36827;&#34892;&#26465;&#20214;&#20559;&#31227;&#30340;&#20272;&#35745;&#21644;&#26368;&#23567;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#25511;&#21046;&#24615;&#32508;&#21512;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#23545;&#20998;&#24067;&#20559;&#31227;&#30340;&#40065;&#26834;&#24615;&#65292;&#30456;&#23545;&#31532;&#20108;&#20248;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#36798;10%&#30340;ROC AUC&#32477;&#23545;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#23545;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20840;&#38754;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown remarkable performance on graph-structured data. However, recent empirical studies suggest that GNNs are very susceptible to distribution shift. There is still significant ambiguity about why graph-based models seem more vulnerable to these shifts. In this work we provide a thorough theoretical analysis on it by quantifying the magnitude of conditional shift between the input features and the output label. Our findings show that both graph heterophily and model architecture exacerbate conditional shifts, leading to performance degradation. To address this, we propose an approach that involves estimating and minimizing the conditional shift for unsupervised domain adaptation on graphs. In our controlled synthetic experiments, our algorithm demonstrates robustness towards distribution shift, resulting in up to 10% absolute ROC AUC improvement versus the second-best algorithm. Furthermore, comprehensive experiments on both node classification and gr
&lt;/p&gt;</description></item><item><title>&#27010;&#29575;&#23637;&#24320;&#26159;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#26080;&#21453;&#28436;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23398;&#20064;&#28508;&#22312;&#39640;&#26031;&#27169;&#22411;&#26102;&#27604;&#20256;&#32479;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#27169;&#22411;&#24615;&#33021;&#25439;&#22833;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2306.03249</link><description>&lt;p&gt;
&#27010;&#29575;&#23637;&#24320;&#65306;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#12289;&#26080;&#21453;&#28436;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Unrolling: Scalable, Inverse-Free Maximum Likelihood Estimation for Latent Gaussian Models. (arXiv:2306.03249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03249
&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#23637;&#24320;&#26159;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#26080;&#21453;&#28436;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23398;&#20064;&#28508;&#22312;&#39640;&#26031;&#27169;&#22411;&#26102;&#27604;&#20256;&#32479;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#27169;&#22411;&#24615;&#33021;&#25439;&#22833;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#39640;&#26031;&#27169;&#22411;&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#22240;&#23376;&#20998;&#26512;&#21040;&#21387;&#32553;&#24863;&#30693;&#20877;&#21040;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#26368;&#22823;&#21270;&#36825;&#20123;&#27169;&#22411;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#20256;&#32479;&#26041;&#27861;&#26159;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#31639;&#27861;&#12290;&#23545;&#20110;&#20855;&#26377;&#39640;&#32500;&#28508;&#22312;&#21464;&#37327;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#38656;&#35201;&#27714;&#35299;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;&#19968;&#26679;&#22810;&#30340;&#22823;&#22411;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#36870;&#30697;&#38453;&#65292;&#22240;&#27492;EM&#30340;&#21487;&#25193;&#23637;&#24615;&#24456;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27010;&#29575;&#23637;&#24320;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#19982;&#36845;&#20195;&#32447;&#24615;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32469;&#36807;&#30697;&#38453;&#27714;&#36870;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#23637;&#24320;&#21644;&#21453;&#21521;&#20256;&#25773;&#36890;&#36807;&#27714;&#35299;&#22120;&#30340;&#36845;&#20195;&#21487;&#20197;&#21152;&#36895;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27010;&#29575;&#23637;&#24320;&#23398;&#20064;&#28508;&#22312;&#39640;&#26031;&#27169;&#22411;&#30340;&#36895;&#24230;&#27604;&#26799;&#24230;EM&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#27169;&#22411;&#24615;&#33021;&#30340;&#25439;&#22833;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Gaussian models have a rich history in statistics and machine learning, with applications ranging from factor analysis to compressed sensing to time series analysis. The classical method for maximizing the likelihood of these models is the expectation-maximization (EM) algorithm. For problems with high-dimensional latent variables and large datasets, EM scales poorly because it needs to invert as many large covariance matrices as the number of data points. We introduce probabilistic unrolling, a method that combines Monte Carlo sampling with iterative linear solvers to circumvent matrix inversion. Our theoretical analyses reveal that unrolling and backpropagation through the iterations of the solver can accelerate gradient estimation for maximum likelihood estimation. In experiments on simulated and real data, we demonstrate that probabilistic unrolling learns latent Gaussian models up to an order of magnitude faster than gradient EM, with minimal losses in model performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03241</link><description>&lt;p&gt;
&#29702;&#35299;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#23545;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models. (arXiv:2306.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#20215;&#39640;&#26114;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33267;&#25910;&#25947;&#24182;&#19981;&#39640;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24819;&#27861;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27839;&#30528;&#36712;&#36857;&#36827;&#34892;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#65292;&#20197;&#22312;&#27169;&#22411;&#25910;&#25947;&#20043;&#21069;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35757;&#32451;&#25110;&#25512;&#29702;&#26399;&#38388;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;10&#20159;&#21040;120&#20159;&#21442;&#25968;&#30340;Pythia LLM&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#24182;&#35777;&#26126;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#21644;&#20013;&#26399;&#38454;&#27573;&#65292;&#36825;&#31181;&#24819;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#24182;&#25552;&#39640;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#12290;&#25439;&#22833;&#27874;&#21160;&#26159;LLM&#35757;&#32451;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65307;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#20004;&#31181;&#22522;&#30784;&#36712;&#36857;&#30340;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#24179;&#22343;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#20004;&#31181;&#24773;&#20917;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#19968;&#20010;&#25317;&#26377;69&#20159;&#21442;&#25968;&#30340;LLM&#65292;&#25105;&#20204;&#30340;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#37197;&#26041;&#21487;&#20197;&#33410;&#30465;&#39640;&#36798;4200&#23567;&#26102;&#30340;GPU&#26102;&#38388;&#65292;&#36825;&#23545;&#20113;&#35745;&#31639;&#25104;&#26412;&#26469;&#35828;&#26159;&#26174;&#33879;&#30340;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training LLMs is expensive, and recent evidence indicates training all the way to convergence is inefficient. In this paper, we investigate the ability of a simple idea, checkpoint averaging along the trajectory of a training run to improve the quality of models before they have converged. This approach incurs no extra cost during training or inference. Specifically, we analyze the training trajectories of Pythia LLMs with 1 to 12 billion parameters and demonstrate that, particularly during the early to mid stages of training, this idea accelerates convergence and improves both test and zero-shot generalization. Loss spikes are a well recognized problem in LLM training; in our analysis we encountered two instances of this in the underlying trajectories, and both instances were mitigated by our averaging.  For a 6.9B parameter LLM, for example, our early weight averaging recipe can save upto 4200 hours of GPU time, which corresponds to significant savings in cloud compute costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#21387;&#32553;&#21644;&#37325;&#35201;&#24615;&#25277;&#26679;&#25552;&#39640;&#21152;&#36895;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#26412;&#22320;&#35757;&#32451;&#12289;&#21387;&#32553;&#21644;&#37096;&#20998;&#21442;&#19982;&#32467;&#21512;&#24212;&#29992;&#65292;&#20197;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03240</link><description>&lt;p&gt;
&#21033;&#29992;&#21387;&#32553;&#21644;&#37325;&#35201;&#24615;&#25277;&#26679;&#25552;&#39640;&#21152;&#36895;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Accelerated Federated Learning with Compression and Importance Sampling. (arXiv:2306.03240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#21387;&#32553;&#21644;&#37325;&#35201;&#24615;&#25277;&#26679;&#25552;&#39640;&#21152;&#36895;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#26412;&#22320;&#35757;&#32451;&#12289;&#21387;&#32553;&#21644;&#37096;&#20998;&#21442;&#19982;&#32467;&#21512;&#24212;&#29992;&#65292;&#20197;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21033;&#29992;&#20998;&#24067;&#22312;&#22823;&#37327;&#23458;&#25143;&#31471;&#19978;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#21327;&#20316;&#35757;&#32451;&#26694;&#26550;&#12290;&#22312;&#32858;&#21512;&#27493;&#39588;&#20013;&#65292;&#35201;&#27714;&#24182;&#22788;&#29702;&#25152;&#26377;&#23458;&#25143;&#31471;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#24517;&#39035;&#25903;&#25345;&#37096;&#20998;&#21442;&#19982;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26381;&#21153;&#22120;&#19982;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36890;&#20449;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#65292;&#26377;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#21387;&#32553;&#21644;&#26412;&#22320;&#27493;&#39588;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#27493;&#39588;&#25216;&#26415;&#24341;&#20837;&#20102;&#26032;&#30340;ProxSkip&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#29575;&#12290;&#21518;&#32493;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#23558;&#26412;&#22320;&#27493;&#39588;&#21152;&#36895;&#19982;&#37096;&#20998;&#21442;&#19982;&#21644;&#26799;&#24230;&#21387;&#32553;&#30456;&#32467;&#21512;&#12290;&#26412;&#25991;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#25152;&#26377;&#24517;&#35201;&#30340;&#25104;&#20998;&#65306;&#26412;&#22320;&#35757;&#32451;&#12289;&#21387;&#32553;&#21644;&#37096;&#20998;&#21442;&#19982;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is a collaborative training framework that leverages heterogeneous data distributed across a vast number of clients. Since it is practically infeasible to request and process all clients during the aggregation step, partial participation must be supported. In this setting, the communication between the server and clients poses a major bottleneck. To reduce communication loads, there are two main approaches: compression and local steps. Recent work by Mishchenko et al. [2022] introduced the new ProxSkip method, which achieves an accelerated rate using the local steps technique. Follow-up works successfully combined local steps acceleration with partial participation [Grudzie\'n et al., 2023, Condat et al. 2023] and gradient compression [Condat et al. [2022]. In this paper, we finally present a complete method for Federated Learning that incorporates all necessary ingredients: Local Training, Compression, and Partial Participation. We obtain state-of-the-art convergenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;MoE&#26550;&#26500;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20165;&#22522;&#20110;&#35775;&#38382;&#31574;&#30053;&#21551;&#29992;&#23376;&#38598;&#30340;&#19987;&#23478;&#65292;&#23454;&#29616;&#20102;&#23545;&#23433;&#20840;&#35775;&#38382;&#25511;&#21046;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.03235</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#27169;&#22411;&#26550;&#26500;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Information Flow Control in Machine Learning through Modular Model Architecture. (arXiv:2306.03235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;MoE&#26550;&#26500;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20165;&#22522;&#20110;&#35775;&#38382;&#31574;&#30053;&#21551;&#29992;&#23376;&#38598;&#30340;&#19987;&#23478;&#65292;&#23454;&#29616;&#20102;&#23545;&#23433;&#20840;&#35775;&#38382;&#25511;&#21046;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#20219;&#20309;&#37096;&#20998;&#37117;&#21487;&#20197;&#24433;&#21709;&#20854;&#36755;&#20986;&#12290;&#24403;&#35775;&#38382;&#25511;&#21046;&#21482;&#20801;&#35768;&#20010;&#20154;&#29992;&#25143;&#35775;&#38382;&#25968;&#25454;&#23376;&#38598;&#26102;&#65292;&#20174;&#35757;&#32451;&#25968;&#25454;&#21040;&#27169;&#22411;&#36755;&#20986;&#30340;&#20449;&#24687;&#27969;&#25511;&#21046;&#19981;&#36275;&#25104;&#20026;&#35757;&#32451;&#25935;&#24863;&#25968;&#25454;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#23454;&#29616;&#35775;&#38382;&#25511;&#21046;&#25968;&#25454;&#30340;&#23433;&#20840;&#26426;&#22120;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;&#30340;&#27010;&#24565;&#65292;&#24182;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#24320;&#21457;&#20102;&#19968;&#20010;&#23433;&#20840;Transformer&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#38480;&#21046;&#26469;&#33258;&#27599;&#20010;&#23433;&#20840;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#21333;&#20010;&#19987;&#23478;&#27169;&#22359;&#30340;&#24433;&#21709;&#65292;&#24182;&#20165;&#22522;&#20110;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#22312;&#25512;&#29702;&#26102;&#21551;&#29992;&#19987;&#23478;&#30340;&#23376;&#38598;&#65292;&#23433;&#20840;MoE&#26550;&#26500;&#25511;&#21046;&#20102;&#20449;&#24687;&#27969;&#12290;&#20351;&#29992;&#22823;&#22411;&#25991;&#26412;&#25968;&#25454;&#35821;&#26009;&#24211;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MoE&#26550;&#26500;&#20855;&#26377;&#26368;&#23567;&#30340;&#24615;&#33021;&#24320;&#38144;&#65288;1.9%&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65288;&#26368;&#39640;&#21487;&#36798;37%&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#35757;&#32451;&#20934;&#30830;&#21644;&#23433;&#20840;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's machine learning (ML) models, any part of the training data can affect its output. This lack of control for information flow from training data to model output is a major obstacle in training models on sensitive data when access control only allows individual users to access a subset of data. To enable secure machine learning for access controlled data, we propose the notion of information flow control for machine learning, and develop a secure Transformer-based language model based on the Mixture-of-Experts (MoE) architecture. The secure MoE architecture controls information flow by limiting the influence of training data from each security domain to a single expert module, and only enabling a subset of experts at inference time based on an access control policy. The evaluation using a large corpus of text data shows that the proposed MoE architecture has minimal (1.9%) performance overhead and can significantly improve model accuracy (up to 37%) by enabling training on acc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#36827;&#21270;&#26641;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#27809;&#26377;&#29305;&#24449;&#26631;&#31614;&#30340;&#29983;&#29289;&#22270;&#20687;&#20013;&#21457;&#29616;&#36827;&#21270;&#29305;&#24449;&#65292;&#33021;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20135;&#29983;&#29983;&#29289;&#23398;&#19978;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03228</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#29983;&#29289;&#36827;&#21270;&#26641;&#30340;&#31070;&#32463;&#32593;&#32476;&#20174;&#22270;&#20687;&#20013;&#21457;&#29616;&#26032;&#30340;&#29983;&#29289;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Discovering Novel Biological Traits From Images Using Phylogeny-Guided Neural Networks. (arXiv:2306.03228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#36827;&#21270;&#26641;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#27809;&#26377;&#29305;&#24449;&#26631;&#31614;&#30340;&#29983;&#29289;&#22270;&#20687;&#20013;&#21457;&#29616;&#36827;&#21270;&#29305;&#24449;&#65292;&#33021;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20135;&#29983;&#29983;&#29289;&#23398;&#19978;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#27839;&#29983;&#21629;&#20043;&#26641;&#65288;&#20063;&#31216;&#20026;&#36827;&#21270;&#26641;&#65289;&#36328;&#29289;&#31181;&#36951;&#20256;&#30340;&#36827;&#21270;&#29305;&#24449;&#23545;&#29983;&#29289;&#23398;&#23478;&#29702;&#35299;&#29983;&#29289;&#22810;&#26679;&#24615;&#21644;&#36827;&#21270;&#27169;&#24335;&#20855;&#26377;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#29305;&#24449;&#30340;&#27979;&#37327;&#36890;&#24120;&#26159;&#19968;&#20010;&#20027;&#35266;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#20351;&#24471;&#29305;&#24449;&#21457;&#29616;&#25104;&#20026;&#19968;&#20010;&#39640;&#24230;&#31232;&#32570;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#27809;&#26377;&#29305;&#24449;&#26631;&#31614;&#30340;&#22270;&#20687;&#20013;&#21457;&#29616;&#36827;&#21270;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861; Phylo-NN&#65292;&#23558;&#19968;&#20010;&#29983;&#29289;&#20307;&#30340;&#22270;&#20687;&#32534;&#30721;&#25104;&#19968;&#31995;&#21015;&#37327;&#21270;&#30340;&#29305;&#24449;&#21521;&#37327;&#24207;&#21015;&#65292;&#20854;&#20013;&#24207;&#21015;&#30340;&#19981;&#21516;&#27573;&#22312;&#36827;&#21270;&#26641;&#30340;&#19981;&#21516;&#31062;&#20808;&#23618;&#27425;&#25429;&#33719;&#36827;&#21270;&#20449;&#21495;&#12290;&#25105;&#20204;&#20351;&#29992;&#40060;&#31867;&#29289;&#31181;&#20316;&#20026;&#30446;&#26631;&#31034;&#20363;&#65292;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20135;&#29983;&#29983;&#29289;&#23398;&#19978;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#29289;&#31181;&#22270;&#20687;&#29983;&#25104;&#21644;&#29289;&#31181;&#21040;&#29289;&#31181;&#30340;&#22270;&#20687;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering evolutionary traits that are heritable across species on the tree of life (also referred to as a phylogenetic tree) is of great interest to biologists to understand how organisms diversify and evolve. However, the measurement of traits is often a subjective and labor-intensive process, making trait discovery a highly label-scarce problem. We present a novel approach for discovering evolutionary traits directly from images without relying on trait labels. Our proposed approach, Phylo-NN, encodes the image of an organism into a sequence of quantized feature vectors -- or codes -- where different segments of the sequence capture evolutionary signals at varying ancestry levels in the phylogeny. We demonstrate the effectiveness of our approach in producing biologically meaningful results in a number of downstream tasks including species image generation and species-to-species image translation, using fish species as a target example.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#37325;&#21152;&#26435;&#65288;StruRW&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;GDA&#65289;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#26465;&#20214;&#32467;&#26500;&#20559;&#31227;&#65288;CSS&#65289;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.03221</link><description>&lt;p&gt;
&#32467;&#26500;&#37325;&#21152;&#26435;&#25913;&#21892;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Structural Re-weighting Improves Graph Domain Adaptation. (arXiv:2306.03221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#37325;&#21152;&#26435;&#65288;StruRW&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;GDA&#65289;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#26465;&#20214;&#32467;&#26500;&#20559;&#31227;&#65288;CSS&#65289;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#65292;&#20363;&#22914;&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;&#30340;&#27169;&#25311;&#25968;&#25454;&#21487;&#33021;&#19982;&#23454;&#39564;&#19981;&#21305;&#37197;&#12290;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;GDA&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#24046;&#24322;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;GDA&#20027;&#35201;&#36890;&#36807;&#23545;&#21333;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#36755;&#20986;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#26469;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#20250;&#20135;&#29983;&#27425;&#20248;&#35299;&#12290;&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#30001;&#22270;&#32467;&#26500;&#25110;&#33410;&#28857;&#23646;&#24615;&#24341;&#36215;&#30340;&#19981;&#21516;&#20998;&#24067;&#20559;&#31227;&#30340;&#19981;&#21516;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#32467;&#26500;&#20559;&#31227;&#65288;CSS&#65289;&#30340;&#26032;&#31867;&#22411;&#20559;&#31227;&#65292;&#35777;&#26126;&#20102;&#24403;&#21069;&#30340;GDA&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32467;&#26500;&#37325;&#21152;&#26435;&#65288;StruRW&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#21512;&#25104;&#22270;&#12289;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26032;&#30340;&#39640;&#33021;&#29289;&#29702;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;StruRW&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world applications, graph-structured data used for training and testing have differences in distribution, such as in high energy physics (HEP) where simulation data used for training may not match real experiments. Graph domain adaptation (GDA) is a method used to address these differences. However, current GDA primarily works by aligning the distributions of node representations output by a single graph neural network encoder shared across the training and testing domains, which may often yield sub-optimal solutions. This work examines different impacts of distribution shifts caused by either graph structure or node attributes and identifies a new type of shift, named conditional structure shift (CSS), which current GDA approaches are provably sub-optimal to deal with. A novel approach, called structural reweighting (StruRW), is proposed to address this issue and is tested on synthetic graphs, four benchmark datasets, and a new application in HEP. StruRW has shown signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GromovMatcher&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#33258;&#21160;&#21512;&#24182;LC-MS&#25968;&#25454;&#38598;&#65292;&#21487;&#25552;&#39640;&#25968;&#25454;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#21512;&#24182;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.03218</link><description>&lt;p&gt;
&#29992;&#20110;&#26080;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#33258;&#21160;&#23545;&#40784;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal transport for automatic alignment of untargeted metabolomic data. (arXiv:2306.03218v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GromovMatcher&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#33258;&#21160;&#21512;&#24182;LC-MS&#25968;&#25454;&#38598;&#65292;&#21487;&#25552;&#39640;&#25968;&#25454;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#21512;&#24182;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28082;&#30456;&#33394;&#35889;-&#36136;&#35889;&#65288;LC-MS&#65289;&#36890;&#36807;&#27979;&#37327;&#29983;&#29289;&#26631;&#26412;&#20013;&#30340;&#22823;&#37327;&#20195;&#35874;&#29289;&#25512;&#21160;&#33647;&#29289;&#30740;&#21457;&#65292;&#30142;&#30149;&#35786;&#26029;&#21644;&#39118;&#38505;&#39044;&#27979;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;LC-MS&#30340;&#20302;&#36890;&#37327;&#23545;&#20110;&#29983;&#29289;&#26631;&#35760;&#29289;&#21457;&#29616;&#65292;&#27880;&#37322;&#21644;&#23454;&#39564;&#27604;&#36739;&#26500;&#25104;&#20102;&#20027;&#35201;&#25361;&#25112;&#65292;&#38656;&#35201;&#21512;&#24182;&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;&#24403;&#21069;&#30340;&#25968;&#25454;&#27744;&#21270;&#26041;&#27861;&#30001;&#20110;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#33030;&#24369;&#24615;&#32780;&#36935;&#21040;&#23454;&#38469;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GromovMatcher&#65292;&#19968;&#31181;&#28789;&#27963;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#33258;&#21160;&#32467;&#21512;LC-MS&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#24378;&#24230;&#30456;&#20851;&#32467;&#26500;&#65292;GromovMatcher&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#23545;&#40784;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#35813;&#31639;&#27861;&#21487;&#25193;&#23637;&#21040;&#38656;&#35201;&#26368;&#23567;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25968;&#21315;&#20010;&#29305;&#24449;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#32925;&#30284;&#21644;&#33008;&#33146;&#30284;&#30340;&#23454;&#39564;&#24739;&#32773;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Untargeted metabolomic profiling through liquid chromatography-mass spectrometry (LC-MS) measures a vast array of metabolites within biospecimens, advancing drug development, disease diagnosis, and risk prediction. However, the low throughput of LC-MS poses a major challenge for biomarker discovery, annotation, and experimental comparison, necessitating the merging of multiple datasets. Current data pooling methods encounter practical limitations due to their vulnerability to data variations and hyperparameter dependence. Here we introduce GromovMatcher, a flexible and user-friendly algorithm that automatically combines LC-MS datasets using optimal transport. By capitalizing on feature intensity correlation structures, GromovMatcher delivers superior alignment accuracy and robustness compared to existing approaches. This algorithm scales to thousands of features requiring minimal hyperparameter tuning. Applying our method to experimental patient studies of liver and pancreatic cancer, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#32852;&#35760;&#24518;&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;&#32858;&#31867;&#31639;&#27861;ClAM&#65292;&#32467;&#21512;&#27169;&#24335;&#23436;&#25104;&#25216;&#26415;&#24320;&#21457;&#33258;&#30417;&#30563;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#23545;&#20256;&#32479;&#31639;&#27861;&#21644;&#26368;&#26032;&#30340;&#36830;&#32493;&#32858;&#31867;&#26494;&#24347;&#26041;&#26696;&#22312;&#36718;&#24275;&#31995;&#25968;&#26041;&#38754;&#25552;&#39640;&#20102;60%&#12290;</title><link>http://arxiv.org/abs/2306.03209</link><description>&lt;p&gt;
&#24102;&#26377;&#20851;&#32852;&#35760;&#24518;&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
End-to-end Differentiable Clustering with Associative Memories. (arXiv:2306.03209v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#32852;&#35760;&#24518;&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;&#32858;&#31867;&#31639;&#27861;ClAM&#65292;&#32467;&#21512;&#27169;&#24335;&#23436;&#25104;&#25216;&#26415;&#24320;&#21457;&#33258;&#30417;&#30563;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#23545;&#20256;&#32479;&#31639;&#27861;&#21644;&#26368;&#26032;&#30340;&#36830;&#32493;&#32858;&#31867;&#26494;&#24347;&#26041;&#26696;&#22312;&#36718;&#24275;&#31995;&#25968;&#26041;&#38754;&#25552;&#39640;&#20102;60%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#28041;&#21450;&#24378;&#28872;&#30340;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#12290;&#20851;&#32852;&#35760;&#24518;&#27169;&#22411;&#25110;AM&#26159;&#21487;&#24494;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23450;&#20041;&#20102;&#36882;&#24402;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#24050;&#19982;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#38598;&#25104;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;AM&#21160;&#24577;&#21644;&#32858;&#31867;&#20013;&#22266;&#26377;&#30340;&#31163;&#25955;&#20998;&#37197;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#65292;&#20197;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#32422;&#26463;&#36830;&#32493;&#20256;&#25773;&#31163;&#25955;&#32858;&#31867;&#38382;&#39064;&#65292;&#20351;&#24471;AM&#21487;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#21487;&#24494;&#30340;&#32858;&#31867;&#65292;&#31216;&#20026;ClAM&#12290;&#21033;&#29992;AM&#30340;&#27169;&#24335;&#23436;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#25439;&#22833;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;ClAM&#20174;&#33258;&#30417;&#30563;&#20013;&#21463;&#30410;&#65292;&#24182;&#19988;&#22312;&#20256;&#32479;&#30340;Lloyd's k-means&#31639;&#27861;&#21644;&#26368;&#36817;&#30340;&#36830;&#32493;&#32858;&#31867;&#26494;&#24347;&#26041;&#26696;&#19978;&#22343;&#26174;&#30528;&#25913;&#21892;&#20102;&#65288;&#22312;&#36718;&#24275;&#31995;&#25968;&#26041;&#38754;&#25552;&#39640;&#20102;60&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is a widely used unsupervised learning technique involving an intensive discrete optimization problem. Associative Memory models or AMs are differentiable neural networks defining a recursive dynamical system, which have been integrated with various deep learning architectures. We uncover a novel connection between the AM dynamics and the inherent discrete assignment necessary in clustering to propose a novel unconstrained continuous relaxation of the discrete clustering problem, enabling end-to-end differentiable clustering with AM, dubbed ClAM. Leveraging the pattern completion ability of AMs, we further develop a novel self-supervised clustering loss. Our evaluations on varied datasets demonstrate that ClAM benefits from the self-supervision, and significantly improves upon both the traditional Lloyd's k-means algorithm, and more recent continuous clustering relaxations (by upto 60% in terms of the Silhouette Coefficient).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19968;&#31867;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807; Gateaux Derivative &#22788;&#29702;&#19968;&#33324;&#39118;&#38505;&#24230;&#37327;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22788;&#29702;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.03202</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Distributionally Robust Optimization. (arXiv:2306.03202v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19968;&#31867;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807; Gateaux Derivative &#22788;&#29702;&#19968;&#33324;&#39118;&#38505;&#24230;&#37327;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22788;&#29702;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#19968;&#31867;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#22312;&#20998;&#24067;&#19978;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#25991;&#29486;&#26377;&#25152;&#19981;&#21516;&#12290;&#20026;&#35299;&#20915;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#20248;&#21270;&#38750;&#32447;&#24615;&#20989;&#25968;&#38754;&#20020;&#30340;&#29702;&#35770;&#21644;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Derivative&#21644;&#30456;&#24212;&#30340;&#24179;&#28369;&#24230;&#27010;&#24565;&#65292;&#22522;&#20110;Gateaux Derivative&#26469;&#22788;&#29702;&#19968;&#33324;&#39118;&#38505;&#24230;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;Var&#12289;entropic risk&#21644;&#26377;&#38480;&#25903;&#25345;&#38598;&#19978;&#30340;&#19977;&#20010;&#36816;&#34892;&#39118;&#38505;&#24230;&#37327;&#31034;&#20363;&#26469;&#35299;&#37322;&#36825;&#20123;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#27010;&#29575;&#31354;&#38388;&#20013;&#19968;&#33324;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;G-derivative&#30340;Frank-Wolfe&#65288;FW&#65289;&#31639;&#27861;&#65292;&#24182;&#20197;&#23436;&#20840;&#29420;&#31435;&#20110;&#33539;&#25968;&#30340;&#26041;&#24335;&#25512;&#23548;&#20986;&#20854;&#25910;&#25947;&#24615;&#22312;&#25552;&#20986;&#30340;&#24179;&#28369;&#24230;&#27010;&#24565;&#19979;&#12290;&#25105;&#20204;&#21033;&#29992;FW&#31639;&#27861;&#30340;&#35774;&#32622;&#26469;&#35774;&#35745;&#19968;&#31181;&#35745;&#31639;&#38750;&#32447;&#24615;DRO&#38382;&#39064;&#38797;&#28857;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22788;&#29702;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article focuses on a class of distributionally robust optimization (DRO) problems where, unlike the growing body of the literature, the objective function is potentially non-linear in the distribution. Existing methods to optimize nonlinear functions in probability space use the Frechet derivatives, which present both theoretical and computational challenges. Motivated by this, we propose an alternative notion for the derivative and corresponding smoothness based on Gateaux (G)-derivative for generic risk measures. These concepts are explained via three running risk measure examples of variance, entropic risk, and risk on finite support sets. We then propose a G-derivative based Frank-Wolfe~(FW) algorithm for generic non-linear optimization problems in probability spaces and establish its convergence under the proposed notion of smoothness in a completely norm-independent manner. We use the set-up of the FW algorithm to devise a methodology to compute a saddle point of the non-lin
&lt;/p&gt;</description></item><item><title>&#22812;Pulse&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#22812;&#38388;&#20809;&#65288;NTL&#65289;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#20998;&#21106;&#12289;&#32858;&#31867;&#21644;&#21464;&#21270;&#27169;&#24335;&#26816;&#27979;&#26469;&#35782;&#21035;&#22478;&#24066;&#21457;&#23637;&#21644;&#25193;&#23637;&#27169;&#24335;&#65292;&#24182;&#22238;&#31572;&#26377;&#20851;&#20154;&#21475;&#22240;&#32032;&#12289;&#22478;&#24066;&#36793;&#30028;&#21644;&#24322;&#24120;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03195</link><description>&lt;p&gt;
&#22812;&#31354;&#20013;&#30340;Lumos&#65306;&#29992;&#20110;&#25506;&#32034;&#22812;&#38388;&#20809;&#27169;&#24335;&#30340;AI&#21487;&#35270;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Lumos in the Night Sky: AI-enabled Visual Tool for Exploring Night-Time Light Patterns. (arXiv:2306.03195v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03195
&lt;/p&gt;
&lt;p&gt;
&#22812;Pulse&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#22812;&#38388;&#20809;&#65288;NTL&#65289;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#20998;&#21106;&#12289;&#32858;&#31867;&#21644;&#21464;&#21270;&#27169;&#24335;&#26816;&#27979;&#26469;&#35782;&#21035;&#22478;&#24066;&#21457;&#23637;&#21644;&#25193;&#23637;&#27169;&#24335;&#65292;&#24182;&#22238;&#31572;&#26377;&#20851;&#20154;&#21475;&#22240;&#32032;&#12289;&#22478;&#24066;&#36793;&#30028;&#21644;&#24322;&#24120;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;NightPulse&#65292;&#36825;&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#29992;&#20110;&#22812;&#38388;&#20809;&#65288;NTL&#65289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#20351;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;&#24179;&#21488;&#25506;&#32034;&#21644;&#20998;&#26512;NTL&#25968;&#25454;&#12290;&#30001;&#39640;&#25928;&#30340;&#31995;&#32479;&#26550;&#26500;&#39537;&#21160;&#65292;NightPulse&#25903;&#25345;&#22270;&#20687;&#20998;&#21106;&#12289;&#32858;&#31867;&#21644;&#21464;&#21270;&#27169;&#24335;&#26816;&#27979;&#65292;&#20197;&#35782;&#21035;&#22478;&#24066;&#21457;&#23637;&#21644;&#25193;&#23637;&#27169;&#24335;&#12290;&#23427;&#25429;&#25417;NTL&#30340;&#26102;&#38388;&#36235;&#21183;&#21644;&#22478;&#24066;&#30340;&#35821;&#20041;&#65292;&#22238;&#31572;&#20851;&#20110;&#20154;&#21475;&#22240;&#32032;&#12289;&#22478;&#24066;&#36793;&#30028;&#21644;&#24322;&#24120;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce NightPulse, an interactive tool for Night-time light (NTL) data visualization and analytics, which enables researchers and stakeholders to explore and analyze NTL data with a user-friendly platform. Powered by efficient system architecture, NightPulse supports image segmentation, clustering, and change pattern detection to identify urban development and sprawl patterns. It captures temporal trends of NTL and semantics of cities, answering questions about demographic factors, city boundaries, and unusual differences.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#22312;&#32570;&#20047;&#22823;&#37327;&#20851;&#31995;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20026;&#39033;&#30446;&#25512;&#33616;&#24314;&#31435;&#26377;&#25928;&#30340;&#28909;&#21551;&#21160;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.03191</link><description>&lt;p&gt;
&#38754;&#21521;&#39033;&#30446;&#25512;&#33616;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Domain Adaptation for Item-to-Item Recommendation. (arXiv:2306.03191v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#22312;&#32570;&#20047;&#22823;&#37327;&#20851;&#31995;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20026;&#39033;&#30446;&#25512;&#33616;&#24314;&#31435;&#26377;&#25928;&#30340;&#28909;&#21551;&#21160;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39033;&#30446;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#39033;&#30446;&#23545;&#39033;&#30446;&#65288;I2I&#65289;&#25512;&#33616;&#22312;&#22823;&#37096;&#20998;&#25512;&#33616;&#31995;&#32479;&#20013;&#37117;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#21151;&#33021;&#65292;&#21487;&#20197;&#20026;&#29305;&#23450;&#30340;&#39033;&#30446;&#29983;&#25104;&#26367;&#25442;&#25110;&#34917;&#20805;&#24314;&#35758;&#12290;&#22522;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26576;&#20123;&#39033;&#30446;&#23376;&#38598;&#21487;&#33021;&#30001;&#21516;&#19968;&#32452;&#23458;&#25143;&#20849;&#21516;&#20132;&#20114;&#65292;&#22270;&#24418;&#27169;&#22411;&#65288;&#22914;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26694;&#26550;&#26469;&#32467;&#21512;&#12289;&#25668;&#21462;&#21644;&#25552;&#21462;&#26469;&#33258;&#36825;&#20123;&#30446;&#24405;&#21270;&#39033;&#30446;&#30340;&#39640;&#38454;&#20851;&#31995;&#20132;&#20114;&#20197;&#21450;&#23427;&#20204;&#30340;&#20803;&#25968;&#25454;&#29305;&#24615;&#25152;&#25552;&#20379;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#20026;I2I&#23398;&#20064;GNN&#38656;&#35201;&#25668;&#21462;&#22823;&#37327;&#30340;&#20851;&#31995;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20852;&#24066;&#22330;&#39046;&#22495;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#29942;&#39048;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#20197;&#20174;&#29616;&#26377;&#25104;&#29087;&#24066;&#22330;&#39046;&#22495;&#65288;&#20855;&#26377;&#31169;&#26377;&#25968;&#25454;&#65289;&#20013;&#23398;&#20064;&#21040;&#30340;&#25512;&#33616;&#27169;&#24335;&#21487;&#20197;&#34987;&#36866;&#24212;&#65292;&#20174;&#32780;&#24314;&#31435;&#36215;&#26377;&#25928;&#30340;&#28909;&#21551;&#21160;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Item-to-Item (I2I) recommendation is an important function in most recommendation systems, which generates replacement or complement suggestions for a particular item based on its semantic similarities to other cataloged items. Given that subsets of items in a recommendation system might be co-interacted with by the same set of customers, graph-based models, such as graph neural networks (GNNs), provide a natural framework to combine, ingest and extract valuable insights from such high-order relational interactions between cataloged items, as well as their metadata features, as has been shown in many recent studies. However, learning GNNs effectively for I2I requires ingesting a large amount of relational data, which might not always be available, especially in new, emerging market segments. To mitigate this data bottleneck, we postulate that recommendation patterns learned from existing mature market segments (with private data) could be adapted to build effective warm-start models fo
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#30828;&#24065;&#32763;&#36716;&#26469;&#25512;&#23548;&#29366;&#24577;&#30340;&#35775;&#38382;&#35745;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22870;&#21169;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.03186</link><description>&lt;p&gt;
&#32763;&#36716;&#30828;&#24065;&#26469;&#20272;&#35745;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#30340;&#20266;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement Learning. (arXiv:2306.03186v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03186
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#30828;&#24065;&#32763;&#36716;&#26469;&#25512;&#23548;&#29366;&#24577;&#30340;&#35775;&#38382;&#35745;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22870;&#21169;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#22522;&#20110;&#35745;&#25968;&#30340;&#25506;&#32034;&#26032;&#26041;&#27861;&#12290;&#19982;&#20381;&#36182;&#23494;&#24230;&#27169;&#22411;&#30340;&#20197;&#24448;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35745;&#25968;&#21487;&#20197;&#36890;&#36807;&#20174;Rademacher&#20998;&#24067;&#65288;&#25110;&#30828;&#24065;&#32763;&#36716;&#65289;&#20013;&#24179;&#22343;&#26679;&#26412;&#24471;&#21040;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#32622;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#24403;&#20248;&#21270;&#26102;&#65292;&#20250;&#20135;&#29983;&#19968;&#20010;&#29366;&#24577;&#30340;&#35775;&#38382;&#35745;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#33021;&#26377;&#25928;&#22320;&#25512;&#23548;&#20986;&#30495;&#27491;&#30340;&#35775;&#38382;&#35745;&#25968;&#12290;&#24403;&#20316;&#20026;&#27169;&#22411;&#26080;&#20851;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25506;&#32034;&#22870;&#21169;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;Atari&#28216;&#25103;Montezuma's Revenge&#22312;&#20869;&#30340;9&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25506;&#32034;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method for count-based exploration in high-dimensional state spaces. Unlike previous work which relies on density models, we show that counts can be derived by averaging samples from the Rademacher distribution (or coin flips). This insight is used to set up a simple supervised learning objective which, when optimized, yields a state's visitation count. We show that our method is significantly more effective at deducing ground-truth visitation counts than previous work; when used as an exploration bonus for a model-free reinforcement learning algorithm, it outperforms existing approaches on most of 9 challenging exploration tasks, including the Atari game Montezuma's Revenge.
&lt;/p&gt;</description></item><item><title>LatFormer&#26159;&#19968;&#31181;&#23558;&#26684;&#28857;&#23545;&#31216;&#20808;&#39564;&#34701;&#20837;&#21040;&#27880;&#24847;&#21147;&#25513;&#30721;&#20013;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#29992;&#21367;&#31215;&#32593;&#32476;&#29983;&#25104;&#36719;&#25513;&#30721;&#26469;&#35843;&#25972;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#20960;&#20309;&#25512;&#29702;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03175</link><description>&lt;p&gt;
&#22522;&#20110;&#26684;&#28857;&#23545;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#20808;&#39564;&#21152;&#20837;&#65292;&#20197;&#25552;&#39640;&#25277;&#35937;&#20960;&#20309;&#25512;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning. (arXiv:2306.03175v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03175
&lt;/p&gt;
&lt;p&gt;
LatFormer&#26159;&#19968;&#31181;&#23558;&#26684;&#28857;&#23545;&#31216;&#20808;&#39564;&#34701;&#20837;&#21040;&#27880;&#24847;&#21147;&#25513;&#30721;&#20013;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#29992;&#21367;&#31215;&#32593;&#32476;&#29983;&#25104;&#36719;&#25513;&#30721;&#26469;&#35843;&#25972;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#20960;&#20309;&#25512;&#29702;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#21450;&#20854;&#26368;&#36817;&#30340;&#35821;&#35328;&#23436;&#25972;&#23454;&#20363;&#65288;LARC&#65289;&#34987;&#35748;&#20026;&#26159;&#36890;&#24448;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#20063;&#38590;&#20197;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#24615;&#33021;&#65292;&#33853;&#21518;&#20110;&#38750;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26497;&#31471;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21482;&#26377;&#36890;&#36807;&#36866;&#24403;&#32771;&#34385;&#26680;&#24515;&#30693;&#35782;&#20808;&#39564;&#25165;&#33021;&#23454;&#29616;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#20960;&#20309;&#20808;&#39564;&#65292;&#24182;&#24341;&#20837;LatFormer&#27169;&#22411;&#65292;&#23558;&#26684;&#28857;&#23545;&#31216;&#20808;&#39564;&#34701;&#20837;&#21040;&#27880;&#24847;&#21147;&#25513;&#30721;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#36229;&#31435;&#26041;&#26684;&#30340;&#20219;&#20309;&#21464;&#25442;&#65292;&#37117;&#23384;&#22312;&#19968;&#20010;&#20108;&#20540;&#27880;&#24847;&#21147;&#25513;&#30721;&#26469;&#23454;&#29616;&#35813;&#32676;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#28608;&#21457;&#20102;&#23545;&#26631;&#20934;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20462;&#25913;&#65292;&#20854;&#20013;&#20351;&#29992;&#21367;&#31215;&#32593;&#32476;&#29983;&#25104;&#30340;&#36719;&#25513;&#30721;&#26469;&#35843;&#25972;&#20851;&#27880;&#26435;&#37325;&#12290;&#22312;&#21512;&#25104;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LatFormer
&lt;/p&gt;
&lt;p&gt;
The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks. We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action. Hence, our study motivates a modification to the standard attention mechanism, where attention weights are scaled using soft masks generated by a convolutional network. Experiments on synthetic geometric reasoning show that LatFormer 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#23398;&#20064;&#20986;&#21478;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#26144;&#23556;&#65292;&#21363;&#20351;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#20063;&#21487;&#20197;&#20197;&#20219;&#24847;&#31934;&#24230;&#23398;&#20064;&#30495;&#23454;&#30340;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20302;&#31209;&#27169;&#22411;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03173</link><description>&lt;p&gt;
&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Linear Distance Metric Learning. (arXiv:2306.03173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#23398;&#20064;&#20986;&#21478;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#26144;&#23556;&#65292;&#21363;&#20351;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#20063;&#21487;&#20197;&#20197;&#20219;&#24847;&#31934;&#24230;&#23398;&#20064;&#30495;&#23454;&#30340;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20302;&#31209;&#27169;&#22411;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#20013;&#65292;&#32473;&#23450;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#65292;&#30446;&#26631;&#26159;&#23547;&#25214;&#19968;&#20010;&#36866;&#24403;&#30340;&#32447;&#24615;&#26144;&#23556;&#21040;&#21478;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#65292;&#23613;&#21487;&#33021;&#22320;&#28385;&#36275;&#19968;&#23450;&#30340;&#36317;&#31163;&#26465;&#20214;&#12290;&#26412;&#25991;&#35268;&#33539;&#20102;&#19968;&#31181;&#31616;&#21333;&#20248;&#32654;&#30340;&#26041;&#27861;&#65292;&#23427;&#31616;&#21270;&#20026;&#19968;&#20010;&#36830;&#32493;&#30340;&#20984;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#30456;&#24212;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#25968;&#25454;&#26377;&#22122;&#22768;&#65292;&#21482;&#35201;&#26377;&#36275;&#22815;&#30340;&#26679;&#26412;&#65292;&#23601;&#21487;&#20197;&#20197;&#20219;&#24847;&#31934;&#24230;&#23398;&#20064;&#30495;&#23454;&#30340;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#23558;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#25130;&#26029;&#20026;&#20302;&#31209;&#27169;&#22411;&#65292;&#21487;&#20197;&#35777;&#26126;&#22312;&#25439;&#22833;&#20989;&#25968;&#21644;&#21442;&#25968;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#20445;&#25345;&#31934;&#24230;&#65292;&#36825;&#26159;&#36825;&#31181;&#31867;&#22411;&#30340;&#39318;&#20010;&#32467;&#26524;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20960;&#20010;&#23454;&#39564;&#35266;&#23519;&#25903;&#25345;&#21644;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In linear distance metric learning, we are given data in one Euclidean metric space and the goal is to find an appropriate linear map to another Euclidean metric space which respects certain distance conditions as much as possible. In this paper, we formalize a simple and elegant method which reduces to a general continuous convex loss optimization problem, and for different noise models we derive the corresponding loss functions. We show that even if the data is noisy, the ground truth linear metric can be learned with any precision provided access to enough samples, and we provide a corresponding sample complexity bound. Moreover, we present an effective way to truncate the learned model to a low-rank model that can provably maintain the accuracy in loss function and in parameters -- the first such results of this type. Several experimental observations on synthetic and real data sets support and inform our theoretical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22312;&#19981;&#21516;&#22823;&#38470;&#12289;&#20113;&#20379;&#24212;&#21830;&#21644;&#25968;&#25454;&#20013;&#24515;&#33539;&#22260;&#20869;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03163</link><description>&lt;p&gt;
&#22914;&#20309;&#36328;&#36234;&#20113;&#21644;&#22823;&#38470;&#22521;&#35757;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65311;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study. (arXiv:2306.03163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22312;&#19981;&#21516;&#22823;&#38470;&#12289;&#20113;&#20379;&#24212;&#21830;&#21644;&#25968;&#25454;&#20013;&#24515;&#33539;&#22260;&#20869;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#31471;&#25110;&#19987;&#29992;&#30828;&#20214;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#26114;&#36149;&#30340;&#12290;&#19968;&#31181;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#26159;&#25552;&#20379;&#28857;&#23454;&#20363;&#30340;&#39640;&#36229;&#35268;&#27169;&#20113;&#65292;&#36825;&#26159;&#19968;&#20010;&#20415;&#23452;&#20294;&#30701;&#26242;&#30340;&#36873;&#25321;&#65292;&#29992;&#20110;&#26367;&#20195;&#25353;&#38656;&#36164;&#28304;&#12290;&#30001;&#20110;&#28857;&#23454;&#20363;&#30340;&#21487;&#29992;&#24615;&#21487;&#33021;&#20250;&#22240;&#26085;&#26399;&#12289;&#22823;&#38470;&#21644;&#20113;&#20379;&#24212;&#21830;&#19981;&#21516;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20998;&#37197;&#36164;&#28304;&#21487;&#33021;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;&#20294;&#26159;&#65292;&#23578;&#26410;&#35843;&#26597;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#21542;&#22312;&#35206;&#30422;&#19981;&#21516;&#25968;&#25454;&#20013;&#24515;&#21644;&#20113;&#25552;&#20379;&#21830;&#30340;&#28857; VM &#20840;&#29699;&#24066;&#22330;&#19978;&#20197;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65311;&#20026;&#20102;&#25552;&#20379;&#25351;&#23548;&#65292;&#25105;&#20204;&#24191;&#27867;&#35780;&#20272;&#20102;&#19981;&#21516;&#21306;&#22495;&#12289;&#22823;&#38470;&#21644;&#20113;&#23545;&#20195;&#34920;&#24615; CV &#21644; NLP &#27169;&#22411;&#30340;&#25104;&#26412;&#21644;&#21534;&#21520;&#37327;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25193;&#23637;&#24403;&#21069;&#30340;&#22521;&#35757;&#36873;&#25321;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep learning models in the cloud or on dedicated hardware is expensive. A more cost-efficient option are hyperscale clouds offering spot instances, a cheap but ephemeral alternative to on-demand resources. As spot instance availability can change depending on the time of day, continent, and cloud provider, it could be more cost-efficient to distribute resources over the world. Still, it has not been investigated whether geo-distributed, data-parallel spot deep learning training could be a more cost-efficient alternative to centralized training.  This paper aims to answer the question: Can deep learning models be cost-efficiently trained on a global market of spot VMs spanning different data centers and cloud providers? To provide guidance, we extensively evaluate the cost and throughput implications of training in different zones, continents, and clouds for representative CV and NLP models. To expand the current training options further, we compare the scalability potential f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#23398;&#20064;&#20013;&#37327;&#23376;&#32416;&#32544;&#21644;&#32479;&#35745;&#23398;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#32416;&#32544;&#27979;&#37327;&#19982;&#21487;&#20998;&#31163;&#27979;&#37327;&#20197;&#21450;&#32416;&#32544;&#27979;&#37327;&#19982;&#32479;&#35745;&#27979;&#37327;&#22312;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21306;&#21035;&#65292;&#35777;&#26126;&#20102;QSQ&#23398;&#20064;&#19982;&#21033;&#29992;&#32416;&#32544;&#27979;&#37327;&#30340;&#37327;&#23376;&#23398;&#20064;&#20043;&#38388;&#30340;&#25351;&#25968;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03161</link><description>&lt;p&gt;
&#35770;&#37327;&#23376;&#23398;&#20064;&#20013;&#37327;&#23376;&#32416;&#32544;&#21644;&#32479;&#35745;&#23398;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Entanglement and Statistics in Learning. (arXiv:2306.03161v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#23398;&#20064;&#20013;&#37327;&#23376;&#32416;&#32544;&#21644;&#32479;&#35745;&#23398;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#32416;&#32544;&#27979;&#37327;&#19982;&#21487;&#20998;&#31163;&#27979;&#37327;&#20197;&#21450;&#32416;&#32544;&#27979;&#37327;&#19982;&#32479;&#35745;&#27979;&#37327;&#22312;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21306;&#21035;&#65292;&#35777;&#26126;&#20102;QSQ&#23398;&#20064;&#19982;&#21033;&#29992;&#32416;&#32544;&#27979;&#37327;&#30340;&#37327;&#23376;&#23398;&#20064;&#20043;&#38388;&#30340;&#25351;&#25968;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;(QSQ)&#27169;&#22411;&#20013;&#21033;&#29992;&#37327;&#23376;&#32416;&#32544;&#12289;&#21487;&#20998;&#31163;&#20197;&#21450;&#32479;&#35745;&#27979;&#37327;&#26041;&#27861;&#36827;&#34892;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#24471;&#20986;&#19979;&#21015;&#32467;&#35770;&#12290;$\textbf{&#32416;&#32544;&#27979;&#37327;&#19982;&#21487;&#20998;&#31163;&#27979;&#37327;}$&#65306;&#22312;&#19968;&#20010;&#32473;&#23450;&#30340;&#27010;&#24565;&#31867;$C\subseteq \{f:\{0,1\}^n\rightarrow [k]\}$&#20013;&#65292;&#21033;&#29992;$\frac{1}{\sqrt{2^n}}\sum_x\vert x,f(x)\rangle$&#30340;&#21103;&#26412;&#26469;&#23398;&#20064;&#19968;&#20010;&#26410;&#30693;&#20989;&#25968;$f$&#65292;&#22914;&#26524;&#21033;&#29992;&#32416;&#32544;&#27979;&#37327;&#65292;&#21017;&#38656;&#35201;$T$&#20010;&#21103;&#26412;&#21363;&#21487;&#23436;&#25104;&#23398;&#20064;&#65292;&#21017;&#21482;&#38656;&#21033;&#29992;&#21487;&#20998;&#31163;&#27979;&#37327;&#65292;&#23601;&#38656;&#35201;$O(nT^2)$&#20010;&#21103;&#26412;&#12290;$\textbf{&#32416;&#32544;&#27979;&#37327;&#19982;&#32479;&#35745;&#27979;&#37327;}$&#65306;&#22312;&#21487;&#20998;&#31163;&#27979;&#37327;&#21644;&#32479;&#35745;&#27979;&#37327;&#30340;&#22522;&#30784;&#19978;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;$f\in C$&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31867;$C$&#65292;&#35777;&#26126;&#20102;QSQ&#23398;&#20064;&#19982;&#21033;&#29992;&#32416;&#32544;&#27979;&#37327;&#30340;&#37327;&#23376;&#23398;&#20064;&#20043;&#38388;&#30340;&#25351;&#25968;&#24046;&#24322;&#65288;&#21363;&#20351;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65289;&#65292;&#36825;&#35777;&#26126;&#20102;&#37327;&#23376;&#23398;&#20064;&#30340;&#37327;&#23376;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we make progress in understanding the relationship between learning models with access to entangled, separable and statistical measurements in the quantum statistical query (QSQ) model. To this end, we show the following results.  $\textbf{Entangled versus separable measurements.}$ The goal here is to learn an unknown $f$ from the concept class $C\subseteq \{f:\{0,1\}^n\rightarrow [k]\}$ given copies of $\frac{1}{\sqrt{2^n}}\sum_x \vert x,f(x)\rangle$. We show that, if $T$ copies suffice to learn $f$ using entangled measurements, then $O(nT^2)$ copies suffice to learn $f$ using just separable measurements.  $\textbf{Entangled versus statistical measurements}$ The goal here is to learn a function $f \in C$ given access to separable measurements and statistical measurements. We exhibit a class $C$ that gives an exponential separation between QSQ learning and quantum learning with entangled measurements (even in the presence of noise). This proves the "quantum analogue" of th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#27979;&#22120;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#35745;&#25968;&#26041;&#27861;DISCount&#65292;&#36890;&#36807;&#23558;&#19981;&#23436;&#32654;&#30340;&#26816;&#27979;&#22120;&#19982;&#20154;&#24037;&#31579;&#36873;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#26080;&#20559;&#20272;&#35745;&#30340;&#35745;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#22810;&#20010;&#31354;&#38388;&#25110;&#26102;&#38388;&#21306;&#22495;&#30340;&#35745;&#25968;&#38382;&#39064;&#65292;&#24182;&#21487;&#26681;&#25454;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#23545;&#31579;&#36873;&#26679;&#26412;&#25968;&#21644;&#32622;&#20449;&#21306;&#38388;&#36827;&#34892;&#20272;&#35745;&#21644;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2306.03151</link><description>&lt;p&gt;
DISCount: &#22522;&#20110;&#26816;&#27979;&#22120;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#35745;&#25968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DISCount: Counting in Large Image Collections with Detector-Based Importance Sampling. (arXiv:2306.03151v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03151
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#27979;&#22120;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#35745;&#25968;&#26041;&#27861;DISCount&#65292;&#36890;&#36807;&#23558;&#19981;&#23436;&#32654;&#30340;&#26816;&#27979;&#22120;&#19982;&#20154;&#24037;&#31579;&#36873;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#26080;&#20559;&#20272;&#35745;&#30340;&#35745;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#22810;&#20010;&#31354;&#38388;&#25110;&#26102;&#38388;&#21306;&#22495;&#30340;&#35745;&#25968;&#38382;&#39064;&#65292;&#24182;&#21487;&#26681;&#25454;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#23545;&#31579;&#36873;&#26679;&#26412;&#25968;&#21644;&#32622;&#20449;&#21306;&#38388;&#36827;&#34892;&#20272;&#35745;&#21644;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#20195;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#28023;&#37327;&#22270;&#20687;&#38598;&#21512;&#20013;&#26816;&#27979;&#21644;&#35745;&#25968;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#24403;&#26816;&#27979;&#20219;&#21153;&#38750;&#24120;&#22256;&#38590;&#25110;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#26102;&#65292;&#21363;&#20351;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#22521;&#35757;&#25968;&#25454;&#21644;&#27169;&#22411;&#24320;&#21457;&#65292;&#35745;&#25968;&#32467;&#26524;&#20063;&#21487;&#33021;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DISCount&#8212;&#8212;&#19968;&#31181;&#22522;&#20110;&#26816;&#27979;&#22120;&#37325;&#35201;&#24615;&#37319;&#26679;&#26694;&#26550;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#35745;&#25968;&#26041;&#27861;&#65292;&#23558;&#19968;&#20010;&#19981;&#23436;&#32654;&#30340;&#26816;&#27979;&#22120;&#19982;&#20154;&#24037;&#31579;&#36873;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#26080;&#20559;&#20272;&#35745;&#30340;&#35745;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#22810;&#20010;&#31354;&#38388;&#25110;&#26102;&#38388;&#21306;&#22495;&#35745;&#25968;&#38382;&#39064;&#30340;&#25216;&#26415;&#65292;&#21482;&#38656;&#20351;&#29992;&#23569;&#37327;&#30340;&#31579;&#36873;&#26679;&#26412;&#24182;&#20272;&#35745;&#32622;&#20449;&#21306;&#38388;&#12290;&#36825;&#20351;&#24471;&#26368;&#32456;&#29992;&#25143;&#22312;&#20272;&#35745;&#36275;&#22815;&#20934;&#30830;&#26102;&#21487;&#20197;&#20572;&#27490;&#31579;&#36873;&#65292;&#36825;&#36890;&#24120;&#26159;&#31185;&#23398;&#30740;&#31350;&#30340;&#30446;&#26631;&#12290;&#22312;&#25216;&#26415;&#26041;&#38754;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#25511;&#21046;&#21464;&#37327;&#30340;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#65292;&#24182;&#35777;&#26126;&#20102;&#20272;&#35745;&#22120;&#30340;&#65288;&#26465;&#20214;&#65289;&#26080;&#20559;&#24615;&#12290;DISCount&#23548;&#33268;&#20102;9-12&#20493;&#30340;&#38477;&#20302;&#35745;&#25968;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many modern applications use computer vision to detect and count objects in massive image collections. However, when the detection task is very difficult or in the presence of domain shifts, the counts may be inaccurate even with significant investments in training data and model development. We propose DISCount -- a detector-based importance sampling framework for counting in large image collections that integrates an imperfect detector with human-in-the-loop screening to produce unbiased estimates of counts. We propose techniques for solving counting problems over multiple spatial or temporal regions using a small number of screened samples and estimate confidence intervals. This enables end-users to stop screening when estimates are sufficiently accurate, which is often the goal in a scientific study. On the technical side we develop variance reduction techniques based on control variates and prove the (conditional) unbiasedness of the estimators. DISCount leads to a 9-12x reduction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03116</link><description>&lt;p&gt;
&#36890;&#36807;&#36716;&#21270;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds. (arXiv:2306.03116v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20174;&#20247;&#21253;&#26381;&#21153;&#20013;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;&#27599;&#20010;&#27880;&#37322;&#32773;&#37117;&#23436;&#25104;&#33258;&#24049;&#30340;&#23567;&#37096;&#20998;&#27880;&#37322;&#65292;&#19981;&#21516;&#27880;&#37322;&#32773;&#30340;&#26631;&#27880;&#38169;&#35823;&#24448;&#24448;&#19981;&#21516;&#12290;&#36890;&#36807;&#26631;&#31614;&#22122;&#22768;&#30340;&#36716;&#31227;&#30697;&#38453;&#26469;&#24314;&#27169;&#22122;&#22768;&#20135;&#29983;&#36807;&#31243;&#26159;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#30340;&#19968;&#31181;&#26377;&#25928;&#24037;&#20855;&#12290;&#22312;&#23454;&#38469;&#20247;&#21253;&#27169;&#22411;&#20013;&#65292;&#36716;&#31227;&#30697;&#38453;&#26082;&#30001;&#27880;&#37322;&#32773;&#20381;&#36182;&#65292;&#20063;&#30001;&#23454;&#20363;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#37322;&#32773;&#21644;&#23454;&#20363;&#20381;&#36182;&#30340;&#36716;&#31227;&#30697;&#38453;(AIDTM)&#20855;&#26377;&#39640;&#22797;&#26434;&#24230;&#65292;&#32780;&#23454;&#38469;&#27880;&#37322;&#24448;&#24448;&#28041;&#21450;&#27880;&#37322;&#31232;&#30095;&#24615;&#65292;&#36825;&#20351;&#24471;&#24314;&#31435;AIDTM&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26082;&#35201;&#20445;&#25345;&#24314;&#27169;&#30340;&#24191;&#27867;&#24615;&#65292;&#21448;&#33021;&#26356;&#30495;&#23454;&#22320;&#35299;&#20915;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#31639;AIDTM&#21644;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from crowds describes that the annotations of training data are obtained with crowd-sourcing services. Multiple annotators each complete their own small part of the annotations, where labeling mistakes that depend on annotators occur frequently. Modeling the label-noise generation process by the noise transition matrix is a power tool to tackle the label noise. In real-world crowd-sourcing scenarios, noise transition matrices are both annotator- and instance-dependent. However, due to the high complexity of annotator- and instance-dependent transition matrices (AIDTM), \textit{annotation sparsity}, which means each annotator only labels a little part of instances, makes modeling AIDTM very challenging. Prior works simplify the problem by assuming the transition matrix is instance-independent or using simple parametric way, while lose modeling generality. Motivated by this, we target a more realistic problem, estimating general AIDTM in practice. Without losing modeling general
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#22810;&#23398;&#31185;&#12289;&#22810;&#20256;&#24863;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#20056;&#23458;&#30340;&#27963;&#21160;&#65292;&#24182;&#22312;&#26368;&#36817;&#23454;&#38469;&#26465;&#20214;&#19979;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#26469;&#21019;&#24314;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.03115</link><description>&lt;p&gt;
AutoExp: &#19968;&#31181;&#22810;&#23398;&#31185;&#12289;&#22810;&#20256;&#24863;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#30340;&#20154;&#31867;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
AutoExp: A multidisciplinary, multi-sensor framework to evaluate human activities in self-driving cars. (arXiv:2306.03115v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#22810;&#23398;&#31185;&#12289;&#22810;&#20256;&#24863;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#20056;&#23458;&#30340;&#27963;&#21160;&#65292;&#24182;&#22312;&#26368;&#36817;&#23454;&#38469;&#26465;&#20214;&#19979;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#26469;&#21019;&#24314;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#26222;&#21450;&#23558;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#20840;&#33258;&#20027;&#21270;&#21487;&#33021;&#27604;&#26368;&#21021;&#39044;&#35745;&#30340;&#38656;&#35201;&#26356;&#38271;&#26102;&#38388;&#12290;&#30446;&#21069;&#65292;&#35813;&#25216;&#26415;&#30340;&#31532;&#19968;&#25209;&#36710;&#36742;&#24050;&#32463;&#20986;&#29616;&#22312;&#19990;&#30028;&#26576;&#20123;&#22478;&#24066;&#20013;&#65292;&#20316;&#20026;&#23454;&#39564;&#24615;&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#26381;&#21153;&#30340;&#19968;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#24615;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#23398;&#31185;&#26041;&#27861;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#20154;&#25991;&#31038;&#20250;&#31185;&#23398;&#30456;&#32467;&#21512;&#65289;&#65292;&#29305;&#21035;&#26159;&#38750;&#39550;&#39542;&#30456;&#20851;&#27963;&#21160;&#65292;&#26469;&#30740;&#31350;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20056;&#23458;&#30340;&#27963;&#21160;&#12290;&#35813;&#26694;&#26550;&#30001;&#23454;&#39564;&#22330;&#26223;&#21644;&#25968;&#25454;&#37319;&#38598;&#27169;&#22359;&#32452;&#25104;&#65292;&#26088;&#22312;&#39318;&#20808;&#22312;&#26368;&#36817;&#21487;&#33021;&#30340;&#23454;&#38469;&#26465;&#20214;&#19979;&#25429;&#33719;&#26377;&#20851;&#36710;&#36742;&#20351;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#26085;&#24535;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of self-driving cars will certainly revolutionize our lives, even though they may take more time to become fully autonomous than initially predicted. The first vehicles are already present in certain cities of the world, as part of experimental robot-taxi services. However, most existing studies focus on the navigation part of such vehicles. We currently miss methods, datasets, and studies to assess the in-cabin human component of the adoption of such technology in real-world conditions. This paper proposes an experimental framework to study the activities of occupants of self-driving cars using a multidisciplinary approach (computer vision associated with human and social sciences), particularly non-driving related activities. The framework is composed of an experimentation scenario, and a data acquisition module. We seek firstly to capture real-world data about the usage of the vehicle in the nearest possible, real-world conditions, and secondly to create a dataset conta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#31070;&#32463;&#29983;&#29702;&#20449;&#21495;&#26469;&#35299;&#20915;&#20844;&#20849;&#24773;&#24863;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#29983;&#25104;&#27169;&#22411;&#22312;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#24212;&#29992;&#30340;&#20248;&#21183;&#12289;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#28145;&#20837;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.03112</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#24773;&#24863;&#31070;&#32463;&#29983;&#29702;&#20449;&#21495;&#30340;&#32508;&#36848;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Affective Neurophysiological Signals Using Generative Models: A Review Paper. (arXiv:2306.03112v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#31070;&#32463;&#29983;&#29702;&#20449;&#21495;&#26469;&#35299;&#20915;&#20844;&#20849;&#24773;&#24863;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#29983;&#25104;&#27169;&#22411;&#22312;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#24212;&#29992;&#30340;&#20248;&#21183;&#12289;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#28145;&#20837;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#20013;&#65292;&#23558;&#24773;&#24863;&#26234;&#33021;&#24341;&#20837;&#26426;&#22120;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#36825;&#38656;&#35201;&#24320;&#21457;&#21487;&#38752;&#30340;&#31471;&#21040;&#31471;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20844;&#20849;&#24773;&#24863;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#29486;&#35780;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#35299;&#20915;&#31070;&#32463;&#29983;&#29702;&#23398;&#20449;&#21495;&#20013;&#65292;&#23588;&#20854;&#26159;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#21644;&#21151;&#33021;&#24615;&#36817;&#32418;&#22806;&#20809;&#35889;&#65288;fNIRS&#65289;&#20013;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#26816;&#39564;&#20102;&#23427;&#20204;&#30340;&#36755;&#20837;&#26684;&#24335;&#12289;&#37096;&#32626;&#31574;&#30053;&#20197;&#21450;&#29992;&#20110;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#32508;&#36848;&#26159;&#19968;&#31687;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#29983;&#25104;&#27169;&#22411;&#22312;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12289;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#28145;&#20837;&#35265;&#35299;&#12290;&#36890;&#36807;&#36825;&#31687;&#32508;&#36848;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#31070;&#32463;&#29983;&#29702;&#25968;&#25454;&#22686;&#24378;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of emotional intelligence in machines is an important step in advancing human-computer interaction. This demands the development of reliable end-to-end emotion recognition systems. However, the scarcity of public affective datasets presents a challenge. In this literature review, we emphasize the use of generative models to address this issue in neurophysiological signals, particularly Electroencephalogram (EEG) and Functional Near-Infrared Spectroscopy (fNIRS). We provide a comprehensive analysis of different generative models used in the field, examining their input formulation, deployment strategies, and methodologies for evaluating the quality of synthesized data. This review serves as a comprehensive overview, offering insights into the advantages, challenges, and promising future directions in the application of generative models in emotion recognition systems. Through this review, we aim to facilitate the progression of neurophysiological data augmentation, there
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;BootGen&#31639;&#27861;&#65292;&#20351;&#29992;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#22686;&#24378;&#29983;&#29289;&#24207;&#21015;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#20248;&#21270;&#29983;&#29289;&#24207;&#21015;&#65292;&#21462;&#24471;&#20102;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03111</link><description>&lt;p&gt;
&#38024;&#23545;&#31163;&#32447;&#35774;&#35745;&#29983;&#29289;&#24207;&#21015;&#30340;&#24471;&#20998;&#26465;&#20214;&#29983;&#25104;&#22120;&#30340;&#33258;&#21161;&#22686;&#24378;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences. (arXiv:2306.03111v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;BootGen&#31639;&#27861;&#65292;&#20351;&#29992;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#22686;&#24378;&#29983;&#29289;&#24207;&#21015;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#20248;&#21270;&#29983;&#29289;&#24207;&#21015;&#65292;&#21462;&#24471;&#20102;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#21270;&#29983;&#29289;&#24207;&#21015;&#65288;&#22914;&#34507;&#30333;&#36136;&#12289;DNA&#21644;RNA&#65289;&#20197;&#26368;&#22823;&#21270;&#20165;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35780;&#20272;&#30340;&#40657;&#21283;&#23376;&#24471;&#20998;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#24471;&#20998;&#26465;&#20214;&#29983;&#25104;&#22120;&#30340;&#33258;&#21161;&#22686;&#24378;&#35757;&#32451;&#65288;BootGen&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#37325;&#22797;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#25490;&#21517;&#21152;&#26435;&#27861;&#35757;&#32451;&#29983;&#29289;&#24207;&#21015;&#29983;&#25104;&#22120;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#39640;&#20998;&#25968;&#30340;&#24207;&#21015;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#12290;&#25509;&#19979;&#26469;&#30340;&#38454;&#27573;&#28041;&#21450;&#21040;&#33258;&#21161;&#22686;&#24378;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#24182;&#26631;&#35760;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#65292;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#19982;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#23545;&#40784;&#65292;&#23558;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#29983;&#25104;&#22120;&#12290;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#33258;&#21161;&#22686;&#24378;&#29983;&#25104;&#22120;&#21644;&#20195;&#29702;&#30340;&#26679;&#26412;&#65292;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#29289;&#24207;&#21015;&#20248;&#21270;&#26041;&#38754;&#32988;&#36807;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of optimizing biological sequences, e.g., proteins, DNA, and RNA, to maximize a black-box score function that is only evaluated in an offline dataset. We propose a novel solution, bootstrapped training of score-conditioned generator (BootGen) algorithm. Our algorithm repeats a two-stage process. In the first stage, our algorithm trains the biological sequence generator with rank-based weights to enhance the accuracy of sequence generation based on high scores. The subsequent stage involves bootstrapping, which augments the training dataset with self-generated data labeled by a proxy score function. Our key idea is to align the score-based generation with a proxy score function, which distills the knowledge of the proxy score function to the generator. After training, we aggregate samples from multiple bootstrapped generators and proxies to produce a diverse design. Extensive experiments show that our method outperforms competitive baselines on biological sequential
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ASTEROID&#26694;&#26550;&#65292;&#21033;&#29992;&#20415;&#23452;&#32780;&#19981;&#20934;&#30830;&#30340;&#25968;&#25454;&#21644;&#26114;&#36149;&#32780;&#20934;&#30830;&#30340;&#25968;&#25454;&#26469;&#38477;&#20302;MLFF&#30340;&#25968;&#25454;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#20559;&#24046;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#35753;MLFF&#27169;&#22411;&#22312;&#38450;&#27490;&#36807;&#25311;&#21512;&#30340;&#21516;&#26102;&#65292;&#20063;&#33021;&#25429;&#25417;&#22522;&#30784;&#21147;&#22330;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.03109</link><description>&lt;p&gt;
&#25968;&#25454;&#36153;&#29992;&#24863;&#30693;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Force Fields with Data Cost Aware Training. (arXiv:2306.03109v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ASTEROID&#26694;&#26550;&#65292;&#21033;&#29992;&#20415;&#23452;&#32780;&#19981;&#20934;&#30830;&#30340;&#25968;&#25454;&#21644;&#26114;&#36149;&#32780;&#20934;&#30830;&#30340;&#25968;&#25454;&#26469;&#38477;&#20302;MLFF&#30340;&#25968;&#25454;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#20559;&#24046;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#35753;MLFF&#27169;&#22411;&#22312;&#38450;&#27490;&#36807;&#25311;&#21512;&#30340;&#21516;&#26102;&#65292;&#20063;&#33021;&#25429;&#25417;&#22522;&#30784;&#21147;&#22330;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;(MLFF)&#34987;&#25552;&#20986;&#20197;&#21152;&#36895;&#20998;&#23376;&#21160;&#21147;&#23398;(MD)&#27169;&#25311;&#65292;&#35813;&#27169;&#25311;&#22312;&#21270;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#21363;&#20351;&#23545;&#20110;&#26368;&#33410;&#32422;&#25968;&#25454;&#30340;MLFF&#65292;&#35201;&#36798;&#21040;&#21270;&#23398;&#31934;&#24230;&#21487;&#33021;&#38656;&#35201;&#25968;&#30334;&#24103;&#30001;&#26114;&#36149;&#30340;&#37327;&#23376;&#21147;&#23398;&#31639;&#27861;&#29983;&#25104;&#30340;&#21147;&#21644;&#33021;&#37327;&#26631;&#31614;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#35268;&#27169;&#21487;&#33021;&#20026;$O(n^3)$&#33267;$O(n^7)$&#65292;&#20854;&#20013;$n$&#19982;&#22522;&#20989;&#25968;&#30340;&#25968;&#37327;&#25104;&#27604;&#20363;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#35745;&#31639;&#26694;&#26550;ASTEROID&#65292;&#36890;&#36807;&#21033;&#29992;&#20415;&#23452;&#30340;&#19981;&#20934;&#30830;&#25968;&#25454;&#21644;&#26114;&#36149;&#30340;&#20934;&#30830;&#25968;&#25454;&#38477;&#20302;&#20102;MLFF&#30340;&#25968;&#25454;&#25104;&#26412;&#12290;ASTEROID&#30340;&#21160;&#26426;&#26159;&#65292;&#23613;&#31649;&#19981;&#20934;&#30830;&#30340;&#25968;&#25454;&#20250;&#20135;&#29983;&#24456;&#22823;&#30340;&#20559;&#24046;&#65292;&#20294;&#23427;&#21487;&#20197;&#24110;&#21161;&#25429;&#25417;&#22522;&#30784;&#21147;&#22330;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#22823;&#37327;&#19981;&#20934;&#30830;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#20351;&#29992;&#20559;&#24046;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;MLFF&#27169;&#22411;&#65292;&#20197;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#21487;&#33021;&#23548;&#33268;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning force fields (MLFF) have been proposed to accelerate molecular dynamics (MD) simulation, which finds widespread applications in chemistry and biomedical research. Even for the most data-efficient MLFFs, reaching chemical accuracy can require hundreds of frames of force and energy labels generated by expensive quantum mechanical algorithms, which may scale as $O(n^3)$ to $O(n^7)$, with $n$ proportional to the number of basis functions. To address this issue, we propose a multi-stage computational framework -ASTEROID, which lowers the data cost of MLFFs by leveraging a combination of cheap inaccurate data and expensive accurate data. The motivation behind ASTEROID is that inaccurate data, though incurring large bias, can help capture the sophisticated structures of the underlying force field. Therefore, we first train a MLFF model on a large amount of inaccurate training data, employing a bias-aware loss function to prevent the model from overfitting tahe potential bia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25913;&#36827;&#30340;PINN&#26041;&#27861;&#22522;&#20110;&#25968;&#25454;&#65292;&#25104;&#21151;&#33719;&#24471;&#20102;Fokas-Lenells&#26041;&#31243;&#20013;&#30340;&#26126;&#26263;&#23396;&#23376;&#35299;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#23432;&#24658;&#37327;&#20449;&#24687;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#21644;&#31934;&#30830;&#35299;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03105</link><description>&lt;p&gt;
&#20351;&#29992;&#25913;&#36827;&#30340;PINN&#26041;&#27861;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23616;&#22495;&#27874;&#35299;&#20915;Fokas-Lenells&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Data driven localized wave solution of the Fokas-Lenells equation using modified PINN. (arXiv:2306.03105v1 [nlin.PS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25913;&#36827;&#30340;PINN&#26041;&#27861;&#22522;&#20110;&#25968;&#25454;&#65292;&#25104;&#21151;&#33719;&#24471;&#20102;Fokas-Lenells&#26041;&#31243;&#20013;&#30340;&#26126;&#26263;&#23396;&#23376;&#35299;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#23432;&#24658;&#37327;&#20449;&#24687;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#21644;&#31934;&#30830;&#35299;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#30740;&#31350;&#20102;Fokas-Lenells&#26041;&#31243;&#30340;&#25968;&#25454;&#39537;&#21160;&#23616;&#22495;&#27874;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27531;&#24046;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#25511;&#21046;&#21442;&#25968;&#26469;&#25913;&#36827;&#22522;&#26412;&#30340;PINN&#65292;&#24182;&#28155;&#21152;&#23432;&#24658;&#37327;&#20316;&#20026;&#21478;&#19968;&#20010;&#25439;&#22833;&#39033;&#20197;&#20462;&#25913;PINN&#12290;&#20351;&#29992;&#25913;&#36827;&#30340;PINN&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;Fokas-Lenells&#26041;&#31243;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#26126;&#26263;&#23396;&#23376;&#35299;&#12290;&#23432;&#24658;&#37327;&#20449;&#24687;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#39044;&#27979;&#21644;&#31934;&#30830;&#23396;&#23376;&#35299;&#20043;&#38388;&#30340;&#30456;&#23545;L2&#35823;&#24046;&#26041;&#38754;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#30740;&#31350;&#23545;&#20110;&#30740;&#31350;&#22312;&#38750;&#32447;&#24615;&#20809;&#23398;&#21644;&#20854;&#20182;&#38750;&#32447;&#24615;&#29289;&#29702;&#23398;&#39046;&#22495;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#24212;&#29992;&#26377;&#29992;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/gautamksaharia/Fokas-Lenells&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate data driven localized wave solutions of the Fokas-Lenells equation by using physics informed neural network(PINN). We improve basic PINN by incorporating control parameters into the residual loss function. We also add conserve quantity as another loss term to modify the PINN. Using modified PINN we obtain the data driven bright soliton and dark soliton solutions of Fokas-Lenells equation. Conserved quantities informed loss function achieve more accuracy in terms of relative L2 error between predicted and exact soliton solutions. We hope that the present investigation would be useful to study the applications of deep learning in nonlinear optics and other branches of nonlinear physics. Source codes are available at https://github.com/gautamksaharia/Fokas-Lenells
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#30340;&#26032;&#22411;&#26694;&#26550;CrystalGPT&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#31639;&#27861;&#20013;&#30340;&#24378;&#22823;&#36716;&#31227;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#26230;&#20307;&#39044;&#27979;&#19982;&#25511;&#21046;&#20013;&#30340;&#31995;&#32479;&#23545;&#31995;&#32479;&#36716;&#31227;&#24615;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;CrystalGPT&#30340;&#32047;&#31215;&#35823;&#24046;&#38477;&#20302;&#20102;8&#20493;&#12290;</title><link>http://arxiv.org/abs/2306.03099</link><description>&lt;p&gt;
CrystalGPT&#65306;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#30340;&#26230;&#20307;&#39044;&#27979;&#19982;&#25511;&#21046;&#20013;&#31995;&#32479;&#23545;&#31995;&#32479;&#36716;&#31227;&#24615;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
CrystalGPT: Enhancing system-to-system transferability in crystallization prediction and control using time-series-transformers. (arXiv:2306.03099v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#30340;&#26032;&#22411;&#26694;&#26550;CrystalGPT&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#31639;&#27861;&#20013;&#30340;&#24378;&#22823;&#36716;&#31227;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#26230;&#20307;&#39044;&#27979;&#19982;&#25511;&#21046;&#20013;&#30340;&#31995;&#32479;&#23545;&#31995;&#32479;&#36716;&#31227;&#24615;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;CrystalGPT&#30340;&#32047;&#31215;&#35823;&#24046;&#38477;&#20302;&#20102;8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39044;&#27979;&#21644;&#23454;&#26102;&#25511;&#21046;&#20219;&#21153;&#65292;&#32463;&#24120;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290; &#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20934;&#30830;&#65292;&#20294;&#23427;&#20204;&#26159;&#20026;&#21333;&#20010;&#31995;&#32479;&#23450;&#21046;&#30340;&#65292;&#20351;&#24471;&#31995;&#32479;&#23545;&#31995;&#32479;&#65288;S2S&#65289;&#36716;&#31227;&#22256;&#38590;&#12290; &#21363;&#20351;&#22312;&#19981;&#21516;&#21270;&#23398;&#31995;&#32479;&#30340;&#36807;&#31243;&#21160;&#21147;&#23398;&#23384;&#22312;&#37325;&#35201;&#30456;&#20284;&#24615;&#26102;&#20063;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290; &#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#22120;&#65288;TST&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#31639;&#27861;&#20013;&#20869;&#22312;&#30340;&#24378;&#22823;&#36716;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290; &#36825;&#26159;&#20351;&#29992;&#20174;&#22312;&#21508;&#31181;&#25805;&#20316;&#22330;&#26223;&#19979;&#24037;&#20316;&#30340;&#19981;&#21516;&#26230;&#20307;&#22120;&#20013;&#33719;&#24471;&#30340;&#21487;&#29992;&#36807;&#31243;&#25968;&#25454;&#23637;&#31034;&#30340;&#12290; &#20351;&#29992;&#36825;&#20010;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;TST&#27169;&#22411;&#65288;CrystalGPT&#65289;&#65292;&#23427;&#22312;&#25152;&#26377;&#39044;&#20808;&#24314;&#31435;&#30340;&#31995;&#32479;&#20043;&#38388;&#23637;&#29616;&#20102;&#26174;&#30528;&#30340;S2S&#21487;&#36716;&#31227;&#24615;&#65292;&#29978;&#33267;&#23545;&#20110;&#26410;&#36935;&#21040;&#30340;&#31995;&#32479;&#20063;&#26159;&#22914;&#27492;&#12290; CrystalGPT&#22312;&#25152;&#26377;&#31995;&#32479;&#20013;&#20849;&#21516;&#23454;&#29616;&#30340;&#32047;&#31215;&#35823;&#24046;&#26159;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
For prediction and real-time control tasks, machine-learning (ML)-based digital twins are frequently employed. However, while these models are typically accurate, they are custom-designed for individual systems, making system-to-system (S2S) transferability difficult. This occurs even when substantial similarities exist in the process dynamics across different chemical systems. To address this challenge, we developed a novel time-series-transformer (TST) framework that exploits the powerful transfer learning capabilities inherent in transformer algorithms. This was demonstrated using readily available process data obtained from different crystallizers operating under various operational scenarios. Using this extensive dataset, we trained a TST model (CrystalGPT) to exhibit remarkable S2S transferability not only across all pre-established systems, but also to an unencountered system. CrystalGPT achieved a cumulative error across all systems, which is eight times superior to that of exi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#26694;&#26550;SEER&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#34987;&#23458;&#25143;&#31471;&#20390;&#27979;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2306.03013</link><description>&lt;p&gt;
&#26126;&#34255;&#26263;&#31363;&#65306;&#20266;&#35013;&#25968;&#25454;&#31363;&#21462;&#25915;&#20987;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning. (arXiv:2306.03013v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#26694;&#26550;SEER&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#34987;&#23458;&#25143;&#31471;&#20390;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#24050;&#32463;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#31363;&#21462;&#22312;&#22823;&#25209;&#37327;&#21644;&#23433;&#20840;&#32858;&#21512;&#31561;&#20043;&#21069;&#34987;&#35270;&#20026;&#31169;&#23494;&#30340;&#35774;&#32622;&#20013;&#21464;&#24471;&#21487;&#34892;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20851;&#20110;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#23458;&#25143;&#31471;&#20390;&#27979;&#24615;&#30340;&#30097;&#34385;&#34987;&#25552;&#20986;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#34987;&#20844;&#24320;&#30693;&#26195;&#21518;&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#36136;&#30097;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20390;&#27979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35768;&#22810;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#21407;&#21017;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#37117;&#21487;&#20197;&#36890;&#36807;&#21512;&#29702;&#30340;&#23458;&#25143;&#31471;&#26816;&#26597;&#26469;&#26816;&#27979;&#20986;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#23454;&#29992;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#30340;&#29702;&#24819;&#35201;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;SEER&#25915;&#20987;&#26694;&#26550;&#65292;&#23427;&#28385;&#36275;&#25152;&#26377;&#29702;&#24819;&#35201;&#27714;&#65292;&#21487;&#20197;&#20174;&#29616;&#23454;&#32593;&#32476;&#30340;&#26799;&#24230;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#21363;&#20351;&#26159;&#22312;&#22823;&#25209;&#37327;(&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#26368;&#22823;&#21487;&#36798;512)&#21644;&#23433;&#20840;&#32858;&#21512;&#30340;&#24773;&#20917;&#19979;&#12290;SEER&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#20351;&#29992;&#31192;&#23494;&#35299;&#30721;&#22120;&#65292;&#22312;&#20849;&#20139;&#27169;&#22411;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#36808;&#21521;&#23454;&#29992;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#30340;&#26377;&#21069;&#36884;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malicious server (MS) attacks have enabled the scaling of data stealing in federated learning to large batch sizes and secure aggregation, settings previously considered private. However, many concerns regarding client-side detectability of MS attacks were raised, questioning their practicality once they are publicly known. In this work, for the first time, we thoroughly study the problem of client-side detectability.We demonstrate that most prior MS attacks, which fundamentally rely on one of two key principles, are detectable by principled client-side checks. Further, we formulate desiderata for practical MS attacks and propose SEER, a novel attack framework that satisfies all desiderata, while stealing user data from gradients of realistic networks, even for large batch sizes (up to 512 in our experiments) and under secure aggregation. The key insight of SEER is the use of a secret decoder, which is jointly trained with the shared model. Our work represents a promising first step to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#38750;&#21442;&#25968;&#30446;&#26631;&#27169;&#22411;&#30340;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#21644;&#36138;&#24515;&#27867;&#20989;&#25945;&#23398;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03007</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Iterative Machine Teaching. (arXiv:2306.03007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#38750;&#21442;&#25968;&#30446;&#26631;&#27169;&#22411;&#30340;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#21644;&#36138;&#24515;&#27867;&#20989;&#25945;&#23398;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;(IMT)&#38382;&#39064;&#12290;&#29616;&#26377;IMT&#31639;&#27861;&#22522;&#20110;&#21442;&#25968;&#21270;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#26222;&#36941;&#30340;&#20219;&#21153;&#8212;&#8212;&#38750;&#21442;&#25968;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;(NIMT)&#65292;&#26088;&#22312;&#20197;&#36845;&#20195;&#26041;&#24335;&#21521;&#23398;&#20064;&#32773;&#25945;&#25480;&#38750;&#21442;&#25968;&#30446;&#26631;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;NIMT&#35270;&#20026;&#19968;&#20010;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#27867;&#20989;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#38543;&#26426;&#21644;&#36138;&#24515;&#27867;&#20989;&#25945;&#23398;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of Iterative Machine Teaching (IMT), where the teacher provides examples to the learner iteratively such that the learner can achieve fast convergence to a target model. However, existing IMT algorithms are solely based on parameterized families of target models. They mainly focus on convergence in the parameter space, resulting in difficulty when the target models are defined to be functions without dependency on parameters. To address such a limitation, we study a more general task -Nonparametric Iterative Machine Teaching (NIMT), which aims to teach nonparametric target models to learners in an iterative fashion. Unlike parametric IMT that merely operates in the parameter space, we cast NIMT as a functional optimization problem in the function space. To solve it, we propose both random and greedy functional teaching algorithms. We obtain the iterative teaching dimension (ITD) of the random teaching algorithm under proper assumptions, which se
&lt;/p&gt;</description></item><item><title>Time Interpret&#26159;&#19968;&#20010;&#22522;&#20110;Captum&#30340;&#27169;&#22411;&#35299;&#37322;&#24211;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#31181;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#21508;&#31181;PyTorch&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.02968</link><description>&lt;p&gt;
Time Interpret: &#19968;&#31181;&#24207;&#21015;&#25968;&#25454;&#21487;&#35299;&#37322;&#24615;&#32479;&#19968;&#27169;&#22411;&#24211;
&lt;/p&gt;
&lt;p&gt;
Time Interpret: a Unified Model Interpretability Library for Time Series. (arXiv:2306.02968v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02968
&lt;/p&gt;
&lt;p&gt;
Time Interpret&#26159;&#19968;&#20010;&#22522;&#20110;Captum&#30340;&#27169;&#22411;&#35299;&#37322;&#24211;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#31181;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#21508;&#31181;PyTorch&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#27454;&#21517;&#20026;$\texttt{time_interpret}$&#30340;&#24211;&#65292;&#23427;&#26159;&#20197;Captum&#20026;&#22522;&#30784;&#35774;&#35745;&#30340;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#35813;&#24211;&#23454;&#29616;&#20102;&#22810;&#31181;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#37322;&#20219;&#20309;Pytorch&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;$\texttt{time_interpret}$&#36824;&#25552;&#20379;&#20102;&#22810;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12289;&#21508;&#31181;PyTorch&#27169;&#22411;&#20197;&#21450;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#35813;&#24211;&#20027;&#35201;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;&#26102;&#38388;&#25968;&#25454;&#30340;&#39044;&#27979;&#65292;&#20294;&#23427;&#30340;&#26576;&#20123;&#32452;&#20214;&#20063;&#26377;&#19981;&#21516;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#35813;&#24211;&#30340;&#22522;&#26412;&#20869;&#23481;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#19982;$\texttt{time_interpret}$&#21516;&#26102;&#24320;&#21457;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce $\texttt{time_interpret}$, a library designed as an extension of Captum, with a specific focus on temporal data. As such, this library implements several feature attribution methods that can be used to explain predictions made by any Pytorch model. $\texttt{time_interpret}$ also provides several synthetic and real world time series datasets, various PyTorch models, as well as a set of methods to evaluate feature attributions. Moreover, while being primarily developed to explain predictions based on temporal data, some of its components have a different application, including for instance methods explaining predictions made by language models. In this paper, we give a general introduction of this library. We also present several previously unpublished feature attribution methods, which have been developed along with $\texttt{time_interpret}$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.02865</link><description>&lt;p&gt;
&#25235;&#20303;&#24847;&#22806;&#25910;&#33719;&#65306;&#22312;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#20215;&#20540;(arXiv:2306.02865v2 [cs.LG]&#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39640;&#36136;&#37327;&#30340; Q &#20540;&#20989;&#25968;&#22312;&#35768;&#22810;&#29616;&#20195;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (RL) &#31639;&#27861;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#35299;&#20915;&#37319;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#21644;&#31163;&#32447;&#23398;&#20064;&#25152;&#23548;&#33268;&#30340;&#20540;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#19982;&#36825;&#31181;&#26222;&#36941;&#35266;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040; Q &#20540;&#22312; RL &#35757;&#32451;&#36807;&#31243;&#30340;&#21518;&#26399;&#23454;&#38469;&#19978;&#34987;&#20302;&#20272;&#20102;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36125;&#23572;&#26364;&#26356;&#26032;&#20013;&#65292;&#24403;&#21069;&#31574;&#30053;&#20351;&#29992;&#27604;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#26356;&#20248;&#30340;&#21160;&#20316;&#26679;&#26412;&#24046;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20010;&#38271;&#26399;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#21487;&#33021;&#38459;&#30861;&#20102;&#31574;&#30053;&#23398;&#20064;&#65292;&#38477;&#20302;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#22312;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#30340;&#21516;&#26102;&#65292;&#32467;&#21512;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#21033;&#29992;&#21644;&#25506;&#32034; (BEE) &#25805;&#20316;&#31526;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21382;&#21490;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#21160;&#20316;&#21644;&#24403;&#21069;&#31574;&#30053;&#29983;&#25104;&#30340;&#21160;&#20316;&#26469;&#26356;&#26032; Q &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D-CLOSE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#20219;&#20309;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20998;&#21106;&#32423;&#21035;&#21644;&#19968;&#31181;&#32452;&#21512;&#23427;&#20204;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#26356;&#23569;&#30340;&#22122;&#38899;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.02744</link><description>&lt;p&gt;
&#36808;&#21521;&#26356;&#22909;&#30340;&#30446;&#26631;&#26816;&#27979;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards Better Explanations for Object Detection. (arXiv:2306.02744v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D-CLOSE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#20219;&#20309;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20998;&#21106;&#32423;&#21035;&#21644;&#19968;&#31181;&#32452;&#21512;&#23427;&#20204;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#26356;&#23569;&#30340;&#22122;&#38899;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#23427;&#20204;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#30340;&#20351;&#29992;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#20351;&#35299;&#37322;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#21644;&#20915;&#31574;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#21644;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#37322;&#20998;&#31867;&#20219;&#21153;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D-CLOSE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#20219;&#20309;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#23494;&#20999;&#36319;&#36394;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#19978;&#20351;&#29992;&#20102;&#22810;&#20010;&#20998;&#21106;&#32423;&#21035;&#21644;&#19968;&#31181;&#32452;&#21512;&#23427;&#20204;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;YOLOX&#27169;&#22411;&#22312;MS-COCO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;D-RISE&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#26356;&#23569;&#30340;&#22122;&#38899;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Artificial Intelligence (AI) technology have promoted their use in almost every field. The growing complexity of deep neural networks (DNNs) makes it increasingly difficult and important to explain the inner workings and decisions of the network. However, most current techniques for explaining DNNs focus mainly on interpreting classification tasks. This paper proposes a method to explain the decision for any object detection model called D-CLOSE. To closely track the model's behavior, we used multiple levels of segmentation on the image and a process to combine them. We performed tests on the MS-COCO dataset with the YOLOX model, which shows that our method outperforms D-RISE and can give a better quality and less noise explanation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26657;&#20934;&#21490;&#22612;&#20811;&#20271;&#26684;&#21338;&#24328;&#65288;CSG&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#26681;&#25454;&#26657;&#20934;&#39044;&#27979;&#36827;&#34892;&#26368;&#20339;&#21709;&#24212;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#26657;&#20934;&#27010;&#24565;&#65292;&#25552;&#20379;&#31934;&#32454;&#30340;&#20219;&#20309;&#26102;&#20505;&#26657;&#20934;&#20445;&#35777;&#12290;&#22312;&#26377;&#38480;CSG&#20013;&#65292;&#20027;&#20307;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.02704</link><description>&lt;p&gt;
&#26657;&#20934;&#21490;&#22612;&#20811;&#20271;&#26684;&#21338;&#24328;&#65306;&#23398;&#20064;&#23545;&#25239;&#26657;&#20934;&#26234;&#33021;&#20307;&#30340;&#26368;&#20248;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Calibrated Stackelberg Games: Learning Optimal Commitments Against Calibrated Agents. (arXiv:2306.02704v1 [cs.GT] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26657;&#20934;&#21490;&#22612;&#20811;&#20271;&#26684;&#21338;&#24328;&#65288;CSG&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#26681;&#25454;&#26657;&#20934;&#39044;&#27979;&#36827;&#34892;&#26368;&#20339;&#21709;&#24212;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#26657;&#20934;&#27010;&#24565;&#65292;&#25552;&#20379;&#31934;&#32454;&#30340;&#20219;&#20309;&#26102;&#20505;&#26657;&#20934;&#20445;&#35777;&#12290;&#22312;&#26377;&#38480;CSG&#20013;&#65292;&#20027;&#20307;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26631;&#20934;&#21490;&#22612;&#20811;&#20271;&#26684;&#21338;&#24328;&#65288;SG&#65289;&#26694;&#26550;&#30340;&#19968;&#31181;&#25512;&#24191;&#65306;&#26657;&#20934;&#21490;&#22612;&#20811;&#20271;&#26684;&#21338;&#24328;&#65288;CSG&#65289;&#12290;&#22312;CSG&#20013;&#65292;&#19968;&#20010;&#20027;&#20307;&#19982;&#19968;&#20010;&#26234;&#33021;&#20307;&#21453;&#22797;&#20132;&#20114;&#65292;&#21518;&#32773;&#19981;&#20687;&#26631;&#20934;SG&#19968;&#26679;&#30452;&#25509;&#35775;&#38382;&#20027;&#20307;&#30340;&#21160;&#20316;&#65292;&#32780;&#26159;&#23545;&#20854;&#36827;&#34892;&#26657;&#20934;&#39044;&#27979;&#65292;&#20197;&#36798;&#21040;&#26368;&#20339;&#21709;&#24212;&#12290;CSG&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24314;&#27169;&#24037;&#20855;&#65292;&#36229;&#36234;&#20102;&#20551;&#23450;&#20195;&#29702;&#20351;&#29992;&#29305;&#23450;&#31639;&#27861;&#36827;&#34892;&#25112;&#30053;&#20132;&#20114;&#30340;&#20570;&#27861;&#65292;&#22240;&#27492;&#26356;&#21152;&#40065;&#26834;&#22320;&#24212;&#23545;&#20102;SG&#26368;&#21021;&#26088;&#22312;&#25429;&#25417;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#38500;&#20102;CSG&#22806;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#26356;&#24378;&#30340;&#26657;&#20934;&#27010;&#24565;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#26657;&#20934;&#65292;&#21487;&#38024;&#23545;&#25932;&#23545;&#24207;&#21015;&#25552;&#20379;&#31934;&#32454;&#30340;&#20219;&#20309;&#26102;&#20505;&#26657;&#20934;&#20445;&#35777;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#33719;&#24471;&#33258;&#36866;&#24212;&#26657;&#20934;&#31639;&#27861;&#30340;&#19968;&#33324;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19987;&#38376;&#29992;&#20110;&#26377;&#38480;CSG&#12290;&#22312;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;CSG&#20013;&#65292;&#20027;&#20307;&#21487;&#20197;&#33719;&#24471;&#25910;&#25947;&#20110;&#26368;&#20248;&#35299;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a generalization of the standard Stackelberg Games (SGs) framework: Calibrated Stackelberg Games (CSGs). In CSGs, a principal repeatedly interacts with an agent who (contrary to standard SGs) does not have direct access to the principal's action but instead best-responds to calibrated forecasts about it. CSG is a powerful modeling tool that goes beyond assuming that agents use ad hoc and highly specified algorithms for interacting in strategic settings and thus more robustly addresses real-life applications that SGs were originally intended to capture. Along with CSGs, we also introduce a stronger notion of calibration, termed adaptive calibration, that provides fine-grained any-time calibration guarantees against adversarial sequences. We give a general approach for obtaining adaptive calibration algorithms and specialize them for finite CSGs. In our main technical result, we show that in CSGs, the principal can achieve utility that converges to the optimum
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#30111;&#30142;&#39044;&#27979;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#24067;&#38534;&#36842;&#30111;&#30142;&#26102;&#31354;&#21160;&#24577;&#65292;&#20026;&#30111;&#30142;&#38450;&#27835;&#21644;&#24178;&#39044;&#35774;&#35745;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.02685</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#24067;&#38534;&#36842;&#30111;&#30142;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Predicting malaria dynamics in Burundi using deep Learning Models. (arXiv:2306.02685v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02685
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#30111;&#30142;&#39044;&#27979;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#24067;&#38534;&#36842;&#30111;&#30142;&#26102;&#31354;&#21160;&#24577;&#65292;&#20026;&#30111;&#30142;&#38450;&#27835;&#21644;&#24178;&#39044;&#35774;&#35745;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30111;&#30142;&#32487;&#32493;&#22312;&#38750;&#27954;&#22823;&#38470;&#29305;&#21035;&#26159;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#22320;&#21306;&#25104;&#20026;&#20027;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#22312;&#21162;&#21147;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#24067;&#38534;&#36842;&#65292;&#30111;&#30142;&#26159;&#20027;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#24067;&#38534;&#36842;&#30111;&#30142;&#39044;&#27979;&#27169;&#22411;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#25105;&#20204;&#24314;&#31435;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#20272;&#35745;&#24067;&#38534;&#36842;&#30340;&#30111;&#30142;&#30149;&#20363;&#12290;&#21033;&#29992;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#22240;&#32032;&#22914;&#28201;&#24230;&#12289;&#38477;&#38632;&#21644;&#30456;&#23545;&#28287;&#24230;&#20197;&#21450;&#30111;&#30142;&#21382;&#21490;&#25968;&#25454;&#21644;&#20154;&#21475;&#25968;&#25454;&#65292;&#37319;&#29992;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;LSTM&#27169;&#22411;&#65292;&#23545;&#30465;&#32423;&#21644;&#22269;&#23478;&#33539;&#22260;&#20869;&#30340;&#30111;&#30142;&#24773;&#20917;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;&#26681;&#25454;&#27169;&#22411;&#32467;&#26524;&#65292;&#21487;&#20197;&#30830;&#23450;&#22312;&#19981;&#21516;&#21442;&#25968;&#35843;&#25972;&#19979;&#65292;&#22269;&#23478;&#32423;&#21035;&#30340;&#26368;&#20302;&#21644;&#26368;&#39640;&#39044;&#26399;&#30111;&#30142;&#30149;&#20363;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malaria continues to be a major public health problem on the African continent, particularly in Sub-Saharan Africa. Nonetheless, efforts are ongoing, and significant progress has been made. In Burundi, malaria is among the main public health concerns. In the literature, there are limited prediction models for Burundi. We know that such tools are much needed for interventions design. In our study, we built machine-learning based models to estimates malaria cases in Burundi. The forecast of malaria cases was carried out at province level and national scale as well. Long short term memory (LSTM) model, a type of deep learning model has been used to achieve best results using climate-change related factors such as temperature, rainfal, and relative humidity, together with malaria historical data and human population. With this model, the results showed that at country level different tuning of parameters can be used in order to determine the minimum and maximum expected malaria cases. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Polyak-{\L}ojasiewicz&#26465;&#20214;&#30340;&#21452;&#23618;&#23398;&#20064;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;&#65292;&#21363;GALET&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;$\tilde{\cal O}(\epsilon^{-1})$&#36845;&#20195;&#27425;&#25968;&#20869;&#23454;&#29616;&#38382;&#39064;&#30340;$\epsilon$-&#38745;&#24577;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.02422</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Polyak-{\L}ojasiewicz&#26465;&#20214;&#30340;&#21452;&#23618;&#23398;&#20064;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Generalized Alternating Method for Bilevel Learning under the Polyak-{\L}ojasiewicz Condition. (arXiv:2306.02422v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Polyak-{\L}ojasiewicz&#26465;&#20214;&#30340;&#21452;&#23618;&#23398;&#20064;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;&#65292;&#21363;GALET&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;$\tilde{\cal O}(\epsilon^{-1})$&#36845;&#20195;&#27425;&#25968;&#20869;&#23454;&#29616;&#38382;&#39064;&#30340;$\epsilon$-&#38745;&#24577;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#36817;&#24180;&#26469;&#22240;&#20854;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#26032;&#20852;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#32780;&#37325;&#26032;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20110;&#20855;&#26377;&#24378;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#38382;&#39064;&#65292;&#31616;&#21333;&#30340;&#20132;&#26367;&#65288;&#38544;&#24335;&#65289;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#21333;&#23618;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36229;&#20986;&#27492;&#22522;&#26412;&#35774;&#32622;&#30340;&#21452;&#23618;&#38382;&#39064;&#65292;&#23578;&#19981;&#28165;&#26970;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#35813;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28385;&#36275;Polyak-{\L}ojasiewicz (PL)&#26465;&#20214;&#30340;&#38750;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#20248;&#21270;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;&#65288;GALET&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#25152;&#32771;&#34385;&#30340;&#21452;&#23618;&#38382;&#39064;&#30340;&#19968;&#20010;&#38745;&#24577;&#24230;&#37327;&#65292;&#23427;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;&#24230;&#37327;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;GALET&#22312;$\tilde{\cal O}(\epsilon^{-1})$&#36845;&#20195;&#27425;&#25968;&#20869;&#23454;&#29616;&#20102;&#25152;&#32771;&#34385;&#38382;&#39064;&#30340;$\epsilon$-&#38745;&#24577;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization has recently regained interest owing to its applications in emerging machine learning fields such as hyperparameter optimization, meta-learning, and reinforcement learning. Recent results have shown that simple alternating (implicit) gradient-based algorithms can achieve the same convergence rate of single-level gradient descent (GD) for bilevel problems with a strongly convex lower-level objective. However, it remains unclear whether this result can be generalized to bilevel problems beyond this basic setting. In this paper, we propose a Generalized ALternating mEthod for bilevel opTimization (GALET) with a nonconvex lower-level objective that satisfies the Polyak-{\L}ojasiewicz (PL) condition. We first introduce a stationary metric for the considered bilevel problems, which generalizes the existing metric. We then establish that GALET achieves an $\epsilon$-stationary metric for the considered problem within $\tilde{\cal O}(\epsilon^{-1})$ iterations, which match
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#30340;&#24341;&#20837;&#65292;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#29289;&#20307;&#24212;&#26144;&#23556;&#21040;&#19981;&#21516;&#27133;&#20301;&#30340;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02204</link><description>&lt;p&gt;
&#24490;&#29615;&#19968;&#33268;&#24615;&#39537;&#21160;&#30340;&#29289;&#20307;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cycle Consistency Driven Object Discovery. (arXiv:2306.02204v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#30340;&#24341;&#20837;&#65292;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#29289;&#20307;&#24212;&#26144;&#23556;&#21040;&#19981;&#21516;&#27133;&#20301;&#30340;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#30340;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#26550;&#26500;&#20808;&#39564;&#25110;&#36741;&#21161;&#20449;&#24687;&#65288;&#20363;&#22914;&#28145;&#24230;&#22270;&#25110;&#27969;&#22330;&#22270;&#65289;&#26469;&#25506;&#32034;&#22522;&#20110;&#27133;&#20301;&#30340;&#26041;&#27861;&#65292;&#20197;&#34920;&#31034;&#23545;&#35937;&#20026;&#31216;&#20026;&#8220;&#27133;&#20301;&#8221;&#25110;&#8220;&#23545;&#35937;&#25991;&#20214;&#8221;&#30340;&#22266;&#23450;&#22823;&#23567;&#30340;&#21521;&#37327;&#65292;&#20174;&#32780;&#20419;&#36827;&#29289;&#20307;&#21457;&#29616;&#12290; &#28982;&#32780;&#65292;&#20381;&#36182;&#20110;&#26550;&#26500;&#20808;&#39564;&#20250;&#24341;&#20837;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#25165;&#33021;&#35782;&#21035;&#27491;&#30830;&#30340;&#23545;&#35937;&#12290; &#21516;&#26679;&#65292;&#20381;&#36182;&#36741;&#21161;&#20449;&#24687;&#30340;&#26041;&#27861;&#20063;&#19981;&#22815;&#20248;&#36234;&#65292;&#22240;&#20026;&#36825;&#31181;&#20449;&#24687;&#36890;&#24120;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#24773;&#20917;&#19979;&#19981;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#23545;&#35937;&#24212;&#26144;&#23556;&#21040;&#19968;&#20010;&#19981;&#21516;&#27133;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#24418;&#24335;&#21270;&#36825;&#20010;&#32422;&#26463;&#65292;&#31216;&#20043;&#20026;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290; &#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#29289;&#20307;&#21457;&#29616;&#21644;&#23569;&#26679;&#26412;&#29289;&#20307;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches have explored slot-based methods utilizing architectural priors or auxiliary information such as depth maps or flow maps to facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. However, reliance on architectural priors introduces unreliability and requires meticulous engineering to identify the correct objects. Likewise, methods relying on auxiliary information are suboptimal as such information is often unavailable for most natural scenes. To address these limitations, we propose a method that explicitly optimizes the constraint that each object in a scene should be mapped to a distinct slot. We formalize this constraint by introducing consistency objectives which are cyclic in nature. We refer to them as the \textit{cycle-consistency} objectives. By applying these con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;YNN&#30340;&#26041;&#27861;&#65292;&#23558;&#21516;&#19968;&#32423;&#21035;&#30340;ANN&#33410;&#28857;&#36830;&#25509;&#22312;&#19968;&#36215;&#24418;&#25104;&#31070;&#32463;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#26222;&#36890;ANN&#26080;&#27861;&#20849;&#20139;&#20449;&#24687;&#30340;&#32570;&#38519;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#24687;&#20256;&#36755;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02157</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;Yoked&#31070;&#32463;&#32593;&#32476;&#20197;&#25913;&#36827;ANN&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Transforming to Yoked Neural Networks to Improve ANN Structure. (arXiv:2306.02157v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;YNN&#30340;&#26041;&#27861;&#65292;&#23558;&#21516;&#19968;&#32423;&#21035;&#30340;ANN&#33410;&#28857;&#36830;&#25509;&#22312;&#19968;&#36215;&#24418;&#25104;&#31070;&#32463;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#26222;&#36890;ANN&#26080;&#27861;&#20849;&#20139;&#20449;&#24687;&#30340;&#32570;&#38519;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#24687;&#20256;&#36755;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#24050;&#32463;&#23384;&#22312;&#30340;&#32463;&#20856;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#37117;&#34987;&#35774;&#35745;&#25104;&#26641;&#24418;&#32467;&#26500;&#20197;&#27169;&#25311;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#26641;&#24418;&#32467;&#26500;&#30340;&#36830;&#25509;&#19981;&#36275;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#12290;&#21516;&#19968;&#32423;&#21035;&#30340;&#33410;&#28857;&#19981;&#33021;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#21363;&#36825;&#20123;&#31070;&#32463;&#20803;&#19981;&#33021;&#30456;&#20114;&#20849;&#20139;&#20449;&#24687;&#65292;&#36825;&#26159;ANN&#30340;&#19968;&#20010;&#37325;&#22823;&#32570;&#38519;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#20026;&#21516;&#19968;&#32423;&#21035;&#30340;&#33410;&#28857;&#24314;&#31435;&#21452;&#21521;&#23436;&#20840;&#22270;&#65292;&#23558;&#21516;&#19968;&#32423;&#21035;&#30340;&#33410;&#28857;&#36830;&#25509;&#21040;&#19968;&#36215;&#24418;&#25104;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#25226;&#25105;&#20204;&#30340;&#27169;&#22411;&#31216;&#20026;YNN&#12290;YNN&#26174;&#33879;&#20419;&#36827;&#20102;&#20449;&#24687;&#20256;&#36755;&#65292;&#26126;&#26174;&#26377;&#21161;&#20110;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;YNN&#21487;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#20854;&#20182;ANN&#26041;&#27861;&#26377;&#30528;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing classical artificial neural networks (ANN) are designed as a tree structure to imitate neural networks. In this paper, we argue that the connectivity of a tree is not sufficient to characterize a neural network. The nodes of the same level of a tree cannot be connected with each other, i.e., these neural unit cannot share information with each other, which is a major drawback of ANN. Although ANN has been significantly improved in recent years to more complex structures, such as the directed acyclic graph (DAG), these methods also have unidirectional and acyclic bias for ANN. In this paper, we propose a method to build a bidirectional complete graph for the nodes in the same level of an ANN, which yokes the nodes of the same level to formulate a neural module. We call our model as YNN in short. YNN promotes the information transfer significantly which obviously helps in improving the performance of the method. Our YNN can imitate neural networks much better compared with 
&lt;/p&gt;</description></item><item><title>MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.02069</link><description>&lt;p&gt;
MultiLegalPile&#65306;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile: A 689GB Multilingual Legal Corpus. (arXiv:2306.02069v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02069
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#27490;&#65292;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#27861;&#24459;&#65289;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#32780;&#19988;&#32463;&#24120;&#20165;&#38480;&#20110;&#33521;&#35821;&#12290;&#25105;&#20204;&#25972;&#29702;&#24182;&#21457;&#24067;&#20102;MultiLegalPile&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;689GB&#35821;&#26009;&#24211;&#12290;MultiLegalPile&#35821;&#26009;&#24211;&#21253;&#25324;&#21508;&#31181;&#35768;&#21487;&#35777;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#65292;&#23545;&#20110;Eurlex Resources&#21644;Legal mC4&#23376;&#38598;&#25317;&#26377;&#26356;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;RoBERTa&#27169;&#22411;&#21644;&#19968;&#20010;&#22810;&#35821;&#35328;Longformer&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#20998;&#21035;&#22312;&#27599;&#31181;&#29305;&#23450;&#35821;&#35328;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;24&#20010;&#21333;&#35821;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;LEXTREME&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;LexGLUE&#19978;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;LEXTREME&#19978;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;(SotA)&#65292;&#33521;&#35821;&#27169;&#22411;&#21017;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#20195;&#30721;&#20840;&#37096;&#37322;&#25918;&#22312;&#26368;&#24320;&#25918;&#30340;&#35768;&#21487;&#35777;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, there are few datasets available for specialized critical domains such as law and the available ones are often only for the English language. We curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. The MultiLegalPile corpus, which includes diverse legal data sources with varying licenses, allows for pretraining NLP models under fair use, with more permissive licenses for the Eurlex Resources and Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, the trained models, and all of the code under the most open possible licenses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#27867;&#21270;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#23545;&#21487;&#35777;&#26126;&#21160;&#24577;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#30340;&#35299;&#37322;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#22810;&#27169;&#24577;&#34701;&#21512;&#65288;QMF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02050</link><description>&lt;p&gt;
&#23545;&#20302;&#36136;&#37327;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21487;&#35777;&#26126;&#21160;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Provable Dynamic Fusion for Low-Quality Multimodal Data. (arXiv:2306.02050v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#27867;&#21270;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#23545;&#21487;&#35777;&#26126;&#21160;&#24577;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#30340;&#35299;&#37322;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#22810;&#27169;&#24577;&#34701;&#21512;&#65288;QMF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#22256;&#38590;&#22312;&#20110;&#31934;&#30830;&#25429;&#25417;&#36328;&#27169;&#24577;&#30456;&#20851;&#24615;&#21644;&#28789;&#27963;&#36827;&#34892;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#20026;&#20102;&#20805;&#20998;&#37322;&#25918;&#27599;&#20010;&#27169;&#24577;&#30340;&#20215;&#20540;&#24182;&#20943;&#36731;&#20302;&#36136;&#37327;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#21160;&#24577;&#22810;&#27169;&#24577;&#34701;&#21512;&#25104;&#20026;&#26377;&#21069;&#36884;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#35813;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#29702;&#35770;&#35777;&#26126;&#12290;&#26412;&#25991;&#20174;&#27867;&#21270;&#35282;&#24230;&#30340;&#26368;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#20986;&#21457;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25509;&#30528;&#25581;&#31034;&#65292;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#35299;&#20915;&#26041;&#27861;&#21487;&#20197;&#33258;&#28982;&#22320;&#23454;&#29616;&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#36136;&#37327;&#24863;&#30693;&#22810;&#27169;&#24577;&#34701;&#21512;&#65288;QMF&#65289;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherent challenge of multimodal fusion is to precisely capture the cross-modal correlation and flexibly conduct cross-modal interaction. To fully release the value of each modality and mitigate the influence of low-quality multimodal data, dynamic multimodal fusion emerges as a promising learning paradigm. Despite its widespread use, theoretical justifications in this field are still notably lacking. Can we design a provably robust multimodal fusion method? This paper provides theoretical understandings to answer this question under a most popular multimodal fusion framework from the generalization perspective. We proceed to reveal that several uncertainty estimation solutions are naturally available to achieve robust multimodal fusion. Then a novel multimodal fusion framework termed Quality-aware Multimodal Fusion (QMF) is proposed, which can improve the performance in terms of classification accuracy and model robustness. Extensive experimental results on multiple benchmarks can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;GAD-NR&#65292;&#19982;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;GAD-NR&#37319;&#29992;&#37051;&#22495;&#37325;&#26500;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#26356;&#22797;&#26434;&#38750;&#32858;&#31867;&#30340;&#32467;&#26500;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2306.01951</link><description>&lt;p&gt;
GAD-NR: &#36890;&#36807;&#37051;&#22495;&#37325;&#26500;&#23454;&#29616;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction. (arXiv:2306.01951v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;GAD-NR&#65292;&#19982;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;GAD-NR&#37319;&#29992;&#37051;&#22495;&#37325;&#26500;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#26356;&#22797;&#26434;&#38750;&#32858;&#31867;&#30340;&#32467;&#26500;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#12289;&#27450;&#35784;&#26816;&#27979;&#12289;&#31038;&#20132;&#23186;&#20307;&#22403;&#22334;&#26816;&#27979;&#21644;&#20854;&#20182;&#21508;&#31181;&#39046;&#22495;&#20013;&#26377;&#24212;&#29992;&#12290;GAD&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAEs&#65289;&#65292;&#23427;&#23558;&#22270;&#24418;&#25968;&#25454;&#32534;&#30721;&#25104;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#34920;&#31034;&#26469;&#35780;&#20272;&#22270;&#24418;&#30340;&#37325;&#26500;&#36136;&#37327;&#65292;&#20197;&#35782;&#21035;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#20027;&#35201;&#38024;&#23545;&#30452;&#25509;&#38142;&#25509;&#37325;&#26500;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36830;&#25509;&#22270;&#20013;&#30340;&#33410;&#28857;&#34987;&#32858;&#31867;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#25797;&#38271;&#26816;&#27979;&#32858;&#31867;&#22411;&#32467;&#26500;&#24322;&#24120;&#65292;&#20294;&#23545;&#19981;&#31526;&#21512;&#32858;&#31867;&#30340;&#26356;&#22797;&#26434;&#30340;&#32467;&#26500;&#24322;&#24120;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;GAD-NR&#65292;&#23427;&#26159;GAE&#30340;&#19968;&#20010;&#26032;&#21464;&#20307;&#65292;&#34701;&#21512;&#37051;&#22495;&#37325;&#26500;&#36827;&#34892;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#12290;GAD-NR&#30340;&#30446;&#26631;&#26159;&#37325;&#26500;&#33410;&#28857;&#30340;&#25972;&#20010;&#37051;&#22495;&#65292;&#28085;&#30422;&#26412;&#22320;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes within graphs, finding applications in network security, fraud detection, social media spam detection, and various other domains. A common method for GAD is Graph Auto-Encoders (GAEs), which encode graph data into node representations and identify anomalies by assessing the reconstruction quality of the graphs based on these representations. However, existing GAE models are primarily optimized for direct link reconstruction, resulting in nodes connected in the graph being clustered in the latent space. As a result, they excel at detecting cluster-type structural anomalies but struggle with more complex structural anomalies that do not conform to clusters. To address this limitation, we propose a novel solution called GAD-NR, a new variant of GAE that incorporates neighborhood reconstruction for graph anomaly detection. GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the local structu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.01788</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Responsible Design Patterns for Machine Learning Pipelines. (arXiv:2306.01788v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36947;&#24503;&#23454;&#36341;&#25972;&#21512;&#21040;&#20154;&#24037;&#26234;&#33021;(AI)&#24320;&#21457;&#36807;&#31243;&#20013;&#23545;&#20110;&#30830;&#20445;AI&#30340;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36127;&#36131;&#20219;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;AI&#20262;&#29702;&#28041;&#21450;&#23558;&#20262;&#29702;&#21407;&#21017;&#24212;&#29992;&#20110;AI&#31995;&#32479;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#12290;&#36825;&#23545;&#20110;&#20943;&#36731;&#19982;AI&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#20260;&#23475;&#65288;&#22914;&#31639;&#27861;&#20559;&#35265;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#65288;RDPs&#65289;&#23545;&#20110;&#30830;&#20445;&#20262;&#29702;&#21644;&#20844;&#24179;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;RDPs&#32435;&#20837;ML&#27969;&#31243;&#20013;&#65292;&#20197;&#20943;&#36731;&#39118;&#38505;&#24182;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#36890;&#36807;&#23545;AI&#20262;&#29702;&#21644;&#25968;&#25454;&#31649;&#29702;&#19987;&#23478;&#30340;&#35843;&#26597;&#30830;&#23450;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#21453;&#39304;&#30340;&#23454;&#38469;&#24773;&#20917;&#36827;&#34892;&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating ethical practices into the AI development process for artificial intelligence (AI) is essential to ensure safe, fair, and responsible operation. AI ethics involves applying ethical principles to the entire life cycle of AI systems. This is essential to mitigate potential risks and harms associated with AI, such as algorithm biases. To achieve this goal, responsible design patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee ethical and fair outcomes. In this paper, we propose a comprehensive framework incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical development of AI systems. Our framework comprises new responsible AI design patterns for ML pipelines identified through a survey of AI ethics and data management experts and validated through real-world scenarios with expert feedback. The framework guides AI developers, data scientists, and policy-makers to implement ethical practices in AI development and deploy responsibl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26415;&#21518;&#24182;&#21457;&#30151;&#65292;&#21253;&#25324;&#32958;&#34928;&#12289;&#32954;&#37096;&#38382;&#39064;&#21644;&#20303;&#38498;&#27515;&#20129;&#65292;&#24182;&#19988;&#34920;&#29616;&#24456;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.00698</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#39044;&#27979;&#26415;&#21518;&#32958;&#21151;&#33021;&#21644;&#32954;&#21151;&#33021;&#24182;&#21457;&#30151;
&lt;/p&gt;
&lt;p&gt;
Prediction of Post-Operative Renal and Pulmonary Complications Using Transformers. (arXiv:2306.00698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26415;&#21518;&#24182;&#21457;&#30151;&#65292;&#21253;&#25324;&#32958;&#34928;&#12289;&#32954;&#37096;&#38382;&#39064;&#21644;&#20303;&#38498;&#27515;&#20129;&#65292;&#24182;&#19988;&#34920;&#29616;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26415;&#21518;&#24182;&#21457;&#30151;&#22312;&#21307;&#30103;&#34892;&#19994;&#20013;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#21307;&#30103;&#36153;&#29992;&#19978;&#21319;&#21644;&#20303;&#38498;&#26102;&#38388;&#24310;&#38271;&#65292;&#23569;&#25968;&#24773;&#20917;&#19979;&#29978;&#33267;&#20250;&#23548;&#33268;&#24739;&#32773;&#27515;&#20129;&#12290;&#20026;&#20102;&#25913;&#21892;&#24739;&#32773;&#30340;&#39044;&#21518;&#21644;&#38477;&#20302;&#21307;&#30103;&#36153;&#29992;&#65292;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#21830;&#20381;&#36182;&#20110;&#21508;&#31181;&#22260;&#25163;&#26415;&#26399;&#39118;&#38505;&#35780;&#20998;&#26469;&#25351;&#23548;&#20020;&#24202;&#20915;&#31574;&#21644;&#20248;&#20808;&#22788;&#29702;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#26415;&#21518;&#24182;&#21457;&#30151;&#21644;&#27515;&#20129;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26415;&#20013;&#40635;&#37257;&#31649;&#29702;&#25968;&#25454;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#26415;&#21518;&#24613;&#24615;&#32958;&#34928;&#31469;&#12289;&#26415;&#21518;&#32954;&#37096;&#24182;&#21457;&#30151;&#21644;&#26415;&#21518;&#20303;&#38498;&#27515;&#20129;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21253;&#25324;Gradient Boosting&#21644;LightGBM&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Postoperative complications pose a significant challenge in the healthcare industry, resulting in elevated healthcare expenses and prolonged hospital stays, and in rare instances, patient mortality. To improve patient outcomes and reduce healthcare costs, healthcare providers rely on various perioperative risk scores to guide clinical decisions and prioritize care. In recent years, machine learning techniques have shown promise in predicting postoperative complications and fatality, with deep learning models achieving remarkable success in healthcare applications. However, research on the application of deep learning models to intra-operative anesthesia management data is limited. In this paper, we evaluate the performance of transformer-based models in predicting postoperative acute renal failure, postoperative pulmonary complications, and postoperative in-hospital mortality. We compare our method's performance with state-of-the-art tabular data prediction models, including gradient b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;GMRL&#65292;&#23427;&#21487;&#20197;&#21333;&#29420;&#27169;&#25311;&#26102;&#38388;&#12289;&#20301;&#32622;&#21644;&#28304;&#21464;&#37327;&#20013;&#25152;&#26263;&#31034;&#30340;&#27599;&#20010;&#24322;&#26500;&#24615;&#32452;&#20214;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00390</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#34920;&#31034;&#29992;&#20110;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Gaussian Mixture Representations for Tensor Time Series Forecasting. (arXiv:2306.00390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;GMRL&#65292;&#23427;&#21487;&#20197;&#21333;&#29420;&#27169;&#25311;&#26102;&#38388;&#12289;&#20301;&#32622;&#21644;&#28304;&#21464;&#37327;&#20013;&#25152;&#26263;&#31034;&#30340;&#27599;&#20010;&#24322;&#26500;&#24615;&#32452;&#20214;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;TTS&#65289;&#25968;&#25454;&#26159;&#39640;&#32500;&#31354;&#38388;&#20013;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#33324;&#21270;&#65292;&#26159;&#29616;&#23454;&#22330;&#26223;&#20013;&#19975;&#33021;&#30340;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22810;&#28304;&#26102;&#31354;&#25968;&#25454;&#30340;&#30417;&#27979;&#31995;&#32479;&#20013;&#65288;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#31354;&#27668;&#27745;&#26579;&#29289;&#65289;&#12290;&#19982;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30456;&#27604;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#30340;&#24773;&#20917;&#19979;&#65292;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#20184;&#20986;&#30340;&#21162;&#21147;&#36739;&#23569;&#12290;&#30001;&#20110;&#20854;&#39640;&#32500;&#21644;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#27491;&#30830;&#22788;&#29702;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#26159;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;TTS&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#21333;&#29420;&#27169;&#25311;&#26102;&#38388;&#12289;&#20301;&#32622;&#21644;&#28304;&#21464;&#37327;&#20013;&#25152;&#26263;&#31034;&#30340;&#27599;&#20010;&#24322;&#26500;&#24615;&#32452;&#20214;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#21629;&#21517;&#20026;GMRL&#65292;&#21363;&#39640;&#26031;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#20004;&#20010;&#23454;&#38469;TTS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor time series (TTS) data, a generalization of one-dimensional time series on a high-dimensional space, is ubiquitous in real-world scenarios, especially in monitoring systems involving multi-source spatio-temporal data (e.g., transportation demands and air pollutants). Compared to modeling time series or multivariate time series, which has received much attention and achieved tremendous progress in recent years, tensor time series has been paid less effort. Properly coping with the tensor time series is a much more challenging task, due to its high-dimensional and complex inner structure. In this paper, we develop a novel TTS forecasting framework, which seeks to individually model each heterogeneity component implied in the time, the location, and the source variables. We name this framework as GMRL, short for Gaussian Mixture Representation Learning. Experiment results on two real-world TTS datasets verify the superiority of our approach compared with the state-of-the-art baseli
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#22312;CIDEr&#25351;&#26631;&#19978;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.00301</link><description>&lt;p&gt;
CapText: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#20869;&#23481;&#21644;&#25551;&#36848;&#29983;&#25104;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
CapText: Large Language Model-based Caption Generation From Image Context and Description. (arXiv:2306.00301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00301
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#22312;CIDEr&#25351;&#26631;&#19978;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22270;&#29255;&#23383;&#24149;&#24448;&#24448;&#26159;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#25552;&#20379;&#26377;&#20851;&#22270;&#20687;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#32780;&#27169;&#22411;&#24448;&#24448;&#29983;&#25104;&#25551;&#36848;&#22270;&#20687;&#35270;&#35273;&#29305;&#24449;&#30340;&#8220;&#25551;&#36848;&#8221;&#12290;&#22312;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#24050;&#25506;&#32034;&#20102;&#20351;&#29992;&#27169;&#22411;&#22312;&#25552;&#20379;&#23545;&#24212;&#30340;&#25551;&#36848;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#23383;&#24149;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; CIDEr &#25351;&#26631;&#19978;&#32988;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#65292;&#22914; OSCAR-VinVL&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep-learning models have been shown to perform well on image-to-text datasets, it is difficult to use them in practice for captioning images. This is because \textit{captions} traditionally tend to be context-dependent and offer complementary information about an image, while models tend to produce \textit{descriptions} that describe the visual features of the image. Prior research in caption generation has explored the use of models that generate captions when provided with the images alongside their respective descriptions or contexts. We propose and evaluate a new approach, which leverages existing large language models to generate captions from textual descriptions and context alone, without ever processing the image directly. We demonstrate that after fine-tuning, our approach outperforms current state-of-the-art image-text alignment models like OSCAR-VinVL on this task on the CIDEr metric.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;Mixup&#35757;&#32451;&#20855;&#26377;&#21487;&#35777;&#23454;&#30340;&#30410;&#22788;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#22312;&#26356;&#21487;&#20998;&#31163;&#25968;&#25454;&#20998;&#24067;&#20013;&#23547;&#25214;&#26368;&#20339;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.00267</link><description>&lt;p&gt;
Mixup&#22312;&#23547;&#25214;&#26368;&#20339;&#20915;&#31574;&#36793;&#30028;&#20013;&#30340;&#21487;&#35777;&#23454;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Provable Benefit of Mixup for Finding Optimal Decision Boundaries. (arXiv:2306.00267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;Mixup&#35757;&#32451;&#20855;&#26377;&#21487;&#35777;&#23454;&#30340;&#30410;&#22788;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#22312;&#26356;&#21487;&#20998;&#31163;&#25968;&#25454;&#20998;&#24067;&#20013;&#23547;&#25214;&#26368;&#20339;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20687;Mixup&#36825;&#26679;&#30340;&#25104;&#23545;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22914;&#20309;&#24433;&#21709;&#22312;&#20108;&#20803;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#20013;&#23547;&#25214;&#26368;&#20339;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#38024;&#23545;&#19968;&#31867;&#20855;&#26377;&#21487;&#20998;&#31163;&#24120;&#25968;$\kappa$&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35757;&#32451;&#25439;&#22833;&#26368;&#20248;&#20998;&#31867;&#22120;&#19982;&#27979;&#35797;&#20934;&#30830;&#29575;&#26368;&#20248;&#20998;&#31867;&#22120;&#65288;&#21363;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;&#65289;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#23545;&#20110;&#27809;&#26377;&#22686;&#24378;&#30340;&#26222;&#36890;&#35757;&#32451;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#21487;&#20998;&#31163;&#24615;&#30340;&#35781;&#21650;&#12290;&#38543;&#30528;&#25105;&#20204;&#22686;&#21152;$\kappa$&#20351;&#25968;&#25454;&#20998;&#24067;&#26356;&#21152;&#21487;&#20998;&#31163;&#65292;&#26222;&#36890;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20250;&#22312;$\kappa$&#20013;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20063;&#35768;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;&#20110;&#26356;&#21487;&#20998;&#31163;&#30340;&#25968;&#25454;&#20998;&#24067;&#32780;&#35328;&#65292;&#23547;&#25214;&#26368;&#20339;&#20915;&#31574;&#36793;&#30028;&#30340;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#38024;&#23545;Mixup&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Mixup&#20943;&#36731;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;Mixup&#32771;&#34385;&#30340;$n^2$&#25104;&#23545;&#22686;&#24378;&#25968;&#25454;&#28857;&#30340;&#26032;&#30340;&#38598;&#20013;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;Mixup&#30340;&#27867;&#21270;&#30410;&#22788;&#30340;&#21487;&#35777;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35299;Mixup&#20026;&#20160;&#20040;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate how pair-wise data augmentation techniques like Mixup affect the sample complexity of finding optimal decision boundaries in a binary linear classification problem. For a family of data distributions with a separability constant $\kappa$, we analyze how well the optimal classifier in terms of training loss aligns with the optimal one in test accuracy (i.e., Bayes optimal classifier). For vanilla training without augmentation, we uncover an interesting phenomenon named the curse of separability. As we increase $\kappa$ to make the data distribution more separable, the sample complexity of vanilla training increases exponentially in $\kappa$; perhaps surprisingly, the task of finding optimal decision boundaries becomes harder for more separable distributions. For Mixup training, we show that Mixup mitigates this problem by significantly reducing the sample complexity. To this end, we develop new concentration results applicable to $n^2$ pair-wise augmented data points cons
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse Invariant Detector&#65288;SID&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#21457;&#29616;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20445;&#23432;&#24459;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#30340;&#20445;&#23432;&#24459;&#65292;&#29978;&#33267;&#21457;&#29616;&#26032;&#30340;&#20445;&#23432;&#24459;&#65292;&#24182;&#19988;&#24050;&#21457;&#29616;&#30340;&#20445;&#23432;&#24459;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19525</link><description>&lt;p&gt;
&#21457;&#29616;&#26032;&#30340;&#21487;&#35299;&#37322;&#20445;&#23432;&#24459;&#20316;&#20026;&#31232;&#30095;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Discovering New Interpretable Conservation Laws as Sparse Invariants. (arXiv:2305.19525v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19525
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse Invariant Detector&#65288;SID&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#21457;&#29616;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20445;&#23432;&#24459;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#30340;&#20445;&#23432;&#24459;&#65292;&#29978;&#33267;&#21457;&#29616;&#26032;&#30340;&#20445;&#23432;&#24459;&#65292;&#24182;&#19988;&#24050;&#21457;&#29616;&#30340;&#20445;&#23432;&#24459;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32473;&#23450;&#21160;&#21147;&#31995;&#32479;&#30340;&#20445;&#23432;&#24459;&#26159;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#29702;&#35770;&#35774;&#32622;&#65288;&#24050;&#30693;&#24494;&#20998;&#26041;&#31243;&#21644;&#22522;&#20989;&#25968;&#65289;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sparse Invariant Detector&#65288;SID&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#24494;&#20998;&#26041;&#31243;&#20013;&#33258;&#21160;&#21457;&#29616;&#20445;&#23432;&#24459;&#30340;&#31639;&#27861;&#12290;&#20854;&#31639;&#27861;&#31616;&#21333;&#24615;&#30830;&#20445;&#20102;&#24050;&#21457;&#29616;&#20445;&#23432;&#25968;&#37327;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SID&#33021;&#22815;&#22312;&#21508;&#31181;&#31995;&#32479;&#20013;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#20445;&#23432;&#24459;&#65292;&#29978;&#33267;&#21457;&#29616;&#26032;&#30340;&#20445;&#23432;&#24459;&#12290;&#22312;&#27969;&#20307;&#21147;&#23398;&#21644;&#22823;&#27668;&#21270;&#23398;&#30340;&#20004;&#20010;&#20363;&#23376;&#20013;&#65292;SID&#20998;&#21035;&#21457;&#29616;&#20102;14&#20010;&#21644;3&#20010;&#23432;&#24658;&#37327;&#65292;&#32780;&#36825;&#20123;&#39046;&#22495;&#19987;&#23478;&#20808;&#21069;&#21482;&#30693;&#36947;12&#20010;&#21644;2&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering conservation laws for a given dynamical system is important but challenging. In a theorist setup (differential equations and basis functions are both known), we propose the Sparse Invariant Detector (SID), an algorithm that auto-discovers conservation laws from differential equations. Its algorithmic simplicity allows robustness and interpretability of the discovered conserved quantities. We show that SID is able to rediscover known and even discover new conservation laws in a variety of systems. For two examples in fluid mechanics and atmospheric chemistry, SID discovers 14 and 3 conserved quantities, respectively, where only 12 and 2 were previously known to domain experts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.18885</link><description>&lt;p&gt;
&#26631;&#20934;&#27604;&#35780;&#20998;&#26356;&#37325;&#35201;&#65306;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20934;&#21017;&#25512;&#33616;&#31995;&#32479;&#29616;&#22312;&#22312;&#24191;&#27867;&#30340;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#21033;&#29992;&#22810;&#20934;&#21017; (MC) &#35780;&#20998;&#20449;&#24687;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#21457;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;GNN&#36741;&#21161;&#35774;&#35745;MC&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#26041;&#27861;(CPA-LGC),&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#20197;&#21450;&#22797;&#26434;&#39640;&#38454;&#36830;&#25509;&#20013;&#30340;&#21327;&#20316;&#20449;&#21495;&#12290;&#26412;&#25991;&#22312;MC&#25193;&#23637;&#22270;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#29992;&#25143;-&#29289;&#21697;MC&#35780;&#20998;&#36716;&#25442;&#20026;&#25193;&#23637;&#20108;&#20998;&#22270;&#30340;MC&#25193;&#23637;&#22270;&#65292;&#20877;&#36827;&#19968;&#27493;&#23558;&#26631;&#20934;&#37325;&#35201;&#24615;&#32534;&#30721;&#21040;&#22270;&#21367;&#31215;&#36807;&#31243;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#32858;&#21512;&#26041;&#27861;&#26469;&#23558;&#29992;&#25143;&#23545;&#19981;&#21516;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SVD&#26080;&#30417;&#30563;&#19977;&#32500;&#24179;&#38754;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#21644;&#35821;&#20041;&#22320;&#22270;&#21363;&#21487;&#25913;&#21892;NeRF&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35757;&#32451;&#35270;&#22270;&#30340;&#36807;&#25311;&#21512;&#23548;&#33268;&#20302;&#32441;&#29702;&#21306;&#22495;&#30340;&#20960;&#20309;&#37325;&#24314;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16914</link><description>&lt;p&gt;
PlaNeRF&#65306;SVD&#26080;&#30417;&#30563;&#19977;&#32500;&#24179;&#38754;&#27491;&#21017;&#21270;&#29992;&#20110;NeRF&#22823;&#35268;&#27169;&#22330;&#26223;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction. (arXiv:2305.16914v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SVD&#26080;&#30417;&#30563;&#19977;&#32500;&#24179;&#38754;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#21644;&#35821;&#20041;&#22320;&#22270;&#21363;&#21487;&#25913;&#21892;NeRF&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35757;&#32451;&#35270;&#22270;&#30340;&#36807;&#25311;&#21512;&#23548;&#33268;&#20302;&#32441;&#29702;&#21306;&#22495;&#30340;&#20960;&#20309;&#37325;&#24314;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21033;&#29992;2D&#22270;&#20687;&#21644;&#30456;&#26426;&#23039;&#24577;&#36827;&#34892;3D&#22330;&#26223;&#37325;&#24314;&#20197;&#36827;&#34892;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#12290;&#23613;&#31649;NeRF&#33021;&#20135;&#29983;&#36924;&#30495;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#32463;&#24120;&#36973;&#21463;&#36807;&#25311;&#21512;&#20110;&#35757;&#32451;&#35270;&#22270;&#30340;&#22256;&#25200;&#65292;&#23548;&#33268;&#20960;&#20309;&#37325;&#24314;&#19981;&#20339;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#32441;&#29702;&#21306;&#22495;&#12290;&#36825;&#31181;&#38480;&#21046;&#38480;&#21046;&#20102;&#35768;&#22810;&#38656;&#35201;&#20934;&#30830;&#20960;&#20309;&#24418;&#24577;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#20363;&#22914;&#22806;&#25512;NVS&#65292;&#39640;&#28165;&#26144;&#23556;&#21644;&#22330;&#26223;&#32534;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#21644;&#35821;&#20041;&#22320;&#22270;&#21363;&#21487;&#25913;&#21892;NeRF&#30340;&#19977;&#32500;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#26032;&#39062;&#24179;&#38754;&#27491;&#21017;&#21270;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25439;&#22833;&#35774;&#35745;&#20013;&#21033;&#29992;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#27979;&#37327;&#65288;SSIM&#65289;&#26469;&#27491;&#30830;&#21021;&#22987;&#21270;NeRF&#30340;&#20307;&#31215;&#34920;&#31034;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#27969;&#34892;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20960;&#20309;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images and camera poses for Novel View Synthesis (NVS). Although NeRF can produce photorealistic results, it often suffers from overfitting to training views, leading to poor geometry reconstruction, especially in low-texture areas. This limitation restricts many important applications which require accurate geometry, such as extrapolated NVS, HD mapping and scene editing. To address this limitation, we propose a new method to improve NeRF's 3D structure using only RGB images and semantic maps. Our approach introduces a novel plane regularization based on Singular Value Decomposition (SVD), that does not rely on any geometric prior. In addition, we leverage the Structural Similarity Index Measure (SSIM) in our loss design to properly initialize the volumetric representation of NeRF. Quantitative and qualitative results show that our method outperforms popular regularization approaches in accurate geometry reconstructi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#22312;ReLU&#32593;&#32476;&#20013;&#28155;&#21152;&#32447;&#24615;&#23618;&#26377;&#21161;&#20110;&#36924;&#36817;&#20855;&#26377;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20302;&#34920;&#31034;&#25104;&#26412;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15598</link><description>&lt;p&gt;
&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#23618;&#20419;&#36827;&#23398;&#20064;&#21333;&#25351;&#25968;&#21644;&#22810;&#25351;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Linear Neural Network Layers Promote Learning Single- and Multiple-Index Models. (arXiv:2305.15598v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#22312;ReLU&#32593;&#32476;&#20013;&#28155;&#21152;&#32447;&#24615;&#23618;&#26377;&#21161;&#20110;&#36924;&#36817;&#20855;&#26377;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20302;&#34920;&#31034;&#25104;&#26412;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28145;&#24230;&#22823;&#20110;&#20004;&#23618;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#21547;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32771;&#34385;&#20102;&#19968;&#31867;&#28145;&#24230;&#19981;&#21516;&#20294;&#23481;&#37327;&#30456;&#21516;&#30340;&#32593;&#32476;&#65292;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#26174;&#24335;&#23450;&#20041;&#30340;&#34920;&#31034;&#25104;&#26412;&#12290;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35825;&#23548;&#30340;&#20989;&#25968;&#30340;&#34920;&#31034;&#25104;&#26412;&#26159;&#32593;&#32476;&#34920;&#31034;&#35813;&#20989;&#25968;&#25152;&#38656;&#30340;&#24179;&#26041;&#26435;&#37325;&#20043;&#21644;&#30340;&#26368;&#23567;&#20540;&#65307;&#23427;&#21453;&#26144;&#20102;&#19982;&#35813;&#26550;&#26500;&#30456;&#20851;&#30340;&#20989;&#25968;&#31354;&#38388;&#20559;&#24046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#32447;&#24615;&#23618;&#28155;&#21152;&#21040;ReLU&#32593;&#32476;&#20250;&#20135;&#29983;&#19968;&#20010;&#34920;&#31034;&#25104;&#26412;&#65292;&#36825;&#26377;&#21033;&#20110;&#20351;&#29992;&#20004;&#23618;&#32593;&#32476;&#26469;&#36924;&#36817;&#30001;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20855;&#26377;&#20302;&#34920;&#31034;&#25104;&#26412;&#30340;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20197;&#26368;&#23567;&#30340;&#34920;&#31034;&#25104;&#26412;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#20250;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the implicit bias of overparameterized neural networks of depth greater than two layers. Our framework considers a family of networks of varying depths that all have the same capacity but different implicitly defined representation costs. The representation cost of a function induced by a neural network architecture is the minimum sum of squared weights needed for the network to represent the function; it reflects the function space bias associated with the architecture. Our results show that adding linear layers to a ReLU network yields a representation cost that favors functions that can be approximated by a low-rank linear operator composed with a function with low representation cost using a two-layer network. Specifically, using a neural network to fit training data with minimum representation cost yields an interpolating function that is nearly constant in directions orthogonal to a low-dimensional subspace. This means that the learned network will approximate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#20002;&#22833;&#27169;&#24335;&#19979;&#26377;&#25928;&#22320;&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37327;&#21270;&#20102;&#32570;&#22833;&#23545;&#39044;&#27979;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.11640</link><description>&lt;p&gt;
&#20219;&#24847;&#32570;&#22833;&#27169;&#24335;&#19979;&#30340;&#26080;&#20998;&#24067;&#30697;&#38453;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern. (arXiv:2305.11640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#20002;&#22833;&#27169;&#24335;&#19979;&#26377;&#25928;&#22320;&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37327;&#21270;&#20102;&#32570;&#22833;&#23545;&#39044;&#27979;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#34892;/&#21015;&#21487;&#20132;&#25442;&#30697;&#38453;&#20013;&#39044;&#27979;&#32570;&#22833;&#26465;&#30446;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#30697;&#38453;&#35774;&#32622;&#25552;&#20986;&#20102;&#26032;&#39062;&#21644;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20294;&#26159;&#22312;&#36825;&#20010;&#26377;&#36259;&#30340;&#20027;&#39064;&#19978;&#23384;&#22312;&#24456;&#23569;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#31934;&#32454;&#22320;&#23450;&#20041;&#20102;&#38382;&#39064;&#65292;&#23558;&#20854;&#19982;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#20005;&#26684;&#21010;&#20998;&#20102;&#21487;&#36798;&#25104;&#21644;&#19981;&#21487;&#33021;&#30340;&#30446;&#26631;&#30340;&#36793;&#30028;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#39044;&#27979;&#30340;&#24555;&#36895;&#20223;&#30495;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#21033;&#29992;&#31639;&#27861;&#31283;&#23450;&#24615;&#25216;&#26415;&#21152;&#36895;&#35745;&#31639;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#20002;&#22833;&#27169;&#24335;&#19979;&#26377;&#25928;&#22320;&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#32570;&#22833;&#23545;&#39044;&#27979;&#31934;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#22522;&#26412;&#30340;&#26497;&#38480;&#32467;&#26524;&#12290;&#26469;&#33258;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#32463;&#39564;&#35777;&#25454;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the open problem of conformalized entry prediction in a row/column-exchangeable matrix. The matrix setting presents novel and unique challenges, but there exists little work on this interesting topic. We meticulously define the problem, differentiate it from closely related problems, and rigorously delineate the boundary between achievable and impossible goals. We then propose two practical algorithms. The first method provides a fast emulation of the full conformal prediction, while the second method leverages the technique of algorithmic stability for acceleration. Both methods are computationally efficient and can effectively safeguard coverage validity in presence of arbitrary missing pattern. Further, we quantify the impact of missingness on prediction accuracy and establish fundamental limit results. Empirical evidence from synthetic and real-world data sets corroborates the superior performance of our proposed methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38543;&#26426;&#25628;&#32034;&#21450;&#20854;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#30028;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#36755;&#20986;&#20998;&#21035;&#20197;&#19968;&#23450;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.11509</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#25628;&#32034;&#21040;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38543;&#26426;&#25628;&#32034;&#21450;&#20854;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#30028;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#36755;&#20986;&#20998;&#21035;&#20197;&#19968;&#23450;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#25628;&#32034;&#26159;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20854;&#24615;&#33021;&#20196;&#20154;&#24778;&#21497;&#65292;&#20294;&#24456;&#23569;&#26377;&#38750;&#21551;&#21457;&#24335;&#30340;&#29702;&#35770;&#29992;&#20110;&#25551;&#36848;&#20854;&#24037;&#20316;&#26426;&#21046;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#20851;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#24182;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#29615;&#22659;&#27809;&#26377;&#22122;&#22768;&#26102;&#65292;&#38543;&#26426;&#25628;&#32034;&#30340;&#36755;&#20986;&#20197;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#20854;&#36895;&#29575;&#20026;$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $&#65292;&#20854;&#20013;$ d_s \ge 0 $&#26159;&#24213;&#23618;&#20989;&#25968;&#30340;&#25955;&#23556;&#32500;&#24230;&#12290;&#24403;&#35266;&#23519;&#21040;&#30340;&#20989;&#25968;&#20540;&#21463;&#21040;&#26377;&#30028;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#24433;&#21709;&#26102;&#65292;&#38543;&#26426;&#25628;&#32034;&#30340;&#36755;&#20986;&#20197;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#36895;&#29575;&#20026;$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{2}{2+d_s} } \right) $&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Search is one of the most widely-used method for Hyperparameter Optimization, and is critical to the success of deep learning models. Despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. This paper gives a theoretical accounting of Random Search. We introduce the concept of \emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. We show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering dimension of the underlying function. When the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \rig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#957;&#20351;&#24471;&#24615;&#33021;&#26356;&#20339;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;GP&#38598;&#25104;&#25928;&#26524;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#25439;&#22833;&#39046;&#22495;&#30340;&#29289;&#29702;&#23646;&#24615;&#30340;&#25237;&#31080;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10748</link><description>&lt;p&gt;
&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#29702;&#35299;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics Inspired Approaches Towards Understanding Gaussian Processes. (arXiv:2305.10748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#957;&#20351;&#24471;&#24615;&#33021;&#26356;&#20339;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;GP&#38598;&#25104;&#25928;&#26524;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#25439;&#22833;&#39046;&#22495;&#30340;&#29289;&#29702;&#23646;&#24615;&#30340;&#25237;&#31080;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#26680;&#21487;&#20197;&#23558;&#20808;&#39564;&#26377;&#20851;&#28508;&#22312;&#20989;&#25968;&#30340;&#20449;&#24565;&#32435;&#20837;&#39640;&#26031;&#36807;&#31243;(GP)&#20013;&#20197;&#24418;&#25104;&#24402;&#32435;&#20559;&#32622;&#65292;&#20294;&#38500;&#20102;&#20869;&#26680;&#36873;&#25321;&#22806;&#65292;GP&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20173;&#28982;&#24456;&#38590;&#29702;&#35299;&#12290;&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#23545;GP&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#28436;&#31034;&#20102;Matern&#20869;&#26680;&#30340;&#957;&#36830;&#32493;&#24615;&#65292;&#24182;&#27010;&#36848;&#20102;&#26799;&#24230;&#22330;&#20851;&#38190;&#28857;&#30340;&#28798;&#21464;&#29702;&#35770;&#26041;&#38754;&#12290;&#36890;&#36807;&#23558;&#957;&#30452;&#25509;&#21253;&#21547;&#22312;Matern&#20869;&#26680;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#22312;&#25991;&#29486;&#20013;&#957;&#30340;&#20856;&#22411;&#20540;&#22686;&#21152;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#20294;&#20854;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#38750;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20107;&#20808;&#35780;&#20272;GP&#38598;&#21512;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;&#25439;&#22833;&#26223;&#35266;&#29289;&#29702;&#23646;&#24615;&#30340;&#21508;&#31181;&#25237;&#31080;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#22312;&#22810;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;GP&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#27169;&#22411;&#36873;&#25321;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior beliefs about the latent function to shape inductive biases can be incorporated into a Gaussian Process (GP) via the kernel. However, beyond kernel choices, the decision-making process of GP models remains poorly understood. In this work, we contribute an analysis of the loss landscape for GP models using methods from physics. We demonstrate $\nu$-continuity for Matern kernels and outline aspects of catastrophe theory at critical points in the loss landscape. By directly including $\nu$ in the hyperparameter optimisation for Matern kernels, we find that typical values of $\nu$ are far from optimal in terms of performance, yet prevail in the literature due to the increased computational speed. We also provide an a priori method for evaluating the effect of GP ensembles and discuss various voting approaches based on physical properties of the loss landscape. The utility of these approaches is demonstrated for various synthetic and real datasets. Our findings provide an enhanced und
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#31216;&#20854;&#20026;Gap Filler (GaFi)&#27969;&#31243;&#24182;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.10118</link><description>&lt;p&gt;
&#26550;&#36215;&#26725;&#26753;&#65306;&#36890;&#36807;&#21518;&#22788;&#29702;&#25216;&#26415;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques. (arXiv:2305.10118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#31216;&#20854;&#20026;Gap Filler (GaFi)&#27969;&#31243;&#24182;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#21644;&#27880;&#37322;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29983;&#25104;&#26367;&#20195;&#25110;&#22686;&#24378;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20854;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#38543;&#21518;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#65306;&#21160;&#24577;&#26679;&#26412;&#36807;&#28388;&#65292;&#21160;&#24577;&#25968;&#25454;&#38598;&#22238;&#25910;&#21644;&#25193;&#23637;&#25216;&#24039;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220; Gap Filler (GaFi)&#8221;&#30340;&#27969;&#31243;&#65292;&#22312;&#26368;&#20339;&#21644;&#21327;&#35843;&#30340;&#26041;&#24335;&#19979;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring and annotating suitable datasets for training deep learning models is challenging. This often results in tedious and time-consuming efforts that can hinder research progress. However, generative models have emerged as a promising solution for generating synthetic datasets that can replace or augment real-world data. Despite this, the effectiveness of synthetic data is limited by their inability to fully capture the complexity and diversity of real-world data. To address this issue, we explore the use of Generative Adversarial Networks to generate synthetic datasets for training classifiers that are subsequently evaluated on real-world images. To improve the quality and diversity of the synthetic dataset, we propose three novel post-processing techniques: Dynamic Sample Filtering, Dynamic Dataset Recycle, and Expansion Trick. In addition, we introduce a pipeline called Gap Filler (GaFi), which applies these techniques in an optimal and coordinated manner to maximise classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#25105;&#30417;&#30563;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#35299;&#20915;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10071</link><description>&lt;p&gt;
&#20919;&#21551;&#21160;&#38382;&#39064;&#65306;&#26080;&#30417;&#30563;&#30340;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cold PAWS: Unsupervised class discovery and the cold-start problem. (arXiv:2305.10071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#25105;&#30417;&#30563;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#35299;&#20915;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#38598;&#24120;&#24120;&#26159;&#19968;&#39033;&#33392;&#33510;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#30740;&#31350;&#34920;&#26126;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#26631;&#31614;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#20851;&#27880;&#22914;&#20309;&#36873;&#25321;&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#36827;&#34892;&#26631;&#35760;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#39318;&#27425;&#36873;&#25321;&#20449;&#24687;&#22270;&#20687;&#23376;&#38598;&#36827;&#34892;&#26631;&#35760;&#30340;&#25361;&#25112;&#65292;&#21363;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20960;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;CIFAR10&#12289;Imagenette&#12289;DeepWeeds&#21644;EuroSAT&#65289;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#24403;&#20351;&#29992;&#25105;&#20204;&#30340;&#26631;&#31614;&#36873;&#25321;&#31574;&#30053;&#26102;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#22343;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#22312;d&#26041;&#38754;&#33719;&#24471;&#20102;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
In many machine learning applications, labeling datasets can be an arduous and time-consuming task. Although research has shown that semi-supervised learning techniques can achieve high accuracy with very few labels within the field of computer vision, little attention has been given to how images within a dataset should be selected for labeling. In this paper, we propose a novel approach based on well-established self-supervised learning, clustering, and manifold learning techniques that address this challenge of selecting an informative image subset to label in the first instance, which is known as the cold-start or unsupervised selective labelling problem. We test our approach using several publicly available datasets, namely CIFAR10, Imagenette, DeepWeeds, and EuroSAT, and observe improved performance with both supervised and semi-supervised learning strategies when our label selection strategy is used, in comparison to random sampling. We also obtain superior performance for the d
&lt;/p&gt;</description></item><item><title>BIMT&#26041;&#27861;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#65292;&#24182;&#19988;&#33021;&#22815;&#30452;&#25509;&#23637;&#31034;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.08746</link><description>&lt;p&gt;
&#35265;&#35777;&#23601;&#26159;&#20449;&#20208;&#65306;&#33041;&#21551;&#21457;&#27169;&#22359;&#21270;&#35757;&#32451;&#20419;&#36827;&#26426;&#29702;&#35808;&#37322;
&lt;/p&gt;
&lt;p&gt;
Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability. (arXiv:2305.08746v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08746
&lt;/p&gt;
&lt;p&gt;
BIMT&#26041;&#27861;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#65292;&#24182;&#19988;&#33021;&#22815;&#30452;&#25509;&#23637;&#31034;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33041;&#21551;&#21457;&#27169;&#22359;&#21270;&#35757;&#32451;&#65288;Brain-Inspired Modular Training, BIMT&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#12290;BIMT&#20174;&#22823;&#33041;&#21463;&#21551;&#21457;&#65292;&#23558;&#31070;&#32463;&#20803;&#23884;&#20837;&#21040;&#20960;&#20309;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#25104;&#26412;&#19982;&#31070;&#32463;&#20803;&#36830;&#25509;&#38271;&#24230;&#25104;&#27491;&#27604;&#30340;&#26041;&#24335;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;BIMT&#21487;&#20197;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#21457;&#29616;&#26377;&#29992;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25581;&#31034;&#20102;&#31526;&#21495;&#20844;&#24335;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#12289;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#36793;&#30028;&#21644;&#20998;&#31867;&#29305;&#24449;&#65292;&#20197;&#21450;&#31639;&#27861;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#12290;&#30452;&#25509;&#30524;&#30555;&#30475;&#21040;&#27169;&#22359;&#30340;&#33021;&#21147;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#65292;&#20363;&#22914;&#25506;&#38024;&#65292;&#24178;&#39044;&#25110;&#20957;&#35270;&#25152;&#26377;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08040</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#27744;&#21270;&#30340;&#21487;&#35777;&#26126;&#22810;&#23454;&#20363;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26032;&#22411;&#24212;&#29992;&#65292;&#29992;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#65292;&#20854;&#20013;&#23558;&#21333;&#20010;&#31867;&#26631;&#31614;&#20998;&#37197;&#32473;&#19968;&#32452;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#24739;&#32773;&#30340;&#22810;&#20010;CT&#25195;&#25551;&#30340;&#22810;&#20010;2D&#20999;&#29255;&#65289;&#12290;&#25105;&#20204;&#22312;DAM&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;MIL&#20013;&#34987;&#24573;&#30053;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#21363;&#21253;&#22823;&#23567;&#36807;&#22823;&#65292;&#26080;&#27861;&#22312;&#21453;&#21521;&#20256;&#25773;&#26102;&#21152;&#36733;&#21040;GPU&#20869;&#23384;&#20013;&#65292;&#36825;&#26159;MIL&#26631;&#20934;&#27744;&#21270;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20851;&#20110;&#27719;&#32858;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#26500;&#36896;&#20026;&#22810;&#32423;&#32452;&#21512;&#20989;&#25968;&#12290;&#36890;&#36807;&#32508;&#21512;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#21644;&#38750;&#20984;&#26497;&#23567;&#26368;&#22823;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19988;&#21487;&#35777;&#26126;&#30340;&#22810;&#23454;&#20363;DAM&#65288;MIDAM&#65289;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#26368;&#22823;&#27744;&#21270;&#25110;&#38543;&#26426;&#27880;&#24847;&#21147;&#27744;&#21270;&#65292;&#20165;&#23545;&#27599;&#20010;&#21253;&#23545;&#24212;&#30340;&#23454;&#20363;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#26469;&#35745;&#31639; sto&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.07598</link><description>&lt;p&gt;
RHINO&#65306;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#30340;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#30340;&#26059;&#36716;DETR
&lt;/p&gt;
&lt;p&gt;
RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection. (arXiv:2305.07598v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;DINO&#30340;&#21457;&#24067;&#65292;&#19968;&#31181;DETR&#30340;&#21464;&#20307;&#65292;&#26816;&#27979;&#21464;&#21387;&#22120;&#27491;&#22312;&#36890;&#36807;&#20854;&#31471;&#21040;&#31471;&#35774;&#35745;&#21644;&#21487;&#25193;&#23637;&#24615;&#22312;&#30446;&#26631;&#26816;&#27979;&#22522;&#20934;&#20013;&#21047;&#26032;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#39044;&#35745;&#20174;&#20854;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#33719;&#24471;&#26356;&#22810;&#30340;&#22909;&#22788;&#65292;&#22914;&#28040;&#38500;NMS&#21644;&#19982;&#38170;&#30456;&#20851;&#30340;&#25104;&#26412;&#65292;&#20294;&#23578;&#26410;&#24443;&#24213;&#30740;&#31350;DETR&#22312;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#30340;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20351;&#29992;DETR&#36827;&#34892;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#24182;&#19981;&#33021;&#20445;&#35777;&#19981;&#37325;&#22797;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25104;&#26412;&#26469;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#21256;&#29273;&#21033;&#21305;&#37197;&#26469;&#36807;&#28388;&#20887;&#20313;&#30340;&#24102;&#22122;&#22768;&#30340;&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#26597;&#35810;&#23545;&#40784;&#26469;&#20445;&#25345;Transformer&#35299;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#21305;&#37197;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#26059;&#36716;DETR&#21644;&#20854;&#20182;&#23545;&#25163;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a novel denoising strategy that uses Hungarian matching to filter redundant noised queries and query alignment to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;GNN&#12290;</title><link>http://arxiv.org/abs/2305.07580</link><description>&lt;p&gt;
&#22522;&#20110;Fisher&#20449;&#24687;&#23884;&#20837;&#30340;&#33410;&#28857;&#21644;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fisher Information Embedding for Node and Graph Learning. (arXiv:2305.07580v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#20363;&#22914;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#65292;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#21644;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#30340;&#27969;&#34892;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#26631;&#27880;&#25968;&#25454;&#65292;&#19988;&#36825;&#20123;&#27169;&#22411;&#30340;&#29702;&#35770;&#23646;&#24615;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#31181;&#22810;&#37325;&#38598;&#21512;&#20869;&#33410;&#28857;&#21608;&#22260;&#23376;&#22270;&#30340;&#20998;&#23618;&#26680;&#20043;&#19978;&#65288;&#20363;&#22914;&#65292;&#37051;&#22495;&#65289;&#65292;&#24182;&#19988;&#27599;&#20010;&#26680;&#21033;&#29992;&#24179;&#28369;&#32479;&#35745;&#27969;&#24418;&#30340;&#20960;&#20309;&#26469;&#27604;&#36739;&#22810;&#37325;&#38598;&#21512;&#30340;&#25104;&#23545;&#24046;&#24322;&#65292;&#36890;&#36807;&#23558;&#22810;&#37325;&#38598;&#21512;&#8220;&#26144;&#23556;&#8221;&#21040;&#27969;&#24418;&#19978;&#12290;&#36890;&#36807;&#26174;&#24335;&#35745;&#31639;&#39640;&#26031;&#28151;&#21512;&#29289;&#27969;&#24418;&#20013;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#23548;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#26426;&#21046;&#36827;&#34892;&#37051;&#22495;&#32858;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#23884;&#20837;&#30340;&#27867;&#21270;&#21644;&#34920;&#36798;&#33021;&#21147;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#20026;&#26356;&#28145;&#20837;&#29702;&#35299;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;GNN&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based graph neural networks (GNNs), such as graph attention networks (GATs), have become popular neural architectures for processing graph-structured data and learning node embeddings. Despite their empirical success, these models rely on labeled data and the theoretical properties of these models have yet to be fully understood. In this work, we propose a novel attention-based node embedding framework for graphs. Our framework builds upon a hierarchical kernel for multisets of subgraphs around nodes (e.g. neighborhoods) and each kernel leverages the geometry of a smooth statistical manifold to compare pairs of multisets, by "projecting" the multisets onto the manifold. By explicitly computing node embeddings with a manifold of Gaussian mixtures, our method leads to a new attention mechanism for neighborhood aggregation. We provide theoretical insights into genralizability and expressivity of our embeddings, contributing to a deeper understanding of attention-based GNNs. We p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#31038;&#20250;&#23398;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#35282;&#24230;&#21644;&#24037;&#20855;&#65292;&#30528;&#30524;&#20110;&#30456;&#20851;&#24615;&#22312;&#20844;&#24179;&#25490;&#24207;&#20013;&#30340;&#24212;&#29992;&#21644;&#20316;&#29992;&#65292;&#24182;&#25512;&#23548;&#20986;&#30456;&#20851;&#24615;&#35780;&#20998;&#24212;&#28385;&#36275;&#30340;&#19968;&#32452;&#26399;&#26395;&#26631;&#20934;&#20197;&#23454;&#29616;&#26377;&#24847;&#20041;&#22320;&#25351;&#23548;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2305.05608</link><description>&lt;p&gt;
&#30456;&#20851;&#24615;&#22312;&#20844;&#24179;&#25490;&#24207;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Relevance in Fair Ranking. (arXiv:2305.05608v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#31038;&#20250;&#23398;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#35282;&#24230;&#21644;&#24037;&#20855;&#65292;&#30528;&#30524;&#20110;&#30456;&#20851;&#24615;&#22312;&#20844;&#24179;&#25490;&#24207;&#20013;&#30340;&#24212;&#29992;&#21644;&#20316;&#29992;&#65292;&#24182;&#25512;&#23548;&#20986;&#30456;&#20851;&#24615;&#35780;&#20998;&#24212;&#28385;&#36275;&#30340;&#19968;&#32452;&#26399;&#26395;&#26631;&#20934;&#20197;&#23454;&#29616;&#26377;&#24847;&#20041;&#22320;&#25351;&#23548;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24179;&#21488;&#22312;&#26426;&#20250;&#33719;&#21462;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65306;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#25490;&#21517;&#36890;&#36807;&#22312;&#25307;&#32856;&#24179;&#21488;&#30340;&#24037;&#20316;&#32844;&#20301;&#12289;&#27714;&#32844;&#32773;&#25110;&#22312;&#32447;&#24066;&#22330;&#30340;&#21334;&#23478;&#20013;&#20998;&#37197;&#26333;&#20809;&#26426;&#20250;&#26469;&#21019;&#24314;&#21644;&#38480;&#21046;&#36873;&#39033;&#12290;&#20026;&#20102;&#36127;&#36131;&#20219;&#22320;&#36825;&#26679;&#20570;&#65292;&#36825;&#20123;&#31038;&#20250;&#30456;&#20851;&#31995;&#32479;&#37319;&#29992;&#21508;&#31181;&#20844;&#24179;&#25514;&#26045;&#21644;&#24178;&#39044;&#25514;&#26045;&#65292;&#20854;&#20013;&#35768;&#22810;&#25514;&#26045;&#35797;&#22270;&#26681;&#25454;&#20215;&#20540;&#20998;&#37197;&#26333;&#20809;&#26426;&#20250;&#12290;&#20294;&#26159;&#65292;&#22240;&#20026;&#36825;&#20123;&#26500;&#36896;&#36890;&#24120;&#19981;&#26159;&#30452;&#25509;&#21487;&#35266;&#23519;&#30340;&#65292;&#25152;&#20197;&#24179;&#21488;&#24517;&#39035;&#20351;&#29992;&#20195;&#29702;&#35780;&#20998;&#65292;&#22914;&#30456;&#20851;&#24615;&#65292;&#24182;&#20174;&#25628;&#32034;&#32773;&#30340;&#34892;&#20026;&#20449;&#21495;&#20013;&#25512;&#26029;&#20986;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#21363;&#30456;&#20851;&#24615;&#22312;&#39640;&#39118;&#38505;&#30340;&#20844;&#24179;&#25490;&#24207;&#20013;&#26159;&#21542;&#23653;&#34892;&#20854;&#20316;&#20026;&#20215;&#20540;&#35780;&#20998;&#36825;&#26679;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#32467;&#21512;&#31038;&#20250;&#23398;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#35282;&#24230;&#21644;&#24037;&#20855;&#65292;&#25512;&#23548;&#20986;&#30456;&#20851;&#24615;&#35780;&#20998;&#24212;&#28385;&#36275;&#30340;&#19968;&#32452;&#26399;&#26395;&#26631;&#20934;&#65292;&#20197;&#20415;&#26377;&#24847;&#20041;&#22320;&#25351;&#23548;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online platforms mediate access to opportunity: relevance-based rankings create and constrain options by allocating exposure to job openings and job candidates in hiring platforms, or sellers in a marketplace. In order to do so responsibly, these socially consequential systems employ various fairness measures and interventions, many of which seek to allocate exposure based on worthiness. Because these constructs are typically not directly observable, platforms must instead resort to using proxy scores such as relevance and infer them from behavioral signals such as searcher clicks. Yet, it remains an open question whether relevance fulfills its role as such a worthiness score in high-stakes fair rankings.  In this paper, we combine perspectives and tools from the social sciences, information retrieval, and fairness in machine learning to derive a set of desired criteria that relevance scores should satisfy in order to meaningfully guide fairness interventions. We then empirically show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04990</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#24494;&#35843;&#20351;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#24378;&#38887;
&lt;/p&gt;
&lt;p&gt;
Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38750;&#24120;&#24378;&#22823;&#65292;&#26377;&#26102;&#20250;&#23398;&#20064;&#21040;&#26631;&#31614;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#35299;&#37322;&#24615;&#24494;&#35843;&#20316;&#20026;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#30340;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#21482;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#31572;&#26696;&#19981;&#21516;&#65292;&#25105;&#20204;&#24494;&#35843;&#27169;&#22411;&#20197;&#29983;&#25104;&#25903;&#25345;&#20854;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#20154;&#24037;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#35813;&#35757;&#32451;&#38598;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#34394;&#20551;&#25552;&#31034;&#65292;&#24182;&#22312;&#27809;&#26377;&#36825;&#20123;&#25552;&#31034;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26041;&#38754;&#20351;&#27169;&#22411;&#26497;&#20854;&#24378;&#38887;&#65306;ComVE&#65288;+1.2&#65289;&#65292;CREAK&#65288;+9.1&#65289;&#65292;e-SNLI&#65288;+15.4&#65289;&#21644;SBIC&#65288;+6.5&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#21516;&#26679;&#26377;&#25928;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a novel and general approach to mitigate LLMs' reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes models remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover, our method works equally well with explanations generated by the model, implyin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#31639;&#27861;&#21644;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00169</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#35777;&#25454;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Evidential Real-Time Multi-Mode Fault Diagnosis Approach Based on Broad Learning System. (arXiv:2305.00169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#31639;&#27861;&#21644;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22810;&#31181;&#24037;&#20917;&#34920;&#29616;&#20986;&#30340;&#38750;&#39640;&#26031;&#12289;&#22810;&#27169;&#24577;&#21644;&#20013;&#24515;&#28418;&#31227;&#29305;&#24449;&#65292;&#25925;&#38556;&#35786;&#26029;&#26159;&#24037;&#19994;&#30028;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#30446;&#21069;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#20294;&#23427;&#20204;&#22312;&#36830;&#32493;&#25925;&#38556;&#20998;&#31867;&#21644;&#25925;&#38556;&#20998;&#31867;&#22120;&#21442;&#25968;&#26356;&#26032;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#22810;&#31181;&#25805;&#20316;&#27169;&#24335;&#21644;&#23454;&#26102;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#23454;&#29616;&#24037;&#19994;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26159;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35777;&#25454;&#25512;&#29702;&#65288;ER&#65289;&#31639;&#27861;&#26469;&#34701;&#21512;&#20449;&#24687;&#24182;&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#22522;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#36825;&#20123;&#22522;&#20998;&#31867;&#22120;&#20351;&#29992;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#65288;BLS&#65289;&#24320;&#21457;&#65292;&#20197;&#25552;&#39640;&#33391;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#26102;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault diagnosis is a crucial area of research in the industry due to diverse operating conditions that exhibit non-Gaussian, multi-mode, and center-drift characteristics. Currently, data-driven approaches are the main focus in the field, but they pose challenges for continuous fault classification and parameter updates of fault classifiers, particularly in multiple operating modes and real-time settings. Therefore, a pressing issue is to achieve real-time multi-mode fault diagnosis for industrial systems. To address this problem, this paper proposes a novel approach that utilizes an evidence reasoning (ER) algorithm to fuse information and merge outputs from different base classifiers. These base classifiers are developed using a broad learning system (BLS) to improve good fault diagnosis performance. Moreover, in this approach, the pseudo-label learning method is employed to update model parameters in real-time. To demonstrate the effectiveness of the proposed approach, we perform exp
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20108;&#20803;&#23646;&#24615;&#34920;&#31034;&#27169;&#22411;&#23545;Netflix&#35266;&#20247;&#23545;&#30005;&#24433;&#30340;&#35780;&#20998;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#34920;&#31034;&#65292;&#23646;&#24615;&#26131;&#20110;&#35299;&#37322;&#65292;&#19988;&#38656;&#35201;&#36739;&#23569;&#23646;&#24615;&#21363;&#21487;&#36798;&#21040;&#30456;&#21516;&#27700;&#24179;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.14209</link><description>&lt;p&gt;
&#19968;&#31181;&#36879;&#26126;&#21270;&#25968;&#25454;&#34920;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transparent approach to data representation. (arXiv:2304.14209v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14209
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20108;&#20803;&#23646;&#24615;&#34920;&#31034;&#27169;&#22411;&#23545;Netflix&#35266;&#20247;&#23545;&#30005;&#24433;&#30340;&#35780;&#20998;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#34920;&#31034;&#65292;&#23646;&#24615;&#26131;&#20110;&#35299;&#37322;&#65292;&#19988;&#38656;&#35201;&#36739;&#23569;&#23646;&#24615;&#21363;&#21487;&#36798;&#21040;&#30456;&#21516;&#27700;&#24179;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#20108;&#20803;&#23646;&#24615;&#34920;&#31034;&#65288;BAR&#65289;&#27169;&#22411;&#26469;&#25551;&#36848;Netflix&#35266;&#20247;&#23545;&#30005;&#24433;&#30340;&#35780;&#20998;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#30340;&#20108;&#36827;&#21046;&#20301;&#32780;&#19981;&#26159;&#36830;&#32493;&#30340;&#21442;&#25968;&#23545;&#35266;&#20247;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#20351;&#24471;&#34920;&#31034;&#32039;&#20945;&#32780;&#36879;&#26126;&#12290;&#36825;&#20123;&#23646;&#24615;&#26131;&#20110;&#35299;&#37322;&#65292;&#25105;&#20204;&#38656;&#35201;&#27604;&#31867;&#20284;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#23646;&#24615;&#25165;&#33021;&#36798;&#21040;&#30456;&#21516;&#27700;&#24179;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30005;&#24433;&#35780;&#20998;&#30340;&#38750;&#22343;&#21248;&#20998;&#24067;&#65292;&#22312;&#19981;&#24433;&#21709;&#20854;&#20313;&#30005;&#24433;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#36873;&#25321;&#23569;&#37327;&#30005;&#24433;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use a binary attribute representation (BAR) model to describe a data set of Netflix viewers' ratings of movies. We classify the viewers with discrete bits rather than continuous parameters, which makes the representation compact and transparent. The attributes are easy to interpret, and we need far fewer attributes than similar methods do to achieve the same level of error. We also take advantage of the nonuniform distribution of ratings among the movies in the data set to train on a small selection of movies without compromising performance on the rest of the movies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21463;&#38480;&#36890;&#20449;&#21644;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20449;&#24687;&#29702;&#35770;&#19979;&#38480;&#12290;</title><link>http://arxiv.org/abs/2304.12680</link><description>&lt;p&gt;
&#21463;&#38480;&#36890;&#20449;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Communication-Constrained Bandits under Additive Gaussian Noise. (arXiv:2304.12680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21463;&#38480;&#36890;&#20449;&#21644;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20449;&#24687;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;,&#20854;&#20013;&#23458;&#25143;&#31471;&#26681;&#25454;&#30456;&#24212;&#30340;&#25289;&#33218;&#22870;&#21169;&#25552;&#20379;&#21463;&#38480;&#36890;&#20449;&#21453;&#39304;&#32473;&#23398;&#20064;&#32773;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#23450;&#19979;,&#23458;&#25143;&#31471;&#24517;&#39035;&#32534;&#30721;&#22870;&#21169;&#65292;&#20351;&#24471;&#32534;&#30721;&#22870;&#21169;&#30340;&#20108;&#38454;&#30697;&#19981;&#36229;&#36807;P&#65292;&#24182;&#19988;&#36825;&#20010;&#32534;&#30721;&#22870;&#21169;&#20250;&#34987;&#26041;&#24046;&#20026;$\sigma^2$&#30340;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#25152;&#27745;&#26579;&#65307;&#23398;&#20064;&#32773;&#21482;&#33021;&#35775;&#38382;&#36825;&#20010;&#34987;&#27745;&#26579;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#23548;&#20986;&#20102;&#20219;&#20309;&#26041;&#26696;&#30340;&#26368;&#23567;&#21270;&#21518;&#24724;&#30340;&#20449;&#24687;&#35770;&#19979;&#38480;$\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$&#65292;&#20854;&#20013; $ \mathtt{SNR} := \frac{P}{\sigma^2}$&#65292;$K$&#21644;$T$&#20998;&#21035;&#26159;&#33218;&#25968;&#21644;&#26102;&#38388;&#38271;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#36172;&#21338;&#31639;&#27861;$\mathtt{UE\text{-}UCB++}$&#65292;&#23427;&#21487;&#20197;&#23558;&#36825;&#20010;&#19979;&#38480;&#30340;&#20540;&#21152;&#19978;&#19968;&#20010;&#24494;&#23567;&#30340;&#21487;&#21152;&#24615;&#22240;&#23376;&#12290;$\mathtt{UE\text{-}UCB++}$&#22312;&#20854;&#21021;&#22987;&#38454;&#27573;&#25191;&#34892;&#22343;&#21248;&#25506;&#32034;&#65292;&#28982;&#21518;&#22312;&#21518;&#32493;&#38454;&#27573;&#20351;&#29992;&#8220;&#19978;&#32622;&#20449;&#30028;&#8221;(UCB)&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25968;&#20540;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#38656;&#35201;&#36825;&#26679;&#30340;&#36890;&#20449;&#26377;&#25928;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a distributed stochastic multi-armed bandit where a client supplies the learner with communication-constrained feedback based on the rewards for the corresponding arm pulls. In our setup, the client must encode the rewards such that the second moment of the encoded rewards is no more than $P$, and this encoded reward is further corrupted by additive Gaussian noise of variance $\sigma^2$; the learner only has access to this corrupted reward. For this setting, we derive an information-theoretic lower bound of $\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$ on the minimax regret of any scheme, where $ \mathtt{SNR} := \frac{P}{\sigma^2}$, and $K$ and $T$ are the number of arms and time horizon, respectively. Furthermore, we propose a multi-phase bandit algorithm, $\mathtt{UE\text{-}UCB++}$, which matches this lower bound to a minor additive factor. $\mathtt{UE\text{-}UCB++}$ performs uniform exploration in its initial phases and then utilizes the {\em upper confidence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#20197;&#21450;&#22312;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#31561;&#22810;&#23618;&#38754;&#26041;&#27861;&#26469;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#22797;&#65292;&#24182;&#36991;&#20813;&#29983;&#25104;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10611</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26041;&#38754;&#37325;&#22797;&#25233;&#21046;&#21644;&#20869;&#23481;&#35843;&#25511;
&lt;/p&gt;
&lt;p&gt;
Multi-aspect Repetition Suppression and Content Moderation of Large Language Models. (arXiv:2304.10611v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#20197;&#21450;&#22312;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#31561;&#22810;&#23618;&#38754;&#26041;&#27861;&#26469;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#22797;&#65292;&#24182;&#36991;&#20813;&#29983;&#25104;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#22312;NLP&#39046;&#22495;&#26159;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#36817;&#24180;&#26469;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24102;&#26469;&#30340;&#36827;&#27493;&#24471;&#21040;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#32534;&#20889;&#21161;&#25163;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#23427;&#20204;&#36890;&#24120;&#23481;&#26131;&#22797;&#21046;&#25110;&#25193;&#23637;&#36755;&#20837;&#20013;&#25552;&#20379;&#30340;&#20855;&#26377;&#25915;&#20987;&#24615;&#30340;&#20869;&#23481;&#12290;&#22312;&#20302;&#36164;&#28304;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#23548;&#33268;&#36755;&#20986;&#37325;&#22797;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31934;&#30830;&#21644;&#38750;&#31934;&#30830;&#37325;&#22797;&#25233;&#21046;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22810;&#32423;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#30340;&#33539;&#22260;&#65292;&#20197;&#36171;&#20104;&#27169;&#22411;&#36991;&#20813;&#20174;&#19968;&#24320;&#22987;&#20135;&#29983;&#25915;&#20987;&#24615;&#35789;&#27719;&#21644;&#30701;&#35821;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#22312;&#22810;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language generation is one of the most impactful fields in NLP, and recent years have witnessed its evolution brought about by large language models (LLMs). As the key instrument for writing assistance applications, they are generally prone to replicating or extending offensive content provided in the input. In low-resource data regime, they can also lead to repetitive outputs (Holtzman et al., 2019) [1]. Usually, offensive content and repetitions are mitigated with post-hoc methods, including n-gram level blocklists, top-k and nucleus sampling. In this paper, we introduce a combination of exact and non-exact repetition suppression using token and sequence level unlikelihood loss, repetition penalty during training, inference, and post-processing respectively. We further explore multi-level unlikelihood loss to the extent that it endows the model with abilities to avoid generating offensive words and phrases from the beginning. Finally, with comprehensive experiments, we demons
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#31181;&#39118;&#26684;&#21644;&#35270;&#35282;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#24494;&#35843;&#39592;&#24178;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10226</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#26512;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization for Mammographic Image Analysis via Contrastive Learning. (arXiv:2304.10226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10226
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#31181;&#39118;&#26684;&#21644;&#35270;&#35282;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#24494;&#35843;&#39592;&#24178;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#26512;&#26159;&#21307;&#23398;&#24433;&#20687;&#23398;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19981;&#21516;&#21378;&#21830;&#30340;&#22270;&#20687;&#39118;&#26684;&#65292;&#36825;&#24448;&#24448;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26679;&#26412;&#38598;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#21040;&#19981;&#21516;&#21378;&#21830;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mammographic image analysis is a fundamental problem in the computer-aided diagnosis scheme, which has recently made remarkable progress with the advance of deep learning. However, the construction of a deep learning model requires training data that are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, mammogram collection from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning models to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor styles. Afterward, the backbone network is then recalibrated to the downstream tasks of mas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#21644;&#21151;&#29575;&#20998;&#26512;&#30830;&#23450;&#20102;&#22810;&#20803;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35780;&#20998;&#35268;&#21017;&#30340;&#21487;&#38752;&#24615;&#21306;&#22495;&#65292;&#24182;&#22312;&#30005;&#21147;&#29983;&#20135;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#32467;&#26524;&#23545;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09836</link><description>&lt;p&gt;
&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#21306;&#22495;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts. (arXiv:2304.09836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#21644;&#21151;&#29575;&#20998;&#26512;&#30830;&#23450;&#20102;&#22810;&#20803;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35780;&#20998;&#35268;&#21017;&#30340;&#21487;&#38752;&#24615;&#21306;&#22495;&#65292;&#24182;&#22312;&#30005;&#21147;&#29983;&#20135;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#32467;&#26524;&#23545;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35780;&#20272;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#36827;&#34892;&#35780;&#20272;&#65292;&#21363;&#23545;&#20110;&#22522;&#20934;&#20998;&#24067;&#26399;&#26395;&#26368;&#23567;&#30340;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#28176;&#36827;&#24773;&#20917;&#19979;&#65292;&#36825;&#19968;&#23646;&#24615;&#19981;&#33021;&#20445;&#35777;&#20855;&#26377;&#33391;&#22909;&#30340;&#21306;&#20998;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#31687;&#31995;&#32479;&#30340;&#26377;&#38480;&#26679;&#26412;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30740;&#31350;&#65292;&#36890;&#36807;&#21151;&#29575;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#20998;&#25968;&#35268;&#21017;&#30340;&#8220;&#21487;&#38752;&#24615;&#21306;&#22495;&#8221;&#65292;&#21363;&#23427;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#39044;&#27979;&#35823;&#24046;&#30340;&#19968;&#32452;&#23454;&#38469;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#38754;&#30340;&#20154;&#36896;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#35813;&#27979;&#35797;&#19987;&#38376;&#35774;&#35745;&#20197;&#27979;&#35797;&#22522;&#20934;&#20998;&#24067;&#19982;&#39044;&#27979;&#20998;&#24067;&#20043;&#38388;&#30340;&#20960;&#20010;&#20851;&#38190;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#22312;&#30005;&#21147;&#29983;&#20135;&#38382;&#39064;&#19978;&#24212;&#29992;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22312;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#30340;&#35780;&#20272;&#20013;&#30340;&#37325;&#22823;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate probabilistic time series forecasts are commonly evaluated via proper scoring rules, i.e., functions that are minimal in expectation for the ground-truth distribution. However, this property is not sufficient to guarantee good discrimination in the non-asymptotic regime. In this paper, we provide the first systematic finite-sample study of proper scoring rules for time-series forecasting evaluation. Through a power analysis, we identify the "region of reliability" of a scoring rule, i.e., the set of practical conditions where it can be relied on to identify forecasting errors. We carry out our analysis on a comprehensive synthetic benchmark, specifically designed to test several key discrepancies between ground-truth and forecast distributions, and we gauge the generalizability of our findings to real-world tasks with an application to an electricity production problem. Our results reveal critical shortcomings in the evaluation of multivariate probabilistic forecasts as co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21452;&#26354;&#34920;&#31034;&#25429;&#25417;&#22270;&#20687;&#21644;&#25991;&#26412;&#23618;&#27425;&#32467;&#26500;&#30340;&#23545;&#27604;&#27169;&#22411;MERU&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#19982;CLIP&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2304.09172</link><description>&lt;p&gt;
&#21452;&#26354;&#32447;&#22270;&#20687;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Image-Text Representations. (arXiv:2304.09172v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21452;&#26354;&#34920;&#31034;&#25429;&#25417;&#22270;&#20687;&#21644;&#25991;&#26412;&#23618;&#27425;&#32467;&#26500;&#30340;&#23545;&#27604;&#27169;&#22411;MERU&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#19982;CLIP&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#27010;&#24565;&#33258;&#28982;&#32780;&#28982;&#22320;&#32452;&#32455;&#25104;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#19968;&#20010;&#25991;&#26412;&#27010;&#24565;&#8220;&#29399;&#8221;&#21253;&#21547;&#25152;&#26377;&#21253;&#21547;&#29399;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#30452;&#35273;&#19978;&#36825;&#26159;&#27491;&#30830;&#30340;&#65292;&#20294;&#30446;&#21069;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MERU&#65292;&#19968;&#20010;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#21452;&#26354;&#34920;&#31034;&#30340;&#23545;&#27604;&#27169;&#22411;&#12290;&#21452;&#26354;&#31354;&#38388;&#20855;&#26377;&#23884;&#20837;&#26641;&#29366;&#25968;&#25454;&#30340;&#21512;&#36866;&#20960;&#20309;&#23646;&#24615;&#65292;&#22240;&#27492;MERU&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#30340;&#24213;&#23618;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MERU&#23398;&#20064;&#21040;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#21516;&#26102;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#31561;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#19982;CLIP&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual and linguistic concepts naturally organize themselves in a hierarchy, where a textual concept ``dog'' entails all images that contain dogs. Despite being intuitive, current large-scale vision and language models such as CLIP do not explicitly capture such hierarchy. We propose MERU, a contrastive model that yields hyperbolic representations of images and text. Hyperbolic spaces have suitable geometric properties to embed tree-like data, so MERU can better capture the underlying hierarchy in image-text data. Our results show that MERU learns a highly interpretable representation space while being competitive with CLIP's performance on multi-modal tasks like image classification and image-text retrieval.
&lt;/p&gt;</description></item><item><title>&#23545;&#38472;&#31561;&#20154;&#21457;&#34920;&#22312;&#12298;&#33258;&#28982;&#8212;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#12299;&#26434;&#24535;&#19978;&#30340;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#19968;&#25991;&#30340;&#35780;&#35770;&#21644;&#20851;&#20999;&#12290;</title><link>http://arxiv.org/abs/2304.08297</link><description>&lt;p&gt;
&#23545;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#30340;&#35780;&#35770;&#65288;arXiv: 2304.08297v2 [eess.IV] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Comments on 'Fast and scalable search of whole-slide images via self-supervised deep learning'. (arXiv:2304.08297v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08297
&lt;/p&gt;
&lt;p&gt;
&#23545;&#38472;&#31561;&#20154;&#21457;&#34920;&#22312;&#12298;&#33258;&#28982;&#8212;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#12299;&#26434;&#24535;&#19978;&#30340;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#19968;&#25991;&#30340;&#35780;&#35770;&#21644;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38472;&#31561;&#20154;&#65288;Chen2022&#65289;&#22312;&#12298;&#33258;&#28982;&#8212;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#20102;&#39064;&#20026;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#30340;&#25991;&#31456;&#12290;&#35813;&#25991;&#31456;&#20316;&#32773;&#31216;&#20854;&#26041;&#27861;&#20026;&#8220;&#32452;&#32455;&#23398;&#33258;&#30417;&#30563;&#22270;&#20687;&#25628;&#32034;&#65292;&#31616;&#31216;SISH&#12290;&#8221;&#25105;&#20204;&#23545;SISH&#34920;&#31034;&#20102;&#20851;&#20999;&#65292;&#22240;&#20026;&#23427;&#26159;Yottixel&#30340;&#22686;&#37327;&#20462;&#25913;&#65292;&#20351;&#29992;&#20102;MinMax&#20108;&#20540;&#21270;&#20294;&#26410;&#24341;&#29992;&#21407;&#22987;&#20316;&#21697;&#65292;&#24182;&#22522;&#20110;&#19968;&#20010;&#35823;&#31216;&#8220;&#33258;&#30417;&#30563;&#22270;&#20687;&#25628;&#32034;&#8221;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#38472;&#31561;&#20154;&#36827;&#34892;&#23454;&#39564;&#21644;&#27604;&#36739;&#26102;&#23384;&#22312;&#30340;&#20960;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chen et al. [Chen2022] recently published the article 'Fast and scalable search of whole-slide images via self-supervised deep learning' in Nature Biomedical Engineering. The authors call their method 'self-supervised image search for histology', short SISH. We express our concerns that SISH is an incremental modification of Yottixel, has used MinMax binarization but does not cite the original works, and is based on a misnomer 'self-supervised image search'. As well, we point to several other concerns regarding experiments and comparisons performed by Chen et al.
&lt;/p&gt;</description></item><item><title>G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.06653</link><description>&lt;p&gt;
G2T: &#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06653
&lt;/p&gt;
&lt;p&gt;
G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#32858;&#31867;&#30340;&#20027;&#39064;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36866;&#24403;&#30340;&#35789;&#35821;&#31579;&#36873;&#26041;&#27861;&#32858;&#31867;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#29983;&#25104;&#27604;&#29983;&#25104;&#24335;&#27010;&#29575;&#20027;&#39064;&#27169;&#22411;&#26356;&#22909;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#36873;&#25321;&#21512;&#36866;&#21442;&#25968;&#30340;&#22256;&#38590;&#20197;&#21450;&#19981;&#23436;&#25972;&#30340;&#27169;&#22411;&#24573;&#30053;&#21333;&#35789;&#19982;&#20027;&#39064;&#21450;&#20027;&#39064;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#23450;&#37327;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#20294;&#26377;&#25928;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#22270;&#20027;&#39064;&#65288;G2T&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been reported that clustering-based topic models, which cluster high-quality sentence embeddings with an appropriate word selection method, can generate better topics than generative probabilistic topic models. However, these approaches suffer from the inability to select appropriate parameters and incomplete models that overlook the quantitative relation between words with topics and topics with text. To solve these issues, we propose graph to topic (G2T), a simple but effective framework for topic modelling. The framework is composed of four modules. First, document representation is acquired using pretrained language models. Second, a semantic graph is constructed according to the similarity between document representations. Third, communities in document semantic graphs are identified, and the relationship between topics and documents is quantified accordingly. Fourth, the word--topic distribution is computed based on a variant of TFIDF. Automatic evaluation suggests that G2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120; ImageCaptioner$^2$ &#65292;&#29992;&#20110;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.04874</link><description>&lt;p&gt;
ImageCaptioner$^2$: &#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#35780;&#20272;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment. (arXiv:2304.04874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120; ImageCaptioner$^2$ &#65292;&#29992;&#20110;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#23398;&#20064;&#31995;&#32479;&#37117;&#20250;&#21463;&#21040;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#36825;&#36890;&#24120;&#26469;&#33258;&#25968;&#25454;&#12289;&#27169;&#22411;&#25110;&#20004;&#32773;&#12290;&#34913;&#37327;&#21644;&#37327;&#21270;&#20559;&#24046;&#21450;&#20854;&#26469;&#28304;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#22312;&#21253;&#25324;&#35270;&#35273;&#20449;&#21495;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#20559;&#24046;&#35780;&#20272;&#25351;&#26631;&#65292;&#31216;&#20026; ImageCaptioner$^2$&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20165;&#22522;&#20110;&#29983;&#25104;&#30340;&#23383;&#24149;&#35780;&#20272;&#22270;&#20687;&#23383;&#24149;&#31639;&#27861;&#19981;&#21516;&#65292;ImageCaptioner$^2$&#22312;&#27979;&#37327;&#20559;&#24046;&#26102;&#32771;&#34385;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20844;&#24335;&#26469;&#20316;&#20026;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26469;&#27979;&#37327;&#29983;&#25104;&#23383;&#24149;&#30340;&#20559;&#24046;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most pre-trained learning systems are known to suffer from bias, which typically emerges from the data, the model, or both. Measuring and quantifying bias and its sources is a challenging task and has been extensively studied in image captioning. Despite the significant effort in this direction, we observed that existing metrics lack consistency in the inclusion of the visual signal. In this paper, we introduce a new bias assessment metric, dubbed $ImageCaptioner^2$, for image captioning. Instead of measuring the absolute bias in the model or the data, $ImageCaptioner^2$ pay more attention to the bias introduced by the model w.r.t the data bias, termed bias amplification. Unlike the existing methods, which only evaluate the image captioning algorithms based on the generated captions only, $ImageCaptioner^2$ incorporates the image while measuring the bias. In addition, we design a formulation for measuring the bias of generated captions as prompt-based image captioning instead of using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#39044;&#27979;&#22810;&#23380;&#20171;&#36136;&#30340;&#22522;&#26412;&#29305;&#24615;&#65292;&#21253;&#25324;&#23380;&#38553;&#29575;&#21644;&#26377;&#25928;&#25193;&#25955;&#31995;&#25968;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;U-Net&#26550;&#26500;&#25104;&#21151;&#37325;&#26500;&#20102;&#22810;&#23380;&#20171;&#36136;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#27987;&#24230;&#20998;&#24067;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.02104</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#22810;&#23380;&#20171;&#36136;&#20013;&#30340;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Deep learning for diffusion in porous media. (arXiv:2304.02104v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#39044;&#27979;&#22810;&#23380;&#20171;&#36136;&#30340;&#22522;&#26412;&#29305;&#24615;&#65292;&#21253;&#25324;&#23380;&#38553;&#29575;&#21644;&#26377;&#25928;&#25193;&#25955;&#31995;&#25968;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;U-Net&#26550;&#26500;&#25104;&#21151;&#37325;&#26500;&#20102;&#22810;&#23380;&#20171;&#36136;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#27987;&#24230;&#20998;&#24067;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#39044;&#27979;&#22810;&#23380;&#20171;&#36136;&#30340;&#22522;&#26412;&#29305;&#24615;&#65292;&#32771;&#34385;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20171;&#36136;&#31867;&#22411;&#65306;&#19968;&#31181;&#27169;&#25311;&#30722;&#23721;&#65292;&#21478;&#19968;&#31181;&#27169;&#25311;&#29983;&#29289;&#32452;&#32455;&#30340;&#32454;&#32990;&#22806;&#31354;&#38388;&#34893;&#29983;&#31995;&#32479;&#12290;&#37319;&#29992;Lattice Boltzmann&#26041;&#27861;&#33719;&#24471;&#24517;&#35201;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#20004;&#20010;&#20219;&#21153;&#65292;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#31995;&#32479;&#20960;&#20309;&#20998;&#26512;&#30340;&#32593;&#32476;&#39044;&#27979;&#23380;&#38553;&#29575;&#21644;&#26377;&#25928;&#25193;&#25955;&#31995;&#25968;&#65292;&#31532;&#20108;&#20010;&#20219;&#21153;&#20013;&#65292;&#32593;&#32476;&#37325;&#26500;&#20102;&#31995;&#32479;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#27987;&#24230;&#20998;&#24067;&#12290;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;CNN&#27169;&#22411;&#65306;C-Net&#21644;U-Net&#30340;&#32534;&#30721;&#22120;&#37096;&#20998;&#12290;&#20004;&#20010;&#32593;&#32476;&#22343;&#28155;&#21152;&#20102;&#33258;&#24402;&#19968;&#21270;&#27169;&#22359;&#12290;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#20165;&#22312;&#20854;&#35757;&#32451;&#30340;&#25968;&#25454;&#31867;&#22411;&#20869;&#26377;&#21512;&#29702;&#20934;&#30830;&#24615;&#12290;&#22312;&#31532;&#20108;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;ResNet&#32534;&#30721;&#22120;&#30340;U-Net&#26550;&#26500;&#37325;&#26500;&#22810;&#23380;&#20171;&#36136;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#27987;&#24230;&#20998;&#24067;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#35813;&#26550;&#26500;&#21487;&#20197;&#25104;&#21151;&#37325;&#26500;&#20004;&#31181;&#20171;&#36136;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We adopt convolutional neural networks (CNN) to predict the basic properties of the porous media. Two different media types are considered: one mimics the sandstone, and the other mimics the systems derived from the extracellular space of biological tissues. The Lattice Boltzmann Method is used to obtain the labeled data necessary for performing supervised learning. We distinguish two tasks. In the first, networks based on the analysis of the system's geometry predict porosity and effective diffusion coefficient. In the second, networks reconstruct the system's geometry and concentration map. In the first task, we propose two types of CNN models: the C-Net and the encoder part of the U-Net. Both networks are modified by adding a self-normalization module. The models predict with reasonable accuracy but only within the data type, they are trained on. For instance, the model trained on sandstone-like samples overshoots or undershoots for biological-like samples. In the second task, we pr
&lt;/p&gt;</description></item><item><title>oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17612</link><description>&lt;p&gt;
oBERTa: &#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#21644;&#21098;&#26525;&#26469;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17612
&lt;/p&gt;
&lt;p&gt;
oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;oBERTa&#35821;&#35328;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23427;&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#19994;&#32773;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;3.8&#21040;24.3&#20493;&#30340;&#26356;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;oBERTa&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#21098;&#26525;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#24037;&#20316;&#65292;&#24182;&#21033;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#65292;&#24182;&#25913;&#36827;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20197;&#22312;&#24191;&#27867;&#30340;&#20256;&#36882;&#20219;&#21153;&#19978;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#29983;&#25104;oBERTa&#26102;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;RoBERTa&#19982;BERT&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26399;&#38388;&#21098;&#26525;&#26041;&#38754;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#24494;&#35843;&#26399;&#38388;&#19981;&#22826;&#36866;&#21512;&#21387;&#32553;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;oBERTa&#22312;&#19971;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#30340;&#20351;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#36827;&#30340;&#21387;&#32553;&#25216;&#26415;&#20351;&#24471;&#32463;&#36807;&#21098;&#26525;&#30340;oBERTa&#27169;&#22411;&#33021;&#22815;&#21305;&#37197;BERTBASE&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;SQUAD V1.1&#38382;&#31572;&#25968;&#25454;&#30340;Prune OFA Large&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22238;&#25918;&#20197;&#24448;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#21644;&#31867;&#21407;&#22411;&#26469;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2303.14771</link><description>&lt;p&gt;
&#21407;&#22411;&#26679;&#26412;&#20851;&#31995;&#33976;&#39311;&#65306;&#23454;&#29616;&#26080;&#38656;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prototype-Sample Relation Distillation: Towards Replay-Free Continual Learning. (arXiv:2303.14771v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22238;&#25918;&#20197;&#24448;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#21644;&#31867;&#21407;&#22411;&#26469;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#24179;&#34913;&#26377;&#25928;&#36866;&#24212;&#21644;&#25269;&#24481;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#19968;&#20010;&#26680;&#24515;&#38590;&#39064;&#12290;&#35768;&#22810;&#26368;&#36817;&#34920;&#29616;&#26368;&#20339;&#30340;&#26041;&#27861;&#21033;&#29992;&#21508;&#31181;&#24418;&#24335;&#30340;&#20808;&#21069;&#20219;&#21153;&#25968;&#25454;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#35775;&#38382;&#20197;&#21069;&#30340;&#20219;&#21153;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#38480;&#21046;&#24615;&#65292;&#20363;&#22914;&#24403;&#20219;&#21153;&#25968;&#25454;&#26159;&#25935;&#24863;&#30340;&#25110;&#19987;&#26377;&#30340;&#26102;&#20505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#20849;&#21516;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#21644;&#31867;&#21407;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#26087;&#31867;&#21407;&#22411;&#21450;&#20854;&#20869;&#22312;&#30456;&#20284;&#24615;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20351;&#29992;&#20808;&#21069;&#20219;&#21153;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Continual learning (CL) balancing effective adaptation while combating catastrophic forgetting is a central challenge. Many of the recent best-performing methods utilize various forms of prior task data, e.g. a replay buffer, to tackle the catastrophic forgetting problem. Having access to previous task data can be restrictive in many real-world scenarios, for example when task data is sensitive or proprietary. To overcome the necessity of using previous tasks' data, in this work, we start with strong representation learning methods that have been shown to be less prone to forgetting. We propose a holistic approach to jointly learn the representation and class prototypes while maintaining the relevance of old class prototypes and their embedded similarities. Specifically, samples are mapped to an embedding space where the representations are learned using a supervised contrastive loss. Class prototypes are evolved continually in the same latent space, enabling learning and prediction
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#30340;&#29366;&#24577;&#20998;&#24067;&#26469;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#23376;&#20363;&#31243;&#30340;&#20840;&#23616;&#25506;&#32034;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#30340;&#21152;&#36895;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14623</link><description>&lt;p&gt;
&#26080;&#38656;&#24378;&#21270;&#23398;&#20064;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning without Reinforcement Learning. (arXiv:2303.14623v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14623
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#30340;&#29366;&#24577;&#20998;&#24067;&#26469;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#23376;&#20363;&#31243;&#30340;&#20840;&#23616;&#25506;&#32034;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#30340;&#21152;&#36895;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064; (IRL) &#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#23398;&#20064;&#21512;&#20046;&#36923;&#36753;&#30340;&#19987;&#23478;&#28436;&#31034;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;IRL&#26041;&#27861;&#23384;&#22312;&#35745;&#31639;&#19978;&#30340;&#24369;&#28857;&#65306;&#23427;&#20204;&#38656;&#35201;&#23558;&#35299;&#20915;&#38590;&#24230;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#20316;&#20026;&#23376;&#20363;&#31243;&#36827;&#34892;&#21453;&#22797;&#27714;&#35299;&#12290;&#36825;&#19982;&#24402;&#32422;&#30340;&#35266;&#28857;&#30456;&#30683;&#30462;&#65306;&#25105;&#20204;&#24050;&#23558;&#27169;&#20223;&#23398;&#20064;&#30340;&#36739;&#26131;&#38382;&#39064;&#24402;&#32422;&#20026;&#21453;&#22797;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#30340;&#26356;&#38590;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#24037;&#20316;&#35777;&#26126;&#65292;&#35775;&#38382;&#24378;&#31574;&#30053;&#33457;&#36153;&#26102;&#38388;&#30340;&#29366;&#24577;&#20998;&#24067;&#30340;&#20391;&#38754;&#20449;&#24687;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#35299;&#20915;RL&#38382;&#39064;&#30340;&#26679;&#26412;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#21152;&#26126;&#26234;&#30340;&#27169;&#20223;&#23398;&#20064;&#31616;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#30340;&#29366;&#24577;&#20998;&#24067;&#26469;&#32531;&#35299;RL&#23376;&#20363;&#31243;&#30340;&#20840;&#23616;&#25506;&#32034;&#37096;&#20998;&#65292;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#25351;&#25968;&#32423;&#30340;&#21152;&#36895;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#20013;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#38754;&#37117;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;IRL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) is a powerful set of techniques for imitation learning that aims to learn a reward function that rationalizes expert demonstrations. Unfortunately, traditional IRL methods suffer from a computational weakness: they require repeatedly solving a hard reinforcement learning (RL) problem as a subroutine. This is counter-intuitive from the viewpoint of reductions: we have reduced the easier problem of imitation learning to repeatedly solving the harder problem of RL. Another thread of work has proved that access to the side-information of the distribution of states where a strong policy spends time can dramatically reduce the sample and computational complexities of solving an RL problem. In this work, we demonstrate for the first time a more informed imitation learning reduction where we utilize the state distribution of the expert to alleviate the global exploration component of the RL subroutine, providing an exponential speedup in theory. In practice
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11249</link><description>&lt;p&gt;
&#20160;&#20040;&#35753;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65311;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#32416;&#32544;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#26469;&#33258;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#38024;&#23545;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#22312;&#26576;&#20123;&#29305;&#24449;&#30340;&#35268;&#33539;&#21010;&#20998;&#19979;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#25509;&#21463;&#20302;&#37327;&#23376;&#32416;&#32544;&#26102;&#65292;&#29305;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25165;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#35813;&#25968;&#25454;&#20998;&#24067;&#12290;&#20316;&#20026;&#26412;&#32467;&#26524;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25968;&#25454;&#20998;&#24067;&#36866;&#21512;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545;&#24191;&#27867;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#32416;&#32544;&#23558;&#40723;&#21169;&#24418;&#24335;&#25512;&#29702;&#30340;&#29289;&#29702;&#24037;&#20855;&#26469;&#36827;&#19968;&#27493;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;(STG-NRDE)&#65292;&#36890;&#36807;&#20004;&#20010;NRDE&#36827;&#34892;&#26102;&#31354;&#22788;&#29702;&#24182;&#32452;&#21512;&#36215;&#26469;&#26500;&#25104;&#19968;&#20010;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;27&#20010;&#22522;&#32447;&#27169;&#22411;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.10909</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Rough Differential Equations for Traffic Forecasting. (arXiv:2303.10909v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;(STG-NRDE)&#65292;&#36890;&#36807;&#20004;&#20010;NRDE&#36827;&#34892;&#26102;&#31354;&#22788;&#29702;&#24182;&#32452;&#21512;&#36215;&#26469;&#26500;&#25104;&#19968;&#20010;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;27&#20010;&#22522;&#32447;&#27169;&#22411;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#26102;&#31354;&#20219;&#21153;&#20043;&#19968;&#12290;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#26159;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#26102;&#31354;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#65288;STG-NRDE&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#65288;NRDE&#65289;&#30340;&#23545;&#25968;&#31614;&#21517;&#21464;&#25442;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#25442;&#20026;&#36739;&#30701;&#30340;&#29305;&#24449;&#21521;&#37327;&#24207;&#21015;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#24212;&#29992;&#20110;&#26102;&#31354;&#22788;&#29702;&#12290;&#25105;&#20204;&#23558;&#20004;&#31181;NRDE&#35774;&#35745;&#25104;&#19968;&#20010;&#26694;&#26550;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#23454;&#39564;&#25968;&#25454;&#38598;&#21253;&#25324;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;27&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;STG-NRDE&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20986;&#26368;&#20339;&#20934;&#30830;&#24615;&#65292;&#32988;&#36807;&#36825;27&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is one of the most popular spatio-temporal tasks in the field of machine learning. A prevalent approach in the field is to combine graph convolutional networks and recurrent neural networks for the spatio-temporal processing. There has been fierce competition and many novel methods have been proposed. In this paper, we present the method of spatio-temporal graph neural rough differential equation (STG-NRDE). Neural rough differential equations (NRDEs) are a breakthrough concept for processing time-series data. Their main concept is to use the log-signature transform to convert a time-series sample into a relatively shorter series of feature vectors. We extend the concept and design two NRDEs: one for the temporal processing and the other for the spatial processing. After that, we combine them into a single framework. We conduct experiments with 6 benchmark datasets and 27 baselines. STG-NRDE shows the best accuracy in all cases, outperforming all those 27 baselines 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#31232;&#30095;&#22870;&#21169;&#19979;&#30340;&#26368;&#22823;&#29109;&#25506;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#26368;&#22823;&#29109;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20102;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.08059</link><description>&lt;p&gt;
&#24555;&#36895;&#29575;&#30340;&#26368;&#22823;&#29109;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Rates for Maximum Entropy Exploration. (arXiv:2303.08059v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#31232;&#30095;&#22870;&#21169;&#19979;&#30340;&#26368;&#22823;&#29109;&#25506;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#26368;&#22823;&#29109;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20102;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#26410;&#30693;&#30340;&#12289;&#31232;&#30095;&#25110;&#27809;&#26377;&#22870;&#21169;&#30340;&#29615;&#22659;&#20013;&#25805;&#20316;&#26102;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#25506;&#32034;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26368;&#22823;&#29109;&#25506;&#32034;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#31867;&#22411;&#26159;&#22238;&#35775;&#29109;&#26368;&#22823;&#21270;&#65292;&#36825;&#22312;&#25240;&#25187;&#35774;&#32622;&#20013;&#24050;&#32463;&#30001;Hazan et al.&#65288;2019&#65289;&#32771;&#34385;&#36807;&#12290;&#23545;&#20110;&#36825;&#31181;&#31867;&#22411;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21338;&#24328;&#35770;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24615;&#20026;$\widetilde{\mathcal{O}}(H^3S^2A/\varepsilon^2)$&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#30340;$\varepsilon$&#20381;&#36182;&#20851;&#31995;&#65292;&#20854;&#20013;$S$&#26159;&#29366;&#24577;&#25968;&#65292;$A$&#26159;&#34892;&#21160;&#25968;&#65292;$H$&#26159;&#27599;&#20010;&#22238;&#21512;&#30340;&#38271;&#24230;&#65292;$\varepsilon$&#26159;&#26399;&#26395;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#31532;&#20108;&#31181;&#29109;&#26159;&#36712;&#36857;&#29109;&#12290;&#36825;&#20010;&#30446;&#26631;&#20989;&#25968;&#19982;&#29109;&#27491;&#21017;&#21270;MDPs&#23494;&#20999;&#30456;&#20851;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde{\mathcal{O}}(\mathrm{poly}(S,A,H)/\varepsilon)$&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#20855;&#26377;$\mathrm{poly}(S,A,H)$&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36712;&#36857;&#29109;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenge of exploration in reinforcement learning (RL) when the agent operates in an unknown environment with sparse or no rewards. In this work, we study the maximum entropy exploration problem of two different types. The first type is visitation entropy maximization previously considered by Hazan et al.(2019) in the discounted setting. For this type of exploration, we propose a game-theoretic algorithm that has $\widetilde{\mathcal{O}}(H^3S^2A/\varepsilon^2)$ sample complexity thus improving the $\varepsilon$-dependence upon existing results, where $S$ is a number of states, $A$ is a number of actions, $H$ is an episode length, and $\varepsilon$ is a desired accuracy. The second type of entropy we study is the trajectory entropy. This objective function is closely related to the entropy-regularized MDPs, and we propose a simple algorithm that has a sample complexity of order $\widetilde{\mathcal{O}}(\mathrm{poly}(S,A,H)/\varepsilon)$. Interestingly, it is the first th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ODE-Net&#22312;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#32422;&#26463;&#21442;&#25968;ODE&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#24230;&#35770;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#24182;&#38024;&#23545;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.05924</link><description>&lt;p&gt;
ODE-Net&#30340;&#21464;&#20998;&#24418;&#24335;&#65306;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21450;&#23384;&#22312;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Variational formulations of ODE-Net as a mean-field optimal control problem and existence results. (arXiv:2303.05924v2 [math.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ODE-Net&#22312;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#32422;&#26463;&#21442;&#25968;ODE&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#24230;&#35770;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#24182;&#38024;&#23545;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ODE-Net&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#65292;&#23427;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#36830;&#32493;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#29992;ODE&#20195;&#26367;DNN&#28145;&#24230;&#32467;&#26500;&#20316;&#20026;&#36830;&#32493;&#26497;&#38480;&#30340;&#24819;&#27861;&#12290;&#36825;&#20123;&#30740;&#31350;&#23558;ODE-Net&#30340;"&#23398;&#20064;"&#35270;&#20026;&#26368;&#23567;&#21270;&#30001;&#21442;&#25968;ODE&#32422;&#26463;&#30340;"&#25439;&#22833;"&#12290;&#34429;&#28982;&#38656;&#35201;&#20551;&#23450;&#35813;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#23384;&#22312;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#35814;&#32454;&#22320;&#20998;&#26512;&#20102;&#20854;&#23384;&#22312;&#24615;&#12290;&#26412;&#25991;&#23558;ODE-Net&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#20026;&#19968;&#31181;&#27979;&#24230;&#35770;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#27492;&#35752;&#35770;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#32467;&#26524;&#12290;&#24403;&#25551;&#36848;ODE-Net&#21521;&#37327;&#22330;&#30340;&#31070;&#32463;&#32593;&#32476;&#38024;&#23545;&#21487;&#23398;&#20064;&#21442;&#25968;&#26159;&#32447;&#24615;&#30340;&#26102;&#65292;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#12290;&#35777;&#26126;&#20351;&#29992;&#27979;&#24230;&#35770;&#24418;&#24335;&#21270;&#21644;&#21464;&#20998;&#27861;&#30340;&#30452;&#25509;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20854;&#27425;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20197;&#28040;&#38500;&#38024;&#23545;&#22522;&#20110;&#36793;&#30028;&#26465;&#20214;&#30340;"&#24658;&#31561;"ODE&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#19968;&#20123;&#25216;&#26415;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a mathematical analysis of ODE-Net, a continuum model of deep neural networks (DNNs). In recent years, Machine Learning researchers have introduced ideas of replacing the deep structure of DNNs with ODEs as a continuum limit. These studies regard the "learning" of ODE-Net as the minimization of a "loss" constrained by a parametric ODE. Although the existence of a minimizer for this minimization problem needs to be assumed, only a few studies have investigated its existence analytically in detail. In the present paper, the existence of a minimizer is discussed based on a formulation of ODE-Net as a measure-theoretic mean-field optimal control problem. The existence result is proved when a neural network, which describes a vector field of ODE-Net, is linear with respect to learnable parameters. The proof employs the measure-theoretic formulation combined with the direct method of Calculus of Variations. Secondly, an idealized minimization problem is proposed to remove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RepUX-Net&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#20855;&#26377;&#31616;&#21333;&#30340;&#22823;&#20869;&#26680;&#22359;&#35774;&#35745;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20855;&#26377;&#21313;&#20998;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05785</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#39057;&#29575;&#37325;&#21442;&#25968;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#19977;&#32500;&#21367;&#31215;&#26680;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Scaling Up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation. (arXiv:2303.05785v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RepUX-Net&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#20855;&#26377;&#31616;&#21333;&#30340;&#22823;&#20869;&#26680;&#22359;&#35774;&#35745;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20855;&#26377;&#21313;&#20998;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28789;&#24863;&#28304;&#33258;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#37325;&#26032;&#22238;&#24402;&#28145;&#24230;&#21367;&#31215;&#27010;&#24565;&#65292;&#25552;&#20986;&#20351;&#29992;&#22823;&#20869;&#26680;&#23610;&#23544;&#26469;&#25552;&#20379;&#22823;&#26377;&#25928;&#24863;&#21463;&#37326;&#65288;ERF&#65289;&#26469;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;$21\times21\times21$&#31561;&#22823;&#20869;&#26680;&#23610;&#23544;&#26102;&#65292;&#20998;&#21106;&#24615;&#33021;&#21487;&#33021;&#20250;&#39281;&#21644;&#29978;&#33267;&#19979;&#38477;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#20351;&#29992;&#22823;&#20869;&#26680;&#23610;&#23544;&#30340;&#21367;&#31215;&#21463;&#38480;&#20110;&#32500;&#25252;&#23616;&#37096;&#23398;&#20064;&#30340;&#26368;&#20248;&#25910;&#25947;&#24615;&#12290;&#32780;&#32467;&#26500;&#37325;&#21442;&#25968;&#21270;&#65288;SR&#65289;&#36890;&#36807;&#20351;&#29992;&#23567;&#20869;&#26680;&#24182;&#34892;&#22686;&#24378;&#20102;&#23616;&#37096;&#25910;&#25947;&#24615;&#65292;&#20294;&#26159;&#20248;&#21270;&#30340;&#23567;&#20869;&#26680;&#20998;&#25903;&#21487;&#33021;&#20250;&#38459;&#30861;&#35757;&#32451;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepUX-Net&#65292;&#36825;&#26159;&#19968;&#20010;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#20855;&#26377;&#31616;&#21333;&#30340;&#22823;&#20869;&#26680;&#22359;&#35774;&#35745;&#65292;&#20351;&#29992;6&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#27604;&#25340;&#20102;&#24403;&#21069;&#32593;&#32476;&#30340;&#26368;&#26032;&#25216;&#26415;&#65288;&#20363;&#22914;3D UX-Net&#65292;SwinUNETR&#65289;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20869;&#26680;&#37325;&#21442;&#25968;&#21270;&#21644;&#25903;&#36335;&#38271;&#24230;&#31561;&#25928;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the inspiration of vision transformers, the concept of depth-wise convolution revisits to provide a large Effective Receptive Field (ERF) using Large Kernel (LK) sizes for medical image segmentation. However, the segmentation performance might be saturated and even degraded as the kernel sizes scaled up (e.g., $21\times 21\times 21$) in a Convolutional Neural Network (CNN). We hypothesize that convolution with LK sizes is limited to maintain an optimal convergence for locality learning. While Structural Re-parameterization (SR) enhances the local convergence with small kernels in parallel, optimal small kernel branches may hinder the computational efficiency for training. In this work, we propose RepUX-Net, a pure CNN architecture with a simple large kernel block design, which competes favorably with current network state-of-the-art (SOTA) (e.g., 3D UX-Net, SwinUNETR) using 6 challenging public datasets. We derive an equivalency between kernel re-parameterization and the branch-wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Ewald&#28040;&#24687;&#20256;&#36882;&#30340;&#38750;&#23616;&#37096;&#20613;&#31435;&#21494;&#31354;&#38388;&#26041;&#26696;&#26469;&#35299;&#20915;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#20013;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#38590;&#20197;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04791</link><description>&lt;p&gt;
&#20998;&#23376;&#22270;&#30340;Ewald&#38271;&#31243;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Ewald-based Long-Range Message Passing for Molecular Graphs. (arXiv:2303.04791v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04791
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Ewald&#28040;&#24687;&#20256;&#36882;&#30340;&#38750;&#23616;&#37096;&#20613;&#31435;&#21494;&#31354;&#38388;&#26041;&#26696;&#26469;&#35299;&#20915;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#20013;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#38590;&#20197;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#20998;&#23376;&#25968;&#25454;&#20013;&#23398;&#20064;&#21183;&#33021;&#34920;&#38754;&#30340;&#31070;&#32463;&#26550;&#26500;&#26377;&#20102;&#24555;&#36895;&#30340;&#25913;&#36827;&#65292;&#36825;&#20010;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#12290;&#20854;&#20013;&#65292;&#23545;&#28040;&#24687;&#30340;&#31354;&#38388;&#36317;&#31163;&#38480;&#21046;&#26159;&#37096;&#20998;&#20248;&#21270;&#32593;&#31449;&#22823;&#23567;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#34429;&#28982;&#36825;&#31181;&#23616;&#37096;&#20851;&#27880;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20294;&#23427;&#20063;&#38459;&#30861;&#20102;&#23545;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65288;&#22914;&#38745;&#30005;&#30456;&#20114;&#20316;&#29992;&#21644;&#33539;&#24503;&#21326;&#21147;&#65289;&#30340;&#23398;&#20064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ewald&#28040;&#24687;&#20256;&#36882;&#65306;&#19968;&#31181;&#38750;&#23616;&#37096;&#20613;&#31435;&#21494;&#31354;&#38388;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#39057;&#29575;&#25130;&#26029;&#32780;&#19981;&#26159;&#36317;&#31163;&#38480;&#21046;&#20132;&#20114;&#65292;&#24182;&#22312;Ewald&#27714;&#21644;&#26041;&#27861;&#20013;&#29702;&#35770;&#19978;&#30830;&#23450;&#12290;&#23427;&#21487;&#20197;&#20316;&#20026;&#29616;&#26377;MPNN&#26550;&#26500;&#30340;&#22686;&#24378;&#65292;&#22240;&#20026;&#23427;&#35745;&#31639;&#24265;&#20215;&#19988;&#19981;&#20851;&#27880;&#26550;&#26500;&#32454;&#33410;&#12290;&#25105;&#20204;&#29992;&#22235;&#20010;&#22522;&#32447;&#27169;&#22411;&#21644;&#20004;&#20010;&#21253;&#21547;&#19981;&#21516;&#21608;&#26399;&#65288;OC20&#65289;&#21644;&#38750;&#21608;&#26399;&#32467;&#26500;&#65288;OE62&#65289;&#30340;&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;Ewald&#28040;&#24687;&#20256;&#36882;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#27604;&#22522;&#32447;&#27169;&#22411;&#26377;&#26356;&#24378;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural architectures that learn potential energy surfaces from molecular data have undergone fast improvement in recent years. A key driver of this success is the Message Passing Neural Network (MPNN) paradigm. Its favorable scaling with system size partly relies upon a spatial distance limit on messages. While this focus on locality is a useful inductive bias, it also impedes the learning of long-range interactions such as electrostatics and van der Waals forces. To address this drawback, we propose Ewald message passing: a nonlocal Fourier space scheme which limits interactions via a cutoff on frequency instead of distance, and is theoretically well-founded in the Ewald summation method. It can serve as an augmentation on top of existing MPNN architectures as it is computationally inexpensive and agnostic to architectural details. We test the approach with four baseline models and two datasets containing diverse periodic (OC20) and aperiodic structures (OE62). We observe robust impro
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#27169;&#22411;&#23450;&#21521;&#26080;&#24046;&#21035;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26497;&#38480;&#12290;&#20316;&#32773;&#24341;&#20837;&#20102;&#27169;&#22411;&#27745;&#26579;&#21487;&#36798;&#24615;&#30340;&#27010;&#24565;&#65292;&#25512;&#23548;&#20986;&#20102;&#26131;&#20110;&#35745;&#31639;&#30340;&#38408;&#20540;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#24778;&#20154;&#30340;&#30456;&#21464;&#29616;&#35937;&#65306;&#21482;&#26377;&#24403;&#27745;&#26579;&#27604;&#36229;&#36807;&#38408;&#20540;&#26102;&#65292;&#25915;&#20987;&#25165;&#33021;&#23454;&#29616;&#26576;&#20123;&#30446;&#26631;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.03592</link><description>&lt;p&gt;
&#25506;&#31350;&#27169;&#22411;&#23450;&#21521;&#26080;&#24046;&#21035;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks. (arXiv:2303.03592v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#27169;&#22411;&#23450;&#21521;&#26080;&#24046;&#21035;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26497;&#38480;&#12290;&#20316;&#32773;&#24341;&#20837;&#20102;&#27169;&#22411;&#27745;&#26579;&#21487;&#36798;&#24615;&#30340;&#27010;&#24565;&#65292;&#25512;&#23548;&#20986;&#20102;&#26131;&#20110;&#35745;&#31639;&#30340;&#38408;&#20540;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#24778;&#20154;&#30340;&#30456;&#21464;&#29616;&#35937;&#65306;&#21482;&#26377;&#24403;&#27745;&#26579;&#27604;&#36229;&#36807;&#38408;&#20540;&#26102;&#65292;&#25915;&#20987;&#25165;&#33021;&#23454;&#29616;&#26576;&#20123;&#30446;&#26631;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#24046;&#21035;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#23569;&#37327;&#25439;&#22351;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#38477;&#20302;&#27169;&#22411;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#29616;&#26377;&#25915;&#20987;&#23545;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26550;&#26500;&#20173;&#28982;&#30456;&#23545;&#26080;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#27745;&#26579;&#21487;&#36798;&#24615;&#30340;&#27010;&#24565;&#20316;&#20026;&#25506;&#32034;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#21521;&#30446;&#26631;&#21442;&#25968;&#65288;&#21363;&#27169;&#22411;&#23450;&#21521;&#25915;&#20987;&#65289;&#30340;&#22266;&#26377;&#38480;&#21046;&#30340;&#25216;&#26415;&#24037;&#20855;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#26131;&#20110;&#35745;&#31639;&#30340;&#38408;&#20540;&#65292;&#20197;&#30830;&#23450;&#21644;&#37327;&#21270;&#21463;&#27426;&#36814;&#30340;ML&#27169;&#22411;&#20043;&#38388;&#30340;&#24778;&#20154;&#30456;&#21464;&#29616;&#35937;&#65306;&#21482;&#26377;&#24403;&#27745;&#26579;&#27604;&#36229;&#36807;&#25105;&#20204;&#30340;&#38408;&#20540;&#26102;&#65292;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#25165;&#33021;&#23454;&#29616;&#26576;&#20123;&#30446;&#26631;&#21442;&#25968;&#12290;&#22312;&#29616;&#26377;&#21442;&#25968;&#25439;&#22351;&#25915;&#20987;&#30340;&#22522;&#30784;&#19978;&#65292;&#24182;&#25913;&#36827;&#20102;&#26799;&#24230;&#21462;&#28040;&#25915;&#20987;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#27979;&#35797;&#25105;&#20204;&#30340;&#36716;&#25442;&#38408;&#20540;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#24182;&#26174;&#30528;&#25913;&#21892;&#29616;&#26377;&#30340;&#26080;&#24046;&#21035;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indiscriminate data poisoning attacks aim to decrease a model's test accuracy by injecting a small amount of corrupted training data. Despite significant interest, existing attacks remain relatively ineffective against modern machine learning (ML) architectures. In this work, we introduce the notion of model poisoning reachability as a technical tool to explore the intrinsic limits of data poisoning attacks towards target parameters (i.e., model-targeted attacks). We derive an easily computable threshold to establish and quantify a surprising phase transition phenomenon among popular ML models: data poisoning attacks can achieve certain target parameters only when the poisoning ratio exceeds our threshold. Building on existing parameter corruption attacks and refining the Gradient Canceling attack, we perform extensive experiments to confirm our theoretical findings, test the predictability of our transition threshold, and significantly improve existing indiscriminate data poisoning ba
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLGD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#30340;&#33976;&#39311;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#36845;&#20195;&#20998;&#24067;&#21305;&#37197;&#26469;&#22788;&#29702;&#21516;&#27493;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02278</link><description>&lt;p&gt;
&#22522;&#20110;&#26412;&#22320;&#20840;&#23616;&#33976;&#39311;&#30340;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#34394;&#25311;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Virtual Learning on Heterogeneous Data with Local-global Distillation. (arXiv:2303.02278v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02278
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLGD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#30340;&#33976;&#39311;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#36845;&#20195;&#20998;&#24067;&#21305;&#37197;&#26469;&#22788;&#29702;&#21516;&#27493;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#32852;&#37030;&#23398;&#20064;&#24050;&#25104;&#20026;&#20998;&#24067;&#24335;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#65292;&#20294;&#22312;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#26102;&#65292;&#20854;&#24615;&#33021;&#23481;&#26131;&#20986;&#29616;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#32852;&#37030;&#23398;&#20064;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#21516;&#27493;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#31561;&#25361;&#25112;&#12290;&#36817;&#26469;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#24050;&#34987;&#30740;&#31350;&#65292;&#20197;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20445;&#30041;&#26412;&#22320;&#31169;&#26377;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#24615;&#33021;&#30340;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;FL&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#20351;&#29992;&#33976;&#39311;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#20250;&#25918;&#22823;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#26412;&#22320;&#20840;&#23616;&#33976;&#39311;&#30340;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#34394;&#25311;&#23398;&#20064;&#65288;FedLGD&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#34394;&#25311;&#25968;&#25454;&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#32452;&#21512;&#21019;&#24314;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#22788;&#29702;&#21516;&#27493;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#20998;&#24067;&#21305;&#37197;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#20174;&#20840;&#23616;&#27169;&#22411;&#20013;&#33719;&#21462;&#30693;&#35782;&#24182;&#36890;&#36807;&#27169;&#22411;&#21453;&#39304;&#26469;&#20849;&#21516;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite Federated Learning (FL)'s trend for learning machine learning models in a distributed manner, it is susceptible to performance drops when training on heterogeneous data. In addition, FL inevitability faces the challenges of synchronization, efficiency, and privacy. Recently, dataset distillation has been explored in order to improve the efficiency and scalability of FL by creating a smaller, synthetic dataset that retains the performance of a model trained on the local private datasets. We discover that using distilled local datasets can amplify the heterogeneity issue in FL. To address this, we propose a new method, called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FedLGD), which trains FL using a smaller synthetic dataset (referred as virtual data) created through a combination of local and global dataset distillation. Specifically, to handle synchronization and class imbalance, we propose iterative distribution matching to allow clients 
&lt;/p&gt;</description></item><item><title>&#36866;&#29992;&#20110;&#24314;&#31435;&#39044;&#27979;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#30103;&#39046;&#22495;&#21644;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#30340;&#21464;&#21270;&#21644;&#20256;&#36755;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2303.01513</link><description>&lt;p&gt;
&#23398;&#20064;&#26426;&#22120;&#22312;&#21307;&#30103;&#21450;&#20854;&#20182;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning machines for health and beyond. (arXiv:2303.01513v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01513
&lt;/p&gt;
&lt;p&gt;
&#36866;&#29992;&#20110;&#24314;&#31435;&#39044;&#27979;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#30103;&#39046;&#22495;&#21644;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#30340;&#21464;&#21270;&#21644;&#20256;&#36755;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#25928;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#25797;&#38271;&#35782;&#21035;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#24320;&#21457;&#24448;&#24448;&#20572;&#30041;&#22312;&#21457;&#34920;&#35770;&#25991;&#12289;&#27010;&#24565;&#39564;&#35777;&#25110;&#36890;&#36807;&#26576;&#31181;&#37096;&#32626;&#27169;&#24335;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#39046;&#22495;&#37324;&#65292;&#27169;&#22411;&#30340;&#24739;&#32773;&#20154;&#21475;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#26159;&#30830;&#20445;&#20854;&#38271;&#26399;&#23433;&#20840;&#26377;&#25928;&#20351;&#29992;&#30340;&#20851;&#38190;&#12290;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26159;&#26377;&#25928;&#22320;&#35757;&#32451;&#20197;&#22312;&#21487;&#29992;&#25968;&#25454;&#38598;&#20013;&#23547;&#25214;&#27169;&#24335;&#30340;&#65292;&#22240;&#27492;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#19981;&#20250;&#22312;&#21457;&#34920;&#25110;&#37096;&#32626;&#26102;&#36798;&#21040;&#23792;&#20540;&#21518;&#22266;&#23450;&#19981;&#21464;&#12290;&#30456;&#21453;&#65292;&#25968;&#25454;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#21464;&#21270;&#32780;&#20135;&#29983;&#21464;&#21270;&#65292;&#32780;&#24403;&#27169;&#22411;&#34987;&#36816;&#24448;&#26032;&#30340;&#22320;&#26041;&#20379;&#26032;&#30340;&#20154;&#32676;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques are effective for building predictive models because they are good at identifying patterns in large datasets. Development of a model for complex real life problems often stops at the point of publication, proof of concept or when made accessible through some mode of deployment. However, a model in the medical domain risks becoming obsolete as soon as patient demographic changes. The maintenance and monitoring of predictive models post-publication is crucial to guarantee their safe and effective long term use. As machine learning techniques are effectively trained to look for patterns in available datasets, the performance of a model for complex real life problems will not peak and remain fixed at the point of publication or even point of deployment. Rather, data changes over time, and they also changed when models are transported to new places to be used by new demography.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#8220;&#23433;&#20840;&#21093;&#31163;&#8221;&#26041;&#27861;&#21152;&#36895;&#35299;&#20915;L0&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#32553;&#26494;&#24347;&#24230;&#20801;&#35768;&#26356;&#28608;&#36827;&#30340;&#21098;&#26525;&#65292;&#26174;&#33879;&#38477;&#20302;&#27714;&#35299;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2302.14471</link><description>&lt;p&gt;
&#23433;&#20840;&#21093;&#31163;L0&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Safe Peeling for L0-Regularized Least-Squares with supplementary material. (arXiv:2302.14471v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14471
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#8220;&#23433;&#20840;&#21093;&#31163;&#8221;&#26041;&#27861;&#21152;&#36895;&#35299;&#20915;L0&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#32553;&#26494;&#24347;&#24230;&#20801;&#35768;&#26356;&#28608;&#36827;&#30340;&#21098;&#26525;&#65292;&#26174;&#33879;&#38477;&#20302;&#27714;&#35299;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#23433;&#20840;&#21093;&#31163;&#8221;&#65292;&#36890;&#36807;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#21152;&#36895;&#35299;&#20915;L0&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31243;&#24207;&#20351;&#24471;&#22312;BnB&#20915;&#31574;&#26641;&#30340;&#27599;&#20010;&#33410;&#28857;&#22788;&#32771;&#34385;&#21040;&#25910;&#32553;&#26494;&#24347;&#24230;&#65292;&#22240;&#27492;&#21487;&#33021;&#20801;&#35768;&#26356;&#21152;&#28608;&#36827;&#30340;&#21098;&#26525;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25506;&#32034;&#33410;&#28857;&#25968;&#37327;&#21644;&#25972;&#20307;&#27714;&#35299;&#26102;&#38388;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new methodology dubbed ``safe peeling'' to accelerate the resolution of L0-regularized least-squares problems via a Branch-and-Bound (BnB) algorithm. Our procedure enables to tighten the convex relaxation considered at each node of the BnB decision tree and therefore potentially allows for more aggressive pruning. Numerical simulations show that our proposed methodology leads to significant gains in terms of number of nodes explored and overall solving time.s show that our proposed methodology leads to significant gains in terms of number of nodes explored and overall solving time.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22240;&#26524;&#31561;&#20445;&#26657;&#20934;&#26041;&#27861;&#21450;&#20854;&#25968;&#25454;&#26377;&#25928;&#30340;&#21464;&#20307;&#20132;&#21449;&#26657;&#20934;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#24555;&#36895;&#31283;&#20581;&#22320;&#26657;&#20934;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#30340;&#39044;&#27979;&#22120;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.14011</link><description>&lt;p&gt;
&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#30340;&#22240;&#26524;&#31561;&#20445;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal isotonic calibration for heterogeneous treatment effects. (arXiv:2302.14011v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14011
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22240;&#26524;&#31561;&#20445;&#26657;&#20934;&#26041;&#27861;&#21450;&#20854;&#25968;&#25454;&#26377;&#25928;&#30340;&#21464;&#20307;&#20132;&#21449;&#26657;&#20934;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#24555;&#36895;&#31283;&#20581;&#22320;&#26657;&#20934;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#30340;&#39044;&#27979;&#22120;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#8212;&#8212;&#22240;&#26524;&#31561;&#20445;&#26657;&#20934;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#30340;&#39044;&#27979;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20132;&#21449;&#26657;&#20934;&#65292;&#36825;&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#26657;&#20934;&#21464;&#20307;&#65292;&#28040;&#38500;&#20102;&#20445;&#30041;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#12290;&#20132;&#21449;&#26657;&#20934;&#21033;&#29992;&#20132;&#21449;&#25311;&#21512;&#30340;&#39044;&#27979;&#22120;&#65292;&#24182;&#20351;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#26657;&#20934;&#39044;&#27979;&#22120;&#12290;&#22312;&#19981;&#35201;&#27714;&#21333;&#35843;&#24615;&#30340;&#24369;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22240;&#26524;&#31561;&#20445;&#26657;&#20934;&#21644;&#20132;&#21449;&#26657;&#20934;&#37117;&#33021;&#23454;&#29616;&#24555;&#36895;&#21452;&#37325;&#31283;&#20581;&#26657;&#20934;&#36895;&#29575;&#65292;&#21482;&#35201;&#21033;&#29992;&#31867;&#20284;&#24847;&#20041;&#19979;&#31934;&#30830;&#20272;&#35745;&#20102;&#20542;&#21521;&#24471;&#20998;&#25110;&#21518;&#26524;&#22238;&#24402;&#12290;&#36825;&#31181;&#22240;&#26524;&#31561;&#20445;&#26657;&#20934;&#22120;&#21487;&#20197;&#21253;&#35013;&#22312;&#20219;&#20309;&#40657;&#30418;&#23398;&#20064;&#31639;&#27861;&#21608;&#22260;&#65292;&#25552;&#20379;&#24378;&#20581;&#21644;&#20998;&#24067;&#33258;&#30001;&#30340;&#26657;&#20934;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose causal isotonic calibration, a novel nonparametric method for calibrating predictors of heterogeneous treatment effects. Furthermore, we introduce cross-calibration, a data-efficient variant of calibration that eliminates the need for hold-out calibration sets. Cross-calibration leverages cross-fitted predictors and generates a single calibrated predictor using all available data. Under weak conditions that do not assume monotonicity, we establish that both causal isotonic calibration and cross-calibration achieve fast doubly-robust calibration rates, as long as either the propensity score or outcome regression is estimated accurately in a suitable sense. The proposed causal isotonic calibrator can be wrapped around any black-box learning algorithm, providing robust and distribution-free calibration guarantees while preserving predictive performance.
&lt;/p&gt;</description></item><item><title>PITS&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#25512;&#26029;&#30340;&#31471;&#21040;&#31471;&#38899;&#39640;&#21487;&#25511;TTS&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#22522;&#39057;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21512;&#25104;&#35821;&#38899;&#26041;&#24046;&#21644;&#38899;&#39640;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.12391</link><description>&lt;p&gt;
PITS&#65306;&#22522;&#20110;&#21464;&#20998;&#25512;&#26029;&#30340;&#26080;&#22522;&#39057;&#31471;&#21040;&#31471;&#38899;&#39640;&#21487;&#25511;TTS
&lt;/p&gt;
&lt;p&gt;
PITS: Variational Pitch Inference without Fundamental Frequency for End-to-End Pitch-controllable TTS. (arXiv:2302.12391v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12391
&lt;/p&gt;
&lt;p&gt;
PITS&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#25512;&#26029;&#30340;&#31471;&#21040;&#31471;&#38899;&#39640;&#21487;&#25511;TTS&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#22522;&#39057;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21512;&#25104;&#35821;&#38899;&#26041;&#24046;&#21644;&#38899;&#39640;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#38899;&#39640;&#21487;&#25511;TTS&#27169;&#22411;&#20381;&#36182;&#20110;&#30452;&#25509;&#24314;&#27169;&#22522;&#39057;&#65292;&#23548;&#33268;&#21512;&#25104;&#35821;&#38899;&#30340;&#26041;&#24046;&#24456;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PITS&#65292;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#23545;&#38899;&#39640;&#36827;&#34892;&#24314;&#27169;&#30340;&#31471;&#21040;&#31471;&#38899;&#39640;&#21487;&#25511;TTS&#27169;&#22411;&#12290;&#22522;&#20110;VITS&#65292;PITS&#32467;&#21512;&#20102;Yingram&#32534;&#30721;&#22120;&#65292;Yingram&#35299;&#30721;&#22120;&#20197;&#21450;&#38899;&#39640;&#31227;&#20301;&#21512;&#25104;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#38899;&#39640;&#21487;&#25511;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;PITS&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#35821;&#38899;&#19982;&#21407;&#22987;&#35821;&#38899;&#19981;&#21487;&#21306;&#20998;&#65292;&#24182;&#20855;&#26377;&#39640;&#21697;&#36136;&#30340;&#38899;&#39640;&#21487;&#25511;&#24615;&#12290;&#20195;&#30721;&#12289;&#38899;&#39057;&#31034;&#20363;&#21644;&#28436;&#31034;&#21487;&#20197;&#22312;https://github.com/anonymous-pits/pits &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous pitch-controllable text-to-speech (TTS) models rely on directly modeling fundamental frequency, leading to low variance in synthesized speech. To address this issue, we propose PITS, an end-to-end pitch-controllable TTS model that utilizes variational inference to model pitch. Based on VITS, PITS incorporates the Yingram encoder, the Yingram decoder, and adversarial training of pitch-shifted synthesis to achieve pitch-controllability. Experiments demonstrate that PITS generates high-quality speech that is indistinguishable from ground truth speech and has high pitch-controllability without quality degradation. Code, audio samples, and demo are available at https://github.com/anonymous-pits/pits.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;ProbConserv&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23432;&#24658;&#32422;&#26463;&#19982;&#36125;&#21494;&#26031;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23558;&#23432;&#24658;&#32422;&#26463;&#32435;&#20837;&#36890;&#29992;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#20197;&#20415;&#22312;&#23398;&#20064;&#39640;&#38590;&#24230;&#30340;PDE&#36816;&#31639;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.11002</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#20197;&#36981;&#23432;&#23432;&#24658;&#23450;&#24459;&#30340;&#29289;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Physical Models that Can Respect Conservation Laws. (arXiv:2302.11002v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11002
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;ProbConserv&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23432;&#24658;&#32422;&#26463;&#19982;&#36125;&#21494;&#26031;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23558;&#23432;&#24658;&#32422;&#26463;&#32435;&#20837;&#36890;&#29992;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#20197;&#20415;&#22312;&#23398;&#20064;&#39640;&#38590;&#24230;&#30340;PDE&#36816;&#31639;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#36817;&#19968;&#20123;&#24037;&#20316;&#38598;&#20013;&#22312;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#20449;&#24687;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#20854;&#20013;&#35768;&#22810;&#24037;&#20316;&#38598;&#20013;&#22312;&#30456;&#23545;&#8220;&#23481;&#26131;&#8221;&#30340;PDE&#31639;&#23376;&#65288;&#20363;&#22914;&#26925;&#22278;&#21644;&#25243;&#29289;&#32447;&#65289;&#19978;&#65292;&#32780;&#30456;&#23545;&#8220;&#22256;&#38590;&#8221;&#30340;PDE&#31639;&#23376;&#65288;&#20363;&#22914;&#21452;&#26354;&#32447;&#65289;&#21017;&#36739;&#23569;&#12290;&#22312;&#25968;&#20540;PDE&#26041;&#38754;&#65292;&#21518;&#19968;&#31181;&#38382;&#39064;&#31867;&#38656;&#35201;&#25511;&#21046;&#19968;&#31181;&#20307;&#31215;&#20803;&#32032;&#25110;&#23432;&#24658;&#32422;&#26463;&#31867;&#22411;&#65292;&#36825;&#34987;&#35270;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#25215;&#35834;&#65292;&#38656;&#35201;&#26080;&#32541;&#22320;&#23558;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in scientific machine learning (SciML) has focused on incorporating partial differential equation (PDE) information into the learning process. Much of this work has focused on relatively ``easy'' PDE operators (e.g., elliptic and parabolic), with less emphasis on relatively ``hard'' PDE operators (e.g., hyperbolic). Within numerical PDEs, the latter problem class requires control of a type of volume element or conservation constraint, which is known to be challenging. Delivering on the promise of SciML requires seamlessly incorporating both types of problems into the learning process. To address this issue, we propose ProbConserv, a framework for incorporating conservation constraints into a generic SciML architecture. To do so, ProbConserv combines the integral form of a conservation law with a Bayesian update. We provide a detailed analysis of ProbConserv on learning with the Generalized Porous Medium Equation (GPME), a widely-applicable parameterized family of PDEs that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DPT&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#33021;&#23454;&#29616;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;SOTA&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#19968;&#20123;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10586</link><description>&lt;p&gt;
&#20998;&#24067;&#27169;&#22411;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#22312;&#23569;&#37327;&#26631;&#31614;&#19978;&#20114;&#30456;&#21463;&#30410;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels. (arXiv:2302.10586v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DPT&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#33021;&#23454;&#29616;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;SOTA&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#19968;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#8212;&#8212;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#65292;&#35813;&#31574;&#30053;&#24314;&#31435;&#22312;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#19978;&#12290;DPT&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#20351;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#20266;&#26631;&#31614;&#65307;&#20351;&#29992;&#36825;&#20123;&#20266;&#26631;&#31614;&#35757;&#32451;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20197;&#29983;&#25104;&#20266;&#22270;&#20687;&#65307;&#24182;&#20351;&#29992;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;&#22270;&#20687;&#28151;&#21512;&#37325;&#26032;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;DPT&#22987;&#32456;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#30340;SOTA&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;ImageNet 256x256&#19978;&#65292;DPT&#30340;Fr\'echet Inception Distance&#65288;FID&#65289;&#24471;&#20998;&#20998;&#21035;&#20026;3.08&#25110;2.52&#65292;&#36229;&#36807;&#20102;&#20855;&#26377;&#23436;&#25972;&#26631;&#31614;&#30340;&#24378;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;IDDPM&#65292;CDM&#65292;ADM&#21644;LDM&#65289;&#12290;&#27492;&#22806;&#65292;DPT&#22312;ImageNet&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#31454;&#20105;&#24615;&#30340;&#21322;&#30417;&#30563;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#39030;&#32423;1&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called dual pseudo training (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images. Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\'echet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet 256x256, surpassing strong diffusion models with full labels, such as IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification tasks, achieving top-1 accuracies o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;f-DPG&#65292;&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20559;&#22909;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#35780;&#20272;&#20219;&#20309;&#30446;&#26631;&#20998;&#24067;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#21508;&#31181;&#26694;&#26550;&#21644;&#36924;&#36817;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08215</link><description>&lt;p&gt;
&#36890;&#36807;f-&#25955;&#24230;&#26368;&#23567;&#21270;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Aligning Language Models with Preferences through f-divergence Minimization. (arXiv:2302.08215v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;f-DPG&#65292;&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20559;&#22909;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#35780;&#20272;&#20219;&#20309;&#30446;&#26631;&#20998;&#24067;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#21508;&#31181;&#26694;&#26550;&#21644;&#36924;&#36817;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20559;&#22909;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#23545;&#30446;&#26631;&#20998;&#24067;&#36827;&#34892;&#36924;&#36817;&#65292;&#20197;&#26399;&#36798;&#21040;&#26576;&#31181;&#25152;&#38656;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#30446;&#26631;&#20998;&#24067;&#30340;&#20989;&#25968;&#24418;&#24335;&#21644;&#29992;&#20110;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#31639;&#27861;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;f-DPG&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#20219;&#20309;&#21487;&#35780;&#20272;&#30340;f-&#25955;&#24230;&#36924;&#36817;&#20219;&#20309;&#30446;&#26631;&#20998;&#24067;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#21508;&#31181;&#26694;&#26550;&#21644;&#36924;&#36817;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#25955;&#24230;&#30446;&#26631;&#30340;&#23454;&#38469;&#22909;&#22788;&#65292;&#24182;&#35777;&#26126;&#20102;&#27809;&#26377;&#26222;&#36866;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning language models with preferences can be posed as approximating a target distribution representing some desired behavior. Existing approaches differ both in the functional form of the target distribution and the algorithm used to approximate it. For instance, Reinforcement Learning from Human Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target distribution arising from a KL penalty in the objective. On the other hand, Generative Distributional Control (GDC) has an explicit target distribution and minimizes a forward KL from it using the Distributional Policy Gradient (DPG) algorithm. In this paper, we propose a new approach, f-DPG, which allows the use of any f-divergence to approximate any target distribution that can be evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation methods (DPG, RL with KL penalties). We show the practical benefits of various choices of divergence objectives and demonstrate that there is no universally o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#26377;&#25928;&#30005;&#38459;&#26469;&#29702;&#35299;&#21644;&#20943;&#36731;GNN&#20013;&#36807;&#24230;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24635;&#26377;&#25928;&#30005;&#38459;&#20316;&#20026;&#21387;&#32553;&#24635;&#37327;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#36827;&#32780;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#20197;&#20943;&#36731;&#36807;&#24230;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2302.06835</link><description>&lt;p&gt;
&#20174;&#26377;&#25928;&#30005;&#38459;&#30340;&#35282;&#24230;&#29702;&#35299;GNN&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Understanding Oversquashing in GNNs through the Lens of Effective Resistance. (arXiv:2302.06835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#26377;&#25928;&#30005;&#38459;&#26469;&#29702;&#35299;&#21644;&#20943;&#36731;GNN&#20013;&#36807;&#24230;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24635;&#26377;&#25928;&#30005;&#38459;&#20316;&#20026;&#21387;&#32553;&#24635;&#37327;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#36827;&#32780;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#20197;&#20943;&#36731;&#36807;&#24230;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#27969;&#34892;&#23398;&#20064;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;GNN&#25152;&#36935;&#21040;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#36807;&#24230;&#21387;&#32553;&#65292;&#21363;GNN&#22312;&#36828;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#20256;&#36882;&#20449;&#24687;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#26368;&#36817;&#65292;&#29702;&#35299;&#21644;&#32531;&#35299;&#36807;&#24230;&#21387;&#32553;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#37325;&#35270;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#26377;&#25928;&#30005;&#38459;&#26469;&#29702;&#35299;&#21644;&#20943;&#36731;&#36807;&#24230;&#21387;&#32553;&#12290;&#26377;&#25928;&#30005;&#38459;&#30452;&#35266;&#22320;&#25429;&#25417;&#22270;&#20013;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#36335;&#24452;&#30340;&#8220;&#24378;&#24230;&#8221;&#65292;&#24182;&#22312;&#22270;&#29702;&#35770;&#30340;&#35768;&#22810;&#39046;&#22495;&#20855;&#26377;&#20016;&#23500;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#24635;&#26377;&#25928;&#30005;&#38459;&#20316;&#20026;&#22270;&#20013;&#36807;&#24230;&#21387;&#32553;&#24635;&#37327;&#30340;&#38480;&#21046;&#65292;&#24182;&#20026;&#20854;&#20351;&#29992;&#25552;&#20379;&#29702;&#35770;&#39564;&#35777;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20197;&#35782;&#21035;&#35201;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#20013;&#30340;&#36793;&#20197;&#26368;&#23567;&#21270;&#24635;&#26377;&#25928;&#30005;&#38459;&#65292;&#20174;&#32780;&#20943;&#36731;&#36807;&#24230;&#21387;&#32553;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing graph neural networks (GNNs) are a popular learning architectures for graph-structured data. However, one problem GNNs experience is oversquashing, where a GNN has difficulty sending information between distant nodes. Understanding and mitigating oversquashing has recently received significant attention from the research community. In this paper, we continue this line of work by analyzing oversquashing through the lens of the effective resistance between nodes in the input graph. Effective resistance intuitively captures the ``strength'' of connection between two nodes by paths in the graph, and has a rich literature spanning many areas of graph theory. We propose to use total effective resistance as a bound of the total amount of oversquashing in a graph and provide theoretical justification for its use. We further develop an algorithm to identify edges to be added to an input graph to minimize the total effective resistance, thereby alleviating oversquashing. We provi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;ConCerNet&#65292;&#29992;&#20110;&#25552;&#39640;DNN&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#23545;&#31995;&#32479;&#19981;&#21464;&#37327;&#30340;&#33258;&#21160;&#25429;&#25417;&#21644;&#20445;&#30041;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.05783</link><description>&lt;p&gt;
ConCerNet&#65306;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#21160;&#21457;&#29616;&#23432;&#24658;&#24459;&#21644;&#21487;&#38752;&#21160;&#21147;&#23398;&#31995;&#32479;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction. (arXiv:2302.05783v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;ConCerNet&#65292;&#29992;&#20110;&#25552;&#39640;DNN&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#23545;&#31995;&#32479;&#19981;&#21464;&#37327;&#30340;&#33258;&#21160;&#25429;&#25417;&#21644;&#20445;&#30041;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#22312;&#21160;&#21147;&#23398;&#31995;&#32479;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#19981;&#36981;&#23432;&#29289;&#29702;&#32422;&#26463;&#65292;&#22914;&#23432;&#24658;&#23450;&#24459;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConCerNet&#30340;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;DNN&#30340;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#21487;&#38752;&#24615;&#65292;&#36171;&#20104;&#19981;&#21464;&#30340;&#23646;&#24615;&#12290;ConCerNet&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;:(i)&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#33258;&#21160;&#25429;&#25417;&#36712;&#36857;&#35266;&#27979;&#20013;&#30340;&#31995;&#32479;&#19981;&#21464;&#37327;(&#21363;&#23432;&#24658;&#24615;&#36136;)&#65307;(ii)&#31070;&#32463;&#25237;&#24433;&#23618;&#65292;&#20445;&#35777;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#20445;&#30041;&#23398;&#20064;&#21040;&#30340;&#19981;&#21464;&#37327;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#34920;&#31034;&#21644;&#26410;&#30693;&#31995;&#32479;&#19981;&#21464;&#37327;&#20989;&#25968;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22352;&#26631;&#35823;&#24046;&#21644;&#23432;&#24658;&#25351;&#26631;&#26041;&#38754;&#22987;&#32456;&#27604;&#22522;&#32447;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#19988;&#19981;&#20381;&#36182;&#20110;&#20808;&#21069;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#21147;&#23398;&#26041;&#38754;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have shown great capacity of modeling a dynamical system; nevertheless, they usually do not obey physics constraints such as conservation laws. This paper proposes a new learning framework named ConCerNet to improve the trustworthiness of the DNN based dynamics modeling to endow the invariant properties. ConCerNet consists of two steps: (i) a contrastive learning method to automatically capture the system invariants (i.e. conservation properties) along the trajectory observations; (ii) a neural projection layer to guarantee that the learned dynamics models preserve the learned invariants. We theoretically prove the functional relationship between the learned latent representation and the unknown system invariant function. Experiments show that our method consistently outperforms the baseline neural networks in both coordinate error and conservation metrics by a large margin. With neural network based parameterization and no dependence on prior knowledge, our 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#26469;&#20445;&#25252;&#20154;&#31867;&#21019;&#36896;&#30340;&#33402;&#26415;&#21697;&#65292;&#23545;&#25239;&#20405;&#26435;&#32773;&#21033;&#29992;&#26410;&#32463;&#25480;&#26435;&#30340;&#32472;&#30011;&#35757;&#32451;DMs&#29983;&#25104;&#31867;&#20284;&#39118;&#26684;&#30340;&#26032;&#39062;&#32472;&#30011;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04578</link><description>&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#36215;&#21040;&#20102;&#31215;&#26497;&#20316;&#29992;&#65306;&#36890;&#36807;&#23545;&#25239;&#26679;&#26412;&#38450;&#27490;&#25193;&#25955;&#27169;&#22411;&#27169;&#20223;&#32472;&#30011;
&lt;/p&gt;
&lt;p&gt;
Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples. (arXiv:2302.04578v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#26469;&#20445;&#25252;&#20154;&#31867;&#21019;&#36896;&#30340;&#33402;&#26415;&#21697;&#65292;&#23545;&#25239;&#20405;&#26435;&#32773;&#21033;&#29992;&#26410;&#32463;&#25480;&#26435;&#30340;&#32472;&#30011;&#35757;&#32451;DMs&#29983;&#25104;&#31867;&#20284;&#39118;&#26684;&#30340;&#26032;&#39062;&#32472;&#30011;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#33402;&#26415;&#39046;&#22495;&#25472;&#36215;&#20102;&#19968;&#32929;&#28909;&#28526;&#65292;&#20294;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#26032;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#21363;&#20405;&#26435;&#32773;&#21033;&#29992;&#26410;&#32463;&#25480;&#26435;&#30340;&#32472;&#30011;&#35757;&#32451;DMs&#29983;&#25104;&#31867;&#20284;&#39118;&#26684;&#30340;&#26032;&#39062;&#32472;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#26032;&#20852;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#24182;&#25552;&#20986;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20445;&#25252;&#20154;&#31867;&#21019;&#36896;&#30340;&#33402;&#26415;&#21697;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#23450;&#20041;&#21644;&#35780;&#20272;DMs&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;AdvDM&#65292;&#23427;&#36890;&#36807;&#23545;&#20174;DMs&#30340;&#21453;&#21521;&#36807;&#31243;&#20013;&#25277;&#26679;&#30340;&#19981;&#21516;&#28508;&#21464;&#37327;&#36827;&#34892;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#30340;&#23545;&#25239;&#26679;&#26412;&#36827;&#34892;&#20248;&#21270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#21487;&#20197;&#26377;&#25928;&#22320;&#38459;&#30861;DMs&#25552;&#21462;&#23427;&#20204;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#20026;&#20445;&#25252;&#20154;&#31867;&#33402;&#26415;&#23478;&#29256;&#26435;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#65292;&#20197;&#23545;&#25239;&#35013;&#22791;&#26377;DMs&#30340;&#20405;&#26435;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs to generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs. Then, based on this framework, we design a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. Extensive experiments show that the generated adversarial examples can effectively hinder DMs from extracting their features. Therefore, our method can be a powerful tool for human artists to protect their copyright against infringers equipped with DM-based AI-for-A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#20855;&#26377;&#39640;&#39118;&#38505;&#24212;&#29992;&#30340;&#20010;&#24615;&#21270;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#65292;&#19981;&#21516;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.02923</link><description>&lt;p&gt;
&#19981;&#26159;&#31070;&#22855;&#33647;&#20024;&#65292;&#32780;&#26159;&#27934;&#23519;&#21147;&#20043;&#25628;&#23547;&#65306;&#28040;&#38500;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation. (arXiv:2302.02923v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#20855;&#26377;&#39640;&#39118;&#38505;&#24212;&#29992;&#30340;&#20010;&#24615;&#21270;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#65292;&#19981;&#21516;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#32463;&#24120;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#27492;&#65292;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#20272;&#35745;&#36825;&#31181;&#25928;&#24212;&#30340;&#27169;&#22411;&#20043;&#21069;&#65292;&#38656;&#35201;&#30830;&#20449;&#24050;&#32463;&#36873;&#25321;&#20102;&#26368;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#31665;&#20013;&#30340;&#20505;&#36873;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#23454;&#36341;&#20013;&#32570;&#20047;&#21453;&#20107;&#23454;&#20449;&#24687;&#65292;&#36890;&#24120;&#26080;&#27861;&#20381;&#38752;&#26631;&#20934;&#39564;&#35777;&#25351;&#26631;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#23548;&#33268;&#20102;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#25991;&#29486;&#20013;&#24050;&#30693;&#30340;&#27169;&#22411;&#36873;&#25321;&#22256;&#22659;&#12290;&#34429;&#28982;&#26368;&#36817;&#24050;&#32463;&#30740;&#31350;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#19981;&#21516;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#30340;&#20248;&#32570;&#28857;&#30340;&#31995;&#32479;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24182;&#27809;&#26377;&#35797;&#22270;&#23459;&#24067;&#20840;&#23616;&#8220;&#32988;&#32773;&#8221;&#65292;&#32780;&#26159;&#23545;&#19981;&#21516;&#36873;&#25321;&#26631;&#20934;&#30340;&#25104;&#21151;&#21644;&#22833;&#36133;&#27169;&#24335;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#24378;&#35843;&#36873;&#25321;&#31574;&#30053;&#65292;&#20505;&#36873;&#20272;&#35745;&#37327;&#21644;&#29992;&#20110;&#27604;&#36739;&#23427;&#20204;&#30340;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized treatment effect estimates are often of interest in high-stakes applications -- thus, before deploying a model estimating such effects in practice, one needs to be sure that the best candidate from the ever-growing machine learning toolbox for this task was chosen. Unfortunately, due to the absence of counterfactual information in practice, it is usually not possible to rely on standard validation metrics for doing so, leading to a well-known model selection dilemma in the treatment effect estimation literature. While some solutions have recently been investigated, systematic understanding of the strengths and weaknesses of different model selection criteria is still lacking. In this paper, instead of attempting to declare a global `winner', we therefore empirically investigate success- and failure modes of different selection criteria. We highlight that there is a complex interplay between selection strategies, candidate estimators and the data used for comparing them, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#22240;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#32780;&#24341;&#36215;&#30340;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#21069;&#32773;&#23545;&#24212;&#20110;&#26377;&#38480;&#26041;&#24046;&#27874;&#21160;&#65292;&#32780;&#21518;&#32773;&#21487;&#20197;&#34987;&#35270;&#20026;&#27969;&#24418;&#19978;&#30340;&#26377;&#25928;&#25193;&#25955;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.02563</link><description>&lt;p&gt;
&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#24341;&#36215;&#30340;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent-Induced Drift of Representation in a Two-Layer Neural Network. (arXiv:2302.02563v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#22240;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#32780;&#24341;&#36215;&#30340;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#21069;&#32773;&#23545;&#24212;&#20110;&#26377;&#38480;&#26041;&#24046;&#27874;&#21160;&#65292;&#32780;&#21518;&#32773;&#21487;&#20197;&#34987;&#35270;&#20026;&#27969;&#24418;&#19978;&#30340;&#26377;&#25928;&#25193;&#25955;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#28418;&#31227;&#26159;&#25351;&#31070;&#32463;&#20803;&#28608;&#27963;&#38543;&#26102;&#38388;&#21464;&#21270;&#32780;&#20219;&#21153;&#34920;&#29616;&#20445;&#25345;&#31283;&#23450;&#12290;&#23613;&#31649;&#22312;&#22823;&#33041;&#21644;&#20154;&#24037;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#65292;&#20294;&#28418;&#31227;&#30340;&#26426;&#21046;&#21450;&#20854;&#24433;&#21709;&#20173;&#19981;&#23436;&#20840;&#20102;&#35299;&#12290;&#21463;&#26368;&#36817;&#22312;&#28023;&#39532;&#30382;&#36136;&#20013;&#21457;&#29616;&#30340;&#21050;&#28608;&#20381;&#36182;&#24615;&#28418;&#31227;&#30340;&#23454;&#39564;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#29702;&#35770;&#21644;&#27169;&#25311;&#30740;&#31350;&#20102;&#19968;&#20010;&#20004;&#23618;&#32447;&#24615;&#21069;&#39304;&#32593;&#32476;&#20013;&#30340;&#36825;&#31181;&#29616;&#35937;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36830;&#32493;&#22312;&#32447;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22266;&#26377;&#22122;&#22768;&#24341;&#36215;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#23558;&#23398;&#20064;&#21160;&#24577;&#20998;&#35299;&#20026;&#26368;&#23567;&#25439;&#22833;&#27969;&#24418;&#30340;&#27491;&#20999;&#31354;&#38388;&#21644;&#20999;&#31354;&#38388;&#65292;&#25105;&#20204;&#21457;&#29616;&#21069;&#32773;&#23545;&#24212;&#20110;&#26377;&#38480;&#26041;&#24046;&#27874;&#21160;&#65292;&#32780;&#21518;&#32773;&#21487;&#20197;&#34987;&#35270;&#20026;&#27969;&#24418;&#19978;&#30340;&#26377;&#25928;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#20998;&#26512;&#22320;&#35745;&#31639;&#20102;&#38544;&#34255;&#23618;&#21050;&#28608;&#34920;&#31034;&#30340;&#27874;&#21160;&#21644;&#25193;&#25955;&#31995;&#25968;&#20316;&#20026;&#32593;&#32476;&#21442;&#25968;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representational drift refers to over-time changes in neural activation accompanied by a stable task performance. Despite being observed in the brain and in artificial networks, the mechanisms of drift and its implications are not fully understood. Motivated by recent experimental findings of stimulus-dependent drift in the piriform cortex, we use theory and simulations to study this phenomenon in a two-layer linear feedforward network. Specifically, in a continual online learning scenario, we study the drift induced by the noise inherent in the Stochastic Gradient Descent (SGD). By decomposing the learning dynamics into the normal and tangent spaces of the minimum-loss manifold, we show the former corresponds to a finite variance fluctuation, while the latter could be considered as an effective diffusion process on the manifold. We analytically compute the fluctuation and the diffusion coefficients for the stimuli representations in the hidden layer as functions of network parameters 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;LSBO&#65289;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#26041;&#27861;&#21253;&#25324;&#28508;&#22312;&#19968;&#33268;&#24615;&#24863;&#30693;&#33719;&#21462;&#20989;&#25968;&#65288;LCA-AF&#65289;&#21644;&#22686;&#21152;&#19968;&#33268;&#24615;&#28857;&#30340;&#28508;&#31354;&#38388;&#29983;&#25104;&#26041;&#27861;&#65288;LCA-VAE&#65289;&#65292;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#24418;&#25104;&#20102;LCA-LSBO&#12290;&#23454;&#39564;&#35777;&#26126;LCA-LSBO&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#20840;&#26032;&#30340;&#21270;&#23398;&#35774;&#35745;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02399</link><description>&lt;p&gt;
&#22686;&#24378;&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Exploration in Latent Space Bayesian Optimization. (arXiv:2302.02399v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;LSBO&#65289;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#26041;&#27861;&#21253;&#25324;&#28508;&#22312;&#19968;&#33268;&#24615;&#24863;&#30693;&#33719;&#21462;&#20989;&#25968;&#65288;LCA-AF&#65289;&#21644;&#22686;&#21152;&#19968;&#33268;&#24615;&#28857;&#30340;&#28508;&#31354;&#38388;&#29983;&#25104;&#26041;&#27861;&#65288;LCA-VAE&#65289;&#65292;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#24418;&#25104;&#20102;LCA-LSBO&#12290;&#23454;&#39564;&#35777;&#26126;LCA-LSBO&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#20840;&#26032;&#30340;&#21270;&#23398;&#35774;&#35745;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;LSBO&#65289;&#23558;&#29983;&#25104;&#27169;&#22411;&#65288;&#36890;&#24120;&#26159;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65289;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#24863;&#20852;&#36259;&#30340;&#20840;&#26032;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#65292;&#23548;&#33268;&#20102;LSBO&#38754;&#20020;&#25361;&#25112;&#21644;&#25512;&#24191;&#33021;&#21147;&#30340;&#20943;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;LSBO&#25928;&#29575;&#24182;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#24605;&#36335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;LSBO&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#36215;&#28304;&#20110;BO-VAE&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#19968;&#33268;&#24847;&#35782;&#33719;&#21462;&#20989;&#25968;&#65288;LCA-AF&#65289;&#65292;&#21033;&#29992;LSBO&#20013;&#30340;&#19968;&#33268;&#24615;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LCA-VAE&#65292;&#19968;&#31181;&#26032;&#30340;VAE&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#20855;&#26377;&#22686;&#21152;&#30340;&#19968;&#33268;&#24615;&#28857;&#30340;&#28508;&#31354;&#38388;&#65292;&#25552;&#39640;&#20102;BO&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#32467;&#21512;LCA-VAE&#21644;LCA-AF&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;LCA-LSBO&#12290;&#23454;&#39564;&#35780;&#20272;&#35777;&#23454;&#20102;LCA-LSBO&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#20840;&#26032;&#30340;&#21270;&#23398;&#35774;&#35745;&#20219;&#21153;&#20013;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Space Bayesian Optimization (LSBO) combines generative models, typically Variational Autoencoders (VAE), with Bayesian Optimization (BO) to generate de novo objects of interest. However, LSBO faces challenges due to the mismatch between the objectives of BO and VAE, resulting in poor extrapolation capabilities. In this paper, we propose novel contributions to enhance LSBO efficiency and overcome this challenge. We first introduce the concept of latent consistency/inconsistency as a crucial problem in LSBO, arising from the BO-VAE mismatch. To address this, we propose the Latent Consistent Aware-Acquisition Function (LCA-AF) that leverages consistent regions in LSBO. Additionally, we present LCA-VAE, a novel VAE method that generates a latent space with increased consistent points, improving BO's extrapolation capabilities. Combining LCA-VAE and LCA-AF, we develop LCA-LSBO. Experimental evaluations validate the improved performance of LCA-LSBO in image generation and de-novo chem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.02209</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#30340;&#38142;&#36335;&#39044;&#27979;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Link Prediction via Relational Weisfeiler-Leman. (arXiv:2302.02209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#27169;&#22411;&#12290;&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#31616;&#21333;&#22270;&#19978;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20294;&#23545;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#30340;&#29702;&#35299;&#20173;&#28982;&#19981;&#23436;&#25972;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#31995;&#32479;&#24615;&#30340;&#29702;&#35299;&#65292;&#20197;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#31561;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#12289;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#27169;&#22411;&#65292;&#24182;&#35299;&#38145;&#20102;&#19968;&#31995;&#21015;&#20854;&#20182;&#27169;&#22411;&#12290;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#65292;&#34920;&#24449;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#20998;&#26512;&#34987;&#25193;&#23637;&#20197;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#25429;&#25417;&#30340;&#20989;&#25968;&#31867;&#36827;&#34892;&#31934;&#30830;&#36923;&#36753;&#25551;&#36848;&#12290;&#25552;&#20986;&#30340;&#29702;&#35770;&#21457;&#29616;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#65292;&#24182;&#24471;&#21040;&#20102;&#32463;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.01567</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Online Error Detection in Cyber-Physical Systems. (arXiv:2302.01567v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#24615;&#26159;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#20027;&#35201;&#30340;&#35774;&#35745;&#26631;&#20934;&#20043;&#19968;&#12290;&#36825;&#26159;&#30001;&#20110;CPS&#20013;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#20204;&#30340;&#22833;&#25928;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;CPS&#20013;&#20351;&#29992;&#24378;&#22823;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#26426;&#21046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#20256;&#32479;&#30340;&#23481;&#38169;&#26041;&#27861;&#21253;&#25324;&#20887;&#20313;&#26102;&#38388;&#12289;&#30828;&#20214;&#12289;&#20449;&#24687;&#21644;/&#25110;&#36719;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38500;&#20102;&#20302;&#38169;&#35823;&#35206;&#30422;&#29575;&#22806;&#65292;&#36824;&#20250;&#24102;&#26469;&#26497;&#22823;&#30340;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability is one of the major design criteria in Cyber-Physical Systems (CPSs). This is because of the existence of some critical applications in CPSs and their failure is catastrophic. Therefore, employing strong error detection and correction mechanisms in CPSs is inevitable. CPSs are composed of a variety of units, including sensors, networks, and microcontrollers. Each of these units is probable to be in a faulty state at any time and the occurred fault can result in erroneous output. The fault may cause the units of CPS to malfunction and eventually crash. Traditional fault-tolerant approaches include redundancy time, hardware, information, and/or software. However, these approaches impose significant overheads besides their low error coverage, which limits their applicability. In addition, the interval between error occurrence and detection is too long in these approaches. In this paper, based on Deep Reinforcement Learning (DRL), a new error detection approach is proposed that
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ScaledGD(&#120582;)&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#26102;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.01186</link><description>&lt;p&gt;
&#39044;&#26465;&#20214;&#23545;&#36229;&#21442;&#21270;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ScaledGD(&#120582;)&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#26102;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ScaledGD(&#120582;)&#26041;&#27861;&#26469;&#35299;&#20915;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#20013;&#30697;&#38453;&#21487;&#33021;&#30149;&#24577;&#20197;&#21450;&#30495;&#23454;&#31209;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#36229;&#21442;&#24335;&#34920;&#31034;&#65292;&#20174;&#19968;&#20010;&#23567;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#24418;&#24335;&#30340;&#38459;&#23612;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26469;&#23545;&#25239;&#36229;&#21442;&#21270;&#21644;&#30149;&#24577;&#26354;&#29575;&#30340;&#24433;&#21709;&#12290;&#19982;&#22522;&#20934;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30456;&#27604;&#65292;&#23613;&#31649;&#39044;&#22788;&#29702;&#38656;&#35201;&#36731;&#24494;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20294;ScaledGD&#65288;&#120582;&#65289;&#22312;&#38754;&#23545;&#30149;&#24577;&#38382;&#39064;&#26102;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#39640;&#26031;&#35774;&#35745;&#19979;&#65292;ScaledGD($\lambda$) &#20250;&#22312;&#20165;&#36845;&#20195;&#25968;&#23545;&#25968;&#32423;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose $\textsf{ScaledGD($\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\textsf{ScaledGD($\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\textsf{ScaledGD($\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\textsf{ScaledGD($\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270; Bellman Errors&#65292;&#21457;&#29616;&#20043;&#21069;&#30340;Bellman Errors &#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#20934;&#30830;&#30340; MSBE &#20272;&#35745;&#22120;&#65292;&#22312;&#31163;&#25955;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2302.00141</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270; Bellman Errors &#29992;&#20110;&#31163;&#32447;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Revisiting Bellman Errors for Offline Model Selection. (arXiv:2302.00141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270; Bellman Errors&#65292;&#21457;&#29616;&#20043;&#21069;&#30340;Bellman Errors &#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#20934;&#30830;&#30340; MSBE &#20272;&#35745;&#22120;&#65292;&#22312;&#31163;&#25955;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#27169;&#22411;&#36873;&#25321;&#65288;OMS&#65289;&#21363;&#22312;&#21482;&#26377;&#24050;&#35760;&#24405;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20174;&#20247;&#22810;&#31574;&#30053;&#20013;&#36873;&#25321;&#26368;&#20339;&#31574;&#30053;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#24212;&#29992;&#31163;&#32447;RL&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#32463;&#36807;&#24191;&#27867;&#25506;&#35752;&#30340;&#24819;&#27861;&#26159;&#26681;&#25454;&#30456;&#20851;Q&#20989;&#25968;&#30340;&#22343;&#26041;Bellman&#35823;&#24046;&#65288;MSBE&#65289;&#36873;&#25321;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#19968;&#30452;&#22312;&#20351;&#29992;Bellman&#35823;&#24046;&#26102;&#26080;&#27861;&#33719;&#24471;&#36275;&#22815;&#30340;OMS&#24615;&#33021;&#65292;&#23548;&#33268;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25918;&#24323;&#27492;&#24819;&#27861;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#38416;&#36848;&#20102;&#20026;&#20160;&#20040;&#20043;&#21069;&#30340;&#32467;&#26524;&#20351;&#29992;Bellman&#35823;&#24046;&#26102;&#20250;&#30475;&#21040;&#24754;&#35266;&#30340;&#32467;&#26524;&#65292;&#24182;&#30830;&#23450;&#20102;&#22522;&#20110;Bellman&#35823;&#24046;&#30340;OMS&#31639;&#27861;&#23558;&#34920;&#29616;&#33391;&#22909;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#20934;&#30830;&#30340;MSBE&#30340;&#26032;&#30340;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22312;&#19981;&#21516;&#30340;&#31163;&#25955;&#25511;&#21046;&#20219;&#21153;&#65288;&#21253;&#25324; Atari &#28216;&#25103;&#65289;&#19978;&#33719;&#24471;&#20102;&#20986;&#33394;&#30340;OMS&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline model selection (OMS), that is, choosing the best policy from a set of many policies given only logged data, is crucial for applying offline RL in real-world settings. One idea that has been extensively explored is to select policies based on the mean squared Bellman error (MSBE) of the associated Q-functions. However, previous work has struggled to obtain adequate OMS performance with Bellman errors, leading many researchers to abandon the idea. To this end, we elucidate why previous work has seen pessimistic results with Bellman errors and identify conditions under which OMS algorithms based on Bellman errors will perform well. Moreover, we develop a new estimator of the MSBE that is more accurate than prior methods. Our estimator obtains impressive OMS performance on diverse discrete control tasks, including Atari games.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.13757</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#25928;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Toward Efficient Gradient-Based Value Estimation. (arXiv:2301.13757v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#23450;&#24615;&#65292;&#20294;&#36890;&#24120;&#27604;&#26102;&#38388;&#24046;&#24322;&#65288;TD&#65289;&#23398;&#20064;&#26041;&#27861;&#24930;&#24471;&#22810;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#32531;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#34920;&#26126;&#22343;&#26041;&#36125;&#23572;&#26364;&#35823;&#24046;&#65288;MSBE&#65289;&#26159;&#19968;&#31181;&#30149;&#24577;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#40657;&#22622;&#30697;&#38453;&#20855;&#26377;&#36739;&#22823;&#30340;&#26465;&#20214;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;MSBE&#30340;&#19981;&#33391;&#26465;&#20214;&#23545;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26080;&#25209;&#22788;&#29702;&#36817;&#31471;&#26041;&#27861;&#65292;&#23427;&#36817;&#20284;&#36981;&#24490;&#39640;&#26031;&#29275;&#39039;&#26041;&#21521;&#65292;&#24182;&#22312;&#21442;&#25968;&#21270;&#26041;&#38754;&#28176;&#36817;&#40065;&#26834;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#31639;&#27861;&#31216;&#20026;RANS&#65292;&#23427;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based methods for value estimation in reinforcement learning have favorable stability properties, but they are typically much slower than Temporal Difference (TD) learning methods. We study the root causes of this slowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned loss function in the sense that its Hessian has large condition-number. To resolve the adverse effect of poor conditioning of MSBE on gradient based methods, we propose a low complexity batch-free proximal method that approximately follows the Gauss-Newton direction and is asymptotically robust to parameterization. Our main algorithm, called RANS, is efficient in the sense that it is significantly faster than the residual gradient methods while having almost the same computational complexity, and is competitive with TD on the classic problems that we tested.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20026;&#26426;&#22120;&#21487;&#34920;&#31034;&#25968;&#23383;&#26102;&#33258;&#21160;&#24494;&#20998;&#30340;&#27491;&#30830;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#24102;&#20559;&#32622;&#21442;&#25968;&#26102;&#33258;&#21160;&#24494;&#20998;&#22987;&#32456;&#27491;&#30830;&#65292;&#32473;&#20986;&#20102;&#38480;&#21046;&#19981;&#21487;&#24494;&#24615;&#22312;&#28608;&#27963;&#20989;&#25968;&#20013;&#25968;&#30446;&#30340;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#21028;&#26029;&#21442;&#25968;&#26159;&#21542;&#22312;&#19981;&#21487;&#24494;&#21442;&#25968;&#32452;&#20013;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2301.13370</link><description>&lt;p&gt;
&#20851;&#20110;&#20855;&#26377;&#26426;&#22120;&#21487;&#34920;&#31034;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#24494;&#20998;&#27491;&#30830;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Correctness of Automatic Differentiation for Neural Networks with Machine-Representable Parameters. (arXiv:2301.13370v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20026;&#26426;&#22120;&#21487;&#34920;&#31034;&#25968;&#23383;&#26102;&#33258;&#21160;&#24494;&#20998;&#30340;&#27491;&#30830;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#24102;&#20559;&#32622;&#21442;&#25968;&#26102;&#33258;&#21160;&#24494;&#20998;&#22987;&#32456;&#27491;&#30830;&#65292;&#32473;&#20986;&#20102;&#38480;&#21046;&#19981;&#21487;&#24494;&#24615;&#22312;&#28608;&#27963;&#20989;&#25968;&#20013;&#25968;&#30446;&#30340;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#21028;&#26029;&#21442;&#25968;&#26159;&#21542;&#22312;&#19981;&#21487;&#24494;&#21442;&#25968;&#32452;&#20013;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23454;&#25968;&#22495;&#19978;&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20960;&#20046;&#22987;&#32456;&#22312;&#25968;&#23398;&#19978;&#26159;&#20934;&#30830;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#32534;&#31243;&#20351;&#29992;&#30340;&#26159;&#26426;&#22120;&#21487;&#34920;&#31034;&#30340;&#25968;&#23383;&#65288;&#20363;&#22914;&#28014;&#28857;&#25968;&#65289;&#65292;&#32780;&#19981;&#26159;&#23454;&#25968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#31354;&#38388;&#20165;&#30001;&#26426;&#22120;&#21487;&#34920;&#31034;&#30340;&#25968;&#23383;&#32452;&#25104;&#26102;&#65292;&#33258;&#21160;&#24494;&#20998;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#32452;&#21487;&#33021;&#23548;&#33268;&#33258;&#21160;&#24494;&#20998;&#19981;&#27491;&#30830;&#30340;&#21442;&#25968;&#65306;&#19968;&#32452;&#26159;&#32593;&#32476;&#21487;&#24494;&#20294;&#33258;&#21160;&#24494;&#20998;&#26080;&#27861;&#35745;&#31639;&#20854;&#23548;&#25968;&#30340;&#21442;&#25968;&#32452;&#65292;&#21478;&#19968;&#32452;&#26159;&#32593;&#32476;&#19981;&#21487;&#24494;&#30340;&#21442;&#25968;&#32452;&#12290;&#23545;&#20110;&#24102;&#26377;&#20559;&#32622;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#31532;&#19968;&#32452;&#21442;&#25968;&#32452;&#22987;&#32456;&#20026;&#31354;&#12290;&#28982;&#21518;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#19978;&#38480;&#26469;&#38480;&#21046;&#31532;&#20108;&#32452;&#21442;&#25968;&#32452;&#20013;&#19981;&#21487;&#24494;&#24615;&#22312;&#28608;&#27963;&#20989;&#25968;&#20013;&#30340;&#25968;&#30446;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#26469;&#21028;&#26029;&#19968;&#20010;&#21442;&#25968;&#26159;&#21542;&#22312;&#36825;&#20010;&#21442;&#25968;&#32452;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that forward- and reverse- mode automatic differentiation (AD) over the reals is almost always correct in a mathematically precise sense. However, actual programs work with machine-representable numbers (e.g., floating-point numbers), not reals. In this paper, we study the correctness of AD when the parameter space of a neural network consists solely of machine-representable numbers. In particular, we analyze two sets of parameters on which AD can be incorrect: the incorrect set on which the network is differentiable but AD does not compute its derivative, and the non-differentiable set on which the network is non-differentiable. For a neural network with bias parameters, we first prove that the incorrect set is always empty. We then prove a tight bound on the size of the non-differentiable set, which is linear in the number of non-differentiabilities in activation functions, and give a simple necessary and sufficient condition for a parameter to be in this set. W
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#32593;&#32476;&#36739;&#20302;&#23618;&#26131;&#35745;&#31639;&#34394;&#20551;&#29305;&#24449;&#65292;&#20351;&#24471;&#26356;&#39640;&#23618;&#30340;&#32593;&#32476;&#25552;&#21462;&#21644;&#21033;&#29992;&#26356;&#20016;&#23500;&#12289;&#26356;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31616;&#21333;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2301.13293</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#31579;&#36873;&#20811;&#26381;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31616;&#21333;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Overcoming Simplicity Bias in Deep Networks using a Feature Sieve. (arXiv:2301.13293v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13293
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#32593;&#32476;&#36739;&#20302;&#23618;&#26131;&#35745;&#31639;&#34394;&#20551;&#29305;&#24449;&#65292;&#20351;&#24471;&#26356;&#39640;&#23618;&#30340;&#32593;&#32476;&#25552;&#21462;&#21644;&#21033;&#29992;&#26356;&#20016;&#23500;&#12289;&#26356;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31616;&#21333;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#20559;&#24046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#20381;&#36182;&#31616;&#21333;&#19988;&#39044;&#27979;&#24615;&#36739;&#24369;&#29305;&#24449;&#30340;&#20196;&#20154;&#25285;&#24551;&#30340;&#36235;&#21183;&#65292;&#20174;&#32780;&#25490;&#38500;&#26356;&#24378;&#12289;&#26356;&#22797;&#26434;&#30340;&#29305;&#24449;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#21644;&#34394;&#20551;&#29305;&#24449;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#20102;&#20559;&#21521;&#24615;&#12289;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#12289;&#24178;&#39044;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#31616;&#21333;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29305;&#24449;&#31579;&#36873;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#33258;&#21160;&#35782;&#21035;&#21644;&#25233;&#21046;&#32593;&#32476;&#36739;&#20302;&#23618;&#30340;&#26131;&#35745;&#31639;&#34394;&#20551;&#29305;&#24449;&#65292;&#20174;&#32780;&#35753;&#26356;&#39640;&#23618;&#30340;&#32593;&#32476;&#25552;&#21462;&#21644;&#21033;&#29992;&#26356;&#20016;&#23500;&#12289;&#26356;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25511;&#21046;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#19978;&#26377;&#20851;&#26377;&#25928;&#29305;&#24449;&#19981;&#21516;&#21387;&#21046;&#21644;&#22686;&#24378;&#30340;&#20855;&#20307;&#35777;&#25454;&#65292;&#24182;&#22312;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#21435;&#20559;&#24046;&#22522;&#20934;&#27979;&#35797;&#20013;&#25253;&#21578;&#20102;&#26174;&#33879;&#24615;&#25552;&#39640;&#65288;Imagenet-A&#30456;&#23545;&#22686;&#30410;11.4&#65285;&#65307;BAR 3.2&#65285;&#31561;&#65289;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#19981;&#20381;&#36182;&#34394;&#20551;&#23646;&#24615;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplicity bias is the concerning tendency of deep networks to over-depend on simple, weakly predictive features, to the exclusion of stronger, more complex features. This is exacerbated in real-world applications by limited training data and spurious feature-label correlations, leading to biased, incorrect predictions. We propose a direct, interventional method for addressing simplicity bias in DNNs, which we call the feature sieve. We aim to automatically identify and suppress easily-computable spurious features in lower layers of the network, thereby allowing the higher network levels to extract and utilize richer, more meaningful representations. We provide concrete evidence of this differential suppression &amp; enhancement of relevant features on both controlled datasets and real-world images, and report substantial gains on many real-world debiasing benchmarks (11.4% relative gain on Imagenet-A; 3.2% on BAR, etc). Crucially, we do not depend on prior knowledge of spurious attributes
&lt;/p&gt;</description></item><item><title>AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12132</link><description>&lt;p&gt;
AutoPEFT&#65306;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#33258;&#21160;&#37197;&#32622;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12132
&lt;/p&gt;
&lt;p&gt;
AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19987;&#38376;&#30340;&#24494;&#35843;&#29992;&#20110;&#19979;&#28216;NLP&#20219;&#21153;&#65292;&#20294;&#36825;&#26679;&#30340;&#36807;&#31243;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#26368;&#36817;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#36890;&#36807;&#26356;&#26032;&#27604;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#65288;FFT&#65289;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;PEFT&#37197;&#32622;&#26041;&#38754;&#20570;&#20986;&#26126;&#26234;&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#19981;&#23481;&#26131;&#30340;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#29978;&#33267;&#26159;PEFT&#27169;&#22359;&#25554;&#20837;&#30340;&#22270;&#23618;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#25163;&#21160;&#35774;&#35745;&#37197;&#32622;&#24456;&#21487;&#33021;&#22312;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#26041;&#38754;&#26159;&#27425;&#20248;&#30340;&#12290;&#21463;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoPEFT&#26469;&#33258;&#21160;&#36873;&#25321;PEFT&#37197;&#32622;&#65306;&#39318;&#20808;&#35774;&#35745;&#20855;&#26377;&#22810;&#20010;&#20195;&#34920;&#24615;PEFT&#27169;&#22359;&#30340;&#34920;&#36798;&#37197;&#32622;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#21518;&#20351;&#29992;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#20302;&#25104;&#26412;&#30340;&#35774;&#32622;&#65292;&#20174;&#32780;&#21457;&#29616;&#20248;&#21270;&#20219;&#21153;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#30340;Pareto&#20248;&#21270;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#19978;&#35780;&#20272;&#20102;AutoPEFT&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#25163;&#21160;&#35774;&#35745;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained language models are widely used in downstream NLP tasks via task-specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating a much smaller number of parameters compared to full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: we first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimisation in a low-cost setup, we then disc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20855;&#26377;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#65292;&#36890;&#36807;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;</title><link>http://arxiv.org/abs/2301.11526</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#26377;&#30028;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30452;&#25509;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Parameterization of Lipschitz-Bounded Deep Networks. (arXiv:2301.11526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20855;&#26377;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#65292;&#36890;&#36807;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#26041;&#24335;&#65288;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#20855;&#26377;&#26377;&#38480;&#28789;&#25935;&#24230;&#30340;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#12290;&#19982;SDP&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;"&#30452;&#25509;"&#21442;&#25968;&#21270;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#30340;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed Lipschitz bounds, i.e. limited sensitivity to perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program (SDP), which does not scale to large models. In contrast to the SDP approach, we provide a ``direct'' parameterization, i.e. a smooth mapping from $\mathbb R^N$ onto the set of weights of Lipschitz-bounded networks. This enables training via standard gradient methods, without any computationally intensive projections or barrier terms. The new parameterization can equivalently be thought of as either a new layer type (the \textit{sandwich layer}), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. Finally, the comprehensive set of experiments on image classification shows that sandwich layers outperform previous approaches on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#27969;&#65292;&#19987;&#20026;&#19977;&#32500;&#31354;&#38388;&#20013;&#22810;&#20010;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#24314;&#27169;&#32780;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#21333;&#20301;&#22235;&#20803;&#25968;&#32676;&#19978;&#23450;&#20041;&#24179;&#28369;&#21644;&#34920;&#29616;&#21147;&#24378;&#30340;&#27969;&#20197;&#21450;&#23450;&#20041;&#36866;&#24403;&#30340;&#23494;&#24230;&#65292;&#22312;&#26059;&#36716;&#32676;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#25104;&#21151;&#22320;&#37319;&#26679;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2301.11355</link><description>&lt;p&gt;
&#29992;&#20110;&#37319;&#26679;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#30340;&#21018;&#20307;&#27969;
&lt;/p&gt;
&lt;p&gt;
Rigid body flows for sampling molecular crystal structures. (arXiv:2301.11355v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#27969;&#65292;&#19987;&#20026;&#19977;&#32500;&#31354;&#38388;&#20013;&#22810;&#20010;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#24314;&#27169;&#32780;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#21333;&#20301;&#22235;&#20803;&#25968;&#32676;&#19978;&#23450;&#20041;&#24179;&#28369;&#21644;&#34920;&#29616;&#21147;&#24378;&#30340;&#27969;&#20197;&#21450;&#23450;&#20041;&#36866;&#24403;&#30340;&#23494;&#24230;&#65292;&#22312;&#26059;&#36716;&#32676;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#25104;&#21151;&#22320;&#37319;&#26679;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#27969;(NF)&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#39640;&#24230;&#28789;&#27963;&#21644;&#34920;&#29616;&#21147;&#65292;&#36817;&#24180;&#26469;&#24191;&#21463;&#27426;&#36814;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#27969;&#65292;&#19987;&#20026;&#19977;&#32500;&#31354;&#38388;&#20013;&#22810;&#20010;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#24314;&#27169;&#32780;&#35774;&#35745;&#65292;&#20363;&#22914;&#26230;&#20307;&#20013;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#24605;&#24819;:&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#21333;&#20301;&#22235;&#20803;&#25968;&#32676;&#19978;&#23450;&#20041;&#24179;&#28369;&#21644;&#34920;&#29616;&#21147;&#24378;&#30340;&#27969;&#65292;&#20174;&#32780;&#21487;&#20197;&#25429;&#25417;&#21018;&#20307;&#30340;&#36830;&#32493;&#26059;&#36716;&#36816;&#21160;;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#21333;&#20301;&#22235;&#20803;&#25968;&#30340;&#21452;&#35206;&#30422;&#29305;&#24615;&#65292;&#22312;&#26059;&#36716;&#32676;&#19978;&#23450;&#20041;&#19968;&#20010;&#36866;&#24403;&#30340;&#23494;&#24230;&#12290;&#36825;&#30830;&#20445;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#30340;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#25110;&#22522;&#20110;&#28909;&#21147;&#23398;&#30446;&#26631;&#23494;&#24230;&#30340;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#20998;&#23376;&#31034;&#20363;&#30340;Boltzmann&#29983;&#25104;&#22120;&#26469;&#35780;&#20272;&#35813;&#26041;&#27861;&#65292;&#21363;&#22235;&#38754;&#20307;&#31995;&#32479;&#30340;&#22810;&#27169;&#24577;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows (NF) are a class of powerful generative models that have gained popularity in recent years due to their ability to model complex distributions with high flexibility and expressiveness. In this work, we introduce a new type of normalizing flow that is tailored for modeling positions and orientations of multiple objects in three-dimensional space, such as molecules in a crystal. Our approach is based on two key ideas: first, we define smooth and expressive flows on the group of unit quaternions, which allows us to capture the continuous rotational motion of rigid bodies; second, we use the double cover property of unit quaternions to define a proper density on the rotation group. This ensures that our model can be trained using standard likelihood-based methods or variational inference with respect to a thermodynamic target density. We evaluate the method by training Boltzmann generators for two molecular examples, namely the multi-modal density of a tetrahedral system 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#27700;&#21360;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#25991;&#26412;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#23884;&#20837;&#20449;&#21495;&#65292;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#35813;&#25216;&#26415;&#21313;&#20998;&#40065;&#26834;&#21644;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2301.10226</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Watermark for Large Language Models. (arXiv:2301.10226v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#27700;&#21360;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#25991;&#26412;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#23884;&#20837;&#20449;&#21495;&#65292;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#35813;&#25216;&#26415;&#21313;&#20998;&#40065;&#26834;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20449;&#21495;&#65292;&#21363;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#22411;&#36755;&#20986;&#65292;&#21487;&#20197;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#22312;&#30340;&#21361;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#26694;&#26550;&#12290;&#27700;&#21360;&#21487;&#20197;&#23884;&#20837;&#21040;&#25991;&#26412;&#20013;&#65292;&#23545;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#22312;&#19981;&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;API&#25110;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26816;&#27979;&#12290;&#27700;&#21360;&#25216;&#26415;&#36890;&#36807;&#22312;&#29983;&#25104;&#21333;&#35789;&#20043;&#21069;&#36873;&#25321;&#19968;&#32452;&#38543;&#26426;&#30340;&#8220;&#32511;&#33394;&#8221;&#26631;&#35760;&#65292;&#28982;&#21518;&#22312;&#25277;&#26679;&#36807;&#31243;&#20013;&#36719;&#24615;&#22320;&#25512;&#24191;&#20351;&#29992;&#36825;&#20123;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;P&#20540;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#27700;&#21360;&#25216;&#26415;&#65292; &#24182;&#25512;&#23548;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#27700;&#21360;&#25216;&#26415;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;Open Pretrained Transformer&#65288;OPT&#65289;&#23478;&#26063;&#30340;&#19968;&#20010;&#25968;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#26469;&#27979;&#35797;&#27700;&#21360;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#21017;&#21270;&#22343;&#34913;&#65292;&#21487;&#20197;&#23558;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#25277;&#35937;&#20986;&#26469;&#24182;&#20316;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2301.09159</link><description>&lt;p&gt;
&#20174;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25277;&#35937;&#20986;&#19981;&#23436;&#32654;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Abstracting Imperfect Information Away from Two-Player Zero-Sum Games. (arXiv:2301.09159v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09159
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#22343;&#34913;&#65292;&#21487;&#20197;&#23558;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#25277;&#35937;&#20986;&#26469;&#24182;&#20316;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Nayyar&#31561;&#20154;&#22312;&#20854;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#20013;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#28216;&#25103;&#36807;&#31243;&#20013;&#35753;&#29609;&#23478;&#20844;&#24320;&#23459;&#24067;&#20854;&#31574;&#30053;&#65292;&#19981;&#23436;&#32654;&#20449;&#24687;&#21487;&#20197;&#34987;&#20174;&#20849;&#21516;&#25928;&#30410;&#28216;&#25103;&#20013;&#25277;&#35937;&#20986;&#26469;&#12290;&#36825;&#20010;&#35265;&#35299;&#26159;&#25903;&#25745;&#20849;&#21516;&#25928;&#30410;&#28216;&#25103;&#21512;&#29702;&#30340;&#27714;&#35299;&#22120;&#21644;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23558;&#21516;&#26679;&#30340;&#35265;&#35299;&#31616;&#21333;&#24212;&#29992;&#20110;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#20855;&#26377;&#20844;&#24320;&#31574;&#30053;&#23459;&#24067;&#30340;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#21487;&#33021;&#19982;&#21407;&#22987;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#19981;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#21512;&#29702;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#38656;&#35201;&#22797;&#26434;&#30340;&#39069;&#22806;&#26426;&#21046;&#65292;&#20854;&#20855;&#26377;&#19981;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23637;&#31034;&#26576;&#20123;&#27491;&#21017;&#21270;&#22343;&#34913;&#19981;&#20855;&#26377;&#19978;&#36848;&#30340;&#19981;&#23545;&#24212;&#38382;&#39064;&#65292;&#22240;&#27492;&#65292;&#35745;&#31639;&#23427;&#20204;&#21487;&#20197;&#34987;&#35270;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#12290;&#22240;&#20026;&#36825;&#20123;&#27491;&#21017;&#21270;&#22343;&#34913;&#21487;&#20197;&#34987;&#26080;&#38480;&#25509;&#36817;&#32435;&#20160;&#22343;&#34913;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In their seminal work, Nayyar et al. (2013) showed that imperfect information can be abstracted away from common-payoff games by having players publicly announce their policies as they play. This insight underpins sound solvers and decision-time planning algorithms for common-payoff games. Unfortunately, a naive application of the same insight to two-player zero-sum games fails because Nash equilibria of the game with public policy announcements may not correspond to Nash equilibria of the original game. As a consequence, existing sound decision-time planning algorithms require complicated additional mechanisms that have unappealing properties. The main contribution of this work is showing that certain regularized equilibria do not possess the aforementioned non-correspondence problem -- thus, computing them can be treated as perfect-information problems. Because these regularized equilibria can be made arbitrarily close to Nash equilibria, our result opens the door to a new perspectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23618;&#27425;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#25429;&#25417;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#22240;&#26524;&#22240;&#32032;&#30340;&#24773;&#20917;&#21464;&#21270;&#65292;&#24182;&#25506;&#35752;&#20102;&#21160;&#24577;&#20844;&#24179;&#24615;&#30340;&#23454;&#29616;&#12290;&#22312;&#25351;&#23450;&#30340;&#21160;&#24577;&#19979;&#65292;&#36890;&#24120;&#19981;&#33021;&#36890;&#36807;&#19968;&#27493;&#24178;&#39044;&#26469;&#23454;&#29616;&#38271;&#26399;&#20844;&#24179;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2301.08987</link><description>&lt;p&gt;
&#23618;&#27425;&#24179;&#34913;&#65306;&#23454;&#29616;&#22522;&#20110;&#28508;&#22312;&#22240;&#26524;&#22240;&#32032;&#30340;&#21160;&#24577;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tier Balancing: Towards Dynamic Fairness over Underlying Causal Factors. (arXiv:2301.08987v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23618;&#27425;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#25429;&#25417;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#22240;&#26524;&#22240;&#32032;&#30340;&#24773;&#20917;&#21464;&#21270;&#65292;&#24182;&#25506;&#35752;&#20102;&#21160;&#24577;&#20844;&#24179;&#24615;&#30340;&#23454;&#29616;&#12290;&#22312;&#25351;&#23450;&#30340;&#21160;&#24577;&#19979;&#65292;&#36890;&#24120;&#19981;&#33021;&#36890;&#36807;&#19968;&#27493;&#24178;&#39044;&#26469;&#23454;&#29616;&#38271;&#26399;&#20844;&#24179;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#38271;&#26399;&#20844;&#24179;&#24615;&#38656;&#35201;&#20915;&#31574;&#21644;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#20915;&#31574;&#21644;&#20998;&#24067;&#20132;&#20114;&#20316;&#29992;&#19978;&#36827;&#34892;&#22240;&#26524;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#23618;&#27425;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#20174;&#21160;&#24577;&#30340;&#35282;&#24230;&#25506;&#35752;&#23454;&#29616;&#38271;&#26399;&#20844;&#24179;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#19982;&#20043;&#21069;&#20165;&#22312;&#35266;&#23519;&#21464;&#37327;&#19978;&#23450;&#20041;&#20844;&#24179;&#27010;&#24565;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36827;&#19968;&#27493;&#25429;&#25417;&#26410;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#22240;&#26524;&#22240;&#32032;&#26041;&#38754;&#26356;&#20026;&#31934;&#30830;&#12290;&#22312;&#25351;&#23450;&#30340;&#21160;&#24577;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#24120;&#19981;&#33021;&#36890;&#36807;&#19968;&#27493;&#24178;&#39044;&#26469;&#23454;&#29616;&#38271;&#26399;&#20844;&#24179;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#22312;&#21162;&#21147;&#23454;&#29616;&#38271;&#26399;&#20844;&#24179;&#24615;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#35843;&#33410;&#20844;&#24179;&#24230;&#21644;&#24615;&#33021;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pursuit of long-term fairness involves the interplay between decision-making and the underlying data generating process. In this paper, through causal modeling with a directed acyclic graph (DAG) on the decision-distribution interplay, we investigate the possibility of achieving long-term fairness from a dynamic perspective. We propose Tier Balancing, a technically more challenging but more natural notion to achieve in the context of long-term, dynamic fairness analysis. Different from previous fairness notions that are defined purely on observed variables, our notion goes one step further, capturing behind-the-scenes situation changes on the unobserved latent causal factors that directly carry out the influence from the current decision to the future data distribution. Under the specified dynamics, we prove that in general one cannot achieve the long-term fairness goal only through one-step interventions. Furthermore, in the effort of approaching long-term fairness, we consider th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#33258;&#36866;&#24212;&#31639;&#27861;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20302;&#27604;&#29305;&#25968;&#30340;&#20256;&#36755;&#23454;&#29616;&#20102;&#26368;&#20248;&#32047;&#31215;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2301.08869</link><description>&lt;p&gt;
&#19968;&#31181;&#32047;&#31215;&#25439;&#22833;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#39640;&#25928;&#33258;&#36866;&#24212;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Communication-Efficient Adaptive Algorithm for Federated Learning under Cumulative Regret. (arXiv:2301.08869v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#33258;&#36866;&#24212;&#31639;&#27861;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20302;&#27604;&#29305;&#25968;&#30340;&#20256;&#36755;&#23454;&#29616;&#20102;&#26368;&#20248;&#32047;&#31215;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20998;&#24067;&#24335;&#35774;&#32622;&#19979;$M$&#20010;&#23458;&#25143;&#31471;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#22312;&#32447;&#38543;&#26426;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#20256;&#36755;&#30340;&#27604;&#29305;&#25968;&#20302;&#32780;&#23454;&#29616;&#20102;&#19968;&#38454;&#26368;&#20248;&#32047;&#31215;&#25439;&#22833;&#12290;&#36825;&#19982;&#29616;&#26377;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#29992;&#31616;&#21333;&#22833;&#25928;&#29575;&#26469;&#34913;&#37327;&#31163;&#32447;&#24230;&#37327;&#30340;&#20570;&#27861;&#24418;&#25104;&#20102;&#23545;&#27604;&#12290;&#32508;&#21512;&#34913;&#37327;&#36890;&#20449;&#25104;&#26412;&#30340;&#26041;&#27861;&#20063;&#19982;&#29616;&#26377;&#20570;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#20570;&#27861;&#36890;&#24120;&#23558;&#36890;&#20449;&#39057;&#29575;&#21644;&#27599;&#27425;&#36890;&#20449;&#20256;&#36755;&#30340;&#27604;&#29305;&#25968;&#20998;&#24320;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of online stochastic optimization in a distributed setting with $M$ clients connected through a central server. We develop a distributed online learning algorithm that achieves order-optimal cumulative regret with low communication cost measured in the total number of bits transmitted over the entire learning horizon. This is in contrast to existing studies which focus on the offline measure of simple regret for learning efficiency. The holistic measure for communication cost also departs from the prevailing approach that \emph{separately} tackles the communication frequency and the number of bits in each communication round.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#21487;&#20449;&#24230;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.08839</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Trustworthiness Score to Evaluate CNNs Predictions. (arXiv:2301.08839v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#21487;&#20449;&#24230;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a trustworthiness score (TS) metric to evaluate the confidence of CNN predictions, which quantifies the trustworthiness by checking for the existence of certain features in the predictions made by the CNN.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#26080;&#27861;&#22312;&#25805;&#20316;&#26399;&#38388;&#25345;&#32493;&#39564;&#35777;CNN&#65292;&#36825;&#20351;&#24471;&#24320;&#21457;&#20154;&#21592;&#21644;&#30417;&#31649;&#26426;&#26500;&#38590;&#20197;&#23545;&#20351;&#29992;CNN&#30340;&#33258;&#20027;&#31995;&#32479;&#30340;&#37096;&#32626;&#33719;&#24471;&#20449;&#24515;&#12290;&#22312;&#25805;&#20316;&#26399;&#38388;&#65292;&#20102;&#35299;CNN&#30340;&#39044;&#27979;&#20309;&#26102;&#21487;&#20449;&#25110;&#21487;&#30097;&#23545;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#26412;&#26041;&#27861;&#26159;&#20351;&#29992;&#27169;&#22411;&#30340;&#36755;&#20986;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#35780;&#20272;&#39044;&#27979;&#26159;&#21542;&#21487;&#20449;&#25110;&#21487;&#30097;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26159;&#26469;&#33258;&#40657;&#30418;&#35745;&#31639;&#30340;&#32467;&#26524;&#65292;&#22240;&#27492;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#20351;&#24471;&#24456;&#38590;&#23558;&#21487;&#20449;&#24230;&#24402;&#22240;&#20110;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#36879;&#26126;&#21644;&#26377;&#25928;&#30340;&#26041;&#24335;&#26469;&#25552;&#20379;CNN&#39044;&#27979;&#30340;&#20449;&#24515;&#12290;&#35813;&#24230;&#37327;&#26631;&#20934;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the black box nature of Convolutional neural networks (CNNs), the continuous validation of CNNs during operation is infeasible. As a result this makes it difficult for developers and regulators to gain confidence in the deployment of autonomous systems employing CNNs. It is critical for safety during operation to know when a CNN's predictions are trustworthy or suspicious. The basic approach is to use the model's output confidence score to assess if predictions are trustworthy or suspicious. However, the model's confidence score is a result of computations coming from a black box, therefore lacks transparency and makes it challenging to credit trustworthiness to predictions. We introduce the trustworthiness score (TS), a simple metric that provides a more transparent and effective way of providing confidence in CNNs predictions. The metric quantifies the trustworthiness in a prediction by checking for the existence of certain features in the predictions made by the CNN. The TS m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#36882;&#24402;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26356;&#39640;&#25928;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#26080;&#24179;&#28369;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$&#12290;</title><link>http://arxiv.org/abs/2301.06428</link><description>&lt;p&gt;
&#26080;&#24179;&#28369;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#26356;&#24555;&#26080;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization. (arXiv:2301.06428v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#36882;&#24402;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26356;&#39640;&#25928;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#26080;&#24179;&#28369;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#24418;&#22914; $\min_{x \in \mathbb{R}^d} f(x) \triangleq \mathbb{E}_{\xi} [F(x; \xi)]$ &#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20998;&#37327; $F(x;\xi)$ &#26159; $L$ &#24179;&#22343;&#22343;&#26041;&#20559;&#24046;&#30340; Lipschitz &#20294;&#21487;&#33021;&#26159;&#38750;&#20984;&#38750;&#20809;&#28369;&#20989;&#25968;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#26080;&#26799;&#24230;&#26041;&#27861;&#26368;&#22810;&#38656;&#35201; $\mathcal{O}( L^4 d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$ &#30340;&#38543;&#26426;&#38646;&#38454;&#39044;&#22788;&#29702;&#22120;&#22797;&#26434;&#24230;&#26469;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340; $(\delta,\epsilon)$-Goldstein &#38745;&#27490;&#28857;&#65292;&#20854;&#20013; $\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$&#65292;$x_0$ &#26159;&#31639;&#27861;&#30340;&#21021;&#22987;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#36882;&#24402;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#23558;&#22797;&#26434;&#24230;&#25913;&#36827;&#20026; $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the optimization problem of the form $\min_{x \in \mathbb{R}^d} f(x) \triangleq \mathbb{E}_{\xi} [F(x; \xi)]$, where the component $F(x;\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The recently proposed gradient-free method requires at most $\mathcal{O}( L^4 d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\delta,\epsilon)$-Goldstein stationary point of objective function, where $\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$.
&lt;/p&gt;</description></item><item><title>NarrowBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#31232;&#30095;&#21270;&#27169;&#22411;&#24182;&#20165;&#23545;&#25513;&#30721;&#20196;&#29260;&#36827;&#34892;&#25805;&#20316;&#65292;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#25552;&#39640;&#20102;$2\times$&#20197;&#19978;&#30340;&#21534;&#21520;&#37327;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#23558;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#22810;&#36798;$3.5\times$&#12290;NarrowBERT&#22312;&#20960;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19982;&#26631;&#20934;BERT&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2301.04761</link><description>&lt;p&gt;
NarrowBERT: &#21152;&#36895;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NarrowBERT: Accelerating Masked Language Model Pretraining and Inference. (arXiv:2301.04761v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04761
&lt;/p&gt;
&lt;p&gt;
NarrowBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#31232;&#30095;&#21270;&#27169;&#22411;&#24182;&#20165;&#23545;&#25513;&#30721;&#20196;&#29260;&#36827;&#34892;&#25805;&#20316;&#65292;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#25552;&#39640;&#20102;$2\times$&#20197;&#19978;&#30340;&#21534;&#21520;&#37327;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#23558;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#22810;&#36798;$3.5\times$&#12290;NarrowBERT&#22312;&#20960;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19982;&#26631;&#20934;BERT&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38750;&#24120;&#25104;&#21151;&#30340;&#24418;&#24335;&#65292;&#20294;&#38543;&#30528;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;&#25191;&#34892;&#39044;&#35757;&#32451;&#30340;&#25104;&#26412;&#21464;&#24471;&#36234;&#26469;&#36234;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NarrowBERT&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;$2\times$&#20197;&#19978;&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21534;&#21520;&#37327;&#12290;NarrowBERT&#31232;&#30095;&#21270;&#20102;Transformer&#27169;&#22411;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#65292;&#33258;&#27880;&#24847;&#21147;&#26597;&#35810;&#21644;&#21069;&#39304;&#23618;&#20165;&#23545;&#27599;&#20010;&#21477;&#23376;&#30340;&#25513;&#30721;&#20196;&#29260;&#36827;&#34892;&#25805;&#20316;&#65292;&#32780;&#19981;&#26159;&#20687;&#36890;&#24120;&#30340;Transformer&#32534;&#30721;&#22120;&#37027;&#26679;&#23545;&#25152;&#26377;&#20196;&#29260;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;NarrowBERT&#22312;&#25512;&#29702;&#26102;&#23558;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#22810;&#36798;$3.5\times$&#65292;&#22312;&#20687;MNLI&#36825;&#26679;&#30340;&#21477;&#23376;&#32534;&#30721;&#20219;&#21153;&#19978;&#65292;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#25110;&#27809;&#26377;&#26126;&#26174;&#38477;&#20302;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;NarrowBERT&#22312;IMDB&#21644;Amazon&#35780;&#35770;&#20998;&#31867;&#20197;&#21450;CoNLL NER&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#26631;&#20934;BERT&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than $2\times$. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as $3.5\times$ with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedAlign&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26631;&#31614;&#21644;&#25968;&#25454;&#30340;&#23618;&#38754;&#19978;&#23545;&#20110;&#38750;&#30456;&#21516;&#23458;&#25143;&#31867;&#21035;&#38598;&#21512;&#36827;&#34892;&#28508;&#22312;&#31354;&#38388;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#19981;&#21516;&#23458;&#25143;&#31471;&#26412;&#22320;&#32534;&#30721;&#22120;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.00489</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#30456;&#21516;&#23458;&#25143;&#31867;&#21035;&#38598;&#21512;&#30340;&#23545;&#40784;&#23548;&#33322;&#65306;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#21517;&#31216;&#38170;&#23450;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Navigating Alignment for Non-identical Client Class Sets: A Label Name-Anchored Federated Learning Framework. (arXiv:2301.00489v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedAlign&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26631;&#31614;&#21644;&#25968;&#25454;&#30340;&#23618;&#38754;&#19978;&#23545;&#20110;&#38750;&#30456;&#21516;&#23458;&#25143;&#31867;&#21035;&#38598;&#21512;&#36827;&#34892;&#28508;&#22312;&#31354;&#38388;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#19981;&#21516;&#23458;&#25143;&#31471;&#26412;&#22320;&#32534;&#30721;&#22120;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#20998;&#31867;&#26041;&#27861;&#21363;&#20351;&#26159;&#20026;&#38750;IID&#23458;&#25143;&#35774;&#35745;&#30340;&#65292;&#20063;&#20551;&#35774;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#26681;&#25454;&#30456;&#21516;&#30340;&#36890;&#29992;&#31867;&#38598;&#23545;&#20854;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#26356;&#19968;&#33324;&#20294;&#23454;&#29992;&#30340;&#22330;&#26223;&#8212;&#8212;&#38750;&#30456;&#21516;&#23458;&#25143;&#31867;&#38598;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20851;&#27880;&#20110;&#33258;&#24049;&#65288;&#19981;&#21516;&#29978;&#33267;&#19981;&#37325;&#21472;&#65289;&#30340;&#31867;&#38598;&#65292;&#24182;&#23547;&#27714;&#36866;&#29992;&#20110;&#36825;&#20123;&#31867;&#30340;&#32852;&#21512;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#22914;&#26524;&#23558;&#20998;&#31867;&#35270;&#20026;&#22312;&#25968;&#25454;/&#26631;&#31614;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#34920;&#31034;&#20043;&#38388;&#25214;&#21040;&#26368;&#20339;&#21305;&#37197;&#65292;&#21017;&#23458;&#25143;&#31867;&#21035;&#38598;&#21512;&#30340;&#24322;&#36136;&#24615;&#20250;&#24102;&#26469;&#26032;&#30340;&#37325;&#22823;&#25361;&#25112;-&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#32534;&#30721;&#22120;&#21487;&#33021;&#22312;&#19981;&#21516;&#29978;&#33267;&#29420;&#31435;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#20351;&#24471;&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#32858;&#21512;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;FedAlign&#65292;&#20174;&#26631;&#31614;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#26469;&#23545;&#40784;&#23458;&#25143;&#31471;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#20174;&#26631;&#31614;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#21033;&#29992;&#34920;&#36798;&#33258;&#28982;&#35821;&#35328;&#30340;&#31867;&#21517;&#20316;&#20026;&#26631;&#31614;&#32534;&#30721;&#22120;&#30340;&#20849;&#21516;&#22522;&#30784;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional federated classification methods, even those designed for non-IID clients, assume that each client annotates its local data with respect to the same universal class set. In this paper, we focus on a more general yet practical setting, non-identical client class sets, where clients focus on their own (different or even non-overlapping) class sets and seek a global model that works for the union of these classes. If one views classification as finding the best match between representations produced by data/label encoder, such heterogeneity in client class sets poses a new significant challenge -local encoders at different clients may operate in different and even independent latent spaces, making it hard to aggregate at the server. We propose a novel framework, FedAlign, to align the latent spaces across clients from both label and data perspectives. From a label perspective, we leverage the expressive natural language class names as a common ground for label encoders to an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#24335;&#27169;&#25311;&#29615;&#22659;&#30340;&#20154;&#26426;&#21327;&#21516;&#20307;&#29616;&#26234;&#33021;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#26032;&#24179;&#21488;&#65292;&#20197;&#21450;&#20154;&#31867;&#19987;&#23478;&#25351;&#23548;&#19979;&#26426;&#22120;&#20154;&#23398;&#20064;&#34920;&#29616;&#30340;&#25552;&#39640;&#21644;&#20154;&#20307;&#31034;&#33539;&#23545;&#25163;&#26415;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2301.00452</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#20114;&#24335;&#27169;&#25311;&#29615;&#22659;&#30340;&#20154;&#26426;&#21327;&#21516;&#20307;&#29616;&#26234;&#33021;&#22312;&#22806;&#31185;&#25163;&#26415;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning. (arXiv:2301.00452v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#24335;&#27169;&#25311;&#29615;&#22659;&#30340;&#20154;&#26426;&#21327;&#21516;&#20307;&#29616;&#26234;&#33021;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#26032;&#24179;&#21488;&#65292;&#20197;&#21450;&#20154;&#31867;&#19987;&#23478;&#25351;&#23548;&#19979;&#26426;&#22120;&#20154;&#23398;&#20064;&#34920;&#29616;&#30340;&#25552;&#39640;&#21644;&#20154;&#20307;&#31034;&#33539;&#23545;&#25163;&#26415;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#22806;&#31185;&#25163;&#26415;&#26426;&#22120;&#20154;&#33258;&#21160;&#21270;&#24050;&#32463;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#39044;&#26399;&#20854;&#28508;&#21147;&#33021;&#22815;&#24800;&#21450;&#22806;&#31185;&#21307;&#29983;&#12289;&#25252;&#22763;&#21644;&#24739;&#32773;&#12290;&#26368;&#36817;&#65292;&#20855;&#26377;&#20307;&#29616;&#26234;&#33021;&#30340;&#23398;&#20064;&#33539;&#24335;&#23637;&#31034;&#20102;&#23398;&#20064;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#30340;&#33391;&#22909;&#25511;&#21046;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#20307;&#29616;&#26234;&#33021;&#27169;&#25311;&#22120;&#22312;&#20419;&#36827;&#30456;&#20851;&#30740;&#31350;&#26041;&#38754;&#21457;&#25381;&#20102;&#26680;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22806;&#31185;&#25163;&#26415;&#26426;&#22120;&#20154;&#24320;&#28304;&#27169;&#25311;&#22120;&#20173;&#26410;&#36275;&#22815;&#25903;&#25345;&#36890;&#36807;&#29289;&#29702;&#36755;&#20837;&#35774;&#22791;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#65292;&#36825;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#23545;&#20154;&#20307;&#31034;&#33539;&#22914;&#20309;&#24433;&#21709;&#31574;&#30053;&#23398;&#20064;&#30340;&#26377;&#25928;&#35843;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#24335;&#27169;&#25311;&#24179;&#21488;&#30340;&#20154;&#26426;&#21327;&#21516;&#20307;&#29616;&#26234;&#33021;&#65292;&#29992;&#20110;&#22806;&#31185;&#25163;&#26415;&#26426;&#22120;&#20154;&#30340;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;&#20808;&#21069;&#21457;&#24067;&#30340;SurRoL&#27169;&#25311;&#22120;&#30340;&#24179;&#21488;&#65292;&#24182;&#19982;&#20854;&#20182;&#24037;&#31243;&#24072;&#19968;&#36215;&#24320;&#21457;&#20102;&#20960;&#20010;&#26032;&#21151;&#33021;&#65292;&#20197;&#20801;&#35768;&#36890;&#36807;&#36755;&#20837;&#35774;&#22791;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20154;&#31867;&#19987;&#23478;&#30340;&#25351;&#23548;&#19979;&#65292;&#26426;&#22120;&#20154;&#23398;&#20064;&#34920;&#29616;&#30340;&#25552;&#39640;&#65292;&#24182;&#30830;&#23450;&#20102;&#19981;&#21516;&#31867;&#22411;&#20154;&#20307;&#31034;&#33539;&#23545;&#25163;&#26415;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25511;&#21046;&#31574;&#30053;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical robot automation has attracted increasing research interest over the past decade, expecting its potential to benefit surgeons, nurses and patients. Recently, the learning paradigm of embodied intelligence has demonstrated promising ability to learn good control policies for various complex tasks, where embodied AI simulators play an essential role to facilitate relevant research. However, existing open-sourced simulators for surgical robot are still not sufficiently supporting human interactions through physical input devices, which further limits effective investigations on how the human demonstrations would affect policy learning. In this work, we study human-in-the-loop embodied intelligence with a new interactive simulation platform for surgical robot learning. Specifically, we establish our platform based on our previously released SurRoL simulator with several new features co-developed to allow high-quality human interaction via an input device. We showcase the improveme
&lt;/p&gt;</description></item><item><title>QuantArt&#26159;&#19968;&#20010;&#26032;&#30340;&#39118;&#26684;&#36716;&#31227;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#23558;&#29983;&#25104;&#30340;&#33402;&#26415;&#21697;&#30340;&#28508;&#22312;&#34920;&#31034;&#25512;&#21521;&#30495;&#23454;&#33402;&#26415;&#21697;&#20998;&#24067;&#30340;&#20013;&#24515;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#21487;&#35270;&#21270;&#20445;&#30495;&#30340;&#26679;&#24335;&#21270;&#12290;&#35813;&#26694;&#26550;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#35270;&#35273;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.10431</link><description>&lt;p&gt;
QuantArt&#65306;&#37327;&#21270;&#22270;&#20687;&#39118;&#26684;&#36716;&#31227;&#20197;&#23454;&#29616;&#35270;&#35273;&#39640;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity. (arXiv:2212.10431v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10431
&lt;/p&gt;
&lt;p&gt;
QuantArt&#26159;&#19968;&#20010;&#26032;&#30340;&#39118;&#26684;&#36716;&#31227;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#23558;&#29983;&#25104;&#30340;&#33402;&#26415;&#21697;&#30340;&#28508;&#22312;&#34920;&#31034;&#25512;&#21521;&#30495;&#23454;&#33402;&#26415;&#21697;&#20998;&#24067;&#30340;&#20013;&#24515;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#21487;&#35270;&#21270;&#20445;&#30495;&#30340;&#26679;&#24335;&#21270;&#12290;&#35813;&#26694;&#26550;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#35270;&#35273;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#39118;&#26684;&#36716;&#31227;&#31639;&#27861;&#30340;&#26426;&#21046;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#25512;&#21160;&#29983;&#25104;&#30340;&#22270;&#20687;&#26397;&#21521;&#20869;&#23481;&#21644;&#39118;&#26684;&#30340;&#39640;&#24230;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#35270;&#35273;&#20445;&#30495;&#24230;&#65292;&#21363;&#29983;&#25104;&#30340;&#33402;&#26415;&#20316;&#21697;&#24212;&#35813;&#19982;&#30495;&#23454;&#30340;&#33402;&#26415;&#21697;&#38590;&#20197;&#21306;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;QuantArt&#30340;&#26032;&#39118;&#26684;&#36716;&#31227;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39640;&#21487;&#35270;&#21270;&#20445;&#30495;&#30340;&#26679;&#24335;&#21270;&#12290;QuantArt&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#23558;&#29983;&#25104;&#30340;&#33402;&#26415;&#21697;&#30340;&#28508;&#22312;&#34920;&#31034;&#25512;&#21521;&#30495;&#23454;&#33402;&#26415;&#21697;&#20998;&#24067;&#30340;&#20013;&#24515;&#12290;&#36890;&#36807;&#34701;&#21512;&#37327;&#21270;&#21644;&#36830;&#32493;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;QuantArt&#20801;&#35768;&#22312;&#20869;&#23481;&#20445;&#30041;&#12289;&#39118;&#26684;&#30456;&#20284;&#24615;&#21644;&#35270;&#35273;&#20445;&#30495;&#24230;&#26041;&#38754;&#23545;&#29983;&#25104;&#30340;&#33402;&#26415;&#21697;&#36827;&#34892;&#28789;&#27963;&#25511;&#21046;&#12290;&#22312;&#21508;&#31181;&#39118;&#26684;&#36716;&#31227;&#35774;&#32622;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;QuantArt&#26694;&#26550;&#19982;&#29616;&#26377;&#30340;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#26356;&#39640;&#30340;&#35270;&#35273;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mechanism of existing style transfer algorithms is by minimizing a hybrid loss function to push the generated image toward high similarities in both content and style. However, this type of approach cannot guarantee visual fidelity, i.e., the generated artworks should be indistinguishable from real ones. In this paper, we devise a new style transfer framework called QuantArt for high visual-fidelity stylization. QuantArt pushes the latent representation of the generated artwork toward the centroids of the real artwork distribution with vector quantization. By fusing the quantized and continuous latent representations, QuantArt allows flexible control over the generated artworks in terms of content preservation, style similarity, and visual fidelity. Experiments on various style transfer settings show that our QuantArt framework achieves significantly higher visual fidelity compared with the existing style transfer methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01692</link><description>&lt;p&gt;
&#22312;&#22330;&#23398;&#20064;&#32773;&#33021;&#21542;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#25512;&#29702;&#27010;&#24565;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22330;&#23398;&#20064;&#32773;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#20182;&#20204;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#22914;&#26631;&#31614;&#30340;&#24773;&#24863;&#65292;&#32780;&#19981;&#26159;&#22312;&#36755;&#20837;&#20013;&#25214;&#21040;&#26032;&#30340;&#20851;&#32852;&#24615;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#23569;&#26679;&#26412;&#35780;&#20272;&#35774;&#32622;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#30340;&#22312;&#22330;&#28436;&#31034;&#26080;&#27861;&#21306;&#20998;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#24182;&#19981;&#21576;&#29616;&#36229;&#36234;&#26292;&#38706;&#20110;&#26032;&#20219;&#21153;&#20998;&#24067;&#30340;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#12290;&#25105;&#20204;&#20174;&#27880;&#37322;&#35299;&#37322;&#20013;&#25552;&#21462;&#20102;&#19968;&#32452;&#36825;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#23637;&#31034;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#33719;&#24471;&#22810;&#23569;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.  To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#20110;&#35821;&#38899;&#20998;&#31867;&#38382;&#39064;&#21644;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#20869;&#30465;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#27880;&#24847;&#21147;&#22270;&#21644;&#23884;&#20837;&#30340;&#25299;&#25169;&#21644;&#20195;&#25968;&#29305;&#24449;&#12290;&#22312;&#36825;&#20123;&#29305;&#24449;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#31616;&#21333;&#32447;&#24615;&#20998;&#31867;&#22120;&#32988;&#36807;&#31934;&#35843;&#20998;&#31867;&#22120;&#22836;&#37096;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#26368;&#20248;&#24615;&#33021;&#12290;&#25299;&#25169;&#29305;&#24449;&#33021;&#22815;&#25581;&#31034;&#35821;&#38899;Transformer&#22836;&#30340;&#21151;&#33021;&#35282;&#33394;&#65292;&#36825;&#34920;&#26126;TDA&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35821;&#38899;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.17223</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#35821;&#38899;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Topological Data Analysis for Speech Processing. (arXiv:2211.17223v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#20110;&#35821;&#38899;&#20998;&#31867;&#38382;&#39064;&#21644;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#20869;&#30465;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#27880;&#24847;&#21147;&#22270;&#21644;&#23884;&#20837;&#30340;&#25299;&#25169;&#21644;&#20195;&#25968;&#29305;&#24449;&#12290;&#22312;&#36825;&#20123;&#29305;&#24449;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#31616;&#21333;&#32447;&#24615;&#20998;&#31867;&#22120;&#32988;&#36807;&#31934;&#35843;&#20998;&#31867;&#22120;&#22836;&#37096;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#26368;&#20248;&#24615;&#33021;&#12290;&#25299;&#25169;&#29305;&#24449;&#33021;&#22815;&#25581;&#31034;&#35821;&#38899;Transformer&#22836;&#30340;&#21151;&#33021;&#35282;&#33394;&#65292;&#36825;&#34920;&#26126;TDA&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35821;&#38899;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#24212;&#29992;&#20110;&#35821;&#38899;&#20998;&#31867;&#38382;&#39064;&#21450;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;HuBERT&#30340;&#20869;&#30465;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20123;&#22522;&#20110;Transformer&#27880;&#24847;&#21147;&#22270;&#21644;&#23884;&#20837;&#30340;&#25299;&#25169;&#21644;&#20195;&#25968;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20123;&#29305;&#24449;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#31616;&#21333;&#32447;&#24615;&#20998;&#31867;&#22120;&#32988;&#36807;&#31934;&#35843;&#20998;&#31867;&#22120;&#22836;&#37096;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#22235;&#20010;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#32422;9%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#21644;5%&#30340;ERR&#25552;&#39640;&#12290;&#22312;CREMA-D&#25968;&#25454;&#38598;&#19978;&#65292;&#25552;&#20986;&#30340;&#29305;&#24449;&#38598;&#36798;&#21040;&#20102;&#20934;&#30830;&#29575;80.155&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25299;&#25169;&#29305;&#24449;&#33021;&#22815;&#25581;&#31034;&#35821;&#38899;Transformer&#22836;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#20219;&#20309;&#19979;&#28216;&#31934;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#22836;&#21487;&#21306;&#20998;&#26679;&#26412;&#26469;&#28304;&#65288;&#33258;&#28982;/&#21512;&#25104;&#65289;&#25110;&#22768;&#38899;&#23545;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;TDA&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35821;&#38899;&#20998;&#26512;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38656;&#35201;&#32467;&#26500;&#39044;&#27979;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#38468;&#24405;&#12289;&#23545;TDA&#21644;HuBERT&#27169;&#22411;&#30340;&#20171;&#32461;&#20197;&#21450;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply topological data analysis (TDA) to speech classification problems and to the introspection of a pretrained speech model, HuBERT. To this end, we introduce a number of topological and algebraic features derived from Transformer attention maps and embeddings. We show that a simple linear classifier built on top of such features outperforms a fine-tuned classification head. In particular, we achieve an improvement of about $9\%$ accuracy and $5\%$ ERR on four common datasets; on CREMA-D, the proposed feature set reaches a new state of the art performance with accuracy $80.155$. We also show that topological features are able to reveal functional roles of speech Transformer heads; e.g., we find the heads capable to distinguish between pairs of sample sources (natural/synthetic) or voices without any downstream fine-tuning. Our results demonstrate that TDA is a promising new approach for speech analysis, especially for tasks that require structural prediction. Appendices, an introd
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#37197;&#32622;&#36879;&#26126;&#27969;&#27700;&#32447;&#30340;&#24182;&#34892;&#38453;&#21015;&#65292;ArrayFlex&#65292;&#21487;&#20197;&#36873;&#25321;&#27599;&#20010;CNN&#23618;&#30340;&#26368;&#20339;&#27969;&#27700;&#32447;&#37197;&#32622;&#65292;&#20174;&#32780;&#32553;&#30701;&#25512;&#29702;&#24310;&#36831;11&#65285;&#65292;&#20351;&#29992;&#27604;&#20256;&#32479;&#22266;&#23450;&#27969;&#27700;&#32447;&#38453;&#21015;&#23569;13&#65285;&#30340;&#33021;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.12600</link><description>&lt;p&gt;
ArrayFlex:&#19968;&#31181;&#21487;&#37197;&#32622;&#36879;&#26126;&#27969;&#27700;&#32447;&#30340;&#24182;&#34892;&#38453;&#21015;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
ArrayFlex: A Systolic Array Architecture with Configurable Transparent Pipelining. (arXiv:2211.12600v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12600
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#37197;&#32622;&#36879;&#26126;&#27969;&#27700;&#32447;&#30340;&#24182;&#34892;&#38453;&#21015;&#65292;ArrayFlex&#65292;&#21487;&#20197;&#36873;&#25321;&#27599;&#20010;CNN&#23618;&#30340;&#26368;&#20339;&#27969;&#27700;&#32447;&#37197;&#32622;&#65292;&#20174;&#32780;&#32553;&#30701;&#25512;&#29702;&#24310;&#36831;11&#65285;&#65292;&#20351;&#29992;&#27604;&#20256;&#32479;&#22266;&#23450;&#27969;&#27700;&#32447;&#38453;&#21015;&#23569;13&#65285;&#30340;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26159;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#26368;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#26368;&#22823;&#31243;&#24230;&#30340;&#21487;&#20280;&#32553;&#24615;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#24212;&#35813;&#32467;&#21512;&#39640;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#27599;&#20010;CNN&#23618;&#30340;&#21367;&#31215;&#37117;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#30697;&#38453;&#20056;&#27861;&#20013;&#65292;&#24182;&#21253;&#25324;&#27599;&#23618;&#30340;&#25152;&#26377;&#36755;&#20837;&#29305;&#24449;&#21644;&#21367;&#31215;&#26680;&#65292;&#36825;&#20010;&#36807;&#31243;&#26159;&#20351;&#29992;&#24182;&#34892;&#38453;&#21015;&#36827;&#34892;&#35745;&#31639;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#24102;&#26377;&#21487;&#37197;&#32622;&#27969;&#27700;&#32447;&#30340;&#24182;&#34892;&#38453;&#21015;&#30340;&#35774;&#35745;&#65292;&#30446;&#26631;&#26159;&#36873;&#25321;&#27599;&#20010;CNN&#23618;&#30340;&#26368;&#20339;&#27969;&#27700;&#32447;&#37197;&#32622;&#12290;&#25152;&#25552;&#20986;&#30340;&#24182;&#34892;&#38453;&#21015;&#31216;&#20026;ArrayFlex&#65292;&#21487;&#20197;&#22312;&#27491;&#24120;&#25110;&#27973;&#27969;&#27700;&#32447;&#27169;&#24335;&#19979;&#36816;&#34892;&#65292;&#20174;&#32780;&#24179;&#34913;&#25191;&#34892;&#21608;&#26399;&#26102;&#38388;&#21644;&#25805;&#20316;&#26102;&#38047;&#39057;&#29575;&#12290;&#36890;&#36807;&#36873;&#25321;&#27599;&#20010;CNN&#23618;&#30340;&#36866;&#24403;&#27969;&#27700;&#32447;&#37197;&#32622;&#65292;ArrayFlex&#23558;&#26368;&#20808;&#36827;&#30340;CNN&#30340;&#25512;&#29702;&#24310;&#36831;&#24179;&#22343;&#32553;&#30701;&#20102;11&#65285;&#65292;&#19982;&#20256;&#32479;&#30340;&#22266;&#23450;&#27969;&#27700;&#32447;&#24182;&#34892;&#38453;&#21015;&#30456;&#27604;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#20351;&#29992;&#27604;&#22266;&#23450;&#27969;&#27700;&#32447;&#38453;&#21015;&#23569;13&#65285;&#30340;&#33021;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36825;&#20010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) are the state-of-the-art solution for many deep learning applications. For maximum scalability, their computation should combine high performance and energy efficiency. In practice, the convolutions of each CNN layer are mapped to a matrix multiplication that includes all input features and kernels of each layer and is computed using a systolic array. In this work, we focus on the design of a systolic array with configurable pipeline with the goal to select an optimal pipeline configuration for each CNN layer. The proposed systolic array, called ArrayFlex, can operate in normal, or in shallow pipeline mode, thus balancing the execution time in cycles and the operating clock frequency. By selecting the appropriate pipeline configuration per CNN layer, ArrayFlex reduces the inference latency of state-of-the-art CNNs by 11%, on average, as compared to a traditional fixed-pipeline systolic array. Most importantly, this result is achieved while using 13%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20351;&#20854;&#21487;&#20197;&#20174;&#22806;&#37096;&#35760;&#24518;&#26816;&#32034;&#30456;&#20851;&#22270;&#29255;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#26356;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#38598;&#25104;&#30693;&#35782;&#65292;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.12561</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Multimodal Language Modeling. (arXiv:2211.12561v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20351;&#20854;&#21487;&#20197;&#20174;&#22806;&#37096;&#35760;&#24518;&#26816;&#32034;&#30456;&#20851;&#22270;&#29255;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#26356;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#38598;&#25104;&#30693;&#35782;&#65292;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22914;DALL-E&#21644;CM3&#22312;&#25991;&#26412;&#19982;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#22467;&#33778;&#23572;&#38081;&#22612;&#30340;&#22806;&#35266;&#65289;&#23384;&#20648;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#65292;&#38656;&#35201;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#26469;&#25429;&#25417;&#26356;&#22810;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#20197;&#26356;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#38598;&#25104;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20351;&#22522;&#26412;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#29983;&#25104;&#22120;&#65289;&#33021;&#22815;&#21442;&#32771;&#30001;&#26816;&#32034;&#22120;&#20174;&#22806;&#37096;&#35760;&#24518;&#65288;&#20363;&#22914;&#32593;&#19978;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#20013;&#25552;&#21462;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#20316;&#20026;&#26816;&#32034;&#22120;&#65292;&#20351;&#29992;&#22312;LAION&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;CM3 Transformer&#20316;&#20026;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#27169;&#22411;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;CM3&#65288;RA-CM3&#65289;&#65292;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#26816;&#32034;&#21644;&#29983;&#25104;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;RA-CM3&#22312;&#22270;&#20687;&#21644;&#22270;&#29255;&#29983;&#25104;&#26041;&#38754;&#22343;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#22810;&#27169;&#24577;&#27169;&#22411;&#22914;DALL-E&#21644;CM3&#65292;&#24182;&#19988;&#20854;&#26816;&#32034;&#27169;&#22359;&#21487;&#20197;&#26377;&#25928;&#22320;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#25152;&#29983;&#25104;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate both text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption gen
&lt;/p&gt;</description></item><item><title>SmoothQuant&#26159;&#19968;&#31181;&#35757;&#32451;&#26080;&#38656;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;8&#20301;&#26435;&#37325;&#12289;8&#20301;&#28608;&#27963;&#65288;W8A8&#65289;&#37327;&#21270;&#12290;SmoothQuant&#36890;&#36807;&#25968;&#23398;&#31561;&#25928;&#36716;&#25442;&#23558;&#37327;&#21270;&#38590;&#24230;&#20174;&#28608;&#27963;&#31227;&#21040;&#26435;&#37325;&#65292;&#20351;&#24471;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;INT8&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#65292;&#20855;&#26377;&#26368;&#39640;1.56&#20493;&#21152;&#36895;&#21644;2&#20493;&#20869;&#23384;&#20943;&#23569;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.10438</link><description>&lt;p&gt;
SmoothQuant&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#39640;&#25928;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. (arXiv:2211.10438v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10438
&lt;/p&gt;
&lt;p&gt;
SmoothQuant&#26159;&#19968;&#31181;&#35757;&#32451;&#26080;&#38656;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;8&#20301;&#26435;&#37325;&#12289;8&#20301;&#28608;&#27963;&#65288;W8A8&#65289;&#37327;&#21270;&#12290;SmoothQuant&#36890;&#36807;&#25968;&#23398;&#31561;&#25928;&#36716;&#25442;&#23558;&#37327;&#21270;&#38590;&#24230;&#20174;&#28608;&#27963;&#31227;&#21040;&#26435;&#37325;&#65292;&#20351;&#24471;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;INT8&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#65292;&#20855;&#26377;&#26368;&#39640;1.56&#20493;&#21152;&#36895;&#21644;2&#20493;&#20869;&#23384;&#20943;&#23569;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21644;&#20869;&#23384;&#12290;&#37327;&#21270;&#21487;&#20197;&#20943;&#23569;&#20869;&#23384;&#24182;&#21152;&#36895;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22312;&#20445;&#25345;&#31934;&#24230;&#21644;&#30828;&#20214;&#25928;&#29575;&#30340;&#21516;&#26102;&#32500;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothQuant&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#12289;&#20445;&#25345;&#31934;&#24230;&#21644;&#36890;&#29992;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;LLMs&#30340;8&#20301;&#26435;&#37325;&#12289;8&#20301;&#28608;&#27963;&#65288;W8A8&#65289;&#37327;&#21270;&#12290;&#22522;&#20110;&#26435;&#37325;&#26131;&#20110;&#37327;&#21270;&#32780;&#28608;&#27963;&#19981;&#26131;&#37327;&#21270;&#30340;&#20107;&#23454;&#65292;SmoothQuant&#36890;&#36807;&#25968;&#23398;&#31561;&#25928;&#36716;&#25442;&#23558;&#37327;&#21270;&#38590;&#24230;&#20174;&#28608;&#27963;&#31227;&#33267;&#26435;&#37325;&#65292;&#36890;&#36807;&#31163;&#32447;&#24179;&#28369;&#28608;&#27963;&#30340;&#24322;&#24120;&#20540;&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;SmoothQuant&#20351;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;INT8&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;OPT&#12289;BLOOM&#12289;GLM&#12289;MT-NLG&#21644;LLaMA&#31995;&#21015;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;LLMs&#30340;&#26368;&#39640;1.56&#20493;&#21152;&#36895;&#21644;2&#20493;&#20869;&#23384;&#20943;&#23569;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#20250;&#26377;&#31934;&#24230;&#25439;&#22833;&#12290;SmoothQuant&#21487;&#20197;&#20026;530B LLM&#25552;&#20379;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM wi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#36924;&#36817;Calabi-Yau&#24230;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20809;&#28369;&#21644;&#22855;&#24322;K3&#26354;&#38754;&#21644;Calabi-Yau&#19977;&#20307;&#19978;&#30340;&#25968;&#20540;Ricci-flat&#24230;&#37327;&#65292;&#24182;&#35266;&#23519;&#21040;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#20110;&#35745;&#31639;&#20986;&#30340;&#25299;&#25169;&#29305;&#24449;&#30340;&#31283;&#23450;&#24615;&#26377;&#36739;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.09801</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;Calabi-Yau&#24230;&#37327;&#21644;&#26354;&#29575;
&lt;/p&gt;
&lt;p&gt;
Machine Learned Calabi-Yau Metrics and Curvature. (arXiv:2211.09801v3 [hep-th] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#36924;&#36817;Calabi-Yau&#24230;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20809;&#28369;&#21644;&#22855;&#24322;K3&#26354;&#38754;&#21644;Calabi-Yau&#19977;&#20307;&#19978;&#30340;&#25968;&#20540;Ricci-flat&#24230;&#37327;&#65292;&#24182;&#35266;&#23519;&#21040;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#20110;&#35745;&#31639;&#20986;&#30340;&#25299;&#25169;&#29305;&#24449;&#30340;&#31283;&#23450;&#24615;&#26377;&#36739;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;Ricci-flat&#65288;Calabi-Yau&#65289;&#24230;&#37327;&#26159;&#20960;&#20309;&#23398;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#24358;&#35770;&#21644;&#29616;&#35937;&#23398;&#26377;&#30528;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#26159;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#22312;&#32473;&#23450;K&#228;hler&#31867;&#20013;&#36924;&#36817;Calabi-Yau&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20809;&#28369;&#21644;&#22855;&#24322;K3&#26354;&#38754;&#21644;Calabi-Yau&#19977;&#20307;&#19978;&#30340;&#25968;&#20540;Ricci-flat&#24230;&#37327;&#12290;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#36924;&#36817;Cefal&#249;&#23478;&#26063;&#30340;&#22235;&#27425;&#22411;&#21452;&#26354;&#38754;&#21644;Dwork&#23478;&#26063;&#30340;&#20116;&#27425;&#22411;&#19977;&#20307;&#65292;&#25105;&#20204;&#30740;&#31350;&#36825;&#20123;&#20960;&#20309;&#20307;&#19978;&#30340;&#29305;&#24449;&#24418;&#24335;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25968;&#20540;&#35745;&#31639;&#20986;&#30340;&#25299;&#25169;&#29305;&#24449;&#30340;&#31283;&#23450;&#24615;&#21463;&#21040;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36873;&#25321;&#30340;&#24433;&#21709;&#36739;&#22823;&#65292;&#25105;&#20204;&#31616;&#35201;&#35752;&#35770;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21363;&#35889;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#27491;&#30830;&#36924;&#36817;Calabi-Yau&#30340;&#25299;&#25169;&#29305;&#24615;&#12290;&#20351;&#29992;&#25345;&#20037;&#21516;&#35843;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#24418;&#30340;&#39640;&#26354;&#29575;&#21306;&#22495;&#24418;&#25104;&#19968;&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding Ricci-flat (Calabi-Yau) metrics is a long standing problem in geometry with deep implications for string theory and phenomenology. A new attack on this problem uses neural networks to engineer approximations to the Calabi-Yau metric within a given K\"ahler class. In this paper we investigate numerical Ricci-flat metrics over smooth and singular K3 surfaces and Calabi-Yau threefolds. Using these Ricci-flat metric approximations for the Cefal\'u family of quartic twofolds and the Dwork family of quintic threefolds, we study characteristic forms on these geometries. We observe that the numerical stability of the numerically computed topological characteristic is heavily influenced by the choice of the neural network model, in particular, we briefly discuss a different neural network model, namely Spectral networks, which correctly approximate the topological characteristic of a Calabi-Yau. Using persistent homology, we show that high curvature regions of the manifolds form cluster
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#23545;&#20110;&#28385;&#36275;&#36125;&#23572;&#26364;&#23436;&#25972;&#24615;&#26465;&#20214;&#30340;&#35201;&#27714;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20110;&#31574;&#30053;&#22806;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.05311</link><description>&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#22806;&#23398;&#20064;&#30340;&#21487;&#23454;&#29616;&#24615;&#20309;&#26102;&#36275;&#22815;&#65311;
&lt;/p&gt;
&lt;p&gt;
When is Realizability Sufficient for Off-Policy Reinforcement Learning?. (arXiv:2211.05311v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#23545;&#20110;&#28385;&#36275;&#36125;&#23572;&#26364;&#23436;&#25972;&#24615;&#26465;&#20214;&#30340;&#35201;&#27714;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20110;&#31574;&#30053;&#22806;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#32780;&#35328;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#28385;&#36275;&#36125;&#23572;&#26364;&#23436;&#25972;&#24615;&#26465;&#20214;&#65292;&#20197;&#20415;&#22312;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#22320;&#36827;&#34892;&#22522;&#20110;&#31574;&#30053;&#22806;&#30340;&#25805;&#20316;&#12290;&#20294;&#36125;&#23572;&#26364;&#23436;&#25972;&#24615;&#26159;&#19968;&#31181;&#27604;&#23454;&#38469;&#35201;&#27714;&#26356;&#24378;&#30340;&#26465;&#20214;&#65292;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#28385;&#36275;&#12290;&#26412;&#25991;&#25918;&#26494;&#20102;&#36825;&#19968;&#26465;&#20214;&#65292;&#30740;&#31350;&#20102;&#20165;&#22312;&#25152;&#35268;&#23450;&#30340;&#20989;&#25968;&#31867;&#20855;&#26377;&#21487;&#23454;&#29616;&#24615;&#26102;&#65292;&#22522;&#20110;&#31574;&#30053;&#22806;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31574;&#30053;&#22806;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#19981;&#21463;&#36924;&#36817;&#35823;&#24046;&#24433;&#21709;&#65292;&#21462;&#20915;&#20110;&#19977;&#20010;&#22240;&#32032;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-free algorithms for reinforcement learning typically require a condition called Bellman completeness in order to successfully operate off-policy with function approximation, unless additional conditions are met. However, Bellman completeness is a requirement that is much stronger than realizability and that is deemed to be too strong to hold in practice. In this work, we relax this structural assumption and analyze the statistical complexity of off-policy reinforcement learning when only realizability holds for the prescribed function class.  We establish finite-sample guarantees for off-policy reinforcement learning that are free of the approximation error term known as inherent Bellman error, and that depend on the interplay of three factors. The first two are well known: they are the metric entropy of the function class and the concentrability coefficient that represents the cost of learning off-policy. The third factor is new, and it measures the violation of Bellman complete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#35777;&#20102;&#26597;&#35810;&#20844;&#24179;&#25351;&#26631;&#21487;&#33021;&#20250;&#27844;&#38706;&#21463;&#20445;&#25252;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2211.02139</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#28369;&#25935;&#24863;&#24230;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#20559;&#24046;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity. (arXiv:2211.02139v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#35777;&#20102;&#26597;&#35810;&#20844;&#24179;&#25351;&#26631;&#21487;&#33021;&#20250;&#27844;&#38706;&#21463;&#20445;&#25252;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#27861;&#35268;&#31105;&#27490;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;&#21463;&#20445;&#25252;&#23646;&#24615;&#65288;&#24615;&#21035;&#65292;&#31181;&#26063;&#31561;&#65289;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#22312;&#19981;&#30693;&#36947;&#20182;&#20204;&#30340;&#21463;&#20445;&#25252;&#32452;&#30340;&#24773;&#20917;&#19979;&#23545;&#20154;&#32676;&#36827;&#34892;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26426;&#26500;&#36890;&#24120;&#37319;&#29992;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#65288;&#19981;&#21487;&#35775;&#38382;&#21463;&#20445;&#25252;&#23646;&#24615;&#35757;&#32451;&#27169;&#22411;&#65289;&#21644;&#21512;&#35268;&#22242;&#38431;&#65288;&#21487;&#33021;&#20840;&#38754;&#35775;&#38382;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#23457;&#35745;&#30446;&#30340;&#65289;&#20043;&#38388;&#30340;&#20998;&#31163;&#12290;&#20294;&#26159;&#65292;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#21487;&#33021;&#34987;&#20801;&#35768;&#36890;&#36807;&#26597;&#35810;&#21512;&#35268;&#22242;&#38431;&#33719;&#21462;&#32452;&#20844;&#24179;&#24615;&#25351;&#26631;&#26469;&#27979;&#35797;&#20854;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#39318;&#20808;&#35777;&#26126;&#20102;&#20165;&#20165;&#26597;&#35810;&#20844;&#24179;&#25351;&#26631;&#65288;&#20363;&#22914;&#32479;&#35745;&#24179;&#31561;&#21644;&#24179;&#31561;&#36180;&#29575;&#65289;&#21487;&#33021;&#20250;&#27844;&#38706;&#20010;&#20154;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#32473;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#24635;&#26159;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#26597;&#35810;&#20174;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#30446;&#26631;&#20010;&#20307;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#12290;&#25105;&#20204;&#29305;&#21035;&#23637;&#31034;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23427;&#21487;&#20197;&#24179;&#28369;&#22320;&#20943;&#23567;&#25935;&#24863;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regulations prohibit model developers from accessing protected attributes (gender, race, etc.), often resulting in fairness assessments on populations without knowing their protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset for auditing purposes). However, the model developers might be allowed to test their models for bias by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. In particular, we show that one can rec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;&#30340;PUC&#27010;&#24565;&#65292;&#20026;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2210.13954</link><description>&lt;p&gt;
&#25105;&#19981;&#24819;&#35828;&#65306;&#22312;&#21487;&#36873;&#20010;&#20154;&#25968;&#25454;&#27169;&#22411;&#20013;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;
&lt;/p&gt;
&lt;p&gt;
I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data. (arXiv:2210.13954v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;&#30340;PUC&#27010;&#24565;&#65292;&#20026;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#65292;&#36825;&#22312;&#29616;&#20195;&#20445;&#38505;&#23450;&#20215;&#27169;&#22411;&#20013;&#24456;&#24120;&#35265;&#12290;&#19968;&#20123;&#29992;&#25143;&#21516;&#24847;&#20351;&#29992;&#20182;&#20204;&#30340;&#25968;&#25454;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#21453;&#23545;&#24182;&#20445;&#25345;&#20854;&#25968;&#25454;&#26410;&#20844;&#24320;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#20915;&#23450;&#26412;&#36523;&#21487;&#20197;&#34987;&#35270;&#20026;&#20449;&#24687;&#65292;&#24212;&#35813;&#21463;&#21040;&#20445;&#25252;&#65292;&#20197;&#23562;&#37325;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#24341;&#21457;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#30830;&#20445;&#20445;&#25252;&#20854;&#20010;&#20154;&#25968;&#25454;&#30340;&#29992;&#25143;&#19981;&#20250;&#22240;&#27492;&#21463;&#21040;&#20219;&#20309;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#20165;&#20351;&#29992;&#33719;&#24471;&#31215;&#26497;&#29992;&#25143;&#21516;&#24847;&#30340;&#20449;&#24687;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20445;&#25252;&#35201;&#27714;&#30340;&#27491;&#24335;&#21270;&#12290;&#36825;&#25490;&#38500;&#20102;&#20316;&#20986;&#20849;&#20139;&#25968;&#25454;&#19982;&#21542;&#20915;&#23450;&#25152;&#21253;&#21547;&#30340;&#38544;&#21547;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Protected User Consent (PUC)&#27010;&#24565;&#65292;&#36825;&#26159;&#25105;&#20204;&#35777;&#26126;&#22312;&#20445;&#25252;&#35201;&#27714;&#19979;&#25439;&#22833;&#26368;&#23567;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine machine learning models in a setup where individuals have the choice to share optional personal information with a decision-making system, as seen in modern insurance pricing models. Some users consent to their data being used whereas others object and keep their data undisclosed. In this work, we show that the decision not to share data can be considered as information in itself that should be protected to respect users' privacy. This observation raises the overlooked problem of how to ensure that users who protect their personal data do not suffer any disadvantages as a result. To address this problem, we formalize protection requirements for models which only use the information for which active user consent was obtained. This excludes implicit information contained in the decision to share data or not. We offer the first solution to this problem by proposing the notion of Protected User Consent (PUC), which we prove to be loss-optimal under our protection requirement. To
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#38376;&#25511;&#26426;&#21046;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#21035;&#24314;&#27169;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#21644;&#20020;&#24202;&#31508;&#35760;&#34920;&#24449;&#65292;&#36827;&#32780;&#36890;&#36807;&#20132;&#26367;&#27880;&#24847;&#21147;&#26426;&#21046;&#25972;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.12156</link><description>&lt;p&gt;
&#36890;&#36807;&#24314;&#27169;&#19981;&#35268;&#21017;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Predictions by Irregular Multimodal Electronic Health Records Modeling. (arXiv:2210.12156v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#38376;&#25511;&#26426;&#21046;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#21035;&#24314;&#27169;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#21644;&#20020;&#24202;&#31508;&#35760;&#34920;&#24449;&#65292;&#36827;&#32780;&#36890;&#36807;&#20132;&#26367;&#27880;&#24847;&#21147;&#26426;&#21046;&#25972;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405; (EHRs) &#36890;&#24120;&#21253;&#21547;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#21644;&#38271;&#30340;&#20020;&#24202;&#31508;&#35760;&#24207;&#21015;&#65292;&#36825;&#20123;&#25968;&#25454;&#37117;&#26159;&#22312;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#20869;&#37319;&#38598;&#30340;&#12290;&#22914;&#20309;&#22788;&#29702;&#27599;&#20010;&#27169;&#24577;&#20013;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#22810;&#27169;&#24577;&#34920;&#31034;&#20013;&#20197;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;&#65292;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#20197;&#19979;&#20004;&#31181;&#26041;&#24335;&#22788;&#29702;&#21333;&#20010;&#27169;&#24577;&#20013;&#30340;&#19981;&#35268;&#21017;&#24615;&#65306;(1)&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#21160;&#24577;&#22320;&#23558;&#25163;&#24037;&#21046;&#20316;&#30340;&#22635;&#20805;&#23884;&#20837;&#24335;&#34920;&#24449;&#19982;&#23398;&#20064;&#21040;&#30340;&#25554;&#20540;&#23884;&#20837;&#24335;&#34920;&#24449;&#30456;&#32467;&#21512;&#26469;&#24314;&#27169;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;(2)&#36890;&#36807;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#19968;&#31995;&#21015;&#20020;&#24202;&#31508;&#35760;&#34920;&#24449;&#36716;&#25442;&#25104;&#22810;&#21464;&#37327;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#35299;&#20915;&#19981;&#35268;&#21017;&#24615;&#12290;&#23545;&#20110;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#36328;&#36234;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#24443;&#24213;&#23545;&#22810;&#27169;&#24577;&#19981;&#35268;&#21017;&#24615;&#36827;&#34892;&#24314;&#27169;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health conditions among patients in intensive care units (ICUs) are monitored via electronic health records (EHRs), composed of numerical time series and lengthy clinical note sequences, both taken at irregular time intervals. Dealing with such irregularity in every modality, and integrating irregularity into multimodal representations to improve medical predictions, is a challenging problem. Our method first addresses irregularity in each single modality by (1) modeling irregular time series by dynamically incorporating hand-crafted imputation embeddings into learned interpolation embeddings via a gating mechanism, and (2) casting a series of clinical note representations as multivariate irregular time series and tackling irregularity via a time attention mechanism. We further integrate irregularity in multimodal fusion with an interleaved attention mechanism across temporal steps. To the best of our knowledge, this is the first work to thoroughly model irregularity in multimodalities
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23376;&#38598;&#23618;&#65288;&#25163;&#26415;&#24494;&#35843;&#65289;&#22312;&#36866;&#24212;&#20998;&#24067;&#20559;&#31227;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#36824;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#22312;&#29702;&#24819;&#29615;&#22659;&#19979;&#65292;&#25163;&#26415;&#24494;&#35843;&#21487;&#20197;&#20248;&#20110;&#20840;&#23618;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2210.11466</link><description>&lt;p&gt;
&#25163;&#26415;&#24494;&#35843;&#25552;&#39640;&#20102;&#36866;&#24212;&#20998;&#24067;&#20559;&#31227;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. (arXiv:2210.11466v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23376;&#38598;&#23618;&#65288;&#25163;&#26415;&#24494;&#35843;&#65289;&#22312;&#36866;&#24212;&#20998;&#24067;&#20559;&#31227;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#36824;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#22312;&#29702;&#24819;&#29615;&#22659;&#19979;&#65292;&#25163;&#26415;&#24494;&#35843;&#21487;&#20197;&#20248;&#20110;&#20840;&#23618;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#21518;&#20960;&#23618;&#65292;&#20445;&#30041;&#24050;&#23398;&#29305;&#24449;&#21516;&#26102;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26377;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23376;&#38598;&#23618;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#25163;&#26415;&#24494;&#35843;&#65289;&#21487;&#20197;&#36798;&#21040;&#19982;&#25110;&#20248;&#20110;&#24120;&#29992;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#19988;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#20559;&#31227;&#24433;&#21709;&#30528;&#33021;&#22815;&#24494;&#35843;&#30340;&#23618;&#25968;&#12290;&#25105;&#20204;&#22312;&#19971;&#20010;&#30495;&#23454;&#25968;&#25454;&#20219;&#21153;&#20013;&#31995;&#32479;&#39564;&#35777;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#29702;&#24819;&#29615;&#22659;&#19979;&#65292;&#25163;&#26415;&#24494;&#35843;&#21487;&#20197;&#32988;&#36807;&#20840;&#23618;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach to transfer learning under distribution shift is to fine-tune the last few layers of a pre-trained model, preserving learned features while also adapting to the new task. This paper shows that in such settings, selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms commonly used fine-tuning approaches. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best. We validate our findings systematically across seven real-world data tasks spanning three types of distribution shifts. Theoretically, we prove that for two-layer neural networks in an idealized setting, first-layer tuning can outperform fine-tuning all layers. Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#27169;&#22411;&#24615;&#33021;&#21464;&#21270;&#24402;&#22240;&#20110;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25512;&#23548;&#19968;&#31181;&#37325;&#35201;&#24615;&#26435;&#37325;&#26041;&#27861;&#26469;&#35745;&#31639;&#20219;&#24847;&#19968;&#32452;&#20998;&#24067;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.10769</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#27169;&#22411;&#20250;&#22833;&#36133;&#65311;&#23558;&#27169;&#22411;&#24615;&#33021;&#21464;&#21270;&#24402;&#22240;&#20110;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
"Why did the Model Fail?": Attributing Model Performance Changes to Distribution Shifts. (arXiv:2210.10769v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#27169;&#22411;&#24615;&#33021;&#21464;&#21270;&#24402;&#22240;&#20110;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25512;&#23548;&#19968;&#31181;&#37325;&#35201;&#24615;&#26435;&#37325;&#26041;&#27861;&#26469;&#35745;&#31639;&#20219;&#24847;&#19968;&#32452;&#20998;&#24067;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#32463;&#24120;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;&#36825;&#31181;&#20559;&#31227;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#22810;&#37325;&#30340;&#22240;&#32032;&#65292;&#27604;&#22914;&#25968;&#25454;&#36136;&#37327;&#30340;&#21464;&#21270;&#12289;&#29305;&#23450;&#21327;&#21464;&#37327;&#20998;&#24067;&#30340;&#24046;&#24322;&#25110;&#32773;&#26631;&#31614;&#19982;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#21464;&#21270;&#31561;&#12290;&#24403;&#27169;&#22411;&#22312;&#37096;&#32626;&#26102;&#22833;&#36133;&#26102;&#65292;&#23558;&#24615;&#33021;&#21464;&#21270;&#24402;&#22240;&#20110;&#36825;&#20123;&#22240;&#32032;&#23545;&#20110;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#35782;&#21035;&#26681;&#26412;&#21407;&#22240;&#24182;&#37319;&#21462;&#32531;&#35299;&#25514;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23558;&#29615;&#22659;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#24402;&#22240;&#20110;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#26500;&#36896;&#20026;&#19968;&#31181;&#21512;&#20316;&#21338;&#24328;&#30340;&#24418;&#24335;&#65292;&#20854;&#20013;&#29609;&#23478;&#26159;&#20998;&#24067;&#12290;&#25105;&#20204;&#23450;&#20041;&#19968;&#32452;&#20998;&#24067;&#30340;&#20215;&#20540;&#20026;&#24403;&#21482;&#26377;&#36825;&#32452;&#20998;&#24067;&#22312;&#29615;&#22659;&#20043;&#38388;&#21457;&#29983;&#21464;&#21270;&#26102;&#27169;&#22411;&#24615;&#33021;&#30340;&#21464;&#21270;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31181;&#37325;&#35201;&#24615;&#26435;&#37325;&#26041;&#27861;&#20197;&#35745;&#31639;&#20219;&#24847;&#19968;&#32452;&#20998;&#24067;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models frequently experience performance drops under distribution shifts. The underlying cause of such shifts may be multiple simultaneous factors such as changes in data quality, differences in specific covariate distributions, or changes in the relationship between label and features. When a model does fail during deployment, attributing performance change to these factors is critical for the model developer to identify the root cause and take mitigating actions. In this work, we introduce the problem of attributing performance differences between environments to distribution shifts in the underlying data generating mechanisms. We formulate the problem as a cooperative game where the players are distributions. We define the value of a set of distributions to be the change in model performance when only this set of distributions has changed between environments, and derive an importance weighting method for computing the value of an arbitrary set of distributions. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#20999;&#32447;&#26680; (NTK) &#22312;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;14&#20010;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#25513;&#30721;&#35789;&#39044;&#27979;&#38382;&#39064;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#21487;&#20197;&#21462;&#24471;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.05643</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Kernel-Based View of Language Model Fine-Tuning. (arXiv:2210.05643v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#20999;&#32447;&#26680; (NTK) &#22312;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;14&#20010;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#25513;&#30721;&#35789;&#39044;&#27979;&#38382;&#39064;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#21487;&#20197;&#21462;&#24471;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LMs) &#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#35299;&#20915; NLP &#20219;&#21153;&#24050;&#32463;&#25104;&#20026;&#26631;&#20934;&#20570;&#27861;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#23545;&#20110;&#32463;&#39564;&#25104;&#21151;&#32972;&#21518;&#30340;&#29702;&#35770;&#26426;&#21046;&#20102;&#35299;&#24456;&#23569;&#65292;&#20363;&#22914;&#20026;&#20160;&#20040;&#22312;&#20960;&#21313;&#20010;&#35757;&#32451;&#28857;&#19978;&#24494;&#35843;&#19968;&#20010;&#26377; $10^8$ &#20010;&#25110;&#26356;&#22810;&#21442;&#25968;&#30340;&#27169;&#22411;&#19981;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20999;&#32447;&#26680; (NTK) &#22312;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102; NTK &#24418;&#24335;&#21270;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110; Adam&#65292;&#24182;&#20351;&#29992; Tensor Programs &#25551;&#36848;&#20102; NTK &#36866;&#29992;&#20110;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26356;&#26032;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312; 14 &#20010; NLP &#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#25552;&#31034;&#23558;&#19979;&#28216;&#20219;&#21153;&#34920;&#36848;&#20026;&#25513;&#30721;&#35789;&#39044;&#27979;&#38382;&#39064;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has become standard to solve NLP tasks by fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result in overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization - describes fine-tuning of pre-trained LMs. This study was inspired by the decent performance of NTK for computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam and use Tensor Programs (Yang, 2020) to characterize conditions under which the NTK lens may describe fine-tuning updates to pre-trained language models. Extensive experiments on 14 NLP tasks validate our theory and show that formulating the downstream task as a masked word prediction problem through prompting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#26041;&#27861;&#65288;TED&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#20219;&#21153;&#24863;&#30693;&#30340;&#28388;&#27874;&#22120;&#26469;&#23545;&#40784;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#36873;&#25321;&#26377;&#29992;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#30693;&#35782;&#24046;&#36317;&#65292;&#20351;&#23398;&#29983;&#27169;&#22411;&#26356;&#22909;&#22320;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.01351</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Less is More: Task-aware Layer-wise Distillation for Language Model Compression. (arXiv:2210.01351v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#26041;&#27861;&#65288;TED&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#20219;&#21153;&#24863;&#30693;&#30340;&#28388;&#27874;&#22120;&#26469;&#23545;&#40784;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#36873;&#25321;&#26377;&#29992;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#30693;&#35782;&#24046;&#36317;&#65292;&#20351;&#23398;&#29983;&#27169;&#22411;&#26356;&#22909;&#22320;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#33976;&#39311;&#26159;&#23558;&#22823;&#27169;&#22411;&#65288;&#21363;&#25945;&#24072;&#27169;&#22411;&#65289;&#21387;&#32553;&#20026;&#23567;&#27169;&#22411;&#65288;&#21363;&#23398;&#29983;&#27169;&#22411;&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23398;&#29983;&#36890;&#36807;&#22312;&#27599;&#20010;&#20013;&#38388;&#23618;&#27169;&#20223;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#26469;&#20174;&#25945;&#24072;&#20013;&#33976;&#39311;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20998;&#23618;&#33976;&#39311;&#20063;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;&#30001;&#20110;&#23398;&#29983;&#30340;&#27169;&#22411;&#23481;&#37327;&#27604;&#25945;&#24072;&#23567;&#65292;&#23427;&#36890;&#24120;&#20250;&#20986;&#29616;&#27424;&#25311;&#21512;;&#27492;&#22806;&#65292;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#21253;&#21547;&#20102;&#23398;&#29983;&#26410;&#24517;&#38656;&#35201;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#65288;TED&#65289;&#26041;&#27861;&#12290;TED&#35774;&#35745;&#20219;&#21153;&#24863;&#30693;&#28388;&#27874;&#22120;&#26469;&#23545;&#40784;&#27599;&#19968;&#23618;&#30340;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#36825;&#20123;&#28388;&#27874;&#22120;&#20174;&#38544;&#34255;&#34920;&#31034;&#20013;&#36873;&#25321;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;TED&#20943;&#23569;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#24182;&#24110;&#21161;&#23398;&#29983;&#26356;&#22909;&#22320;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;TED&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#25110;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layer-wise distillation is a powerful tool to compress large models (i.e. teacher models) into small ones (i.e., student models). The student distills knowledge from the teacher by mimicking the hidden representations of the teacher at every intermediate layer. However, layer-wise distillation is difficult. Since the student has a smaller model capacity than the teacher, it is often under-fitted. Furthermore, the hidden representations of the teacher contain redundant information that the student does not necessarily need for the target task's learning. To address these challenges, we propose a novel Task-aware layEr-wise Distillation (TED). TED designs task-aware filters to align the hidden representations of the student and the teacher at each layer. The filters select the knowledge that is useful for the target task from the hidden representations. As such, TED reduces the knowledge gap between the two models and helps the student to fit better on the target task. We evaluate TED in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#21644;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2210.01212</link><description>&lt;p&gt;
&#36890;&#36807;&#20887;&#20313;&#24615;&#23454;&#29616;&#31232;&#30095;&#24615;&#65306;&#29992;SGD&#27714;&#35299;$L_1$
&lt;/p&gt;
&lt;p&gt;
Sparsity by Redundancy: Solving $L_1$ with SGD. (arXiv:2210.01212v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#21644;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method called "spred" to minimize a generic differentiable loss function with $L_1$ penalty using redundant reparametrization and straightforward stochastic gradient descent. It is an exact solver of $L_1$ and can be used to train sparse neural networks for gene selection tasks and neural network compression tasks, bridging the gap between sparsity in deep learning and conventional statistical learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26159;$L_1$&#24809;&#32602;&#31561;&#20215;&#20110;&#24102;&#26377;&#26435;&#37325;&#34928;&#20943;&#30340;&#21487;&#24494;&#37325;&#21442;&#25968;&#21270;&#30340;&#30452;&#25509;&#25512;&#24191;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21363;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#23545;&#20110;&#36890;&#29992;&#30340;&#38750;&#20984;&#20989;&#25968;&#65292;&#37325;&#21442;&#25968;&#21270;&#25216;&#24039;&#26159;&#23436;&#20840;&#8220;&#33391;&#24615;&#8221;&#30340;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;(1)&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#22312;&#38750;&#24120;&#39640;&#32500;&#31354;&#38388;&#20013;&#25214;&#21040;&#30456;&#20851;&#29305;&#24449;&#65292;&#20197;&#21450;(2)&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#20808;&#21069;&#23581;&#35797;&#24212;&#29992;$L_1$&#24809;&#32602;&#30340;&#26041;&#27861;&#22343;&#26410;&#25104;&#21151;&#12290;&#20174;&#27010;&#24565;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to minimize a generic differentiable loss function with $L_1$ penalty with a redundant reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of a series of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \textit{spred}, is an exact solver of $L_1$ and that the reparametrization trick is completely ``benign" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;ResNets&#30340;&#22359;&#29366;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#31471;&#21040;&#31471;&#21453;&#21521;&#20256;&#25773;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#19979;&#26080;&#27861;&#21152;&#36733;&#25972;&#20010;&#27169;&#22411;&#20197;&#21450;&#31105;&#27490;&#24182;&#34892;&#35757;&#32451;&#21508;&#23618;&#31561;&#65292;&#24182;&#19988;&#33021;&#22815;&#32531;&#35299;&#23618;&#27425;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#20572;&#28382;&#38382;&#39064;&#65292;&#20854;&#27979;&#35797;&#31934;&#24230;&#19982;&#23436;&#25972;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2210.00949</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#21270;&#21160;&#24577;&#35843;&#25972;&#26041;&#26696;&#30340;&#22359;&#29366;&#27531;&#24046;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Block-wise Training of Residual Networks via the Minimizing Movement Scheme. (arXiv:2210.00949v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;ResNets&#30340;&#22359;&#29366;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#31471;&#21040;&#31471;&#21453;&#21521;&#20256;&#25773;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#19979;&#26080;&#27861;&#21152;&#36733;&#25972;&#20010;&#27169;&#22411;&#20197;&#21450;&#31105;&#27490;&#24182;&#34892;&#35757;&#32451;&#21508;&#23618;&#31561;&#65292;&#24182;&#19988;&#33021;&#22815;&#32531;&#35299;&#23618;&#27425;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#20572;&#28382;&#38382;&#39064;&#65292;&#20854;&#27979;&#35797;&#31934;&#24230;&#19982;&#23436;&#25972;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#21453;&#21521;&#20256;&#25773;&#23384;&#22312;&#19968;&#20123;&#19981;&#36275;&#20043;&#22788;&#65306;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#21152;&#36733;&#25972;&#20010;&#27169;&#22411;&#65292;&#36825;&#22312;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#19979;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#24182;&#19988;&#21463;&#21040;&#19977;&#20010;&#38145;&#23450;&#38382;&#39064;&#30340;&#22256;&#25200;&#65288;&#21069;&#21521;&#38145;&#23450;&#12289;&#26356;&#26032;&#38145;&#23450;&#21644;&#21518;&#21521;&#38145;&#23450;&#65289;&#65292;&#36825;&#20123;&#38382;&#39064;&#31105;&#27490;&#24182;&#34892;&#35757;&#32451;&#21508;&#23618;&#12290;&#36890;&#36807;&#36880;&#23618;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#24050;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#31471;&#19978;&#20351;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;ResNets&#30340;&#36880;&#23618;&#35757;&#32451;&#26041;&#27861;&#65292;&#21463;&#21551;&#21457;&#20110;&#20998;&#24067;&#31354;&#38388;&#26799;&#24230;&#27969;&#30340;&#26368;&#23567;&#21270;&#36816;&#21160;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#30456;&#24403;&#20110;&#27599;&#20010;&#22359;&#30340;&#21160;&#33021;&#27491;&#21017;&#21270;&#65292;&#20351;&#22359;&#25104;&#20026;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#65292;&#24182;&#36171;&#20104;&#20854;&#27491;&#21017;&#21270;&#24615;&#36136;&#12290;&#23427;&#36890;&#36807;&#32531;&#35299;&#23618;&#27425;&#35757;&#32451;&#20013;&#35266;&#23519;&#21040;&#30340;&#20572;&#28382;&#38382;&#39064;&#24037;&#20316;&#65292;&#21363;&#36138;&#23146;&#35757;&#32451;&#26089;&#26399;&#23618;&#20250;&#36807;&#25311;&#21512;&#65292;&#26356;&#28145;&#30340;&#23618;&#22312;&#19968;&#23450;&#28145;&#24230;&#21518;&#20572;&#27490;&#25552;&#39640;&#27979;&#35797;&#31934;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#22359;&#29366;&#35757;&#32451;ResNets&#30340;&#27979;&#35797;&#31934;&#24230;&#19982;&#23436;&#25972;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#21487;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19978;&#20063;&#33021;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end backpropagation has a few shortcomings: it requires loading the entire model during training, which can be impossible in constrained settings, and suffers from three locking problems (forward locking, update locking and backward locking), which prohibit training the layers in parallel. Solving layer-wise optimization problems can address these problems and has been used in on-device training of neural networks. We develop a layer-wise training method, particularly welladapted to ResNets, inspired by the minimizing movement scheme for gradient flows in distribution space. The method amounts to a kinetic energy regularization of each block that makes the blocks optimal transport maps and endows them with regularity. It works by alleviating the stagnation problem observed in layer-wise training, whereby greedily-trained early layers overfit and deeper layers stop increasing test accuracy after a certain depth. We show on classification tasks that the test accuracy of block-wise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#31354;&#38388;&#29109;&#30340;&#26080;&#30417;&#30563;&#35270;&#39057;&#20851;&#38190;&#28857;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#25277;&#35937;&#20026;&#20851;&#38190;&#28857;&#30340;&#21313;&#20998;&#31616;&#26126;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#29109;&#35745;&#31639;&#30340;&#20449;&#24687;&#29109;&#25439;&#22833;&#25351;&#23548;&#27169;&#22411;&#21457;&#29616;&#19968;&#33268;&#30340;&#20851;&#38190;&#28857;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.15404</link><description>&lt;p&gt;
&#35270;&#39057;&#20013;&#22522;&#20110;&#20449;&#24687;&#29109;&#30340;&#26080;&#30417;&#30563;&#20851;&#38190;&#28857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Entropy-driven Unsupervised Keypoint Representation Learning in Videos. (arXiv:2209.15404v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#31354;&#38388;&#29109;&#30340;&#26080;&#30417;&#30563;&#35270;&#39057;&#20851;&#38190;&#28857;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#25277;&#35937;&#20026;&#20851;&#38190;&#28857;&#30340;&#21313;&#20998;&#31616;&#26126;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#29109;&#35745;&#31639;&#30340;&#20449;&#24687;&#29109;&#25439;&#22833;&#25351;&#23548;&#27169;&#22411;&#21457;&#29616;&#19968;&#33268;&#30340;&#20851;&#38190;&#28857;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#26159;&#26377;&#25928;&#22320;&#23398;&#20064;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#31354;&#38388;&#29109;&#65288;ISE&#65289;&#30340;&#27010;&#24565;&#20174;&#35270;&#39057;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#20687;&#32032;&#37051;&#22495;&#30340;&#23616;&#37096;&#29109;&#21644;&#23427;&#20204;&#30340;&#26102;&#38388;&#21464;&#21270;&#21487;&#20197;&#20026;&#23398;&#20064;&#31361;&#20986;&#30340;&#29305;&#24449;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20869;&#22312;&#30417;&#30563;&#20449;&#21495;&#12290;&#22522;&#20110;&#36825;&#20010;&#24605;&#24819;&#65292;&#25105;&#20204;&#23558;&#35270;&#35273;&#29305;&#24449;&#25277;&#35937;&#20026;&#20851;&#38190;&#28857;&#30340;&#31616;&#26126;&#34920;&#31034;&#65292;&#20316;&#20026;&#21160;&#24577;&#20449;&#24687;&#20256;&#36882;&#32773;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#35270;&#39057;&#24103;&#20013;&#26080;&#30417;&#30563;&#22320;&#30452;&#25509;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#34920;&#31034;&#12290;&#21033;&#29992;&#23616;&#37096;&#29109;&#35745;&#31639;&#30340;&#20004;&#20010;&#21407;&#22987;&#30340;&#20449;&#24687;&#29109;&#25439;&#22833;&#25351;&#23548;&#25105;&#20204;&#30340;&#27169;&#22411;&#21457;&#29616;&#19968;&#33268;&#30340;&#20851;&#38190;&#28857;&#34920;&#31034;&#65307;&#19968;&#20010;&#26368;&#22823;&#21270;&#20851;&#38190;&#28857;&#35206;&#30422;&#30340;&#31354;&#38388;&#20449;&#24687;&#30340;&#25439;&#22833;&#21644;&#19968;&#20010;&#26368;&#22823;&#21270;&#20851;&#38190;&#28857;&#22312;&#36830;&#32493;&#24103;&#20043;&#38388;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20851;&#38190;&#28857;&#26816;&#27979;&#12289;&#21160;&#20316;&#35782;&#21035;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting informative representations from videos is fundamental for effectively learning various downstream tasks. We present a novel approach for unsupervised learning of meaningful representations from videos, leveraging the concept of image spatial entropy (ISE) that quantifies the per-pixel information in an image. We argue that \textit{local entropy} of pixel neighborhoods and their temporal evolution create valuable intrinsic supervisory signals for learning prominent features. Building on this idea, we abstract visual features into a concise representation of keypoints that act as dynamic information transmitters, and design a deep learning model that learns, purely unsupervised, spatially and temporally consistent representations \textit{directly} from video frames. Two original information-theoretic losses, computed from local entropy, guide our model to discover consistent keypoint representations; a loss that maximizes the spatial information covered by the keypoints and a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Fed-CBS&#26426;&#21046;&#65292;&#36890;&#36807;&#31867;&#21035;&#19981;&#24179;&#34913;&#24615;&#38477;&#20302;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#21155;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;federated learning&#65288;FL&#65289;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.15245</link><description>&lt;p&gt;
Fed-CBS&#65306;&#19968;&#31181;&#32771;&#34385;&#24322;&#26500;&#24615;&#30340;&#23458;&#25143;&#31471;&#25277;&#26679;&#26426;&#21046;&#65292;&#36890;&#36807;&#31867;&#21035;&#19981;&#24179;&#34913;&#24615;&#38477;&#20302;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#21155;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fed-CBS: A Heterogeneity-Aware Client Sampling Mechanism for Federated Learning via Class-Imbalance Reduction. (arXiv:2209.15245v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Fed-CBS&#26426;&#21046;&#65292;&#36890;&#36807;&#31867;&#21035;&#19981;&#24179;&#34913;&#24615;&#38477;&#20302;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#21155;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;federated learning&#65288;FL&#65289;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36793;&#32536;&#35774;&#22791;&#36890;&#20449;&#33021;&#21147;&#26377;&#38480;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#27861;&#20165;&#38543;&#26426;&#36873;&#25321;&#37096;&#20998;&#35774;&#22791;&#21442;&#19982;&#27599;&#19968;&#36718;&#36890;&#20449;&#30340;&#35757;&#32451;&#12290;&#19982;&#21516;&#26102;&#28041;&#21450;&#25152;&#26377;&#21487;&#29992;&#23458;&#25143;&#31471;&#30456;&#27604;&#65292;&#38543;&#26426;&#36873;&#25321;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#30340;&#26174;&#30528;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65306;&#38543;&#26426;&#36873;&#25321;&#23458;&#25143;&#31471;&#25152;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#25152;&#36873;&#23458;&#25143;&#31471;&#20013;&#30340;&#25968;&#25454;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24322;&#26500;&#24863;&#30693;&#23458;&#25143;&#31471;&#25277;&#26679;&#26426;&#21046;&#8212;&#8212;&#32852;&#37030;&#31867;&#24179;&#34913;&#25277;&#26679;&#65288;Fed-CBS&#65289;&#65292;&#20854;&#21487;&#26377;&#25928;&#22320;&#38477;&#20302;&#20174;&#26377;&#24847;&#36873;&#25321;&#30340;&#23458;&#25143;&#31471;&#20013;&#33719;&#21462;&#30340;&#32452;&#25968;&#25454;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#31243;&#24230;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#21035;&#19981;&#24179;&#34913;&#24230;&#37327;&#65292;&#24182;&#37319;&#29992;&#21516;&#24577;&#21152;&#23494;&#26041;&#24335;&#20197;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#24335;&#25512;&#23548;&#20986;&#35813;&#24230;&#37327;&#12290;&#22522;&#20110;&#35813;&#24230;&#37327;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#24322;&#26500;&#24863;&#30693;&#30340;&#23458;&#25143;&#31471;&#25277;&#26679;&#26426;&#21046;&#65292;&#20351;&#23458;&#25143;&#31471;&#23454;&#29616;&#20102;&#31867;&#24179;&#34913;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;FL&#22312;&#38750;IID&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Fed-CBS&#26426;&#21046;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to limited communication capacities of edge devices, most existing federated learning (FL) methods randomly select only a subset of devices to participate in training for each communication round. Compared with engaging all the available clients, the random-selection mechanism can lead to significant performance degradation on non-IID (independent and identically distributed) data. In this paper, we show our key observation that the essential reason resulting in such performance degradation is the class-imbalance of the grouped data from randomly selected clients. Based on our key observation, we design an efficient heterogeneity-aware client sampling mechanism, i.e., Federated Class-balanced Sampling (Fed-CBS), which can effectively reduce class-imbalance of the group dataset from the intentionally selected clients. In particular, we propose a measure of class-imbalance and then employ homomorphic encryption to derive this measure in a privacy-preserving way. Based on this measure
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#65292;&#20174;&#21018;&#20307;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.11355</link><description>&lt;p&gt;
&#20174;&#21018;&#20307;&#22270;&#20687;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65288;&#26410;&#30693;&#36136;&#37327;&#20998;&#24067;&#65289;
&lt;/p&gt;
&lt;p&gt;
Learning to predict 3D rotational dynamics from images of a rigid body with unknown mass distribution. (arXiv:2209.11355v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#65292;&#20174;&#21018;&#20307;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#24403;&#20302;&#32500;&#24230;&#27979;&#37327;&#19981;&#21487;&#29992;&#26102;&#65292;&#20250;&#26377;&#33258;&#30001;&#26059;&#36716;&#30340;3D&#21018;&#20307;&#30340;&#22270;&#20687;&#35266;&#23519;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#25968;&#25454;&#30340;&#39640;&#32500;&#25968;&#38459;&#27490;&#20102;&#20351;&#29992;&#32463;&#20856;&#20272;&#35745;&#25216;&#26415;&#26469;&#23398;&#20064;&#21160;&#24577;&#12290;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#20063;&#21463;&#38480;&#20110;&#19968;&#20010;&#21018;&#20307;&#22270;&#20687;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#65292;&#32780;&#36136;&#37327;&#20998;&#24067;&#19982;&#21021;&#22987;&#35282;&#36895;&#24230;&#19968;&#36215;&#20915;&#23450;&#21018;&#20307;&#26059;&#36716;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#20272;&#35745;&#21644;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#65292;&#35813;&#27969;&#31243;&#23558;&#21333;&#20010;&#22270;&#20687;&#26144;&#23556;&#21040;&#19982; $\mathbf{SO}(3)$ &#21516;&#32986;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#65292;&#20174;&#28508;&#22312;&#23545;&#20013;&#35745;&#31639;&#35282;&#36895;&#24230;&#65292;&#24182;&#20351;&#29992;Hamilton&#36816;&#21160;&#26041;&#31243;&#39044;&#27979;&#26410;&#26469;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#26059;&#36716;&#21018;&#20307;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world settings, image observations of freely rotating 3D rigid bodies, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics. The usefulness of standard deep learning methods is also limited because an image of a rigid body reveals nothing about the distribution of mass inside the body, which, together with initial angular velocity, is what determines how the body will rotate. We present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to $\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion. We demonstrate the efficacy of our approach on new rotating rigid-body datasets of sequenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#39640;&#25928;&#23398;&#20064;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#23398;&#39044;&#22788;&#29702;&#21644;&#31561;&#21464;&#32593;&#32476;&#26500;&#24314;&#26679;&#26412;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#65292;&#24182;&#21457;&#24067;&#20102;&#20004;&#20010;TO&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#27604;&#36739;&#24615;&#21644;&#26410;&#26469;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2209.05098</link><description>&lt;p&gt;
SELTO&#65306;&#39640;&#25928;&#23398;&#20064;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SELTO: Sample-Efficient Learned Topology Optimization. (arXiv:2209.05098v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#39640;&#25928;&#23398;&#20064;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#23398;&#39044;&#22788;&#29702;&#21644;&#31561;&#21464;&#32593;&#32476;&#26500;&#24314;&#26679;&#26412;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#65292;&#24182;&#21457;&#24067;&#20102;&#20004;&#20010;TO&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#27604;&#36739;&#24615;&#21644;&#26410;&#26469;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#20026;&#25299;&#25169;&#20248;&#21270;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#30446;&#21069;&#20173;&#32570;&#20047;&#20851;&#20110;&#22522;&#26412;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#31995;&#32479;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#39044;&#22788;&#29702;&#21644;&#31561;&#21464;&#32593;&#32476;&#26500;&#24314;&#26679;&#26412;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#30417;&#30563;&#23398;&#20064;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#39044;&#27979;&#29289;&#29702;&#27491;&#30830;&#24615;&#26041;&#38754;&#37117;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#25552;&#39640;&#27604;&#36739;&#24615;&#21644;&#26410;&#26469;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#21069;&#20004;&#20010;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#30456;&#24212;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in Deep Learning (DL) suggest a vast potential for Topology Optimization (TO). However, while there are some promising attempts, the subfield still lacks a firm footing regarding basic methods and datasets. We aim to address both points. First, we explore physics-based preprocessing and equivariant networks to create sample-efficient components for TO DL pipelines. We evaluate them in a large-scale ablation study using end-to-end supervised training. The results demonstrate a drastic improvement in sample efficiency and the predictions' physical correctness. Second, to improve comparability and future progress, we publish the two first TO datasets containing problems and corresponding ground truth solutions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;(DL-DRL)&#26469;&#35299;&#20915;&#22810;UAV&#22823;&#35268;&#27169;&#20219;&#21153;&#35843;&#24230;&#38382;&#39064;&#65292;&#37319;&#29992;&#20998;&#27835;&#26694;&#26550;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#65292;&#33021;&#22815;&#26174;&#30528;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#20248;&#21270;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.02447</link><description>&lt;p&gt;
DL-DRL&#65306;&#38754;&#21521;&#22810;&#26080;&#20154;&#26426;&#22823;&#35268;&#27169;&#20219;&#21153;&#35843;&#24230;&#30340;&#21452;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DL-DRL: A double-level deep reinforcement learning approach for large-scale task scheduling of multi-UAV. (arXiv:2208.02447v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;(DL-DRL)&#26469;&#35299;&#20915;&#22810;UAV&#22823;&#35268;&#27169;&#20219;&#21153;&#35843;&#24230;&#38382;&#39064;&#65292;&#37319;&#29992;&#20998;&#27835;&#26694;&#26550;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#65292;&#33021;&#22815;&#26174;&#30528;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#20248;&#21270;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#25191;&#34892;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20026;&#35299;&#20915;&#28508;&#22312;&#30340;&#20219;&#21153;&#35843;&#24230;&#38382;&#39064;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#19982;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#30528;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#23569;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#38382;&#39064;&#35268;&#27169;&#30340;&#19981;&#26029;&#25193;&#22823;&#65292;&#23427;&#20204;&#30340;&#20915;&#31574;&#31354;&#38388;&#20063;&#20250;&#21464;&#24471;&#38750;&#24120;&#24222;&#22823;&#65292;&#36825;&#20250;&#30772;&#22351;&#35745;&#31639;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DL-DRL&#65289;&#26041;&#27861;&#65292;&#22522;&#20110;&#19968;&#20010;&#20998;&#27835;&#26694;&#26550;&#65288;DCF&#65289;&#65292;&#23558;&#22810;UAV&#30340;&#20219;&#21153;&#35843;&#24230;&#20998;&#35299;&#20026;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#20004;&#20010;&#23618;&#27425;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#25105;&#20204;&#30340;&#19978;&#23618;DRL&#27169;&#22411;&#20013;&#35774;&#35745;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#21270;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#23558;&#20219;&#21153;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;UAV&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#19979;&#23618;DRL&#27169;&#22411;&#20013;&#21033;&#29992;&#21478;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31574;&#30053;&#32593;&#32476;&#20026;&#27599;&#20010;UAV&#26500;&#36896;&#36335;&#24452;&#65292;&#20197;&#26368;&#22823;&#21270;&#23436;&#25104;&#30340;&#20219;&#21153;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting unmanned aerial vehicles (UAVs) to execute tasks is gaining growing popularity recently. To solve the underlying task scheduling problem, the deep reinforcement learning (DRL) based methods demonstrate notable advantage over the conventional heuristics as they rely less on hand-engineered rules. However, their decision space will become prohibitively huge as the problem scales up, thus deteriorating the computation efficiency. To alleviate this issue, we propose a double-level deep reinforcement learning (DL-DRL) approach based on a divide and conquer framework (DCF), where we decompose the task scheduling of multi-UAV into task allocation and route planning. Particularly, we design an encoder-decoder structured policy network in our upper-level DRL model to allocate the tasks to different UAVs, and we exploit another attention based policy network in our lower-level DRL model to construct the route for each UAV, with the objective to maximize the number of executed tasks gi
&lt;/p&gt;</description></item><item><title>&#23567;&#25968;&#25454;&#23398;&#20064;&#32508;&#36848;&#65292;&#21253;&#25324;&#20027;&#21160;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#38754;&#65292;&#20294;&#23427;&#20204;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65307;&#32771;&#34385;&#21040;&#19981;&#21516;&#30446;&#30340;&#30340;&#22810;&#20010;&#23398;&#20064;&#31038;&#21306;&#37117;&#21487;&#20197;&#20135;&#29983;&#23567;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#23567;&#25968;&#25454;&#20998;&#20026;&#19981;&#21516;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#20809;&#35889;&#12290;</title><link>http://arxiv.org/abs/2207.14443</link><description>&lt;p&gt;
&#23567;&#25968;&#25454;&#23398;&#20064;&#32508;&#36848;&#65306;&#27867;&#21270;&#12289;&#20248;&#21270;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Learning on Small Data: Generalization, Optimization, and Challenge. (arXiv:2207.14443v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14443
&lt;/p&gt;
&lt;p&gt;
&#23567;&#25968;&#25454;&#23398;&#20064;&#32508;&#36848;&#65292;&#21253;&#25324;&#20027;&#21160;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#38754;&#65292;&#20294;&#23427;&#20204;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65307;&#32771;&#34385;&#21040;&#19981;&#21516;&#30446;&#30340;&#30340;&#22810;&#20010;&#23398;&#20064;&#31038;&#21306;&#37117;&#21487;&#20197;&#20135;&#29983;&#23567;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#23567;&#25968;&#25454;&#20998;&#20026;&#19981;&#21516;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#20809;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#20170;&#22825;&#65292;&#22823;&#35268;&#27169;&#25968;&#25454;&#23398;&#20064;&#24102;&#26469;&#30340;&#26631;&#27880;&#21644;&#35757;&#32451;&#25104;&#26412;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#22312;&#26410;&#26469;&#65292;&#21033;&#29992;&#23567;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#20197;&#36924;&#36817;&#22823;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#32456;&#26497;&#30446;&#26631;&#20043;&#19968;&#65292;&#36825;&#38656;&#35201;&#26426;&#22120;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#22312;&#23569;&#37327;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#21644;&#22330;&#26223;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#20027;&#35201;&#21253;&#25324;&#20027;&#21160;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#27867;&#21270;&#24615;&#33021;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#22823;&#22810;&#25968;&#35774;&#32622;&#20026;&#34987;&#21160;&#23398;&#20064;&#65292;&#21363;&#24050;&#30693;&#20998;&#24067;&#19979;&#26377;&#38480;&#30340;&#35757;&#32451;&#36164;&#28304;&#20013;&#25511;&#21046;&#26631;&#31614;&#20998;&#24067;&#12290;&#26412;&#32508;&#36848;&#37319;&#29992;&#20197; PAC (Probably Approximately Correct) &#26694;&#26550;&#19979;&#30340;&#19981;&#21487;&#30693;&#20027;&#21160;&#37319;&#26679;&#29702;&#35770;&#26469;&#20998;&#26512;&#27169;&#22411;&#26080;&#20851;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#23567;&#25968;&#25454;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#21644;&#26631;&#31614;&#22797;&#26434;&#24230;&#12290;&#32771;&#34385;&#21040;&#19981;&#21516;&#30446;&#30340;&#30340;&#22810;&#20010;&#23398;&#20064;&#31038;&#21306;&#37117;&#21487;&#20197;&#20135;&#29983;&#23567;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#23567;&#25968;&#25454;&#20998;&#20026;&#19981;&#21516;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#20809;&#35889;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#23567;&#25968;&#25454;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#20027;&#21160;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#21450;&#23427;&#20204;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#22914;&#39046;&#22495;&#33258;&#36866;&#24212;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#38899;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#23567;&#25968;&#25454;&#23398;&#20064;&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning on big data brings success for artificial intelligence (AI), but the annotation and training costs are expensive. In future, learning on small data that approximates the generalization ability of big data is one of the ultimate purposes of AI, which requires machines to recognize objectives and scenarios relying on small data as humans. A series of learning topics is going on this way such as active learning and few-shot learning. However, there are few theoretical guarantees for their generalization performance. Moreover, most of their settings are passive, that is, the label distribution is explicitly controlled by finite training resources from known distributions. This survey follows the agnostic active sampling theory under a PAC (Probably Approximately Correct) framework to analyze the generalization error and label complexity of learning on small data in model-agnostic supervised and unsupervised fashion. Considering multiple learning communities could produce small dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#24577;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#33021;&#22815;&#22312;&#34892;&#21160;&#20013;&#23398;&#20064;&#21040;&#19982;&#20854;&#34892;&#20026;&#30456;&#19968;&#33268;&#30340;&#24863;&#30693;&#20449;&#24687;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#25429;&#33719;&#29615;&#22659;&#20013;&#30340;&#36716;&#25442;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2207.12067</link><description>&lt;p&gt;
&#21516;&#24577;&#33258;&#32534;&#30721;&#22120; - &#20174;&#35266;&#23519;&#21040;&#36716;&#21270;&#23398;&#20064;&#32676;&#32452;&#32467;&#26500;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions. (arXiv:2207.12067v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#24577;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#33021;&#22815;&#22312;&#34892;&#21160;&#20013;&#23398;&#20064;&#21040;&#19982;&#20854;&#34892;&#20026;&#30456;&#19968;&#33268;&#30340;&#24863;&#30693;&#20449;&#24687;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#25429;&#33719;&#29615;&#22659;&#20013;&#30340;&#36716;&#25442;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35753;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23398;&#20064;&#21040;&#20934;&#30830;&#34920;&#31034;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#30340;&#20869;&#22312;&#27169;&#22411;&#26159;&#19968;&#20010;&#23578;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#26500;&#24314;&#19981;&#20165;&#21253;&#21547;&#35266;&#23519;&#24615;&#30693;&#35782;&#65292;&#20063;&#21253;&#21547;&#24178;&#39044;&#24615;&#30693;&#35782;&#30340;&#34920;&#29616;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#34920;&#31034;&#23398;&#20064;&#21644;&#32676;&#35770;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#33021;&#22815;&#22312;&#34892;&#21160;&#36807;&#31243;&#20013;&#23398;&#20064;&#21040;&#19982;&#20043;&#30456;&#19968;&#33268;&#30340;&#24863;&#30693;&#20449;&#24687;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#32780;&#36825;&#20123;&#34892;&#21160;&#23454;&#38469;&#19978;&#26159;&#21464;&#25442;&#36825;&#20123;&#20449;&#24687;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#24182;&#22312;&#20854;&#28508;&#22312;&#31354;&#38388;&#19978;&#24212;&#29992;&#32676;&#32452;&#34920;&#31034;&#65292;&#36890;&#36807;&#21033;&#29992;&#31561;&#21464;&#25439;&#22833;&#24378;&#21046;&#23454;&#26045;&#36866;&#24403;&#30340;&#21516;&#24577;&#24615;&#36136;&#20197;&#23436;&#25104;&#35757;&#32451;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20808;&#39564;&#32676;&#32452;&#30693;&#35782;&#65292;&#24182;&#19988;&#19981;&#38480;&#21046;&#20195;&#29702;&#21487;&#25191;&#34892;&#30340;&#34892;&#21160;&#38598;&#21512;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#23398;&#20064;&#21040;&#34892;&#21160;&#30340;&#32676;&#32452;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#33719;&#20102;&#29615;&#22659;&#20013;&#30340;&#36716;&#25442;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can agents learn internal models that veridically represent interactions with the real world is a largely open question. As machine learning is moving towards representations containing not just observational but also interventional knowledge, we study this problem using tools from representation learning and group theory. We propose methods enabling an agent acting upon the world to learn internal representations of sensory information that are consistent with actions that modify it. We use an autoencoder equipped with a group representation acting on its latent space, trained using an equivariance-derived loss in order to enforce a suitable homomorphism property on the group representation. In contrast to existing work, our approach does not require prior knowledge of the group and does not restrict the set of actions the agent can perform. We motivate our method theoretically, and show empirically that it can learn a group representation of the actions, thereby capturing the str
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.13872</link><description>&lt;p&gt;
&#21518;&#39564;&#27010;&#24565;&#35299;&#37322;&#20309;&#26102;&#21487;&#35782;&#21035;&#65311;
&lt;/p&gt;
&lt;p&gt;
When are Post-hoc Conceptual Explanations Identifiable?. (arXiv:2206.13872v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23884;&#20837;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#27010;&#24565;&#35299;&#37322;&#26469;&#29702;&#35299;&#21644;&#20998;&#35299;&#65292;&#36825;&#31181;&#38656;&#27714;&#22312;&#35299;&#37322;&#20013;&#19981;&#21253;&#21547;&#26377;&#25928;&#27010;&#24565;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26174;&#33879;&#12290;&#20026;&#20102;&#25552;&#20379;&#21518;&#39564;&#35299;&#37322;&#65292;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#20250;&#22312;&#24050;&#35757;&#32451;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#37322;&#24615;&#24378;&#30340;&#27010;&#24565;&#65292;&#20363;&#22914;&#29289;&#20307;&#24418;&#29366;&#25110;&#39068;&#33394;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#35748;&#20026;&#27010;&#24565;&#21457;&#29616;&#24212;&#35813;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#34987;&#35777;&#26126;&#22320;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20316;&#20026;&#19968;&#20010;&#36215;&#28857;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#23558;&#27010;&#24565;&#21457;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#20363;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65289;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#38750;&#39640;&#26031;&#20998;&#24067;&#30340;&#29420;&#31435;&#27010;&#24565;&#26469;&#38416;&#26126;&#36825;&#19968;&#28857;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#21487;&#35777;&#26126;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest in understanding and factorizing learned embedding spaces through conceptual explanations is steadily growing. When no human concept labels are available, concept discovery methods search trained embedding spaces for interpretable concepts like object shape or color that can be used to provide post-hoc explanations for decisions. Unlike previous work, we argue that concept discovery should be identifiable, meaning that a number of known concepts can be provably recovered to guarantee reliability of the explanations. As a starting point, we explicitly make the connection between concept discovery and classical methods like Principal Component Analysis and Independent Component Analysis by showing that they can recover independent concepts with non-Gaussian distributions. For dependent concepts, we propose two novel approaches that exploit functional compositionality properties of image-generating processes. Our provably identifiable concept discovery methods substantially outpe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#29616;&#23454;&#19990;&#30028;&#21464;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#29983;&#25104;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2206.10858</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Robust Universal Adversarial Perturbations. (arXiv:2206.10858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#29616;&#23454;&#19990;&#30028;&#21464;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#29983;&#25104;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAP&#65289;&#26159;&#19968;&#31181;&#19981;&#21487;&#23519;&#35273;&#30340;&#22270;&#20687;&#26080;&#20851;&#21521;&#37327;&#65292;&#21487;&#20197;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#39640;&#27010;&#29575;&#22320;&#23558;&#36755;&#20837;&#38169;&#35823;&#20998;&#31867;&#12290;&#22312;&#23454;&#38469;&#25915;&#20987;&#22330;&#26223;&#20013;&#65292;&#23545;&#25239;&#24615;&#25200;&#21160;&#21487;&#33021;&#20250;&#22312;&#28155;&#21152;&#21040;DNN&#36755;&#20837;&#20043;&#21069;&#32463;&#21382;&#20687;&#32032;&#24378;&#24230;&#30340;&#21464;&#21270;&#12289;&#32553;&#25918;&#31561;&#21464;&#25442;&#12290;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#21019;&#24314;&#23545;&#36825;&#20123;&#29616;&#23454;&#19990;&#30028;&#21464;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;UAP&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#25915;&#20987;&#22330;&#26223;&#20013;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#21046;&#23450;&#20102;&#23545;&#29616;&#23454;&#19990;&#30028;&#21464;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;UAP&#12290;&#25105;&#20204;&#20351;&#29992;&#27010;&#29575;&#40065;&#26834;&#24615;&#30028;&#38480;&#26500;&#24314;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#26500;&#36896;&#20102;&#36825;&#26679;&#30340;UAP&#65292;&#23427;&#20204;&#23545;&#30001;&#20219;&#24847;&#23376;&#21487;&#24494;&#21464;&#25442;&#20989;&#25968;&#32452;&#25104;&#30340;&#21464;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;CIFAR-10&#21644;ILSVRC 2012&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#34913;&#37327;&#20102;&#25105;&#20204;&#30340;UAP&#22312;&#19968;&#31995;&#21015;&#24120;&#35265;&#30340;&#29616;&#23454;&#19990;&#30028;&#21464;&#25442;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;&#26059;&#36716;&#12289;&#23545;&#27604;&#24230;&#21464;&#21270;&#31561;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Universal Adversarial Perturbations (UAPs) are imperceptible, image-agnostic vectors that cause deep neural networks (DNNs) to misclassify inputs with high probability. In practical attack scenarios, adversarial perturbations may undergo transformations such as changes in pixel intensity, scaling, etc. before being added to DNN inputs. Existing methods do not create UAPs robust to these real-world transformations, thereby limiting their applicability in practical attack scenarios. In this work, we introduce and formulate UAPs robust against real-world transformations. We build an iterative algorithm using probabilistic robustness bounds and construct such UAPs robust to transformations generated by composing arbitrary sub-differentiable transformation functions. We perform an extensive evaluation on the popular CIFAR-10 and ILSVRC 2012 datasets measuring our UAPs' robustness under a wide range common, real-world transformations such as rotation, contrast changes, etc. We further show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#20854;&#23427;&#31639;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#38750;&#22343;&#21248;&#26446;&#26222;&#24076;&#33576;&#24773;&#24418;&#65292;&#24182;&#19988;&#22312;&#20855;&#20307;&#24212;&#29992;&#20013;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#21442;&#25968;&#35843;&#25972;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2206.10713</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#20013;&#36229;&#36234;&#32479;&#19968;&#26446;&#26222;&#24076;&#33576;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Beyond Uniform Lipschitz Condition in Differentially Private Optimization. (arXiv:2206.10713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#20854;&#23427;&#31639;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#38750;&#22343;&#21248;&#26446;&#26222;&#24076;&#33576;&#24773;&#24418;&#65292;&#24182;&#19988;&#22312;&#20855;&#20307;&#24212;&#29992;&#20013;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#21442;&#25968;&#35843;&#25972;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20851;&#20110;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#30340;&#20808;&#21069;&#32467;&#26524;&#37117;&#26159;&#22312;&#32479;&#19968;&#26446;&#26222;&#24076;&#33576;&#24615;&#30340;&#31616;&#21270;&#20551;&#35774;&#19979;&#23548;&#20986;&#30340;&#65292;&#21363;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#37117;&#26159;&#22343;&#21248;&#26377;&#30028;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#20551;&#23450;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#20855;&#26377;&#26679;&#26412;&#30456;&#20851;&#30340;&#19978;&#30028;&#65292;&#21363;&#27599;&#20010;&#26679;&#26412;&#30340;&#26446;&#26222;&#24076;&#33576;&#24120;&#25968;&#65292;&#20174;&#32780;&#25512;&#24191;&#20102;&#32479;&#19968;&#26446;&#26222;&#24076;&#33576;&#24615;&#12290;&#36825;&#20123;&#26412;&#36523;&#21487;&#33021;&#26159;&#26080;&#30028;&#30340;&#12290;&#24403;&#27599;&#20010;&#26679;&#26412;&#30340;&#26446;&#26222;&#24076;&#33576;&#24120;&#25968;&#26159;&#26377;&#30028;&#30340;&#26102;&#65292;&#25105;&#20204;&#20026;DP-SGD&#22312;&#20984;&#36229;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#36873;&#25321;&#21098;&#36753;&#33539;&#25968;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#25351;&#23548;&#65307;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20165;&#35843;&#25972;&#21098;&#36753;&#33539;&#25968;&#65292;&#30452;&#21040;&#26368;&#23567;&#27599;&#20010;&#26679;&#26412;&#26446;&#26222;&#24076;&#33576;&#24120;&#25968;&#30340;&#20540;&#12290;&#36825;&#22312;&#28145;&#24230;&#32593;&#32476;&#19978;&#39044;&#20808;&#35757;&#32451;&#20844;&#20849;&#25968;&#25454;&#30340; softmax &#23618;&#30340;&#31169;&#20154;&#35757;&#32451;&#20013;&#26377;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;8&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24314;&#35758;&#30340;&#21151;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;DP-SGD&#22312;&#20984;&#21644;&#38750;&#20984;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#26032;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most prior results on differentially private stochastic gradient descent (DP-SGD) are derived under the simplistic assumption of uniform Lipschitzness, i.e., the per-sample gradients are uniformly bounded. We generalize uniform Lipschitzness by assuming that the per-sample gradients have sample-dependent upper bounds, i.e., per-sample Lipschitz constants, which themselves may be unbounded. We provide principled guidance on choosing the clip norm in DP-SGD for convex over-parameterized settings satisfying our general version of Lipschitzness when the per-sample Lipschitz constants are bounded; specifically, we recommend tuning the clip norm only till values up to the minimum per-sample Lipschitz constant. This finds application in the private training of a softmax layer on top of a deep network pre-trained on public data. We verify the efficacy of our recommendation via experiments on 8 datasets. Furthermore, we provide new convergence results for DP-SGD on convex and nonconvex function
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20840;&#23616;&#19978;&#19979;&#25991;&#35270;&#35273;Transformer (GC ViT) &#26550;&#26500;&#65292;&#21033;&#29992;&#20840;&#23616;&#19978;&#19979;&#25991;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26631;&#20934;&#30340;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#23545;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#31354;&#38388;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#26377;&#25928;&#32780;&#39640;&#25928;&#30340;&#24314;&#27169;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;ViTs&#20013;&#32570;&#20047;&#24402;&#32435;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.09959</link><description>&lt;p&gt;
&#20840;&#23616;&#19978;&#19979;&#25991;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
Global Context Vision Transformers. (arXiv:2206.09959v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20840;&#23616;&#19978;&#19979;&#25991;&#35270;&#35273;Transformer (GC ViT) &#26550;&#26500;&#65292;&#21033;&#29992;&#20840;&#23616;&#19978;&#19979;&#25991;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26631;&#20934;&#30340;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#23545;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#31354;&#38388;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#26377;&#25928;&#32780;&#39640;&#25928;&#30340;&#24314;&#27169;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;ViTs&#20013;&#32570;&#20047;&#24402;&#32435;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8212;&#8212;&#20840;&#23616;&#19978;&#19979;&#25991;&#35270;&#35273;Transformer (GC ViT), &#21487;&#20197;&#22686;&#24378;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20840;&#23616;&#19978;&#19979;&#25991;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26631;&#20934;&#30340;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#65292;&#23545;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#31354;&#38388;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#26377;&#25928;&#32780;&#39640;&#25928;&#30340;&#24314;&#27169;&#65292;&#26080;&#38656;&#36827;&#34892;&#20687;&#35745;&#31639;&#27880;&#24847;&#21147;&#25513;&#30721;&#25110;&#31227;&#21160;&#26412;&#22320;&#31383;&#21475;&#36825;&#26679;&#30340;&#26114;&#36149;&#25805;&#20316;&#12290;&#24182;&#19988;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;ViTs&#20013;&#32570;&#20047;&#24402;&#32435;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#26550;&#26500;&#20013;&#20351;&#29992;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#34701;&#21512;&#21453;&#21521;&#27531;&#24046;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;GC ViT&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20998;&#31867;&#65292;GC ViT&#30340;51M&#12289;90M&#21644;201M&#21442;&#25968;&#21464;&#20307;&#22312;224&#20687;&#32032;&#20998;&#36776;&#29575;&#19979;&#37117;&#33021;&#22815;&#36798;&#21040;84.3%&#12289;85.0%&#21644;85.7%&#30340;Top-1&#31934;&#24230;&#65292;&#32780;&#19988;&#26080;&#38656;&#20219;&#20309;&#39044;&#35757;&#32451;&#65292;&#22240;&#27492;&#36229;&#36234;&#20102;CNN-based Conv&#31561;&#20808;&#21069;&#30340;&#33402;&#26415;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and 85.7% Top-1 accuracy, respectively, at 224 image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based Conv
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#30340;&#26222;&#21450;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#26368;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#20294;&#20063;&#21361;&#21450;&#20102;MLaaS&#25552;&#20379;&#21830;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#22791;&#30456;&#21516;&#34892;&#20026;&#30340;&#27169;&#22411;&#21103;&#26412;&#65292;&#26412;&#25991;&#38024;&#23545;&#27169;&#22411;&#31363;&#21462;&#30340;&#25915;&#20987;&#21644;&#30456;&#24212;&#30340;&#23545;&#31574;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;</title><link>http://arxiv.org/abs/2206.08451</link><description>&lt;p&gt;
&#25105;&#30693;&#36947;&#20320;&#21435;&#24180;&#35757;&#32451;&#20102;&#20160;&#20040;&#65306;&#20851;&#20110;&#31363;&#21462;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#38450;&#24481;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences. (arXiv:2206.08451v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08451
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#30340;&#26222;&#21450;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#26368;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#20294;&#20063;&#21361;&#21450;&#20102;MLaaS&#25552;&#20379;&#21830;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#22791;&#30456;&#21516;&#34892;&#20026;&#30340;&#27169;&#22411;&#21103;&#26412;&#65292;&#26412;&#25991;&#38024;&#23545;&#27169;&#22411;&#31363;&#21462;&#30340;&#25915;&#20987;&#21644;&#30456;&#24212;&#30340;&#23545;&#31574;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#30340;&#33539; paradigm&#65292;&#36890;&#36807;&#25353;&#38656;&#20184;&#36153;&#30340;&#21407;&#21017;&#65292;&#29978;&#33267;&#21487;&#20197;&#20026;&#23458;&#25143;&#25552;&#20379;&#26368;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#36825;&#20351;&#29992;&#25143;&#21487;&#20197;&#36991;&#20813;&#32791;&#26102;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#35775;&#38382;&#65288;&#20854;&#39044;&#27979;&#30340;&#65289;&#27169;&#22411;&#65292;MLaaS &#25552;&#20379;&#21830;&#21361;&#21450;&#20854;&#30693;&#35782;&#20135;&#26435;&#65292;&#22914;&#25935;&#24863;&#30340;&#35757;&#32451;&#25968;&#25454;&#12289;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#25110;&#23398;&#20064;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#20165;&#39044;&#27979;&#26631;&#31614;&#21019;&#24314;&#20855;&#26377;&#65288;&#20960;&#20046;&#65289;&#30456;&#21516;&#34892;&#20026;&#30340;&#27169;&#22411;&#21103;&#26412;&#12290;&#34429;&#28982;&#25551;&#36848;&#20102;&#35768;&#22810;&#36825;&#31181;&#25915;&#20987;&#30340;&#21464;&#20307;&#65292;&#20294;&#21482;&#25552;&#20986;&#20102;&#20998;&#25955;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#28041;&#21450;&#23396;&#31435;&#30340;&#23041;&#32961;&#12290;&#36825;&#25552;&#20986;&#20102;&#23545;&#27169;&#22411;&#31363;&#21462;&#39046;&#22495;&#36827;&#34892;&#24443;&#24213;&#31995;&#32479;&#21270;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#20840;&#38754;&#20102;&#35299;&#20026;&#20160;&#20040;&#36825;&#20123;&#25915;&#20987;&#25104;&#21151;&#20197;&#21450;&#22914;&#20309;&#20840;&#38754;&#22320;&#36827;&#34892;&#38450;&#24481;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#39033;&#32508;&#21512;&#24615;&#35843;&#26597;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#28085;&#30422;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#21644;&#30456;&#24212;&#30340;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-as-a-Service (MLaaS) has become a widespread paradigm, making even the most complex machine learning models available for clients via e.g. a pay-per-query principle. This allows users to avoid time-consuming processes of data collection, hyperparameter tuning, and model training. However, by giving their customers access to the (predictions of their) models, MLaaS providers endanger their intellectual property, such as sensitive training data, optimised hyperparameters, or learned model parameters. Adversaries can create a copy of the model with (almost) identical behavior using the the prediction labels only. While many variants of this attack have been described, only scattered defence strategies have been proposed, addressing isolated threats. This raises the necessity for a thorough systematisation of the field of model stealing, to arrive at a comprehensive understanding why these attacks are successful, and how they could be holistically defended against. We addr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26354;&#27491;&#35268;&#21270;&#25216;&#26415;&#65292;&#23558;&#21452;&#26354;&#32447;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#27431;&#20960;&#37324;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#26377;&#26426;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25670;&#33073;&#22810;&#20010;&#19981;&#19968;&#33268;&#21521;&#37327;&#31354;&#38388;&#20043;&#38388;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#20445;&#30041;&#21452;&#26354;&#32447;&#32593;&#32476;&#27169;&#25311;&#20998;&#23618;&#22270;&#32467;&#26500;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.04285</link><description>&lt;p&gt;
&#27431;&#20960;&#37324;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#26354;&#32447;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unification Framework for Euclidean and Hyperbolic Graph Neural Networks. (arXiv:2206.04285v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26354;&#27491;&#35268;&#21270;&#25216;&#26415;&#65292;&#23558;&#21452;&#26354;&#32447;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#27431;&#20960;&#37324;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#26377;&#26426;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25670;&#33073;&#22810;&#20010;&#19981;&#19968;&#33268;&#21521;&#37327;&#31354;&#38388;&#20043;&#38388;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#20445;&#30041;&#21452;&#26354;&#32447;&#32593;&#32476;&#27169;&#25311;&#20998;&#23618;&#22270;&#32467;&#26500;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#26354;&#32447;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#23618;&#27425;&#32467;&#26500;&#65292;&#25104;&#20026;&#24378;&#22823;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22270;&#23618;&#20869;&#32416;&#32544;&#30528;&#22810;&#20010;&#19981;&#19968;&#33268;&#30340;&#65288;&#38464;&#34746;&#65289;&#21521;&#37327;&#31354;&#38388;&#65292;&#36825;&#20351;&#23427;&#20204;&#22312;&#27867;&#21270;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#21463;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;Poincare disk&#27169;&#22411;&#20316;&#20026;&#25105;&#20204;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#23558;&#25152;&#26377;&#36817;&#20284;&#37117;&#24212;&#29992;&#20110;&#35813;&#30913;&#30424;&#19978;&#65288;&#23601;&#22909;&#20687;&#35813;&#30913;&#30424;&#26159;&#20174;&#21407;&#28857;&#27966;&#29983;&#20986;&#30340;&#20999;&#31354;&#38388;&#65289;&#65292;&#20174;&#32780;&#25670;&#33073;&#20102;&#25152;&#26377;&#31354;&#38388;&#36716;&#25442;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19968;&#20010;&#21452;&#26354;&#27491;&#35268;&#21270;&#23618;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#25972;&#20010;&#21452;&#26354;&#32447;&#27169;&#22411;&#31616;&#21270;&#20026;&#27431;&#20960;&#37324;&#24471;&#27169;&#22411;&#65292;&#21518;&#32773;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;&#21452;&#26354;&#27491;&#35268;&#21270;&#23618;&#30340;&#32423;&#32852;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#38750;&#32447;&#24615;&#21452;&#26354;&#27491;&#35268;&#21270;&#24212;&#29992;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21516;&#36136;&#21644;&#22810;&#20851;&#31995;&#22270;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#21033;&#29992;&#20102;&#27431;&#20960;&#37324;&#24471;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#21644;&#21508;&#31181;&#27169;&#22411;&#32452;&#20214;&#30340;&#39640;&#25928;&#25191;&#34892;&#30340;&#20248;&#21183;&#65292;&#32780;&#19988;&#36824;&#20445;&#30041;&#20102;&#21452;&#26354;&#32447;&#32593;&#32476;&#27169;&#25311;&#20998;&#23618;&#22270;&#32467;&#26500;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic neural networks can effectively capture the inherent hierarchy of graph datasets, and consequently a powerful choice of GNNs. However, they entangle multiple incongruent (gyro-)vector spaces within a layer, which makes them limited in terms of generalization and scalability. In this work, we propose the Poincare disk model as our search space, and apply all approximations on the disk (as if the disk is a tangent space derived from the origin), thus getting rid of all inter-space transformations. Such an approach enables us to propose a hyperbolic normalization layer and to further simplify the entire hyperbolic model to a Euclidean model cascaded with our hyperbolic normalization layer. We applied our proposed nonlinear hyperbolic normalization to the current state-of-the-art homogeneous and multi-relational graph networks. We demonstrate that our model not only leverages the power of Euclidean networks such as interpretability and efficient execution of various model compon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36890;&#20449;&#31639;&#27861;&#65292;&#29992;&#20110;&#32852;&#37030;&#26497;&#23567;&#20540;&#23398;&#20064;&#65292;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#29305;&#24615;&#12290;&#30740;&#31350;&#20102;&#37325;&#35201;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#35813;&#30446;&#26631;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#38024;&#23545;&#32852;&#37030;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;Local Stochastic Gradient Descent Ascent (SGDA)&#31639;&#27861;&#65292;&#21487;&#20445;&#35777;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.01132</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#30340;&#39640;&#25928;&#36890;&#20449;&#31639;&#27861;&#65292;&#29992;&#20110;&#32852;&#37030;&#26497;&#23567;&#20540;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Communication-efficient Algorithm with Linear Convergence for Federated Minimax Learning. (arXiv:2206.01132v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36890;&#20449;&#31639;&#27861;&#65292;&#29992;&#20110;&#32852;&#37030;&#26497;&#23567;&#20540;&#23398;&#20064;&#65292;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#29305;&#24615;&#12290;&#30740;&#31350;&#20102;&#37325;&#35201;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#35813;&#30446;&#26631;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#38024;&#23545;&#32852;&#37030;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;Local Stochastic Gradient Descent Ascent (SGDA)&#31639;&#27861;&#65292;&#21487;&#20445;&#35777;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#26497;&#23567;&#21270;&#26368;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#27169;&#25311;&#20102;&#32479;&#35745;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#35768;&#22810;&#26377;&#36259;&#24212;&#29992;&#65292;&#21253;&#25324;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;(GAN)&#12290;&#24635;&#20307;&#30446;&#26631;&#26159;&#20195;&#29702;&#30340;&#31169;&#26377;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#20043;&#21644;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#37325;&#35201;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#21363;&#32463;&#39564;&#26368;&#23567;&#26497;&#23567;&#20540;&#38382;&#39064;&#65292;&#22312;&#20854;&#20013;&#65292;&#25972;&#20307;&#30446;&#26631;&#36890;&#36807;&#32479;&#35745;&#26679;&#26412;&#36817;&#20284;&#19968;&#20010;&#30495;&#23454;&#30340;&#26497;&#23567;&#20540;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;Rademacher&#22797;&#26434;&#24230;&#20998;&#26512;&#25552;&#20379;&#20102;&#23398;&#20064;&#27492;&#30446;&#26631;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20851;&#27880;&#32852;&#37030;&#35774;&#32622;&#65292;&#20854;&#20013;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#26412;&#22320;&#35745;&#31639;&#24182;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32852;&#37030;&#26497;&#23567;&#20540;&#31639;&#27861;&#35201;&#20040;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#36827;&#34892;&#36890;&#20449;&#65292;&#35201;&#20040;&#32570;&#20047;&#24615;&#33021;&#20445;&#35777;&#65292;&#38500;&#20102;&#26412;&#22320;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;(SGDA)&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#19979;&#38477;&#19978;&#21319;&#31639;&#27861;&#65292;&#22312;&#32553;&#23567;&#27493;&#38271;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study a large-scale multi-agent minimax optimization problem, which models many interesting applications in statistical learning and game theory, including Generative Adversarial Networks (GANs). The overall objective is a sum of agents' private local objective functions. We first analyze an important special case, empirical minimax problem, where the overall objective approximates a true population minimax risk by statistical samples. We provide generalization bounds for learning with this objective through Rademacher complexity analysis. Then, we focus on the federated setting, where agents can perform local computation and communicate with a central server. Most existing federated minimax algorithms either require communication per iteration or lack performance guarantees with the exception of Local Stochastic Gradient Descent Ascent (SGDA), a multiple-local-update descent ascent algorithm which guarantees convergence under a diminishing stepsize. By analyzing Loca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#8212;&#8212;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#36864;&#28779;&#65288;CMA-MAE&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36807;&#26089;&#22320;&#25918;&#24323;&#30446;&#26631;&#20197;&#36827;&#34892;&#25506;&#32034;&#12289;&#38590;&#20197;&#25506;&#32034;&#24179;&#22374;&#30446;&#26631;&#20197;&#21450;&#20302;&#20998;&#36776;&#29575;&#26723;&#26696;&#24615;&#33021;&#24046;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.10752</link><description>&lt;p&gt;
&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#36864;&#28779;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Covariance Matrix Adaptation MAP-Annealing. (arXiv:2205.10752v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#8212;&#8212;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#36864;&#28779;&#65288;CMA-MAE&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36807;&#26089;&#22320;&#25918;&#24323;&#30446;&#26631;&#20197;&#36827;&#34892;&#25506;&#32034;&#12289;&#38590;&#20197;&#25506;&#32034;&#24179;&#22374;&#30446;&#26631;&#20197;&#21450;&#20302;&#20998;&#36776;&#29575;&#26723;&#26696;&#24615;&#33021;&#24046;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#23547;&#25214;&#26368;&#39640;&#36136;&#37327;&#30340;&#21333;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#20248;&#21270;&#31639;&#27861;&#65292;&#20363;&#22914;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP-&#31934;&#33521;&#65288;CMA-ME&#65289;&#65292;&#23547;&#25214;&#19968;&#32452;&#26082;&#22312;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#39640;&#36136;&#37327;&#12289;&#21448;&#22312;&#29305;&#23450;&#24230;&#37327;&#20989;&#25968;&#26041;&#38754;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#12290;&#20294;&#26159;CMA-ME&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#30340;&#38480;&#21046;&#65306;&#36807;&#26089;&#22320;&#25918;&#24323;&#30446;&#26631;&#20197;&#36827;&#34892;&#25506;&#32034;&#12289;&#38590;&#20197;&#25506;&#32034;&#24179;&#22374;&#30446;&#26631;&#20197;&#21450;&#20302;&#20998;&#36776;&#29575;&#26723;&#26696;&#24615;&#33021;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#65292;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#36864;&#28779;&#65288;CMA-MAE&#65289;&#65292;&#20197;&#35299;&#20915;&#25152;&#26377;&#19977;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27599;&#20010;&#38480;&#21046;&#30340;&#26032;&#31639;&#27861;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25903;&#25745;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;CMA-MAE&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-objective optimization algorithms search for the single highest-quality solution with respect to an objective. Quality diversity (QD) optimization algorithms, such as Covariance Matrix Adaptation MAP-Elites (CMA-ME), search for a collection of solutions that are both high-quality with respect to an objective and diverse with respect to specified measure functions. However, CMA-ME suffers from three major limitations highlighted by the QD community: prematurely abandoning the objective in favor of exploration, struggling to explore flat objectives, and having poor performance for low-resolution archives. We propose a new quality diversity algorithm, Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), that addresses all three limitations. We provide theoretical justifications for the new algorithm with respect to each limitation. Our theory informs our experiments, which support the theory and show that CMA-MAE achieves state-of-the-art performance and robustness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23454;&#20363;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#35821;&#20041;&#20998;&#21106;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;Blob Loss&#65292;&#29992;&#20110;&#22810;&#23454;&#20363;&#26816;&#27979;&#65292;&#21487;&#36890;&#36807;&#25552;&#39640;F1&#20998;&#25968;&#21644;&#28789;&#25935;&#24230;&#31561;&#25351;&#26631;&#26469;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.08209</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#20363;&#19981;&#24179;&#34913;&#24863;&#30693;&#30340;&#35821;&#20041;&#20998;&#21106;&#25439;&#22833;&#20989;&#25968;&#65306;Blob Loss
&lt;/p&gt;
&lt;p&gt;
blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23454;&#20363;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#35821;&#20041;&#20998;&#21106;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;Blob Loss&#65292;&#29992;&#20110;&#22810;&#23454;&#20363;&#26816;&#27979;&#65292;&#21487;&#36890;&#36807;&#25552;&#39640;F1&#20998;&#25968;&#21644;&#28789;&#25935;&#24230;&#31561;&#25351;&#26631;&#26469;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#24050;&#26377;&#30340;&#25439;&#22833;&#20989;&#25968;&#34429;&#28982;&#38024;&#23545;&#25913;&#36827;&#20307;&#37327;&#24471;&#20998;&#36827;&#34892;&#20102;&#35774;&#35745;&#65292;&#22914;Dice&#31995;&#25968;&#65288;DSC&#65289;&#65292;&#20294;&#20854;&#26080;&#27861;&#35782;&#21035;&#31867;&#21035;&#20869;&#30340;&#23454;&#20363;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#23548;&#33268;&#22823;&#30340;&#21069;&#26223;&#23454;&#20363;&#21487;&#20197;&#25903;&#37197;&#23567;&#30340;&#23454;&#20363;&#32780;&#20173;&#28982;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;DSC&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;Blob Loss&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#24863;&#30693;&#24615;&#33021;&#25351;&#26631;, &#22914;F1&#20998;&#25968;&#21644;&#28789;&#25935;&#24230;&#65292;&#38024;&#23545;&#22810;&#23454;&#20363;&#26816;&#27979;&#30340;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional neural networks (CNN) have proven to be remarkably effective in semantic segmentation tasks. Most popular loss functions were introduced targeting improved volumetric scores, such as the Dice coefficient (DSC). By design, DSC can tackle class imbalance, however, it does not recognize instance imbalance within a class. As a result, a large foreground instance can dominate minor instances and still produce a satisfactory DSC. Nevertheless, detecting tiny instances is crucial for many applications, such as disease monitoring. For example, it is imperative to locate and surveil small-scale lesions in the follow-up of multiple sclerosis patients. We propose a novel family of loss functions, \emph{blob loss}, primarily aimed at maximizing instance-level detection metrics, such as F1 score and sensitivity. \emph{Blob loss} is designed for semantic segmentation problems where detecting multiple instances matters. We extensively evaluate a DSC-based \emph{blob loss} in five c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;RKHS&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;&#65292;&#38024;&#23545;&#20004;&#20010;&#19981;&#21516;&#30340;&#20284;&#28982;&#27604;&#26063;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;KRR&#20272;&#35745;&#37327;&#20855;&#26377;&#26497;&#23567;&#21270;&#29575;&#26368;&#20248;&#30340;&#29305;&#28857;&#65292;&#23588;&#20854;&#26159;&#22312;&#20284;&#28982;&#27604;&#34987;&#22343;&#21248;&#26377;&#30028;&#26102;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26412;&#25991;&#20063;&#35777;&#26126;&#20102;&#65292;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#19968;&#20010;naive&#30340;&#20272;&#35745;&#22120;&#30456;&#27604;&#20110;KRR&#26159;&#20005;&#26684;&#27425;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2205.02986</link><description>&lt;p&gt;
&#22312;RKHS&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#26368;&#20248;&#35299;&#20915;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Optimally tackling covariate shift in RKHS-based nonparametric regression. (arXiv:2205.02986v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;RKHS&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;&#65292;&#38024;&#23545;&#20004;&#20010;&#19981;&#21516;&#30340;&#20284;&#28982;&#27604;&#26063;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;KRR&#20272;&#35745;&#37327;&#20855;&#26377;&#26497;&#23567;&#21270;&#29575;&#26368;&#20248;&#30340;&#29305;&#28857;&#65292;&#23588;&#20854;&#26159;&#22312;&#20284;&#28982;&#27604;&#34987;&#22343;&#21248;&#26377;&#30028;&#26102;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26412;&#25991;&#20063;&#35777;&#26126;&#20102;&#65292;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#19968;&#20010;naive&#30340;&#20272;&#35745;&#22120;&#30456;&#27604;&#20110;KRR&#26159;&#20005;&#26684;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#19978;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#20851;&#27880;&#20004;&#20010;&#20351;&#29992;&#28304;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#20284;&#28982;&#27604;&#23450;&#20041;&#30340;&#33258;&#28982;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;&#26063;&#12290;&#24403;&#20284;&#28982;&#27604;&#34987;&#22343;&#21248;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#24102;&#26377;&#31934;&#24515;&#36873;&#25321;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26680;&#23725;&#22238;&#24402;(KRR)&#20272;&#35745;&#37327;&#26159;&#26497;&#23567;&#21270;&#29575;&#26368;&#20248;&#30340;&#65288;&#26368;&#22810;&#24046;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#65289;&#65292;&#23545;&#20110;&#19968;&#22823;&#31867;&#20855;&#26377;&#27491;&#21017;&#26680;&#29305;&#24449;&#20540;&#30340;RKHS&#32780;&#35328;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38500;&#20102;&#20284;&#28982;&#27604;&#19978;&#30028;&#20043;&#22806;&#65292;KRR&#19981;&#38656;&#35201;&#23545;&#20284;&#28982;&#27604;&#26377;&#23436;&#20840;&#30340;&#30693;&#35782;&#12290;&#19982;&#27809;&#26377;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#26631;&#20934;&#32479;&#35745;&#35774;&#32622;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#20272;&#35745;&#22120;&#65292;&#21363;&#22312;&#20989;&#25968;&#31867;&#20013;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#65292;&#19982;KRR&#30456;&#27604;&#65292;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#26159;&#20005;&#26684;&#27425;&#20248;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26356;&#22823;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;&#31867;&#65292;&#20854;&#20013;&#20284;&#28982;&#27604;&#21487;&#33021;&#26159;&#26080;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the covariate shift problem in the context of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We focus on two natural families of covariate shift problems defined using the likelihood ratios between the source and target distributions. When the likelihood ratios are uniformly bounded, we prove that the kernel ridge regression (KRR) estimator with a carefully chosen regularization parameter is minimax rate-optimal (up to a log factor) for a large family of RKHSs with regular kernel eigenvalues. Interestingly, KRR does not require full knowledge of likelihood ratios apart from an upper bound on them. In striking contrast to the standard statistical setting without covariate shift, we also demonstrate that a naive estimator, which minimizes the empirical risk over the function class, is strictly sub-optimal under covariate shift as compared to KRR. We then address the larger class of covariate shift problems where the likelihood ratio is possibly unbounde
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#39640;&#32423;&#27010;&#24565;&#25552;&#20379;OOD&#26816;&#27979;&#22120;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23398;&#20064;&#19968;&#32452;&#20855;&#26377;&#39640;&#26816;&#27979;&#23436;&#25972;&#24615;&#21644;&#27010;&#24565;&#21487;&#20998;&#31163;&#24615;&#29305;&#24449;&#30340;&#27010;&#24565;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;OOD&#26816;&#27979;&#22120;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.02586</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#29992;&#20110;&#22806;&#20998;&#24067;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Concept-based Explanations for Out-Of-Distribution Detectors. (arXiv:2203.02586v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#39640;&#32423;&#27010;&#24565;&#25552;&#20379;OOD&#26816;&#27979;&#22120;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23398;&#20064;&#19968;&#32452;&#20855;&#26377;&#39640;&#26816;&#27979;&#23436;&#25972;&#24615;&#21644;&#27010;&#24565;&#21487;&#20998;&#31163;&#24615;&#29305;&#24449;&#30340;&#27010;&#24565;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;OOD&#26816;&#27979;&#22120;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#22312;&#30830;&#20445;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20998;&#31867;&#22120;&#23433;&#20840;&#37096;&#32626;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#34429;&#28982;&#26377;&#22823;&#37327;&#26041;&#27861;&#33268;&#21147;&#20110;&#25552;&#39640;OOD&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#20915;&#31574;&#30340;&#35299;&#37322;&#20173;&#23384;&#22312;&#37325;&#35201;&#32570;&#38519;&#12290;&#20026;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#39640;&#32423;&#27010;&#24565;&#25552;&#20379;OOD&#26816;&#27979;&#22120;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#19968;&#32452;&#29305;&#23450;&#27010;&#24565;&#35299;&#37322;OOD&#26816;&#27979;&#22120;&#30340;&#25928;&#26524;&#65306;1&#65289;&#26816;&#27979;&#23436;&#25972;&#24615;&#65292;&#29992;&#20110;&#37327;&#21270;&#27010;&#24565;&#23545;&#20110;&#35299;&#37322;OOD&#26816;&#27979;&#22120;&#20915;&#31574;&#30340;&#20805;&#20998;&#31243;&#24230;&#65307;2&#65289;&#27010;&#24565;&#21487;&#20998;&#31163;&#24230;&#65292;&#29992;&#20110;&#25429;&#25417;&#27010;&#24565;&#31354;&#38388;&#20013;&#27491;&#24120;&#20998;&#24067;&#21644;OOD&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#22522;&#20110;&#36825;&#20123;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#32452;&#20855;&#26377;&#39640;&#26816;&#27979;&#23436;&#25972;&#24615;&#21644;&#27010;&#24565;&#21487;&#20998;&#31163;&#24615;&#29305;&#24449;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#35299;&#37322;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection plays a crucial role in ensuring the safe deployment of deep neural network (DNN) classifiers. While a myriad of methods have focused on improving the performance of OOD detectors, a critical gap remains in interpreting their decisions. We help bridge this gap by providing explanations for OOD detectors based on learned high-level concepts. We first propose two new metrics for assessing the effectiveness of a particular set of concepts for explaining OOD detectors: 1) detection completeness, which quantifies the sufficiency of concepts for explaining an OOD-detector's decisions, and 2) concept separability, which captures the distributional separation between in-distribution and OOD data in the concept space. Based on these metrics, we propose an unsupervised framework for learning a set of concepts that satisfy the desired properties of high detection completeness and concept separability, and demonstrate its effectiveness in providing concept-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEXNet&#30340;&#36731;&#37327;&#32423;&#12289;&#39640;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#27531;&#24046;&#22359;&#21644;&#21407;&#22411;&#23618;&#35299;&#20915;&#20102;&#29616;&#26377;&#27969;&#37327;&#20998;&#31867;&#22120;&#25152;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21830;&#19994;&#32423;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2202.05535</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#39640;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Lightweight, Efficient and Explainable-by-Design Convolutional Neural Network for Internet Traffic Classification. (arXiv:2202.05535v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEXNet&#30340;&#36731;&#37327;&#32423;&#12289;&#39640;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#27531;&#24046;&#22359;&#21644;&#21407;&#22411;&#23618;&#35299;&#20915;&#20102;&#29616;&#26377;&#27969;&#37327;&#20998;&#31867;&#22120;&#25152;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21830;&#19994;&#32423;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#37327;&#20998;&#31867;&#65292;&#21363;&#35782;&#21035;&#22312;&#32593;&#32476;&#20013;&#27969;&#21160;&#30340;&#24212;&#29992;&#31867;&#22411;&#65292;&#23545;&#20110;&#20247;&#22810;&#27963;&#21160;&#65288;&#20363;&#22914;&#20837;&#20405;&#26816;&#27979;&#12289;&#36335;&#30001;&#65289;&#37117;&#26159;&#19968;&#20010;&#25112;&#30053;&#24615;&#20219;&#21153;&#12290;&#28982;&#32780;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24182;&#27809;&#26377;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#35774;&#35745;&#27809;&#26377;&#32771;&#34385;&#21040;&#32593;&#32476;&#30828;&#20214;&#65288;&#20363;&#22914;&#36335;&#30001;&#22120;&#65289;&#36890;&#24120;&#26159;&#20351;&#29992;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#27809;&#26377;&#28385;&#36275;&#30417;&#31649;&#26426;&#26500;&#25152;&#24378;&#35843;&#30340;&#31934;&#30830;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#26368;&#21518;&#65292;&#36825;&#20123;&#27969;&#37327;&#20998;&#31867;&#22120;&#26159;&#22522;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#65292;&#36825;&#26080;&#27861;&#21453;&#26144;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36731;&#37327;&#32423;&#12289;&#39640;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;LEXNet&#65289;&#65292;&#29992;&#20110;&#20114;&#32852;&#32593;&#27969;&#37327;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#30340;&#27531;&#24046;&#22359;&#65288;&#29992;&#20110;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#29575;&#30446;&#30340;&#65289;&#21644;&#21407;&#22411;&#23618;&#65288;&#29992;&#20110;&#35299;&#37322;&#24615;&#65289;&#12290;&#36890;&#36807;&#21830;&#19994;&#32423;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;LEXNet &#22312;&#20934;&#30830;&#29575;&#21644;&#36895;&#24230;&#31561;&#26041;&#38754;&#37117;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic classification, i.e. the identification of the type of applications flowing in a network, is a strategic task for numerous activities (e.g., intrusion detection, routing). This task faces some critical challenges that current deep learning approaches do not address. The design of current approaches do not take into consideration the fact that networking hardware (e.g., routers) often runs with limited computational resources. Further, they do not meet the need for faithful explainability highlighted by regulatory bodies. Finally, these traffic classifiers are evaluated on small datasets which fail to reflect the diversity of applications in real-world settings.  Therefore, this paper introduces a new Lightweight, Efficient and eXplainable-by-design convolutional neural network (LEXNet) for Internet traffic classification, which relies on a new residual block (for lightweight and efficiency purposes) and prototype layer (for explainability). Based on a commercial-grade dataset, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;L-SVRG&#21644;L-Katyusha&#20248;&#21270;&#26041;&#27861;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21487;&#20197;&#22312;&#23569;&#37327;&#35745;&#31639;&#24320;&#38144;&#20869;&#23454;&#29616;&#37319;&#26679;&#20998;&#24067;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2201.13387</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;L-SVRG&#21644;L-Katyusha&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
L-SVRG and L-Katyusha with Adaptive Sampling. (arXiv:2201.13387v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;L-SVRG&#21644;L-Katyusha&#20248;&#21270;&#26041;&#27861;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21487;&#20197;&#22312;&#23569;&#37327;&#35745;&#31639;&#24320;&#38144;&#20869;&#23454;&#29616;&#37319;&#26679;&#20998;&#24067;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;L-SVRG&#21450;&#20854;&#21152;&#36895;&#21464;&#31181;L-Katyusha&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#23569;&#37327;&#35745;&#31639;&#24320;&#38144;&#20869;&#23398;&#20064;&#37319;&#26679;&#20998;&#24067;&#65292;&#21516;&#26102;&#21487;&#20197;&#38543;&#30528;&#36845;&#20195;&#32780;&#25913;&#21464;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#23545;&#20110;&#20984;&#30446;&#26631;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;L-SVRG&#21644;L-Katyusha&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient-based optimization methods, such as L-SVRG and its accelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train machine learning models.The theoretical and empirical performance of L-SVRG and L-Katyusha can be improved by sampling observations from a non-uniform distribution (Qian et al., 2021). However,designing a desired sampling distribution requires prior knowledge of smoothness constants, which can be computationally intractable to obtain in practice when the dimension of the model parameter is high. To address this issue, we propose an adaptive sampling strategy for L-SVRG and L-Katyusha that can learn the sampling distribution with little computational overhead, while allowing it to change with iterates, and at the same time does not require any prior knowledge of the problem parameters. We prove convergence guarantees for L-SVRG and L-Katyusha for convex objectives when the sampling distribution changes with iterates. Our results show that
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#23545;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#21160;&#20316;&#29305;&#24449;&#21644;&#35266;&#27979;&#29305;&#24449;&#20043;&#38388;&#35821;&#20041;&#20851;&#31995;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#32852;&#21512;&#22788;&#29702;&#35266;&#23519;&#29305;&#24449;&#21644;&#21160;&#20316;&#29305;&#24449;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26550;&#26500;&#21487;&#20197;&#23398;&#20064;&#30452;&#35273;&#31574;&#30053;&#65292;&#24182;&#19988;&#36825;&#26679;&#30340;&#20195;&#29702;&#19982;&#20154;&#31867;&#21327;&#20316;&#32780;&#26080;&#38656;&#25509;&#21463;&#20154;&#31867;&#25968;&#25454;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2201.12658</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#20316;&#29305;&#24449;&#23398;&#20064;&#30452;&#35273;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Intuitive Policies Using Action Features. (arXiv:2201.12658v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#23545;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#21160;&#20316;&#29305;&#24449;&#21644;&#35266;&#27979;&#29305;&#24449;&#20043;&#38388;&#35821;&#20041;&#20851;&#31995;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#32852;&#21512;&#22788;&#29702;&#35266;&#23519;&#29305;&#24449;&#21644;&#21160;&#20316;&#29305;&#24449;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26550;&#26500;&#21487;&#20197;&#23398;&#20064;&#30452;&#35273;&#31574;&#30053;&#65292;&#24182;&#19988;&#36825;&#26679;&#30340;&#20195;&#29702;&#19982;&#20154;&#31867;&#21327;&#20316;&#32780;&#26080;&#38656;&#25509;&#21463;&#20154;&#31867;&#25968;&#25454;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#26159;&#20351;AI&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#21160;&#20316;&#29305;&#24449;&#21644;&#35266;&#27979;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#20154;&#31867;&#20197;&#39640;&#24230;&#30452;&#35273;&#30340;&#26041;&#24335;&#21033;&#29992;&#36825;&#20123;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#23545;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#36825;&#20123;&#35821;&#20041;&#20851;&#31995;&#30340;&#20542;&#21521;&#30340;&#24433;&#21709;&#12290;&#22312;&#19968;&#20010;&#31243;&#24207;&#29983;&#25104;&#30340;&#21327;&#20316;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#32852;&#21512;&#22788;&#29702;&#35266;&#23519;&#29305;&#24449;&#21644;&#21160;&#20316;&#29305;&#24449;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26550;&#26500;&#20855;&#26377;&#26356;&#22909;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#21487;&#20197;&#23398;&#20064;&#30452;&#35273;&#31574;&#30053;&#12290;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35780;&#20272;&#21644;&#22330;&#26223;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24471;&#21040;&#30340;&#31574;&#30053;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#26679;&#30340;&#20195;&#29702;&#19982;&#20154;&#31867;&#21327;&#20316;&#32780;&#26080;&#38656;&#25509;&#21463;&#20219;&#20309;&#20154;&#31867;&#25968;&#25454;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
An unaddressed challenge in multi-agent coordination is to enable AI agents to exploit the semantic relationships between the features of actions and the features of observations. Humans take advantage of these relationships in highly intuitive ways. For instance, in the absence of a shared language, we might point to the object we desire or hold up our fingers to indicate how many objects we want. To address this challenge, we investigate the effect of network architecture on the propensity of learning algorithms to exploit these semantic relationships. Across a procedurally generated coordination task, we find that attention-based architectures that jointly process a featurized representation of observations and actions have a better inductive bias for learning intuitive policies. Through fine-grained evaluation and scenario analysis, we show that the resulting policies are human-interpretable. Moreover, such agents coordinate with people without training on any human data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#30340;&#32531;&#23384;&#31639;&#27861;&#65292;&#20855;&#26377;&#26080;&#36951;&#25022;&#30340;&#29305;&#24615;&#65292;&#20851;&#38190;&#20381;&#36182;&#20110;&#35831;&#27714;&#36807;&#31243;&#30340;&#22810;&#26679;&#24615;&#27604;&#29575;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#22312;&#38656;&#35201;&#25972;&#20010;&#25991;&#20214;&#32531;&#23384;&#26102;&#65292;&#20063;&#33021;&#20445;&#35777;&#26080;&#36951;&#25022;&#24615;&#12290;</title><link>http://arxiv.org/abs/2101.12588</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#23454;&#29616;&#26080;&#36951;&#25022;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
No-Regret Caching via Online Mirror Descent. (arXiv:2101.12588v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.12588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#30340;&#32531;&#23384;&#31639;&#27861;&#65292;&#20855;&#26377;&#26080;&#36951;&#25022;&#30340;&#29305;&#24615;&#65292;&#20851;&#38190;&#20381;&#36182;&#20110;&#35831;&#27714;&#36807;&#31243;&#30340;&#22810;&#26679;&#24615;&#27604;&#29575;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#22312;&#38656;&#35201;&#25972;&#20010;&#25991;&#20214;&#32531;&#23384;&#26102;&#65292;&#20063;&#33021;&#20445;&#35777;&#26080;&#36951;&#25022;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#32531;&#23384;&#38382;&#39064;&#65292;&#20854;&#20013;&#35831;&#27714;&#21487;&#20197;&#36890;&#36807;&#26412;&#22320;&#32531;&#23384;&#26469;&#26381;&#21153;&#65292;&#20197;&#36991;&#20813;&#20174;&#36828;&#31243;&#26381;&#21153;&#22120;&#26816;&#32034;&#25104;&#26412;&#12290;&#32531;&#23384;&#21487;&#20197;&#22312;&#35831;&#27714;&#30340;&#19968;&#25209;&#20043;&#21518;&#26356;&#26032;&#20854;&#29366;&#24577;&#65292;&#24182;&#23384;&#20648;&#27599;&#20010;&#25991;&#20214;&#30340;&#20219;&#24847;&#23567;&#37096;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65288;OMD&#65289;&#31574;&#30053;&#30340;&#26080;&#36951;&#25022;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36951;&#25022;&#30340;&#30028;&#38480;&#20851;&#38190;&#21462;&#20915;&#20110;&#35831;&#27714;&#36807;&#31243;&#30340;&#22810;&#26679;&#24615;&#65292;&#30001;&#35831;&#27714;&#25209;&#22788;&#29702;&#22823;&#23567;R&#21644;&#32473;&#23450;&#25209;&#22788;&#29702;&#20013;&#35831;&#27714;&#30340;&#26368;&#22823;&#37325;&#22797;&#24230;h&#25152;&#25552;&#20379;&#30340;&#22810;&#26679;&#24615;&#27604;&#29575;R/h&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#19981;&#21516;&#22810;&#26679;&#24615;&#26041;&#26696;&#19979;OMD&#32531;&#23384;&#31574;&#30053;&#20851;&#20110;&#36951;&#25022;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#24403;&#32531;&#23384;&#24517;&#39035;&#23384;&#20648;&#25972;&#20010;&#25991;&#20214;&#32780;&#19981;&#26159;&#19968;&#37096;&#20998;&#26102;&#65292;OMD&#31574;&#30053;&#21487;&#20197;&#19982;&#38543;&#26426;&#33293;&#20837;&#26041;&#26696;&#30456;&#32806;&#21512;&#65292;&#21363;&#20351;&#19981;&#33021;&#24573;&#30053;&#26356;&#26032;&#25104;&#26412;&#65292;&#20063;&#33021;&#20445;&#25345;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#20026;&#33293;&#20837;&#38382;&#39064;&#25552;&#20379;&#20102;&#27491;&#24335;&#30340;&#34920;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31574;&#30053;&#30340;&#32531;&#23384;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study an online caching problem in which requests can be served by a local cache to avoid retrieval costs from a remote server. The cache can update its state after a batch of requests and store an arbitrarily small fraction of each file. We study no-regret algorithms based on Online Mirror Descent (OMD) strategies. We show that bounds for the regret crucially depend on the diversity of the request process, provided by the diversity ratio R/h, where R is the size of the batch, and h is the maximum multiplicity of a request in a given batch. We characterize the optimality of OMD caching policies w.r.t. regret under different diversity regimes. We also prove that, when the cache must store the entire file, rather than a fraction, OMD strategies can be coupled with a randomized rounding scheme that preserves regret guarantees, even when update costs cannot be neglected. We provide a formal characterization of the rounding problem through optimal transport theory, and moreover we propos
&lt;/p&gt;</description></item><item><title>&#23545;&#31216;&#25439;&#22833;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#20195;&#29702;&#25439;&#22833;&#65292;&#33021;&#22815;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#23545;&#20110;&#21463;&#25439;&#26631;&#31614;&#26356;&#21152;&#40065;&#26834;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2101.01366</link><description>&lt;p&gt;
&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;&#30340;&#23545;&#31216;&#25439;&#22833;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Symmetric Loss Perspective of Reliable Machine Learning. (arXiv:2101.01366v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.01366
&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#25439;&#22833;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#20195;&#29702;&#25439;&#22833;&#65292;&#33021;&#22815;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#23545;&#20110;&#21463;&#25439;&#26631;&#31614;&#26356;&#21152;&#40065;&#26834;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#26102;&#65292;&#24120;&#24120;&#23558;&#38646;&#19968;&#25439;&#22833;&#26367;&#25442;&#20026;&#20195;&#29702;&#25439;&#22833;&#65292;&#20197;&#20351;&#23398;&#20064;&#30446;&#26631;&#26131;&#20110;&#20248;&#21270;&#12290;&#20108;&#20803;&#20998;&#31867;&#30340;&#20195;&#29702;&#25439;&#22833;&#20363;&#22914;&#36923;&#36753;&#25439;&#22833;&#65292;hinge&#25439;&#22833;&#21644;sigmoid&#25439;&#22833;&#24191;&#20026;&#20154;&#30693;&#12290;&#24050;&#30693;&#20195;&#29702;&#25439;&#22833;&#30340;&#36873;&#25321;&#20250;&#26497;&#22823;&#22320;&#24433;&#21709;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#24212;&#35813;&#20180;&#32454;&#36873;&#25321;&#12290;&#26368;&#36817;&#65292;&#28385;&#36275;&#26576;&#20123;&#23545;&#31216;&#26465;&#20214;&#65288;&#31216;&#20026;&#23545;&#31216;&#25439;&#22833;&#65289;&#30340;&#20195;&#29702;&#25439;&#22833;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#23398;&#20064;&#26469;&#33258;&#25439;&#22351;&#26631;&#31614;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#23545;&#31216;&#25439;&#22833;&#21450;&#20854;&#24212;&#29992;&#30340;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#23545;&#31216;&#25439;&#22833;&#22914;&#20309;&#22312;&#24179;&#34913;&#35823;&#24046;&#29575;&#65288;BER&#65289;&#26368;&#23567;&#21270;&#21644;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#26368;&#22823;&#21270;&#20013;&#20135;&#29983;&#40065;&#26834;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#40065;&#26834;AUC&#26368;&#22823;&#21270;&#26041;&#27861;&#21487;&#20197;&#21463;&#30410;&#20110;&#21463;&#25439;&#26631;&#31614;&#65292;&#23588;&#20854;&#26159;&#19982;&#20854;&#20182;&#20195;&#29702;&#25439;&#22833;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
When minimizing the empirical risk in binary classification, it is a common practice to replace the zero-one loss with a surrogate loss to make the learning objective feasible to optimize. Examples of well-known surrogate losses for binary classification include the logistic loss, hinge loss, and sigmoid loss. It is known that the choice of a surrogate loss can highly influence the performance of the trained classifier and therefore it should be carefully chosen. Recently, surrogate losses that satisfy a certain symmetric condition (aka., symmetric losses) have demonstrated their usefulness in learning from corrupted labels. In this article, we provide an overview of symmetric losses and their applications. First, we review how a symmetric loss can yield robust classification from corrupted labels in balanced error rate (BER) minimization and area under the receiver operating characteristic curve (AUC) maximization. Then, we demonstrate how the robust AUC maximization method can benefi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32467;&#26500;&#21270;&#36830;&#32493;&#31232;&#30095;&#21270;&#30340;&#28145;&#24230;&#32593;&#32476;&#32467;&#26500;&#29983;&#38271;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#32493;&#26494;&#24347;&#21644;&#37319;&#26679;&#31232;&#30095;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36798;&#21040;&#32039;&#20945;&#30340;&#20462;&#21098;&#32593;&#32476;&#32467;&#26500;&#65292;&#21516;&#26102;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2007.15353</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#36830;&#32493;&#31232;&#30095;&#21270;&#22686;&#24378;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Growing Efficient Deep Networks by Structured Continuous Sparsification. (arXiv:2007.15353v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.15353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32467;&#26500;&#21270;&#36830;&#32493;&#31232;&#30095;&#21270;&#30340;&#28145;&#24230;&#32593;&#32476;&#32467;&#26500;&#29983;&#38271;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#32493;&#26494;&#24347;&#21644;&#37319;&#26679;&#31232;&#30095;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36798;&#21040;&#32039;&#20945;&#30340;&#20462;&#21098;&#32593;&#32476;&#32467;&#26500;&#65292;&#21516;&#26102;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20197;&#31934;&#24230;&#21644;&#31232;&#30095;&#24615;&#20026;&#39537;&#21160;&#30340;&#28145;&#24230;&#32593;&#32476;&#32467;&#26500;&#29983;&#38271;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#23436;&#25972;&#27169;&#22411;&#25110;&#36229;&#32593;&#26684;&#26550;&#26500;&#30340;&#21098;&#26525;&#25110;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#19968;&#20010;&#23567;&#32780;&#31616;&#21333;&#30340;&#31181;&#23376;&#26550;&#26500;&#24320;&#22987;&#65292;&#21160;&#24577;&#22320;&#22686;&#38271;&#21644;&#20462;&#21098;&#23618;&#21644;&#36807;&#28388;&#22120;&#12290;&#36890;&#36807;&#23558;&#31163;&#25955;&#32593;&#32476;&#32467;&#26500;&#20248;&#21270;&#30340;&#36830;&#32493;&#26494;&#24347;&#19982;&#37319;&#26679;&#31232;&#30095;&#23376;&#32593;&#32476;&#26041;&#26696;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#20135;&#29983;&#32039;&#20945;&#30340;&#20462;&#21098;&#32593;&#32476;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#20363;&#22914;&#65292;&#22312;ImageNet&#19978;&#65292;&#19982;&#22522;&#32447;ResNet-50&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;49.7&#65285;&#30340;&#25512;&#29702;FLOPs&#21644;47.4&#65285;&#30340;&#35757;&#32451;FLOPs&#33410;&#30465;&#65292;&#21516;&#26102;&#20445;&#25345;75.2&#65285;&#30340;top-1&#31934;&#24230;--&#25152;&#26377;&#36825;&#20123;&#37117;&#27809;&#26377;&#20219;&#20309;&#19987;&#38376;&#30340;&#24494;&#35843;&#38454;&#27573;&#12290;&#22312;CIFAR&#65292;ImageNet&#65292;PASCAL VOC&#21644;Penn Treebank&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20351;&#29992;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve $49.7\%$ inference FLOPs and $47.4\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\%$ top-1 accuracy -- all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#31649;&#36947;&#29992;&#20110;&#20174;&#26410;&#30699;&#27491;&#30340;&#21333;&#30446;&#35270;&#39057;&#20013;&#20272;&#35745;&#28145;&#24230;&#12289;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#35270;&#35273;&#37324;&#31243;&#35745;&#65292;&#20854;&#21487;&#38544;&#24335;&#22320;&#32452;&#32455;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#30699;&#27491;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#20854;&#23545;&#27169;&#22411;&#22797;&#26434;&#24230;&#25110;&#25512;&#29702;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2007.06676</link><description>&lt;p&gt;
UnRectDepthNet&#65306;&#20351;&#29992;&#36890;&#29992;&#22788;&#29702;&#24120;&#35265;&#30456;&#26426;&#22833;&#30495;&#27169;&#22411;&#30340;&#26694;&#26550;&#36827;&#34892;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models. (arXiv:2007.06676v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.06676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#31649;&#36947;&#29992;&#20110;&#20174;&#26410;&#30699;&#27491;&#30340;&#21333;&#30446;&#35270;&#39057;&#20013;&#20272;&#35745;&#28145;&#24230;&#12289;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#35270;&#35273;&#37324;&#31243;&#35745;&#65292;&#20854;&#21487;&#38544;&#24335;&#22320;&#32452;&#32455;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#30699;&#27491;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#20854;&#23545;&#27169;&#22411;&#22797;&#26434;&#24230;&#25110;&#25512;&#29702;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#30699;&#27491;&#26159;&#22810;&#35270;&#35282;&#28145;&#24230;&#20272;&#35745;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#36890;&#24120;&#21253;&#25324;&#26497;&#32447;&#26657;&#27491;&#21644;&#38236;&#22836;&#30072;&#21464;&#26657;&#27491;&#12290;&#36825;&#20010;&#36807;&#31243;&#26174;&#30528;&#31616;&#21270;&#20102;&#28145;&#24230;&#20272;&#35745;&#65292;&#22240;&#27492;&#34987;CNN&#26041;&#27861;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#30699;&#27491;&#26377;&#19968;&#20123;&#21103;&#20316;&#29992;&#65292;&#21253;&#25324;&#35270;&#37326;&#32553;&#23567;&#12289;&#37325;&#37319;&#26679;&#30072;&#21464;&#21644;&#23545;&#26657;&#20934;&#35823;&#24046;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#25928;&#24212;&#22312;&#23384;&#22312;&#26126;&#26174;&#30072;&#21464;&#65288;&#20363;&#22914;&#24191;&#35282;&#40060;&#30524;&#25668;&#20687;&#22836;&#65289;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#31649;&#36947;&#29992;&#20110;&#20174;&#26410;&#30699;&#27491;&#30340;&#21333;&#30446;&#35270;&#39057;&#20013;&#20272;&#35745;&#28145;&#24230;&#12289;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#35270;&#35273;&#37324;&#31243;&#35745;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#26742;&#24418;&#30072;&#21464;&#30340;&#26410;&#30699;&#27491;KITTI&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19982;&#30699;&#27491;&#21518;KITTI&#25968;&#25454;&#38598;&#30456;&#24403;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;&#20854;&#35843;&#25972;&#30340;&#36807;&#31243;&#34987;CNN&#27169;&#22411;&#38544;&#24335;&#22320;&#32452;&#32455;&#65292;&#23398;&#20064;&#22833;&#30495;&#27169;&#22411;&#32780;&#19981;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24230;&#25110;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classical computer vision, rectification is an integral part of multi-view depth estimation. It typically includes epipolar rectification and lens distortion correction. This process simplifies the depth estimation significantly, and thus it has been adopted in CNN approaches. However, rectification has several side effects, including a reduced field of view (FOV), resampling distortion, and sensitivity to calibration errors. The effects are particularly pronounced in case of significant distortion (e.g., wide-angle fisheye cameras). In this paper, we propose a generic scale-aware self-supervised pipeline for estimating depth, euclidean distance, and visual odometry from unrectified monocular videos. We demonstrate a similar level of precision on the unrectified KITTI dataset with barrel distortion comparable to the rectified KITTI dataset. The intuition being that the rectification step can be implicitly absorbed within the CNN model, which learns the distortion model without incre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#27979;&#24230;&#26465;&#20214;&#37319;&#26679;&#26694;&#26550;&#65292;&#20351;&#29992;&#21333;&#35843;GAN&#23398;&#20064;&#22359;&#29366;&#19977;&#35282;&#24418;&#26144;&#23556;&#65292;&#20165;&#20351;&#29992;&#26469;&#33258;&#24213;&#23618;&#32852;&#21512;&#27010;&#29575;&#27979;&#24230;&#30340;&#26679;&#26412;&#23454;&#29616;&#26080;&#20284;&#28982;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2006.06755</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#35843;GAN&#30340;&#26465;&#20214;&#37319;&#26679;&#65306;&#20174;&#29983;&#25104;&#27169;&#22411;&#21040;&#26080;&#20284;&#28982;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Conditional Sampling with Monotone GANs: from Generative Models to Likelihood-Free Inference. (arXiv:2006.06755v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.06755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#27979;&#24230;&#26465;&#20214;&#37319;&#26679;&#26694;&#26550;&#65292;&#20351;&#29992;&#21333;&#35843;GAN&#23398;&#20064;&#22359;&#29366;&#19977;&#35282;&#24418;&#26144;&#23556;&#65292;&#20165;&#20351;&#29992;&#26469;&#33258;&#24213;&#23618;&#32852;&#21512;&#27010;&#29575;&#27979;&#24230;&#30340;&#26679;&#26412;&#23454;&#29616;&#26080;&#20284;&#28982;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#27979;&#24230;&#26465;&#20214;&#37319;&#26679;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#22359;&#29366;&#19977;&#35282;&#24418;&#20256;&#36755;&#26144;&#23556;&#12290;&#25105;&#20204;&#22312;Banach&#31354;&#38388;&#35774;&#32622;&#19979;&#24320;&#21457;&#20102;&#22359;&#29366;&#19977;&#35282;&#24418;&#20256;&#36755;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24314;&#31435;&#20102;&#21487;&#20197;&#23454;&#29616;&#26465;&#20214;&#37319;&#26679;&#30340;&#19968;&#33324;&#26465;&#20214;&#65292;&#24182;&#22312;&#21333;&#35843;&#22359;&#29366;&#19977;&#35282;&#24418;&#26144;&#23556;&#19982;&#26368;&#20248;&#20256;&#36755;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#12290;&#22522;&#20110;&#35813;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#65292;&#31216;&#20026;&#21333;&#35843;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;M-GAN&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#21512;&#36866;&#30340;&#22359;&#29366;&#19977;&#35282;&#24418;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20165;&#20351;&#29992;&#26469;&#33258;&#24213;&#23618;&#32852;&#21512;&#27010;&#29575;&#27979;&#24230;&#30340;&#26679;&#26412;&#65292;&#22240;&#27492;&#26080;&#38656;&#20284;&#28982;&#12290;M-GAN&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#21512;&#25104;&#31034;&#20363;&#12289;&#28041;&#21450;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#65292;&#20197;&#21450;&#27010;&#29575;&#22270;&#20687;&#20462;&#22797;&#20013;&#20934;&#30830;&#37319;&#26679;&#26465;&#20214;&#27979;&#24230;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel framework for conditional sampling of probability measures, using block triangular transport maps. We develop the theoretical foundations of block triangular transport in a Banach space setting, establishing general conditions under which conditional sampling can be achieved and drawing connections between monotone block triangular maps and optimal transport. Based on this theory, we then introduce a computational approach, called monotone generative adversarial networks (M-GANs), to learn suitable block triangular maps. Our algorithm uses only samples from the underlying joint probability measure and is hence likelihood-free. Numerical experiments with M-GAN demonstrate accurate sampling of conditional measures in synthetic examples, Bayesian inverse problems involving ordinary and partial differential equations, and probabilistic image in-painting.
&lt;/p&gt;</description></item><item><title>Denise&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#21327;&#26041;&#24046;&#30697;&#38453;&#36827;&#34892;&#20302;&#31209;&#21152;&#31232;&#30095;&#20998;&#35299;&#65292;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#24615;&#33021;&#32780;&#19988;&#36817;&#20046;&#25509;&#36817;20&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2004.13612</link><description>&lt;p&gt;
Denise: &#38754;&#21521;&#21322;&#27491;&#23450;&#30697;&#38453;&#30340;&#28145;&#24230;&#20581;&#22766;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Denise: Deep Robust Principal Component Analysis for Positive Semidefinite Matrices. (arXiv:2004.13612v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.13612
&lt;/p&gt;
&lt;p&gt;
Denise&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#21327;&#26041;&#24046;&#30697;&#38453;&#36827;&#34892;&#20302;&#31209;&#21152;&#31232;&#30095;&#20998;&#35299;&#65292;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#24615;&#33021;&#32780;&#19988;&#36817;&#20046;&#25509;&#36817;20&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20581;&#22766;&#20027;&#25104;&#20998;&#20998;&#26512;&#22312;&#38548;&#31163;&#20851;&#38190;&#35299;&#37322;&#29305;&#24449;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30446;&#21069;&#21487;&#29992;&#30340;&#25191;&#34892;&#20302;&#31209;&#21152;&#31232;&#30095;&#20998;&#35299;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#29305;&#23450;&#30697;&#38453;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#36825;&#20123;&#31639;&#27861;&#24517;&#39035;&#38024;&#23545;&#27599;&#20010;&#26032;&#30340;&#30697;&#38453;&#37325;&#26032;&#36816;&#34892;&#12290;&#30001;&#20110;&#36825;&#20123;&#31639;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#27492;&#26368;&#22909;&#23398;&#20064;&#21644;&#23384;&#20648;&#19968;&#20010;&#20989;&#25968;&#65292;&#22312;&#35780;&#20272;&#26102;&#20960;&#20046;&#31435;&#21363;&#25191;&#34892;&#27492;&#20998;&#35299;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Denise&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20581;&#22766;&#20027;&#25104;&#20998;&#20998;&#26512;&#31639;&#27861;&#65292;&#25110;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#23545;&#31216;&#21322;&#27491;&#23450;&#30697;&#38453;&#65292;&#23427;&#23398;&#20064;&#21040;&#20102;&#36825;&#26679;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102; Denise &#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#36825;&#20123;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#36235;&#20110;&#23398;&#20064;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Denise &#22312;&#20998;&#35299;&#36136;&#37327;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#36817;&#20046;&#25509;&#36817;20&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robust PCA of covariance matrices plays an essential role when isolating key explanatory features. The currently available methods for performing such a low-rank plus sparse decomposition are matrix specific, meaning, those algorithms must re-run for every new matrix. Since these algorithms are computationally expensive, it is preferable to learn and store a function that nearly instantaneously performs this decomposition when evaluated. Therefore, we introduce Denise, a deep learning-based algorithm for robust PCA of covariance matrices, or more generally, of symmetric positive semidefinite matrices, which learns precisely such a function. Theoretical guarantees for Denise are provided. These include a novel universal approximation theorem adapted to our geometric deep learning problem and convergence to an optimal solution to the learning problem. Our experiments show that Denise matches state-of-the-art performance in terms of decomposition quality, while being approximately $20
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#26469;&#21046;&#23450;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#30446;&#26631;&#65292;&#23558;LTL&#23646;&#24615;&#36716;&#21270;&#20026;LDGBA&#33258;&#21160;&#26426;&#65292;&#36890;&#36807;&#35843;&#25972;&#21516;&#27493;&#22870;&#21169;&#20989;&#25968;&#26368;&#22823;&#27010;&#29575;&#33719;&#24471;&#28385;&#36275;LTL&#35268;&#23450;&#35201;&#27714;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/1902.00778</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#25351;&#23548;&#30340;&#35748;&#35777;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Certified Reinforcement Learning with Logic Guidance. (arXiv:1902.00778v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.00778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#26469;&#21046;&#23450;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#30446;&#26631;&#65292;&#23558;LTL&#23646;&#24615;&#36716;&#21270;&#20026;LDGBA&#33258;&#21160;&#26426;&#65292;&#36890;&#36807;&#35843;&#25972;&#21516;&#27493;&#22870;&#21169;&#20989;&#25968;&#26368;&#22823;&#27010;&#29575;&#33719;&#24471;&#28385;&#36275;LTL&#35268;&#23450;&#35201;&#27714;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#25511;&#21046;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#30340;&#24212;&#29992;&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#21644;&#27491;&#24335;&#30340;&#26041;&#27861;&#26469;&#25351;&#23450;&#20219;&#21153;&#25110;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#20351;&#29992;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#26469;&#21046;&#23450;&#26410;&#30693;&#36830;&#32493;&#29366;&#24577;/&#21160;&#20316;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#30446;&#26631;&#12290;&#32473;&#23450;&#30340;LTL&#23646;&#24615;&#34987;&#36716;&#21270;&#20026;&#26497;&#38480;&#30830;&#23450;&#21270;&#24191;&#20041;&#24067;&#27663;&#33258;&#21160;&#26426;&#65288;LDGBA&#65289;&#65292;&#36890;&#36807;LDGBA&#22312;&#34892;&#36827;&#36807;&#31243;&#20013;&#19981;&#26029;&#35843;&#25972;&#21516;&#27493;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#35813;&#31639;&#27861;&#23558;&#20445;&#35777;&#21512;&#25104;&#20986;&#19968;&#20010;&#25511;&#21046;&#31574;&#30053;&#65292;&#20854;&#36712;&#36857;&#26368;&#22823;&#27010;&#29575;&#28385;&#36275;LTL&#35268;&#23450;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) is a widely employed machine learning architecture that has been applied to a variety of control problems. However, applications in safety-critical domains require a systematic and formal approach to specifying requirements as tasks or goals. We propose a model-free RL algorithm that enables the use of Linear Temporal Logic (LTL) to formulate a goal for unknown continuous-state/action Markov Decision Processes (MDPs). The given LTL property is translated into a Limit-Deterministic Generalised Buchi Automaton (LDGBA), which is then used to shape a synchronous reward function on-the-fly. Under certain assumptions, the algorithm is guaranteed to synthesise a control policy whose traces satisfy the LTL specification with maximal probability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26679;&#26412;&#25286;&#20998;&#30340;&#20803;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#35780;&#20272;&#24635;&#20307;&#39118;&#38505;&#26102;&#32771;&#34385;&#24178;&#25200;&#21442;&#25968;&#65292;&#24182;&#19988;&#23454;&#29616;&#30340;&#36229;&#39069;&#39118;&#38505;&#30028;&#30340;&#24433;&#21709;&#20026;&#20108;&#27425;&#12290;</title><link>http://arxiv.org/abs/1901.09036</link><description>&lt;p&gt;
&#27491;&#20132;&#32479;&#35745;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Orthogonal Statistical Learning. (arXiv:1901.09036v4 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.09036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26679;&#26412;&#25286;&#20998;&#30340;&#20803;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#35780;&#20272;&#24635;&#20307;&#39118;&#38505;&#26102;&#32771;&#34385;&#24178;&#25200;&#21442;&#25968;&#65292;&#24182;&#19988;&#23454;&#29616;&#30340;&#36229;&#39069;&#39118;&#38505;&#30028;&#30340;&#24433;&#21709;&#20026;&#20108;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#32479;&#35745;&#23398;&#20064;&#30340;&#35774;&#32622;&#19979;&#25552;&#20379;&#20102;&#20851;&#20110;&#38750;&#28176;&#36817;&#36229;&#39069;&#39118;&#38505;&#20445;&#35777;&#65292;&#20854;&#20013;&#30446;&#26631;&#21442;&#25968;&#25152;&#35780;&#20272;&#30340;&#24635;&#20307;&#39118;&#38505;&#21462;&#20915;&#20110;&#24517;&#39035;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#30340;&#26410;&#30693;&#24178;&#25200;&#21442;&#25968;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26679;&#26412;&#25286;&#20998;&#30340;&#20803;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#20219;&#24847;&#20272;&#35745;&#30446;&#26631;&#21442;&#25968;&#21644;&#24178;&#25200;&#21442;&#25968;&#30340;&#31639;&#27861;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#24635;&#20307;&#39118;&#38505;&#28385;&#36275;&#19968;&#20010;&#31216;&#20026;Neyman&#27491;&#20132;&#24615;&#30340;&#26465;&#20214;&#65292;&#21017;&#24178;&#25200;&#20272;&#35745;&#35823;&#24046;&#23545;&#20803;&#31639;&#27861;&#23454;&#29616;&#30340;&#36229;&#39069;&#39118;&#38505;&#30028;&#30340;&#24433;&#21709;&#20026;&#20108;&#27425;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#19981;&#20851;&#24515;&#29992;&#20110;&#30446;&#26631;&#21644;&#24178;&#25200;&#30340;&#29305;&#23450;&#31639;&#27861;&#65292;&#21482;&#20570;&#20986;&#20102;&#26377;&#20851;&#23427;&#20204;&#21508;&#33258;&#24615;&#33021;&#30340;&#20551;&#35774;&#12290;&#36825;&#26679;&#65292;&#23601;&#21487;&#20197;&#21033;&#29992;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#30340;&#22823;&#37327;&#32467;&#26524;&#65292;&#20026;&#24102;&#26377;&#24178;&#25200;&#32452;&#25104;&#30340;&#23398;&#20064;&#25552;&#20379;&#26032;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20851;&#27880;&#36229;&#39069;&#39118;&#38505;&#32780;&#19981;&#26159;&#21442;&#25968;&#20272;&#35745;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#24369;&#21270;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide non-asymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input arbitrary estimation algorithms for the target parameter and nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can provide rates under weaker a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;GELU&#12290;&#36890;&#36807;&#23545;&#36755;&#20837;&#20540;&#36827;&#34892;&#21152;&#26435;&#32780;&#38750;&#31526;&#21495;&#38376;&#38480;&#25511;&#21046;&#65292;GELU&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/1606.08415</link><description>&lt;p&gt;
&#39640;&#26031;&#35823;&#24046;&#32447;&#24615;&#21333;&#20803;&#65288;Gaussian Error Linear Units&#65289;
&lt;/p&gt;
&lt;p&gt;
Gaussian Error Linear Units (GELUs). (arXiv:1606.08415v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1606.08415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;GELU&#12290;&#36890;&#36807;&#23545;&#36755;&#20837;&#20540;&#36827;&#34892;&#21152;&#26435;&#32780;&#38750;&#31526;&#21495;&#38376;&#38480;&#25511;&#21046;&#65292;GELU&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#39640;&#26031;&#35823;&#24046;&#32447;&#24615;&#21333;&#20803;&#65288;GELU&#65289;&#12290;GELU&#28608;&#27963;&#20989;&#25968;&#20026;$x\Phi(x)$&#65292;&#20854;&#20013;$\Phi(x)$&#20195;&#34920;&#26631;&#20934;&#39640;&#26031;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#12290;&#19982;ReLU&#65288;$x\mathbf{1}_{x&gt;0}$&#65289;&#28608;&#27963;&#20989;&#25968;&#36890;&#36807;&#36755;&#20837;&#30340;&#31526;&#21495;&#36827;&#34892;&#38376;&#38480;&#25511;&#21046;&#19981;&#21516;&#65292;GELU&#38750;&#32447;&#24615;&#25353;&#36755;&#20837;&#20540;&#21152;&#26435;&#12290;&#25105;&#20204;&#23545;GELU&#38750;&#32447;&#24615;&#12289;ReLU&#20197;&#21450;ELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#22312;&#25152;&#26377;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#20219;&#21153;&#20013;&#65292;GELU&#22343;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\Phi(x)$, where $\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\mathbf{1}_{x&gt;0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.
&lt;/p&gt;</description></item></channel></rss>