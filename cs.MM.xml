<rss version="2.0"><channel><title>Chat Arxiv cs.MM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MM</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.03857</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#65306;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study. (arXiv:2303.03857v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the advantages of using pre-trained AudioLDM as the backbone for sound generation, demonstrates the benefits of using pre-trained models for text-to-sound generation in data-scarcity scenarios, and evaluates various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols to provide a basis for future research.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#36817;&#22312;&#25991;&#26412;&#25552;&#31034;&#19979;&#23454;&#29616;&#20102;&#22768;&#38899;&#29983;&#25104;&#30340;&#31361;&#30772;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#34920;&#29616;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#24403;&#21069;&#30340;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#36807;&#24230;&#25311;&#21512;&#65289;&#19978;&#38754;&#20020;&#38382;&#39064;&#65292;&#20174;&#32780;&#26174;&#33879;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65288;&#20363;&#22914;&#35757;&#32451;&#26465;&#20214;&#65289;&#21487;&#33021;&#20250;&#24433;&#21709;AudioLDM&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#36825;&#20123;&#21327;&#35758;&#20801;&#35768;&#22312;&#20849;&#21516;&#22522;&#30784;&#19978;&#20844;&#24179;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have recently achieved breakthroughs in sound generation with text prompts. Despite their promising performance, current text-to-sound generation models face issues on small-scale datasets (e.g., overfitting), significantly limiting their performance. In this paper, we investigate the use of pre-trained AudioLDM, the state-of-the-art model for text-to-audio generation, as the backbone for sound generation. Our study demonstrates the advantages of using pre-trained models for text-to-sound generation, especially in data-scarcity scenarios. In addition, experiments show that different training strategies (e.g., training conditions) may affect the performance of AudioLDM on datasets of different scales. To facilitate future studies, we also evaluate various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols, which allow fair comparisons and benchmarking of these methods on the common ground.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#65292;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.02665</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#23398;&#20064;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Learning for Acoustic Event Classification. (arXiv:2303.02665v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#65292;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new model, Heterogeneous Graph Crossmodal Network (HGCN), which learns crossmodal edges and can adapt to various spatial and temporal scales, effectively connecting relevant nodes across modalities. It achieves state-of-the-art performance in acoustic event classification.
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#32039;&#20945;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#26469;&#24314;&#27169;&#28041;&#21450;&#22810;&#20010;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#24314;&#27169;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#25104;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22270;&#32467;&#26500;&#22312;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#20013;&#24182;&#19981;&#33258;&#28982;&#12290;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#30340;&#22270;&#26159;&#25163;&#21160;&#26500;&#24314;&#30340;&#65292;&#36825;&#26082;&#22256;&#38590;&#21448;&#27425;&#20248;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#65288;i&#65289;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#21270;&#22270;&#26500;&#24314;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#22240;&#20026;&#23427;&#26159;&#21442;&#25968;&#21270;&#26500;&#24314;&#30340;&#65292;&#32780;&#21487;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#36793;&#32536;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#12290;&#22312;&#19968;&#20010;&#22823;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;AudioSet&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#65288;0.53&#24179;&#22343;&#31934;&#24230;&#65289;&#65292;&#20248;&#20110;transfo&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graphs provide a compact, efficient, and scalable way to model data involving multiple disparate modalities. This makes modeling audiovisual data using heterogeneous graphs an attractive option. However, graph structure does not appear naturally in audiovisual data. Graphs for audiovisual data are constructed manually which is both difficult and sub-optimal. In this work, we address this problem by (i) proposing a parametric graph construction strategy for the intra-modal edges, and (ii) learning the crossmodal edges. To this end, we develop a new model, heterogeneous graph crossmodal network (HGCN) that learns the crossmodal edges. Our proposed model can adapt to various spatial and temporal scales owing to its parametric construction, while the learnable crossmodal edges effectively connect the relevant nodes across modalities. Experiments on a large benchmark dataset (AudioSet) show that our model is state-of-the-art (0.53 mean average precision), outperforming transfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20843;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#37197;&#22791;&#22270;&#20687;&#30340;&#23454;&#20307;&#23545;&#40784;&#22522;&#20934;Multi-OpenEA&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;EA&#26041;&#27861;LODEME&#65292;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;KG&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.08774</link><description>&lt;p&gt;
&#35270;&#35273;&#12289;&#25512;&#29702;&#21644;&#23545;&#40784;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Vision, Deduction and Alignment: An Empirical Study on Multi-modal Knowledge Graph Alignment. (arXiv:2302.08774v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20843;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#37197;&#22791;&#22270;&#20687;&#30340;&#23454;&#20307;&#23545;&#40784;&#22522;&#20934;Multi-OpenEA&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;EA&#26041;&#27861;LODEME&#65292;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;KG&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study constructed eight large-scale, image-equipped entity alignment benchmarks named Multi-OpenEA, and developed a new multi-modal EA method named LODEME, which utilizes logical deduction and multi-modal KG embedding, achieving state-of-the-art performance on Multi-OpenEA and other existing multi-modal EA benchmarks.
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#23545;&#40784;&#22312;&#30693;&#35782;&#24037;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#22270;&#24418;&#32467;&#26500;&#21644;&#23454;&#20307;&#23646;&#24615;&#65288;&#21253;&#25324;&#25991;&#23383;&#65289;&#65292;&#20294;&#24573;&#30053;&#20102;&#29616;&#20195;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#24120;&#35265;&#30340;&#22270;&#20687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;Multi-OpenEA&#8212;&#8212;&#20843;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#37197;&#22791;&#22270;&#20687;&#30340;&#23454;&#20307;&#23545;&#40784;&#22522;&#20934;&#65292;&#24182;&#35780;&#20272;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#22270;&#20687;&#12290;&#37492;&#20110;&#35270;&#35273;&#27169;&#24577;&#20449;&#24687;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#20114;&#34917;&#24615;&#36136;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;EA&#26041;&#27861;&#65292;&#21517;&#20026;LODEME&#65292;&#20351;&#29992;&#36923;&#36753;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;KG&#23884;&#20837;&#65292;&#22312;Multi-OpenEA&#21644;&#20854;&#20182;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;EA&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in knowledge engineering. Existing EA methods mostly focus on utilizing the graph structures and entity attributes (including literals), but ignore images that are common in modern multi-modal KGs. In this study we first constructed Multi-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then evaluated some existing embedding-based methods for utilizing images. In view of the complementary nature of visual modal information and logical deduction, we further developed a new multi-modal EA method named LODEME using logical deduction and multi-modal KG embedding, with state-of-the-art performance achieved on Multi-OpenEA and other existing multi-modal EA benchmarks.
&lt;/p&gt;</description></item><item><title>LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2301.09799</link><description>&lt;p&gt;
LDMIC&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LDMIC: Learning-based Distributed Multi-view Image Coding. (arXiv:2301.09799v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09799
&lt;/p&gt;
&lt;p&gt;
LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
LDMIC is a learning-based distributed multi-view image coding framework that captures global inter-view correlations through independent encoders and a joint context transfer module based on the cross-attention mechanism, which is insensitive to geometric relations.
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#22270;&#20687;&#21387;&#32553;&#22312;3D&#30456;&#20851;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#39044;&#27979;&#32534;&#30721;&#26550;&#26500;&#65292;&#38656;&#35201;&#32852;&#21512;&#32534;&#30721;&#21387;&#32553;&#30456;&#24212;&#30340;&#35270;&#24046;&#21644;&#27531;&#24046;&#20449;&#24687;&#12290;&#36825;&#35201;&#27714;&#30456;&#26426;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#65292;&#24182;&#24378;&#21046;&#25191;&#34892;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#26497;&#32447;&#20960;&#20309;&#32422;&#26463;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#38543;&#26426;&#37325;&#21472;&#35270;&#37326;&#30340;&#20998;&#24067;&#24335;&#30456;&#26426;&#31995;&#32479;&#20013;&#37096;&#32626;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#29702;&#35770;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#21644;&#32852;&#21512;&#35299;&#30721;&#23454;&#29616;&#30456;&#20851;&#28304;&#30340;&#39640;&#25928;&#25968;&#25454;&#21387;&#32553;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#35774;&#35745;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#65288;LDMIC&#65289;&#26694;&#26550;&#30340;&#21160;&#26426;&#12290;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#65292;LDMIC&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;TSGV&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#65292;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#26159;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#12290;</title><link>http://arxiv.org/abs/2201.08071</link><description>&lt;p&gt;
&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65306;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;TSGV&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#65292;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#26159;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey summarizes the fundamental concepts and current research status of temporal sentence grounding in videos (TSGV), also known as natural language video localization (NLVL) or video moment retrieval (VMR), as well as future research directions. TSGV aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video, connecting computer vision and natural language, and has drawn significant attention from researchers in both communities.
&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#65292;&#21448;&#31216;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#65288;NLVL&#65289;&#25110;&#35270;&#39057;&#26102;&#21051;&#26816;&#32034;&#65288;VMR&#65289;&#65292;&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#12290;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;TSGV&#24341;&#36215;&#20102;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#35270;&#12290;&#26412;&#32508;&#36848;&#35797;&#22270;&#25552;&#20379;TSGV&#20013;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#30340;&#24635;&#32467;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20316;&#20026;&#32972;&#26223;&#65292;&#25105;&#20204;&#20197;&#25945;&#31243;&#30340;&#24418;&#24335;&#20171;&#32461;&#20102;TSGV&#20013;&#21151;&#33021;&#32452;&#20214;&#30340;&#24120;&#35265;&#32467;&#26500;&#65306;&#20174;&#21407;&#22987;&#35270;&#39057;&#21644;&#35821;&#35328;&#26597;&#35810;&#30340;&#29305;&#24449;&#25552;&#21462;&#21040;&#30446;&#26631;&#26102;&#21051;&#30340;&#31572;&#26696;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#25216;&#26415;&#65292;&#36825;&#26159;TSGV&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#65292;&#20197;&#23454;&#29616;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#26377;&#25928;&#23545;&#40784;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;TSGV&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#21450;&#20854;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal sentence grounding in videos (TSGV), \aka natural language video localization (NLVL) or video moment retrieval (VMR), aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video. Connecting computer vision and natural language, TSGV has drawn significant attention from researchers in both communities. This survey attempts to provide a summary of fundamental concepts in TSGV and current research status, as well as future research directions. As the background, we present a common structure of functional components in TSGV, in a tutorial style: from feature extraction from raw video and language query, to answer prediction of the target moment. Then we review the techniques for multimodal understanding and interaction, which is the key focus of TSGV for effective alignment between the two modalities. We construct a taxonomy of TSGV techniques and elaborate the methods in different categories with their strengths and weaknesses. La
&lt;/p&gt;</description></item></channel></rss>