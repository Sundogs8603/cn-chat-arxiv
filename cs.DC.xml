<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.06360</link><description>&lt;p&gt;
FedLP: &#19968;&#31181;&#29992;&#20110;&#36890;&#20449;&#35745;&#31639;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#23618;&#27425;&#21098;&#26525;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning. (arXiv:2303.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an explicit FL pruning framework, FedLP, which adopts layer-wise pruning in local training and federated updating, and is model-agnostic and universal for different types of deep learning models. FedLP can relieve the system bottlenecks of communication and computation with marginal performance decay.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#39640;&#25928;&#19988;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65288;Federated Layer-wise Pruning&#65289;&#65292;&#35813;&#26694;&#26550;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#20026;&#20855;&#26377;&#21516;&#36136;&#26412;&#22320;&#27169;&#22411;&#21644;&#24322;&#36136;&#26412;&#22320;&#27169;&#22411;&#30340;&#22330;&#26223;&#35774;&#35745;&#20102;&#20004;&#31181;&#29305;&#23450;&#30340;FedLP&#26041;&#26696;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FedLP&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FedLP&#26159;&#31532;&#19968;&#20010;&#27491;&#24335;&#23558;&#23618;&#27425;&#21098;&#26525;&#24341;&#20837;FL&#30340;&#26694;&#26550;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#22260;&#20869;&#65292;&#21487;&#20197;&#22522;&#20110;FedLP&#36827;&#19968;&#27493;&#35774;&#35745;&#26356;&#22810;&#30340;&#21464;&#20307;&#21644;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has prevailed as an efficient and privacy-preserved scheme for distributed learning. In this work, we mainly focus on the optimization of computation and communication in FL from a view of pruning. By adopting layer-wise pruning in local training and federated updating, we formulate an explicit FL pruning framework, FedLP (Federated Layer-wise Pruning), which is model-agnostic and universal for different types of deep learning models. Two specific schemes of FedLP are designed for scenarios with homogeneous local models and heterogeneous ones. Both theoretical and experimental evaluations are developed to verify that FedLP relieves the system bottlenecks of communication and computation with marginal performance decay. To the best of our knowledge, FedLP is the first framework that formally introduces the layer-wise pruning into FL. Within the scope of federated learning, more variants and combinations can be further designed based on FedLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.06318</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#19987;&#23478;&#28151;&#21512;&#24182;&#34892;&#26041;&#27861;&#26469;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixture-of-Experts&#65288;MoE&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#28155;&#21152;&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#22359;&#26469;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#65288;&#22522;&#26412;&#27169;&#22411;&#65289;&#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#25913;&#21464;&#35757;&#32451;&#25110;&#25512;&#29702;&#30340;&#24635;&#28014;&#28857;&#25805;&#20316;&#25968;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#31181;&#26550;&#26500;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#20219;&#24847;&#22823;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19982;&#22522;&#26412;&#27169;&#22411;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;64&#21040;128&#20010;&#19987;&#23478;&#22359;&#20043;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35266;&#23519;&#21040;&#36825;&#20123;MoE&#27169;&#22411;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#36882;&#20943;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;MoE&#27169;&#22411;&#38656;&#35201;&#25105;&#20204;&#25193;&#23637;&#22522;&#26412;&#27169;&#22411;&#30340;&#22823;&#23567;&#20197;&#21450;&#19987;&#23478;&#22359;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#32500;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#27493;&#39588;&#20013;&#25552;&#20986;&#20102;&#20869;&#23384;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#65292;&#20197;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.06314</link><description>&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#36890;&#36807;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#23458;&#25143;&#31471;dropout&#26469;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stabilizing and Improving Federated Learning with Non-IID Data and Client Dropout in IoT Systems. (arXiv:2303.06314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#65292;&#20197;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple yet effective framework to stabilize and improve federated learning by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based feature extraction to maintain a balanced classifier head.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#19981;&#26292;&#38706;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22312;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#19978;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#65292;&#28982;&#32780;&#23427;&#21463;&#21040;&#26631;&#31614;&#20998;&#24067;&#20559;&#26012;&#30340;&#24433;&#21709;&#65292;&#36890;&#24120;&#23548;&#33268;&#25910;&#25947;&#32531;&#24930;&#21644;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#24403;&#21442;&#19982;&#30340;&#23458;&#25143;&#31471;&#22788;&#20110;&#19981;&#31283;&#23450;&#30340;&#29615;&#22659;&#24182;&#32463;&#24120;&#25481;&#32447;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21487;&#33021;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an emerging technique for training deep models over decentralized clients without exposing private data, which however suffers from label distribution skew and usually results in slow convergence and degraded model performance. This challenge could be more serious when the participating clients are in unstable circumstances and dropout frequently. Previous work and our empirical observations demonstrate that the classifier head for classification task is more sensitive to label skew and the unstable performance of FedAvg mainly lies in the imbalanced training samples across different classes. The biased classifier head will also impact the learning of feature representations. Therefore, maintaining a balanced classifier head is of significant importance for building a better global model. To tackle this issue, we propose a simple yet effective framework by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based fe
&lt;/p&gt;</description></item><item><title>Papaya&#26159;&#19968;&#31181;&#28857;&#23545;&#28857;&#23398;&#20064;&#31995;&#32479;&#65292;&#33410;&#28857;&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23450;&#26399;&#26681;&#25454;&#23398;&#20064;&#30340;&#20449;&#20219;&#30697;&#38453;&#23558;&#20854;&#21442;&#25968;&#19982;&#21516;&#20276;&#30340;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#20174;&#32780;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#36991;&#20813;&#20102;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#30340;&#24102;&#23485;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06189</link><description>&lt;p&gt;
Papaya&#65306;&#32852;&#37030;&#23398;&#20064;&#65292;&#20294;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;
&lt;/p&gt;
&lt;p&gt;
Papaya: Federated Learning, but Fully Decentralized. (arXiv:2303.06189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06189
&lt;/p&gt;
&lt;p&gt;
Papaya&#26159;&#19968;&#31181;&#28857;&#23545;&#28857;&#23398;&#20064;&#31995;&#32479;&#65292;&#33410;&#28857;&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23450;&#26399;&#26681;&#25454;&#23398;&#20064;&#30340;&#20449;&#20219;&#30697;&#38453;&#23558;&#20854;&#21442;&#25968;&#19982;&#21516;&#20276;&#30340;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#20174;&#32780;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#36991;&#20813;&#20102;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#30340;&#24102;&#23485;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Papaya is a peer-to-peer learning system that allows nodes to train on their own data and periodically perform a weighted average of their parameters with that of their peers according to a learned trust matrix, achieving decentralized federated learning and avoiding the bandwidth and resource-heavy constraint and privacy concerns of a centralized server.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20351;&#29992;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#26469;&#32858;&#21512;&#27169;&#22411;&#26356;&#26032;&#12290;&#36825;&#26159;&#19968;&#31181;&#24102;&#23485;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#38480;&#21046;&#65292;&#24182;&#26292;&#38706;&#31995;&#32479;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#28857;&#23545;&#28857;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#33410;&#28857;&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23450;&#26399;&#26681;&#25454;&#23398;&#20064;&#30340;&#20449;&#20219;&#30697;&#38453;&#23558;&#20854;&#21442;&#25968;&#19982;&#21516;&#20276;&#30340;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#25105;&#20204;&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#23458;&#25143;&#31471;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#34394;&#25311;&#33410;&#28857;&#22312;&#21516;&#19968;&#21488;&#35745;&#31639;&#26426;&#19978;&#36816;&#34892;&#23454;&#39564;&#26469;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25105;&#20204;&#25552;&#26696;&#30340;&#31532;&#19968;&#36718;&#20013;&#25152;&#36848;&#30340;&#31574;&#30053;&#26469;&#35777;&#26126;&#20849;&#20139;&#21442;&#25968;&#30340;&#28857;&#23545;&#28857;&#23398;&#20064;&#27010;&#24565;&#12290;&#25105;&#20204;&#29616;&#22312;&#24076;&#26395;&#36816;&#34892;&#26356;&#22810;&#23454;&#39564;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#26356;&#21487;&#37096;&#32626;&#30340;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning systems use a centralized server to aggregate model updates. This is a bandwidth and resource-heavy constraint and exposes the system to privacy concerns. We instead implement a peer to peer learning system in which nodes train on their own data and periodically perform a weighted average of their parameters with that of their peers according to a learned trust matrix. So far, we have created a model client framework and have been using this to run experiments on the proposed system using multiple virtual nodes which in reality exist on the same computer. We used this strategy as stated in Iteration 1 of our proposal to prove the concept of peer to peer learning with shared parameters. We now hope to run more experiments and build a more deployable real world system for the same.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06182</link><description>&lt;p&gt;
&#36808;&#21521;MoE&#37096;&#32626;&#65306;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#25512;&#29702;&#20013;&#30340;&#20302;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference. (arXiv:2303.06182v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes three optimization techniques to mitigate inefficiencies in Mixture-of-Experts (MoE) models during inference, including dynamic gating, expert buffering, and expert load balancing. These techniques can significantly improve execution time and reduce memory usage.
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#26368;&#36817;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24191;&#27867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#26377;&#25928;&#22320;&#25193;&#23637;&#20102;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#23567;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#37096;&#32626;&#36825;&#26679;&#30340;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;MoE&#24037;&#20316;&#36127;&#36733;&#30340;&#29305;&#24449;&#21270;&#65292;&#21363;&#35821;&#35328;&#24314;&#27169;&#65288;LM&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#30340;&#20302;&#25928;&#29575;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#20302;&#25928;&#29575;&#30340;&#26469;&#28304;&#65292;&#21363;&#65288;1&#65289;&#21160;&#24577;&#38376;&#25511;&#65292;&#65288;2&#65289;&#19987;&#23478;&#32531;&#20914;&#21644;&#65288;3&#65289;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21160;&#24577;&#38376;&#25511;&#21487;&#20197;&#20351;LM&#30340;&#25191;&#34892;&#26102;&#38388;&#25552;&#39640;1.25-4&#20493;&#65292;MT&#32534;&#30721;&#22120;&#25552;&#39640;2-5&#20493;&#65292;MT&#35299;&#30721;&#22120;&#25552;&#39640;1.09-1.5&#20493;&#12290;&#23427;&#36824;&#21487;&#20197;&#23558;LM&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.36&#20493;&#65292;MT&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Experts (MoE) models have recently gained steam in achieving the state-of-the-art performance in a wide range of tasks in computer vision and natural language processing. They effectively expand the model capacity while incurring a minimal increase in computation cost during training. However, deploying such models for inference is difficult due to their large model size and complex communication pattern. In this work, we provide a characterization of two MoE workloads, namely Language Modeling (LM) and Machine Translation (MT) and identify their sources of inefficiencies at deployment.  We propose three optimization techniques to mitigate sources of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show that dynamic gating improves execution time by 1.25-4$\times$ for LM, 2-5$\times$ for MT Encoder and 1.09-1.5$\times$ for MT Decoder. It also reduces memory usage by up to 1.36$\times$ for LM and up to 1.1$\times$ for MT. We f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#33258;&#24049;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;Q-learning&#31639;&#27861;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.06155</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Digital Twin-Assisted Knowledge Distillation Framework for Heterogeneous Federated Learning. (arXiv:2303.06155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#33258;&#24049;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;Q-learning&#31639;&#27861;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a digital twin-assisted knowledge distillation framework for heterogeneous federated learning, where users can select their own neural network models and distill knowledge from a big teacher model, and the teacher model can be trained on a digital twin located in the server. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming problem and solved using Q-learning and optimization algorithms.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#39537;&#21160;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#65292;&#20854;&#20013;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36873;&#25321;&#20854;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#33258;&#24049;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#12290;&#20026;&#20102;&#20811;&#26381;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29992;&#25143;&#35774;&#22791;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#30340;&#26041;&#24335;&#65292;&#25945;&#24072;&#27169;&#22411;&#21487;&#20197;&#22312;&#20855;&#26377;&#36275;&#22815;&#35745;&#31639;&#36164;&#28304;&#30340;&#26381;&#21153;&#22120;&#19978;&#30340;&#25968;&#23383;&#23402;&#29983;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22312;&#27169;&#22411;&#33976;&#39311;&#26399;&#38388;&#65292;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#22312;&#29289;&#29702;&#23454;&#20307;&#25110;&#25968;&#23383;&#20195;&#29702;&#22788;&#26356;&#26032;&#20854;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#20026;&#29992;&#25143;&#36873;&#25321;&#27169;&#22411;&#21644;&#35757;&#32451;&#21368;&#36733;&#21644;&#36164;&#28304;&#20998;&#37197;&#21046;&#23450;&#20102;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32852;&#21512;&#20351;&#29992;Q-learning&#21644;&#20248;&#21270;&#65292;&#20854;&#20013;Q-learning&#20026;&#29992;&#25143;&#36873;&#25321;&#27169;&#22411;&#24182;&#30830;&#23450;&#26159;&#22312;&#26412;&#22320;&#36824;&#26159;&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#20248;&#21270;&#21017;&#29992;&#20110;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, to deal with the heterogeneity in federated learning (FL) systems, a knowledge distillation (KD) driven training framework for FL is proposed, where each user can select its neural network model on demand and distill knowledge from a big teacher model using its own private dataset. To overcome the challenge of train the big teacher model in resource limited user devices, the digital twin (DT) is exploit in the way that the teacher model can be trained at DT located in the server with enough computing resources. Then, during model distillation, each user can update the parameters of its model at either the physical entity or the digital agent. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming (MIP) problem. To solve the problem, Q-learning and optimization are jointly used, where Q-learning selects models for users and determines whether to train locally or on the server, and optimiz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;</title><link>http://arxiv.org/abs/2302.05045</link><description>&lt;p&gt;
&#21033;&#29992;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31232;&#30095;&#24615;&#26469;&#20248;&#21270;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training. (arXiv:2302.05045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel approach that exploits sparse subnetworks to optimize memory utilization and communication in two popular algorithms for parallel deep learning, and demonstrates significant reductions in memory consumption and communication time on a 2.7 billion parameter model using 512 NVIDIA V100 GPUs.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36890;&#20449;&#24320;&#38144;&#30340;&#26174;&#33879;&#22686;&#21152;&#65292;&#35268;&#27169;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#34892;&#35757;&#32451;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#21098;&#26525;&#31639;&#27861;&#65292;&#33021;&#22815;&#21098;&#26525;&#65288;&#21363;&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#35774;&#32622;&#20026;&#38646;&#65289;80-90&#65285;&#30340;&#21442;&#25968;&#65292;&#20197;&#20135;&#29983;&#19982;&#26410;&#21098;&#26525;&#29238;&#32593;&#32476;&#30456;&#31561;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;AxoNN&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20381;&#36182;&#20110;&#25968;&#25454;&#21644;&#23618;&#38388;&#24182;&#34892;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#20449;&#26102;&#38388;&#21644;&#20869;&#23384;&#21033;&#29992;&#30340;&#20943;&#23569;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Parallel training of neural networks at scale is challenging due to significant overheads arising from communication. Recently, deep learning researchers have developed a variety of pruning algorithms that are capable of pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to yield sparse subnetworks that equal the accuracy of the unpruned parent network. In this work, we propose a novel approach that exploits these sparse subnetworks to optimize the memory utilization and communication in two popular algorithms for parallel deep learning namely -- data and inter-layer parallelism. We integrate our approach into AxoNN, a highly scalable framework for parallel deep learning that relies on data and inter-layer parallelism, and demonstrate the reduction in communication time and memory utilization. On 512 NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7 billion parameter model by 74%, and the total communication time by 40%, thus providing 
&lt;/p&gt;</description></item><item><title>Egeria&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#36339;&#36807;DNN&#23618;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26469;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#65292;&#21033;&#29992;&#21442;&#32771;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#20934;&#30830;&#35780;&#20272;&#21333;&#20010;&#23618;&#30340;&#35757;&#32451;&#21487;&#22609;&#24615;&#65292;&#24182;&#23433;&#20840;&#22320;&#20923;&#32467;&#24050;&#25910;&#25947;&#30340;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#30456;&#24212;&#30340;&#21453;&#21521;&#35745;&#31639;&#21644;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2201.06227</link><description>&lt;p&gt;
Egeria: &#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#23618;&#20923;&#32467;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;DNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Egeria: Efficient DNN Training with Knowledge-Guided Layer Freezing. (arXiv:2201.06227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06227
&lt;/p&gt;
&lt;p&gt;
Egeria&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#36339;&#36807;DNN&#23618;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26469;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#65292;&#21033;&#29992;&#21442;&#32771;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#20934;&#30830;&#35780;&#20272;&#21333;&#20010;&#23618;&#30340;&#35757;&#32451;&#21487;&#22609;&#24615;&#65292;&#24182;&#23433;&#20840;&#22320;&#20923;&#32467;&#24050;&#25910;&#25947;&#30340;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#30456;&#24212;&#30340;&#21453;&#21521;&#35745;&#31639;&#21644;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Egeria is a knowledge-guided DNN training system that skips computing and communication through DNN layer freezing, accurately evaluates individual layers' training plasticity using semantic knowledge from a reference model, and safely freezes the converged ones, saving their corresponding backward computation and communication.
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;Egeria&#65292;&#36890;&#36807;&#36339;&#36807;DNN&#23618;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26469;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#20869;&#37096;DNN&#23618;&#30340;&#35757;&#32451;&#36827;&#24230;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#21069;&#23618;&#36890;&#24120;&#27604;&#28145;&#23618;&#26356;&#26089;&#22320;&#24471;&#21040;&#24456;&#22909;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#35757;&#32451;&#21487;&#22609;&#24615;&#30340;&#27010;&#24565;&#65292;&#20197;&#37327;&#21270;&#20869;&#37096;DNN&#23618;&#30340;&#35757;&#32451;&#36827;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Egeria&#65292;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;&#65292;&#21033;&#29992;&#21442;&#32771;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#20934;&#30830;&#35780;&#20272;&#21333;&#20010;&#23618;&#30340;&#35757;&#32451;&#21487;&#22609;&#24615;&#65292;&#24182;&#23433;&#20840;&#22320;&#20923;&#32467;&#24050;&#25910;&#25947;&#30340;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#30456;&#24212;&#30340;&#21453;&#21521;&#35745;&#31639;&#21644;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) is time-consuming. While most existing solutions try to overlap/schedule computation and communication for efficient training, this paper goes one step further by skipping computing and communication through DNN layer freezing. Our key insight is that the training progress of internal DNN layers differs significantly, and front layers often become well-trained much earlier than deep layers. To explore this, we first introduce the notion of training plasticity to quantify the training progress of internal DNN layers. Then we design Egeria, a knowledge-guided DNN training system that employs semantic knowledge from a reference model to accurately evaluate individual layers' training plasticity and safely freeze the converged ones, saving their corresponding backward computation and communication. Our reference model is generated on the fly using quantization techniques and runs forward operations asynchronously on available CPUs to minimize the overhe
&lt;/p&gt;</description></item></channel></rss>