<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.06484</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#22635;&#34917;&#31070;&#32463;&#22349;&#22604;&#30340;&#27867;&#21270;&#21644;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap. (arXiv:2303.06484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a generalized neural collapse hypothesis that effectively subsumes the original neural collapse and decomposes it into two objectives: minimizing intra-class variability and maximizing inter-class separability. The authors use hyperspherical uniformity as a unified framework to quantify these objectives and propose a general objective, hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical uniformity.
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22349;&#22604;&#29616;&#35937;&#25551;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24213;&#23618;&#20960;&#20309;&#23545;&#31216;&#24615;&#65292;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#37117;&#25910;&#25947;&#20110;&#19968;&#20010;&#31561;&#35282;&#32039;&#26694;&#26550;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22343;&#26041;&#35823;&#24046;&#37117;&#21487;&#20197;&#23548;&#33268;&#31070;&#32463;&#22349;&#22604;&#12290;&#25105;&#20204;&#28040;&#38500;&#20102;&#31070;&#32463;&#22349;&#22604;&#23545;&#29305;&#24449;&#32500;&#24230;&#21644;&#31867;&#21035;&#25968;&#37327;&#30340;&#20851;&#38190;&#20551;&#35774;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#12290;&#21463;&#31070;&#32463;&#22349;&#22604;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30446;&#26631;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#65288;&#23427;&#25551;&#36848;&#20102;&#21333;&#20301;&#36229;&#29699;&#19978;&#22343;&#21248;&#24615;&#30340;&#31243;&#24230;&#65289;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural collapse (NC) phenomenon describes an underlying geometric symmetry for deep neural networks, where both deeply learned features and classifiers converge to a simplex equiangular tight frame. It has been shown that both cross-entropy loss and mean square error can provably lead to NC. We remove NC's key assumption on the feature dimension and the number of classes, and then present a generalized neural collapse (GNC) hypothesis that effectively subsumes the original NC. Inspired by how NC characterizes the training target of neural networks, we decouple GNC into two objectives: minimal intra-class variability and maximal inter-class separability. We then use hyperspherical uniformity (which characterizes the degree of uniformity on the unit hypersphere) as a unified framework to quantify these two objectives. Finally, we propose a general objective -- hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20808;&#21069;&#36816;&#34892;&#20013;&#30340;&#35745;&#31639;&#26469;&#20943;&#23569;&#26410;&#26469;&#36816;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#65292;&#36890;&#36807;&#23558;&#26410;&#26469;&#36816;&#34892;&#19982;&#26469;&#33258;&#20808;&#21069;&#36816;&#34892;&#30340;KD&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;KD&#30340;&#24320;&#38144;&#38477;&#20302;&#20102;80-90&#65285;&#65292;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#65292;&#24182;&#22312;&#25972;&#20307;&#25104;&#26412;&#26041;&#38754;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.06480</link><description>&lt;p&gt;
&#39640;&#25928;&#35757;&#32451;&#24207;&#21015;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation for Efficient Sequences of Training Runs. (arXiv:2303.06480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20808;&#21069;&#36816;&#34892;&#20013;&#30340;&#35745;&#31639;&#26469;&#20943;&#23569;&#26410;&#26469;&#36816;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#65292;&#36890;&#36807;&#23558;&#26410;&#26469;&#36816;&#34892;&#19982;&#26469;&#33258;&#20808;&#21069;&#36816;&#34892;&#30340;KD&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;KD&#30340;&#24320;&#38144;&#38477;&#20302;&#20102;80-90&#65285;&#65292;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#65292;&#24182;&#22312;&#25972;&#20307;&#25104;&#26412;&#26041;&#38754;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies how to reduce the cost of future runs by utilizing the computation invested in previous runs using knowledge distillation (KD). Augmenting future runs with KD from previous runs dramatically reduces the time necessary to train these models, and the overhead of KD can be reduced by 80-90% with minimal effect on accuracy, resulting in vast pareto-improvements in overall cost.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#22914;&#36229;&#21442;&#25968;&#25628;&#32034;&#25110;&#20351;&#29992;&#26032;&#25968;&#25454;&#36827;&#34892;&#25345;&#32493;&#37325;&#26032;&#35757;&#32451;&#65292;&#30456;&#20851;&#30340;&#35757;&#32451;&#36816;&#34892;&#20250;&#25353;&#39034;&#24207;&#25191;&#34892;&#22810;&#27425;&#12290;&#30446;&#21069;&#30340;&#20570;&#27861;&#26159;&#20174;&#22836;&#24320;&#22987;&#29420;&#31435;&#35757;&#32451;&#27599;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#20808;&#21069;&#36816;&#34892;&#20013;&#30340;&#35745;&#31639;&#26469;&#20943;&#23569;&#26410;&#26469;&#36816;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23558;&#26410;&#26469;&#36816;&#34892;&#19982;&#26469;&#33258;&#20808;&#21069;&#36816;&#34892;&#30340;KD&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#21363;&#20351;&#32771;&#34385;&#21040;KD&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#25913;&#36827;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#23558;KD&#30340;&#24320;&#38144;&#38477;&#20302;&#20102;80-90&#65285;&#65292;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#65292;&#24182;&#22312;&#25972;&#20307;&#25104;&#26412;&#26041;&#38754;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;KD&#26159;&#20943;&#23569;&#23454;&#36341;&#20013;&#35757;&#32451;&#26368;&#32456;&#27169;&#22411;&#20043;&#21069;&#26114;&#36149;&#30340;&#20934;&#22791;&#24037;&#20316;&#25104;&#26412;&#30340;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many practical scenarios -- like hyperparameter search or continual retraining with new data -- related training runs are performed many times in sequence. Current practice is to train each of these models independently from scratch. We study the problem of exploiting the computation invested in previous runs to reduce the cost of future runs using knowledge distillation (KD). We find that augmenting future runs with KD from previous runs dramatically reduces the time necessary to train these models, even taking into account the overhead of KD. We improve on these results with two strategies that reduce the overhead of KD by 80-90% with minimal effect on accuracy and vast pareto-improvements in overall cost. We conclude that KD is a promising avenue for reducing the cost of the expensive preparatory work that precedes training final models in practice.
&lt;/p&gt;</description></item><item><title>ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.06458</link><description>&lt;p&gt;
ZeroNLG: &#23558;&#39046;&#22495;&#23545;&#40784;&#21644;&#33258;&#32534;&#30721;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06458
&lt;/p&gt;
&lt;p&gt;
ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG is a zero-shot learning framework that can handle multiple NLG tasks, including image-to-text, video-to-text, and text-to-text, across English, Chinese, German, and French. It does not require any labeled downstream pairs for training, and bridges the differences between different domains by projecting them to corresponding coordinates in a shared common latent space.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#25509;&#21463;&#20197;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#24418;&#24335;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20316;&#20026;&#36755;&#20986;&#12290;&#29616;&#26377;&#30340;NLG&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#32806;&#21512;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#23545;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26377;&#38024;&#23545;&#24615;&#30340;&#22330;&#26223;&#21644;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#24448;&#24448;&#27809;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#25918;&#26494;&#23545;&#19979;&#28216;&#20219;&#21153;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#26377;&#25928;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;ZeroNLG&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#65288;&#22270;&#20687;&#23383;&#24149;&#65289;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#65288;&#35270;&#39057;&#23383;&#24149;&#65289;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65288;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65289;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#12290;ZeroNLG&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;ZeroNLG&#65288;i&#65289;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#65288;&#36328;&#27169;&#24577;&#21644;&#35821;&#35328;&#65289;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65307;&#65288;ii&#65289;&#26725;&#25509;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Natural Language Generation (NLG) accepts input data in the form of images, videos, or text and generates corresponding natural language text as output. Existing NLG methods mainly adopt a supervised approach and rely heavily on coupled data-to-text pairs. However, for many targeted scenarios and for non-English languages, sufficient quantities of labeled data are often not available. To relax the dependency on labeled data of downstream tasks, we propose an intuitive and effective zero-shot learning framework, ZeroNLG, which can deal with multiple NLG tasks, including image-to-text (image captioning), video-to-text (video captioning), and text-to-text (neural machine translation), across English, Chinese, German, and French within a unified framework. ZeroNLG does not require any labeled downstream pairs for training. During training, ZeroNLG (i) projects different domains (across modalities and languages) to corresponding coordinates in a shared common latent space; (ii) bridges diff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#29699;&#35270;&#39057;&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#25216;&#26415;DECOMPL&#65292;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#20998;&#25903;&#32452;&#25104;&#65292;&#20351;&#29992;&#36873;&#25321;&#24615;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#25552;&#21462;&#29305;&#24449;&#65292;&#32771;&#34385;&#21442;&#19982;&#32773;&#30340;&#24403;&#21069;&#37197;&#32622;&#65292;&#24182;&#20174;&#26694;&#22352;&#26631;&#20013;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#25490;&#29699;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#26041;&#26696;&#38477;&#20302;&#20102;&#27963;&#21160;&#20013;&#30340;&#22242;&#20307;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2303.06439</link><description>&lt;p&gt;
DECOMPL: &#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#27744;&#21270;&#30340;&#20998;&#35299;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#21333;&#20010;&#25490;&#29699;&#22270;&#20687;&#20013;&#35782;&#21035;&#22242;&#20307;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
DECOMPL: Decompositional Learning with Attention Pooling for Group Activity Recognition from a Single Volleyball Image. (arXiv:2303.06439v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#29699;&#35270;&#39057;&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#25216;&#26415;DECOMPL&#65292;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#20998;&#25903;&#32452;&#25104;&#65292;&#20351;&#29992;&#36873;&#25321;&#24615;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#25552;&#21462;&#29305;&#24449;&#65292;&#32771;&#34385;&#21442;&#19982;&#32773;&#30340;&#24403;&#21069;&#37197;&#32622;&#65292;&#24182;&#20174;&#26694;&#22352;&#26631;&#20013;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#25490;&#29699;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#26041;&#26696;&#38477;&#20302;&#20102;&#27963;&#21160;&#20013;&#30340;&#22242;&#20307;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel GAR technique for volleyball videos, DECOMPL, which consists of two complementary branches, using selective attention pooling to extract features, considering the current configuration of actors and extracting spatial information from box coordinates. The paper also reveals that the labeling scheme of the Volleyball dataset degrades the group concept in activities.
&lt;/p&gt;
&lt;p&gt;
&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#26088;&#22312;&#26816;&#27979;&#22330;&#26223;&#20013;&#22810;&#20010;&#21442;&#19982;&#32773;&#25191;&#34892;&#30340;&#27963;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#22522;&#20110;RGB&#12289;&#20809;&#27969;&#25110;&#20851;&#38190;&#28857;&#25968;&#25454;&#31867;&#22411;&#23545;&#26102;&#31354;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#20351;&#29992;&#26102;&#38388;&#24615;&#21644;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#20165;&#20351;&#29992;RGB&#25968;&#25454;&#32780;&#19981;&#32771;&#34385;&#26102;&#38388;&#24615;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#29699;&#35270;&#39057;&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#25216;&#26415;DECOMPL&#65292;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#20998;&#25903;&#32452;&#25104;&#12290;&#22312;&#35270;&#35273;&#20998;&#25903;&#20013;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#25552;&#21462;&#29305;&#24449;&#12290;&#22312;&#22352;&#26631;&#20998;&#25903;&#20013;&#65292;&#23427;&#32771;&#34385;&#21442;&#19982;&#32773;&#30340;&#24403;&#21069;&#37197;&#32622;&#65292;&#24182;&#20174;&#26694;&#22352;&#26631;&#20013;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25490;&#29699;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20854;&#26631;&#31614;&#26041;&#26696;&#38477;&#20302;&#20102;&#27963;&#21160;&#20013;&#30340;&#22242;&#20307;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group Activity Recognition (GAR) aims to detect the activity performed by multiple actors in a scene. Prior works model the spatio-temporal features based on the RGB, optical flow or keypoint data types. However, using both the temporality and these data types altogether increase the computational complexity significantly. Our hypothesis is that by only using the RGB data without temporality, the performance can be maintained with a negligible loss in accuracy. To that end, we propose a novel GAR technique for volleyball videos, DECOMPL, which consists of two complementary branches. In the visual branch, it extracts the features using attention pooling in a selective way. In the coordinate branch, it considers the current configuration of the actors and extracts the spatial information from the box coordinates. Moreover, we analyzed the Volleyball dataset that the recent literature is mostly based on, and realized that its labeling scheme degrades the group concept in the activities to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25506;&#31350;&#22330;&#26223;&#24863;&#30693;&#30340;&#31070;&#32463;&#34920;&#24449;&#22312;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#22411;&#22330;&#26223;&#24863;&#30693;&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;DNNs&#21487;&#20197;&#23398;&#20064;&#20174;&#19981;&#21516;&#33258;&#25105;&#20013;&#24515;&#35270;&#35282;&#35266;&#23519;&#30340;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.06367</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25506;&#31350;&#22330;&#26223;&#24863;&#30693;&#30340;&#31070;&#32463;&#34920;&#24449;&#22312;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Probing neural representations of scene perception in a hippocampally dependent task using artificial neural networks. (arXiv:2303.06367v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25506;&#31350;&#22330;&#26223;&#24863;&#30693;&#30340;&#31070;&#32463;&#34920;&#24449;&#22312;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#22411;&#22330;&#26223;&#24863;&#30693;&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;DNNs&#21487;&#20197;&#23398;&#20064;&#20174;&#19981;&#21516;&#33258;&#25105;&#20013;&#24515;&#35270;&#35282;&#35266;&#23519;&#30340;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uses artificial neural networks to explore neural representations of scene perception in a hippocampally dependent task, and demonstrates that DNNs can learn the ability to transform scenes viewed from different egocentric perspectives, using a novel scene perception benchmark.
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#21754;&#20083;&#21160;&#29289;&#35270;&#35273;&#31995;&#32479;&#30340;&#26377;&#25928;&#27169;&#22411;&#65292;&#20934;&#30830;&#22320;&#25429;&#25417;&#20102;&#20174;&#21021;&#32423;&#35270;&#35273;&#30382;&#23618;&#21040;&#19979;&#39070;&#30382;&#36136;(IT)&#30340;&#31070;&#32463;&#21709;&#24212;&#23618;&#27425;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#35299;&#37322;&#26356;&#39640;&#30382;&#23618;&#21306;&#22495;&#30340;&#34920;&#24449;&#33021;&#21147;&#30456;&#23545;&#36739;&#24369;&#65292;&#30740;&#31350;&#20063;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#21463;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#21551;&#21457;&#30340;&#26032;&#22411;&#22330;&#26223;&#24863;&#30693;&#22522;&#20934;&#65292;&#26088;&#22312;&#25506;&#31350;DNNs&#36716;&#25442;&#20174;&#19981;&#21516;&#33258;&#25105;&#20013;&#24515;&#35270;&#35282;&#35266;&#23519;&#30340;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;&#20351;&#29992;&#21463;&#39070;&#21494;&#32467;&#26500;&#21644;&#28023;&#39532;&#20043;&#38388;&#36830;&#25509;&#21551;&#21457;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#35757;&#32451;&#30340;DNNs&#21487;&#20197;&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#20998;&#35299;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Deep artificial neural networks (DNNs) trained through backpropagation provide effective models of the mammalian visual system, accurately capturing the hierarchy of neural responses through primary visual cortex to inferior temporal cortex (IT). However, the ability of these networks to explain representations in higher cortical areas is relatively lacking and considerably less well researched. For example, DNNs have been less successful as a model of the egocentric to allocentric transformation embodied by circuits in retrosplenial and posterior parietal cortex. We describe a novel scene perception benchmark inspired by a hippocampal dependent task, designed to probe the ability of DNNs to transform scenes viewed from different egocentric perspectives. Using a network architecture inspired by the connectivity between temporal lobe structures and the hippocampus, we demonstrate that DNNs trained using a triplet loss can learn this task. Moreover, by enforcing a factorized latent space
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;</title><link>http://arxiv.org/abs/2303.06365</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#26816;&#26597;&#23618;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Time Series via Virtual Inspection Layers. (arXiv:2303.06365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a virtual inspection layer that transforms time series into an interpretable representation and allows for relevance attributions to be propagated to this representation via local XAI methods. The applicability of a family of XAI methods is extended to domains where the input is only interpretable after a transformation. The usefulness of DFT-LRP is demonstrated in various time series classification settings, such as audio and electronic health records.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#12290;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#30001;&#20110;&#36755;&#20837;&#36890;&#24120;&#19981;&#21487;&#35299;&#37322;&#65292;&#22240;&#27492;&#21482;&#26377;&#26377;&#38480;&#30340;XAI&#30740;&#31350;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#65288;&#22914;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#65289;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#35821;&#38899;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#36825;&#22312;&#26102;&#38388;&#24207;&#21015;&#35299;&#37322;&#21644;LRP&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;DFT-LRP&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;DFT-LRP&#26469;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of eXplainable Artificial Intelligence (XAI) has greatly advanced in recent years, but progress has mainly been made in computer vision and natural language processing. For time series, where the input is often not interpretable, only limited research on XAI is available. In this work, we put forward a virtual inspection layer, that transforms the time series to an interpretable representation and allows to propagate relevance attributions to this representation via local XAI methods like layer-wise relevance propagation (LRP). In this way, we extend the applicability of a family of XAI methods to domains (e.g. speech) where the input is only interpretable after a transformation. Here, we focus on the Fourier transformation which is prominently applied in the interpretation of time series and LRP and refer to our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time series classification settings like audio and electronic health records. We showcase how 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;MetaViewer&#65292;&#36890;&#36807;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#39044;&#20808;&#25351;&#23450;&#30340;&#34701;&#21512;&#20989;&#25968;&#21644;&#28151;&#21512;&#22312;&#29305;&#24449;&#20013;&#30340;&#35270;&#22270;&#19987;&#29992;&#20887;&#20313;&#20449;&#24687;&#21487;&#33021;&#20250;&#38477;&#20302;&#25152;&#24471;&#34920;&#31034;&#30340;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06329</link><description>&lt;p&gt;
MetaViewer: &#26397;&#30528;&#32479;&#19968;&#30340;&#22810;&#35270;&#22270;&#34920;&#31034;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
MetaViewer: Towards A Unified Multi-View Representation. (arXiv:2303.06329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;MetaViewer&#65292;&#36890;&#36807;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#39044;&#20808;&#25351;&#23450;&#30340;&#34701;&#21512;&#20989;&#25968;&#21644;&#28151;&#21512;&#22312;&#29305;&#24449;&#20013;&#30340;&#35270;&#22270;&#19987;&#29992;&#20887;&#20313;&#20449;&#24687;&#21487;&#33021;&#20250;&#38477;&#20302;&#25152;&#24471;&#34920;&#31034;&#30340;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel bi-level-optimization-based multi-view learning framework, MetaViewer, which learns the representation in a uniform-to-specific manner, avoiding the problem of manually pre-specify fusion functions and view-private redundant information mixed in features that potentially degrade the quality of the derived representation.
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36981;&#24490;&#29305;&#23450;&#21040;&#32479;&#19968;&#30340;&#27969;&#31243;&#65292;&#20174;&#27599;&#20010;&#35270;&#22270;&#20013;&#25552;&#21462;&#28508;&#22312;&#29305;&#24449;&#65292;&#28982;&#21518;&#34701;&#21512;&#25110;&#23545;&#40784;&#23427;&#20204;&#20197;&#33719;&#24471;&#32479;&#19968;&#30340;&#23545;&#35937;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#39044;&#20808;&#25351;&#23450;&#30340;&#34701;&#21512;&#20989;&#25968;&#21644;&#28151;&#21512;&#22312;&#29305;&#24449;&#20013;&#30340;&#35270;&#22270;&#19987;&#29992;&#20887;&#20313;&#20449;&#24687;&#21487;&#33021;&#20250;&#38477;&#20302;&#25152;&#24471;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#34920;&#31034;&#26159;&#20197;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#26041;&#24335;&#23398;&#20064;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20803;&#23398;&#20064;&#22120;&#65292;&#21363;MetaViewer&#65292;&#22312;&#22806;&#23618;&#20248;&#21270;&#20013;&#23398;&#20064;&#34701;&#21512;&#21644;&#24314;&#27169;&#35270;&#22270;&#20849;&#20139;&#30340;&#20803;&#34920;&#31034;&#12290;&#20174;&#36825;&#20010;&#20803;&#34920;&#31034;&#24320;&#22987;&#65292;&#38656;&#35201;&#22312;&#20869;&#23618;&#35757;&#32451;&#35270;&#22270;&#29305;&#23450;&#30340;&#22522;&#23398;&#20064;&#22120;&#65292;&#20197;&#24555;&#36895;&#37325;&#26500;&#30456;&#24212;&#30340;&#35270;&#22270;&#12290;MetaViewer&#26368;&#32456;&#36890;&#36807;&#35266;&#23519;&#25152;&#26377;&#35270;&#22270;&#19978;&#20174;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#37325;&#26500;&#36807;&#31243;&#26469;&#26356;&#26032;&#65292;&#24182;&#23398;&#20064;&#26368;&#20339;&#34701;&#21512;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multi-view representation learning methods typically follow a specific-to-uniform pipeline, extracting latent features from each view and then fusing or aligning them to obtain the unified object representation. However, the manually pre-specify fusion functions and view-private redundant information mixed in features potentially degrade the quality of the derived representation. To overcome them, we propose a novel bi-level-optimization-based multi-view learning framework, where the representation is learned in a uniform-to-specific manner. Specifically, we train a meta-learner, namely MetaViewer, to learn fusion and model the view-shared meta representation in outer-level optimization. Start with this meta representation, view-specific base-learners are then required to rapidly reconstruct the corresponding view in inner-level. MetaViewer eventually updates by observing reconstruction processes from uniform to specific over all views, and learns an optimal fusion scheme that
&lt;/p&gt;</description></item><item><title>MLP-SRGAN&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN&#65292;&#20351;&#29992;MLP-Mixer&#21644;&#21367;&#31215;&#23618;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#21487;&#29992;&#20110;FLAIR MRI&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.06298</link><description>&lt;p&gt;
MLP-SRGAN: &#20351;&#29992;MLP-Mixer&#30340;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer. (arXiv:2303.06298v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06298
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN&#65292;&#20351;&#29992;MLP-Mixer&#21644;&#21367;&#31215;&#23618;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#21487;&#29992;&#20110;FLAIR MRI&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN is a single-dimension Super Resolution GAN that utilizes MLP-Mixers and convolutional layers for upsampling, and can be used for super-resolution reconstruction of FLAIR MRI images. New image quality metrics were proposed.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;MLP-SRGAN&#65292;&#23427;&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SRGAN&#65289;&#65292;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#28151;&#21512;&#22120;&#65288;MLP-Mixer&#65289;&#20197;&#21450;&#21367;&#31215;&#23618;&#22312;&#20999;&#29255;&#26041;&#21521;&#19978;&#36827;&#34892;&#19978;&#37319;&#26679;&#12290; MLP-SRGAN&#20351;&#29992;MSSEG2&#25361;&#25112;&#25968;&#25454;&#38598;&#20013;&#30340;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;FLAIR MRI&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#20010;&#20302;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22810;&#20013;&#24515;FLAIR&#25968;&#25454;&#38598;&#65288;CAIN&#65292;ADNI&#65292;CCNA&#65289;&#30340;&#22270;&#20687;&#65292;&#20197;&#26816;&#26597;&#22312;&#20445;&#30041;&#65288;&#26410;&#35265;&#65289;&#20020;&#24202;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#23558;&#19978;&#37319;&#26679;&#32467;&#26524;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;SR&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22522;&#26412;&#20107;&#23454;&#30340;&#22270;&#20687;&#65292;&#20351;&#29992;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#26469;&#34913;&#37327;&#19978;&#37319;&#26679;&#24615;&#33021;&#12290;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#32467;&#26500;&#65292;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#65292;&#20197;&#22312;&#32570;&#20047;&#22522;&#30784;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#37327;&#21270;&#38160;&#24230;&#65288;&#36793;&#32536;&#24378;&#24230;&#65289;&#65292;&#22122;&#22768;&#65288;&#29109;&#65289;&#21644;&#27169;&#31946;&#24230;&#65288;&#20302;&#39057;&#20449;&#24687;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel architecture called MLP-SRGAN, which is a single-dimension Super Resolution Generative Adversarial Network (SRGAN) that utilizes Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to upsample in the slice direction. MLP-SRGAN is trained and validated using high resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with low spatial resolution in the slice dimension to examine performance on held-out (unseen) clinical data. Upsampled results are compared to several state-of-the-art SR networks. For images with high resolution (HR) ground truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index (SSIM) are used to measure upsampling performance. Several new structural, no-reference image quality metrics were proposed to quantify sharpness (edge strength), noise (entropy), and blurriness (low frequency information) in the absence of groun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.06296</link><description>&lt;p&gt;
&#38450;&#27490;&#27880;&#24847;&#21147;&#29109;&#23849;&#28291;&#30340;Transformer&#35757;&#32451;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31283;&#23450;&#24615;&#23545;&#20110;Transformer&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27880;&#24847;&#21147;&#23618;&#30340;&#28436;&#21464;&#26469;&#25506;&#31350;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36319;&#36394;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#30340;&#27880;&#24847;&#21147;&#29109;&#65292;&#36825;&#26159;&#27169;&#22411;&#38160;&#24230;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23384;&#22312;&#19968;&#31181;&#24120;&#35265;&#27169;&#24335;&#65292;&#21363;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#37319;&#21462;&#25391;&#33633;&#25439;&#22833;&#25110;&#21457;&#25955;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#23558;&#30149;&#24577;&#20302;&#27880;&#24847;&#21147;&#29109;&#65292;&#23545;&#24212;&#39640;&#24230;&#38598;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#31216;&#20026;$\textit{&#29109;&#23849;&#28291;}$&#12290;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\sigma$Reparam&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;&#21644;&#39069;&#22806;&#30340;&#23398;&#20064;&#26631;&#37327;&#37325;&#26032;&#21442;&#25968;&#21270;&#25152;&#26377;&#32447;&#24615;&#23618;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
&lt;/p&gt;</description></item><item><title>CoNIC&#25361;&#25112;&#20351;&#29992;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#21457;&#29616;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06274</link><description>&lt;p&gt;
CoNIC&#25361;&#25112;&#65306;&#25512;&#21160;&#26680;&#26816;&#27979;&#12289;&#20998;&#21106;&#12289;&#20998;&#31867;&#21644;&#35745;&#25968;&#30340;&#21069;&#27839;&#65288;arXiv:2303.06274v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
CoNIC Challenge: Pushing the Frontiers of Nuclear Detection, Segmentation, Classification and Counting. (arXiv:2303.06274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06274
&lt;/p&gt;
&lt;p&gt;
CoNIC&#25361;&#25112;&#20351;&#29992;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#21457;&#29616;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CoNIC challenge used the largest dataset to evaluate nuclear segmentation and cellular composition, stimulated the development of reproducible algorithms for cellular recognition, and found that eosinophils and neutrophils play an important role in tumors.
&lt;/p&gt;
&lt;p&gt;
&#26680;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#24418;&#24577;&#27979;&#37327;&#26159;&#24110;&#21161;&#25105;&#20204;&#36827;&#19968;&#27493;&#20102;&#35299;&#32452;&#32455;&#23398;&#21644;&#24739;&#32773;&#39044;&#21518;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#21019;&#26032;&#65292;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35774;&#32622;&#20102;&#19968;&#20010;&#31038;&#21306;&#24191;&#27867;&#30340;&#25361;&#25112;&#65292;&#20197;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#25361;&#25112;&#21517;&#20026;CoNIC&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#20849;&#25490;&#34892;&#27036;&#19978;&#36827;&#34892;&#23454;&#26102;&#32467;&#26524;&#26816;&#26597;&#12290;&#25105;&#20204;&#22522;&#20110;1,658&#20010;&#32467;&#32928;&#32452;&#32455;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#21518;&#25361;&#25112;&#20998;&#26512;&#12290;&#27599;&#20010;&#27169;&#22411;&#26816;&#27979;&#21040;&#32422;7&#20159;&#20010;&#32454;&#32990;&#26680;&#65292;&#30456;&#20851;&#29305;&#24449;&#29992;&#20110;&#19981;&#33391;&#22686;&#29983;&#20998;&#32423;&#21644;&#29983;&#23384;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25361;&#25112;&#23545;&#20808;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#25913;&#36827;&#23548;&#33268;&#20102;&#19979;&#28216;&#24615;&#33021;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#34920;&#26126;&#65292;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nuclear detection, segmentation and morphometric profiling are essential in helping us further understand the relationship between histology and patient outcome. To drive innovation in this area, we setup a community-wide challenge using the largest available dataset of its kind to assess nuclear segmentation and cellular composition. Our challenge, named CoNIC, stimulated the development of reproducible algorithms for cellular recognition with real-time result inspection on public leaderboards. We conducted an extensive post-challenge analysis based on the top-performing models using 1,658 whole-slide images of colon tissue. With around 700 million detected nuclei per model, associated features were used for dysplasia grading and survival analysis, where we demonstrated that the challenge's improvement over the previous state-of-the-art led to significant boosts in downstream performance. Our findings also suggest that eosinophils and neutrophils play an important role in the tumour m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#21305;&#37197;&#26469;&#23398;&#20064;&#65292;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06242</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#35270;&#35282;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#23398;&#20064;&#29992;&#20110;&#33258;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations. (arXiv:2303.06242v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#21305;&#37197;&#26469;&#23398;&#20064;&#65292;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations, which adopts self-supervision and uses data augmentations to generate two views of the same sample, and learns by matching one to the other. It uses hyperbolic uncertainty to determine the algorithmic learning pace, assuming that less uncertain samples should be more strongly driving the training, with a larger weight and pace.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23398;&#20064;&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#26377;&#30410;&#65292;&#20363;&#22914;&#24369;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#21487;&#20197;&#36873;&#25321;&#21644;&#25490;&#24207;&#35757;&#32451;&#26679;&#26412;&#24207;&#21015;&#65292;&#20174;&#26131;&#21040;&#38590;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#20854;&#20013;&#20219;&#21153;&#30340;&#30693;&#35782;&#22312;&#35757;&#32451;&#26399;&#38388;&#25104;&#29087;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#12290;HYSP&#37319;&#29992;&#33258;&#30417;&#30563;&#65306;&#23427;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#65288;&#31216;&#20026;&#22312;&#32447;&#65289;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#65288;&#30446;&#26631;&#65289;&#21305;&#37197;&#26469;&#23398;&#20064;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26159;&#37319;&#29992;&#30340;&#36229;&#20284;&#26354;&#31070;&#32463;&#32593;&#32476;&#30340;&#21103;&#20135;&#21697;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#25104;&#29087;&#65292;&#19982;&#39069;&#22806;&#25104;&#26412;&#30456;&#27604;&#65292;&#27809;&#26377;&#39069;&#22806;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-paced learning has been beneficial for tasks where some initial knowledge is available, such as weakly supervised learning and domain adaptation, to select and order the training sample sequence, from easy to complex. However its applicability remains unexplored in unsupervised learning, whereby the knowledge of the task matures during training. We propose a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations. HYSP adopts self-supervision: it uses data augmentations to generate two views of the same sample, and it learns by matching one (named online) to the other (the target). We propose to use hyperbolic uncertainty to determine the algorithmic learning pace, under the assumption that less uncertain samples should be more strongly driving the training, with a larger weight and pace. Hyperbolic uncertainty is a by-product of the adopted hyperbolic neural networks, it matures during training and it comes with no extra cost, compared to the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.06241</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#38656;&#35201;&#20351;&#29992;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do we need entire training data for adversarial training?. (arXiv:2303.06241v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new adversarial training method that reduces training time by selecting only the adversarially-prone samples from the training dataset.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#29992;&#20110;&#35299;&#20915;&#35768;&#22810;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#21307;&#23398;&#22270;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;DNN&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#24050;&#32463;&#34987;&#24191;&#27867;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#26041;&#27861;&#37117;&#20250;&#20026;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20165;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#23569;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#36873;&#25321;&#23376;&#38598;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36807;&#28388;&#20986;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25191;&#34892;&#31616;&#21333;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#20197;&#36807;&#28388;&#20986;&#36825;&#20010;&#23376;&#38598;&#12290;&#22312;&#36825;&#20010;&#25915;&#20987;&#20013;&#65292;&#25105;&#20204;&#21521;&#27599;&#20010;&#20687;&#32032;&#28155;&#21152;&#19968;&#20010;&#23567;&#25200;&#21160;&#21644;&#20960;&#26465;&#32593;&#26684;&#32447;&#21040;&#36755;&#20837;&#22270;&#20687;&#20013;&#12290;&#25105;&#20204;&#23545;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#24182;&#19988;...
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are being used to solve a wide range of problems in many domains including safety-critical domains like self-driving cars and medical imagery. DNNs suffer from vulnerability against adversarial attacks. In the past few years, numerous approaches have been proposed to tackle this problem by training networks using adversarial training. Almost all the approaches generate adversarial examples for the entire training dataset, thus increasing the training time drastically. We show that we can decrease the training time for any adversarial training algorithm by using only a subset of training data for adversarial training. To select the subset, we filter the adversarially-prone samples from the training data. We perform a simple adversarial attack on all training examples to filter this subset. In this attack, we add a small perturbation to each pixel and a few grid lines to the input image.  We perform adversarial training on the adversarially-prone subset and mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24352;&#37327;&#29615;&#22240;&#23376;&#20998;&#35299;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#24674;&#22797;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20462;&#22797;&#21644;&#21435;&#22122;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06235</link><description>&lt;p&gt;
&#24102;&#24352;&#37327;&#33258;&#32534;&#30721;&#22120;&#30340;&#21387;&#32553;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Compressive Sensing with Tensorized Autoencoder. (arXiv:2303.06235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24352;&#37327;&#29615;&#22240;&#23376;&#20998;&#35299;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#24674;&#22797;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20462;&#22797;&#21644;&#21435;&#22122;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for image recovery using an autoencoder with tensor ring factorization, which achieves better reconstruction quality in inpainting and denoising applications.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#21487;&#20197;&#35757;&#32451;&#25104;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#24037;&#20855;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#38598;&#21512;&#20013;&#30340;&#19981;&#21516;&#22270;&#20687;&#26159;&#24444;&#27492;&#30340;&#20851;&#33410;&#29256;&#26412;&#65307;&#20363;&#22914;&#65292;&#21516;&#19968;&#29289;&#20307;&#20855;&#26377;&#19981;&#21516;&#30340;&#29031;&#26126;&#12289;&#32972;&#26223;&#25110;&#23039;&#21183;&#12290;&#27492;&#22806;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#22270;&#20687;&#30340;&#26576;&#20123;&#37096;&#20998;&#21487;&#33021;&#20250;&#21463;&#21040;&#22122;&#22768;&#25110;&#32570;&#22833;&#26465;&#30446;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#25968;&#25454;&#30340;&#32467;&#26500;&#20808;&#39564;&#26469;&#24674;&#22797;&#22270;&#20687;&#65292;&#32780;&#27809;&#26377;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#65288;&#24178;&#20928;&#65289;&#22270;&#20687;&#12290;&#36825;&#26679;&#30340;&#24674;&#22797;&#38382;&#39064;&#23646;&#20110;&#21387;&#32553;&#24863;&#30693;&#39046;&#22495;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#23884;&#20837;&#31354;&#38388;&#19978;&#23398;&#20064;&#24102;&#26377;&#24352;&#37327;&#29615;&#22240;&#23376;&#20998;&#35299;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20197;&#23545;&#25968;&#25454;&#26045;&#21152;&#32467;&#26500;&#32422;&#26463;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#29942;&#39048;&#23618;&#20013;&#20351;&#29992;&#24352;&#37327;&#29615;&#32467;&#26500;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#30340;&#36719;&#26631;&#31614;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20462;&#22797;&#21644;&#21435;&#22122;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#24471;&#21040;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep networks can be trained to map images into a low-dimensional latent space. In many cases, different images in a collection are articulated versions of one another; for example, same object with different lighting, background, or pose. Furthermore, in many cases, parts of images can be corrupted by noise or missing entries. In this paper, our goal is to recover images without access to the ground-truth (clean) images using the articulations as structural prior of the data. Such recovery problems fall under the domain of compressive sensing. We propose to learn autoencoder with tensor ring factorization on the the embedding space to impose structural constraints on the data. In particular, we use a tensor ring structure in the bottleneck layer of the autoencoder that utilizes the soft labels of the structured dataset. We empirically demonstrate the effectiveness of the proposed approach for inpainting and denoising applications. The resulting method achieves better reconstruction qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#22810;&#31867;OOD&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#22312;&#38647;&#36798;&#36317;&#31163;&#22810;&#26222;&#21202;&#22270;&#20687;&#65288;RDIs&#65289;&#19978;&#36816;&#34892;&#12290;&#26816;&#27979;&#22120;&#26088;&#22312;&#23558;&#38500;&#22352;&#12289;&#31449;&#25110;&#36208;&#30340;&#20154;&#20197;&#22806;&#30340;&#20219;&#20309;&#31227;&#21160;&#29289;&#20307;&#20998;&#31867;&#20026;OOD&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#26816;&#27979;&#21628;&#21560;&#31561;&#24494;&#23567;&#30340;&#20154;&#20307;&#36816;&#21160;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.06232</link><description>&lt;p&gt;
MCROOD: &#22810;&#31867;&#38647;&#36798;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MCROOD: Multi-Class Radar Out-Of-Distribution Detection. (arXiv:2303.06232v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#22810;&#31867;OOD&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#22312;&#38647;&#36798;&#36317;&#31163;&#22810;&#26222;&#21202;&#22270;&#20687;&#65288;RDIs&#65289;&#19978;&#36816;&#34892;&#12290;&#26816;&#27979;&#22120;&#26088;&#22312;&#23558;&#38500;&#22352;&#12289;&#31449;&#25110;&#36208;&#30340;&#20154;&#20197;&#22806;&#30340;&#20219;&#20309;&#31227;&#21160;&#29289;&#20307;&#20998;&#31867;&#20026;OOD&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#26816;&#27979;&#21628;&#21560;&#31561;&#24494;&#23567;&#30340;&#20154;&#20307;&#36816;&#21160;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a reconstruction-based multi-class OOD detector that operates on radar range doppler images (RDIs). The detector aims to classify any moving object other than a person sitting, standing, or walking as OOD. The authors also provide a simple yet effective pre-processing technique to detect minor human body movements like breathing. The method outperforms state-of-the-art OOD detection methods in experiments.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#23433;&#20840;&#37096;&#32626;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26550;&#26500;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#21463;&#21040;&#29305;&#21035;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#22810;&#31867;OOD&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#22312;&#38647;&#36798;&#36317;&#31163;&#22810;&#26222;&#21202;&#22270;&#20687;&#65288;RDIs&#65289;&#19978;&#36816;&#34892;&#12290;&#26816;&#27979;&#22120;&#26088;&#22312;&#23558;&#38500;&#22352;&#12289;&#31449;&#25110;&#36208;&#30340;&#20154;&#20197;&#22806;&#30340;&#20219;&#20309;&#31227;&#21160;&#29289;&#20307;&#20998;&#31867;&#20026;OOD&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#26816;&#27979;&#21628;&#21560;&#31561;&#24494;&#23567;&#30340;&#20154;&#20307;&#36816;&#21160;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#34987;&#31216;&#20026;&#21628;&#21560;&#26816;&#27979;&#22120;&#65288;RESPD&#65289;&#65292;&#21487;&#20197;&#20943;&#36731;OOD&#26816;&#27979;&#30340;&#36127;&#25285;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20154;&#22352;&#21644;&#20154;&#31449;&#30340;&#31867;&#21035;&#12290;&#22312;&#25105;&#20204;&#25910;&#38598;&#30340;60GHz&#30701;&#36317;&#31163;FMCW&#38647;&#36798;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#20998;&#21035;&#20026;&#22352;&#12289;&#31449;&#21644;&#36208;&#19977;&#20010;&#31867;&#21035;&#23454;&#29616;&#20102;97.45&#65285;&#12289;92.13&#65285;&#21644;96.58&#65285;&#30340;AUROC&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#27604;&#31532;&#20108;&#22909;&#30340;&#26041;&#27861;&#24555;24&#20493;&#65292;&#24182;&#19988;&#26159;v
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection has recently received special attention due to its critical role in safely deploying modern deep learning (DL) architectures. This work proposes a reconstruction-based multi-class OOD detector that operates on radar range doppler images (RDIs). The detector aims to classify any moving object other than a person sitting, standing, or walking as OOD. We also provide a simple yet effective pre-processing technique to detect minor human body movements like breathing. The simple idea is called respiration detector (RESPD) and eases the OOD detection, especially for human sitting and standing classes. On our dataset collected by 60GHz short-range FMCW Radar, we achieve AUROCs of 97.45%, 92.13%, and 96.58% for sitting, standing, and walking classes, respectively. We perform extensive experiments and show that our method outperforms state-of-the-art (SOTA) OOD detection methods. Also, our pipeline performs 24 times faster than the second-best method and is v
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;POV&#30340;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#26550;&#26500;&#65292;&#20854;&#20013;&#30340;Carolinas Highway Dataset&#65288;CHD&#65289;&#21253;&#25324;160&#19975;&#24103;&#21644;338,000&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2303.06202</link><description>&lt;p&gt;
&#22522;&#20110;POV&#30340;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A POV-based Highway Vehicle Trajectory Dataset and Prediction Architecture. (arXiv:2303.06202v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06202
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;POV&#30340;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#26550;&#26500;&#65292;&#20854;&#20013;&#30340;Carolinas Highway Dataset&#65288;CHD&#65289;&#21253;&#25324;160&#19975;&#24103;&#21644;338,000&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introducing a POV-based highway vehicle trajectory dataset and prediction architecture, including Carolinas Highway Dataset (CHD) with 1.6 million frames and 338,000 vehicle trajectories captured at eight locations in Carolinas.
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#22810;&#20010;&#35270;&#35282;&#65288;POV&#65289;&#30340;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#23545;&#20110;&#21508;&#31181;&#20132;&#36890;&#23433;&#20840;&#21644;&#31649;&#29702;&#24212;&#29992;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23613;&#31649;&#36712;&#36857;&#25968;&#25454;&#38598;&#24456;&#20016;&#23500;&#65292;&#20294;&#24456;&#23569;&#25552;&#20379;&#20840;&#38754;&#21644;&#22810;&#26679;&#21270;&#30340;&#39550;&#39542;&#22330;&#26223;&#65292;&#25429;&#25417;&#21508;&#31181;&#39640;&#36895;&#20844;&#36335;&#24067;&#23616;&#12289;&#21512;&#24182;&#36710;&#36947;&#21644;&#37197;&#32622;&#30340;&#22810;&#20010;&#35270;&#28857;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#39550;&#39542;&#21592;&#12289;&#36710;&#36742;&#21644;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#20043;&#38388;&#24494;&#22937;&#20114;&#21160;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Carolinas Highway Dataset&#65288;CHD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36710;&#36742;&#36712;&#36857;&#12289;&#26816;&#27979;&#21644;&#36319;&#36394;&#25968;&#25454;&#38598;&#12290;CHD&#26159;&#22312;Carolinas&#30340;&#20843;&#20010;&#20301;&#32622;&#25293;&#25668;&#30340;&#39640;&#36895;&#20844;&#36335;&#35270;&#39057;&#20013;&#25429;&#33719;&#30340;160&#19975;&#24103;&#65292;&#21253;&#25324;338,000&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle Trajectory datasets that provide multiple point-of-views (POVs) can be valuable for various traffic safety and management applications. Despite the abundance of trajectory datasets, few offer a comprehensive and diverse range of driving scenes, capturing multiple viewpoints of various highway layouts, merging lanes, and configurations. This limits their ability to capture the nuanced interactions between drivers, vehicles, and the roadway infrastructure. We introduce the \emph{Carolinas Highway Dataset (CHD\footnote{\emph{CHD} available at: \url{https://github.com/TeCSAR-UNCC/Carolinas\_Dataset}})}, a vehicle trajectory, detection, and tracking dataset. \emph{CHD} is a collection of 1.6 million frames captured in highway-based videos from eye-level and high-angle POVs at eight locations across Carolinas with 338,000 vehicle trajectories. The locations, timing of recordings, and camera angles were carefully selected to capture various road geometries, traffic patterns, lighting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedFBN&#65292;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#20248;&#21270;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.06180</link><description>&lt;p&gt;
&#38024;&#23545;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels. (arXiv:2303.06180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedFBN&#65292;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#20248;&#21270;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes FedFBN, a federated learning framework that uses pretrained networks as the model backend and freezes the batch normalization layers throughout the training process to optimize medical image classification on distributed non-iid datasets with partial labels.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#33016;&#37096;X&#20809;&#25968;&#25454;&#38598;&#24050;&#32463;&#24102;&#22836;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#26816;&#27979;&#21487;&#33021;&#23384;&#22312;&#30340;&#19968;&#37096;&#20998;&#30142;&#30149;&#26631;&#31614;&#65292;&#22240;&#27492;&#20351;&#23427;&#20204;&#25104;&#20026;&#20998;&#24067;&#24335;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#25351;&#20986;&#65292;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#20855;&#26377;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#20855;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30456;&#20851;&#30340;&#22495;&#28418;&#31227;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedFBN&#65292;&#36825;&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#36801;&#31227;&#23398;&#20064;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;iid&#29609;&#20855;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#38750;iid&#25968;&#25454;&#38598;&#35780;&#20272;FedFBN&#19982;&#24403;&#21069;FL&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FedFBN&#20248;&#20110;&#20351;&#29992;&#20998;&#24067;&#24335;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#30340;&#24403;&#21069;&#32858;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous large-scale chest x-ray datasets have spearheaded expert-level detection of abnormalities using deep learning. However, these datasets focus on detecting a subset of disease labels that could be present, thus making them distributed and non-iid with partial labels. Recent literature has indicated the impact of batch normalization layers on the convergence of federated learning due to domain shift associated with non-iid data with partial labels. To that end, we propose FedFBN, a federated learning framework that draws inspiration from transfer learning by using pretrained networks as the model backend and freezing the batch normalization layers throughout the training process. We evaluate FedFBN with current FL strategies using synthetic iid toy datasets and large-scale non-iid datasets across scenarios with partial and complete labels. Our results demonstrate that FedFBN outperforms current aggregation strategies for training global models using distributed and non-iid data w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#21457;&#29616;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#20294;&#36890;&#36807;&#23545;&#24494;&#35843;&#25968;&#25454;&#38598;&#36827;&#34892;&#24178;&#39044;&#21487;&#20197;&#32416;&#27491;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;&#36825;&#34920;&#26126;&#20180;&#32454;&#31574;&#21010;&#24494;&#35843;&#25968;&#25454;&#38598;&#23545;&#20110;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20559;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#26679;&#20570;&#29978;&#33267;&#21487;&#20197;&#24357;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2303.06167</link><description>&lt;p&gt;
&#36890;&#36807;&#25805;&#20316;&#24494;&#35843;&#25968;&#25454;&#38598;&#26469;&#20811;&#26381;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Overcoming Bias in Pretrained Models by Manipulating the Finetuning Dataset. (arXiv:2303.06167v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#21457;&#29616;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#20294;&#36890;&#36807;&#23545;&#24494;&#35843;&#25968;&#25454;&#38598;&#36827;&#34892;&#24178;&#39044;&#21487;&#20197;&#32416;&#27491;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;&#36825;&#34920;&#26126;&#20180;&#32454;&#31574;&#21010;&#24494;&#35843;&#25968;&#25454;&#38598;&#23545;&#20110;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20559;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#26679;&#20570;&#29978;&#33267;&#21487;&#20197;&#24357;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the bias problem in pretrained models and finds that finetuned models can inherit the biases of pretrained models, but these biases can be corrected by manipulating the finetuning dataset with little impact on performance. This implies that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.
&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#36890;&#36807;&#20801;&#35768;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#34920;&#36798;&#29305;&#24449;&#34987;&#24494;&#35843;&#21040;&#26356;&#23567;&#12289;&#26356;&#20855;&#39046;&#22495;&#29305;&#23450;&#24615;&#30340;&#25968;&#25454;&#38598;&#30340;&#30446;&#26631;&#20219;&#21153;&#20013;&#32780;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#26377;&#20154;&#25285;&#24515;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#24102;&#26377;&#33258;&#24049;&#30340;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#20250;&#20256;&#25773;&#21040;&#24494;&#35843;&#27169;&#22411;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20559;&#35265;&#65292;&#24403;&#20559;&#35265;&#34987;&#27010;&#24565;&#21270;&#20026;&#30446;&#26631;&#20219;&#21153;&#21644;&#25935;&#24863;&#23646;&#24615;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#25454;&#38598;&#20013;&#29305;&#23450;&#32676;&#20307;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#26102;&#12290;&#22312;&#20559;&#35265;&#30340;&#20004;&#31181;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;(1)&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#30830;&#23454;&#21487;&#20197;&#32487;&#25215;&#23427;&#20204;&#30340;&#20559;&#35265;&#65292;&#20294;(2)&#36890;&#36807;&#23545;&#24494;&#35843;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#23545;&#36739;&#23567;&#30340;&#24178;&#39044;&#65292;&#36825;&#31181;&#20559;&#35265;&#21487;&#20197;&#24471;&#21040;&#32416;&#27491;&#65292;&#32780;&#19988;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24448;&#24448;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24847;&#21619;&#30528;&#65292;&#20180;&#32454;&#31574;&#21010;&#24494;&#35843;&#25968;&#25454;&#38598;&#23545;&#20110;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20559;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#26679;&#20570;&#29978;&#33267;&#21487;&#20197;&#24357;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is beneficial by allowing the expressive features of models pretrained on large-scale datasets to be finetuned for the target task of smaller, more domain-specific datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the finetuned model. In this work, we investigate bias when conceptualized as both spurious correlations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we find that (1) models finetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance. Our findings imply that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#25200;&#21160;&#23383;&#31526;&#20018;&#30340;&#26131;&#35835;&#24615;&#65292;&#24182;&#26681;&#25454;&#20854;&#26131;&#35835;&#24615;&#23545;&#20505;&#36873;&#25200;&#21160;&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20445;&#25345;&#26131;&#35835;&#24615;&#30340;&#25991;&#26412;&#25200;&#21160;&#30340;&#31995;&#32479;&#24615;&#34920;&#24449;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2303.05077</link><description>&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;&#25991;&#26412;&#25200;&#21160;&#30340;&#26131;&#35835;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning the Legibility of Visual Text Perturbations. (arXiv:2303.05077v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#25200;&#21160;&#23383;&#31526;&#20018;&#30340;&#26131;&#35835;&#24615;&#65292;&#24182;&#26681;&#25454;&#20854;&#26131;&#35835;&#24615;&#23545;&#20505;&#36873;&#25200;&#21160;&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20445;&#25345;&#26131;&#35835;&#24615;&#30340;&#25991;&#26412;&#25200;&#21160;&#30340;&#31995;&#32479;&#24615;&#34920;&#24449;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method to learn models that predict the legibility of a perturbed string and rank candidate perturbations based on their legibility, filling the gap in systematically characterizing the legibility of text perturbations while preserving it.
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;NLP&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#20250;&#25200;&#21160;&#36755;&#20837;&#20197;&#20135;&#29983;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#20294;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#36127;&#38754;&#24433;&#21709;&#30340;&#23383;&#31526;&#20018;&#65288;&#20363;&#22914;'ergo' $\rightarrow$ '$\epsilon$rgo'&#65289;&#65292;&#36825;&#20123;&#23383;&#31526;&#20018;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26131;&#35835;&#30340;&#12290;&#23613;&#31649;&#20445;&#25345;&#26131;&#35835;&#24615;&#26159;&#25991;&#26412;&#25200;&#21160;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#23545;&#20854;&#36827;&#34892;&#31995;&#32479;&#24615;&#30340;&#34920;&#24449;&#65307;&#30456;&#21453;&#65292;&#26131;&#35835;&#24615;&#36890;&#24120;&#36890;&#36807;&#23545;&#25200;&#21160;&#30340;&#24615;&#36136;&#21644;&#31243;&#24230;&#30340;&#30452;&#35273;&#32422;&#26463;&#26469;&#23454;&#29616;&#12290;&#29305;&#21035;&#22320;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#20445;&#25345;&#26131;&#35835;&#24615;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25200;&#21160;&#22810;&#23569;&#36755;&#20837;&#65292;&#25110;&#22914;&#20309;&#37327;&#21270;&#25200;&#21160;&#23383;&#31526;&#20018;&#30340;&#26131;&#35835;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#25200;&#21160;&#23383;&#31526;&#20018;&#30340;&#26131;&#35835;&#24615;&#65292;&#24182;&#26681;&#25454;&#20854;&#26131;&#35835;&#24615;&#23545;&#20505;&#36873;&#25200;&#21160;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;LEGIT&#65292;&#19968;&#20010;&#20154;&#31867;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#35270;&#35273;&#19978;&#25200;&#21160;&#25991;&#26412;&#30340;&#26131;&#35835;&#24615;&#12290;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;&#35270;&#35273;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;$0.91$&#30340;F1&#20998;&#25968;&#65292;&#20197;&#39044;&#27979;&#36755;&#20837;&#26159;&#21542;&#26131;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many adversarial attacks in NLP perturb inputs to produce visually similar strings ('ergo' $\rightarrow$ '$\epsilon$rgo') which are legible to humans but degrade model performance. Although preserving legibility is a necessary condition for text perturbation, little work has been done to systematically characterize it; instead, legibility is typically loosely enforced via intuitions around the nature and extent of perturbations. Particularly, it is unclear to what extent can inputs be perturbed while preserving legibility, or how to quantify the legibility of a perturbed string. In this work, we address this gap by learning models that predict the legibility of a perturbed string, and rank candidate perturbations based on their legibility. To do so, we collect and release LEGIT, a human-annotated dataset comprising the legibility of visually perturbed text. Using this dataset, we build both text- and vision-based models which achieve up to $0.91$ F1 score in predicting whether an input
&lt;/p&gt;</description></item><item><title>Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.02506</link><description>&lt;p&gt;
Prismer: &#19968;&#31181;&#20855;&#26377;&#19987;&#23478;&#38598;&#21512;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prismer: A Vision-Language Model with An Ensemble of Experts. (arXiv:2303.02506v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02506
&lt;/p&gt;
&lt;p&gt;
Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prismer is a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts, achieving fine-tuned and few-shot learning performance competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#23427;&#20204;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24222;&#22823;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Prismer&#65292;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#12290;Prismer&#21482;&#38656;&#35201;&#35757;&#32451;&#23569;&#37327;&#32452;&#20214;&#65292;&#22823;&#37096;&#20998;&#32593;&#32476;&#26435;&#37325;&#20174;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#39046;&#22495;&#19987;&#23478;&#20013;&#32487;&#25215;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#20923;&#32467;&#29366;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#21487;&#20197;&#26377;&#25928;&#22320;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/NVlabs/prismer&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.
&lt;/p&gt;</description></item><item><title>Mesh-SORT&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20301;&#32622;&#36319;&#36394;&#22120;&#65292;&#36890;&#36807;&#32593;&#26684;&#20998;&#21106;&#24103;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#20301;&#32622;&#24863;&#30693;&#30340;&#20002;&#22833;&#31649;&#29702;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#21305;&#37197;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36319;&#36394;&#20013;&#30340;&#19981;&#33391;&#26816;&#27979;&#22120;&#21644;&#36974;&#25377;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14415</link><description>&lt;p&gt;
Mesh-SORT: &#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20301;&#32622;&#36319;&#36394;&#22120;&#21450;&#20854;&#20002;&#22833;&#31649;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Mesh-SORT: Simple and effective location-wise tracker with lost management strategies. (arXiv:2302.14415v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14415
&lt;/p&gt;
&lt;p&gt;
Mesh-SORT&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20301;&#32622;&#36319;&#36394;&#22120;&#65292;&#36890;&#36807;&#32593;&#26684;&#20998;&#21106;&#24103;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#20301;&#32622;&#24863;&#30693;&#30340;&#20002;&#22833;&#31649;&#29702;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#21305;&#37197;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36319;&#36394;&#20013;&#30340;&#19981;&#33391;&#26816;&#27979;&#22120;&#21644;&#36974;&#25377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mesh-SORT is a simple and effective location-wise tracker that solves the problem of bad detectors and occlusions in tracking by dividing frames into grids and proposing corresponding location-aware loss management strategies and different matching strategies.
&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36319;&#36394;(MOT)&#30001;&#20110;&#20854;&#22312;&#20132;&#36890;&#21644;&#34892;&#20154;&#26816;&#27979;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#36890;&#36807;&#26816;&#27979;&#36827;&#34892;&#36319;&#36394;&#21487;&#33021;&#20250;&#21463;&#21040;&#22122;&#22768;&#26816;&#27979;&#22120;&#20135;&#29983;&#30340;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#22312;&#36974;&#25377;&#20043;&#21069;&#30340;&#19981;&#31934;&#30830;&#36793;&#30028;&#26694;&#65292;&#32780;&#19988;&#22312;&#22823;&#22810;&#25968;&#36319;&#36394;&#22330;&#26223;&#20013;&#65292;&#29289;&#20307;&#24448;&#24448;&#20250;&#22312;&#29305;&#23450;&#20301;&#32622;&#31227;&#21160;&#21644;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36319;&#36394;&#22120;&#26469;&#22788;&#29702;&#19981;&#33391;&#26816;&#27979;&#22120;&#21644;&#36974;&#25377;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20301;&#32622;&#24863;&#30693;&#30340;&#23376;&#21306;&#22495;&#35782;&#21035;&#26041;&#27861;&#65292;&#23558;&#24103;&#31561;&#20998;&#20026;&#32593;&#26684;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#20301;&#32622;&#24863;&#30693;&#30340;&#20002;&#22833;&#31649;&#29702;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#21305;&#37197;&#31574;&#30053;&#12290;&#32467;&#26524;&#65292;Mesh-SORT&#30340;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#20351;MOT17&#25968;&#25454;&#38598;&#19978;&#30340;3%&#30862;&#29255;&#21270;&#12289;7.2% ID&#20999;&#25442;&#19979;&#38477;&#21644;0.4% MOTA&#25913;&#36827;&#19982;&#22522;&#32447;&#30456;&#27604;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20854;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Object Tracking (MOT) has gained extensive attention in recent years due to its potential applications in traffic and pedestrian detection. We note that tracking by detection may suffer from errors generated by noise detectors, such as an imprecise bounding box before the occlusions, and observed that in most tracking scenarios, objects tend to move and lost within specific locations. To counter this, we present a novel tracker to deal with the bad detector and occlusions. Firstly, we proposed a location-wise sub-region recognition method which equally divided the frame, which we called mesh. Then we proposed corresponding location-wise loss management strategies and different matching strategies. The resulting Mesh-SORT, ablation studies demonstrate its effectiveness and made 3% fragmentation 7.2% ID switches drop and 0.4% MOTA improvement compared to the baseline on MOT17 datasets. Finally, we analyze its limitation on the specific scene and discussed what future works can be e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.01735</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#26041;&#24046;&#32553;&#20943;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective. (arXiv:2302.01735v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. The concept of variance-reduced estimation is used to build ARCO, and certain variance-reduction techniques are shown to be particularly beneficial in medical image segmentation.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#25552;&#39640;&#35270;&#35273;&#34920;&#31034;&#36136;&#37327;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#26679;&#26412;&#23545;&#26469;&#23454;&#29616;&#12290;&#36825;&#26159;&#36890;&#36807;&#35266;&#23519;&#21040;&#65292;&#22312;&#27809;&#26377;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#37319;&#26679;&#20855;&#26377;&#30495;&#27491;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#30340;&#36127;&#26679;&#26412;&#65292;&#21017;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#26469;&#33258;&#30456;&#20284;&#30340;&#35299;&#21078;&#29305;&#24449;&#65292;&#27169;&#22411;&#21487;&#33021;&#38590;&#20197;&#21306;&#20998;&#23569;&#25968;&#23614;&#31867;&#26679;&#26412;&#65292;&#20351;&#24471;&#23614;&#31867;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth label, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in medical image se
&lt;/p&gt;</description></item><item><title>LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2301.09799</link><description>&lt;p&gt;
LDMIC&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LDMIC: Learning-based Distributed Multi-view Image Coding. (arXiv:2301.09799v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09799
&lt;/p&gt;
&lt;p&gt;
LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
LDMIC is a learning-based distributed multi-view image coding framework that captures global inter-view correlations through independent encoders and a joint context transfer module based on the cross-attention mechanism, which is insensitive to geometric relations.
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#22270;&#20687;&#21387;&#32553;&#22312;3D&#30456;&#20851;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#39044;&#27979;&#32534;&#30721;&#26550;&#26500;&#65292;&#38656;&#35201;&#32852;&#21512;&#32534;&#30721;&#21387;&#32553;&#30456;&#24212;&#30340;&#35270;&#24046;&#21644;&#27531;&#24046;&#20449;&#24687;&#12290;&#36825;&#35201;&#27714;&#30456;&#26426;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#65292;&#24182;&#24378;&#21046;&#25191;&#34892;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#26497;&#32447;&#20960;&#20309;&#32422;&#26463;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#38543;&#26426;&#37325;&#21472;&#35270;&#37326;&#30340;&#20998;&#24067;&#24335;&#30456;&#26426;&#31995;&#32479;&#20013;&#37096;&#32626;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#29702;&#35770;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#21644;&#32852;&#21512;&#35299;&#30721;&#23454;&#29616;&#30456;&#20851;&#28304;&#30340;&#39640;&#25928;&#25968;&#25454;&#21387;&#32553;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#35774;&#35745;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#65288;LDMIC&#65289;&#26694;&#26550;&#30340;&#21160;&#26426;&#12290;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#65292;LDMIC&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relation
&lt;/p&gt;</description></item><item><title>SegViz&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;</title><link>http://arxiv.org/abs/2301.07074</link><description>&lt;p&gt;
SegViz&#65306;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#22120;&#23448;&#20998;&#21106;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations. (arXiv:2301.07074v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07074
&lt;/p&gt;
&lt;p&gt;
SegViz&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;
&lt;/p&gt;
&lt;p&gt;
SegViz is a federated learning-based framework for training segmentation models from distributed non-i.i.d datasets with partial annotations. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation.
&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#26159;&#21307;&#23398;&#22270;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#22810;&#20010;&#19979;&#28216;&#20020;&#24202;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#25163;&#21160;&#27880;&#37322;&#26159;&#32791;&#26102;&#30340;&#12289;&#38656;&#35201;&#39640;&#25216;&#33021;&#30340;&#12289;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;3D&#22270;&#20687;&#12290;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20174;&#22810;&#20010;&#32452;&#30340;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#32858;&#21512;&#30693;&#35782;&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21327;&#20316;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SegViz&#65292;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#23558;SegViz&#30340;&#24615;&#33021;&#19982;&#20998;&#21035;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#38598;&#20013;&#32858;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation is one of the most primary tasks in deep learning for medical imaging, owing to its multiple downstream clinical applications. However, generating manual annotations for medical images is time-consuming, requires high skill, and is an expensive effort, especially for 3D images. One potential solution is to aggregate knowledge from partially annotated datasets from multiple groups to collaboratively train global models using Federated Learning. To this end, we propose SegViz, a federated learning-based framework to train a segmentation model from distributed non-i.i.d datasets with partial annotations. The performance of SegViz was compared against training individual models separately on each dataset as well as centrally aggregating all the datasets in one place and training a single model. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#21644;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#32473;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.02830</link><description>&lt;p&gt;
&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Image Data Augmentation Approaches: A Comprehensive Survey and Future directions. (arXiv:2301.02830v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#21644;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#32473;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a comprehensive survey of advanced data augmentation techniques for computer vision tasks, including a novel taxonomy and evaluation of each technique's strengths and weaknesses. The article also presents comprehensive results of the data augmentation effect on popular computer vision tasks such as image classification, object detection, and semantic segmentation.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#65292;&#23548;&#33268;&#32593;&#32476;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#21363;&#32593;&#32476;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#27604;&#35757;&#32451;&#25968;&#25454;&#24046;&#12290;&#22240;&#27492;&#65292;&#23427;&#38480;&#21046;&#20102;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#22914;dropout&#12289;&#24402;&#19968;&#21270;&#21644;&#39640;&#32423;&#25968;&#25454;&#22686;&#24378;&#12290;&#20854;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#26088;&#22312;&#36890;&#36807;&#21253;&#25324;&#26679;&#26412;&#22810;&#26679;&#24615;&#26469;&#25193;&#22823;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#36817;&#26469;&#25104;&#20026;&#28909;&#38376;&#35805;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#39640;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#32972;&#26223;&#12289;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#12289;&#20197;&#21450;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65288;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#19977;&#20010;&#27969;&#34892;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65289;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) algorithms have shown significant performance in various computer vision tasks. However, having limited labelled data lead to a network overfitting problem, where network performance is bad on unseen data as compared to training data. Consequently, it limits performance improvement. To cope with this problem, various techniques have been proposed such as dropout, normalization and advanced data augmentation. Among these, data augmentation, which aims to enlarge the dataset size by including sample diversity, has been a hot topic in recent times. In this article, we focus on advanced data augmentation techniques. we provide a background of data augmentation, a novel and comprehensive taxonomy of reviewed data augmentation techniques, and the strengths and weaknesses (wherever possible) of each technique. We also provide comprehensive results of the data augmentation effect on three popular computer vision tasks, such as image classification, object detection and seman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#65292;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#21644;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#65292;&#20026;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25552;&#20379;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2301.00812</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-shot domain adaptation in video-based assessment of surgical skills. (arXiv:2301.00812v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#65292;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#21644;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#65292;&#20026;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25552;&#20379;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. The model successfully adapts to simulated tasks and laparoscopic cholecystectomy, providing a domain-agnostic procedure for video-based assessment of surgical skills.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#23454;&#29616;&#20102;&#25163;&#26415;&#25216;&#33021;&#30340;&#33258;&#21160;&#21644;&#23458;&#35266;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20854;&#35757;&#32451;&#39046;&#22495;&#12290;&#36825;&#38459;&#27490;&#20102;&#23427;&#20204;&#36807;&#28193;&#21040;&#25968;&#25454;&#26377;&#38480;&#30340;&#26032;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#33145;&#33108;&#38236;&#21644;&#26426;&#22120;&#20154;&#25163;&#26415;&#27169;&#25311;&#22120;&#19978;&#24320;&#21457;&#20102;A-VBANet&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#30340;&#25163;&#26415;&#23460;&#35270;&#39057;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;99.5%&#65288;&#19968;&#27425;&#24615;&#65289;&#21644;99.9%&#65288;&#23569;&#37327;&#26679;&#26412;&#65289;&#65292;&#22312;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#20013;&#30340;&#20934;&#30830;&#29575;&#20026;89.7%&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#30340;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#24433;&#21709;&#26159;&#23427;&#20801;&#35768;&#20351;&#29992;&#26469;&#33258;&#25163;&#26415;&#27169;&#25311;&#22120;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#25163;&#26415;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) has achieved automatic and objective assessment of surgical skills. However, DL models are data-hungry and restricted to their training domain. This prevents them from transitioning to new tasks where data is limited. Hence, domain adaptation is crucial to implement DL in real life. Here, we propose a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. We develop the A-VBANet on five laparoscopic and robotic surgical simulators. Additionally, we test it on operating room (OR) videos of laparoscopic cholecystectomy. Our model successfully adapts with accuracies up to 99.5% in one-shot and 99.9% in few-shot settings for simulated tasks and 89.7% for laparoscopic cholecystectomy. For the first time, we provide a domain-agnostic procedure for video-based assessment of surgical skills. A significant implication of this approach is that it allows the use of data from surgical simulators to assess performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniDA3D&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;&#65292;&#36890;&#36807;&#35774;&#35745;&#32479;&#19968;&#30340;&#28304;&#21644;&#30446;&#26631;&#20027;&#21160;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#19977;&#32500;&#20998;&#21106;&#39046;&#22495;&#20013;&#30340;&#22810;&#20010;&#33258;&#36866;&#24212;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#37319;&#26679;&#31574;&#30053;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10390</link><description>&lt;p&gt;
UniDA3D: &#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
UniDA3D: Unified Domain Adaptive 3D Semantic Segmentation Pipeline. (arXiv:2212.10390v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniDA3D&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;&#65292;&#36890;&#36807;&#35774;&#35745;&#32479;&#19968;&#30340;&#28304;&#21644;&#30446;&#26631;&#20027;&#21160;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#19977;&#32500;&#20998;&#21106;&#39046;&#22495;&#20013;&#30340;&#22810;&#20010;&#33258;&#36866;&#24212;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#37319;&#26679;&#31574;&#30053;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes UniDA3D, a unified domain adaptive 3D semantic segmentation pipeline, which can tackle several adaptation tasks in 3D segmentation field by designing a unified source-and-target active sampling strategy, and investigates the possibility of achieving a multi-modal sampling strategy.
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#26159;&#22312;&#29616;&#25104;&#30340;&#20844;&#20849;&#22522;&#20934;&#19978;&#35757;&#32451;&#30340;&#65292;&#20294;&#24403;&#36825;&#20123;&#35757;&#32451;&#33391;&#22909;&#30340;&#27169;&#22411;&#37096;&#32626;&#21040;&#26032;&#39046;&#22495;&#26102;&#65292;&#23427;&#20204;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#35782;&#21035;&#31934;&#24230;&#19979;&#38477;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;&#65288;UniDA3D&#65289;&#65292;&#20197;&#22686;&#24378;&#24369;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24357;&#21512;&#22495;&#20043;&#38388;&#30340;&#28857;&#20998;&#24067;&#24046;&#36317;&#12290;&#19982;&#20043;&#21069;&#21482;&#20851;&#27880;&#21333;&#19968;&#33258;&#36866;&#24212;&#20219;&#21153;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;UniDA3D&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#32479;&#19968;&#30340;&#28304;&#21644;&#30446;&#26631;&#20027;&#21160;&#37319;&#26679;&#31574;&#30053;&#26469;&#35299;&#20915;&#19977;&#32500;&#20998;&#21106;&#39046;&#22495;&#20013;&#30340;&#22810;&#20010;&#33258;&#36866;&#24212;&#20219;&#21153;&#65292;&#35813;&#31574;&#30053;&#20174;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23376;&#38598;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#27169;&#22411;&#33258;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#22810;&#27169;&#24577;&#20108;&#32500;-&#19977;&#32500;&#25968;&#25454;&#38598;&#30340;&#23835;&#36215;&#30340;&#24433;&#21709;&#65292;UniDA3D&#25506;&#32034;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#37319;&#26679;&#31574;&#30053;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#24320;&#21457;&#36328;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#65292;&#21487;&#20197;&#25552;&#21462;&#20195;&#34920;&#24615;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art 3D semantic segmentation models are trained on off-the-shelf public benchmarks, but they will inevitably face the challenge of recognition accuracy drop when these well-trained models are deployed to a new domain. In this paper, we introduce a Unified Domain Adaptive 3D semantic segmentation pipeline (UniDA3D) to enhance the weak generalization ability, and bridge the point distribution gap between domains. Different from previous studies that only focus on a single adaptation task, UniDA3D can tackle several adaptation tasks in 3D segmentation field, by designing a unified source-and-target active sampling strategy, which selects a maximally-informative subset from both source and target domains for effective model adaptation. Besides, benefiting from the rise of multi-modal 2D-3D datasets, UniDA3D investigates the possibility of achieving a multi-modal sampling strategy, by developing a cross-modality feature interaction module that can extract a representative pair 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#39057;&#32454;&#35843;CLIP&#65288;ViFi-CLIP&#65289;&#22522;&#32447;&#65292;&#36890;&#36807;&#20174;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24103;&#32423;&#22788;&#29702;&#65292;&#25509;&#30528;&#36827;&#34892;&#29305;&#24449;&#27744;&#21270;&#21644;&#19982;&#30456;&#24212;&#25991;&#26412;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#21305;&#37197;&#65292;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#32423;&#21035;&#30340;CLIP&#34920;&#31034;&#36716;&#31227;&#21040;&#35270;&#39057;&#20013;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#20174;&#22270;&#20687;&#21040;&#35270;&#39057;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2212.03640</link><description>&lt;p&gt;
&#32454;&#35843;CLIP&#27169;&#22411;&#26159;&#39640;&#25928;&#30340;&#35270;&#39057;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fine-tuned CLIP Models are Efficient Video Learners. (arXiv:2212.03640v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#39057;&#32454;&#35843;CLIP&#65288;ViFi-CLIP&#65289;&#22522;&#32447;&#65292;&#36890;&#36807;&#20174;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24103;&#32423;&#22788;&#29702;&#65292;&#25509;&#30528;&#36827;&#34892;&#29305;&#24449;&#27744;&#21270;&#21644;&#19982;&#30456;&#24212;&#25991;&#26412;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#21305;&#37197;&#65292;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#32423;&#21035;&#30340;CLIP&#34920;&#31034;&#36716;&#31227;&#21040;&#35270;&#39057;&#20013;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#20174;&#22270;&#20687;&#21040;&#35270;&#39057;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline, which effectively transfers image-level CLIP representations to videos by frame-level processing from CLIP image-encoder followed by feature pooling and similarity matching with corresponding text embeddings, thus bridging the domain gap from images to videos.
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35757;&#32451;&#65292;CLIP&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30001;&#20110;&#22312;&#31867;&#20284;&#35268;&#27169;&#19978;&#23545;&#35270;&#39057;&#36827;&#34892;&#35757;&#32451;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#23558;&#22522;&#20110;&#22270;&#20687;&#30340;CLIP&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#35270;&#39057;&#39046;&#22495;&#12290;&#22312;&#36825;&#20010;&#36861;&#27714;&#20013;&#65292;&#28155;&#21152;&#20102;&#26032;&#30340;&#21442;&#25968;&#27169;&#22359;&#26469;&#23398;&#20064;&#26102;&#38388;&#20449;&#24687;&#21644;&#24103;&#38388;&#20851;&#31995;&#65292;&#36825;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#24403;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#35270;&#39057;&#19978;&#36827;&#34892;&#23398;&#20064;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#36807;&#24230;&#25311;&#21512;&#32473;&#23450;&#30340;&#20219;&#21153;&#20998;&#24067;&#65292;&#24182;&#19988;&#32570;&#20047;&#27867;&#21270;&#26041;&#38754;&#12290;&#36825;&#24341;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#32423;&#21035;&#30340;CLIP&#34920;&#31034;&#36716;&#31227;&#21040;&#35270;&#39057;&#20013;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#39057;&#32454;&#35843;CLIP&#65288;ViFi-CLIP&#65289;&#22522;&#32447;&#36890;&#24120;&#36275;&#20197;&#24357;&#21512;&#20174;&#22270;&#20687;&#21040;&#35270;&#39057;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#20174;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24103;&#32423;&#22788;&#29702;&#65292;&#25509;&#30528;&#36827;&#34892;&#29305;&#24449;&#27744;&#21270;&#21644;&#19982;&#30456;&#24212;&#25991;&#26412;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#21305;&#37197;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale multi-modal training with image-text pairs imparts strong generalization to CLIP model. Since training on a similar scale for videos is infeasible, recent approaches focus on the effective transfer of image-based CLIP to the video domain. In this pursuit, new parametric modules are added to learn temporal information and inter-frame relationships which require meticulous design efforts. Furthermore, when the resulting models are learned on videos, they tend to overfit on the given task distribution and lack in generalization aspect. This begs the following question: How to effectively transfer image-level CLIP representations to videos? In this work, we show that a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient to bridge the domain gap from images to videos. Our qualitative analysis illustrates that the frame-level processing from CLIP image-encoder followed by feature pooling and similarity matching with corresponding text embeddings helps in imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#20248;&#21270;&#28304;&#29305;&#24449;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#23558;&#20854;&#24341;&#23548;&#21040;&#30446;&#26631;&#25991;&#26412;&#23884;&#20837;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20869;&#23481;&#21644;&#35821;&#20041;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;CLIP&#30340;&#39118;&#26684;&#36716;&#31227;&#22522;&#32447;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2212.03241</link><description>&lt;p&gt;
P{\O}DA: &#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
P{\O}DA: Prompt-driven Zero-shot Domain Adaptation. (arXiv:2212.03241v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#20248;&#21270;&#28304;&#29305;&#24449;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#23558;&#20854;&#24341;&#23548;&#21040;&#30446;&#26631;&#25991;&#26412;&#23884;&#20837;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20869;&#23481;&#21644;&#35821;&#20041;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;CLIP&#30340;&#39118;&#26684;&#36716;&#31227;&#22522;&#32447;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a prompt-driven zero-shot domain adaptation method, which leverages a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Experiments demonstrate that the method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand.
&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20173;&#38656;&#35201;&#22312;&#35757;&#32451;&#26102;&#35775;&#38382;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#22312;&#26576;&#20123;&#19981;&#24120;&#35265;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#8221;&#20219;&#21153;&#65292;&#20854;&#20013;&#25105;&#20204;&#20165;&#20351;&#29992;&#30446;&#26631;&#22495;&#30340;&#21333;&#20010;&#36890;&#29992;&#25991;&#26412;&#25551;&#36848;&#65288;&#21363;&#25552;&#31034;&#65289;&#26469;&#35843;&#25972;&#22312;&#28304;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#20248;&#21270;&#28304;&#29305;&#24449;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#23558;&#20854;&#24341;&#23548;&#21040;&#30446;&#26631;&#25991;&#26412;&#23884;&#20837;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20869;&#23481;&#21644;&#35821;&#20041;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22686;&#24378;&#30340;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;CLIP&#30340;&#39118;&#26684;&#36716;&#31227;&#22522;&#32447;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#19968;&#27425;&#24615;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#24182;&#19988;gi
&lt;/p&gt;
&lt;p&gt;
Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of `Prompt-driven Zero-shot Domain Adaptation', where we adapt a model trained on a source domain using only a single general textual description of the target domain, i.e., a prompt. First, we leverage a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Second, we show that augmented features can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand. Our prompt-driven approach even outperforms one-shot unsupervised domain adaptation on some datasets, and gi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;</title><link>http://arxiv.org/abs/2212.02623</link><description>&lt;p&gt;
&#32479;&#19968;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unifying Vision, Text, and Layout for Universal Document Processing. (arXiv:2212.02623v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the Universal Document Processing (UDOP) model, which unifies text, image, and layout modalities together with varied task formats, and achieves pretraining and multi-domain downstream tasks unification through a novel Transformer model. It also achieves high-quality neural document editing and content customization.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#25991;&#26723;AI&#27169;&#22411;&#65292;&#23427;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#65288;&#21253;&#25324;&#25991;&#26723;&#29702;&#35299;&#21644;&#29983;&#25104;&#65289;&#32479;&#19968;&#36215;&#26469;&#12290;UDOP&#21033;&#29992;&#25991;&#26412;&#20869;&#23481;&#21644;&#25991;&#26723;&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#26469;&#24314;&#27169;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#24577;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Vision-Text-Layout Transformer&#65292;UDOP&#23558;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#32479;&#19968;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#24207;&#21015;&#29983;&#25104;&#26041;&#26696;&#20013;&#12290;UDOP&#22312;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#22810;&#26679;&#21270;&#26631;&#35760;&#25968;&#25454;&#19978;&#20351;&#29992;&#21019;&#26032;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;UDOP&#36824;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#37325;&#24314;&#23398;&#20064;&#20174;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#24577;&#29983;&#25104;&#25991;&#26723;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#25991;&#26723;AI&#39046;&#22495;&#20013;&#31532;&#19968;&#27425;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;8&#20010;&#25991;&#26723;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 8 Docu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#25958;&#29004;&#22721;&#30011;&#36718;&#24275;&#29983;&#25104;&#32593;&#32476;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24863;&#21463;&#37326;&#25193;&#22823;&#26102;&#20002;&#22833;&#23616;&#37096;&#32454;&#33410;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20197;&#29983;&#25104;&#21512;&#29702;&#30340;&#22721;&#30011;&#36718;&#24275;&#22270;&#12290;</title><link>http://arxiv.org/abs/2212.00935</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#25958;&#29004;&#22721;&#30011;&#36718;&#24275;&#29983;&#25104;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dunhuang murals contour generation network based on convolution and self-attention fusion. (arXiv:2212.00935v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#25958;&#29004;&#22721;&#30011;&#36718;&#24275;&#29983;&#25104;&#32593;&#32476;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24863;&#21463;&#37326;&#25193;&#22823;&#26102;&#20002;&#22833;&#23616;&#37096;&#32454;&#33410;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20197;&#29983;&#25104;&#21512;&#29702;&#30340;&#22721;&#30011;&#36718;&#24275;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Dunhuang murals contour generation network based on convolution and self-attention fusion, aiming to solve the problem of losing local detail information when the receptive field is enlarged in traditional convolutional neural networks, in order to generate reasonable mural contour drawings.
&lt;/p&gt;
&lt;p&gt;
&#25958;&#29004;&#22721;&#30011;&#26159;&#20013;&#22269;&#39118;&#26684;&#21644;&#27665;&#26063;&#39118;&#26684;&#30340;&#38598;&#21512;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#20013;&#22269;&#24335;&#20315;&#25945;&#33402;&#26415;&#12290;&#23427;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#21382;&#21490;&#21644;&#25991;&#21270;&#20215;&#20540;&#20197;&#21450;&#30740;&#31350;&#24847;&#20041;&#12290;&#20854;&#20013;&#65292;&#25958;&#29004;&#22721;&#30011;&#30340;&#32447;&#26465;&#39640;&#24230;&#27010;&#25324;&#21644;&#34920;&#29616;&#21147;&#24378;&#65292;&#21453;&#26144;&#20102;&#35282;&#33394;&#29420;&#29305;&#30340;&#24615;&#26684;&#21644;&#22797;&#26434;&#30340;&#20869;&#24515;&#24773;&#24863;&#12290;&#22240;&#27492;&#65292;&#22721;&#30011;&#30340;&#36718;&#24275;&#22270;&#23545;&#20110;&#25958;&#29004;&#25991;&#21270;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25958;&#29004;&#22721;&#30011;&#30340;&#36718;&#24275;&#29983;&#25104;&#23646;&#20110;&#22270;&#20687;&#36793;&#32536;&#26816;&#27979;&#65292;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#37325;&#35201;&#20998;&#25903;&#65292;&#26088;&#22312;&#25552;&#21462;&#22270;&#20687;&#20013;&#26174;&#33879;&#30340;&#36718;&#24275;&#20449;&#24687;&#12290;&#34429;&#28982;&#22522;&#20110;&#21367;&#31215;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#36890;&#36807;&#25506;&#32034;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#29305;&#24449;&#22312;&#22270;&#20687;&#36793;&#32536;&#25552;&#21462;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#38543;&#30528;&#24863;&#21463;&#37326;&#30340;&#25193;&#22823;&#65292;&#19968;&#20123;&#23616;&#37096;&#32454;&#33410;&#20449;&#24687;&#20250;&#20002;&#22833;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#29983;&#25104;&#21512;&#29702;&#30340;&#22721;&#30011;&#36718;&#24275;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#25958;&#29004;&#22721;&#30011;&#36718;&#24275;&#29983;&#25104;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dunhuang murals are a collection of Chinese style and national style, forming a self-contained Chinese-style Buddhist art. It has very high historical and cultural value and research significance. Among them, the lines of Dunhuang murals are highly general and expressive. It reflects the character's distinctive character and complex inner emotions. Therefore, the outline drawing of murals is of great significance to the research of Dunhuang Culture. The contour generation of Dunhuang murals belongs to image edge detection, which is an important branch of computer vision, aims to extract salient contour information in images. Although convolution-based deep learning networks have achieved good results in image edge extraction by exploring the contextual and semantic features of images. However, with the enlargement of the receptive field, some local detail information is lost. This makes it impossible for them to generate reasonable outline drawings of murals. In this paper, we propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#20998;&#31867;&#27169;&#22359;&#20316;&#20026;&#29616;&#26377;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30340;&#26032;&#38454;&#27573;&#28155;&#21152;&#65292;&#29992;&#20110;&#25913;&#21892;&#25163;&#26415;&#22120;&#26800;&#23454;&#20363;&#20998;&#21106;&#20013;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#27169;&#22359;&#21253;&#25324;&#22810;&#23610;&#24230;&#25513;&#27169;&#27880;&#24847;&#21147;&#65292;&#29992;&#20110;&#20851;&#27880;&#22120;&#26800;&#21306;&#22495;&#24182;&#25513;&#30422;&#20998;&#25955;&#30340;&#32972;&#26223;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2211.16200</link><description>&lt;p&gt;
&#20174;&#21449;&#23376;&#21040;&#38067;&#23376;&#65306;&#19968;&#31181;&#26032;&#30340;&#25163;&#26415;&#22120;&#26800;&#23454;&#20363;&#20998;&#21106;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From Forks to Forceps: A New Framework for Instance Segmentation of Surgical Instruments. (arXiv:2211.16200v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#20998;&#31867;&#27169;&#22359;&#20316;&#20026;&#29616;&#26377;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30340;&#26032;&#38454;&#27573;&#28155;&#21152;&#65292;&#29992;&#20110;&#25913;&#21892;&#25163;&#26415;&#22120;&#26800;&#23454;&#20363;&#20998;&#21106;&#20013;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#27169;&#22359;&#21253;&#25324;&#22810;&#23610;&#24230;&#25513;&#27169;&#27880;&#24847;&#21147;&#65292;&#29992;&#20110;&#20851;&#27880;&#22120;&#26800;&#21306;&#22495;&#24182;&#25513;&#30422;&#20998;&#25955;&#30340;&#32972;&#26223;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#21019;&#25163;&#26415;&#21644;&#30456;&#20851;&#24212;&#29992;&#38656;&#35201;&#22312;&#23454;&#20363;&#32423;&#21035;&#19978;&#23545;&#25163;&#26415;&#24037;&#20855;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#21106;&#12290;&#25163;&#26415;&#24037;&#20855;&#22312;&#22806;&#35266;&#19978;&#30456;&#20284;&#65292;&#38271;&#32780;&#32454;&#65292;&#19988;&#20197;&#35282;&#24230;&#22788;&#29702;&#12290;&#23558;&#33258;&#28982;&#22270;&#20687;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#24494;&#35843;&#29992;&#20110;&#22120;&#26800;&#20998;&#21106;&#26102;&#65292;&#24448;&#24448;&#38590;&#20197;&#21306;&#20998;&#22120;&#26800;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#34429;&#28982;&#36793;&#30028;&#26694;&#21644;&#20998;&#21106;&#25513;&#27169;&#36890;&#24120;&#20934;&#30830;&#65292;&#20294;&#20998;&#31867;&#22836;&#35823;&#20998;&#31867;&#20102;&#25163;&#26415;&#22120;&#26800;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#20998;&#31867;&#27169;&#22359;&#20316;&#20026;&#29616;&#26377;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30340;&#26032;&#38454;&#27573;&#28155;&#21152;&#12290;&#35813;&#27169;&#22359;&#19987;&#38376;&#29992;&#20110;&#25913;&#21892;&#29616;&#26377;&#27169;&#22411;&#29983;&#25104;&#30340;&#22120;&#26800;&#25513;&#27169;&#30340;&#20998;&#31867;&#12290;&#35813;&#27169;&#22359;&#21253;&#25324;&#22810;&#23610;&#24230;&#25513;&#27169;&#27880;&#24847;&#21147;&#65292;&#35813;&#27880;&#24847;&#21147;&#20851;&#27880;&#22120;&#26800;&#21306;&#22495;&#24182;&#25513;&#30422;&#20998;&#25955;&#30340;&#32972;&#26223;&#29305;&#24449;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#24230;&#37327;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#31867;&#22120;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimally invasive surgeries and related applications demand surgical tool classification and segmentation at the instance level. Surgical tools are similar in appearance and are long, thin, and handled at an angle. The fine-tuning of state-of-the-art (SOTA) instance segmentation models trained on natural images for instrument segmentation has difficulty discriminating instrument classes. Our research demonstrates that while the bounding box and segmentation mask are often accurate, the classification head mis-classifies the class label of the surgical instrument. We present a new neural network framework that adds a classification module as a new stage to existing instance segmentation models. This module specializes in improving the classification of instrument masks generated by the existing model. The module comprises multi-scale mask attention, which attends to the instrument region and masks the distracting background features. We propose training our classifier module using metr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;VideoFACT&#65292;&#23427;&#21033;&#29992;&#21462;&#35777;&#23884;&#20837;&#12289;&#19978;&#19979;&#25991;&#23884;&#20837;&#21644;&#28145;&#24230;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#26816;&#27979;&#21644;&#23450;&#20301;&#21508;&#31181;&#35270;&#39057;&#20266;&#36896;&#21644;&#25805;&#32437;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#22312;&#20998;&#26512;&#35270;&#39057;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.15775</link><description>&lt;p&gt;
VideoFACT: &#20351;&#29992;&#27880;&#24847;&#21147;&#12289;&#22330;&#26223;&#19978;&#19979;&#25991;&#21644;&#21462;&#35777;&#30165;&#36857;&#26816;&#27979;&#35270;&#39057;&#20266;&#36896;
&lt;/p&gt;
&lt;p&gt;
VideoFACT: Detecting Video Forgeries Using Attention, Scene Context, and Forensic Traces. (arXiv:2211.15775v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;VideoFACT&#65292;&#23427;&#21033;&#29992;&#21462;&#35777;&#23884;&#20837;&#12289;&#19978;&#19979;&#25991;&#23884;&#20837;&#21644;&#28145;&#24230;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#26816;&#27979;&#21644;&#23450;&#20301;&#21508;&#31181;&#35270;&#39057;&#20266;&#36896;&#21644;&#25805;&#32437;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#22312;&#20998;&#26512;&#35270;&#39057;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new network, VideoFACT, which utilizes forensic embeddings, context embeddings, and a deep self-attention mechanism to detect and localize a wide variety of video forgeries and manipulations, overcoming challenges faced by existing networks when analyzing videos.
&lt;/p&gt;
&lt;p&gt;
&#20551;&#35270;&#39057;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#35823;&#23548;&#23041;&#32961;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#21462;&#35777;&#32593;&#32476;&#24050;&#32463;&#22312;&#22270;&#20687;&#20266;&#36896;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26368;&#36817;&#22312;Adobe VideoSham&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#32593;&#32476;&#26080;&#27861;&#35782;&#21035;&#35270;&#39057;&#20013;&#30340;&#34394;&#20551;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26159;&#30001;&#20110;&#35270;&#39057;&#32534;&#30721;&#24341;&#20837;&#20102;&#21462;&#35777;&#30165;&#36857;&#30340;&#23616;&#37096;&#21464;&#21270;&#12290;&#20026;&#20102;&#24212;&#23545;&#29616;&#26377;&#32593;&#32476;&#22312;&#20998;&#26512;&#35270;&#39057;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#21033;&#29992;&#21462;&#35777;&#23884;&#20837;&#26469;&#25429;&#25417;&#25805;&#32437;&#30041;&#19979;&#30340;&#30165;&#36857;&#65292;&#19978;&#19979;&#25991;&#23884;&#20837;&#26469;&#25511;&#21046;&#35270;&#39057;&#32534;&#30721;&#24341;&#20837;&#30340;&#21462;&#35777;&#30165;&#36857;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#28145;&#24230;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#20272;&#35745;&#23616;&#37096;&#21462;&#35777;&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#20960;&#20010;&#26032;&#30340;&#35270;&#39057;&#20266;&#36896;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#20197;&#21450;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fake videos represent an important misinformation threat. While existing forensic networks have demonstrated strong performance on image forgeries, recent results reported on the Adobe VideoSham dataset show that these networks fail to identify fake content in videos. In this paper, we show that this is due to video coding, which introduces local variation into forensic traces. In response, we propose VideoFACT - a new network that is able to detect and localize a wide variety of video forgeries and manipulations. To overcome challenges that existing networks face when analyzing videos, our network utilizes both forensic embeddings to capture traces left by manipulation, context embeddings to control for variation in forensic traces introduced by video coding, and a deep self-attention mechanism to estimate the quality and relative importance of local forensic embeddings. We create several new video forgery datasets and use these, along with publicly available data, to experimentally e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#24352;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SinFusion&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#30340;&#36816;&#21160;&#21644;&#21160;&#24577;&#65292;&#29983;&#25104;&#30456;&#21516;&#21160;&#24577;&#22330;&#26223;&#30340;&#22810;&#26679;&#21270;&#26032;&#35270;&#39057;&#26679;&#26412;&#65292;&#23558;&#30701;&#35270;&#39057;&#25512;&#24191;&#20026;&#38271;&#35270;&#39057;&#65288;&#21521;&#21069;&#21644;&#21521;&#21518;&#65289;&#24182;&#25191;&#34892;&#35270;&#39057;&#19978;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2211.11743</link><description>&lt;p&gt;
SinFusion&#65306;&#22312;&#21333;&#24352;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SinFusion: Training Diffusion Models on a Single Image or Video. (arXiv:2211.11743v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#24352;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SinFusion&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#30340;&#36816;&#21160;&#21644;&#21160;&#24577;&#65292;&#29983;&#25104;&#30456;&#21516;&#21160;&#24577;&#22330;&#26223;&#30340;&#22810;&#26679;&#21270;&#26032;&#35270;&#39057;&#26679;&#26412;&#65292;&#23558;&#30701;&#35270;&#39057;&#25512;&#24191;&#20026;&#38271;&#35270;&#39057;&#65288;&#21521;&#21069;&#21644;&#21521;&#21518;&#65289;&#24182;&#25191;&#34892;&#35270;&#39057;&#19978;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#36229;&#36807;&#20102;GAN&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#26159;&#22312;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#19981;&#33258;&#28982;&#22320;&#36866;&#24212;&#20110;&#25805;&#20316;&#32473;&#23450;&#30340;&#36755;&#20837;&#22270;&#20687;&#25110;&#35270;&#39057;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#25193;&#25955;&#27169;&#22411;&#65288;SinFusion&#65289;&#23398;&#20064;&#21333;&#20010;&#22270;&#20687;&#25110;&#35270;&#39057;&#30340;&#22806;&#35266;&#21644;&#21160;&#24577;&#65292;&#21516;&#26102;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26465;&#20214;&#33021;&#21147;&#12290;&#23427;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#30340;&#36816;&#21160;&#21644;&#21160;&#24577;&#12290;&#28982;&#21518;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#30456;&#21516;&#21160;&#24577;&#22330;&#26223;&#30340;&#22810;&#26679;&#21270;&#26032;&#35270;&#39057;&#26679;&#26412;&#65292;&#23558;&#30701;&#35270;&#39057;&#25512;&#24191;&#20026;&#38271;&#35270;&#39057;&#65288;&#21521;&#21069;&#21644;&#21521;&#21518;&#65289;&#24182;&#25191;&#34892;&#35270;&#39057;&#19978;&#37319;&#26679;&#12290;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#22810;&#25968;&#37117;&#26080;&#27861;&#36890;&#36807;&#24403;&#21069;&#30340;&#35270;&#39057;&#29305;&#23450;&#29983;&#25104;&#26041;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models exhibited tremendous progress in image and video generation, exceeding GANs in quality and diversity. However, they are usually trained on very large datasets and are not naturally adapted to manipulate a given input image or video. In this paper we show how this can be resolved by training a diffusion model on a single input image or video. Our image/video-specific diffusion model (SinFusion) learns the appearance and dynamics of the single image or video, while utilizing the conditioning capabilities of diffusion models. It can solve a wide array of image/video-specific manipulation tasks. In particular, our model can learn from few frames the motion and dynamics of a single input video. It can then generate diverse new video samples of the same dynamic scene, extrapolate short videos into long ones (both forward and backward in time) and perform video upsampling. Most of these tasks are not realizable by current video-specific generation methods.
&lt;/p&gt;</description></item><item><title>LA-VocE&#26159;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#23558;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;&#30340;mel&#39057;&#35889;&#22270;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#21644;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2211.10999</link><description>&lt;p&gt;
LA-VocE: &#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#20302;&#20449;&#22122;&#27604;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders. (arXiv:2211.10999v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10999
&lt;/p&gt;
&lt;p&gt;
LA-VocE&#26159;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#23558;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;&#30340;mel&#39057;&#35889;&#22270;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#21644;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
LA-VocE is a new audio-visual speech enhancement method that uses a neural vocoder to convert mel-spectrograms predicted from noisy audio-visual speech via a transformer-based architecture into waveform audio, and is applicable to multiple languages and different levels of background noise and speech interference.
&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#26412;&#36523;&#20197;&#21450;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#21767;&#37096;&#36816;&#21160;&#20174;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#35821;&#38899;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#27604;&#20165;&#20351;&#29992;&#38899;&#39057;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28040;&#38500;&#24178;&#25200;&#35821;&#38899;&#12290;&#23613;&#31649;&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#38899;&#39057;&#35270;&#35273;&#26041;&#27861;&#20173;&#28982;&#20351;&#29992;&#39057;&#35889;&#26144;&#23556;/&#25513;&#34109;&#26469;&#37325;&#29616;&#24178;&#20928;&#30340;&#38899;&#39057;&#65292;&#36890;&#24120;&#20250;&#22312;&#29616;&#26377;&#30340;&#35821;&#38899;&#22686;&#24378;&#26550;&#26500;&#20013;&#28155;&#21152;&#35270;&#35273;&#39592;&#24178;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LA-VocE&#65292;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;mel&#39057;&#35889;&#22270;&#65292;&#28982;&#21518;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#65288;HiFi-GAN&#65289;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#12290;&#25105;&#20204;&#22312;&#25968;&#21315;&#20010;&#35828;&#35805;&#32773;&#21644;11&#31181;&#20197;&#19978;&#19981;&#21516;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#25105;&#20204;&#30340;&#27169;&#22411;&#36866;&#24212;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Audio-visual speech enhancement aims to extract clean speech from a noisy environment by leveraging not only the audio itself but also the target speaker's lip movements. This approach has been shown to yield improvements over audio-only speech enhancement, particularly for the removal of interfering speech. Despite recent advances in speech synthesis, most audio-visual approaches continue to use spectral mapping/masking to reproduce the clean audio, often resulting in visual backbones added to existing speech enhancement architectures. In this work, we propose LA-VocE, a new two-stage approach that predicts mel-spectrograms from noisy audio-visual speech via a transformer-based architecture, and then converts them into waveform audio using a neural vocoder (HiFi-GAN). We train and evaluate our framework on thousands of speakers and 11+ different languages, and study our model's ability to adapt to different levels of background noise and speech interference. Our experiments show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;HMOE&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#26631;&#31614;&#65292;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.08253</link><description>&lt;p&gt;
HMOE: &#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization. (arXiv:2211.08253v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;HMOE&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#26631;&#31614;&#65292;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel domain generalization method called HMOE, which does not rely on domain labels and is more interpretable. HMOE uses hypernetworks to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. Experimental results show that HMOE can divide mixed data and achieve better performance.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#36825;&#23601;&#26159;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#30340;&#30446;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;DG&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#21487;&#29992;&#30340;&#39046;&#22495;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DG&#26041;&#27861;&#65292;&#31216;&#20026;HMOE&#65306;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#39046;&#22495;&#26631;&#31614;&#65292;&#24182;&#19988;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#12290;MoE&#22312;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#27169;&#24335;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#23545;&#20110;DG&#38382;&#39064;&#65292;&#24322;&#36136;&#24615;&#27491;&#26159;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#32780;&#20135;&#29983;&#30340;&#12290;HMOE&#20351;&#29992;&#36229;&#32593;&#32476;&#23558;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#36825;&#20351;&#24471;&#19987;&#23478;&#21487;&#20197;&#20849;&#20139;&#26377;&#29992;&#30340;&#20803;&#30693;&#35782;&#65292;&#24182;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#20844;&#24179;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;-DomainBed&#19979;&#23558;HMOE&#19982;&#20854;&#20182;DG&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to domain shift, machine learning systems typically fail to generalize well to domains different from those of training data, which is what domain generalization (DG) aims to address. Although various DG methods have been developed, most of them lack interpretability and require domain labels that are not available in many real-world scenarios. This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does not rely on domain labels and is more interpretable. MoE proves effective in identifying heterogeneous patterns in data. For the DG problem, heterogeneity arises exactly from domain shift. HMOE uses hypernetworks taking vectors as input to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. We compare HMOE with other DG algorithms under a fair and unified benchmark-DomainBed. Our extensive experiments show that HMOE can divide mixe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;2D&#25237;&#24433;&#20174;3D MRI&#20307;&#31215;&#20013;&#39640;&#25928;&#39044;&#27979;&#33041;&#40836;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;3D CNN&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36895;&#24230;&#19978;&#26377;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25552;&#21319;&#65292;&#23545;&#20110;&#27809;&#26377;3D CNN&#26114;&#36149;GPU&#30828;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.05762</link><description>&lt;p&gt;
&#20351;&#29992;2D&#25237;&#24433;&#20174;3D MRI&#20307;&#31215;&#20013;&#39640;&#25928;&#39044;&#27979;&#33041;&#40836;
&lt;/p&gt;
&lt;p&gt;
Efficient brain age prediction from 3D MRI volumes using 2D projections. (arXiv:2211.05762v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;2D&#25237;&#24433;&#20174;3D MRI&#20307;&#31215;&#20013;&#39640;&#25928;&#39044;&#27979;&#33041;&#40836;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;3D CNN&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36895;&#24230;&#19978;&#26377;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25552;&#21319;&#65292;&#23545;&#20110;&#27809;&#26377;3D CNN&#26114;&#36149;GPU&#30828;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient method for predicting brain age from 3D MRI volumes using 2D projections, which is two orders of magnitude faster than using 3D CNNs and is important for researchers without access to expensive GPU hardware.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#20998;&#36776;&#29575;&#21307;&#23398;&#20307;&#31215;&#19978;&#20351;&#29992;3D CNN&#38750;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20687;&#33521;&#22269;&#29983;&#29289;&#24211;&#36825;&#26679;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#35813;&#24211;&#26088;&#22312;&#25195;&#25551;10&#19975;&#20010;&#21463;&#35797;&#32773;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;2D CNN&#22312;3D&#20307;&#31215;&#30340;&#20960;&#20010;2D&#25237;&#24433;&#65288;&#20195;&#34920;&#36724;&#21521;&#65292;&#30690;&#29366;&#38754;&#21644;&#20896;&#29366;&#38754;&#20999;&#29255;&#30340;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#65289;&#19978;&#36827;&#34892;&#39044;&#27979;&#33041;&#40836;&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#21512;&#29702;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#20010;GPU&#36827;&#34892;&#30340;&#19968;&#27425;&#35757;&#32451;&#26102;&#65292;20324&#20010;&#21463;&#35797;&#32773;&#38656;&#35201;20-50&#31186;&#65292;&#27604;&#23567;&#22411;3D CNN&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#27809;&#26377;3D CNN&#26114;&#36149;GPU&#30828;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using 3D CNNs on high resolution medical volumes is very computationally demanding, especially for large datasets like the UK Biobank which aims to scan 100,000 subjects. Here we demonstrate that using 2D CNNs on a few 2D projections (representing mean and standard deviation across axial, sagittal and coronal slices) of the 3D volumes leads to reasonable test accuracy when predicting the age from brain volumes. Using our approach, one training epoch with 20,324 subjects takes 20 - 50 seconds using a single GPU, which two orders of magnitude faster compared to a small 3D CNN. These results are important for researchers who do not have access to expensive GPU hardware for 3D CNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#27668;&#35937;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#28085;&#30422;&#20102;&#24863;&#30693;&#22120;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;U&#22411;&#32593;&#32476;&#31561;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.00147</link><description>&lt;p&gt;
&#25805;&#20316;&#27668;&#35937;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#25945;&#31243;&#65292;&#31532;&#20108;&#37096;&#20998;&#65306;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Tutorial for Operational Meteorology, Part II: Neural Networks and Deep Learning. (arXiv:2211.00147v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#27668;&#35937;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#28085;&#30422;&#20102;&#24863;&#30693;&#22120;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;U&#22411;&#32593;&#32476;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the application of machine learning in meteorology, specifically neural networks and deep learning. It covers methods such as perceptrons, artificial neural networks, convolutional neural networks, and U-networks.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#27668;&#35937;&#23398;&#20013;&#30340;&#24212;&#29992;&#36805;&#36895;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20351;&#29992;&#29575;&#21069;&#25152;&#26410;&#26377;&#12290;&#20026;&#20102;&#22635;&#34917;&#32570;&#20047;&#20197;&#27668;&#35937;&#23398;&#35270;&#35282;&#28085;&#30422;&#31070;&#32463;&#32593;&#32476;&#30340;&#36164;&#28304;&#65292;&#26412;&#25991;&#20197;&#24179;&#26131;&#36817;&#20154;&#30340;&#35821;&#35328;&#26684;&#24335;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#25805;&#20316;&#27668;&#35937;&#23398;&#30028;&#12290;&#36825;&#26159;&#19968;&#23545;&#26088;&#22312;&#20026;&#27668;&#35937;&#23398;&#23478;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#36164;&#28304;&#30340;&#35770;&#25991;&#20013;&#30340;&#31532;&#20108;&#31687;&#12290;&#31532;&#19968;&#31687;&#35770;&#25991;&#20391;&#37325;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;&#38543;&#26426;&#26862;&#26519;&#65289;&#65292;&#32780;&#26412;&#25991;&#21017;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#28085;&#30422;&#20102;&#24863;&#30693;&#22120;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;U&#22411;&#32593;&#32476;&#12290;&#19982;&#31532;&#19968;&#31687;&#35770;&#25991;&#19968;&#26679;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#19982;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#35757;&#32451;&#30456;&#20851;&#30340;&#26415;&#35821;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#27599;&#31181;&#26041;&#27861;&#32972;&#21518;&#30340;&#19968;&#20123;&#30452;&#35273;&#65292;&#24182;&#20197;&#23637;&#31034;&#27599;&#31181;&#26041;&#27861;&#30340;&#23454;&#20363;&#26469;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade the use of machine learning in meteorology has grown rapidly. Specifically neural networks and deep learning have been used at an unprecedented rate. In order to fill the dearth of resources covering neural networks with a meteorological lens, this paper discusses machine learning methods in a plain language format that is targeted for the operational meteorological community. This is the second paper in a pair that aim to serve as a machine learning resource for meteorologists. While the first paper focused on traditional machine learning methods (e.g., random forest), here a broad spectrum of neural networks and deep learning methods are discussed. Specifically this paper covers perceptrons, artificial neural networks, convolutional neural networks and U-networks. Like the part 1 paper, this manuscript discusses the terms associated with neural networks and their training. Then the manuscript provides some intuition behind every method and concludes by showing ea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Vision Transformer&#36827;&#34892;&#20799;&#31461;ASD&#35745;&#31639;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22823;&#22411;&#38754;&#37096;&#34920;&#24773;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#27169;&#22411;&#32467;&#26500;&#21487;&#36716;&#31227;&#24615;&#12290;&#22312;&#26631;&#20934;ASD&#38754;&#37096;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;ViTASD-L&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2210.16943</link><description>&lt;p&gt;
ViTASD&#65306;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#38754;&#37096;&#35786;&#26029;&#30340;&#24378;&#20581;&#35270;&#35273;Transformer&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
ViTASD: Robust Vision Transformer Baselines for Autism Spectrum Disorder Facial Diagnosis. (arXiv:2210.16943v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Vision Transformer&#36827;&#34892;&#20799;&#31461;ASD&#35745;&#31639;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22823;&#22411;&#38754;&#37096;&#34920;&#24773;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#27169;&#22411;&#32467;&#26500;&#21487;&#36716;&#31227;&#24615;&#12290;&#22312;&#26631;&#20934;ASD&#38754;&#37096;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;ViTASD-L&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for computational analysis of pediatric ASD using Vision Transformer, which extracts knowledge from large facial expression datasets and offers model structure transferability. Extensive experiments on standard ASD facial analysis benchmarks show that ViTASD-L achieves a new state-of-the-art.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#32456;&#36523;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#65292;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#24739;&#30149;&#29575;&#12290;&#30001;&#20110;&#32570;&#20047;&#33391;&#22909;&#30340;&#22522;&#32447;&#65292;ASD&#38754;&#37096;&#20998;&#26512;&#22312;&#20799;&#31185;&#24739;&#32773;&#20013;&#30340;&#30740;&#31350;&#36827;&#23637;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Vision Transformer&#65288;ViT&#65289;&#36827;&#34892;&#20799;&#31461;ASD&#35745;&#31639;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;ViTASD&#65292;&#20174;&#22823;&#22411;&#38754;&#37096;&#34920;&#24773;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#27169;&#22411;&#32467;&#26500;&#21487;&#36716;&#31227;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ViTASD&#37319;&#29992;&#26222;&#36890;&#30340;ViT&#20174;&#24739;&#32773;&#30340;&#38754;&#37096;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#21644;&#39640;&#26031;&#36807;&#31243;&#23618;&#26469;&#22686;&#24378;ASD&#20998;&#26512;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#26631;&#20934;ASD&#38754;&#37096;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25152;&#26377;&#20195;&#34920;&#24615;&#30340;ASD&#38754;&#37096;&#20998;&#26512;&#26041;&#27861;&#65292;&#32780;ViTASD-L&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/IrohX&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autism spectrum disorder (ASD) is a lifelong neurodevelopmental disorder with very high prevalence around the world. Research progress in the field of ASD facial analysis in pediatric patients has been hindered due to a lack of well-established baselines. In this paper, we propose the use of the Vision Transformer (ViT) for the computational analysis of pediatric ASD. The presented model, known as ViTASD, distills knowledge from large facial expression datasets and offers model structure transferability. Specifically, ViTASD employs a vanilla ViT to extract features from patients' face images and adopts a lightweight decoder with a Gaussian Process layer to enhance the robustness for ASD analysis. Extensive experiments conducted on standard ASD facial analysis benchmarks show that our method outperforms all of the representative approaches in ASD facial analysis, while the ViTASD-L achieves a new state-of-the-art. Our code and pretrained models are available at https://github.com/IrohX
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.11328</link><description>&lt;p&gt;
&#22238;&#25918;&#65306;&#36845;&#20195;&#27880;&#24847;&#21147;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Play It Back: Iterative Attention for Audio Recognition. (arXiv:2210.11328v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes an end-to-end attention-based architecture that attends over the most discriminative sounds across the audio sequence through selective repetition, achieving consistently state-of-the-art performance across three audio-classification benchmarks.
&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#35748;&#30693;&#30340;&#19968;&#20010;&#20851;&#38190;&#21151;&#33021;&#26159;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#23558;&#29305;&#24449;&#22768;&#38899;&#19982;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#32852;&#36215;&#26469;&#12290;&#20154;&#31867;&#35797;&#22270;&#21306;&#20998;&#32454;&#31890;&#24230;&#38899;&#39057;&#31867;&#21035;&#26102;&#65292;&#36890;&#24120;&#20250;&#37325;&#25773;&#30456;&#21516;&#30340;&#21306;&#20998;&#24615;&#22768;&#38899;&#20197;&#22686;&#21152;&#20854;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26368;&#21021;&#20351;&#29992;&#23436;&#25972;&#30340;&#38899;&#39057;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#25554;&#27133;&#27880;&#24847;&#21147;&#36845;&#20195;&#22320;&#32454;&#21270;&#37325;&#25773;&#30340;&#26102;&#38388;&#27573;&#12290;&#22312;&#27599;&#27425;&#25773;&#25918;&#26102;&#65292;&#25152;&#36873;&#27573;&#20351;&#29992;&#36739;&#23567;&#30340;&#36339;&#36291;&#38271;&#24230;&#37325;&#25773;&#65292;&#36825;&#20195;&#34920;&#20102;&#36825;&#20123;&#27573;&#20869;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65306;AudioSet&#12289;VGG-Sound&#21644;EPIC-KITCHENS-100&#12290;
&lt;/p&gt;
&lt;p&gt;
A key function of auditory cognition is the association of characteristic sounds with their corresponding semantics over time. Humans attempting to discriminate between fine-grained audio categories, often replay the same discriminative sounds to increase their prediction confidence. We propose an end-to-end attention-based architecture that through selective repetition attends over the most discriminative sounds across the audio sequence. Our model initially uses the full audio sequence and iteratively refines the temporal segments replayed based on slot attention. At each playback, the selected segments are replayed using a smaller hop length which represents higher resolution features within these segments. We show that our method can consistently achieve state-of-the-art performance across three audio-classification benchmarks: AudioSet, VGG-Sound, and EPIC-KITCHENS-100.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.07199</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#20960;&#20309;&#23545;&#24212;&#29992;&#20110;&#37326;&#22806;&#31867;&#21035;&#32423;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild. (arXiv:2210.07199v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a self-supervised learning approach for category-level 6D object pose estimation in the wild, which reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. The proposed novel geometrical cycle-consistency losses construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and other tasks.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#65292;&#23427;&#20173;&#28982;&#36828;&#26410;&#35299;&#20915;&#12290;&#24403;&#36716;&#21521;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#26102;&#65292;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#23545;&#26410;&#35265;&#23454;&#20363;&#36827;&#34892;&#27867;&#21270;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#21463;&#21040;&#20174;&#27169;&#25311;&#25110;&#20174;&#20154;&#31867;&#25910;&#38598;&#30340;&#27880;&#37322;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#65292;&#20811;&#26381;&#20102;&#36825;&#19968;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37325;&#26500;&#20102;&#29289;&#20307;&#31867;&#21035;&#30340;&#35268;&#33539;3D&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;&#23545;&#20110;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#65292;&#23427;&#20204;&#22312;2D-3D&#31354;&#38388;&#12289;&#19981;&#21516;&#23454;&#20363;&#21644;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#26500;&#24314;&#24490;&#29615;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While 6D object pose estimation has wide applications across computer vision and robotics, it remains far from being solved due to the lack of annotations. The problem becomes even more challenging when moving to category-level 6D pose, which requires generalization to unseen instances. Current approaches are restricted by leveraging annotations from simulation or collected from humans. In this paper, we overcome this barrier by introducing a self-supervised learning approach trained directly on large-scale real-world object videos for category-level 6D pose estimation in the wild. Our framework reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. For training, we propose novel geometrical cycle-consistency losses which construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and othe
&lt;/p&gt;</description></item><item><title>PDEBench&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#27169;&#25311;&#20219;&#21153;&#22522;&#20934;&#22871;&#20214;&#65292;&#21253;&#25324;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#23545;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#19982;&#32463;&#20856;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2210.07182</link><description>&lt;p&gt;
PDEBENCH&#65306;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
PDEBENCH: An Extensive Benchmark for Scientific Machine Learning. (arXiv:2210.07182v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07182
&lt;/p&gt;
&lt;p&gt;
PDEBench&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#27169;&#25311;&#20219;&#21153;&#22522;&#20934;&#22871;&#20214;&#65292;&#21253;&#25324;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#23545;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#19982;&#32463;&#20856;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
PDEBench is a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs), which includes code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29289;&#29702;&#31995;&#32479;&#24314;&#27169;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#32570;&#20047;&#26131;&#20110;&#20351;&#29992;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20195;&#34920;&#24615;&#30340;&#31185;&#23398;ML&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PDEBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#27169;&#25311;&#20219;&#21153;&#22522;&#20934;&#22871;&#20214;&#12290;PDEBench&#21253;&#25324;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#23545;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#19982;&#32463;&#20856;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20934;&#38382;&#39064;&#38598;&#20855;&#26377;&#20197;&#19979;&#29420;&#29305;&#29305;&#24449;&#65306;&#65288;1&#65289;&#19982;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;PDE&#30340;&#33539;&#22260;&#26356;&#24191;&#65292;&#20174;&#30456;&#23545;&#24120;&#35265;&#30340;&#31034;&#20363;&#21040;&#26356;&#29616;&#23454;&#21644;&#22256;&#38590;&#30340;&#38382;&#39064;&#65307;&#65288;2&#65289;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#20934;&#22791;&#22909;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#26356;&#22823;&#65292;&#21253;&#25324;&#36328;&#26356;&#22810;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#20197;&#21450;PDE&#21442;&#25968;&#30340;&#22810;&#20010;&#27169;&#25311;&#36816;&#34892;&#65307;&#65288;3&#65289;&#26356;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26356;&#22810;&#30340;&#24615;&#33021;&#25351;&#26631;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based modeling of physical systems has experienced increased interest in recent years. Despite some impressive progress, there is still a lack of benchmarks for Scientific ML that are easy to use but still challenging and representative of a wide range of problems. We introduce PDEBench, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs). PDEBench comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines. Our proposed set of benchmark problems contribute the following unique features: (1) A much wider range of PDEs compared to existing benchmarks, ranging from relatively common examples to more realistic and difficult problems; (2) much larger ready-to-use datasets compared to prior work, comprising multiple simulation runs across a larger number of initial and boundary conditions and PDE parameters; (3) more exte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#25110;&#27700;&#24179;&#32763;&#36716;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#23376;&#32676;&#23545;&#31216;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2210.04087</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;CNN&#25200;&#21160;&#25915;&#20987;&#30340;&#23545;&#31216;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Symmetry Defense Against CNN Adversarial Perturbation Attacks. (arXiv:2210.04087v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#25110;&#27700;&#24179;&#32763;&#36716;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#23376;&#32676;&#23545;&#31216;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a symmetry defense method to improve adversarial robustness by flipping or horizontally flipping symmetric adversarial samples, and uses subgroup symmetries for classification.
&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65288;CNN&#65289;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#20250;&#25200;&#21160;&#21407;&#22987;&#26679;&#26412;&#20197;&#27450;&#39575;&#20998;&#31867;&#22120;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#36947;&#36335;&#26631;&#24535;&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;CNN&#22312;&#23545;&#31216;&#26679;&#26412;&#30340;&#20998;&#31867;&#20013;&#20063;&#32570;&#20047;&#19981;&#21464;&#24615;&#65292;&#22240;&#20026;CNN&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#23545;&#31216;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#32771;&#34385;&#21040;CNN&#32570;&#20047;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;CNN&#32570;&#20047;&#19981;&#21464;&#24615;&#65292;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#30340;&#20998;&#31867;&#21487;&#33021;&#19982;&#20854;&#38169;&#35823;&#20998;&#31867;&#19981;&#21516;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#23545;&#31216;&#38450;&#24481;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#23545;&#25239;&#32773;&#19981;&#30693;&#36947;&#38450;&#24481;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#32763;&#36716;&#25110;&#27700;&#24179;&#32763;&#36716;&#21518;&#20877;&#36827;&#34892;&#20998;&#31867;&#12290;&#23545;&#20110;&#30693;&#36947;&#38450;&#24481;&#30340;&#23545;&#25163;&#65292;&#38450;&#24481;&#35774;&#35745;&#20102;&#19968;&#20010;Klein&#22235;&#20010;&#23545;&#31216;&#23376;&#32676;&#65292;&#20854;&#20013;&#21253;&#25324;&#27700;&#24179;&#32763;&#36716;&#21644;&#20687;&#32032;&#21453;&#36716;&#23545;&#31216;&#24615;&#12290;&#23545;&#31216;&#38450;&#24481;&#20351;&#29992;&#23376;&#32676;&#23545;&#31216;&#24615;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural network classifiers (CNNs) are susceptible to adversarial attacks that perturb original samples to fool classifiers such as an autonomous vehicle's road sign image classifier. CNNs also lack invariance in the classification of symmetric samples because CNNs can classify symmetric samples differently. Considered together, the CNN lack of adversarial robustness and the CNN lack of invariance mean that the classification of symmetric adversarial samples can differ from their incorrect classification. Could symmetric adversarial samples revert to their correct classification? This paper answers this question by designing a symmetry defense that inverts or horizontally flips adversarial samples before classification against adversaries unaware of the defense. Against adversaries aware of the defense, the defense devises a Klein four symmetry subgroup that includes the horizontal flip and pixel inversion symmetries. The symmetry defense uses the subgroup symmetries in ac
&lt;/p&gt;</description></item><item><title>ParaGon&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#21644;&#35270;&#35273;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#20026;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#21333;&#29420;&#23450;&#20301;&#23545;&#35937;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#29702;&#20851;&#20110;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29289;&#20307;&#25918;&#32622;&#12290;</title><link>http://arxiv.org/abs/2210.00215</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#21644;&#35270;&#35273;&#23450;&#20301;&#22312;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentiable Parsing and Visual Grounding of Natural Language Instructions for Object Placement. (arXiv:2210.00215v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00215
&lt;/p&gt;
&lt;p&gt;
ParaGon&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#21644;&#35270;&#35273;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#20026;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#21333;&#29420;&#23450;&#20301;&#23545;&#35937;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#29702;&#20851;&#20110;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29289;&#20307;&#25918;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
ParaGon is a differentiable method for natural language instruction parsing and visual grounding in object placement tasks. It parses language instructions into an object-centric graph representation to ground objects individually and uses a novel particle-based graph neural network to reason about object placements with uncertainty.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;PARsing And visual GrOuNding (ParaGon)&#65292;&#29992;&#20110;&#22312;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#20013;&#23545;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23450;&#20301;&#12290;&#33258;&#28982;&#35821;&#35328;&#36890;&#24120;&#29992;&#32452;&#21512;&#24615;&#21644;&#27495;&#20041;&#24615;&#25551;&#36848;&#23545;&#35937;&#21644;&#31354;&#38388;&#20851;&#31995;&#65292;&#36825;&#26159;&#26377;&#25928;&#35821;&#35328;&#23450;&#20301;&#30340;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#32452;&#21512;&#24615;&#65292;ParaGon&#23558;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#20026;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#21333;&#29420;&#23450;&#20301;&#23545;&#35937;&#12290;&#23545;&#20110;&#27495;&#20041;&#24615;&#65292;ParaGon&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#29702;&#20851;&#20110;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29289;&#20307;&#25918;&#32622;&#12290;&#26412;&#36136;&#19978;&#65292;ParaGon&#23558;&#35299;&#26512;&#31639;&#27861;&#38598;&#25104;&#21040;&#27010;&#29575;&#30340;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#23427;&#26159;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#20174;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#20197;&#23545;&#25239;&#22797;&#26434;&#30340;&#65292;&#27169;&#31946;&#30340;&#35821;&#35328;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new method, PARsing And visual GrOuNding (ParaGon), for grounding natural language in object placement tasks. Natural language generally describes objects and spatial relations with compositionality and ambiguity, two major obstacles to effective language grounding. For compositionality, ParaGon parses a language instruction into an object-centric graph representation to ground objects individually. For ambiguity, ParaGon uses a novel particle-based graph neural network to reason about object placements with uncertainty. Essentially, ParaGon integrates a parsing algorithm into a probabilistic, data-driven learning framework. It is fully differentiable and trained end-to-end from data for robustness against complex, ambiguous language input.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30693;&#35782;&#26469;&#25552;&#39640;&#21487;&#36716;&#31227;&#30340;&#26102;&#31354;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#20851;&#27880;&#23398;&#20064;&#21040;&#30340;&#35270;&#39057;&#34920;&#31034;&#26469;&#23545;&#25171;&#20081;&#30340;ASR&#33050;&#26412;&#36827;&#34892;&#25490;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#39057;&#29702;&#35299;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2209.15280</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#30693;&#35782;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#26102;&#31354;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Transferable Spatiotemporal Representations from Natural Script Knowledge. (arXiv:2209.15280v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30693;&#35782;&#26469;&#25552;&#39640;&#21487;&#36716;&#31227;&#30340;&#26102;&#31354;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#20851;&#27880;&#23398;&#20064;&#21040;&#30340;&#35270;&#39057;&#34920;&#31034;&#26469;&#23545;&#25171;&#20081;&#30340;ASR&#33050;&#26412;&#36827;&#34892;&#25490;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#39057;&#29702;&#35299;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new pretext task to boost transferable spatiotemporal representation learning by exploiting natural language knowledge, which sorts shuffled ASR scripts by attending to learned video representations, and thus improves the progress of video understanding.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#24050;&#25104;&#20026;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#26102;&#31354;&#34920;&#31034;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#23616;&#38480;&#20110;&#39640;&#24230;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;K400&#65289;&#65292;&#24182;&#23637;&#31034;&#20986;&#20196;&#20154;&#19981;&#28385;&#24847;&#30340;&#24320;&#31665;&#21363;&#29992;&#34920;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#21482;&#25429;&#25417;&#20687;&#32032;&#32423;&#21035;&#30340;&#30693;&#35782;&#32780;&#19981;&#26159;&#26102;&#31354;&#35821;&#20041;&#65292;&#36825;&#38459;&#30861;&#20102;&#35270;&#39057;&#29702;&#35299;&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#12290;&#21463;&#21040;&#22270;&#20687;&#25991;&#26412;&#39044;&#35757;&#32451;&#65288;&#20363;&#22914;CLIP&#65289;&#30340;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#21033;&#29992;&#35821;&#35328;&#35821;&#20041;&#25552;&#39640;&#21487;&#36716;&#31227;&#30340;&#26102;&#31354;&#34920;&#31034;&#23398;&#20064;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#21363;&#8220;Turning to Video for Transcript Sorting&#65288;TVTS&#65289;&#8221;&#65292;&#36890;&#36807;&#20851;&#27880;&#23398;&#20064;&#21040;&#30340;&#35270;&#39057;&#34920;&#31034;&#26469;&#23545;&#25171;&#20081;&#30340;ASR&#33050;&#26412;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;&#25551;&#36848;&#24615;&#26631;&#39064;&#65292;&#32431;&#31929;&#20174;&#35270;&#39057;&#20013;&#23398;&#20064;&#65292;&#21363;&#21033;&#29992;&#33258;&#28982;&#36716;&#24405;&#30340;&#35821;&#38899;&#30693;&#35782;&#22312;&#26102;&#38388;&#19978;&#25552;&#20379;&#22024;&#26434;&#20294;&#26377;&#29992;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on large-scale video data has become a common recipe for learning transferable spatiotemporal representations in recent years. Despite some progress, existing methods are mostly limited to highly curated datasets (e.g., K400) and exhibit unsatisfactory out-of-the-box representations. We argue that it is due to the fact that they only capture pixel-level knowledge rather than spatiotemporal semantics, which hinders further progress in video understanding. Inspired by the great success of image-text pre-training (e.g., CLIP), we take the first step to exploit language semantics to boost transferable spatiotemporal representation learning. We introduce a new pretext task, Turning to Video for Transcript Sorting (TVTS), which sorts shuffled ASR scripts by attending to learned video representations. We do not rely on descriptive captions and learn purely from video, i.e., leveraging the natural transcribed speech knowledge to provide noisy but useful semantics over time. Our me
&lt;/p&gt;</description></item><item><title>Recipro-CAM&#26159;&#19968;&#31181;&#24555;&#36895;&#26080;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25552;&#21462;&#30340;&#29305;&#24449;&#22270;&#36827;&#34892;&#31354;&#38388;&#25513;&#34109;&#65292;&#21033;&#29992;&#28608;&#27963;&#22270;&#21644;&#30446;&#26631;&#31867;&#21035;&#30340;&#32593;&#32476;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35299;&#20915;&#20102;CAM&#21644;Grad-CAM&#26041;&#27861;&#30340;&#26550;&#26500;&#38480;&#21046;&#21644;&#26799;&#24230;&#35745;&#31639;&#36127;&#25285;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#30701;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#36866;&#29992;&#20110;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2209.14074</link><description>&lt;p&gt;
Recipro-CAM: &#22522;&#20110;&#24555;&#36895;&#26080;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recipro-CAM: Fast gradient-free visual explanations for convolutional neural networks. (arXiv:2209.14074v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14074
&lt;/p&gt;
&lt;p&gt;
Recipro-CAM&#26159;&#19968;&#31181;&#24555;&#36895;&#26080;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25552;&#21462;&#30340;&#29305;&#24449;&#22270;&#36827;&#34892;&#31354;&#38388;&#25513;&#34109;&#65292;&#21033;&#29992;&#28608;&#27963;&#22270;&#21644;&#30446;&#26631;&#31867;&#21035;&#30340;&#32593;&#32476;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35299;&#20915;&#20102;CAM&#21644;Grad-CAM&#26041;&#27861;&#30340;&#26550;&#26500;&#38480;&#21046;&#21644;&#26799;&#24230;&#35745;&#31639;&#36127;&#25285;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#30701;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#36866;&#29992;&#20110;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recipro-CAM is a fast gradient-free visual explanation method for interpretable convolutional neural networks. It solves the architectural constraints and gradient computing burden issues of CAM and Grad-CAM methods by spatially masking the extracted feature maps to exploit the correlation between activation maps and network predictions for target classes, with shorter execution time and practical applicability.
&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#40657;&#30418;&#26412;&#36136;&#20351;&#24471;&#38590;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;AI&#20174;&#19994;&#32773;&#25506;&#32034;&#20102;&#21487;&#35299;&#37322;&#24615;AI&#26041;&#27861;&#65292;&#22914;Class Activation Map&#65288;CAM&#65289;&#21644;Grad-CAM&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#26550;&#26500;&#38480;&#21046;&#25110;&#26799;&#24230;&#35745;&#31639;&#36127;&#25285;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Score-CAM&#21644;Ablation-CAM&#20316;&#20026;&#26080;&#26799;&#24230;&#26041;&#27861;&#65292;&#20294;&#19982;&#22522;&#20110;CAM&#25110;Grad-CAM&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20204;&#20855;&#26377;&#26356;&#38271;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#21512;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#65292;&#23613;&#31649;&#23427;&#20204;&#35299;&#20915;&#20102;&#26799;&#24230;&#30456;&#20851;&#38382;&#39064;&#24182;&#21551;&#29992;&#20102;&#25512;&#29702;&#27169;&#24335;XAI&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26080;&#26799;&#24230;&#30340;Recipro-CAM&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#25552;&#21462;&#30340;&#29305;&#24449;&#22270;&#36827;&#34892;&#31354;&#38388;&#25513;&#34109;&#65292;&#20197;&#21033;&#29992;&#28608;&#27963;&#22270;&#21644;&#30446;&#26631;&#31867;&#21035;&#30340;&#32593;&#32476;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Convolutional Neural Network (CNN) is a widely used deep learning architecture for computer vision. However, its black box nature makes it difficult to interpret the behavior of the model. To mitigate this issue, AI practitioners have explored explainable AI methods like Class Activation Map (CAM) and Grad-CAM. Although these methods have shown promise, they are limited by architectural constraints or the burden of gradient computing. To overcome this issue, Score-CAM and Ablation-CAM have been proposed as gradient-free methods, but they have longer execution times compared to CAM or Grad-CAM based methods, making them unsuitable for real-world solution though they resolved gradient related issues and enabled inference mode XAI. To address this challenge, we propose a fast gradient-free Reciprocal CAM (Recipro-CAM) method. Our approach involves spatially masking the extracted feature maps to exploit the correlation between activation maps and network predictions for target classes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#32954;&#27963;&#26816;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#23545;EGFR&#31361;&#21464;&#30340;&#39044;&#27979;&#65292;&#20026;&#32954;&#30284;&#27835;&#30103;&#25552;&#20379;&#20102;&#26356;&#32463;&#27982;&#12289;&#26356;&#24555;&#25463;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.12506</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#32954;&#27963;&#26816;&#22270;&#20687;&#20013;&#30340;EGFR&#31361;&#21464;
&lt;/p&gt;
&lt;p&gt;
EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning. (arXiv:2208.12506v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#32954;&#27963;&#26816;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#23545;EGFR&#31361;&#21464;&#30340;&#39044;&#27979;&#65292;&#20026;&#32954;&#30284;&#27835;&#30103;&#25552;&#20379;&#20102;&#26356;&#32463;&#27982;&#12289;&#26356;&#24555;&#25463;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper uses deep learning technology to predict EGFR mutations through analysis of lung biopsy images, providing a more economical and faster diagnostic method for lung cancer treatment.
&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#27835;&#30103;&#30340;&#26631;&#20934;&#35786;&#26029;&#31243;&#24207;&#28041;&#21450;&#32452;&#32455;&#23398;&#20122;&#22411;&#20998;&#22411;&#21644;&#38543;&#21518;&#30340;&#20851;&#38190;&#39537;&#21160;&#22522;&#22240;&#31361;&#21464;&#26816;&#27979;&#65292;&#22914;EGFR&#12290;&#23613;&#31649;&#20998;&#23376;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;&#39537;&#21160;&#22522;&#22240;&#31361;&#21464;&#65292;&#20294;&#35813;&#36807;&#31243;&#36890;&#24120;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#32463;&#27982;&#30340;&#36873;&#25321;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#20013;&#21457;&#29616;&#39537;&#21160;&#22522;&#22240;&#31361;&#21464;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#36827;&#34892;&#24369;&#30417;&#30563;&#65292;&#20197;&#35782;&#21035;hematoxylin&#21644;eosin&#26579;&#33394;&#30340;WSIs&#20013;EGFR&#31361;&#21464;&#30340;&#24418;&#24577;&#23398;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#26816;&#27979;&#32959;&#30244;&#21644;&#32452;&#32455;&#23398;&#20122;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#32954;&#30284;&#25968;&#25454;&#38598;&#65288;TCGA&#21644;&#26469;&#33258;&#21360;&#24230;&#30340;&#31169;&#20154;&#25968;&#25454;&#38598;&#65289;&#19978;&#36827;&#34892;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#26469;&#35777;&#26126;&#25105;&#20204;&#31649;&#36947;&#30340;&#26377;&#25928;&#24615;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31649;&#36947;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#32959;&#30244;&#26816;&#27979;&#30340;&#24179;&#22343;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20026;0.964&#65292;&#32452;&#32455;&#23398;&#20122;&#22411;&#20026;0.942&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard diagnostic procedures for targeted therapies in lung cancer treatment involve histological subtyping and subsequent detection of key driver mutations, such as EGFR. Even though molecular profiling can uncover the driver mutation, the process is often expensive and time-consuming. Deep learning-oriented image analysis offers a more economical alternative for discovering driver mutations directly from whole slide images (WSIs). In this work, we used customized deep learning pipelines with weak supervision to identify the morphological correlates of EGFR mutation from hematoxylin and eosin-stained WSIs, in addition to detecting tumor and histologically subtyping it. We demonstrate the effectiveness of our pipeline by conducting rigorous experiments and ablation studies on two lung cancer datasets - TCGA and a private dataset from India. With our pipeline, we achieved an average area under the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological subtyping betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#38024;&#23545;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#39046;&#22495;&#28418;&#31227;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65292;&#26080;&#38656;&#26631;&#31614;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;CoTTA&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.08767</link><description>&lt;p&gt;
&#35780;&#20272;&#38024;&#23545;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#39046;&#22495;&#28418;&#31227;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Continual Test-Time Adaptation for Contextual and Semantic Domain Shifts. (arXiv:2208.08767v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#38024;&#23545;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#39046;&#22495;&#28418;&#31227;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65292;&#26080;&#38656;&#26631;&#31614;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;CoTTA&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper evaluates continual test-time adaptation for contextual and semantic domain shifts without labels. The study finds that Continual Test-Time Adaptation (CoTTA) is an effective method.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#36830;&#32493;&#36866;&#24212;&#39046;&#22495;&#28418;&#31227;&#65292;&#26080;&#38656;&#26631;&#31614;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#22914;&#39044;&#27979;&#26102;&#38388;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#12289;&#27979;&#35797;&#29109;&#26368;&#23567;&#21270;&#65288;TENT&#65289;&#21644;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;CoTTA&#65289;&#65292;&#24182;&#22312;&#20004;&#20010;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#28418;&#31227;&#28304;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, our goal is to adapt a pre-trained convolutional neural network to domain shifts at test time. We do so continually with the incoming stream of test batches, without labels. The existing literature mostly operates on artificial shifts obtained via adversarial perturbations of a test image. Motivated by this, we evaluate the state of the art on two realistic and challenging sources of domain shifts, namely contextual and semantic shifts. Contextual shifts correspond to the environment types, for example, a model pre-trained on indoor context has to adapt to the outdoor context on CORe-50. Semantic shifts correspond to the capture types, for example a model pre-trained on natural images has to adapt to cliparts, sketches, and paintings on DomainNet. We include in our analysis recent techniques such as Prediction-Time Batch Normalization (BN), Test Entropy Minimization (TENT) and Continual Test-Time Adaptation (CoTTA). Our findings are three-fold: i) Test-time adaptation me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#31232;&#30095;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;LSIMVC&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#31232;&#30095;&#27491;&#21017;&#21270;&#21644;&#26032;&#39062;&#30340;&#22270;&#23884;&#20837;&#22810;&#35270;&#22270;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#20174;&#19981;&#23436;&#25972;&#30340;&#22810;&#35270;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#31232;&#30095;&#21644;&#32467;&#26500;&#21270;&#30340;&#20849;&#35782;&#28508;&#22312;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.02998</link><description>&lt;p&gt;
&#23616;&#37096;&#31232;&#30095;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Localized Sparse Incomplete Multi-view Clustering. (arXiv:2208.02998v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#31232;&#30095;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;LSIMVC&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#31232;&#30095;&#27491;&#21017;&#21270;&#21644;&#26032;&#39062;&#30340;&#22270;&#23884;&#20837;&#22810;&#35270;&#22270;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#20174;&#19981;&#23436;&#25972;&#30340;&#22810;&#35270;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#31232;&#30095;&#21644;&#32467;&#26500;&#21270;&#30340;&#20849;&#35782;&#28508;&#22312;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method named Localized Sparse Incomplete Multi-view Clustering (LSIMVC) to learn a sparse and structured consensus latent representation from incomplete multi-view data by optimizing a sparse regularized and novel graph embedded multi-view matrix factorization model, which aims to solve the clustering problem on incomplete multi-view data with partial view missing.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#22312;&#35299;&#20915;&#37096;&#20998;&#35270;&#22270;&#32570;&#22833;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#25968;&#25454;&#32858;&#31867;&#38382;&#39064;&#26041;&#38754;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#35201;&#20040;&#19981;&#33021;&#28789;&#27963;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#32570;&#22833;&#35270;&#22270;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#25968;&#25454;&#65292;&#35201;&#20040;&#19981;&#32771;&#34385;&#35270;&#22270;&#20043;&#38388;&#20449;&#24687;&#19981;&#24179;&#34913;&#30340;&#36127;&#38754;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#25506;&#32034;&#25152;&#26377;&#19981;&#23436;&#25972;&#35270;&#22270;&#30340;&#23616;&#37096;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23616;&#37096;&#31232;&#30095;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;LSIMVC&#65289;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;LSIMVC&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#31232;&#30095;&#27491;&#21017;&#21270;&#21644;&#26032;&#39062;&#30340;&#22270;&#23884;&#20837;&#22810;&#35270;&#22270;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#26469;&#20174;&#19981;&#23436;&#25972;&#30340;&#22810;&#35270;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#31232;&#30095;&#21644;&#32467;&#26500;&#21270;&#30340;&#20849;&#35782;&#28508;&#22312;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#26032;&#39062;&#27169;&#22411;&#20013;&#65292;&#22522;&#20110;l1&#33539;&#25968;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#34987;&#29992;&#20110;&#23398;&#20064;&#23616;&#37096;&#32467;&#26500;&#65292;&#20197;&#21450;&#26032;&#39062;&#30340;&#22270;&#23884;&#20837;&#34987;&#29992;&#20110;&#22788;&#29702;&#35270;&#22270;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incomplete multi-view clustering, which aims to solve the clustering problem on the incomplete multi-view data with partial view missing, has received more and more attention in recent years. Although numerous methods have been developed, most of the methods either cannot flexibly handle the incomplete multi-view data with arbitrary missing views or do not consider the negative factor of information imbalance among views. Moreover, some methods do not fully explore the local structure of all incomplete views. To tackle these problems, this paper proposes a simple but effective method, named localized sparse incomplete multi-view clustering (LSIMVC). Different from the existing methods, LSIMVC intends to learn a sparse and structured consensus latent representation from the incomplete multi-view data by optimizing a sparse regularized and novel graph embedded multi-view matrix factorization model. Specifically, in such a novel model based on the matrix factorization, a l1 norm based spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$L_2$BN&#26041;&#27861;&#65292;&#36890;&#36807;&#31561;&#21270;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#26469;&#22686;&#24378;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#21487;&#20197;&#22686;&#24378;&#20869;&#31867;&#21035;&#29305;&#24449;&#30340;&#32039;&#20945;&#24615;&#24182;&#25193;&#22823;&#36328;&#31867;&#21035;&#29305;&#24449;&#30340;&#24046;&#24322;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#29992;&#20316;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#24402;&#19968;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.02625</link><description>&lt;p&gt;
$L_2$BN: &#36890;&#36807;&#31561;&#21270;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#26469;&#22686;&#24378;&#25209;&#37327;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
$L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of Features. (arXiv:2207.02625v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$L_2$BN&#26041;&#27861;&#65292;&#36890;&#36807;&#31561;&#21270;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#26469;&#22686;&#24378;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#21487;&#20197;&#22686;&#24378;&#20869;&#31867;&#21035;&#29305;&#24449;&#30340;&#32039;&#20945;&#24615;&#24182;&#25193;&#22823;&#36328;&#31867;&#21035;&#29305;&#24449;&#30340;&#24046;&#24322;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#29992;&#20316;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#24402;&#19968;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an $L_2$BN method to enhance batch normalization by equalizing the $l_2$ norms of sample features, which can strengthen the compactness of intra-class features and enlarge the discrepancy of inter-class features, easy to implement, and can be used as a basic normalization method for neural networks.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#24046;&#24322;&#20250;&#22952;&#30861;&#25209;&#37327;&#24402;&#19968;&#21270;&#33719;&#24471;&#26356;&#21152;&#26174;&#33879;&#30340;&#36328;&#31867;&#21035;&#29305;&#24449;&#21644;&#26356;&#21152;&#32039;&#20945;&#30340;&#20869;&#31867;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35266;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#31561;&#21270;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#23558;&#26679;&#26412;&#29305;&#24449;&#36755;&#20837;&#25209;&#37327;&#24402;&#19968;&#21270;&#20043;&#21069;&#23545;&#27599;&#20010;&#26679;&#26412;&#29305;&#24449;&#36827;&#34892;$L_2$&#24402;&#19968;&#21270;&#65292;&#22240;&#27492;&#29305;&#24449;&#20855;&#26377;&#30456;&#21516;&#30340;&#25968;&#37327;&#32423;&#12290;&#30001;&#20110;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;$L_2$&#24402;&#19968;&#21270;&#21644;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;$L_2$BN&#12290;$L_2$BN&#21487;&#20197;&#22686;&#24378;&#20869;&#31867;&#21035;&#29305;&#24449;&#30340;&#32039;&#20945;&#24615;&#24182;&#25193;&#22823;&#36328;&#31867;&#21035;&#29305;&#24449;&#30340;&#24046;&#24322;&#12290;$L_2$BN&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#21442;&#25968;&#25110;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#20854;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20316;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#24402;&#19968;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21508;&#31181;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;$L_2$BN&#30340;&#26377;&#25928;&#24615;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show that the difference in $l_2$ norms of sample features can hinder batch normalization from obtaining more distinguished inter-class features and more compact intra-class features. To address this issue, we propose an intuitive but effective method to equalize the $l_2$ norms of sample features. Concretely, we $l_2$-normalize each sample feature before feeding them into batch normalization, and therefore the features are of the same magnitude. Since the proposed method combines the $l_2$ normalization and batch normalization, we name our method $L_2$BN. The $L_2$BN can strengthen the compactness of intra-class features and enlarge the discrepancy of inter-class features. The $L_2$BN is easy to implement and can exert its effect without any additional parameters or hyper-parameters. Therefore, it can be used as a basic normalization method for neural networks. We evaluate the effectiveness of $L_2$BN through extensive experiments with various models on image classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#21644;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.02307</link><description>&lt;p&gt;
&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#21644;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a semi-supervised medical image segmentation bootstrapping method based on anatomical-aware contrastive distillation, which solves the problem of imbalanced medical image data by softly labeling negative samples and capturing more semantically similar features.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#27880;&#37322;&#31232;&#32570;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#21307;&#23398;&#22270;&#20687;&#20855;&#26377;&#24179;&#34913;&#30340;&#31867;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#65288;&#21363;&#22810;&#31867;&#26631;&#31614;&#19981;&#24179;&#34913;&#65289;&#65292;&#36825;&#33258;&#28982;&#22320;&#20135;&#29983;&#27169;&#31946;&#30340;&#36718;&#24275;&#24182;&#36890;&#24120;&#38169;&#35823;&#22320;&#26631;&#35760;&#32597;&#35265;&#30340;&#23545;&#35937;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#36127;&#26679;&#26412;&#26159;&#21542;&#21516;&#26679;&#36127;&#38754;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTION&#65292;&#19968;&#31181;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#32780;&#19981;&#26159;&#27491;&#36127;&#23545;&#20043;&#38388;&#30340;&#20108;&#20803;&#30417;&#30563;&#26469;&#24320;&#21457;&#36845;&#20195;&#23545;&#27604;&#33976;&#39311;&#31639;&#27861;&#12290;&#19982;&#27491;&#26679;&#26412;&#30456;&#27604;&#65292;&#25105;&#20204;&#36824;&#20174;&#38543;&#26426;&#36873;&#25321;&#30340;&#36127;&#26679;&#26412;&#38598;&#20013;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#65292;&#20197;&#24378;&#21046;&#25191;&#34892;&#37319;&#26679;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#30340;&#21551;&#21160;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has shown great promise over annotation scarcity problems in the context of medical image segmentation. Existing approaches typically assume a balanced class distribution for both labeled and unlabeled medical images. However, medical image data in reality is commonly imbalanced (i.e., multi-class label imbalance), which naturally yields blurry contours and usually incorrectly labels rare objects. Moreover, it remains unclear whether all negative samples are equally negative. In this work, we present ACTION, an Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised medical image segmentation. Specifically, we first develop an iterative contrastive distillation algorithm by softly labeling the negatives rather than binary supervision between positive and negative pairs. We also capture more semantically similar features from the randomly chosen negative set compared to the positives to enforce the diversity of the sampled data. Second, we raise a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#19981;&#33021;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.06333</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#36827;&#34892;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Visuomotor Control in Multi-Object Scenes Using Object-Aware Representations. (arXiv:2205.06333v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#19981;&#33021;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the effectiveness of using object-aware representation learning techniques for robotic tasks, to address the problem that current methodologies learn task specific representations that do not necessarily transfer well to other tasks, and that representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world.
&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#30340;&#24863;&#30693;&#29702;&#35299;&#20197;&#21450;&#20854;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#23545;&#20110;&#25104;&#21151;&#23436;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#34920;&#31034;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#20294;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#65292;&#19981;&#19968;&#23450;&#33021;&#22815;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25910;&#38598;&#36215;&#26469;&#24456;&#26114;&#36149;&#12290;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#33719;&#21462;&#34920;&#31034;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#26159;&#29289;&#20307;&#26080;&#20851;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#27492;&#24471;&#21040;&#30340;&#34920;&#31034;&#23545;&#20110;&#20855;&#26377;&#35768;&#22810;&#32452;&#20214;&#30340;&#22330;&#26223;&#30340;&#36890;&#29992;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceptual understanding of the scene and the relationship between its different components is important for successful completion of robotic tasks. Representation learning has been shown to be a powerful technique for this, but most of the current methodologies learn task specific representations that do not necessarily transfer well to other tasks. Furthermore, representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world. Using self-supervised learning to obtain representations from unlabeled data can mitigate this problem. However, current self-supervised representation learning methods are mostly object agnostic, and we demonstrate that the resulting representations are insufficient for general purpose robotics tasks as they fail to capture the complexity of scenes with many components. In this paper, we explore the effectiveness of using object-aware representation learning techniques for robotic tasks. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#26816;&#27979;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;DILEMMA&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;MoCoV3&#12289;DINO&#21644;SimCLR&#65292;&#20998;&#21035;&#26174;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4.41%&#12289;3.97%&#21644;0.5%&#12290;</title><link>http://arxiv.org/abs/2204.04788</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#27979;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning by Detecting Incorrect Location Embeddings. (arXiv:2204.04788v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#26816;&#27979;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;DILEMMA&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;MoCoV3&#12289;DINO&#21644;SimCLR&#65292;&#20998;&#21035;&#26174;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4.41%&#12289;3.97%&#21644;0.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#35748;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#20854;&#21306;&#20998;&#23545;&#35937;&#24418;&#29366;&#30340;&#33021;&#21147;&#26377;&#20851;&#12290;&#30001;&#20110;&#23545;&#35937;&#24418;&#29366;&#19982;&#20854;&#37096;&#20214;&#30340;&#20301;&#32622;&#26377;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#26816;&#27979;&#37027;&#20123;&#34987;&#20154;&#20026;&#31227;&#20301;&#30340;&#37096;&#20214;&#12290;&#25105;&#20204;&#29992;&#22270;&#20687;&#20196;&#29260;&#34920;&#31034;&#23545;&#35937;&#37096;&#20214;&#65292;&#24182;&#35757;&#32451;ViT&#26816;&#27979;&#21738;&#20010;&#20196;&#29260;&#19982;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#36755;&#20837;&#30340;&#31232;&#30095;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#20197;&#24212;&#23545;&#36974;&#25377;&#24182;&#21152;&#36895;&#35757;&#32451;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;DILEMMA&#65292;&#21363;&#26816;&#27979;&#38169;&#35823;&#20301;&#32622;&#23884;&#20837;&#21644;&#25513;&#34109;&#36755;&#20837;&#12290;&#25105;&#20204;&#23558;DILEMMA&#24212;&#29992;&#20110;MoCoV3&#12289;DINO&#21644;SimCLR&#65292;&#24182;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#65292;&#22312;ImageNet-1K&#19978;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#65292;&#20998;&#21035;&#26174;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4.41%&#12289;3.97%&#21644;0.5%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;MAE&#19982;&#25105;&#20204;&#30340;&#23436;&#20840;&#24494;&#35843;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel self-supervised learning (SSL) loss for image representation learning. There is a growing belief that generalization in deep neural networks is linked to their ability to discriminate object shapes. Since object shape is related to the location of its parts, we propose to detect those that have been artificially misplaced. We represent object parts with image tokens and train a ViT to detect which token has been combined with an incorrect positional embedding. We then introduce sparsity in the inputs to make the model more robust to occlusions and to speed up the training. We call our method DILEMMA, which stands for Detection of Incorrect Location EMbeddings with MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an improvement in their performance of respectively 4.41%, 3.97%, and 0.5% under the same training time and with a linear probing transfer on ImageNet-1K. We also show full fine-tuning improvements of MAE combined with our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#31243;&#24207;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#40784;&#31163;&#25955;&#28508;&#22312;&#21464;&#37327;&#21644;&#24369;&#30417;&#30563;&#27966;&#29983;&#30340;&#26631;&#31614;&#20272;&#35745;&#65292;&#25913;&#21892;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#30340;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2203.12023</link><description>&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#26377;&#21161;&#20110;&#24369;&#30417;&#30563;&#65288;&#21453;&#20043;&#20134;&#28982;&#65289;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#31243;&#24207;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#40784;&#31163;&#25955;&#28508;&#22312;&#21464;&#37327;&#21644;&#24369;&#30417;&#30563;&#27966;&#29983;&#30340;&#26631;&#31614;&#20272;&#35745;&#65292;&#25913;&#21892;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#30340;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model that fuses programmatic weak supervision and generative adversarial networks, improving the estimate of unobserved labels by aligning discrete latent variables and weak supervision derived label estimate, and enabling data augmentation through weak supervision.
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26377;&#21069;&#36884;&#30340;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22312;&#33719;&#21462;&#36275;&#22815;&#25968;&#37327;&#21644;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#20174;&#32780;&#36896;&#25104;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#30740;&#31350;&#20102;&#19981;&#20381;&#36182;&#20110;&#22522;&#26412;&#30495;&#23454;&#26631;&#31614;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#24314;&#27169;&#12290;&#34429;&#28982;&#36825;&#20123;&#25216;&#26415;&#20284;&#20046;&#21487;&#20197;&#20849;&#21516;&#20351;&#29992;&#65292;&#30456;&#20114;&#25913;&#36827;&#65292;&#20294;&#22914;&#20309;&#22312;&#23427;&#20204;&#20043;&#38388;&#24314;&#31435;&#25509;&#21475;&#23578;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#31243;&#24207;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#36825;&#31181;&#34701;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#31163;&#25955;&#28508;&#22312;&#21464;&#37327;&#20197;&#21450;&#24369;&#30417;&#30563;&#27966;&#29983;&#30340;&#26631;&#31614;&#20272;&#35745;&#12290;&#20004;&#32773;&#30340;&#23545;&#40784;&#20801;&#35768;&#26356;&#22909;&#22320;&#24314;&#27169;&#24369;&#30417;&#30563;&#26469;&#28304;&#30340;&#26679;&#26412;&#30456;&#20851;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#30340;&#20272;&#35745;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#36890;&#36807;&#24369;&#30417;&#30563;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many promising applications of supervised machine learning face hurdles in the acquisition of labeled data in sufficient quantity and quality, creating an expensive bottleneck. To overcome such limitations, techniques that do not depend on ground truth labels have been studied, including weak supervision and generative modeling. While these techniques would seem to be usable in concert, improving one another, how to build an interface between them is not well-understood. In this work, we propose a model fusing programmatic weak supervision and generative adversarial networks and provide theoretical justification motivating this fusion. The proposed approach captures discrete latent variables in the data alongside the weak supervision derived label estimate. Alignment of the two allows for better modeling of sample-dependent accuracies of the weak supervision sources, improving the estimate of unobserved labels. It is the first approach to enable data augmentation through weakly supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#21644;&#21333;&#31867;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#29983;&#24577;&#31995;&#32479;&#26576;&#20123;&#24178;&#25200;&#30340;&#24369;&#20449;&#21495;&#65292;&#20197;&#32472;&#21046;&#30001;&#20960;&#20309;&#34558;&#29190;&#21457;&#24341;&#36215;&#30340;&#26862;&#26519;&#27515;&#20129;&#29575;&#22312;&#31232;&#30095;&#26862;&#26519;-&#33492;&#21407;&#29983;&#24577;&#36807;&#28193;&#24102;&#20013;&#30340;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2203.00049</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#30340;&#30446;&#26631;&#21464;&#21270;&#26816;&#27979;&#65292;&#29992;&#20110;&#26862;&#26519;&#27515;&#20129;&#29575;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Towards Targeted Change Detection with Heterogeneous Remote Sensing Images for Forest Mortality Mapping. (arXiv:2203.00049v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#21644;&#21333;&#31867;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#29983;&#24577;&#31995;&#32479;&#26576;&#20123;&#24178;&#25200;&#30340;&#24369;&#20449;&#21495;&#65292;&#20197;&#32472;&#21046;&#30001;&#20960;&#20309;&#34558;&#29190;&#21457;&#24341;&#36215;&#30340;&#26862;&#26519;&#27515;&#20129;&#29575;&#22312;&#31232;&#30095;&#26862;&#26519;-&#33492;&#21407;&#29983;&#24577;&#36807;&#28193;&#24102;&#20013;&#30340;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#29992;&#20110;&#24322;&#26500;&#36965;&#24863;&#25968;&#25454;&#65288;&#20363;&#22914;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#21644;&#22810;&#20809;&#35889;&#36752;&#23556;&#35745;&#65289;&#30340;&#21464;&#21270;&#26816;&#27979;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#21512;&#26816;&#27979;&#29983;&#24577;&#31995;&#32479;&#26576;&#20123;&#24178;&#25200;&#30340;&#24369;&#20449;&#21495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#21644;&#21333;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26088;&#22312;&#20351;&#29992;&#22810;&#28304;&#21355;&#26143;&#22270;&#20687;&#32472;&#21046;&#30001;&#20960;&#20309;&#34558;&#29190;&#21457;&#24341;&#36215;&#30340;&#26862;&#26519;&#27515;&#20129;&#29575;&#22312;&#31232;&#30095;&#26862;&#26519;-&#33492;&#21407;&#29983;&#24577;&#36807;&#28193;&#24102;&#20013;&#30340;&#22320;&#22270;&#12290;&#20107;&#20214;&#21069;&#21644;&#20107;&#20214;&#21518;&#30340;&#22270;&#20687;&#20998;&#21035;&#30001;Landsat-5&#21644;RADARSAT-2&#25910;&#38598;&#12290;&#20351;&#29992;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#21464;&#21270;&#24863;&#30693;&#22270;&#20687;&#36716;&#25442;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#21355;&#26143;&#21508;&#33258;&#30340;&#22495;&#20013;&#35745;&#31639;&#24046;&#24322;&#22270;&#20687;&#12290;&#36825;&#20123;&#24046;&#24322;&#19982;&#21407;&#22987;&#30340;&#20107;&#20214;&#21069;&#21644;&#20107;&#20214;&#21518;&#30340;&#22270;&#20687;&#22534;&#21472;&#65292;&#24182;&#20256;&#36882;&#32473;&#22312;&#30446;&#26631;&#21464;&#21270;&#31867;&#30340;&#23567;&#26679;&#26412;&#19978;&#35757;&#32451;&#30340;OCC&#12290;&#20998;&#31867;&#22120;&#20135;&#29983;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Several generic methods have recently been developed for change detection in heterogeneous remote sensing data, such as images from synthetic aperture radar (SAR) and multispectral radiometers. However, these are not well suited to detect weak signatures of certain disturbances of ecological systems. To resolve this problem we propose a new approach based on image-to-image translation and one-class classification (OCC). We aim to map forest mortality caused by an outbreak of geometrid moths in a sparsely forested forest-tundra ecotone using multisource satellite images. The images preceding and following the event are collected by Landsat-5 and RADARSAT-2, respectively. Using a recent deep learning method for change-aware image translation, we compute difference images in both satellites' respective domains. These differences are stacked with the original pre- and post-event images and passed to an OCC trained on a small sample from the targeted change class. The classifier produces a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21024;&#38500;&#22359;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31232;&#32570;&#35757;&#32451;&#26679;&#26412;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24674;&#22797;&#24615;&#27010;&#24565;&#36873;&#25321;&#35201;&#21024;&#38500;&#30340;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRACTISE&#30340;&#31639;&#27861;&#65292;&#20165;&#20351;&#29992;&#24494;&#23567;&#30340;&#35757;&#32451;&#22270;&#20687;&#38598;&#26469;&#21152;&#36895;&#32593;&#32476;&#65292;PRACTISE&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.07861</link><description>&lt;p&gt;
&#29992;&#24494;&#23567;&#25968;&#25454;&#38598;&#23454;&#29616;&#32593;&#32476;&#21152;&#36895;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Practical Network Acceleration with Tiny Sets. (arXiv:2202.07861v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21024;&#38500;&#22359;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31232;&#32570;&#35757;&#32451;&#26679;&#26412;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24674;&#22797;&#24615;&#27010;&#24565;&#36873;&#25321;&#35201;&#21024;&#38500;&#30340;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRACTISE&#30340;&#31639;&#27861;&#65292;&#20165;&#20351;&#29992;&#24494;&#23567;&#30340;&#35757;&#32451;&#22270;&#20687;&#38598;&#26469;&#21152;&#36895;&#32593;&#32476;&#65292;PRACTISE&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#20351;&#29992;&#24494;&#23567;&#30340;&#35757;&#32451;&#38598;&#21152;&#36895;&#32593;&#32476;&#24050;&#25104;&#20026;&#23454;&#36341;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38656;&#27714;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#28388;&#27874;&#22120;&#32423;&#21035;&#30340;&#21098;&#26525;&#26469;&#21152;&#36895;&#31232;&#32570;&#35757;&#32451;&#26679;&#26412;&#30340;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21024;&#38500;&#22359;&#26159;&#19968;&#31181;&#22522;&#26412;&#19978;&#26356;&#20248;&#36234;&#30340;&#26041;&#27861;&#12290;&#23427;&#20139;&#26377;&#26356;&#39640;&#30340;&#21152;&#36895;&#27604;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20135;&#29983;&#26356;&#22909;&#30340;&#24310;&#36831;-&#20934;&#30830;&#24615;&#24615;&#33021;&#12290;&#20026;&#20102;&#36873;&#25321;&#35201;&#21024;&#38500;&#30340;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#21363;&#21487;&#24674;&#22797;&#24615;&#65292;&#29992;&#20110;&#34913;&#37327;&#21387;&#32553;&#32593;&#32476;&#30340;&#24674;&#22797;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#21487;&#24674;&#22797;&#24615;&#23545;&#20110;&#36873;&#25321;&#35201;&#21024;&#38500;&#30340;&#22359;&#26159;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRACTISE&#30340;&#31639;&#27861;&#65292;&#20165;&#20351;&#29992;&#24494;&#23567;&#30340;&#35757;&#32451;&#22270;&#20687;&#38598;&#26469;&#21152;&#36895;&#32593;&#32476;&#12290;PRACTISE&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;22&#65285;&#30340;&#24310;&#36831;&#38477;&#20302;&#65292;PRACTISE&#22312;ImageNet-1k&#19978;&#24179;&#22343;&#36229;&#36807;&#20197;&#24448;&#26041;&#27861;7&#65285;&#12290;&#23427;&#36824;&#20855;&#26377;&#39640;&#24230;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#25968;&#25454;&#38544;&#31169;&#22330;&#26223;&#21644;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to data privacy issues, accelerating networks with tiny training sets has become a critical need in practice. Previous methods mainly adopt filter-level pruning to accelerate networks with scarce training samples. In this paper, we reveal that dropping blocks is a fundamentally superior approach in this scenario. It enjoys a higher acceleration ratio and results in a better latency-accuracy performance under the few-shot setting. To choose which blocks to drop, we propose a new concept namely recoverability to measure the difficulty of recovering the compressed network. Our recoverability is efficient and effective for choosing which blocks to drop. Finally, we propose an algorithm named PRACTISE to accelerate networks using only tiny sets of training images. PRACTISE outperforms previous methods by a significant margin. For 22% latency reduction, PRACTISE surpasses previous methods by on average 7% on ImageNet-1k. It also enjoys high generalization ability, working well under data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;&#65288;I-Tuning&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#23558;&#19981;&#21487;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#35299;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#36830;&#25509;&#36215;&#26469;&#65292;&#20351;&#24471;&#27169;&#22411;&#21253;&#21547;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#23569;&#65292;&#35757;&#32451;&#36895;&#24230;&#24555;&#65292;&#21516;&#26102;&#22312;&#19977;&#20010;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#19982;&#22823;&#35268;&#27169;&#22522;&#32447;&#31995;&#32479;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#37117;&#23569;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2202.06574</link><description>&lt;p&gt;
I-Tuning: &#21033;&#29992;&#22270;&#20687;&#23545;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36731;&#37327;&#32423;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning. (arXiv:2202.06574v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;&#65288;I-Tuning&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#23558;&#19981;&#21487;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#35299;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#36830;&#25509;&#36215;&#26469;&#65292;&#20351;&#24471;&#27169;&#22411;&#21253;&#21547;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#23569;&#65292;&#35757;&#32451;&#36895;&#24230;&#24555;&#65292;&#21516;&#26102;&#22312;&#19977;&#20010;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#19982;&#22823;&#35268;&#27169;&#22522;&#32447;&#31995;&#32479;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#37117;&#23569;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a lightweight image captioning framework (I-Tuning) that connects the non-trainable pre-trained language decoder GPT2 and vision encoder CLIP-ViT with a novel I-Tuning cross-attention module. The framework contains fewer trainable parameters and achieves comparable or better performance than large-scale baseline systems on three image captioning benchmarks, while requiring much fewer trainable parameters and training data compared with state-of-the-art baselines.
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26159;&#19968;&#39033;&#20256;&#32479;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20219;&#21153;&#65292;&#26088;&#22312;&#29983;&#25104;&#22270;&#20687;&#30340;&#35821;&#35328;&#25551;&#36848;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#19982;&#36825;&#20123;&#39640;&#25104;&#26412;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;&#65288;I-Tuning&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;I-Tuning&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23558;&#19981;&#21487;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#35299;&#30721;&#22120;GPT2&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;CLIP-ViT&#36830;&#25509;&#36215;&#26469;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#21442;&#25968;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#38656;&#35201;&#26356;&#26032;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26694;&#26550;&#36731;&#24039;&#24555;&#36895;&#12290;&#22312;&#19977;&#20010;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#19982;&#22823;&#35268;&#27169;&#22522;&#32447;&#31995;&#32479;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20294;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#21547;&#22810;&#36798;10&#20493;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning is a traditional vision-and-language task that aims to generate the language description of an image. Recent studies focus on scaling up the model size and the number of training data, which significantly increase the cost of model training. Different to these heavy-cost models, we introduce a lightweight image captioning framework (I-Tuning), which contains a small number of trainable parameters. We design a novel I-Tuning cross-attention module to connect the non-trainable pre-trained language decoder GPT2 and vision encoder CLIP-ViT. Since most parameters are not required to be updated during training, our framework is lightweight and fast. Experimental results conducted on three image captioning benchmarks reveal that our framework achieves comparable or better performance than the large-scale baseline systems. But our models contain up to 10 times fewer trainable parameters and require much fewer data for training compared with state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;TSGV&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#65292;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#26159;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#12290;</title><link>http://arxiv.org/abs/2201.08071</link><description>&lt;p&gt;
&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65306;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;TSGV&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#65292;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#26159;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey summarizes the fundamental concepts and current research status of temporal sentence grounding in videos (TSGV), also known as natural language video localization (NLVL) or video moment retrieval (VMR), as well as future research directions. TSGV aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video, connecting computer vision and natural language, and has drawn significant attention from researchers in both communities.
&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#65292;&#21448;&#31216;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#65288;NLVL&#65289;&#25110;&#35270;&#39057;&#26102;&#21051;&#26816;&#32034;&#65288;VMR&#65289;&#65292;&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#12290;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;TSGV&#24341;&#36215;&#20102;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#35270;&#12290;&#26412;&#32508;&#36848;&#35797;&#22270;&#25552;&#20379;TSGV&#20013;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#30340;&#24635;&#32467;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20316;&#20026;&#32972;&#26223;&#65292;&#25105;&#20204;&#20197;&#25945;&#31243;&#30340;&#24418;&#24335;&#20171;&#32461;&#20102;TSGV&#20013;&#21151;&#33021;&#32452;&#20214;&#30340;&#24120;&#35265;&#32467;&#26500;&#65306;&#20174;&#21407;&#22987;&#35270;&#39057;&#21644;&#35821;&#35328;&#26597;&#35810;&#30340;&#29305;&#24449;&#25552;&#21462;&#21040;&#30446;&#26631;&#26102;&#21051;&#30340;&#31572;&#26696;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#25216;&#26415;&#65292;&#36825;&#26159;TSGV&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#65292;&#20197;&#23454;&#29616;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#26377;&#25928;&#23545;&#40784;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;TSGV&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#21450;&#20854;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal sentence grounding in videos (TSGV), \aka natural language video localization (NLVL) or video moment retrieval (VMR), aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video. Connecting computer vision and natural language, TSGV has drawn significant attention from researchers in both communities. This survey attempts to provide a summary of fundamental concepts in TSGV and current research status, as well as future research directions. As the background, we present a common structure of functional components in TSGV, in a tutorial style: from feature extraction from raw video and language query, to answer prediction of the target moment. Then we review the techniques for multimodal understanding and interaction, which is the key focus of TSGV for effective alignment between the two modalities. We construct a taxonomy of TSGV techniques and elaborate the methods in different categories with their strengths and weaknesses. La
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#36947;&#29305;&#24449;&#21435;&#22122;&#30340;&#25351;&#32441;&#21576;&#29616;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#20808;&#21069;&#30740;&#31350;&#20013;&#24573;&#30053;&#30340;&#20887;&#20313;&#22122;&#22768;&#20449;&#24687;&#26469;&#23398;&#20064;&#25351;&#32441;&#22270;&#20687;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.07620</link><description>&lt;p&gt;
&#22522;&#20110;&#36890;&#36947;&#29305;&#24449;&#21435;&#22122;&#30340;&#25351;&#32441;&#21576;&#29616;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fingerprint Presentation Attack Detection by Channel-wise Feature Denoising. (arXiv:2111.07620v2 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#36947;&#29305;&#24449;&#21435;&#22122;&#30340;&#25351;&#32441;&#21576;&#29616;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#20808;&#21069;&#30740;&#31350;&#20013;&#24573;&#30053;&#30340;&#20887;&#20313;&#22122;&#22768;&#20449;&#24687;&#26469;&#23398;&#20064;&#25351;&#32441;&#22270;&#20687;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel channel-wise feature denoising fingerprint presentation attack detection (CFD-PAD) method, which learns important features of fingerprint images by handling the redundant noise information ignored in previous studies, and exhibits good robustness and accuracy under various attack types.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25915;&#20987;&#26448;&#26009;&#30340;&#22810;&#26679;&#24615;&#65292;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#65288;AFRS&#65289;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#26377;&#25928;&#30340;&#25351;&#32441;&#21576;&#29616;&#25915;&#20987;&#26816;&#27979;&#65288;PAD&#65289;&#26041;&#27861;&#23545;&#20110;AFRS&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;PAD&#26041;&#27861;&#22312;&#26032;&#30340;&#25915;&#20987;&#31867;&#22411;&#35774;&#32622;&#19979;&#24448;&#24448;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36890;&#36947;&#29305;&#24449;&#21435;&#22122;&#30340;&#25351;&#32441;PAD&#65288;CFD-PAD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#20808;&#21069;&#30740;&#31350;&#20013;&#24573;&#30053;&#30340;&#20887;&#20313;&#22122;&#22768;&#20449;&#24687;&#26469;&#23398;&#20064;&#25351;&#32441;&#22270;&#20687;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26435;&#34913;&#27599;&#20010;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#24182;&#35782;&#21035;&#20986;&#26377;&#21306;&#21035;&#24615;&#30340;&#36890;&#36947;&#21644;&#8220;&#22122;&#22768;&#8221;&#36890;&#36947;&#26469;&#23398;&#20064;&#25351;&#32441;&#22270;&#20687;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#22312;&#29305;&#24449;&#22270;&#20013;&#25233;&#21046;&#8220;&#22122;&#22768;&#8221;&#36890;&#36947;&#30340;&#20256;&#25773;&#20197;&#20943;&#23569;&#24178;&#25200;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35774;&#35745;&#20102;PA-Adaptation&#25439;&#22833;&#26469;&#32422;&#26463;&#29305;&#24449;&#20998;&#24067;&#65292;&#20351;&#27963;&#20307;&#25351;&#32441;&#30340;&#29305;&#24449;&#20998;&#24067;&#26356;&#32858;&#21512;&#65292;&#32780;&#27450;&#35784;&#25351;&#32441;&#30340;&#29305;&#24449;&#20998;&#24067;&#26356;&#20998;&#25955;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#19979;&#22343;&#20855;&#26377;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the diversity of attack materials, fingerprint recognition systems (AFRSs) are vulnerable to malicious attacks. It is thus important to propose effective fingerprint presentation attack detection (PAD) methods for the safety and reliability of AFRSs. However, current PAD methods often exhibit poor robustness under new attack types settings. This paper thus proposes a novel channel-wise feature denoising fingerprint PAD (CFD-PAD) method by handling the redundant noise information ignored in previous studies. The proposed method learns important features of fingerprint images by weighing the importance of each channel and identifying discriminative channels and "noise" channels. Then, the propagation of "noise" channels is suppressed in the feature map to reduce interference. Specifically, a PA-Adaptation loss is designed to constrain the feature distribution to make the feature distribution of live fingerprints more aggregate and that of spoof fingerprints more disperse. Experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;1&#20301;&#28608;&#27963;&#21644;1&#20301;&#21367;&#31215;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#22312;&#24494;&#23567;&#30340;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#21644;&#23884;&#20837;&#65292;&#24182;&#33410;&#30465;&#22823;&#37327;&#23384;&#20648;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2110.06804</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A comprehensive review of Binary Neural Network. (arXiv:2110.06804v4 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;1&#20301;&#28608;&#27963;&#21644;1&#20301;&#21367;&#31215;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#22312;&#24494;&#23567;&#30340;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#21644;&#23884;&#20837;&#65292;&#24182;&#33410;&#30465;&#22823;&#37327;&#23384;&#20648;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a comprehensive overview of recent developments in Binary Neural Networks (BNN), with a focus on 1-bit activations and 1-bit convolution networks. These networks can be implemented and embedded on tiny restricted devices, saving significant storage, computation cost, and energy consumption.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26368;&#36817;&#25913;&#21464;&#20102;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#23613;&#31649;DL&#20855;&#26377;&#21508;&#31181;&#22909;&#22788;&#21644;&#28508;&#21147;&#65292;&#20294;&#22312;&#19981;&#21516;&#30340;&#35745;&#31639;&#21463;&#38480;&#21644;&#33021;&#37327;&#21463;&#38480;&#35774;&#22791;&#20013;&#38656;&#35201;&#36827;&#34892;DL&#22788;&#29702;&#12290;&#30740;&#31350;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#31561;&#20855;&#26377;&#25913;&#21464;&#28216;&#25103;&#35268;&#21017;&#30340;&#25216;&#26415;&#20197;&#22686;&#21152;&#28145;&#24230;&#23398;&#20064;&#33021;&#21147;&#26159;&#24456;&#33258;&#28982;&#30340;&#12290;&#26368;&#36817;&#22312;BNN&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#24494;&#23567;&#30340;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#21644;&#23884;&#20837;&#65292;&#24182;&#33410;&#30465;&#22823;&#37327;&#23384;&#20648;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;BNN&#34892;&#20026;&#37117;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#20869;&#23384;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;BNN&#26368;&#36817;&#21457;&#23637;&#30340;&#23436;&#25972;&#27010;&#36848;&#12290;&#26412;&#25991;&#19987;&#38376;&#20851;&#27880;1&#20301;&#28608;&#27963;&#21644;1&#20301;&#21367;&#31215;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#19982;&#20197;&#21069;&#30340;&#35843;&#26597;&#28151;&#21512;&#20351;&#29992;&#20302;&#20301;&#20316;&#21697;&#30456;&#21453;&#12290;&#23427;&#23545;BNN&#30340;&#24320;&#21457;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has recently changed the development of intelligent systems and is widely adopted in many real-life applications. Despite their various benefits and potentials, there is a high demand for DL processing in different computationally limited and energy-constrained devices. It is natural to study game-changing technologies such as Binary Neural Networks (BNN) to increase deep learning capabilities. Recently remarkable progress has been made in BNN since they can be implemented and embedded on tiny restricted devices and save a significant amount of storage, computation cost, and energy consumption. However, nearly all BNN acts trade with extra memory, computation cost, and higher performance. This article provides a complete overview of recent developments in BNN. This article focuses exclusively on 1-bit activations and weights 1-bit convolution networks, contrary to previous surveys in which low-bit works are mixed in. It conducted a complete investigation of BNN's dev
&lt;/p&gt;</description></item></channel></rss>