<rss version="2.0"><channel><title>Chat Arxiv eess</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for eess</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#36153;&#32773;&#26159;&#21542;&#24895;&#24847;&#20026;&#22522;&#20110;&#20247;&#21253;&#37197;&#36865;&#30340;&#30005;&#23376;&#26434;&#36135;&#37197;&#36865;&#25903;&#20184;&#39069;&#22806;&#36153;&#29992;&#65292;&#32467;&#26524;&#34920;&#26126;&#22914;&#26524;&#20247;&#21253;&#37197;&#36865;&#25552;&#20379;&#26356;&#28789;&#27963;&#21644;&#20197;&#28040;&#36153;&#32773;&#20026;&#23548;&#21521;&#30340;&#26381;&#21153;&#65292;&#30005;&#23376;&#36141;&#29289;&#32773;&#26377;&#24847;&#24895;&#25903;&#20184;&#39069;&#22806;&#36153;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.07044</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#26159;&#21542;&#24895;&#24847;&#20026;&#20247;&#21253;&#37197;&#36865;&#30340;&#30005;&#23376;&#26434;&#36135;&#25903;&#20184;&#39069;&#22806;&#36153;&#29992;&#65311;&#20026;&#21457;&#23637;&#20013;&#22269;&#23478;&#25552;&#20379;&#28151;&#21512;&#36873;&#25321;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Are consumers ready to pay extra for crowd-shipping e-groceries and why? A hybrid choice analysis for developing economies. (arXiv:2303.07044v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#36153;&#32773;&#26159;&#21542;&#24895;&#24847;&#20026;&#22522;&#20110;&#20247;&#21253;&#37197;&#36865;&#30340;&#30005;&#23376;&#26434;&#36135;&#37197;&#36865;&#25903;&#20184;&#39069;&#22806;&#36153;&#29992;&#65292;&#32467;&#26524;&#34920;&#26126;&#22914;&#26524;&#20247;&#21253;&#37197;&#36865;&#25552;&#20379;&#26356;&#28789;&#27963;&#21644;&#20197;&#28040;&#36153;&#32773;&#20026;&#23548;&#21521;&#30340;&#26381;&#21153;&#65292;&#30005;&#23376;&#36141;&#29289;&#32773;&#26377;&#24847;&#24895;&#25903;&#20184;&#39069;&#22806;&#36153;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies whether consumers are willing to pay extra for crowd-shipping e-groceries deliveries, and the results indicate that e-shoppers are willing to pay extra if crowd-shipping provides more flexible and consumer-oriented service.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#28040;&#36153;&#32773;&#24895;&#24847;&#20026;&#22522;&#20110;&#20247;&#21253;&#37197;&#36865;&#30340;&#30005;&#23376;&#26434;&#36135;&#37197;&#36865;&#25903;&#20184;&#39069;&#22806;&#36153;&#29992;&#30340;&#34892;&#20026;&#30740;&#31350;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#22312;&#20044;&#20811;&#20848;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21363;&#19968;&#20010;&#20247;&#21253;&#37197;&#36865;&#26381;&#21153;&#27491;&#22312;&#21457;&#23637;&#30340;&#21457;&#23637;&#20013;&#22269;&#23478;&#12290;&#20026;&#20102;&#32771;&#34385;&#21040;&#28040;&#36153;&#32773;&#30340;&#34892;&#20026;&#22797;&#26434;&#24615;&#65292;&#36873;&#25321;&#27169;&#22411;&#34987;&#22686;&#21152;&#20102;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#20247;&#21253;&#37197;&#36865;&#25552;&#20379;&#26356;&#28789;&#27963;&#21644;&#20197;&#28040;&#36153;&#32773;&#20026;&#23548;&#21521;&#30340;&#26381;&#21153;&#65292;&#30005;&#23376;&#36141;&#29289;&#32773;&#26377;&#24847;&#24895;&#20026;&#20247;&#21253;&#37197;&#36865;&#37197;&#36865;&#25903;&#20184;&#39069;&#22806;&#36153;&#29992;&#12290;&#20247;&#21253;&#37197;&#36865;&#30340;&#39044;&#26399;&#29615;&#22659;&#24433;&#21709;&#24182;&#19981;&#34987;&#30005;&#23376;&#36141;&#29289;&#32773;&#35748;&#20026;&#24456;&#37325;&#35201;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#23545;&#25152;&#32771;&#34385;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#29615;&#22659;&#21644;&#20197;&#27773;&#36710;&#20026;&#23548;&#21521;&#30340;&#27969;&#21160;&#24615;&#30340;&#20302;&#20851;&#27880;&#24230;&#26469;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the behavioral study's results on willingness-to-pay the extra money by the customers for e-groceries deliveries based on crowd-shipping. The proposed methodology was tested for Ukraine, i.e., a developing country where the crowd-shipping services are under development conditions. To account for the behavior complexity of the consumers who have not faced the crowd-shipping services in the past, the choice model was enhanced with a latent variable. The findings indicate the revealed readiness of the e-shoppers to pay extra money for crowd-shipping delivery if it provides more flexible and consumer-oriented service. The expected environmental impact of the crowd-shipping delivery was not considered as important by the e-shoppers, which is explained by low concerns about the environment and car-oriented mobility in the considered case study.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#30456;&#20851;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20840;&#19987;&#23478;&#21453;&#39304;&#21644;Bandit&#21453;&#39304;&#35774;&#32622;&#20013;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.06526</link><description>&lt;p&gt;
&#25968;&#25454;&#30456;&#20851;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Data Dependent Regret Guarantees Against General Comparators for Full or Bandit Feedback. (arXiv:2303.06526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#30456;&#20851;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20840;&#19987;&#23478;&#21453;&#39304;&#21644;Bandit&#21453;&#39304;&#35774;&#32622;&#20013;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a data-dependent online learning algorithm framework that has data-dependent regret guarantees in both full expert feedback and bandit feedback settings, applicable for a wide variety of problem scenarios.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#23436;&#20840;&#22312;&#32447;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20855;&#26377;&#22312;&#20840;&#19987;&#23478;&#21453;&#39304;&#21644;Bandit&#21453;&#39304;&#35774;&#32622;&#20013;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#19968;&#33324;&#27604;&#36739;&#22120;&#30340;&#39044;&#26399;&#24615;&#33021;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20174;&#36890;&#29992;&#39044;&#27979;&#35282;&#24230;&#24037;&#20316;&#65292;&#20351;&#29992;&#30340;&#24615;&#33021;&#24230;&#37327;&#26159;&#23545;&#20219;&#24847;&#27604;&#36739;&#22120;&#24207;&#21015;&#30340;&#39044;&#26399;&#36951;&#25022;&#65292;&#21363;&#25105;&#20204;&#30340;&#25439;&#22833;&#19982;&#31454;&#20105;&#25439;&#22833;&#24207;&#21015;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#31454;&#20105;&#31867;&#21487;&#20197;&#35774;&#35745;&#20026;&#21253;&#25324;&#22266;&#23450;&#33218;&#36873;&#25321;&#12289;&#20999;&#25442;Bandit&#12289;&#19978;&#19979;&#25991;Bandit&#12289;&#21608;&#26399;Bandit&#25110;&#20219;&#20309;&#20854;&#20182;&#24863;&#20852;&#36259;&#30340;&#31454;&#20105;&#12290;&#31454;&#20105;&#31867;&#20013;&#30340;&#24207;&#21015;&#36890;&#24120;&#30001;&#20855;&#20307;&#24212;&#29992;&#31243;&#24207;&#30830;&#23450;&#65292;&#24182;&#24212;&#30456;&#24212;&#22320;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26082;&#19981;&#20351;&#29992;&#20063;&#19981;&#38656;&#35201;&#20219;&#20309;&#26377;&#20851;&#25439;&#22833;&#24207;&#21015;&#30340;&#21021;&#27493;&#20449;&#24687;&#65292;&#23436;&#20840;&#22312;&#32447;&#12290;&#20854;
&lt;/p&gt;
&lt;p&gt;
We study the adversarial online learning problem and create a completely online algorithmic framework that has data dependent regret guarantees in both full expert feedback and bandit feedback settings. We study the expected performance of our algorithm against general comparators, which makes it applicable for a wide variety of problem scenarios. Our algorithm works from a universal prediction perspective and the performance measure used is the expected regret against arbitrary comparator sequences, which is the difference between our losses and a competing loss sequence. The competition class can be designed to include fixed arm selections, switching bandits, contextual bandits, periodic bandits or any other competition of interest. The sequences in the competition class are generally determined by the specific application at hand and should be designed accordingly. Our algorithm neither uses nor needs any preliminary information about the loss sequences and is completely online. Its
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#30340;&#39640;&#25928;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06519</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#26465;&#20214;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26080;&#25439;&#28857;&#20113;&#20960;&#20309;&#21644;&#23646;&#24615;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossless Point Cloud Geometry and Attribute Compression Using a Learned Conditional Probability Model. (arXiv:2303.06519v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#30340;&#39640;&#25928;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient lossless point cloud compression method that uses sparse tensor-based deep neural networks to learn point cloud geometry and color probability distributions, achieving higher compression ratio and faster compression speed compared to the state-of-the-art method from Moving Pict.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25105;&#20204;&#22312;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#37117;&#35265;&#35777;&#20102;&#28857;&#20113;&#25968;&#25454;&#30340;&#23384;&#22312;&#65292;&#20174;&#27785;&#28024;&#24335;&#23186;&#20307;&#12289;&#33258;&#21160;&#39550;&#39542;&#21040;&#21307;&#30103;&#20445;&#20581;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32479;&#19968;&#30340;&#31232;&#30095;&#34920;&#31034;&#23558;&#28857;&#20113;&#34920;&#31034;&#20026;&#20855;&#26377;&#19981;&#21516;&#20301;&#28145;&#24230;&#30340;&#21344;&#29992;&#29305;&#24449;&#21644;&#19977;&#20010;&#23646;&#24615;&#29305;&#24449;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#21033;&#29992;&#28857;&#20113;&#20869;&#30340;&#29305;&#24449;&#21644;&#28857;&#20869;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#20026;&#31639;&#26415;&#32534;&#30721;&#22120;&#26500;&#24314;&#20934;&#30830;&#30340;&#33258;&#22238;&#24402;&#19978;&#19979;&#25991;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#26080;&#25439;&#28857;&#20113;&#20960;&#20309;&#21644;&#23646;&#24615;&#21387;&#32553;&#26041;&#27861;&#12290;&#19982;Moving Pict&#30340;&#26368;&#26032;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#26080;&#25439;&#21387;&#32553;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, we have witnessed the presence of point cloud data in many aspects of our life, from immersive media, autonomous driving to healthcare, although at the cost of a tremendous amount of data. In this paper, we present an efficient lossless point cloud compression method that uses sparse tensor-based deep neural networks to learn point cloud geometry and color probability distributions. Our method represents a point cloud with both occupancy feature and three attribute features at different bit depths in a unified sparse representation. This allows us to efficiently exploit feature-wise and point-wise dependencies within point clouds using a sparse tensor-based neural network and thus build an accurate auto-regressive context model for an arithmetic coder. To the best of our knowledge, this is the first learning-based lossless point cloud geometry and attribute compression approach. Compared with the-state-of-the-art lossless point cloud compression method from Moving Pict
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26080;&#25439;&#21487;&#25193;&#23637;&#28857;&#20113;&#23646;&#24615;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#26550;&#26500;&#25552;&#20379;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#32534;&#30721;&#27604;&#29305;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#20174;&#26080;&#25439;&#21387;&#32553;&#30340;&#27604;&#29305;&#27969;&#20013;&#36731;&#26494;&#25552;&#21462;&#36739;&#20302;&#36136;&#37327;&#30340;&#29256;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#26032;&#30340;G-PCC&#29256;&#26412;14&#30456;&#24403;&#65292;&#19988;&#32534;&#30721;&#26102;&#38388;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2303.06517</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#29992;&#20110;&#26080;&#25439;&#21487;&#25193;&#23637;&#28857;&#20113;&#23646;&#24615;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Deep probabilistic model for lossless scalable point cloud attribute compression. (arXiv:2303.06517v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26080;&#25439;&#21487;&#25193;&#23637;&#28857;&#20113;&#23646;&#24615;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#26550;&#26500;&#25552;&#20379;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#32534;&#30721;&#27604;&#29305;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#20174;&#26080;&#25439;&#21387;&#32553;&#30340;&#27604;&#29305;&#27969;&#20013;&#36731;&#26494;&#25552;&#21462;&#36739;&#20302;&#36136;&#37327;&#30340;&#29256;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#26032;&#30340;G-PCC&#29256;&#26412;14&#30456;&#24403;&#65292;&#19988;&#32534;&#30721;&#26102;&#38388;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a deep probabilistic model for lossless scalable point cloud attribute compression, which utilizes a multiscale architecture to provide accurate context for attribute probability modeling and allows for easily extracting lower quality versions from the losslessly compressed bitstream. The method outperforms recently proposed methods and is on par with the latest G-PCC version 14, with substantially faster coding time.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#28857;&#20113;&#20960;&#20309;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#26159;&#20851;&#20110;&#23646;&#24615;&#21387;&#32553;&#65292;&#29305;&#21035;&#26159;&#26080;&#25439;&#21387;&#32553;&#30340;&#24037;&#20316;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#23610;&#24230;&#28857;&#20113;&#23646;&#24615;&#32534;&#30721;&#26041;&#27861;&#65288;MNeT&#65289;&#65292;&#35813;&#26041;&#27861;&#36880;&#27493;&#23558;&#23646;&#24615;&#25237;&#24433;&#21040;&#22810;&#23610;&#24230;&#28508;&#22312;&#31354;&#38388;&#19978;&#12290;&#22810;&#23610;&#24230;&#26550;&#26500;&#20026;&#23646;&#24615;&#27010;&#29575;&#24314;&#27169;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#36890;&#36807;&#21333;&#20010;&#32593;&#32476;&#39044;&#27979;&#26368;&#23567;&#21270;&#32534;&#30721;&#27604;&#29305;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#21487;&#25193;&#23637;&#32534;&#30721;&#65292;&#21487;&#20197;&#20174;&#26080;&#25439;&#21387;&#32553;&#30340;&#27604;&#29305;&#27969;&#20013;&#36731;&#26494;&#25552;&#21462;&#36739;&#20302;&#36136;&#37327;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;MVUB&#21644;MPEG&#30340;&#19968;&#32452;&#28857;&#20113;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#26032;&#30340;G-PCC&#29256;&#26412;14&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32534;&#30721;&#26102;&#38388;&#27604;G-PCC&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, several point cloud geometry compression methods that utilize advanced deep learning techniques have been proposed, but there are limited works on attribute compression, especially lossless compression. In this work, we build an end-to-end multiscale point cloud attribute coding method (MNeT) that progressively projects the attributes onto multiscale latent spaces. The multiscale architecture provides an accurate context for the attribute probability modeling and thus minimizes the coding bitrate with a single network prediction. Besides, our method allows scalable coding that lower quality versions can be easily extracted from the losslessly compressed bitstream. We validate our method on a set of point clouds from MVUB and MPEG and show that our method outperforms recently proposed methods and on par with the latest G-PCC version 14. Besides, our coding time is substantially faster than G-PCC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36716;&#24405;&#30340;&#22635;&#20805;&#35789;&#26816;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#31070;&#32463;&#21322;&#39532;&#23572;&#21487;&#22827;&#26465;&#20214;&#38543;&#26426;&#22330;&#65292;&#33021;&#22815;&#22312;PodcastFillers&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;6.4&#65285;&#65288;&#20998;&#27573;&#32423;&#21035;&#65289;&#21644;3.1&#65285;&#65288;&#20107;&#20214;&#32423;&#21035;&#65289;&#30340;&#32477;&#23545;F1&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.06475</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#21322;&#26465;&#20214;&#38543;&#26426;&#22330;&#30340;&#26080;&#38656;&#36716;&#24405;&#30340;&#22635;&#20805;&#35789;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transcription free filler word detection with Neural semi-CRFs. (arXiv:2303.06475v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36716;&#24405;&#30340;&#22635;&#20805;&#35789;&#26816;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#31070;&#32463;&#21322;&#39532;&#23572;&#21487;&#22827;&#26465;&#20214;&#38543;&#26426;&#22330;&#65292;&#33021;&#22815;&#22312;PodcastFillers&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;6.4&#65285;&#65288;&#20998;&#27573;&#32423;&#21035;&#65289;&#21644;3.1&#65285;&#65288;&#20107;&#20214;&#32423;&#21035;&#65289;&#30340;&#32477;&#23545;F1&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a transcription-free filler word detection system that uses structured state space sequence model and neural semi-Markov conditional random fields, achieving an absolute F1 improvement of 6.4% (segment level) and 3.1% (event level) on the PodcastFillers dataset.
&lt;/p&gt;
&lt;p&gt;
&#38750;&#35821;&#35328;&#22635;&#20805;&#35789;&#65292;&#22914;&#8220;&#21999;&#8221;&#25110;&#8220;&#21834;&#8221;&#65292;&#22312;&#33258;&#21457;&#35821;&#35328;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#29992;&#20110;&#34920;&#36798;&#29369;&#35947;&#25110;&#19981;&#30830;&#23450;&#24615;&#12290;&#20197;&#21069;&#26816;&#27979;&#26576;&#20123;&#38750;&#35821;&#35328;&#22635;&#20805;&#35789;&#30340;&#24037;&#20316;&#39640;&#24230;&#20381;&#36182;&#20110;&#26469;&#33258;&#25104;&#29087;&#21830;&#19994;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#36716;&#24405;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;ASR&#31995;&#32479;&#22312;&#35768;&#22810;&#26041;&#38754;&#65288;&#20363;&#22914;&#39044;&#31639;&#12289;&#30446;&#26631;&#35821;&#35328;&#21644;&#35745;&#31639;&#33021;&#21147;&#65289;&#24182;&#19981;&#26222;&#36941;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#20381;&#36182;&#20110;ASR&#31995;&#32479;&#30340;&#22635;&#20805;&#35789;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;S4&#65289;&#21644;&#31070;&#32463;&#21322;&#39532;&#23572;&#21487;&#22827;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;semi-CRFs&#65289;&#65292;&#25105;&#20204;&#22312;PodcastFillers&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;6.4&#65285;&#65288;&#20998;&#27573;&#32423;&#21035;&#65289;&#21644;3.1&#65285;&#65288;&#20107;&#20214;&#32423;&#21035;&#65289;&#30340;&#32477;&#23545;F1&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#23545;&#26816;&#27979;&#32467;&#26524;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#20998;&#26512;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-linguistic filler words, such as "uh" or "um", are prevalent in spontaneous speech and serve as indicators for expressing hesitation or uncertainty. Previous works for detecting certain non-linguistic filler words are highly dependent on transcriptions from a well-established commercial automatic speech recognition (ASR) system. However, certain ASR systems are not universally accessible from many aspects, e.g., budget, target languages, and computational power. In this work, we investigate filler word detection system that does not depend on ASR systems. We show that, by using the structured state space sequence model (S4) and neural semi-Markov conditional random fields (semi-CRFs), we achieve an absolute F1 improvement of 6.4% (segment level) and 3.1% (event level) on the PodcastFillers dataset. We also conduct a qualitative analysis on the detected results to analyze the limitations of our proposed system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;OFDM&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36890;&#36807;&#21407;&#22411;&#38382;&#39064;&#35780;&#20272;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.06438</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#28304;&#20998;&#31163;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65306;&#20849;&#20449;&#36947;OFDM&#20449;&#21495;&#30340;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
On Neural Architectures for Deep Learning-based Source Separation of Co-Channel OFDM Signals. (arXiv:2303.06438v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;OFDM&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36890;&#36807;&#21407;&#22411;&#38382;&#39064;&#35780;&#20272;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the single-channel source separation problem involving OFDM signals and evaluates the efficacy of using audio-oriented neural architectures in separating co-channel OFDM waveforms. Critical domain-informed modifications to the network parameterization are proposed based on insights from OFDM structures.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36825;&#31181;&#20449;&#21495;&#22312;&#35768;&#22810;&#29616;&#20195;&#25968;&#23383;&#36890;&#20449;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#21333;&#22768;&#36947;&#28304;&#20998;&#31163;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#21162;&#21147;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#38899;&#39057;&#20449;&#21495;&#20998;&#31163;&#22120;&#65288;&#20316;&#20026;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#65289;&#12290;&#36890;&#36807;&#22522;&#20110;OFDM&#28304;&#27169;&#22411;&#30340;&#21407;&#22411;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#24182;&#36136;&#30097;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#22522;&#20110;&#36890;&#20449;&#27874;&#24418;&#30456;&#20851;&#29305;&#24449;&#20998;&#31163;&#20449;&#21495;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#37197;&#32622;&#20013;&#65292;&#21363;&#20351;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#23454;&#29616;&#23436;&#32654;&#20998;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#22522;&#20110;OFDM&#32467;&#26500;&#30340;&#27934;&#23519;&#65292;&#21487;&#20197;&#20849;&#21516;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the single-channel source separation problem involving orthogonal frequency-division multiplexing (OFDM) signals, which are ubiquitous in many modern-day digital communication systems. Related efforts have been pursued in monaural source separation, where state-of-the-art neural architectures have been adopted to train an end-to-end separator for audio signals (as 1-dimensional time series). In this work, through a prototype problem based on the OFDM source model, we assess -- and question -- the efficacy of using audio-oriented neural architectures in separating signals based on features pertinent to communication waveforms. Perhaps surprisingly, we demonstrate that in some configurations, where perfect separation is theoretically attainable, these audio-oriented neural architectures perform poorly in separating co-channel OFDM waveforms. Yet, we propose critical domain-informed modifications to the network parameterization, based on insights from OFDM structures, that can co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;ADNI&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06410</link><description>&lt;p&gt;
Brain Diffuser&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#33041;&#22270;&#20687;&#21040;&#33041;&#32593;&#32476;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline. (arXiv:2303.06410v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;ADNI&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
&lt;/p&gt;
&lt;p&gt;
&#33041;&#32593;&#32476;&#20998;&#26512;&#23545;&#20110;&#35786;&#26029;&#21644;&#24178;&#39044;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#32791;&#26102;&#21644;&#20027;&#35266;&#30340;&#24037;&#20855;&#21253;&#12290;&#21482;&#26377;&#23569;&#25968;&#24037;&#20855;&#21487;&#20197;&#20174;&#33041;&#25193;&#25955;&#24352;&#37327;&#22270;&#20687;&#65288;DTI&#65289;&#20013;&#33719;&#21462;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#19982;&#29616;&#26377;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;Brain Diffuser&#36890;&#36807;&#20998;&#26512;&#21463;&#35797;&#32773;&#20043;&#38388;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#30340;&#24046;&#24322;&#65292;&#21033;&#29992;&#26356;&#22810;&#30340;&#32467;&#26500;&#36830;&#25509;&#29305;&#24449;&#21644;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#65288;ADNI&#65289;&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain network analysis is essential for diagnosing and intervention for Alzheimer's disease (AD). However, previous research relied primarily on specific time-consuming and subjective toolkits. Only few tools can obtain the structural brain networks from brain diffusion tensor images (DTI). In this paper, we propose a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. Compared to existing toolkits, Brain Diffuser exploits more structural connectivity features and disease-related information by analyzing disparities in structural brain networks across subjects. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#30340;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#65292;&#20197;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#29031;&#26126;&#21151;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#26368;&#23567;&#20449;&#24178;&#22122;&#27604;&#65288;SINR&#65289;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23384;&#22312;&#20449;&#36947;&#20272;&#35745;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20135;&#29983;&#36739;&#23567;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.06381</link><description>&lt;p&gt;
&#23398;&#20064;&#39044;&#32534;&#30721;&#29992;&#20110;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning to Precode for Integrated Sensing and Communications Systems. (arXiv:2303.06381v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#30340;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#65292;&#20197;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#29031;&#26126;&#21151;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#26368;&#23567;&#20449;&#24178;&#22122;&#27604;&#65288;SINR&#65289;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23384;&#22312;&#20449;&#36947;&#20272;&#35745;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20135;&#29983;&#36739;&#23567;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an unsupervised learning neural model to design transmit precoders for integrated sensing and communication (ISAC) systems to maximize the worst-case target illumination power while ensuring a minimum signal-to-interference-plus-noise ratio (SINR) for all the users. The proposed method outperforms traditional optimization-based methods in presence of channel estimation errors while incurring lesser computational complexity and generalizing well across different channel conditions that were not shown during training.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#30340;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#65292;&#20197;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#29031;&#26126;&#21151;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#26368;&#23567;&#20449;&#24178;&#22122;&#27604;&#65288;SINR&#65289;&#12290;&#20174;&#19978;&#34892;&#23548;&#39057;&#21644;&#22238;&#27874;&#20013;&#23398;&#20064;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#30340;&#38382;&#39064;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#21442;&#25968;&#21270;&#20989;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#23398;&#20064;&#36825;&#20010;&#20989;&#25968;&#12290;&#20026;&#20102;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#32435;&#20837;SINR&#21644;&#21151;&#29575;&#32422;&#26463;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#20449;&#36947;&#20272;&#35745;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20135;&#29983;&#36739;&#23567;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#20123;&#26465;&#20214;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#26174;&#31034;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an unsupervised learning neural model to design transmit precoders for integrated sensing and communication (ISAC) systems to maximize the worst-case target illumination power while ensuring a minimum signal-to-interference-plus-noise ratio (SINR) for all the users. The problem of learning transmit precoders from uplink pilots and echoes can be viewed as a parameterized function estimation problem and we propose to learn this function using a neural network model. To learn the neural network parameters, we develop a novel loss function based on the first-order optimality conditions to incorporate the SINR and power constraints. Through numerical simulations, we demonstrate that the proposed method outperforms traditional optimization-based methods in presence of channel estimation errors while incurring lesser computational complexity and generalizing well across different channel conditions that were not shown during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22810;&#20013;&#24515;&#29615;&#22659;&#20013;&#23545;&#22522;&#20110;EEG&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#24615;&#21035;&#23376;&#32452;&#32676;&#30340;&#26816;&#27979;&#33021;&#21147;&#20998;&#26512;&#65292;&#21457;&#29616;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;PD&#26816;&#27979;&#33021;&#21147;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.06376</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;EEG&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#20013;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#22810;&#20013;&#24515;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing gender fairness in EEG-based machine learning detection of Parkinson's disease: A multi-center study. (arXiv:2303.06376v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22810;&#20013;&#24515;&#29615;&#22659;&#20013;&#23545;&#22522;&#20110;EEG&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#24615;&#21035;&#23376;&#32452;&#32676;&#30340;&#26816;&#27979;&#33021;&#21147;&#20998;&#26512;&#65292;&#21457;&#29616;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;PD&#26816;&#27979;&#33021;&#21147;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study analyzed the detection ability of gender sub-groups in a multi-center setting of a previously developed machine learning algorithm based on EEG, finding significant differences in Parkinson's disease detection ability between males and females.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#38745;&#24687;&#24577;&#33041;&#30005;&#22270;&#65288;rs-EEG&#65289;&#30340;&#33258;&#21160;&#24037;&#20855;&#22312;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#26816;&#27979;&#20013;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#36890;&#36807;&#20844;&#24179;&#24615;&#21644;&#20559;&#24046;&#20998;&#26512;&#35780;&#20272;&#21487;&#33021;&#21152;&#21095;&#20581;&#24247;&#24046;&#24322;&#30340;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#21463;&#20445;&#25252;&#30340;&#23646;&#24615;&#65292;&#22914;&#24615;&#21035;&#65292;&#22312;PD&#35786;&#26029;&#24320;&#21457;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#19981;&#21516;&#24615;&#21035;&#30340;&#23376;&#32452;&#32676;&#20307;&#30340;&#20998;&#26512;&#24456;&#23569;&#22312;ML&#27169;&#22411;&#30340;&#24320;&#21457;&#25110;PD&#26816;&#27979;&#30340;&#24615;&#33021;&#35780;&#20272;&#20013;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#38745;&#24687;&#24577;&#33041;&#30005;&#22270;&#21151;&#29575;&#35889;&#23494;&#24230;&#65288;PSD&#65289;&#29305;&#24449;&#30340;&#20808;&#21069;&#24320;&#21457;&#30340;ML&#31639;&#27861;&#22312;&#22810;&#20013;&#24515;&#29615;&#22659;&#20013;&#30340;&#24615;&#21035;&#23376;&#32452;&#32676;&#30340;&#26816;&#27979;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27979;&#35797;&#26102;&#38388;&#65288;80.5&#65285;&#23545;63.7&#65285;&#30340;&#20934;&#30830;&#24615;&#65289;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;PD&#26816;&#27979;&#33021;&#21147;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#24182;&#19988;&#19968;&#32452;&#39030;&#37096;&#21644;&#21069;&#39069;&#33041;&#30005;&#22270;&#36890;&#36947;&#21644;&#39057;&#29575;&#23384;&#22312;&#26174;&#30528;&#26356;&#39640;&#30340;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of automatic tools based on machine learning (ML) and resting-state electroencephalography (rs-EEG) for Parkinson's disease (PD) detection keeps growing, the assessment of possible exacerbation of health disparities by means of fairness and bias analysis becomes more relevant. Protected attributes, such as gender, play an important role in PD diagnosis development. However, analysis of sub-group populations stemming from different genders is seldom taken into consideration in ML models' development or the performance assessment for PD detection. In this work, we perform a systematic analysis of the detection ability for gender sub-groups in a multi-center setting of a previously developed ML algorithm based on power spectral density (PSD) features of rs-EEG. We find significant differences in the PD detection ability for males and females at testing time (80.5% vs. 63.7% accuracy) and significantly higher activity for a set of parietal and frontal EEG channels and frequen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.06361</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#38745;&#24577;&#29615;&#22659;&#30340;&#38544;&#31169;&#20445;&#25252;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#65306;&#32852;&#37030;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Cooperative Visible Light Positioning for Nonstationary Environment: A Federated Learning Perspective. (arXiv:2303.06361v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a cooperative visible light positioning scheme based on federated learning, which improves the positioning accuracy and generalization capability in nonstationary environments by jointly training a global model adaptive to environmental changes without sharing private data of users.
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35265;&#20809;&#23450;&#20301;&#65288;VLP&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23460;&#20869;&#23450;&#20301;&#25216;&#26415;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#65292;&#30001;&#20110;&#39640;&#24230;&#26102;&#21464;&#30340;&#20449;&#36947;&#65292;VLP&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#25552;&#39640;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21512;&#20316;VLP&#26041;&#26696;&#12290;&#21033;&#29992;FL&#26694;&#26550;&#65292;&#29992;&#25143;&#21487;&#20197;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#20849;&#20139;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#32593;&#32476;&#65288;CVPosNet&#65289;&#65292;&#20197;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#20248;&#20110;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visible light positioning (VLP) has drawn plenty of attention as a promising indoor positioning technique. However, in nonstationary environments, the performance of VLP is limited because of the highly time-varying channels. To improve the positioning accuracy and generalization capability in nonstationary environments, a cooperative VLP scheme based on federated learning (FL) is proposed in this paper. Exploiting the FL framework, a global model adaptive to environmental changes can be jointly trained by users without sharing private data of users. Moreover, a Cooperative Visible-light Positioning Network (CVPosNet) is proposed to accelerate the convergence rate and improve the positioning accuracy. Simulation results show that the proposed scheme outperforms the benchmark schemes, especially in nonstationary environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2303.06340</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#32954;&#30284;&#26234;&#33021;&#35786;&#26029;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Intelligent diagnostic scheme for lung cancer screening with Raman spectra data by tensor network machine learning. (arXiv:2303.06340v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a tensor-network machine learning method to reliably predict lung cancer patients and their stages via screening Raman spectra data of Volatile organic compounds (VOCs) in exhaled breath.
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22312;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#20013;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24433;&#21709;&#65292;&#20174;&#23398;&#26415;&#30740;&#31350;&#21040;&#20020;&#24202;&#24212;&#29992;&#65292;&#20363;&#22914;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#26816;&#27979;&#21644;&#35786;&#26029;&#12289;&#27835;&#30103;&#20248;&#21270;&#20197;&#21450;&#33647;&#29289;&#21457;&#29616;&#20013;&#26032;&#30340;&#27835;&#30103;&#38774;&#28857;&#30340;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;AI&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#20005;&#37325;&#21463;&#21040;&#38750;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#19981;&#21487;&#25511;&#22320;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#23545;&#20110;ML&#30340;&#21487;&#35299;&#37322;&#24615;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#28040;&#36153;&#32773;&#24517;&#39035;&#20174;&#22362;&#23454;&#30340;&#22522;&#30784;&#25110;&#20196;&#20154;&#20449;&#26381;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#24517;&#35201;&#30340;&#23433;&#20840;&#24863;&#21644;&#20449;&#20219;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;-ML&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#36866;&#29992;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#38750;&#20405;&#20837;&#24615;&#32954;&#30284;&#31579;&#26597;&#30340;&#29702;&#24819;&#26041;&#24335;&#12290;TN-ML&#30340;&#39044;&#27979;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has brought tremendous impacts on biomedical sciences from academic researches to clinical applications, such as in biomarkers' detection and diagnosis, optimization of treatment, and identification of new therapeutic targets in drug discovery. However, the contemporary AI technologies, particularly deep machine learning (ML), severely suffer from non-interpretability, which might uncontrollably lead to incorrect predictions. Interpretability is particularly crucial to ML for clinical diagnosis as the consumers must gain necessary sense of security and trust from firm grounds or convincing interpretations. In this work, we propose a tensor-network (TN)-ML method to reliably predict lung cancer patients and their stages via screening Raman spectra data of Volatile organic compounds (VOCs) in exhaled breath, which are generally suitable as biomarkers and are considered to be an ideal way for non-invasive lung cancer screening. The prediction of TN-ML is based
&lt;/p&gt;</description></item><item><title>MLP-SRGAN&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN&#65292;&#20351;&#29992;MLP-Mixer&#21644;&#21367;&#31215;&#23618;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#21487;&#29992;&#20110;FLAIR MRI&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.06298</link><description>&lt;p&gt;
MLP-SRGAN: &#20351;&#29992;MLP-Mixer&#30340;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer. (arXiv:2303.06298v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06298
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN&#65292;&#20351;&#29992;MLP-Mixer&#21644;&#21367;&#31215;&#23618;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#21487;&#29992;&#20110;FLAIR MRI&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN is a single-dimension Super Resolution GAN that utilizes MLP-Mixers and convolutional layers for upsampling, and can be used for super-resolution reconstruction of FLAIR MRI images. New image quality metrics were proposed.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;MLP-SRGAN&#65292;&#23427;&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SRGAN&#65289;&#65292;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#28151;&#21512;&#22120;&#65288;MLP-Mixer&#65289;&#20197;&#21450;&#21367;&#31215;&#23618;&#22312;&#20999;&#29255;&#26041;&#21521;&#19978;&#36827;&#34892;&#19978;&#37319;&#26679;&#12290; MLP-SRGAN&#20351;&#29992;MSSEG2&#25361;&#25112;&#25968;&#25454;&#38598;&#20013;&#30340;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;FLAIR MRI&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#20010;&#20302;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22810;&#20013;&#24515;FLAIR&#25968;&#25454;&#38598;&#65288;CAIN&#65292;ADNI&#65292;CCNA&#65289;&#30340;&#22270;&#20687;&#65292;&#20197;&#26816;&#26597;&#22312;&#20445;&#30041;&#65288;&#26410;&#35265;&#65289;&#20020;&#24202;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#23558;&#19978;&#37319;&#26679;&#32467;&#26524;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;SR&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22522;&#26412;&#20107;&#23454;&#30340;&#22270;&#20687;&#65292;&#20351;&#29992;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#26469;&#34913;&#37327;&#19978;&#37319;&#26679;&#24615;&#33021;&#12290;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#32467;&#26500;&#65292;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#65292;&#20197;&#22312;&#32570;&#20047;&#22522;&#30784;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#37327;&#21270;&#38160;&#24230;&#65288;&#36793;&#32536;&#24378;&#24230;&#65289;&#65292;&#22122;&#22768;&#65288;&#29109;&#65289;&#21644;&#27169;&#31946;&#24230;&#65288;&#20302;&#39057;&#20449;&#24687;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel architecture called MLP-SRGAN, which is a single-dimension Super Resolution Generative Adversarial Network (SRGAN) that utilizes Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to upsample in the slice direction. MLP-SRGAN is trained and validated using high resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with low spatial resolution in the slice dimension to examine performance on held-out (unseen) clinical data. Upsampled results are compared to several state-of-the-art SR networks. For images with high resolution (HR) ground truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index (SSIM) are used to measure upsampling performance. Several new structural, no-reference image quality metrics were proposed to quantify sharpness (edge strength), noise (entropy), and blurriness (low frequency information) in the absence of groun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#22810;&#31867;OOD&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#22312;&#38647;&#36798;&#36317;&#31163;&#22810;&#26222;&#21202;&#22270;&#20687;&#65288;RDIs&#65289;&#19978;&#36816;&#34892;&#12290;&#26816;&#27979;&#22120;&#26088;&#22312;&#23558;&#38500;&#22352;&#12289;&#31449;&#25110;&#36208;&#30340;&#20154;&#20197;&#22806;&#30340;&#20219;&#20309;&#31227;&#21160;&#29289;&#20307;&#20998;&#31867;&#20026;OOD&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#26816;&#27979;&#21628;&#21560;&#31561;&#24494;&#23567;&#30340;&#20154;&#20307;&#36816;&#21160;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.06232</link><description>&lt;p&gt;
MCROOD: &#22810;&#31867;&#38647;&#36798;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MCROOD: Multi-Class Radar Out-Of-Distribution Detection. (arXiv:2303.06232v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#22810;&#31867;OOD&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#22312;&#38647;&#36798;&#36317;&#31163;&#22810;&#26222;&#21202;&#22270;&#20687;&#65288;RDIs&#65289;&#19978;&#36816;&#34892;&#12290;&#26816;&#27979;&#22120;&#26088;&#22312;&#23558;&#38500;&#22352;&#12289;&#31449;&#25110;&#36208;&#30340;&#20154;&#20197;&#22806;&#30340;&#20219;&#20309;&#31227;&#21160;&#29289;&#20307;&#20998;&#31867;&#20026;OOD&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#26816;&#27979;&#21628;&#21560;&#31561;&#24494;&#23567;&#30340;&#20154;&#20307;&#36816;&#21160;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a reconstruction-based multi-class OOD detector that operates on radar range doppler images (RDIs). The detector aims to classify any moving object other than a person sitting, standing, or walking as OOD. The authors also provide a simple yet effective pre-processing technique to detect minor human body movements like breathing. The method outperforms state-of-the-art OOD detection methods in experiments.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#23433;&#20840;&#37096;&#32626;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26550;&#26500;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#21463;&#21040;&#29305;&#21035;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#22810;&#31867;OOD&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#22312;&#38647;&#36798;&#36317;&#31163;&#22810;&#26222;&#21202;&#22270;&#20687;&#65288;RDIs&#65289;&#19978;&#36816;&#34892;&#12290;&#26816;&#27979;&#22120;&#26088;&#22312;&#23558;&#38500;&#22352;&#12289;&#31449;&#25110;&#36208;&#30340;&#20154;&#20197;&#22806;&#30340;&#20219;&#20309;&#31227;&#21160;&#29289;&#20307;&#20998;&#31867;&#20026;OOD&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#26816;&#27979;&#21628;&#21560;&#31561;&#24494;&#23567;&#30340;&#20154;&#20307;&#36816;&#21160;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#34987;&#31216;&#20026;&#21628;&#21560;&#26816;&#27979;&#22120;&#65288;RESPD&#65289;&#65292;&#21487;&#20197;&#20943;&#36731;OOD&#26816;&#27979;&#30340;&#36127;&#25285;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20154;&#22352;&#21644;&#20154;&#31449;&#30340;&#31867;&#21035;&#12290;&#22312;&#25105;&#20204;&#25910;&#38598;&#30340;60GHz&#30701;&#36317;&#31163;FMCW&#38647;&#36798;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#20998;&#21035;&#20026;&#22352;&#12289;&#31449;&#21644;&#36208;&#19977;&#20010;&#31867;&#21035;&#23454;&#29616;&#20102;97.45&#65285;&#12289;92.13&#65285;&#21644;96.58&#65285;&#30340;AUROC&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#27604;&#31532;&#20108;&#22909;&#30340;&#26041;&#27861;&#24555;24&#20493;&#65292;&#24182;&#19988;&#26159;v
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection has recently received special attention due to its critical role in safely deploying modern deep learning (DL) architectures. This work proposes a reconstruction-based multi-class OOD detector that operates on radar range doppler images (RDIs). The detector aims to classify any moving object other than a person sitting, standing, or walking as OOD. We also provide a simple yet effective pre-processing technique to detect minor human body movements like breathing. The simple idea is called respiration detector (RESPD) and eases the OOD detection, especially for human sitting and standing classes. On our dataset collected by 60GHz short-range FMCW Radar, we achieve AUROCs of 97.45%, 92.13%, and 96.58% for sitting, standing, and walking classes, respectively. We perform extensive experiments and show that our method outperforms state-of-the-art (SOTA) OOD detection methods. Also, our pipeline performs 24 times faster than the second-best method and is v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#33258;&#24049;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;Q-learning&#31639;&#27861;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.06155</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Digital Twin-Assisted Knowledge Distillation Framework for Heterogeneous Federated Learning. (arXiv:2303.06155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#33258;&#24049;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;Q-learning&#31639;&#27861;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a digital twin-assisted knowledge distillation framework for heterogeneous federated learning, where users can select their own neural network models and distill knowledge from a big teacher model, and the teacher model can be trained on a digital twin located in the server. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming problem and solved using Q-learning and optimization algorithms.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#39537;&#21160;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#65292;&#20854;&#20013;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36873;&#25321;&#20854;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#33258;&#24049;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#12290;&#20026;&#20102;&#20811;&#26381;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29992;&#25143;&#35774;&#22791;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#30340;&#26041;&#24335;&#65292;&#25945;&#24072;&#27169;&#22411;&#21487;&#20197;&#22312;&#20855;&#26377;&#36275;&#22815;&#35745;&#31639;&#36164;&#28304;&#30340;&#26381;&#21153;&#22120;&#19978;&#30340;&#25968;&#23383;&#23402;&#29983;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22312;&#27169;&#22411;&#33976;&#39311;&#26399;&#38388;&#65292;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#22312;&#29289;&#29702;&#23454;&#20307;&#25110;&#25968;&#23383;&#20195;&#29702;&#22788;&#26356;&#26032;&#20854;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#20026;&#29992;&#25143;&#36873;&#25321;&#27169;&#22411;&#21644;&#35757;&#32451;&#21368;&#36733;&#21644;&#36164;&#28304;&#20998;&#37197;&#21046;&#23450;&#20102;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32852;&#21512;&#20351;&#29992;Q-learning&#21644;&#20248;&#21270;&#65292;&#20854;&#20013;Q-learning&#20026;&#29992;&#25143;&#36873;&#25321;&#27169;&#22411;&#24182;&#30830;&#23450;&#26159;&#22312;&#26412;&#22320;&#36824;&#26159;&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#20248;&#21270;&#21017;&#29992;&#20110;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, to deal with the heterogeneity in federated learning (FL) systems, a knowledge distillation (KD) driven training framework for FL is proposed, where each user can select its neural network model on demand and distill knowledge from a big teacher model using its own private dataset. To overcome the challenge of train the big teacher model in resource limited user devices, the digital twin (DT) is exploit in the way that the teacher model can be trained at DT located in the server with enough computing resources. Then, during model distillation, each user can update the parameters of its model at either the physical entity or the digital agent. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming (MIP) problem. To solve the problem, Q-learning and optimization are jointly used, where Q-learning selects models for users and determines whether to train locally or on the server, and optimiz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.05737</link><description>&lt;p&gt;
&#20020;&#24202;BERTScore&#65306;&#20020;&#24202;&#29615;&#22659;&#19979;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#25913;&#36827;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26377;&#28508;&#21147;&#33410;&#30465;&#26102;&#38388;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#25552;&#39640;&#25253;&#21578;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#21307;&#29983;&#30340;&#30130;&#21171;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36991;&#20813;&#21307;&#23398;&#30456;&#20851;&#30340;&#36716;&#24405;&#38169;&#35823;&#30340;&#37325;&#35201;&#24615;&#65292;&#21307;&#30103;&#34892;&#19994;&#37319;&#29992;&#36825;&#31181;&#25216;&#26415;&#30340;&#36895;&#24230;&#36739;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;ASR&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#65288;WER&#12289;BLUE&#12289;METEOR&#31561;&#65289;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24230;&#37327;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#65292;&#26377;&#26102;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05205</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#21147;&#31995;&#32479;&#23454;&#26102;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Real-time scheduling of renewable power systems through planning-based reinforcement learning. (arXiv:2303.05205v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment, which enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's ability.
&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#22686;&#38271;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#26469;&#28304;&#23545;&#20256;&#32479;&#30005;&#21147;&#35843;&#24230;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36816;&#33829;&#21830;&#38590;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#26085;&#21069;&#39044;&#27979;&#65292;&#22240;&#27492;&#38656;&#35201;&#26410;&#26469;&#35843;&#24230;&#31995;&#32479;&#26681;&#25454;&#36229;&#30701;&#26399;&#39044;&#27979;&#36827;&#34892;&#23454;&#26102;&#35843;&#24230;&#20915;&#31574;&#12290;&#21463;&#35745;&#31639;&#36895;&#24230;&#38480;&#21046;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#21457;&#23637;&#24050;&#32463;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RL&#26041;&#27861;&#22312;&#32422;&#26463;&#22797;&#26434;&#24615;&#12289;&#31639;&#27861;&#24615;&#33021;&#21644;&#29615;&#22659;&#20445;&#30495;&#24230;&#26041;&#38754;&#19981;&#36275;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#26426;&#32452;&#32452;&#21512;&#21644;&#32463;&#27982;&#35843;&#24230;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing renewable energy sources have posed significant challenges to traditional power scheduling. It is difficult for operators to obtain accurate day-ahead forecasts of renewable generation, thereby requiring the future scheduling system to make real-time scheduling decisions aligning with ultra-short-term forecasts. Restricted by the computation speed, traditional optimization-based methods can not solve this problem. Recent developments in reinforcement learning (RL) have demonstrated the potential to solve this challenge. However, the existing RL methods are inadequate in terms of constraint complexity, algorithm performance, and environment fidelity. We are the first to propose a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment. The proposed approach enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's abilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.03857</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#65306;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study. (arXiv:2303.03857v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the advantages of using pre-trained AudioLDM as the backbone for sound generation, demonstrates the benefits of using pre-trained models for text-to-sound generation in data-scarcity scenarios, and evaluates various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols to provide a basis for future research.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#36817;&#22312;&#25991;&#26412;&#25552;&#31034;&#19979;&#23454;&#29616;&#20102;&#22768;&#38899;&#29983;&#25104;&#30340;&#31361;&#30772;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#34920;&#29616;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#24403;&#21069;&#30340;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#36807;&#24230;&#25311;&#21512;&#65289;&#19978;&#38754;&#20020;&#38382;&#39064;&#65292;&#20174;&#32780;&#26174;&#33879;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65288;&#20363;&#22914;&#35757;&#32451;&#26465;&#20214;&#65289;&#21487;&#33021;&#20250;&#24433;&#21709;AudioLDM&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#36825;&#20123;&#21327;&#35758;&#20801;&#35768;&#22312;&#20849;&#21516;&#22522;&#30784;&#19978;&#20844;&#24179;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have recently achieved breakthroughs in sound generation with text prompts. Despite their promising performance, current text-to-sound generation models face issues on small-scale datasets (e.g., overfitting), significantly limiting their performance. In this paper, we investigate the use of pre-trained AudioLDM, the state-of-the-art model for text-to-audio generation, as the backbone for sound generation. Our study demonstrates the advantages of using pre-trained models for text-to-sound generation, especially in data-scarcity scenarios. In addition, experiments show that different training strategies (e.g., training conditions) may affect the performance of AudioLDM on datasets of different scales. To facilitate future studies, we also evaluate various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols, which allow fair comparisons and benchmarking of these methods on the common ground.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;PreFallKD&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#26816;&#27979;&#30693;&#35782;&#36716;&#31227;&#21040;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#29983;&#27169;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.03634</link><description>&lt;p&gt;
PreFallKD: &#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation. (arXiv:2303.03634v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;PreFallKD&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#26816;&#27979;&#30693;&#35782;&#36716;&#31227;&#21040;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#29983;&#27169;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a pre-impact fall detection system called PreFallKD, which uses CNN-ViT knowledge distillation to transfer detection knowledge from a pre-trained teacher model to a lightweight convolutional neural network student model. The system achieves a balance between detection performance and computational complexity.
&lt;/p&gt;
&lt;p&gt;
&#36300;&#20498;&#20107;&#25925;&#26159;&#32769;&#40836;&#21270;&#31038;&#20250;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;&#65292;&#20197;&#25903;&#25345;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#36300;&#20498;&#20445;&#25252;&#31995;&#32479;&#65292;&#20197;&#39044;&#38450;&#20005;&#37325;&#30340;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#21482;&#20351;&#29992;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#21040;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#35774;&#22791;&#21644;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;&#65292;&#21363;PreFallKD&#65292;&#20197;&#22312;&#26816;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#25152;&#25552;&#20986;&#30340;PreFallKD&#23558;&#26816;&#27979;&#30693;&#35782;&#20174;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#65288;&#35270;&#35273;&#21464;&#25442;&#22120;&#65289;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#65288;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;KFall&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23558;PreFallKD&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fall accidents are critical issues in an aging and aged society. Recently, many researchers developed pre-impact fall detection systems using deep learning to support wearable-based fall protection systems for preventing severe injuries. However, most works only employed simple neural network models instead of complex models considering the usability in resource-constrained mobile devices and strict latency requirements. In this work, we propose a novel pre-impact fall detection via CNN-ViT knowledge distillation, namely PreFallKD, to strike a balance between detection performance and computational complexity. The proposed PreFallKD transfers the detection knowledge from the pre-trained teacher model (vision transformer) to the student model (lightweight convolutional neural networks). Additionally, we apply data augmentation techniques to tackle issues of data imbalance. We conduct the experiment on the KFall public dataset and compare PreFallKD with other state-of-the-art models. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#65292;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.02665</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#23398;&#20064;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Learning for Acoustic Event Classification. (arXiv:2303.02665v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#65292;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new model, Heterogeneous Graph Crossmodal Network (HGCN), which learns crossmodal edges and can adapt to various spatial and temporal scales, effectively connecting relevant nodes across modalities. It achieves state-of-the-art performance in acoustic event classification.
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#32039;&#20945;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#26469;&#24314;&#27169;&#28041;&#21450;&#22810;&#20010;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#24314;&#27169;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#25104;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22270;&#32467;&#26500;&#22312;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#20013;&#24182;&#19981;&#33258;&#28982;&#12290;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#30340;&#22270;&#26159;&#25163;&#21160;&#26500;&#24314;&#30340;&#65292;&#36825;&#26082;&#22256;&#38590;&#21448;&#27425;&#20248;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#65288;i&#65289;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#21270;&#22270;&#26500;&#24314;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#22240;&#20026;&#23427;&#26159;&#21442;&#25968;&#21270;&#26500;&#24314;&#30340;&#65292;&#32780;&#21487;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#36793;&#32536;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#12290;&#22312;&#19968;&#20010;&#22823;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;AudioSet&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#65288;0.53&#24179;&#22343;&#31934;&#24230;&#65289;&#65292;&#20248;&#20110;transfo&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graphs provide a compact, efficient, and scalable way to model data involving multiple disparate modalities. This makes modeling audiovisual data using heterogeneous graphs an attractive option. However, graph structure does not appear naturally in audiovisual data. Graphs for audiovisual data are constructed manually which is both difficult and sub-optimal. In this work, we address this problem by (i) proposing a parametric graph construction strategy for the intra-modal edges, and (ii) learning the crossmodal edges. To this end, we develop a new model, heterogeneous graph crossmodal network (HGCN) that learns the crossmodal edges. Our proposed model can adapt to various spatial and temporal scales owing to its parametric construction, while the learnable crossmodal edges effectively connect the relevant nodes across modalities. Experiments on a large benchmark dataset (AudioSet) show that our model is state-of-the-art (0.53 mean average precision), outperforming transfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2303.01508</link><description>&lt;p&gt;
&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#25511;&#21046;&#65306;&#23398;&#20064;&#25490;&#21517;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#24773;&#24863;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Emotional Control of Text-To-Speech: Learning To Rank Inter- And Intra-Class Emotion Intensities. (arXiv:2303.01508v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a fine-grained controllable emotional TTS, that considers both interand intra-class distances and be able to synthesize speech with recognizable intensity difference.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#35821;&#38899;&#36890;&#24120;&#22312;&#24773;&#24863;&#34920;&#36798;&#19978;&#26159;&#20013;&#24615;&#30340;&#65292;&#32780;&#24456;&#22810;&#26102;&#20505;&#20154;&#20204;&#24076;&#26395;&#23545;&#21333;&#35789;&#25110;&#38899;&#32032;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#25511;&#21046;&#12290;&#34429;&#28982;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#26368;&#36817;&#24050;&#32463;&#25552;&#20986;&#20102;&#31532;&#19968;&#25209;TTS&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#25163;&#21160;&#20998;&#37197;&#24773;&#24863;&#24378;&#24230;&#26469;&#25511;&#21046;&#35821;&#38899;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#24573;&#30053;&#20102;&#20869;&#37096;&#31867;&#36317;&#31163;&#65292;&#24378;&#24230;&#24046;&#24322;&#32463;&#24120;&#26080;&#27861;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#20027;&#35266;&#21644;&#23458;&#35266;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21487;&#25511;&#24615;&#12289;&#24773;&#24863;&#34920;&#36798;&#21644;&#33258;&#28982;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#21487;&#25511;TTS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art Text-To-Speech (TTS) models are capable of producing high-quality speech. The generated speech, however, is usually neutral in emotional expression, whereas very often one would want fine-grained emotional control of words or phonemes. Although still challenging, the first TTS models have been recently proposed that are able to control voice by manually assigning emotion intensity. Unfortunately, due to the neglect of intra-class distance, the intensity differences are often unrecognizable. In this paper, we propose a fine-grained controllable emotional TTS, that considers both interand intra-class distances and be able to synthesize speech with recognizable intensity difference. Our subjective and objective experiments demonstrate that our model exceeds two state-of-the-art controllable TTS models for controllability, emotion expressiveness and naturalness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.01735</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#26041;&#24046;&#32553;&#20943;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective. (arXiv:2302.01735v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. The concept of variance-reduced estimation is used to build ARCO, and certain variance-reduction techniques are shown to be particularly beneficial in medical image segmentation.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#25552;&#39640;&#35270;&#35273;&#34920;&#31034;&#36136;&#37327;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#26679;&#26412;&#23545;&#26469;&#23454;&#29616;&#12290;&#36825;&#26159;&#36890;&#36807;&#35266;&#23519;&#21040;&#65292;&#22312;&#27809;&#26377;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#37319;&#26679;&#20855;&#26377;&#30495;&#27491;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#30340;&#36127;&#26679;&#26412;&#65292;&#21017;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#26469;&#33258;&#30456;&#20284;&#30340;&#35299;&#21078;&#29305;&#24449;&#65292;&#27169;&#22411;&#21487;&#33021;&#38590;&#20197;&#21306;&#20998;&#23569;&#25968;&#23614;&#31867;&#26679;&#26412;&#65292;&#20351;&#24471;&#23614;&#31867;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth label, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in medical image se
&lt;/p&gt;</description></item><item><title>LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2301.09799</link><description>&lt;p&gt;
LDMIC&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LDMIC: Learning-based Distributed Multi-view Image Coding. (arXiv:2301.09799v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09799
&lt;/p&gt;
&lt;p&gt;
LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
LDMIC is a learning-based distributed multi-view image coding framework that captures global inter-view correlations through independent encoders and a joint context transfer module based on the cross-attention mechanism, which is insensitive to geometric relations.
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#22270;&#20687;&#21387;&#32553;&#22312;3D&#30456;&#20851;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#39044;&#27979;&#32534;&#30721;&#26550;&#26500;&#65292;&#38656;&#35201;&#32852;&#21512;&#32534;&#30721;&#21387;&#32553;&#30456;&#24212;&#30340;&#35270;&#24046;&#21644;&#27531;&#24046;&#20449;&#24687;&#12290;&#36825;&#35201;&#27714;&#30456;&#26426;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#65292;&#24182;&#24378;&#21046;&#25191;&#34892;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#26497;&#32447;&#20960;&#20309;&#32422;&#26463;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#38543;&#26426;&#37325;&#21472;&#35270;&#37326;&#30340;&#20998;&#24067;&#24335;&#30456;&#26426;&#31995;&#32479;&#20013;&#37096;&#32626;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#29702;&#35770;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#21644;&#32852;&#21512;&#35299;&#30721;&#23454;&#29616;&#30456;&#20851;&#28304;&#30340;&#39640;&#25928;&#25968;&#25454;&#21387;&#32553;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#35774;&#35745;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#65288;LDMIC&#65289;&#26694;&#26550;&#30340;&#21160;&#26426;&#12290;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#65292;LDMIC&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22768;&#38899;&#21305;&#37197;&#31639;&#27861;&#65292;&#31216;&#20026;&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#25439;&#22833;&#65288;PNP&#65289;&#65292;&#23427;&#26159;&#39057;&#35889;&#25439;&#22833;&#30340;&#26368;&#20248;&#20108;&#27425;&#36817;&#20284;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#21442;&#25968;&#30340;&#24863;&#30693;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2301.02886</link><description>&lt;p&gt;
&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#22768;&#38899;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Perceptual-Neural-Physical Sound Matching. (arXiv:2301.02886v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22768;&#38899;&#21305;&#37197;&#31639;&#27861;&#65292;&#31216;&#20026;&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#25439;&#22833;&#65288;PNP&#65289;&#65292;&#23427;&#26159;&#39057;&#35889;&#25439;&#22833;&#30340;&#26368;&#20248;&#20108;&#27425;&#36817;&#20284;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#21442;&#25968;&#30340;&#24863;&#30693;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new sound matching algorithm called Perceptual-Neural-Physical loss (PNP), which is the optimal quadratic approximation of spectral loss and can better accommodate the differing perceptual significance of each parameter while having fast convergence.
&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#21305;&#37197;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#21442;&#25968;&#21270;&#38899;&#39057;&#21512;&#25104;&#26469;&#36817;&#20284;&#30446;&#26631;&#27874;&#24418;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21305;&#37197;&#25345;&#32493;&#35856;&#27874;&#38899;&#35843;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#30446;&#26631;&#26159;&#38750;&#24179;&#31283;&#21644;&#38750;&#35856;&#27874;&#30340;&#26102;&#20505;&#65292;&#20363;&#22914;&#25171;&#20987;&#20048;&#22120;&#65292;&#20219;&#21153;&#23601;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24402;&#22240;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#19981;&#36275;&#12290;&#19968;&#26041;&#38754;&#65292;&#21442;&#25968;&#22495;&#20013;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#31216;&#20026;&#8220;P-loss&#8221;&#65292;&#31616;&#21333;&#24555;&#36895;&#65292;&#20294;&#26410;&#33021;&#36866;&#24212;&#27599;&#20010;&#21442;&#25968;&#30340;&#19981;&#21516;&#24863;&#30693;&#37325;&#35201;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#39057;&#35889;&#26102;&#38388;&#22495;&#20013;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#31216;&#20026;&#8220;&#39057;&#35889;&#25439;&#22833;&#8221;&#65292;&#22312;&#24863;&#30693;&#19978;&#26159;&#26377;&#21160;&#26426;&#30340;&#65292;&#24182;&#22312;&#21487;&#24494;&#20998;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DDSP&#65289;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#39057;&#35889;&#25439;&#22833;&#26159;&#38899;&#39640;&#38388;&#38548;&#30340;&#19981;&#33391;&#39044;&#27979;&#22240;&#32032;&#65292;&#20854;&#26799;&#24230;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#22312;&#36825;&#20010;&#22256;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#25439;&#22833;&#65288;PNP&#65289;&#12290;PNP&#26159;&#39057;&#35889;&#25439;&#22833;&#30340;&#26368;&#20248;&#20108;&#27425;&#36817;&#20284;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound matching algorithms seek to approximate a target waveform by parametric audio synthesis. Deep neural networks have achieved promising results in matching sustained harmonic tones. However, the task is more challenging when targets are nonstationary and inharmonic, e.g., percussion. We attribute this problem to the inadequacy of loss function. On one hand, mean square error in the parametric domain, known as "P-loss", is simple and fast but fails to accommodate the differing perceptual significance of each parameter. On the other hand, mean square error in the spectrotemporal domain, known as "spectral loss", is perceptually motivated and serves in differentiable digital signal processing (DDSP). Yet, spectral loss is a poor predictor of pitch intervals and its gradient may be computationally expensive; hence a slow convergence. Against this conundrum, we present Perceptual-Neural-Physical loss (PNP). PNP is the optimal quadratic approximation of spectral loss while being as fast 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.00815</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23156;&#20799;&#33041;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A attention way in Explainable methods for infant brain. (arXiv:2301.00815v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an explainable geometric deep network that enhances discriminative representation extraction by end-to-end learning of explanation factors, which is a more intuitive strategy to inversely assure fine-grained explainability, suitable for high-dimensional data in neuroimaging and neuroscience studies containing noisy, redundant, and task-irrelevant information.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#23398;&#31185;&#24212;&#29992;&#20013;&#37096;&#32626;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#20934;&#30830;&#19988;&#65288;&#26356;&#37325;&#35201;&#30340;&#26159;&#65289;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20197;&#20107;&#21518;&#26041;&#24335;&#35299;&#37322;&#32593;&#32476;&#36755;&#20986;&#65292;&#38544;&#21547;&#22320;&#20551;&#35774;&#24544;&#23454;&#30340;&#35299;&#37322;&#26469;&#33258;&#20934;&#30830;&#30340;&#39044;&#27979;/&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30456;&#21453;&#30340;&#35266;&#28857;&#65292;&#21363;&#35299;&#37322;&#25552;&#21319;&#65288;&#29978;&#33267;&#20915;&#23450;&#65289;&#20998;&#31867;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#21487;&#33021;&#26159;&#19968;&#31181;&#26356;&#30452;&#35266;&#30340;&#31574;&#30053;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20363;&#22914;&#22312;&#37027;&#20123;&#21253;&#21547;&#22122;&#22768;&#65292;&#20887;&#20313;&#21644;&#20219;&#21153;&#26080;&#20851;&#20449;&#24687;&#30340;&#39640;&#32500;&#25968;&#25454;&#30340;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying reliable deep learning techniques in interdisciplinary applications needs learned models to output accurate and ({even more importantly}) explainable predictions. Existing approaches typically explicate network outputs in a post-hoc fashion, under an implicit assumption that faithful explanations come from accurate predictions/classifications. We have an opposite claim that explanations boost (or even determine) classification. That is, end-to-end learning of explanation factors to augment discriminative representation extraction could be a more intuitive strategy to inversely assure fine-grained explainability, e.g., in those neuroimaging and neuroscience studies with high-dimensional data containing noisy, redundant, and task-irrelevant information. In this paper, we propose such an explainable geometric deep network dubbed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#65292;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#21644;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#65292;&#20026;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25552;&#20379;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2301.00812</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-shot domain adaptation in video-based assessment of surgical skills. (arXiv:2301.00812v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#65292;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#21644;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#65292;&#20026;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25552;&#20379;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. The model successfully adapts to simulated tasks and laparoscopic cholecystectomy, providing a domain-agnostic procedure for video-based assessment of surgical skills.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#23454;&#29616;&#20102;&#25163;&#26415;&#25216;&#33021;&#30340;&#33258;&#21160;&#21644;&#23458;&#35266;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20854;&#35757;&#32451;&#39046;&#22495;&#12290;&#36825;&#38459;&#27490;&#20102;&#23427;&#20204;&#36807;&#28193;&#21040;&#25968;&#25454;&#26377;&#38480;&#30340;&#26032;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#33145;&#33108;&#38236;&#21644;&#26426;&#22120;&#20154;&#25163;&#26415;&#27169;&#25311;&#22120;&#19978;&#24320;&#21457;&#20102;A-VBANet&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#30340;&#25163;&#26415;&#23460;&#35270;&#39057;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;99.5%&#65288;&#19968;&#27425;&#24615;&#65289;&#21644;99.9%&#65288;&#23569;&#37327;&#26679;&#26412;&#65289;&#65292;&#22312;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#20013;&#30340;&#20934;&#30830;&#29575;&#20026;89.7%&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#30340;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#24433;&#21709;&#26159;&#23427;&#20801;&#35768;&#20351;&#29992;&#26469;&#33258;&#25163;&#26415;&#27169;&#25311;&#22120;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#25163;&#26415;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) has achieved automatic and objective assessment of surgical skills. However, DL models are data-hungry and restricted to their training domain. This prevents them from transitioning to new tasks where data is limited. Hence, domain adaptation is crucial to implement DL in real life. Here, we propose a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. We develop the A-VBANet on five laparoscopic and robotic surgical simulators. Additionally, we test it on operating room (OR) videos of laparoscopic cholecystectomy. Our model successfully adapts with accuracies up to 99.5% in one-shot and 99.9% in few-shot settings for simulated tasks and 89.7% for laparoscopic cholecystectomy. For the first time, we provide a domain-agnostic procedure for video-based assessment of surgical skills. A significant implication of this approach is that it allows the use of data from surgical simulators to assess performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniDA3D&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;&#65292;&#36890;&#36807;&#35774;&#35745;&#32479;&#19968;&#30340;&#28304;&#21644;&#30446;&#26631;&#20027;&#21160;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#19977;&#32500;&#20998;&#21106;&#39046;&#22495;&#20013;&#30340;&#22810;&#20010;&#33258;&#36866;&#24212;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#37319;&#26679;&#31574;&#30053;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10390</link><description>&lt;p&gt;
UniDA3D: &#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
UniDA3D: Unified Domain Adaptive 3D Semantic Segmentation Pipeline. (arXiv:2212.10390v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniDA3D&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;&#65292;&#36890;&#36807;&#35774;&#35745;&#32479;&#19968;&#30340;&#28304;&#21644;&#30446;&#26631;&#20027;&#21160;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#19977;&#32500;&#20998;&#21106;&#39046;&#22495;&#20013;&#30340;&#22810;&#20010;&#33258;&#36866;&#24212;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#37319;&#26679;&#31574;&#30053;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes UniDA3D, a unified domain adaptive 3D semantic segmentation pipeline, which can tackle several adaptation tasks in 3D segmentation field by designing a unified source-and-target active sampling strategy, and investigates the possibility of achieving a multi-modal sampling strategy.
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#26159;&#22312;&#29616;&#25104;&#30340;&#20844;&#20849;&#22522;&#20934;&#19978;&#35757;&#32451;&#30340;&#65292;&#20294;&#24403;&#36825;&#20123;&#35757;&#32451;&#33391;&#22909;&#30340;&#27169;&#22411;&#37096;&#32626;&#21040;&#26032;&#39046;&#22495;&#26102;&#65292;&#23427;&#20204;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#35782;&#21035;&#31934;&#24230;&#19979;&#38477;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;&#65288;UniDA3D&#65289;&#65292;&#20197;&#22686;&#24378;&#24369;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24357;&#21512;&#22495;&#20043;&#38388;&#30340;&#28857;&#20998;&#24067;&#24046;&#36317;&#12290;&#19982;&#20043;&#21069;&#21482;&#20851;&#27880;&#21333;&#19968;&#33258;&#36866;&#24212;&#20219;&#21153;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;UniDA3D&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#32479;&#19968;&#30340;&#28304;&#21644;&#30446;&#26631;&#20027;&#21160;&#37319;&#26679;&#31574;&#30053;&#26469;&#35299;&#20915;&#19977;&#32500;&#20998;&#21106;&#39046;&#22495;&#20013;&#30340;&#22810;&#20010;&#33258;&#36866;&#24212;&#20219;&#21153;&#65292;&#35813;&#31574;&#30053;&#20174;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23376;&#38598;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#27169;&#22411;&#33258;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#22810;&#27169;&#24577;&#20108;&#32500;-&#19977;&#32500;&#25968;&#25454;&#38598;&#30340;&#23835;&#36215;&#30340;&#24433;&#21709;&#65292;UniDA3D&#25506;&#32034;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#37319;&#26679;&#31574;&#30053;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#24320;&#21457;&#36328;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#65292;&#21487;&#20197;&#25552;&#21462;&#20195;&#34920;&#24615;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art 3D semantic segmentation models are trained on off-the-shelf public benchmarks, but they will inevitably face the challenge of recognition accuracy drop when these well-trained models are deployed to a new domain. In this paper, we introduce a Unified Domain Adaptive 3D semantic segmentation pipeline (UniDA3D) to enhance the weak generalization ability, and bridge the point distribution gap between domains. Different from previous studies that only focus on a single adaptation task, UniDA3D can tackle several adaptation tasks in 3D segmentation field, by designing a unified source-and-target active sampling strategy, which selects a maximally-informative subset from both source and target domains for effective model adaptation. Besides, benefiting from the rise of multi-modal 2D-3D datasets, UniDA3D investigates the possibility of achieving a multi-modal sampling strategy, by developing a cross-modality feature interaction module that can extract a representative pair 
&lt;/p&gt;</description></item><item><title>&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#65292;&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2212.07738</link><description>&lt;p&gt;
&#19968;&#20221;&#22823;&#35268;&#27169;&#30340;&#12289;&#22522;&#20110;PCR&#30340;COVID-19&#22768;&#38899;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A large-scale and PCR-referenced vocal audio dataset for COVID-19. (arXiv:2212.07738v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07738
&lt;/p&gt;
&lt;p&gt;
&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#65292;&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The UK COVID-19 Vocal Audio Dataset is the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date, designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio.
&lt;/p&gt;
&lt;p&gt;
&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;&#33521;&#22269;&#21355;&#29983;&#23433;&#20840;&#23616;&#36890;&#36807;&#22269;&#23478;&#27979;&#35797;&#21644;&#36861;&#36394;&#35745;&#21010;&#21644;REACT-1&#35843;&#26597;&#22312;2021&#24180;3&#26376;&#33267;2022&#24180;3&#26376;&#26399;&#38388;&#25307;&#21215;&#20102;&#33258;&#24895;&#21442;&#19982;&#32773;&#65292;&#25910;&#38598;&#20102;&#33258;&#24895;&#21683;&#22013;&#12289;&#21628;&#27668;&#21644;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;SARS-CoV-2&#26816;&#27979;&#32467;&#26524;&#30456;&#20851;&#32852;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The UK COVID-19 Vocal Audio Dataset is designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio. The UK Health Security Agency recruited voluntary participants through the national Test and Trace programme and the REACT-1 survey in England from March 2021 to March 2022, during dominant transmission of the Alpha and Delta SARS-CoV-2 variants and some Omicron variant sublineages. Audio recordings of volitional coughs, exhalations, and speech were collected in the 'Speak up to help beat coronavirus' digital survey alongside demographic, self-reported symptom and respiratory condition data, and linked to SARS-CoV-2 test results. The UK COVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date. PCR results were linked to 70,794 of 72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms were reported by 45.6
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26680;&#21270;Stein&#24046;&#24322;&#65288;KSD&#65289;&#21644;&#24076;&#23572;&#20271;&#29305;&#20869;&#31215;&#65288;HIP&#65289;&#30340;DSVGD&#36873;&#25321;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#32852;&#37030;&#36125;&#21494;&#26031;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.05492</link><description>&lt;p&gt;
&#32852;&#37030;&#36125;&#21494;&#26031;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Client Selection for Federated Bayesian Learning. (arXiv:2212.05492v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26680;&#21270;Stein&#24046;&#24322;&#65288;KSD&#65289;&#21644;&#24076;&#23572;&#20271;&#29305;&#20869;&#31215;&#65288;HIP&#65289;&#30340;DSVGD&#36873;&#25321;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#32852;&#37030;&#36125;&#21494;&#26031;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes two selection schemes for Distributed Stein Variational Gradient Descent (DSVGD) based on Kernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP) to improve the model convergence and communication efficiency in federated Bayesian learning.
&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;DSVGD&#65289;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#36890;&#36807;&#19982;&#26381;&#21153;&#22120;&#36890;&#20449;&#19968;&#23450;&#25968;&#37327;&#30340;&#38750;&#38543;&#26426;&#21644;&#20132;&#20114;&#31890;&#23376;&#26469;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#30001;&#20110;&#36890;&#20449;&#36164;&#28304;&#26377;&#38480;&#65292;&#36873;&#25321;&#20855;&#26377;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#26412;&#22320;&#23398;&#20064;&#26356;&#26032;&#30340;&#23458;&#25143;&#31471;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26680;&#21270;Stein&#24046;&#24322;&#65288;KSD&#65289;&#21644;&#24076;&#23572;&#20271;&#29305;&#20869;&#31215;&#65288;HIP&#65289;&#30340;DSVGD&#36873;&#25321;&#26041;&#26696;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20004;&#31181;&#26041;&#26696;&#27599;&#27425;&#36845;&#20195;&#20840;&#23616;&#33258;&#30001;&#33021;&#19979;&#38477;&#30340;&#19978;&#30028;&#65292;&#28982;&#21518;&#23558;&#20854;&#26368;&#23567;&#21270;&#20197;&#21152;&#36895;&#27169;&#22411;&#25910;&#25947;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#19982;&#20256;&#32479;&#26041;&#26696;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Stein Variational Gradient Descent (DSVGD) is a non-parametric distributed learning framework for federated Bayesian learning, where multiple clients jointly train a machine learning model by communicating a number of non-random and interacting particles with the server. Since communication resources are limited, selecting the clients with most informative local learning updates can improve the model convergence and communication efficiency. In this paper, we propose two selection schemes for DSVGD based on Kernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP). We derive the upper bound on the decrease of the global free energy per iteration for both schemes, which is then minimized to speed up the model convergence. We evaluate and compare our schemes with conventional schemes in terms of model accuracy, convergence speed, and stability using various learning tasks and datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#31070;&#32463;&#36716;&#24405;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#36880;&#20010;&#26679;&#26412;&#35745;&#31639;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#22312;&#19982;&#40664;&#35748;&#25209;&#37327;&#35745;&#31639;&#30456;&#27604;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.16270</link><description>&lt;p&gt;
&#31070;&#32463;&#36716;&#24405;&#22120;&#35757;&#32451;&#65306;&#37319;&#29992;&#36880;&#26679;&#26412;&#35745;&#31639;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;
Neural Transducer Training: Reduced Memory Consumption with Sample-wise Computation. (arXiv:2211.16270v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#31070;&#32463;&#36716;&#24405;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#36880;&#20010;&#26679;&#26412;&#35745;&#31639;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#22312;&#19982;&#40664;&#35748;&#25209;&#37327;&#35745;&#31639;&#30456;&#27604;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a memory-efficient training method for neural transducer, which computes the transducer loss and gradients sample by sample, significantly reducing memory usage and performing at competitive speed compared to the default batched computation.
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36716;&#24405;&#22120;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;&#34429;&#28982;&#35813;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#27969;&#24335;ASR&#65292;&#20294;&#35757;&#32451;&#36807;&#31243;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20869;&#23384;&#38656;&#27714;&#21487;&#33021;&#20250;&#36805;&#36895;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;GPU&#30340;&#23481;&#37327;&#65292;&#38480;&#21046;&#25209;&#37327;&#22823;&#23567;&#21644;&#24207;&#21015;&#38271;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20856;&#22411;&#36716;&#24405;&#22120;&#35757;&#32451;&#35774;&#32622;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36880;&#20010;&#26679;&#26412;&#35745;&#31639;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#36880;&#26679;&#26412;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#24182;&#34892;&#24615;&#12290;&#22312;&#19968;&#32452;&#24443;&#24213;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36880;&#26679;&#26412;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#22312;&#19982;&#40664;&#35748;&#25209;&#37327;&#35745;&#31639;&#30456;&#27604;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#36895;&#24230;&#12290;&#20316;&#20026;&#20142;&#28857;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#20351;&#29992;&#20165;6 GB&#30340;&#20869;&#23384;&#35745;&#31639;&#20102;&#25209;&#37327;&#22823;&#23567;&#20026;1024&#65292;&#38899;&#39057;&#38271;&#24230;&#20026;40&#31186;&#30340;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural transducer is an end-to-end model for automatic speech recognition (ASR). While the model is well-suited for streaming ASR, the training process remains challenging. During training, the memory requirements may quickly exceed the capacity of state-of-the-art GPUs, limiting batch size and sequence lengths. In this work, we analyze the time and space complexity of a typical transducer training setup. We propose a memory-efficient training method that computes the transducer loss and gradients sample by sample. We present optimizations to increase the efficiency and parallelism of the sample-wise method. In a set of thorough benchmarks, we show that our sample-wise method significantly reduces memory usage, and performs at competitive speed when compared to the default batched computation. As a highlight, we manage to compute the transducer loss and gradients for a batch size of 1024, and audio length of 40 seconds, using only 6 GB of memory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DL&#30340;Mel-Subband&#26102;&#31354;&#27874;&#26463;&#25104;&#24418;&#22120;&#65292;&#29992;&#20110;&#22312;&#36710;&#36733;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#65292;&#36890;&#36807;&#22522;&#20110;Mel&#23610;&#24230;&#30340;&#23376;&#24102;&#36873;&#25321;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#20302;&#39057;&#30340;&#32454;&#31890;&#24230;&#22788;&#29702;&#21644;&#23545;&#39640;&#39057;&#30340;&#31895;&#31890;&#24230;&#22788;&#29702;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2211.12590</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;Mel-Subband&#27874;&#26463;&#25104;&#24418;&#22120;&#29992;&#20110;&#36710;&#36733;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Mel-Subband Beamformer for In-car Speech Separation. (arXiv:2211.12590v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DL&#30340;Mel-Subband&#26102;&#31354;&#27874;&#26463;&#25104;&#24418;&#22120;&#65292;&#29992;&#20110;&#22312;&#36710;&#36733;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#65292;&#36890;&#36807;&#22522;&#20110;Mel&#23610;&#24230;&#30340;&#23376;&#24102;&#36873;&#25321;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#20302;&#39057;&#30340;&#32454;&#31890;&#24230;&#22788;&#29702;&#21644;&#23545;&#39640;&#39057;&#30340;&#31895;&#31890;&#24230;&#22788;&#29702;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a DL-based Mel-Subband spatio-temporal beamformer for speech separation in a car environment, which reduces computational costs and inference time by using a Mel-scale based subband selection strategy for fine-grained processing of lower frequencies and coarse-grained processing of higher frequencies.
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22522;&#20110;&#27874;&#26463;&#25104;&#24418;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#35821;&#38899;&#20998;&#31163;&#20013;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#34987;&#35774;&#35745;&#20026;&#29420;&#31435;&#22788;&#29702;&#31364;&#24102;&#65288;NB&#65289;&#39057;&#29575;&#65292;&#36825;&#23548;&#33268;&#26356;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#21512;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;DL&#30340;Mel-Subband&#26102;&#31354;&#27874;&#26463;&#25104;&#24418;&#22120;&#65292;&#20197;&#22312;&#36710;&#36733;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#19982;&#20256;&#32479;&#30340;&#23376;&#24102;&#65288;SB&#65289;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#22522;&#20110;Mel&#23610;&#24230;&#30340;&#23376;&#24102;&#36873;&#25321;&#31574;&#30053;&#65292;&#30830;&#20445;&#23545;&#22823;&#22810;&#25968;&#35821;&#38899;&#20849;&#25391;&#32467;&#26500;&#23384;&#22312;&#30340;&#20302;&#39057;&#36827;&#34892;&#32454;&#31890;&#24230;&#22788;&#29702;&#65292;&#23545;&#39640;&#39057;&#36827;&#34892;&#31895;&#31890;&#24230;&#22788;&#29702;&#12290;&#20197;&#36882;&#24402;&#26041;&#24335;&#65292;&#20174;&#20272;&#35745;&#30340;&#23376;&#24102;&#35821;&#38899;&#21644;&#22122;&#22768;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#30830;&#23450;&#27599;&#20010;&#25196;&#22768;&#22120;&#20301;&#32622;/&#21306;&#22495;&#30340;&#40065;&#26834;&#24103;&#32423;&#27874;&#26463;&#25104;&#24418;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#20272;&#35745;&#24182;&#25233;&#21046;&#20219;&#20309;&#22238;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
While current deep learning (DL)-based beamforming techniques have been proved effective in speech separation, they are often designed to process narrow-band (NB) frequencies independently which results in higher computational costs and inference times, making them unsuitable for real-world use. In this paper, we propose DL-based mel-subband spatio-temporal beamformer to perform speech separation in a car environment with reduced computation cost and inference time. As opposed to conventional subband (SB) approaches, our framework uses a mel-scale based subband selection strategy which ensures a fine-grained processing for lower frequencies where most speech formant structure is present, and coarse-grained processing for higher frequencies. In a recursive way, robust frame-level beamforming weights are determined for each speaker location/zone in a car from the estimated subband speech and noise covariance matrices. Furthermore, proposed framework also estimates and suppresses any echo
&lt;/p&gt;</description></item><item><title>LA-VocE&#26159;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#23558;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;&#30340;mel&#39057;&#35889;&#22270;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#21644;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2211.10999</link><description>&lt;p&gt;
LA-VocE: &#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#20302;&#20449;&#22122;&#27604;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders. (arXiv:2211.10999v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10999
&lt;/p&gt;
&lt;p&gt;
LA-VocE&#26159;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#23558;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;&#30340;mel&#39057;&#35889;&#22270;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#21644;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
LA-VocE is a new audio-visual speech enhancement method that uses a neural vocoder to convert mel-spectrograms predicted from noisy audio-visual speech via a transformer-based architecture into waveform audio, and is applicable to multiple languages and different levels of background noise and speech interference.
&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#26412;&#36523;&#20197;&#21450;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#21767;&#37096;&#36816;&#21160;&#20174;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#35821;&#38899;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#27604;&#20165;&#20351;&#29992;&#38899;&#39057;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28040;&#38500;&#24178;&#25200;&#35821;&#38899;&#12290;&#23613;&#31649;&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#38899;&#39057;&#35270;&#35273;&#26041;&#27861;&#20173;&#28982;&#20351;&#29992;&#39057;&#35889;&#26144;&#23556;/&#25513;&#34109;&#26469;&#37325;&#29616;&#24178;&#20928;&#30340;&#38899;&#39057;&#65292;&#36890;&#24120;&#20250;&#22312;&#29616;&#26377;&#30340;&#35821;&#38899;&#22686;&#24378;&#26550;&#26500;&#20013;&#28155;&#21152;&#35270;&#35273;&#39592;&#24178;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LA-VocE&#65292;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;mel&#39057;&#35889;&#22270;&#65292;&#28982;&#21518;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#65288;HiFi-GAN&#65289;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#12290;&#25105;&#20204;&#22312;&#25968;&#21315;&#20010;&#35828;&#35805;&#32773;&#21644;11&#31181;&#20197;&#19978;&#19981;&#21516;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#25105;&#20204;&#30340;&#27169;&#22411;&#36866;&#24212;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Audio-visual speech enhancement aims to extract clean speech from a noisy environment by leveraging not only the audio itself but also the target speaker's lip movements. This approach has been shown to yield improvements over audio-only speech enhancement, particularly for the removal of interfering speech. Despite recent advances in speech synthesis, most audio-visual approaches continue to use spectral mapping/masking to reproduce the clean audio, often resulting in visual backbones added to existing speech enhancement architectures. In this work, we propose LA-VocE, a new two-stage approach that predicts mel-spectrograms from noisy audio-visual speech via a transformer-based architecture, and then converts them into waveform audio using a neural vocoder (HiFi-GAN). We train and evaluate our framework on thousands of speakers and 11+ different languages, and study our model's ability to adapt to different levels of background noise and speech interference. Our experiments show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#25104;&#20687;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#27979;&#37327;&#22352;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#26679;&#26465;&#25554;&#20540;&#22120;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;2D&#21644;3D&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65292;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.10525</link><description>&lt;p&gt;
&#21487;&#24494;&#38750;&#26631;&#23450;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Differentiable Uncalibrated Imaging. (arXiv:2211.10525v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#25104;&#20687;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#27979;&#37327;&#22352;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#26679;&#26465;&#25554;&#20540;&#22120;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;2D&#21644;3D&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65292;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a differentiable imaging framework to address uncertainty in measurement coordinates, using implicit neural networks and differentiable spline interpolators. The method is applied to 2D and 3D computed tomography and produces improved reconstructions.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#25104;&#20687;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#27979;&#37327;&#22352;&#26631;&#65288;&#22914;&#20256;&#24863;&#22120;&#20301;&#32622;&#21644;&#25237;&#24433;&#35282;&#24230;&#65289;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#22312;&#26410;&#30693;&#33410;&#28857;&#22788;&#30340;&#27979;&#37327;&#25554;&#20540;&#65292;&#36890;&#36807;&#27491;&#21521;&#31639;&#23376;&#36827;&#34892;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#20063;&#31216;&#20026;&#31070;&#32463;&#22330;&#65292;&#23427;&#20204;&#22312;&#36755;&#20837;&#22352;&#26631;&#26041;&#38754;&#33258;&#28982;&#21487;&#24494;&#20998;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#21487;&#24494;&#20998;&#26679;&#26465;&#25554;&#20540;&#22120;&#65292;&#20854;&#24615;&#33021;&#19982;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#22909;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#20248;&#21270;&#26102;&#38388;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#36136;&#12290;&#21487;&#24494;&#24615;&#26159;&#20851;&#38190;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#25105;&#20204;&#20849;&#21516;&#25311;&#21512;&#27979;&#37327;&#34920;&#31034;&#65292;&#20248;&#21270;&#19981;&#30830;&#23450;&#30340;&#27979;&#37327;&#22352;&#26631;&#65292;&#24182;&#25191;&#34892;&#22270;&#20687;&#37325;&#24314;&#65292;&#20174;&#32780;&#30830;&#20445;&#19968;&#33268;&#30340;&#26631;&#23450;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;2D&#21644;3D&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#19981;&#32771;&#34385;&#32570;&#20047;&#26631;&#23450;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
We propose a differentiable imaging framework to address uncertainty in measurement coordinates such as sensor locations and projection angles. We formulate the problem as measurement interpolation at unknown nodes supervised through the forward operator. To solve it we apply implicit neural networks, also known as neural fields, which are naturally differentiable with respect to the input coordinates. We also develop differentiable spline interpolators which perform as well as neural networks, require less time to optimize and have well-understood properties. Differentiability is key as it allows us to jointly fit a measurement representation, optimize over the uncertain measurement coordinates, and perform image reconstruction which in turn ensures consistent calibration. We apply our approach to 2D and 3D computed tomography and show that it produces improved reconstructions compared to baselines that do not account for the lack of calibration. The flexibility of the proposed framew
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;2D&#25237;&#24433;&#20174;3D MRI&#20307;&#31215;&#20013;&#39640;&#25928;&#39044;&#27979;&#33041;&#40836;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;3D CNN&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36895;&#24230;&#19978;&#26377;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25552;&#21319;&#65292;&#23545;&#20110;&#27809;&#26377;3D CNN&#26114;&#36149;GPU&#30828;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.05762</link><description>&lt;p&gt;
&#20351;&#29992;2D&#25237;&#24433;&#20174;3D MRI&#20307;&#31215;&#20013;&#39640;&#25928;&#39044;&#27979;&#33041;&#40836;
&lt;/p&gt;
&lt;p&gt;
Efficient brain age prediction from 3D MRI volumes using 2D projections. (arXiv:2211.05762v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;2D&#25237;&#24433;&#20174;3D MRI&#20307;&#31215;&#20013;&#39640;&#25928;&#39044;&#27979;&#33041;&#40836;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;3D CNN&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36895;&#24230;&#19978;&#26377;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25552;&#21319;&#65292;&#23545;&#20110;&#27809;&#26377;3D CNN&#26114;&#36149;GPU&#30828;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient method for predicting brain age from 3D MRI volumes using 2D projections, which is two orders of magnitude faster than using 3D CNNs and is important for researchers without access to expensive GPU hardware.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#20998;&#36776;&#29575;&#21307;&#23398;&#20307;&#31215;&#19978;&#20351;&#29992;3D CNN&#38750;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20687;&#33521;&#22269;&#29983;&#29289;&#24211;&#36825;&#26679;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#35813;&#24211;&#26088;&#22312;&#25195;&#25551;10&#19975;&#20010;&#21463;&#35797;&#32773;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;2D CNN&#22312;3D&#20307;&#31215;&#30340;&#20960;&#20010;2D&#25237;&#24433;&#65288;&#20195;&#34920;&#36724;&#21521;&#65292;&#30690;&#29366;&#38754;&#21644;&#20896;&#29366;&#38754;&#20999;&#29255;&#30340;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#65289;&#19978;&#36827;&#34892;&#39044;&#27979;&#33041;&#40836;&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#21512;&#29702;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#20010;GPU&#36827;&#34892;&#30340;&#19968;&#27425;&#35757;&#32451;&#26102;&#65292;20324&#20010;&#21463;&#35797;&#32773;&#38656;&#35201;20-50&#31186;&#65292;&#27604;&#23567;&#22411;3D CNN&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#27809;&#26377;3D CNN&#26114;&#36149;GPU&#30828;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using 3D CNNs on high resolution medical volumes is very computationally demanding, especially for large datasets like the UK Biobank which aims to scan 100,000 subjects. Here we demonstrate that using 2D CNNs on a few 2D projections (representing mean and standard deviation across axial, sagittal and coronal slices) of the 3D volumes leads to reasonable test accuracy when predicting the age from brain volumes. Using our approach, one training epoch with 20,324 subjects takes 20 - 50 seconds using a single GPU, which two orders of magnitude faster compared to a small 3D CNN. These results are important for researchers who do not have access to expensive GPU hardware for 3D CNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#33539;&#24335;&#20013;&#23581;&#35797;Conformer&#26550;&#26500;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#33258;&#30417;&#30563;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#12290;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#36739;&#20302;&#23618;&#20013;&#26368;&#20248;&#22320;&#32534;&#30721;&#20102;&#35821;&#35328;&#21306;&#20998;&#20449;&#24687;&#65292;&#20174;&#36825;&#20123;&#23618;&#33719;&#24471;&#30340;&#23884;&#20837;&#33021;&#22815;&#26174;&#33879;&#22320;&#31283;&#20581;&#22320;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#22768;&#23398;&#29615;&#22659;&#12290;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;Conformer&#27169;&#22411;&#22312;VoxLingua107&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#35782;&#21035;&#31995;&#32479;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#19988;&#20351;&#29992;&#30340;&#21442;&#25968;&#37327;&#20165;&#20026;&#20854;&#23427;&#27169;&#22411;&#30340;&#20116;&#20998;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2211.05103</link><description>&lt;p&gt;
&#24847;&#22806;&#23398;&#20064;&#32773;&#65306;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21475;&#35821;&#35821;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models. (arXiv:2211.05103v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#33539;&#24335;&#20013;&#23581;&#35797;Conformer&#26550;&#26500;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#33258;&#30417;&#30563;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#12290;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#36739;&#20302;&#23618;&#20013;&#26368;&#20248;&#22320;&#32534;&#30721;&#20102;&#35821;&#35328;&#21306;&#20998;&#20449;&#24687;&#65292;&#20174;&#36825;&#20123;&#23618;&#33719;&#24471;&#30340;&#23884;&#20837;&#33021;&#22815;&#26174;&#33879;&#22320;&#31283;&#20581;&#22320;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#22768;&#23398;&#29615;&#22659;&#12290;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;Conformer&#27169;&#22411;&#22312;VoxLingua107&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#35782;&#21035;&#31995;&#32479;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#19988;&#20351;&#29992;&#30340;&#21442;&#25968;&#37327;&#20165;&#20026;&#20854;&#23427;&#27169;&#22411;&#30340;&#20116;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper extends previous self-supervised approaches for language identification by experimenting with Conformer based architecture in a multilingual pre-training paradigm. The pre-trained speech models optimally encode language discriminatory information in lower layers, and the embeddings obtained from these layers are significantly robust to classify unseen languages and different acoustic environments without additional training. After fine-tuning a pre-trained Conformer model on the VoxLingua107 dataset, the authors achieve results similar to current state-of-the-art systems for language identification, with 5x less parameters. The model is open-sourced through the NVIDIA NeMo toolkit.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#33539;&#24335;&#20013;&#23581;&#35797;Conformer&#26550;&#26500;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#33258;&#30417;&#30563;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#36739;&#20302;&#23618;&#20013;&#26368;&#20248;&#22320;&#32534;&#30721;&#20102;&#35821;&#35328;&#21306;&#20998;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;&#36825;&#20123;&#23618;&#33719;&#24471;&#30340;&#23884;&#20837;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#33879;&#22320;&#31283;&#20581;&#22320;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#22768;&#23398;&#29615;&#22659;&#12290;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;Conformer&#27169;&#22411;&#22312;VoxLingua107&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#35782;&#21035;&#31995;&#32479;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#30340;&#21442;&#25968;&#37327;&#20165;&#20026;&#20854;&#23427;&#27169;&#22411;&#30340;&#20116;&#20998;&#20043;&#19968;&#12290;&#25105;&#20204;&#36890;&#36807;NVIDIA NeMo&#24037;&#20855;&#21253;&#24320;&#28304;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we extend previous self-supervised approaches for language identification by experimenting with Conformer based architecture in a multilingual pre-training paradigm. We find that pre-trained speech models optimally encode language discriminatory information in lower layers. Further, we demonstrate that the embeddings obtained from these layers are significantly robust to classify unseen languages and different acoustic environments without additional training. After fine-tuning a pre-trained Conformer model on the VoxLingua107 dataset, we achieve results similar to current state-of-the-art systems for language identification. More, our model accomplishes this with 5x less parameters. We open-source the model through the NVIDIA NeMo toolkit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#35757;&#32451;&#23567;&#35268;&#27169;CNN&#65292;&#20811;&#26381;&#20102;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#19982;&#23454;&#20540;CNN&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#20351;&#29992;&#20102;&#26174;&#30528;&#36739;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2211.02678</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#30340;&#39640;&#25928;ECG&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient ECG-based Atrial Fibrillation Detection via Parameterised Hypercomplex Neural Networks. (arXiv:2211.02678v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#35757;&#32451;&#23567;&#35268;&#27169;CNN&#65292;&#20811;&#26381;&#20102;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#19982;&#23454;&#20540;CNN&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#20351;&#29992;&#20102;&#26174;&#30528;&#36739;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a lightweight convolutional neural network method based on parameterized hypercomplex neural networks for atrial fibrillation detection. The method trains small-scale CNNs on wearable devices, overcoming limited computing resources. The approach shows comparable performance to real-valued CNNs on two publicly available ECG datasets using significantly fewer model parameters.
&lt;/p&gt;
&lt;p&gt;
&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#24459;&#22833;&#24120;&#65292;&#19982;&#20013;&#39118;&#31561;&#20005;&#37325;&#30142;&#30149;&#30340;&#39640;&#39118;&#38505;&#30456;&#20851;&#12290;&#23884;&#20837;&#33258;&#21160;&#21644;&#21450;&#26102;&#30340;AF&#35780;&#20272;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#20351;&#29992;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#39044;&#38450;&#21361;&#21450;&#29983;&#21629;&#30340;&#24773;&#20917;&#26041;&#38754;&#20855;&#26377;&#21069;&#26223;&#12290;&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#30340;&#20351;&#29992;&#21463;&#21040;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24102;&#26377;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#65288;PH&#65289;&#23618;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#22522;&#20110;ECG&#36827;&#34892;AF&#26816;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35757;&#32451;&#23567;&#35268;&#27169;CNN&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#30340;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#20351;&#29992;&#26174;&#30528;&#36739;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19982;&#30456;&#24212;&#30340;&#23454;&#20540;CNN&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;PH&#27169;&#22411;&#27604;&#20854;&#20182;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#26356;&#28789;&#27963;&#65292;&#21487;&#20197;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Atrial fibrillation (AF) is the most common cardiac arrhythmia and associated with a high risk for serious conditions like stroke. The use of wearable devices embedded with automatic and timely AF assessment from electrocardiograms (ECGs) has shown to be promising in preventing life-threatening situations. Although deep neural networks have demonstrated superiority in model performance, their use on wearable devices is limited by the trade-off between model performance and complexity. In this work, we propose to use lightweight convolutional neural networks (CNNs) with parameterised hypercomplex (PH) layers for AF detection based on ECGs. The proposed approach trains small-scale CNNs, thus overcoming the limited computing resources on wearable devices. We show comparable performance to corresponding real-valued CNNs on two publicly available ECG datasets using significantly fewer model parameters. PH models are more flexible than other hypercomplex neural networks and can operate on an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.02641</link><description>&lt;p&gt;
&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65306;&#26469;&#33258;&#26102;&#39057;&#20998;&#26512;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#30340;&#20998;&#31867;&#26159;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#22522;&#30784;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#22791;&#21463;&#36861;&#25447;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#65292;MI-EEG&#20998;&#31867;&#22120;&#30340;&#36235;&#21183;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#30340;&#36716;&#21464;&#65292;&#20854;&#24615;&#33021;&#36880;&#28176;&#25552;&#39640;&#12290; Tensor-CSPNet&#30340;&#20986;&#29616;&#26159;BCI&#30740;&#31350;&#20013;&#31532;&#19968;&#20010;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#65292;&#20854;&#24402;&#22240;&#20110;&#20449;&#21495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#30340;&#29305;&#24449;&#21270;&#12290;&#20174;&#26681;&#26412;&#19978;&#35762;&#65292;Tensor-CSPNet&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#12290;&#19982;&#21033;&#29992;EEG&#20449;&#21495;&#30340;&#19968;&#38454;&#32479;&#35745;&#37327;&#30340;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#21033;&#29992;&#36825;&#20123;&#20108;&#38454;&#32479;&#35745;&#37327;&#20195;&#34920;&#20102;&#32463;&#20856;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#36825;&#20123;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#21306;&#20998;&#20449;&#24687;&#65292;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;MI-EEG&#20998;&#31867;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;GDL&#20998;&#31867;&#22120;&#65292;
&lt;/p&gt;
&lt;p&gt;
The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#21644;&#22122;&#22768;&#40065;&#26834;&#30340;&#21548;&#35273;&#26816;&#26597;&#31995;&#32479;&#65292;&#29992;&#20110;&#26816;&#27979;&#26426;&#26800;&#37096;&#20214;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#21253;&#32476;&#29305;&#24449;&#19982;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#24212;&#29992;&#26102;&#21464;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#20154;&#12290;</title><link>http://arxiv.org/abs/2211.01704</link><description>&lt;p&gt;
&#21435;&#38500;&#22122;&#38899;&#65306;&#24515;&#29702;&#22768;&#23398;&#21644;&#22522;&#20110;&#21253;&#32476;&#30340;&#29305;&#24449;&#22312;&#26426;&#26800;&#25925;&#38556;&#26816;&#27979;&#20013;&#30340;&#23454;&#35777;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Cutting Through the Noise: An Empirical Comparison of Psychoacoustic and Envelope-based Features for Machinery Fault Detection. (arXiv:2211.01704v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#21644;&#22122;&#22768;&#40065;&#26834;&#30340;&#21548;&#35273;&#26816;&#26597;&#31995;&#32479;&#65292;&#29992;&#20110;&#26816;&#27979;&#26426;&#26800;&#37096;&#20214;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#21253;&#32476;&#29305;&#24449;&#19982;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#24212;&#29992;&#26102;&#21464;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an automated and noise-robust auditory inspection system for detecting the health condition of mechanical parts. A benchmark is provided to compare different types of envelope features with psychoacoustic features. The authors are the first to apply time-varying psychoacoustic features for fault detection.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22768;&#23398;&#30340;&#25925;&#38556;&#26816;&#27979;&#20855;&#26377;&#30417;&#27979;&#26426;&#26800;&#37096;&#20214;&#20581;&#24247;&#29366;&#20917;&#30340;&#39640;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24037;&#19994;&#29615;&#22659;&#30340;&#32972;&#26223;&#22122;&#38899;&#21487;&#33021;&#20250;&#23545;&#25925;&#38556;&#26816;&#27979;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#30446;&#21069;&#23545;&#20110;&#25552;&#39640;&#25925;&#38556;&#26816;&#27979;&#23545;&#24037;&#19994;&#29615;&#22659;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lenze&#29983;&#20135;&#32972;&#26223;&#22122;&#22768;&#65288;LPBN&#65289;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#40831;&#36718;&#30005;&#26426;&#26411;&#31471;&#26816;&#26597;&#30340;&#33258;&#21160;&#21270;&#21644;&#22122;&#22768;&#40065;&#26834;&#30340;&#21548;&#35273;&#26816;&#26597;&#65288;ARAI&#65289;&#31995;&#32479;&#12290;&#37319;&#29992;&#22768;&#23398;&#38453;&#21015;&#20174;&#20855;&#26377;&#36731;&#24494;&#25925;&#38556;&#12289;&#37325;&#22823;&#25925;&#38556;&#25110;&#20581;&#24247;&#30340;&#30005;&#26426;&#20013;&#33719;&#21462;&#25968;&#25454;&#12290;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#22522;&#20110;&#19987;&#23478;&#23545;&#40831;&#36718;&#31665;&#30340;&#30693;&#35782;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#21253;&#32476;&#29305;&#24449;&#19982;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#24212;&#29992;&#26102;&#21464;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#20154;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#26469;&#33258;&#20581;&#24247;&#30005;&#26426;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acoustic-based fault detection has a high potential to monitor the health condition of mechanical parts. However, the background noise of an industrial environment may negatively influence the performance of fault detection. Limited attention has been paid to improving the robustness of fault detection against industrial environmental noise. Therefore, we present the Lenze production background-noise (LPBN) real-world dataset and an automated and noise-robust auditory inspection (ARAI) system for the end-of-line inspection of geared motors. An acoustic array is used to acquire data from motors with a minor fault, major fault, or which are healthy. A benchmark is provided to compare the psychoacoustic features with different types of envelope features based on expert knowledge of the gearbox. To the best of our knowledge, we are the first to apply time-varying psychoacoustic features for fault detection. We train a state-of-the-art one-class-classifier, on samples from healthy motors an
&lt;/p&gt;</description></item><item><title>WiserVR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#36890;&#20449;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26080;&#32447;&#34394;&#25311;&#29616;&#23454;&#20256;&#36755;&#65292;&#20854;&#20013;&#21253;&#25324;&#35821;&#20041;&#20301;&#32622;&#22270;&#21644;&#32852;&#21512;&#35821;&#20041;&#36890;&#36947;&#32534;&#30721;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.01241</link><description>&lt;p&gt;
WiserVR&#65306;&#35821;&#20041;&#36890;&#20449;&#25903;&#25345;&#30340;&#26080;&#32447;&#34394;&#25311;&#29616;&#23454;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
WiserVR: Semantic Communication Enabled Wireless Virtual Reality Delivery. (arXiv:2211.01241v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01241
&lt;/p&gt;
&lt;p&gt;
WiserVR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#36890;&#20449;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26080;&#32447;&#34394;&#25311;&#29616;&#23454;&#20256;&#36755;&#65292;&#20854;&#20013;&#21253;&#25324;&#35821;&#20041;&#20301;&#32622;&#22270;&#21644;&#32852;&#21512;&#35821;&#20041;&#36890;&#36947;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
WiserVR proposes a novel framework that utilizes semantic communication and deep learning techniques to achieve efficient wireless virtual reality delivery, including semantic location graph and joint-semantic-channel-coding method with knowledge sharing.
&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#34394;&#25311;&#29616;&#23454;&#34987;&#35748;&#20026;&#26159;&#19979;&#19968;&#20195;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#26432;&#25163;&#32423;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#38480;&#30340;&#24102;&#23485;&#36164;&#28304;&#19979;&#65292;&#24040;&#22823;&#30340;&#25968;&#25454;&#37327;&#20197;&#21450;&#23545;&#24310;&#36831;&#21644;&#21487;&#38752;&#24615;&#30340;&#20005;&#26684;&#35201;&#27714;&#20351;&#24471;&#26080;&#32447;VR&#20256;&#36755;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#29942;&#39048;&#20419;&#20351;&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#35821;&#20041;&#36890;&#20449;&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#36164;&#28304;&#21387;&#21147;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;VR&#20256;&#36755;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;WiserVR&#65288;WIreless SEmantic deliveRy for VR&#65289;&#65292;&#29992;&#20110;&#21521;VR&#29992;&#25143;&#20256;&#36882;&#36830;&#32493;&#30340;360&#24230;&#35270;&#39057;&#24103;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#20010;&#27169;&#22359;&#65292;&#29992;&#20110;WiserVR&#20013;&#30340;&#25910;&#21457;&#22120;&#65292;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#35821;&#20041;&#24674;&#22797;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#19987;&#38376;&#24320;&#21457;&#20102;&#35821;&#20041;&#20301;&#32622;&#22270;&#30340;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#30693;&#35782;&#20849;&#20139;&#30340;&#32852;&#21512;&#35821;&#20041;&#36890;&#36947;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual reality (VR) over wireless is expected to be one of the killer applications in next-generation communication networks. Nevertheless, the huge data volume along with stringent requirements on latency and reliability under limited bandwidth resources makes untethered wireless VR delivery increasingly challenging. Such bottlenecks, therefore, motivate this work to seek the potential of using semantic communication, a new paradigm that promises to significantly ease the resource pressure, for efficient VR delivery. To this end, we propose a novel framework, namely WIreless SEmantic deliveRy for VR (WiserVR), for delivering consecutive 360{\deg} video frames to VR users. Specifically, deep learning-based multiple modules are well-devised for the transceiver in WiserVR to realize high-performance feature extraction and semantic recovery. Among them, we dedicatedly develop a concept of semantic location graph and leverage the joint-semantic-channel-coding method with knowledge sharing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#31867;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2210.16192</link><description>&lt;p&gt;
&#21033;&#29992;&#20803;&#25968;&#25454;&#21644;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Learning Audio Features with Metadata and Contrastive Learning. (arXiv:2210.16192v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#31867;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uses supervised contrastive learning combined with available metadata to solve multiple pretext tasks that learn a good representation of data. Learning representations using only metadata obtains similar performance as using cross entropy with class labels only. State-of-the-art score is obtained when combining class labels with metadata using multiple supervised contrastive learning.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#37322;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19968;&#30452;&#26159;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;ICBHI&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#21512;&#36825;&#31181;&#24773;&#20917;&#30340;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#65292;&#32780;&#19981;&#20351;&#29992;&#31867;&#26631;&#31614;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#36825;&#20123;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#30417;&#30563;&#23545;&#27604;&#35774;&#32622;&#20013;&#20351;&#29992;&#22810;&#20010;&#20803;&#25968;&#25454;&#28304;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#19981;&#24179;&#34913;&#21644;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods based on supervised learning using annotations in an end-to-end fashion have been the state-of-the-art for classification problems. However, they may be limited in their generalization capability, especially in the low data regime. In this study, we address this issue using supervised contrastive learning combined with available metadata to solve multiple pretext tasks that learn a good representation of data. We apply our approach on ICBHI, a respiratory sound classification dataset suited for this setting. We show that learning representations using only metadata, without class labels, obtains similar performance as using cross entropy with those labels only. In addition, we obtain state-of-the-art score when combining class labels with metadata using multiple supervised contrastive learning. This work suggests the potential of using multiple metadata sources in supervised contrastive settings, in particular in settings with class imbalance and few data. Our code is released 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.15173</link><description>&lt;p&gt;
Articulation GAN: &#26080;&#30417;&#30563;&#24314;&#27169;&#20851;&#33410;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Articulation GAN: Unsupervised modeling of articulatory learning. (arXiv:2210.15173v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new unsupervised generative model that learns to generate articulatory representations (electromagnetic articulography or EMA) in a fully unsupervised manner, which more closely mimics human speech production and better simulates the process of human speech production.
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#27874;&#24418;&#25110;&#39057;&#35889;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#36807;&#25511;&#21046;&#20851;&#33410;&#26469;&#20135;&#29983;&#35821;&#38899;&#65292;&#36825;&#36890;&#36807;&#22768;&#38899;&#20256;&#25773;&#30340;&#29289;&#29702;&#29305;&#24615;&#23548;&#33268;&#35821;&#38899;&#22768;&#38899;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20851;&#33410;&#29983;&#25104;&#22120;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#33539;&#20363;&#20013;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#20135;&#29983;/&#21512;&#25104;&#12290;&#20851;&#33410;&#29983;&#25104;&#22120;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;&#39044;&#35757;&#32451;&#29289;&#29702;&#27169;&#22411;&#65288;ema2wav&#65289;&#23558;&#29983;&#25104;&#30340;EMA&#34920;&#31034;&#36716;&#25442;&#20026;&#35821;&#38899;&#27874;&#24418;&#65292;&#36825;&#20123;&#27874;&#24418;&#34987;&#21457;&#36865;&#21040;&#37492;&#21035;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#20851;&#33410;&#20998;&#26512;&#34920;&#26126;&#65292;&#32593;&#32476;&#23398;&#20064;&#25511;&#21046;&#20851;&#33410;&#30340;&#26041;&#24335;&#31867;&#20284;&#20110;&#20154;&#31867;&#22312;&#35821;&#38899;&#20135;&#29983;&#36807;&#31243;&#20013;&#30340;&#26041;&#24335;&#12290;&#36755;&#20986;&#30340;&#22768;&#23398;&#20998;&#26512;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Generative deep neural networks are widely used for speech synthesis, but most existing models directly generate waveforms or spectral outputs. Humans, however, produce speech by controlling articulators, which results in the production of speech sounds through physical properties of sound propagation. We introduce the Articulatory Generator to the Generative Adversarial Network paradigm, a new unsupervised generative model of speech production/synthesis. The Articulatory Generator more closely mimics human speech production by learning to generate articulatory representations (electromagnetic articulography or EMA) in a fully unsupervised manner. A separate pre-trained physical model (ema2wav) then transforms the generated EMA representations to speech waveforms, which get sent to the Discriminator for evaluation. Articulatory analysis suggests that the network learns to control articulators in a similar manner to humans during speech production. Acoustic analysis of the outputs sugge
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.11328</link><description>&lt;p&gt;
&#22238;&#25918;&#65306;&#36845;&#20195;&#27880;&#24847;&#21147;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Play It Back: Iterative Attention for Audio Recognition. (arXiv:2210.11328v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes an end-to-end attention-based architecture that attends over the most discriminative sounds across the audio sequence through selective repetition, achieving consistently state-of-the-art performance across three audio-classification benchmarks.
&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#35748;&#30693;&#30340;&#19968;&#20010;&#20851;&#38190;&#21151;&#33021;&#26159;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#23558;&#29305;&#24449;&#22768;&#38899;&#19982;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#32852;&#36215;&#26469;&#12290;&#20154;&#31867;&#35797;&#22270;&#21306;&#20998;&#32454;&#31890;&#24230;&#38899;&#39057;&#31867;&#21035;&#26102;&#65292;&#36890;&#24120;&#20250;&#37325;&#25773;&#30456;&#21516;&#30340;&#21306;&#20998;&#24615;&#22768;&#38899;&#20197;&#22686;&#21152;&#20854;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26368;&#21021;&#20351;&#29992;&#23436;&#25972;&#30340;&#38899;&#39057;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#25554;&#27133;&#27880;&#24847;&#21147;&#36845;&#20195;&#22320;&#32454;&#21270;&#37325;&#25773;&#30340;&#26102;&#38388;&#27573;&#12290;&#22312;&#27599;&#27425;&#25773;&#25918;&#26102;&#65292;&#25152;&#36873;&#27573;&#20351;&#29992;&#36739;&#23567;&#30340;&#36339;&#36291;&#38271;&#24230;&#37325;&#25773;&#65292;&#36825;&#20195;&#34920;&#20102;&#36825;&#20123;&#27573;&#20869;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65306;AudioSet&#12289;VGG-Sound&#21644;EPIC-KITCHENS-100&#12290;
&lt;/p&gt;
&lt;p&gt;
A key function of auditory cognition is the association of characteristic sounds with their corresponding semantics over time. Humans attempting to discriminate between fine-grained audio categories, often replay the same discriminative sounds to increase their prediction confidence. We propose an end-to-end attention-based architecture that through selective repetition attends over the most discriminative sounds across the audio sequence. Our model initially uses the full audio sequence and iteratively refines the temporal segments replayed based on slot attention. At each playback, the selected segments are replayed using a smaller hop length which represents higher resolution features within these segments. We show that our method can consistently achieve state-of-the-art performance across three audio-classification benchmarks: AudioSet, VGG-Sound, and EPIC-KITCHENS-100.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#26041;&#27861;PSVRF&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;&#65292;&#21487;&#20197;&#22686;&#24378;ASV&#31995;&#32479;&#23545;&#38899;&#39640;&#32553;&#25918;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24615;&#33021;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.02731</link><description>&lt;p&gt;
PSVRF: &#26080;&#21442;&#32771;&#23398;&#20064;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
PSVRF: Learning to restore Pitch-Shifted Voice without reference. (arXiv:2210.02731v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#26041;&#27861;PSVRF&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;&#65292;&#21487;&#20197;&#22686;&#24378;ASV&#31995;&#32479;&#23545;&#38899;&#39640;&#32553;&#25918;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24615;&#33021;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a no-reference approach called PSVRF for high-quality restoration of pitch-shifted voice, which enhances the robustness of ASV systems to pitch-scaling attacks and even outperforms the state-of-the-art reference-based approach.
&lt;/p&gt;
&lt;p&gt;
&#38899;&#39640;&#32553;&#25918;&#31639;&#27861;&#23545;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#65288;ASV&#65289;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21453;&#27450;&#39575;&#31639;&#27861;&#26469;&#35782;&#21035;&#21464;&#35843;&#35821;&#38899;&#24182;&#23558;&#20854;&#24674;&#22797;&#21040;&#21407;&#22987;&#29256;&#26412;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#24615;&#33021;&#36739;&#24046;&#65292;&#35201;&#20040;&#38656;&#35201;&#21407;&#22987;&#35821;&#38899;&#20316;&#20026;&#21442;&#32771;&#65292;&#38480;&#21046;&#20102;&#24212;&#29992;&#21069;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#26041;&#27861;PSVRF&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;&#12290;&#22312;AISHELL-1&#21644;AISHELL-3&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PSVRF&#21487;&#20197;&#24674;&#22797;&#34987;&#21508;&#31181;&#38899;&#39640;&#32553;&#25918;&#25216;&#26415;&#20266;&#35013;&#30340;&#35821;&#38899;&#65292;&#26174;&#28982;&#22686;&#24378;&#20102;ASV&#31995;&#32479;&#23545;&#38899;&#39640;&#32553;&#25918;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;PSVRF&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pitch scaling algorithms have a significant impact on the security of Automatic Speaker Verification (ASV) systems. Although numerous anti-spoofing algorithms have been proposed to identify the pitch-shifted voice and even restore it to the original version, they either have poor performance or require the original voice as a reference, limiting the prospects of applications. In this paper, we propose a no-reference approach termed PSVRF$^1$ for high-quality restoration of pitch-shifted voice. Experiments on AISHELL-1 and AISHELL-3 demonstrate that PSVRF can restore the voice disguised by various pitch-scaling techniques, which obviously enhances the robustness of ASV systems to pitch-scaling attacks. Furthermore, the performance of PSVRF even surpasses that of the state-of-the-art reference-based approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#21363;&#24320;&#25918;&#24335;FL&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#20551;&#35774;&#26412;&#22320;&#23458;&#25143;&#31471;&#20989;&#25968;&#26159;&#24378;&#20984;&#21644;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35770;&#19978;&#37327;&#21270;&#20102;&#20004;&#31181;FL&#31639;&#27861;&#30340;&#31283;&#23450;&#21322;&#24452;&#12290;</title><link>http://arxiv.org/abs/2209.12307</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Stability Analysis of Open Federated Learning Systems. (arXiv:2209.12307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#21363;&#24320;&#25918;&#24335;FL&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#20551;&#35774;&#26412;&#22320;&#23458;&#25143;&#31471;&#20989;&#25968;&#26159;&#24378;&#20984;&#21644;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35770;&#19978;&#37327;&#21270;&#20102;&#20004;&#31181;FL&#31639;&#27861;&#30340;&#31283;&#23450;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the stability issue of open federated learning systems, proposes a new performance metric, namely the stability of open FL systems, and theoretically quantifies the stability radius of two FL algorithms under the assumption that local clients' functions are strongly convex and smooth.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24320;&#25918;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#21487;&#33021;&#22312;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#21644;/&#25110;&#31163;&#24320;&#31995;&#32479;&#12290;&#30001;&#20110;&#23384;&#22312;&#23458;&#25143;&#31471;&#25968;&#37327;&#30340;&#21464;&#21270;&#65292;&#26080;&#27861;&#20445;&#35777;&#22312;&#24320;&#25918;&#31995;&#32479;&#20013;&#25910;&#25947;&#21040;&#22266;&#23450;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#31216;&#20026;&#24320;&#25918;&#24335;FL&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#23427;&#37327;&#21270;&#20102;&#22312;&#24320;&#25918;&#31995;&#32479;&#20013;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#22312;&#20551;&#35774;&#26412;&#22320;&#23458;&#25143;&#31471;&#20989;&#25968;&#26159;&#24378;&#20984;&#21644;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#37327;&#21270;&#20102;&#20004;&#31181;FL&#31639;&#27861;&#65288;&#21363;&#26412;&#22320;SGD&#21644;&#26412;&#22320;Adam&#65289;&#30340;&#31283;&#23450;&#21322;&#24452;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#20010;&#21322;&#24452;&#20381;&#36182;&#20110;&#20960;&#20010;&#20851;&#38190;&#21442;&#25968;&#65292;&#21253;&#25324;&#20989;&#25968;&#26465;&#20214;&#25968;&#20197;&#21450;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the open federated learning (FL) systems, where clients may join and/or leave the system during the FL process. Given the variability of the number of present clients, convergence to a fixed model cannot be guaranteed in open systems. Instead, we resort to a new performance metric that we term the stability of open FL systems, which quantifies the magnitude of the learned model in open systems. Under the assumption that local clients' functions are strongly convex and smooth, we theoretically quantify the radius of stability for two FL algorithms, namely local SGD and local Adam. We observe that this radius relies on several key parameters, including the function condition number as well as the variance of the stochastic gradient. Our theoretical results are further verified by numerical simulations on both synthetic and real-world benchmark data-sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;U-Sleep&#30561;&#30496;&#35780;&#20998;&#31639;&#27861;&#21487;&#20197;&#24377;&#24615;&#22320;&#20351;&#29992;&#38750;&#25512;&#33616;&#25110;&#38750;&#20256;&#32479;&#30340;&#23548;&#32852;&#65292;&#32780;&#19981;&#38656;&#35201;&#20005;&#26684;&#36981;&#23432;AASM&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2209.11173</link><description>&lt;p&gt;
U-Sleep&#23545;AASM&#25351;&#21335;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
U-Sleep's resilience to AASM guidelines. (arXiv:2209.11173v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;U-Sleep&#30561;&#30496;&#35780;&#20998;&#31639;&#27861;&#21487;&#20197;&#24377;&#24615;&#22320;&#20351;&#29992;&#38750;&#25512;&#33616;&#25110;&#38750;&#20256;&#32479;&#30340;&#23548;&#32852;&#65292;&#32780;&#19981;&#38656;&#35201;&#20005;&#26684;&#36981;&#23432;AASM&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study shows that the deep learning-based U-Sleep sleep scoring algorithm can flexibly use non-recommended or non-traditional derivations without strictly adhering to AASM guidelines.
&lt;/p&gt;
&lt;p&gt;
AASM&#25351;&#21335;&#26159;&#20960;&#21313;&#24180;&#21162;&#21147;&#30340;&#32467;&#26524;&#65292;&#26088;&#22312;&#26631;&#20934;&#21270;&#30561;&#30496;&#35780;&#20998;&#31243;&#24207;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#20849;&#20139;&#20840;&#29699;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#35813;&#25351;&#21335;&#28085;&#30422;&#20102;&#20174;&#25216;&#26415;/&#25968;&#23383;&#35268;&#33539;&#65288;&#20363;&#22914;&#65292;&#25512;&#33616;&#30340;EEG&#23548;&#32852;&#65289;&#21040;&#26681;&#25454;&#24180;&#40836;&#35814;&#32454;&#30340;&#30561;&#30496;&#35780;&#20998;&#35268;&#21017;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#33258;&#21160;&#30561;&#30496;&#35780;&#20998;&#31995;&#32479;&#22987;&#32456;&#23558;&#26631;&#20934;&#20316;&#20026;&#22522;&#26412;&#25351;&#21335;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#35780;&#20998;&#31639;&#27861;&#21487;&#33021;&#19981;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25110;&#20005;&#26684;&#36981;&#23432;AASM&#25351;&#21335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;U-Sleep&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#30561;&#30496;&#35780;&#20998;&#31639;&#27861;&#65292;&#21363;&#20351;&#20351;&#29992;&#20020;&#24202;&#38750;&#25512;&#33616;&#25110;&#38750;&#20256;&#32479;&#30340;&#23548;&#32852;&#65292;&#20063;&#21487;&#20197;&#36275;&#22815;&#24378;&#22823;&#22320;&#35299;&#20915;&#35780;&#20998;&#20219;&#21153;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#21033;&#29992;&#26377;&#20851;&#21463;&#35797;&#32773;&#24180;&#40836;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
AASM guidelines are the result of decades of efforts aiming at standardizing sleep scoring procedure, with the final goal of sharing a worldwide common methodology. The guidelines cover several aspects from the technical/digital specifications,e.g., recommended EEG derivations, to detailed sleep scoring rules accordingly to age. Automated sleep scoring systems have always largely exploited the standards as fundamental guidelines. In this context, deep learning has demonstrated better performance compared to classical machine learning. Our present work shows that a deep learning based sleep scoring algorithm may not need to fully exploit the clinical knowledge or to strictly adhere to the AASM guidelines. Specifically, we demonstrate that U-Sleep, a state-of-the-art sleep scoring algorithm, can be strong enough to solve the scoring task even using clinically non-recommended or non-conventional derivations, and with no need to exploit information about the chronological age of the subjec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#65292;&#36890;&#36807;&#28608;&#27963;&#35821;&#35328;&#29305;&#23450;&#30340;&#23376;&#32593;&#32476;&#26469;&#26174;&#24335;&#22320;&#23398;&#20064;&#27599;&#31181;&#35821;&#35328;&#30340;&#21442;&#25968;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#21512;&#22810;&#35821;&#35328;&#35757;&#32451;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#30456;&#27604;&#20110;&#23494;&#38598;&#27169;&#22411;&#21644;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05735</link><description>&lt;p&gt;
&#23398;&#20064;ASR&#36335;&#24452;&#65306;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning ASR pathways: A sparse multilingual ASR model. (arXiv:2209.05735v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#65292;&#36890;&#36807;&#28608;&#27963;&#35821;&#35328;&#29305;&#23450;&#30340;&#23376;&#32593;&#32476;&#26469;&#26174;&#24335;&#22320;&#23398;&#20064;&#27599;&#31181;&#35821;&#35328;&#30340;&#21442;&#25968;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#21512;&#22810;&#35821;&#35328;&#35757;&#32451;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#30456;&#27604;&#20110;&#23494;&#38598;&#27169;&#22411;&#21644;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a sparse multilingual ASR model, which explicitly learns the parameters for each language by activating language-specific sub-networks, and enables knowledge transfer for lower-resource languages via joint multilingual training. The proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models.
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26377;&#25928;&#22320;&#21387;&#32553;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#35821;&#35328;ASR&#20013;&#65292;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#21487;&#33021;&#20250;&#23548;&#33268;&#26576;&#20123;&#35821;&#35328;&#30340;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#22240;&#20026;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#25513;&#30721;&#21487;&#33021;&#19981;&#36866;&#21512;&#25152;&#26377;&#35821;&#35328;&#24182;&#19988;&#20002;&#24323;&#37325;&#35201;&#30340;&#35821;&#35328;&#29305;&#23450;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ASR&#36335;&#24452;&#65292;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#65292;&#23427;&#28608;&#27963;&#35821;&#35328;&#29305;&#23450;&#30340;&#23376;&#32593;&#32476;&#65288;&#8220;&#36335;&#24452;&#8221;&#65289;&#65292;&#20197;&#20415;&#20026;&#27599;&#31181;&#35821;&#35328;&#26174;&#24335;&#22320;&#23398;&#20064;&#21442;&#25968;&#12290;&#36890;&#36807;&#37325;&#21472;&#30340;&#23376;&#32593;&#32476;&#65292;&#20849;&#20139;&#21442;&#25968;&#36824;&#21487;&#20197;&#36890;&#36807;&#32852;&#21512;&#22810;&#35821;&#35328;&#35757;&#32451;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#23398;&#20064;ASR&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#27969;&#24335;RNN-T&#27169;&#22411;&#22312;4&#31181;&#35821;&#35328;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;ASR&#36335;&#24452;&#27169;&#22411;&#20248;&#20110;&#23494;&#38598;&#27169;&#22411;&#21644;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;&#21333;&#35821;&#31232;&#30095;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning compresses automatic speech recognition (ASR) models effectively. However, in multilingual ASR, language-agnostic pruning may lead to severe performance drops on some languages because language-agnostic pruning masks may not fit all languages and discard important language-specific parameters. In this work, we present ASR pathways, a sparse multilingual ASR model that activates language-specific sub-networks ("pathways"), such that the parameters for each language are learned explicitly. With the overlapping sub-networks, the shared parameters can also enable knowledge transfer for lower-resource languages via joint multilingual training. We propose a novel algorithm to learn ASR pathways, and evaluate the proposed method on 4 languages with a streaming RNN-T model. Our proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;Uconv-Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#23558;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;16&#20493;&#65292;&#21152;&#36895;&#20013;&#38388;&#23618;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19978;&#37319;&#26679;&#22359;&#35299;&#20915;&#20102;&#25910;&#25947;&#38382;&#39064;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;WER&#21644;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2208.07657</link><description>&lt;p&gt;
Uconv-Conformer: &#38024;&#23545;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#22823;&#24133;&#32553;&#20943;&#30340;&#26032;&#22411;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Uconv-Conformer: High Reduction of Input Sequence Length for End-to-End Speech Recognition. (arXiv:2208.07657v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;Uconv-Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#23558;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;16&#20493;&#65292;&#21152;&#36895;&#20013;&#38388;&#23618;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19978;&#37319;&#26679;&#22359;&#35299;&#20915;&#20102;&#25910;&#25947;&#38382;&#39064;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;WER&#21644;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a new Uconv-Conformer architecture that reduces the input sequence length by 16 times, speeds up the work of intermediate layers, and solves the convergence issue by using upsampling blocks. The Uconv-Conformer architecture shows better WER and faster training and inference speed.
&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#29616;&#20195;ASR&#26550;&#26500;&#26159;&#26368;&#39640;&#20248;&#20808;&#32423;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#33410;&#30465;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#35768;&#22810;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;Conformer&#27169;&#22411;&#30340;&#26032;&#22411;Uconv-Conformer&#26550;&#26500;&#12290;&#23427;&#36890;&#36807;16&#20493;&#30340;&#19968;&#33268;&#24615;&#32553;&#30701;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#20013;&#38388;&#23618;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#19982;&#26102;&#38388;&#32500;&#24230;&#22823;&#24133;&#32553;&#20943;&#30456;&#20851;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20687;U-Net&#26550;&#26500;&#20013;&#30340;&#19978;&#37319;&#26679;&#22359;&#26469;&#30830;&#20445;&#27491;&#30830;&#30340;CTC&#25439;&#22833;&#35745;&#31639;&#21644;&#31283;&#23450;&#32593;&#32476;&#35757;&#32451;&#12290;Uconv-Conformer&#26550;&#26500;&#19981;&#20165;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#26356;&#24555;&#65292;&#32780;&#19988;&#19982;&#22522;&#32447;Conformer&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;WER&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;Uconv-Conformer&#27169;&#22411;&#22312;CPU&#21644;GPU&#19978;&#20998;&#21035;&#26174;&#31034;&#20986;47.8&#65285;&#21644;23.5&#65285;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#30456;&#23545;WER&#30340;&#20943;&#23569;&#20998;&#21035;&#20026;7.3&#65285;&#21644;9.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization of modern ASR architectures is among the highest priority tasks since it saves many computational resources for model training and inference. The work proposes a new Uconv-Conformer architecture based on the standard Conformer model. It consistently reduces the input sequence length by 16 times, which results in speeding up the work of the intermediate layers. To solve the convergence issue connected with such a significant reduction of the time dimension, we use upsampling blocks like in the U-Net architecture to ensure the correct CTC loss calculation and stabilize network training. The Uconv-Conformer architecture appears to be not only faster in terms of training and inference speed but also shows better WER compared to the baseline Conformer. Our best Uconv-Conformer model shows 47.8% and 23.5% inference acceleration on the CPU and GPU, respectively. Relative WER reduction is 7.3% and 9.2% on LibriSpeech test_clean and test_other respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#35821;&#38899;&#25968;&#25454;&#38598;DailyTalk&#65292;&#19987;&#38376;&#20026;&#23545;&#35805;TTS&#35774;&#35745;&#12290;DailyTalk&#21487;&#20197;&#29992;&#20316;&#36890;&#29992;&#30340;TTS&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#22522;&#32447;&#21487;&#20197;&#34920;&#31034;DailyTalk&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2207.01063</link><description>&lt;p&gt;
DailyTalk&#65306;&#38754;&#21521;&#23545;&#35805;&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#21475;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech. (arXiv:2207.01063v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#35821;&#38899;&#25968;&#25454;&#38598;DailyTalk&#65292;&#19987;&#38376;&#20026;&#23545;&#35805;TTS&#35774;&#35745;&#12290;DailyTalk&#21487;&#20197;&#29992;&#20316;&#36890;&#29992;&#30340;TTS&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#22522;&#32447;&#21487;&#20197;&#34920;&#31034;DailyTalk&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a high-quality conversational speech dataset DailyTalk designed for conversational TTS. DailyTalk can be used as a general TTS dataset, and the baseline can represent contextual information from DailyTalk.
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#25968;&#25454;&#38598;&#37117;&#26159;&#30001;&#21333;&#20010;&#35805;&#35821;&#32452;&#25104;&#65292;&#32570;&#20047;&#23545;&#35805;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DailyTalk&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#20026;&#23545;&#35805;TTS&#35774;&#35745;&#12290;&#25105;&#20204;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;DailyDialog&#20013;&#25277;&#26679;&#12289;&#20462;&#25913;&#21644;&#24405;&#21046;&#20102;2,541&#20010;&#23545;&#35805;&#65292;&#24182;&#32487;&#25215;&#20102;&#20854;&#27880;&#37322;&#23646;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#20316;&#20026;&#25105;&#20204;&#30340;&#22522;&#32447;&#65292;&#20854;&#20013;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;TTS&#22312;&#23545;&#35805;&#20013;&#30340;&#21382;&#21490;&#20449;&#24687;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#12290;&#36890;&#36807;&#22522;&#32447;&#23454;&#39564;&#21644;&#25105;&#20204;&#30340;&#26032;&#39062;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DailyTalk&#21487;&#20197;&#29992;&#20316;&#36890;&#29992;&#30340;TTS&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#22522;&#32447;&#21487;&#20197;&#34920;&#31034;DailyTalk&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;DailyTalk&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#20195;&#30721;&#21487;&#20379;&#23398;&#26415;&#29992;&#36884;&#20813;&#36153;&#20351;&#29992;&#65292;&#37319;&#29992;CC-BY-SA 4.0&#35768;&#21487;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of current Text-to-Speech (TTS) datasets, which are collections of individual utterances, contain few conversational aspects. In this paper, we introduce DailyTalk, a high-quality conversational speech dataset designed for conversational TTS. We sampled, modified, and recorded 2,541 dialogues from the open-domain dialogue dataset DailyDialog inheriting its annotated attributes. On top of our dataset, we extend prior work as our baseline, where a non-autoregressive TTS is conditioned on historical information in a dialogue. From the baseline experiment with both general and our novel metrics, we show that DailyTalk can be used as a general TTS dataset, and more than that, our baseline can represent contextual information from DailyTalk. The DailyTalk dataset and baseline code are freely available for academic use with CC-BY-SA 4.0 license.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#21644;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.02307</link><description>&lt;p&gt;
&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#21644;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a semi-supervised medical image segmentation bootstrapping method based on anatomical-aware contrastive distillation, which solves the problem of imbalanced medical image data by softly labeling negative samples and capturing more semantically similar features.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#27880;&#37322;&#31232;&#32570;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#21307;&#23398;&#22270;&#20687;&#20855;&#26377;&#24179;&#34913;&#30340;&#31867;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#65288;&#21363;&#22810;&#31867;&#26631;&#31614;&#19981;&#24179;&#34913;&#65289;&#65292;&#36825;&#33258;&#28982;&#22320;&#20135;&#29983;&#27169;&#31946;&#30340;&#36718;&#24275;&#24182;&#36890;&#24120;&#38169;&#35823;&#22320;&#26631;&#35760;&#32597;&#35265;&#30340;&#23545;&#35937;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#36127;&#26679;&#26412;&#26159;&#21542;&#21516;&#26679;&#36127;&#38754;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTION&#65292;&#19968;&#31181;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#32780;&#19981;&#26159;&#27491;&#36127;&#23545;&#20043;&#38388;&#30340;&#20108;&#20803;&#30417;&#30563;&#26469;&#24320;&#21457;&#36845;&#20195;&#23545;&#27604;&#33976;&#39311;&#31639;&#27861;&#12290;&#19982;&#27491;&#26679;&#26412;&#30456;&#27604;&#65292;&#25105;&#20204;&#36824;&#20174;&#38543;&#26426;&#36873;&#25321;&#30340;&#36127;&#26679;&#26412;&#38598;&#20013;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#65292;&#20197;&#24378;&#21046;&#25191;&#34892;&#37319;&#26679;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#30340;&#21551;&#21160;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has shown great promise over annotation scarcity problems in the context of medical image segmentation. Existing approaches typically assume a balanced class distribution for both labeled and unlabeled medical images. However, medical image data in reality is commonly imbalanced (i.e., multi-class label imbalance), which naturally yields blurry contours and usually incorrectly labels rare objects. Moreover, it remains unclear whether all negative samples are equally negative. In this work, we present ACTION, an Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised medical image segmentation. Specifically, we first develop an iterative contrastive distillation algorithm by softly labeling the negatives rather than binary supervision between positive and negative pairs. We also capture more semantically similar features from the randomly chosen negative set compared to the positives to enforce the diversity of the sampled data. Second, we raise a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24050;&#34987;&#25764;&#22238;&#65292;&#21407;&#22240;&#26159;&#35821;&#35328;&#21644;&#29702;&#35770;&#25551;&#36848;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#20316;&#32773;&#24050;&#32463;&#36827;&#34892;&#20102;&#20462;&#35746;&#21644;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2204.03471</link><description>&lt;p&gt;
DynLight: &#22810;&#32423;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#23454;&#29616;&#21160;&#24577;&#30456;&#20301;&#26102;&#38271;
&lt;/p&gt;
&lt;p&gt;
DynLight: Realize dynamic phase duration with multi-level traffic signal control. (arXiv:2204.03471v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24050;&#34987;&#25764;&#22238;&#65292;&#21407;&#22240;&#26159;&#35821;&#35328;&#21644;&#29702;&#35770;&#25551;&#36848;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#20316;&#32773;&#24050;&#32463;&#36827;&#34892;&#20102;&#20462;&#35746;&#21644;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article has been withdrawn due to unsatisfactory language and theoretical description, and the authors have revised and updated it.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22240;&#20197;&#19979;&#21407;&#22240;&#25764;&#22238;&#26412;&#25991;&#65306;1.&#26412;&#25991;&#30340;&#35821;&#35328;&#21644;&#29702;&#35770;&#25551;&#36848;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65307;2.&#25105;&#20204;&#22312;&#20854;&#20182;&#20316;&#32773;&#30340;&#24110;&#21161;&#19979;&#20016;&#23500;&#21644;&#20462;&#35746;&#20102;&#26412;&#25991;&#65307;3.&#25105;&#20204;&#24517;&#39035;&#26356;&#26032;&#20316;&#32773;&#36129;&#29486;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to withdraw this article for the following reasons: 1 this article is not satisfactory for limited language and theoretical description; 2 we have enriched and revised this article with the help of other authors; 3 we must update the author contribution information.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#33410;&#21644;&#23383;&#31526;&#30446;&#26631;&#30340;&#20132;&#26367;&#20013;&#38388;&#26465;&#20214;&#26041;&#27861;&#65292;&#21033;&#29992;&#23383;&#31526;&#32423;&#21644;&#38899;&#33410;&#32423;&#20013;&#38388;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#26469;&#22788;&#29702;&#26085;&#35821;ASR&#20013;&#30340;&#22810;&#23545;&#19968;&#21644;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2204.00175</link><description>&lt;p&gt;
&#26085;&#35821;ASR&#20013;&#22522;&#20110;&#38899;&#33410;&#21644;&#23383;&#31526;&#30446;&#26631;&#30340;&#20132;&#26367;&#20013;&#38388;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Alternate Intermediate Conditioning with Syllable-level and Character-level Targets for Japanese ASR. (arXiv:2204.00175v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00175
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#33410;&#21644;&#23383;&#31526;&#30446;&#26631;&#30340;&#20132;&#26367;&#20013;&#38388;&#26465;&#20214;&#26041;&#27861;&#65292;&#21033;&#29992;&#23383;&#31526;&#32423;&#21644;&#38899;&#33410;&#32423;&#20013;&#38388;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#26469;&#22788;&#29702;&#26085;&#35821;ASR&#20013;&#30340;&#22810;&#23545;&#19968;&#21644;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an alternate intermediate conditioning method with syllable-level and character-level targets to deal with the many-to-one and one-to-many mapping problems in Japanese ASR, and achieves better performance than conventional multi-task and Self-conditioned CTC methods in experiments.
&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30452;&#25509;&#23558;&#36755;&#20837;&#35821;&#38899;&#26144;&#23556;&#21040;&#23383;&#31526;&#12290;&#28982;&#32780;&#65292;&#24403;&#22810;&#20010;&#19981;&#21516;&#30340;&#21457;&#38899;&#24212;&#35813;&#26144;&#23556;&#21040;&#19968;&#20010;&#23383;&#31526;&#25110;&#19968;&#20010;&#21457;&#38899;&#34987;&#22810;&#20010;&#19981;&#21516;&#30340;&#23383;&#31526;&#20849;&#20139;&#26102;&#65292;&#26144;&#23556;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#30001;&#20110;&#26085;&#35821;&#27721;&#23383;&#30340;&#23384;&#22312;&#65292;&#26085;&#35821;ASR&#26368;&#23481;&#26131;&#36973;&#21463;&#36825;&#31181;&#22810;&#23545;&#19968;&#21644;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23383;&#31526;&#21644;&#38899;&#33410;&#20043;&#38388;&#30340;&#26174;&#24335;&#20132;&#20114;&#65292;&#20351;&#29992;&#33258;&#25105;&#26465;&#20214;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#65292;&#20854;&#20013;&#19978;&#23618;&#8220;&#33258;&#25105;&#26465;&#20214;&#8221;&#20110;&#19979;&#23618;&#30340;&#20013;&#38388;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#23383;&#31526;&#32423;&#21644;&#38899;&#33410;&#32423;&#20013;&#38388;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#26469;&#22788;&#29702;&#23383;&#31526;&#21644;&#38899;&#33410;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#33258;&#21457;&#26085;&#35821;&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#22810;&#20219;&#21153;&#21644;&#33258;&#25105;&#26465;&#20214;CTC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end automatic speech recognition directly maps input speech to characters. However, the mapping can be problematic when several different pronunciations should be mapped into one character or when one pronunciation is shared among many different characters. Japanese ASR suffers the most from such many-to-one and one-to-many mapping problems due to Japanese kanji characters. To alleviate the problems, we introduce explicit interaction between characters and syllables using Self-conditioned connectionist temporal classification (CTC), in which the upper layers are ``self-conditioned'' on the intermediate predictions from the lower layers. The proposed method utilizes character-level and syllable-level intermediate predictions as conditioning features to deal with mutual dependency between characters and syllables. Experimental results on Corpus of Spontaneous Japanese show that the proposed method outperformed the conventional multi-task and Self-conditioned CTC methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25197;&#30697;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#30452;&#25509;&#39044;&#27979;&#20851;&#33410;&#25197;&#30697;&#65292;&#36991;&#20813;&#20351;&#29992;PD&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#22235;&#36275;&#21160;&#29289;&#33021;&#22815;&#31359;&#36234;&#21508;&#31181;&#22320;&#24418;&#24182;&#25269;&#25239;&#22806;&#37096;&#24178;&#25200;&#65292;&#21516;&#26102;&#20445;&#25345;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2203.05194</link><description>&lt;p&gt;
&#23398;&#20064;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#30340;&#25197;&#30697;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Torque Control for Quadrupedal Locomotion. (arXiv:2203.05194v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25197;&#30697;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#30452;&#25509;&#39044;&#27979;&#20851;&#33410;&#25197;&#30697;&#65292;&#36991;&#20813;&#20351;&#29992;PD&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#22235;&#36275;&#21160;&#29289;&#33021;&#22815;&#31359;&#36234;&#21508;&#31181;&#22320;&#24418;&#24182;&#25269;&#25239;&#22806;&#37096;&#24178;&#25200;&#65292;&#21516;&#26102;&#20445;&#25345;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a torque-based reinforcement learning framework that directly predicts joint torques, avoiding the use of a PD controller. The framework is validated through extensive experiments, where a quadruped is capable of traversing various terrain and resisting external disturbances while maintaining locomotion.
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#20026;&#24320;&#21457;&#22235;&#36275;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#29992;&#20110;&#36816;&#21160;&#30340;RL&#35774;&#35745;&#36981;&#24490;&#22522;&#20110;&#20301;&#32622;&#30340;&#33539;&#20363;&#65292;&#20854;&#20013;RL&#31574;&#30053;&#20197;&#20302;&#39057;&#29575;&#36755;&#20986;&#30446;&#26631;&#20851;&#33410;&#20301;&#32622;&#65292;&#28982;&#21518;&#30001;&#39640;&#39057;&#27604;&#20363;-&#23548;&#25968;&#65288;PD&#65289;&#25511;&#21046;&#22120;&#36319;&#36394;&#20197;&#20135;&#29983;&#20851;&#33410;&#25197;&#30697;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23545;&#20110;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#24050;&#32463;&#20174;&#22522;&#20110;&#20301;&#32622;&#30340;&#25511;&#21046;&#33539;&#20363;&#36716;&#21521;&#22522;&#20110;&#25197;&#30697;&#30340;&#25511;&#21046;&#12290;&#37492;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25197;&#30697;&#30340;RL&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20301;&#32622;&#30340;RL&#33539;&#20363;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;RL&#31574;&#30053;&#30452;&#25509;&#22312;&#39640;&#39057;&#29575;&#19979;&#39044;&#27979;&#20851;&#33410;&#25197;&#30697;&#65292;&#20174;&#32780;&#36991;&#20813;&#20351;&#29992;PD&#25511;&#21046;&#22120;&#12290;&#25152;&#25552;&#20986;&#30340;&#23398;&#20064;&#25197;&#30697;&#25511;&#21046;&#26694;&#26550;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#22235;&#36275;&#21160;&#29289;&#33021;&#22815;&#31359;&#36234;&#21508;&#31181;&#22320;&#24418;&#24182;&#25269;&#25239;&#22806;&#37096;&#24178;&#25200;&#65292;&#21516;&#26102;&#20445;&#25345;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has become a promising approach to developing controllers for quadrupedal robots. Conventionally, an RL design for locomotion follows a position-based paradigm, wherein an RL policy outputs target joint positions at a low frequency that are then tracked by a high-frequency proportional-derivative (PD) controller to produce joint torques. In contrast, for the model-based control of quadrupedal locomotion, there has been a paradigm shift from position-based control to torque-based control. In light of the recent advances in model-based control, we explore an alternative to the position-based RL paradigm, by introducing a torque-based RL framework, where an RL policy directly predicts joint torques at a high frequency, thus circumventing the use of a PD controller. The proposed learning torque control framework is validated with extensive experiments, in which a quadruped is capable of traversing various terrain and resisting external disturbances while followi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#65292;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2111.09445</link><description>&lt;p&gt;
FLSys&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#31227;&#21160;&#24212;&#29992;&#30340;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FLSys: Toward an Open Ecosystem for Federated Learning Mobile Apps. (arXiv:2111.09445v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#65292;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces FLSys, a mobile-cloud federated learning (FL) system that can be a key component for an open ecosystem of FL models and apps. FLSys is designed to work on smart phones with mobile sensing data. It balances model performance with resource consumption, tolerates communication failures, and achieves scalability. FLSys provides advanced privacy preserving mechanisms and a common API for third-party app developers to access FL models.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#30340;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;FLSys&#20013;&#65292;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#21644;&#19981;&#21516;&#30340;FL&#32858;&#21512;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#34987;&#19981;&#21516;&#30340;&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;&#21644;&#35775;&#38382;&#12290;&#27492;&#22806;&#65292;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;FLSys&#37319;&#29992;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#23454;&#29616;&#22312;Android&#21644;AWS&#20113;&#20013;&#12290;&#25105;&#20204;&#19982;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#27169;&#22411;&#20849;&#21516;&#35774;&#35745;&#20102;FLSys&#12290;&#22312;4&#20010;&#26376;&#30340;&#26102;&#38388;&#37324;&#65292;&#20174;100&#22810;&#21517;&#22823;&#23398;&#29983;&#20013;&#25910;&#38598;&#20102;HAR&#24863;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;HAR-Wild&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#31227;&#21160;&#35774;&#22791;&#37327;&#36523;&#23450;&#21046;&#30340;CNN&#27169;&#22411;&#65292;&#20855;&#26377;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#20197;&#20943;&#36731;p
&lt;/p&gt;
&lt;p&gt;
This article presents the design, implementation, and evaluation of FLSys, a mobile-cloud federated learning (FL) system, which can be a key component for an open ecosystem of FL models and apps. FLSys is designed to work on smart phones with mobile sensing data. It balances model performance with resource consumption, tolerates communication failures, and achieves scalability. In FLSys, different DL models with different FL aggregation methods can be trained and accessed concurrently by different apps. Furthermore, FLSys provides advanced privacy preserving mechanisms and a common API for third-party app developers to access FL models. FLSys adopts a modular design and is implemented in Android and AWS cloud. We co-designed FLSys with a human activity recognition (HAR) model. HAR sensing data was collected in the wild from 100+ college students during a 4-month period. We implemented HAR-Wild, a CNN model tailored to mobile devices, with a data augmentation mechanism to mitigate the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#35745;&#31639;&#21487;&#36798;&#38598;&#30340;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#32447;&#24615;&#12289;&#22810;&#39033;&#24335;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;zonotope&#65292;&#21487;&#20197;&#25552;&#20379;&#36739;&#23569;&#20445;&#23432;&#30340;&#21487;&#36798;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#20851;&#20110;&#26410;&#30693;&#31995;&#32479;&#27169;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#32435;&#20837;&#35745;&#31639;&#12290;&#31639;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#20540;&#31034;&#20363;&#21644;&#23454;&#38469;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2105.07229</link><description>&lt;p&gt;
&#20174;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Reachability Analysis from Noisy Data. (arXiv:2105.07229v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.07229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#35745;&#31639;&#21487;&#36798;&#38598;&#30340;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#32447;&#24615;&#12289;&#22810;&#39033;&#24335;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;zonotope&#65292;&#21487;&#20197;&#25552;&#20379;&#36739;&#23569;&#20445;&#23432;&#30340;&#21487;&#36798;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#20851;&#20110;&#26410;&#30693;&#31995;&#32479;&#27169;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#32435;&#20837;&#35745;&#31639;&#12290;&#31639;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#20540;&#31034;&#20363;&#21644;&#23454;&#38469;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an algorithm for computing reachable sets directly from noisy data without a given system model, which is applicable to different types of systems including linear, polynomial, and nonlinear systems. The algorithm is based on matrix zonotopes and can provide less conservative reachable sets while incorporating prior knowledge about the unknown system model. Theoretical guarantees are given and the applicability of the algorithm is demonstrated through numerical examples and real experiments.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#27809;&#26377;&#32473;&#23450;&#31995;&#32479;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#20174;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#35745;&#31639;&#21487;&#36798;&#38598;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#25968;&#25454;&#30340;&#19981;&#21516;&#31867;&#22411;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;zonotope&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#32447;&#24615;&#31995;&#32479;&#30340;&#36807;&#20272;&#35745;&#21487;&#36798;&#38598;&#12290;&#24341;&#20837;&#20102;&#32422;&#26463;&#30697;&#38453;zonotope&#20197;&#25552;&#20379;&#36739;&#23569;&#20445;&#23432;&#30340;&#21487;&#36798;&#38598;&#65292;&#20294;&#20195;&#20215;&#26159;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#29992;&#20110;&#23558;&#20851;&#20110;&#26410;&#30693;&#31995;&#32479;&#27169;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#32435;&#20837;&#35745;&#31639;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#39033;&#24335;&#31995;&#32479;&#65292;&#24182;&#22312;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#19979;&#25193;&#23637;&#21040;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#26159;&#23427;&#20204;&#32473;&#20986;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;&#21487;&#36798;&#38598;&#30340;&#36866;&#24403;&#36807;&#20272;&#35745;&#21487;&#36798;&#38598;&#12290;&#22810;&#20010;&#25968;&#20540;&#31034;&#20363;&#21644;&#23454;&#38469;&#23454;&#39564;&#26174;&#31034;&#20102;&#24341;&#20837;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#31639;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of computing reachable sets directly from noisy data without a given system model. Several reachability algorithms are presented for different types of systems generating the data. First, an algorithm for computing over-approximated reachable sets based on matrix zonotopes is proposed for linear systems. Constrained matrix zonotopes are introduced to provide less conservative reachable sets at the cost of increased computational expenses and utilized to incorporate prior knowledge about the unknown system model. Then we extend the approach to polynomial systems and, under the assumption of Lipschitz continuity, to nonlinear systems. Theoretical guarantees are given for these algorithms in that they give a proper over-approximate reachable set containing the true reachable set. Multiple numerical examples and real experiments show the applicability of the introduced algorithms, and comparisons are made between algorithms.
&lt;/p&gt;</description></item></channel></rss>