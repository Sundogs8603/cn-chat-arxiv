# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Instruction Mining: High-Quality Instruction Data Selection for Large Language Models.](http://arxiv.org/abs/2307.06290) | 本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。 |
| [^2] | [MMBench: Is Your Multi-modal Model an All-around Player?.](http://arxiv.org/abs/2307.06281) | MMBench是一个新型的多模态基准测试，旨在解决大型视觉语言模型评估的挑战，通过开发全面的评估流程和精心策划的数据集进行细粒度能力评估。 |
| [^3] | [Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches.](http://arxiv.org/abs/2307.06218) | Ashaar是一个使用深度学习方法自动分析和生成阿拉伯诗歌的框架，涵盖了韵律、主题和时代分类等多个方面，并具备自动诗歌音标化的功能。 |
| [^4] | [Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems.](http://arxiv.org/abs/2307.06187) | 本文提出了将大型语言模型（LLMs）集成到多智能体系统中的方法。通过自适应系统和有效通信之间的相互作用，我们可以实现对复杂环境变化的监测、分析、计划和执行系统自适应的支持。 |
| [^5] | [Enhancing Portuguese Sign Language Animation with Dynamic Timing and Mouthing.](http://arxiv.org/abs/2307.06124) | 本文提出了一个新的动态方法，用于改善葡萄牙手语动画，重点是改进手语之间的过渡，并通过包含嘴型行为提高了初学者的理解和手语自然度表现。这对计算语言学、人机交互和手语动画的合成有重要意义。 |
| [^6] | [VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View.](http://arxiv.org/abs/2307.06082) | VELMA是一个口头化的LLM智能体，利用人类写作的导航指令中的地标并结合CLIP来进行视觉环境的理解，以实现在街景中的视觉和语言导航。 |
| [^7] | [Interpreting deep embeddings for disease progression clustering.](http://arxiv.org/abs/2307.06060) | 本文提出了一种在疾病进展聚类中解读深度嵌入的新方法，并通过评估2型糖尿病参与者数据集展示了对疾病进展模式的临床意义性见解。 |
| [^8] | [A Study on the Appropriate size of the Mongolian general corpus.](http://arxiv.org/abs/2307.06050) | 本研究通过使用 Heaps 函数和类型标记比来确定蒙古文通用语料库的合适规模，观察结果表明标记数量超过3900万至4200万时，类型标记比值几乎不变。 |
| [^9] | [Pluggable Neural Machine Translation Models via Memory-augmented Adapters.](http://arxiv.org/abs/2307.06029) | 通过记忆增强的适配器，我们提出了一种可插拔的方法来控制神经机器翻译模型的生成行为。实验证明，我们的方法可以胜过几个代表性的可插拔基准模型。 |
| [^10] | [PolyLM: An Open Source Polyglot Large Language Model.](http://arxiv.org/abs/2307.06018) | PolyLM是一个开源的多语言大语言模型，通过整合双语数据和采用课程学习策略进行训练，提高了多语言能力，并且使用了多语言自学习方法进行模型微调。模型性能得到了多个现有多语言任务的确认。 |
| [^11] | [DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification.](http://arxiv.org/abs/2307.06005) | 这篇论文提出了一种名为DDNAS的离散化可微分神经架构搜索方法，用于文本分类。通过使用连续松弛的架构表示和互信息最大化的离散化层，DDNAS在文本表示学习和分类任务中表现优于其他NAS方法。 |
| [^12] | [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models.](http://arxiv.org/abs/2307.05973) | VoxPoser提出了一种新方法，通过组合3D价值映射和语言模型，实现了机器人在多种操作任务下根据自由形式的指令和对象合成机器人轨迹的能力。 |
| [^13] | [Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models.](http://arxiv.org/abs/2307.05972) | 本研究探讨了事后训练量化和量化感知训练对Transformer语言模型泛化效果的影响，并提出了自学习量化（SDQ）方法，该方法最小化累积量化误差并在多语言模型上优于基准模型。通过在XGLUE基准上的实验证明，SDQ方法可以将模型从32位浮点权重减少到8位整数权重，同时保持高水平的性能。 |
| [^14] | [Prototypical Contrastive Transfer Learning for Multimodal Language Understanding.](http://arxiv.org/abs/2307.05942) | 提出了一种用于多模态语言理解的原型对比迁移学习方法PCTL，通过引入双重原型例外损失函数，在家庭环境中根据自由形式的自然语言指令识别目标物体的任务中取得了优于现有方法的性能。 |
| [^15] | [Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding.](http://arxiv.org/abs/2307.05908) | 本论文提出了一种预测性流水线解码（PPD）方法，通过并行启动后续令牌解码来加速大型语言模型（LLMs）中的贪婪解码过程，同时保持完全相同的输出。该方法在减少解码延迟方面具有潜力，提供了新的LLM解码策略权衡理解。 |
| [^16] | [Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks.](http://arxiv.org/abs/2307.05827) | 使用卷积和记忆网络，在维基百科的表格数据中进行关系抽取。该模型在关系抽取任务中表现出色，并且经过全面的分析和研究，展示了各个模型组件的贡献。 |
| [^17] | [Improved POS tagging for spontaneous, clinical speech using data augmentation.](http://arxiv.org/abs/2307.05796) | 本文提出了一种使用数据增强技术改进临床语音的词性标注问题的方法，通过在新闻树库上进行训练，使解析器更适应自然、即兴的语音，测试结果表明其性能良好。 |
| [^18] | [Large Language Models.](http://arxiv.org/abs/2307.05782) | 这篇论文介绍了大型语言模型的发展历史和现状，详细描述了底层的Transformer架构，并探讨了如何训练模型来实现多种智能任务。 |
| [^19] | [Neural Machine Translation Data Generation and Augmentation using ChatGPT.](http://arxiv.org/abs/2307.05779) | 使用ChatGPT生成和增强神经机器翻译数据，尽管缺乏多样性，幻象数据在翻译信号方面改善了结果。 |
| [^20] | [Towards Robust and Efficient Continual Language Learning.](http://arxiv.org/abs/2307.05741) | 本文研究了如何实现持续的语言学习，并构建了一个任务序列基准用于评估。最终提出了一个简单但有效的学习算法，以最大程度地利用具有积极传递潜力的任务，并避免负迁移的影响。 |
| [^21] | [Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations.](http://arxiv.org/abs/2307.05722) | 本论文探索了大规模语言模型在在线职位推荐中对图数据的理解能力，并提出了新的框架来分析行为图，发现其中的潜在模式和关系。 |
| [^22] | [A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data.](http://arxiv.org/abs/2307.05696) | 该论文提出了一种个性化强化学习总结服务，通过使用层级个性化基于概念的总结方法，在文本数据呈指数级增长的背景下，帮助用户提取有意义的见解。 |
| [^23] | [Stack More Layers Differently: High-Rank Training Through Low-Rank Updates.](http://arxiv.org/abs/2307.05695) | 本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。 |
| [^24] | [Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models.](http://arxiv.org/abs/2307.05646) | 本研究提出了一个框架，通过在高推断任务上进行微调，来改善方面级情感分类中共指消解的处理。研究表明，该框架可以显著提高模型的共指消解能力，从而改善在具有共指消解的评论中的性能。 |
| [^25] | [Separate-and-Aggregate: A Transformer-based Patch Refinement Model for Knowledge Graph Completion.](http://arxiv.org/abs/2307.05627) | 本文提出了一种基于Transformer的补丁细化模型（PatReFormer）用于知识图谱补全。该模型通过分割嵌入并使用交叉注意力模块来改进实体和关系之间的嵌入特征交互，从而提高了模型性能。 |
| [^26] | [SITTA: A Semantic Image-Text Alignment for Image Captioning.](http://arxiv.org/abs/2307.05591) | SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。 |
| [^27] | [Hate Speech Detection via Dual Contrastive Learning.](http://arxiv.org/abs/2307.05578) | 该论文提出了一种用于恶意言论检测的新颖框架，通过应对恶意言论中的复杂语义信息和恶意言辞的干扰以及恶意言论和非恶意言论的不平衡分布等挑战，实现了跨度级信息的捕捉。 |
| [^28] | [Event Extraction as Question Generation and Answering.](http://arxiv.org/abs/2307.05567) | 本文提出了一种名为QGA-EE的方法，该方法通过问题生成和回答的方式进行事件提取，解决了传统方法中的错误传播问题，并且在预测事件参数上取得了更好的表现。 |
| [^29] | [Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion.](http://arxiv.org/abs/2307.05564) | 本文描述了增强CLIP在处理复合性和歧义性方面，在零样本视觉词义消歧任务中的应用。作者采用Augment-CLIP和Stable Diffusion Sampling（SD Sampling）两个系统来解决CLIP的局限性，通过生成包含上下文短语的句子和使用稳定扩散算法生成多个图像，提高了任务的表现。 |
| [^30] | [Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System.](http://arxiv.org/abs/2307.05560) | 该论文描述了在智利公共医疗系统中设计和部署的一个用于自动编码转诊疾病的系统。该系统使用最先进的NER模型识别疾病提及，并基于Elasticsearch的搜索引擎系统进行编码。 |
| [^31] | [Review of feedback in Automated Essay Scoring.](http://arxiv.org/abs/2307.05553) | 这篇论文综述了自动化论文评分中的反馈研究，包括不同类型的反馈和论文特征，并回顾了提供反馈的最新案例研究。 |
| [^32] | [Advancements in Scientific Controllable Text Generation Methods.](http://arxiv.org/abs/2307.05538) | 本研究提供了一种新的模式来组织可控文本生成的先前工作，通过描述各种调制策略来实现科学文献的可控生成，为基于这些组成部分的组合提供了新的架构，未来研究将通过实证比较这些方法以了解它们的优势和实用性。 |
| [^33] | [Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators.](http://arxiv.org/abs/2307.05532) | 评估了各个项目在代码、数据、模型等方面的开放程度，并发现有许多项目虽自称为开源，却存在不确定合法性的未记录数据，而很少有项目分享重要的指令调整功能。 |
| [^34] | [Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT.](http://arxiv.org/abs/2307.05493) | 本文研究了中学英语作为外语学生与ChatGPT合作完成写作任务的案例，并对提示的质量和数量进行了分析。结果发现，非技术用户在为ChatGPT编写适当的提示上遇到了困难，需要提供更好的支持和指导。 |
| [^35] | [GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study.](http://arxiv.org/abs/2307.05492) | 通过比较GPT和人类审稿人生成的评论，我们发现GPT在同行评审中具有一定的帮助性，为解决同行评审资源限制提供了新途径。 |
| [^36] | [Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference.](http://arxiv.org/abs/2307.05034) | 该论文介绍了一个名为SICCK的合成数据集以及一种新颖的分析方法，用于评估自然语言推理中复杂组合知识的性能。研究发现，在零-shot和微调情况下，神经网络推理模型能够很好地捕捉结构和语义组合的变化。 |
| [^37] | [On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion.](http://arxiv.org/abs/2307.04518) | 本文探索了词汇产生意义的过程，重点研究了儿童语言习得和语言理解模型。作者强调了体验感知、情感和认知在语言习得中的重要性，并提出了对类似儿童环境中语言学习智能体的要求。 |
| [^38] | [A Survey on Evaluation of Large Language Models.](http://arxiv.org/abs/2307.03109) | 本文综述了大型语言模型（LLMs）的评估方法，关注三个关键维度：评估什么、在哪里评估以及如何评估。评估任务包括自然语言处理、推理、医学应用、伦理学、教育、自然和社会科学、代理应用等多个领域。本文为社会层面对LLMs潜在风险的理解提供了重要参考。 |
| [^39] | [Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain.](http://arxiv.org/abs/2307.03042) | 本研究提出了一种参数高效微调（PEFT）方法，在临床领域使用临床记录训练了一个专门适配临床领域的LLaMA-LoRA模型，同时提出了一个两步PEFT框架，用于将其与Downstream LLaMA-LoRA适配器进行融合，以实现领域适应。 |
| [^40] | [Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment.](http://arxiv.org/abs/2307.02682) | 通过在训练阶段不使用视频和标注，而是在测试时仅优化输入，我们提出了一种零样本的密集视频字幕生成方法。通过联合优化文本和时刻，我们的方法能够在视频中准确地定位和描述事件。 |
| [^41] | [PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation.](http://arxiv.org/abs/2307.00470) | PatternGPT是一种基于模式驱动的大型语言模型文本生成框架，通过利用大型语言模型的提取能力生成多样化的模式，并使用联邦学习的思想实现模式共享，最终通过搜索高质量模式指导生成模型。该框架具有生成多样化模式、保护数据隐私、结合外部知识等优势。 |
| [^42] | [Identity Construction in a Misogynist Incels Forum.](http://arxiv.org/abs/2306.15745) | 本研究使用定量文本和网络分析方法，研究了最大的黑洞incels论坛如何讨论身份群体。研究发现该社区产生了许多新的身份术语，存在物质主义的意识形态。对此我们讨论了对自动化 misogynist hate speech 检测研究的影响。 |
| [^43] | [Evaluating the Zero-shot Robustness of Instruction-tuned Language Models.](http://arxiv.org/abs/2306.11270) | 本文评估了指导微调语言模型的零样本鲁棒性，并发现使用新颖但合适的指导措辞会降低模型性能。 |
| [^44] | [Sparse Modular Activation for Efficient Sequence Modeling.](http://arxiv.org/abs/2306.11197) | 本论文引入了稀疏模块激活 (SMA) 机制，用于高效的序列建模。这种机制可以动态地稀疏激活序列元素的子模块，减少计算和内存消耗。 |
| [^45] | [Does mBERT understand Romansh? Evaluating word embeddings using word alignment.](http://arxiv.org/abs/2306.08702) | 本研究通过在德语和罗曼什语的平行句子中使用mBERT和XLM-R的词嵌入，结合相似性对齐模型，评估了mBERT对罗曼什语的理解能力。结果显示mBERT的词嵌入能够有效地对罗曼什语进行词语对齐，为进一步研究提供了有意义和适用的信息。 |
| [^46] | [Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.](http://arxiv.org/abs/2306.05685) | 研究使用强大的LLM作为评判员，通过引入两个基准测试来确认LLM评判员和人类偏好之间的一致性，并可调整聊天助手的模型架构和微调方法来提高其性能。 |
| [^47] | [KIT's Multilingual Speech Translation System for IWSLT 2023.](http://arxiv.org/abs/2306.05320) | 本文介绍了一个为IWSLT 2023多语言翻译贡献的翻译系统，其重点在于翻译科学会议演讲。用“检索式方法”（kNN-MT）进行有效的适应，该系统采用适配器轻松集成来自数据增强的增量训练数据，并展示级联系统更容易适应特定目标领域的优势。 |
| [^48] | [Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models.](http://arxiv.org/abs/2305.18703) | 本文综述了大型语言模型的领域专门化，包括动机、挑战、方法论和评估指标。此外，还提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。 |
| [^49] | [Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network.](http://arxiv.org/abs/2305.12493) | 本研究引入了一个上下文短语预测网络用于基于注意力的深度偏置方法，通过计算偏置损失以帮助训练上下文化模型，在多种端到端语音识别模型上实现了显著的WER降低，相对于基线模型相对WER提高了12.1％，上下文短语的WER相对降低了40.5％。 |
| [^50] | [RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs.](http://arxiv.org/abs/2305.08844) | 本论文提出了RL4F（Reinforcement Learning for Feedback），这是一个多智能体协作框架，通过强化学习生成自然语言反馈来修复模型输出。该方法适用于黑盒或仅有有限访问权限的模型，避免了传统微调方法的计算效率和空间效率问题。 |
| [^51] | [ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification.](http://arxiv.org/abs/2305.04003) | 本文介绍了一种名为ANTONIO的Python库，它基于抽象解释方法提供了一种实用的方法和启发式规则，以便为自然语言处理（NLP）数据集和模型生成已知验证方法的基准。因为其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。 |
| [^52] | [DataComp: In search of the next generation of multimodal datasets.](http://arxiv.org/abs/2304.14108) | DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。 |
| [^53] | [GPT detectors are biased against non-native English writers.](http://arxiv.org/abs/2304.02819) | 该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。 |
| [^54] | [Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference.](http://arxiv.org/abs/2302.09582) | 本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，探讨了语言是否会因果支持情绪推断。实验结果显示，属性特定的神经元操纵导致情绪推断任务的性能下降，这与人类心理空间中不同属性的重要性有关。这些发现为支持基于语言的情绪推断机制提供了因果证据，并凸显了情绪概念知识的贡献。 |
| [^55] | [A data science and machine learning approach to continuous analysis of Shakespeare's plays.](http://arxiv.org/abs/2301.06024) | 通过数据科学和机器学习的方法，对莎士比亚的作品进行了连续分析，发现他的写作风格随着时间的推移发生了变化，其中包括句子长度、形容词和副词的频率以及文本中表达的情感。此外，研究还发现，部分戏剧的风格特征更类似于其写作时间之前或之后的作品。 |
| [^56] | [In and Out-of-Domain Text Adversarial Robustness via Label Smoothing.](http://arxiv.org/abs/2212.10258) | 本文研究了标签平滑策略在不同领域的自然语言处理任务中对对抗鲁棒性的影响。实验证明，标签平滑显著提高了预训练模型的鲁棒性，并减少了在对抗样本上出现的过度自信错误。 |
| [^57] | [Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?.](http://arxiv.org/abs/2212.09747) | 本文评估了CoNLL-2003上超过20种模型的泛化性能，发现NER模型的泛化能力各异。令人惊讶的是，即使使用几十年前的数据进行微调，预训练的Transformer模型仍然保持着很好的性能。研究还发现，预训练语料库与下游测试集之间的时间不匹配是导致性能恶化的主要因素。 |
| [^58] | [Evaluating Human-Language Model Interaction.](http://arxiv.org/abs/2212.09746) | 为了评估人机交互，研究人员开发了一个框架HALIE，该框架捕捉了交互过程、主观体验和偏好概念，并设计了五个任务来涵盖不同形式的交互。 |
| [^59] | [SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation.](http://arxiv.org/abs/2211.00923) | SpeechBlender是一个用于生成发音错误的数据增强框架，在发音错误检测模型的音素级上获得了ASR相关的最新技术水平，具有更有效的样本。 |
| [^60] | [Contrastive Decoding: Open-ended Text Generation as Optimization.](http://arxiv.org/abs/2210.15097) | 对比解码是一种可靠的解码方法，它通过优化对比目标来生成合理且高质量的文本，与仅使用较大的语言模型进行解码相比，对比解码能够避免短而重复的文本和不连贯的文本，并适用于不同的模型规模。 |
| [^61] | [Local Grammar-Based Coding Revisited.](http://arxiv.org/abs/2209.13636) | 本文重新审视了最小局部基于语法的编码问题，并提出了一种新的、更简单、更普遍的证明方法，证明了最小分块编码具有强大的普遍性。同时，通过实验也表明，最小分块编码中的规则数量不能明确区分长记忆和无记忆的源。 |
| [^62] | [Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for Natural Language Processing Tasks.](http://arxiv.org/abs/2208.12081) | Kencorpus项目旨在收集和存储肯尼亚斯瓦希里语、多卢奥语和卢雅语的文本和语音数据，以解决原生非洲语言在自然语言处理中的数字包容性和信息访问问题。这个数据集包含超过5,000个项目，为多卢奥语和卢雅语提供了词性标注集。 |
| [^63] | [A Customized Text Sanitization Mechanism with Differential Privacy.](http://arxiv.org/abs/2207.01193) | 一种基于差分隐私的定制文本消毒机制 (CusText) 提供了更好的隐私保护与效用权衡，适用于任何相似度度量，并在标记级别实现了先进的隐私保护。 |
| [^64] | [Discriminative Models Can Still Outperform Generative Models in Aspect Based Sentiment Analysis.](http://arxiv.org/abs/2206.02892) | 本文评估了判别模型和生成模型在基于方面的情感分析中的表现，发现判别模型在几乎所有设置中仍然可以优于生成模型。 |
| [^65] | [UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text.](http://arxiv.org/abs/2108.08614) | 本文提出了一个名为UNIQORN的问答系统，它能够无缝地处理RDF数据和文本，使用fine-tuned BERT模型为问题构建上下文图，并使用图算法确定与问题相关的子图来回答问题。 |

# 详细

[^1]: 指令挖掘：大语言模型的高质量指令数据选择

    Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])

    [http://arxiv.org/abs/2307.06290](http://arxiv.org/abs/2307.06290)

    本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。

    

    大型语言模型通常经历预训练和微调两个训练阶段。尽管大规模预训练赋予模型强大的生成自然语言回应的能力，但这些预训练模型有时仍然无法理解人类指令。为了增强语言模型解释和响应指令的能力，指令微调已成为该领域的关键方法。最近的研究发现，即使只有少量高质量的指令跟随数据，大型语言模型也可以进行良好的微调。然而，选择用于微调语言模型的高质量数据集仍缺乏明确的指导方针。在本文中，我们提出了InstructMining，一个用于评估指令跟随数据质量的线性规则。我们使用具体的自然语言指标来进行InstructMining的建模。为了研究数据质量与这些指标之间的关系，我们还进行了广泛的细致研究。

    Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
    
[^2]: MMBench: 您的多模态模型是全能球员吗？

    MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])

    [http://arxiv.org/abs/2307.06281](http://arxiv.org/abs/2307.06281)

    MMBench是一个新型的多模态基准测试，旨在解决大型视觉语言模型评估的挑战，通过开发全面的评估流程和精心策划的数据集进行细粒度能力评估。

    

    最近，大型视觉语言模型在视觉信息的感知和推理能力方面取得了显著进展。然而，如何有效评估这些大型视觉语言模型仍然是一个主要障碍，阻碍了未来模型的发展。传统的基准测试，如VQAv2或COCO Caption提供了定量的性能测量，但在细粒度能力评估和非鲁棒评估指标方面存在不足。最近的主观基准测试，如OwlEval，通过整合人力资源，对模型的能力进行了全面评估，但不可扩展并且存在显著的偏见。针对这些挑战，我们提出了MMBench，一种新型的多模态基准测试。MMBench系统地开发了一个全面的评估流程，主要由两个元素组成。第一个元素是精心策划的数据集，在评估数量和多样性方面超越了现有的类似基准测试。

    Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluatio
    
[^3]: Ashaar: 使用深度学习方法自动分析和生成阿拉伯诗歌

    Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches. (arXiv:2307.06218v1 [cs.CL])

    [http://arxiv.org/abs/2307.06218](http://arxiv.org/abs/2307.06218)

    Ashaar是一个使用深度学习方法自动分析和生成阿拉伯诗歌的框架，涵盖了韵律、主题和时代分类等多个方面，并具备自动诗歌音标化的功能。

    

    诗歌在任何国家的文化和传统中都具有极大的重要性。它是诗人表达情感、保留习俗和传达文化精髓的工具。阿拉伯诗歌也不例外，在阿拉伯社区的传统中扮演着珍贵的角色，并在当今时代保持其重要性。通常，理解阿拉伯诗歌需要语言学家的专业知识，他们可以分析其内容并评估其质量。本文介绍了一个名为“Ashaar”的框架，该框架包括一系列用于阿拉伯诗歌分析和生成的数据集和预训练模型。我们提出的方法建立了一个流程，包括诗歌的各个方面，如韵律、主题和时代分类。它还包括自动诗歌音标化，使得可以进行更复杂的分析，如自动生成新的诗歌。

    Poetry holds immense significance within the cultural and traditional fabric of any nation. It serves as a vehicle for poets to articulate their emotions, preserve customs, and convey the essence of their culture. Arabic poetry is no exception, having played a cherished role in the heritage of the Arabic community throughout history and maintaining its relevance in the present era. Typically, comprehending Arabic poetry necessitates the expertise of a linguist who can analyze its content and assess its quality. This paper presents the introduction of a framework called \textit{Ashaar} https://github.com/ARBML/Ashaar, which encompasses a collection of datasets and pre-trained models designed specifically for the analysis and generation of Arabic poetry. The pipeline established within our proposed approach encompasses various aspects of poetry, such as meter, theme, and era classification. It also incorporates automatic poetry diacritization, enabling more intricate analyses like automa
    
[^4]: 基于自适应大型语言模型（LLM）的多智能体系统

    Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems. (arXiv:2307.06187v1 [cs.MA])

    [http://arxiv.org/abs/2307.06187](http://arxiv.org/abs/2307.06187)

    本文提出了将大型语言模型（LLMs）集成到多智能体系统中的方法。通过自适应系统和有效通信之间的相互作用，我们可以实现对复杂环境变化的监测、分析、计划和执行系统自适应的支持。

    

    在自主计算中，自适应被提出作为管理多智能体系统（MASs）复杂性的基本范式。通过添加对系统的监测和自适应支持，以实现特定关注点的目标。在涉及智能体互动的场景中，通信是关键，它通过直接、清晰的信息交流增强合作并减少协调挑战。然而，提高与MASs的交互通信的表达能力并非没有挑战。因此，自适应系统对有效通信的相互作用对于未来MAS的进步至关重要。在本文中，我们提出将基于GPT的大型语言模型（LLMs）集成到多智能体系统中。我们的方法论基于MAPE-K模型，该模型以其在响应动态环境变化中监测、分析、计划和执行系统自适应的强大支持而闻名。

    In autonomic computing, self-adaptation has been proposed as a fundamental paradigm to manage the complexity of multiagent systems (MASs). This achieved by extending a system with support to monitor and adapt itself to achieve specific concerns of interest. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, improving the expressiveness of the interaction communication with MASs is not without challenges. In this sense, the interplay between self-adaptive systems and effective communication is crucial for future MAS advancements. In this paper, we propose the integration of large language models (LLMs) such as GPT-based technologies into multiagent systems. We anchor our methodology on the MAPE-K model, which is renowned for its robust support in monitoring, analyzing, planning, and executing system adaptations in response to dynami
    
[^5]: 用动态定时和嘴型提升葡萄牙手语动画

    Enhancing Portuguese Sign Language Animation with Dynamic Timing and Mouthing. (arXiv:2307.06124v1 [cs.CL])

    [http://arxiv.org/abs/2307.06124](http://arxiv.org/abs/2307.06124)

    本文提出了一个新的动态方法，用于改善葡萄牙手语动画，重点是改进手语之间的过渡，并通过包含嘴型行为提高了初学者的理解和手语自然度表现。这对计算语言学、人机交互和手语动画的合成有重要意义。

    

    当前的手语动画经常被认为是不自然的，因为它们不能准确地复制人类手语者的同步身体行为的细微差别。在本文中，我们提出一种新的动态方法，用于手语之间的过渡，重点放在葡萄牙手语的嘴型动画上。虽然母语手语者更喜欢具有动态过渡的动画，但我们并未发现在理解和感知自然度评分方面存在显著差异。另一方面，我们表明包含嘴型行为对于初学手语的学习者来说，提高了理解和感知自然度。这些结果对计算语言学、人机交互和手语动画的合成具有重要意义。

    Current signing avatars are often described as unnatural as they cannot accurately reproduce all the subtleties of synchronized body behaviors of a human signer. In this paper, we propose a new dynamic approach for transitions between signs, focusing on mouthing animations for Portuguese Sign Language. Although native signers preferred animations with dynamic transitions, we did not find significant differences in comprehension and perceived naturalness scores. On the other hand, we show that including mouthing behaviors improved comprehension and perceived naturalness for novice sign language learners. Results have implications in computational linguistics, human-computer interaction, and synthetic animation of signing avatars.
    
[^6]: VELMA: LLM智能体在街景中进行视觉和语言导航的口头化体现

    VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v1 [cs.AI])

    [http://arxiv.org/abs/2307.06082](http://arxiv.org/abs/2307.06082)

    VELMA是一个口头化的LLM智能体，利用人类写作的导航指令中的地标并结合CLIP来进行视觉环境的理解，以实现在街景中的视觉和语言导航。

    

    在现实世界环境中的增量决策是具有挑战性的以体现人工智能的任务之一。其中最具挑战性的场景之一是视觉和语言导航(VLN)，它需要视觉和自然语言理解以及空间和时间推理能力。这个体现智能体需要在街景等真实世界环境的观察基础上准确理解导航指令。尽管LLM在其他研究领域取得了令人印象深刻的结果，但如何最好地将它们与交互式视觉环境连接起来仍然是一个持续的问题。在这项工作中，我们提出了VELMA，一种使用轨迹和视觉环境观察的口头化作为下一步操作的上下文提示的LLM智能体。视觉信息通过一个流程进行口头化，该流程从人类编写的导航指令中提取地标，并使用CLIP来确定它们在当前全景视图中的可见性。

    Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that
    
[^7]: 解读疾病进展聚类中的深度嵌入

    Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v1 [stat.ML])

    [http://arxiv.org/abs/2307.06060](http://arxiv.org/abs/2307.06060)

    本文提出了一种在疾病进展聚类中解读深度嵌入的新方法，并通过评估2型糖尿病参与者数据集展示了对疾病进展模式的临床意义性见解。

    

    我们提出了一种在患者聚类的背景下解读深度嵌入的新方法。我们在来自英国生物库的2型糖尿病参与者数据集上评估我们的方法，并展示出对疾病进展模式的临床意义性见解。

    We propose a novel approach for interpreting deep embeddings in the context of patient clustering. We evaluate our approach on a dataset of participants with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful insights into disease progression patterns.
    
[^8]: 对蒙古文通用语料库合适规模的研究

    A Study on the Appropriate size of the Mongolian general corpus. (arXiv:2307.06050v1 [cs.CL])

    [http://arxiv.org/abs/2307.06050](http://arxiv.org/abs/2307.06050)

    本研究通过使用 Heaps 函数和类型标记比来确定蒙古文通用语料库的合适规模，观察结果表明标记数量超过3900万至4200万时，类型标记比值几乎不变。

    

    本研究旨在确定蒙古文通用语料库的合适规模。研究使用了 Heaps 函数和类型标记比来确定蒙古文通用语料库的合适规模。样本语料库包含来自10个领域的文本，包括报纸政治、经济、社会、文化、体育、国际文章和法律、中高学校文学教材、访谈文章和播客转录。首先，我们利用样本语料库估计了 Heaps 函数。接下来，我们观察了通过使用估计的 Heaps 函数增加一百万个标记的方式，类型数量和类型标记比值的变化。观察结果发现，当标记数量超过3900万至4200万时，类型标记比值几乎未发生变化。因此，我们得出结论，蒙古文通用语料库的合适规模为3900万至4200万个标记。

    This study aims to determine the appropriate size of the Mongolian general corpus. This study used the Heaps function and Type Token Ratio to determine the appropriate size of the Mongolian general corpus. The sample corpus of 906,064 tokens comprised texts from 10 domains of newspaper politics, economy, society, culture, sports, world articles and laws, middle and high school literature textbooks, interview articles, and podcast transcripts. First, we estimated the Heaps function with this sample corpus. Next, we observed changes in the number of types and TTR values while increasing the number of tokens by one million using the estimated Heaps function. As a result of observation, we found that the TTR value hardly changed when the number of tokens exceeded from 39 to 42 million. Thus, we conclude that an appropriate size for a Mongolian general corpus is from 39 to 42 million tokens.
    
[^9]: 通过记忆增强的适配器实现可插拔的神经机器翻译模型

    Pluggable Neural Machine Translation Models via Memory-augmented Adapters. (arXiv:2307.06029v1 [cs.CL])

    [http://arxiv.org/abs/2307.06029](http://arxiv.org/abs/2307.06029)

    通过记忆增强的适配器，我们提出了一种可插拔的方法来控制神经机器翻译模型的生成行为。实验证明，我们的方法可以胜过几个代表性的可插拔基准模型。

    

    尽管神经机器翻译（NMT）模型在普通领域表现出色，但是控制其生成行为以满足不同用户需求仍然具有一定挑战性。鉴于每个用户需求都需要从头开始学习新模型的高昂训练成本和数据稀缺的挑战，我们提出了一种记忆增强适配器，以可插拔的方式引导预训练的NMT模型。具体而言，我们基于用户提供的文本样本构建了一个多粒度记忆，并提出了一种新的适配器架构来结合模型表示和检索结果。同时，我们提出了一种使用记忆丢弃的训练策略，以减少NMT模型和记忆之间的虚假依赖关系。我们在风格和领域特定实验中验证了我们的方法，结果表明，我们的方法可以胜过几个代表性的可插拔基准模型。

    Although neural machine translation (NMT) models perform well in the general domain, it remains rather challenging to control their generation behavior to satisfy the requirement of different users. Given the expensive training cost and the data scarcity challenge of learning a new model from scratch for each user requirement, we propose a memory-augmented adapter to steer pretrained NMT models in a pluggable manner. Specifically, we construct a multi-granular memory based on the user-provided text samples and propose a new adapter architecture to combine the model representations and the retrieved results. We also propose a training strategy using memory dropout to reduce spurious dependencies between the NMT model and the memory. We validate our approach on both style- and domain-specific experiments and the results indicate that our method can outperform several representative pluggable baselines.
    
[^10]: PolyLM:一个开源的多语言大语言模型

    PolyLM: An Open Source Polyglot Large Language Model. (arXiv:2307.06018v1 [cs.CL])

    [http://arxiv.org/abs/2307.06018](http://arxiv.org/abs/2307.06018)

    PolyLM是一个开源的多语言大语言模型，通过整合双语数据和采用课程学习策略进行训练，提高了多语言能力，并且使用了多语言自学习方法进行模型微调。模型性能得到了多个现有多语言任务的确认。

    

    大语言模型（LLMs）展现了出色的能力，能够理解、推理和生成自然语言指令。然而，LLMs的开发主要集中在高资源语言，如英语，限制了它们在其他语言中的适用性和研究。因此，我们提出了PolyLM，一个训练了6400亿个标记的多语言LLM，有两种模型大小：1.7B和13B。为了增强其多语言能力，我们1）将双语数据整合到训练数据中；2）采用课程学习策略，在预训练的第一阶段将非英语数据的比例从30%增加到最后阶段的60%。此外，我们提出了一种多语言自学习方法，自动为模型的微调生成了132.7K个多语言指令。为了评估模型的性能，我们收集了一些现有的多语言任务，包括多语言理解、问题回答、生成等。

    Large language models (LLMs) demonstrate remarkable ability to comprehend, reason, and generate following nature language instructions. However, the development of LLMs has been primarily focused on high-resource languages, such as English, thereby limiting their applicability and research in other languages. Consequently, we present PolyLM, a multilingual LLM trained on 640 billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, we 1) integrate bilingual data into training data; and 2) adopt a curriculum learning strategy that increases the proportion of non-English data from 30% in the first stage to 60% in the final stage during pre-training. Further, we propose a multilingual self-instruct method which automatically generates 132.7K diverse multilingual instructions for model fine-tuning. To assess the model's performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, gener
    
[^11]: DDNAS: 离散化可微分神经架构搜索用于文本分类

    DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification. (arXiv:2307.06005v1 [cs.CL])

    [http://arxiv.org/abs/2307.06005](http://arxiv.org/abs/2307.06005)

    这篇论文提出了一种名为DDNAS的离散化可微分神经架构搜索方法，用于文本分类。通过使用连续松弛的架构表示和互信息最大化的离散化层，DDNAS在文本表示学习和分类任务中表现优于其他NAS方法。

    

    神经架构搜索（NAS）在学习文本表示方面展现出了很好的能力。然而，现有的基于文本的NAS既未对架构进行可学习的融合以优化，也未对文本输入背后的潜在层级分类进行编码。本文提出了一种新颖的NAS方法，即Discretized Differentiable Neural Architecture Search (DDNAS)，用于文本表示学习和分类。通过架构表示的连续松弛，DDNAS可以使用梯度下降来进行搜索优化。我们还提出了一种新颖的离散化层，通过最大化互信息将其施加于每个搜索节点上，以对文本表示中的潜在层级分类进行建模。在八个不同的真实数据集上进行的大量实验表明，DDNAS始终能够优于最先进的NAS方法。尽管DDNAS仅依赖于卷积，池化和无操作这三个基本操作，作为候选操作。

    Neural Architecture Search (NAS) has shown promising capability in learning text representation. However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture, nor encodes the latent hierarchical categorization behind text input. This paper presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification. With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search. We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation. Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods. While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the cand
    
[^12]: VoxPoser: 用于带有语言模型的机器人操作的可组合的3D价值映射

    VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])

    [http://arxiv.org/abs/2307.05973](http://arxiv.org/abs/2307.05973)

    VoxPoser提出了一种新方法，通过组合3D价值映射和语言模型，实现了机器人在多种操作任务下根据自由形式的指令和对象合成机器人轨迹的能力。

    

    研究表明，大型语言模型（LLMs）具有丰富的可行动知识，可以以推理和规划的形式提取出用于机器人操作的信息。尽管取得了进展，大多数模型仍然依赖于预定义的运动原语来执行与环境的物理交互，这仍然是一个重大瓶颈。在这项工作中，我们的目标是在给定开集指令和开集对象的情况下，为各种操作任务合成机器人轨迹，即一系列密集的6-DoF末端执行器路径点。我们首先观察到LLMs在给定自由形式的语言指令时擅长推断可行性和约束。更重要的是，通过利用它们的代码编写能力，它们可以与视觉-语言模型（VLM）交互，以组合3D价值映射将知识接地到Agent的观测空间中。然后在基于模型的规划框架中使用组合的价值映射来零试合成闭环轨迹。

    Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop ro
    
[^13]: 自学习量化：在基于Transformer的语言模型中实现高压缩率

    Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models. (arXiv:2307.05972v1 [cs.CL])

    [http://arxiv.org/abs/2307.05972](http://arxiv.org/abs/2307.05972)

    本研究探讨了事后训练量化和量化感知训练对Transformer语言模型泛化效果的影响，并提出了自学习量化（SDQ）方法，该方法最小化累积量化误差并在多语言模型上优于基准模型。通过在XGLUE基准上的实验证明，SDQ方法可以将模型从32位浮点权重减少到8位整数权重，同时保持高水平的性能。

    

    我们研究了事后训练量化和量化感知训练对Transformer语言模型的泛化效果。我们提出了一种称为自学习量化（SDQ）的新方法，该方法最小化累积量化误差并优于基准模型。我们将SDQ应用于多语言模型XLM-R-Base和InfoXLM-Base，并证明这两个模型可以从32位浮点权重减少到8位整数权重，同时在XGLUE基准上保持高水平的性能。我们的结果还凸显了量化多语言模型面临的挑战，这些模型必须能够推广到未经微调的语言。

    We investigate the effects of post-training quantization and quantization-aware training on the generalization of Transformer language models. We present a new method called self-distilled quantization (SDQ) that minimizes accumulative quantization errors and outperforms baselines. We apply SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that both models can be reduced from 32-bit floating point weights to 8-bit integer weights while maintaining a high level of performance on the XGLUE benchmark. Our results also highlight the challenges of quantizing multilingual models, which must generalize to languages they were not fine-tuned on.
    
[^14]: 对于多模态语言理解的原型对比迁移学习

    Prototypical Contrastive Transfer Learning for Multimodal Language Understanding. (arXiv:2307.05942v1 [cs.RO])

    [http://arxiv.org/abs/2307.05942](http://arxiv.org/abs/2307.05942)

    提出了一种用于多模态语言理解的原型对比迁移学习方法PCTL，通过引入双重原型例外损失函数，在家庭环境中根据自由形式的自然语言指令识别目标物体的任务中取得了优于现有方法的性能。

    

    尽管国内服务机器人被期望能够帮助需要支持的个人，但他们目前无法通过自然语言与人们平滑地进行交互。例如，对于指令“从厨房给我拿一个瓶子”，这些机器人很难在室内环境中明确指定瓶子。大多数传统模型都是通过耗费大量人力来收集真实世界数据进行训练的，并且它们尚未充分利用通过迁移学习框架的模拟数据。在本研究中，我们提出了一种新颖的多模态语言理解的迁移学习方法，称为原型对比迁移学习（PCTL），它使用了一种新的对比损失函数——双重原型例外。我们将PCTL引入到根据自由形式的自然语言指令识别家庭环境中的目标物体的任务中。为了验证PCTL，我们构建了新的真实世界和模拟数据集。我们的实验证明了PCTL的性能优于现有方法。

    Although domestic service robots are expected to assist individuals who require support, they cannot currently interact smoothly with people through natural language. For example, given the instruction "Bring me a bottle from the kitchen," it is difficult for such robots to specify the bottle in an indoor environment. Most conventional models have been trained on real-world datasets that are labor-intensive to collect, and they have not fully leveraged simulation data through a transfer learning framework. In this study, we propose a novel transfer learning approach for multimodal language understanding called Prototypical Contrastive Transfer Learning (PCTL), which uses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task of identifying target objects in domestic environments according to free-form natural language instructions. To validate PCTL, we built new real-world and simulation datasets. Our experiment demonstrated that PCTL outperformed existing methods. 
    
[^15]: 预测性流水线解码：准确LLM解码中的计算延迟权衡

    Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding. (arXiv:2307.05908v1 [cs.CL])

    [http://arxiv.org/abs/2307.05908](http://arxiv.org/abs/2307.05908)

    本论文提出了一种预测性流水线解码（PPD）方法，通过并行启动后续令牌解码来加速大型语言模型（LLMs）中的贪婪解码过程，同时保持完全相同的输出。该方法在减少解码延迟方面具有潜力，提供了新的LLM解码策略权衡理解。

    

    本论文提出了一种名为"预测性流水线解码（PPD）"的方法，该方法可以加速大型语言模型（LLMs）中的贪婪解码，同时保持与原始解码完全相同的输出。与传统策略不同，PPD利用额外的计算资源在当前令牌解码期间并行启动后续令牌解码。这种创新方法减少了解码延迟，并重新塑造了LLM解码策略中的权衡理解。我们开发了一个理论框架，可以分析计算和延迟之间的权衡关系。使用这个框架，我们可以通过评估匹配率（表示为p_correct）来对我们提出的方法可能的延迟减少进行分析估计。结果表明，使用额外的计算资源有潜力加速LLM的贪婪解码过程。

    This paper presents "Predictive Pipelined Decoding (PPD)," an approach that speeds up greedy decoding in Large Language Models (LLMs) while maintaining the exact same output as the original decoding. Unlike conventional strategies, PPD employs additional compute resources to parallelize the initiation of subsequent token decoding during the current token decoding. This innovative method reduces decoding latency and reshapes the understanding of trade-offs in LLM decoding strategies. We have developed a theoretical framework that allows us to analyze the trade-off between computation and latency. Using this framework, we can analytically estimate the potential reduction in latency associated with our proposed method, achieved through the assessment of the match rate, represented as p_correct. The results demonstrate that the use of extra computational resources has the potential to accelerate LLM greedy decoding.
    
[^16]: 使用卷积和记忆网络在维基百科表格上进行关系抽取

    Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks. (arXiv:2307.05827v1 [cs.CL])

    [http://arxiv.org/abs/2307.05827](http://arxiv.org/abs/2307.05827)

    使用卷积和记忆网络，在维基百科的表格数据中进行关系抽取。该模型在关系抽取任务中表现出色，并且经过全面的分析和研究，展示了各个模型组件的贡献。

    

    关系抽取是从文本中提取实体之间关系的任务。大部分关系抽取方法从自由格式的连续文本中提取关系，而忽略了其他丰富的数据来源，比如表格。我们从应用神经网络方法处理表格化数据的角度探索关系抽取。我们引入了一个新模型，由卷积神经网络（CNN）和双向长短期记忆（BiLSTM）网络组成，分别用于编码实体和学习它们之间的依赖关系。我们在一个大规模且最新的数据集上评估了我们的模型，并与之前的神经网络方法进行了比较。实验结果显示，我们的模型在表格数据上的关系抽取任务中始终优于之前的模型。我们进行了全面的错误分析和剥离研究，以展示我们的模型的各个组成部分的贡献。最后，我们讨论了我们方法的实用性和权衡，并提供了进一步研究的建议。

    Relation extraction (RE) is the task of extracting relations between entities in text. Most RE methods extract relations from free-form running text and leave out other rich data sources, such as tables. We explore RE from the perspective of applying neural methods on tabularly organized data. We introduce a new model consisting of Convolutional Neural Network (CNN) and Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and learn dependencies among them, respectively. We evaluate our model on a large and recent dataset and compare results with previous neural methods. Experimental results show that our model consistently outperforms the previous model for the task of relation extraction on tabular data. We perform comprehensive error analyses and ablation study to show the contribution of various components of our model. Finally, we discuss the usefulness and trade-offs of our approach, and provide suggestions for fostering further research.
    
[^17]: 使用数据增强技术改进临床语音的词性标注问题

    Improved POS tagging for spontaneous, clinical speech using data augmentation. (arXiv:2307.05796v1 [cs.CL])

    [http://arxiv.org/abs/2307.05796](http://arxiv.org/abs/2307.05796)

    本文提出了一种使用数据增强技术改进临床语音的词性标注问题的方法，通过在新闻树库上进行训练，使解析器更适应自然、即兴的语音，测试结果表明其性能良好。

    

    本文解决了改进临床人群语音转录的词性标注问题。与之前关于转录语音的分析和词性标注工作不同，我们不使用一个针对该领域的树库进行训练。相反，我们使用数据增强技术在一个新闻树库中进行训练，使这些结构更接近自然、即兴的语音。我们训练了一个带有增强数据和没有增强数据的解析器，并使用手动验证的临床语音词性标签测试了其性能，这些语音来自患有不同类型神经退行性疾病的患者。

    This paper addresses the problem of improving POS tagging of transcripts of speech from clinical populations. In contrast to prior work on parsing and POS tagging of transcribed speech, we do not make use of an in domain treebank for training. Instead, we train on an out of domain treebank of newswire using data augmentation techniques to make these structures resemble natural, spontaneous speech. We trained a parser with and without the augmented data and tested its performance using manually validated POS tags in clinical speech produced by patients with various types of neurodegenerative conditions.
    
[^18]: 大型语言模型

    Large Language Models. (arXiv:2307.05782v1 [cs.CL])

    [http://arxiv.org/abs/2307.05782](http://arxiv.org/abs/2307.05782)

    这篇论文介绍了大型语言模型的发展历史和现状，详细描述了底层的Transformer架构，并探讨了如何训练模型来实现多种智能任务。

    

    人工智能正在取得惊人的进展，其中一个最好的例子是大型语言模型（LLMs）的发展，如OpenAI的GPT系列。在这些针对具有数学或物理背景的读者编写的讲座中，我们简要介绍了LLMs的历史和现状，并详细描述了底层的Transformer架构。然后，我们探讨了一些关于LLMs工作原理的当前观点，以及训练用于预测文本中下一个单词的模型如何能够执行显示智能的其他任务。

    Artificial intelligence is making spectacular progress, and one of the best examples is the development of large language models (LLMs) such as OpenAI's GPT series. In these lectures, written for readers with a background in mathematics or physics, we give a brief history and survey of the state of the art, and describe the underlying transformer architecture in detail. We then explore some current ideas on how LLMs work and how models trained to predict the next word in a text are able to perform other tasks displaying intelligence.
    
[^19]: 使用ChatGPT进行神经机器翻译数据生成和增强

    Neural Machine Translation Data Generation and Augmentation using ChatGPT. (arXiv:2307.05779v1 [cs.CL])

    [http://arxiv.org/abs/2307.05779](http://arxiv.org/abs/2307.05779)

    使用ChatGPT生成和增强神经机器翻译数据，尽管缺乏多样性，幻象数据在翻译信号方面改善了结果。

    

    神经模型已经在机器翻译领域取得了革命性的进展，但创建平行语料库耗时耗力。我们研究了一种替代手工平行语料库的方法-使用生成语言模型创建幻象平行语料库。尽管这些模型本身是基于平行数据训练的，但它们可以利用多语言向量空间来创建数据，并且可以补充小规模手工采集的语料库。我们的实验证明了两个关键发现-尽管其输出缺乏多样性，但幻象数据改善了翻译信号，即使与原始数据集的领域不匹配。

    Neural models have revolutionized the field of machine translation, but creating parallel corpora is expensive and time-consuming. We investigate an alternative to manual parallel corpora - hallucinated parallel corpora created by generative language models. Although these models are themselves trained on parallel data, they can leverage a multilingual vector space to create data, and may be able to supplement small manually-procured corpora. Our experiments highlight two key findings - despite a lack of diversity in their output, the hallucinated data improves the translation signal, even when the domain clashes with the original dataset.
    
[^20]: 迈向强大和高效的持续语言学习

    Towards Robust and Efficient Continual Language Learning. (arXiv:2307.05741v1 [cs.CL])

    [http://arxiv.org/abs/2307.05741](http://arxiv.org/abs/2307.05741)

    本文研究了如何实现持续的语言学习，并构建了一个任务序列基准用于评估。最终提出了一个简单但有效的学习算法，以最大程度地利用具有积极传递潜力的任务，并避免负迁移的影响。

    

    随着语言模型应用领域的不断发展，一个自然的问题是如何快速适应模型到新的任务。我们从持续学习的角度来研究这个经典问题，我们的目标是在新的任务上继续微调过去任务上训练的模型，以"传输"相关知识。然而，这种策略也存在更多害处的风险，即负迁移。在本文中，我们构建了一个新的任务序列基准，针对可能面临的不同传递场景，比如具有积极传递潜力的任务序列、具有负迁移潜力的任务序列、没有预期效果的任务序列或者混合的任务序列。理想的学习者应该能够充分利用所有具有积极传递潜力的任务的信息，同时避免任何可能混淆它的干扰性任务的负面影响。然后，我们提出了一个简单但有效的学习者，满足这些要求。

    As the application space of language models continues to evolve, a natural question to ask is how we can quickly adapt models to new tasks. We approach this classic question from a continual learning perspective, in which we aim to continue fine-tuning models trained on past tasks on new tasks, with the goal of "transferring" relevant knowledge. However, this strategy also runs the risk of doing more harm than good, i.e., negative transfer. In this paper, we construct a new benchmark of task sequences that target different possible transfer scenarios one might face, such as a sequence of tasks with high potential of positive transfer, high potential for negative transfer, no expected effect, or a mixture of each. An ideal learner should be able to maximally exploit information from all tasks that have any potential for positive transfer, while also avoiding the negative effects of any distracting tasks that may confuse it. We then propose a simple, yet effective, learner that satisfies
    
[^21]: 探索大规模语言模型在在线职位推荐中对图数据的理解

    Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v1 [cs.AI])

    [http://arxiv.org/abs/2307.05722](http://arxiv.org/abs/2307.05722)

    本论文探索了大规模语言模型在在线职位推荐中对图数据的理解能力，并提出了新的框架来分析行为图，发现其中的潜在模式和关系。

    

    大规模语言模型（LLMs）在各个领域展示了其出色的能力，彻底改变了自然语言处理任务。然而，它们在职位推荐中对行为图的理解潜力仍然未被充分探索。本文旨在揭示大规模语言模型在理解行为图方面的能力，并利用这种理解来提升在线招聘中的推荐，包括促进非分布式的应用。我们提出了一个新的框架，利用大规模语言模型提供的丰富上下文信息和语义表示来分析行为图并揭示其中的潜在模式和关系。具体而言，我们提出了一个元路径提示构造器，利用LLM推荐器首次理解行为图，并设计了相应的路径增强模块来缓解基于路径的序列输入引入的提示偏差。通过利用将LM的特点引入到行为图的大规模数据分析中，我们取得了显著的实验结果，证明了我们提出的方法的有效性和性能。

    Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for behavior graph understanding in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including the promotion of out-of-distribution (OOD) application. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that leverages LLM recommender to understand behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By leveragin
    
[^22]: 个性化强化学习总结服务：从非结构化数据中学习结构

    A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data. (arXiv:2307.05696v1 [cs.IR])

    [http://arxiv.org/abs/2307.05696](http://arxiv.org/abs/2307.05696)

    该论文提出了一种个性化强化学习总结服务，通过使用层级个性化基于概念的总结方法，在文本数据呈指数级增长的背景下，帮助用户提取有意义的见解。

    

    文本数据呈指数级增长，需要工具来帮助用户提取有意义的见解。传统的文档摘要方法通常无法满足个人用户需求，并且缺乏高效信息处理的结构。为解决这些问题，我们提出了一种层级个性化基于概念的总结方法。该方法将文档综合成简洁的层级概念图，并通过学习和适应用户偏好来积极参与用户。使用强化学习算法，该方法为特定主题的未见文档生成个性化摘要。该框架提高了理解能力，实现了有效的导航，并使用户能够根据自己独特的需求从大量文档集合中提取有意义的见解。

    The exponential growth of textual data has created a crucial need for tools that assist users in extracting meaningful insights. Traditional document summarization approaches often fail to meet individual user requirements and lack structure for efficient information processing. To address these limitations, we propose Summation, a hierarchical personalized concept-based summarization approach. It synthesizes documents into a concise hierarchical concept map and actively engages users by learning and adapting to their preferences. Using a Reinforcement Learning algorithm, Summation generates personalized summaries for unseen documents on specific topics. This framework enhances comprehension, enables effective navigation, and empowers users to extract meaningful insights from large document collections aligned with their unique requirements.
    
[^23]: 以不同方式堆叠更多层：通过低秩更新进行高秩训练

    Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])

    [http://arxiv.org/abs/2307.05695](http://arxiv.org/abs/2307.05695)

    本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。

    

    尽管大规模网络拥有数百亿个参数的规模已经占主导地位并且效果显著，但对于过度参数化模型的训练必要性仍然缺乏清晰的理解，而替代方法不一定能够降低训练高性能模型的成本。本文探索了低秩训练技术作为训练大型神经网络的替代方法。我们引入了一种称为ReLoRA的新方法，它利用低秩更新来训练高秩网络。我们将ReLoRA应用于预训练的Transformer语言模型，参数量高达350M，并且证明了与常规神经网络训练相当的性能。此外，我们观察到ReLoRA的效率随着模型大小的增加而提高，这使得它成为高效训练千亿级参数网络的有希望的方法。我们的研究结果揭示了低秩训练技术的潜力及其对于缩放定律的影响。

    Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
    
[^24]: 通过微调语言模型改进方面级情感分类中的共指消解处理

    Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models. (arXiv:2307.05646v1 [cs.CL])

    [http://arxiv.org/abs/2307.05646](http://arxiv.org/abs/2307.05646)

    本研究提出了一个框架，通过在高推断任务上进行微调，来改善方面级情感分类中共指消解的处理。研究表明，该框架可以显著提高模型的共指消解能力，从而改善在具有共指消解的评论中的性能。

    

    对于公司来说，客户反馈是非常宝贵的，因为它们可以优化产品。通过方面级情感分类(ALSC)可以自动监测客户反馈，从而分析评论中的具体方面。大型语言模型(LLMs)是许多最先进的ALSC解决方案的核心，但在某些需要共指消解(CR)的场景中表现较差。在本研究中，我们提出了一个框架，通过在高推断任务上进行微调，来改善LLM在具有CR的评论中的性能。我们表明，性能的改进很可能归因于改善了模型的CR能力。我们还发布了一个关于ALSC中CR的新数据集。

    Customer feedback is invaluable to companies as they refine their products. Monitoring customer feedback can be automated with Aspect Level Sentiment Classification (ALSC) which allows us to analyse specific aspects of the products in reviews. Large Language Models (LLMs) are the heart of many state-of-the-art ALSC solutions, but they perform poorly in some scenarios requiring Coreference Resolution (CR). In this work, we propose a framework to improve an LLM's performance on CR-containing reviews by fine tuning on highly inferential tasks. We show that the performance improvement is likely attributed to the improved model CR ability. We also release a new dataset that focuses on CR in ALSC.
    
[^25]: 基于Transformer的补丁细化模型，用于知识图谱补全

    Separate-and-Aggregate: A Transformer-based Patch Refinement Model for Knowledge Graph Completion. (arXiv:2307.05627v1 [cs.CL])

    [http://arxiv.org/abs/2307.05627](http://arxiv.org/abs/2307.05627)

    本文提出了一种基于Transformer的补丁细化模型（PatReFormer）用于知识图谱补全。该模型通过分割嵌入并使用交叉注意力模块来改进实体和关系之间的嵌入特征交互，从而提高了模型性能。

    

    知识图谱补全是从给定的知识图谱中推断缺失事实的任务。先前的方法通常将知识图谱实体和关系表示为可训练的连续嵌入，并将实体$h$（或$t$）和关系$r$的嵌入融合为查询的隐藏表示$(h, r, ?)$（或$(?, r, t$)）以近似缺失实体。为了实现这个目标，他们要么使用浅层线性变换，要么使用深度卷积模块。然而，线性变换存在表达能力问题，而深度卷积模块引入了不必要的归纳偏差，可能降低模型性能。因此，我们提出了一种新颖的基于Transformer的补丁细化模型（PatReFormer）用于知识图谱补全。PatReFormer首先将嵌入分割成一系列补丁，然后使用交叉注意力模块允许实体和关系之间的双向嵌入特征交互，从而获得更好的性能。

    Knowledge graph completion (KGC) is the task of inferencing missing facts from any given knowledge graphs (KG). Previous KGC methods typically represent knowledge graph entities and relations as trainable continuous embeddings and fuse the embeddings of the entity $h$ (or $t$) and relation $r$ into hidden representations of query $(h, r, ?)$ (or $(?, r, t$)) to approximate the missing entities. To achieve this, they either use shallow linear transformations or deep convolutional modules. However, the linear transformations suffer from the expressiveness issue while the deep convolutional modules introduce unnecessary inductive bias, which could potentially degrade the model performance. Thus, we propose a novel Transformer-based Patch Refinement Model (PatReFormer) for KGC. PatReFormer first segments the embedding into a sequence of patches and then employs cross-attention modules to allow bi-directional embedding feature interaction between the entities and relations, leading to a bet
    
[^26]: SITTA: 一种用于图像描述的语义图像文本对齐方法

    SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])

    [http://arxiv.org/abs/2307.05591](http://arxiv.org/abs/2307.05591)

    SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。

    

    对图像的文本和语义理解对于生成适当的描述非常重要。这需要检测图像中的对象，建模它们之间的关系，评估场景的语义，并将提取的知识表示在语言空间中。为了在保证良好的图像-语言映射的同时实现丰富的语言能力，预训练的语言模型（LMs）被条件化为预训练的多模态（图像-文本）模型，允许使用图像输入。这要求将多模态模型的视觉编码器中检测到的语义与生成性LM的语言表示进行对齐。然而，如何最好地将视觉编码器检测到的语义传递给LM还不清楚。我们介绍了两种构建线性映射的新方法，成功地将两个预训练模型的嵌入空间之间的语义转移。第一种方法是将多模态语言编码器的嵌入空间与生成性LM的嵌入空间进行对齐。

    Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
    
[^27]: 恶意言论通过双对比学习进行检测

    Hate Speech Detection via Dual Contrastive Learning. (arXiv:2307.05578v1 [cs.CL])

    [http://arxiv.org/abs/2307.05578](http://arxiv.org/abs/2307.05578)

    该论文提出了一种用于恶意言论检测的新颖框架，通过应对恶意言论中的复杂语义信息和恶意言辞的干扰以及恶意言论和非恶意言论的不平衡分布等挑战，实现了跨度级信息的捕捉。

    

    恶意言论在社交媒体上的快速传播影响着互联网环境和我们的社会，增加了偏见并伤害了人们。检测恶意言论在自然语言处理领域引起了广泛关注。尽管最近的研究已经解决了恶意言论检测中的一些问题，但这个任务仍然面临着两个固有的未解决挑战。第一个挑战在于恶意言论中传达的复杂语义信息，特别是恶意言论检测中侮辱性言辞的干扰。第二个挑战是恶意言论和非恶意言论的不平衡分布，可能会严重损害模型的性能。为了解决这些挑战，我们提出了一种新颖的恶意言论检测双对比学习（DCL）框架。我们的框架通过联合优化自监督对比学习损失和有监督对比学习损失，捕捉超出现有模型中使用的基于令牌级情感语义的跨度级信息。

    The fast spread of hate speech on social media impacts the Internet environment and our society by increasing prejudice and hurting people. Detecting hate speech has aroused broad attention in the field of natural language processing. Although hate speech detection has been addressed in recent work, this task still faces two inherent unsolved challenges. The first challenge lies in the complex semantic information conveyed in hate speech, particularly the interference of insulting words in hate speech detection. The second challenge is the imbalanced distribution of hate speech and non-hate speech, which may significantly deteriorate the performance of models. To tackle these challenges, we propose a novel dual contrastive learning (DCL) framework for hate speech detection. Our framework jointly optimizes the self-supervised and the supervised contrastive learning loss for capturing span-level information beyond the token-level emotional semantics used in existing models, particularly 
    
[^28]: 事件提取作为问题生成和回答的方法

    Event Extraction as Question Generation and Answering. (arXiv:2307.05567v1 [cs.CL])

    [http://arxiv.org/abs/2307.05567](http://arxiv.org/abs/2307.05567)

    本文提出了一种名为QGA-EE的方法，该方法通过问题生成和回答的方式进行事件提取，解决了传统方法中的错误传播问题，并且在预测事件参数上取得了更好的表现。

    

    近期对于事件提取的研究将任务重新定义为问题回答 (QA)，并取得了有望的结果。这种方法的优势在于它通过直接预测事件参数而不是先提取候选项，从而解决了传统基于标记的分类方法中的错误传播问题。然而，这些问题通常基于固定模板，并且很少利用相关参数等语境信息。此外，现有的QA方法在处理同一角色有多个参数的情况下存在困难。在本文中，我们提出了一种名为QGA-EE的方法，它使问题生成模型能够生成包含丰富上下文信息而不是使用固定模板的问题。我们还提出了动态模板来辅助问题生成模型的训练。实验证明，QGA-EE在ACE05英文数据集上优于所有之前单任务模型。

    Recent work on Event Extraction has reframed the task as Question Answering (QA), with promising results. The advantage of this approach is that it addresses the error propagation issue found in traditional token-based classification approaches by directly predicting event arguments without extracting candidates first. However, the questions are typically based on fixed templates and they rarely leverage contextual information such as relevant arguments. In addition, prior QA-based approaches have difficulty handling cases where there are multiple arguments for the same role. In this paper, we propose QGA-EE, which enables a Question Generation (QG) model to generate questions that incorporate rich contextual information instead of using fixed templates. We also propose dynamic templates to assist the training of QG model. Experiments show that QGA-EE outperforms all prior single-task-based models on the ACE05 English dataset.
    
[^29]: SemEval-2023任务1的增强器: 通过提示增强和文本到图像扩散改进CLIP处理复合性和歧义性，以实现零样本视觉VWSD

    Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion. (arXiv:2307.05564v1 [cs.CL])

    [http://arxiv.org/abs/2307.05564](http://arxiv.org/abs/2307.05564)

    本文描述了增强CLIP在处理复合性和歧义性方面，在零样本视觉词义消歧任务中的应用。作者采用Augment-CLIP和Stable Diffusion Sampling（SD Sampling）两个系统来解决CLIP的局限性，通过生成包含上下文短语的句子和使用稳定扩散算法生成多个图像，提高了任务的表现。

    

    本文描述了我们针对英语视觉词义消歧（VWSD）任务的零样本方法。我们的初步研究表明，使用CLIP将候选图像与短语进行匹配的简单方法受到图像-文本对中多对多性质的影响。我们发现，CLIP文本编码器在捕捉自然语言的复合性方面能力有限。相反，短语的描述性焦点因实例而异。我们在两个系统中解决了这些问题，Augment-CLIP和Stable Diffusion Sampling（SD Sampling）。Augment-CLIP通过利用大型语言模型（LLMs）生成包含上下文短语的句子来增强文本提示。我们进一步探索了其他语言的CLIP模型，因为一个有歧义的词可能会在另一种语言中被翻译为一个无歧义的词。SD Sampling使用文本到图像的稳定扩散来从给定的短语生成多个图像，增加子集的可能性。

    This paper describes our zero-shot approaches for the Visual Word Sense Disambiguation (VWSD) Task in English. Our preliminary study shows that the simple approach of matching candidate images with the phrase using CLIP suffers from the many-to-many nature of image-text pairs. We find that the CLIP text encoder may have limited abilities in capturing the compositionality in natural language. Conversely, the descriptive focus of the phrase varies from instance to instance. We address these issues in our two systems, Augment-CLIP and Stable Diffusion Sampling (SD Sampling). Augment-CLIP augments the text prompt by generating sentences that contain the context phrase with the help of large language models (LLMs). We further explore CLIP models in other languages, as the an ambiguous word may be translated into an unambiguous one in the other language. SD Sampling uses text-to-image Stable Diffusion to generate multiple images from the given phrase, increasing the likelihood that a subset 
    
[^30]: 在智利公共医疗系统中设计和部署一个用于规范转诊的全国范围内自动编码系统

    Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System. (arXiv:2307.05560v1 [cs.CL])

    [http://arxiv.org/abs/2307.05560](http://arxiv.org/abs/2307.05560)

    该论文描述了在智利公共医疗系统中设计和部署的一个用于自动编码转诊疾病的系统。该系统使用最先进的NER模型识别疾病提及，并基于Elasticsearch的搜索引擎系统进行编码。

    

    疾病编码任务涉及为临床文档中提到的每种疾病分配一个受控词汇中的唯一标识符。这个任务很重要，因为它允许从非结构化数据中提取信息，例如在特定背景下进行流行病学研究。然而，手动编码过程容易出错，要求医务人员精通编码规则和术语。此外，这个过程耗费大量时间和精力，这些资源可以用于更具临床意义的任务。这些困难可以通过开发能够自动为疾病分配代码的计算系统来解决。因此，我们提出了一个两步法的系统，用于自动编码智利公共医疗系统中的转诊疾病。具体而言，我们的模型使用了最先进的NER模型来识别疾病提及，并基于Elasticsearch的搜索引擎系统来进行编码。

    The disease coding task involves assigning a unique identifier from a controlled vocabulary to each disease mentioned in a clinical document. This task is relevant since it allows information extraction from unstructured data to perform, for example, epidemiological studies about the incidence and prevalence of diseases in a determined context. However, the manual coding process is subject to errors as it requires medical personnel to be competent in coding rules and terminology. In addition, this process consumes a lot of time and energy, which could be allocated to more clinically relevant tasks. These difficulties can be addressed by developing computational systems that automatically assign codes to diseases. In this way, we propose a two-step system for automatically coding diseases in referrals from the Chilean public healthcare system. Specifically, our model uses a state-of-the-art NER model for recognizing disease mentions and a search engine system based on Elasticsearch for 
    
[^31]: 自动化论文评分中的反馈综述

    Review of feedback in Automated Essay Scoring. (arXiv:2307.05553v1 [cs.CL])

    [http://arxiv.org/abs/2307.05553](http://arxiv.org/abs/2307.05553)

    这篇论文综述了自动化论文评分中的反馈研究，包括不同类型的反馈和论文特征，并回顾了提供反馈的最新案例研究。

    

    第一个自动化论文评分系统诞生于50年前。自动化论文评分系统正在发展成为比以前简单评分系统更加功能丰富的系统。它的目的不仅仅是评分，还作为一个学习工具来提高用户的写作能力。反馈是使自动化论文评分系统在实际生活中有用的最重要方面。在第一个自动化论文评分系统中已经强调了反馈的重要性。本文综述了关于自动化论文评分的反馈研究，包括不同类型的反馈和论文特征。我们还回顾了提供反馈的最新案例研究。

    The first automated essay scoring system was developed 50 years ago. Automated essay scoring systems are developing into systems with richer functions than the previous simple scoring systems. Its purpose is not only to score essays but also as a learning tool to improve the writing skill of users. Feedback is the most important aspect of making an automated essay scoring system useful in real life. The importance of feedback was already emphasized in the first AES system. This paper reviews research on feedback including different feedback types and essay traits on automated essay scoring. We also reviewed the latest case studies of the automated essay scoring system that provides feedback.
    
[^32]: 科学可控文本生成方法的进展

    Advancements in Scientific Controllable Text Generation Methods. (arXiv:2307.05538v1 [cs.CL])

    [http://arxiv.org/abs/2307.05538](http://arxiv.org/abs/2307.05538)

    本研究提供了一种新的模式来组织可控文本生成的先前工作，通过描述各种调制策略来实现科学文献的可控生成，为基于这些组成部分的组合提供了新的架构，未来研究将通过实证比较这些方法以了解它们的优势和实用性。

    

    这项研究提供了一种新的模式来组织可控文本生成的先前工作。该模式由七个组成部分组成，每个组成部分对于生成过程至关重要。为了实现科学文献的可控生成，我们描述了用于调制这七个组成部分的各种调制策略。我们还提供了对这些方法的理论研究和定性检验。这一洞察使得基于这些组成部分的组合可能性成为可能。未来的研究将通过实证比较这些方法以了解它们的优势和实用性。

    The previous work on controllable text generation is organized using a new schema we provide in this study. Seven components make up the schema, and each one is crucial to the creation process. To accomplish controlled generation for scientific literature, we describe the various modulation strategies utilised to modulate each of the seven components. We also offer a theoretical study and qualitative examination of these methods. This insight makes possible new architectures based on combinations of these components. Future research will compare these methods empirically to learn more about their strengths and utility.
    
[^33]: 打开ChatGPT：跟踪指令调整文本生成器的开放性、透明度和问责制

    Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators. (arXiv:2307.05532v1 [cs.CL])

    [http://arxiv.org/abs/2307.05532](http://arxiv.org/abs/2307.05532)

    评估了各个项目在代码、数据、模型等方面的开放程度，并发现有许多项目虽自称为开源，却存在不确定合法性的未记录数据，而很少有项目分享重要的指令调整功能。

    

    展示了开放性的差异，并提供了该快速发展领域中开放程度的科学文档。通过评估项目的代码、训练数据、模型权重、强化学习数据、许可、科学文档和访问方式的开放程度，研究结果发现虽然有越来越多的自称为“开源”的项目，但许多项目继承了不确定合法性的未记录的数据，很少有项目分享重要的指令调整功能。

    Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of OpenAI's ChatGPT, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (LLM+RLHF). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as 'open source', many inherit undocumented data of dubious legality, few share the all-important instruction-tunin
    
[^34]: 中学英语作为外语学生与ChatGPT合作完成写作任务的案例研究

    Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT. (arXiv:2307.05493v1 [cs.HC])

    [http://arxiv.org/abs/2307.05493](http://arxiv.org/abs/2307.05493)

    本文研究了中学英语作为外语学生与ChatGPT合作完成写作任务的案例，并对提示的质量和数量进行了分析。结果发现，非技术用户在为ChatGPT编写适当的提示上遇到了困难，需要提供更好的支持和指导。

    

    ChatGPT是一个最先进的聊天机器人。尽管它有潜力支持英语作为外语学生的写作，但要有效地与之合作，学生必须学会设计提示，即制作适当的指令，以使ChatGPT产生期望的输出。然而，对于非技术用户来说，为ChatGPT编写适当的提示并非易事，他们经历了反复试错的过程。本文研究了中学英语作为外语学生在完成写作任务时使用ChatGPT的提示内容，并探讨了提示的质量和数量的模式。数据来自iPad屏幕录像，记录了首次使用ChatGPT和其他最先进聊天机器人完成相同写作任务的中学英语作为外语学生的情况。本文通过四个不同的路径的案例研究，展示了试错过程和提示内容和数量的不同组合。这些案例为提供支持非技术用户有效使用ChatGPT的证据做出了贡献。

    ChatGPT is a state-of-the-art (SOTA) chatbot. Although it has potential to support English as a foreign language (EFL) students' writing, to effectively collaborate with it, a student must learn to engineer prompts, that is, the skill of crafting appropriate instructions so that ChatGPT produces desired outputs. However, writing an appropriate prompt for ChatGPT is not straightforward for non-technical users who suffer a trial-and-error process. This paper examines the content of EFL students' ChatGPT prompts when completing a writing task and explores patterns in the quality and quantity of the prompts. The data come from iPad screen recordings of secondary school EFL students who used ChatGPT and other SOTA chatbots for the first time to complete the same writing task. The paper presents a case study of four distinct pathways that illustrate the trial-and-error process and show different combinations of prompt content and quantity. The cases contribute evidence for the need to provid
    
[^35]: GPT4对同行评审存在一定帮助：一项实验性研究。

    GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study. (arXiv:2307.05492v1 [cs.HC])

    [http://arxiv.org/abs/2307.05492](http://arxiv.org/abs/2307.05492)

    通过比较GPT和人类审稿人生成的评论，我们发现GPT在同行评审中具有一定的帮助性，为解决同行评审资源限制提供了新途径。

    

    在这项实验性研究中，我们调查了GPT4在同行评审过程中的应用。我们的主要假设是，通过与人类审稿人生成的评论进行比较，GPT所生成的评论可以达到相似的有用性。通过比较在一个重要的机器学习会议上提交的学术论文的人类审稿人和GPT模型生成的评论，我们提供了初步证据，表明人工智能可以有效地为同行评审过程做出贡献。我们还进行了对插入了错误的鲁棒性实验，以了解模型更倾向于关注论文的哪些部分。我们的研究结果为利用机器学习工具解决同行评审资源限制开辟了新途径。该结果还为改进评审过程提供了启示，并为在人类反馈资源稀缺的领域中进一步研究扩大监督打下了基础。

    In this pilot study, we investigate the use of GPT4 to assist in the peer-review process. Our key hypothesis was that GPT-generated reviews could achieve comparable helpfulness to human reviewers. By comparing reviews generated by both human reviewers and GPT models for academic papers submitted to a major machine learning conference, we provide initial evidence that artificial intelligence can contribute effectively to the peer-review process. We also perform robustness experiments with inserted errors to understand which parts of the paper the model tends to focus on. Our findings open new avenues for leveraging machine learning tools to address resource constraints in peer review. The results also shed light on potential enhancements to the review process and lay the groundwork for further research on scaling oversight in a domain where human-feedback is increasingly a scarce resource.
    
[^36]: 用于评估自然语言推理中复杂组合知识的合成数据集

    Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference. (arXiv:2307.05034v1 [cs.CL])

    [http://arxiv.org/abs/2307.05034](http://arxiv.org/abs/2307.05034)

    该论文介绍了一个名为SICCK的合成数据集以及一种新颖的分析方法，用于评估自然语言推理中复杂组合知识的性能。研究发现，在零-shot和微调情况下，神经网络推理模型能够很好地捕捉结构和语义组合的变化。

    

    我们介绍了一个名为Sentences Involving Complex Compositional Knowledge (SICCK)的合成数据集，以及一种新颖的分析方法，用于研究自然语言推理模型对逻辑组成性的性能。我们通过修改SICK数据集中的15个示例，生成了1,304个句子对。为此，我们使用一组短语 - 与自然逻辑中的普遍量词、存在量词、否定和其他概念修饰符相对应的修饰符 - 修改了原始文本。我们使用这些短语修改前提和假设的主语、谓语和宾语部分。最后，我们根据自然逻辑规则为这些修改后的文本标注相应的包含关系标签。我们对神经网络推理模型在零-shot和微调情况下对结构和语义组合变化的捕捉能力进行了初步验证。我们发现在这些情况下，NLI模型的性能表现良好。

    We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic. We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014). To this end, we modify the original texts using a set of phrases - modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis. Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules. We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios. We found that the performance of NLI models under th
    
[^37]: 论意义的计算建模：融合情感的体验认知

    On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion. (arXiv:2307.04518v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.04518](http://arxiv.org/abs/2307.04518)

    本文探索了词汇产生意义的过程，重点研究了儿童语言习得和语言理解模型。作者强调了体验感知、情感和认知在语言习得中的重要性，并提出了对类似儿童环境中语言学习智能体的要求。

    

    本文记录了作者探索词汇产生意义的过程，重点关注儿童语言习得以及对语言理解模型的影响。文中解释了儿童语言习得的背景，说明了体验感知和实践在认知过程中的重要性，以及情感与认知如何相互关联以及与语言习得过程的关系。最后，阐述了作者对在类似儿童环境中学习语言的语言学习智能体的要求。本文可作为未来对语言建模工作的潜在指南。

    This document chronicles this author's attempt to explore how words come to mean what they do, with a particular focus on child language acquisition and what that means for models of language understanding.\footnote{I say \emph{historical} because I synthesize the ideas based on when I discovered them and how those ideas influenced my later thinking.} I explain the setting for child language learning, how embodiment -- being able to perceive and enact in the world, including knowledge of concrete and abstract concepts -- is crucial, and how emotion and cognition relate to each other and the language learning process. I end with what I think are some of the requirements for a language-learning agent that learns language in a setting similar to that of children. This paper can act as a potential guide for ongoing and future work in modeling language.
    
[^38]: 对大型语言模型评估的调查

    A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])

    [http://arxiv.org/abs/2307.03109](http://arxiv.org/abs/2307.03109)

    本文综述了大型语言模型（LLMs）的评估方法，关注三个关键维度：评估什么、在哪里评估以及如何评估。评估任务包括自然语言处理、推理、医学应用、伦理学、教育、自然和社会科学、代理应用等多个领域。本文为社会层面对LLMs潜在风险的理解提供了重要参考。

    

    大型语言模型（LLMs）由于在各种应用中表现出的前所未有的性能而在学术界和工业界越来越受欢迎。随着LLMs在研究和日常使用中继续发挥着重要作用，它们的评估变得越来越关键，不仅在任务水平上，而且在社会层面上，以更好地了解它们的潜在风险。在过去的几年里，已经做出了相当大的努力来从不同的角度来研究LLMs。本文综述了LLMs的这些评估方法，重点关注三个关键维度：评估什么、在哪里评估以及如何评估。首先，我们从评估任务的角度提供了一个概述，涵盖了一般的自然语言处理任务、推理、医学应用、伦理学、教育、自然科学和社会科学、代理应用和其他领域。其次，我们通过深入探讨评估方法和基准答案来回答“在哪里”和“如何”这两个问题。

    Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
    
[^39]: LLaMA在临床领域的参数高效微调

    Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v1 [cs.CL])

    [http://arxiv.org/abs/2307.03042](http://arxiv.org/abs/2307.03042)

    本研究提出了一种参数高效微调（PEFT）方法，在临床领域使用临床记录训练了一个专门适配临床领域的LLaMA-LoRA模型，同时提出了一个两步PEFT框架，用于将其与Downstream LLaMA-LoRA适配器进行融合，以实现领域适应。

    

    传统上，将预训练的语言模型适应到新领域，如临床应用，需要重新训练所有参数。然而，由于训练这些大型语言模型所需的计算资源巨大，这种方法的实践性越来越被证明是不切实际的。为了解决这个问题，参数高效微调（PEFT）技术提供了一种可行的解决方案，通过选择性地微调一个小的附加参数集，显著减少了领域适应所需的计算资源。在本研究中，我们提出了临床LLaMA-LoRA，这是一个构建在开源LLaMA模型上的PEFT适配器层。临床LLaMA-LoRA使用从MIMIC-IV数据库中获取的临床记录进行训练，从而创建了一个专为临床领域设计的专用适配器。此外，我们提出了一个两步PEFT框架，将临床LLaMA-LoRA与Downstream LLaMA-LoRA进行融合，后者是另一个专为下游任务设计的PEFT适配器。

    Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. W
    
[^40]: 通过联合优化文本和时刻实现零样本密集视频字幕生成

    Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment. (arXiv:2307.02682v1 [cs.CV])

    [http://arxiv.org/abs/2307.02682](http://arxiv.org/abs/2307.02682)

    通过在训练阶段不使用视频和标注，而是在测试时仅优化输入，我们提出了一种零样本的密集视频字幕生成方法。通过联合优化文本和时刻，我们的方法能够在视频中准确地定位和描述事件。

    

    密集视频字幕生成是一项将有意义的时刻定位和相关字幕生成应用于视频中的任务，通常需要一个昂贵的带有标注的视频片段和文本的语料库。为了降低标注成本，我们提出了一种新颖的零样本密集视频字幕生成方法ZeroTA。我们的方法在训练阶段不需要任何视频或标注，而是通过仅在输入上进行优化，在测试时定位和描述每个输入视频中的事件。这通过引入一个表示视频中的时间段的软时刻掩码，并与语言模型的前缀参数进行联合优化来实现。这种联合优化通过最大化生成文本与视频中的某个时刻之间的匹配分数，将固定的语言生成模型（即GPT-2）与固定的视觉-语言对比模型（即CLIP）进行对齐。我们还引入了一对一的时间IoU损失，使一组软时刻掩码之间进行比对。

    Dense video captioning, a task of localizing meaningful moments and generating relevant captions for videos, often requires a large, expensive corpus of annotated video segments paired with text. In an effort to minimize the annotation cost, we propose ZeroTA, a novel method for dense video captioning in a zero-shot manner. Our method does not require any videos or annotations for training; instead, it localizes and describes events within each input video at test time by optimizing solely on the input. This is accomplished by introducing a soft moment mask that represents a temporal segment in the video and jointly optimizing it with the prefix parameters of a language model. This joint optimization aligns a frozen language generation model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e., CLIP) by maximizing the matching score between the generated text and a moment within the video. We also introduce a pairwise temporal IoU loss to let a set of soft moment masks c
    
[^41]: PatternGPT: 一种基于模式的大型语言模型文本生成框架

    PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.00470](http://arxiv.org/abs/2307.00470)

    PatternGPT是一种基于模式驱动的大型语言模型文本生成框架，通过利用大型语言模型的提取能力生成多样化的模式，并使用联邦学习的思想实现模式共享，最终通过搜索高质量模式指导生成模型。该框架具有生成多样化模式、保护数据隐私、结合外部知识等优势。

    

    大型语言模型(LLMs)展示了出色的文本生成能力，能够为许多下游任务生成流畅的响应。然而，将大型语言模型应用于现实世界的关键任务仍然具有挑战性，因为它们容易出现幻觉，并且无法直接使用外部知识。为解决上述问题，本文提出了PatternGPT，一种基于模式驱动的大型语言模型文本生成框架。首先，该框架利用大型语言模型的提取能力生成丰富多样的模式，然后借鉴联邦学习的思想，使用多个代理实现共享以获取更多样的模式。最后，它使用判断标准和优化算法搜索高质量的模式，并使用搜索到的模式指导模型进行生成。该框架具有生成多样化模式、保护数据隐私、结合外部知识等优势。

    Large language models(LLMS) have shown excellent text generation capabilities,capable of generating fluent responses for many downstream tasks. However,applying large language models to real-world critical tasks remains challenging due to their susceptibility to hallucinations and inability to directly use external knowledge. To address the above challenges,this paper proposes PatternGPT, a pattern-driven text generation framework for large language models. First,the framework utilizes the extraction capabilities of large language models to generate rich and diverse patterns and later draws on the idea of federated learning. Using multiple agents to achieve sharing to obtain more diverse patterns. Finally, it searches for high-quality patterns using judgment criteria and optimization algorithms and uses the searched patterns to guide the model for generation. This framework has the advantages of generating diversified patterns, protecting data privacy,combining external knowledge, and 
    
[^42]: 《在一个对女性厌恶的incels论坛中的身份建构》

    Identity Construction in a Misogynist Incels Forum. (arXiv:2306.15745v1 [cs.CL])

    [http://arxiv.org/abs/2306.15745](http://arxiv.org/abs/2306.15745)

    本研究使用定量文本和网络分析方法，研究了最大的黑洞incels论坛如何讨论身份群体。研究发现该社区产生了许多新的身份术语，存在物质主义的意识形态。对此我们讨论了对自动化 misogynist hate speech 检测研究的影响。

    

    本文使用定量文本和网络分析方法，研究了incels.is，即最大的黑洞incels论坛如何讨论身份群体。我们发现该社区产生了许多新的身份术语，尽管女性的术语最常见，但其他少数群体的提及也在增加。对身份群体的关联分析表明，这个社区存在着物质主义的意识形态，其中身体外貌、性别和种族等决定了人的价值。我们讨论了对自动化 misogynist hate speech 检测研究的影响。

    Online communities of involuntary celibates (incels) are a prominent source of misogynist hate speech. In this paper, we use quantitative text and network analysis approaches to examine how identity groups are discussed on incels.is, the largest black-pilled incels forum. We find that this community produces a wide range of novel identity terms and, while terms for women are most common, mentions of other minoritized identities are increasing. An analysis of the associations made with identity groups suggests an essentialist ideology where physical appearance, as well as gender and racial hierarchies, determine human value. We discuss implications for research into automated misogynist hate speech detection.
    
[^43]: 评估指导微调语言模型的零样本鲁棒性

    Evaluating the Zero-shot Robustness of Instruction-tuned Language Models. (arXiv:2306.11270v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11270](http://arxiv.org/abs/2306.11270)

    本文评估了指导微调语言模型的零样本鲁棒性，并发现使用新颖但合适的指导措辞会降低模型性能。

    

    指导微调最近被提出作为提高大型语言模型在新任务上的零样本能力的一种有希望的方法。这种技术在改善中等大小的语言模型（LLMs）的性能方面表现出了特别的优势，有时甚至与更大的模型变种相竞争。在本文中，我们提出两个问题：（1）指导微调的模型对指导的特定措辞有多敏感，（2）如何使它们更能抵抗自然语言的变化。为了回答前一个问题，我们收集了由NLP从业者手工编写的319个指导，涵盖了广泛使用的基准测试中的80多个独特任务，并评估了这些指导与指导微调期间观察到的指导措辞之间的方差和平均性能。我们发现，使用新颖（未观察到的）但合适的指导措辞会一致地降低模型的性能，有时甚至会大幅降低。

    Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further,
    
[^44]: 稀疏模块激活用于高效的序列建模

    Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11197](http://arxiv.org/abs/2306.11197)

    本论文引入了稀疏模块激活 (SMA) 机制，用于高效的序列建模。这种机制可以动态地稀疏激活序列元素的子模块，减少计算和内存消耗。

    

    线性状态空间模型 (SSM) 在各种序列建模任务中表现出了很强的性能，因为它们有效地编码了循环结构。然而，在更综合的任务中，如语言建模和机器翻译中，基于自注意力的模型仍然优于SSM。同时使用SSM和自注意力的混合模型通常显示出有希望的性能，但当前方法将注意力模块静态且均匀地应用于输入序列中的所有元素，导致了质量和效率之间的次优权衡。在这项工作中，我们引入了稀疏模块激活 (SMA)，这是一种通用机制，使神经网络能够以可微分的方式稀疏地动态激活序列元素的子模块。通过允许每个元素跳过非激活的子模块，SMA可以在序列建模的训练和推理阶段降低计算和内存消耗。作为SMA的一个特定实例，我们设计了一种新颖的神经网络模型。

    Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural a
    
[^45]: mBERT是否理解罗曼什语？使用词语对齐评估词嵌入

    Does mBERT understand Romansh? Evaluating word embeddings using word alignment. (arXiv:2306.08702v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.08702](http://arxiv.org/abs/2306.08702)

    本研究通过在德语和罗曼什语的平行句子中使用mBERT和XLM-R的词嵌入，结合相似性对齐模型，评估了mBERT对罗曼什语的理解能力。结果显示mBERT的词嵌入能够有效地对罗曼什语进行词语对齐，为进一步研究提供了有意义和适用的信息。

    

    我们在德语和罗曼什语之间的平行句子中，使用mBERT和XLM-R的词嵌入，结合基于相似性的词语对齐模型（SimAlign和awesome-align）进行测试。由于罗曼什语是一种未知语言，我们处于零样本的情况。使用mBERT的词嵌入，两个模型的对齐错误率为0.22，优于统计模型fast_align，并且与对于已知语言的基于相似性的词语对齐相当。我们将这些结果解释为mBERT包含了对罗曼什语有意义和适用的信息的证据。

    We test similarity-based word alignment models (SimAlign and awesome-align) in combination with word embeddings from mBERT and XLM-R on parallel sentences in German and Romansh. Since Romansh is an unseen language, we are dealing with a zero-shot setting. Using embeddings from mBERT, both models reach an alignment error rate of 0.22, which outperforms fast_align, a statistical model, and is on par with similarity-based word alignment for seen languages. We interpret these results as evidence that mBERT contains information that can be meaningful and applicable to Romansh.  To evaluate performance, we also present a new trilingual corpus, which we call the DERMIT (DE-RM-IT) corpus, containing press releases made by the Canton of Grisons in German, Romansh and Italian in the past 25 years. The corpus contains 4 547 parallel documents and approximately 100 000 sentence pairs in each language combination. We additionally present a gold standard for German-Romansh word alignment. The data i
    
[^46]: 用MT-Bench和Chatbot Arena评估以LLM为基础的聊天助手

    Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])

    [http://arxiv.org/abs/2306.05685](http://arxiv.org/abs/2306.05685)

    研究使用强大的LLM作为评判员，通过引入两个基准测试来确认LLM评判员和人类偏好之间的一致性，并可调整聊天助手的模型架构和微调方法来提高其性能。

    

    评估基于大语言模型（LLM）的聊天助手会面临挑战，因为它们具有广泛的功能，而现有的基准无法衡量人类偏好。为了解决这个问题，我们探索使用强大的LLM作为评判员，在更加开放的问题上评估这些模型。我们研究了LLM作为评判员的使用和局限性，如位置和冗余偏见以及有限的推理能力，并提出解决方案来迁移其中一些问题。然后，我们通过引入两个基准测试（一个多轮问答集和一个众包竞技平台）来确认LLM评判员和人类偏好之间的一致性。我们的结果显示，像GPT-4这样的强大LLM评判员可以很好地匹配受控和众包人类偏好，达到了80％以上的一致性，与人类一致性水平相同。因此，LLM作为评判员是一种可扩展且可解释的逼近人类偏好的方式，而这些偏好是非常昂贵获取的。此外，我们证明，通过使用LLM作为评判员，可以通过调整聊天助手的模型架构和微调方法来提高其性能。

    Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we
    
[^47]: KIT的多语言演讲翻译系统在IWSLT 2023上的应用

    KIT's Multilingual Speech Translation System for IWSLT 2023. (arXiv:2306.05320v1 [cs.CL])

    [http://arxiv.org/abs/2306.05320](http://arxiv.org/abs/2306.05320)

    本文介绍了一个为IWSLT 2023多语言翻译贡献的翻译系统，其重点在于翻译科学会议演讲。用“检索式方法”（kNN-MT）进行有效的适应，该系统采用适配器轻松集成来自数据增强的增量训练数据，并展示级联系统更容易适应特定目标领域的优势。

    

    许多现有的语音翻译基准测试都针对高品质录音条件下的以英语为母语的语音，这通常与实际使用情况中的条件不符。在本文中，我们描述了我们为IWSLT 2023多语言轨道设计的语音翻译系统，重点翻译科学会议演讲。测试条件包括口音重的输入语音和术语密集的内容，并且需要翻译成10种资源数量不同的语言。在没有来自目标领域的训练数据的情况下，我们使用了检索式方法（kNN-MT）进行有效的适应（语音翻译+0.8 BLEU）。我们还使用适配器轻松集成来自数据增强的增量训练数据，并展示其与重新训练的性能相匹配。我们观察到，级联系统更容易适应特定目标领域，因为它们是由多个独立模块组成的。我们的级联语音系统远远优于其端到端系统。

    Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system substantially outperforms its end-to-end 
    
[^48]: 超越一个模型适用于所有领域：大型语言模型的领域专门化综述

    Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18703](http://arxiv.org/abs/2305.18703)

    本文综述了大型语言模型的领域专门化，包括动机、挑战、方法论和评估指标。此外，还提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。

    

    大型语言模型（LLM）已经大大推动了自然语言处理（NLP）领域的发展，为广泛应用提供了高度实用、任务无关的基础。LLMs 作为通用任务求解器的巨大潜力，促使人们将其用于特定领域，如医疗保健、金融和教育，并将其用作助手甚至替代特定领域的专家和工具。但是，将LLMs直接应用于特定领域中的复杂问题会遇到许多困难，包括领域数据的异质性、领域知识的复杂性、领域目标的独特性以及约束的多样性。为了填补这种差距，最近几年进行了急剧增加的研究和实践致力于大型语言模型的领域专门化，然而这方面的研究尚未被系统地总结。在这篇综述中，我们对LLMs的领域专门化进行了全面概述，包括动机、挑战、方法论和评估指标。此外，我们提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。

    Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
    
[^49]: 上下文化的短语预测网络在端到端语音识别中的应用

    Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.12493](http://arxiv.org/abs/2305.12493)

    本研究引入了一个上下文短语预测网络用于基于注意力的深度偏置方法，通过计算偏置损失以帮助训练上下文化模型，在多种端到端语音识别模型上实现了显著的WER降低，相对于基线模型相对WER提高了12.1％，上下文短语的WER相对降低了40.5％。

    

    上下文信息在语音识别技术中发挥着至关重要的作用，将其融入端到端语音识别模型近年来引起了极大的兴趣。然而，先前的深度偏置方法缺乏偏置任务的显式监督。本研究引入了一个上下文短语预测网络用于基于注意力的深度偏置方法。该网络利用上下文嵌入预测发音中的上下文短语，并计算偏置损失以帮助训练上下文化模型。我们的方法在多种端到端语音识别模型上实现了显著的单词错误率(WER)降低。对LibriSpeech语料库的实验结果表明，在基线模型上，我们提出的模型相对WER提高了12.1％，上下文短语的WER相对降低了40.5％。此外，通过应用上下文短语过滤策略，我们还有效消除了使用更大的偏置列表时的WER降级现象。

    Contextual information plays a crucial role in speech recognition technologies and incorporating it into the end-to-end speech recognition models has drawn immense interest recently. However, previous deep bias methods lacked explicit supervision for bias tasks. In this study, we introduce a contextual phrase prediction network for an attention-based deep bias method. This network predicts context phrases in utterances using contextual embeddings and calculates bias loss to assist in the training of the contextualized model. Our method achieved a significant word error rate (WER) reduction across various end-to-end speech recognition models. Experiments on the LibriSpeech corpus show that our proposed model obtains a 12.1% relative WER improvement over the baseline model, and the WER of the context phrases decreases relatively by 40.5%. Moreover, by applying a context phrase filtering strategy, we also effectively eliminate the WER degradation when using a larger biasing list.
    
[^50]: RL4F: 使用强化学习生成自然语言反馈修复模型输出

    RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs. (arXiv:2305.08844v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08844](http://arxiv.org/abs/2305.08844)

    本论文提出了RL4F（Reinforcement Learning for Feedback），这是一个多智能体协作框架，通过强化学习生成自然语言反馈来修复模型输出。该方法适用于黑盒或仅有有限访问权限的模型，避免了传统微调方法的计算效率和空间效率问题。

    

    尽管最大的语言模型取得了前所未有的成功，但它们仍然会出错。与人类通过反馈学习和改进类似，先前的研究提出为语言模型提供自然语言反馈，以指导它们修复输出。由于人工生成的批评在获取上较为昂贵，研究人员设计了学习批评生成器来替代人类批评者，并假设可以训练下游模型利用生成的反馈。然而，这种方法无法适用于黑盒或仅有有限访问权限的模型，比如ChatGPT，因为它们无法进行微调。此外，在大型通用语言模型时代，微调既不具备计算效率也不具备空间效率，因为它会导致网络的多个副本。在这项工作中，我们引入了RL4F（强化学习反馈），这是一个多智能体协作框架，其中批评生成器的训练目标是最大化GPT-3的终端任务性能，GPT-3是一个固定模型，比前代模型更加优秀。

    Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 time
    
[^51]: ANTONIO:面向NLP验证的系统化基准生成方法

    ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])

    [http://arxiv.org/abs/2305.04003](http://arxiv.org/abs/2305.04003)

    本文介绍了一种名为ANTONIO的Python库，它基于抽象解释方法提供了一种实用的方法和启发式规则，以便为自然语言处理（NLP）数据集和模型生成已知验证方法的基准。因为其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。

    

    自然语言处理（NLP）中使用的机器学习模型的验证被认为是一个难题。现有的神经网络验证方法常用于计算机视觉和其他数字数据集，但并不适用于NLP。本研究探讨了造成这一问题的技术原因，并在此基础上提出了实用的方法和启发式规则，以便将NLP数据集和模型准备为适合基于抽象解释的已知验证方法。我们将这些方法实现为一个名为ANTONIO的Python库，该库连接到神经网络验证器ERAN和Marabou。我们使用一个名为R-U-A-Robot的NLP数据集对工具进行了评估，该数据集被提议作为验证具有法律重要性的NLP应用的基准。我们希望，由于其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。

    Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
    
[^52]: DataComp：寻找下一代多模态数据集

    DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])

    [http://arxiv.org/abs/2304.14108](http://arxiv.org/abs/2304.14108)

    DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。

    

    大型的多模态数据集在近期的突破中起到了关键作用，比如CLIP、Stable Diffusion和GPT-4等。与此同时，数据集很少得到与模型架构或训练算法同等的研究关注。为了解决这个在机器学习生态系统中的缺陷，我们介绍了DataComp，一个基准测试，其中训练代码是固定的，研究人员通过提出新的训练集来进行创新。我们提供了一个基于Common Crawl的新候选池，其中包含12.8B个图像-文本对的数据集实验测试平台。参加我们基准测试的研究人员可以设计新的过滤技术或策划新的数据源，并通过运行我们标准化的CLIP训练代码并在38个下游测试集上进行测试来评估他们的新数据集。我们的基准测试包含多个规模，四个候选池大小和相应的计算预算，在训练期间涵盖了从12.8M到12.8B个样本。这种多规模设计有助于研究规模趋势，并为研究人员提供了更多的选择余地。

    Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
    
[^53]: GPT检测器对非英语母语的作者存在偏见。

    GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])

    [http://arxiv.org/abs/2304.02819](http://arxiv.org/abs/2304.02819)

    该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。

    

    生成语言模型的快速推广带来了数字通信方面的实质性进展，同时也引发了AI生成内容潜在误用的担忧。虽然已经提出了许多检测方法来区分AI和人类生成的内容，但这些检测器的公平性和鲁棒性仍未得到充分探讨。在这项研究中，我们使用来自英语母语和非英语母语作者的写作样本评估了几种广泛使用的GPT检测器的性能表现。我们的研究发现，这些检测器持续将非英语母语的写作样本错误地分类为AI生成的内容，而原生写作样本则能够被准确识别。此外，我们证明了简单的提示策略不仅可以缓解这种偏见，而且还可以有效地规避GPT检测器，这表明GPT检测器可能无意中惩罚具有受限语言表达能力的作者。我们的研究结果呼吁进行更广泛的讨论。

    The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
    
[^54]: 语言特定的情绪概念知识表示对情绪推断的因果支持

    Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.09582](http://arxiv.org/abs/2302.09582)

    本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，探讨了语言是否会因果支持情绪推断。实验结果显示，属性特定的神经元操纵导致情绪推断任务的性能下降，这与人类心理空间中不同属性的重要性有关。这些发现为支持基于语言的情绪推断机制提供了因果证据，并凸显了情绪概念知识的贡献。

    

    在情绪科学中，如何理解语言支持情绪推断仍然是一个争议的话题。本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，调查了语言是否会因果支持情绪推断。使用提示技术，发现了14个情绪概念的属性由不同的人工神经元群体表示。通过操纵这些属性相关的神经元，与随机操纵相比，大多数情绪推断任务的表现出现了下降。属性特定的表现下降与人类心理空间中不同属性的重要性有关。我们的发现提供了支持基于语言的情绪推断机制的因果证据，并强调了情绪概念知识的贡献。

    Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge.
    
[^55]: 对莎士比亚戏剧的连续分析的数据科学和机器学习方法

    A data science and machine learning approach to continuous analysis of Shakespeare's plays. (arXiv:2301.06024v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.06024](http://arxiv.org/abs/2301.06024)

    通过数据科学和机器学习的方法，对莎士比亚的作品进行了连续分析，发现他的写作风格随着时间的推移发生了变化，其中包括句子长度、形容词和副词的频率以及文本中表达的情感。此外，研究还发现，部分戏剧的风格特征更类似于其写作时间之前或之后的作品。

    

    数量化的文本分析方法的可行性为文学分析提供了新的途径。本文将综合机器学习分析应用于威廉·莎士比亚的作品。分析结果显示，莎士比亚的写作风格随着时间的推移发生了明显的变化，其中最显著的变化包括句子长度、形容词和副词的频率以及文本中表达的情感。将机器学习应用于对戏剧年份的风格预测表明，实际年份和预测年份之间的皮尔逊相关系数为0.71，表明莎士比亚的写作风格在数量化测量方面随时间变化。此外，研究还发现，某些戏剧的风格特征更类似于它们的写作年份之前或之后的作品。例如，"罗密欧与朱丽叶"的日期为1596年，但在风格特征上更类似于1600年之后莎士比亚的其他作品。

    The availability of quantitative text analysis methods has provided new ways of analyzing literature in a manner that was not available in the pre-information era. Here we apply comprehensive machine learning analysis to the work of William Shakespeare. The analysis shows clear changes in the style of writing over time, with the most significant changes in the sentence length, frequency of adjectives and adverbs, and the sentiments expressed in the text. Applying machine learning to make a stylometric prediction of the year of the play shows a Pearson correlation of 0.71 between the actual and predicted year, indicating that Shakespeare's writing style as reflected by the quantitative measurements changed over time. Additionally, it shows that the stylometrics of some of the plays is more similar to plays written either before or after the year they were written. For instance, Romeo and Juliet is dated 1596, but is more similar in stylometrics to plays written by Shakespeare after 1600
    
[^56]: 利用标签平滑实现领域内外文本对抗鲁棒性

    In and Out-of-Domain Text Adversarial Robustness via Label Smoothing. (arXiv:2212.10258v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10258](http://arxiv.org/abs/2212.10258)

    本文研究了标签平滑策略在不同领域的自然语言处理任务中对对抗鲁棒性的影响。实验证明，标签平滑显著提高了预训练模型的鲁棒性，并减少了在对抗样本上出现的过度自信错误。

    

    最近研究表明，最新的自然语言处理模型容易受到对抗性攻击，即对输入进行细微修改（如同义词替换）会极大地改变模型的预测结果。虽然已经提出了几种针对文本对抗性攻击的防御技术，并将其调整至离散性质的文本数据上，但是综合性的规则化方法，如语言模型的标签平滑对于文本模型的鲁棒性提供的效果还没有被研究过。在本文中，我们研究了各种标签平滑策略在领域内和领域外的基础模型中对多样化自然语言处理任务的对抗鲁棒性。我们的实验证明，标签平滑显著提高了预训练模型（如BERT）在各种流行攻击中的对抗鲁棒性。我们还分析了预测可信度与鲁棒性之间的关系，并显示标签平滑减少了在对抗样本上出现的过度自信错误。

    Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the adversarial robustness provided by various label smoothing strategies in foundational models for diverse NLP tasks in both in-domain and out-of-domain settings. Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks. We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples.
    
[^57]: CoNLL-2003命名实体标注器在2023年仍然有效吗?

    Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?. (arXiv:2212.09747v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09747](http://arxiv.org/abs/2212.09747)

    本文评估了CoNLL-2003上超过20种模型的泛化性能，发现NER模型的泛化能力各异。令人惊讶的是，即使使用几十年前的数据进行微调，预训练的Transformer模型仍然保持着很好的性能。研究还发现，预训练语料库与下游测试集之间的时间不匹配是导致性能恶化的主要因素。

    

    CoNLL-2003英文命名实体识别（NER）数据集已经被广泛用于训练和评估NER模型几乎20年。然而，目前不清楚在现代数据上应用在此20年前的数据上训练并经过几十年发展的模型的性能如何。本文评估了超过20种在CoNLL-2003上训练的模型的泛化情况，并表明NER模型的泛化能力差异很大。令人惊讶的是，我们发现即使使用几十年前的数据进行微调，如RoBERTa和T5等预训练的Transformer模型仍然没有性能下降的证据。我们研究了为什么一些模型能够很好地泛化到新数据，而其他模型则不能，并试图解释由于测试集重用而导致的时间漂移和过拟合的影响。我们的分析表明，大部分恶化是由于预训练语料库与下游测试集之间的时间不匹配所致。我们发现四个因素对于良好的泛化性能很重要。

    The CoNLL-2003 English named entity recognition (NER) dataset has been widely used to train and evaluate NER models for almost 20 years. However, it is unclear how well models that are trained on this 20-year-old data and developed over a period of decades using the same test set will perform when applied on modern data. In this paper, we evaluate the generalization of over 20 different models trained on CoNLL-2003, and show that NER models have very different generalization. Surprisingly, we find no evidence of performance degradation in pre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using decades-old data. We investigate why some models generalize well to new data while others do not, and attempt to disentangle the effects of temporal drift and overfitting due to test reuse. Our analysis suggests that most deterioration is due to temporal mismatch between the pre-training corpora and the downstream test sets. We found that four factors are important for good g
    
[^58]: 评估人机语言模型交互

    Evaluating Human-Language Model Interaction. (arXiv:2212.09746v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09746](http://arxiv.org/abs/2212.09746)

    为了评估人机交互，研究人员开发了一个框架HALIE，该框架捕捉了交互过程、主观体验和偏好概念，并设计了五个任务来涵盖不同形式的交互。

    

    许多语言模型（LM）的实际应用，例如写作辅助和代码自动完成，涉及到人机交互。然而，大多数基准测试都是非交互式的，模型在没有人类参与的情况下产生输出。为了评估人机交互，我们开发了一个新的框架，人机语言交互评估（HALIE），该框架定义了交互式系统的组成部分和设计评估指标时要考虑的维度。与标准的非交互式评估相比，HALIE捕捉到了（i）交互过程，而不仅仅是最终输出；（ii）第一人称主观体验，而不仅仅是第三方评估；（iii）除了质量之外的偏好概念（例如享受和所有权）。然后，我们设计了五个任务，涵盖不同形式的交互：社交对话、问答、填字游戏、摘要和隐喻生成。使用四个最先进的LM（OpenAI的GPT-3的三个变体和AI21 Labs的Jurass）

    Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurass
    
[^59]: SpeechBlender: 用于发音错误数据生成的语音增强框架

    SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation. (arXiv:2211.00923v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.00923](http://arxiv.org/abs/2211.00923)

    SpeechBlender是一个用于生成发音错误的数据增强框架，在发音错误检测模型的音素级上获得了ASR相关的最新技术水平，具有更有效的样本。

    

    设计发音错误检测模型所面临的主要问题是缺乏标记的第二语言（L2）语音数据。我们提出了SpeechBlender - 这是一个用于生成发音错误的细粒度数据增强流水线，以克服这种数据稀缺性。SpeechBlender利用各种掩模以针对语音单元的不同区域，并在增强发音时使用混合因子以线性插值原始语音信号。这些掩模有助于平滑混合信号，产生比“剪切/粘贴”方法更有效的样本。我们的建议技术在与先前的最新技术[1]相比的ASR依赖性发音错误检测模型的音素级上达到了最新的技术水平，Speechocean762，Pearson相关系数（PCC）提高了2.0％。此外，与我们的基线相比，我们在音素级别上展示了5.0％的改进。我们还观察到，在阿拉伯语AraVoiceL2测试集上的F1得分提高了4.6％。

    The lack of labeled second language (L2) speech data is a major challenge in designing mispronunciation detection models. We introduce SpeechBlender - a fine-grained data augmentation pipeline for generating mispronunciation errors to overcome such data scarcity. The SpeechBlender utilizes varieties of masks to target different regions of phonetic units, and use the mixing factors to linearly interpolate raw speech signals while augmenting pronunciation. The masks facilitate smooth blending of the signals, generating more effective samples than the `Cut/Paste' method. Our proposed technique achieves state-of-the-art results, with Speechocean762, on ASR dependent mispronunciation detection models at phoneme level, with a 2.0% gain in Pearson Correlation Coefficient (PCC) compared to the previous state-of-the-art [1]. Additionally, we demonstrate a 5.0% improvement at the phoneme level compared to our baseline. We also observed a 4.6% increase in F1-score with Arabic AraVoiceL2 testset.
    
[^60]: 对比解码：将开放式文本生成视为优化问题

    Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.15097](http://arxiv.org/abs/2210.15097)

    对比解码是一种可靠的解码方法，它通过优化对比目标来生成合理且高质量的文本，与仅使用较大的语言模型进行解码相比，对比解码能够避免短而重复的文本和不连贯的文本，并适用于不同的模型规模。

    

    鉴于语言模型（LM），最大概率是开放式生成的较差解码目标，因为它会产生短而重复的文本。另一方面，采样往往会产生与原始主题偏离的不连贯文本。我们提出了对比解码（CD），这是一种可靠的解码方法，它在满足合理性约束条件的前提下优化对比目标。对比目标返回一个大型LM（被称为专家，例如OPT-13B）和一个小型LM（被称为业余者，例如OPT-125M）之间的似然差异，并且约束条件确保输出是合理的。CD的灵感来自于这样一个事实，即较大的LM（例如重复、不连贯）在较小的LM中更为普遍，并且这种差异表明哪些文本应优先考虑。CD不需要额外的培训，并且比仅从较大的LM进行解码的情况下产生更高质量的文本。它还适用于不同的模型规模（OPT-13B和GPT2-1.5B）。

    Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and 
    
[^61]: 本文重新审视了局部基于语法的编码问题

    Local Grammar-Based Coding Revisited. (arXiv:2209.13636v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2209.13636](http://arxiv.org/abs/2209.13636)

    本文重新审视了最小局部基于语法的编码问题，并提出了一种新的、更简单、更普遍的证明方法，证明了最小分块编码具有强大的普遍性。同时，通过实验也表明，最小分块编码中的规则数量不能明确区分长记忆和无记忆的源。

    

    本文重新审视了最小局部基于语法的编码问题。在这个设置中，局部基于语法的编码器逐个符号地对语法进行编码，而最小语法变换通过局部语法编码的长度在预设的语法类别中最小化语法长度。已知，这样的最小编码对于严格正熵率的情况具有强大的普遍性，而最小语法中的规则数量构成了源的互信息的上界。尽管完全最小编码可能是不可行的，但受限的最小分块编码可以有效计算。本文提出了一种新的、更简单、更普适的最小分块编码强大普遍性的证明方法，不受熵率的限制。该证明基于对排名概率的简单的Zipfian界限。顺便提一下，我们还通过实验证明，最小分块编码中的规则数量不能明确区分长记忆和无记忆的源。

    We revisit the problem of minimal local grammar-based coding. In this setting, the local grammar encoder encodes grammars symbol by symbol, whereas the minimal grammar transform minimizes the grammar length in a preset class of grammars as given by the length of local grammar encoding. It has been known that such minimal codes are strongly universal for a strictly positive entropy rate, whereas the number of rules in the minimal grammar constitutes an upper bound for the mutual information of the source. Whereas the fully minimal code is likely intractable, the constrained minimal block code can be efficiently computed. In this article, we present a new, simpler, and more general proof of strong universality of the minimal block code, regardless of the entropy rate. The proof is based on a simple Zipfian bound for ranked probabilities. By the way, we also show empirically that the number of rules in the minimal block code cannot clearly discriminate between long-memory and memoryless s
    
[^62]: Kencorpus: 一份用于自然语言处理任务的肯尼亚斯瓦希里语、多卢奥语和卢雅语的肯尼亚语料库

    Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for Natural Language Processing Tasks. (arXiv:2208.12081v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.12081](http://arxiv.org/abs/2208.12081)

    Kencorpus项目旨在收集和存储肯尼亚斯瓦希里语、多卢奥语和卢雅语的文本和语音数据，以解决原生非洲语言在自然语言处理中的数字包容性和信息访问问题。这个数据集包含超过5,000个项目，为多卢奥语和卢雅语提供了词性标注集。

    

    原生非洲语言在自然语言处理中被归类为不受关注的语言，在数字包容性和信息访问方面存在问题。这些语言的处理挑战在于如何在没有必要的数据的情况下使用机器学习和深度学习模型。Kencorpus项目旨在通过收集和存储足够好的文本和语音数据来填补这一空白，以应用于多语言社区中的机器翻译、问答和转录等数据驱动的解决方案。Kencorpus数据集是一个包含肯尼亚三种主要使用语言（斯瓦希里语、多卢奥语和卢雅语）的文本和语音语料库。研究人员从社区、学校、媒体和出版商收集了这些数据。Kencorpus数据集包含5,594个项目，其中包括4,442个文本（5.6百万词）和1,152个语音文件（177小时）。基于这些数据，为多卢奥语和卢雅语分别设置了5万和9.3万个词的词性标注集。

    Indigenous African languages are categorized as under-served in Natural Language Processing. They therefore experience poor digital inclusivity and information access. The processing challenge with such languages has been how to use machine learning and deep learning models without the requisite data. The Kencorpus project intends to bridge this gap by collecting and storing text and speech data that is good enough for data-driven solutions in applications such as machine translation, question answering and transcription in multilingual communities. The Kencorpus dataset is a text and speech corpus for three languages predominantly spoken in Kenya: Swahili, Dholuo and Luhya. Data collection was done by researchers from communities, schools, media, and publishers. The Kencorpus' dataset has a collection of 5,594 items - 4,442 texts (5.6M words) and 1,152 speech files (177hrs). Based on this data, Part of Speech tagging sets for Dholuo and Luhya (50,000 and 93,000 words respectively) wer
    
[^63]: 一种基于差分隐私的定制文本消毒机制

    A Customized Text Sanitization Mechanism with Differential Privacy. (arXiv:2207.01193v2 [cs.CR] CROSS LISTED)

    [http://arxiv.org/abs/2207.01193](http://arxiv.org/abs/2207.01193)

    一种基于差分隐私的定制文本消毒机制 (CusText) 提供了更好的隐私保护与效用权衡，适用于任何相似度度量，并在标记级别实现了先进的隐私保护。

    

    随着隐私问题在自然语言处理 (NLP) 社区中引起越来越多的关注，已提出了许多方法来按照差分隐私原则对文本进行消毒。然而，基于度量局部差分隐私 (MLDP) 的最先进文本消毒机制不适用于非度量语义相似度度量，并且无法在隐私和效用之间取得良好的权衡。为了解决上述局限性，我们提出了一种基于原始 ε-差分隐私 (DP) 定义的新型定制文本 (CusText) 消毒机制，该机制与任何相似度度量兼容。此外，CusText为每个输入标记分配一个定制的输出标记集，以在标记级别提供更高级的隐私保护。在几个基准数据集上进行的大量实验证明，CusText在隐私和效用之间实现了更好的权衡。代码可在 https://github.com/sai4july/CusT 获取。

    As privacy issues are receiving increasing attention within the Natural Language Processing (NLP) community, numerous methods have been proposed to sanitize texts subject to differential privacy. However, the state-of-the-art text sanitization mechanisms based on metric local differential privacy (MLDP) do not apply to non-metric semantic similarity measures and cannot achieve good trade-offs between privacy and utility. To address the above limitations, we propose a novel Customized Text (CusText) sanitization mechanism based on the original $\epsilon$-differential privacy (DP) definition, which is compatible with any similarity measure. Furthermore, CusText assigns each input token a customized output set of tokens to provide more advanced privacy protection at the token level. Extensive experiments on several benchmark datasets show that CusText achieves a better trade-off between privacy and utility than existing mechanisms. The code is available at https://github.com/sai4july/CusT
    
[^64]: 判别模型在基于方面的情感分析中仍然优于生成模型

    Discriminative Models Can Still Outperform Generative Models in Aspect Based Sentiment Analysis. (arXiv:2206.02892v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.02892](http://arxiv.org/abs/2206.02892)

    本文评估了判别模型和生成模型在基于方面的情感分析中的表现，发现判别模型在几乎所有设置中仍然可以优于生成模型。

    

    基于方面的情感分析（ABSA）有助于解释用户对产品和服务的意见。过去，ABSA模型是判别性的，但最近生成模型已被用于直接从文本中生成方面和极性。相反，判别模型通常首先从文本中选择方面，然后对方面的极性进行分类。以前的结果表明，在几个英语ABSA数据集上，生成模型胜过判别模型。在这里，我们在几个设置中评估和对比两种最先进的判别和生成模型：跨语言、跨领域和跨语言和领域，以了解除了英语单一领域之外的设置中的泛化能力。我们更全面的评估结果表明，与以前的研究相反，在几乎所有的设置中，判别模型仍然可以优于生成模型。

    Aspect-based Sentiment Analysis (ABSA) helps to explain customers' opinions towards products and services. In the past, ABSA models were discriminative, but more recently generative models have been used to generate aspects and polarities directly from text. In contrast, discriminative models commonly first select aspects from the text, and then classify the aspect's polarity. Previous results showed that generative models outperform discriminative models on several English ABSA datasets. Here, we evaluate and contrast two state-of-the-art discriminative and generative models in several settings: cross-lingual, cross-domain, and cross-lingual and domain, to understand generalizability in settings other than English mono-lingual in-domain. Our more thorough evaluation shows that, contrary to previous studies, discriminative models can still outperform generative models in almost all settings.
    
[^65]: UNIQORN：统一的RDF知识图谱与自然语言文本问答系统

    UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2108.08614](http://arxiv.org/abs/2108.08614)

    本文提出了一个名为UNIQORN的问答系统，它能够无缝地处理RDF数据和文本，使用fine-tuned BERT模型为问题构建上下文图，并使用图算法确定与问题相关的子图来回答问题。

    

    问题回答在知识图谱和其他RDF数据上已经取得了巨大的进展，许多优秀的系统可以为自然语言问题或电报查询提供清晰的答案。其中一些系统将文本源作为附加证据纳入回答过程，但不能计算仅存在于文本中的答案。相反，IR和NLP社区的系统已经解决了有关文本的QA问题，但是这些系统几乎不利用语义数据和知识。本文提出了第一个可以无缝操作混合RDF数据集和文本语料库或单个来源的复杂问题的系统，在统一框架中进行操作。我们的方法称为UNIQORN，通过使用经过精细调整的BERT模型从RDF数据和/或文本语料库中检索与问题相关的证据来动态构建上下文图。结果图通常非常丰富但高度嘈杂。UNIQORN通过用于组Steiner树的图算法来处理这个输入，从而确定与问题相关的子图，进而回答问题。

    Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first system for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifi
    

