# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [StoryBench: A Multifaceted Benchmark for Continuous Story Visualization.](http://arxiv.org/abs/2308.11606) | StoryBench是一个新的，具有挑战性的多任务基准，用于评估文本到视频模型。它包括动作执行，故事延续和故事生成三个难度逐渐增加的视频生成任务。我们提出了一些小而强大的文本到视频基线，并展示了它们的好处。 |
| [^2] | [Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model.](http://arxiv.org/abs/2308.11601) | Tryage是一个上下文感知的路由系统，能够根据对个体输入提示的分析，从模型库中选择最佳的专家模型，以消除模型选择和定制化的负担，释放庞大的新兴模型库的巨大威力给最终用户。 |
| [^3] | [SeamlessM4T-Massively Multilingual & Multimodal Machine Translation.](http://arxiv.org/abs/2308.11596) | 本文介绍了SeamlessM4T，这是一个支持多语言和多模态机器翻译的模型，通过使用大量语音数据和自监督学习，实现了统一的语音到语音翻译、语音到文本翻译、文本到语音翻译和文本到文本翻译，以及自动语音识别的功能。 |
| [^4] | [UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding.](http://arxiv.org/abs/2308.11592) | UniDoc是一种通用的大型多模态模型，具备文本检测和识别能力，并通过任务之间的有益交互提高每个任务的性能，达到了在多个基准测试中的最先进水平。 |
| [^5] | [Indonesian Automatic Speech Recognition with XLSR-53.](http://arxiv.org/abs/2308.11589) | 本研究使用XLSR-53模型开发了印尼语自动语音识别系统，使用较少的数据量以实现竞争性的词错误率，结果显示模型与使用Common Voice数据集的相似模型具有竞争力。通过使用语言模型，词错误率可以降低约8%。 |
| [^6] | [Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes.](http://arxiv.org/abs/2308.11585) | 本篇论文探讨了因果交叉性和双重梯度下降在多模态分析中的应用，以仇恨迷因检测为例。通过结合因果分析和基于梯度的方法，研究发现模型的内部机制可以揭示其因果效应，并介绍了交叉性和模态的梯度注意力的摘要化方法。 |
| [^7] | [Building Emotional Support Chatbots in the Era of LLMs.](http://arxiv.org/abs/2308.11584) | 本研究利用大型语言模型（LLMs）的能力，通过合成人类见解与计算能力，构建了一个广泛的情感支持对话数据集。最终提出了一种名为ExTES的可扩展情感支持对话数据集，并部署了高级调优技术，以解决情感支持聊天机器人在实际应用中遇到的挑战。 |
| [^8] | [Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models.](http://arxiv.org/abs/2308.11578) | 本论文综合调查了大型语言模型（LLMs）在情绪识别中表现的各个方面，包括上下文学习、少样本学习和准确度等。LLMs的出现为情绪识别建模带来了新的潜力和机会。 |
| [^9] | [Using ChatGPT as a CAT tool in Easy Language translation.](http://arxiv.org/abs/2308.11563) | 本研究探索使用ChatGPT将以公民为导向的行政文本翻译成德语Easy Language的可行性，并分析了生成文本的质量。结果表明生成的文本比标准文本更易理解，但仍未完全符合Easy Language的标准。 |
| [^10] | [BELB: a Biomedical Entity Linking Benchmark.](http://arxiv.org/abs/2308.11537) | BELB是一种生物医学实体链接基准，提供了对7个知识库链接的11个语料库的访问，并涵盖了基因、疾病、化学物质、物种、细胞系和变种等六种实体类型。这项工作解决了生物医学实体链接任务在现有基准中的缺失问题，并为该领域的研究提供了一个统一标准的测试平台。 |
| [^11] | [Large Language Model as a User Simulator.](http://arxiv.org/abs/2308.11534) | 本文创新性地将从真实人机对话中提取的人类问题作为学习目标，并且训练了一个用户模拟器UserGPT，并使用生成的高质量合成对话数据集RealChat来训练助手模型ReaLM。实验证明，ReaLM在多个基准测试中超过了基准模型。 |
| [^12] | [Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law.](http://arxiv.org/abs/2308.11531) | 这项研究的目标是通过使用机器学习来提高难民法决策的智能化和透明度，实现更好的决策结果。研究涉及检索过去的案例和分析加拿大案例数据集的法律决策流程。通过基于自然语言处理的解决方案和新的基准，研究希望为所有相关利益相关者带来包容性和预期的好处。 |
| [^13] | [BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction.](http://arxiv.org/abs/2308.11527) | BERT4CTR是一种高效框架，将预训练语言模型与非文本特征相结合，用于点击率预测。它探索了两种整合多模态输入的方法，并解决了文本和非文本输入之间的交叉信息学习问题。 |
| [^14] | [Learning Representations on Logs for AIOps.](http://arxiv.org/abs/2308.11526) | 这篇论文介绍了一种针对AIOps的学习表示方法，通过训练大型语言模型（LLM）在公共和专有数据上，能够有效应对有限标记数据的日志分析任务。 |
| [^15] | [Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models.](http://arxiv.org/abs/2308.11521) | 这篇论文研究了大型语言模型的越狱问题，并提出了一种自动越狱方法，介绍了语义防火墙的概念和三种技术实现方法。 |
| [^16] | [Exploring the Power of Topic Modeling Techniques in Analyzing Customer Reviews: A Comparative Analysis.](http://arxiv.org/abs/2308.11520) | 本研究对常用主题建模方法进行了综合研究和比较，特别应用于客户评论。我们展示了这些方法在检测重要主题方面的优势，并旨在突出它们的有效性。 |
| [^17] | [Optimizing Multi-Class Text Classification: A Diverse Stacking Ensemble Framework Utilizing Transformers.](http://arxiv.org/abs/2308.11519) | 本研究提出了一种利用转换器的多样堆叠集成框架，以优化多类文本分类。通过将多个单一转换器作为基层分类器，并引入基于RoBERTa的元层分类器，实现了最优的预测模型。 |
| [^18] | [Unsupervised Prototype Adapter for Vision-Language Models.](http://arxiv.org/abs/2308.11507) | 本文介绍了一种无监督的视觉语言模型微调方法，称为无监督原型适配器（UP-Adapter）。该方法利用CLIP的文本-图像对齐能力，针对未标注的目标数据集自动选择自信度最高的样本，并生成类别原型，以实现无监督的微调。 |
| [^19] | [Can Authorship Representation Learning Capture Stylistic Features?.](http://arxiv.org/abs/2308.11490) | 本论文研究了作者身份表征学习能否捕捉文体特征的问题，并通过实验验证了这些表征能够有效地捕捉写作风格的特征。 |
| [^20] | [Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.](http://arxiv.org/abs/2308.11483) | 本文研究了大型语言模型对多选题选项顺序的敏感性。实验证明，当对回答选项进行重新排序时，大型语言模型的性能差距可以达到13%至75%。这种敏感性主要在大型语言模型对前两个/三个选项的预测不确定时出现。 |
| [^21] | [Sentence-Level Multimodal and Language-Agnostic Representations.](http://arxiv.org/abs/2308.11466) | 这项研究引入了SONAR，一个多语言和多模态的句子嵌入空间，通过单一文本编码器在相似性搜索任务中取得显著优势，并提供了用于200种语言的文本解码器，可以进行文本到文本和语音到文本的机器翻译。这些结果相比现有模型具有竞争力，并且对语音到文本模型也取得了良好的结果。 |
| [^22] | [LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models.](http://arxiv.org/abs/2308.11462) | LegalBench是一个协同构建的法律推理基准库，涵盖了162个任务，可用于衡量大型语言模型在法律推理方面的能力，为律师和LLM开发者提供了共同的词汇表。 |
| [^23] | [Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification.](http://arxiv.org/abs/2308.11447) | 本文提出了一种面向方面的观点对齐网络(AOAN)，用于解决基于方面的情感分类中的语义不匹配问题。通过引入邻近区间增强模块和多角度注意机制，该网络能够有效捕捉观点词与相应方面之间的上下文关联。 |
| [^24] | [A Survey on Large Language Model based Autonomous Agents.](http://arxiv.org/abs/2308.11432) | 该论文综述了基于大型语言模型的自主代理的研究，提供了从整体角度对该领域的系统审查，其创新之处在于利用大量网络知识实现人类水平的智能决策。 |
| [^25] | [Extracting Relational Triples Based on Graph Recursive Neural Network via Dynamic Feedback Forest Algorithm.](http://arxiv.org/abs/2308.11411) | 本文提出了一种基于图递归神经网络和动态反馈森林算法的新方法，用于从文本中提取关系三元组，并成功通过实验证明了其有效性。 |
| [^26] | [Convoifilter: A case study of doing cocktail party speech recognition.](http://arxiv.org/abs/2308.11380) | 本文通过使用单声道语音增强模块与ASR模块，成功将ASR的词错误率从80%降低到26.4%，并通过联合微调策略将其进一步降低到14.5%。 |
| [^27] | [M3PS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization in E-commerce.](http://arxiv.org/abs/2308.11351) | M3PS是一种全面的多粒度多模态属性感知产品摘要方法，能够同时建模并生成高质量的产品摘要，解决了电子商务中产品摘要的端到端建模、多粒度多模态建模和多模态属性建模的问题。 |
| [^28] | [LEAP: Efficient and Automated Test Method for NLP Software.](http://arxiv.org/abs/2308.11284) | LEAP是一种用于NLP软件的高效自动化测试方法，它通过结合LEvy飞行的自适应粒子群算法和文本特征来生成对抗性测试用例，解决了现有方法在错误发现能力和时间效率方面的限制。 |
| [^29] | [Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning.](http://arxiv.org/abs/2308.11276) | 本研究提出了一个名为音乐理解LLaMA（MU-LLaMA）的模型，通过应用问答和字幕生成的方法，解决了文本到音乐生成面临的数据稀缺问题。我们设计了一个新的MusicQA数据集，用于训练MU-LLaMA模型，并在音乐问答方面取得了出色的性能。 |
| [^30] | [HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge.](http://arxiv.org/abs/2308.11257) | 本文提出了一种针对多跳问答的自我迭代程序生成框架（HopPG），该框架解决了处理异构知识和多跳问题时所面临的困难，并利用了前几跳的执行结果来生成下一跳的程序。 |
| [^31] | [Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis.](http://arxiv.org/abs/2308.11224) | 本研究评估了四个大型语言模型在图数据上解决分析问题的能力，结果显示LLM在理解图数据、生成正确结果和进行结构推理方面表现出色，但在真实性和矫正能力方面存在一些挑战。 |
| [^32] | [Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries.](http://arxiv.org/abs/2308.11189) | 本文提出了一种基于回应多样性的大型语言模型错误量化指标，这些指标独立于领域特定信息，并与失败概率强相关。实证结果展示了这些指标在少样本提示、思维链推理和错误检测方面的应用。 |
| [^33] | [ViCo: Engaging Video Comment Generation with Human Preference Rewards.](http://arxiv.org/abs/2308.11171) | 本文提出了ViCo，通过三种新颖的设计来解决视频评论生成中的挑战，包括评论的主观性难以量化和评估，以及高质量训练样本的稀缺性。利用人类偏好奖励的方法，ViCo能够生成有趣的视频评论。 |
| [^34] | [LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report).](http://arxiv.org/abs/2308.11148) | 本文提出了LLaMA-Reviewer框架，通过参数高效微调方法，利用流行的大型语言模型LLaMA在代码审查领域能力，实现对代码审查任务的自动化。研究表明，即使仅使用不到1%的可训练参数，该框架仍能取得显著的成果。 |
| [^35] | [NLP-based detection of systematic anomalies among the narratives of consumer complaints.](http://arxiv.org/abs/2308.11138) | 本文开发了一种基于自然语言处理的方法，用于检测消费者投诉叙述中的系统异常。这种方法可以解决分类算法对于较小且频繁出现的系统异常检测的问题，并将投诉叙述转化为定量数据进行分析。 |
| [^36] | [Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models.](http://arxiv.org/abs/2308.11103) | 本研究评估了大型语言模型在重新识别匿名个人方面的能力，并发现模型大小、输入长度和指令调整是最重要的决定因素。 |
| [^37] | [Towards Objective Evaluation of Socially-Situated Conversational Robots: Assessing Human-Likeness through Multimodal User Behaviors.](http://arxiv.org/abs/2308.11020) | 本文提出了一种客观评价社交位置对话机器人的方法，利用多模态用户行为来评估机器人的人类相似度，增强了客观性和可复现性。 |
| [^38] | [Using language models in the implicit automated assessment of mathematical short answer items.](http://arxiv.org/abs/2308.11006) | 这项研究提出了一种使用语言模型的新方法来评估数学简答题的正确性和误解，并通过值识别流程提供针对性反馈。 |
| [^39] | [Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT.](http://arxiv.org/abs/2308.11001) | 本文利用可解释的人工智能方法，分析了研究人员对ChatGPT在不同使用方面的情感。通过提出一种新的面向方面情感分析技术，使得这种分析不受文本数据长度限制，并提供了对新数据集的宝贵洞见。 |
| [^40] | [DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering.](http://arxiv.org/abs/2308.10959) | 本文提出了一个名为DocPrompt的方法，用于处理文档问答任务，具有强大的零样本和少样本性能。实验结果表明，DocPrompt模型经过连续预训练后在文档问答任务中表现优异，大大提高了交付效率和模型性能，降低了注释成本和劳动成本。 |
| [^41] | [WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models.](http://arxiv.org/abs/2308.10755) | 本论文提出了一个名为"Wan Juan"的大规模多模态数据集，包含中英文数据。这个数据集通过各种网络来源采集，包括文本、图像文本和视频模态，总量超过2TB。它被用于训练InternLM模型，该模型在多维度评估中表现出明显优势。 |
| [^42] | [An Effective Method using Phrase Mechanism in Neural Machine Translation.](http://arxiv.org/abs/2308.10482) | 本论文介绍了一种使用短语机制的神经机器翻译方法，PhraseTransformer，在越南语-中文平行语料库上取得了较高的BLEU得分。 |
| [^43] | [LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework.](http://arxiv.org/abs/2308.10390) | 本论文提出了LibriSQA，一个自由形式和开放式的口语问答数据集和框架，通过改进ASR任务并使用轻量级的端到端框架，实现了在LLMs上执行口语问答任务的显著结果。 |
| [^44] | [WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning.](http://arxiv.org/abs/2308.10195) | 本文提出了一种名为WMFormer++的嵌套Transformer模型，通过隐式联合学习实现可见水印的去除。通过整合水印定位和背景恢复任务的信息，该模型能够自主地导航信息流动，并采用交叉通道注意力来促进定位和恢复的过程。 |
| [^45] | [Taken by Surprise: Contrast effect for Similarity Scores.](http://arxiv.org/abs/2308.09765) | 提出了一种新的相似度度量方法，称为“惊喜分数”，该方法能够考虑对象的上下文信息并显著提高零样本和少样本文档分类任务的性能。 |
| [^46] | [MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models.](http://arxiv.org/abs/2308.09729) | 本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。 |
| [^47] | [PMET: Precise Model Editing in a Transformer.](http://arxiv.org/abs/2308.08742) | 该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。 |
| [^48] | [Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits.](http://arxiv.org/abs/2308.03212) | 本文研究表明平均困难注意力变换器和对数精度变换器都可以模拟常深度阈值电路，其中后者由于生成统一的电路族而更健壮。 |
| [^49] | [LARCH: Large Language Model-based Automatic Readme Creation with Heuristics.](http://arxiv.org/abs/2308.03099) | LARCH是一种基于大型语言模型的自动readme创建方法，通过识别代表性代码和启发式方法，能够生成连贯和事实正确的readme，优于不依赖代表性代码识别的基线算法。 |
| [^50] | [Exploring the Landscape of Natural Language Processing Research.](http://arxiv.org/abs/2307.10652) | 该论文系统分类和分析了ACL Anthology中的研究论文，提供了对研究领域的结构化概述和NLP领域的分类学。本研究总结了最新的NLP发展，并提出了未来工作的方向。 |
| [^51] | [Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs.](http://arxiv.org/abs/2307.07312) | 本论文通过使用大型语言模型，实现了基于图数据的零-shot自然语言生成。实验结果表明，该方法在部分指标上接近最新技术水平，在事实、反事实和虚构陈述的对比中也有显著的关联。 |
| [^52] | [Survey on Sociodemographic Bias in Natural Language Processing.](http://arxiv.org/abs/2306.08158) | 本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。 |
| [^53] | [Blockwise Parallel Transformer for Long Context Large Models.](http://arxiv.org/abs/2305.19370) | 本文提出了块级并行Transformer方法，以最小化内存成本，能够处理长序列，并且可以处理比先前的内存高效方法更长32倍的训练序列。 |
| [^54] | [NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification.](http://arxiv.org/abs/2305.10971) | 本论文提出了一个新的数据集NollySenti，用于不同领域适应的情感分类任务，基于尼日利亚五种常用语言的诺利木电影评论。研究表明，从具有相同目标域的英语进行迁移可以提高5％以上的准确性。 |
| [^55] | [Fairness in Image Search: A Study of Occupational Stereotyping in Image Retrieval and its Debiasing.](http://arxiv.org/abs/2305.03881) | 本文针对职业模式刻板印象问题，研究了网络搜索中的偏见和公平性问题。实验表明当前的图像搜索引擎存在相当严重的职业模式刻板印象，提出了一种去偏见方法以减轻此类偏见并提高图像搜索引擎的公平性。 |
| [^56] | [Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models.](http://arxiv.org/abs/2304.01852) | 本文全面介绍了最先进的大型语言模型ChatGPT和GPT-4，包括其在各个领域的前景应用，并着重介绍了大规模预训练、指令微调和人类反馈的强化学习创新。ChatGPT/GPT-4在自然语言处理应用方面表现突出，同时在其他领域也具有潜力。 |
| [^57] | [TrojText: Test-time Invisible Textual Trojan Insertion.](http://arxiv.org/abs/2303.02242) | 本文提出了一种名为TrojText的解决方案，旨在确定无需训练数据是否可以更高效、更经济地进行隐形文本特洛伊攻击。 |
| [^58] | [Truveta Mapper: A Zero-shot Ontology Alignment Framework.](http://arxiv.org/abs/2301.09767) | 提出了一个将无监督本体匹配或本体对齐视为翻译任务的新视角的Truveta Mapper框架，在零样本、统一和端到端的方式下执行多本体对齐。该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案，无需显式跨本体手动标注数据。 |
| [^59] | [A Measure-Theoretic Characterization of Tight Language Models.](http://arxiv.org/abs/2212.10502) | 本论文使用测度论的方法对语言建模进行了特征化，证明了很多流行的语言模型家族是紧密的，不会存在泄漏现象。 |
| [^60] | [Measuring Social Biases in Grounded Vision and Language Embeddings.](http://arxiv.org/abs/2002.08911) | 该论文推广了社会偏见的概念，从语言嵌入扩展到了基于图像和语言的嵌入。研究表明，基于图像和语言的嵌入中的偏见与未经培训的嵌入中的偏见同等重要甚至更重要。并通过引入新的度量方法来研究偏见、语言和视觉之间的交互作用。 |

# 详细

[^1]: StoryBench: 一个多方面的连续故事可视化基准

    StoryBench: A Multifaceted Benchmark for Continuous Story Visualization. (arXiv:2308.11606v1 [cs.CV])

    [http://arxiv.org/abs/2308.11606](http://arxiv.org/abs/2308.11606)

    StoryBench是一个新的，具有挑战性的多任务基准，用于评估文本到视频模型。它包括动作执行，故事延续和故事生成三个难度逐渐增加的视频生成任务。我们提出了一些小而强大的文本到视频基线，并展示了它们的好处。

    

    从文本提示生成视频故事是一项复杂的任务。除了具有高质量的视觉效果外，视频还需要在整个帧中保持与文本提示序列的一致。创建视频生成的基准需要在时间上对数据进行注释，这与视频数据集中经常使用的单个标题形成对比。为了填补这一空白，我们收集了三个现有数据集上的全面的人类注释，并推出了StoryBench：一个新的，具有挑战性的多任务基准，可可靠地评估即将发布的文本到视频模型。我们的基准包括三个难度逐渐增加的视频生成任务：动作执行，在从一个条件视频开始生成下一个动作；故事延续，在从一个条件视频开始执行一系列动作；故事生成，仅从文本提示中生成一个视频。我们评估了一些小而强大的文本到视频基线，并展示了它们的好处。

    Generating video stories from text prompts is a complex task. In addition to having high visual quality, videos need to realistically adhere to a sequence of text prompts whilst being consistent throughout the frames. Creating a benchmark for video generation requires data annotated over time, which contrasts with the single caption used often in video datasets. To fill this gap, we collect comprehensive human annotations on three existing datasets, and introduce StoryBench: a new, challenging multi-task benchmark to reliably evaluate forthcoming text-to-video models. Our benchmark includes three video generation tasks of increasing difficulty: action execution, where the next action must be generated starting from a conditioning video; story continuation, where a sequence of actions must be executed starting from a conditioning video; and story generation, where a video must be generated from only text prompts. We evaluate small yet strong text-to-video baselines, and show the benefit
    
[^2]: Tryage: 实时智能路由用户提示到大型语言模型

    Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])

    [http://arxiv.org/abs/2308.11601](http://arxiv.org/abs/2308.11601)

    Tryage是一个上下文感知的路由系统，能够根据对个体输入提示的分析，从模型库中选择最佳的专家模型，以消除模型选择和定制化的负担，释放庞大的新兴模型库的巨大威力给最终用户。

    

    变压器架构和自注意机制的引入导致了在特定下游任务和数据领域训练的语言模型的爆炸性增长。在Hugging Face生态系统中有超过200,000个模型，用户在选择和优化模型以适应多方面的工作流程和数据领域的同时，还要解决计算、安全和时效性等问题。迫切需要机器学习框架来消除模型选择和定制化的负担，并释放庞大的新兴模型库的巨大威力给最终用户。在这里，我们提出了一个上下文感知的路由系统Tryage，它利用语言模型路由器根据对个体输入提示的分析，从模型库中选择最佳的专家模型。受大脑中的丘脑路由器启发，Tryage采用感知路由器来预测下游模型在提示上的性能，并根据目标做出路由决策。

    The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an ob
    
[^3]: SeamlessM4T-大规模多语言和多模态机器翻译

    SeamlessM4T-Massively Multilingual & Multimodal Machine Translation. (arXiv:2308.11596v1 [cs.CL])

    [http://arxiv.org/abs/2308.11596](http://arxiv.org/abs/2308.11596)

    本文介绍了SeamlessM4T，这是一个支持多语言和多模态机器翻译的模型，通过使用大量语音数据和自监督学习，实现了统一的语音到语音翻译、语音到文本翻译、文本到语音翻译和文本到文本翻译，以及自动语音识别的功能。

    

    创造一种类似于巴别鱼的工具，能够帮助个人在任意两种语言之间进行语音翻译，需要付出什么样的努力？虽然最近在基于文本的模型方面取得了突破，使机器翻译的覆盖范围超过了200种语言，但统一的语音到语音翻译模型还没有取得类似的进展。更具体地说，传统的语音到语音翻译系统依赖于渐进式的级联系统进行翻译，使高性能的统一系统难以实现。为了弥补这些差距，我们引入了SeamlessM4T，一种支持语音到语音翻译、语音到文本翻译、文本到语音翻译、文本到文本翻译以及自动语音识别的单一模型，支持多达100种语言。为了构建这个模型，我们使用了100万小时的开放式语音音频数据，使用了w2v-BERT 2.0来学习自监督的语音表示。随后，我们创建了一个多模态的自动对齐语音翻译的语料库。

    What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages? While recent breakthroughs in text-based models have pushed machine translation coverage beyond 200 languages, unified speech-to-speech translation models have yet to achieve similar strides. More specifically, conventional speech-to-speech translation systems rely on cascaded systems that perform translation progressively, putting high-performing unified systems out of reach. To address these gaps, we introduce SeamlessM4T, a single model that supports speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition for up to 100 languages. To build this, we used 1 million hours of open speech audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with h
    
[^4]: UniDoc: 一种通用的大型多模态模型，用于同时进行文本检测、识别、定位和理解

    UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding. (arXiv:2308.11592v1 [cs.AI])

    [http://arxiv.org/abs/2308.11592](http://arxiv.org/abs/2308.11592)

    UniDoc是一种通用的大型多模态模型，具备文本检测和识别能力，并通过任务之间的有益交互提高每个任务的性能，达到了在多个基准测试中的最先进水平。

    

    在大语言模型（LLMs）时代，多模态理解领域取得了巨大的进展。然而，现有的高级算法受限于有效利用大型预训练模型所固有的巨大表示能力和丰富的世界知识，并且在文本丰富场景中任务之间的有益连接尚未充分探索。在这项工作中，我们介绍了UniDoc，一种新颖的多模态模型，具备现有方法所缺乏的文本检测和识别能力。此外，UniDoc利用任务之间的有益交互来提高每个单独任务的性能。为了实现UniDoc，我们对贡献的大规模指令跟随数据集进行了统一的多模态指导调优。定量和定性实验结果表明，UniDoc在多个具有挑战性的基准测试中取得了最先进的分数。

    In the era of Large Language Models (LLMs), tremendous strides have been made in the field of multimodal understanding. However, existing advanced algorithms are limited to effectively utilizing the immense representation capabilities and rich world knowledge inherent to these large pre-trained models, and the beneficial connections among tasks within the context of text-rich scenarios have not been sufficiently explored. In this work, we introduce UniDoc, a novel multimodal model equipped with text detection and recognition capabilities, which are deficient in existing approaches. Moreover, UniDoc capitalizes on the beneficial interactions among tasks to enhance the performance of each individual task. To implement UniDoc, we perform unified multimodal instruct tuning on the contributed large-scale instruction following datasets. Quantitative and qualitative experimental results show that UniDoc sets state-of-the-art scores across multiple challenging benchmarks. To the best of our kn
    
[^5]: 印尼语自动语音识别与XLSR-53模型

    Indonesian Automatic Speech Recognition with XLSR-53. (arXiv:2308.11589v1 [cs.CL])

    [http://arxiv.org/abs/2308.11589](http://arxiv.org/abs/2308.11589)

    本研究使用XLSR-53模型开发了印尼语自动语音识别系统，使用较少的数据量以实现竞争性的词错误率，结果显示模型与使用Common Voice数据集的相似模型具有竞争力。通过使用语言模型，词错误率可以降低约8%。

    

    本研究侧重于利用XLSR-53预训练模型开发印尼语的自动语音识别（ASR）系统。XLSR代表跨语言语音表示。使用XLSR-53预训练模型的目的是显著减少在非英语语言下获得竞争性词错误率（WER）所需的训练数据量。本研究使用共计24小时18分钟1秒的数据，其中包括：（1）14小时31分钟的TITML-IDN数据；（2）3小时33分钟的Magic Data数据；以及（3）6小时14分钟1秒的Common Voice数据。在WER为20%的情况下，本研究构建的模型能够与使用Common Voice数据集分割测试的类似模型竞争。通过使用语言模型，WER可以降低约8%，使其从20%降至12%。因此，本研究的结果成功完善了之前的研究，为创造一个更好的印尼语自动语音识别系统提供了更少的数据支持。

    This study focuses on the development of Indonesian Automatic Speech Recognition (ASR) using the XLSR-53 pre-trained model, the XLSR stands for cross-lingual speech representations. The use of this XLSR-53 pre-trained model is to significantly reduce the amount of training data in non-English languages required to achieve a competitive Word Error Rate (WER). The total amount of data used in this study is 24 hours, 18 minutes, and 1 second: (1) TITML-IDN 14 hours and 31 minutes; (2) Magic Data 3 hours and 33 minutes; and (3) Common Voice 6 hours, 14 minutes, and 1 second. With a WER of 20%, the model built in this study can compete with similar models using the Common Voice dataset split test. WER can be decreased by around 8% using a language model, resulted in WER from 20% to 12%. Thus, the results of this study have succeeded in perfecting previous research in contributing to the creation of a better Indonesian ASR with a smaller amount of data.
    
[^6]: 因果交叉性和双重梯度下降在多模态分析中的应用：以仇恨迷因为例（arXiv:2308.11585v1 [cs.AI]）

    Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes. (arXiv:2308.11585v1 [cs.AI])

    [http://arxiv.org/abs/2308.11585](http://arxiv.org/abs/2308.11585)

    本篇论文探讨了因果交叉性和双重梯度下降在多模态分析中的应用，以仇恨迷因检测为例。通过结合因果分析和基于梯度的方法，研究发现模型的内部机制可以揭示其因果效应，并介绍了交叉性和模态的梯度注意力的摘要化方法。

    

    随着机器学习（ML）的爆炸性增长，特别是在新兴的大语言模型（LLM）的背景下，理解其内部工作中的语义意义至关重要。虽然因果分析侧重于定义语义及其量化，基于梯度的方法是可解释的人工智能（XAI）的核心，用于解释黑盒子的解释。通过协同这些方法，探索模型的内部机制如何阐明其因果效应已成为基于证据的决策的必要条件。一系列并行的研究表明，交叉性--个体的多个人口统计学因素的组合影响--可以以平均处理效应（ATE）的形式进行结构化。最初，本研究阐述了仇恨迷因检测问题可以作为一个ATE来描述，借助交叉性原则，以及基于模态的梯度注意力的摘要化。

    In the wake of the explosive growth of machine learning (ML) usage, particularly within the context of emerging Large Language Models (LLMs), comprehending the semantic significance rooted in their internal workings is crucial. While causal analyses focus on defining semantics and its quantification, the gradient-based approach is central to explainable AI (XAI), tackling the interpretation of the black box. By synergizing these approaches, the exploration of how a model's internal mechanisms illuminate its causal effect has become integral for evidence-based decision-making. A parallel line of research has revealed that intersectionality - the combinatory impact of multiple demographics of an individual - can be structured in the form of an Averaged Treatment Effect (ATE). Initially, this study illustrates that the hateful memes detection problem can be formulated as an ATE, assisted by the principles of intersectionality, and that a modality-wise summarization of gradient-based atten
    
[^7]: 在LLMs时代构建情感支持聊天机器人

    Building Emotional Support Chatbots in the Era of LLMs. (arXiv:2308.11584v1 [cs.CL])

    [http://arxiv.org/abs/2308.11584](http://arxiv.org/abs/2308.11584)

    本研究利用大型语言模型（LLMs）的能力，通过合成人类见解与计算能力，构建了一个广泛的情感支持对话数据集。最终提出了一种名为ExTES的可扩展情感支持对话数据集，并部署了高级调优技术，以解决情感支持聊天机器人在实际应用中遇到的挑战。

    

    将情感支持集成到各种对话场景中具有深远的社会效益，如社交互动、心理健康咨询和客户服务。然而，在这一领域存在一些未解决的挑战，包括有限的数据可用性和缺乏被广泛接受的模型训练范例。本研究通过利用大型语言模型（LLMs）的能力，试图解决这些挑战。我们引入一种创新方法，将人类见解与LLMs的计算能力相结合，构建了一个广泛的情感支持对话数据集。我们的方法从精心设计的涵盖多种场景的对话集合作为生成种子开始。通过利用ChatGPT的上下文学习潜力，我们递归地生成了一个可扩展的情感支持对话数据集，命名为ExTES。在此基础上，我们对LLaMA模型进行了高级调优技术的部署，检验了其性能。

    The integration of emotional support into various conversational scenarios presents profound societal benefits, such as social interactions, mental health counseling, and customer service. However, there are unsolved challenges that hinder real-world applications in this field, including limited data availability and the absence of well-accepted model training paradigms. This work endeavors to navigate these challenges by harnessing the capabilities of Large Language Models (LLMs). We introduce an innovative methodology that synthesizes human insights with the computational prowess of LLMs to curate an extensive emotional support dialogue dataset. Our approach is initiated with a meticulously designed set of dialogues spanning diverse scenarios as generative seeds. By utilizing the in-context learning potential of ChatGPT, we recursively generate an ExTensible Emotional Support dialogue dataset, named ExTES. Following this, we deploy advanced tuning techniques on the LLaMA model, exami
    
[^8]: 改变情绪识别建模方式:通用大型模型的出现

    Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models. (arXiv:2308.11578v1 [cs.CL])

    [http://arxiv.org/abs/2308.11578](http://arxiv.org/abs/2308.11578)

    本论文综合调查了大型语言模型（LLMs）在情绪识别中表现的各个方面，包括上下文学习、少样本学习和准确度等。LLMs的出现为情绪识别建模带来了新的潜力和机会。

    

    在情绪识别或情感计算的诞生之后，由于其广泛应用，它已经成为一个越来越活跃的研究课题。在过去的几十年里，情绪识别模型逐渐从统计浅层模型迁移到基于神经网络的深度模型，可以显著提升情绪识别模型的性能，并在不同的基准测试中始终取得最佳结果。因此，近年来，深度模型一直被视为情绪识别的首选。然而，大型语言模型（LLMs）的出现，如ChatGPT，由于它们具备的零/少样本学习、上下文学习、连贯思维等能力，在情绪识别方面引起了巨大的惊讶，而这些能力在以前的深度模型中从未出现。在本文中，我们全面调查了LLMs在情绪识别方面的表现，包括上下文学习、少样本学习、准确度等各方面。

    After the inception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, acc
    
[^9]: 使用ChatGPT作为Easy Language翻译中的CAT工具

    Using ChatGPT as a CAT tool in Easy Language translation. (arXiv:2308.11563v1 [cs.CL])

    [http://arxiv.org/abs/2308.11563](http://arxiv.org/abs/2308.11563)

    本研究探索使用ChatGPT将以公民为导向的行政文本翻译成德语Easy Language的可行性，并分析了生成文本的质量。结果表明生成的文本比标准文本更易理解，但仍未完全符合Easy Language的标准。

    

    本研究旨在探讨使用ChatGPT将以公民为导向的行政文本翻译成德语Easy Language的可行性，Easy Language是一种简化的、受控的语言变体，适应了阅读障碍人士的需求。我们使用ChatGPT通过两种策略，即语言和整体，翻译来自德国公共机构网站的选定文本。我们基于正确性、可读性和句法复杂度等不同标准分析生成文本的质量。结果表明生成的文本比标准文本更容易理解，但仍未完全符合Easy Language的标准。此外，内容并不总是正确传达。

    This study sets out to investigate the feasibility of using ChatGPT to translate citizen-oriented administrative texts into German Easy Language, a simplified, controlled language variety that is adapted to the needs of people with reading impairments. We use ChatGPT to translate selected texts from websites of German public authorities using two strategies, i.e. linguistic and holistic. We analyse the quality of the generated texts based on different criteria, such as correctness, readability, and syntactic complexity. The results indicated that the generated texts are easier than the standard texts, but that they still do not fully meet the established Easy Language standards. Additionally, the content is not always rendered correctly.
    
[^10]: BELB: 一种生物医学实体链接基准

    BELB: a Biomedical Entity Linking Benchmark. (arXiv:2308.11537v1 [cs.CL])

    [http://arxiv.org/abs/2308.11537](http://arxiv.org/abs/2308.11537)

    BELB是一种生物医学实体链接基准，提供了对7个知识库链接的11个语料库的访问，并涵盖了基因、疾病、化学物质、物种、细胞系和变种等六种实体类型。这项工作解决了生物医学实体链接任务在现有基准中的缺失问题，并为该领域的研究提供了一个统一标准的测试平台。

    

    生物医学实体链接（BEL）是将实体提及与知识库对齐的任务。它在生命科学文献的信息提取流程中起着重要作用。我们回顾了该领域的最新研究，并发现由于现有的生物医学文本挖掘基准中缺乏该任务，不同的研究采用了不同的实验设置，使得基于已发表数据的比较存在问题。此外，神经系统主要在与广泛覆盖的知识库UMLS链接的实例上进行测试，对于更专门的知识库，如基因或变种，其性能研究较少。因此，我们开发了BELB，一种用统一格式提供对7个知识库链接的11个语料库的访问，并涵盖了基因、疾病、化学物质、物种、细胞系和变种六种实体类型的生物医学实体链接基准。BELB大大减少了在多个语料库上测试BEL系统的预处理开销，为可重复实验提供了标准化的测试平台。

    Biomedical entity linking (BEL) is the task of grounding entity mentions to a knowledge base. It plays a vital role in information extraction pipelines for the life sciences literature. We review recent work in the field and find that, as the task is absent from existing benchmarks for biomedical text mining, different studies adopt different experimental setups making comparisons based on published numbers problematic. Furthermore, neural systems are tested primarily on instances linked to the broad coverage knowledge base UMLS, leaving their performance to more specialized ones, e.g. genes or variants, understudied. We therefore developed BELB, a Biomedical Entity Linking Benchmark, providing access in a unified format to 11 corpora linked to 7 knowledge bases and spanning six entity types: gene, disease, chemical, species, cell line and variant. BELB greatly reduces preprocessing overhead in testing BEL systems on multiple corpora offering a standardized testbed for reproducible exp
    
[^11]: 作为用户模拟器的大型语言模型

    Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])

    [http://arxiv.org/abs/2308.11534](http://arxiv.org/abs/2308.11534)

    本文创新性地将从真实人机对话中提取的人类问题作为学习目标，并且训练了一个用户模拟器UserGPT，并使用生成的高质量合成对话数据集RealChat来训练助手模型ReaLM。实验证明，ReaLM在多个基准测试中超过了基准模型。

    

    闭源ChatGPT的卓越性能引发了对其民主化的努力，借助真实用户和ChatGPT对话的努力取得了显著进展，Vicuna是一个很好的例子。然而，目前的Baize和UltraChat等努力主要依靠ChatGPT根据指令模拟人类行为，而不是真实的人类学习，导致范围有限，多样性减弱，缺乏真正的多轮对话动态。为了解决上述问题，我们创新性地把从真实人机对话中提取的人类问题作为学习目标，并训练一个用户模拟器UserGPT来生成高质量的以人为中心的合成对话数据集RealChat。随后，该数据集训练我们的助手模型ReaLM。实验证明，ReaLM在Vicuna-Bench和MT-Bench中均超过了基准模型。

    The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
    
[^12]: 授权难民申请者及其律师：使用机器学习研究难民法决策

    Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law. (arXiv:2308.11531v1 [cs.CL])

    [http://arxiv.org/abs/2308.11531](http://arxiv.org/abs/2308.11531)

    这项研究的目标是通过使用机器学习来提高难民法决策的智能化和透明度，实现更好的决策结果。研究涉及检索过去的案例和分析加拿大案例数据集的法律决策流程。通过基于自然语言处理的解决方案和新的基准，研究希望为所有相关利益相关者带来包容性和预期的好处。

    

    我们的项目旨在通过数据驱动的智能来帮助和支持难民地位审理的相关利益相关者，如律师、法官、管理机构和申请人，以便更好地做出决策，并增加难民申请流程的理解和透明度。这个博士项目有两个主要目标：（1）检索过去的案例，（2）在加拿大案例数据集上分析法律决策流程。在本文中，我们介绍了我们工作的当前状态，包括对（1）部分的完成试验和与（2）部分相关的持续努力。我们相信基于自然语言处理的解决方案很适合应对这些挑战，并研究了自动化所涉及的所有步骤的可行性。此外，我们介绍了未来难民法自然语言处理研究的新基准。我们的方法旨在包容所有最终用户和利益相关者，并带来预期的好处，包括减少决策时间，更公平地决策过程。

    Our project aims at helping and supporting stakeholders in refugee status adjudications, such as lawyers, judges, governing bodies, and claimants, in order to make better decisions through data-driven intelligence and increase the understanding and transparency of the refugee application process for all involved parties. This PhD project has two primary objectives: (1) to retrieve past cases, and (2) to analyze legal decision-making processes on a dataset of Canadian cases. In this paper, we present the current state of our work, which includes a completed experiment on part (1) and ongoing efforts related to part (2). We believe that NLP-based solutions are well-suited to address these challenges, and we investigate the feasibility of automating all steps involved. In addition, we introduce a novel benchmark for future NLP research in refugee law. Our methodology aims to be inclusive to all end-users and stakeholders, with expected benefits including reduced time-to-decision, fairer a
    
[^13]: BERT4CTR:一种将预训练语言模型与非文本特征相结合用于点击率预测的高效框架

    BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction. (arXiv:2308.11527v1 [cs.CL])

    [http://arxiv.org/abs/2308.11527](http://arxiv.org/abs/2308.11527)

    BERT4CTR是一种高效框架，将预训练语言模型与非文本特征相结合，用于点击率预测。它探索了两种整合多模态输入的方法，并解决了文本和非文本输入之间的交叉信息学习问题。

    

    虽然深度预训练语言模型在许多工业场景中显示出了很好的效益，包括点击率（CTR）预测，但如何将只处理文本信号的预训练语言模型与具有非文本特征的预测流程相结合是一个具有挑战性的问题。目前有两个方向来整合多模态输入并进行预训练语言模型的微调。一个方向是通过聚合层将语言模型和非文本特征的结果进行融合，形成集成框架，其中文本和非文本输入之间的交叉信息仅在聚合层中学习。另一个方向是将非文本特征分割成细粒度片段，并将这些片段转换为与文本片段相结合的新标记，以便可以直接输入到语言模型的Transformer层中。然而，这种方法增加了学习和推断的复杂性。

    Although deep pre-trained language models have shown promising benefit in a large set of industrial scenarios, including Click-Through-Rate (CTR) prediction, how to integrate pre-trained language models that handle only textual signals into a prediction pipeline with non-textual features is challenging.  Up to now two directions have been explored to integrate multi-modal inputs in fine-tuning of pre-trained language models. One consists of fusing the outcome of language models and non-textual features through an aggregation layer, resulting into ensemble framework, where the cross-information between textual and non-textual inputs are only learned in the aggregation layer. The second one consists of splitting non-textual features into fine-grained fragments and transforming the fragments to new tokens combined with textual ones, so that they can be fed directly to transformer layers in language models. However, this approach increases the complexity of the learning and inference becau
    
[^14]: 在AIOps上学习日志表示

    Learning Representations on Logs for AIOps. (arXiv:2308.11526v1 [cs.CL])

    [http://arxiv.org/abs/2308.11526](http://arxiv.org/abs/2308.11526)

    这篇论文介绍了一种针对AIOps的学习表示方法，通过训练大型语言模型（LLM）在公共和专有数据上，能够有效应对有限标记数据的日志分析任务。

    

    AI for IT Operations (AIOps)是一种功能强大的平台，Site Reliability Engineers (SREs)可以使用它来在最小的人工干预下自动化和优化操作工作流程。自动化日志分析是AIOps中的重要任务，因为它为SREs提供了关键洞察力，使其能够识别和解决持续的故障。日志格式检测、日志分类和日志解析等任务是自动化日志分析的关键组件。大部分这些任务需要有监督学习；然而，由于有限的带标签日志数据和日志数据的多样性，存在多个挑战。大型语言模型（LLMs）如BERT和GPT3使用自我监督训练了大量未标记数据。这些模型提供了广义表示，可以有效地用于利用有限标记数据的各种下游任务。在科学和生物学等特定领域的LLMs的成功的基础上，本文介绍了一种针对日志数据的LLM，它是在公共和专有数据上进行训练的。

    AI for IT Operations (AIOps) is a powerful platform that Site Reliability Engineers (SREs) use to automate and streamline operational workflows with minimal human intervention. Automated log analysis is a critical task in AIOps as it provides key insights for SREs to identify and address ongoing faults. Tasks such as log format detection, log classification, and log parsing are key components of automated log analysis. Most of these tasks require supervised learning; however, there are multiple challenges due to limited labelled log data and the diverse nature of log data. Large Language Models (LLMs) such as BERT and GPT3 are trained using self-supervision on a vast amount of unlabeled data. These models provide generalized representations that can be effectively used for various downstream tasks with limited labelled data. Motivated by the success of LLMs in specific domains like science and biology, this paper introduces a LLM for log data which is trained on public and proprietary 
    
[^15]: 自我欺骗：逆向破解大型语言模型的语义防火墙

    Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])

    [http://arxiv.org/abs/2308.11521](http://arxiv.org/abs/2308.11521)

    这篇论文研究了大型语言模型的越狱问题，并提出了一种自动越狱方法，介绍了语义防火墙的概念和三种技术实现方法。

    

    大型语言模型（LLM），如ChatGPT，具有接近人工通用智能的惊人能力。虽然为各种社会需求提供了便利，但LLM也降低了生成有害内容的成本。因此，LLM开发人员已经部署了语义级的防御机制，用于识别和拒绝可能导致不适当内容的提示。不幸的是，这些防御机制并不完全可靠，一些攻击者已经设计出了“越狱”提示，临时使LLM忘记内容防御规则并回答任何不适当的问题。迄今为止，业界和学术界尚无关于这些语义级攻击和防御原则的明确解释。本文研究了LLM越狱问题，并首次提出了一种自动越狱方法。我们提出了语义防火墙的概念，并提供了三种技术实现方法。

    Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.  This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates trad
    
[^16]: 探索主题建模技术在分析客户评论中的应用：一项比较分析

    Exploring the Power of Topic Modeling Techniques in Analyzing Customer Reviews: A Comparative Analysis. (arXiv:2308.11520v1 [cs.CL])

    [http://arxiv.org/abs/2308.11520](http://arxiv.org/abs/2308.11520)

    本研究对常用主题建模方法进行了综合研究和比较，特别应用于客户评论。我们展示了这些方法在检测重要主题方面的优势，并旨在突出它们的有效性。

    

    在线社交网络平台和应用的指数级增长导致了用户生成的文本内容（包括评论和评价）数量的激增。因此，用户通常在从这些内容中提取有价值的见解或相关信息时面临困难。为了解决这一挑战，机器学习和自然语言处理算法已被用于分析在线可获得的大量文本数据。近年来，主题建模技术在这个领域中取得了显著的流行度。在本研究中，我们全面地研究并比较了五种经常使用的主题建模方法，特别是应用于客户评论的方法。所研究的方法包括潜在语义分析（LSA）、潜在狄利克雷分配（LDA）、非负矩阵分解（NMF）、彩球推理模型（PAM）、Top2Vec和BERTopic。通过实际展示它们在检测重要主题方面的优势，我们旨在突出它们的有效性。

    The exponential growth of online social network platforms and applications has led to a staggering volume of user-generated textual content, including comments and reviews. Consequently, users often face difficulties in extracting valuable insights or relevant information from such content. To address this challenge, machine learning and natural language processing algorithms have been deployed to analyze the vast amount of textual data available online. In recent years, topic modeling techniques have gained significant popularity in this domain. In this study, we comprehensively examine and compare five frequently used topic modeling methods specifically applied to customer reviews. The methods under investigation are latent semantic analysis (LSA), latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF), pachinko allocation model (PAM), Top2Vec, and BERTopic. By practically demonstrating their benefits in detecting important topics, we aim to highlight their effica
    
[^17]: 优化多类文本分类：利用转换器的多样堆叠集成框架

    Optimizing Multi-Class Text Classification: A Diverse Stacking Ensemble Framework Utilizing Transformers. (arXiv:2308.11519v1 [cs.CL])

    [http://arxiv.org/abs/2308.11519](http://arxiv.org/abs/2308.11519)

    本研究提出了一种利用转换器的多样堆叠集成框架，以优化多类文本分类。通过将多个单一转换器作为基层分类器，并引入基于RoBERTa的元层分类器，实现了最优的预测模型。

    

    客户评论在评估客户满意度、收集反馈和推动业务改进方面起着至关重要的作用。分析这些评论可以为客户情绪提供有价值的见解，包括赞美、评论和建议。文本分类技术使企业能够将客户评论分为不同的类别，为更好地了解客户反馈提供便利。然而，过拟合和偏见等挑战限制了单个分类器在确保最佳预测方面的有效性。本研究提出了一种新的方法来解决这些挑战，通过引入基于转换器模型的堆叠集成多文本分类方法。通过将多个单一转换器（包括BERT、ELECTRA和DistilBERT）作为基层分类器，以及基于RoBERTa的元层分类器，生成一个最优的预测模型。

    Customer reviews play a crucial role in assessing customer satisfaction, gathering feedback, and driving improvements for businesses. Analyzing these reviews provides valuable insights into customer sentiments, including compliments, comments, and suggestions. Text classification techniques enable businesses to categorize customer reviews into distinct categories, facilitating a better understanding of customer feedback. However, challenges such as overfitting and bias limit the effectiveness of a single classifier in ensuring optimal prediction. This study proposes a novel approach to address these challenges by introducing a stacking ensemble-based multi-text classification method that leverages transformer models. By combining multiple single transformers, including BERT, ELECTRA, and DistilBERT, as base-level classifiers, and a meta-level classifier based on RoBERTa, an optimal predictive model is generated. The proposed stacking ensemble-based multi-text classification method aims
    
[^18]: 无监督原型适配器用于视觉语言模型

    Unsupervised Prototype Adapter for Vision-Language Models. (arXiv:2308.11507v1 [cs.CV])

    [http://arxiv.org/abs/2308.11507](http://arxiv.org/abs/2308.11507)

    本文介绍了一种无监督的视觉语言模型微调方法，称为无监督原型适配器（UP-Adapter）。该方法利用CLIP的文本-图像对齐能力，针对未标注的目标数据集自动选择自信度最高的样本，并生成类别原型，以实现无监督的微调。

    

    最近，大规模预训练的视觉语言模型（如CLIP和ALIGN）在获取可转移的视觉表示方面显示出了显著的有效性。为了利用这些模型中编码的宝贵知识用于下游任务，已经开发了几种微调方法，包括提示调整方法和适配器方法，以有效地适应视觉语言模型的监督。然而，这些方法依赖于可获得的标注样本，这可能耗时且费力，从而限制了可扩展性。为了解决这个问题，在这项工作中，我们设计了一种无监督的视觉语言模型微调方法，称为无监督原型适配器（UP-Adapter）。具体而言，对于未标注的目标数据集，我们利用CLIP的文本-图像对齐能力自动选择每个类别的最自信样本。利用这些选择的样本，我们生成类别原型，这将为无监督的微调提供指导。

    Recently, large-scale pre-trained vision-language models (e.g. CLIP and ALIGN) have demonstrated remarkable effectiveness in acquiring transferable visual representations. To leverage the valuable knowledge encoded within these models for downstream tasks, several fine-tuning approaches, including prompt tuning methods and adapter-based methods, have been developed to adapt vision-language models effectively with supervision. However, these methods rely on the availability of annotated samples, which can be labor-intensive and time-consuming to acquire, thus limiting scalability. To address this issue, in this work, we design an unsupervised fine-tuning approach for vision-language models called Unsupervised Prototype Adapter (UP-Adapter). Specifically, for the unannotated target datasets, we leverage the text-image aligning capability of CLIP to automatically select the most confident samples for each class. Utilizing these selected samples, we generate class prototypes, which serve a
    
[^19]: 作者身份表征学习能够捕捉文体特征吗？

    Can Authorship Representation Learning Capture Stylistic Features?. (arXiv:2308.11490v1 [cs.CL])

    [http://arxiv.org/abs/2308.11490](http://arxiv.org/abs/2308.11490)

    本论文研究了作者身份表征学习能否捕捉文体特征的问题，并通过实验验证了这些表征能够有效地捕捉写作风格的特征。

    

    在计算语言学中，自动将作者的风格从其写作内容中分离出来是一个长期存在且可能不可解决的问题。同时，最近有大量带有作者标签的文本语料库可用，使得以纯数据驱动的方式学习作者身份表征成为可能，用于作者归属这一任务，该任务显然更多地依赖于编码写作风格而不是编码内容。然而，对这一替代任务的成功并不能确保这些表征能够捕捉写作风格，因为作者身份也可能与其他潜在变量（如主题）相关。为了更好地理解这些表征所传递的信息的本质，特别是为了验证其主要编码的是写作风格的假设，我们通过一系列有针对性的实验系统地检查了这些表征。这些实验的结果表明，为作者表示学习的表征能够有效地捕捉写作风格的特征。

    Automatically disentangling an author's style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the 
    
[^20]: 大型语言模型对多选题选项顺序的敏感性

    Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions. (arXiv:2308.11483v1 [cs.CL])

    [http://arxiv.org/abs/2308.11483](http://arxiv.org/abs/2308.11483)

    本文研究了大型语言模型对多选题选项顺序的敏感性。实验证明，当对回答选项进行重新排序时，大型语言模型的性能差距可以达到13%至75%。这种敏感性主要在大型语言模型对前两个/三个选项的预测不确定时出现。

    

    大型语言模型在各种自然语言处理任务中展现了出色的能力。然而，先前的研究表明，这些模型对提示文字的敏感性以及少样本展示的顺序敏感性，给对这些模型的公正评估带来了挑战。随着这些模型变得更加强大，了解和解决这些局限性变得迫切。本文关注在多选题任务中，对大型语言模型对选项顺序的鲁棒性进行研究，这是研究大型语言模型推理和事实检索能力常用的任务。通过对大型语言模型在不同基准测试中在重新排序回答选项时的表现差距的调查，我们证明了在少样本情况下，大型语言模型的性能相差约13%至75%。通过详细分析，我们推测这种敏感性是在大型语言模型在前两个/三个选项之间的预测不确定时产生的。

    Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specif
    
[^21]: 句子级多模态和语言无关表示

    Sentence-Level Multimodal and Language-Agnostic Representations. (arXiv:2308.11466v1 [cs.CL])

    [http://arxiv.org/abs/2308.11466](http://arxiv.org/abs/2308.11466)

    这项研究引入了SONAR，一个多语言和多模态的句子嵌入空间，通过单一文本编码器在相似性搜索任务中取得显著优势，并提供了用于200种语言的文本解码器，可以进行文本到文本和语音到文本的机器翻译。这些结果相比现有模型具有竞争力，并且对语音到文本模型也取得了良好的结果。

    

    我们引入了SONAR，一个新的多语言和多模态的定长句子嵌入空间。我们的单一文本编码器覆盖了200种语言，在xsim和xsim++多语言相似性搜索任务上明显优于现有的句子嵌入模型LASER3和LabSE。使用特定语言的语音编码器在师生设置下训练语音转录数据后，语音片段可以在同一SONAR嵌入空间中进行嵌入。我们的编码器在相似性搜索任务上优于现有的语音编码器。我们还提供了一个适用于200种语言的文本解码器，可以进行文本到文本和语音到文本的机器翻译，包括零翻译语言和模态组合。尽管存在定长瓶颈表示，我们的文本到文本结果在与最先进的NLLB~1B模型相比中具有竞争力。我们的零翻译语音到文本结果与强有力的监督基线模型Whisper相比表现良好。

    We introduce SONAR, a new multilingual and multimodal fixed-size sentence embedding space. Our single text encoder, covering 200 languages, substantially outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks. Speech segments can be embedded in the same SONAR embedding space using language-specific speech encoders trained in a teacher-student setting on speech transcription data. Our encoders outperform existing speech encoders on similarity search tasks. We also provide a text decoder for 200 languages, which allows us to perform text-to-text and speech-to-text machine translation, including for zero-shot language and modality combinations. Our text-to-text results are competitive compared to the state-of-the-art NLLB~1B model, despite the fixed-size bottleneck representation. Our zero-shot speech-to-text translation results compare favorably with strong supervised baselines such as Whisper.
    
[^22]: LegalBench：一个用于衡量大型语言模型的法律推理的协同构建基准库

    LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models. (arXiv:2308.11462v1 [cs.CL])

    [http://arxiv.org/abs/2308.11462](http://arxiv.org/abs/2308.11462)

    LegalBench是一个协同构建的法律推理基准库，涵盖了162个任务，可用于衡量大型语言模型在法律推理方面的能力，为律师和LLM开发者提供了共同的词汇表。

    

    大型语言模型（LLMs）的出现和法律界对其的采用引发了一个问题：LLMs能够执行哪些类型的法律推理？为了更深入地研究这个问题，我们提出了LegalBench：一个协同构建的法律推理基准库，包含162个任务，涵盖了六种不同类型的法律推理。LegalBench是通过跨学科的过程构建的，我们收集了由法律专业人员设计和手工制作的任务。因为这些专业人员在构建过程中起了主导作用，所以任务要么衡量了实际有用的法律推理能力，要么衡量了律师们感兴趣的推理技能。为了促进跨学科关于法律界LLMs的对话，我们还展示了流行的法律框架如何描述法律推理，这些框架区分了许多形式，与LegalBench的任务对应起来，从而给律师和LLM开发者提供了共同的词汇表。

    The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning -- which distinguish between its many forms -- correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This p
    
[^23]: 面向方面的观点对齐网络用于基于方面的情感分类

    Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification. (arXiv:2308.11447v1 [cs.CL])

    [http://arxiv.org/abs/2308.11447](http://arxiv.org/abs/2308.11447)

    本文提出了一种面向方面的观点对齐网络(AOAN)，用于解决基于方面的情感分类中的语义不匹配问题。通过引入邻近区间增强模块和多角度注意机制，该网络能够有效捕捉观点词与相应方面之间的上下文关联。

    

    基于方面的情感分类是细粒度情感分析中的一个关键问题，其旨在根据上下文预测给定方面的情感极性。以往的研究在利用注意机制提取不同方面的观点词方面已经取得了显著进展。然而，一个持久的挑战是有效处理语义不匹配，这是由于注意机制在多方面句子中未能充分对齐观点词与相应方面引起的。为了解决这个问题，我们提出了一种新颖的面向方面的观点对齐网络(AOAN)，以捕捉观点词与相应方面之间的上下文关联。具体而言，我们首先引入一个邻近区间增强模块，强调邻近单词和给定方面的各种组合。此外，我们设计了一种多角度注意机制，以相应给定方面对齐相关观点信息。

    Aspect-based sentiment classification is a crucial problem in fine-grained sentiment analysis, which aims to predict the sentiment polarity of the given aspect according to its context. Previous works have made remarkable progress in leveraging attention mechanism to extract opinion words for different aspects. However, a persistent challenge is the effective management of semantic mismatches, which stem from attention mechanisms that fall short in adequately aligning opinions words with their corresponding aspect in multi-aspect sentences. To address this issue, we propose a novel Aspect-oriented Opinion Alignment Network (AOAN) to capture the contextual association between opinion words and the corresponding aspect. Specifically, we first introduce a neighboring span enhanced module which highlights various compositions of neighboring words and given aspects. In addition, we design a multi-perspective attention mechanism that align relevant opinion information with respect to the giv
    
[^24]: 基于大型语言模型的自主代理的调查

    A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])

    [http://arxiv.org/abs/2308.11432](http://arxiv.org/abs/2308.11432)

    该论文综述了基于大型语言模型的自主代理的研究，提供了从整体角度对该领域的系统审查，其创新之处在于利用大量网络知识实现人类水平的智能决策。

    

    自主代理长期以来一直是学术界的研究热点。以往的研究往往集中在对有限知识的代理进行训练，而这与人类的学习过程存在明显差异，因此很难实现人类般的决策。近年来，通过获取大量的网络知识，大型语言模型（LLM）展现出了实现人类水平智能的显著潜力。这引发了对基于LLM的自主代理的研究的高涨兴趣。为了发挥LLM的全部潜力，研究人员设计了各种不同应用的代理体系结构。本论文综述了这些研究，从整体的角度对自主代理领域进行了系统的审查。具体而言，我们的重点是基于LLM的代理构建，为此我们提出了一个统一的框架。

    Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
    
[^25]: 基于图递归神经网络和动态反馈森林算法的关系三元组提取

    Extracting Relational Triples Based on Graph Recursive Neural Network via Dynamic Feedback Forest Algorithm. (arXiv:2308.11411v1 [cs.CL])

    [http://arxiv.org/abs/2308.11411](http://arxiv.org/abs/2308.11411)

    本文提出了一种基于图递归神经网络和动态反馈森林算法的新方法，用于从文本中提取关系三元组，并成功通过实验证明了其有效性。

    

    从文本中提取关系三元组（主体，谓词，客体）可以将非结构化文本数据转化为结构化知识。命名实体识别（NER）和关系抽取（RE）是知识生成流程中的两个基础子任务。由于它们的差异性，子任务的整合带来了相当大的挑战。本文提出了一种新的方法，将三元组提取任务转化为图标记问题，利用依赖解析和图递归神经网络（GRNN）的结构信息。为了整合子任务，本文提出了一种动态反馈森林算法，在模型训练期间通过推理操作连接子任务的表示。实验结果表明了所提方法的有效性。

    Extracting relational triples (subject, predicate, object) from text enables the transformation of unstructured text data into structured knowledge. The named entity recognition (NER) and the relation extraction (RE) are two foundational subtasks in this knowledge generation pipeline. The integration of subtasks poses a considerable challenge due to their disparate nature. This paper presents a novel approach that converts the triple extraction task into a graph labeling problem, capitalizing on the structural information of dependency parsing and graph recursive neural networks (GRNNs). To integrate subtasks, this paper proposes a dynamic feedback forest algorithm that connects the representations of subtasks by inference operations during model training. Experimental results demonstrate the effectiveness of the proposed method.
    
[^26]: Convoifilter: 鸡尾酒会语音识别的案例研究

    Convoifilter: A case study of doing cocktail party speech recognition. (arXiv:2308.11380v1 [cs.SD])

    [http://arxiv.org/abs/2308.11380](http://arxiv.org/abs/2308.11380)

    本文通过使用单声道语音增强模块与ASR模块，成功将ASR的词错误率从80%降低到26.4%，并通过联合微调策略将其进一步降低到14.5%。

    

    本文提出了一个端到端的模型，用于改进拥挤、嘈杂环境下的自动语音识别（ASR），针对特定说话者。该模型利用单声道语音增强模块将说话者的声音与背景噪声分离，结合ASR模块。通过这种方法，该模型能够将ASR的词错误率（WER）从80%降低到26.4%。通常情况下，由于数据要求的变化，这两个组件会独立调整。然而，语音增强可能会导致ASR效率下降。通过实施联合微调策略，该模型可以将分别调整的WER从26.4%降低到14.5%。

    This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise, along with an ASR module. Through this approach, the model is able to decrease the word error rate (WER) of ASR from 80% to 26.4%. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning.
    
[^27]: M3PS：电子商务中全面的多粒度多模态属性感知产品摘要

    M3PS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization in E-commerce. (arXiv:2308.11351v1 [cs.MM])

    [http://arxiv.org/abs/2308.11351](http://arxiv.org/abs/2308.11351)

    M3PS是一种全面的多粒度多模态属性感知产品摘要方法，能够同时建模并生成高质量的产品摘要，解决了电子商务中产品摘要的端到端建模、多粒度多模态建模和多模态属性建模的问题。

    

    多模态产品摘要（MMPS）旨在通过突出产品特点的短文本摘要来吸引客户的兴趣并增加其购买欲望。现有的MMPS方法已经取得了令人满意的性能。然而，仍然存在几个问题：1）缺乏端到端的产品摘要，2）缺乏多粒度多模态建模，以及3）缺乏多模态属性建模。为了解决这些问题，我们提出了一种用于在电子商务中生成高质量产品摘要的端到端多粒度多模态属性感知产品摘要方法（M3PS）。M3PS同时对产品属性进行建模并生成产品摘要。同时，我们设计了几个多粒度多模态任务，以更好地指导M3PS的多模态学习。此外，我们基于文本和图像模态对产品属性进行建模，以使多模态产品特性能够得到体现。

    Given the long textual product information and the product image, Multi-Modal Product Summarization (MMPS) aims to attract customers' interest and increase their desire to purchase by highlighting product characteristics with a short textual summary. Existing MMPS methods have achieved promising performance. Nevertheless, there still exist several problems: 1) lack end-to-end product summarization, 2) lack multi-grained multi-modal modeling, and 3) lack multi-modal attribute modeling. To address these issues, we propose an end-to-end multi-grained multi-modal attribute-aware product summarization method (M3PS) for generating high-quality product summaries in e-commerce. M3PS jointly models product attributes and generates product summaries. Meanwhile, we design several multi-grained multi-modal tasks to better guide the multi-modal learning of M3PS. Furthermore, we model product attributes based on both text and image modalities so that multi-modal product characteristics can be manife
    
[^28]: LEAP: NLP软件的高效自动化测试方法

    LEAP: Efficient and Automated Test Method for NLP Software. (arXiv:2308.11284v1 [cs.SE])

    [http://arxiv.org/abs/2308.11284](http://arxiv.org/abs/2308.11284)

    LEAP是一种用于NLP软件的高效自动化测试方法，它通过结合LEvy飞行的自适应粒子群算法和文本特征来生成对抗性测试用例，解决了现有方法在错误发现能力和时间效率方面的限制。

    

    在NLP软件中广泛使用的深度神经网络的普及凸显了对鲁棒性的需求。研究人员提出了各种用于对抗性测试用例的自动化测试技术。然而，现有方法存在两个限制：错误发现能力较弱，对基于BERT的NLP软件的成功率从0%到24.6%不等；时间效率低，每个测试用例需要177.8秒到205.28秒，使其在时间受限的场景中具有挑战性。为解决这些问题，本文提出了LEAP，一种自动化测试方法，它使用基于LEvy飞行的自适应粒子群算法与文本特征相结合来生成对抗性测试用例。具体而言，我们采用Levy飞行进行种群初始化，以增加生成测试用例的多样性。我们还设计了一种惯性权重自适应更新操作符，以提高LEAP在高维文本示例的全局优化效率，并基于贪婪策略设计了一种变异操作符，以减少计算开销。

    The widespread adoption of DNNs in NLP software has highlighted the need for robustness. Researchers proposed various automatic testing techniques for adversarial test cases. However, existing methods suffer from two limitations: weak error-discovering capabilities, with success rates ranging from 0% to 24.6% for BERT-based NLP software, and time inefficiency, taking 177.8s to 205.28s per test case, making them challenging for time-constrained scenarios. To address these issues, this paper proposes LEAP, an automated test method that uses LEvy flight-based Adaptive Particle swarm optimization integrated with textual features to generate adversarial test cases. Specifically, we adopt Levy flight for population initialization to increase the diversity of generated test cases. We also design an inertial weight adaptive update operator to improve the efficiency of LEAP's global optimization of high-dimensional text examples and a mutation operator based on the greedy strategy to reduce the
    
[^29]: 音乐理解LLaMA：应用问答和字幕推进文本到音乐生成

    Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning. (arXiv:2308.11276v1 [cs.SD])

    [http://arxiv.org/abs/2308.11276](http://arxiv.org/abs/2308.11276)

    本研究提出了一个名为音乐理解LLaMA（MU-LLaMA）的模型，通过应用问答和字幕生成的方法，解决了文本到音乐生成面临的数据稀缺问题。我们设计了一个新的MusicQA数据集，用于训练MU-LLaMA模型，并在音乐问答方面取得了出色的性能。

    

    由于缺乏具有自然语言字幕的大规模公开音乐数据集，文本到音乐生成（T2M-Gen）面临重大障碍。为了解决这个问题，我们提出了音乐理解LLaMA（MU-LLaMA），能够回答与音乐相关的问题并为音乐文件生成字幕。我们的模型利用预训练的MERT模型从音频中提取音乐特征。然而，获取适用于训练MU-LLaMA模型的合适数据集仍然具有挑战性，因为现有的公开可访问的音频问答数据集缺乏开放式音乐问答所需的深度。为了填补这一空白，我们提出了一种从现有音频字幕数据集生成问答对的方法，并介绍了设计用于回答开放式音乐相关问题的MusicQA数据集。实验证明，经过我们设计的MusicQA数据集训练的MU-LLaMA模型在音乐问答方面取得了优秀的性能。

    Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music qu
    
[^30]: HopPG：自我迭代的异构知识多跳问答程序生成

    HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge. (arXiv:2308.11257v1 [cs.CL])

    [http://arxiv.org/abs/2308.11257](http://arxiv.org/abs/2308.11257)

    本文提出了一种针对多跳问答的自我迭代程序生成框架（HopPG），该框架解决了处理异构知识和多跳问题时所面临的困难，并利用了前几跳的执行结果来生成下一跳的程序。

    

    语义解析方法是基于知识的问答研究中的一个重要分支。它通常基于问题生成可执行程序，并通过知识库进行推理得出答案。由于这种内在机制，它在性能和可解释性方面具有优势。然而，传统的语义解析方法通常在执行之前生成完整的程序，这在处理多跳问题和异构知识时存在困难。首先，完整的多跳程序依赖于多个异构的支持事实，模型很难同时获取这些事实。其次，这些方法忽视了前几跳执行结果与当前跳程序生成之间的交互信息。为了解决这些挑战，我们提出了一种针对异构知识的自我迭代多跳程序生成框架（HopPG），它利用了前几跳的执行结果，并根据它们生成下一跳的程序。

    The semantic parsing-based method is an important research branch for knowledge-based question answering. It usually generates executable programs lean upon the question and then conduct them to reason answers over a knowledge base. Benefit from this inherent mechanism, it has advantages in the performance and the interpretability. However,traditional semantic parsing methods usually generate a complete program before executing it, which struggles with multi-hop question answering over heterogeneous knowledge. Firstly,a complete multi-hop program relies on multiple heterogeneous supporting facts, and it is difficult for models to receive these facts simultaneously. Secondly,these methods ignore the interaction information between the previous-hop execution result and the current-hop program generation. To alleviate these challenges, we propose a self-iterative framework for multi-hop program generation (HopPG) over heterogeneous knowledge, which leverages the previous-hop execution res
    
[^31]: 在图上评估大型语言模型：性能洞察与比较分析

    Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis. (arXiv:2308.11224v1 [cs.AI])

    [http://arxiv.org/abs/2308.11224](http://arxiv.org/abs/2308.11224)

    本研究评估了四个大型语言模型在图数据上解决分析问题的能力，结果显示LLM在理解图数据、生成正确结果和进行结构推理方面表现出色，但在真实性和矫正能力方面存在一些挑战。

    

    大型语言模型(LLM)引起了学术界和工业界的广泛关注，然而LLM在图数据上的应用仍然未被充分探索。在本研究中，我们评估了四个LLM在解决几个图数据分析问题时的能力。我们采用了四个不同的评估指标：理解能力、正确性、真实性和矫正能力。我们的结果表明：1) LLM能够有效地理解自然语言中的图数据，并推理图的拓扑结构。2) GPT模型能够生成逻辑和连贯的结果，在正确性方面优于其他替代方案。3) 所有被检测的LLM在结构推理方面都面临挑战，零样本思维链和少样本提示等技术显示出效果下降。4) GPT模型在多答案任务中经常产生错误答案，引发真实性方面的担忧。5) GPT模型对其输出表现出较高的信心，可能阻碍其矫正能力。值得注意的是，GPT-4显示出了不同水平的性能。

    Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has dem
    
[^32]: 多样性指标：语言模型查询中失败的领域无关代理

    Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries. (arXiv:2308.11189v1 [cs.CL])

    [http://arxiv.org/abs/2308.11189](http://arxiv.org/abs/2308.11189)

    本文提出了一种基于回应多样性的大型语言模型错误量化指标，这些指标独立于领域特定信息，并与失败概率强相关。实证结果展示了这些指标在少样本提示、思维链推理和错误检测方面的应用。

    

    大型语言模型中的错误预测通常依赖于领域特定的信息。本文提出了一种基于回应多样性的大型语言模型错误量化指标，因此独立于底层应用。我们描述了如何使用基于熵、基尼不纯度和质心距离的三个指标。我们进行了一系列的实验，涉及多个数据集和温度设置，证明这些指标与失败概率强相关。此外，我们还提出了实证结果，展示了如何将这些指标应用于少样本提示、思维链推理和错误检测。

    Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.
    
[^33]: ViCo: 利用人类偏好奖励进行有趣视频评论生成

    ViCo: Engaging Video Comment Generation with Human Preference Rewards. (arXiv:2308.11171v1 [cs.CV])

    [http://arxiv.org/abs/2308.11171](http://arxiv.org/abs/2308.11171)

    本文提出了ViCo，通过三种新颖的设计来解决视频评论生成中的挑战，包括评论的主观性难以量化和评估，以及高质量训练样本的稀缺性。利用人类偏好奖励的方法，ViCo能够生成有趣的视频评论。

    

    有趣的视频评论在视频社交媒体中起着重要作用，因为它们是观众情感、思想或幽默的载体。早期的研究通过采用标题样式的编码器-解码器模型对视频评论生成进行了初步探索。然而，评论生成与标题生成存在一些独特的挑战，这使得这些方法在生成有趣评论方面不够有效。与标题的客观描述性质不同，评论往往具有固有的主观性，这使得评论的参与度难以量化和评估。此外，真正有趣的评论的稀缺性给收集足够高质量的训练样本带来了困难。在本文中，我们提出了ViCo，通过三种新颖的设计来解决上述挑战，以生成有趣的视频评论。首先，为了量化评论的参与度，我们利用每条评论收到的“赞”数作为人类偏好的代理指标。

    Engaging video comments play an important role in video social media, as they are the carrier of feelings, thoughts, or humor of the audience. Preliminary works have made initial exploration for video comment generation by adopting caption-style encoder-decoder models. However, comment generation presents some unique challenges distinct from caption generation, which makes these methods somewhat less effective at generating engaging comments. In contrast to the objective and descriptive nature of captions, comments tend to be inherently subjective, making it hard to quantify and evaluate the engagement of comments. Furthermore, the scarcity of truly engaging comments brings difficulty to collecting enough high-quality training examples. In this paper, we propose ViCo with three novel designs to tackle the above challenges for generating engaging Video Comments. Firstly, to quantify the engagement of comments, we utilize the number of "likes" each comment receives as a proxy of human pr
    
[^34]: LLaMA-Reviewer: 通过参数高效微调推进大型语言模型在代码审查自动化中的应用（实证研究）

    LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report). (arXiv:2308.11148v1 [cs.SE])

    [http://arxiv.org/abs/2308.11148](http://arxiv.org/abs/2308.11148)

    本文提出了LLaMA-Reviewer框架，通过参数高效微调方法，利用流行的大型语言模型LLaMA在代码审查领域能力，实现对代码审查任务的自动化。研究表明，即使仅使用不到1%的可训练参数，该框架仍能取得显著的成果。

    

    代码审查活动的自动化长期以来一直是软件工程领域的追求，主要通过许多领域特定的预训练模型来解决。尽管这些模型取得了一定的成功，但它们经常需要大量的资源从头开始进行预训练。相比之下，大型语言模型（LLMs）在补充领域特定知识的情况下展现出了令人着迷的潜力。然而，它们在自动化代码审查任务方面的潜力仍然很少被探索。为了填补这一研究空白，我们提出了LLaMA-Reviewer，这是一个创新的框架，它利用了流行的LLM——LLaMA在代码审查领域的能力。考虑到资源限制，该框架采用了参数高效微调（PEFT）方法，以极少的可训练参数提供高性能。我们对LLaMA-Reviewer进行了广泛的评估，使用了两个不同的公开数据集。值得注意的是，即使在只使用不到1%的可训练参数的情况下，它也取得了显著的成果。

    The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.  In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the 
    
[^35]: 基于自然语言处理的消费者投诉叙述中系统异常的检测方法

    NLP-based detection of systematic anomalies among the narratives of consumer complaints. (arXiv:2308.11138v1 [stat.ME])

    [http://arxiv.org/abs/2308.11138](http://arxiv.org/abs/2308.11138)

    本文开发了一种基于自然语言处理的方法，用于检测消费者投诉叙述中的系统异常。这种方法可以解决分类算法对于较小且频繁出现的系统异常检测的问题，并将投诉叙述转化为定量数据进行分析。

    

    我们开发了一种基于自然语言处理的方法，用于检测投诉叙述中的系统异常，简称为系统异常。尽管分类算法被用于检测明显的异常，但在较小且频繁出现的系统异常情况下，算法可能会因为各种原因而失效，包括技术原因和人工分析师的自然限制。因此，在分类之后的下一步中，我们将投诉叙述转化为定量数据，然后使用一种算法来检测系统异常。我们使用消费者金融保护局的消费者投诉数据库中的投诉叙述来说明整个过程。

    We develop an NLP-based procedure for detecting systematic nonmeritorious consumer complaints, simply called systematic anomalies, among complaint narratives. While classification algorithms are used to detect pronounced anomalies, in the case of smaller and frequent systematic anomalies, the algorithms may falter due to a variety of reasons, including technical ones as well as natural limitations of human analysts. Therefore, as the next step after classification, we convert the complaint narratives into quantitative data, which are then analyzed using an algorithm for detecting systematic anomalies. We illustrate the entire procedure using complaint narratives from the Consumer Complaint Database of the Consumer Financial Protection Bureau.
    
[^36]: 大型语言模型的再识别能力：匿名面临风险吗？

    Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])

    [http://arxiv.org/abs/2308.11103](http://arxiv.org/abs/2308.11103)

    本研究评估了大型语言模型在重新识别匿名个人方面的能力，并发现模型大小、输入长度和指令调整是最重要的决定因素。

    

    在欧盟和瑞士，法院裁决中自然人和法人的匿名性是隐私保护的关键方面。随着大型语言模型（LLMs）的出现，对于匿名人员的大规模再识别的担忧日益增长。根据瑞士联邦最高法院的要求，我们通过使用来自瑞士联邦最高法院的实际法律数据构建了一个概念验证，来探讨LLMs重新识别法院裁决中个人的潜力。在最初的实验之后，我们构建了一个经过匿名化处理的维基百科数据集，作为一个更严格的测试场地来进一步研究研究结果。通过引入并应用文本中再识别人员的新任务，我们还引入了新的性能衡量指标。我们系统地分析了影响成功再识别的因素，确定模型大小、输入长度和指令调整是最重要的决定因素之一。尽管在匿名化处理后，LLMs在重新识别上的成功率很高，但在某些情况下仍然存在风险。

    Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on
    
[^37]: 评估社交位置对话机器人的客观评价：通过多模态用户行为评估人类相似度

    Towards Objective Evaluation of Socially-Situated Conversational Robots: Assessing Human-Likeness through Multimodal User Behaviors. (arXiv:2308.11020v1 [cs.CL])

    [http://arxiv.org/abs/2308.11020](http://arxiv.org/abs/2308.11020)

    本文提出了一种客观评价社交位置对话机器人的方法，利用多模态用户行为来评估机器人的人类相似度，增强了客观性和可复现性。

    

    本文解决了评估社交位置对话机器人的挑战性任务，并提出了一种新颖的客观评价方法，该方法依赖于多模态用户行为。在本研究中，我们的主要关注点是评估机器人的人类相似度作为主要评价指标。而以往的研究常常依赖于用户的主观评价，我们的方法旨在通过间接观察用户行为来评估机器人的人类相似度，从而增强客观性和可复现性。首先，我们创建了一个使用关注性对话语料库中的用户行为来标注人类相似度分数的数据集。然后我们进行了分析，确定了多模态用户行为与人类相似度分数之间的相关性，证明了我们提出的基于行为的评估方法的可行性。

    This paper tackles the challenging task of evaluating socially situated conversational robots and presents a novel objective evaluation approach that relies on multimodal user behaviors. In this study, our main focus is on assessing the human-likeness of the robot as the primary evaluation metric. While previous research often relied on subjective evaluations from users, our approach aims to evaluate the robot's human-likeness based on observable user behaviors indirectly, thus enhancing objectivity and reproducibility. To begin, we created an annotated dataset of human-likeness scores, utilizing user behaviors found in an attentive listening dialogue corpus. We then conducted an analysis to determine the correlation between multimodal user behaviors and human-likeness scores, demonstrating the feasibility of our proposed behavior-based evaluation method.
    
[^38]: 在隐式数学简答题的自动评估中使用语言模型的方法

    Using language models in the implicit automated assessment of mathematical short answer items. (arXiv:2308.11006v1 [cs.CL])

    [http://arxiv.org/abs/2308.11006](http://arxiv.org/abs/2308.11006)

    这项研究提出了一种使用语言模型的新方法来评估数学简答题的正确性和误解，并通过值识别流程提供针对性反馈。

    

    我们提出了一种评估特定数学简答题构造回答的新方法。我们的方法使用一个流程来识别学生回答中指定的关键值。这使得我们可以确定回答的正确性，并识别任何误解。来自值识别流程的信息可以用来向教师和学生提供反馈。值识别流程由两个经过调优的语言模型组成。第一个模型判断一个值是否隐含在学生回答中。第二个模型识别关键值在回答中的位置。我们考虑了一个通用模型，可以用于任何提示和值，以及每个提示和值特定的模型。值识别流程比传统的基于评分标准的评估更准确和有信息量。它可以用来向学生提供更有针对性的反馈。

    We propose a new way to assess certain short constructed responses to mathematics items. Our approach uses a pipeline that identifies the key values specified by the student in their response. This allows us to determine the correctness of the response, as well as identify any misconceptions. The information from the value identification pipeline can then be used to provide feedback to the teacher and student. The value identification pipeline consists of two fine-tuned language models. The first model determines if a value is implicit in the student response. The second model identifies where in the response the key value is specified. We consider both a generic model that can be used for any prompt and value, as well as models that are specific to each prompt and value. The value identification pipeline is a more accurate and informative way to assess short constructed responses than traditional rubric-based scoring. It can be used to provide more targeted feedback to students, which
    
[^39]: 利用可解释的人工智能来分析研究人员对ChatGPT的面向方面情感

    Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT. (arXiv:2308.11001v1 [cs.CL])

    [http://arxiv.org/abs/2308.11001](http://arxiv.org/abs/2308.11001)

    本文利用可解释的人工智能方法，分析了研究人员对ChatGPT在不同使用方面的情感。通过提出一种新的面向方面情感分析技术，使得这种分析不受文本数据长度限制，并提供了对新数据集的宝贵洞见。

    

    ChatGPT的创新性发明在各个领域引发了广泛的讨论。尽管庆祝其各种优点，但对其正确性和使用伦理产生了疑问。已经在努力捕捉用户对其的情感，但如何分析研究界关于ChatGPT不同使用方面的情感是一个值得探讨的问题。在标准的面向方面情感分析中，通常只应用于少量数据集，并且在短文本数据上仅获得有限的成功。我们提出了一种方法，利用可解释的人工智能来促进对研究数据的分析。我们的技术为在新数据集上拓展面向方面情感分析的最新技术提供了宝贵的洞见，使得这种分析不受文本数据长度的限制。

    The groundbreaking invention of ChatGPT has triggered enormous discussion among users across all fields and domains. Among celebration around its various advantages, questions have been raised with regards to its correctness and ethics of its use. Efforts are already underway towards capturing user sentiments around it. But it begs the question as to how the research community is analyzing ChatGPT with regards to various aspects of its usage. It is this sentiment of the researchers that we analyze in our work. Since Aspect-Based Sentiment Analysis has usually only been applied on a few datasets, it gives limited success and that too only on short text data. We propose a methodology that uses Explainable AI to facilitate such analysis on research data. Our technique presents valuable insights into extending the state of the art of Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not hampered by the length of the text data.
    
[^40]: DocPrompt: 大规模连续预训练用于零样本和少样本文档问答

    DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering. (arXiv:2308.10959v1 [cs.CL])

    [http://arxiv.org/abs/2308.10959](http://arxiv.org/abs/2308.10959)

    本文提出了一个名为DocPrompt的方法，用于处理文档问答任务，具有强大的零样本和少样本性能。实验结果表明，DocPrompt模型经过连续预训练后在文档问答任务中表现优异，大大提高了交付效率和模型性能，降低了注释成本和劳动成本。

    

    本文提出了一个名为DocPrompt的方法，用于处理文档问答任务，具有强大的零样本和少样本性能。我们提出了一种新颖的弱监督数据生成方法、一种新颖的多阶段训练方法，以及一种新颖的理解模型和生成模型集成方法。实验结果表明，在文档问答任务中，经过连续预训练的DocPrompt模型明显优于现有的强基线模型。这种方法极大地提高了文档问答客户项目的交付效率和模型性能，降低了注释成本和劳动成本。我们的演示可以在https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout找到。

    In this paper, we propose Docprompt for document question answering tasks with powerful zero-shot and few-shot performance. We proposed a novel weakly supervised data generation method, a novel multl-stage training method and a novel understanding model & generation model ensemble method. Experiment results show that the Docprompt model after continue pretrain significantly outperforms the existing strong baseline models on document question answering tasks. This method greatly improves the delivery efficiency and model performance of document question answering customer projects, reducing annotation costs and labor costs. Our demo can be found at https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
    
[^41]: WanJuan: 用于推进英文和中文大模型的综合多模态数据集

    WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models. (arXiv:2308.10755v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10755](http://arxiv.org/abs/2308.10755)

    本论文提出了一个名为"Wan Juan"的大规模多模态数据集，包含中英文数据。这个数据集通过各种网络来源采集，包括文本、图像文本和视频模态，总量超过2TB。它被用于训练InternLM模型，该模型在多维度评估中表现出明显优势。

    

    ChatGPT和GPT-4的普及显著加速了大模型的开发，导致了许多引人注目的大语言模型(LLMs)和多模态大语言模型(MLLMs)的创建。这些尖端模型的出色表现归功于高质量的数据。然而，领先范式中使用的训练数据的细节通常被保密。这种缺乏透明度，加上开源数据的稀缺，阻碍了社区的进一步发展。为了应对这个问题，本文介绍了一个名为"Wan Juan"的大规模多模态数据集，包含中文和英文数据，采集自广泛的网络来源。该数据集包括文本、图像文本和视频模态，总量超过2TB。它被用于训练InternLM模型，在多维度评估中表现出明显优势，与相似规模的模型相比。所有数据可在htt

    The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents "Wan Juan", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at htt
    
[^42]: 一种使用短语机制的神经机器翻译有效方法

    An Effective Method using Phrase Mechanism in Neural Machine Translation. (arXiv:2308.10482v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10482](http://arxiv.org/abs/2308.10482)

    本论文介绍了一种使用短语机制的神经机器翻译方法，PhraseTransformer，在越南语-中文平行语料库上取得了较高的BLEU得分。

    

    机器翻译是自然语言处理中的基本任务之一，广泛应用于现实生活，并对NLP研究社区的其他任务做出了贡献。最近，基于Transformer的方法在此领域吸引了许多研究人员，并在大多数语言对中取得了最先进的结果。在本文中，我们报告了一种使用短语机制的有效方法，PhraseTransformer，用于改进基线模型Transformer构建平行语料库越南语-中文的神经机器翻译（NMT）系统。我们在VLSP 2022竞赛的MT数据集上，实现了越南语到中文的35.3 BLEU得分和中文到越南语的33.2 BLEU得分。我们的代码可在https://github.com/phuongnm94/PhraseTransformer获得。

    Machine Translation is one of the essential tasks in Natural Language Processing (NLP), which has massive applications in real life as well as contributing to other tasks in the NLP research community. Recently, Transformer -based methods have attracted numerous researchers in this domain and achieved state-of-the-art results in most of the pair languages. In this paper, we report an effective method using a phrase mechanism, PhraseTransformer, to improve the strong baseline model Transformer in constructing a Neural Machine Translation (NMT) system for parallel corpora Vietnamese-Chinese. Our experiments on the MT dataset of the VLSP 2022 competition achieved the BLEU score of 35.3 on Vietnamese to Chinese and 33.2 BLEU scores on Chinese to Vietnamese data. Our code is available at https://github.com/phuongnm94/PhraseTransformer.
    
[^43]: LibriSQA：通过新型数据集和框架推进自由形式和开放式的口语问答

    LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework. (arXiv:2308.10390v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10390](http://arxiv.org/abs/2308.10390)

    本论文提出了LibriSQA，一个自由形式和开放式的口语问答数据集和框架，通过改进ASR任务并使用轻量级的端到端框架，实现了在LLMs上执行口语问答任务的显著结果。

    

    虽然大型语言模型在许多领域和任务中展现出了可称赞的性能，但现有的语言模型在处理多模态功能方面仍存在明显不足，特别是对于需要语音和文本特征之间精确对齐和深度交互的口语问答（SQA）任务。为了解决LLM上的SQA挑战，我们从Librispeech创造了自由形式和开放式的LibriSQA数据集，包括自然对话格式的第一部分和包含多项选择题和答案以及分析片段的第二部分。这两部分共包含107k个涵盖各种主题的SQA对。鉴于现有语音-文本LLM的明显匮乏，我们提出了一个轻量级的端到端框架，在LibriSQA上执行SQA任务，并取得了显著的结果。通过将ASR改为SQA格式，我们进一步证实了我们框架处理ASR任务的能力。

    While Large Language Models (LLMs) have demonstrated commendable performance across a myriad of domains and tasks, existing LLMs still exhibit a palpable deficit in handling multimodal functionalities, especially for the Spoken Question Answering (SQA) task which necessitates precise alignment and deep interaction between speech and text features. To address the SQA challenge on LLMs, we initially curated the free-form and open-ended LibriSQA dataset from Librispeech, comprising Part I with natural conversational formats and Part II encompassing multiple-choice questions followed by answers and analytical segments. Both parts collectively include 107k SQA pairs that cover various topics. Given the evident paucity of existing speech-text LLMs, we propose a lightweight, end-to-end framework to execute the SQA task on the LibriSQA, witnessing significant results. By reforming ASR into the SQA format, we further substantiate our framework's capability in handling ASR tasks. Our empirical f
    
[^44]: WMFormer++: 通过隐式联合学习的嵌套Transformer实现可见水印的去除

    WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning. (arXiv:2308.10195v2 [cs.MM] UPDATED)

    [http://arxiv.org/abs/2308.10195](http://arxiv.org/abs/2308.10195)

    本文提出了一种名为WMFormer++的嵌套Transformer模型，通过隐式联合学习实现可见水印的去除。通过整合水印定位和背景恢复任务的信息，该模型能够自主地导航信息流动，并采用交叉通道注意力来促进定位和恢复的过程。

    

    水印是一种广泛采用的保护媒体版权的方法。与此同时，研究重点已经扩展到水印去除技术，提供了一种对抗性的手段来增强水印的稳健性，并推动水印技术的发展。现有的水印去除方法主要依赖于UNet，并具有专门的解码分支——一个用于水印定位，另一个用于背景图像恢复。然而，水印定位和背景恢复不是独立的任务；精确的水印定位本质上意味着需要恢复的区域，而背景恢复过程有助于更准确的水印定位。为了从两个分支综合地整合信息，我们引入了隐式联合学习范式。这使得网络能够通过门机制自主地导航隐式分支之间的信息流动。此外，我们还采用交叉通道注意力来促进定位和恢复的过程。

    Watermarking serves as a widely adopted approach to safeguard media copyright. In parallel, the research focus has extended to watermark removal techniques, offering an adversarial means to enhance watermark robustness and foster advancements in the watermarking field. Existing watermark removal methods mainly rely on UNet with task-specific decoder branches--one for watermark localization and the other for background image restoration. However, watermark localization and background restoration are not isolated tasks; precise watermark localization inherently implies regions necessitating restoration, and the background restoration process contributes to more accurate watermark localization. To holistically integrate information from both branches, we introduce an implicit joint learning paradigm. This empowers the network to autonomously navigate the flow of information between implicit branches through a gate mechanism. Furthermore, we employ cross-channel attention to facilitate loc
    
[^45]: 受冷落: 相似度分数的反差效应

    Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])

    [http://arxiv.org/abs/2308.09765](http://arxiv.org/abs/2308.09765)

    提出了一种新的相似度度量方法，称为“惊喜分数”，该方法能够考虑对象的上下文信息并显著提高零样本和少样本文档分类任务的性能。

    

    准确评估物体向量嵌入的相似度对于自然语言处理、信息检索和分类任务至关重要。流行的相似度分数（如余弦相似度）基于嵌入向量对，并忽略了从中提取对象的分布。人类对物体相似度的感知显著取决于对象出现的上下文。在这项工作中，我们提出了“惊喜分数”，这是一个对整体进行归一化的相似度度量，包括了人类感知的反差效应，并显著提高了零样本和少样本文档分类任务的性能。此分数量化了在两个元素之间找到给定相似度的惊喜，相对于成对的整体相似度。我们在零样本/少样本分类和聚类任务上评估了这个度量，通常发现与原始余弦相似度相比，性能提高了10-15\%。我们的代码...

    Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
    
[^46]: MindMap：知识图谱激发大型语言模型的思维图思考方法

    MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])

    [http://arxiv.org/abs/2308.09729](http://arxiv.org/abs/2308.09729)

    本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。

    

    通常，大型语言模型存在无法整合新知识、产生幻觉和决策过程不透明等限制。本文探讨了如何利用知识图谱（KG）来激发大型语言模型，以解决整合最新知识和引发模型思维路径的问题。具体来说，我们构建了一个提示管道，使大型语言模型能够理解KG输入并利用隐含知识和检索到的外部知识进行推理。此外，我们研究了引发大型语言模型执行推理和生成答案的思维导图。研究发现，生成的思维导图基于知识的本体论，展示了大型语言模型的推理路径，从而为生产环境中的推理提供了探索和评估的可能性。对三个问答数据集的实验证明，MindMap提示方法带来了显著的实证增益。

    LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question & answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
    
[^47]: PMET: 在Transformer中的精确模型编辑

    PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])

    [http://arxiv.org/abs/2308.08742](http://arxiv.org/abs/2308.08742)

    该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。

    

    模型编辑技术可以以较低的成本修改大型语言模型中的少量知识，并且已经取得了显著的成功。现有方法假设Transformer层隐藏状态是前馈网络的键值内存的值。它们通常优化Transformer层隐藏状态来记忆目标知识，并将其用于更新大型语言模型中前馈网络的权重。然而，Transformer层隐藏状态的信息流来自三个部分：多头自注意力、前馈网络和残差连接。现有方法忽视了Transformer层隐藏状态包含了前馈网络特别需要的信息这一事实。因此，模型编辑的性能下降。为了实现更精确的模型编辑，我们分析了多头自注意力和前馈网络的隐藏状态，发现多头自注意力编码了某些通用知识提取模式。这意味着当引入新知识时，多头自注意力的权重不需要更新。

    Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
    
[^48]: 平均困难注意力变换器是常深度均匀阈值电路

    Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits. (arXiv:2308.03212v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03212](http://arxiv.org/abs/2308.03212)

    本文研究表明平均困难注意力变换器和对数精度变换器都可以模拟常深度阈值电路，其中后者由于生成统一的电路族而更健壮。

    

    转换器已成为各种自然语言处理任务中广泛使用的神经网络模型。先前的研究探索了它们与常深度阈值电路的关系，做出了两个假设：平均困难注意力和相对于输入长度的对数精度的内部计算。Merrill等人证明了平均困难注意力变换器可以识别属于复杂性类别TC0的语言，该类别表示可以由常深度多项式大小的阈值电路识别的语言集合。同样地，Merrill和Sabharwal证明了对数精度的变换器可以识别统一的TC0类语言。这表明这两种转换器模型都可以通过常深度阈值电路来模拟，而后者由于生成统一的电路族而更健壮。我们的论文表明，第一个结果也可以延伸到产生统一电路。

    Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.
    
[^49]: LARCH: 基于大型语言模型的自动readme创建与启发式方法

    LARCH: Large Language Model-based Automatic Readme Creation with Heuristics. (arXiv:2308.03099v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03099](http://arxiv.org/abs/2308.03099)

    LARCH是一种基于大型语言模型的自动readme创建方法，通过识别代表性代码和启发式方法，能够生成连贯和事实正确的readme，优于不依赖代表性代码识别的基线算法。

    

    编写readme是软件开发的关键，它在管理和重用程序代码中起着重要作用。尽管这是许多开发人员的痛点，但即使在最新的大型语言模型(LLMs)的进展下，自动创建readme仍然是一个挑战，因为需要从成千上万行代码中生成一个抽象描述。在这篇演示文章中，我们展示了如果我们能确定一个代表仓库的代码片段，LLMs有能力生成一份连贯和事实正确的readme。建立在这一发现的基础上，我们开发了LARCH（LLM-based Automatic Readme Creation with Heuristics），它利用启发式和弱监督的方式进行代表性代码的识别。通过人工和自动评估，我们证明LARCH在大多数情况下可以生成连贯和事实正确的readme，并且优于不依赖于代表性代码识别的基线算法。

    Writing a readme is a crucial aspect of software development as it plays a vital role in managing and reusing program code. Though it is a pain point for many developers, automatically creating one remains a challenge even with the recent advancements in large language models (LLMs), because it requires generating an abstract description from thousands of lines of code. In this demo paper, we show that LLMs are capable of generating a coherent and factually correct readmes if we can identify a code fragment that is representative of the repository. Building upon this finding, we developed LARCH (LLM-based Automatic Readme Creation with Heuristics) which leverages representative code identification with heuristics and weak supervision. Through human and automated evaluations, we illustrate that LARCH can generate coherent and factually correct readmes in the majority of cases, outperforming a baseline that does not rely on representative code identification. We have made LARCH open-sour
    
[^50]: 探讨自然语言处理研究领域的发展趋势

    Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v1 [cs.CL])

    [http://arxiv.org/abs/2307.10652](http://arxiv.org/abs/2307.10652)

    该论文系统分类和分析了ACL Anthology中的研究论文，提供了对研究领域的结构化概述和NLP领域的分类学。本研究总结了最新的NLP发展，并提出了未来工作的方向。

    

    自然语言处理(NLP)作为理解、生成和处理自然语言文本的一种高效方法，在近年来得到了快速传播和广泛应用。鉴于该领域研究工作的不断增加，研究界已对数个与NLP相关的方法进行了调查。然而，到目前为止，仍缺少一项全面的研究，对已建立的主题进行分类、识别趋势并概括未来研究方向。为填补这一空白，我们对ACL Anthology中包含的研究论文进行了系统分类和分析。结果呈现了研究领域的结构化概述，为NLP领域的研究提供了一个分类学，分析了NLP的最新发展，总结了我们的研究发现，并突出了未来工作的方向。

    As an efficient approach to understand, generate, and process natural language texts, research in natural language processing (NLP) has exhibited a rapid spread and wide adoption in recent years. Given the increasing amount of research work in this area, several NLP-related approaches have been surveyed in the research community. However, a comprehensive study that categorizes established topics, identifies trends, and outlines areas for future research remains absent to this day. Contributing to closing this gap, we have systematically classified and analyzed research papers included in the ACL Anthology. As a result, we present a structured overview of the research landscape, provide a taxonomy of fields-of-study in NLP, analyze recent developments in NLP, summarize our findings, and highlight directions for future work.
    
[^51]: 使用大型语言模型从知识图生成零-shot自然语言生成

    Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs. (arXiv:2307.07312v1 [cs.CL])

    [http://arxiv.org/abs/2307.07312](http://arxiv.org/abs/2307.07312)

    本论文通过使用大型语言模型，实现了基于图数据的零-shot自然语言生成。实验结果表明，该方法在部分指标上接近最新技术水平，在事实、反事实和虚构陈述的对比中也有显著的关联。

    

    在任何使用结构化知识图（KG）数据作为其底层知识表示的系统中，KG到文本生成是将图数据的部分转化为人类可理解的文本的有用工具。最近的工作表明，使用大量文本数据进行预训练的模型即使在特定图到文本任务的相对小的训练集上也能表现出良好的性能。在本文中，我们在这个概念的基础上利用大型语言模型执行零-shot生成，仅仅根据模型对三元组结构的理解进行生成。我们展示了ChatGPT在WebNLG 2020挑战赛的某些指标上实现了接近最新技术水平的性能，但在其他指标上落后。此外，我们比较了事实、反事实和虚构陈述，并展示了LLM已经对其解析的数据有关的知识与输出文本质量之间的显著关联。

    In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans. Recent work has shown that models that make use of pretraining on large amounts of text data can perform well on the KG-to-text task even with relatively small sets of training data on the specific graph-to-text task. In this paper, we build on this concept by using large language models to perform zero-shot generation based on nothing but the model's understanding of the triple structure from what it can read. We show that ChatGPT achieves near state-of-the-art performance on some measures of the WebNLG 2020 challenge, but falls behind on others. Additionally, we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows about the data it is parsing and the quality of the output text
    
[^52]: 自然语言处理中社会人口统计偏见的调查

    Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])

    [http://arxiv.org/abs/2306.08158](http://arxiv.org/abs/2306.08158)

    本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。

    

    深度神经网络在训练过程中往往会学习到非预期的偏见，这在实际应用中可能会产生有害的影响。本文对209篇关于NLP模型中偏见的论文进行了调查，其中大部分论文涉及社会人口统计偏见。为了更好地理解偏见与真实世界的危害之间的区别，我们借鉴心理学和行为经济学的思想，提出了社会人口统计偏见的定义。我们确定了NLP偏见研究的三个主要类别：偏见类型、量化偏见和去偏见。我们认为当前对于量化偏见的方法存在可靠性问题，许多偏见度量并不涉及真实世界中的偏见，当前的去偏见技术是表面的，只是隐藏了偏见，而不是真正去除它。最后，我们提供了未来工作的建议。

    Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
    
[^53]: 大型长序列模型的块级并行Transformer

    Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v1 [cs.CL])

    [http://arxiv.org/abs/2305.19370](http://arxiv.org/abs/2305.19370)

    本文提出了块级并行Transformer方法，以最小化内存成本，能够处理长序列，并且可以处理比先前的内存高效方法更长32倍的训练序列。

    

    Transformer已经成为最先进的自然语言处理模型的基石，在各种AI应用中展现出出色的性能。然而，Transformer中的自我注意机制和大型前馈网络所需的内存容量限制了它们处理长序列的能力，从而为涉及多个长序列或长期依赖的任务带来了挑战。我们提出了一种独特的方法，块级并行Transformer（BPT），它利用块级计算自我注意和前馈网络融合以最小化内存成本。通过在保持内存效率的同时处理更长的输入序列，BPT使训练序列的长度比原始的Transformer长32倍，比先前的内存高效方法长2到4倍。对语言建模和强化学习任务进行的大量实验证明了BPT在减少内存需求和提高性能方面的有效性。

    Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving perfo
    
[^54]: NollySenti：利用迁移学习和机器翻译进行尼日利亚电影情感分类

    NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification. (arXiv:2305.10971v1 [cs.CL])

    [http://arxiv.org/abs/2305.10971](http://arxiv.org/abs/2305.10971)

    本论文提出了一个新的数据集NollySenti，用于不同领域适应的情感分类任务，基于尼日利亚五种常用语言的诺利木电影评论。研究表明，从具有相同目标域的英语进行迁移可以提高5％以上的准确性。

    

    非洲有超过2000种本土语言，但由于缺乏数据集，它们在自然语言处理研究中的代表性不高。最近几年，已经开始开发非洲语言的标注语料库，但这些语料库通常只在单一领域中可用，可能无法推广到其他领域。本文针对不同领域适应的情感分类任务，创建了一个新的数据集——NollySenti，它基于尼日利亚五种常用语言（英语、豪萨语、伊博语、尼日利亚皮钦语和约鲁巴语）的诺利木电影评论。我们使用传统的机器学习方法和预训练的语言模型进行了全面的经验评估。利用迁移学习，我们将来自Twitter领域的跨域适应和来自英语语言的跨语言适应的性能进行了比较。我们的评估结果表明，从具有相同目标域的英语进行迁移比从Twitter进行迁移可以提高5％以上的准确性。

    Africa has over 2000 indigenous languages but they are under-represented in NLP research due to lack of datasets. In recent years, there have been progress in developing labeled corpora for African languages. However, they are often available in a single domain and may not generalize to other domains. In this paper, we focus on the task of sentiment classification for cross domain adaptation. We create a new dataset, NollySenti - based on the Nollywood movie reviews for five languages widely spoken in Nigeria (English, Hausa, Igbo, Nigerian-Pidgin, and Yoruba. We provide an extensive empirical evaluation using classical machine learning methods and pre-trained language models. Leveraging transfer learning, we compare the performance of cross-domain adaptation from Twitter domain, and cross-lingual adaptation from English language. Our evaluation shows that transfer from English in the same target domain leads to more than 5% improvement in accuracy compared to transfer from Twitter in 
    
[^55]: 图像搜索中的公平性：关于从图像检索与去偏见角度探究职业模式刻板印象的研究。

    Fairness in Image Search: A Study of Occupational Stereotyping in Image Retrieval and its Debiasing. (arXiv:2305.03881v1 [cs.IR])

    [http://arxiv.org/abs/2305.03881](http://arxiv.org/abs/2305.03881)

    本文针对职业模式刻板印象问题，研究了网络搜索中的偏见和公平性问题。实验表明当前的图像搜索引擎存在相当严重的职业模式刻板印象，提出了一种去偏见方法以减轻此类偏见并提高图像搜索引擎的公平性。

    

    多模式搜索引擎近年来经历了显著的增长和广泛的使用，成为继信息检索之后第二常见的互联网使用方式。尽管搜索引擎系统提供了一系列服务，但图像搜索领域最近成为信息检索社区的焦点，因为常言道“一图胜千言”。虽然像谷歌这样的流行搜索引擎在图像搜索精度和敏捷性方面表现出色，但人们对它们的搜索结果是否会存在性别、语言、人口统计、社会文化方面的偏见存在争议。这种潜在的偏见可能会对个人的认知产生重要影响，并影响他们的视角。本文主要研究网络搜索中的偏见和公平性问题，重点关注基于关键字的图像搜索方面。我们首先讨论了搜索系统中存在的几种偏见类型以及为什么有必要加以缓解。我们将研究重点缩小到评估和缓解图像检索中的职业模式刻板印象。我们的实验表明，当前的图像搜索引擎存在相当严重的职业模式刻板印象，这可能对个人和整个社会产生不利影响。我们提出了一种去偏见方法，以减轻此类偏见并提高图像搜索引擎的公平性。

    Multi-modal search engines have experienced significant growth and widespread use in recent years, making them the second most common internet use. While search engine systems offer a range of services, the image search field has recently become a focal point in the information retrieval community, as the adage goes, "a picture is worth a thousand words". Although popular search engines like Google excel at image search accuracy and agility, there is an ongoing debate over whether their search results can be biased in terms of gender, language, demographics, socio-cultural aspects, and stereotypes. This potential for bias can have a significant impact on individuals' perceptions and influence their perspectives.  In this paper, we present our study on bias and fairness in web search, with a focus on keyword-based image search. We first discuss several kinds of biases that exist in search systems and why it is important to mitigate them. We narrow down our study to assessing and mitigat
    
[^56]: ChatGPT/GPT-4研究综述及对大语言模型未来的展望

    Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v1 [cs.CL])

    [http://arxiv.org/abs/2304.01852](http://arxiv.org/abs/2304.01852)

    本文全面介绍了最先进的大型语言模型ChatGPT和GPT-4，包括其在各个领域的前景应用，并着重介绍了大规模预训练、指令微调和人类反馈的强化学习创新。ChatGPT/GPT-4在自然语言处理应用方面表现突出，同时在其他领域也具有潜力。

    

    本文全面介绍了来自GPT系列的最先进的大型语言模型（LLM）ChatGPT和GPT-4及其在各个领域的前景应用。实际上，大规模预训练、指令微调和人类反馈的强化学习是提高LLMs的适应性和性能的重要创新。我们在arXiv上深入分析了194篇相关文献，包括趋势分析、词云表现和在各个应用领域的分布分析。研究发现ChatGPT/GPT-4研究显著增长，主要集中在直接的自然语言处理应用上，同时还展示了在从教育和历史到数学、医学和物理等领域具有相当的潜力。本研究旨在提供有关ChatGPT的能力的见解。

    This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabi
    
[^57]: TrojText：测试时隐形文本特洛伊木马插入

    TrojText: Test-time Invisible Textual Trojan Insertion. (arXiv:2303.02242v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.02242](http://arxiv.org/abs/2303.02242)

    本文提出了一种名为TrojText的解决方案，旨在确定无需训练数据是否可以更高效、更经济地进行隐形文本特洛伊攻击。

    

    在自然语言处理（NLP）中，智能神经模型容易受到文本特洛伊攻击。当特洛伊模型对于标准输入表现正常，但是对于包含特定触发器的输入生成恶意输出时，就会发生这种攻击。语法结构触发器，作为一种隐形触发器，越来越受到特洛伊攻击的欢迎，因为它们很难被检测和防御。然而，这些类型的攻击需要大量的训练数据，以生成具有特洛伊插入所需语法结构的毒化样本。对于攻击者来说，获取这样的数据可能很困难，而生成语法毒化触发器和插入特洛伊木马的过程可能非常耗时。本文提出了一种名为TrojText的解决方案，旨在确定是否可以更高效、更经济地进行无需训练数据的隐形文本特洛伊攻击。所提出的方法称为表示-逻辑特洛伊插入（RLI）算法。

    In Natural Language Processing (NLP), intelligent neuron models can be susceptible to textual Trojan attacks. Such attacks occur when Trojan models behave normally for standard inputs but generate malicious output for inputs that contain a specific trigger. Syntactic-structure triggers, which are invisible, are becoming more popular for Trojan attacks because they are difficult to detect and defend against. However, these types of attacks require a large corpus of training data to generate poisoned samples with the necessary syntactic structures for Trojan insertion. Obtaining such data can be difficult for attackers, and the process of generating syntactic poisoned triggers and inserting Trojans can be time-consuming. This paper proposes a solution called TrojText, which aims to determine whether invisible textual Trojan attacks can be performed more efficiently and cost-effectively without training data. The proposed approach, called the Representation-Logit Trojan Insertion (RLI) al
    
[^58]: Truveta Mapper：一个零样本本体映射框架

    Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09767](http://arxiv.org/abs/2301.09767)

    提出了一个将无监督本体匹配或本体对齐视为翻译任务的新视角的Truveta Mapper框架，在零样本、统一和端到端的方式下执行多本体对齐。该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案，无需显式跨本体手动标注数据。

    

    本文提出了一种将无监督本体匹配(Ontology Matching, OM)或本体对齐(Ontology Alignment, OA)视为翻译任务的新视角。将本体表示为图形，在源本体图中的节点到目标本体图中的路径之间进行翻译。所提出的Truveta Mapper (TM)框架利用多任务序列到序列转换器模型，在零样本、统一和端到端的方式下执行多本体对齐。多任务使模型能够通过迁移学习来隐含地学习不同本体之间的关系，无需任何显式的跨本体手动标注数据。这也使得该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案。模型仅在公开可用的文本语料库和内部本体数据上进行预训练和微调。该方案优于现有标准基准解决方案，如Edit-Similarity和MINTE+。

    In this paper, a new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task. Ontologies are represented as graphs, and the translation is performed from a node in the source ontology graph to a path in the target ontology graph. The proposed framework, Truveta Mapper (TM), leverages a multi-task sequence-to-sequence transformer model to perform alignment across multiple ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables the model to implicitly learn the relationship between different ontologies via transfer-learning without requiring any explicit cross-ontology manually labeled data. This also enables the formulated framework to outperform existing solutions for both runtime latency and alignment quality. The model is pre-trained and fine-tuned only on publicly available text corpus and inner-ontologies data. The proposed solution outperforms state-of-the-art approaches, Edit-Similari
    
[^59]: 一种测度论的紧密语言模型特征化方法

    A Measure-Theoretic Characterization of Tight Language Models. (arXiv:2212.10502v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10502](http://arxiv.org/abs/2212.10502)

    本论文使用测度论的方法对语言建模进行了特征化，证明了很多流行的语言模型家族是紧密的，不会存在泄漏现象。

    

    自然语言处理中的核心任务之一是语言建模，涉及对字符串的概率分布进行估计。在大多数情况下，估计的分布在所有有限字符串上求和等于1。然而，在某些病态情况下，概率质量可能“泄漏”到无限序列集合上。为了更精确地刻画泄漏的概念，本文提出了一种测度论的语言建模处理方法。我们证明了许多流行的语言模型家族实际上是紧密的，也就是说它们在这个意义上不会泄漏。我们还推广了之前工作中提出的紧密性特征化方法。

    Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can ``leak'' onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works.
    
[^60]: 在基于图像和语言嵌入中测量社会偏见

    Measuring Social Biases in Grounded Vision and Language Embeddings. (arXiv:2002.08911v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2002.08911](http://arxiv.org/abs/2002.08911)

    该论文推广了社会偏见的概念，从语言嵌入扩展到了基于图像和语言的嵌入。研究表明，基于图像和语言的嵌入中的偏见与未经培训的嵌入中的偏见同等重要甚至更重要。并通过引入新的度量方法来研究偏见、语言和视觉之间的交互作用。

    

    我们将社会偏见的概念从语言嵌入推广到了基于图像和语言的嵌入中。存在于基于图像和语言嵌入中的偏见似乎与未经培训的嵌入中的偏见同等甚至更为重要。尽管视觉和语言可能受到不同的偏见，人们可能希望这些偏见可以相互衰减，但实际情况并非如此。我们提出了多种泛化度量嵌入中的偏见的方法，并引入了泛化空间（Grounded-WEAT和Grounded-SEAT），并展示了三种不同的泛化方法对于偏见、语言和视觉交互作用的重要问题具有不同的回答。我们使用这些度量方法在一个新的数据集上进行实验，这是第一个用于基于图像的偏见的数据集，通过在COCO、概念字幕和谷歌图像等标准语言偏见基准上增加10,228张图像来构建。数据集的构建具有挑战性，因为视觉数据集本身就存在很大的偏见。

    We generalize the notion of social biases from language embeddings to grounded vision and language embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting extending standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these
    

