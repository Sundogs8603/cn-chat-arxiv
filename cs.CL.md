# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance.](http://arxiv.org/abs/2303.16894) | 本文提出了ViewRefer，这是一个多视角的三维视觉定位框架，利用大规模语言模型和多视角原型，从文本和3D模态中获取视角知识并增强框架的表现。 |
| [^2] | [End-to-End $n$-ary Relation Extraction for Combination Drug Therapies.](http://arxiv.org/abs/2303.16886) | 使用端到端抽取方法从文献中提取组合药物治疗中关键的$n$元关系是一个挑战，本研究采用序列到序列的方法在CombDrugExt测试集中为正（或有效）组合实现了66.7\%的F1得分。 |
| [^3] | [Did You Mean...? Confidence-based Trade-offs in Semantic Parsing.](http://arxiv.org/abs/2303.16857) | 该论文介绍了如何通过校准的置信分数，平衡解析任务中的成本、标注员负担、准确性、可用性和安全性等多个权衡，提出了一个可以更好地平衡可用性和安全性的DidYouMean系统。 |
| [^4] | [AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators.](http://arxiv.org/abs/2303.16854) | 本文提出了一种两步法，即“先解释再注释”，以使大型语言模型（LLMs）成为更好的众包标注器，首先为每个演示实例创建提示，随后利用这些提示提示LLM提供解释。 |
| [^5] | [MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks.](http://arxiv.org/abs/2303.16839) | 提出了一种名为MaMMUT的简单模型，可以通过两步方法容纳对比和生成学习，并在联合训练不同的视觉语言任务时表现出很高的效力。 |
| [^6] | [Zero-shot Entailment of Leaderboards for Empirical AI Research.](http://arxiv.org/abs/2303.16835) | 本文探讨了自动挖掘 Empirical AI Research 领域排行榜这一任务类别中的零样本学习现象。实验测试了先前报告的最新技术模型在其未见过的排行榜标签上的泛化能力或 entailment 能力。本文创建了一个零样本标记的数据集。 |
| [^7] | [Language Models Trained on Media Diets Can Predict Public Opinion.](http://arxiv.org/abs/2303.16779) | 该论文介绍了一种新的方法来探索媒体偏好模型——适用于在线新闻、电视广播或广播节目内容的语言模型，可以预测公众舆论并具有实际应用价值。 |
| [^8] | [Assorted, Archetypal and Annotated Two Million (3A2M) Cooking Recipes Dataset based on Active Learning.](http://arxiv.org/abs/2303.16778) | 本研究利用领域专家的知识和主动学习技术呈现了一个两百万份烹饪食谱新数据集，通过将30万份食谱按照命名实体识别进行分类到9个类别中，然后使用混合方法对其余的1900K份食谱进行分类。 |
| [^9] | [Not cool, calm or collected: Using emotional language to detect COVID-19 misinformation.](http://arxiv.org/abs/2303.16777) | 本论文提出一种使用情感语言检测COVID-19错误信息的模型，该模型相比于单一的错误信息分类器具有更好的效果，但该研究的主要限制因素是低质量标签和不匹配的标签分布。 |
| [^10] | [Meeting Action Item Detection with Regularized Context Modeling.](http://arxiv.org/abs/2303.16763) | 本研究提出了一种轻量级模型集成方法和上下文丢失技术，用于自动检测带有行动项的会议内容，并构建并发布了第一个带有手动行动项注释的中文会议语料库，实验结果表明提出的方法具有较好的准确性和鲁棒性。 |
| [^11] | [ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony Optimization.](http://arxiv.org/abs/2303.16760) | 本研究提出了一种基于蚁群优化的高效词性标注方法ACO-tagger，实现了高达96.867％的准确率，优于几种最先进的方法。 |
| [^12] | [Exploring celebrity influence on public attitude towards the COVID-19 pandemic: social media shared sentiment analysis.](http://arxiv.org/abs/2303.16759) | 本文研究了公众人物在社交媒体上共享的信息对 COVID-19 疫情中的公众情感和大众意见的影响。通过收集和分析推文，发现公众人物的信息对公众情感和大众意见具有显著的影响。 |
| [^13] | [How can Deep Learning Retrieve the Write-Missing Additional Diagnosis from Chinese Electronic Medical Record For DRG.](http://arxiv.org/abs/2303.16757) | 该论文提出了一种使用深度学习的方法，从电子病历中检索漏诊的额外诊断，以适用于DRG分组，解决了漏诊问题导致医疗记录不完整、影响DRG招生正确率的问题。 |
| [^14] | [LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability.](http://arxiv.org/abs/2303.16756) | 本文提出了一种隐私感知数据增强的LLM-PTM方法，有效地提高了患者-试验匹配的性能和泛化能力。 |
| [^15] | [Training Language Models with Language Feedback at Scale.](http://arxiv.org/abs/2303.16755) | 本文提出一种新方法，即利用更丰富的语言反馈进行模仿学习，通过三个迭代步骤对语言模型进行训练以生成更符合人类偏好的输出。 |
| [^16] | [Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture.](http://arxiv.org/abs/2303.16753) | 本文提出了一种参数有效的结构，通过MPO分解共享中央张量并保持层特定的辅助张量，将预训练语言模型扩展到更深的深度。 |
| [^17] | [Judicial Intelligent Assistant System: Extracting Events from Divorce Cases to Detect Disputes for the Judge.](http://arxiv.org/abs/2303.16751) | 本文提出了一种基于两轮标注事件提取技术的离婚案件争议检测方法，实现了司法智能助手（JIA）系统，以自动从离婚案件材料中提取重点事件，通过识别其中的共指来对事件进行对齐，并检测冲突。 |
| [^18] | [Improving Code Generation by Training with Natural Language Feedback.](http://arxiv.org/abs/2303.16749) | 该论文提出了一种新算法ILF，通过从自然语言反馈中进行学习来显著提高代码生成模型的性能，即使只有少量反馈，也可以获得很好的效果。 |
| [^19] | [Evaluating NLG systems: A brief introduction.](http://arxiv.org/abs/2303.16742) | 本文介绍了自然语言生成中评估的关键术语和区别，并阐述了INLG评估最佳论文奖设立的意义。 |
| [^20] | [Text revision in Scientific Writing Assistance: An Overview.](http://arxiv.org/abs/2303.16726) | 本文概述了科技写作中的文本修订，包括科学写作的特点、常见格式和约定，以及各种类型的文本修订写作辅助工具，然而这些工具仍然存在许多挑战，如工具可访问性、对上下文的有限考虑和对话信息的含义不清等。 |
| [^21] | [Using Semantic Similarity and Text Embedding to Measure the Social Media Echo of Strategic Communications.](http://arxiv.org/abs/2303.16694) | 本文介绍了一种使用语义相似性来衡量特定信息传播后在线讨论变化的新技术，并使用环境新闻发布和气候变化推文来揭示了战略传播的响应呈重尾分布的特征。 |
| [^22] | [Summarizing Indian Languages using Multilingual Transformers based Models.](http://arxiv.org/abs/2303.16657) | 本文研究了在印度语言作为源和目标文本的情况下使用多语言变形金刚模型进行摘要的表现，并报告了ROUGE分数。 |
| [^23] | [GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment.](http://arxiv.org/abs/2303.16634) | 本文介绍了GPTEval，一个利用链式思考和形式填充评价NLG生成的质量。实验表明，在文本摘要任务中，GPTEval结合GPT-4取得了0.514的斯皮尔曼相关系数，胜过其他方法。 |
| [^24] | [AraSpot: Arabic Spoken Command Spotting.](http://arxiv.org/abs/2303.16621) | AraSpot是一款使用ConformerGRU模型架构训练40个阿拉伯语关键词的口语命令识别工具，其通过在线数据增强和文本到语音模型的训练提高了性能，并以99.59%的准确率超出以往的方法。 |
| [^25] | [Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations.](http://arxiv.org/abs/2303.16618) | 本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。 |
| [^26] | [LMExplainer: a Knowledge-Enhanced Explainer for Language Models.](http://arxiv.org/abs/2303.16537) | LMExplainer是一种知识增强的语言模型解释模块，使用知识图和图注意力神经网络来提取关键决策信号，为用户提供可理解的解释。 |
| [^27] | [Building a Knowledge Graph of Distributed Ledger Technologies.](http://arxiv.org/abs/2303.16528) | 本文建立了一个包括安全、应用领域、标准法规等方面的分布式分类账技术的知识图谱，有助于拓展人们对该技术的理解和寻找新的用例。 |
| [^28] | [Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning.](http://arxiv.org/abs/2303.16445) | 本文通过上下文学习扩展否定和角色反转数据集，发现过去的结论可能被小型测试集误导。同时，BERT和ALBERT等模型表现出较高的否定敏感度。 |
| [^29] | [TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs.](http://arxiv.org/abs/2303.16434) | TaskMatrix.AI是一个任务完成系统，可以将基础模型与数百万个API连接，提高完成任务的效率和全面性。 |
| [^30] | [ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models.](http://arxiv.org/abs/2303.16421) | ChatGPT是一个知识渊博但经验不足的LLM，能够回答常识问题，但在某些类型问题上仍存在困难。 |
| [^31] | [Zero-shot Clinical Entity Recognition using ChatGPT.](http://arxiv.org/abs/2303.16416) | 本研究探讨了使用 ChatGPT 进行零样本临床实体识别任务，并发现 ChatGPT 在松弛匹配 F1 分数方面显著优于 GPT-3。虽然其性能仍低于 BioClinicalBERT 模型，但我们的研究表明了 ChatGPT 在零样本设置下有很大的临床 NER 任务潜力。 |
| [^32] | [Hierarchical Video-Moment Retrieval and Step-Captioning.](http://arxiv.org/abs/2303.16406) | 这篇论文提出了HiREST数据集和一个新的基准，将分层信息检索和视觉/文本逐步总结从教学视频语料库中推广，使得在一个端到端的设置下可以共同搜索视频语料库，并生成摘要。 |
| [^33] | [ChatGPT or academic scientist? Distinguishing authorship with over 99% accuracy using off-the-shelf machine learning tools.](http://arxiv.org/abs/2303.16352) | 该论文设计了一种用于区分ChatGPT生成文本和学术科学家撰写文本的方法，并在2.5k个样本上测试，实现了99.2%的准确率。 |
| [^34] | [Language-Guided Audio-Visual Source Separation via Trimodal Consistency.](http://arxiv.org/abs/2303.16342) | 该论文提出了一种自监督学习的方法，通过使用自然语言查询来进行音频源分离，实现了语言、视觉和音频的一致性对齐，并在多个数据集上表现出比现有方法更好的效果。 |
| [^35] | [A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube.](http://arxiv.org/abs/2303.16281) | 研究发现在Google、ChatGPT、维基百科和YouTube上，搜索结果受限于语言，反映了与复杂主题相关的文化刻板印象，缺乏跨文化视角。 |
| [^36] | [Writing Assistants Should Model Social Factors of Language.](http://arxiv.org/abs/2303.16275) | 当前，由大型语言模型构建的智能写作助手越来越流行，但它们的表现并不优秀。作者认为，这种状况的主要原因是过于专注于语言的信息内容而忽略了其社会因素。他们提出将社会因素融入到写作助手的建设中，以构建智能、有效和个性化的写作助手，从而提高用户的使用体验和接受度。 |
| [^37] | [Scalable handwritten text recognition system for lexicographic sources of under-resourced languages and alphabets.](http://arxiv.org/abs/2303.16256) | 本论文介绍了一种可扩展的手写文字识别系统，可用于破译历史未开发语言和字母的词典，提供了一个可以读取手写索引卡的解决方案，并将它们的引用链接到可搜索的词典条目列表，该系统可以高效地识别手写文本并取得了很高的准确率。 |
| [^38] | [Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using Context Summarization and Domain Schema.](http://arxiv.org/abs/2303.16252) | 本论文提出了一种使用领域架构和对话历史摘要实现泛化的零样本端到端任务导向对话系统，并在实验中证明其在标准领域转移和少样本学习中优于现有方法。 |
| [^39] | [Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP.](http://arxiv.org/abs/2303.16166) | 在NLP研究中，我们不能仅凭感知质量假定代码正确性，应该推动采用编码最佳实践以提高实验结果的正确性和可靠性。 |
| [^40] | [TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models.](http://arxiv.org/abs/2303.15430) | 本文提出了一种将声学和视觉信息转化为文本描述并与口语文本串联，从而将非语言信息纳入预训练语言模型中的方法。该方法可以不需要昂贵的多模态数据收集和建模，并且可以使非语言线索与语言无缝集成，对于多模态行为理解任务具有实际应用意义。 |
| [^41] | [DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph.](http://arxiv.org/abs/2303.13351) | 这篇论文在DBLP学术知识图上创建了一个包含10000个问题-答案对的问答数据集，是最大的学术问答数据集。 |
| [^42] | [Trained on 100 million words and still in shape: BERT meets British National Corpus.](http://arxiv.org/abs/2303.09859) | 本文探讨了在英国国家语料库上预训练的效果，并展示它可以比原始BERT模型达到更好的表现。在公平、可重复且数据有效的比较研究中，他们证明了这样的语料库有作为语言建模基准的巨大潜力。他们提出了一个经过优化的LM体系结构称为LTG-BERT。 |
| [^43] | [Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential.](http://arxiv.org/abs/2303.09038) | 本文研究探讨利用ChatGPT将放射学报告翻译成通俗易懂的语言，平均得分为5分制的4.1分，信息缺失率和信息错误率均较低，ChatGPT提供的建议大都与放射学报告相关。 |
| [^44] | [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective.](http://arxiv.org/abs/2302.12095) | 本研究评估了ChatGPT的鲁棒性，发现其在对抗性和超出分布任务上有一致的优势，但绝对表现仍有提高空间，鲁棒性仍是一个重要的挑战。 |
| [^45] | [Does CLIP Bind Concepts? Probing Compositionality in Large Image Models.](http://arxiv.org/abs/2212.10537) | 本文分析了大型神经网络模型CLIP的组合性能力以及以结构敏感的方式捆绑变量的能力，发现其能够在单一对象的情况下组合概念，但在需要概念捆绑的情况下性能显著下降。 |
| [^46] | [Large Language Models are reasoners with Self-Verification.](http://arxiv.org/abs/2212.09561) | 本文提出了一种新的自我验证方法，使用CoT的结论来构建新样本并要求LLM重新预测原始条件，以提高推理准确性。实验证明，LLMs可以对其自己的结论进行自我验证并实现竞争性的推理性能。 |
| [^47] | [Context-aware Fine-tuning of Self-supervised Speech Models.](http://arxiv.org/abs/2212.08542) | 本文研究了一种新方法，称为上下文感知微调。在微调期间使用上下文信息可以使模型在推理时进行预测，而无需访问这些周围片段。该方法能够有效减小运行开销，并在S数据集上取得了良好的评估结果。 |
| [^48] | [Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations.](http://arxiv.org/abs/2212.04231) | 本文提出了一种新的解决方案利用多任务预训练生成Transformer模型的技术来应对VL-NLE任务的问题，有效提高了解释的准确性和可信度，具有良好的应用前景。 |
| [^49] | [Editing Models with Task Arithmetic.](http://arxiv.org/abs/2212.04089) | 本文提出了一种使用任务向量进行模型编辑的新范式，任务向量可通过算术操作进行修改和组合，可以提高目标任务性能且对控制任务影响较小。 |
| [^50] | [Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning.](http://arxiv.org/abs/2212.01117) | 本文提出了一种基于Prompt学习和传播结构的零样本谣言检测框架，其能够有效地检测不同领域和语言的谣言，并可适应非预料中断事件的影响。 |
| [^51] | [Calibrated Interpretation: Confidence Estimation in Semantic Parsing.](http://arxiv.org/abs/2211.07443) | 该论文研究了常见的生成模型在四个流行的语义解析数据集上的校准性，并分析了与校准误差相关的因素。为了方便将校准纳入语义解析评估中，作者们发布了一个计算校准度量的库。 |
| [^52] | [Emergent Linguistic Structures in Neural Networks are Fragile.](http://arxiv.org/abs/2210.17406) | 本文提出了一个框架来评估语言模型对于语法的表述的一致性和稳健性，通过多项实验证据表明，神经网络中 emergent 语言结构是脆弱的。 |
| [^53] | [Multi-lingual Evaluation of Code Generation Models.](http://arxiv.org/abs/2210.14868) | 本研究提出了多种用于评估代码生成模型的新基准测试，能够以多语言方式评估模型性能，并探讨了多语言模型优于单语言模型、少量提示能教授模型新语言以及在单语言设置下拥有零-shot翻译能力等问题。 |
| [^54] | [Zero-Shot Retrieval with Search Agents and Hybrid Environments.](http://arxiv.org/abs/2209.15469) | 本文研究带有搜索代理和混合环境的零样本检索，实验表明，此方法可以超越传统的基于术语的检索和神经检索器，简单的启发式混合检索环境还可以将基线性能提高。 |
| [^55] | [SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation.](http://arxiv.org/abs/2209.15323) | SmallCap是一种轻量级图像字幕生成模型，通过从数据存储库中检索到相关字幕为条件生成文字描述。模型训练速度快，无需额外微调即可跨领域迁移，还能够充分利用数据存储库中的大规模数据。在实验中表现出竞争力，并可以通过利用人工标注和网络数据来进一步提高性能。 |
| [^56] | [A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America.](http://arxiv.org/abs/2207.06591) | 本研究提出了一种自动检测和表征拉丁美洲自然语言处理系统中偏见和有害刻板印象的方法。该方法基于单词嵌入之间的相似性和数据源的分析，并在西班牙语、葡萄牙语和克丘亚语的有害刻板印象检测案例中进行测试。结果表明，不同的自然语言处理系统以不同甚至意想不到的方式存在偏见和放大有害的刻板印象。 |
| [^57] | [Ousiometrics and Telegnomics: The essence of meaning conforms to a two-dimensional powerful-weak and dangerous-safe framework with diverse corpora presenting a safety bias.](http://arxiv.org/abs/2110.06847) | 本文提出了ousiometrics和Telegnomics，发现单词传达的基本含义最好用指南针般的强度-危险（PD）框架来描述，而自然语言对安全低危险的词汇有系统偏见。 |

# 详细

[^1]: ViewRefer: 基于GPT和样例引导的多视角知识处理的三维视觉定位

    ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v1 [cs.CV])

    [http://arxiv.org/abs/2303.16894](http://arxiv.org/abs/2303.16894)

    本文提出了ViewRefer，这是一个多视角的三维视觉定位框架，利用大规模语言模型和多视角原型，从文本和3D模态中获取视角知识并增强框架的表现。

    

    通过利用多视角输入的3D场景，可以缓解3D视觉定位中的视角差异问题。然而，现有方法通常忽略了嵌入在文本模态中的视角线索，并且未能权衡不同视图的相对重要性。本文提出了ViewRefer，这是一个多视角的三维视觉定位框架，探索如何从文本和3D模态中获取视角知识。其中，ViewRefer利用大规模语言模型（例如GPT）的多样化语言知识，将单一的定位文本扩展为多个几何一致的描述；同时，在3D模态中，引入了基于Transformer的融合模块和视图间注意力，以增强视图之间物体的交互。此外，还提出了一组可学习的多视角原型，用于记忆不同视角下的场景无关知识，从两个方面增强了框架。

    Understanding 3D scenes from multi-view inputs has been proven to alleviate the view discrepancy issue in 3D visual grounding. However, existing methods normally neglect the view cues embedded in the text modality and fail to weigh the relative importance of different views. In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding exploring how to grasp the view knowledge from both text and 3D modalities. For the text branch, ViewRefer leverages the diverse linguistic knowledge of large-scale language models, e.g., GPT, to expand a single grounding text to multiple geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer fusion module with inter-view attention is introduced to boost the interaction of objects across views. On top of that, we further present a set of learnable multi-view prototypes, which memorize scene-agnostic knowledge for different views, and enhance the framework from two perspectives: a view-guided attention module 
    
[^2]: 组合药物治疗的端到端$n$元关系抽取

    End-to-End $n$-ary Relation Extraction for Combination Drug Therapies. (arXiv:2303.16886v1 [cs.CL])

    [http://arxiv.org/abs/2303.16886](http://arxiv.org/abs/2303.16886)

    使用端到端抽取方法从文献中提取组合药物治疗中关键的$n$元关系是一个挑战，本研究采用序列到序列的方法在CombDrugExt测试集中为正（或有效）组合实现了66.7\%的F1得分。

    

    组合药物疗法是涉及两种或多种药物的治疗方案，通常用于癌症、艾滋病、疟疾或结核病患者。目前，PubMed上有超过35万篇文章使用了“组合药物治疗”MeSH(化学物质主题词表)头衔，每年至少发表1万篇文章，持续时间已达两个十年。从科学文献中提取出组合疗法固有地构成了一个$n$元关系抽取问题。不同于普通的$n$元设置，其中$n$是固定的（例如药物-基因-突变关系中的$n=3$），提取组合疗法是一个特殊的设置，其中$n \geq2$是动态的，具体取决于每个实例。最近，Tiktinsky等人（NAACL 2022）推出了一个首个数据集CombDrugExt，用于从文献中提取此类疗法。在此，我们采用了序列到序列的端到端抽取方法，在CombDrugExt测试集中为正（或有效）组合实现了66.7\%的F1得分。

    Combination drug therapies are treatment regimens that involve two or more drugs, administered more commonly for patients with cancer, HIV, malaria, or tuberculosis. Currently there are over 350K articles in PubMed that use the "combination drug therapy" MeSH heading with at least 10K articles published per year over the past two decades. Extracting combination therapies from scientific literature inherently constitutes an $n$-ary relation extraction problem. Unlike in the general $n$-ary setting where $n$ is fixed (e.g., drug-gene-mutation relations where $n=3$), extracting combination therapies is a special setting where $n \geq 2$ is dynamic, depending on each instance. Recently, Tiktinsky et al. (NAACL 2022) introduced a first of its kind dataset, CombDrugExt, for extracting such therapies from literature. Here, we use a sequence-to-sequence style end-to-end extraction method to achieve an F1-Score of $66.7\%$ on the CombDrugExt test set for positive (or effective) combinations. Th
    
[^3]: “你指的是...？”：语义解析中的置信度权衡

    Did You Mean...? Confidence-based Trade-offs in Semantic Parsing. (arXiv:2303.16857v1 [cs.CL])

    [http://arxiv.org/abs/2303.16857](http://arxiv.org/abs/2303.16857)

    该论文介绍了如何通过校准的置信分数，平衡解析任务中的成本、标注员负担、准确性、可用性和安全性等多个权衡，提出了一个可以更好地平衡可用性和安全性的DidYouMean系统。

    

    我们演示了如何通过一个校准好的模型来平衡任务导向解析中的常见权衡。在一个模拟的标注员交互的实验中，我们展示了校准的置信分数如何平衡成本和标注员负担，用较少的交互提高准确性。然后，我们研究了置信度分数如何帮助优化可用性和安全性的权衡。我们展示了基于置信度阈值的解析错误数量大幅减少的系统DidYouMean，然而这也牺牲了可用性。

    We illustrate how a calibrated model can help balance common trade-offs in task-oriented parsing. In a simulated annotator-in-the-loop experiment, we show that well-calibrated confidence scores allow us to balance cost with annotator load, improving accuracy with a small number of interactions. We then examine how confidence scores can help optimize the trade-off between usability and safety. We show that confidence-based thresholding can substantially reduce the number of incorrect low-confidence programs executed; however, this comes at a cost to usability. We propose the DidYouMean system which better balances usability and safety.
    
[^4]: AnnoLLM：使大型语言模型成为更好的众包标注器

    AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators. (arXiv:2303.16854v1 [cs.CL])

    [http://arxiv.org/abs/2303.16854](http://arxiv.org/abs/2303.16854)

    本文提出了一种两步法，即“先解释再注释”，以使大型语言模型（LLMs）成为更好的众包标注器，首先为每个演示实例创建提示，随后利用这些提示提示LLM提供解释。

    

    许多自然语言处理（NLP）任务依赖于带标签的数据，以训练机器学习模型实现高性能。然而，数据注释可能是一个耗时且昂贵的过程，特别是当任务涉及大量数据或需要专业领域时。最近，GPT-3.5系列模型在各种NLP任务中展示出了令人瞩目的少样本和零样本能力。本文首先声称，大型语言模型（LLMs），如GPT-3.5，可以通过为它们提供充分的指导和演示示例来作为优秀的众包标注器。为了使LLMs成为更好的标注器，我们提出了一种两步方法，“先解释再注释”。更确切地说，我们首先为每个演示示例创建提示，随后利用这些提示提示LLM提供关于为什么对于特定示例选择了特定的基础真相回答/标签的解释。随后，我们构建了few-shot思维链。

    Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thoug
    
[^5]: MaMMUT: 一种用于多模态任务联合学习的简单架构

    MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])

    [http://arxiv.org/abs/2303.16839](http://arxiv.org/abs/2303.16839)

    提出了一种名为MaMMUT的简单模型，可以通过两步方法容纳对比和生成学习，并在联合训练不同的视觉语言任务时表现出很高的效力。

    

    语言模型的发展已从编码-解码转向仅解码的设计。此外，普遍认为，最流行的两种多模态任务，生成任务和对比任务，往往互相冲突，难以在一个架构中容纳，并进一步需要用于下游任务的复杂调整。我们提出了一种新的培训范式，用于多模态任务的仅解码模型，这在联合学习这些不同的视觉语言任务方面非常有效。这是通过一个简单的模型MaMMUT实现的。它由单一的视觉编码器和一个文本解码器组成，并能够通过文本解码器上的新的两步方法容纳对比和生成学习。我们证明这些不同目标任务的联合训练是简单的，有效的，并最大化了模型的权重共享。此外，相同的架构使得对开放词汇对象检测的简单扩展成为可能。

    The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection 
    
[^6]: 零样本 Entrailment 用于 Empirical AI Research 领域排行榜

    Zero-shot Entailment of Leaderboards for Empirical AI Research. (arXiv:2303.16835v1 [cs.CL])

    [http://arxiv.org/abs/2303.16835](http://arxiv.org/abs/2303.16835)

    本文探讨了自动挖掘 Empirical AI Research 领域排行榜这一任务类别中的零样本学习现象。实验测试了先前报告的最新技术模型在其未见过的排行榜标签上的泛化能力或 entailment 能力。本文创建了一个零样本标记的数据集。

    

    本文在一个特定的文本蕴含（RTE）任务类别中进行了零样本学习现象的大规模实证研究，即自动挖掘 Empirical AI Research 领域排行榜。该领域的排行榜提取先前报告的最新技术模型，在非零样本设置下表现良好，报告了高于90%的性能。然而，一个核心的研究问题仍未被检验：这些模型真的学习了 entailment 吗？因此，在本文的实验中，测试了两个先前报告的最新技术模型，在其训练过程中没有见过的排行榜标签上，测试它们的泛化能力或 entailment 能力。我们假设，如果模型学习了 entailment，它们的零样本性能也可能是中等的，或者具体来说，好于随机猜测。本文通过远程标注创建了零样本标记数据集。

    We present a large-scale empirical investigation of the zero-shot learning phenomena in a specific recognizing textual entailment (RTE) task category, i.e. the automated mining of leaderboards for Empirical AI Research. The prior reported state-of-the-art models for leaderboards extraction formulated as an RTE task, in a non-zero-shot setting, are promising with above 90% reported performances. However, a central research question remains unexamined: did the models actually learn entailment? Thus, for the experiments in this paper, two prior reported state-of-the-art models are tested out-of-the-box for their ability to generalize or their capacity for entailment, given leaderboard labels that were unseen during training. We hypothesize that if the models learned entailment, their zero-shot performances can be expected to be moderately high as well--perhaps, concretely, better than chance. As a result of this work, a zero-shot labeled dataset is created via distant labeling formulating
    
[^7]: 基于媒体偏好训练的语言模型可以预测公众舆论

    Language Models Trained on Media Diets Can Predict Public Opinion. (arXiv:2303.16779v1 [cs.CL])

    [http://arxiv.org/abs/2303.16779](http://arxiv.org/abs/2303.16779)

    该论文介绍了一种新的方法来探索媒体偏好模型——适用于在线新闻、电视广播或广播节目内容的语言模型，可以预测公众舆论并具有实际应用价值。

    

    公众舆论反映和塑造社会行为，但传统的基于调查的工具存在局限性。我们介绍一种新的方法来探索媒体偏好模型——适用于在线新闻、电视广播或广播节目内容的语言模型，可以模拟已消费一组媒体的亚群体的意见。为了验证这种方法，我们使用美国全国代表性调查中针对COVID-19和消费者信心的观点表达作为基本事实。我们的研究表明，这种方法（1）可以预测调查响应分布中的人类判断，并且对措辞和媒体曝光渠道具有鲁棒性，（2）在建模更密切关注媒体的人方面更准确，（3）符合关于哪些类型的观点受媒体消费影响的文献。探测语言模型提供了一种强大的新方法来研究媒体效应，具有在补充调查和预测公众舆论方面的实际应用，也涉及解决与媒体偏见有关的问题。

    Public opinion reflects and shapes societal behavior, but the traditional survey-based tools to measure it are limited. We introduce a novel approach to probe media diet models -- language models adapted to online news, TV broadcast, or radio show content -- that can emulate the opinions of subpopulations that have consumed a set of media. To validate this method, we use as ground truth the opinions expressed in U.S. nationally representative surveys on COVID-19 and consumer confidence. Our studies indicate that this approach is (1) predictive of human judgements found in survey response distributions and robust to phrasing and channels of media exposure, (2) more accurate at modeling people who follow media more closely, and (3) aligned with literature on which types of opinions are affected by media consumption. Probing language models provides a powerful new method for investigating media effects, has practical applications in supplementing polls and forecasting public opinion, and 
    
[^8]: 基于主动学习策略的两 百万份标记美食食谱数据集 - 3A2M

    Assorted, Archetypal and Annotated Two Million (3A2M) Cooking Recipes Dataset based on Active Learning. (arXiv:2303.16778v1 [cs.CL])

    [http://arxiv.org/abs/2303.16778](http://arxiv.org/abs/2303.16778)

    本研究利用领域专家的知识和主动学习技术呈现了一个两百万份烹饪食谱新数据集，通过将30万份食谱按照命名实体识别进行分类到9个类别中，然后使用混合方法对其余的1900K份食谱进行分类。

    

    烹饪食谱可以交换烹饪思想，并提供食品的制作说明。然而，在该领域内由于缺乏足够的标记数据，将在线找到的原始食谱分类到合适的食品类型是一项具有挑战性的任务。本研究利用领域专家的知识将食谱分类可能是一种解决方案。我们基于一个主动学习技术呈现了一个两百万份烹饪食谱的新数据集，通过利用领域专家的知识将其标记在各自的类别中。为了构建数据集，我们从RecipeNLG数据集中获取食谱。然后，我们使用三个可信度得分高于86.667％的人类专家按照其命名实体识别（NER）将30万份食谱分类到九个类别之一：烘焙、饮料、荤菜、蔬菜、快餐、麦片、餐点、配菜和融合。最后，我们使用Query-by-Committee和Human的混合方法，将剩余的1900K份食谱进行分类。

    Cooking recipes allow individuals to exchange culinary ideas and provide food preparation instructions. Due to a lack of adequate labeled data, categorizing raw recipes found online to the appropriate food genres is a challenging task in this domain. Utilizing the knowledge of domain experts to categorize recipes could be a solution. In this study, we present a novel dataset of two million culinary recipes labeled in respective categories leveraging the knowledge of food experts and an active learning technique. To construct the dataset, we collect the recipes from the RecipeNLG dataset. Then, we employ three human experts whose trustworthiness score is higher than 86.667% to categorize 300K recipe by their Named Entity Recognition (NER) and assign it to one of the nine categories: bakery, drinks, non-veg, vegetables, fast food, cereals, meals, sides and fusion. Finally, we categorize the remaining 1900K recipes using Active Learning method with a blend of Query-by-Committee and Human 
    
[^9]: 不冷静，不冷静，也不镇定：使用情感语言检测COVID-19的错误信息。

    Not cool, calm or collected: Using emotional language to detect COVID-19 misinformation. (arXiv:2303.16777v1 [cs.CL])

    [http://arxiv.org/abs/2303.16777](http://arxiv.org/abs/2303.16777)

    本论文提出一种使用情感语言检测COVID-19错误信息的模型，该模型相比于单一的错误信息分类器具有更好的效果，但该研究的主要限制因素是低质量标签和不匹配的标签分布。

    

    社交媒体平台如Twitter上的COVID-19错误信息对有效的疫情管理构成威胁。先前在推特上的COVID-19错误信息的工作否认了推特上普遍存在的诸如带电情感的语义特征的作用。因此，我们提出了一种新的COVID-19错误信息模型，该模型使用推特情感编码器和COVID-19错误信息编码器来预测推文是否包含COVID-19错误信息。我们的情感编码器在一组新的注释数据集上进行了微调，我们的COVID-19错误信息编码器在COVID-HeRA数据集的子集上进行了微调。实验结果表明，使用情感和错误信息编码器的组合比单独的错误信息分类器产生了更好的结果。此外，进行了广泛的结果分析，强调了低质量标签和不匹配的标签分布是我们研究的主要限制因素。

    COVID-19 misinformation on social media platforms such as twitter is a threat to effective pandemic management. Prior works on tweet COVID-19 misinformation negates the role of semantic features common to twitter such as charged emotions. Thus, we present a novel COVID-19 misinformation model, which uses both a tweet emotion encoder and COVID-19 misinformation encoder to predict whether a tweet contains COVID-19 misinformation. Our emotion encoder was fine-tuned on a novel annotated dataset and our COVID-19 misinformation encoder was fine-tuned on a subset of the COVID-HeRA dataset. Experimental results show superior results using the combination of emotion and misinformation encoders as opposed to a misinformation classifier alone. Furthermore, extensive result analysis was conducted, highlighting low quality labels and mismatched label distributions as key limitations to our study.
    
[^10]: 带正则化上下文建模的会议行动项检测

    Meeting Action Item Detection with Regularized Context Modeling. (arXiv:2303.16763v1 [cs.CL])

    [http://arxiv.org/abs/2303.16763](http://arxiv.org/abs/2303.16763)

    本研究提出了一种轻量级模型集成方法和上下文丢失技术，用于自动检测带有行动项的会议内容，并构建并发布了第一个带有手动行动项注释的中文会议语料库，实验结果表明提出的方法具有较好的准确性和鲁棒性。

    

    会议对于协作越来越重要。会议内容中的行动项对于管理会后任务至关重要，而这通常需要耗费大量时间进行总结。行动项检测任务旨在自动检测与行动项相关的会议内容。然而，用于手动注释行动项检测标签的数据集很少且规模较小。我们构建并发布了第一个带有手动行动项注释的中文会议语料库。此外，我们提出了一种上下文丢失方法，通过对比学习利用局部和全局上下文，并实现了更好的行动项检测准确性和鲁棒性。我们还提出了一个轻量级模型集成方法来利用不同的预训练模型。我们在我们的中文会议语料库和英语AMI语料库上的实验结果表明了所提出方法的有效性。

    Meetings are increasingly important for collaborations. Action items in meeting transcripts are crucial for managing post-meeting to-do tasks, which usually are summarized laboriously. The Action Item Detection task aims to automatically detect meeting content associated with action items. However, datasets manually annotated with action item detection labels are scarce and in small scale. We construct and release the first Chinese meeting corpus with manual action item annotations. In addition, we propose a Context-Drop approach to utilize both local and global contexts by contrastive learning, and achieve better accuracy and robustness for action item detection. We also propose a Lightweight Model Ensemble method to exploit different pre-trained models. Experimental results on our Chinese meeting corpus and the English AMI corpus demonstrate the effectiveness of the proposed approaches.
    
[^11]: 使用蚁群优化的新型词性标注方法ACO-tagger

    ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony Optimization. (arXiv:2303.16760v1 [cs.CL])

    [http://arxiv.org/abs/2303.16760](http://arxiv.org/abs/2303.16760)

    本研究提出了一种基于蚁群优化的高效词性标注方法ACO-tagger，实现了高达96.867％的准确率，优于几种最先进的方法。

    

    近年来，群智能算法因其解决复杂和非确定性问题的能力而备受关注。这些算法受自然生物的集体行为启发，模拟这种行为以开发用于计算任务的智能 agent。其中一种算法是受到蚂蚁觅食行为及其信息素释放机制启发的蚁群优化（ACO）算法，用于解决离散和组合性的困难问题。词性标注是自然语言处理中的基础任务，旨在为句子中的每个单词分配一个词性角色。本研究提出了一种基于ACO的高性能词性标注方法ACO-tagger。该方法实现了高达96.867％的准确率，优于几种最先进的方法。该方法快速高效，是实际应用的可行选择。

    Swarm Intelligence algorithms have gained significant attention in recent years as a means of solving complex and non-deterministic problems. These algorithms are inspired by the collective behavior of natural creatures, and they simulate this behavior to develop intelligent agents for computational tasks. One such algorithm is Ant Colony Optimization (ACO), which is inspired by the foraging behavior of ants and their pheromone laying mechanism. ACO is used for solving difficult problems that are discrete and combinatorial in nature. Part-of-Speech (POS) tagging is a fundamental task in natural language processing that aims to assign a part-of-speech role to each word in a sentence. In this research paper, proposed a high-performance POS-tagging method based on ACO called ACO-tagger. This method achieved a high accuracy rate of 96.867%, outperforming several state-of-the-art methods. The proposed method is fast and efficient, making it a viable option for practical applications.
    
[^12]: 探究名人对公众态度影响的研究：基于社交媒体情感分析的 COVID-19 研究

    Exploring celebrity influence on public attitude towards the COVID-19 pandemic: social media shared sentiment analysis. (arXiv:2303.16759v1 [cs.CL])

    [http://arxiv.org/abs/2303.16759](http://arxiv.org/abs/2303.16759)

    本文研究了公众人物在社交媒体上共享的信息对 COVID-19 疫情中的公众情感和大众意见的影响。通过收集和分析推文，发现公众人物的信息对公众情感和大众意见具有显著的影响。

    

    COVID-19 疫情为健康沟通带来了新机遇，增加了公众使用在线渠道获取与健康相关情绪的机会。人们已经转向社交媒体网络分享与 COVID-19 疫情影响相关的情感。本文研究了公众人物（即运动员、政治家、新闻工作者）共享的社交信息在决定整体公共话语方向中的作用。我们从 2020 年 1 月 1 日到 2022 年 3 月 1 日收集了约 1300 万条推特。使用一个经过调优的 DistilRoBERTa 模型计算了每条推文的情绪，该模型用于比较与公众人物提及同时出现的 COVID-19 疫苗相关推特发布。我们的发现表明，在 COVID-19 疫情的前两年里，与公众人物共享的信息同时出现的情感内容具有一致的模式，影响了公众舆论和大众。

    The COVID-19 pandemic has introduced new opportunities for health communication, including an increase in the public use of online outlets for health-related emotions. People have turned to social media networks to share sentiments related to the impacts of the COVID-19 pandemic. In this paper we examine the role of social messaging shared by Persons in the Public Eye (i.e. athletes, politicians, news personnel) in determining overall public discourse direction. We harvested approximately 13 million tweets ranging from 1 January 2020 to 1 March 2022. The sentiment was calculated for each tweet using a fine-tuned DistilRoBERTa model, which was used to compare COVID-19 vaccine-related Twitter posts (tweets) that co-occurred with mentions of People in the Public Eye. Our findings suggest the presence of consistent patterns of emotional content co-occurring with messaging shared by Persons in the Public Eye for the first two years of the COVID-19 pandemic influenced public opinion and larg
    
[^13]: 深度学习如何从电子病历中检测漏诊的额外诊断以适用于DRG

    How can Deep Learning Retrieve the Write-Missing Additional Diagnosis from Chinese Electronic Medical Record For DRG. (arXiv:2303.16757v1 [cs.CL])

    [http://arxiv.org/abs/2303.16757](http://arxiv.org/abs/2303.16757)

    该论文提出了一种使用深度学习的方法，从电子病历中检索漏诊的额外诊断，以适用于DRG分组，解决了漏诊问题导致医疗记录不完整、影响DRG招生正确率的问题。

    

    漏诊的检测的目的是找到已经在医疗记录中清晰诊断但被漏掉的疾病。不同于漏诊的定义，漏诊在医疗记录中明显表现，无须进一步推理。漏诊问题很常见，通常是由于医生疏忽造成的。漏诊会导致医疗记录的不完整性。在DRG的分组下，漏诊将错过重要的额外诊断（CC，MCC），从而影响DRG招生的正确率。在国家普遍开始采用DRG招生和支付的情况下，漏诊问题是普遍存在的严重问题。当前的基于手动方法由于全面医疗记录的复杂内容而昂贵。因此，我们认为自然语言处理可以用于解决这个问题。但据我所知，该论文提出了一种使用深度学习的方法，从中国电子病历中检索漏诊的额外诊断，以适用于DRG分组。

    The purpose of write-missing diagnosis detection is to find diseases that have been clearly diagnosed from medical records but are missed in the discharge diagnosis. Unlike the definition of missed diagnosis, the write-missing diagnosis is clearly manifested in the medical record without further reasoning. The write-missing diagnosis is a common problem, often caused by physician negligence. The write-missing diagnosis will result in an incomplete diagnosis of medical records. While under DRG grouping, the write-missing diagnoses will miss important additional diagnoses (CC, MCC), thus affecting the correct rate of DRG enrollment.  Under the circumstance that countries generally start to adopt DRG enrollment and payment, the problem of write-missing diagnosis is a common and serious problem. The current manual-based method is expensive due to the complex content of the full medical record. We think this problem is suitable to be solved as natural language processing. But to the best of
    
[^14]: LLM用于患者-试验匹配: 面向更好的性能和泛化能力的隐私感知数据增强

    LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability. (arXiv:2303.16756v1 [cs.CL])

    [http://arxiv.org/abs/2303.16756](http://arxiv.org/abs/2303.16756)

    本文提出了一种隐私感知数据增强的LLM-PTM方法，有效地提高了患者-试验匹配的性能和泛化能力。

    

    将患者与适合的临床试验进行匹配是推进医学研究和提供最佳护理的关键。然而，现有方法面临数据标准化、伦理考虑和电子健康记录与临床试验标准之间互操作性缺乏等挑战。在本文中，我们探索利用大型语言模型（LLMs）解决这些挑战的潜力，通过利用其先进的自然语言生成能力来改善EHRs和临床试验描述之间的兼容性。我们提出了一种创新的基于LLM的患者-试验匹配（LLM-PTM）的隐私感知数据增强方法，平衡了LLMs的好处，同时确保敏感患者数据的安全和保密。我们的实验表明，使用所提出的LLM-PTM方法，性能平均提高了7.32％，新数据的泛化能力提高了12.12％。此外，我们还提供了案例研究。

    The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12%. Additionally, we present case stud
    
[^15]: 使用语言反馈规模化训练语言模型

    Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])

    [http://arxiv.org/abs/2303.16755](http://arxiv.org/abs/2303.16755)

    本文提出一种新方法，即利用更丰富的语言反馈进行模仿学习，通过三个迭代步骤对语言模型进行训练以生成更符合人类偏好的输出。

    

    预训练的语言模型经常生成不符合人类偏好的输出，例如有害的文本或事实不正确的摘要。最近的研究尝试通过学习一种简单的人类反馈形式（即模型生成输出之间的比较）来解决这些问题。但是，比较反馈只能传达有限的关于人类偏好的信息。在本文中，我们介绍了一种新的方法——使用语言反馈进行模仿学习（ILF），它利用了更丰富的语言反馈。ILF由三个迭代步骤组成：第一步，根据输入，初始LM输出和反馈对语言模型进行调节以生成改进。第二步，选择最多反馈的改进。第三步，微调语言模型，以最大化在给定输入的情况下选择的改进的可能性。我们在理论上证明了ILF可以被看作是贝叶斯推断，类似于从人类反馈中进行强化学习。我们还评估了ILF在各种基准测试中的性能。

    Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
    
[^16]: 使用参数有效的结构将预训练语言模型扩展至更深的深度

    Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture. (arXiv:2303.16753v1 [cs.CL])

    [http://arxiv.org/abs/2303.16753](http://arxiv.org/abs/2303.16753)

    本文提出了一种参数有效的结构，通过MPO分解共享中央张量并保持层特定的辅助张量，将预训练语言模型扩展到更深的深度。

    

    本文提出了一种高度参数有效的方法，通过使用基于矩阵乘积算子（MPO）的更具能力的参数共享架构，将预训练语言模型（PLMs）扩展到更深的模型深度。通过MPO分解，我们的架构跨所有层共享中央张量以减少模型大小，并保持层特定的辅助张量以增强适应性灵活性。

    In this paper, we propose a highly parameter-efficient approach to scaling pre-trained language models (PLMs) to a deeper model depth. Unlike prior work that shares all parameters or uses extra blocks, we design a more capable parameter-sharing architecture based on matrix product operator (MPO). MPO decomposition can reorganize and factorize the information of a parameter matrix into two parts: the major part that contains the major information (central tensor) and the supplementary part that only has a small proportion of parameters (auxiliary tensors). Based on such a decomposition, our architecture shares the central tensor across all layers for reducing the model size and meanwhile keeps layer-specific auxiliary tensors (also using adapters) for enhancing the adaptation flexibility. To improve the model training, we further propose a stable initialization algorithm tailored for the MPO-based architecture. Extensive experiments have demonstrated the effectiveness of our proposed mo
    
[^17]: 司法智能助手系统：从离婚案件中提取事件以检测裁判中的争议

    Judicial Intelligent Assistant System: Extracting Events from Divorce Cases to Detect Disputes for the Judge. (arXiv:2303.16751v1 [cs.CL])

    [http://arxiv.org/abs/2303.16751](http://arxiv.org/abs/2303.16751)

    本文提出了一种基于两轮标注事件提取技术的离婚案件争议检测方法，实现了司法智能助手（JIA）系统，以自动从离婚案件材料中提取重点事件，通过识别其中的共指来对事件进行对齐，并检测冲突。

    

    在民事案件的正式程序中，由不同当事人提供的文本资料描述了案件的发展过程。从这些文本材料中提取案件的关键信息并澄清相关当事人的争议焦点是一项困难而必要的任务。本文提出了一种基于两轮标注事件提取技术的离婚案件争议检测方法。我们按照所提出的方法实现了司法智能助手（JIA）系统，以自动从离婚案件材料中提取重点事件，通过识别其中的共指来对事件进行对齐，并检测冲突。

    In formal procedure of civil cases, the textual materials provided by different parties describe the development process of the cases. It is a difficult but necessary task to extract the key information for the cases from these textual materials and to clarify the dispute focus of related parties. Currently, officers read the materials manually and use methods, such as keyword searching and regular matching, to get the target information. These approaches are time-consuming and heavily depending on prior knowledge and carefulness of the officers. To assist the officers to enhance working efficiency and accuracy, we propose an approach to detect disputes from divorce cases based on a two-round-labeling event extracting technique in this paper. We implement the Judicial Intelligent Assistant (JIA) system according to the proposed approach to 1) automatically extract focus events from divorce case materials, 2) align events by identifying co-reference among them, and 3) detect conflicts a
    
[^18]: 利用自然语言反馈进行代码生成的改进

    Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])

    [http://arxiv.org/abs/2303.16749](http://arxiv.org/abs/2303.16749)

    该论文提出了一种新算法ILF，通过从自然语言反馈中进行学习来显著提高代码生成模型的性能，即使只有少量反馈，也可以获得很好的效果。

    

    预先训练好的大型语言模型（LLM）在推理时使用自然语言反馈的潜力是最近的一个令人兴奋的发展。我们在此基础上提出一种名为Language Feedback（ILF）的算法，用于从自然语言反馈中进行学习。ILF在训练期间仅需要少量的人工编写反馈，并且在测试时不需要相同的反馈，因此使用起来既方便又高效。此外，我们进一步证明ILF可以被视为最小化与基准分布的KL散度的一种形式，并在神经程序合成任务上进行了概念验证。我们使用ILF在Mostly Basic Python Problems(MBPP)基准测试上将Codegen-Mono 6.1B模型的pass @ 1覆盖率相对提高了38%（绝对提高了10%），胜过了在MBPP上微调和在人类修复的程序上微调的模型。总的来说，我们的结果表明，即使只有少量反馈，从人类编写的自然语言反馈中进行学习也可以显著改进代码生成模型。

    The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h
    
[^19]: 评估自然语言生成系统：简介

    Evaluating NLG systems: A brief introduction. (arXiv:2303.16742v1 [cs.CL])

    [http://arxiv.org/abs/2303.16742](http://arxiv.org/abs/2303.16742)

    本文介绍了自然语言生成中评估的关键术语和区别，并阐述了INLG评估最佳论文奖设立的意义。

    

    今年，国际自然语言生成会议（INLG）将评选评估最佳论文奖项。该奖项的目的是为了激励NLG研究者们更加关注如何评估他们系统的输出。本文提供了一个简短的NLG评估导论，解释了关键术语和区别。

    This year the International Conference on Natural Language Generation (INLG) will feature an award for the paper with the best evaluation. The purpose of this award is to provide an incentive for NLG researchers to pay more attention to the way they assess the output of their systems. This essay provides a short introduction to evaluation in NLG, explaining key terms and distinctions.
    
[^20]: 科技写作辅助中的文本修订：概述

    Text revision in Scientific Writing Assistance: An Overview. (arXiv:2303.16726v1 [cs.CL])

    [http://arxiv.org/abs/2303.16726](http://arxiv.org/abs/2303.16726)

    本文概述了科技写作中的文本修订，包括科学写作的特点、常见格式和约定，以及各种类型的文本修订写作辅助工具，然而这些工具仍然存在许多挑战，如工具可访问性、对上下文的有限考虑和对话信息的含义不清等。

    

    科学论文撰写是一项具有挑战性的任务，因为它是一种高度规范化的文体。良好的写作技巧对于恰当地传达研究工作的思想和结果至关重要。由于大多数科学文章目前是用英语写的，因此非英语母语的人面临的语言问题更加困难。本文旨在提供科学领域写作辅助中文本修订的概述。我们将审查科学写作的特点，包括在研究文章中常用的格式和约定。此外，这篇概述将探讨文本修订的各种类型的写作辅助工具。尽管这些工具背后的技术经过了多年的发展，从基于规则的方法到基于深度神经的方法，但仍存在挑战（工具可访问性，对上下文的有限考虑，对话信息的含义不清等）。

    Writing a scientific article is a challenging task as it is a highly codified genre. Good writing skills are essential to properly convey ideas and results of research work. Since the majority of scientific articles are currently written in English, this exercise is all the more difficult for non-native English speakers as they additionally have to face language issues. This article aims to provide an overview of text revision in writing assistance in the scientific domain.  We will examine the specificities of scientific writing, including the format and conventions commonly used in research articles.  Additionally, this overview will explore the various types of writing assistance tools available for text revision. Despite the evolution of the technology behind these tools through the years, from rule-based approaches to deep neural-based ones, challenges still exist (tools' accessibility, limited consideration of the context, inexplicit use of discursive information, etc.)
    
[^21]: 使用语义相似性和文本嵌入衡量战略传播的社交媒体回响

    Using Semantic Similarity and Text Embedding to Measure the Social Media Echo of Strategic Communications. (arXiv:2303.16694v1 [cs.SI])

    [http://arxiv.org/abs/2303.16694](http://arxiv.org/abs/2303.16694)

    本文介绍了一种使用语义相似性来衡量特定信息传播后在线讨论变化的新技术，并使用环境新闻发布和气候变化推文来揭示了战略传播的响应呈重尾分布的特征。

    

    在线讨论涉及各种话题，许多参与者通过精心制作的信息和有针对性的活动影响在线讨论。然而，在线媒体内容的规模和多样性使得评估特定信息的影响变得困难。本文提出了一种利用语义相似性定量衡量特定信息发布后讨论变化的新技术。我们使用环境组织的新闻发布和气候变化辩论中的推文，展示了我们的新方法揭示了战略传播的在线讨论响应呈重尾分布的特点。

    Online discourse covers a wide range of topics and many actors tailor their content to impact online discussions through carefully crafted messages and targeted campaigns. Yet the scale and diversity of online media content make it difficult to evaluate the impact of a particular message. In this paper, we present a new technique that leverages semantic similarity to quantify the change in the discussion after a particular message has been published. We use a set of press releases from environmental organisations and tweets from the climate change debate to show that our novel approach reveals a heavy-tailed distribution of response in online discourse to strategic communications.
    
[^22]: 使用多语言变形金刚模型总结印度语言

    Summarizing Indian Languages using Multilingual Transformers based Models. (arXiv:2303.16657v1 [cs.CL])

    [http://arxiv.org/abs/2303.16657](http://arxiv.org/abs/2303.16657)

    本文研究了在印度语言作为源和目标文本的情况下使用多语言变形金刚模型进行摘要的表现，并报告了ROUGE分数。

    

    随着类似mBART、mT5、IndicBART等多语言模型的出现，低资源印度语言的总结正受到越来越多的关注。但是现有数据集的数量仍然很少。在本文中，我们( HakunaMatata团队)研究了这些多语言模型在印度语言作为源和目标文本的情况下执行摘要的表现。我们使用IndicBART和mT5模型进行实验，并报告ROUGE-1、ROUGE-2、ROUGE-3和ROUGE-4分数作为性能指标。

    With the advent of multilingual models like mBART, mT5, IndicBART etc., summarization in low resource Indian languages is getting a lot of attention now a days. But still the number of datasets is low in number. In this work, we (Team HakunaMatata) study how these multilingual models perform on the datasets which have Indian languages as source and target text while performing summarization. We experimented with IndicBART and mT5 models to perform the experiments and report the ROUGE-1, ROUGE-2, ROUGE-3 and ROUGE-4 scores as a performance metric.
    
[^23]: GPTEval：使用GPT-4和更好的人类对齐来评估NLG

    GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v1 [cs.CL])

    [http://arxiv.org/abs/2303.16634](http://arxiv.org/abs/2303.16634)

    本文介绍了GPTEval，一个利用链式思考和形式填充评价NLG生成的质量。实验表明，在文本摘要任务中，GPTEval结合GPT-4取得了0.514的斯皮尔曼相关系数，胜过其他方法。

    

    自然语言生成（NLG）系统生成的文本质量很难进行自动测量。传统的基于参考的度量标准，如BLEU和ROUGE已被证明在需要创造性和多样性的任务中与人类判断的相关性相对较低。最近的研究建议使用大型语言模型（LLMs）作为无参考的NLG评估度量标准，这些模型适用于缺乏人类参考的新任务。然而，这些基于LLM的评估器仍然比中等规模的神经评估器的人类对应度低。在这项工作中，我们提出了GPTEval，一个使用链式思考（CoT）和形式填充范式来评估NLG输出质量的框架。我们针对两个生成任务，文本摘要和对话生成进行了实验。我们展示出，GPTEval结合GPT-4作为骨干模型，在摘要任务上实现了0.514的斯皮尔曼相关系数，胜过其他方法。

    The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present GPTEval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that GPTEval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperform
    
[^24]: AraSpot：阿拉伯语口语命令识别

    AraSpot: Arabic Spoken Command Spotting. (arXiv:2303.16621v1 [cs.CL])

    [http://arxiv.org/abs/2303.16621](http://arxiv.org/abs/2303.16621)

    AraSpot是一款使用ConformerGRU模型架构训练40个阿拉伯语关键词的口语命令识别工具，其通过在线数据增强和文本到语音模型的训练提高了性能，并以99.59%的准确率超出以往的方法。

    

    口语关键识别（KWS）是指在音频流中识别关键词，广泛用于智能边缘设备上，以启动语音助手和进行免提任务。该任务面临着高精度和在低功率和可能的有限计算能力设备上保持系统运行效率的需求。本文介绍了使用不同的在线数据增强和引入ConformerGRU模型架构的AraSpot，用于训练40个阿拉伯语关键词的识别。最后，我们通过训练文本到语音模型进行合成数据生成，进一步提高了模型的性能。AraSpot以SOTA 99.59％超过以往的方法。

    Spoken keyword spotting (KWS) is the task of identifying a keyword in an audio stream and is widely used in smart devices at the edge in order to activate voice assistants and perform hands-free tasks. The task is daunting as there is a need, on the one hand, to achieve high accuracy while at the same time ensuring that such systems continue to run efficiently on low power and possibly limited computational capabilities devices. This work presents AraSpot for Arabic keyword spotting trained on 40 Arabic keywords, using different online data augmentation, and introducing ConformerGRU model architecture. Finally, we further improve the performance of the model by training a text-to-speech model for synthetic data generation. AraSpot achieved a State-of-the-Art SOTA 99.59% result outperforming previous approaches.
    
[^25]: 使用丰富的元数据注释的屏幕角色的个性化语言建模

    Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])

    [http://arxiv.org/abs/2303.16618](http://arxiv.org/abs/2303.16618)

    本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。

    

    语言模型的个性化为对话敏感，能更好地捕捉特定特征的人员和/或特定环境中的说话模式。然而，丰富的角色注释难以得到和成功利用。在此工作中，我们发布并描述了一组新颖的手动注释，涵盖了来自流行的 Cornell 电影对话语料库的 863 名演讲者，包括特征引用和角色描述，以及超过 95％ 的特色电影的一组自动提取的六个元数据。我们对两个语料库进行了广泛的实验，并表明可以有效地使用此类注释来个性化语言模型，将困惑减少高达 8.5％。我们的方法甚至可以应用于零样本的演讲者，即对于没有先前培训数据的演讲者，依赖于角色的人口特征的组合。由于收集此类元数据成本高昂，因此我们还贡献了一项成本效益分析，以突出显示

    Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
    
[^26]: LMExplainer：一种加强语言模型解释能力的知识提升模块

    LMExplainer: a Knowledge-Enhanced Explainer for Language Models. (arXiv:2303.16537v1 [cs.CL])

    [http://arxiv.org/abs/2303.16537](http://arxiv.org/abs/2303.16537)

    LMExplainer是一种知识增强的语言模型解释模块，使用知识图和图注意力神经网络来提取关键决策信号，为用户提供可理解的解释。

    

    巨型语言模型（如GPT-4）非常强大，可以处理各种自然语言处理（NLP）任务。然而，由于多层非线性模型结构和数百万个参数，很难解释其结果。对于用户而言，了解模型的工作方式缺乏理解，可能使模型在现实世界的应用中具有不可靠性和危险性。大多数最近的工作利用注意力权重来提供模型预测的解释。但是，基于注意力的解释无法支持不断增长的模型复杂性，并且无法推理其决策过程。因此，我们提出了LMExplainer，一种为语言模型提供人类可理解解释的知识增强模块。我们使用知识图和图注意力神经网络来提取LM的关键决策信号。同时，我们探讨解释能否也帮助人工智能更好地理解任务。

    Large language models (LMs) such as GPT-4 are very powerful and can process different kinds of natural language processing (NLP) tasks. However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. Lack of understanding of how the model works can make the model unreliable and dangerous for everyday users in real-world scenarios. Most recent works exploit the weights of attention to provide explanations for model predictions. However, pure attention-based explanation is unable to support the growing complexity of the models, and cannot reason about their decision-making processes. Thus, we propose LMExplainer, a knowledge-enhanced interpretation module for language models that can provide human-understandable explanations. We use a knowledge graph (KG) and a graph attention neural network to extract the key decision signals of the LM. We further explore whether interpretation can also help AI understand the task better
    
[^27]: 建立分布式分类账技术知识图谱

    Building a Knowledge Graph of Distributed Ledger Technologies. (arXiv:2303.16528v1 [cs.CL])

    [http://arxiv.org/abs/2303.16528](http://arxiv.org/abs/2303.16528)

    本文建立了一个包括安全、应用领域、标准法规等方面的分布式分类账技术的知识图谱，有助于拓展人们对该技术的理解和寻找新的用例。

    

    近年来，分布式分类账系统越来越受到关注和成功推广，尤其是区块链和加密货币方面。这导致了人们对该技术及其能力的各种误解，因为很多情况下，区块链和加密货币被视作同义词，其他用途也被经常忽视。因此，对分布式分类账技术的认识和应用被限制于区块链和加密货币领域。现有的词汇表和本体论往往只关注技术的某些方面，有时甚至只关注单个产品，这可能会忽略其他类型的分布式分类账及其潜在的用途。本文提出了一个分布式分类账技术的知识图谱和本体论，包括安全考虑、应用领域、相关标准和法规等，以改善对该技术的整体理解，并有助于发现区块链和加密货币以外的新用例。

    Distributed ledger systems have become more prominent and successful in recent years, with a focus on blockchains and cryptocurrency. This has led to various misunderstandings about both the technology itself and its capabilities, as in many cases blockchain and cryptocurrency is used synonymously and other applications are often overlooked. Therefore, as a whole, the view of distributed ledger technology beyond blockchains and cryptocurrencies is very limited. Existing vocabularies and ontologies often focus on single aspects of the technology, or in some cases even just on one product. This potentially leads to other types of distributed ledgers and their possible use cases being neglected. In this paper, we present a knowledge graph and an ontology for distributed ledger technologies, which includes security considerations to model aspects such as threats and vulnerabilities, application domains, as well as relevant standards and regulations. Such a knowledge graph improves the over
    
[^28]: 更大的探针讲述不同的故事: 通过上下文学习扩展心理语言学数据集

    Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning. (arXiv:2303.16445v1 [cs.CL])

    [http://arxiv.org/abs/2303.16445](http://arxiv.org/abs/2303.16445)

    本文通过上下文学习扩展否定和角色反转数据集，发现过去的结论可能被小型测试集误导。同时，BERT和ALBERT等模型表现出较高的否定敏感度。

    

    语言模型探测通常用来测试这些模型的特定能力。然而，当探测基准小且缺乏统计功效时，这类研究的结论可能受到限制。在这项工作中，我们介绍了受心理语言学研究启发的否定（NEG-1500-SIMP）和角色反转（ROLE-1500）的新的、更大的数据集。我们使用GPT3将现有的NEG-136和ROLE-88基准进行了大幅扩展，将它们的规模从18和44个句对分别增加到了750个。我们还创建了另一个使用基于模板的生成创建的扩展否定数据集(NEG-1500-SIMP-TEMP)，它由770个句对组成。我们在扩展数据集上评估了22个模型，发现模型性能与原始较小基准相比下降了20-57%。我们观察到BERT和ALBERT等模型具有较高的否定敏感性，这表明以前的研究结果可能由于较小的测试集而存在误差。最后，我们观察到，虽然GPT3生成了所有的实例，但句子的语法质量受到一些限制。

    Language model probing is often used to test specific capabilities of these models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It consists of 770 sentence pairs. We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks. We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets. Finally, we observe that while GPT3 has generated all the ex
    
[^29]: TaskMatrix.AI：将基础模型与数百万个API连接以完成任务

    TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs. (arXiv:2303.16434v1 [cs.AI])

    [http://arxiv.org/abs/2303.16434](http://arxiv.org/abs/2303.16434)

    TaskMatrix.AI是一个任务完成系统，可以将基础模型与数百万个API连接，提高完成任务的效率和全面性。

    

    人工智能（AI）近年来取得了令人瞩目的进展。本文提出了一个任务完成系统TaskMatrix.AI，它可以自动将基础模型与数百万个API连接以完成不同的任务。具体而言，TaskMatrix.AI使用API数据集来将基础模型的输入与可用的API进行匹配，从而完成任务。我们在几个开放域和领域特定任务上进行实验，证明了TaskMatrix.AI的效果和效率。

    Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation 
    
[^30]: ChatGPT是一个知识渊博但经验不足的问题求解器：对大型语言模型中常识问题的调查研究。

    ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models. (arXiv:2303.16421v1 [cs.CL])

    [http://arxiv.org/abs/2303.16421](http://arxiv.org/abs/2303.16421)

    ChatGPT是一个知识渊博但经验不足的LLM，能够回答常识问题，但在某些类型问题上仍存在困难。

    

    大型语言模型（LLMs），如ChatGPT和GPT-4，在NLP方面取得了重大进展。然而，它们记忆、表达和利用常识知识的能力一直是LLMs的一个众所周知的痛点。目前仍不清楚以下几点：（1）GPT能否有效回答常识问题？（2）GPT对常识知识是否精通？（3）GPT是否了解用于回答特定问题的底层常识知识？（4）GPT能否有效利用常识回答问题？为了评估以上常识问题，我们进行了一系列实验来评估ChatGPT的常识能力，实验结果表明：(1) GPT在常识任务中能够获得良好的问答准确性，但仍然无法解决某些类型的问题。(2) ChatGPT具有学识渊博，可以使用知识提示准确地产生大部分常识知识。(3)尽管具有知识，ChatGPT是一个缺乏经验的常识问题求解器，无法有效地利用常识知识回答某些问题。

    Large language models (LLMs) such as ChatGPT and GPT-4 have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point for LLMs. It remains unclear that: (1) Can GPTs effectively answer commonsense questions? (2) Are GPTs knowledgeable in commonsense? (3) Are GPTs aware of the underlying commonsense knowledge for answering a specific question? (4) Can GPTs effectively leverage commonsense for answering questions? To evaluate the above commonsense problems, we conduct a series of experiments to evaluate ChatGPT's commonsense abilities, and the experimental results show that: (1) GPTs can achieve good QA accuracy in commonsense tasks, while they still struggle with certain types of knowledge. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cann
    
[^31]: 利用ChatGPT进行零样本临床实体识别

    Zero-shot Clinical Entity Recognition using ChatGPT. (arXiv:2303.16416v1 [cs.CL])

    [http://arxiv.org/abs/2303.16416](http://arxiv.org/abs/2303.16416)

    本研究探讨了使用 ChatGPT 进行零样本临床实体识别任务，并发现 ChatGPT 在松弛匹配 F1 分数方面显著优于 GPT-3。虽然其性能仍低于 BioClinicalBERT 模型，但我们的研究表明了 ChatGPT 在零样本设置下有很大的临床 NER 任务潜力。

    

    本研究探讨了OpenAI开发的大型语言模型ChatGPT在2010年i2b2挑战中指定的临床命名实体识别任务中的潜力，使用两种不同的提示策略进行了零样本设置。同时，我们将其性能与GPT-3在类似的零样本设置下进行了比较，以及使用MTSamples的一组合成的临床笔记对BioClinicalBERT模型进行优化微调。研究结果显示，ChatGPT在零样本设置中表现优异，精确匹配和松弛匹配的F1分别为0.418（vs.0.250）和0.620（vs.0.480），相比之下，GPT-3的表现较差。另外，提示策略极大地影响了ChatGPT的性能，在两种不同提示策略下松弛匹配的F1分别为0.628和0.541。虽然ChatGPT的性能仍低于受监督的BioClinicalBERT模型（即松弛匹配F1分数分别为0.628和0.870），但我们的研究表明了ChatGPT在零样本设置下临床NER任务中的巨大潜力。

    In this study, we investigated the potential of ChatGPT, a large language model developed by OpenAI, for the clinical named entity recognition task defined in the 2010 i2b2 challenge, in a zero-shot setting with two different prompt strategies. We compared its performance with GPT-3 in a similar zero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of synthetic clinical notes from MTSamples. Our findings revealed that ChatGPT outperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250) and 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover, prompts affected ChatGPT's performance greatly, with relaxed-matching F1 scores of 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's performance was still lower than that of the supervised BioClinicalBERT model (i.e., relaxed-matching F1 scores of 0.628 vs. 0.870), our study demonstrates the great potential of ChatGPT for clinical NER tasks in a zero-shot setting, 
    
[^32]: 层次化视频瞬间检索和分步字幕

    Hierarchical Video-Moment Retrieval and Step-Captioning. (arXiv:2303.16406v1 [cs.CV])

    [http://arxiv.org/abs/2303.16406](http://arxiv.org/abs/2303.16406)

    这篇论文提出了HiREST数据集和一个新的基准，将分层信息检索和视觉/文本逐步总结从教学视频语料库中推广，使得在一个端到端的设置下可以共同搜索视频语料库，并生成摘要。

    

    目前人们在寻找大型视频语料库中的信息方面越来越感兴趣。先前的工作独立研究了相关任务，如基于文本的视频检索、瞬间检索、视频摘要和视频字幕，没有一个端到端的设置可以共同搜索视频语料库，并生成摘要。这样的端到端设置将允许许多有趣的应用程序，例如基于文本的搜索，从视频语料库中找到相关的视频，提取最相关的瞬间，并将瞬间分成重要的步骤，并加上字幕。为了解决这个问题，我们提出了HiREST(Hierarchical REtrieval and STep-captioning)数据集，并提出了一个新的基准，涵盖了来自教学视频语料库的分层信息检索和视觉/文本分阶段总结。HiREST由来自教学视频数据集的3.4K个文本-视频对组成，其中1.1K个视频具有与文本查询相关的瞬间跨度注释和细分。

    There is growing interest in searching for information from large video corpora. Prior works have studied relevant tasks, such as text-based video retrieval, moment retrieval, video summarization, and video captioning in isolation, without an end-to-end setup that can jointly search from video corpora and generate summaries. Such an end-to-end setup would allow for many interesting applications, e.g., a text-based search that finds a relevant video from a video corpus, extracts the most relevant moment from that video, and segments the moment into important steps with captions. To address this, we present the HiREST (HIerarchical REtrieval and STep-captioning) dataset and propose a new benchmark that covers hierarchical information retrieval and visual/textual stepwise summarization from an instructional video corpus. HiREST consists of 3.4K text-video pairs from an instructional video dataset, where 1.1K videos have annotations of moment spans relevant to text query and breakdown of e
    
[^33]: 用现有机器学习工具以超过99%的准确度区分ChatGPT和学术科学家的作者身份

    ChatGPT or academic scientist? Distinguishing authorship with over 99% accuracy using off-the-shelf machine learning tools. (arXiv:2303.16352v1 [cs.LG])

    [http://arxiv.org/abs/2303.16352](http://arxiv.org/abs/2303.16352)

    该论文设计了一种用于区分ChatGPT生成文本和学术科学家撰写文本的方法，并在2.5k个样本上测试，实现了99.2%的准确率。

    

    ChatGPT使广大群众能够访问AI生成的文章，在短短几个月内，这个产品颠覆了知识经济，引发了人们工作、学习和写作方式的文化变革。现在区分人类写作和AI写作的需求变得至关重要和紧迫，特别是在高等教育和学术写作等领域，AI在之前不曾是一个重要的威胁或者贡献者。针对这一需求，我们开发了一种区分ChatGPT生成文本和（人类）学术科学家撰写文本的方法，依靠普及且易于获取的监督分类方法。我们的重点在于，学术科学家这一特定人群的写作与ChatGPT有何不同，这种有针对性的方法导致我们发现了用于区分（这些）人类和AI的新特征。例如，科学家经常写长段落，喜欢使用模糊语言，经常使用but、however和although等词汇。我们训练和测试了这种方法，使用了2.5k个样本，成功实现了99.2%的准确率，区分ChatGPT和学术科学家的写作。

    ChatGPT has enabled access to AI-generated writing for the masses, and within just a few months, this product has disrupted the knowledge economy, initiating a culture shift in the way people work, learn, and write. The need to discriminate human writing from AI is now both critical and urgent, particularly in domains like higher education and academic writing, where AI had not been a significant threat or contributor to authorship. Addressing this need, we developed a method for discriminating text generated by ChatGPT from (human) academic scientists, relying on prevalent and accessible supervised classification methods. We focused on how a particular group of humans, academic scientists, write differently than ChatGPT, and this targeted approach led to the discovery of new features for discriminating (these) humans from AI; as examples, scientists write long paragraphs and have a penchant for equivocal language, frequently using words like but, however, and although. With a set of 2
    
[^34]: 利用语言指导的三模态一致性进行音频-视频源分离

    Language-Guided Audio-Visual Source Separation via Trimodal Consistency. (arXiv:2303.16342v1 [cs.CV])

    [http://arxiv.org/abs/2303.16342](http://arxiv.org/abs/2303.16342)

    该论文提出了一种自监督学习的方法，通过使用自然语言查询来进行音频源分离，实现了语言、视觉和音频的一致性对齐，并在多个数据集上表现出比现有方法更好的效果。

    

    我们提出了一种自监督学习的方法，基于自然语言查询在视频中学习执行音频源分离，仅使用未标记的视频和音频对作为训练数据。这项任务的一个关键挑战是学习将发声物体的语言描述与其视觉特征和相应的音频波形组件联系起来，而在训练期间没有访问注释。为了克服这个挑战，我们改进了现成的视觉语言基础模型，通过两种新的损失函数提供伪目标监督，并促进音频、视觉和自然语言模态之间更强的对齐。在推理过程中，我们的方法可以在给定文本、视频和音频输入或仅给定文本和音频输入时分离声音。我们在三个音频-视频分离数据集（包括MUSIC、SOLOS和AudioSet）上展示了我们自监督方法的有效性，其中我们的模型胜过了现有强监督方法的最新成果。

    We propose a self-supervised approach for learning to perform audio source separation in videos based on natural language queries, using only unlabeled video and audio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform, all without access to annotations during training. To overcome this challenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities. During inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone. We demonstrate the effectiveness of our self-supervised approach on three audio-visual separation datasets, including MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly supervised 
    
[^35]: 大象的透视镜：调查谷歌、ChatGPT、维基百科和YouTube上的语言偏见

    A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])

    [http://arxiv.org/abs/2303.16281](http://arxiv.org/abs/2303.16281)

    研究发现在Google、ChatGPT、维基百科和YouTube上，搜索结果受限于语言，反映了与复杂主题相关的文化刻板印象，缺乏跨文化视角。

    

    与谷歌搜索“从多个角度获取信息，以便你可以形成自己对世界的理解”的任务相反，我们发现谷歌及其最突出的搜索结果 - 维基百科和YouTube，仅反映与“佛教”、“自由主义”、“殖民化”、“伊朗”和“美国”等复杂主题相关的文化刻板印象。简单地说，在不同语言的相同搜索中，它们以不同程度呈现不同的信息（我们称之为“语言偏见”），而不是呈现复杂主题的全球图片。我们的在线搜索使我们成为谚语中的盲人，仅触摸小象的一小部分，不知道其他文化的视角的存在。我们用于搜索的语言最终成为促进本族中心主义观点的文化过滤器，其中一个人根据自己的文化评估其他人或思想。我们还发现ChatGPT中深深嵌入了语言偏见。

    Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
    
[^36]: 写作助手应该模拟语言的社会因素

    Writing Assistants Should Model Social Factors of Language. (arXiv:2303.16275v1 [cs.CL])

    [http://arxiv.org/abs/2303.16275](http://arxiv.org/abs/2303.16275)

    当前，由大型语言模型构建的智能写作助手越来越流行，但它们的表现并不优秀。作者认为，这种状况的主要原因是过于专注于语言的信息内容而忽略了其社会因素。他们提出将社会因素融入到写作助手的建设中，以构建智能、有效和个性化的写作助手，从而提高用户的使用体验和接受度。

    

    根据这篇文章，智能写作助手在当今比以往任何时候都更受欢迎，但他们的进一步普及受到亚优化表现的限制。作者们认为造成这种状态的主要原因是只关注语言的信息内容而忽略了其社会因素。本文分析了这些社会因素在写作助手中的不同方面，并提出将其纳入更智能、更有效和真正个性化的写作助手的建设中，从而丰富用户体验并促进用户采纳。

    Intelligent writing assistants powered by large language models (LLMs) are more popular today than ever before, but their further widespread adoption is precluded by sub-optimal performance. In this position paper, we argue that a major reason for this sub-optimal performance and adoption is a singular focus on the information content of language while ignoring its social aspects. We analyze the different dimensions of these social factors in the context of writing assistants and propose their incorporation into building smarter, more effective, and truly personalized writing assistants that would enrich the user experience and contribute to increased user adoption.
    
[^37]: 适用于低资源语言及字母词典的可扩展手写文字识别系统

    Scalable handwritten text recognition system for lexicographic sources of under-resourced languages and alphabets. (arXiv:2303.16256v1 [cs.CL])

    [http://arxiv.org/abs/2303.16256](http://arxiv.org/abs/2303.16256)

    本论文介绍了一种可扩展的手写文字识别系统，可用于破译历史未开发语言和字母的词典，提供了一个可以读取手写索引卡的解决方案，并将它们的引用链接到可搜索的词典条目列表，该系统可以高效地识别手写文本并取得了很高的准确率。

    

    本文介绍了一种破译历史词典大量手写索引卡的方法。我们提供了一个可工作的解决方案，读取这些索引卡，并将它们的引文链接到可搜索的词典条目列表，用于一个名为“17世纪和18世纪波兰语词典”的大型历史词典，其包括280万个索引卡。我们应用了量身定制的手写文本识别解决方案，包括（1）优化的检测模型；（2）用于解密手写内容的识别模型，设计为一个空间变换网络(STN)，后跟一层卷积神经网络（RCNN）和一个联结时序分类层(CTC)，使用一个由50万个不同长度的波兰语单词生成的合成数据集进行训练； (3) 使用受限制的Word Beam Search(WBC)进行后处理:预测被与事先已知的词典条目列表匹配。我们的模型在单词级别的准确度上达到了0.881。

    The paper discusses an approach to decipher large collections of handwritten index cards of historical dictionaries. Our study provides a working solution that reads the cards, and links their lemmas to a searchable list of dictionary entries, for a large historical dictionary entitled the Dictionary of the 17thand 18th-century Polish, which comprizes 2.8 million index cards. We apply a tailored handwritten text recognition (HTR) solution that involves (1) an optimized detection model; (2) a recognition model to decipher the handwritten content, designed as a spatial transformer network (STN) followed by convolutional neural network (RCNN) with a connectionist temporal classification layer (CTC), trained using a synthetic set of 500,000 generated Polish words of different length; (3) a post-processing step using constrained Word Beam Search (WBC): the predictions were matched against a list of dictionary entries known in advance. Our model achieved the accuracy of 0.881 on the word l
    
[^38]: 使用上下文摘要和领域架构的零样本泛化端到端任务导向对话系统

    Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using Context Summarization and Domain Schema. (arXiv:2303.16252v1 [cs.CL])

    [http://arxiv.org/abs/2303.16252](http://arxiv.org/abs/2303.16252)

    本论文提出了一种使用领域架构和对话历史摘要实现泛化的零样本端到端任务导向对话系统，并在实验中证明其在标准领域转移和少样本学习中优于现有方法。

    

    任务导向的对话系统通过促进直观和表达性的自然语言交互，使用户能够完成他们的目标。 在任务导向的对话系统中，最先进的方法将问题制定为条件序列生成任务，并在监督设置中微调预先训练的因果语言模型。 这需要为每个新领域或任务提供标记训练数据，并获取这样的数据是极其费力和昂贵的，从而使其成为扩展系统到各种领域的瓶颈。 为了克服这个挑战，我们引入了一种新的零样本泛化端到端任务导向对话系统ZS-ToD，它利用领域架构，允许对未知领域进行强大的泛化，并利用对话历史的有效摘要。 我们使用GPT-2作为骨干模型，并引入了一个两步训练过程，其中第一步的目标是学习对话数据的一般结构，第二步通过最大似然估计和强化学习的组合来优化端到端任务导向对话系统。 实验结果展示了所提出的方法在标准领域转移和少样本学习基准测试中优于现有的零样本任务导向对话系统。

    Task-oriented dialog systems empower users to accomplish their goals by facilitating intuitive and expressive natural language interactions. State-of-the-art approaches in task-oriented dialog systems formulate the problem as a conditional sequence generation task and fine-tune pre-trained causal language models in the supervised setting. This requires labeled training data for each new domain or task, and acquiring such data is prohibitively laborious and expensive, thus making it a bottleneck for scaling systems to a wide range of domains. To overcome this challenge, we introduce a novel Zero-Shot generalizable end-to-end Task-oriented Dialog system, ZS-ToD, that leverages domain schemas to allow for robust generalization to unseen domains and exploits effective summarization of the dialog history. We employ GPT-2 as a backbone model and introduce a two-step training process where the goal of the first step is to learn the general structure of the dialog data and the second step opti
    
[^39]: 没有正确性的可重复性并不重要：在NLP领域中测试代码的重要性。

    Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])

    [http://arxiv.org/abs/2303.16166](http://arxiv.org/abs/2303.16166)

    在NLP研究中，我们不能仅凭感知质量假定代码正确性，应该推动采用编码最佳实践以提高实验结果的正确性和可靠性。

    

    尽管其在研究实验中发挥了关键作用，但代码正确性往往仅基于结果的感知质量而被假定。这带来了错误结果和潜在误导性发现的风险。为了解决这个问题，我们认为当前关注结果重现应该与强调编码最佳实践相辅相成。我们通过一个案例研究来支持我们向NLP社区发出的号召，在这个案例研究中，我们识别出并纠正了广泛使用的最先进Conformer架构的开源实现中的三个Bug。通过在各种语言环境下进行的自动语音识别和翻译的比较实验，我们证明了Bug的存在并不会妨碍获得良好的和可重复的结果，反而可能导致不正确的结论，为未来的研究可能提供错误的指导。为了应对这一问题，这项研究呼吁采用旨在促进NLP研究中正确性的编码最佳实践，并提高实验结果的可靠性。

    Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
    
[^40]: TextMI:将多模态信息转化为文本形式，使预训练语言模型集成非语言线索

    TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models. (arXiv:2303.15430v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.15430](http://arxiv.org/abs/2303.15430)

    本文提出了一种将声学和视觉信息转化为文本描述并与口语文本串联，从而将非语言信息纳入预训练语言模型中的方法。该方法可以不需要昂贵的多模态数据收集和建模，并且可以使非语言线索与语言无缝集成，对于多模态行为理解任务具有实际应用意义。

    

    预训练大型语言模型最近在各种语言理解任务中取得了突破性的性能。然而，同一模型无法应用于多模式行为理解任务（例如，视频情感/幽默检测），除非可以将非语言特征（例如，声学和视觉）与语言集成。联合建模多个模态显着增加了模型复杂性，并使训练过程变得需数据量大。虽然通过网络可以获得大量的文本数据，但收集大规模的多模态行为视频数据集极其昂贵，无论是时间上还是金钱上。在本文中，我们研究了当以文本形式呈现的情况下，大型语言模型是否可以成功地将非语言信息纳入其中。我们提出了一种将声音和视觉信息转化为对应的文本描述，并将其与口语文本串联的方法。我们将这种增强的输入馈送给预先训练的语言模型，并显示其性能与直接使用非语言特征的相同模型相当。我们的方法TextMI不仅避免了昂贵的多模态数据收集和建模过程，而且实现了非语言线索与语言的无缝集成，这对于实际应用至关重要。

    Pre-trained large language models have recently achieved ground-breaking performance in a wide variety of language understanding tasks. However, the same model can not be applied to multimodal behavior understanding tasks (e.g., video sentiment/humor detection) unless non-verbal features (e.g., acoustic and visual) can be integrated with language. Jointly modeling multiple modalities significantly increases the model complexity, and makes the training process data-hungry. While an enormous amount of text data is available via the web, collecting large-scale multimodal behavioral video datasets is extremely expensive, both in terms of time and money. In this paper, we investigate whether large language models alone can successfully incorporate non-verbal information when they are presented in textual form. We present a way to convert the acoustic and visual information into corresponding textual descriptions and concatenate them with the spoken text. We feed this augmented input to a pr
    
[^41]: DBLP-QuAD：DBLP学术知识图上的问答数据集

    DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph. (arXiv:2303.13351v1 [cs.DL])

    [http://arxiv.org/abs/2303.13351](http://arxiv.org/abs/2303.13351)

    这篇论文在DBLP学术知识图上创建了一个包含10000个问题-答案对的问答数据集，是最大的学术问答数据集。

    

    本文在DBLP学术知识图上创建了一个问答数据集。DBLP是一个在线计算机科学主要出版物的参考文献信息索引，索引了超过440万篇论文，由220万多位作者发表。我们的数据集包含了10000个问题-答案对以及相应的SPARQL查询，可以在DBLP KG上执行以获得正确的答案。DBLP-QuAD是最大的学术问答数据集。

    In this work we create a question answering dataset over the DBLP scholarly knowledge graph (KG). DBLP is an on-line reference for bibliographic information on major computer science publications that indexes over 4.4 million publications published by more than 2.2 million authors. Our dataset consists of 10,000 question answer pairs with the corresponding SPARQL queries which can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD is the largest scholarly question answering dataset.
    
[^42]: 经过训练的1亿单词仍然保持状态：BERT结合英国国家语料库

    Trained on 100 million words and still in shape: BERT meets British National Corpus. (arXiv:2303.09859v1 [cs.CL])

    [http://arxiv.org/abs/2303.09859](http://arxiv.org/abs/2303.09859)

    本文探讨了在英国国家语料库上预训练的效果，并展示它可以比原始BERT模型达到更好的表现。在公平、可重复且数据有效的比较研究中，他们证明了这样的语料库有作为语言建模基准的巨大潜力。他们提出了一个经过优化的LM体系结构称为LTG-BERT。

    

    当前，现代遮蔽语言模型（LMs）训练的语料库规模越来越大。在本文中，我们探讨了缩小训练规模到一个规模适中、代表性好、平衡性好且公开可用的英文文本源-英国国家语料库的效果。我们展示了在这个精心策划的语料库上预训练可以达到比原始BERT模型更好的表现。我们认为这种类型的语料库具有作为语言建模基准的巨大潜力。为了展示这种潜力，我们以公平、可重复和数据有效的比较研究为特色，在其中评估了几个训练目标和模型架构，并以系统性的方式复制了先前的经验结果。我们提出了一个经过优化的LM体系结构称为LTG-BERT。

    While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source -the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.
    
[^43]: 利用ChatGPT和Prompt Learning将放射学报告翻译成通俗易懂的语言：结果、限制和潜力。

    Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. (arXiv:2303.09038v1 [cs.CL])

    [http://arxiv.org/abs/2303.09038](http://arxiv.org/abs/2303.09038)

    本文研究探讨利用ChatGPT将放射学报告翻译成通俗易懂的语言，平均得分为5分制的4.1分，信息缺失率和信息错误率均较低，ChatGPT提供的建议大都与放射学报告相关。

    

    ChatGPT作为一种大型语言模型，以其类似人类表达和推理能力而备受关注。本研究探讨使用ChatGPT将放射学报告翻译成通俗易懂的语言的可行性，以便患者和医疗服务提供者得到更好的医疗教育。研究采集了62份低剂量胸部CT肺癌筛查扫描和76份脑MRI转移性筛查扫描的放射学报告。根据放射科医师的评价，ChatGPT可以成功将放射学报告翻译成通俗易懂的语言，平均得分为5分制的4.1分，信息缺失0.07处，信息错误0.11处。就ChatGPT提供的建议而言，它们是一般性的相关建议，例如保持与医生的随访和密切监测任何症状，对于共138个病例中的约37％，ChatGPT提供了与放射学报告有关的建议。

    The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities. In this study, we investigate the feasibility of using ChatGPT in experiments on using ChatGPT to translate radiology reports into plain language for patients and healthcare providers so that they are educated for improved healthcare. Radiology reports from 62 low-dose chest CT lung cancer screening scans and 76 brain MRI metastases screening scans were collected in the first half of February for this study. According to the evaluation by radiologists, ChatGPT can successfully translate radiology reports into plain language with an average score of 4.1 in the five-point system with 0.07 places of information missing and 0.11 places of misinformation. In terms of the suggestions provided by ChatGPT, they are general relevant such as keeping following-up with doctors and closely monitoring any symptoms, and for about 37% of 138 cases in total ChatGPT offer
    
[^44]: 论ChatGPT的鲁棒性：对抗性和超出分布的视角。

    On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.12095](http://arxiv.org/abs/2302.12095)

    本研究评估了ChatGPT的鲁棒性，发现其在对抗性和超出分布任务上有一致的优势，但绝对表现仍有提高空间，鲁棒性仍是一个重要的挑战。

    

    ChatGPT是OpenAI最近发布的聊天机器人服务，并在过去几个月中受到越来越多的关注。虽然已对ChatGPT的各个方面进行了评估，但其鲁棒性，即对于未预期输入的表现，仍不清楚。鲁棒性在负责任的AI中特别受关注，特别是对于安全关键应用程序。在本文中，我们从对抗性和超出分布（OOD）的角度对ChatGPT的鲁棒性进行了彻底评估。为此，我们采用了AdvGLUE和ANLI基准来评估对抗性鲁棒性，采用Flipkart评论和DDXPlus医学诊断数据集进行OOD评估。我们选择了几个流行的基础模型作为基准。结果表明，ChatGPT在大多数对抗性和OOD分类和翻译任务上表现出一致的优势。但是，绝对的表现远非完美，这表明对抗性和OOD鲁棒性仍然是一个重要的威胁。

    ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat 
    
[^45]: CLIP是否捆绑概念？探索大型图像模型的组合性。

    Does CLIP Bind Concepts? Probing Compositionality in Large Image Models. (arXiv:2212.10537v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.10537](http://arxiv.org/abs/2212.10537)

    本文分析了大型神经网络模型CLIP的组合性能力以及以结构敏感的方式捆绑变量的能力，发现其能够在单一对象的情况下组合概念，但在需要概念捆绑的情况下性能显著下降。

    

    近年来，结合文本和图像的大型神经网络模型取得了令人瞩目的进展。然而，这些模型在多大程度上编码了它们操作的概念的组成性表示，如通过对“红色立方体”进行推理以正确识别“红色”和“立方体”这些成分，这仍然是一个开放性问题。本文关注一个大型预训练的视觉和语言模型（CLIP）编码组合概念的能力以及以结构敏感的方式捆绑变量的能力（例如区分“立方体在球体后面”和“球体在立方体后面”）。为了检查CLIP的性能，我们比较了许多来自组合分布语义模型（CDSMs）的架构，这是一种试图在嵌入空间中实现传统组合语言结构的研究方向。我们发现CLIP能够在单一对象的情况下组合概念，但在需要概念捆绑的情况下性能显著下降。我们的分析凸显了评估大型模型组合性的重要性，并为未来研究提出了方向。

    Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ''red cube'' by reasoning over the constituents ''red'' and ''cube''. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). In order to inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance
    
[^46]: 大型语言模型是带有自我验证的推理器

    Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.09561](http://arxiv.org/abs/2212.09561)

    本文提出了一种新的自我验证方法，使用CoT的结论来构建新样本并要求LLM重新预测原始条件，以提高推理准确性。实验证明，LLMs可以对其自己的结论进行自我验证并实现竞争性的推理性能。

    

    当大型语言模型（LLM）通过思维链（CoT）进行复杂推理时，它非常敏感于个别错误。为了解决这个问题，我们必须训练验证器。我们提出一种称为自我验证的新方法，该方法使用CoT的结论作为条件来构建一个新样本，并要求LLM重新预测被掩盖的原始条件。我们基于准确性计算可解释的验证分数。该方法可以在使用少量样本学习时提高多个算术和逻辑推理数据集的准确性。我们已经证明LLM可以对其自己的结论进行可解释的自我验证并实现竞争性的推理性能。全面的实验表明，我们的方法可以帮助多种带有自我验证功能的大型语言模型避免混淆。

    When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from inco
    
[^47]: 自监督语音模型的上下文感知微调研究

    Context-aware Fine-tuning of Self-supervised Speech Models. (arXiv:2212.08542v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2212.08542](http://arxiv.org/abs/2212.08542)

    本文研究了一种新方法，称为上下文感知微调。在微调期间使用上下文信息可以使模型在推理时进行预测，而无需访问这些周围片段。该方法能够有效减小运行开销，并在S数据集上取得了良好的评估结果。

    

    自监督预训练的transformer在各种语音任务方面都取得了突破。由于自注意力的二次时间和空间复杂度，它们通常在相对较短的片段（例如语音）级别上操作。本文研究了在微调期间使用上下文（即周围片段）并提出了一种新方法，称为上下文感知微调。我们在预训练模型的最后一层上附加了一个上下文模块，将整个片段编码为一个上下文嵌入向量，然后将其用作最终预测的另一个特征。在微调阶段，我们引入了一个辅助损失，鼓励该上下文嵌入向量与周围片段的上下文向量相似。这允许模型在推理时进行预测，而无需访问这些周围片段，与标准微调模型相比，仅需要极小的开销。我们使用S数据集完成了所提出方法的评估。

    Self-supervised pre-trained transformers have improved the state of the art on a variety of speech tasks. Due to the quadratic time and space complexity of self-attention, they usually operate at the level of relatively short (e.g., utterance) segments. In this paper, we study the use of context, i.e., surrounding segments, during fine-tuning and propose a new approach called context-aware fine-tuning. We attach a context module on top of the last layer of a pre-trained model to encode the whole segment into a context embedding vector which is then used as an additional feature for the final prediction. During the fine-tuning stage, we introduce an auxiliary loss that encourages this context embedding vector to be similar to context vectors of surrounding segments. This allows the model to make predictions without access to these surrounding segments at inference time and requires only a tiny overhead compared to standard fine-tuned models. We evaluate the proposed approach using the S
    
[^48]: 利用多任务预训练技术，提升自然语言解释的准确性

    Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations. (arXiv:2212.04231v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.04231](http://arxiv.org/abs/2212.04231)

    本文提出了一种新的解决方案利用多任务预训练生成Transformer模型的技术来应对VL-NLE任务的问题，有效提高了解释的准确性和可信度，具有良好的应用前景。

    

    在复杂的视觉语言任务中，自然语言解释能够提供对神经网络决策过程的直观理解。当前的VL-NLE模型在任务准确性和解释可信度上表现出色。然而，它们存在一系列问题：一些模型设计上存在缺陷，解释生成模块与任务答案预测模块集成不良；在训练骨干模型时，存在训练数据集较小的情况；模型采用临时性的解决方案来在单个数据集上提高性能等。我们提出了一种解决方案，利用最新的大规模多任务预训练生成Transformer模型的技术来应对VL-NLE任务的问题。我们的方法大大优于最近的模型，在三个评估数据集中，人类注释员更倾向于使用我们生成的解释而非真实解释。这是VL-NLE研究中的一项新挑战。

    Natural language explanations promise to offer intuitively understandable explanations of a neural network's decision process in complex vision-language tasks, as pursued in recent VL-NLE models. While current models offer impressive performance on task accuracy and explanation plausibility, they suffer from a range of issues: Some models feature a modular design where the explanation generation module is poorly integrated with a separate module for task-answer prediction, employ backbone models trained on limited sets of tasks, or incorporate ad hoc solutions to increase performance on single datasets. We propose to evade these limitations by applying recent advances in large-scale multi-task pretraining of generative Transformer models to the problem of VL-NLE tasks. Our approach outperforms recent models by a large margin, with human annotators preferring the generated explanations over the ground truth in two out of three evaluated datasets. As a novel challenge in VL-NLE research,
    
[^49]: 使用任务算术编辑模型

    Editing Models with Task Arithmetic. (arXiv:2212.04089v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04089](http://arxiv.org/abs/2212.04089)

    本文提出了一种使用任务向量进行模型编辑的新范式，任务向量可通过算术操作进行修改和组合，可以提高目标任务性能且对控制任务影响较小。

    

    改变预训练模型的行为方式（比如提高其在下游任务上的表现或减轻预训练期间学习到的偏差）是开发机器学习系统时常见的做法。本文提出了一种围绕“任务向量”来引导神经网络行为的新范式。任务向量指定了一个方向，即预训练模型权重空间中的方向，沿着该方向移动可以提高任务的表现。我们通过从经过微调任务后的相同模型的权重中减去预训练模型的权重来构建任务向量。我们展示了这些任务向量可以通过否定和加法等算术操作进行修改和组合，从而引导生成模型的行为。否定任务向量会降低目标任务的性能，而对控制任务的模型行为影响不大。此外，将任务向量相加可以提高目标任务的性能和控制任务的模型行为。

    Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performanc
    
[^50]: 基于Prompt学习与传播结构的零样本谣言检测

    Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01117](http://arxiv.org/abs/2212.01117)

    本文提出了一种基于Prompt学习和传播结构的零样本谣言检测框架，其能够有效地检测不同领域和语言的谣言，并可适应非预料中断事件的影响。

    

    在社交媒体时代，谣言随着事件的发生而传播，严重影响了真相的传播。之前的研究表明，由于缺乏标注资源，很难检测出使用少数语言的谣言。而且，昨天没有涉及到的非预料中断事件加剧了数据资源的稀缺性。本文提出了一种基于Prompt学习的新型零样本框架，用于检测不同领域或用不同语言展现的谣言。具体地说，我们首先将社交媒体上传播的谣言表示为多样的传播线程，然后设计了一个分层Prompt编码机制，学习了无语言环境下的上下文表示，以用于促进对不同领域和语言下的谣言数据转换。此外，我们从传播线程中建模领域不变的结构特征，以整合有影响力的社区反应的结构位置表示，以进一步增强领域适应性。同时，我们引入新的虚拟响应机制，以加强模型的推断能力。

    The spread of rumors along with breaking events seriously hinders the truth in the era of social media. Previous studies reveal that due to the lack of annotated resources, rumors presented in minority languages are hard to be detected. Furthermore, the unforeseen breaking events not involved in yesterday's news exacerbate the scarcity of data resources. In this work, we propose a novel zero-shot framework based on prompt learning to detect rumors falling in different domains or presented in different languages. More specifically, we firstly represent rumor circulated on social media as diverse propagation threads, then design a hierarchical prompt encoding mechanism to learn language-agnostic contextual representations for both prompts and rumor data. To further enhance domain adaptation, we model the domain-invariant structural features from the propagation threads, to incorporate structural position representations of influential community response. In addition, a new virtual respon
    
[^51]: 校准解释：语义解析中的置信度估计

    Calibrated Interpretation: Confidence Estimation in Semantic Parsing. (arXiv:2211.07443v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.07443](http://arxiv.org/abs/2211.07443)

    该论文研究了常见的生成模型在四个流行的语义解析数据集上的校准性，并分析了与校准误差相关的因素。为了方便将校准纳入语义解析评估中，作者们发布了一个计算校准度量的库。

    

    序列生成模型越来越被用来将语言翻译成可执行程序，即执行语义解析。语义解析旨在执行现实世界中的动作，因此开发安全系统是有必要的，而测量校准则是安全的核心组成部分，因此尤其重要。我们研究常见生成模型在四个流行的语义解析数据集上的校准性，发现其在模型和数据集之间变化巨大。然后，我们分析与校准误差相关的因素，并发布了两个解析数据集的基于置信度的挑战拆分。为了方便将校准纳入语义解析评估中，我们发布了一个用于计算校准度量的库。

    Sequence generation models are increasingly being used to translate language into executable programs, i.e. to perform executable semantic parsing. The fact that semantic parsing aims to execute actions in the real world motivates developing safe systems, which in turn makes measuring calibration -- a central component to safety -- particularly important. We investigate the calibration of common generation models across four popular semantic parsing datasets, finding that it varies across models and datasets. We then analyze factors associated with calibration error and release new confidence-based challenge splits of two parsing datasets. To facilitate the inclusion of calibration in semantic parsing evaluations, we release a library for computing calibration metrics.
    
[^52]: 神经网络中的 emergent 语言结构是脆弱的

    Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17406](http://arxiv.org/abs/2210.17406)

    本文提出了一个框架来评估语言模型对于语法的表述的一致性和稳健性，通过多项实验证据表明，神经网络中 emergent 语言结构是脆弱的。

    

    大型语言模型（LLM）在自然语言处理任务中表现强劲。然而，准确度等性能指标并不能衡量模型在代表复杂语言结构方面的质量。本文针对语言模型代表语法的能力，提出了一个评估语言表述的一致性和稳健性的框架。为此，我们介绍了一些稳健性的神经网络模型度量方式，这些度量方式利用最近在通过探测任务从LLM中提取语言结构的先进技术，即用于从语言模型中提取有意义信息的简单任务，如语法重构和根识别。实证上，我们通过分析四种LLM在六个不同的语料库上对语法保持扰动的性能和稳健性来研究所提出的稳健度量方式的表现。我们提供了证据

    Large Language Models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structure. In this paper, focusing on the ability of language models to represent syntax, we propose a framework to assess the consistency and robustness of linguistic representations. To this end, we introduce measures of robustness of neural network models that leverage recent advances in extracting linguistic constructs from LLMs via probing tasks, i.e., simple tasks used to extract meaningful information about a single facet of a language model, such as syntax reconstruction and root identification. Empirically, we study the performance of four LLMs across six different corpora on the proposed robustness measures by analysing their performance and robustness with respect to syntax-preserving perturbations. We provide evide
    
[^53]: 代码生成模型的多语言评估

    Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14868](http://arxiv.org/abs/2210.14868)

    本研究提出了多种用于评估代码生成模型的新基准测试，能够以多语言方式评估模型性能，并探讨了多语言模型优于单语言模型、少量提示能教授模型新语言以及在单语言设置下拥有零-shot翻译能力等问题。

    

    我们提出了新的基准测试，用于评估代码生成模型：MBXP和Multilingual HumanEval，以及MathQA-X。这些数据集涵盖了10种以上的编程语言，并使用可扩展的转换框架将原始Python数据集中的提示和测试用例转译成目标语言中的相应数据。利用这些基准测试，我们能够以多语言方式评估代码生成模型的性能，并发现了语言模型在跨领域语言上的泛化能力、多语言模型在单语言模型上的优势、少量提示教授模型新语言的能力，以及在单语言设置下的零-shot翻译能力。此外，我们使用我们的代码生成模型进行大规模引导，以获取多种语言的合成规范解，这些解可用于其他与代码相关的评估，如代码插入、鲁棒性或摘要任务。总的来说，

    We present new benchmarks on evaluation code generation models: MBXP and Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming languages and are generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language. Using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities even on mono-lingual settings. Furthermore, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks. Overall, 
    
[^54]: 带有搜索代理和混合环境的零样本检索

    Zero-Shot Retrieval with Search Agents and Hybrid Environments. (arXiv:2209.15469v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.15469](http://arxiv.org/abs/2209.15469)

    本文研究带有搜索代理和混合环境的零样本检索，实验表明，此方法可以超越传统的基于术语的检索和神经检索器，简单的启发式混合检索环境还可以将基线性能提高。

    

    学习搜索是建立人工智能代理的任务，这些代理可以学习使用搜索框来自主地查找信息。目前已经表明，目前的语言模型可以学习符号查询重构策略，结合传统的基于术语的检索，但无法超越神经检索器。我们将之前的学习搜索设置扩展到混合环境中，该环境在通过双编码器的第一遍检索步骤后接受离散的查询细化操作。在BEIR任务中的实验表明，通过行为克隆训练的搜索代理优于基于组合双编码器检索器和交叉编码器重新排名的底层搜索系统。此外，我们发现简单的启发式混合检索环境（HRE）可以将基线性能提高几个nDCG点。基于HRE的搜索代理（HARE）与最先进的性能相匹配，在零样本和领域内评估中都平衡，并具有可解释性行动。

    Learning to search is the task of building artificial agents that learn to autonomously use a search box to find information. So far, it has been shown that current language models can learn symbolic query reformulation policies, in combination with traditional term-based retrieval, but fall short of outperforming neural retrievers. We extend the previous learning to search setup to a hybrid environment, which accepts discrete query refinement operations, after a first-pass retrieval step via a dual encoder. Experiments on the BEIR task show that search agents, trained via behavioral cloning, outperform the underlying search system based on a combined dual encoder retriever and cross encoder reranker. Furthermore, we find that simple heuristic Hybrid Retrieval Environments (HRE) can improve baseline performance by several nDCG points. The search agent based on HRE (HARE) matches state-of-the-art performance, balanced in both zero-shot and in-domain evaluations, via interpretable action
    
[^55]: SmallCap：以检索增强为条件的轻量级图像字幕生成模型

    SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation. (arXiv:2209.15323v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.15323](http://arxiv.org/abs/2209.15323)

    SmallCap是一种轻量级图像字幕生成模型，通过从数据存储库中检索到相关字幕为条件生成文字描述。模型训练速度快，无需额外微调即可跨领域迁移，还能够充分利用数据存储库中的大规模数据。在实验中表现出竞争力，并可以通过利用人工标注和网络数据来进一步提高性能。

    

    近年来，图像字幕生成技术的进展主要集中在扩展数据和模型规模，大大增加了预训练和微调的成本。为了避免大模型的缺点，我们提出了SmallCap模型，通过从数据存储库中检索到的相关字幕为条件生成文字描述。该模型轻量且训练速度快，仅需要在预训练的CLIP编码器和GPT-2解码器之间加入新的交叉注意力层进行学习。SmallCap模型无需额外微调即可完成跨领域迁移，还能够轻松利用数据存储库中的大规模数据。实验表明，在仅使用COCO数据集进行训练的情况下，SmallCap模型在此基准测试中表现竞争力，并且仅通过从目标领域数据中检索就可以进行跨域迁移。通过充分利用人工标注和网络数据，SmallCap模型的性能还可以进一步提高。

    Recent advances in image captioning have focused on scaling the data and model size, substantially increasing the cost of pre-training and finetuning. As an alternative to large models, we present SmallCap, which generates a caption conditioned on an input image and related captions retrieved from a datastore. Our model is lightweight and fast to train, as the only learned parameters are in newly introduced cross-attention layers between a pre-trained CLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without additional finetuning and can exploit large-scale data in a training-free fashion since the contents of the datastore can be readily replaced. Our experiments show that SmallCap, trained only on COCO, has competitive performance on this benchmark, and also transfers to other domains without retraining, solely through retrieval from target-domain data. Further improvement is achieved through the training-free exploitation of diverse human-labeled and web data, whi
    
[^56]: 在拉丁美洲的自然语言处理中表现出的偏见和有害刻板印象的特征化方法

    A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America. (arXiv:2207.06591v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.06591](http://arxiv.org/abs/2207.06591)

    本研究提出了一种自动检测和表征拉丁美洲自然语言处理系统中偏见和有害刻板印象的方法。该方法基于单词嵌入之间的相似性和数据源的分析，并在西班牙语、葡萄牙语和克丘亚语的有害刻板印象检测案例中进行测试。结果表明，不同的自然语言处理系统以不同甚至意想不到的方式存在偏见和放大有害的刻板印象。

    

    自动化决策系统，尤其是基于自然语言处理的系统在我们的生活中无处不在。它们不仅是我们每天使用的互联网搜索引擎的背后，还有更为关键的作用：筛选工作候选人，确定犯罪嫌疑人，诊断自闭症等。这些自动化系统会出现错误，这些错误可能在很多方面都是有害的，无论是因为后果的严重性（例如健康问题）还是因为所涉及的人数之多。当由自动化系统造成的错误对某个群体的影响超过其他群体时，我们称此系统存在偏见。大多数现代自然语言技术是基于使用机器学习从大量文本中获得的分析结果，即语言模型和单词嵌入。由于它们是通过应用子符号机器学习创建的，主要是人工神经网络，所以它们是不透明的，且无法通过直接检查来解释，因此很难理解这些系统何时存在偏见或何时放大有害的刻板印象。在本文中，我们提出了一种方法，用于自动检测和表征自然语言处理系统中的偏见和有害刻板印象，应用于拉丁美洲语言。我们的方法基于分析来自不同地区的单词嵌入之间的相似性，并分析用于训练自然语言处理模型的数据源。我们在一个针对西班牙语、葡萄牙语和克丘亚语的有害刻板印象检测案例研究中测试了我们的方法。我们的研究结果表明，不同的自然语言处理系统以不同甚至意想不到的方式存在偏见和放大有害的刻板印象，这凸显了测试和改善此类系统质量的重要性。

    Automated decision-making systems, especially those based on natural language processing, are pervasive in our lives. They are not only behind the internet search engines we use daily, but also take more critical roles: selecting candidates for a job, determining suspects of a crime, diagnosing autism and more. Such automated systems make errors, which may be harmful in many ways, be it because of the severity of the consequences (as in health issues) or because of the sheer number of people they affect. When errors made by an automated system affect a population more than others, we call the system \textit{biased}.  Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to 
    
[^57]: Ousiometrics和Telegnomics：基于强度-弱度和危险-安全两个维度的意义本质框架，具有安全偏见的多样语料库。

    Ousiometrics and Telegnomics: The essence of meaning conforms to a two-dimensional powerful-weak and dangerous-safe framework with diverse corpora presenting a safety bias. (arXiv:2110.06847v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.06847](http://arxiv.org/abs/2110.06847)

    本文提出了ousiometrics和Telegnomics，发现单词传达的基本含义最好用指南针般的强度-危险（PD）框架来描述，而自然语言对安全低危险的词汇有系统偏见。

    

    我们定义“ousiometrics”为在任何有意义信号传递的上下文中研究基本意义的学科，“telegnomics”为远程感知知识的研究。通过中期出现的工作，基本意义已被普遍接受为由评估（evaluation）、效能（potency）和激活（activation）三个正交维度很好地捕捉。通过重新检查英语语言的类型和标记，以及通过使用自动注释的直方图——“ousiograms”，我们发现：1. 用单词传达的基本含义最好用指南针般的强度-危险（PD）框架来描述。2. 对大规模英语语言语料库（文学、新闻、维基百科、脱口秀和社交媒体）的分散集合进行分析显示，自然语言对安全、低危险的词汇存在系统偏见，这是对Pollyanna原则的书面表达积极偏差的重新解释。

    We define `ousiometrics' to be the study of essential meaning in whatever context that meaningful signals are communicated, and `telegnomics' as the study of remotely sensed knowledge. From work emerging through the middle of the 20th century, the essence of meaning has become generally accepted as being well captured by the three orthogonal dimensions of evaluation, potency, and activation (EPA). By re-examining first types and then tokens for the English language, and through the use of automatically annotated histograms -`ousiograms' -- we find here that: 1. The essence of meaning conveyed by words is instead best described by a compass-like power-danger (PD) framework, and 2. Analysis of a disparate collection of large-scale English language corpora -literature, news, Wikipedia, talk radio, and social media -- shows that natural language exhibits a systematic bias toward safe, low danger words -- a reinterpretation of the Pollyanna principle's positivity bias for written expres
    

