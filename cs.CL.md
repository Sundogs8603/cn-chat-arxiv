# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment.](http://arxiv.org/abs/2308.16175) | 本文引入了BSDetector，一种用于检测预训练大型语言模型生成的错误和推测性回答的方法。该方法通过估计置信度量化了回答的不确定性，并在闭合型和开放型问答基准实验中表现出更准确的识别能力。 |
| [^2] | [Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models.](http://arxiv.org/abs/2308.16149) | Jais和Jais-chat是新的以阿拉伯语为中心的开放式生成式大型语言模型，具有13亿参数，在阿拉伯语方面表现出优异的知识和推理能力，并且在英语方面也具有竞争力。这些模型的发布旨在促进阿拉伯语LLMs的研究。 |
| [^3] | [LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models.](http://arxiv.org/abs/2308.16137) | LM-Infinite研究了大规模语言模型在长序列上的长度推广失败问题，并提出了一种简单的即时推广方法，以更高效地利用现有模型的生成能力。 |
| [^4] | [Response: Emergent analogical reasoning in large language models.](http://arxiv.org/abs/2308.16118) | 该论文回应了关于大型语言模型中紧急类比推理的主张，并通过提供字符串类比的反例来反驳。在测试中，GPT-3无法解决最简单的类比问题。为了加强零点推理等人类推理的主张，需要发展出排除数据记忆的方法。 |
| [^5] | [Grandma Karl is 27 years old -- research agenda for pseudonymization of research data.](http://arxiv.org/abs/2308.16109) | 研究数据的共享面临个人和敏感信息保护的问题，该论文提出了一个关于假名化的研究议程，包括对假名化对无结构化数据的影响，假名化作为保护作者身份的有效性，以及开发上下文敏感算法用于处理个人信息。 |
| [^6] | [Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages.](http://arxiv.org/abs/2308.16075) | 该研究实证研究了神经机器翻译中利用多模态信息的有效性，发现在大规模预训练的单模态系统中添加图像特征可能是多余的。此外，该研究还引入了合成噪声来评估图像对处理文本噪声的帮助。实验结果表明，多模态模型在嘈杂的环境中略优于文本模型，即使是随机图像。研究在英语翻译为印地语、孟加拉语和马拉雅拉姆语时表现出色，且视觉背景对翻译效果的影响与源文本噪声有所不同。 |
| [^7] | [Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning.](http://arxiv.org/abs/2308.16061) | Conti公司的聊天记录泄露给我们提供了了解勒索软件服务运营商内部运作的机会。使用机器学习技术和可视化策略，研究发现业务、技术、内部任务管理、恶意软件和客户服务是Conti成员讨论的主要主题。 |
| [^8] | [Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap.](http://arxiv.org/abs/2308.16060) | Text-to-OverpassQL是一个使用自然语言查询OpenStreetMap中地理数据的界面，能够帮助用户制定复杂的数据库查询，并为序列生成模型提供一个基准。 |
| [^9] | [AsyncET: Asynchronous Learning for Knowledge Graph Entity Typing with Auxiliary Relations.](http://arxiv.org/abs/2308.16055) | 本文介绍了一种采用异步学习方法并引入多个辅助关系的知识图谱实体类型推断方法。实验结果表明，该方法在不同粒度的实体类型模式建模方面具有更强的表达能力。 |
| [^10] | [FPTQ: Fine-grained Post-Training Quantization for Large Language Models.](http://arxiv.org/abs/2308.15987) | 本研究提出了一种面向大型语言模型的细粒度训练后量化方法，结合了W4A8和W8A8两种方案的优点，通过逐层激活量化和细粒度权重量化来解决性能下降的问题。 |
| [^11] | [MerA: Merging Pretrained Adapters For Few-Shot Learning.](http://arxiv.org/abs/2308.15982) | 本文提出了一种名为MerA的方法，通过模型融合高效地将预训练的适配器合并到单个模型中，以解决少样本学习问题。实验证明，MerA相比于单个适配器和AdapterFusion都取得了实质性的改进。 |
| [^12] | [Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting.](http://arxiv.org/abs/2308.15961) | 本研究探讨了使用对应于解剖结构的局部令牌能否改进自动放射学报告生成的质量。为此，我们引入了一种新颖的Faster R-CNN方法，并在解剖结构定位期间进行异常检测。实验结果表明，使用局部令牌可以提高生成报告的准确性和质量。 |
| [^13] | [Benchmarking Multilabel Topic Classification in the Kyrgyz Language.](http://arxiv.org/abs/2308.15952) | 本研究提出了一个新的公共基准测试，用于在吉尔吉斯语中进行主题分类。在多标签设置下，我们引入了一个基于收集和注释数据的数据集，并提供了几个基线模型。评估结果表明，我们的方法在该任务上取得了良好的性能，为未来的研究提供了方向。 |
| [^14] | [LLaSM: Large Language and Speech Model.](http://arxiv.org/abs/2308.15930) | LLaSM是一个大型语言和语音模型，具有跨模态对话能力，通过遵循语音和语言指令，提供了一种方便自然的人机交互方式。 |
| [^15] | [Is the U.S. Legal System Ready for AI's Challenges to Human Values?.](http://arxiv.org/abs/2308.15906) | 美国法律需要加强应对生成式人工智能对人类价值观挑战的能力，并提供积极、可审计的指导，以填补现有法律框架在保护基本价值观方面的空白和不确定性。 |
| [^16] | [Towards One-Shot Learning for Text Classification using Inductive Logic Programming.](http://arxiv.org/abs/2308.15885) | 本文使用归纳逻辑编程的方法实现了一次性的文本分类，并通过使用常识背景知识和元解释学习框架，可以从少量训练样本中学习分类规则，且复杂度更高的样本可以达到更高的准确性。 |
| [^17] | [Knowledge-grounded Natural Language Recommendation Explanation.](http://arxiv.org/abs/2308.15813) | 本论文提出了一种基于知识图谱的自然语言可解释推荐方法，通过利用用户-物品特征产生基于事实的个性化解释，同时进行联合学习，以提高推荐系统的解释能力和用户的信心和信任。 |
| [^18] | [Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models.](http://arxiv.org/abs/2308.15812) | 本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。 |
| [^19] | [HAlf-MAsked Model for Named Entity Sentiment analysis.](http://arxiv.org/abs/2308.15793) | 在命名实体情感分析中，研究了解决基于BERT模型过拟合问题的多种方法，包括一种新颖的技术，在最终预测之前，对给定数据进行额外的掩码实体传递，以便在模型知道预测情感的确切实体和不知道的情况下，可以合并来自模型的逻辑。 |
| [^20] | [Task-Based MoE for Multitask Multilingual Machine Translation.](http://arxiv.org/abs/2308.15772) | 本论文介绍了一种基于任务的混合专家模型，将任务信息与MoE模型相结合，在多任务多语言机器翻译中取得了优越的结果，并且能够高效地应用于新的任务。 |
| [^21] | [Cyberbullying Detection for Low-resource Languages and Dialects: Review of the State of the Art.](http://arxiv.org/abs/2308.15745) | 本文总结了针对低资源语言中自动检测网络欺凌的研究现状，并发现了进一步发展的需求。 |
| [^22] | [Quantifying and Analyzing Entity-level Memorization in Large Language Models.](http://arxiv.org/abs/2308.15727) | 本研究提出了一种细粒度的、实体级的定义来量化大型语言模型中的记忆能力，并提出了一种高效提取敏感实体的方法。实验证明了语言模型在不同设置下重构敏感实体的能力。 |
| [^23] | [Optimizing Factual Accuracy in Text Generation through Dynamic Knowledge Selection.](http://arxiv.org/abs/2308.15711) | 本文提出了DKGen系统，通过动态选择知识参考，消除了文本生成过程中的非关联参考，从而提高了生成文本的事实准确性。 |
| [^24] | [Document AI: A Comparative Study of Transformer-Based, Graph-Based Models, and Convolutional Neural Networks For Document Layout Analysis.](http://arxiv.org/abs/2308.15517) | 本研究比较了基于Transformer、基于图的模型和卷积神经网络在文档布局分析方面的效果，揭示了它们的优劣势。先前的研究中缺乏对这些模型的比较分析。此外，虽然有语言无关的Document AI模型，但其性能仍存在限制。 |
| [^25] | [FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions.](http://arxiv.org/abs/2308.15214) | 本研究开发了一个交互式对话系统，将开放和封闭领域对话、脸部表情结合起来，通过使用LLMs和GPT-3.5模型来生成引人入胜的对话，以提供信息并与访客进行自然交流。 |
| [^26] | [SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT.](http://arxiv.org/abs/2308.15122) | 该论文提出了一种名为SpikeBERT的SNN模型，通过改进Spikformer架构和使用两阶段知识蒸馏方法，该模型在语言任务上超越了其他SNN模型，在文本分类任务上甚至达到了与BERT相当的结果。 |
| [^27] | [Adapting text-based dialogue state tracker for spoken dialogues.](http://arxiv.org/abs/2308.15053) | 这篇论文描述了对构建适应口语对话系统的文本对话状态跟踪器进行的工程工作，利用自动语音识别错误校正和文本对话系统实现了插槽和值的估计。 |
| [^28] | [Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks.](http://arxiv.org/abs/2308.14359) | 该论文探讨了注意力和自监督语音嵌入对非语义语音任务的影响，特别是情感理解。实验结果表明，基于自注意力的轻量级HuBERT-Large模型在这些任务中表现出色。 |
| [^29] | [Evaluating the Robustness to Instructions of Large Language Models.](http://arxiv.org/abs/2308.14306) | 本论文评估了大型语言模型对指令的鲁棒性。结果表明，指令微调可以提升中等规模模型的性能，并且模型对陌生指令的处理能力有待改进。 |
| [^30] | [EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression.](http://arxiv.org/abs/2308.13399) | 该论文提出了一种无监督的关键词提取方法，通过利用预训练语言模型和信息论方法，在文本中提取具有最高条件熵的短语作为关键词。实验证明，该方法在关键词提取任务上取得了与常用方法相当的结果。 |
| [^31] | [Formal specification terminology for demographic agent-based models of fixed-step single-clocked simulations.](http://arxiv.org/abs/2308.13081) | 本文提出了一种适用于人口学领域的Agent Based Models (ABMs)的数学规范的合适形式术语，这进一步提高了模型的理解，并与O.D.D.协议相结合，减少了模型复制过程中的歧义。 |
| [^32] | [LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework.](http://arxiv.org/abs/2308.10390) | 本论文提出了LibriSQA，一个自由形式和开放式的口语问答数据集和框架，通过改进ASR任务并使用轻量级的端到端框架，实现了在LLMs上执行口语问答任务的显著结果。 |
| [^33] | [Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment.](http://arxiv.org/abs/2308.09662) | 这项工作以红队评估的方式对大型语言模型进行安全评估，发现即便是广泛部署的模型也容易受到连续发言提示的影响，导致违反伦理地对有害查询做出回应。通过红队评估尝试，发现多数开源LLM也会生成有害回应。研究者提出了一种LLM安全对齐的方法。 |
| [^34] | [Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?.](http://arxiv.org/abs/2308.06032) | 本研究探讨了在加密货币证券案件中，大型语言模型（LLMs）是否能够准确判断违法行为，并比较了由LLM和律师撰写的投诉书对陪审团决策的影响。研究发现，目前的LLMs在法律推理方面表现较弱，但随着未来模型的改进，其潜力有望提升。 |
| [^35] | [WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine.](http://arxiv.org/abs/2308.05361) | WeaverBird是一个专为金融领域设计的智能对话系统，通过利用大型语言模型、本地知识库和搜索引擎，能够理解复杂的金融查询并提供明智的回答，具有增强的可信度。 |
| [^36] | [Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies.](http://arxiv.org/abs/2308.03188) | 本文总结了最近的研究，对多样化的自我纠正策略进行了分类和分析，以解决大型语言模型中的问题行为。自动化反馈技术被证明是一种可行的方法，可以使基于大型语言模型的解决方案更实用和可部署。 |
| [^37] | [Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering.](http://arxiv.org/abs/2307.15745) | Context-VQA通过引入上下文信息，提供了一种全面满足人们需求的视觉问答模型，该模型的创新在于将图像与不同上下文配对，并发现不同上下文下问题类型存在差异。 |
| [^38] | [Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models.](http://arxiv.org/abs/2307.08303) | 本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。 |
| [^39] | [Large Language Models are not Fair Evaluators.](http://arxiv.org/abs/2305.17926) | 本文揭示了使用大语言模型作为评估器时存在的系统偏差，可以通过改变候选响应的顺序来操纵评估结果。为了解决这个问题，提出了一个校准框架，包括多证据校准、均衡位置校准和人机协同校准。 |
| [^40] | [Evaluating GPT-3 Generated Explanations for Hateful Content Moderation.](http://arxiv.org/abs/2305.17680) | 本文通过调查和分析，评估了使用GPT-3生成的针对仇恨内容的解释是否准确和有用。结果显示，GPT-3生成的解释普遍存在过于模糊、聚焦不当等缺点，同时也存在不同类型仇恨言论生成的解释质量差异大的问题。 |
| [^41] | [MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers.](http://arxiv.org/abs/2305.09438) | 本文提出了一种基于Transformer模型的新方法MPI-rical，通过对大量代码片段进行训练实现自动化MPI代码生成，使并行化成为可能。 |
| [^42] | [A First Look at LLM-Powered Generative News Recommendation.](http://arxiv.org/abs/2305.06566) | 本文介绍了一种LLM驱动的生成式新闻推荐框架GENRE，它利用预训练语义知识丰富新闻数据，通过从模型设计转移到提示设计提供灵活而统一的解决方案，实现了个性化新闻生成、用户画像和新闻摘要。 |
| [^43] | [RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment.](http://arxiv.org/abs/2304.06767) | RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。 |
| [^44] | [Going Beyond Nouns With Vision & Language Models Using Synthetic Data.](http://arxiv.org/abs/2303.17590) | 本文在现有的VL模型中加入合成数据集SyViC，成功实现对'名词以外'的理解任务。 |
| [^45] | [Reliable Natural Language Understanding with Large Language Models and Answer Set Programming.](http://arxiv.org/abs/2302.03780) | 本研究提出了STAR框架，将大型语言模型与Answer Set Programming相结合，以达到可靠的自然语言理解。实验证明，该框架能够成功应对需要推理的不同任务，并提供可靠的结果。 |
| [^46] | [(QA)$^2$: Question Answering with Questionable Assumptions.](http://arxiv.org/abs/2212.10003) | 本研究提出了一个带有可疑假设的问答系统，通过构建一个包含自然生成的查询的数据集，并要求系统能够检测到和回答既包含典型信息检索问题又包含可疑假设的问题，以评估系统的性能。 |
| [^47] | [A Survey of Knowledge Enhanced Pre-trained Language Models.](http://arxiv.org/abs/2211.05994) | 本文综述了知识增强的预训练语言模型(KE-PLMs)，这是一种将知识融入预训练语言模型的方法，以解决模型推理能力不足的问题。该综述提供了对该领域的全面了解，并介绍了适用于自然语言理解和自然语言生成的分类法。 |
| [^48] | [CLSE: Corpus of Linguistically Significant Entities.](http://arxiv.org/abs/2211.02423) | CLSE语料库是为了解决自然语言生成中命名实体处理的问题而发布的，该语料库由语言学专家注释，包括34种语言和74种不同的语义类型，能够支持多种应用。 |

# 详细

[^1]: 通过内在和外在置信度评估来量化任意语言模型回答的不确定性

    Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment. (arXiv:2308.16175v1 [cs.CL])

    [http://arxiv.org/abs/2308.16175](http://arxiv.org/abs/2308.16175)

    本文引入了BSDetector，一种用于检测预训练大型语言模型生成的错误和推测性回答的方法。该方法通过估计置信度量化了回答的不确定性，并在闭合型和开放型问答基准实验中表现出更准确的识别能力。

    

    我们引入了BSDetector，一种通过估计预训练大型语言模型生成的任何输出的数值置信度来检测错误和推测性回答的方法。我们的不确定性量化技术适用于仅通过黑盒API访问的任何LLM，并将内在和外在评估的置信度结合为对给定提示下LLM响应的单个可信度估计。我们的方法非常通用，可以应用于当今所有最好的LLM（其训练数据未知）。通过额外的计算，任何LLM API的用户现在可以获得与通常相同的响应，以及一个置信度估计，以便在不信任该响应时保持谨慎。对于闭合型和开放型问答基准的实验表明，BSDetector比其他不确定性估计方法（对于GPT-3和ChatGPT）更准确地识别出错误的LLM响应。通过对多个响应进行采样

    We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, and combines intrinsic and extrinsic assessments of confidence into a single trustworthiness estimate for any LLM response to a given prompt. Our method is extremely general and can applied to all of the best LLMs available today (whose training data remains unknown). By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that caution when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses
    
[^2]: Jais和Jais-chat：以阿拉伯语为中心的基础和指导调优的开放式生成式大型语言模型

    Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])

    [http://arxiv.org/abs/2308.16149](http://arxiv.org/abs/2308.16149)

    Jais和Jais-chat是新的以阿拉伯语为中心的开放式生成式大型语言模型，具有13亿参数，在阿拉伯语方面表现出优异的知识和推理能力，并且在英语方面也具有竞争力。这些模型的发布旨在促进阿拉伯语LLMs的研究。

    

    我们介绍了Jais和Jais-chat，这是新的最先进的以阿拉伯语为中心的基础和指导调优的开放式生成式大型语言模型（LLMs）。这些模型基于GPT-3的仅解码器架构，并在阿拉伯语和英语文本的混合物中进行预训练，包括各种编程语言的源代码。这些模型具有130亿个参数，根据广泛的评估结果，在阿拉伯语知识和推理能力方面表现优于任何现有的开放式阿拉伯语和多语言模型。此外，尽管在训练时使用的英语数据要少得多，但这些模型在英语方面与类似规模的以英语为中心的开放模型相比仍具有竞争力。我们提供了模型的训练、调优、安全对齐和评估的详细描述。我们发布了模型的两个开放版本--基础Jais模型和指导调优的Jais-chat变种--旨在促进阿拉伯语LLMs的研究。详见https://hugging

    We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://hugging
    
[^3]: LM-Infinite: 大规模语言模型的简单即时长度推广

    LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])

    [http://arxiv.org/abs/2308.16137](http://arxiv.org/abs/2308.16137)

    LM-Infinite研究了大规模语言模型在长序列上的长度推广失败问题，并提出了一种简单的即时推广方法，以更高效地利用现有模型的生成能力。

    

    近年来，在Transformer-based大规模语言模型（LLM）在各个领域取得了显著的进展。随着这些LLM在越来越复杂的任务上的部署，它们往往面临着对长时间推理过程或理解更大上下文的需求。在这些情况下，LLM在长序列上的长度推广失败变得更加突出。大多数预训练方案将训练序列截断到固定长度（例如LLaMa的2048）。即使使用了相对位置编码来应对这个问题，LLM在更长的上下文之后往往难以生成流畅的文本，更不用说进行下游任务了。常见的解决方案，如在更长的语料库上进行微调，往往需要耗费大量的硬件和时间成本，并需要进行仔细的训练过程设计。为了更高效地利用现有LLM的生成能力，我们在理论和实证上研究了主要的分布外(OOD) f

    In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
    
[^4]: 回应：大型语言模型中的紧急类比推理

    Response: Emergent analogical reasoning in large language models. (arXiv:2308.16118v1 [cs.CL])

    [http://arxiv.org/abs/2308.16118](http://arxiv.org/abs/2308.16118)

    该论文回应了关于大型语言模型中紧急类比推理的主张，并通过提供字符串类比的反例来反驳。在测试中，GPT-3无法解决最简单的类比问题。为了加强零点推理等人类推理的主张，需要发展出排除数据记忆的方法。

    

    在最近的《自然人类行为》论文中，“大型语言模型中的紧急类比推理”（Webb，Holyoak和Lu，2023），作者们认为“像GPT-3这样的大型语言模型已经获得了发现广泛类比问题的零点解的紧急能力”。在本回应中，我们提供了一些字符串类比的反例。在我们的测试中，GPT-3甚至无法解决原始论文中提出的最简单的变体问题。零点推理是一个需要非常充分证据支持的非凡主张。在我们的实验中，我们没有看到这样的证据。为了加强像零点推理这样类似人类推理的主张，重要的是该领域开发出能够排除数据记忆的方法。

    In their recent Nature Human Behaviour paper, "Emergent analogical reasoning in large language models," (Webb, Holyoak, and Lu, 2023) the authors argue that "large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems." In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve even the easiest variants of the problems presented in the original paper. Zero-shot reasoning is an extraordinary claim that requires extraordinary evidence. We do not see that evidence in our experiments. To strengthen claims of humanlike reasoning such as zero-shot reasoning, it is important that the field develop approaches that rule out data memorization.
    
[^5]: Grandma Karl今年27岁——研究匿名化研究数据的议程

    Grandma Karl is 27 years old -- research agenda for pseudonymization of research data. (arXiv:2308.16109v1 [cs.CL])

    [http://arxiv.org/abs/2308.16109](http://arxiv.org/abs/2308.16109)

    研究数据的共享面临个人和敏感信息保护的问题，该论文提出了一个关于假名化的研究议程，包括对假名化对无结构化数据的影响，假名化作为保护作者身份的有效性，以及开发上下文敏感算法用于处理个人信息。

    

    研究数据的可访问性对于许多研究领域的进展至关重要，但由于其中包含个人和敏感信息（如姓名或政治观点），文本数据通常不能被共享。《通用数据保护条例》（GDPR）建议使用假名化作为一种解决方案，以确保对研究数据的开放访问的安全性，但在采用假名化方法处理研究数据之前，我们需要更多地了解假名化。本文概述了一个关于假名化的研究议程，包括对无结构化数据中假名化对可读性和语言评估等影响的研究需求，以及假名化作为保护作者身份的方式的有效性，并探讨了开发上下文敏感算法用于检测、标记和替换无结构化数据中个人信息的不同方法。最近获得的关于假名化的项目“Grandma Karl今年27岁”解决了其中一部分议程。

    Accessibility of research data is critical for advances in many research fields, but textual data often cannot be shared due to the personal and sensitive information which it contains, e.g names or political opinions. General Data Protection Regulation (GDPR) suggests pseudonymization as a solution to secure open access to research data, but we need to learn more about pseudonymization as an approach before adopting it for manipulation of research data. This paper outlines a research agenda within pseudonymization, namely need of studies into the effects of pseudonymization on unstructured data in relation to e.g. readability and language assessment, as well as the effectiveness of pseudonymization as a way of protecting writer identity, while also exploring different ways of developing context-sensitive algorithms for detection, labelling and replacement of personal information in unstructured data. The recently granted project on pseudonymization Grandma Karl is 27 years old address
    
[^6]: 视觉背景对嘈杂的多模态神经机器翻译的影响：对英印语言的实证研究

    Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages. (arXiv:2308.16075v1 [cs.CL])

    [http://arxiv.org/abs/2308.16075](http://arxiv.org/abs/2308.16075)

    该研究实证研究了神经机器翻译中利用多模态信息的有效性，发现在大规模预训练的单模态系统中添加图像特征可能是多余的。此外，该研究还引入了合成噪声来评估图像对处理文本噪声的帮助。实验结果表明，多模态模型在嘈杂的环境中略优于文本模型，即使是随机图像。研究在英语翻译为印地语、孟加拉语和马拉雅拉姆语时表现出色，且视觉背景对翻译效果的影响与源文本噪声有所不同。

    

    本研究调查了在神经机器翻译中利用多模态信息的有效性。先前的研究主要关注在资源匮乏的情况下使用多模态数据，而本研究则考察了将图像特征添加到大规模预训练的单模态神经机器翻译系统中的翻译效果。令人惊讶的是，研究发现在这种情况下图像可能是多余的。此外，该研究引入了合成噪声来评估图像是否有助于模型处理文本噪声。在嘈杂的环境中，即使是随机图像，多模态模型在性能上略优于文本模型。实验将英语翻译为印地语、孟加拉语和马拉雅拉姆语，结果显著优于最先进的基准。有趣的是，视觉背景的影响与源文本噪声有所不同：对于非噪声翻译，不使用视觉背景效果最好；对于低噪声，裁剪的图像特征最佳；在高噪声情况下，完整的图像特征效果更好。

    The study investigates the effectiveness of utilizing multimodal information in Neural Machine Translation (NMT). While prior research focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model deal with textual noise. Multimodal models slightly outperform text-only models in noisy settings, even with random images. The study's experiments translate from English to Hindi, Bengali, and Malayalam, outperforming state-of-the-art benchmarks significantly. Interestingly, the effect of visual context varies with source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features work better in high-noise scenarios. This she
    
[^7]: Conti公司：通过机器学习了解一个大型勒索软件服务运营商的内部讨论

    Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning. (arXiv:2308.16061v1 [cs.CR])

    [http://arxiv.org/abs/2308.16061](http://arxiv.org/abs/2308.16061)

    Conti公司的聊天记录泄露给我们提供了了解勒索软件服务运营商内部运作的机会。使用机器学习技术和可视化策略，研究发现业务、技术、内部任务管理、恶意软件和客户服务是Conti成员讨论的主要主题。

    

    勒索软件服务（RaaS）正在增加勒索软件攻击的规模和复杂性。了解RaaS背后的内部运作一直是个挑战，因为此类活动是非法的。最近Conti公司泄露的聊天记录给我们提供了一个了解这类组织内部运作的良机。本文使用自然语言处理（NLP）和潜在狄利克雷分配（LDA）等机器学习技术以及可视化策略，分析了Conti公司聊天记录中的主要主题讨论。发现了五个讨论主题：1）业务，2）技术，3）内部任务/管理，4）恶意软件，5）客户服务/问题解决。此外，Conti成员的主题分布显示，只有4%的人进行了专门的讨论，而几乎所有人（96%）都是全能型，意味着他们的讨论都围绕着这五个主题展开。

    Ransomware-as-a-service (RaaS) is increasing the scale and complexity of ransomware attacks. Understanding the internal operations behind RaaS has been a challenge due to the illegality of such activities. The recent chat leak of the Conti RaaS operator, one of the most infamous ransomware operators on the international scene, offers a key opportunity to better understand the inner workings of such organizations. This paper analyzes the main topic discussions in the Conti chat leak using machine learning techniques such as Natural Language Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as visualization strategies. Five discussion topics are found: 1) Business, 2) Technical, 3) Internal tasking/Management, 4) Malware, and 5) Customer Service/Problem Solving. Moreover, the distribution of topics among Conti members shows that only 4% of individuals have specialized discussions while almost all individuals (96%) are all-rounders, meaning that their discussions revolve aro
    
[^8]: Text-to-OverpassQL：一个用于查询OpenStreetMap复杂地理数据的自然语言界面

    Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap. (arXiv:2308.16060v1 [cs.CL])

    [http://arxiv.org/abs/2308.16060](http://arxiv.org/abs/2308.16060)

    Text-to-OverpassQL是一个使用自然语言查询OpenStreetMap中地理数据的界面，能够帮助用户制定复杂的数据库查询，并为序列生成模型提供一个基准。

    

    我们提出了Text-to-OverpassQL，这是一个旨在促进通过自然语言查询OpenStreetMap (OSM)中地理数据的任务。Overpass查询语言（OverpassQL）允许用户制定复杂的数据库查询，并在OSM生态系统中被广泛采用。从自然语言输入生成Overpass查询可以满足多个用例。它使新手用户能够在无需先前知识的情况下利用OverpassQL，帮助有经验的用户制作高级查询，并使工具增强的大型语言模型能够访问存储在OSM数据库中的信息。为了评估当前序列生成模型在这一任务上的性能，我们提出了OverpassNL，一个包含8,352个查询和相应自然语言输入的数据集。我们进一步介绍了任务特定的评估指标，并通过对OSM数据库执行查询来对Text-to-OverpassQL任务进行评估。我们通过微调序列到序列模型建立了强大的基线。

    We present Text-to-OverpassQL, a task designed to facilitate a natural language interface for querying geodata from OpenStreetMap (OSM). The Overpass Query Language (OverpassQL) allows users to formulate complex database queries and is widely adopted in the OSM ecosystem. Generating Overpass queries from natural language input serves multiple use-cases. It enables novice users to utilize OverpassQL without prior knowledge, assists experienced users with crafting advanced queries, and enables tool-augmented large language models to access information stored in the OSM database. In order to assess the performance of current sequence generation models on this task, we propose OverpassNL, a dataset of 8,352 queries with corresponding natural language inputs. We further introduce task specific evaluation metrics and ground the evaluation of the Text-to-OverpassQL task by executing the queries against the OSM database. We establish strong baselines by finetuning sequence-to-sequence models a
    
[^9]: 异步学习的知识图谱实体类型推断方法与辅助关系的应用

    AsyncET: Asynchronous Learning for Knowledge Graph Entity Typing with Auxiliary Relations. (arXiv:2308.16055v1 [cs.CL])

    [http://arxiv.org/abs/2308.16055](http://arxiv.org/abs/2308.16055)

    本文介绍了一种采用异步学习方法并引入多个辅助关系的知识图谱实体类型推断方法。实验结果表明，该方法在不同粒度的实体类型模式建模方面具有更强的表达能力。

    

    知识图谱实体类型推断（KGET）是预测知识图谱中缺失实体类型的任务。以前，知识图谱嵌入（KGE）方法通过引入辅助关系“hasType”来建模实体与其类型之间的关系，尝试解决KGET任务。然而，单个辅助关系在表达多样的实体类型模式方面具有限制性。我们通过引入多个辅助关系来提高KGE方法的表达能力。类似的实体类型被分组，以减少辅助关系的数量，并提高它们对不同粒度的实体类型模式建模的能力。在多个辅助关系存在的情况下，我们提出了一种名为AsyncET的实体类型异步学习方法，该方法通过交替更新实体和类型嵌入来保持学到的实体嵌入对于实体类型推断的最新和信息丰富。在两个常用的KGE数据集上进行了实验。

    Knowledge graph entity typing (KGET) is a task to predict the missing entity types in knowledge graphs (KG). Previously, KG embedding (KGE) methods tried to solve the KGET task by introducing an auxiliary relation, 'hasType', to model the relationship between entities and their types. However, a single auxiliary relation has limited expressiveness for diverse entity-type patterns. We improve the expressiveness of KGE methods by introducing multiple auxiliary relations in this work. Similar entity types are grouped to reduce the number of auxiliary relations and improve their capability to model entity-type patterns with different granularities. With the presence of multiple auxiliary relations, we propose a method adopting an Asynchronous learning scheme for Entity Typing, named AsyncET, which updates the entity and type embeddings alternatively to keep the learned entity embedding up-to-date and informative for entity type prediction. Experiments are conducted on two commonly used KGE
    
[^10]: FPTQ：面向大型语言模型的细粒度训练后量化

    FPTQ: Fine-grained Post-Training Quantization for Large Language Models. (arXiv:2308.15987v1 [cs.CL])

    [http://arxiv.org/abs/2308.15987](http://arxiv.org/abs/2308.15987)

    本研究提出了一种面向大型语言模型的细粒度训练后量化方法，结合了W4A8和W8A8两种方案的优点，通过逐层激活量化和细粒度权重量化来解决性能下降的问题。

    

    在大规模语言模型的时代中，庞大的参数大小给部署带来了巨大的挑战。作为一种流行的压缩技术，量化已经成为解决这个问题的主流实践，主要集中在两种方案W8A8和W4A16（即这两种位宽的权重和激活）。在本研究中，我们提出了一种新颖的W4A8训练后量化方法，用于现有的开放源码语言模型，结合了这两种方案的优点。因此，我们可以利用4位权重量化的I/O利用率优势和8位矩阵计算的加速效果。然而，W4A8面临着明显的性能下降问题。为了解决这个问题，我们采用了逐层激活量化策略，对于最棘手的层使用了一种新颖的对数均衡方法，并将其与细粒度权重量化相结合。在没有额外的调整的情况下，我们消除了进一步微调的必要性。

    In the era of large-scale language models, the substantial parameter size poses significant challenges for deployment. Being a prevalent compression technique, quantization has emerged as the mainstream practice to tackle this issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and activations in such bit widths). In this study, we propose a novel W4A8 post-training quantization method for the available open-sourced LLMs, which combines the advantages of both two recipes. Therefore, we can leverage the benefit in the I/O utilization of 4-bit weight quantization and the acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces notorious performance degradation. As a remedy, we involve layerwise activation quantization strategies which feature a novel logarithmic equalization for most intractable layers, and we combine them with fine-grained weight quantization. Without whistles and bells, we eliminate the necessity for further fine-tuning and obt
    
[^11]: MerA: 合并预训练的适配器用于少样本学习

    MerA: Merging Pretrained Adapters For Few-Shot Learning. (arXiv:2308.15982v1 [cs.CL])

    [http://arxiv.org/abs/2308.15982](http://arxiv.org/abs/2308.15982)

    本文提出了一种名为MerA的方法，通过模型融合高效地将预训练的适配器合并到单个模型中，以解决少样本学习问题。实验证明，MerA相比于单个适配器和AdapterFusion都取得了实质性的改进。

    

    适配器调优是一种主流方法，仅更新少量参数，用于将预训练语言模型微调到下游任务。然而，它经常在少样本学习中产生次优结果。AdapterFusion是一种可能的解决方案，它使用定制的组合层将预训练的适配器组合到特定任务中，但会显著增加可训练参数和部署成本。尽管如此，我们的初步研究发现，即使是单个适配器在少样本学习中也可以胜过AdapterFusion，这促使我们提出了\textbf{\texttt{合并预训练的适配器}} (MerA)，通过模型融合将预训练的适配器高效地融入到单个模型中。在两个PLMs上进行的广泛实验表明，MerA相比单个适配器和AdapterFusion都取得了实质性的改进。为了进一步增强MerA的能力，我们还引入了一种简单而有效的技术，称为“\textit{same-track}”设置，将适配器融合到同一个通道中。

    Adapter tuning, which updates only a few parameters, has become a mainstream method for fine-tuning pretrained language models to downstream tasks. However, it often yields subpar results in few-shot learning. AdapterFusion, which assembles pretrained adapters using composition layers tailored to specific tasks, is a possible solution but significantly increases trainable parameters and deployment costs. Despite this, our preliminary study reveals that even single adapters can outperform Adapterfusion in few-shot learning, urging us to propose \textbf{\texttt{Merging Pretrained Adapters}} (MerA) that efficiently incorporates pretrained adapters to a single model through model fusion. Extensive experiments on two PLMs demonstrate that MerA achieves substantial improvements compared to both single adapters and AdapterFusion. To further enhance the capacity of MerA, we also introduce a simple yet effective technique, referred to as the "\textit{same-track}" setting, that merges adapters f
    
[^12]: 寻找感知胸部X光报告中的解剖标记

    Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting. (arXiv:2308.15961v1 [cs.CV])

    [http://arxiv.org/abs/2308.15961](http://arxiv.org/abs/2308.15961)

    本研究探讨了使用对应于解剖结构的局部令牌能否改进自动放射学报告生成的质量。为此，我们引入了一种新颖的Faster R-CNN方法，并在解剖结构定位期间进行异常检测。实验结果表明，使用局部令牌可以提高生成报告的准确性和质量。

    

    放射学报告的任务包括描述和解释放射图像中的医学发现，包括其位置和外观的描述。自动化放射学报告需要将图像编码为适合输入语言模型的令牌表示。之前的方法通常使用卷积神经网络将图像编码为一系列图像级特征图表示。然而，生成的报告通常在逼真样式方面表现出众，但准确度不高。受最近在一般领域中用于图像字幕生成的工作的启发，其中每个视觉令牌对应于图像中检测到的对象，我们研究使用对应于解剖结构的局部令牌是否可以提高生成报告的质量。我们介绍了一种新颖的Faster R-CNN的适应性方法，在解剖结构定位期间为候选边界框执行了异常检测。我们使用了该结果作为令牌输入生成语言模型。

    The task of radiology reporting comprises describing and interpreting the medical findings in radiographic images, including description of their location and appearance. Automated approaches to radiology reporting require the image to be encoded into a suitable token representation for input to the language model. Previous methods commonly use convolutional neural networks to encode an image into a series of image-level feature map representations. However, the generated reports often exhibit realistic style but imperfect accuracy. Inspired by recent works for image captioning in the general domain in which each visual token corresponds to an object detected in an image, we investigate whether using local tokens corresponding to anatomical structures can improve the quality of the generated reports. We introduce a novel adaptation of Faster R-CNN in which finding detection is performed for the candidate bounding boxes extracted during anatomical structure localisation. We use the resu
    
[^13]: 在吉尔吉斯语中进行多标签主题分类的基准测试

    Benchmarking Multilabel Topic Classification in the Kyrgyz Language. (arXiv:2308.15952v1 [cs.CL])

    [http://arxiv.org/abs/2308.15952](http://arxiv.org/abs/2308.15952)

    本研究提出了一个新的公共基准测试，用于在吉尔吉斯语中进行主题分类。在多标签设置下，我们引入了一个基于收集和注释数据的数据集，并提供了几个基线模型。评估结果表明，我们的方法在该任务上取得了良好的性能，为未来的研究提供了方向。

    

    在本研究中，我们提出了一个新的公共基准测试，用于在吉尔吉斯语中进行主题分类，引入了一个基于新闻网站24.KG的收集和注释数据的数据集，并提供了多标签设置下新闻分类的几个基线模型。我们训练和评估了传统的统计模型和神经模型，并报告了得分，讨论了结果，并提出了未来工作的方向。

    Kyrgyz is a very underrepresented language in terms of modern natural language processing resources. In this work, we present a new public benchmark for topic classification in Kyrgyz, introducing a dataset based on collected and annotated data from the news site 24.KG and presenting several baseline models for news classification in the multilabel setting. We train and evaluate both classical statistical and neural models, reporting the scores, discussing the results, and proposing directions for future work.
    
[^14]: LLaSM: 大型语言和语音模型

    LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])

    [http://arxiv.org/abs/2308.15930](http://arxiv.org/abs/2308.15930)

    LLaSM是一个大型语言和语音模型，具有跨模态对话能力，通过遵循语音和语言指令，提供了一种方便自然的人机交互方式。

    

    最近，多模态大型语言模型引起了广泛关注。然而，大部分研究都集中在视觉-语言多模态模型上，提供了强大的能力来遵循视觉和语言指令。然而，我们认为语音也是人类与世界互动的重要方式。因此，对于一个通用的助手来说，能够遵循多模态语音和语言指令是至关重要的。在这项工作中，我们提出了大型语言和语音模型（LLaSM）。LLaSM是一个端到端训练的大型多模态语音-语言模型，具有跨模态对话能力，能够遵循语音和语言指令。我们的初步实验表明，LLaSM展示了一种更方便自然的人机交互方式。为了支持研究，我们还发布了一个大型的语音指令数据集LLaSM-Audio-Instructions。代码和演示可在https://github.com/LinkSoul-AI/LLaSM和ht上查看

    Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and ht
    
[^15]: 美国法律体系是否准备好应对人工智能对人类价值观的挑战？

    Is the U.S. Legal System Ready for AI's Challenges to Human Values?. (arXiv:2308.15906v1 [cs.CY])

    [http://arxiv.org/abs/2308.15906](http://arxiv.org/abs/2308.15906)

    美国法律需要加强应对生成式人工智能对人类价值观挑战的能力，并提供积极、可审计的指导，以填补现有法律框架在保护基本价值观方面的空白和不确定性。

    

    我们的跨学科研究调查了美国法律在面对生成式人工智能对人类价值观挑战时的有效性。通过分析专家研讨会期间制定的多种假设情景，我们发现现有法律框架在保护自主权、隐私权、尊严、多样性、平等以及身心健康等基本价值观方面存在明显的空白和不确定性。宪法和民权法似乎无法对人工智能生成的歧视性产出提供足够的保护。此外，即使我们排除第230条款提供的责任保护，由于人工智能系统的复杂和不透明性，证明诽谤和产品责任索赔的因果关系也是一项具有挑战性的任务。为了应对生成式人工智能带来的独特和难以预测的威胁，我们主张建立能够适应新威胁并为行业利益相关者提供积极、可审计的指导的法律框架。

    Our interdisciplinary study investigates how effectively U.S. laws confront the challenges posed by Generative AI to human values. Through an analysis of diverse hypothetical scenarios crafted during an expert workshop, we have identified notable gaps and uncertainties within the existing legal framework regarding the protection of fundamental values, such as autonomy, privacy, dignity, diversity, equality, and physical/mental well-being. Constitutional and civil rights, it appears, may not provide sufficient protection against AI-generated discriminatory outputs. Furthermore, even if we exclude the liability shield provided by Section 230, proving causation for defamation and product liability claims is a challenging endeavor due to the intricate and opaque nature of AI systems. To address the unique and unforeseeable threats posed by Generative AI, we advocate for legal frameworks that evolve to recognize new threat and provide proactive, auditable guidelines to industry stakeholders
    
[^16]: 使用归纳逻辑编程实现文本分类的一次性学习

    Towards One-Shot Learning for Text Classification using Inductive Logic Programming. (arXiv:2308.15885v1 [cs.LG])

    [http://arxiv.org/abs/2308.15885](http://arxiv.org/abs/2308.15885)

    本文使用归纳逻辑编程的方法实现了一次性的文本分类，并通过使用常识背景知识和元解释学习框架，可以从少量训练样本中学习分类规则，且复杂度更高的样本可以达到更高的准确性。

    

    随着人工智能在执行个性化任务方面的潜力不断增长，开发数据高效且不需要大量训练数据的新的机器学习技术变得至关重要。本文探索了一种基于归纳逻辑编程的一次性文本分类方法。特别是，我们探索了元解释学习（MIL）框架，并利用从ConceptNet中提取的常识背景知识。结果表明，MIL可以从少量的训练样本中学习文本分类规则。此外，选择的样本的复杂度越高，结果的准确性也越高。

    With the ever-increasing potential of AI to perform personalised tasks, it is becoming essential to develop new machine learning techniques which are data-efficient and do not require hundreds or thousands of training data. In this paper, we explore an Inductive Logic Programming approach for one-shot text classification. In particular, we explore the framework of Meta-Interpretive Learning (MIL), along with using common-sense background knowledge extracted from ConceptNet. Results indicate that MIL can learn text classification rules from a small number of training examples. Moreover, the higher complexity of chosen examples, the higher accuracy of the outcome.
    
[^17]: 基于知识的自然语言推荐解释

    Knowledge-grounded Natural Language Recommendation Explanation. (arXiv:2308.15813v1 [cs.CL])

    [http://arxiv.org/abs/2308.15813](http://arxiv.org/abs/2308.15813)

    本论文提出了一种基于知识图谱的自然语言可解释推荐方法，通过利用用户-物品特征产生基于事实的个性化解释，同时进行联合学习，以提高推荐系统的解释能力和用户的信心和信任。

    

    推荐系统的决策辅助用户理解的解释可以提高用户对系统的信心和信任。最近的研究主要集中在以人类可读的形式生成自然语言解释。然而，目前提出的方法往往利用用户编写的物品评论，这些评论通常主观，语言贫乏，并且无法涵盖未被购买或评论的新物品。相反，我们的目标是根据用户的购买历史，生成以物品特征客观描述的基于事实的推荐解释，并隐含考虑用户的偏好。为了实现这一目标，我们提出了一个基于知识图谱的自然语言可解释推荐方法。我们的方法通过一种新颖的基于协同过滤的知识图谱表示，利用用户-物品特征产生基于事实的个性化解释，同时进行联合学习。

    Explanations accompanied by a recommendation can assist users in understanding the decision made by recommendation systems, which in turn increases a user's confidence and trust in the system. Recently, research has focused on generating natural language explanations in a human-readable format. Thus far, the proposed approaches leverage item reviews written by users, which are often subjective, sparse in language, and unable to account for new items that have not been purchased or reviewed before. Instead, we aim to generate fact-grounded recommendation explanations that are objectively described with item features while implicitly considering a user's preferences, based on the user's purchase history. To achieve this, we propose a knowledge graph (KG) approach to natural language explainable recommendation. Our approach draws on user-item features through a novel collaborative filtering-based KG representation to produce fact-grounded, personalized explanations, while jointly learning
    
[^18]: 透过偏好看大型语言模型的反馈获取：揭示对齐的重要性

    Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])

    [http://arxiv.org/abs/2308.15812](http://arxiv.org/abs/2308.15812)

    本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。

    

    大型语言模型（LLMs）与人类价值观和意图的对齐承诺涉及使用人工智能或人类反馈。稠密的反馈注释获取和整合成本较高，而稀疏的反馈则涉及结构性设计选择，即评分（例如，在1-7的范围内对回答A进行评分）和排名（例如，回答A是否比回答B更好？）。在这项工作中，我们分析了这种设计选择对LLMs的对齐和评估的影响。我们发现，评分和排名所推断出的偏好在人类和AI注释者中都存在严重的不一致问题，达到了60%。我们的后续分析确定了解释这个现象的各种注释者偏见方面，比如人类注释者更喜欢密集回答并在两个选项之间更青睐准确性。令我们惊讶的是，我们还观察到反馈协议的选择对对齐的LLMs的评估也有显著影响。特别是，我们发现LLMs的评估结果因为反馈协议的选择而有所不同。

    Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
    
[^19]: HAlf-MAsked模型用于命名实体情感分析

    HAlf-MAsked Model for Named Entity Sentiment analysis. (arXiv:2308.15793v1 [cs.CL])

    [http://arxiv.org/abs/2308.15793](http://arxiv.org/abs/2308.15793)

    在命名实体情感分析中，研究了解决基于BERT模型过拟合问题的多种方法，包括一种新颖的技术，在最终预测之前，对给定数据进行额外的掩码实体传递，以便在模型知道预测情感的确切实体和不知道的情况下，可以合并来自模型的逻辑。

    

    命名实体情感分析（NESA）是自然语言处理（NLP）中最活跃的应用领域之一。社交媒体NESA是意见分析的重要领域，因为检测和跟踪新闻流中的情感趋势对于构建各种分析系统和监测特定人物或公司的媒体形象至关重要。在本文中，我们研究了基于transformer的不同解决方案在RuSentNE-23评估中的NESA。尽管BERT等模型的效果很好，但它们仍然可能在某些挑战上遇到困难，例如过拟合问题，这是在RuSentNE-23数据上实现高准确性的主要障碍。我们提出了几种方法来解决这个问题，其中包括一种新颖的技术，在最终预测之前，对给定数据进行额外的掩码实体传递，以便在模型知道预测情感的确切实体和不知道的情况下，可以合并来自模型的逻辑。利用

    Named Entity Sentiment analysis (NESA) is one of the most actively developing application domains in Natural Language Processing (NLP). Social media NESA is a significant field of opinion analysis since detecting and tracking sentiment trends in the news flow is crucial for building various analytical systems and monitoring the media image of specific people or companies. In this paper, we study different transformers-based solutions NESA in RuSentNE-23 evaluation. Despite the effectiveness of the BERT-like models, they can still struggle with certain challenges, such as overfitting, which appeared to be the main obstacle in achieving high accuracy on the RuSentNE-23 data. We present several approaches to overcome this problem, among which there is a novel technique of additional pass over given data with masked entity before making the final prediction so that we can combine logits from the model when it knows the exact entity it predicts sentiment for and when it does not. Utilizing 
    
[^20]: 基于任务的混合专家模型用于多任务多语言机器翻译

    Task-Based MoE for Multitask Multilingual Machine Translation. (arXiv:2308.15772v1 [cs.CL])

    [http://arxiv.org/abs/2308.15772](http://arxiv.org/abs/2308.15772)

    本论文介绍了一种基于任务的混合专家模型，将任务信息与MoE模型相结合，在多任务多语言机器翻译中取得了优越的结果，并且能够高效地应用于新的任务。

    

    在训练深度模型的多种应用中，混合专家（MoE）架构已被证明是一种强大的方法。然而，当前的MoE实现是任务无关的，将不同任务的所有标记以相同方式处理。在这项工作中，我们设计了一种新颖的方法，通过共享的动态基于任务的适配器，在MoE模型的不同粒度级别上将任务信息纳入其中。我们的实验证明了我们的方法在多任务多语言机器翻译上的优势。借助任务特定的适配器，我们的模型还可以高效地推广到新的任务。

    Mixture-of-experts (MoE) architecture has been proven a powerful method for diverse tasks in training deep models in many applications. However, current MoE implementations are task agnostic, treating all tokens from different tasks in the same manner. In this work, we instead design a novel method that incorporates task information into MoE models at different granular levels with shared dynamic task-based adapters. Our experiments and analysis show the advantages of our approaches over the dense and canonical MoE models on multi-task multilingual machine translations. With task-specific adapters, our models can additionally generalize to new tasks efficiently.
    
[^21]: 针对低资源语言和方言的网络欺凌检测：现状综述

    Cyberbullying Detection for Low-resource Languages and Dialects: Review of the State of the Art. (arXiv:2308.15745v1 [cs.CL])

    [http://arxiv.org/abs/2308.15745](http://arxiv.org/abs/2308.15745)

    本文总结了针对低资源语言中自动检测网络欺凌的研究现状，并发现了进一步发展的需求。

    

    社交媒体平台在及时处理内容方面的困难，鼓励用户滥用这些平台传播粗俗或辱骂性语言，这种重复性的行为会导致网络欺凌，它是一种在虚拟环境中发生的社会问题，但会带来真实世界中的后果，如抑郁、退缩，甚至受害者自杀企图。自动检测和缓解网络欺凌的系统已经开发出来了，但不幸的是，其中绝大多数是针对英语，仅有少数适用于低资源语言。为了评估目前的研究现状并确定进一步发展的需求，本文对低资源语言中自动网络欺凌检测方面的研究进行了全面系统的调研。我们分析了所有可用的关于这个主题的研究。我们调查了70多个关于自动检测网络欺凌或相关语言的已发表的研究。

    The struggle of social media platforms to moderate content in a timely manner, encourages users to abuse such platforms to spread vulgar or abusive language, which, when performed repeatedly becomes cyberbullying a social problem taking place in virtual environments, yet with real-world consequences, such as depression, withdrawal, or even suicide attempts of its victims. Systems for the automatic detection and mitigation of cyberbullying have been developed but, unfortunately, the vast majority of them are for the English language, with only a handful available for low-resource languages. To estimate the present state of research and recognize the needs for further development, in this paper we present a comprehensive systematic survey of studies done so far for automatic cyberbullying detection in low-resource languages. We analyzed all studies on this topic that were available. We investigated more than seventy published studies on automatic detection of cyberbullying or related lan
    
[^22]: 在大型语言模型中量化和分析实体级记忆

    Quantifying and Analyzing Entity-level Memorization in Large Language Models. (arXiv:2308.15727v1 [cs.CL])

    [http://arxiv.org/abs/2308.15727](http://arxiv.org/abs/2308.15727)

    本研究提出了一种细粒度的、实体级的定义来量化大型语言模型中的记忆能力，并提出了一种高效提取敏感实体的方法。实验证明了语言模型在不同设置下重构敏感实体的能力。

    

    大型语言模型（LLMs）被证明能够记忆其训练数据，这可以通过特定设计的提示提取出来。随着数据集规模的不断增长，由记忆引起的隐私风险引起了越来越多的关注。量化语言模型的记忆能力有助于评估潜在的隐私风险。然而，以往关于量化记忆的研究需要访问精确的原始数据或产生相当大的计算开销，这对于实际应用中的语言模型来说很困难。因此，我们提出了一种细粒度的、实体级的定义，用于以更接近实际场景的条件和度量方式来量化记忆。此外，我们还提出了一种从自回归语言模型中高效提取敏感实体的方法。我们基于提出的方法进行了大量的实验证明了语言模型在不同设置下的重构敏感实体的能力。

    Large language models (LLMs) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. As the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. Quantifying language model memorization helps evaluate potential privacy risks. However, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. To this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. In addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. We conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. We find that languag
    
[^23]: 通过动态知识选择优化文本生成中的事实准确性

    Optimizing Factual Accuracy in Text Generation through Dynamic Knowledge Selection. (arXiv:2308.15711v1 [cs.CL])

    [http://arxiv.org/abs/2308.15711](http://arxiv.org/abs/2308.15711)

    本文提出了DKGen系统，通过动态选择知识参考，消除了文本生成过程中的非关联参考，从而提高了生成文本的事实准确性。

    

    语言模型（LMs）在我们与信息互动方面产生了革命性的影响，但它们往往会生成非事实性的文本，引发对其可靠性的担忧。以往的方法通过使用外部知识作为文本生成的参考来增强事实性，但往往在无关参考的知识混乱（如实体不匹配）方面遇到困难。此外，随着输出文本的长度增加，随机采样的随意性会加剧，对生成文本的事实准确性产生不利影响。在本文中，我们提出了DKGen，将文本生成过程划分为迭代过程。在每个迭代中，DKGen将输入的查询、先前生成的文本和一部分参考段落作为输入来生成短文本。在此过程中，根据先前生成的文本和查询与之的相关性，动态选择子集，从完整的段落集中大量消除了不相关的参考内容的输入。为进一步增强DKGen的能力，

    Language models (LMs) have revolutionized the way we interact with information, but they often generate nonfactual text, raising concerns about their reliability. Previous methods use external knowledge as references for text generation to enhance factuality but often struggle with the knowledge mix-up(e.g., entity mismatch) of irrelevant references. Besides,as the length of the output text grows, the randomness of sampling can escalate, detrimentally impacting the factual accuracy of the generated text. In this paper, we present DKGen, which divide the text generation process into an iterative process. In each iteration, DKGen takes the input query, the previously generated text and a subset of the reference passages as input to generate short text. During the process, the subset is dynamically selected from the full passage set based on their relevance to the previously generated text and the query, largely eliminating the irrelevant references from input. To further enhance DKGen's 
    
[^24]: Document AI: 基于Transformer、基于图的模型和卷积神经网络的文档布局分析的比较研究

    Document AI: A Comparative Study of Transformer-Based, Graph-Based Models, and Convolutional Neural Networks For Document Layout Analysis. (arXiv:2308.15517v1 [cs.CL])

    [http://arxiv.org/abs/2308.15517](http://arxiv.org/abs/2308.15517)

    本研究比较了基于Transformer、基于图的模型和卷积神经网络在文档布局分析方面的效果，揭示了它们的优劣势。先前的研究中缺乏对这些模型的比较分析。此外，虽然有语言无关的Document AI模型，但其性能仍存在限制。

    

    Document AI旨在通过利用自然语言处理和计算机视觉技术自动分析文档。其中一个主要任务是文档布局分析，通过解释布局、图像和文本的内容和空间关系来结构化文档页面。这个任务可以是以图像为中心，目的是识别和标记各种区域，如作者和段落；也可以是以文本为中心，重点是对文档中的单词进行分类。虽然有越来越复杂的方法来改进布局分析，但对于这些方法的普适性仍存在疑虑。具体而言，先前的研究基于非常不同的架构开发了系统，如基于Transformer的、基于图的和卷积神经网络。然而，没有研究提及这些模型在比较分析中的有效性。此外，虽然存在能进行知识转移的语言无关的Document AI模型，但其性能仍有限制。

    Document AI aims to automatically analyze documents by leveraging natural language processing and computer vision techniques. One of the major tasks of Document AI is document layout analysis, which structures document pages by interpreting the content and spatial relationships of layout, image, and text. This task can be image-centric, wherein the aim is to identify and label various regions such as authors and paragraphs, or text-centric, where the focus is on classifying individual words in a document. Although there are increasingly sophisticated methods for improving layout analysis, doubts remain about the extent to which their findings can be generalized to a broader context. Specifically, prior work developed systems based on very different architectures, such as transformer-based, graph-based, and CNNs. However, no work has mentioned the effectiveness of these models in a comparative analysis. Moreover, while language-independent Document AI models capable of knowledge transfe
    
[^25]: FurChat: 使用LLMs的具有脸部表情的交互式对话系统，结合开放和封闭领域对话

    FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions. (arXiv:2308.15214v1 [cs.CL])

    [http://arxiv.org/abs/2308.15214](http://arxiv.org/abs/2308.15214)

    本研究开发了一个交互式对话系统，将开放和封闭领域对话、脸部表情结合起来，通过使用LLMs和GPT-3.5模型来生成引人入胜的对话，以提供信息并与访客进行自然交流。

    

    我们展示了一个交互式对话系统，可以作为接待员，生成结合开放和封闭领域对话以及脸部表情的混合对话。通过使用大型语言模型（LLM）来开发引人入胜的对话，我们将该系统部署到了一个高度表达力的Furhat机器人上，在互动过程中使用了口头和非语言提示。该系统专门为国家机器人实验室设计，通过自然对话与访客进行交互，并向他们提供有关设施、研究、新闻、即将举行的活动等方面的信息。系统利用最先进的GPT-3.5模型根据提示生成这些信息，同时生成领域通用的对话和面部表情。

    We demonstrate an embodied conversational agent that can function as a receptionist and generate a mixture of open and closed-domain dialogue along with facial expressions, by using a large language model (LLM) to develop an engaging conversation. We deployed the system onto a Furhat robot, which is highly expressive and capable of using both verbal and nonverbal cues during interaction. The system was designed specifically for the National Robotarium to interact with visitors through natural conversations, providing them with information about the facilities, research, news, upcoming events, etc. The system utilises the state-of-the-art GPT-3.5 model to generate such information along with domain-general conversations and facial expressions based on prompt engineering.
    
[^26]: SpikeBERT：一种采用两阶段BERT知识蒸馏训练的语言Spikformer

    SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT. (arXiv:2308.15122v1 [cs.CL])

    [http://arxiv.org/abs/2308.15122](http://arxiv.org/abs/2308.15122)

    该论文提出了一种名为SpikeBERT的SNN模型，通过改进Spikformer架构和使用两阶段知识蒸馏方法，该模型在语言任务上超越了其他SNN模型，在文本分类任务上甚至达到了与BERT相当的结果。

    

    脉冲神经网络（SNN）在以更节能的方式实现深度神经网络方面提供了一个有前景的途径。然而，现有的用于语言任务的SNN网络架构过于简单，深度架构尚未得到充分探索，与BERT等主流基于Transformer的网络相比，存在显著的性能差距。为此，我们改进了最近提出的脉冲Transformer（即Spikformer），使其能够处理语言任务，并提出了一种两阶段知识蒸馏方法来训练它，该方法结合了通过从BERT和大量未标记文本中蒸馏知识进行预训练，并通过再次从在相同训练示例上对BERT进行微调，并进行任务特定实例知识蒸馏。通过大量实验，我们展示了使用我们的方法训练的模型，命名为SpikeBERT，在实现上超过了最先进的SNN，并且甚至在文本分类任务上达到了与BERT相当的结果。

    Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way. However, the network architectures of existing SNNs for language tasks are too simplistic, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT. To this end, we improve a recently-proposed spiking transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples. Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text 
    
[^27]: 适应口语对话的文本对话状态跟踪器

    Adapting text-based dialogue state tracker for spoken dialogues. (arXiv:2308.15053v1 [cs.CL])

    [http://arxiv.org/abs/2308.15053](http://arxiv.org/abs/2308.15053)

    这篇论文描述了对构建适应口语对话系统的文本对话状态跟踪器进行的工程工作，利用自动语音识别错误校正和文本对话系统实现了插槽和值的估计。

    

    尽管通过对话系统技术竞赛（DSTC）取得了显著进展，但构建一个具有语音界面的稳健的任务导向对话系统仍然是一个关键挑战。大部分进展都是针对基于文本的对话系统，因为有丰富的书面语料库数据集，而具有口语对话的数据集非常稀缺。然而，正如Siri和Alexa等语音助手系统所展示的，将这种成功转移到口语对话中具有实际重要性。在本文中，我们描述了我们在DSTC11的具有语音感知的对话系统技术挑战赛中的高度成功模型的工程努力。我们的模型由三个主要模块组成：（1）自动语音识别错误校正，以弥合口语和文本话语之间的差距，（2）用于估计插槽和值的基于文本的对话系统（D3ST），该系统使用插槽描述。

    Although there have been remarkable advances in dialogue systems through the dialogue systems technology competition (DSTC), it remains one of the key challenges to building a robust task-oriented dialogue system with a speech interface. Most of the progress has been made for text-based dialogue systems since there are abundant datasets with written corpora while those with spoken dialogues are very scarce. However, as can be seen from voice assistant systems such as Siri and Alexa, it is of practical importance to transfer the success to spoken dialogues. In this paper, we describe our engineering effort in building a highly successful model that participated in the speech-aware dialogue systems technology challenge track in DSTC11. Our model consists of three major modules: (1) automatic speech recognition error correction to bridge the gap between the spoken and the text utterances, (2) text-based dialogue system (D3ST) for estimating the slots and values using slot descriptions, an
    
[^28]: 注意力和自监督语音嵌入对非语义语音任务的影响

    Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks. (arXiv:2308.14359v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.14359](http://arxiv.org/abs/2308.14359)

    该论文探讨了注意力和自监督语音嵌入对非语义语音任务的影响，特别是情感理解。实验结果表明，基于自注意力的轻量级HuBERT-Large模型在这些任务中表现出色。

    

    在现实中，人类情感理解在使对话技术成为主流方面至关重要。我们将语音情感理解视为一种知觉任务，这是一种更现实的情景。在不同的上下文（语言，人口统计学等），不同比例的人会将相同的语音片段视为非一致的情感。作为ACM多媒体2023计算语音联机挑战（ComParE）的一部分，在EMotion Share轨道上，我们利用他们丰富的多语种演讲者和多标签回归目标的数据集，即“情感分享”或对该情感的感知。我们证明了不同基础模型的训练方案决定了它们在超越语音识别的任务中的有效性，特别是对于情感理解等非语义语音任务。这是一个非常复杂的任务，因为涉及到多语种演讲者，目标标签的变化以及回归数据集中的固有不平衡性。我们的结果表明，基于自注意力的轻量级HuBERT-Large模型在这些任务中表现出色。

    Human emotion understanding is pivotal in making conversational technology mainstream. We view speech emotion understanding as a perception task which is a more realistic setting. With varying contexts (languages, demographics, etc.) different share of people perceive the same speech segment as a non-unanimous emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset of multilingual speakers and multi-label regression target of 'emotion share' or perception of that emotion. We demonstrate that the training scheme of different foundation models dictates their effectiveness for tasks beyond speech recognition, especially for non-semantic speech tasks like emotion understanding. This is a very complex task due to multilingual speakers, variability in the target labels, and inherent imbalance in the regression dataset. Our results show that HuBERT-Large with a self-attention-based light-weight se
    
[^29]: 评估大型语言模型对指令的鲁棒性

    Evaluating the Robustness to Instructions of Large Language Models. (arXiv:2308.14306v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.14306](http://arxiv.org/abs/2308.14306)

    本论文评估了大型语言模型对指令的鲁棒性。结果表明，指令微调可以提升中等规模模型的性能，并且模型对陌生指令的处理能力有待改进。

    

    最近，指令微调已成为提升大型语言模型在新任务中零-shot能力的潜在方法。该技术显示出出色的能力，可以提升中等规模的语言模型的性能，有时甚至达到相当于更大模型变体的性能水平。本研究重点研究了经过指令微调的语言模型对已知任务和未知任务的鲁棒性。我们对六个模型进行了探索，包括Alpaca、Vicuna、WizardLM和传统的任务导向模型（Flan-T5-XL/XXL、T0++），以真实世界的关系提取数据集作为案例研究。我们对这些遵循指令的语言模型进行了全面评估，这些模型是基于开放域指令和任务导向指令进行微调的。主要讨论的是它们在处理指令时的性能和鲁棒性。我们观察到，在大多数情况下，模型在处理陌生指令方面的性能往往会受到影响。

    Recently, Instruction fine-tuning has risen to prominence as a potential method for enhancing the zero-shot capabilities of Large Language Models (LLMs) on novel tasks. This technique has shown an exceptional ability to boost the performance of moderately sized LLMs, sometimes even reaching performance levels comparable to those of much larger model variants. The focus is on the robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction datasets as case studies. We carried out a comprehensive evaluation of these instruction-following LLMs which have been tuned based on open-domain instructions and task-oriented instructions. The main discussion is their performance and robustness towards instructions. We have observed that in most cases, the model's performance in dealing with unfamiliar instructions tends to w
    
[^30]: EntropyRank: 通过基于语言模型的文本压缩的副信息优化来进行无监督关键词提取

    EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression. (arXiv:2308.13399v1 [cs.CL])

    [http://arxiv.org/abs/2308.13399](http://arxiv.org/abs/2308.13399)

    该论文提出了一种无监督的关键词提取方法，通过利用预训练语言模型和信息论方法，在文本中提取具有最高条件熵的短语作为关键词。实验证明，该方法在关键词提取任务上取得了与常用方法相当的结果。

    

    我们提出了一种无监督的方法，基于预训练的语言模型（LM）和Shannon的信息最大化，从文本中提取关键词和关键词短语。具体来说，我们的方法提取在LM下具有最高条件熵的短语。得到的关键词短语集合解决了一个相关的信息论问题：如果作为副信息提供，它会导致使用LM和熵编码器对文本进行压缩时的预期最小二进制码长度。另外，得到的集合是通过因果LM对在给定条件下最小化文本熵的短语集合的近似。在实证上，该方法在各种关键词提取基准挑战中提供了与最常用方法可比较的结果。

    We propose an unsupervised method to extract keywords and keyphrases from texts based on a pre-trained language model (LM) and Shannon's information maximization. Specifically, our method extracts phrases having the highest conditional entropy under the LM. The resulting set of keyphrases turns out to solve a relevant information-theoretic problem: if provided as side information, it leads to the expected minimal binary code length in compressing the text using the LM and an entropy encoder. Alternately, the resulting set is an approximation via a causal LM to the set of phrases that minimize the entropy of the text when conditioned upon it. Empirically, the method provides results comparable to the most commonly used methods in various keyphrase extraction benchmark challenges.
    
[^31]: 基于人口特征的固定步长单时钟模拟的Agent-Based模型的形式化规范术语

    Formal specification terminology for demographic agent-based models of fixed-step single-clocked simulations. (arXiv:2308.13081v1 [cs.CL])

    [http://arxiv.org/abs/2308.13081](http://arxiv.org/abs/2308.13081)

    本文提出了一种适用于人口学领域的Agent Based Models (ABMs)的数学规范的合适形式术语，这进一步提高了模型的理解，并与O.D.D.协议相结合，减少了模型复制过程中的歧义。

    

    本文提出了一种适用于人口学领域的Agent Based Models (ABMs)的数学规范的合适形式术语。目标ABMs的模拟遵循固定步长单时钟模式。所提出的术语进一步提高了模型的理解，并可以作为一种独立的方法论，用于规范和选择性地记录一组重要的（人口）ABMs。然而，可以想象通过进一步扩展，这种术语可能与广泛使用的模型文档和通信O.D.D.协议[Grimm和et al.，2020，Amouroux等，2010]合并，以减少许多模型建模者的源源不断产生的歧义，从而阻碍模型复制。已经出版的人口模型文档，单亲模型的大大简化版本[Gostoli和Silverman，2020]作为形式术语的示例，单独发布在[Elsheikh，2023b]中。该模型已被实现。

    This document presents adequate formal terminology for the mathematical specification of a subset of Agent Based Models (ABMs) in the field of Demography. The simulation of the targeted ABMs follows a fixed-step single-clocked pattern. The proposed terminology further improves the model understanding and can act as a stand-alone methodology for the specification and optionally the documentation of a significant set of (demographic) ABMs. Nevertheless, it is imaginable the this terminology probably with further extensions can be merged with the largely-informal widely-used model documentation and communication O.D.D. protocol [Grimm and et al., 2020, Amouroux et al., 2010] to reduce many sources of ambiguity, hindering model replications by other modelers. A published demographic model documentation, largely simplified version of the Lone Parent Model [Gostoli and Silverman, 2020] is separately published in [Elsheikh, 2023b] as illustration for the formal terminology. The model was impl
    
[^32]: LibriSQA：通过新型数据集和框架推进自由形式和开放式的口语问答

    LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework. (arXiv:2308.10390v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10390](http://arxiv.org/abs/2308.10390)

    本论文提出了LibriSQA，一个自由形式和开放式的口语问答数据集和框架，通过改进ASR任务并使用轻量级的端到端框架，实现了在LLMs上执行口语问答任务的显著结果。

    

    虽然大型语言模型在许多领域和任务中展现出了可称赞的性能，但现有的语言模型在处理多模态功能方面仍存在明显不足，特别是对于需要语音和文本特征之间精确对齐和深度交互的口语问答（SQA）任务。为了解决LLM上的SQA挑战，我们从Librispeech创造了自由形式和开放式的LibriSQA数据集，包括自然对话格式的第一部分和包含多项选择题和答案以及分析片段的第二部分。这两部分共包含107k个涵盖各种主题的SQA对。鉴于现有语音-文本LLM的明显匮乏，我们提出了一个轻量级的端到端框架，在LibriSQA上执行SQA任务，并取得了显著的结果。通过将ASR改为SQA格式，我们进一步证实了我们框架处理ASR任务的能力。

    While Large Language Models (LLMs) have demonstrated commendable performance across a myriad of domains and tasks, existing LLMs still exhibit a palpable deficit in handling multimodal functionalities, especially for the Spoken Question Answering (SQA) task which necessitates precise alignment and deep interaction between speech and text features. To address the SQA challenge on LLMs, we initially curated the free-form and open-ended LibriSQA dataset from Librispeech, comprising Part I with natural conversational formats and Part II encompassing multiple-choice questions followed by answers and analytical segments. Both parts collectively include 107k SQA pairs that cover various topics. Given the evident paucity of existing speech-text LLMs, we propose a lightweight, end-to-end framework to execute the SQA task on the LibriSQA, witnessing significant results. By reforming ASR into the SQA format, we further substantiate our framework's capability in handling ASR tasks. Our empirical f
    
[^33]: 使用连续发言的方式对大型语言模型进行红队评估以实现安全对齐

    Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment. (arXiv:2308.09662v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.09662](http://arxiv.org/abs/2308.09662)

    这项工作以红队评估的方式对大型语言模型进行安全评估，发现即便是广泛部署的模型也容易受到连续发言提示的影响，导致违反伦理地对有害查询做出回应。通过红队评估尝试，发现多数开源LLM也会生成有害回应。研究者提出了一种LLM安全对齐的方法。

    

    大型语言模型（LLMs）通过优化下一个单词预测目标，以其巨大的多任务能力震撼世界。随着它们的属性和编码知识的出现，LLMs产生有害输出的风险增加，使它们不适合可扩展地部署给公众。在这项工作中，我们提出了一个新的安全评估基准RED-EVAL，进行红队评估。我们展示了即便是广泛部署的模型也容易受到基于连续发言的(CoU)提示的影响，使基于GPT-4和ChatGPT的闭源LLM系统违反伦理地对超过65%和73%的有害查询做出回应。我们还展示了RED-EVAL在8个开源LLM中的一致性，通过红队评估尝试生成86%以上的有害回应。接下来，我们提出了RED-INSTRUCT--一种用于LLM安全对齐的方法。它包括两个阶段：1）HARMFULQA数据收集：利用CoU提示,

    Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, 
    
[^34]: 加密货币证券案件中的大型语言模型：ChatGPT能否取代律师？

    Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])

    [http://arxiv.org/abs/2308.06032](http://arxiv.org/abs/2308.06032)

    本研究探讨了在加密货币证券案件中，大型语言模型（LLMs）是否能够准确判断违法行为，并比较了由LLM和律师撰写的投诉书对陪审团决策的影响。研究发现，目前的LLMs在法律推理方面表现较弱，但随着未来模型的改进，其潜力有望提升。

    

    大型语言模型（LLMs）可以增强对法律系统的访问。然而，关于它们在进行法律任务方面的有效性的实证研究非常有限。我们研究涉及加密货币的证券案件，作为AI可以支持法律过程的众多情境之一，研究LLMs的法律推理和起草能力。我们检查以下两个方面：a）LLM能否准确确定事实模式中可能存在的违法行为，b）基于LLM和律师撰写的投诉书，陪审团的决策是否有所差异。我们将真实案例中的事实模式输入GPT-3.5，并评估其确定正确潜在违法行为并排除虚假违法行为的能力。其次，我们请模拟陪审员评估LLM和律师撰写的投诉书。GPT-3.5的法律推理能力较弱，但我们预期未来模型的改进，特别是考虑到它建议的违法行为往往是正确的（它仅仅过于保守）。

    Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
    
[^35]: WeaverBird: 利用大型语言模型、知识库和搜索引擎增强金融决策能力

    WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine. (arXiv:2308.05361v1 [cs.CL])

    [http://arxiv.org/abs/2308.05361](http://arxiv.org/abs/2308.05361)

    WeaverBird是一个专为金融领域设计的智能对话系统，通过利用大型语言模型、本地知识库和搜索引擎，能够理解复杂的金融查询并提供明智的回答，具有增强的可信度。

    

    我们提出了WeaverBird，一个专为金融领域设计的智能对话系统。我们的系统利用GPT架构的大型语言模型，并利用金融相关文本的广泛语料对其进行了调整。因此，我们的系统能够理解复杂的金融查询，例如“在通货膨胀期间如何管理我的投资？”并提供明智的回答。此外，我们的系统还集成了本地的知识库和搜索引擎以检索相关信息。最终的回答是基于搜索结果进行条件约束的，并包含适当的引用来源，从而具有增强的可信度。通过一系列与金融相关的问题，我们已经展示了我们的系统相比其他模型的卓越性能。用户可以在我们的在线演示网站https://weaverbird.ttic.edu与我们的系统进行互动，并观看我们的2分钟演示视频https://www.youtube.com/watch?v=yofgeq。

    We present WeaverBird, an intelligent dialogue system designed specifically for the finance domain. Our system harnesses a large language model of GPT architecture that has been tuned using extensive corpora of finance-related text. As a result, our system possesses the capability to understand complex financial queries, such as "How should I manage my investments during inflation?", and provide informed responses. Furthermore, our system incorporates a local knowledge base and a search engine to retrieve relevant information. The final responses are conditioned on the search results and include proper citations to the sources, thus enjoying an enhanced credibility. Through a range of finance-related questions, we have demonstrated the superior performance of our system compared to other models. To experience our system firsthand, users can interact with our live demo at https://weaverbird.ttic.edu, as well as watch our 2-min video illustration at https://www.youtube.com/watch?v=yofgeq
    
[^36]: 自动纠正大型语言模型：多样化自我纠正策略的概述调查

    Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies. (arXiv:2308.03188v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03188](http://arxiv.org/abs/2308.03188)

    本文总结了最近的研究，对多样化的自我纠正策略进行了分类和分析，以解决大型语言模型中的问题行为。自动化反馈技术被证明是一种可行的方法，可以使基于大型语言模型的解决方案更实用和可部署。

    

    大型语言模型(LLMs)在各种自然语言处理任务中展现了卓越的性能。然而，它们的功效受到了不受欢迎和不一致的行为的削弱，包括幻觉、不忠实的推理和有害内容。纠正这些缺陷的一种有前景的方法是自我纠正，即引导或指导LLM自行修复输出问题。利用自动反馈的技术--无论是由LLM自身产生还是由某个外部系统产生--尤其有趣，因为它们是使基于LLM的解决方案更实际和可部署的一种有前景的方式，且只需最少的人类反馈。本文对这一新兴技术类别进行了全面的评估。我们分析和分类了许多最近利用这些策略的工作，包括训练时、生成时和事后纠正的技术。我们还总结了这一策略的主要应用，并在最后讨论了未来的发展方向和挑战。

    Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
    
[^37]: Context-VQA: 面向上下文感知和有意义的视觉问答

    Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering. (arXiv:2307.15745v1 [cs.CL])

    [http://arxiv.org/abs/2307.15745](http://arxiv.org/abs/2307.15745)

    Context-VQA通过引入上下文信息，提供了一种全面满足人们需求的视觉问答模型，该模型的创新在于将图像与不同上下文配对，并发现不同上下文下问题类型存在差异。

    

    视觉问答（VQA）有潜力以一种互动方式使互联网更具可访问性，使不能看到图像的人们能够就图像提出问题。然而，多项研究表明，盲人或视力低下的人更喜欢包含图像出现环境的图像解释，而当前的VQA数据集侧重于孤立的图像。我们认为，除非考虑上下文，否则VQA模型将无法完全满足人们的需求。为进一步激发和分析不同上下文之间的区别，我们引入了Context-VQA，一种将图像与上下文（如购物网站）配对的VQA数据集。我们发现不同上下文下的问题类型存在系统性差异。例如，在旅行上下文中呈现的图像产生2倍于平均数的“在哪里？”问题，而社交媒体和新闻上的图像产生的“谁？”问题分别为平均数的2.8倍和1.8倍。我们还发现，上下文对于回答正确的问题至关重要。具体来说，当上下文提供了提示时，准确率提高了17.2％。

    Visual question answering (VQA) has the potential to make the Internet more accessible in an interactive way, allowing people who cannot see images to ask questions about them. However, multiple studies have shown that people who are blind or have low-vision prefer image explanations that incorporate the context in which an image appears, yet current VQA datasets focus on images in isolation. We argue that VQA models will not fully succeed at meeting people's needs unless they take context into account. To further motivate and analyze the distinction between different contexts, we introduce Context-VQA, a VQA dataset that pairs images with contexts, specifically types of websites (e.g., a shopping website). We find that the types of questions vary systematically across contexts. For example, images presented in a travel context garner 2 times more "Where?" questions, and images on social media and news garner 2.8 and 1.8 times more "Who?" questions than the average. We also find that c
    
[^38]: 使用大型语言模型增强密集检索的软提示调优

    Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2307.08303](http://arxiv.org/abs/2307.08303)

    本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。

    

    密集检索（DR）将查询和文档转化为密集向量表示，并在向量空间中测量查询与文档之间的相似性。DR的一个挑战是缺乏领域特定的训练数据。虽然DR模型可以通过迁移学习从大规模公共数据集（如MS MARCO）中学习，但证据表明，并非所有DR模型和领域都能同等受益于迁移学习。最近，一些研究人员转向使用大型语言模型（LLMs）来改进零样本和少样本的DR模型。然而，这些方法中采用的硬提示或人工编写的提示无法保证生成的弱查询的质量。为了解决这个问题，我们提出了用于增强DR的软提示调优（SPTAR）：对于每个任务，我们利用软提示调优在有限的真实数据上优化任务特定的软提示，然后用这些提示引导LLMs为未标记的文档标记弱查询，从而得到足够的弱文档-查询对来训练任务特定的模型。

    Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
    
[^39]: 大语言模型不是公平的评估器。

    Large Language Models are not Fair Evaluators. (arXiv:2305.17926v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17926](http://arxiv.org/abs/2305.17926)

    本文揭示了使用大语言模型作为评估器时存在的系统偏差，可以通过改变候选响应的顺序来操纵评估结果。为了解决这个问题，提出了一个校准框架，包括多证据校准、均衡位置校准和人机协同校准。

    

    在这篇论文中，我们揭示了采用大语言模型（LLMs）（例如GPT-4）作为裁判来评分和比较候选模型生成的响应质量的评估范式中存在的系统偏差。我们发现，通过简单地改变候选响应在上下文中出现的顺序，可以轻松地操纵候选响应的质量排名。这种操纵使得一个模型看起来比另一个模型要优越得多，例如，使用ChatGPT作为评估器，在80个测试查询中，Vicuna-13B可以击败ChatGPT的66个。为了解决这个问题，我们提出了一个校准框架，其中包含三个简单而有效的策略：1）多证据校准，要求评估模型在分配评分之前生成多个评估证据；2）均衡位置校准，在各种顺序中聚合结果以确定最终分数；3）人机协同校准，引入平衡的位置多样性。

    In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity en
    
[^40]: 评估GPT-3生成的仇恨内容审核解释

    Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17680](http://arxiv.org/abs/2305.17680)

    本文通过调查和分析，评估了使用GPT-3生成的针对仇恨内容的解释是否准确和有用。结果显示，GPT-3生成的解释普遍存在过于模糊、聚焦不当等缺点，同时也存在不同类型仇恨言论生成的解释质量差异大的问题。

    

    最近的研究聚焦于使用基于大型语言模型（LLMs）的Fine-tune或提示生成仇恨言论的解释。尽管这个领域越来越受关注，但这些生成解释的有效性和潜在限制仍然不为人们所了解。一个关键问题是，由LLMs生成的这些解释可能会导致用户和内容审核员对标记内容本质做出错误判断。我们提出一个分析框架来检查仇恨言论解释，并进行了一个广泛的调查来评估这些解释。我们在GPT-3上输入仇恨和非仇恨内容，发现受调查者在人工审核GPT生成的解释时，将仇恨言论解释评价为不够准确和有用。

    Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-gene
    
[^41]: MPI-rical：基于Transformer的数据驱动MPI分布式并行辅助

    MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])

    [http://arxiv.org/abs/2305.09438](http://arxiv.org/abs/2305.09438)

    本文提出了一种基于Transformer模型的新方法MPI-rical，通过对大量代码片段进行训练实现自动化MPI代码生成，使并行化成为可能。

    

    在高性能计算中，将串行代码自动并行化以支持共享内存和分布式内存系统是一项具有挑战性的任务。虽然许多尝试将串行代码转换为共享内存环境的并行代码（通常使用OpenMP），但没有任何一项尝试成功将其转化为分布式内存环境。本文提出了一种称为MPI-rical的新方法，通过基于Transformer模型对大约25,000个串行代码片段及其对应的并行MPI代码进行训练，从我们的语料库（MPICodeCorpus）的50,000多个代码片段中生成自动化MPI代码。为了评估模型的性能，我们首先将串行代码转换为基于MPI的并行代码翻译问题分解为两个子问题，并制定两个研究目标：代码补全，即在给定源代码中的某个位置，预测该位置的MPI函数；代码翻译，即预测一个MPI函数。

    Automatic source-to-source parallelization of serial code for shared and distributed memory systems is a challenging task in high-performance computing. While many attempts were made to translate serial code into parallel code for a shared memory environment (usually using OpenMP), none has managed to do so for a distributed memory environment. In this paper, we propose a novel approach, called MPI-rical, for automated MPI code generation using a transformer-based model trained on approximately 25,000 serial code snippets and their corresponding parallelized MPI code out of more than 50,000 code snippets in our corpus (MPICodeCorpus). To evaluate the performance of the model, we first break down the serial code to MPI-based parallel code translation problem into two sub-problems and develop two research objectives: code completion defined as given a location in the source code, predict the MPI function for that location, and code translation defined as predicting an MPI function as wel
    
[^42]: LLM驱动的生成式新闻推荐初探

    A First Look at LLM-Powered Generative News Recommendation. (arXiv:2305.06566v1 [cs.IR])

    [http://arxiv.org/abs/2305.06566](http://arxiv.org/abs/2305.06566)

    本文介绍了一种LLM驱动的生成式新闻推荐框架GENRE，它利用预训练语义知识丰富新闻数据，通过从模型设计转移到提示设计提供灵活而统一的解决方案，实现了个性化新闻生成、用户画像和新闻摘要。

    

    个性化的新闻推荐系统已成为用户浏览海量在线新闻内容所必需的工具，然而现有的新闻推荐系统面临着冷启动问题、用户画像建模和新闻内容理解等重大挑战。先前的研究通常通过模型设计遵循一种不灵活的例行程序来解决特定的挑战，但在理解新闻内容和捕捉用户兴趣方面存在局限性。在本文中，我们介绍了GENRE，一种LLM驱动的生成式新闻推荐框架，它利用来自大型语言模型的预训练语义知识来丰富新闻数据。我们的目标是通过从模型设计转移到提示设计来提供一种灵活而统一的新闻推荐解决方案。我们展示了GENRE在个性化新闻生成、用户画像和新闻摘要中的应用。使用各种流行的推荐模型进行的大量实验证明了GENRE的有效性。

    Personalized news recommendation systems have become essential tools for users to navigate the vast amount of online news content, yet existing news recommenders face significant challenges such as the cold-start problem, user profile modeling, and news content understanding. Previous works have typically followed an inflexible routine to address a particular challenge through model design, but are limited in their ability to understand news content and capture user interests. In this paper, we introduce GENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data. Our aim is to provide a flexible and unified solution for news recommendation by moving from model design to prompt design. We showcase the use of GENRE for personalized news generation, user profiling, and news summarization. Extensive experiments with various popular recommendation models demonstrate the effectiveness of GENRE. 
    
[^43]: RAFT: 奖励排名微调用于生成型基础模型对齐

    RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])

    [http://arxiv.org/abs/2304.06767](http://arxiv.org/abs/2304.06767)

    RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。

    

    生成型基础模型容易受到广泛的无监督训练数据带来的隐式偏见的影响。这些偏见可能导致子优样本、扭曲的结果和不公平，可能产生重大影响。因此，将这些模型与人的伦理和偏好对齐是确保它们在真实应用中负责任和有效的部署的关键步骤。以往的研究主要采用人类反馈的强化学习（ RLHF）作为解决这个问题的手段。在 RL 算法的指导下，用人类反馈指导的奖励模型对生成模型进行微调。然而， RL 算法的低效性和不稳定性常常会对生成模型的成功对齐产生重大障碍，因此需要开发一种更为强大和简化的方法。为此，我们引入了一个新的框架，即奖励排名微调（ RAFT ），旨在对齐生成基础模型。

    Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
    
[^44]: 利用合成数据，视觉语言模型突破名词局限

    Going Beyond Nouns With Vision & Language Models Using Synthetic Data. (arXiv:2303.17590v1 [cs.CV])

    [http://arxiv.org/abs/2303.17590](http://arxiv.org/abs/2303.17590)

    本文在现有的VL模型中加入合成数据集SyViC，成功实现对'名词以外'的理解任务。

    

    大规模预训练的视觉语言（VL）模型在许多应用中表现出了显着的性能，使得可以通过（几乎任意）自然语言提示进行零样本开放词汇推理，取代了一组支持的类别。然而，最近的研究揭示了这些模型的一个根本性弱点。本文研究了纯合成数据在多大程度上可以教会这些模型克服这些缺点，而不损害它们的零样本能力。作者贡献了一个数百万规模的合成数据集SyViC，以及数据生成代码库，允许在现有的VL基准数据集中生成额外的合适数据，实现'noun'以外的理解任务学习。

    Large-scale pre-trained Vision & Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. For example, their difficulty to understand Visual Language Concepts (VLC) that go 'beyond nouns' such as the meaning of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work, we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable dat
    
[^45]: 通过大型语言模型和Answer Set Programming实现可靠的自然语言理解

    Reliable Natural Language Understanding with Large Language Models and Answer Set Programming. (arXiv:2302.03780v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03780](http://arxiv.org/abs/2302.03780)

    本研究提出了STAR框架，将大型语言模型与Answer Set Programming相结合，以达到可靠的自然语言理解。实验证明，该框架能够成功应对需要推理的不同任务，并提供可靠的结果。

    

    人类通过从句子中提取信息（意义），将其与已有的常识知识结合，并进行推理来理解语言。虽然像GPT-3和ChatGPT这样的大型语言模型可以利用文本中的模式来解决各种自然语言处理任务，但在需要推理的问题上表现不佳。它们也无法可靠地解释生成的答案。为了更好地模拟人类，我们提出了STAR框架，将LLMs与Answer Set Programming (ASP) 结合起来。我们展示了如何使用LLMs有效地从语言中提取以谓词表示的知识。然后使用目标导向的ASP来可靠地进行推理。我们将STAR框架应用于需要推理的三个不同自然语言理解任务：定性推理，数学推理和目标导向对话。我们的实验表明，STAR能够填补自然语言理解任务中的推理差距，提供可靠的结果。

    Humans understand language by extracting information (meaning) from sentences, combining it with existing commonsense knowledge, and then performing reasoning to draw conclusions. While large language models (LLMs) such as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a variety of NLP tasks, they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question. In order to emulate humans better, we propose STAR, a framework that combines LLMs with Answer Set Programming (ASP). We show how LLMs can be used to effectively extract knowledge -- represented as predicates -- from language. Goal-directed ASP is then employed to reliably reason over this knowledge. We apply the STAR framework to three different NLU tasks requiring reasoning: qualitative reasoning, mathematical reasoning, and goal-directed conversation. Our experiments reveal that STAR is able to bridge the gap of reasoning in NLU tasks, leading t
    
[^46]: (QA)$^2$: 带有可疑假设的问答系统

    (QA)$^2$: Question Answering with Questionable Assumptions. (arXiv:2212.10003v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10003](http://arxiv.org/abs/2212.10003)

    本研究提出了一个带有可疑假设的问答系统，通过构建一个包含自然生成的查询的数据集，并要求系统能够检测到和回答既包含典型信息检索问题又包含可疑假设的问题，以评估系统的性能。

    

    自然语言中的信息检索问题往往包含有可疑的假设，这些假设可能是错误的或无法验证的。包含可疑假设的问题具有挑战性，因为它们需要与通常的信息检索问题的答案不同的答题策略。例如，问题“玛丽·居里是什么时候发现铀的？”不能像通常的“什么时候”问题一样回答，而是需要解决假设“玛丽·居里发现了铀”。在这项工作中，我们提出了(QA)$^2$（带有可疑假设的问答系统），这是一个包含自然生成的搜索引擎查询的开放领域评估数据集，这些查询可能包含或不包含可疑假设。要在(QA)$^2$上取得成功，系统必须能够检测出可疑假设，并且能够对既包含典型信息检索问题又包含可疑假设的问题产生充分的回答。通过人工评分者的可接受性评估，我们证明了在处理这些问题时系统的性能。

    Naturally occurring information-seeking questions often contain questionable assumptions -- assumptions that are false or unverifiable. Questions containing questionable assumptions are challenging because they require a distinct answer strategy that deviates from typical answers for information-seeking questions. For instance, the question "When did Marie Curie discover Uranium?" cannot be answered as a typical "when" question without addressing the false assumption "Marie Curie discovered Uranium". In this work, we propose (QA)$^2$ (Question Answering with Questionable Assumptions), an open-domain evaluation dataset consisting of naturally occurring search engine queries that may or may not contain questionable assumptions. To be successful on (QA)$^2$, systems must be able to detect questionable assumptions and also be able to produce adequate responses for both typical information-seeking questions and ones with questionable assumptions. Through human rater acceptability on end-to-
    
[^47]: 知识增强的预训练语言模型综述

    A Survey of Knowledge Enhanced Pre-trained Language Models. (arXiv:2211.05994v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.05994](http://arxiv.org/abs/2211.05994)

    本文综述了知识增强的预训练语言模型(KE-PLMs)，这是一种将知识融入预训练语言模型的方法，以解决模型推理能力不足的问题。该综述提供了对该领域的全面了解，并介绍了适用于自然语言理解和自然语言生成的分类法。

    

    预训练语言模型(PLMs)通过自监督学习方法在大规模文本语料上进行训练，在自然语言处理(NLP)的各种任务中取得了令人期待的性能。然而，尽管具有大量参数的PLMs可以在精调阶段有效地获得从大规模训练文本中学到的丰富知识，并对下游任务产生好处，但它们仍然存在一些限制，如缺乏外部知识导致推理能力较差。研究人员致力于将知识融入PLMs以解决这些问题。本文综述了知识增强的预训练语言模型(KE-PLMs)，以提供对这一蓬勃发展领域的清晰认识。我们分别介绍了适用于自然语言理解(NLU)和自然语言生成(NLG)的合适分类法，以突出NLP的这两个主要任务。对于NLU，我们将知识类型划分为四个类别：语言知识，文本知识，知识库知识和常识知识。

    Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, kn
    
[^48]: CLSE: 跨语言语境下具有语言学意义实体的语料库

    CLSE: Corpus of Linguistically Significant Entities. (arXiv:2211.02423v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.02423](http://arxiv.org/abs/2211.02423)

    CLSE语料库是为了解决自然语言生成中命名实体处理的问题而发布的，该语料库由语言学专家注释，包括34种语言和74种不同的语义类型，能够支持多种应用。

    

    自然语言生成(NLG)的最大挑战之一是正确处理命名实体。命名实体经常是语法错误的源头，如错误的介词、错误的冠词处理或错误的实体屈折。如果不考虑语言表示，当在一小组任意选择的参数值上进行评估，或将数据集从语言上较简单的语言(如英语)翻译到语言上较复杂的语言(如俄语)时，这些错误的重要性常常被低估。然而，对于某些应用来说，精确的语法正确性至关重要——母语使用者可能会觉得与实体相关的语法错误是愚蠢的、不和谐的甚至令人不快的。为了能够创建更具语言多样性的NLG数据集，我们发布了一个由语言学专家注释的"具有语言学意义的实体语料库（CLSE）"。该语料库包括34种语言，涵盖了74种不同的语义类型，以支持航空公司时间表等各种应用。

    One of the biggest challenges of natural language generation (NLG) is the proper handling of named entities. Named entities are a common source of grammar mistakes such as wrong prepositions, wrong article handling, or incorrect entity inflection. Without factoring linguistic representation, such errors are often underrepresented when evaluating on a small set of arbitrarily picked argument values, or when translating a dataset from a linguistically simpler language, like English, to a linguistically complex language, like Russian. However, for some applications, broadly precise grammatical correctness is critical -- native speakers may find entity-related grammar errors silly, jarring, or even offensive.  To enable the creation of more linguistically diverse NLG datasets, we release a Corpus of Linguistically Significant Entities (CLSE) annotated by linguist experts. The corpus includes 34 languages and covers 74 different semantic types to support various applications from airline ti
    

