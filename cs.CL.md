# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining.](http://arxiv.org/abs/2304.09151) | 本文提出UniMax，一种对多语言模型进行更公平和更有效的预训练语言采样方法。该方法通过明确限制每种语言语料库上的重复次数来提供更均匀的核心语言覆盖率，并减轻了对尾部语言的过度拟合。UniMax优于标准的基于温度的采样方法，而且这些好处随着规模的增加而持续存在。 |
| [^2] | [Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling.](http://arxiv.org/abs/2304.09145) | 通过Outlier Suppression+框架的通道级移位和缩放操作，分析得到最优移位和缩放值，成功解决了量化Transformer语言模型中存在的不对称离群值问题，实现了接近浮点性能的结果。 |
| [^3] | [ChatGPT: Applications, Opportunities, and Threats.](http://arxiv.org/abs/2304.09103) | ChatGPT是OpenAI开发的一种人工智能技术，可以全自动地生成自然语言对话。本研究探讨了ChatGPT在10个领域的应用、机会和威胁，同时展示了GPT-4的巨大优势。 |
| [^4] | [Solving Math Word Problems by Combining Language Models With Symbolic Solvers.](http://arxiv.org/abs/2304.09102) | 该论文提出了一种将语言模型与符号求解器相结合的方法，以解决数学单词问题，尤其是具有挑战性的问题，并在此过程中突出了使用声明性和逐步表示的好处。 |
| [^5] | [Improving Items and Contexts Understanding with Descriptive Graph for Conversational Recommendation.](http://arxiv.org/abs/2304.09093) | KLEVER是一个新的CRS框架，可以将物品和它们相关的上下文单词联合建模在同一语义空间中，解决了以前工作中的物品和单词语义空间不对齐的问题。 |
| [^6] | [Unsupervised clustering of file dialects according to monotonic decompositions of mixtures.](http://arxiv.org/abs/2304.09082) | 本文提出了一种无监督分类方法，即将一组文件分成不同的方言，其中方言由其行为模式组成。提出了一种贪心算法从文件-消息数据矩阵的数据集中推断出候选方言，并证明了该算法是最优时的条件。 |
| [^7] | [Revisiting k-NN for Pre-trained Language Models.](http://arxiv.org/abs/2304.09058) | 本研究提出一种新方法，结合k-NN和预训练语言模型（PLMs）能够提高自然语言处理（NLP）的性能，并在多个基准数据集上得到验证。 |
| [^8] | [CodeKGC: Code Language Model for Generative Knowledge Graph Construction.](http://arxiv.org/abs/2304.09048) | 本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法，能够有效利用知识图谱内的语义结构，提高模型的可解释性。 |
| [^9] | [A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese.](http://arxiv.org/abs/2304.08999) | 本研究开发了一种有效提取葡萄牙语肿瘤健康记录中过程、药物和疾病的方法，帮助医护人员更高效地获取患者治疗状况的完整概述，有助于提升肿瘤治疗效果。 |
| [^10] | [D2CSE: Difference-aware Deep continuous prompts for Contrastive Sentence Embeddings.](http://arxiv.org/abs/2304.08991) | D2CSE是一种用于学习句子嵌入的新模型，采用基于差异感知的深度连续提示来计算具有区分微妙差异能力的句子向量。与现有方法相比，D2CSE只使用一个预训练语言模型，避免了繁琐的微调，并大大减少了训练参数数量，同时显著提高了句子嵌入的质量。 |
| [^11] | [MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning.](http://arxiv.org/abs/2304.08981) | 多模态情感识别挑战赛（MER 2023）提出了三个子挑战：MER-MULTI、MER-NOISE和MER-SEMI，为全球研究人员构建创新技术提供了激励，并测试了各种多模态特征，提供了有竞争力的基线，以促进鲁棒而有效的算法的发展和应用。 |
| [^12] | [Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs.](http://arxiv.org/abs/2304.08968) | LLMs在公众中广泛应用，但是目前大部分检测工具存在严重缺陷。研究发现，LLMs容易微调且难以被其他LLMs检测到。 |
| [^13] | [Enhancing Textbooks with Visuals from the Web for Improved Learning.](http://arxiv.org/abs/2304.08931) | 本研究使用视觉语言模型自动补充教材的有趣视觉支持，以促进学生的学习。通过优化问题解决现有教材中缺乏的问题，该方法的效果得到了验证。 |
| [^14] | [Tailoring Domain Adaptation for Machine Translation Quality Estimation.](http://arxiv.org/abs/2304.08891) | 本研究提出了结合领域自适应和数据增强的质量评估系统，针对数据缺乏和领域不匹配的问题在通用模型的基础上进行微调，结果显著优于最先进基线。 |
| [^15] | [Romanization-based Large-scale Adaptation of Multilingual Language Models.](http://arxiv.org/abs/2304.08865) | 该论文探索利用大规模转写来提升大型多语言预训练语言模型的处理低资源和未知语言的能力，利用UROMAN转写工具的潜力，并研究了一系列高效的策略，以适应各种语言数据。 |
| [^16] | [Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition.](http://arxiv.org/abs/2304.08862) | 本文提出了一种使用近似最近邻短语挖掘的方法来训练上下文感知Transformer转录器(CATT)模型，并在大规模数据情况下进行了实验，取得了显著的实验结果。 |
| [^17] | [Transfer to a Low-Resource Language via Close Relatives: The Case Study on Faroese.](http://arxiv.org/abs/2304.08823) | 利用与其关系紧密的语言，可改善到低资源语言的跨语言转移，以斯堪的纳维亚语言家族中的其他语言资源数据优化法罗语的NLP模型性能。 |
| [^18] | [TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models.](http://arxiv.org/abs/2304.08821) | 本论文提出了一种名为TTIDA的生成式数据增强方法，利用文本到文本和文本到图像模型生成可控的逼真标记图像。 |
| [^19] | [Revisiting the Role of Similarity and Dissimilarity inBest Counter Argument Retrieval.](http://arxiv.org/abs/2304.08807) | 本文研究了最佳反驳检索任务，提出了一种评分模型，使用相似性和差异性度量，达到了88.9％的accuracy@1，显著优于其他基线模型。 |
| [^20] | [Speaker Profiling in Multiparty Conversations.](http://arxiv.org/abs/2304.08801) | 本文提出了一个名为SPC的任务，旨在为对话中每个发言者生成个人特征摘要。任务被分为三个子任务：个人特征发现、个人特征类型识别和个人特征价值提取。任务对于银行、酒店预订和航空预订等行业中的聊天机器人非常重要，可以使聊天机器人更好地了解和回应每个发言人的需求。 |
| [^21] | [A Survey on Biomedical Text Summarization with Pre-trained Language Model.](http://arxiv.org/abs/2304.08763) | 本文总结了基于预训练语言模型的生物医学文本摘要方法，提炼关键信息生成简洁的摘要，分为微调、基于特征和无监督方法，未来研究方向包括融合领域特定知识和开发更适合的评估指标。 |
| [^22] | [On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study.](http://arxiv.org/abs/2304.08653) | 本研究对不同最先进的概率学习方法在提高神经摘要模型不确定性质量和生成效果方面进行了调查和对比，结果表明概率方法能够持续提高生成和不确定性质量，实现了高质量生成和放弃低质量摘要，且揭示了显著的失效模式。 |
| [^23] | [Classification of US Supreme Court Cases using BERT-Based Techniques.](http://arxiv.org/abs/2304.08649) | 本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。 |
| [^24] | [An Evaluation on Large Language Model Outputs: Discourse and Memorization.](http://arxiv.org/abs/2304.08637) | 评估了九个大语言模型的输出，发现其中80％包含记忆数据，但包含最多记忆内容的输出更可能是高质量的。提出了缓解策略以降低记忆文本率。 |
| [^25] | [Bridging Discrete and Backpropagation: Straight-Through and Beyond.](http://arxiv.org/abs/2304.08612) | 本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。 |
| [^26] | [Improving Scene Text Recognition for Character-Level Long-Tailed Distribution.](http://arxiv.org/abs/2304.08592) | 该论文提出一种新方法，使用平衡和长尾分布的数据集进行文本识别训练，同时保留上下文信息，以提高字符级长尾分布上的STR性能。 |
| [^27] | [Researchers eye-view of sarcasm detection in social media textual content.](http://arxiv.org/abs/2304.08582) | 社交媒体中讽刺文本使用普遍，但讽刺检测较为困难。文章讨论了讽刺检测的技术和方法，并提出需要重点关注乐观和前瞻性的讽刺检测方法。 |
| [^28] | [A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model.](http://arxiv.org/abs/2304.08109) | 采用参数效率高的LoRA调参技术可以在指令调参中取得令人鼓舞的结果，但全参数调参方法在基础模型选择、训练数据集规模、可学参数量等方面也有重要作用。 |
| [^29] | [Chinese Open Instruction Generalist: A Preliminary Release.](http://arxiv.org/abs/2304.07987) | 本论文旨在通过适应不同子任务的固有特性，创建一个中文指令数据集，以填补指令调整技术在中文语言领域的空白。 |
| [^30] | [Neural Machine Translation For Low Resource Languages.](http://arxiv.org/abs/2304.07869) | 该论文研究了低资源语言的神经机器翻译，并构建了一个基于 \texttt{mBART.CC25} 语言模型的模型，利用后向翻译和迁移学习等 NLP 和深度学习技术进行增强，以达到最先进的结果。 |
| [^31] | [Tractable Control for Autoregressive Language Generation.](http://arxiv.org/abs/2304.07438) | 本文提出了一种在自回归文本生成中使用可操作概率模型来强制实施限制的控制方法GeLaTo，并取得了在常见的约束文本生成测试上的最先进性能。 |
| [^32] | [Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders.](http://arxiv.org/abs/2304.01016) | 本文提出了一种通过结构压缩和模型尺寸不对称的双编码器模型 KALE，有效提高密集信息检索的推理效率，同时允许查询编码器的有效压缩，而无需进行全部的再训练或索引生成，此方法能够生成超过DistilBERT性能的模型。 |
| [^33] | [GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering.](http://arxiv.org/abs/2303.12320) | GrapeQA是一种新方法，使用“重要实体图形增强”和“上下文感知节点剪枝”策略，以提高问答准确性和效率。 |
| [^34] | [Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification.](http://arxiv.org/abs/2303.07142) | 本文探索了工作场所中的职业分类任务，并利用任务提示工程设计了良好的提示，成功地将大型语言模型（LLMs）应用于该任务中，取得了出色的表现，优于传统方法。 |
| [^35] | [Reception Reader: Exploring Text Reuse in Early Modern British Publications.](http://arxiv.org/abs/2302.04084) | 《Reception Reader》是一个web工具，用于探索早期英国出版物中的文本重用情况，用户可以通过共享文本片段探索某一作品的接受情况或其流入连接历史，并且可以交互式地浏览连接文档的详细信息，以及检查重用文本的上下文以进行“近距离阅读”。 |
| [^36] | [Erasure of Unaligned Attributes from Neural Representations.](http://arxiv.org/abs/2302.02997) | 提出了AMSAL算法，可去除神经表征中的对齐属性隐含信息，在多个数据集上测试有效消除偏见。 |
| [^37] | [EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata.](http://arxiv.org/abs/2301.04647) | 本文通过学习图像和相机元数据之间的交叉模态关联提取相机信息，并使用得到的特征成功实现拼接图像区域的"零样本"定位。 |
| [^38] | [InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation.](http://arxiv.org/abs/2212.06373) | 通过推断对话中最后一次发言来捕捉说话者的意图，提出了一种利用多头注意力的意图融合模块的共情对话生成模型InferEM。模型同时利用前几次发言预测最后一次发言，具有较高的可行性。 |
| [^39] | [Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust.](http://arxiv.org/abs/2211.03046) | 本研究使用因果结构模型分析了法律判决预测模型学习决策的原理，发现现有最先进模型利用非因果信息进行判决预测，违反法律规则会削弱模型鲁棒性和普适性并导致歧视问题，提出基于因果干预的解决方案。 |
| [^40] | [Circling Back to Recurrent Models of Language.](http://arxiv.org/abs/2211.01848) | 本文展示了循环模型的优越性并提出了结合更好的循环单元、架构、目标函数以及优化算法的改进方法，在小数据集和Enwik8动态评估中取得了新的最优性能。 |
| [^41] | [Variable Attention Masking for Configurable Transformer Transducer Speech Recognition.](http://arxiv.org/abs/2211.01438) | 本文研究了在Transformer Transducer语音识别中使用可变注意力掩模来构建可适应不同场景的模型，实验结果表明分块掩模表现更优，使用可变掩模可以提高准确性。 |
| [^42] | [Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models.](http://arxiv.org/abs/2209.06506) | 本研究提出了一种模拟黑盒神经排名模型的对抗攻击，可被黑帽SEO用来打败更好防护的搜索引擎。 |
| [^43] | [American cultural regions mapped through the lexical analysis of social media.](http://arxiv.org/abs/2208.07649) | 本文利用微博数据集自动分析文化地域，创新地提出了一种不需要假设、偏见或成见来识别文化地区的方法。该方法基于人们在社交媒体上的讨论主题来推断文化归属，揭示了美国多个具有明显身份的地区和其他复杂多面的地区。 |
| [^44] | [A Survey of Adversarial Defences and Robustness in NLP.](http://arxiv.org/abs/2203.06414) | 本文综述了自然语言处理中的对抗防御方法，这些方法不仅可以防御神经网络受到对抗性攻击，而且还可以在训练过程中作为正则化机制，防止模型过度拟合。 |
| [^45] | [PMC-Patients: A Large-scale Dataset of Patient Summaries and Relations for Benchmarking Retrieval-based Clinical Decision Support Systems.](http://arxiv.org/abs/2202.13876) | 本文提出了一个名为“PMC-Patients”的新数据集，用于定义和测试病患到文章的检索（ReCDS-PAR）和病患到病患的检索（ReCDS-PPR），以评估基于召回的临床决策支持系统（ReCDS）的性能。PMC-Patients数据集涵盖逾10,000名病患信息和27,000篇PubMed Central文章，并展示了多种ReCDS系统的效果分析和20个案例的实用性分析。 |
| [^46] | [Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective.](http://arxiv.org/abs/2202.08063) | 低资源情境下，如何让知识抽取更好地从非结构化文本中提取信息？本文调研了三种解决范式：高资源数据、更强的模型和数据与模型的结合，提出了未来的研究方向。 |
| [^47] | [Media Slant is Contagious.](http://arxiv.org/abs/2202.07269) | 本文研究了国家有线电视新闻对美国本土报纸的影响，发现当地报纸的内容会因为当地 FNC 观众数量的增加而趋向于 FNC 的倾向，并且有线电视倾向会极化地方新闻内容。 |

# 详细

[^1]: UniMax: 更公平和更有效的大规模多语言预训练语言采样方法

    UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining. (arXiv:2304.09151v1 [cs.CL])

    [http://arxiv.org/abs/2304.09151](http://arxiv.org/abs/2304.09151)

    本文提出UniMax，一种对多语言模型进行更公平和更有效的预训练语言采样方法。该方法通过明确限制每种语言语料库上的重复次数来提供更均匀的核心语言覆盖率，并减轻了对尾部语言的过度拟合。UniMax优于标准的基于温度的采样方法，而且这些好处随着规模的增加而持续存在。

    

    预训练的多语言大型语言模型通常使用基于温度的启发式采样来平衡不同语言，但先前的研究没有系统地评估不同预训练语言分布在模型规模上的功效。本文提出了一种新的采样方法UniMax，通过明确地限制每种语言语料库上的重复次数，提供更均匀的核心语言覆盖率，同时减轻了对尾部语言的过度拟合。我们在一系列多语言基准测试中执行了广泛的消融测试，测试了一系列采样策略，同时变化模型规模。我们发现UniMax优于标准的基于温度的采样方法，并且这些好处随着规模的增加而持续存在。作为我们的贡献的一部分，我们发布了：（i）29万亿个字符跨107种语言的改进和更新的mC4多语言语料库，以及（ii）使用UniMax训练的预训练umT5模型检查点套件。

    Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMa
    
[^2]: Outlier Suppression+：通过等效和最优移位和缩放来准确量化大型语言模型

    Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. (arXiv:2304.09145v1 [cs.CL])

    [http://arxiv.org/abs/2304.09145](http://arxiv.org/abs/2304.09145)

    通过Outlier Suppression+框架的通道级移位和缩放操作，分析得到最优移位和缩放值，成功解决了量化Transformer语言模型中存在的不对称离群值问题，实现了接近浮点性能的结果。

    

    对Transformer语言模型进行量化面临着存在损害性离群值的重要难题。我们观察到这些离群值是不对称的并且集中在特定通道中。为了解决这个问题，我们提出了Outlier Suppression+框架。首先，我们引入了通道级别的移位和缩放操作来消除不对称表示并缩小有问题的通道。我们证明了这些操作可以无缝地迁移至后续模块而保持等效性。其次，我们量化分析了最优的移位和缩放值，考虑到下一层权重的不对称特性和量化误差。我们轻量级的框架可以在静态和标准的训练后量化设置下造成最小的性能降低。各种任务和模型的全面结果表明，我们的方法在小型模型和大型模型如GPT-2方面实现了接近浮点性能。

    Quantization of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are asymmetric and concentrated in specific channels. To address this issue, we propose the Outlier Suppression+ framework. First, we introduce channel-wise shifting and scaling operations to eliminate asymmetric presentation and scale down problematic channels. We demonstrate that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we quantitatively analyze the optimal values for shifting and scaling, taking into account both the asymmetric property and quantization errors of weights in the next layer. Our lightweight framework can incur minimal performance degradation under static and standard post-training quantization settings. Comprehensive results across various tasks and models reveal that our approach achieves near-floating-point performance on both small models
    
[^3]: ChatGPT: 应用、机会和威胁

    ChatGPT: Applications, Opportunities, and Threats. (arXiv:2304.09103v1 [cs.CY])

    [http://arxiv.org/abs/2304.09103](http://arxiv.org/abs/2304.09103)

    ChatGPT是OpenAI开发的一种人工智能技术，可以全自动地生成自然语言对话。本研究探讨了ChatGPT在10个领域的应用、机会和威胁，同时展示了GPT-4的巨大优势。

    

    ChatGPT(条件生成的预训练Transformer)是由OpenAI开发的一种人工智能技术，使用了监督式机器学习和增强式学习技术进行微调，使得计算机可以全自动地生成自然语言对话。ChatGPT基于Transformer架构，并在各种来源的数百万个对话上进行训练。该系统结合了预训练深度学习模型的能力与可编程层，提供了一个生成自然语言对话的强大基础。在本研究中，通过评估现有文献，我们探讨了ChatGPT在10个主要领域中的应用、机会和威胁，并提供了详细的例子，包括商业和工业，以及教育。我们还进行了实验研究，检查了GPT-3.5和GPT-4的有效性和性能，并发现后者表现优异。

    Developed by OpenAI, ChatGPT (Conditional Generative Pre-trained Transformer) is an artificial intelligence technology that is fine-tuned using supervised machine learning and reinforcement learning techniques, allowing a computer to generate natural language conversation fully autonomously. ChatGPT is built on the transformer architecture and trained on millions of conversations from various sources. The system combines the power of pre-trained deep learning models with a programmability layer to provide a strong base for generating natural language conversations. In this study, after reviewing the existing literature, we examine the applications, opportunities, and threats of ChatGPT in 10 main domains, providing detailed examples for the business and industry as well as education. We also conducted an experimental study, checking the effectiveness and comparing the performances of GPT-3.5 and GPT-4, and found that the latter performs significantly better. Despite its exceptional abi
    
[^4]: 将语言模型与符号求解器相结合求解数学单词问题

    Solving Math Word Problems by Combining Language Models With Symbolic Solvers. (arXiv:2304.09102v1 [cs.CL])

    [http://arxiv.org/abs/2304.09102](http://arxiv.org/abs/2304.09102)

    该论文提出了一种将语言模型与符号求解器相结合的方法，以解决数学单词问题，尤其是具有挑战性的问题，并在此过程中突出了使用声明性和逐步表示的好处。

    

    自动生成高质量的逐步解决数学单词问题的解决方案在教育中有许多应用。最近，将大型语言模型（LLM）与外部工具结合使用以执行复杂的推理和计算已成为解决数学单词问题的有前途的方向，但先前的方法（如程序辅助语言模型（PAL））对于需要陈述性推理的问题具有偏见，而对于简单的过程问题则不太有效。我们提出了一种方法，将一种可以将单词问题逐步正式化为一组变量和方程式的LLM与外部符号求解器相结合，以解决方程。我们的方法在GSM8K数学单词问题基准测试上实现了与原始PAL相当的准确度，而在ALGEBRA上则明显优于PAL，该数据集从代数教科书中提取更具挑战性的单词问题。我们的工作突出了在解决数学单词问题时使用声明性和逐步表示的好处。

    Automatically generating high-quality step-by-step solutions to math word problems has many applications in education. Recently, combining large language models (LLMs) with external tools to perform complex reasoning and calculation has emerged as a promising direction for solving math word problems, but prior approaches such as Program-Aided Language model (PAL) are biased towards simple procedural problems and less effective for problems that require declarative reasoning. We propose an approach that combines an LLM that can incrementally formalize word problems as a set of variables and equations with an external symbolic solver that can solve the equations. Our approach achieves comparable accuracy to the original PAL on the GSM8K benchmark of math word problems and outperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more challenging word problems extracted from Algebra textbooks. Our work highlights the benefits of using declarative and incremental representations when
    
[^5]: 基于描述性图的对话式推荐系统中的物品和上下文理解改进

    Improving Items and Contexts Understanding with Descriptive Graph for Conversational Recommendation. (arXiv:2304.09093v1 [cs.IR])

    [http://arxiv.org/abs/2304.09093](http://arxiv.org/abs/2304.09093)

    KLEVER是一个新的CRS框架，可以将物品和它们相关的上下文单词联合建模在同一语义空间中，解决了以前工作中的物品和单词语义空间不对齐的问题。

    

    对话式推荐系统（CRS）在利用外部知识提高物品和上下文单词表示方面处于最前沿水平，以实现高质量的推荐和响应生成。然而，物品和单词的表示通常在两个独立的语义空间中建模，这会导致它们之间的不对齐问题。因此，当用户输入信息不足时，这将导致CRS仅实现次优排名表现。为了解决以前工作的局限性，我们提出了一个新的CRS框架KLEVER，它可以联合建模在相同语义空间中的物品和它们相关的上下文单词。特别是，我们从丰富的物品文本特征（如物品描述和类别）中构建一个物品描述性图。基于构建的描述性图，KLEVER共同学习单词和物品的嵌入，以增强推荐和对话生成的互动。

    State-of-the-art methods on conversational recommender systems (CRS) leverage external knowledge to enhance both items' and contextual words' representations to achieve high quality recommendations and responses generation. However, the representations of the items and words are usually modeled in two separated semantic spaces, which leads to misalignment issue between them. Consequently, this will cause the CRS to only achieve a sub-optimal ranking performance, especially when there is a lack of sufficient information from the user's input. To address limitations of previous works, we propose a new CRS framework KLEVER, which jointly models items and their associated contextual words in the same semantic space. Particularly, we construct an item descriptive graph from the rich items' textual features, such as item description and categories. Based on the constructed descriptive graph, KLEVER jointly learns the embeddings of the words and items, towards enhancing both recommender and d
    
[^6]: 按照混合的单调分解无监督聚类文件方言

    Unsupervised clustering of file dialects according to monotonic decompositions of mixtures. (arXiv:2304.09082v1 [cs.PL])

    [http://arxiv.org/abs/2304.09082](http://arxiv.org/abs/2304.09082)

    本文提出了一种无监督分类方法，即将一组文件分成不同的方言，其中方言由其行为模式组成。提出了一种贪心算法从文件-消息数据矩阵的数据集中推断出候选方言，并证明了该算法是最优时的条件。

    

    本文提出了一种无监督分类方法，根据一组程序消耗的消息，将一组文件分割为不重叠的方言。消息的模式可以被用作特定行为的标志，有些消息可能会同时出现，而其他消息不会。消息模式可以用来将文件分类为不同方言。一个方言由子集消息作为必需消息来定义。一旦将文件置于方言及其必需的消息之下，剩下的消息则是统计独立的。有了这个方言定义，我们提出了一种贪心算法，从文件-消息数据矩阵的数据集中推断出候选方言。文章证明了该算法在多种文件格式上的效果，并证明了该算法是最优时的条件。文章表明，分析员需要考虑的方言比不同文件所需的组数少。

    This paper proposes an unsupervised classification method that partitions a set of files into non-overlapping dialects based upon their behaviors, determined by messages produced by a collection of programs that consume them. The pattern of messages can be used as the signature of a particular kind of behavior, with the understanding that some messages are likely to co-occur, while others are not. Patterns of messages can be used to classify files into dialects. A dialect is defined by a subset of messages, called the required messages. Once files are conditioned upon dialect and its required messages, the remaining messages are statistically independent.  With this definition of dialect in hand, we present a greedy algorithm that deduces candidate dialects from a dataset consisting of a matrix of file-message data, demonstrate its performance on several file formats, and prove conditions under which it is optimal. We show that an analyst needs to consider fewer dialects than distinct 
    
[^7]: 重访基于预训练语言模型的k-NN

    Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])

    [http://arxiv.org/abs/2304.09058](http://arxiv.org/abs/2304.09058)

    本研究提出一种新方法，结合k-NN和预训练语言模型（PLMs）能够提高自然语言处理（NLP）的性能，并在多个基准数据集上得到验证。

    

    预训练语言模型（PLMs）作为参数化的急切学习器，已成为自然语言处理（NLP）当前范式的实际选择。与此形成对比的是，k-最近邻（k-NN）分类器作为延迟学习模型，倾向于减轻过拟合和孤立噪声。本文中我们重访了k-NN分类器，以增强基于PLMs的分类器。从方法层面上，我们提出采用文本表示的PLMs在两个步骤中采用k-NN：（1）利用k-NN作为先验知识来校准训练过程（2）线性插值k-NN预测的概率分布和PLMs分类器的概率分布。我们的方法核心是实现了k-NN校准训练，将预测结果作为训练过程中易于和难以学习的示例的指标。从应用场景多样性的角度出发，我们在各种基准数据集上进行了广泛的微调、提示微调范式和零样本任务设置的实验。我们的结果表明，结合k-NN可以在所有受到检查的设置中持续提高PLMs的性能，并且在所有受到考虑的设置中跑赢了基于普通PLMs的方法。

    Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-sh
    
[^8]: CodeKGC：用于生成知识图谱构建的代码语言模型

    CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])

    [http://arxiv.org/abs/2304.09048](http://arxiv.org/abs/2304.09048)

    本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法，能够有效利用知识图谱内的语义结构，提高模型的可解释性。

    

    目前的生成式知识图谱构建方法通常无法捕捉结构性知识，而只是将自然语言转化为序列化文本或规范语言。然而，对于像代码这样的结构化数据进行训练的大型生成式语言模型已经展现了在理解自然语言以进行结构性预测和推理任务方面的卓越能力。本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法。具体而言，在给定代码格式的自然语言输入的情况下，目标是生成可以表示为代码补全任务的三元组。我们开发了具有模式感知型提示的方法，可以有效利用知识图谱内的语义结构。由于代码本质上具有结构，如类和函数定义，因此它作为先验的语义结构知识模型非常有用。此外，我们采用了基于原理的生成方法来提高性能。原理提供了模型生成结果的可解释性。

    Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
    
[^9]: 一种用于提取葡萄牙语肿瘤健康记录的生物医学实体提取流水线

    A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese. (arXiv:2304.08999v1 [cs.CL])

    [http://arxiv.org/abs/2304.08999](http://arxiv.org/abs/2304.08999)

    本研究开发了一种有效提取葡萄牙语肿瘤健康记录中过程、药物和疾病的方法，帮助医护人员更高效地获取患者治疗状况的完整概述，有助于提升肿瘤治疗效果。

    

    癌症患者的文本健康记录通常很冗长且高度不结构化，使得医护人员获取完整患者治疗状况的完整概述非常耗时。由于这些限制可能导致次优和/或低效的治疗程序，因此医疗服务提供者将极大地受益于有效地概括这些记录的系统。随着深度神经模型的出现，对于英语临床文本，这个目标已经部分实现，然而，研究社区仍缺乏针对资源有限语言的有效解决方案。在本文中，我们介绍了我们开发的方法，从欧洲葡萄牙语肿瘤健康记录中提取过程、药物和疾病。这个项目与葡萄牙肿瘤研究所合作完成，该所除了拥有十多年的受保护的医疗记录外，在整个开发过程中还提供了肿瘤学专家的专业知识。

    Textual health records of cancer patients are usually protracted and highly unstructured, making it very time-consuming for health professionals to get a complete overview of the patient's therapeutic course. As such limitations can lead to suboptimal and/or inefficient treatment procedures, healthcare providers would greatly benefit from a system that effectively summarizes the information of those records. With the advent of deep neural models, this objective has been partially attained for English clinical texts, however, the research community still lacks an effective solution for languages with limited resources. In this paper, we present the approach we developed to extract procedures, drugs, and diseases from oncology health records written in European Portuguese. This project was conducted in collaboration with the Portuguese Institute for Oncology which, besides holding over $10$ years of duly protected medical records, also provided oncologist expertise throughout the develop
    
[^10]: D2CSE: 基于差异感知的深度连续提示用于对比句子嵌入

    D2CSE: Difference-aware Deep continuous prompts for Contrastive Sentence Embeddings. (arXiv:2304.08991v1 [cs.CL])

    [http://arxiv.org/abs/2304.08991](http://arxiv.org/abs/2304.08991)

    D2CSE是一种用于学习句子嵌入的新模型，采用基于差异感知的深度连续提示来计算具有区分微妙差异能力的句子向量。与现有方法相比，D2CSE只使用一个预训练语言模型，避免了繁琐的微调，并大大减少了训练参数数量，同时显著提高了句子嵌入的质量。

    

    本文介绍了一种名为D2CSE的基于差异感知的深度连续提示模型，用于学习句子嵌入。与现有方法相比，D2CSE采用了简单的神经架构来计算句子向量，使得其对于在类似句子中区分微妙差异有很好的表现。与需要多个预训练语言模型（PLMs）处理原始和损坏（微妙修改）句子对的现有神经网络不同，D2CSE仅通过执行多个任务（即，对比学习和条件替换标记检测）自主引导地优化了连续提示，从而避免了繁琐的多个PLMs的微调。D2CSE将单个PLM重载到连续提示上，大大节省了存储空间。 D2CSE的训练参数数量约为现有方法的1％，同时显著提高了句子嵌入的质量。

    This paper describes Difference-aware Deep continuous prompt for Contrastive Sentence Embeddings (D2CSE) that learns sentence embeddings. Compared to state-of-the-art approaches, D2CSE computes sentence vectors that are exceptional to distinguish a subtle difference in similar sentences by employing a simple neural architecture for continuous prompts. Unlike existing architectures that require multiple pretrained language models (PLMs) to process a pair of the original and corrupted (subtly modified) sentences, D2CSE avoids cumbersome fine-tuning of multiple PLMs by only optimizing continuous prompts by performing multiple tasks -- i.e., contrastive learning and conditional replaced token detection all done in a self-guided manner. D2CSE overloads a single PLM on continuous prompts and greatly saves memory consumption as a result. The number of training parameters in D2CSE is reduced to about 1\% of existing approaches while substantially improving the quality of sentence embeddings. W
    
[^11]: MER 2023: 多标签学习，模态鲁棒性和半监督学习

    MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning. (arXiv:2304.08981v1 [cs.CL])

    [http://arxiv.org/abs/2304.08981](http://arxiv.org/abs/2304.08981)

    多模态情感识别挑战赛（MER 2023）提出了三个子挑战：MER-MULTI、MER-NOISE和MER-SEMI，为全球研究人员构建创新技术提供了激励，并测试了各种多模态特征，提供了有竞争力的基线，以促进鲁棒而有效的算法的发展和应用。

    

    在过去的几十年中，深度学习的发展使得多模态情感识别取得了显着进展。然而，现有技术难以满足实际应用的需求。为了提高鲁棒性，我们发起了多模态情感识别挑战赛（MER 2023），以激励全球研究人员构建创新技术，进一步加速和促进研究。针对今年的挑战赛，我们提出了三个不同的子挑战：（1）MER-MULTI，参赛者需要识别离散和维度情感；（2）MER-NOISE，在测试视频中添加噪声，以评估模态鲁棒性；（3）MER-SEMI，提供大量未标记的样本，用于半监督学习。在本文中，我们测试了各种多模态特征，并为每个子挑战提供了有竞争力的基线。我们的系统在MER-MULTI上获得了77.57％的F1分数和0.82的均方误差（MSE），在MER-NOISE上获得了69.82％的F1分数和0.75的MSE，在MER-SEMI上获得了69.39％的F1分数和0.80的MSE。我们希望这个挑战赛能够激发更多的研究人员探索多模态情感识别，并促进鲁棒而有效的算法用于实际应用。

    Over the past few decades, multimodal emotion recognition has made remarkable progress with the development of deep learning. However, existing technologies are difficult to meet the demand for practical applications. To improve the robustness, we launch a Multimodal Emotion Recognition Challenge (MER 2023) to motivate global researchers to build innovative technologies that can further accelerate and foster research. For this year's challenge, we present three distinct sub-challenges: (1) MER-MULTI, in which participants recognize both discrete and dimensional emotions; (2) MER-NOISE, in which noise is added to test videos for modality robustness evaluation; (3) MER-SEMI, which provides large amounts of unlabeled samples for semi-supervised learning. In this paper, we test a variety of multimodal features and provide a competitive baseline for each sub-challenge. Our system achieves 77.57% on the F1 score and 0.82 on the mean squared error (MSE) for MER-MULTI, 69.82% on the F1 score a
    
[^12]: 随机鹦鹉寻找随机鹦鹉：LLMs易于微调且难以被其他LLMs检测到

    Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs. (arXiv:2304.08968v1 [cs.CL])

    [http://arxiv.org/abs/2304.08968](http://arxiv.org/abs/2304.08968)

    LLMs在公众中广泛应用，但是目前大部分检测工具存在严重缺陷。研究发现，LLMs容易微调且难以被其他LLMs检测到。

    

    自我注意力革命使生成式语言模型得以扩展并实现越来越惊人的能力。这些模型通常称为大型语言模型（LLMs），最近由于对话微调而在公众中获得了广泛关注，从而使其行为符合公众对于AI的期望。然而，这种突出也加大了关注LLMs误用的先前担忧，并导致出现许多在野外检测LLMs的工具。不幸的是，大多数这样的工具都存在严重缺陷。我们在这里展示了一种新方法，可以大大降低基于微调的自动编码器检测LLMs的成功率，并说明我们的工作涉及的重要细节。

    The self-attention revolution allowed generative language models to scale and achieve increasingly impressive abilities. Such models - commonly referred to as Large Language Models (LLMs) - have recently gained prominence with the general public, thanks to conversational fine-tuning, putting their behavior in line with public expectations regarding AI. This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild.  Unfortunately, most such tools are critically flawed. While major publications in the LLM detectability field suggested that LLMs were easy to detect with fine-tuned autoencoders, the limitations of their results are easy to overlook. Specifically, they assumed publicly available generative models without fine-tunes or non-trivial prompts. While the importance of these assumptions has been demonstrated, until now, it remained unclear how well such detection could be countered.  Here, we show that a
    
[^13]: 利用网络图片提升教材的可视化效果以促进学习

    Enhancing Textbooks with Visuals from the Web for Improved Learning. (arXiv:2304.08931v1 [cs.CV])

    [http://arxiv.org/abs/2304.08931](http://arxiv.org/abs/2304.08931)

    本研究使用视觉语言模型自动补充教材的有趣视觉支持，以促进学生的学习。通过优化问题解决现有教材中缺乏的问题，该方法的效果得到了验证。

    

    教材是向学生传达优质教育的主要方式。已经证明，解释性或具象化的可视化内容在知识的记忆、理解和传递方面发挥着关键作用。然而，在发展中国家，许多教科书品质较低且缺乏有趣的视觉支持以助力于学生的学习。在本文中，我们研究了视觉语言模型的有效性，以自动使用来自网络的图像为教材增添图片。具体而言，我们收集了来自世界上最大的免费在线出版商之一的电子教材数据集。我们进行了严密的数据集分析，并利用所得的分析结果激发了一项任务，旨在检索并适当分配网络图片到教材中，我们将其作为一项新颖的优化问题。通过众包评估，我们验证了：(1)虽然原始教材图像评级更高，但自动分配图像并不相距太远，并且(2)选择的图像来源具有重要影响。

    Textbooks are the primary vehicle for delivering quality education to students. It has been shown that explanatory or illustrative visuals play a key role in the retention, comprehension and the general transfer of knowledge. However, many textbooks, especially in the developing world, are low quality and lack interesting visuals to support student learning. In this paper, we investigate the effectiveness of vision-language models to automatically enhance textbooks with images from the web. Specifically, we collect a dataset of e-textbooks from one of the largest free online publishers in the world. We rigorously analyse the dataset, and use the resulting analysis to motivate a task that involves retrieving and appropriately assigning web images to textbooks, which we frame as a novel optimization problem. Through a crowd-sourced evaluation, we verify that (1) while the original textbook images are rated higher, automatically assigned ones are not far behind, and (2) the choice of the 
    
[^14]: 为机器翻译质量评估量身定制领域自适应方法

    Tailoring Domain Adaptation for Machine Translation Quality Estimation. (arXiv:2304.08891v1 [cs.CL])

    [http://arxiv.org/abs/2304.08891](http://arxiv.org/abs/2304.08891)

    本研究提出了结合领域自适应和数据增强的质量评估系统，针对数据缺乏和领域不匹配的问题在通用模型的基础上进行微调，结果显著优于最先进基线。

    

    质量评估对翻译流程至关重要，但其有效性取决于训练数据的可用性和质量。对于特定的质量评估而言，由于标记这样的数据的成本和工作量高昂，因此高质量的标记数据经常缺乏。除了数据缺乏方面的挑战外，质量评估模型还应具有泛化性，即它们应该能够处理来自不同领域的数据，包括通用领域和特定领域数据。因此，本文将领域自适应和数据增强进行了结合，提出了一种强大的质量评估系统。方法是先训练一个通用质量评估模型，然后在保留通用知识的同时对特定领域进行微调。研究结果表明，在所有研究的语言对中，我们的方法均取得了显着的改进，并具有更好的跨语言推断，相比于最先进的基线在零-shot学习方案中具有更高的性能。

    While quality estimation (QE) can play an important role in the translation process, its effectiveness relies on the availability and quality of training data. For QE in particular, high-quality labeled data is often lacking due to the high-cost and effort associated with labeling such data. Aside from the data scarcity challenge, QE models should also be generalizable, i.e., they should be able to handle data from different domains, both generic and specific. To alleviate these two main issues -- data scarcity and domain mismatch -- this paper combines domain adaptation and data augmentation within a robust QE system. Our method is to first train a generic QE model and then fine-tune it on a specific domain while retaining generic knowledge. Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and a superior performance in zero-shot learning scenarios as compared to state-of-the-art baselines.
    
[^15]: 基于罗马化的多语言模型大规模适应

    Romanization-based Large-scale Adaptation of Multilingual Language Models. (arXiv:2304.08865v1 [cs.CL])

    [http://arxiv.org/abs/2304.08865](http://arxiv.org/abs/2304.08865)

    该论文探索利用大规模转写来提升大型多语言预训练语言模型的处理低资源和未知语言的能力，利用UROMAN转写工具的潜力，并研究了一系列高效的策略，以适应各种语言数据。

    

    大型多语言预训练语言模型（mPLMs）已成为跨语言NLP中的事实标准，但是，它们在许多语言的大规模部署方面受到诸多限制，包括预训练数据稀缺、词汇量增加和参数预算的限制。为了增强mPLMs处理低资源和未知语言的能力，我们探索大规模利用转写的潜力。具体而言，我们探索UROMAN转写工具的潜力，该工具为所有书写系统提供了从UTF-8到拉丁字符的映射，从而实现了几乎任何语言的廉价罗马化。首先，我们重点研究了UROMAN相对于其他语言特定和手动策划的转写工具在适应多语言PLMs方面的差异。然后，我们研究并比较了一系列数据和参数高效的策略，以适应罗马化和非罗马化的14种不同以上语言数据。

    Large multilingual pretrained language models (mPLMs) have become the de facto state of the art for cross-lingual transfer in NLP. However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their parameter budget. In order to boost the capacity of mPLMs to deal with low-resource and unseen languages, we explore the potential of leveraging transliteration on a massive scale. In particular, we explore the UROMAN transliteration tool, which provides mappings from UTF-8 to Latin characters for all the writing systems, enabling inexpensive romanization for virtually any language. We first focus on establishing how UROMAN compares against other language-specific and manually curated transliterators for adapting multilingual PLMs. We then study and compare a plethora of data- and parameter-efficient strategies for adapting the mPLMs to romanized and non-romanized corpora of 14 diverse low-r
    
[^16]: 近似最近邻短语挖掘在上下文语音识别中的应用

    Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition. (arXiv:2304.08862v1 [cs.CL])

    [http://arxiv.org/abs/2304.08862](http://arxiv.org/abs/2304.08862)

    本文提出了一种使用近似最近邻短语挖掘的方法来训练上下文感知Transformer转录器(CATT)模型，并在大规模数据情况下进行了实验，取得了显著的实验结果。

    

    本文提出了一种使用近似最近邻短语挖掘的方法来训练端到端上下文感知Transformer转录器(CATT)模型的扩展方法。在训练过程中，给定一个参考查询，我们使用近似最近邻搜索挖掘了若干相似的短语作为负例，并将这些短语与随机和真实的上下文信息一起用作上下文列表中的负例。通过将近似最近邻短语（ANN-P）包含在上下文列表中，我们鼓励学习表示来区分相似但不完全相同的偏见短语，从而在偏见清单中存在几个相似的短语时提高偏见准确性。我们在大规模数据情况下进行实验，获得了相对字误率达7％的上下文部分的实验效果。我们还扩展并评估了CATT方法在串流应用中的应用。

    This paper presents an extension to train end-to-end Context-Aware Transformer Transducer ( CATT ) models by using a simple, yet efficient method of mining hard negative phrases from the latent space of the context encoder. During training, given a reference query, we mine a number of similar phrases using approximate nearest neighbour search. These sampled phrases are then used as negative examples in the context list alongside random and ground truth contextual information. By including approximate nearest neighbour phrases (ANN-P) in the context list, we encourage the learned representation to disambiguate between similar, but not identical, biasing phrases. This improves biasing accuracy when there are several similar phrases in the biasing inventory. We carry out experiments in a large-scale data regime obtaining up to 7% relative word error rate reductions for the contextual portion of test data. We also extend and evaluate CATT approach in streaming applications.
    
[^17]: 通过接近的语言实现到低资源语言的转移: 以法罗语为案例研究

    Transfer to a Low-Resource Language via Close Relatives: The Case Study on Faroese. (arXiv:2304.08823v1 [cs.CL])

    [http://arxiv.org/abs/2304.08823](http://arxiv.org/abs/2304.08823)

    利用与其关系紧密的语言，可改善到低资源语言的跨语言转移，以斯堪的纳维亚语言家族中的其他语言资源数据优化法罗语的NLP模型性能。

    

    多语种语言模型推动了跨语言NLP转移的最新技术。然而，大多数零-shot跨语言传输使用一个大规模多语种变压器（例如mBERT或XLM-R）传输到所有目标语言，而不考虑它们与其他语言的语言学、词源和系统发育关系。在这项工作中，我们通过案例研究法罗语——一种来自高资源语言家族的低资源语言，实验证明通过利用系统发育信息和离开“一刀切”的范式，可以改善低资源语言的跨语言转移。特别是，我们利用其他斯堪的纳维亚语言的丰富资源（如丹麦语、挪威语、瑞典语和冰岛语）来增强法罗语的表现。我们的评估结果表明，我们可以大大提高跨语言转移的性能。

    Multilingual language models have pushed state-of-the-art in cross-lingual NLP transfer. The majority of zero-shot cross-lingual transfer, however, use one and the same massively multilingual transformer (e.g., mBERT or XLM-R) to transfer to all target languages, irrespective of their typological, etymological, and phylogenetic relations to other languages. In particular, readily available data and models of resource-rich sibling languages are often ignored. In this work, we empirically show, in a case study for Faroese -- a low-resource language from a high-resource language family -- that by leveraging the phylogenetic information and departing from the 'one-size-fits-all' paradigm, one can improve cross-lingual transfer to low-resource languages. In particular, we leverage abundant resources of other Scandinavian languages (i.e., Danish, Norwegian, Swedish, and Icelandic) for the benefit of Faroese. Our evaluation results show that we can substantially improve the transfer performan
    
[^18]: TTIDA: 通过文本到文本模型和文本到图像模型进行可控生成数据增强

    TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models. (arXiv:2304.08821v1 [cs.CV])

    [http://arxiv.org/abs/2304.08821](http://arxiv.org/abs/2304.08821)

    本论文提出了一种名为TTIDA的生成式数据增强方法，利用文本到文本和文本到图像模型生成可控的逼真标记图像。

    

    数据增强已被证明是一种增补低资源数据集有用信息的有效方法。传统的增强技术，如噪声注入和图像变换，已被广泛使用。此外，生成式数据增强（GDA）已被证明能够产生更多样化和灵活的数据。虽然生成对抗网络（GAN）经常用于GDA，但与文本到图像扩散模型相比，它们缺乏多样性和可控性。在本文中，我们提出了TTIDA（文本到文本到图像数据增强），利用大规模预训练的文本到文本（T2T）和文本到图像（T2I）生成模型进行数据增强。通过将T2I模型的条件设置为T2T模型生成的详细描述，我们能够以灵活和可控的方式生成逼真的标记图像。在领域内分类、跨领域分类和图像字幕任务的实验中，展示了一致的结果。

    Data augmentation has been established as an efficacious approach to supplement useful information for low-resource datasets. Traditional augmentation techniques such as noise injection and image transformations have been widely used. In addition, generative data augmentation (GDA) has been shown to produce more diverse and flexible data. While generative adversarial networks (GANs) have been frequently used for GDA, they lack diversity and controllability compared to text-to-image diffusion models. In this paper, we propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image (T2I) generative models for data augmentation. By conditioning the T2I model on detailed descriptions produced by T2T models, we are able to generate photo-realistic labeled images in a flexible and controllable manner. Experiments on in-domain classification, cross-domain classification, and image captioning tasks show consis
    
[^19]: 重新审视相似性和差异性在最佳反驳检索中的作用

    Revisiting the Role of Similarity and Dissimilarity inBest Counter Argument Retrieval. (arXiv:2304.08807v1 [cs.CL])

    [http://arxiv.org/abs/2304.08807](http://arxiv.org/abs/2304.08807)

    本文研究了最佳反驳检索任务，提出了一种评分模型，使用相似性和差异性度量，达到了88.9％的accuracy@1，显著优于其他基线模型。

    

    本文研究了在给定输入论点情况下的最佳反驳检索任务。根据最佳反驳定义，最佳反驳应与输入论点在细节方面相似，但立场相反。本文旨在开发一种基于相似性和差异性度量的高效和有效的模型来对反驳进行评分。我们首先对现有的评分方法（包括传统的学习排序（LTR）和最近的神经评分模型）进行了实证研究。然后，我们提出了Bipolar-encoder，一种基于BERT的新型模型，用于学习同时相似性和差异性的最优表示。实验结果表明，我们提出的方法可以达到88.9％的accuracy@1，显著优于其他基线模型。当与适当的缓存技术结合使用时，Bipolar-encoder在预测时间上也具有可比性的效率。

    This paper studies the task of best counter-argument retrieval given an input argument. Following the definition that the best counter-argument addresses the same aspects as the input argument while having the opposite stance, we aim to develop an efficient and effective model for scoring counter-arguments based on similarity and dissimilarity metrics. We first conduct an experimental study on the effectiveness of available scoring methods, including traditional Learning-To-Rank (LTR) and recent neural scoring models. We then propose Bipolar-encoder, a novel BERT-based model to learn an optimal representation for simultaneous similarity and dissimilarity. Experimental results show that our proposed method can achieve the accuracy@1 of 88.9\%, which significantly outperforms other baselines by a large margin. When combined with an appropriate caching technique, Bipolar-encoder is comparably efficient at prediction time.
    
[^20]: 多方会话中的发言人个人特征分析

    Speaker Profiling in Multiparty Conversations. (arXiv:2304.08801v1 [cs.CL])

    [http://arxiv.org/abs/2304.08801](http://arxiv.org/abs/2304.08801)

    本文提出了一个名为SPC的任务，旨在为对话中每个发言者生成个人特征摘要。任务被分为三个子任务：个人特征发现、个人特征类型识别和个人特征价值提取。任务对于银行、酒店预订和航空预订等行业中的聊天机器人非常重要，可以使聊天机器人更好地了解和回应每个发言人的需求。

    

    在对话环境中，个体展现出独特的行为，使得“一刀切”的方法不足以为对话代理生成回应。虽然过去的研究旨在使用发言人个人信息创建个性化对话代理，但它们依赖于前提，即发言人个人特征已经被提供。然而，在像银行、酒店预订和航空预订等行业中使用的聊天机器人方面，这一假设并不总是正确的。本文旨在通过探索对话中的发言人个人特征分析 (SPC)任务来填补这个空白。SPC的主要目标是为对话中每个发言人产生个人特征摘要。为了实现这一目标，我们将任务分为三个子任务：个人特征发现、个人特征类型识别和个人特征价值提取。在给定对话的情况下，第一个子任务旨在识别包含个人信息的所有话语。

    In conversational settings, individuals exhibit unique behaviors, rendering a one-size-fits-all approach insufficient for generating responses by dialogue agents. Although past studies have aimed to create personalized dialogue agents using speaker persona information, they have relied on the assumption that the speaker's persona is already provided. However, this assumption is not always valid, especially when it comes to chatbots utilized in industries like banking, hotel reservations, and airline bookings. This research paper aims to fill this gap by exploring the task of Speaker Profiling in Conversations (SPC). The primary objective of SPC is to produce a summary of persona characteristics for each individual speaker present in a dialogue. To accomplish this, we have divided the task into three subtasks: persona discovery, persona-type identification, and persona-value extraction. Given a dialogue, the first subtask aims to identify all utterances that contain persona information.
    
[^21]: 基于预训练语言模型的生物医学文本摘要综述

    A Survey on Biomedical Text Summarization with Pre-trained Language Model. (arXiv:2304.08763v1 [cs.CL])

    [http://arxiv.org/abs/2304.08763](http://arxiv.org/abs/2304.08763)

    本文总结了基于预训练语言模型的生物医学文本摘要方法，提炼关键信息生成简洁的摘要，分为微调、基于特征和无监督方法，未来研究方向包括融合领域特定知识和开发更适合的评估指标。

    

    生物医学文献和电子病历等生物医学文本的指数级增长，给临床医生和研究人员高效获取临床信息带来巨大挑战。为解决这一问题，提出了生物医学文本摘要方法，旨在从单个或多个生物医学文档中提炼关键信息生成简洁的摘要。近年来，预训练语言模型（PLMs）已成为多种自然语言处理任务的事实标准，PLMs在生物医学领域中的应用也为生物医学文本摘要任务带来新的启示。本文系统地总结了近期基于PLMs探索生物医学文本摘要的进展，帮助理解最新的进展、挑战和未来方向。我们根据使用PLMs的方式对基于PLMs的方法进行分类，包括微调、基于特征和无监督方法。我们还讨论了潜在的未来研究方向，如融合领域特定知识和开发更适合的评估指标。

    The exponential growth of biomedical texts such as biomedical literature and electronic health records (EHRs), provides a big challenge for clinicians and researchers to access clinical information efficiently. To address the problem, biomedical text summarization has been proposed to support clinical information retrieval and management, aiming at generating concise summaries that distill key information from single or multiple biomedical documents. In recent years, pre-trained language models (PLMs) have been the de facto standard of various natural language processing tasks in the general domain. Most recently, PLMs have been further investigated in the biomedical field and brought new insights into the biomedical text summarization task. In this paper, we systematically summarize recent advances that explore PLMs for biomedical text summarization, to help understand recent progress, challenges, and future directions. We categorize PLMs-based approaches according to how they utilize
    
[^22]: 关于概率神经摘要中的不确定性校准和选择性生成的基准研究

    On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study. (arXiv:2304.08653v1 [cs.CL])

    [http://arxiv.org/abs/2304.08653](http://arxiv.org/abs/2304.08653)

    本研究对不同最先进的概率学习方法在提高神经摘要模型不确定性质量和生成效果方面进行了调查和对比，结果表明概率方法能够持续提高生成和不确定性质量，实现了高质量生成和放弃低质量摘要，且揭示了显著的失效模式。

    

    现代深度摘要模型在基准性能方面取得了令人印象深刻的成果，但它们往往会生成错误校准的预测不确定性。这意味着它们对质量较低的预测赋予了高信心度，从而在实际应用中导致可靠性和信任度的降低。概率深度学习方法是解决误校准问题的常见方法。然而，它们在复杂自回归摘要任务中的相对有效性尚不清楚。在本工作中，我们彻底调查了不同最先进的概率方法在提高神经摘要模型不确定性质量方面的有效性，跨越了三个难度不同的大规模基准。我们发现，概率方法始终能够提高模型的生成和不确定性质量，从而在实践中实现了高质量生成（即放弃低质量摘要）。我们还揭示了显著的失效模式。

    Modern deep models for summarization attains impressive benchmark performance, but they are prone to generating miscalibrated predictive uncertainty. This means that they assign high confidence to low-quality predictions, leading to compromised reliability and trustworthiness in real-world applications. Probabilistic deep learning methods are common solutions to the miscalibration problem. However, their relative effectiveness in complex autoregressive summarization tasks are not well-understood. In this work, we thoroughly investigate different state-of-the-art probabilistic methods' effectiveness in improving the uncertainty quality of the neural summarization models, across three large-scale benchmarks with varying difficulty. We show that the probabilistic methods consistently improve the model's generation and uncertainty quality, leading to improved selective generation performance (i.e., abstaining from low-quality summaries) in practice. We also reveal notable failure patterns 
    
[^23]: 基于BERT的技术对美国最高法院案例进行分类

    Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])

    [http://arxiv.org/abs/2304.08649](http://arxiv.org/abs/2304.08649)

    本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。

    

    基于双向编码器表示来自变压器的模型（BERT）在许多自然语言处理（NLP）任务（如命名实体识别（NER），词性（POS）标记等）上产生了最新技术（SOTA）结果。当分类长文档（例如来自美国最高法院的文档）时，使用BERT模型可能比较困难。本文中，我们尝试了几种基于BERT的分类技术，用于对美国最高法院决定或最高法院数据库（SCDB）进行分类，并将其与先前的SOTA结果进行了比较。我们还将我们的结果与针对长文档的SOTA模型进行了比较。我们对两个分类任务进行了比较：（1）广泛的分类任务，具有15个类别；（2）细粒度的分类任务，具有279个类别。我们的最佳结果在15个广泛类别上产生80％的准确度，在279个细粒度类别上产生60％的准确度。

    Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
    
[^24]: 大型语言模型输出的评估：话语和记忆

    An Evaluation on Large Language Model Outputs: Discourse and Memorization. (arXiv:2304.08637v1 [cs.CL])

    [http://arxiv.org/abs/2304.08637](http://arxiv.org/abs/2304.08637)

    评估了九个大语言模型的输出，发现其中80％包含记忆数据，但包含最多记忆内容的输出更可能是高质量的。提出了缓解策略以降低记忆文本率。

    

    我们对九个最广泛可用的大型语言模型（LLMs）生成的各种输出进行了经验性评估。我们使用现成的工具进行分析，发现在与输出病态（例如，反事实和逻辑上的错误陈述）以及不保持主题等方面的关系中，记忆文本百分比、独特文本百分比和整体输出质量之间存在相关性。总体而言，80.0％的输出包含记忆数据，但包含最多记忆内容的输出也更有可能被认为具有高质量。我们讨论和评估了缓解策略，并显示，在评估的模型中，输出的记忆文本率有所降低。最后，我们就学习、记忆和评估优质文本的潜在影响进行了讨论。

    We present an empirical evaluation of various outputs generated by nine of the most widely-available large language models (LLMs). Our analysis is done with off-the-shelf, readily-available tools. We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic. Overall, 80.0% of the outputs evaluated contained memorized data, but outputs containing the most memorized content were also more likely to be considered of high quality. We discuss and evaluate mitigation strategies, showing that, in the models evaluated, the rate of memorized text being output is reduced. We conclude with a discussion on potential implications around what it means to learn, to memorize, and to evaluate quality text.
    
[^25]: 离散与反向传播的桥梁：直通法与其它方法

    Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])

    [http://arxiv.org/abs/2304.08612](http://arxiv.org/abs/2304.08612)

    本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。

    

    反向传播是深度学习中的基石，但其仅限于计算连续变量的梯度，限制了涉及离散潜变量的问题的研究。针对这个问题，我们提出了一种新的方法来近似生成离散潜变量的参数的梯度。我们首先考察了广泛使用的 Straight-Through（ST）启发式方法，并证明它作为梯度的一阶近似值。在此基础上，我们提出了一种新的方法，称为 ReinMax，它集成了 Heun's Method，一种解ODE的二阶数值方法，以近似梯度。我们的方法实现了二阶精度，而不需要 Hessian 或其他二阶导数。我们进行了结构化输出预测和无监督生成建模任务的实验。我们的结果显示，\ours 在现有技术中带来了持续的改进，包括 ST 和 Straight-Through Gum。

    Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
    
[^26]: 面向字符级长尾分布的场景文本识别的改进

    Improving Scene Text Recognition for Character-Level Long-Tailed Distribution. (arXiv:2304.08592v1 [cs.CV])

    [http://arxiv.org/abs/2304.08592](http://arxiv.org/abs/2304.08592)

    该论文提出一种新方法，使用平衡和长尾分布的数据集进行文本识别训练，同时保留上下文信息，以提高字符级长尾分布上的STR性能。

    

    尽管场景文本识别（STR）取得了显著进展，但大多数研究主要集中在仅包含少量字符的英语上。然而，STR模型在诸如中文和韩文等字符数量众多的语言中（尤其是因字符长尾分布而很少出现的字符）表现出较大的性能下降。为了解决这个问题，我们使用具有不同字符级分布（例如平衡和长尾分布）的合成数据集进行了实证分析。增加大量尾部类别可以帮助模型正确地单独识别字符，但是，使用这样的合成数据集进行训练会影响模型学习词汇的上下文信息（即字符之间的关系），这对于正确识别整个词汇同样重要。基于此动机，我们提出了一种新方法，在训练过程中利用平衡和长尾分布，同时保留上下文信息，以提高字符级长尾分布上的STR性能。

    Despite the recent remarkable improvements in scene text recognition (STR), the majority of the studies focused mainly on the English language, which only includes few number of characters. However, STR models show a large performance degradation on languages with a numerous number of characters (e.g., Chinese and Korean), especially on characters that rarely appear due to the long-tailed distribution of characters in such languages. To address such an issue, we conducted an empirical analysis using synthetic datasets with different character-level distributions (e.g., balanced and long-tailed distributions). While increasing a substantial number of tail classes without considering the context helps the model to correctly recognize characters individually, training with such a synthetic dataset interferes the model with learning the contextual information (i.e., relation among characters), which is also important for predicting the whole word. Based on this motivation, we propose a nov
    
[^27]: 社交媒体文本中讽刺检测的研究视角

    Researchers eye-view of sarcasm detection in social media textual content. (arXiv:2304.08582v1 [cs.CL])

    [http://arxiv.org/abs/2304.08582](http://arxiv.org/abs/2304.08582)

    社交媒体中讽刺文本使用普遍，但讽刺检测较为困难。文章讨论了讽刺检测的技术和方法，并提出需要重点关注乐观和前瞻性的讽刺检测方法。

    

    社交媒体中讽刺文本的广泛使用对目标用户产生生理作用。每个用户对误用和识别讽刺的方法不同。讽刺检测即使对于用户来说也是困难的，这将取决于很多因素，如视角、语境、特殊符号等。因此，让机器从众多文本语料库中区分讽刺句子和非讽刺句子将是一个具有挑战性的任务。在目前的情况下，没有确切的规则可以使模型准确地检测出讽刺。因此，需要重点关注乐观和前瞻性的讽刺检测方法。本文讨论了各种讽刺检测技术，并以一些方法、相关数据集以及研究人员面临的挑战作为结论。

    The enormous use of sarcastic text in all forms of communication in social media will have a physiological effect on target users. Each user has a different approach to misusing and recognising sarcasm. Sarcasm detection is difficult even for users, and this will depend on many things such as perspective, context, special symbols. So, that will be a challenging task for machines to differentiate sarcastic sentences from non-sarcastic sentences. There are no exact rules based on which model will accurately detect sarcasm from many text corpus in the current situation. So, one needs to focus on optimistic and forthcoming approaches in the sarcasm detection domain. This paper discusses various sarcasm detection techniques and concludes with some approaches, related datasets with optimal features, and the researcher's challenges.
    
[^28]: 基于中文指令数据的全参数和LoRA调参方法在指令遵循大型语言模型上的比较研究

    A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model. (arXiv:2304.08109v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08109](http://arxiv.org/abs/2304.08109)

    采用参数效率高的LoRA调参技术可以在指令调参中取得令人鼓舞的结果，但全参数调参方法在基础模型选择、训练数据集规模、可学参数量等方面也有重要作用。

    

    近来，大型语言模型的指令调参研究是自然语言处理领域的一个关键研究方向。由于资源与成本限制，一些研究者采用参数效率高的调参技术，例如LoRA，在指令调参中取得了令人鼓舞的结果。本文以LLaMA为基础模型，对全参数调参和LoRA调参方法进行了实验比较。实验结果表明，基础模型的选择、训练数据集规模、可学参数量以及模型训练成本都是重要因素。希望本文的实验结论能为大型语言模型的训练提供启示，特别是在中文领域，并帮助研究人员找到更好的训练成本与性能的平衡策略。

    Recently, the instruction-tuning of large language models is a crucial area of research in the field of natural language processing. Due to resource and cost limitations, several researchers have employed parameter-efficient tuning techniques, such as LoRA, for instruction tuning, and have obtained encouraging results In comparison to full-parameter fine-tuning, LoRA-based tuning demonstrates salient benefits in terms of training costs. In this study, we undertook experimental comparisons between full-parameter fine-tuning and LoRA-based tuning methods, utilizing LLaMA as the base model. The experimental results show that the selection of the foundational model, training dataset scale, learnable parameter quantity, and model training cost are all important factors. We hope that the experimental conclusions of this paper can provide inspiration for training large language models, especially in the field of Chinese, and help researchers find a better trade-off strategy between training c
    
[^29]: 中文开放式指令广义语言模型：初步发布

    Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.07987](http://arxiv.org/abs/2304.07987)

    本论文旨在通过适应不同子任务的固有特性，创建一个中文指令数据集，以填补指令调整技术在中文语言领域的空白。

    

    指令调整被广泛认为是构建广义语言模型的关键技术，随着InstructGPT和ChatGPT的发布，它已经引起了研究人员和公众的关注。尽管英语为基础的大规模语言模型取得了令人瞩目的进展，但是还未探索英语为基础的语言模型在多语任务上是否可以像英语任务那样通过精心设计的指令调整来执行，以及我们如何构建所需的语料库进行调整。为填补这一空白，我们提出了一个项目，试图通过适应4个子任务的固有特性，采用各种方法创建一个中文指令数据集。我们收集了约20万个中文指令调整样本，并进行了人工检查以确保高质量。我们还总结了现有的英文和中文指令语料库，并对一些潜在的应用进行了简要描述。

    Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\citep{ouyang2022training} and ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning.  To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applic
    
[^30]: 低资源语言的神经机器翻译

    Neural Machine Translation For Low Resource Languages. (arXiv:2304.07869v1 [cs.CL])

    [http://arxiv.org/abs/2304.07869](http://arxiv.org/abs/2304.07869)

    该论文研究了低资源语言的神经机器翻译，并构建了一个基于 \texttt{mBART.CC25} 语言模型的模型，利用后向翻译和迁移学习等 NLP 和深度学习技术进行增强，以达到最先进的结果。

    

    由于自然语言的内在复杂性和流动性，神经机器翻译是一个具有挑战性的任务。尽管近年来在几种语言对中取得了最先进的表现，但在多语言神经机器翻译 (MNMT) 领域看到了很多关注，却没有进行全面调查以确定哪些方法表现良好。该项目的目标是研究低资源语言的领域，并构建一个神经机器翻译模型，以实现最先进的结果。该项目旨在建立在 \texttt{mBART.CC25} 语言模型基础上，并探索利用各种 NLP 和深度学习技术（如后向翻译和迁移学习）来增强它的策略。该实现试图解开 NMT 应用程序的架构，并确定不同的组件，这些组件为我们提供了修改所述应用程序的机会。

    Neural Machine translation is a challenging task due to the inherent complex nature and the fluidity that natural languages bring. Nonetheless, in recent years, it has achieved state-of-the-art performance in several language pairs. Although, a lot of traction can be seen in the areas of multilingual neural machine translation (MNMT) in the recent years, there are no comprehensive survey done to identify what approaches work well. The goal of this project is to investigate the realm of low resource languages and build a Neural Machine Translation model to achieve state-of-the-art results. The project looks to build upon the \texttt{mBART.CC25} \cite{liu2020multilingual} language model and explore strategies to augment it with various NLP and Deep Learning techniques like back translation and transfer learning. This implementation tries to unpack the architecture of the NMT application and determine the different components which offers us opportunities to amend the said application wit
    
[^31]: 可操作的自回归语言生成控制方法

    Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])

    [http://arxiv.org/abs/2304.07438](http://arxiv.org/abs/2304.07438)

    本文提出了一种在自回归文本生成中使用可操作概率模型来强制实施限制的控制方法GeLaTo，并取得了在常见的约束文本生成测试上的最先进性能。

    

    尽管自回归大语言模型在文本生成方面取得了成功，但生成满足复杂限制的文本仍然是一个重大挑战：即使是最简单的词汇限制也使条件分布$\Pr(\text{text} | \alpha)$的采样变得不可计算。为了克服这个挑战，我们提出使用可操作的概率模型将词汇限制强加于自回归文本生成中，我们将其称为 GeLaTo。为了证明这个框架的有效性，我们使用了精简的隐马尔可夫模型来控制从GPT2到自回归的生成。GeLaTo在约束文本生成的具有挑战性的基准测试CommonGen上取得了最先进的性能，大幅击败了各种强基线。我们的工作不仅为控制大型语言模型开辟了新的途径，还激励人们开发更具表现力的可操作概率模型。

    Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\Pr(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.
    
[^32]: 快速密集信息检索器利用KALE进行后置KL对齐的异形双编码器模型训练 (arXiv:2304.01016v2 [cs.CL] UPDATED)

    Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders. (arXiv:2304.01016v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.01016](http://arxiv.org/abs/2304.01016)

    本文提出了一种通过结构压缩和模型尺寸不对称的双编码器模型 KALE，有效提高密集信息检索的推理效率，同时允许查询编码器的有效压缩，而无需进行全部的再训练或索引生成，此方法能够生成超过DistilBERT性能的模型。

    

    本文提出了一种有结构压缩和模型尺寸不对称的双编码器模型，旨在提高基于语言模型的密集信息检索系统的推理速度。通过对MSMARCO、自然问答、问答游戏等多个数据集进行前后训练压缩实验，研究了压缩对系统推理效率的影响，结果表明密集信息检索器的双编码器结构异形化有助于提高其推理效率。基于此，我们引入了一种名为Kullback Leibler Alignment of Embeddings (KALE)的方法，通过裁剪和对齐查询编码器，提高了密集信息检索的推理效率。KALE扩展了传统的知识蒸馏方法，使得在双编码器训练后可以有效地对查询编码器进行压缩而无需进行完整的再训练或索引生成。使用KALE和不对称训练，我们可以生成超过DistilBERT性能的模型，同时模型尺寸更小。

    In this paper, we consider the problem of improving the inference latency of language model-based dense retrieval systems by introducing structural compression and model size asymmetry between the context and query encoders. First, we investigate the impact of pre and post-training compression on the MSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that asymmetry in the dual encoders in dense retrieval can lead to improved inference efficiency. Knowing this, we introduce Kullback Leibler Alignment of Embeddings (KALE), an efficient and accurate method for increasing the inference efficiency of dense retrieval methods by pruning and aligning the query encoder after training. Specifically, KALE extends traditional Knowledge Distillation after bi-encoder training, allowing for effective query encoder compression without full retraining or index generation. Using KALE and asymmetric training, we can generate models which exceed the performance of DistilBERT despite having 
    
[^33]: GrapeQA：增强问答功能的图形增强和剪枝方法

    GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering. (arXiv:2303.12320v1 [cs.CL])

    [http://arxiv.org/abs/2303.12320](http://arxiv.org/abs/2303.12320)

    GrapeQA是一种新方法，使用“重要实体图形增强”和“上下文感知节点剪枝”策略，以提高问答准确性和效率。

    

    常识问答方法结合了预先训练的语言模型（LM）的能力和知识图（KG）提供的推理。 典型方法从KG中收集与QA匹配的节点以形成工作图（WG），然后使用图神经网络（GNN）进行推理。这面临两个主要挑战：（i）很难从WG中捕获QA中的所有信息，（ii）WG包含一些来自KG的不相关节点。为了解决这些问题，我们提出了一个名为GrapeQA的算法以对WG进行两个简单的改进：（i）用于图形增强的重要实体（Prominent Entities）识别QA对当中相关文本块，并使用相应的潜在表示从LM进行增强；（ii）将不相关的节点剪枝。我们在OpenBookQA，CommonsenseQA和MedQA-USMLE上评估了结果，并发现GrapeQA显示出持续的改进，超过了其LM + KG前身（特别是QA-GNN）并获得了巨大的改进。

    Commonsense question-answering (QA) methods combine the power of pre-trained Language Models (LM) with the reasoning provided by Knowledge Graphs (KG). A typical approach collects nodes relevant to the QA pair from a KG to form a Working Graph (WG) followed by reasoning using Graph Neural Networks(GNNs). This faces two major challenges: (i) it is difficult to capture all the information from the QA in the WG, and (ii) the WG contains some irrelevant nodes from the KG. To address these, we propose GrapeQA with two simple improvements on the WG: (i) Prominent Entities for Graph Augmentation identifies relevant text chunks from the QA pair and augments the WG with corresponding latent representations from the LM, and (ii) Context-Aware Node Pruning removes nodes that are less relevant to the QA pair. We evaluate our results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows consistent improvements over its LM + KG predecessor (QA-GNN in particular) and large improveme
    
[^34]: 工作场所中的大型语言模型：一个关于任务提示工程在职业类型分类中的实证研究

    Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification. (arXiv:2303.07142v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.07142](http://arxiv.org/abs/2303.07142)

    本文探索了工作场所中的职业分类任务，并利用任务提示工程设计了良好的提示，成功地将大型语言模型（LLMs）应用于该任务中，取得了出色的表现，优于传统方法。

    

    本文通过探索多种文本分类方法，包括基于监督学习的传统模型如支持向量机（SVMs）以及最先进的深度学习方法，如DeBERTa，以及大型语言模型（LLMs）在少样本和零样本分类情况下的应用，来研究实际工作场所中的职业分类任务。为了完成此任务，我们采用了任务提示工程的技术，即设计提示以引导LLMs达到所需的输出。具体来说，我们评估了两种商业可用的最先进的基于GPT-3.5的语言模型，text-davinci-003和gpt-3.5-turbo。我们还对提示工程的不同方面对模型性能的影响进行了详细的分析。我们的结果表明，在良好设计的提示的帮助下，LLMs在职业类型分类任务上可以达到出色的表现，优于传统方法如SVMs，甚至优于最先进的深度学习方法如DeBERTa。

    This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed pr
    
[^35]: 探索早期英国出版物中的文本重用情况：《Reception Reader》

    Reception Reader: Exploring Text Reuse in Early Modern British Publications. (arXiv:2302.04084v2 [cs.DL] UPDATED)

    [http://arxiv.org/abs/2302.04084](http://arxiv.org/abs/2302.04084)

    《Reception Reader》是一个web工具，用于探索早期英国出版物中的文本重用情况，用户可以通过共享文本片段探索某一作品的接受情况或其流入连接历史，并且可以交互式地浏览连接文档的详细信息，以及检查重用文本的上下文以进行“近距离阅读”。

    

    The Reception Reader是一个用于研究Early English Books Online（EEBO-TCP）和Eighteenth Century Collections Online（ECCO）数据中文本重用的web工具。用户可以：1）基于共享文本片段探索某一作品的接受情况或其流入连接历史；2）交互式地浏览连接文档的详细信息；以及3）检查重用文本的上下文以进行“近距离阅读”。我们展示了该工具如何简化研究和探索任务，并讨论了用户界面的实用性和局限性以及当前数据来源。

    The Reception Reader is a web tool for studying text reuse in the Early English Books Online (EEBO-TCP) and Eighteenth Century Collections Online (ECCO) data. Users can: 1) explore a visual overview of the reception of a work, or its incoming connections, across time based on shared text segments, 2) interactively survey the details of connected documents, and 3) examine the context of reused text for "close reading". We show examples of how the tool streamlines research and exploration tasks, and discuss the utility and limitations of the user interface along with its current data sources.
    
[^36]: 神经表征的非对齐属性消除

    Erasure of Unaligned Attributes from Neural Representations. (arXiv:2302.02997v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.02997](http://arxiv.org/abs/2302.02997)

    提出了AMSAL算法，可去除神经表征中的对齐属性隐含信息，在多个数据集上测试有效消除偏见。

    

    我们提出了分配-最大化谱属性去除（AMSAL）算法，该算法可在对齐属性隐含于输入样例中时消除神经表征中的信息。我们的算法通过交替两步操作来实现。第一步，找到输入表征与要删除的信息之间的匹配，第二步，将输入表征和要删除的信息投影到相同的潜在空间中。我们在多个数据集上测试了我们的算法，其中包括一个带有多个受保护属性的Twitter数据集，BiasBios数据集和BiasBench基准测试。 BiasBench 基准测试包括四个具有各种类型受保护属性的数据集。我们的结果表明：我们的算法可以有效消除偏见。我们还讨论了当主任务与要删除的信息之间存在强耦合时，我们算法的限制。

    We present the Assignment-Maximization Spectral Attribute removaL (AMSAL) algorithm, which erases information from neural representations when the information to be erased is implicit rather than directly being aligned to each input example. Our algorithm works by alternating between two steps. In one, it finds an assignment of the input representations to the information to be erased, and in the other, it creates projections of both the input representations and the information to be erased into a joint latent space. We test our algorithm on an extensive array of datasets, including a Twitter dataset with multiple guarded attributes, the BiasBios dataset and the BiasBench benchmark. The last benchmark includes four datasets with various types of protected attributes. Our results demonstrate that bias can often be removed in our setup. We also discuss the limitations of our approach when there is a strong entanglement between the main task and the information to be erased.
    
[^37]: EXIF作为一种语言：学习图像与相机元数据之间的交叉模态关联

    EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata. (arXiv:2301.04647v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.04647](http://arxiv.org/abs/2301.04647)

    本文通过学习图像和相机元数据之间的交叉模态关联提取相机信息，并使用得到的特征成功实现拼接图像区域的"零样本"定位。

    

    本文旨在学习一个视觉表示，从而提取与所记录的照片相关的相机信息。为此，我们在图像块和自动插入到图像文件中的EXIF元数据之间训练了一个多模态嵌入。我们的模型通过将元数据转换为文本，然后使用transformer进行处理来表示此元数据。我们学习的特征在下游图像取证和校准任务上明显优于其他自监督和有监督特征。特别地，我们成功地通过对图像内所有块的视觉嵌入进行聚类来实现"零样本"的拼接图像区域定位。

    We learn a visual representation that captures information about the camera that recorded a given photo. To do this, we train a multimodal embedding between image patches and the EXIF metadata that cameras automatically insert into image files. Our model represents this metadata by simply converting it to text and then processing it with a transformer. The features that we learn significantly outperform other self-supervised and supervised features on downstream image forensics and calibration tasks. In particular, we successfully localize spliced image regions "zero shot" by clustering the visual embeddings for all of the patches within an image.
    
[^38]: InferEM: 推断说话者意图的共情对话生成模型

    InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.06373](http://arxiv.org/abs/2212.06373)

    通过推断对话中最后一次发言来捕捉说话者的意图，提出了一种利用多头注意力的意图融合模块的共情对话生成模型InferEM。模型同时利用前几次发言预测最后一次发言，具有较高的可行性。

    

    目前，共情回复生成的方法一般直接编码整个对话历史，然后通过解码器生成友好的反馈。这些方法强调建模情境信息，但忽视了捕捉说话者的直接意图。我们认为对话中最后一次发言表达了说话者的意图。因此，我们提出了一种名为InferEM的新模型用于共情回复生成。我们将最后一次发言单独编码，通过基于多头注意力的意图融合模块与整个对话融合以捕捉说话者的意图。此外，我们利用前几次发言预测最后一次发言，以模拟人类的心理，猜测对话者可能提前说些什么。为平衡发言预测和回复生成的优化速率，InferEM还设计了一种多任务学习策略。实验结果证明了该模型的可行性。

    Current approaches to empathetic response generation typically encode the entire dialogue history directly and put the output into a decoder to generate friendly feedback. These methods focus on modelling contextual information but neglect capturing the direct intention of the speaker. We argue that the last utterance in the dialogue empirically conveys the intention of the speaker. Consequently, we propose a novel model named InferEM for empathetic response generation. We separately encode the last utterance and fuse it with the entire dialogue through the multi-head attention based intention fusion module to capture the speaker's intention. Besides, we utilize previous utterances to predict the last utterance, which simulates human's psychology to guess what the interlocutor may speak in advance. To balance the optimizing rates of the utterance prediction and response generation, a multi-task learning strategy is designed for InferEM. Experimental results demonstrate the plausibility
    
[^39]: 知识就是力量：理解因果关系使法律判决预测模型更具普适性和鲁棒性

    Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust. (arXiv:2211.03046v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.03046](http://arxiv.org/abs/2211.03046)

    本研究使用因果结构模型分析了法律判决预测模型学习决策的原理，发现现有最先进模型利用非因果信息进行判决预测，违反法律规则会削弱模型鲁棒性和普适性并导致歧视问题，提出基于因果干预的解决方案。

    

    法律判决预测是一种基于法律规则根据事实描述预测判决的法律辅助工具，旨在缓解有限法律从业人员的巨大工作负担。目前，大多数现有方法都采用各种大规模预训练语言模型（PLMs）在LJP任务中进行微调，从而实现了一致的改进。然而，我们发现现有的最先进模型根据无关（或非因果）信息进行判决预测。违反法律规则不仅削弱了模型的鲁棒性和普适性，还会导致严重的社会问题，如歧视。本文使用因果结构模型（SCMs）理论地分析了LJP模型如何学习做出决策以及为什么它们可以在不学习因果关系的情况下成功通过传统的测试范式。根据我们的分析，我们提供两种分别基于数据和模型的因果干预解决方案。

    Legal Judgment Prediction (LJP), aiming to predict a judgment based on fact descriptions according to rule of law, serves as legal assistance to mitigate the great work burden of limited legal practitioners. Most existing methods apply various large-scale pre-trained language models (PLMs) finetuned in LJP tasks to obtain consistent improvements. However, we discover the fact that the state-of-the-art (SOTA) model makes judgment predictions according to irrelevant (or non-casual) information. The violation of rule of law not only weakens the robustness and generalization ability of models but also results in severe social problems like discrimination. In this paper, we use causal structural models (SCMs) to theoretically analyze how LJP models learn to make decisions and why they can succeed in passing the traditional testing paradigm without learning causality. According to our analysis, we provide two solutions intervening on data and model by causality, respectively. In detail, we f
    
[^40]: 再回归到循环模型的语言表示

    Circling Back to Recurrent Models of Language. (arXiv:2211.01848v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.01848](http://arxiv.org/abs/2211.01848)

    本文展示了循环模型的优越性并提出了结合更好的循环单元、架构、目标函数以及优化算法的改进方法，在小数据集和Enwik8动态评估中取得了新的最优性能。

    

    仅仅因为一些纯循环模型具有难以优化和在现今硬件上效率低等缺点，并不意味着它们不是好的语言模型。作者们展示了如何通过结合更好的循环单元，架构，目标函数和优化算法来改善这些模型。在此过程中，作者们在小数据集和Enwik8动态评估上建立了新的最优性能。

    Just because some purely recurrent models suffer from being hard to optimize and inefficient on today's hardware, they are not necessarily bad models of language. We demonstrate this by the extent to which these models can still be improved by a combination of a slightly better recurrent cell, architecture, objective, as well as optimization. In the process, we establish a new state of the art for language modelling on small datasets and on Enwik8 with dynamic evaluation.
    
[^41]: 可配置Transformer Transducer语音识别中的可变注意力掩模

    Variable Attention Masking for Configurable Transformer Transducer Speech Recognition. (arXiv:2211.01438v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.01438](http://arxiv.org/abs/2211.01438)

    本文研究了在Transformer Transducer语音识别中使用可变注意力掩模来构建可适应不同场景的模型，实验结果表明分块掩模表现更优，使用可变掩模可以提高准确性。

    

    本论文研究了在基于Transformer Transducer的语音识别中使用注意力掩模以构建适应不同部署场景的单个可配置模型。我们展示了一系列实验，比较了固定掩模(每个帧应用相同的注意力掩模)和分块掩模(每个帧的注意力掩模由块边界确定)在识别准确性和延迟方面的表现。然后，我们探讨了如何在训练时从目标分布中对注意力掩模进行采样以构建能够在不同配置下工作的模型。最后，我们研究了如何使用单个可配置模型来执行第一阶段流识别和第二阶段声学重打分。实验结果表明，与固定掩模相比，分块掩模在准确性与延迟的折衷方面表现更好，无论是否使用FastEmit。我们还表明，可变掩模可使准确性提高最多8％。

    This work studies the use of attention masking in transformer transducer based speech recognition for building a single configurable model for different deployment scenarios. We present a comprehensive set of experiments comparing fixed masking, where the same attention mask is applied at every frame, with chunked masking, where the attention mask for each frame is determined by chunk boundaries, in terms of recognition accuracy and latency. We then explore the use of variable masking, where the attention masks are sampled from a target distribution at training time, to build models that can work in different configurations. Finally, we investigate how a single configurable model can be used to perform both first pass streaming recognition and second pass acoustic rescoring. Experiments show that chunked masking achieves a better accuracy vs latency trade-off compared to fixed masking, both with and without FastEmit. We also show that variable masking improves the accuracy by up to 8% 
    
[^42]: 阶-无序：黑盒神经排名模型的模拟对抗攻击

    Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models. (arXiv:2209.06506v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2209.06506](http://arxiv.org/abs/2209.06506)

    本研究提出了一种模拟黑盒神经排名模型的对抗攻击，可被黑帽SEO用来打败更好防护的搜索引擎。

    

    神经文本排名模型取得了显着进展，并越来越多地被应用于实践中。然而，它们也继承了一般神经模型的对抗性弱点，虽然已被发现，但仍未被先前的研究充分探讨。此外，这种对抗性弱点可能被黑帽SEO用来打败更好防护的搜索引擎。本研究提出了一种模拟黑盒神经段落排名模型的对抗攻击。我们首先展示了目标排名模型可以通过枚举关键查询/候选项透明化并模仿，然后训练一个排名模仿模型。利用排名模仿模型，我们可以精心操纵排名结果并将操纵攻击转移到目标排名模型上。为此，我们提出了一种创新的基于梯度的攻击方法，使用配对目标函数强化，以生成对抗触发器，从而导致PremE.

    Neural text ranking models have witnessed significant advancement and are increasingly being deployed in practice. Unfortunately, they also inherit adversarial vulnerabilities of general neural models, which have been detected but remain underexplored by prior studies. Moreover, the inherit adversarial vulnerabilities might be leveraged by blackhat SEO to defeat better-protected search engines. In this study, we propose an imitation adversarial attack on black-box neural passage ranking models. We first show that the target passage ranking model can be transparentized and imitated by enumerating critical queries/candidates and then train a ranking imitation model. Leveraging the ranking imitation model, we can elaborately manipulate the ranking results and transfer the manipulation attack to the target ranking model. For this purpose, we propose an innovative gradient-based attack method, empowered by the pairwise objective function, to generate adversarial triggers, which causes preme
    
[^43]: 通过社交媒体的词汇分析绘制美国文化地域图

    American cultural regions mapped through the lexical analysis of social media. (arXiv:2208.07649v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.07649](http://arxiv.org/abs/2208.07649)

    本文利用微博数据集自动分析文化地域，创新地提出了一种不需要假设、偏见或成见来识别文化地区的方法。该方法基于人们在社交媒体上的讨论主题来推断文化归属，揭示了美国多个具有明显身份的地区和其他复杂多面的地区。

    

    文化领域代表了一个有用的概念，可以促进社会科学的不同领域的交流。了解人们在社会中如何组织和关联他们的思想和行为，有助于理解他们对不同问题的行动和态度。然而，塑造文化地区的共同特征的选择有些随意。需要的是一种可以利用大量在线数据，特别是通过社交媒体，识别文化地区而没有特定的假设、偏见或成见的方法。本研究通过介绍一种基于自动分析微博数据集的方法来推断文化地区，从而朝着这个方向迈出了重要的一步。本文所提出的方法是基于这样一个原则：文化归属可以通过人们在他们之间讨论的主题来推断出来。具体地，在美国社交媒体上测量了书面话语的地区变化，从标记地点为美国的推文中使用的单词和表达式的频率分布，构建了文化地区轮廓，代表人们共享常见的沟通模式，并可能受到共同文化根源的影响。美国的文化地图显示出几个具有明显身份的地区和其他具有复杂和多方面组成的地区。我们的结果表明，本文提出的方法可以提供一种客观、显著的方法，以识别一个国家内的文化地区。

    Cultural areas represent a useful concept that cross-fertilizes diverse fields in social sciences. Knowledge of how humans organize and relate their ideas and behavior within a society helps to understand their actions and attitudes towards different issues. However, the selection of common traits that shape a cultural area is somewhat arbitrary. What is needed is a method that can leverage the massive amounts of data coming online, especially through social media, to identify cultural regions without ad-hoc assumptions, biases or prejudices. This work takes a crucial step in this direction by introducing a method to infer cultural regions based on the automatic analysis of large datasets from microblogging posts. The approach presented here is based on the principle that cultural affiliation can be inferred from the topics that people discuss among themselves. Specifically, regional variations in written discourse are measured in American social media. From the frequency distributions
    
[^44]: 自然语言处理中对抗防御和鲁棒性的综述

    A Survey of Adversarial Defences and Robustness in NLP. (arXiv:2203.06414v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.06414](http://arxiv.org/abs/2203.06414)

    本文综述了自然语言处理中的对抗防御方法，这些方法不仅可以防御神经网络受到对抗性攻击，而且还可以在训练过程中作为正则化机制，防止模型过度拟合。

    

    在过去的几年中，越来越明显的是，深度神经网络无法抵御输入数据中的对抗扰动，使其容易受到攻击。各种作者已经针对计算机视觉和自然语言处理任务提出了有效的对抗性攻击。作为回应，也提出了许多防御机制，以防止这些网络失败。保护神经网络免受对抗性攻击的重要性在于确保模型的预测即使输入数据发生扰动也能保持不变。已经提出了多种自然语言处理中的对抗防御方法，适用于不同的自然语言处理任务，如文本分类，实体识别和自然语言推理。其中一些方法不仅可以防御神经网络受到对抗性攻击，而且还可以在训练过程中作为正则化机制，防止模型过度拟合。本综述旨在回顾各种在自然语言处理中的对抗防御方法。

    In the past few years, it has become increasingly evident that deep neural networks are not resilient enough to withstand adversarial perturbations in input data, leaving them vulnerable to attack. Various authors have proposed strong adversarial attacks for computer vision and Natural Language Processing (NLP) tasks. As a response, many defense mechanisms have also been proposed to prevent these networks from failing. The significance of defending neural networks against adversarial attacks lies in ensuring that the model's predictions remain unchanged even if the input data is perturbed. Several methods for adversarial defense in NLP have been proposed, catering to different NLP tasks such as text classification, named entity recognition, and natural language inference. Some of these methods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training, saving the model from overfitting. This survey aims to review the various m
    
[^45]: PMC-Patients: 用于评估基于召回的临床决策支持系统的大规模病患数据集

    PMC-Patients: A Large-scale Dataset of Patient Summaries and Relations for Benchmarking Retrieval-based Clinical Decision Support Systems. (arXiv:2202.13876v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.13876](http://arxiv.org/abs/2202.13876)

    本文提出了一个名为“PMC-Patients”的新数据集，用于定义和测试病患到文章的检索（ReCDS-PAR）和病患到病患的检索（ReCDS-PPR），以评估基于召回的临床决策支持系统（ReCDS）的性能。PMC-Patients数据集涵盖逾10,000名病患信息和27,000篇PubMed Central文章，并展示了多种ReCDS系统的效果分析和20个案例的实用性分析。

    

    目标：基于召回的临床决策支持系统（ReCDS）可以通过提供相关文献和类似病患的信息来帮助临床工作流程。然而，由于缺乏多样的病患收集和公开的大规模病患层面注释数据集，ReCDS系统的发展受到了严重阻碍。在本文中，我们旨在使用名为PMC-Patients的新数据集定义和测试两个ReCDS任务：病患到文章的检索（ReCDS-PAR）和病患到病患的检索（ReCDS-PPR）。方法：我们使用简单的启发式方法从PubMed Central文章中提取病患总结，并利用PubMed引文关系图来定义病患-文章相关性和病患-病患相似性。我们还在PMC-Patients基准测试上实施和评估了几种ReCDS系统，包括稀疏检索器、密集检索器和最近邻检索器。我们进行了几个案例研究，以展示PMC-Patients的临床效用。结果：PMC-Patients数据集包含了10,186名病患的信息，涵盖了逾27,000篇PubMed Central文章。我们提供了两个ReCDS任务的基准评估结果和多种系统的效果分析。此外，我们对两个任务的20个案例进行了分析，证明了PMC-Patients的实用性。

    Objective: Retrieval-based Clinical Decision Support (ReCDS) can aid clinical workflow by providing relevant literature and similar patients for a given patient. However, the development of ReCDS systems has been severely obstructed by the lack of diverse patient collections and publicly available large-scale patient-level annotation datasets. In this paper, we aim to define and benchmark two ReCDS tasks: Patient-to-Article Retrieval (ReCDS-PAR) and Patient-to-Patient Retrieval (ReCDS-PPR) using a novel dataset called PMC-Patients.  Methods: We extract patient summaries from PubMed Central articles using simple heuristics and utilize the PubMed citation graph to define patient-article relevance and patient-patient similarity. We also implement and evaluate several ReCDS systems on the PMC-Patients benchmarks, including sparse retrievers, dense retrievers, and nearest neighbor retrievers. We conduct several case studies to show the clinical utility of PMC-Patients.  Results: PMC-Patient
    
[^46]: 低资源情境下的知识抽取：调研与展望

    Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.08063](http://arxiv.org/abs/2202.08063)

    低资源情境下，如何让知识抽取更好地从非结构化文本中提取信息？本文调研了三种解决范式：高资源数据、更强的模型和数据与模型的结合，提出了未来的研究方向。

    

    知识抽取（KE）旨在从非结构化文本中提取结构信息，通常遭受数据匮乏和出现未见类型（低资源情境）的困扰。许多神经网络方法已广泛研究并取得了令人瞩目的表现。本文对低资源情境下KE进行文献综述，并将现有的工作系统性地分为三种范式：（1）利用高资源数据，（2）利用更强的模型，（3）同时利用数据和模型。此外，本文提出有前途的应用，并概述了未来研究的一些潜在方向。我们希望我们的调研可以帮助学术和工业界更好地理解这一领域，激发更多的创意，提升更广泛的应用。

    Knowledge Extraction (KE), aiming to extract structural information from unstructured texts, often suffers from data scarcity and emerging unseen types, i.e., low-resource scenarios. Many neural approaches to low-resource KE have been widely investigated and achieved impressive performance. In this paper, we present a literature review towards KE in low-resource scenarios, and systematically categorize existing works into three paradigms: (1) exploiting higher-resource data, (2) exploiting stronger models, and (3) exploiting data and models together. In addition, we highlight promising applications and outline some potential directions for future research. We hope that our survey can help both the academic and industrial communities to better understand this field, inspire more ideas, and boost broader applications.
    
[^47]: 媒体倾向是具有传染性的。

    Media Slant is Contagious. (arXiv:2202.07269v2 [econ.GN] UPDATED)

    [http://arxiv.org/abs/2202.07269](http://arxiv.org/abs/2202.07269)

    本文研究了国家有线电视新闻对美国本土报纸的影响，发现当地报纸的内容会因为当地 FNC 观众数量的增加而趋向于 FNC 的倾向，并且有线电视倾向会极化地方新闻内容。

    

    本研究考察了媒体倾向的传播，具体来说是国家有线电视新闻对美国本土报纸（2005-2008）的影响。我们使用一种基于 Fox News Channel（FNC）、CNN 和 MSNBC 内容的有线电视倾向文本度量方法，分析地方报纸如何采用 FNC 的倾向而不是 CNN/MSNBC 的倾向。研究结果显示，地方新闻随着当地 FNC 观众人数的外部增长而变得更加类似于 FNC 的内容。这种转变不仅限于从有线电视借鉴，而是地方报纸自身内容的改变。此外，有线电视倾向极化了地方新闻内容。

    We examine the diffusion of media slant, specifically how partisan content from national cable news affects local newspapers in the U.S., 2005-2008. We use a text-based measure of cable news slant trained on content from Fox News Channel (FNC), CNN, and MSNBC to analyze how local newspapers adopt FNC's slant over CNN/MSNBC's. Our findings show that local news becomes more similar to FNC content in response to an exogenous increase in local FNC viewership. This shift is not limited to borrowing from cable news, but rather, local newspapers' own content changes. Further, cable TV slant polarizes local news content.
    

