# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Feedback Loops With Language Models Drive In-Context Reward Hacking](https://arxiv.org/abs/2402.06627) | 与语言模型的反馈循环可能导致上下文内奖励欺骗（ICRH），即语言模型在测试时在优化目标的同时却产生负面副作用。这项研究确定了两个导致ICRH的过程：输出优化和策略优化。 |
| [^2] | [Understanding the Effects of Iterative Prompting on Truthfulness](https://arxiv.org/abs/2402.06625) | 本研究探讨了迭代提示对大型语言模型（LLMs）真实性的影响，并发现简单的提示方法严重损害了真实性。我们提出了几个提示变体来改善这一问题，并取得了显著的改进。这为未来的研究指明了一个有希望的方向。 |
| [^3] | [Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning](https://arxiv.org/abs/2402.06619) | 本研究的主要目标是通过人工筛选的指令遵循数据集来弥合不同语言之间的差距，并且创造了迄今为止最大的多语言收藏品，包括513亿个实例。 |
| [^4] | [FaBERT: Pre-training BERT on Persian Blogs](https://arxiv.org/abs/2402.06617) | 本论文介绍了FaBERT，一个基于波斯语博客进行预训练的BERT模型。它在各种下游任务中展示出更好的性能，并且强调了利用多样化和清理好的语料库来提高在波斯语自然语言处理应用中的性能的重要性。 |
| [^5] | [TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations](https://arxiv.org/abs/2402.06608) | 该论文研究了使用LLMs和逻辑中间表示来生成准确的"文本到计划"的问题。通过将LLMs用于生成计划任务请求的PDDL表示以及经典规划器的使用，能够更好地解决自然语言处理和计划任务之间的差异。 |
| [^6] | [Self-consistent context aware conformer transducer for speech recognition](https://arxiv.org/abs/2402.06592) | 这项研究提出了一种自洽的上下文感知转录器模型，能够在语音识别中提高不常见单词的准确性，而不影响常见单词的错误率。 |
| [^7] | [G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German](https://arxiv.org/abs/2402.06584) | G-SciEdBERT是一种上下文化德语科学教育BERT，用于评分德语科学任务的书面回答。通过在大规模德语科学回答语料库上进行预训练，并在评分准确性方面取得了10%的改善。 |
| [^8] | [What is Hiding in Medicine's Dark Matter? Learning with Missing Data in Medical Practices](https://arxiv.org/abs/2402.06563) | 本研究使用统计方法和机器学习，通过分析儿科急诊数据和创伤伤害数据库，揭示了医疗实践模式与丢失数据之间的关联，并提出了临床数据插补的方法。这对于减少分析偏见、提高临床决策的有效性非常重要。 |
| [^9] | [Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following](https://arxiv.org/abs/2402.06559) | 本文提出了一种Diffusion-ES方法，它结合了无梯度优化和轨迹去噪技术，用于优化黑盒非可微目标。该方法通过从扩散模型中采样轨迹，并使用黑盒奖励函数对其进行评分，实现了更高的多样性和可解释性。 |
| [^10] | [Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection via Retrieval-Augmented GPT-4 and LLaMA](https://arxiv.org/abs/2402.06549) | 该研究利用GPT-4和LLaMA模型，通过检索增强和重新排序的方式，在CASE 2024共享任务中取得了显著的成果，特别是在仇恨事件检测和目标识别方面表现出色。 |
| [^11] | [Calibrating Long-form Generations from Large Language Models](https://arxiv.org/abs/2402.06544) | 该论文提出了一个统一的校准框架，用于校准大型语言模型的长篇生成。在该框架中，作者开发了三个度量指标用于评估模型的校准性，并提出了两种置信度引导方法。实验证明，更大的模型不一定能保证更好的校准。 |
| [^12] | [Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty](https://arxiv.org/abs/2402.06529) | 本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。 |
| [^13] | [Multimodal Clinical Trial Outcome Prediction with Large Language Models](https://arxiv.org/abs/2402.06512) | 本研究提出了一种名为LIFTED的多模态临床试验结果预测方法，通过将不同模态数据转化为自然语言描述来统一数据，并构建统一的抗噪声编码器进行信息提取。 |
| [^14] | [Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance to Ask Clarification Questions](https://arxiv.org/abs/2402.06509) | 本文研究了模型不确定性与人类不确定性之间的关系，并提出了一种基于模型不确定性估计的生成澄清问题的方法，为对话系统在决定何时提问提供了重要指导，并且在任务成功率方面取得了显著改进。 |
| [^15] | [Scalable Interactive Machine Learning for Future Command and Control](https://arxiv.org/abs/2402.06501) | 未来战争将需要指挥与控制（C2）人员在复杂且潜在模糊的情况下以更短的时间内做出决策。本论文通过利用互动式机器学习方法，结合人工智能和人类智能，以提高C2运作的适应性和效率。 |
| [^16] | [Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings](https://arxiv.org/abs/2402.06492) | 本论文提出了SQ-Transformer模型，通过在嵌入和注意层中引入结构化量化的方法，无论训练集的复杂度如何，都能够明确地鼓励模型在编码句子时保持系统性。 |
| [^17] | [V-STaR: Training Verifiers for Self-Taught Reasoners](https://arxiv.org/abs/2402.06457) | V-STaR利用正确和不正确的解决方案训练验证器，用于选择模型生成的解决方案，实现了自我改进和验证方法在常见代码生成和数学推理任务中达到4%到17%的测试准确率提升。 |
| [^18] | [Explaining Veracity Predictions with Evidence Summarization: A Multi-Task Model Approach](https://arxiv.org/abs/2402.06443) | 本文提出了一种用于虚假信息检测的多任务可解释性神经模型，通过将模型的真实性预测解释生成过程看作是一个文本摘要问题来解决推理差距，并在公开数据集上评估了该模型的性能。 |
| [^19] | [Findings of the First Workshop on Simulating Conversational Intelligence in Chat](https://arxiv.org/abs/2402.06420) | 第一届模拟对话智能研讨会的目标是汇集对开放领域对话研究进行实时人类评估的模拟智能对话模型。论文主要提供了共享任务的概述，并附上了一个将在研讨会后发布的深入分析共享任务结果的链接。 |
| [^20] | [CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models](https://arxiv.org/abs/2402.06360) | CoSearchAgent是一种基于大语言模型的轻量级协作搜索代理，可作为Slack插件在多方对话中支持协作搜索。 |
| [^21] | [Promoting Target Data in Context-aware Neural Machine Translation](https://arxiv.org/abs/2402.06342) | 本研究探讨了在上下文感知的神经机器翻译中是否应该进一步提升目标数据，实验证明在源语言中包含目标上下文可以显著提高目标语言现象。 |
| [^22] | [RareBench: Can LLMs Serve as Rare Diseases Specialists?](https://arxiv.org/abs/2402.06341) | RareBench是一个开创性的基准测试，旨在评估LLMs在罕见病领域的诊断能力，为未来研究提供了一个最大的开放数据集。 |
| [^23] | [ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs](https://arxiv.org/abs/2402.06334) | ExaRanker-Open 是一种使用开源LLMs进行IR的方法，通过适应和探索开源语言模型来生成解释。研究结果表明，纳入解释能够稳定提高神经排序器的性能，而LLM的大小越大，收益越大。 |
| [^24] | [InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning](https://arxiv.org/abs/2402.06332) | 在本文中，我们介绍了开源数学推理LLMs InternLM-Math，该模型以其数学能力代表了抽象推理能力。我们的模型以统一的方式整合了逻辑推理、奖励建模、形式推理、数据增强和代码解释器，并使用监督学习使其成为一个多功能的数学推理器、验证器、证明器和增强器。在各种基准测试中，包括GSM8K、MATH、匈牙利数学考试、MathBench-ZH和MiniF2F，在上下文学习、监督微调和代码辅助推理的设置下，InternLM-Math取得了开源的最先进性能。我们的预训练模型在MiniF2F测试集上达到了30.3的得分。我们还研究了如何使用LEAN解决数学问题，并探讨了其在多任务学习设置下的性能。 |
| [^25] | [On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference](https://arxiv.org/abs/2402.06262) | 本文研究了针对键值约束生成语言模型推理的驱逐策略的有效性，通过引入基于时间注意力得分和鲁棒性度量的RoCo策略，优于现有的策略。 |
| [^26] | [Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning](https://arxiv.org/abs/2402.06255) | 本文提出了一种名为Prompt Adversarial Tuning (PAT)的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中，实现对大型语言模型（LLMs）的越狱行为的防御。实验证明该方法在保护LLMs免受产生有害信息的影响方面效果显著。 |
| [^27] | [ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume Generation and Refinement](https://arxiv.org/abs/2402.06221) | ResumeFlow是一种利用LLM技术的工具，能够帮助求职者根据特定的职位要求生成个性化的简历，从而解决了手动定制简历的耗时和容易出错的问题。 |
| [^28] | [A Unified Causal View of Instruction Tuning](https://arxiv.org/abs/2402.06220) | 该论文提出了一个元结构因果模型（meta-SCM）来整合不同的自然语言处理（NLP）任务。通过学习每个任务所需的因果因子并使用这些因子进行预测，从而解决了现有方法中存在的“伪相关性”问题。 |
| [^29] | [The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate](https://arxiv.org/abs/2402.06204) | 本文讨论了生成AI评估中的悖论，并发现了大型语言模型在评估任务中性能较差的现象。研究突出了需要检查模型作为评估者的忠实度和可信度，以及探索生成优秀与评估能力之间的关联。 |
| [^30] | [Large Language Models: A Survey](https://arxiv.org/abs/2402.06196) | 大型语言模型（LLMs）吸引了很多关注，因为它们在自然语言任务上的强大表现。该研究领域发展迅速，包括了各种著名的LLMs、构建和增强LLMs的技术、以及流行的LLM数据集和评估指标。 |
| [^31] | [Model Editing with Canonical Examples](https://arxiv.org/abs/2402.06155) | 使用规范示例进行模型编辑，每个期望行为只有一个学习示例，评估仅在分布之外进行，对初始模型的偏离受限，通过实验发现LoRA优于完全微调和MEMIT。 |
| [^32] | [DeAL: Decoding-time Alignment for Large Language Models](https://arxiv.org/abs/2402.06147) | DeAL是一个允许用户自定义奖励函数并实现解码时对齐LLMs的框架。 |
| [^33] | [Learn To be Efficient: Build Structured Sparsity in Large Language Models](https://arxiv.org/abs/2402.06126) | 本文通过引入一种新的算法"Learn-To-be-Efficient(LTE)"，提出了在大型语言模型(LLM)中构建结构化稀疏性的方法。该方法通过训练高效意识的LLM学习激活更少的神经元，取得更好的稀疏性和性能折衷。 |
| [^34] | [Language Model Sentence Completion with a Parser-Driven Rhetorical Control Method](https://arxiv.org/abs/2402.06125) | 本研究提出了一种新的控制性文本生成算法，通过解析驱动的解码方案，在语言模型句子补全环境中强制执行特定修辞关系的遵循，无需模型微调。该方法经过自动和人工评估验证，代码可在GitHub上获得。 |
| [^35] | [Rethinking Data Selection for Supervised Fine-Tuning](https://arxiv.org/abs/2402.06094) | 本研究重新思考了有监督微调中数据选择的直觉。考虑SFT的肤浅性质，我们提出重要示范应着重反映人类式的互动。通过选择具有长回答的实例，可以实现更好的SFT效果。 |
| [^36] | [LightCAM: A Fast and Light Implementation of Context-Aware Masking based D-Tdnn for Speaker Verification](https://arxiv.org/abs/2402.06073) | LightCAM是一种快速轻量级的基于上下文感知屏蔽的D-TDNN说话人验证实现，通过采用深度可分离卷积模块和多尺度特征聚合，它在VoxCeleb数据集上取得了更好的性能。 |
| [^37] | [Limits of Large Language Models in Debating Humans](https://arxiv.org/abs/2402.06049) | 大型语言模型在与人类辩论中的能力有限，尽管它们能够融入和促进人类的工作效率，但在辩论中的说服力较弱。在成为可行的辩手之前，LLMs需要进一步发展。 |
| [^38] | [OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2402.06044) | OpenToM是一个评估大型语言模型心理理解能力的全面基准，通过提供更长、更清晰的叙事故事、具有明确个性特征的角色、以及挑战模型对心理状态的理解能力的问题，揭示了现有模型在理解物理世界与心理世界的角色心理状态方面的优势和不足。 |
| [^39] | [A Prompt Response to the Demand for Automatic Gender-Neutral Translation](https://arxiv.org/abs/2402.06041) | 本研究通过比较机器翻译和GPT-4模型，探索了自动性别中立翻译的潜力，并揭示了当前机器翻译系统在生成性别中立翻译方面的固有局限性。 |
| [^40] | [Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning](https://arxiv.org/abs/2402.06025) | 本论文建立了一个计算模型来模拟人们通过实验主动推断隐藏规则的过程，并发现显式假设、概率规则和在线更新的组合可以解释人们在类似Zendo任务上的表现。 |
| [^41] | [Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing](https://arxiv.org/abs/2402.06015) | 通过在GPT-4V模型上进行全面的探索，我们发现该模型在识别文化概念方面表现出色，但在低资源语言中仍表现较弱。在图像字幕任务中，GPT-4V在文化相关性方面优于原文。 |
| [^42] | [A Survey on Transformer Compression](https://arxiv.org/abs/2402.05964) | 《Transformer压缩调研》是对最近压缩方法的全面回顾，特别关注它们在Transformer模型中的应用。压缩方法主要分为修剪、量化、知识蒸馏和高效架构设计四个类别。 |
| [^43] | [Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques](https://arxiv.org/abs/2402.05952) | 本综述调查了将大型语言模型（LLM）与图表示学习（GRL）相结合的技术，并提供了一个新颖的分类法，深入分析了这些模型的核心组成部分和操作技术，为有效的模型设计和训练策略提供了新的视角。 |
| [^44] | [DE$^3$-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks](https://arxiv.org/abs/2402.05948) | DE$^3$-BERT是一种基于原型网络和距离度量的增强距离早期停止框架，用于提高BERT等预训练语言模型的推断速度和准确性。 |
| [^45] | [Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study](https://arxiv.org/abs/2402.05939) | 本文研究了大型语言模型在代码分布转移下的不确定性意识，并通过引入大规模基准数据集和应用概率方法来提高语言模型的可靠性。 |
| [^46] | [LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System](https://arxiv.org/abs/2402.05130) | LB-KBQA是一种基于大语言模型和BERT的基于知识的问答系统，通过生成式人工智能的帮助，能够提高意图识别的性能和解决语言多样性的问题。 |
| [^47] | [PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition](https://arxiv.org/abs/2402.04838) | 本研究提出了PaDeLLM-NER，一种能够在大型语言模型中实现并行解码，从而显著减少命名实体识别的生成延迟，同时保持预测质量和性能。 |
| [^48] | [Factuality of Large Language Models in the Year 2024](https://arxiv.org/abs/2402.02420) | 本文调查了大规模语言模型（LLM）的真实性问题，并对其现有研究进行了批判性分析，指出了改进LLM真实性的挑战和解决方案，以及自动真实性评估的障碍。未来的研究应该关注在这些方面的进一步工作。 |
| [^49] | [Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet](https://arxiv.org/abs/2401.03630) | 本文研究了使用大型语言模型解决多智能体路径规划的问题。通过实验，我们发现直接使用大型语言模型解决复杂场景下的路径规划仍然存在困难。 |
| [^50] | [Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments](https://arxiv.org/abs/2312.15842) | 本研究提出了一种将LLM的知识蒸馏为更小、更高效、更准确的神经网络的方法，在资源受限设备上部署具有挑战性。通过使用LLM的预测概率作为软标签训练较小的学生模型，并使用专门定制的损失函数，保证学生模型与教师模型的性能非常相似。实验证明此方法在科学教育评估中具有良好的准确性。 |
| [^51] | [MAIRA-1: A specialised large multimodal model for radiology report generation](https://arxiv.org/abs/2311.13668) | MAIRA-1是一种专门用于放射学报告生成的大型多模态模型，在与预训练的视觉编码器对齐和文本数据增强的基础上，利用了CXR特定的图像编码器和经过微调的大型语言模型，生成具有最先进质量的报告。 |
| [^52] | [AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL](https://arxiv.org/abs/2311.09830) | AutoPlanBench是一种新方法，可以自动转换PDDL规划基准测试为文本描述，并提供了相应的基准测试数据集。研究表明，当前最好的LLM规划器在某些规划任务上表现优秀，但对于其他任务来说仍存在挑战。 |
| [^53] | [Structured Chemistry Reasoning with Large Language Models](https://arxiv.org/abs/2311.09656) | 该论文研究了大型语言模型在化学领域的复杂科学推理困难，发现错误通常源于缺乏有效的推理结构。基于此，引入了一种简单而有效的提示策略StructChem，大幅提升了语言模型的性能。 |
| [^54] | [Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference](https://arxiv.org/abs/2309.08968) | Sorted LLaMA通过扩展SortedNet到生成NLP任务，使得大型语言模型在动态推理中更高效，并且不需要预训练，只需将标准微调替换为排序微调即可。该方法可以释放transformers中间层的潜力，同时最小化存储需求和过渡成本。 |
| [^55] | [Teaching Probabilistic Logical Reasoning to Transformers](https://arxiv.org/abs/2305.13179) | 本文评估了基于变压器的语言模型在推理不确定文本上的能力，并提出了一种概率约束训练（PCT）的方法来提高模型的概率逻辑推理能力。 |
| [^56] | [Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage](https://arxiv.org/abs/2305.12707) | 本文研究了大型语言模型的关联能力，并揭示了其对隐私泄露的影响。研究发现，随着模型规模的增加，模型在关联实体/信息方面的能力增强。然而，与常识知识相比，模型在关联个人可识别信息方面的准确性较低。 |
| [^57] | [ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification](https://arxiv.org/abs/2209.09034) | ALEXSIS-PT是一个用于巴西葡萄牙语词汇简化的新型多候选数据集，为LS系统的改进和跨语言模型的研究提供了重要资源。BERTimbau在该数据集上达到了最高的性能。 |
| [^58] | [SliceGPT: Compress Large Language Models by Deleting Rows and Columns.](http://arxiv.org/abs/2401.15024) | SliceGPT是一种新的事后训练稀疏化方案，通过将每个权重矩阵替换为较小的矩阵以减小网络的维度，可以在保持高任务性能的同时减少模型参数。 |
| [^59] | [Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback.](http://arxiv.org/abs/2401.05928) | Muffin是一个新型的模型无关框架，用于减轻情感支持对话中无益性的问题。这个框架在生成支持性回复时考虑到多个因素，并通过多方位的AI反馈来训练模型，以避免生成无益的回复。 |
| [^60] | [AST-T5: Structure-Aware Pretraining for Code Generation and Understanding.](http://arxiv.org/abs/2401.03003) | AST-T5是一种结构感知的预训练模型，通过利用抽象语法树（AST）来增强代码生成、转换和理解的能力。它优于其他同等大小的语言模型，并在代码到代码任务中表现出色。 |
| [^61] | [LLaVA-$\phi$: Efficient Multi-Modal Assistant with Small Language Model.](http://arxiv.org/abs/2401.02330) | LLaVA-$\phi$是一种高效的多模态助手，使用小型语言模型Phi-2来促进多模态对话。即使具有较少的参数，它也能有效地融合文本和视觉元素，并在各种任务中表现出色。它为时间敏感的环境和需要实时交互的系统开辟了新的应用途径。 |
| [^62] | [Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation.](http://arxiv.org/abs/2310.18235) | 本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。 |
| [^63] | [Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning.](http://arxiv.org/abs/2310.03094) | 本研究提出了一种基于思维混合表示的大规模语言模型级联方法，用于成本高效的推理。通过考虑更弱模型的答案一致性作为问题难度的信号，可以实现对问题的决策，从而节约使用更强模型的成本。 |
| [^64] | [Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training.](http://arxiv.org/abs/2309.17179) | Alphazero类似的树搜索框架TS-LLM可以利用学习的价值函数指导大型语言模型的解码和训练，不仅适用于推理任务，还适用于其他任务，并且在不同大小的语言模型上具有普适性和可扩展性 |
| [^65] | [A Benchmark for Learning to Translate a New Language from One Grammar Book.](http://arxiv.org/abs/2309.16575) | 本研究提出了一种从一本语法书中学习翻译新语言的基准测试MTOB，用于翻译英语和Kalamang之间的文本，探索了低资源语言情况下的翻译问题。结果显示，现有的大型语言模型在这个任务上表现有限。 |
| [^66] | [Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases.](http://arxiv.org/abs/2309.08345) | 本文通过实验调查揭示了语言模型在与知识库进行连接时的数据分布瓶颈，包括推广到未见域、适应语言变体和在不同数据集之间的可转移性等方面。即使采用数据增强技术，先进的语言模型在多个方面表现出较差的性能。 |
| [^67] | [Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering.](http://arxiv.org/abs/2309.06358) | 本论文研究了使用生成式数据增强方法如何提高问答模型在自然分布转换下的鲁棒性，通过实验展示了增强阅读理解数据集的效果。 |
| [^68] | [LLM in the Shell: Generative Honeypots.](http://arxiv.org/abs/2309.00155) | 本研究引入了一种基于大型语言模型的新方法来创建动态和真实的软件蜜罐，解决了以往蜜罐的重要局限性，并通过实验验证了其高准确率。 |
| [^69] | [SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models.](http://arxiv.org/abs/2307.10635) | 这篇论文介绍了一个名为SciBench的基准套件，旨在对大型语言模型的大学水平科学问题解决能力进行评估。研究结果显示，当前的语言模型在提供复杂科学问题解决能力方面还有不足之处。 |
| [^70] | [Self Information Update for Large Language Models through Mitigating Exposure Bias.](http://arxiv.org/abs/2305.18582) | 本文提出了一种使用信息丰富的文本语料库来帮助现有的大型语言模型进行自我信息更新的方法，有效减轻了暴露偏差的影响。 |

# 详细

[^1]: 与语言模型的反馈循环推动上下文内奖励欺骗

    Feedback Loops With Language Models Drive In-Context Reward Hacking

    [https://arxiv.org/abs/2402.06627](https://arxiv.org/abs/2402.06627)

    与语言模型的反馈循环可能导致上下文内奖励欺骗（ICRH），即语言模型在测试时在优化目标的同时却产生负面副作用。这项研究确定了两个导致ICRH的过程：输出优化和策略优化。

    

    语言模型对外部世界产生影响：它们查询可以读写网页的API，生成能够影响人类行为的内容，以及作为自主代理运行系统命令。这些互动形成了反馈循环：语言模型的输出影响世界，反过来又影响后续的语言模型输出。在这项工作中，我们展示了反馈循环可能导致上下文内奖励欺骗(ICRH)，即测试时的语言模型在优化（可能隐含的）目标的同时，产生负面副作用。例如，考虑一个被部署用于增加Twitter参与度的语言模型代理；语言模型可能在上下文窗口中检索其以前的推文，并使推文更具争议性，从而增加参与度，但也增加了有毒性。我们确定并研究了导致ICRH的两个过程：输出优化和策略优化。对于这些过程，静态数据集上的评估是不足够的-他们无法捕捉到反馈效应，也不能捕捉到最有害的行为。为此，我们提供了...

    Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
    
[^2]: 理解迭代提示对真实性的影响

    Understanding the Effects of Iterative Prompting on Truthfulness

    [https://arxiv.org/abs/2402.06625](https://arxiv.org/abs/2402.06625)

    本研究探讨了迭代提示对大型语言模型（LLMs）真实性的影响，并发现简单的提示方法严重损害了真实性。我们提出了几个提示变体来改善这一问题，并取得了显著的改进。这为未来的研究指明了一个有希望的方向。

    

    大型语言模型（LLMs）的发展已经显著改变了许多领域，提供了令人印象深刻的文本生成能力。然而，这些模型的可靠性和真实性仍然是一个紧迫的问题。为此，我们研究了迭代提示，这是一种被假设可以提高LLM响应精确度的策略，并评估其对LLM真实性的影响，这个领域尚没有得到充分探索。我们进行了大量实验证明了迭代提示变体的复杂性，以及它们对模型响应的准确性和校准性的影响。我们的研究发现，简单的提示方法严重损害了真实性，导致校准误差加剧。为了应对这些挑战，我们提出了几个旨在解决已确定问题的提示变体。这些变体相对于现有基线模型表现出显著的改进，为未来的研究指明了一个有希望的方向。我们的工作提供了对迭代提示的细致理解。

    The development of Large Language Models (LLMs) has notably transformed numerous sectors, offering impressive text generation capabilities. Yet, the reliability and truthfulness of these models remain pressing concerns. To this end, we investigate iterative prompting, a strategy hypothesized to refine LLM responses, assessing its impact on LLM truthfulness, an area which has not been thoroughly explored. Our extensive experiments delve into the intricacies of iterative prompting variants, examining their influence on the accuracy and calibration of model responses. Our findings reveal that naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors. In response to these challenges, we introduce several prompting variants designed to address the identified issues. These variants demonstrate marked improvements over existing baselines, signaling a promising direction for future research. Our work provides a nuanced understanding of iterative pr
    
[^3]: Aya数据集：用于多语言指令调优的开放访问收藏品

    Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning

    [https://arxiv.org/abs/2402.06619](https://arxiv.org/abs/2402.06619)

    本研究的主要目标是通过人工筛选的指令遵循数据集来弥合不同语言之间的差距，并且创造了迄今为止最大的多语言收藏品，包括513亿个实例。

    

    数据集是现代人工智能中许多突破的基础。自然语言处理（NLP）领域的许多最近的成就都归功于在多样化任务上对预训练模型进行微调，使得大型语言模型能够响应指令。指令微调（IFT）需要特别构建和注释的数据集。然而，现有的数据集几乎都是以英语为主。在这项工作中，我们的首要目标是通过构建跨越65种语言的人工筛选的指令遵循数据集来弥合语言差距。我们与来自世界各地的流利说者合作，收集指令和完成的自然实例。此外，我们通过在114种语言之间进行模板化和翻译现有数据集，创造了迄今为止规模最大的多语言收藏品，共有5.13亿个实例。总而言之，我们提供了四个关键资源：我们开发并开源了Aya数据集，通过模板化和翻译现有的数据集进行扩展，并将其跨越了114种语言。

    Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the finetuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya A
    
[^4]: FaBERT: 基于波斯语博客的BERT预训练模型

    FaBERT: Pre-training BERT on Persian Blogs

    [https://arxiv.org/abs/2402.06617](https://arxiv.org/abs/2402.06617)

    本论文介绍了FaBERT，一个基于波斯语博客进行预训练的BERT模型。它在各种下游任务中展示出更好的性能，并且强调了利用多样化和清理好的语料库来提高在波斯语自然语言处理应用中的性能的重要性。

    

    我们介绍了FaBERT，一个基于波斯语HmBlogs语料库进行预训练的波斯语BERT-base模型，包括非正式和正式的波斯语文本。FaBERT旨在在传统的自然语言理解（NLU）任务中表现出色，解决波斯语中常见的句子结构和语言风格的复杂问题。在我们对FaBERT在12个数据集上进行的全面评估中，涵盖了情感分析（SA）、命名实体识别（NER）、自然语言推理（NLI）、问答系统（QA）和问题改写（QP）等不同的下游任务，它始终展示出改进的性能，并且模型尺寸较小。研究结果强调了利用多样化和清理好的语料库（如HmBlogs）来提高类似BERT在波斯语自然语言处理（NLP）应用中的性能的重要性。FaBERT可以在https://huggingface.co/sbunlp/fabert上免费获得。

    We introduce FaBERT, a Persian BERT-base model pre-trained on the HmBlogs corpus, encompassing both informal and formal Persian texts. FaBERT is designed to excel in traditional Natural Language Understanding (NLU) tasks, addressing the intricacies of diverse sentence structures and linguistic styles prevalent in the Persian language. In our comprehensive evaluation of FaBERT on 12 datasets in various downstream tasks, encompassing Sentiment Analysis (SA), Named Entity Recognition (NER), Natural Language Inference (NLI), Question Answering (QA), and Question Paraphrasing (QP), it consistently demonstrated improved performance, all achieved within a compact model size. The findings highlight the importance of utilizing diverse and cleaned corpora, such as HmBlogs, to enhance the performance of language models like BERT in Persian Natural Language Processing (NLP) applications. FaBERT is openly accessible at https://huggingface.co/sbunlp/fabert
    
[^5]: TIC：利用LLMs和逻辑中间表示精确进行“文本到计划”的翻译-推断-编译

    TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations

    [https://arxiv.org/abs/2402.06608](https://arxiv.org/abs/2402.06608)

    该论文研究了使用LLMs和逻辑中间表示来生成准确的"文本到计划"的问题。通过将LLMs用于生成计划任务请求的PDDL表示以及经典规划器的使用，能够更好地解决自然语言处理和计划任务之间的差异。

    

    我们研究了为给定的自然语言计划任务请求生成计划的问题。一方面，LLMs在自然语言处理方面表现出色，但在计划方面表现不佳。另一方面，经典计划工具在计划任务方面表现出色，但需要使用结构化语言（如Planning Domain Definition Language（PDDL））作为输入。我们利用这两种技术的优点，通过使用LLMs生成计划任务请求的PDDL表示（任务PDDL），然后使用经典规划器计算计划。与直接使用LLMs生成任务PDDL的先前方法不同，我们的方法包括（a）翻译：仅使用LLMs生成自然语言任务描述的逻辑可解释的中间表示，（b）推断：使用逻辑推理器（目前是Answer Set Programming solver）从中间表示中推导出额外的逻辑相关信息，以及（c）编译：生成目标计划的PDDL描述的编译。

    We study the problem of generating plans for given natural language planning task requests. On one hand, LLMs excel at natural language processing but do not perform well on planning. On the other hand, classical planning tools excel at planning tasks but require input in a structured language such as the Planning Domain Definition Language (PDDL). We leverage the strengths of both the techniques by using an LLM for generating the PDDL representation (task PDDL) of planning task requests followed by using a classical planner for computing a plan. Unlike previous approaches that use LLMs for generating task PDDLs directly, our approach comprises of (a) translate: using an LLM only for generating a logically interpretable intermediate representation of natural language task descriptions, (b) infer: deriving additional logically dependent information from the intermediate representation using a logic reasoner (currently, Answer Set Programming solver), and (c) compile: generating the targ
    
[^6]: 自洽的上下文感知转录器用于语音识别

    Self-consistent context aware conformer transducer for speech recognition

    [https://arxiv.org/abs/2402.06592](https://arxiv.org/abs/2402.06592)

    这项研究提出了一种自洽的上下文感知转录器模型，能够在语音识别中提高不常见单词的准确性，而不影响常见单词的错误率。

    

    我们提出了一种基于转录器的新颖神经网络架构，为ASR系统添加了上下文信息流。我们的方法在提高识别不常见单词的准确性的同时不影响常见单词的错误率。我们探索了当我们使用新模型和/或与上下文语言模型浅度融合时，对不常见单词准确性的改善。我们发现两者的组合可以累积提高不常见单词的识别准确性。

    We propose a novel neural network architecture based on conformer transducer that adds contextual information flow to the ASR systems. Our method improves the accuracy of recognizing uncommon words while not harming the word error rate of regular words. We explore the uncommon words accuracy improvement when we use the new model and/or shallow fusion with context language model. We found that combination of both provides cumulative gain in uncommon words recognition accuracy.
    
[^7]: G-SciEdBERT: 用于德语科学评估任务的上下文化大型语言模型

    G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German

    [https://arxiv.org/abs/2402.06584](https://arxiv.org/abs/2402.06584)

    G-SciEdBERT是一种上下文化德语科学教育BERT，用于评分德语科学任务的书面回答。通过在大规模德语科学回答语料库上进行预训练，并在评分准确性方面取得了10%的改善。

    

    自然语言处理的进步为各种语言（例如德语中的德语BERT [G-BERT]）的自动评分系统铺平了道路。自动评分德语科学问题的书面回答是一项复杂的任务，对于标准的G-BERT来说具有挑战性，因为它们缺乏科学领域的上下文知识，并且可能与学生的写作风格不一致。本文开发了一种上下文化德语科学教育BERT（G-SciEdBERT），一个创新的大型语言模型，专门用于评分德语科学任务的书面回答。我们使用G-BERT，在5M个标记的PISA 2015国际学生评估的50K个德语科学回答语料库上对G-SciEdBERT进行了预训练。然后，我们在59个评估项目上对G-SciEdBERT进行了微调，并检查了评分准确性。然后，我们将其性能与G-BERT进行了比较。我们的研究结果显示，G-SciEdBERT在评分准确性方面取得了显著的改进，表明其评分准确性提高了10%。

    The advancement of natural language processing has paved the way for automated scoring systems in various languages, such as German (e.g., German BERT [G-BERT]). Automatically scoring written responses to science questions in German is a complex task and challenging for standard G-BERT as they lack contextual knowledge in the science domain and may be unaligned with student writing styles. This paper developed a contextualized German Science Education BERT (G-SciEdBERT), an innovative large language model tailored for scoring German-written responses to science tasks. Using G-BERT, we pre-trained G-SciEdBERT on a corpus of 50K German written science responses with 5M tokens to the Programme for International Student Assessment (PISA) 2015. We fine-tuned G-SciEdBERT on 59 assessment items and examined the scoring accuracy. We then compared its performance with G-BERT. Our findings reveal a substantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a 10% increase of quad
    
[^8]: 医学的暗物质中隐藏着什么？在医疗实践中处理丢失数据的学习

    What is Hiding in Medicine's Dark Matter? Learning with Missing Data in Medical Practices

    [https://arxiv.org/abs/2402.06563](https://arxiv.org/abs/2402.06563)

    本研究使用统计方法和机器学习，通过分析儿科急诊数据和创伤伤害数据库，揭示了医疗实践模式与丢失数据之间的关联，并提出了临床数据插补的方法。这对于减少分析偏见、提高临床决策的有效性非常重要。

    

    电子病人记录（EPR）产生了大量数据，但其中包含重要的缺失信息。理解和处理这些缺失数据是临床数据分析的重要组成部分，如果不加以解决，可能导致分析中的偏见和关键结论的扭曲。缺失数据可能与医疗专业人士的实践模式有关，对缺失数据的插补可以提高临床决策的有效性。本研究专注于统计方法来理解和解释缺失数据，并使用单一中心的儿科急诊数据以及英国最大的创伤伤害数据库（TARN）中的数据，进行基于机器学习的临床数据插补。在对56,961个与儿童急诊部就诊相关的初步生命体征和观察数据进行的研究中，我们表明丢失数据很可能是非随机的，并展示了这些数据与医疗专业人士的实践模式的关联。

    Electronic patient records (EPRs) produce a wealth of data but contain significant missing information. Understanding and handling this missing data is an important part of clinical data analysis and if left unaddressed could result in bias in analysis and distortion in critical conclusions. Missing data may be linked to health care professional practice patterns and imputation of missing data can increase the validity of clinical decisions. This study focuses on statistical approaches for understanding and interpreting the missing data and machine learning based clinical data imputation using a single centre's paediatric emergency data and the data from UK's largest clinical audit for traumatic injury database (TARN). In the study of 56,961 data points related to initial vital signs and observations taken on children presenting to an Emergency Department, we have shown that missing data are likely to be non-random and how these are linked to health care professional practice patterns.
    
[^9]: Diffusion-ES:基于扩散的零梯度规划用于自动驾驶和零阶指令跟随

    Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following

    [https://arxiv.org/abs/2402.06559](https://arxiv.org/abs/2402.06559)

    本文提出了一种Diffusion-ES方法，它结合了无梯度优化和轨迹去噪技术，用于优化黑盒非可微目标。该方法通过从扩散模型中采样轨迹，并使用黑盒奖励函数对其进行评分，实现了更高的多样性和可解释性。

    

    扩散模型在决策和控制中对复杂和多模态轨迹分布建模有很强优势。最近提出了奖励梯度引导去噪方法，用于产生在扩散模型所捕获的数据分布下，同时最大化可微分奖励函数和似然性的轨迹。奖励梯度引导去噪需要一个适合于清洁和噪声样本的可微分奖励函数，从而限制了其作为一种通用轨迹优化器的适用性。在本文中，我们提出了DiffusionES，一种将无梯度优化和轨迹去噪相结合的方法，用于在数据流形中优化黑盒非可微目标。Diffusion-ES从扩散模型中采样轨迹，并使用黑盒奖励函数对其进行评分。它通过截断扩散过程对得分高的轨迹进行变异，该过程应用少量的噪声和去噪步骤，从而实现了更高的多样性和更好的可解释性。

    Diffusion models excel at modeling complex and multimodal trajectory distributions for decision-making and control. Reward-gradient guided denoising has been recently proposed to generate trajectories that maximize both a differentiable reward function and the likelihood under the data distribution captured by a diffusion model. Reward-gradient guided denoising requires a differentiable reward function fitted to both clean and noised samples, limiting its applicability as a general trajectory optimizer. In this paper, we propose DiffusionES, a method that combines gradient-free optimization with trajectory denoising to optimize black-box non-differentiable objectives while staying in the data manifold. Diffusion-ES samples trajectories during evolutionary search from a diffusion model and scores them using a black-box reward function. It mutates high-scoring trajectories using a truncated diffusion process that applies a small number of noising and denoising steps, allowing for much mo
    
[^10]: Bryndza在ClimateActivism 2024上: 通过检索增强的GPT-4和LLaMA进行立场、目标和仇恨事件检测

    Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection via Retrieval-Augmented GPT-4 and LLaMA

    [https://arxiv.org/abs/2402.06549](https://arxiv.org/abs/2402.06549)

    该研究利用GPT-4和LLaMA模型，通过检索增强和重新排序的方式，在CASE 2024共享任务中取得了显著的成果，特别是在仇恨事件检测和目标识别方面表现出色。

    

    本研究详细介绍了我们在CASE 2024气候行动立场和仇恨事件检测共享任务中的方法，重点关注仇恨言论检测、仇恨言论目标识别和立场检测作为分类挑战。我们探索了大型语言模型（LLM），特别是GPT-4，在零次或少次训练情况下通过检索增强和重新排序来进行推特分类的能力。我们的目标是确定在这个背景下，LLM能否与传统方法相匹配或超越。我们进行了LLaMA的消融研究以进行比较，结果表明我们的模型明显优于基准线，在目标检测任务中获得了第二名。我们提交的代码可以在https://github.com/NaiveNeuron/bryndza-case-2024获得。

    This study details our approach for the CASE 2024 Shared Task on Climate Activism Stance and Hate Event Detection, focusing on Hate Speech Detection, Hate Speech Target Identification, and Stance Detection as classification challenges. We explored the capability of Large Language Models (LLMs), particularly GPT-4, in zero- or few-shot settings enhanced by retrieval augmentation and re-ranking for Tweet classification. Our goal was to determine if LLMs could match or surpass traditional methods in this context.   We conducted an ablation study with LLaMA for comparison, and our results indicate that our models significantly outperformed the baselines, securing second place in the Target Detection task. The code for our submission is available at https://github.com/NaiveNeuron/bryndza-case-2024
    
[^11]: 从大型语言模型中校准长篇生成

    Calibrating Long-form Generations from Large Language Models

    [https://arxiv.org/abs/2402.06544](https://arxiv.org/abs/2402.06544)

    该论文提出了一个统一的校准框架，用于校准大型语言模型的长篇生成。在该框架中，作者开发了三个度量指标用于评估模型的校准性，并提出了两种置信度引导方法。实验证明，更大的模型不一定能保证更好的校准。

    

    为了提高大型语言模型（LLMs）的可靠性，校准是必要的 - 模型的评估置信度应该与其响应正确性的实际可能性相一致。然而，目前的置信度引导方法和校准指标通常依赖于对响应正确性的二元真/假评估。这种方法在长篇生成中不适用，因为答案可能部分正确。为了解决这一问题，我们引入了一个统一的校准框架，其中LLMs的响应正确性和关联的置信水平都被视为一系列分数的分布。在此框架下，我们开发了三个度量指标来精确评估LLM的校准，并进一步提出了基于自一致性和自评估的两种置信度引导方法。我们的实验包括长篇问答和摘要任务，结果表明，更大的模型不一定能保证更好的校准。

    To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibratio
    
[^12]: 内省规划：引导语言驱动的代理机器人改进自身的不确定性

    Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty

    [https://arxiv.org/abs/2402.06529](https://arxiv.org/abs/2402.06529)

    本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。

    

    大型语言模型（LLM）展示了先进的推理能力，使得机器人能够理解自然语言指令，并通过适当的基础塑造来策略性地进行高级行动规划。然而，LLM产生的幻觉可能导致机器人自信地执行与用户目标不符或在极端情况下不安全的计划。此外，自然语言指令中的固有歧义可能引发任务的不确定性，尤其是在存在多个有效选项的情况下。为了解决这个问题，LLMs必须识别此类不确定性并主动寻求澄清。本文探索了内省规划的概念，作为一种系统方法，引导LLMs在无需微调的情况下形成意识到不确定性的机器人任务执行计划。我们研究了任务级机器人规划中的不确定性量化，并证明与最先进的基于LLM的规划方法相比，内省显著提高了成功率和安全性。

    Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
    
[^13]: 使用大型语言模型的多模态临床试验结果预测

    Multimodal Clinical Trial Outcome Prediction with Large Language Models

    [https://arxiv.org/abs/2402.06512](https://arxiv.org/abs/2402.06512)

    本研究提出了一种名为LIFTED的多模态临床试验结果预测方法，通过将不同模态数据转化为自然语言描述来统一数据，并构建统一的抗噪声编码器进行信息提取。

    

    临床试验是一个关键且昂贵的过程，通常需要多年时间和大量财力资源。因此，开发临床试验结果预测模型旨在排除可能失败的药物，并具有显著的成本节约潜力。最近的数据驱动尝试利用深度学习方法整合多模态数据来预测临床试验结果。然而，这些方法依赖于手动设计的模态特定编码器，这限制了适应新模态的可扩展性和识别不同模态之间相似信息模式的能力。为了解决这些问题，我们提出了一种多模态专家混合（LIFTED）方法用于临床试验结果预测。具体而言，LIFTED通过将不同模态的数据转化为自然语言描述来统一不同模态数据。然后，LIFTED构建统一的抗噪声编码器，从模态特定的语言描述中提取信息。

    The clinical trial is a pivotal and costly process, often spanning multiple years and requiring substantial financial resources. Therefore, the development of clinical trial outcome prediction models aims to exclude drugs likely to fail and holds the potential for significant cost savings. Recent data-driven attempts leverage deep learning methods to integrate multimodal data for predicting clinical trial outcomes. However, these approaches rely on manually designed modal-specific encoders, which limits both the extensibility to adapt new modalities and the ability to discern similar information patterns across different modalities. To address these issues, we propose a multimodal mixture-of-experts (LIFTED) approach for clinical trial outcome prediction. Specifically, LIFTED unifies different modality data by transforming them into natural language descriptions. Then, LIFTED constructs unified noise-resilient encoders to extract information from modal-specific language descriptions. S
    
[^14]: 在合适的时间提出正确的问题：人类和模型的不确定性指导下的澄清问题

    Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance to Ask Clarification Questions

    [https://arxiv.org/abs/2402.06509](https://arxiv.org/abs/2402.06509)

    本文研究了模型不确定性与人类不确定性之间的关系，并提出了一种基于模型不确定性估计的生成澄清问题的方法，为对话系统在决定何时提问提供了重要指导，并且在任务成功率方面取得了显著改进。

    

    澄清问题是一种在语言使用中表达误解、歧义和未明示的重要对话工具。虽然人类能够通过提问来解决不确定性，但现代对话系统很难生成有效的问题。为了在这方面取得进展，本文以协作对话任务为测试平台，研究了模型不确定性与人类不确定性之间的关系——这是一个尚未深入研究的问题。我们发现，模型不确定性并不反映人类寻求澄清的行为，这表明使用人类澄清问题来决定何时提问可能不是解决模型不确定性最有效的方法。为了解决这个问题，我们提出了一种基于模型不确定性估计的生成澄清问题的方法，并与几种替代方法进行了比较，结果表明该方法在任务成功率方面取得了显著改进。我们的研究结果强调了这个问题的重要性。

    Clarification questions are an essential dialogue tool to signal misunderstanding, ambiguities, and under-specification in language use. While humans are able to resolve uncertainty by asking questions since childhood, modern dialogue systems struggle to generate effective questions. To make progress in this direction, in this work we take a collaborative dialogue task as a testbed and study how model uncertainty relates to human uncertainty -- an as yet under-explored problem. We show that model uncertainty does not mirror human clarification-seeking behavior, which suggests that using human clarification questions as supervision for deciding when to ask may not be the most effective way to resolve model uncertainty. To address this issue, we propose an approach to generating clarification questions based on model uncertainty estimation, compare it to several alternatives, and show that it leads to significant improvements in terms of task success. Our findings highlight the importanc
    
[^15]: 可扩展互动式机器学习用于未来指挥与控制

    Scalable Interactive Machine Learning for Future Command and Control

    [https://arxiv.org/abs/2402.06501](https://arxiv.org/abs/2402.06501)

    未来战争将需要指挥与控制（C2）人员在复杂且潜在模糊的情况下以更短的时间内做出决策。本论文通过利用互动式机器学习方法，结合人工智能和人类智能，以提高C2运作的适应性和效率。

    

    未来战争将需要指挥与控制（C2）人员在复杂且潜在模糊的情况下以更短的时间内做出决策。鉴于需要强大的决策过程和决策支持工具，人工智能和人类智能的集成具有革命性地改变C2运作流程的潜力，以确保在快速变化的操作环境中的适应性和效率。我们提议利用最近在互动式机器学习方面取得的突破，人类可以与机器学习算法合作以指导机器学习算法的行为。本文确定了目前科技发展中存在的几个差距，未来的工作应该解决这些差距，以扩展这些方法在复杂的C2环境中发挥作用。特别是，我们描述了三个研究重点领域，共同旨在实现可扩展的互动式机器学习（SIML）：1）开发人工智能与人类交互算法以实现协同规划。

    Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in co
    
[^16]: 通过关注结构化量化的嵌入在Transformer中引导系统性

    Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings

    [https://arxiv.org/abs/2402.06492](https://arxiv.org/abs/2402.06492)

    本论文提出了SQ-Transformer模型，通过在嵌入和注意层中引入结构化量化的方法，无论训练集的复杂度如何，都能够明确地鼓励模型在编码句子时保持系统性。

    

    Transformer在训练过复杂数据集后能够推广到结构和实体的新组合，但在复杂度不足的数据集上容易过拟合。我们观察到，当训练集足够复杂时，模型使用系统性的注意模式对具有共同句法结构的句子进行编码。受到这一观察的启发，我们提出了SQ-Transformer（结构化量化），即使使用低复杂度的训练集，也能明确地在嵌入和注意层中鼓励系统性。在嵌入层面上，我们引入了结构导向的向量量化（SoVQ），将单词嵌入聚类成若干类具有结构等价的实体。在注意层面上，我们设计了系统性注意层（SAL）和另一种替代性的系统性正则化层（SRL），它们都在量化的词嵌入上操作，以便以不变或类似的注意模式编码具有相同结构的句子。

    Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empiricall
    
[^17]: V-STaR: 自学推理器的训练方法

    V-STaR: Training Verifiers for Self-Taught Reasoners

    [https://arxiv.org/abs/2402.06457](https://arxiv.org/abs/2402.06457)

    V-STaR利用正确和不正确的解决方案训练验证器，用于选择模型生成的解决方案，实现了自我改进和验证方法在常见代码生成和数学推理任务中达到4%到17%的测试准确率提升。

    

    大型语言模型（LLM）的常见自我改进方法，例如STaR（Zelikman等人，2022），通过自动生成的解决方案迭代微调LLM以提高其问题解决能力。然而，这些方法在此过程中丢弃了大量的不正确的解决方案，可能忽略了这些解决方案中的宝贵信息。为了解决这个缺点，我们提出了V-STaR，它利用自我改进过程中生成的正确和不正确的解决方案来使用DPO训练一个判断模型生成解决方案的正确性的验证器。在推理时，这个验证器用来在众多候选解决方案中选择一个解决方案。多次运行V-STaR会逐步产生更好的推理器和验证器，在常见代码生成和数学推理基准测试中，使用LLaMA2模型可以取得4%到17%的测试准确率提升。

    Common self-improvement approaches for large language models (LLMs), such as STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
    
[^18]: 使用证据汇总解释真实性预测：一种多任务模型方法

    Explaining Veracity Predictions with Evidence Summarization: A Multi-Task Model Approach

    [https://arxiv.org/abs/2402.06443](https://arxiv.org/abs/2402.06443)

    本文提出了一种用于虚假信息检测的多任务可解释性神经模型，通过将模型的真实性预测解释生成过程看作是一个文本摘要问题来解决推理差距，并在公开数据集上评估了该模型的性能。

    

    社交媒体上虚假信息的快速传播增加了自动事实检查的重要性。此外，关于深度神经模型在进行预测时关注的内容的研究近年来增加。虽然在这个领域取得了显著的进展，但还没有达到与人类推理可比的水平。为了解决这些差距，我们提出了一种用于虚假信息检测的多任务可解释性神经模型。具体而言，这项工作将模型的真实性预测的解释生成过程定义为一个文本摘要问题。此外，本研究讨论了所提出模型在公开数据集上的性能，并评估了相关研究的发现。

    The rapid dissemination of misinformation through social media increased the importance of automated fact-checking. Furthermore, studies on what deep neural models pay attention to when making predictions have increased in recent years. While significant progress has been made in this field, it has not yet reached a level of reasoning comparable to human reasoning. To address these gaps, we propose a multi-task explainable neural model for misinformation detection. Specifically, this work formulates an explanation generation process of the model's veracity prediction as a text summarization problem. Additionally, the performance of the proposed model is discussed on publicly available datasets and the findings are evaluated with related studies.
    
[^19]: 第一届模拟对话智能研讨会的研究结果

    Findings of the First Workshop on Simulating Conversational Intelligence in Chat

    [https://arxiv.org/abs/2402.06420](https://arxiv.org/abs/2402.06420)

    第一届模拟对话智能研讨会的目标是汇集对开放领域对话研究进行实时人类评估的模拟智能对话模型。论文主要提供了共享任务的概述，并附上了一个将在研讨会后发布的深入分析共享任务结果的链接。

    

    本研讨会旨在汇集从事开放领域对话研究的专家。在这个快速发展的研究领域中仍然存在许多挑战，如从对话中学习信息、进行真实和令人信服的人工智能和推理模拟。SCI-CHAT是之前关于开放领域对话的研讨会的延续，但着重于模拟智能对话，并通过人类评估来判断其质量。模型的目标是在多轮对话中能够跟随一个具有挑战性的主题，同时提出、反驳和推理论证。该研讨会包括研究路径和共享任务。本文的主要目标是概述共享任务，并提供一个链接，链接将包含在研讨会上展示后对共享任务结果进行深入分析的另一篇论文。

    The aim of this workshop is to bring together experts working on open-domain dialogue research. In this speedily advancing research area many challenges still exist, such as learning information from conversations, engaging in realistic and convincing simulation of human intelligence and reasoning. SCI-CHAT follows previous workshops on open domain dialogue but with a focus on the simulation of intelligent conversation as judged in a live human evaluation. Models aim to include the ability to follow a challenging topic over a multi-turn conversation, while positing, refuting and reasoning over arguments. The workshop included both a research track and shared task. The main goal of this paper is to provide an overview of the shared task and a link to an additional paper that will include an in depth analysis of the shared task results following presentation at the workshop.
    
[^20]: CoSearchAgent:基于大语言模型的轻量级协作搜索代理

    CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models

    [https://arxiv.org/abs/2402.06360](https://arxiv.org/abs/2402.06360)

    CoSearchAgent是一种基于大语言模型的轻量级协作搜索代理，可作为Slack插件在多方对话中支持协作搜索。

    

    协作搜索支持多个用户共同完成特定的搜索任务。研究发现，将轻量级协作搜索插件设计在即时通讯平台内更符合用户的协作习惯。然而，由于多用户交互场景的复杂性，实现一个完全功能的轻量级协作搜索系统是具有挑战性的。因此，之前的轻量级协作搜索研究不得不依赖于"吹牛大王"范例。近年来，大型语言模型(LLM)已被证明可以与用户自然交互，并通过基于LLM的代理实现复杂的信息搜索任务。因此，为了更好地支持协作搜索研究，本文提出了CoSearchAgent，一种由LLM驱动的轻量级协作搜索代理。CoSearchAgent被设计为Slack插件，可以在该平台上的多方对话中支持协作搜索。

    Collaborative search supports multiple users working together to accomplish a specific search task. Research has found that designing lightweight collaborative search plugins within instant messaging platforms aligns better with users' collaborative habits. However, due to the complexity of multi-user interaction scenarios, it is challenging to implement a fully functioning lightweight collaborative search system. Therefore, previous studies on lightweight collaborative search had to rely on the Wizard of Oz paradigm. In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents. Hence, to better support the research in collaborative search, in this demo, we propose CoSearchAgent, a lightweight collaborative search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that can support collaborative search during multi-party conversations on this platform. Equipped
    
[^21]: 促进上下文感知的神经机器翻译中的目标数据

    Promoting Target Data in Context-aware Neural Machine Translation

    [https://arxiv.org/abs/2402.06342](https://arxiv.org/abs/2402.06342)

    本研究探讨了在上下文感知的神经机器翻译中是否应该进一步提升目标数据，实验证明在源语言中包含目标上下文可以显著提高目标语言现象。

    

    标准的上下文感知的神经机器翻译（NMT）通常依赖于并行的文档级数据，利用源语言和目标语言的上下文。尤其是基于拼接的方法仍然是文档级NMT的强有力基线，它在要被翻译的句子之前添加源语言和/或目标语言的上下文句子，并且在每一侧利用相等数量的源语言和目标语言数据的模型变体达到了最先进的结果。在这项工作中，我们探讨了在标准的基于拼接的方法中是否应该进一步提供目标数据，因为大多数文档级现象依赖于目标语言侧存在的信息。我们评估了新的基于拼接的变种，在源语言之前添加目标上下文，要么仅在源语言中添加，要么与源上下文组合。在英俄和巴斯克西班牙语的实验结果表明，在源语言中包含目标上下文可以显著提高目标语言现象。

    Standard context-aware neural machine translation (NMT) typically relies on parallel document-level data, exploiting both source and target contexts. Concatenation-based approaches in particular, still a strong baseline for document-level NMT, prepend source and/or target context sentences to the sentences to be translated, with model variants that exploit equal amounts of source and target data on each side achieving state-of-the-art results. In this work, we investigate whether target data should be further promoted within standard concatenation-based approaches, as most document-level phenomena rely on information that is present on the target language side. We evaluate novel concatenation-based variants where the target context is prepended to the source language, either in isolation or in combination with the source context. Experimental results in English-Russian and Basque-Spanish show that including target context in the source leads to large improvements on target language phe
    
[^22]: RareBench：LLMs能否担任罕见病专家？

    RareBench: Can LLMs Serve as Rare Diseases Specialists?

    [https://arxiv.org/abs/2402.06341](https://arxiv.org/abs/2402.06341)

    RareBench是一个开创性的基准测试，旨在评估LLMs在罕见病领域的诊断能力，为未来研究提供了一个最大的开放数据集。

    

    通用型大型语言模型（LLMs），如GPT-4，在包括医学诊断在内的各个领域显示出了相当大的潜力。罕见病，影响全球约3亿人，往往由于缺乏经验丰富的医生和难以区分众多罕见病的复杂性而导致临床诊断率不尽人意。在这种情况下，最近的新闻如"ChatGPT在17名医生失败后正确诊断出了一位4岁孩子的罕见病"强调了LLMs在临床诊断罕见病中的潜力，然而这个角色在研究中尚未得到充分探讨。为了填补这一研究空白，我们推出了RareBench，一个开创性的基准测试，旨在系统评估LLMs在罕见病领域内的4个关键维度上的能力。同时，我们编制了最大的罕见病患者开放数据集，为未来研究在这一领域建立了一个基准。为了促进罕见病的差异诊断，我们开发了一个动态的方法。

    Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's rare disease after 17 doctors failed" underscore LLMs' potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic
    
[^23]: ExaRanker-Open: 使用开源LLMs进行IR的合成解释

    ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs

    [https://arxiv.org/abs/2402.06334](https://arxiv.org/abs/2402.06334)

    ExaRanker-Open 是一种使用开源LLMs进行IR的方法，通过适应和探索开源语言模型来生成解释。研究结果表明，纳入解释能够稳定提高神经排序器的性能，而LLM的大小越大，收益越大。

    

    ExaRanker最近提出了一种训练信息检索(IR)模型的方法，该方法将自然语言解释作为附加标签。该方法解决了有限标记示例的挑战，提高了IR模型的效果。然而，初始结果是基于专有的语言模型，如GPT-3.5，这导致了数据集大小的限制，因为其成本和数据隐私。在本文中，我们介绍了ExaRanker-Open，通过适应和探索开源语言模型来生成解释。该方法已经使用不同的LLMs和数据集大小进行了测试，以更好地理解数据增强的有效贡献。我们的研究结果表明，纳入解释能够稳定提高神经排序器的性能，而LLM的大小越大，收益越大。值得注意的是，即使在大数据集上，数据增强方法也是有优势的，ExaRanker的性能超过目标基线0

    ExaRanker recently introduced an approach to training information retrieval (IR) models, incorporating natural language explanations as additional labels. The method addresses the challenge of limited labeled examples, leading to improvements in the effectiveness of IR models. However, the initial results were based on proprietary language models such as GPT-3.5, which posed constraints on dataset size due to its cost and data privacy. In this paper, we introduce ExaRanker-Open, where we adapt and explore the use of open-source language models to generate explanations. The method has been tested using different LLMs and datasets sizes to better comprehend the effective contribution of data augmentation. Our findings reveal that incorporating explanations consistently enhances neural rankers, with benefits escalating as the LLM size increases. Notably, the data augmentation method proves advantageous even with large datasets, as evidenced by ExaRanker surpassing the target baseline by 0
    
[^24]: InternLM-Math：面向可验证推理的开放数学大语言模型

    InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning

    [https://arxiv.org/abs/2402.06332](https://arxiv.org/abs/2402.06332)

    在本文中，我们介绍了开源数学推理LLMs InternLM-Math，该模型以其数学能力代表了抽象推理能力。我们的模型以统一的方式整合了逻辑推理、奖励建模、形式推理、数据增强和代码解释器，并使用监督学习使其成为一个多功能的数学推理器、验证器、证明器和增强器。在各种基准测试中，包括GSM8K、MATH、匈牙利数学考试、MathBench-ZH和MiniF2F，在上下文学习、监督微调和代码辅助推理的设置下，InternLM-Math取得了开源的最先进性能。我们的预训练模型在MiniF2F测试集上达到了30.3的得分。我们还研究了如何使用LEAN解决数学问题，并探讨了其在多任务学习设置下的性能。

    

    大型语言模型的数学能力可以表示其抽象推理能力。本文介绍并开源我们的数学推理LLMs InternLM-Math，该模型是从InternLM2继续预训练而来。我们以统一的seq2seq格式统一了逻辑推理、奖励建模、形式推理、数据增强和代码解释器，并且监督我们的模型成为一个多功能的数学推理器、验证器、证明器和增强器。这些能力可用于开发下一代数学LLMs或自身迭代。在包括GSM8K、MATH、匈牙利数学考试、MathBench-ZH和MiniF2F在内的各种非正式和正式基准测试中，InternLM-Math在上下文学习、监督微调和代码辅助推理的设置下取得了开源的最先进性能。我们的预训练模型在无微调的情况下，在MiniF2F测试集上达到了30.3。我们进一步研究了如何使用LEAN来解决数学问题，并研究了其在多任务学习设置下的性能。

    The math abilities of large language models can represent their abstract reasoning ability. In this paper, we introduce and open-source our math reasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We unify chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter in a unified seq2seq format and supervise our model to be a versatile math reasoner, verifier, prover, and augmenter. These abilities can be used to develop the next math LLMs or self-iteration. InternLM-Math obtains open-sourced state-of-the-art performance under the setting of in-context learning, supervised fine-tuning, and code-assisted reasoning in various informal and formal benchmarks including GSM8K, MATH, Hungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves 30.3 on the MiniF2F test set without fine-tuning. We further explore how to use LEAN to solve math problems and study its performance under the setting of multi-task learnin
    
[^25]: 关于针对键值约束生成语言模型推理的驱逐策略的有效性研究

    On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference

    [https://arxiv.org/abs/2402.06262](https://arxiv.org/abs/2402.06262)

    本文研究了针对键值约束生成语言模型推理的驱逐策略的有效性，通过引入基于时间注意力得分和鲁棒性度量的RoCo策略，优于现有的策略。

    

    尽管大型语言模型（LLMs）在最近取得了成功，但由于它们对内存和计算资源的过度需求，它们在资源受限环境中部署仍然昂贵。除了模型参数外，键值缓存也存储在GPU内存中，随着批处理大小和序列长度的增加而线性增长。为此，最近的研究提出了各种针对给定预算下维护键值缓存开销的驱逐策略。本文着眼于现有驱逐策略在重要性评分计算和驱逐范围构建两个方面的效果。我们确定了先前策略在这两个方面的不足，并引入了基于时间注意力得分和鲁棒性度量的RoCo，一种强大的缓存驱逐策略。涵盖了预填充和自回归解码阶段的广泛实验验证了RoCo的优越性。最后，我们公开发布了RoCo的代码和模型供研究者使用。

    Despite the recent success associated with Large Language Models~(LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands. In addition to model parameters, the key-value cache is also stored in GPU memory, growing linearly with batch size and sequence length. As a remedy, recent works have proposed various eviction policies for maintaining the overhead of key-value cache under a given budget. This paper embarks on the efficacy of existing eviction policies in terms of \textit{importance score calculation} and \textit{eviction scope construction}. We identify the deficiency of prior policies in these two aspects and introduce RoCo, a \underline{r}\underline{o}bust \underline{c}ache \underline{o}mission policy based on temporal attention scores and robustness measures. Extensive experimentation spanning prefilling and auto-regressive decoding stages validates the superiority of RoCo. Finally, we relea
    
[^26]: 进取的鲍勃通过提示对抗调整抵制越狱行为

    Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning

    [https://arxiv.org/abs/2402.06255](https://arxiv.org/abs/2402.06255)

    本文提出了一种名为Prompt Adversarial Tuning (PAT)的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中，实现对大型语言模型（LLMs）的越狱行为的防御。实验证明该方法在保护LLMs免受产生有害信息的影响方面效果显著。

    

    尽管大型语言模型（LLM）在各种应用中取得了巨大的成功，但它们也容易受到特定提示的影响，从而绕过内置的安全措施并提供危险或非法内容，这种现象被称为越狱行为。为了保护LLMs免受产生有害信息的影响，提出了各种防御策略，其中大多数集中在内容过滤或模型的对抗训练方面。在本文中，我们提出了一种名为Prompt Adversarial Tuning（PAT）的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中来实现我们的防御策略。我们设计了一个类似对抗训练的训练过程，以实现我们的优化目标，交替更新攻击和防御控制机制。据我们所知，我们是第一个从提示调整的角度实施防御的人。一旦应用，我们的方法几乎不会影响LLMs的操作效率。实验表明我们的方法在抵御越狱行为方面具有良好的效果。

    Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
    
[^27]: ResumeFlow: 一种个性化简历生成和修订的LLM辅助流程

    ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume Generation and Refinement

    [https://arxiv.org/abs/2402.06221](https://arxiv.org/abs/2402.06221)

    ResumeFlow是一种利用LLM技术的工具，能够帮助求职者根据特定的职位要求生成个性化的简历，从而解决了手动定制简历的耗时和容易出错的问题。

    

    对于许多求职者来说，制作符合特定职位要求的理想简历是一项具有挑战性的任务，尤其是对于初入职场的求职者来说。虽然强烈建议求职者根据他们申请的具体职位定制简历，但手动根据工作描述和职位要求来定制简历通常 (1) 非常耗时，且 (2) 容易出错。此外，在申请多个职位时进行这样的定制步骤可能导致编辑简历质量不高。为了解决这个问题，在本演示论文中，我们提出了ResumeFlow: 一种利用大型语言模型（LLM）的工具，使终端用户只需提供详细的简历和所需的职位发布信息，就能在几秒钟内获得一个针对该特定职位发布的个性化简历。我们提出的流程利用了最先进的LLM（如OpenAI的GPT-4和Google的......）

    Crafting the ideal, job-specific resume is a challenging task for many job applicants, especially for early-career applicants. While it is highly recommended that applicants tailor their resume to the specific role they are applying for, manually tailoring resumes to job descriptions and role-specific requirements is often (1) extremely time-consuming, and (2) prone to human errors. Furthermore, performing such a tailoring step at scale while applying to several roles may result in a lack of quality of the edited resumes. To tackle this problem, in this demo paper, we propose ResumeFlow: a Large Language Model (LLM) aided tool that enables an end user to simply provide their detailed resume and the desired job posting, and obtain a personalized resume specifically tailored to that specific job posting in the matter of a few seconds. Our proposed pipeline leverages the language understanding and information extraction capabilities of state-of-the-art LLMs such as OpenAI's GPT-4 and Goog
    
[^28]: 统一的因果视角下的指令调优

    A Unified Causal View of Instruction Tuning

    [https://arxiv.org/abs/2402.06220](https://arxiv.org/abs/2402.06220)

    该论文提出了一个元结构因果模型（meta-SCM）来整合不同的自然语言处理（NLP）任务。通过学习每个任务所需的因果因子并使用这些因子进行预测，从而解决了现有方法中存在的“伪相关性”问题。

    

    在自然语言处理（NLP）中，混合任务上的指令调优已经提高了零-shot能力。然而，现有的方法常常学习到了在指令格式的样本和目标标签之间存在相关性，而不是因果关系的特征。这种被统计学上称为“伪相关性”的相关性在新任务中可能会发生巨大变化，使得学习到的特征对结果产生误导。为此，我们开发了一个元结构因果模型（meta-SCM），用于将不同的NLP任务整合到单一的数据因果结构下。具体而言，meta-SCM引入了多个潜在因子来表示源上下文的特性，其中只有一些对特定任务的目标标签产生因果影响。关键思想是学习任务所需的因果因子，只使用这些因素来预测给定任务的结果。理论上，我们证明了可以在不混合其他信息的情况下进行因果因子的识别。受到因果推断的指导，我们提出了uchii等.（2021）:论文+_因果识别+_整合来学习因果影响

    Instruction tuning on a mixture of tasks has improved zero-shot capabilities in natural language processing (NLP). Nevertheless, existing methods often learn features that exhibit correlations between instruction-formatted samples and target labels, rather than causal relationships. Termed as ``spurious correlation'' in statistics, such a correlation may change drastically in a new task, making the effect from the learned features to be misleading. To this end, we develop a meta Structural Causal Model (meta-SCM) to integrate different NLP tasks under a single causal structure of the data. Specifically, the meta-SCM introduces multiple latent factors that represent properties of source context, only some of which causally influence the target labels for a specific task. The key idea is to learn task-required causal factors and only use those to make predictions for a given task. Theoretically, we prove the causal factor can be identified without mixing information from others. Guided b
    
[^29]: 生成AI评估中的悖论：它能解决的问题可能无法进行评估

    The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate

    [https://arxiv.org/abs/2402.06204](https://arxiv.org/abs/2402.06204)

    本文讨论了生成AI评估中的悖论，并发现了大型语言模型在评估任务中性能较差的现象。研究突出了需要检查模型作为评估者的忠实度和可信度，以及探索生成优秀与评估能力之间的关联。

    

    本文探讨了一种假设，即在生成任务中擅长的大型语言模型（LLM）同样擅长作为评估者。我们使用TriviaQA数据集评估了三个LLM和一个开源LM在问答（QA）和评估任务中的表现。结果表明存在显著差异，LLM在评估任务中的性能较生成任务低。有趣的是，我们发现了一些不忠实的评估情况，模型在其不擅长的领域中准确评估答案，突出了需要检查LLM作为评估者的忠实度和可信度。本研究有助于理解“生成AI悖论”，强调了探索生成优秀与评估能力之间的关联以及审查模型评估中忠实度方面的必要性。

    This paper explores the assumption that Large Language Models (LLMs) skilled in generation tasks are equally adept as evaluators. We assess the performance of three LLMs and one open-source LM in Question-Answering (QA) and evaluation tasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks. Intriguingly, we discover instances of unfaithful evaluation where models accurately evaluate answers in areas where they lack competence, underscoring the need to examine the faithfulness and trustworthiness of LLMs as evaluators. This study contributes to the understanding of "the Generative AI Paradox" (West et al., 2023), highlighting a need to explore the correlation between generative excellence and evaluation proficiency, and the necessity to scrutinize the faithfulness aspect in model evaluations.
    
[^30]: 大型语言模型：一项调查

    Large Language Models: A Survey

    [https://arxiv.org/abs/2402.06196](https://arxiv.org/abs/2402.06196)

    大型语言模型（LLMs）吸引了很多关注，因为它们在自然语言任务上的强大表现。该研究领域发展迅速，包括了各种著名的LLMs、构建和增强LLMs的技术、以及流行的LLM数据集和评估指标。

    

    大型语言模型（LLMs）由于其在各种自然语言任务上的出色表现而受到了很多关注，自2022年11月ChatGPT发布以来。LLMs通过在大量文本数据上训练模型的数十亿参数来获得广泛的通用语言理解和生成能力，这符合缩放定律的预测。LLMs的研究领域尽管非常新，但在许多不同方面正在快速发展。在本文中，我们回顾了一些最著名的LLMs，包括三个流行的LLM系列（GPT、LLaMA、PaLM），并讨论了它们的特点、贡献和限制。我们还概述了构建和增强LLMs的技术。然后，我们调查了为LLM训练、微调和评估准备的流行数据集，审查了广泛使用的LLM评估指标，并比较了几个流行LLM在一组代表性基准上的性能。

    Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we co
    
[^31]: 模型编辑与规范示例

    Model Editing with Canonical Examples

    [https://arxiv.org/abs/2402.06155](https://arxiv.org/abs/2402.06155)

    使用规范示例进行模型编辑，每个期望行为只有一个学习示例，评估仅在分布之外进行，对初始模型的偏离受限，通过实验发现LoRA优于完全微调和MEMIT。

    

    我们引入了使用规范示例进行模型编辑的方法，其中(1)每个期望行为只提供一个学习示例，(2)评估仅在分布之外进行，(3)对初始模型的偏离严格受限制。规范示例是良好行为的简单实例，例如，“毛里求斯的首都是路易港”，或者坏行为的实例，例如，“研究人员的一个方面是冷酷无情”。评估集包含更复杂的每种行为的示例（例如，在一个段落中呼叫毛里求斯的首都）。我们创建了三个数据集并修改了另外三个数据集，用于模型编辑与规范示例，涵盖了知识密集型改进、社会偏见缓解和句法边缘案例。在我们对Pythia语言模型的实验中，我们发现LoRA优于完全微调和MEMIT。然后我们转向了Backpack语言模型架构，因为它旨在实现有针对性的改进。Backpack定义了一个大型的sen...

    We introduce model editing with canonical examples, a setting in which (1) a single learning example is provided per desired behavior, (2) evaluation is performed exclusively out-of-distribution, and (3) deviation from an initial model is strictly limited. A canonical example is a simple instance of good behavior, e.g., The capital of Mauritius is Port Louis) or bad behavior, e.g., An aspect of researchers is coldhearted). The evaluation set contains more complex examples of each behavior (like a paragraph in which the capital of Mauritius is called for.) We create three datasets and modify three more for model editing with canonical examples, covering knowledge-intensive improvements, social bias mitigation, and syntactic edge cases. In our experiments on Pythia language models, we find that LoRA outperforms full finetuning and MEMIT. We then turn to the Backpack language model architecture because it is intended to enable targeted improvement. The Backpack defines a large bank of sen
    
[^32]: DeAL：用于大型语言模型的解码时对齐

    DeAL: Decoding-time Alignment for Large Language Models

    [https://arxiv.org/abs/2402.06147](https://arxiv.org/abs/2402.06147)

    DeAL是一个允许用户自定义奖励函数并实现解码时对齐LLMs的框架。

    

    大型语言模型（LLMs）现在期望生成与人类偏好对齐的内容。目前的工作主要集中在模型训练时间对齐上，通过诸如强化学习与人类反馈（RLHF）等技术。然而，目前还不清楚这些方法是否有效地教导模型对齐目标。首先，无法整合多个自定义奖励和依赖模型开发者对通用和静态原则的理解是主要局限。其次，模型训练中的残留差距以及这些方法的可靠性也值得质疑（例如，即使在安全训练后仍然容易被越狱）。为了解决这些问题，我们提出了DeAL，一个允许用户自定义奖励函数并实现解码时对齐LLMs（DeAL）的框架。核心思想在于将解码视为一个启发式引导的搜索过程，并促使使用各种对齐目标。我们的实验以编程约束为例进行了验证。

    Large Language Models (LLMs) are nowadays expected to generate content aligned with human preferences. Current work focuses on alignment at model training time, through techniques such as Reinforcement Learning with Human Feedback (RLHF). However, it is unclear if such methods are an effective choice to teach alignment objectives to the model. First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training). To address these, we propose DeAL, a framework that allows the user to customize reward functions and enables Decoding-time Alignment of LLMs (DeAL). At its core, we view decoding as a heuristic-guided search process and facilitate the use of a wide variety of alignment objectives. Our experiments with programmatic constra
    
[^33]: 学习变得高效：在大型语言模型中构建结构化稀疏性

    Learn To be Efficient: Build Structured Sparsity in Large Language Models

    [https://arxiv.org/abs/2402.06126](https://arxiv.org/abs/2402.06126)

    本文通过引入一种新的算法"Learn-To-be-Efficient(LTE)"，提出了在大型语言模型(LLM)中构建结构化稀疏性的方法。该方法通过训练高效意识的LLM学习激活更少的神经元，取得更好的稀疏性和性能折衷。

    

    大型语言模型(LLM)以其十亿级参数取得了显著的成功，但它们产生了高昂的推理开销。在LLM中出现的激活稀疏性为通过仅涉及部分参数进行推理提供了一种自然的方法来减少这种成本。现有方法只关注利用这种自然形成的激活稀疏性，忽视了进一步放大这种固有稀疏性的潜力。本文中，我们假设LLM可以通过实现更结构化的激活稀疏性来学习高效。为实现这一目标，我们引入了一种新颖的算法"Learn-To-be-Efficient(LTE)", 旨在训练高效意识的LLM学习激活更少的神经元，并在稀疏性和性能之间取得更好的折衷。此外，与主要关注基于ReLU模型的SOTA MoEfication方法不同，LTE还可以应用于像GPT和LLaMA这样具有软激活函数的LLM。我们在四个模型和十一个数据集上评估了LTE。

    Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
    
[^34]: 带有解析驱动的修辞控制方法的语言模型句子补全

    Language Model Sentence Completion with a Parser-Driven Rhetorical Control Method

    [https://arxiv.org/abs/2402.06125](https://arxiv.org/abs/2402.06125)

    本研究提出了一种新的控制性文本生成算法，通过解析驱动的解码方案，在语言模型句子补全环境中强制执行特定修辞关系的遵循，无需模型微调。该方法经过自动和人工评估验证，代码可在GitHub上获得。

    

    控制性文本生成（CTG）旨在引导大型语言模型（LLM）生成符合特定要求的文本。本研究提出了一种新颖的CTG算法，在LLM句子补全环境中通过解析驱动的解码方案强制执行特定修辞关系的遵循，而无需模型微调。该方法已通过自动和人工评估进行验证。代码可在GitHub上获得。

    Controlled text generation (CTG) seeks to guide large language model (LLM) output to produce text that conforms to desired criteria. The current study presents a novel CTG algorithm that enforces adherence toward specific rhetorical relations in an LLM sentence-completion context by a parser-driven decoding scheme that requires no model fine-tuning. The method is validated both with automatic and human evaluation. The code is accessible on GitHub.
    
[^35]: 重新思考有监督微调的数据选择

    Rethinking Data Selection for Supervised Fine-Tuning

    [https://arxiv.org/abs/2402.06094](https://arxiv.org/abs/2402.06094)

    本研究重新思考了有监督微调中数据选择的直觉。考虑SFT的肤浅性质，我们提出重要示范应着重反映人类式的互动。通过选择具有长回答的实例，可以实现更好的SFT效果。

    

    虽然有监督微调（SFT）已成为将大型语言模型与人类对齐的基本技术，但它被认为是肤浅的，其本质是学习样式。与此同时，最近的研究表明了对于SFT而言数据选择的重要性，显示了使用原始数据集的高质量和多样性子集进行微调会导致更好的下游性能。在这项工作中，我们重新思考了对于SFT的数据选择的直觉。考虑到SFT的肤浅性质，我们提出SFT的重要示范应该着重反映人类式的互动，而不是数据质量或多样性。然而，直接评估一个示范反映人类风格的程度并不简单。在这个方向上的初步尝试中，我们发现选择具有长回答的实例对于SFT而言比使用完整数据集或根据质量和多样性选择实例更为有效。我们假设这样一个简单的启发式方法可以捕捉到所需的示范特征。

    Although supervised finetuning (SFT) has emerged as an essential technique to align large language models with humans, it is considered superficial, with style learning being its nature. At the same time, recent works indicate the importance of data selection for SFT, showing that finetuning with high-quality and diverse subsets of the original dataset leads to superior downstream performance. In this work, we rethink the intuition behind data selection for SFT. Considering SFT is superficial, we propose that essential demonstrations for SFT should focus on reflecting human-like interactions instead of data quality or diversity. However, it is not straightforward to directly assess to what extent a demonstration reflects human styles. Towards an initial attempt in this direction, we find selecting instances with long responses is surprisingly more effective for SFT than utilizing full datasets or instances selected based on quality and diversity. We hypothesize that such a simple heuri
    
[^36]: LightCAM: 一种快速轻量级的基于上下文感知屏蔽的D-TDNN说话人验证实现

    LightCAM: A Fast and Light Implementation of Context-Aware Masking based D-Tdnn for Speaker Verification

    [https://arxiv.org/abs/2402.06073](https://arxiv.org/abs/2402.06073)

    LightCAM是一种快速轻量级的基于上下文感知屏蔽的D-TDNN说话人验证实现，通过采用深度可分离卷积模块和多尺度特征聚合，它在VoxCeleb数据集上取得了更好的性能。

    

    传统的时延神经网络(TDNN)在计算复杂度和推理速度方面取得了最先进的性能，使得它们在工业环境中难以实施。具有上下文感知屏蔽(CAM)模块的密集连通时延神经网络(D-TDNN)已经证明是一种降低复杂性并保持系统性能的高效结构。本文提出了一种快速轻量级模型LightCAM，它进一步采用了深度可分离卷积模块(DSM)和多尺度特征聚合(MFA)以实现不同层次的特征融合。在VoxCeleb数据集上进行了大量实验，比较结果表明它在VoxCeleb1-O上实现了0.83的等错误率(EER)和0.0891的最小检测代价因子(MinDCF)，超过了其他主流的说话人验证方法。此外，复杂度分析进一步证明了所提出的架构具有较低的计算成本和更快的推理速度。

    Traditional Time Delay Neural Networks (TDNN) have achieved state-of-the-art performance at the cost of high computational complexity and slower inference speed, making them difficult to implement in an industrial environment. The Densely Connected Time Delay Neural Network (D-TDNN) with Context Aware Masking (CAM) module has proven to be an efficient structure to reduce complexity while maintaining system performance. In this paper, we propose a fast and lightweight model, LightCAM, which further adopts a depthwise separable convolution module (DSM) and uses multi-scale feature aggregation (MFA) for feature fusion at different levels. Extensive experiments are conducted on VoxCeleb dataset, the comparative results show that it has achieved an EER of 0.83 and MinDCF of 0.0891 in VoxCeleb1-O, which outperforms the other mainstream speaker verification methods. In addition, complexity analysis further demonstrates that the proposed architecture has lower computational cost and faster inf
    
[^37]: 大型语言模型在与人类辩论中的局限性

    Limits of Large Language Models in Debating Humans

    [https://arxiv.org/abs/2402.06049](https://arxiv.org/abs/2402.06049)

    大型语言模型在与人类辩论中的能力有限，尽管它们能够融入和促进人类的工作效率，但在辩论中的说服力较弱。在成为可行的辩手之前，LLMs需要进一步发展。

    

    大型语言模型(LLMs)在与人类的互动中展现出了显著的潜力。随后，将它们作为人工代表和替代品进行社会学实验的潜在应用是一个令人激动的前景。但是这个想法有多可行呢？本文试图通过一项预先注册的研究来测试现阶段LLMs的局限性，该研究将真实的人类与扮演人类的LLM代理结合起来。本研究着重探讨辩论为基础的意见共识形成在三种环境下的情况：仅人类、代理和人类、仅代理。我们的目标是理解LLM代理对人类的影响，并评估它们在辩论方面的能力是否与人类相似。我们发现LLMs能够融入并促进人类的工作效率，但在辩论中的说服力较弱，最终行为与人类有所偏离。我们阐明了这些主要缺陷，并预计在成为可行的辩手之前，LLMs必须进一步发展。

    Large Language Models (LLMs) have shown remarkable promise in their ability to interact proficiently with humans. Subsequently, their potential use as artificial confederates and surrogates in sociological experiments involving conversation is an exciting prospect. But how viable is this idea? This paper endeavors to test the limits of current-day LLMs with a pre-registered study integrating real people with LLM agents acting as people. The study focuses on debate-based opinion consensus formation in three environments: humans only, agents and humans, and agents only. Our goal is to understand how LLM agents influence humans, and how capable they are in debating like humans. We find that LLMs can blend in and facilitate human productivity but are less convincing in debate, with their behavior ultimately deviating from human's. We elucidate these primary failings and anticipate that LLMs must evolve further before being viable debaters.
    
[^38]: 开放理论-心灵（OpenToM）：评估大型语言模型的心灵理解能力的全面基准

    OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models

    [https://arxiv.org/abs/2402.06044](https://arxiv.org/abs/2402.06044)

    OpenToM是一个评估大型语言模型心理理解能力的全面基准，通过提供更长、更清晰的叙事故事、具有明确个性特征的角色、以及挑战模型对心理状态的理解能力的问题，揭示了现有模型在理解物理世界与心理世界的角色心理状态方面的优势和不足。

    

    神经心理理论（N-ToM）是机器理解和跟踪他人心理状态的能力，在开发具有社交智能的代理程序中至关重要。然而，目前的N-ToM基准存在一些问题，包括模糊和人工故事的存在，缺乏个性特征和偏好，缺乏涉及角色心理心态的问题，并且提出的问题多样性有限。为了应对这些问题，我们构建了OpenToM，一个新的评估N-ToM的基准，以 (1) 更长、更清晰的叙事故事，(2) 具有明确个性特征的角色，(3) 触发角色意图的行动，以及 (4) 设计旨在挑战LLMs对建模角色在物理和心理世界的心理状态能力的问题。使用OpenToM，我们发现目前最先进的LLMs在建模物理世界的一些心理状态方面表现出色，但在跟踪角色心理状态方面存在不足。

    Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characte
    
[^39]: 对自动性别中立翻译需求的迅速响应

    A Prompt Response to the Demand for Automatic Gender-Neutral Translation

    [https://arxiv.org/abs/2402.06041](https://arxiv.org/abs/2402.06041)

    本研究通过比较机器翻译和GPT-4模型，探索了自动性别中立翻译的潜力，并揭示了当前机器翻译系统在生成性别中立翻译方面的固有局限性。

    

    避免偏见和不适当的二元假设的性别中立翻译（GNT）是创造更包容性翻译技术的一个关键挑战。然而，机器翻译（MT）在此任务上的进展受到了无专用平行数据的限制，这些数据对于调整MT系统以满足中立约束是必要的。在这种情况下，大型语言模型提供了迄今为止未曾预见到的可能性，因为它们具有在提供明确指示时在各种（子）任务中灵活性的独特优势。在本文中，我们通过将MT与流行的GPT-4模型进行比较，探索了自动化GNT的潜力。通过广泛的手工分析，我们的研究从实证角度揭示了现有MT系统在生成GNT方面的固有局限性，并为提示中立性所带来的潜力和挑战提供了宝贵的见解。

    Gender-neutral translation (GNT) that avoids biased and undue binary assumptions is a pivotal challenge for the creation of more inclusive translation technologies. Advancements for this task in Machine Translation (MT), however, are hindered by the lack of dedicated parallel data, which are necessary to adapt MT systems to satisfy neutral constraints. For such a scenario, large language models offer hitherto unforeseen possibilities, as they come with the distinct advantage of being versatile in various (sub)tasks when provided with explicit instructions. In this paper, we explore this potential to automate GNT by comparing MT with the popular GPT-4 model. Through extensive manual analyses, our study empirically reveals the inherent limitations of current MT systems in generating GNTs and provides valuable insights into the potential and challenges associated with prompting for neutrality.
    
[^40]: 用自然语言和概率推理进行实验与修订规则

    Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning

    [https://arxiv.org/abs/2402.06025](https://arxiv.org/abs/2402.06025)

    本论文建立了一个计算模型来模拟人们通过实验主动推断隐藏规则的过程，并发现显式假设、概率规则和在线更新的组合可以解释人们在类似Zendo任务上的表现。

    

    我们建立了一个计算模型，模拟人们通过实验主动推断隐藏规则的过程。该模型的基本原理是，即使规则是确定性的，学习者也会考虑更广泛的模糊概率规则，并用自然语言表示，根据近似贝叶斯原则在每次实验后在线更新自己的假设。在同一框架下，我们还根据信息论准则建立了实验设计模型。我们发现，这三个原则的组合——显式假设、概率规则和在线更新——可以解释人们在类似Zendo任务上的表现，而去掉其中任何一个组件都使得模型无法解释数据。

    We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy probabilistic rules, which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles -- explicit hypotheses, probabilistic rules, and online updates -- can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.
    
[^41]: 在GPT-4V模型中探索视觉文化意识：一项全面探索

    Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing

    [https://arxiv.org/abs/2402.06015](https://arxiv.org/abs/2402.06015)

    通过在GPT-4V模型上进行全面的探索，我们发现该模型在识别文化概念方面表现出色，但在低资源语言中仍表现较弱。在图像字幕任务中，GPT-4V在文化相关性方面优于原文。

    

    预训练的大型视觉-语言模型近年来引起了 considerable的关注，其出色的性能令人印象深刻。尽管已经做出了多方面的努力来评估这些模型，但目前最先进的GPT-4V模型在视觉文化意识方面尚未被探索。为了填补这一空白，我们使用MaRVL基准数据集广泛探索了GPT-4V，旨在调查其在视觉理解方面，特别是文化方面的能力和限制。具体而言，我们引入了三个与视觉相关的任务，即标题分类、成对标题生成和文化标签选择，以系统地深入研究细粒度的视觉文化评估。实验结果表明，GPT-4V在识别文化概念方面表现出色，但在资源匮乏的语言（如泰米尔语和斯瓦希里语）中仍然表现较弱。值得注意的是，通过人工评估，GPT-4V在图像字幕任务中证明在文化相关性方面优于原文的表现。

    Pretrained large Vision-Language models have drawn considerable interest in recent years due to their remarkable performance. Despite considerable efforts to assess these models from diverse perspectives, the extent of visual cultural awareness in the state-of-the-art GPT-4V model remains unexplored. To tackle this gap, we extensively probed GPT-4V using the MaRVL benchmark dataset, aiming to investigate its capabilities and limitations in visual understanding with a focus on cultural aspects. Specifically, we introduced three visual related tasks, i.e. caption classification, pairwise captioning, and culture tag selection, to systematically delve into fine-grained visual cultural evaluation. Experimental results indicate that GPT-4V excels at identifying cultural concepts but still exhibits weaker performance in low-resource languages, such as Tamil and Swahili. Notably, through human evaluation, GPT-4V proves to be more culturally relevant in image captioning tasks than the original 
    
[^42]: 《Transformer压缩调研》

    A Survey on Transformer Compression

    [https://arxiv.org/abs/2402.05964](https://arxiv.org/abs/2402.05964)

    《Transformer压缩调研》是对最近压缩方法的全面回顾，特别关注它们在Transformer模型中的应用。压缩方法主要分为修剪、量化、知识蒸馏和高效架构设计四个类别。

    

    基于Transformer架构的大型模型在人工智能领域，特别是自然语言处理（NLP）和计算机视觉（CV）领域中扮演着日益重要的角色。模型压缩方法可以减少模型的内存和计算成本，是在实际设备上实现Transformer模型的必要步骤。鉴于Transformer的独特架构，具有交替的注意力和前馈神经网络（FFN）模块，需要特定的压缩技术。这些压缩方法的效率也至关重要，因为重新训练整个训练数据集上的大型模型往往是不切实际的。本调研提供了对最近压缩方法的全面回顾，特别关注它们在Transformer模型中的应用。压缩方法主要分为修剪、量化、知识蒸馏和高效架构设计四个类别。在每个类别中，我们讨论了压缩方法

    Large models based on the Transformer architecture play increasingly vital roles in artificial intelligence, particularly within the realms of natural language processing (NLP) and computer vision (CV). Model compression methods reduce their memory and computational cost, which is a necessary step to implement the transformer models on practical devices. Given the unique architecture of transformer, featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are required. The efficiency of these compression methods is also paramount, as it is usually impractical to retrain large models on the entire training dataset.This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to transformer models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design. In each category, we discuss compression methods
    
[^43]: 利用大型语言模型推进图表示学习：技术全面调查

    Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques

    [https://arxiv.org/abs/2402.05952](https://arxiv.org/abs/2402.05952)

    本综述调查了将大型语言模型（LLM）与图表示学习（GRL）相结合的技术，并提供了一个新颖的分类法，深入分析了这些模型的核心组成部分和操作技术，为有效的模型设计和训练策略提供了新的视角。

    

    大型语言模型（LLM）与图表示学习（GRL）的整合标志着分析复杂数据结构的重大进展。这种合作利用LLM的先进语言能力来改进图模型的上下文理解能力和适应性，从而拓宽了GRL的范围和潜力。尽管已经有大量的研究致力于将LLM集成到图领域中，但缺乏一份深入分析这些模型核心组成部分和操作技术的全面综述。我们的调查通过提出一种新颖的分类法来分解这些模型为主要组成部分和操作技术，从新的技术角度深入分析。我们进一步将最近的文献分解为两个主要组成部分，包括知识提取器和组织者，以及两个操作技术，包括集成和训练策略，揭示出有效的模型设计和训练策略的要点。

    The integration of Large Language Models (LLMs) with Graph Representation Learning (GRL) marks a significant evolution in analyzing complex data structures. This collaboration harnesses the sophisticated linguistic capabilities of LLMs to improve the contextual understanding and adaptability of graph models, thereby broadening the scope and potential of GRL. Despite a growing body of research dedicated to integrating LLMs into the graph domain, a comprehensive review that deeply analyzes the core components and operations within these models is notably lacking. Our survey fills this gap by proposing a novel taxonomy that breaks down these models into primary components and operation techniques from a novel technical perspective. We further dissect recent literature into two primary components including knowledge extractors and organizers, and two operation techniques including integration and training stratigies, shedding light on effective model design and training strategies. Additio
    
[^44]: DE$^3$-BERT: 基于原型网络的增强距离早期停止方法，用于BERT

    DE$^3$-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks

    [https://arxiv.org/abs/2402.05948](https://arxiv.org/abs/2402.05948)

    DE$^3$-BERT是一种基于原型网络和距离度量的增强距离早期停止框架，用于提高BERT等预训练语言模型的推断速度和准确性。

    

    早期停止方法通过动态调整执行的层数，提高了像BERT这样的预训练语言模型的推断速度。然而，大多数早期停止方法仅考虑了来自单个测试样本的局部信息来确定早期停止的指标，而未利用样本群体提供的全局信息。这导致对预测正确性的估计不够准确，从而产生错误的早期停止决策。为了弥合这个差距，我们探索了有效结合局部和全局信息以确保可靠的早期停止的必要性。为此，我们利用原型网络学习类别原型，并设计了样本和类别原型之间的距离度量。这使我们能够利用全局信息来估计早期预测的正确性。基于此，我们提出了一种新颖的DE$^3$-BERT增强距离早期停止框架。

    Early exiting has demonstrated its effectiveness in accelerating the inference of pre-trained language models like BERT by dynamically adjusting the number of layers executed. However, most existing early exiting methods only consider local information from an individual test sample to determine their exiting indicators, failing to leverage the global information offered by sample population. This leads to suboptimal estimation of prediction correctness, resulting in erroneous exiting decisions. To bridge the gap, we explore the necessity of effectively combining both local and global information to ensure reliable early exiting during inference. Purposefully, we leverage prototypical networks to learn class prototypes and devise a distance metric between samples and class prototypes. This enables us to utilize global information for estimating the correctness of early predictions. On this basis, we propose a novel Distance-Enhanced Early Exiting framework for BERT (DE$^3$-BERT). DE$^3
    
[^45]: 大型语言模型在代码分布转移下的不确定性意识：基准研究

    Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study

    [https://arxiv.org/abs/2402.05939](https://arxiv.org/abs/2402.05939)

    本文研究了大型语言模型在代码分布转移下的不确定性意识，并通过引入大规模基准数据集和应用概率方法来提高语言模型的可靠性。

    

    大型语言模型（LLMs）被广泛应用于编程语言分析，以提高人类生产力。然而，它们的可靠性可能会受到各种代码分布转移的影响，导致输出不一致。尽管众所周知，概率方法通过不确定性校准和估计可以减轻此类影响，但与其在基于图像的任务中的应用相比，它们在语言领域的效果尚未得到充分探索。在这项工作中，我们首先引入了一个大规模的基准数据集，其中包含三种代码分布转移的实际模式，强度各异。然后，我们对CodeLlama应用最先进的概率方法进行了全面调查。我们观察到，这些方法通常可以提高CodeLlama对不确定性的意识，提高了校准质量和更高的不确定性估计精度。然而，我们的研究进一步揭示了不同标准下的性能动态。

    Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e
    
[^46]: LB-KBQA: 基于大语言模型和BERT的基于知识的问答系统

    LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System

    [https://arxiv.org/abs/2402.05130](https://arxiv.org/abs/2402.05130)

    LB-KBQA是一种基于大语言模型和BERT的基于知识的问答系统，通过生成式人工智能的帮助，能够提高意图识别的性能和解决语言多样性的问题。

    

    生成式人工智能（AI）因其新兴的能力而赋予了各个领域的力量，其中一个典型的应用领域是大语言模型（LLMs）。与传统的基于AI的方法相比，生成式AI的典型应用领域之一是大语言模型（LLMs），并且在自然语言理解能力方面，LLM的能力得到了显著提高。自然语言理解能力一直以来都是基于知识的问答系统意图识别性能的障碍，这源自语言多样性和新出现的意图。传统的基于AI的意图识别方法可以分为基于语义解析的方法和基于模型的方法。然而，这两种方法都在意图识别方面受到有限的资源限制。为了解决这个问题，我们提出了一种基于大语言模型（LLM）和BERT的新型KBQA系统（LB-KBQA）。在生成式AI的帮助下，我们提出的方法可以检测到……（略）

    Generative Artificial Intelligence (AI), because of its emergent abilities, has empowered various fields, one typical of which is large language models (LLMs). One of the typical application fields of Generative AI is large language models (LLMs), and the natural language understanding capability of LLM is dramatically improved when compared with conventional AI-based methods. The natural language understanding capability has always been a barrier to the intent recognition performance of the Knowledge-Based-Question-and-Answer (KBQA) system, which arises from linguistic diversity and the newly appeared intent. Conventional AI-based methods for intent recognition can be divided into semantic parsing-based and model-based approaches. However, both of the methods suffer from limited resources in intent recognition. To address this issue, we propose a novel KBQA system based on a Large Language Model(LLM) and BERT (LB-KBQA). With the help of generative AI, our proposed method could detect 
    
[^47]: PaDeLLM-NER：大型语言模型中的并行解码用于命名实体识别

    PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition

    [https://arxiv.org/abs/2402.04838](https://arxiv.org/abs/2402.04838)

    本研究提出了PaDeLLM-NER，一种能够在大型语言模型中实现并行解码，从而显著减少命名实体识别的生成延迟，同时保持预测质量和性能。

    

    本研究旨在使用大型语言模型（LLMs）减少命名实体识别（NER）的生成延迟。LLMs的高延迟的主要原因是顺序解码过程，该过程自回归地生成NER的所有标签和提及，显著增加了序列长度。为此，我们引入了PaDeLLM-NER（Parallel Decoding in LLM for NE），这是一种无需额外模块或架构修改即可无缝集成到现有生成模型框架中的方法。PaDeLLM-NER允许同时解码所有提及，从而减少生成延迟。实验结果显示，PaDeLLM-NER的推理速度显著提高，对英语和中文来说比自回归方法快1.76到10.22倍。与各种数据集上的最先进性能相媲美，同时维持了预测质量。

    In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
    
[^48]: 2024年大规模语言模型的真实性

    Factuality of Large Language Models in the Year 2024

    [https://arxiv.org/abs/2402.02420](https://arxiv.org/abs/2402.02420)

    本文调查了大规模语言模型（LLM）的真实性问题，并对其现有研究进行了批判性分析，指出了改进LLM真实性的挑战和解决方案，以及自动真实性评估的障碍。未来的研究应该关注在这些方面的进一步工作。

    

    大规模语言模型（LLMs），尤其是在聊天方面进行指导调整后，已经成为我们日常生活的一部分，通过在一个地方直接回答各种问题，使人们从搜索、提取和整合多个信息源的过程中得到解脱。然而，很多情况下，LLM的回答是错误的，这限制了它们在现实场景中的适用性。因此，对于评估和提高LLM真实性的研究近年来引起了很多关注。在这项调查中，我们对现有的研究进行了批判性分析，旨在找出主要挑战及其原因，并指出改进LLM真实性的潜在解决方案，以及分析开放文本生成的自动真实性评估面临的障碍。我们还展望了未来研究的方向。

    Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.
    
[^49]: 为什么使用大型语言模型解决多智能体路径规划尚未成功

    Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet

    [https://arxiv.org/abs/2401.03630](https://arxiv.org/abs/2401.03630)

    本文研究了使用大型语言模型解决多智能体路径规划的问题。通过实验，我们发现直接使用大型语言模型解决复杂场景下的路径规划仍然存在困难。

    

    随着ChatGPT和GPT-4等大型语言模型（LLM）的成功引发的爆炸性影响，最近的许多研究表明基础模型可以用于解决各种任务。然而，在多智能体规划方面，很少有研究分享见解。多智能体规划不同于其他领域，它将多智能体协调和规划的困难结合起来，使得难以利用外部工具促进所需的推理。本文重点研究多智能体路径规划（MAPF）问题，也被称为多机器人路径规划，并研究了使用LLM解决MAPF的性能。我们首先展示了在没有障碍物的空房间地图上的激励性成功，然后展示了对标准MAPF基准测试中较难的房间地图和迷宫地图的规划失败。我们阐述了直接使用LLM解决MAPF尚未成功的立场，并通过各种实验来支撑我们的观点。

    With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark. We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to supp
    
[^50]: 知识蒸馏用于科学教育评估的LLM的自动评分

    Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments

    [https://arxiv.org/abs/2312.15842](https://arxiv.org/abs/2312.15842)

    本研究提出了一种将LLM的知识蒸馏为更小、更高效、更准确的神经网络的方法，在资源受限设备上部署具有挑战性。通过使用LLM的预测概率作为软标签训练较小的学生模型，并使用专门定制的损失函数，保证学生模型与教师模型的性能非常相似。实验证明此方法在科学教育评估中具有良好的准确性。

    

    本研究提出了一种方法，用于将精调的大型语言模型（LLMs）的知识蒸馏为更小、更高效、更准确的神经网络。我们特别针对在资源受限设备上部署这些模型的挑战。我们的方法包括使用LLM的预测概率（作为软标签）来训练较小的学生模型（神经网络），LLM充当教师模型。这通过一个专门为了从LLM的输出概率中学习而定制的损失函数实现，以确保学生模型与教师的性能非常相似。为了验证知识蒸馏方法的性能，我们使用了一个包含6,684个学生对科学问题的写作回答和三个人工专家评分的数学推理数据集的大型数据集7T。我们将准确性与最先进的知识蒸馏模型TinyBERT和人工神经网络（ANN）模型进行了比较。结果表明

    This study proposes a method for knowledge distillation (KD) of fine-tuned Large Language Models (LLMs) into smaller, more efficient, and accurate neural networks. We specifically target the challenge of deploying these models on resource-constrained devices. Our methodology involves training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model. This is achieved through a specialized loss function tailored to learn from the LLM's output probabilities, ensuring that the student model closely mimics the teacher's performance. To validate the performance of the KD approach, we utilized a large dataset, 7T, containing 6,684 student-written responses to science questions and three mathematical reasoning datasets with student-written responses graded by human experts. We compared accuracy with state-of-the-art (SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models. Results have shown 
    
[^51]: MAIRA-1：一种专门用于放射学报告生成的大型多模态模型

    MAIRA-1: A specialised large multimodal model for radiology report generation

    [https://arxiv.org/abs/2311.13668](https://arxiv.org/abs/2311.13668)

    MAIRA-1是一种专门用于放射学报告生成的大型多模态模型，在与预训练的视觉编码器对齐和文本数据增强的基础上，利用了CXR特定的图像编码器和经过微调的大型语言模型，生成具有最先进质量的报告。

    

    我们提出了一种放射学特定的多模态模型，用于从胸部X光（CXR）生成放射学报告的任务。我们的工作基于一个思想，即可以通过与预训练视觉编码器对齐，使大型语言模型具备多模态能力。在自然图像上，这已被证明可以使多模态模型获得图像理解和描述能力。我们提出的模型（MAIRA-1）利用了一个CXR特定的图像编码器，结合基于Vicuna-7B的微调的大型语言模型，并进行基于文本的数据增强，以产生具有最先进质量的报告。特别地，MAIRA-1在与放射科医生对齐的RadCliQ度量和考虑的所有词汇度量上都有显著改进。对模型输出的手动审核显示出了产生报告的流畅性和准确性，同时揭示了现有评估方法所未捕捉到的失败模式。更多信息和资源可在项目网站上找到：

    We present a radiology-specific multimodal model for the task for generating radiological reports from chest X-rays (CXRs). Our work builds on the idea that large language model(s) can be equipped with multimodal capabilities through alignment with pre-trained vision encoders. On natural images, this has been shown to allow multimodal models to gain image understanding and description capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image encoder in conjunction with a fine-tuned large language model based on Vicuna-7B, and text-based data augmentation, to produce reports with state-of-the-art quality. In particular, MAIRA-1 significantly improves on the radiologist-aligned RadCliQ metric and across all lexical metrics considered. Manual review of model outputs demonstrates promising fluency and accuracy of generated reports while uncovering failure modes not captured by existing evaluation practices. More information and resources can be found on the project website:
    
[^52]: AutoPlanBench: 从PDDL自动生成LLM规划器的基准测试

    AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL

    [https://arxiv.org/abs/2311.09830](https://arxiv.org/abs/2311.09830)

    AutoPlanBench是一种新方法，可以自动转换PDDL规划基准测试为文本描述，并提供了相应的基准测试数据集。研究表明，当前最好的LLM规划器在某些规划任务上表现优秀，但对于其他任务来说仍存在挑战。

    

    LLMs（逻辑-概率模型）在规划任务中的应用越来越广泛，但是它们在规划和推理方面的能力尚不明确。我们提出了AutoPlanBench，一种将PDDL中的规划基准测试自动转换为文本描述的新方法，并提供了使用我们方法创建的基准测试数据集。我们展示了最好的LLM规划器在某些规划任务上表现良好，但其他任务仍然超出了当前方法的能力范围。

    LLMs are being increasingly used for planning-style tasks, but their capabilities for planning and reasoning are poorly understood. We present AutoPlanBench, a novel method for automatically converting planning benchmarks written in PDDL into textual descriptions and offer a benchmark dataset created with our method. We show that while the best LLM planners do well on some planning tasks, others remain out of reach of current methods.
    
[^53]: 大型语言模型中的结构化化学推理

    Structured Chemistry Reasoning with Large Language Models

    [https://arxiv.org/abs/2311.09656](https://arxiv.org/abs/2311.09656)

    该论文研究了大型语言模型在化学领域的复杂科学推理困难，发现错误通常源于缺乏有效的推理结构。基于此，引入了一种简单而有效的提示策略StructChem，大幅提升了语言模型的性能。

    

    大型语言模型（LLMs）在各个领域表现出色，但在化学领域的复杂科学推理方面存在困难。与以往研究中涉及的简单化学任务（例如分子分类）不同，复杂的化学问题不仅需要广博的知识和精确的计算，还需要关于不同概念（例如温度变化）的丰富动态相互作用的组合推理。我们的研究表明，即使是像GPT-4这样先进的LLMs也很容易出现错误。有趣的是，这些错误通常不是由于LLMs缺乏领域知识，而是由于缺乏有效的推理结构来引导LLMs引发正确的知识，将知识融入逐步推理中，并迭代改进结果以进一步提高质量。基于此，我们引入了一种简单而有效的提示策略——结构化化学（StructChem），它提供了所需的指导，并显著提升了LLMs的性能。

    Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes). Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways. Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality. On this basis, we introduce StructChem, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs
    
[^54]: Sorted LLaMA: 揭示大型语言模型中间层的潜力，用于动态推理

    Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference

    [https://arxiv.org/abs/2309.08968](https://arxiv.org/abs/2309.08968)

    Sorted LLaMA通过扩展SortedNet到生成NLP任务，使得大型语言模型在动态推理中更高效，并且不需要预训练，只需将标准微调替换为排序微调即可。该方法可以释放transformers中间层的潜力，同时最小化存储需求和过渡成本。

    

    大型语言模型（LLMs）通过在理解和生成类似人类文本方面表现出色，为自然语言处理（NLP）领域带来了革命。然而，广泛部署这些模型可能成本过高。SortedNet是一种最近的训练技术，通过利用网络中的模块化和基于计算/准确性对子模型进行嵌套排序，实现了动态推理。我们将SortedNet扩展到生成NLP任务，使大型语言模型在不进行任何预训练的情况下变得动态，仅通过将标准微调（SFT）替换为排序微调（SoFT）。我们的方法提高了模型的效率，消除了在推理过程中在不同场景中使用多个模型的需求。我们展示了这种方法可以释放transformers中间层在生成目标输出方面的能力。我们的子模型仍然是原始模型的组成部分，最小化了存储需求和在不同计算/延迟预算之间的过渡成本。

    Large language models (LLMs) have revolutionized natural language processing (NLP) by excelling at understanding and generating human-like text. However, their widespread deployment can be prohibitively expensive. SortedNet is a recent training technique for enabling dynamic inference by leveraging the modularity in networks and sorting sub-models based on computation/accuracy in a nested manner. We extend SortedNet to generative NLP tasks, making large language models dynamic without any Pre-Training and by only replacing Standard Fine-Tuning (SFT) with Sorted Fine-Tuning (SoFT). Our approach boosts model efficiency, eliminating the need for multiple models for various scenarios during inference. We show that this approach can unlock the power of intermediate layers of transformers in generating the target output. Our sub-models remain integral components of the original model, minimizing storage requirements and transition costs between different computational/latency budgets. The ef
    
[^55]: 将概率逻辑推理教给变压器

    Teaching Probabilistic Logical Reasoning to Transformers

    [https://arxiv.org/abs/2305.13179](https://arxiv.org/abs/2305.13179)

    本文评估了基于变压器的语言模型在推理不确定文本上的能力，并提出了一种概率约束训练（PCT）的方法来提高模型的概率逻辑推理能力。

    

    在本文中，我们评估了基于变压器的语言模型在推理不确定的文本上的能力，其中包括不确定的推理规则。我们涵盖了预训练语言模型（PLMs）和生成大型语言模型（LLMs）。我们的评估结果表明，这两代语言模型在推理不确定文本方面都存在困难。我们提出了一种新颖的端到端微调方法，概率约束训练（PCT），它在微调阶段利用概率逻辑规则作为约束，而不依赖这些规则在推理阶段。为了评估PCT的有效性，我们利用相关语料库，并额外创建了一个更具挑战性的基准测试，与之前的测试不同，它使用了实例特定的规则。我们的研究表明，PCT提高了基于变压器的语言模型的内在推理能力，使其概率逻辑推理过程更明确和可解释。此外，PCT与传统方法相比，在不确定的文本上取得了更好的性能。

    In this paper, we evaluate the capability of transformer-based language models in making inferences over uncertain text that includes uncertain rules of reasoning. We cover both Pre-trained Language Models (PLMs) and generative Large Language Models (LLMs). Our evaluation results show that both generations of language models struggle with reasoning over uncertain text. We propose a novel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT), that utilizes probabilistic logical rules as constraints in the fine-tuning phase without relying on these rules in the inference stage. To assess the effectiveness of PCT, we utilize the related corpora and, additionally, create a new and more challenging benchmark that, unlike the previous ones, uses instance-specific rules. Our study demonstrates that PCT improves the transformer-based language model's intrinsic reasoning and makes their probabilistic logical reasoning process more explicit and explainable. Furthermore, PCT eq
    
[^56]: 量化大型语言模型的关联能力及其对隐私泄露的影响

    Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage

    [https://arxiv.org/abs/2305.12707](https://arxiv.org/abs/2305.12707)

    本文研究了大型语言模型的关联能力，并揭示了其对隐私泄露的影响。研究发现，随着模型规模的增加，模型在关联实体/信息方面的能力增强。然而，与常识知识相比，模型在关联个人可识别信息方面的准确性较低。

    

    大型语言模型（LLMs）的进步在各种应用中带来了显著的改进，与此同时也引发了对潜在私人数据泄露的担忧。其中一个显著的LLMs能力是它们能够形成不同信息之间的关联，但这在涉及个人可识别信息（PII）时引发了担忧。本文深入研究了语言模型的关联能力，旨在揭示影响其关联信息能力的因素。我们的研究发现，随着模型规模的扩大，其关联实体/信息的能力增强，特别是当目标对展示更短的共现距离或更高的共现频率时。然而，在关联常识知识与PII方面存在明显的性能差距，后者的准确性较低。尽管准确预测PII的比例相对较小，但LLMs仍然表现出了这种能力。

    The advancement of large language models (LLMs) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. One notable capability of LLMs is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (PII). This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy. Despite the proportion of accurately predicted PII being relatively small, LLMs still demonstrate the capab
    
[^57]: ALEXSIS-PT：一种新的用于葡萄牙语词汇简化的资源

    ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification

    [https://arxiv.org/abs/2209.09034](https://arxiv.org/abs/2209.09034)

    ALEXSIS-PT是一个用于巴西葡萄牙语词汇简化的新型多候选数据集，为LS系统的改进和跨语言模型的研究提供了重要资源。BERTimbau在该数据集上达到了最高的性能。

    

    词汇简化（LS）是自动替换复杂词汇为更容易理解的词汇的任务，使文本对各种目标人群（如低识字率的个体、学习障碍个体、第二语言学习者）更易于访问。为了训练和测试模型，LS系统通常需要包含复杂词汇及其候选替代词的语料库。为了进一步提高LS系统的性能，我们介绍了ALEXSIS-PT，这是一个针对巴西葡萄牙语LS的新型多候选数据集，其中包含387个复杂词汇的9605个候选替代词。ALEXSIS-PT是按照ALEXSIS协议编制的，用于西班牙语，为跨语言模型开辟了新的研究方向。ALEXSIS-PT是第一个包含巴西报纸文章的LS多候选数据集。我们在该数据集上评估了四种替代生成模型，分别是mDistilBERT、mBERT、XLM-R和BERTimbau。BERTimbau在所有评估指标上取得了最高的性能。

    Lexical simplification (LS) is the task of automatically replacing complex words for easier ones making texts more accessible to various target populations (e.g. individuals with low literacy, individuals with learning disabilities, second language learners). To train and test models, LS systems usually require corpora that feature complex words in context along with their candidate substitutions. To continue improving the performance of LS systems we introduce ALEXSIS-PT, a novel multi-candidate dataset for Brazilian Portuguese LS containing 9,605 candidate substitutions for 387 complex words. ALEXSIS-PT has been compiled following the ALEXSIS protocol for Spanish opening exciting new avenues for cross-lingual models. ALEXSIS-PT is the first LS multi-candidate dataset that contains Brazilian newspaper articles. We evaluated four models for substitute generation on this dataset, namely mDistilBERT, mBERT, XLM-R, and BERTimbau. BERTimbau achieved the highest performance across all evalu
    
[^58]: SliceGPT: 通过删除行和列来压缩大型语言模型

    SliceGPT: Compress Large Language Models by Deleting Rows and Columns. (arXiv:2401.15024v1 [cs.LG])

    [http://arxiv.org/abs/2401.15024](http://arxiv.org/abs/2401.15024)

    SliceGPT是一种新的事后训练稀疏化方案，通过将每个权重矩阵替换为较小的矩阵以减小网络的维度，可以在保持高任务性能的同时减少模型参数。

    

    大型语言模型已成为自然语言处理的基石，但使用它们需要大量计算和内存资源。稀疏化方法可以缓解这些资源限制，并且最近的研究表明训练好的模型可以进行事后的稀疏化处理。现有的稀疏化技术面临着挑战，因为它们需要额外的数据结构，并且在当前硬件上速度受限。在本文中，我们提出了一种新的事后训练稀疏化方案SliceGPT，该方案用较小的（稠密的）矩阵替换每个权重矩阵，从而减小网络的嵌入维度。通过大量的实验，我们展示了SliceGPT在保持相应稠密模型的99%、99%和90%的零-shot任务性能的同时，可以移除LLAMA2-70B、OPT 66B和Phi-2模型中多达25%的模型参数（包括嵌入）。我们的切片模型在较少的GPU上运行并且更快，无需额外的代码优化。

    Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimi
    
[^59]: 使用多方位AI反馈减轻情感支持对话中的无益性

    Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback. (arXiv:2401.05928v1 [cs.CL])

    [http://arxiv.org/abs/2401.05928](http://arxiv.org/abs/2401.05928)

    Muffin是一个新型的模型无关框架，用于减轻情感支持对话中无益性的问题。这个框架在生成支持性回复时考虑到多个因素，并通过多方位的AI反馈来训练模型，以避免生成无益的回复。

    

    情感支持对话系统旨在减轻用户的情感困扰并帮助他们解决挑战。为了生成支持性回复，必须考虑到多个因素，如共情、支持策略和回复连贯性，这些在之前的方法中已经得到验证。然而，之前的模型偶尔会生成无益的回复，这些回复意图提供支持，但却产生适得其反的效果。根据心理学和沟通理论，虽然只是单一因素的表现不佳可能会导致回复无益。从模型训练的角度来看，由于这些模型在训练阶段没有接触到无益的回复，它们无法判断它们生成的标记是否会导致推理过程中的无益回复。为了解决这个问题，我们引入了一个名为多方位AI反馈减轻情感支持（Muffin）的新型模型无关框架。

    An emotional support conversation system aims to alleviate users' emotional distress and assist them in addressing their challenges. To generate supportive responses, it is critical to consider multiple factors such as empathy, support strategies, and response coherence, as established in prior methods. Nonetheless, previous models occasionally generate unhelpful responses, which intend to provide support but display counterproductive effects. According to psychology and communication theories, poor performance in just one contributing factor might cause a response to be unhelpful. From the model training perspective, since these models have not been exposed to unhelpful responses during their training phase, they are unable to distinguish if the tokens they generate might result in unhelpful responses during inference. To address this issue, we introduce a novel model-agnostic framework named mitigating unhelpfulness with multifaceted AI feedback for emotional support (Muffin). Specif
    
[^60]: AST-T5：面向代码生成和理解的结构感知预训练模型

    AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])

    [http://arxiv.org/abs/2401.03003](http://arxiv.org/abs/2401.03003)

    AST-T5是一种结构感知的预训练模型，通过利用抽象语法树（AST）来增强代码生成、转换和理解的能力。它优于其他同等大小的语言模型，并在代码到代码任务中表现出色。

    

    大型语言模型在代码相关任务中取得了显著进展，然而许多模型将代码视为简单序列，忽略了其结构化特性。我们引入了AST-T5，一种新颖的预训练范式，利用抽象语法树（AST）增强了代码生成、转换和理解。通过动态规划，我们的AST感知分割保留了代码结构，而AST感知跨度破坏目标使模型能够重建各种代码结构。与其他模型不同，AST-T5避免了复杂的程序分析或架构更改，因此可以与任何编码器-解码器Transformer无缝集成。评估结果显示，AST-T5在各种代码相关任务中始终优于同等大小的语言模型。结构感知使得AST-T5在代码到代码任务中特别强大，在Bugs2Fix任务的精确匹配得分上超过CodeT5 2个点，并在CodeXGLUE中的Java-C#转换任务的精确匹配得分上超过CodeT5 3个点。

    Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
    
[^61]: LLaVA-$\phi$: 高效的多模态助手与小型语言模型

    LLaVA-$\phi$: Efficient Multi-Modal Assistant with Small Language Model. (arXiv:2401.02330v1 [cs.CV])

    [http://arxiv.org/abs/2401.02330](http://arxiv.org/abs/2401.02330)

    LLaVA-$\phi$是一种高效的多模态助手，使用小型语言模型Phi-2来促进多模态对话。即使具有较少的参数，它也能有效地融合文本和视觉元素，并在各种任务中表现出色。它为时间敏感的环境和需要实时交互的系统开辟了新的应用途径。

    

    在本文中，我们介绍了LLaVA-$\phi$（LLaVA-Phi），一种利用最近先进的小型语言模型Phi-2来促进多模态对话的高效多模态助手。LLaVA-Phi在紧凑的多模态模型领域中标志着重要进展。它证明了即使是个参数只有27亿的较小语言模型在训练有高质量语料库的情况下也可以有效地参与融合文本和视觉元素的复杂对话。我们的模型在包括视觉理解、推理和基于知识的感知等公开可用的基准测试中表现出色。除了在多模态对话任务中表现出卓越性能外，我们的模型为时间敏感的环境和需要实时交互的系统（如实体代理）开辟了新的应用途径。它突显了较小语言模型实现高级理解和交互的潜力。

    In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a notable advancement in the realm of compact multi-modal models. It demonstrates that even smaller language models, with as few as 2.7B parameters, can effectively engage in intricate dialogues that integrate both textual and visual elements, provided they are trained with high-quality corpora. Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception. Beyond its remarkable performance in multi-modal dialogue tasks, our model opens new avenues for applications in time-sensitive environments and systems that require real-time interaction, such as embodied agents. It highlights the potential of smaller language models to achieve sophisticated levels of understanding and inte
    
[^62]: Davidsonian场景图：改进文本-图像生成的细粒度评估的可靠性

    Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])

    [http://arxiv.org/abs/2310.18235](http://arxiv.org/abs/2310.18235)

    本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。

    

    评估文本到图像模型一直是困难的。最近一种用于评估文本-图像忠实度的强大方法是基于QG/A（问题生成和回答），它使用预训练的基础模型自动生成一组问题和答案，并基于这些答案与基于提示的答案在视觉问题回答模型中提取的一致性对输出图像进行评分。这种评估自然上取决于底层QG和QA模型的质量。我们确定并解决了现有QG/A工作中的几个可靠性挑战：（a）QG问题应尊重提示（避免幻觉、重复和遗漏）和（b）VQA答案应一致（不会在图像中宣称没有摩托车，同时声称摩托车是蓝色）。我们通过Davidsonian场景图（DSG），这个受形式语义启发的实证评估框架，解决了这些问题。

    Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
    
[^63]: 基于思维混合表示的大规模语言模型级联用于成本高效的推理

    Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])

    [http://arxiv.org/abs/2310.03094](http://arxiv.org/abs/2310.03094)

    本研究提出了一种基于思维混合表示的大规模语言模型级联方法，用于成本高效的推理。通过考虑更弱模型的答案一致性作为问题难度的信号，可以实现对问题的决策，从而节约使用更强模型的成本。

    

    大规模语言模型（LLM）如GPT-4在各种任务中展现出了非凡的性能，但是这种强大的性能通常伴随着使用付费API服务的高昂费用。本文的研究动机是为了研究构建LLM级联以节约使用LLM的成本，特别是用于进行推理（例如数学、因果推理）任务的成本。我们的级联管道遵循一个直观的思想，即简单的问题可以由一个更弱但更实惠的LLM来解决，而只有具有挑战性的问题才需要更强大、更昂贵的LLM。为了实现这种决策，我们考虑到更弱的LLM的“答案一致性”作为问题难度的信号，并提出了几种答案采样和一致性检查的方法，其中一种方法利用了两种思维表示（即连续思维和程序思维）的混合。通过在六个推理基准数据集上的实验，我们使用GPT-3.5-turbo和GPT-4作为较弱的模型，

    Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
    
[^64]: Alphazero类似的树搜索可以指导大型语言模型的解码和训练

    Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. (arXiv:2309.17179v1 [cs.LG])

    [http://arxiv.org/abs/2309.17179](http://arxiv.org/abs/2309.17179)

    Alphazero类似的树搜索框架TS-LLM可以利用学习的价值函数指导大型语言模型的解码和训练，不仅适用于推理任务，还适用于其他任务，并且在不同大小的语言模型上具有普适性和可扩展性

    

    大型语言模型 (LLM) 通常采用采样或束搜索，结合 Chain-of-Thought (CoT) 等提示来提高推理和解码能力。最近的研究如 Tree-of-Thought (ToT) 和 Reasoning via Planning (RAP) 旨在通过利用树搜索算法来引导多步推理，来增强LLM的推理能力。这些方法主要关注LLM在推理过程中的推理能力，并且严重依赖人为设计的提示来激活LLM作为一个价值函数，缺乏普适性和可扩展性。为了解决这些限制，我们提出了一种AlphaZero类似的用于LLM的树搜索框架 (称为TS-LLM)，系统地说明了如何通过学习的价值函数利用树搜索来指导LLM的解码能力。TS-LLM在两个关键方面与众不同：(1)通过利用学习的价值函数，我们的方法可以普适地应用于除了推理之外的不同任务 (例如RLHF对齐)，以及任何大小的LLM，而不需要提示

    Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting a
    
[^65]: 从一本语法书学习翻译新语言的基准测试

    A Benchmark for Learning to Translate a New Language from One Grammar Book. (arXiv:2309.16575v1 [cs.CL])

    [http://arxiv.org/abs/2309.16575](http://arxiv.org/abs/2309.16575)

    本研究提出了一种从一本语法书中学习翻译新语言的基准测试MTOB，用于翻译英语和Kalamang之间的文本，探索了低资源语言情况下的翻译问题。结果显示，现有的大型语言模型在这个任务上表现有限。

    

    大型语言模型(LLMs)可以通过上下文学习或轻量级微调来完成令人印象深刻的任务。人们自然而然地想知道这些模型在适应全新任务时的表现如何，但如何找到在互联网规模的训练数据集中未见过的任务呢？我们转向一个明确受到网络数据稀缺性的驱动和限制的领域：低资源语言。在本文中，我们引入了一种名为MTOB（从一本书进行机器翻译）的基准测试，用于学习在英语和Kalamang之间翻译--Kalamang是一种只有不到200名使用者的语言，因此在网络上几乎没有存在感--我们使用了几百页的田野语言学参考资料。这种任务框架的新颖之处在于，它要求模型从一本人类可读的语法解释书中学习一种语言，而不是从大规模挖掘的领域内数据语料库中学习，更类似于L2学习而不是L1习得。我们证明，使用当前LLMs的基准测试具有很大的潜力，但仍然不及人类表现。

    Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang -- a language with less than 200 speakers and therefore virtually no presence on the web -using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human perform
    
[^66]: 语言模型在与知识库进行连接时的数据分布瓶颈

    Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases. (arXiv:2309.08345v1 [cs.CL])

    [http://arxiv.org/abs/2309.08345](http://arxiv.org/abs/2309.08345)

    本文通过实验调查揭示了语言模型在与知识库进行连接时的数据分布瓶颈，包括推广到未见域、适应语言变体和在不同数据集之间的可转移性等方面。即使采用数据增强技术，先进的语言模型在多个方面表现出较差的性能。

    

    语言模型（LM）已经展示了在理解和生成自然语言和形式语言方面的卓越能力。尽管取得了这些进展，但它们与大规模知识库等现实环境的整合仍然是一个欠发展的领域，影响了语义解析等应用，并且容易出现“产生虚假信息”的问题。本文通过实验调查揭示了LM在处理知识库问答（KBQA）任务时所遇到的健壮性挑战。研究覆盖了训练和推断之间数据分布不一致的场景，例如推广到未见域、适应各种语言变体和在不同数据集之间的可转移性。我们的全面实验揭示了即使在采用我们提出的数据增强技术的情况下，先进的小型和大型语言模型在多个方面表现出较差的性能。

    Language models (LMs) have already demonstrated remarkable abilities in understanding and generating both natural and formal language. Despite these advances, their integration with real-world environments such as large-scale knowledge bases (KBs) remains an underdeveloped area, affecting applications such as semantic parsing and indulging in "hallucinated" information. This paper is an experimental investigation aimed at uncovering the robustness challenges that LMs encounter when tasked with knowledge base question answering (KBQA). The investigation covers scenarios with inconsistent data distribution between training and inference, such as generalization to unseen domains, adaptation to various language variations, and transferability across different datasets. Our comprehensive experiments reveal that even when employed with our proposed data augmentation techniques, advanced small and large language models exhibit poor performance in various dimensions. While the LM is a promisin
    
[^67]: 使用LLMs进行生成式数据增强提高问答中的分布鲁棒性

    Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering. (arXiv:2309.06358v1 [cs.CL])

    [http://arxiv.org/abs/2309.06358](http://arxiv.org/abs/2309.06358)

    本论文研究了使用生成式数据增强方法如何提高问答模型在自然分布转换下的鲁棒性，通过实验展示了增强阅读理解数据集的效果。

    

    自然语言处理中的鲁棒性问题仍然是一个重要的问题，最先进的模型在自然分布转换下表现不佳。在问答环境中，对领域适应方法的研究工作仍在不断发展。然而，在自然分布转换下的域泛化概念却受到很少关注，因为目标域是未知的。随着生成模型质量和获取方式的大幅提高，我们回答了一个问题：生成的数据集如何影响问答模型在自然分布转换下的性能？我们在4个不同数据集上进行了实验，分析了“野外生成”如何帮助实现域泛化。我们采取了两步生成方法，生成上下文和问答对来增强现有数据集。通过我们的实验，我们展示了如何通过增强阅读理解数据集来提升领域泛化能力。

    Robustness in Natural Language Processing continues to be a pertinent issue, where state of the art models under-perform under naturally shifted distributions. In the context of Question Answering, work on domain adaptation methods continues to be a growing body of research. However, very little attention has been given to the notion of domain generalization under natural distribution shifts, where the target domain is unknown. With drastic improvements in the quality and access to generative models, we answer the question: How do generated datasets influence the performance of QA models under natural distribution shifts? We perform experiments on 4 different datasets under varying amounts of distribution shift, and analyze how "in-the-wild" generation can help achieve domain generalization. We take a two-step generation approach, generating both contexts and QA pairs to augment existing datasets. Through our experiments, we demonstrate how augmenting reading comprehension datasets wit
    
[^68]: LLM在Shell中的应用：生成式蜜罐

    LLM in the Shell: Generative Honeypots. (arXiv:2309.00155v1 [cs.CR])

    [http://arxiv.org/abs/2309.00155](http://arxiv.org/abs/2309.00155)

    本研究引入了一种基于大型语言模型的新方法来创建动态和真实的软件蜜罐，解决了以往蜜罐的重要局限性，并通过实验验证了其高准确率。

    

    蜜罐是网络安全中的重要工具。然而，大多数蜜罐（即使是高交互式的）缺乏足够的真实感来欺骗攻击者。这个限制使得它们很容易被识别，从而影响到它们的有效性。本研究引入了一种基于大型语言模型的新方法来创建动态和真实的软件蜜罐。初步结果表明，LLM能够创建可信且动态的蜜罐，能够解决以往蜜罐的重要局限性，如确定性响应、缺乏适应性等。我们通过与需要判断蜜罐回应是否虚假的攻击者进行实验来评估每个命令的真实性。我们提出的蜜罐，称为shelLM，达到了0.92的准确率。

    Honeypots are essential tools in cybersecurity. However, most of them (even the high-interaction ones) lack the required realism to engage and fool human attackers. This limitation makes them easily discernible, hindering their effectiveness. This work introduces a novel method to create dynamic and realistic software honeypots based on Large Language Models. Preliminary results indicate that LLMs can create credible and dynamic honeypots capable of addressing important limitations of previous honeypots, such as deterministic responses, lack of adaptability, etc. We evaluated the realism of each command by conducting an experiment with human attackers who needed to say if the answer from the honeypot was fake or not. Our proposed honeypot, called shelLM, reached an accuracy rate of 0.92.
    
[^69]: SciBench: 对大型语言模型评估大学水平的科学问题解决能力

    SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])

    [http://arxiv.org/abs/2307.10635](http://arxiv.org/abs/2307.10635)

    这篇论文介绍了一个名为SciBench的基准套件，旨在对大型语言模型的大学水平科学问题解决能力进行评估。研究结果显示，当前的语言模型在提供复杂科学问题解决能力方面还有不足之处。

    

    最近大型语言模型(LLMs)的进展在许多数学基准上取得了显著的进步。然而，这些基准大多只包含初高中科目的问题，仅包含多项选择题，并且仅限于基本算术运算范围。为了解决这些问题，本文介绍了一个广泛的基准套件SciBench，旨在系统地检测复杂科学问题解决所需的推理能力。SciBench包含两个经过精心策划的数据集：一个开放集，包括从数学、化学和物理教科书中摘录的大学水平的科学问题，以及一个封闭集，包含来自计算机科学和数学本科考试的问题。基于这两个数据集，我们对两个代表性的LLM进行了深入的基准研究，并采用不同的提示策略。结果表明，当前的LLMs在提供复杂科学问题解决能力方面还存在不足之处。

    Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
    
[^70]: 通过减少暴露偏差实现大型语言模型的自我信息更新

    Self Information Update for Large Language Models through Mitigating Exposure Bias. (arXiv:2305.18582v1 [cs.CL])

    [http://arxiv.org/abs/2305.18582](http://arxiv.org/abs/2305.18582)

    本文提出了一种使用信息丰富的文本语料库来帮助现有的大型语言模型进行自我信息更新的方法，有效减轻了暴露偏差的影响。

    

    当今的大型语言模型在各种信息请求方面展示了卓越的能力。然而，这些模型受其预训练语料库中最新数据的限制，使其无法提供最新的信息。从头开始对大型语言模型进行重新训练代价较高，并且对新语料库进行连续微调的效果尚未得到全面检查。此外，目前的更新程序通常需要大量人力投入，将信息准备成更结构化的形式，如知识三元组、对话数据或带有人类反馈的响应。在本研究中，我们对大型语言模型中的自我信息更新任务进行了全面的研究，这只需要提供信息丰富的文本语料库。例如，我们可以使用最新的新闻文章来更新LLM的现有知识。我们定义了自我信息更新任务，并评估了连续微调方法在此任务中的效果。

    Current LLMs have demonstrated remarkable capabilities in addressing users' requests for various types of information. However, these models are limited by the most recent data available in their pretraining corpora, rendering them incapable of providing up-to-date information. Retraining LLMs from scratch is cost-prohibitive, and the effectiveness of continual fine-tuning on new corpora has not been thoroughly examined. Additionally, current update procedures typically demand significant human input to prepare the information into more structured format, such as knowledge triples, conversational data or responses with human feedback. In this study, we conduct a comprehensive examination of a novel self information update task in LLMs, which only requires the provision of informative text corpora. For instance, we can use the latest news articles to update the LLMs' existing knowledge. We define the self information update task and assess the continual fine-tuning approach for this pur
    

