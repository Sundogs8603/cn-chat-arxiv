# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Machine Translation Models are Zero-Shot Detectors of Translation Direction.](http://arxiv.org/abs/2401.06769) | 本文探索了一种基于无监督方法的翻译方向检测，并通过实验证实其在高负载语言对上的有效性。论文标题为“Machine Translation Models are Zero-Shot Detectors of Translation Direction”。 |
| [^2] | [APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding.](http://arxiv.org/abs/2401.06761) | 这项工作介绍了一种名为APAR的并行自回归生成方法，通过对数据进行调整，使得LLMs能够独立规划生成过程以及执行自动并行自回归（APAR）生成，从而显著减少了生成步骤的数量，并在高吞吐量场景中实现了吞吐量增加和延迟降低。 |
| [^3] | [Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies.](http://arxiv.org/abs/2401.06760) | 本文研究了一系列现代度量标准的“动态范围”，以提供对度量标准之间和内部得分差异的共同理解，并通过人类感知水平的系统差异进行衡量。 |
| [^4] | [Stylometry Analysis of Multi-authored Documents for Authorship and Author Style Change Detection.](http://arxiv.org/abs/2401.06752) | 本文研究了多作者文献的文体分析，包括文体分类、单作者变化检测和多作者变化检测。我们提出了一个基于价值的融合框架，并整合了多种自然语言处理算法和权重优化技术。 |
| [^5] | [The Unreasonable Effectiveness of Easy Training Data for Hard Tasks.](http://arxiv.org/abs/2401.06751) | 当困难训练数据很难正确标记时，当前的语言模型通常能够相对良好地从易到难的数据泛化，并且即使在关注于困难数据的性能时，收集和训练易数据可能比困难数据更好。 |
| [^6] | [Using Natural Language Inference to Improve Persona Extraction from Dialogue in a New Domain.](http://arxiv.org/abs/2401.06742) | 本研究通过使用自然语言推理方法，提出了一种后期适应已训练的角色提取模型到新领域中的方法，以解决对话中角色提取的多样性和非真实世界设置的问题。 |
| [^7] | [Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty.](http://arxiv.org/abs/2401.06730) | 本研究调查了语言模型在回答问题时不愿表达不确定性的影响，发现语言模型往往过于自信，导致高错误率。实验还表明用户无论是否标记了确定性都会严重依赖语言模型生成的结果。 |
| [^8] | [Reframing Tax Law Entailment as Analogical Reasoning.](http://arxiv.org/abs/2401.06715) | 本论文将法定推理重新定义为类比任务，通过增加数据集大小和引入解释性因素，展示了这个任务与原始任务的难度相当，并利用检索机制和类比模型解决法定推理问题，在之前的可比工作上取得了一些进展。 |
| [^9] | [Few-Shot Detection of Machine-Generated Text using Style Representations.](http://arxiv.org/abs/2401.06712) | 本研究提出了一种小样本检测方法，通过使用样式表示来检测机器生成的文本与人类撰写的文本的区别，以解决滥用语言模型带来的问题。 |
| [^10] | [Reliability Analysis of Psychological Concept Extraction and Classification in User-penned Text.](http://arxiv.org/abs/2401.06709) | 该论文研究了在用户撰写的文本中心理概念提取和分类的可靠性分析，并通过注解LoST数据集来捕捉表明低自尊存在的微妙文本提示。研究发现，NLP模型对触发词、LoST指标和后果这三类文本提示更加关注。 |
| [^11] | [Multi-Candidate Speculative Decoding.](http://arxiv.org/abs/2401.06706) | 本研究提出了一种多候选推测解码的方法，通过从草稿模型中采样多个候选标记并进行批次验证，显著提高了接受率并优于标准推测解码方法。 |
| [^12] | [An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models.](http://arxiv.org/abs/2401.06692) | 该论文提出了一个实验设计框架来减少大型语言模型有限标签监督微调的注释成本，并解决了主动学习的计算瓶颈问题。 |
| [^13] | [Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation.](http://arxiv.org/abs/2401.06688) | 本文介绍了一种利用质量估计指标合并机器翻译假设的方法，该方法在大型语言模型和多语言翻译模型上提升了翻译质量。通过使用候选池和QE指标，我们的方法能够生成多样且准确的翻译结果。 |
| [^14] | [Proximal Causal Inference With Text Data.](http://arxiv.org/abs/2401.06687) | 本论文提出了一种使用文本数据进行近因果推断的方法，通过将文本数据分割并使用零样本模型推断出代理变量，然后应用于近邻 g-formula，从而解决了混淆变量完全未观察到的情况。实验结果表明该方法产生了低偏差的估计值。 |
| [^15] | [DQNC2S: DQN-based Cross-stream Crisis event Summarizer.](http://arxiv.org/abs/2401.06683) | 本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。 |
| [^16] | [PolyTOPS: Reconfigurable and Flexible Polyhedral Scheduler.](http://arxiv.org/abs/2401.06665) | PolyTOPS是一个可配置的多面体调度器，可以针对不同架构、并行性模型或应用场景进行循环优化。 |
| [^17] | [WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge.](http://arxiv.org/abs/2401.06659) | 本文提出了一个名为智慧M的插件框架，利用从大型视觉语言模型中产生的上下文世界知识来改进多模态情感分析，实验证明该方法在不同任务上有着显著的改进。 |
| [^18] | [Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation.](http://arxiv.org/abs/2401.06643) | 本研究评估了三种文本多样性激励方法对LLM文本增强中生成文本的词汇多样性和下游模型性能的影响。结果表明，使用禁忌词能够最大程度地增加多样性，而使用先前创建的改写作为提示时，下游模型的性能最高。 |
| [^19] | [Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently.](http://arxiv.org/abs/2401.06640) | 本研究通过控制实验环境的方式，发现语言模型在属性继承任务中表现出了一定的非平凡能力，但这种能力是不一致的。 |
| [^20] | [OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models.](http://arxiv.org/abs/2401.06628) | 本研究提出了一种面向对象编程的新型评估基准，包括431个Python程序，采用pass@o度量指标来提供更全面和相关的OOP代码生成评估。评估结果显示代码专用LLMs在OOP方面表现较差，需进一步改进此领域。 |
| [^21] | [TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models.](http://arxiv.org/abs/2401.06620) | 本论文提出了TransliCo，一个对比学习框架，用于解决多语言预训练语言模型中的脚本障碍。通过对比不同脚本的句子及其在统一脚本中的音译，实现了不同脚本的统一表示空间。实验证明，这种方法能够改善跨语言传递的性能。 |
| [^22] | [Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study.](http://arxiv.org/abs/2401.06603) | 通过双向反馈机制，这个研究探索了大型语言模型(LLMs)和强化学习模型的合作。LLM充当教师，强化学习模型充当学生，它们通过递归互助实现了相互协助。这种合作提供了高级信息和实时反馈，促进了优化。 |
| [^23] | [Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation.](http://arxiv.org/abs/2401.06591) | Prometheus-Vision是一个开源的视觉语言模型评估器，使用了一个名为Perception Collection的反馈数据集，训练出的模型能够理解用户定义的评分标准，与人类评估员和GPT-4V之间显示出最高的相关性。 |
| [^24] | [Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation.](http://arxiv.org/abs/2401.06583) | 本研究通过使用预训练的变形器模型和映射方法，探索了跨语言文档表示的方法。实验结果表明，通过映射到跨语言领域的变形器技术文档表示（TLDRs），能够有效地实现跨语言的推荐系统。 |
| [^25] | [Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation.](http://arxiv.org/abs/2401.06568) | 本论文研究了大型语言模型（LLMs）如何利用源语言和参考信息评估机器翻译的质量，并发现参考信息显著提高了评估准确性，而源语言信息有时会适得其反，表明在使用LLMs评估翻译时存在跨语言能力不足的问题。 |
| [^26] | [Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender.](http://arxiv.org/abs/2401.06561) | 本研究提出了一种名为Intention Analysis Prompting (IAPrompt)的方法，通过触发大型语言模型（LLMs）的自我纠正和改进能力来防御越狱攻击。实验证明，该方法能够显著减少响应中的有害行为并保持整体有用性。 |
| [^27] | [Medical Dialogue Generation via Intuitive-then-Analytical Differential Diagnosis.](http://arxiv.org/abs/2401.06541) | 本研究提出了一种医疗对话生成框架，通过直觉-分析式鉴别诊断（IADDx）实现了对医疗对话的生成。该方法使用直觉和分析推理来建立鉴别诊断，并通过图增强的分析方法进行细化，提高了医疗对话系统的实际应用价值。 |
| [^28] | [INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning.](http://arxiv.org/abs/2401.06532) | 本研究探索了指令调优的方法，以增强大型语言模型在信息检索任务中的能力，通过引入一个新的指令调优数据集INTERS，涵盖了21个IR任务，该方法显著提升了性能。 |
| [^29] | [MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection.](http://arxiv.org/abs/2401.06526) | MetaHate是一个用于统一对抗仇恨言论检测的数据集，对现有收集进行了详细研究，强调了它们的优点和局限性，有助于更深入地了解现有数据集，为训练更强大和适应性更强的模型铺平了道路。 |
| [^30] | [AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions.](http://arxiv.org/abs/2401.06509) | 本研究通过使用桌面角色扮演游戏规则创建了一个环境，量化评估智能体社交互动的信息性和表达性，旨在克服隐私问题并促使智能体进行有意义、高质量的互动。 |
| [^31] | [An investigation of structures responsible for gender bias in BERT and DistilBERT.](http://arxiv.org/abs/2401.06495) | 本文研究了BERT和DistilBERT中负责性别偏见的结构，讨论了它们在预测中的公正性问题。 |
| [^32] | [Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation.](http://arxiv.org/abs/2401.06477) | Kun是一种使用指令反向翻译和答案优化的方法，用于创建高质量的指导调整数据集，该方法不依赖于手动注释，通过自我筛选过程来改善和选择最有效的指令-输出对。它的主要创新在于通过算法改进提高数据的保留和清晰度，并通过创新的数据生成方法减少了手动注释的依赖。 |
| [^33] | [Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning.](http://arxiv.org/abs/2401.06469) | 本文提出了批处理ICL方法，通过将ICL视为一个元优化过程，开发出了一个有效、高效且无序的推理算法。通过聚合元梯度并将其应用于零-shot学习，该方法使LLM对ICL示例顺序无关，并且在实验证明其在大多数情况下优于其他排列方式，甚至超过了标准ICL的最佳顺序的性能。 |
| [^34] | [Adapting Large Language Models for Document-Level Machine Translation.](http://arxiv.org/abs/2401.06468) | 本文研究了适应大型语言模型进行文档级机器翻译的过程。实验结果显示，这些专门的模型在某些情况下超过了GPT-4的翻译性能，但在其他情况下仍然存在离标翻译问题，需要进一步改进和探索。 |
| [^35] | [PersianMind: A Cross-Lingual Persian-English Large Language Model.](http://arxiv.org/abs/2401.06466) | PersianMind是一个开源的双语大型语言模型，通过在波斯语中展现与闭源的GPT-3.5-turbo相当的性能，并利用迁移学习在不同语言间传递任务知识的优势，解决了开源模型在非英文语言上性能不佳的问题。 |
| [^36] | [Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers.](http://arxiv.org/abs/2401.06461) | 本文通过分析代码的属性，揭示了机器和人类代码之间的独特模式，尤其是结构分割对于识别代码来源很关键。基于这些发现，我们提出了一种名为DetectCodeGPT的新方法来检测机器生成的代码。 |
| [^37] | [BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via Graph Representation Pretraining.](http://arxiv.org/abs/2401.06443) | 本研究提出了BOK-VQA数据集，包含多语言的视觉问答数据以及与问题-回答内容相关的知识信息。通过以图嵌入的形式预训练数据的知识信息，可以有效地将外部知识注入VQA系统中，实现更好的问答效果。 |
| [^38] | [From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape.](http://arxiv.org/abs/2401.06431) | 本研究调查了大型语言模型（LLM）在自动化作文评分系统中的有效性，并发现LLM AES系统具有更高的准确性、一致性、普适性和可解释性。此外，LLM还能提升人类评分员的性能。 |
| [^39] | [Mission: Impossible Language Models.](http://arxiv.org/abs/2401.06416) | 本文为了支持大型语言模型(LLMs)能够学习不可能的语言的观点，开发了一组人工合成的不可能语言，并通过评估GPT-2小型模型的学习能力得出了结论。 |
| [^40] | [AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters.](http://arxiv.org/abs/2401.06408) | 这项研究调查了十个质量和语言识别过滤器对不同社交维度变化的网页的影响。实验发现在数据筛选过程中存在隐含的偏好，一些质量分类器类似于主题过滤器，而语言识别可能会忽视某些地区的英语内容。我们的研究为促进更公正和全面的模型开发提供了洞察。 |
| [^41] | [DevEval: Evaluating Code Generation in Practical Software Projects.](http://arxiv.org/abs/2401.06401) | 本文提出了一个名为DevEval的新基准测试，用于评估实际软件项目中的代码生成。与之前的基准测试相比，DevEval在真实的项目分布、充足的依赖和足够规模的项目背景等方面更贴合实际。通过对五个流行的大型语言模型进行评估，我们揭示了它们在代码生成中的实际能力。 |
| [^42] | [Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model.](http://arxiv.org/abs/2401.06400) | 本文提出了一种新方法CoQAH，通过连接大型语言模型和训练于合成数据上的VQA模型的QA交互序列，实现了将可视问答从合成问题泛化到人工问题，并在两种类型的人工问题数据集上取得了最先进的准确率，超过了通用视觉语言模型、VQA模型和医学基础模型。 |
| [^43] | [An approach for mistranslation removal from popular dataset for Indic MT Task.](http://arxiv.org/abs/2401.06398) | 本研究提出了一种算法，可以从印度机器翻译任务中的常用数据集中去除误译，以提高机器翻译的质量。 |
| [^44] | [Adaptive Data Augmentation for Aspect Sentiment Quad Prediction.](http://arxiv.org/abs/2401.06394) | 本文提出了一种自适应数据增强（ADA）框架来解决方面情感四元预测（ASQP）任务中的数据不平衡问题。实验证实，数据增强可以改善ASQP任务的性能，而ADA方法优于简单的数据过采样方法。 |
| [^45] | [What should I say? -- Interacting with AI and Natural Language Interfaces.](http://arxiv.org/abs/2401.06382) | 随着人工智能技术的普及，研究人类与AI的交互变得越来越重要。本研究通过探索人类与AI交互过程中心智表征的建立，旨在帮助实现成功和轻松的沟通。 |
| [^46] | [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs.](http://arxiv.org/abs/2401.06373) | 本文通过将LLMs视为人类交流者，探索了每天语言互动和AI安全之间忽视的交叉点，并提出了一种通过说服LLMs进行越狱的方法。研究结果表明，说服显著提高了越狱性能，在多个风险类别上均取得了超过92%的攻击成功率。 |
| [^47] | [Learning from Semi-Factuals: A Debiased and Semantic-Aware Framework for Generalized Relation Discovery.](http://arxiv.org/abs/2401.06327) | 这项研究介绍了一种新的任务，称为广义关系发现（GRD），用于开放世界的关系抽取。研究围绕如何减轻由标记的预定义关系造成的模型偏见和确定新关系的特定语义进行了讨论，并提出了一个名为SFGRD的框架来解决这些问题。 |
| [^48] | [Multi-Task Learning for Front-End Text Processing in TTS.](http://arxiv.org/abs/2401.06321) | 我们提出了一个多任务学习模型，用于在TTS前端处理文本。我们的模型同时解决了文本归一化、词性标注和同形异义词消歧这三个任务，并结合了预训练的语言模型以提高性能。通过逐任务消融实验证明，我们的模型在所有三个任务上表现最好。我们还提供了一个包含各种上下文的新HD数据集，用于同形异义词消歧任务的评估。 |
| [^49] | [Zero-shot Generative Large Language Models for Systematic Review Screening Automation.](http://arxiv.org/abs/2401.06320) | 本研究调查了使用零样本生成式大型语言模型进行系统性综述自动筛选的有效性，结果显示指导微调和校准技术在筛选中起到重要作用，并且与零样本模型的集成相结合可以显著节省筛选时间。 |
| [^50] | [Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation.](http://arxiv.org/abs/2401.06310) | 本论文提出了一种多方面的方法，利用现有的文本资源，基于135个全球范围内的身份群体对文本到图像生成（T2I）模型生成的图像中的地理文化刻板印象进行评估。研究结果表明，刻板属性在图像中的存在可能性是刻板属性的三倍。 |
| [^51] | [Misconfidence-based Demonstration Selection for LLM In-Context Learning.](http://arxiv.org/abs/2401.06301) | 这项研究提出了一种名为ICR的方法，通过基于自信度的策略性演示选择，以降低LLM输出与实际输入输出之间的差异，从而克服了当前LLM上下文学习中演示选择的难题。 |
| [^52] | [LEGOBench: Leaderboard Generation Benchmark for Scientific Models.](http://arxiv.org/abs/2401.06233) | LEGOBench是一个评估生成科学模型排行榜系统的基准测试，使用22年来的论文预印本数据和PapersWithCode门户上的机器学习排行榜的数据，初步结果显示自动排行榜生成存在显著性能差距。 |
| [^53] | [Learning Unsupervised Semantic Document Representation for Fine-grained Aspect-based Sentiment Analysis.](http://arxiv.org/abs/2401.06210) | 这篇论文研究了学习无监督的语义文档表示以进行细粒度的基于方面的情感分析。通过克服现有方法的困难，实验证明该模型在各个任务上的性能优于最先进方法。 |
| [^54] | [EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction.](http://arxiv.org/abs/2401.06201) | EASYTOOL是一个将多样化而冗长的工具文档转化为统一而简洁的工具指示的框架，用于增强基于LLM的代理的能力。通过从多个来源提取关键信息，并提供标准化的工具描述和功能，EasyTool显著降低了标记消耗，并提高了在真实场景中的工具利用性能。 |
| [^55] | [CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network for Crisis Event Classification.](http://arxiv.org/abs/2401.06194) | CrisisKAN是一种知识注入和可解释的多模态注意力网络，用于危机事件分类。它通过结合图像、文本和维基百科的外部知识来弥合图像和文本模态之间的语义差距，并解释模型的结果，以建立在高风险情况下的信任。 |
| [^56] | [End to end Hindi to English speech conversion using Bark, mBART and a finetuned XLSR Wav2Vec2.](http://arxiv.org/abs/2401.06183) | 本论文提出了一个端到端的语音转换框架，用于印地语到英语的转换，采用了Bark、mBART和经过微调的XLSR Wav2Vec2等先进技术，为跨语言交流提供了统一而无缝的解决方案。 |
| [^57] | [Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models.](http://arxiv.org/abs/2401.06102) | 本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。 |
| [^58] | [LEGO:Language Enhanced Multi-modal Grounding Model.](http://arxiv.org/abs/2401.06071) | LEGO是一种语言增强的多模态关联模型，它能够在各种任务中实现细粒度的理解和精确的标识能力。 |
| [^59] | [Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks.](http://arxiv.org/abs/2401.05949) | 本研究发现上下文学习范式在大型语言模型中存在漏洞，攻击者可以通过污染示范上下文来操控模型行为，而无需进行微调。这项研究设计了一种名为ICLAttack的后门攻击方法，可以通过污染示范样本和提示来使模型按照预定义的意图行事。 |
| [^60] | [Generative Deduplication For Socia Media Data Selection.](http://arxiv.org/abs/2401.05883) | 提出了一种名为生成去重的方法，用于解决社交媒体数据中的冗余问题和模型偏差。通过删除重复的文本，可以提高语言理解性能并节省训练时间。 |
| [^61] | [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training.](http://arxiv.org/abs/2401.05566) | 该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。 |
| [^62] | [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation.](http://arxiv.org/abs/2401.04679) | RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。 |
| [^63] | [MERA: A Comprehensive LLM Evaluation in Russian.](http://arxiv.org/abs/2401.04531) | 这项研究提出了MERA，一个多模态俄语基础模型评估指标。该指标包括21个评估任务，涵盖了11个技能领域中生成模型的评估。研究还提出了一种在零样本和少样本固定指令设置下评估FM和LM的方法。 |
| [^64] | [Can AI Be as Creative as Humans?.](http://arxiv.org/abs/2401.01623) | 本文引入了一个新概念【相对创造力】，通过将焦点转向AI是否能够与人类具备相同的创造能力，实现对创造力的统计量化评估和直接比较。 |
| [^65] | [LLaMA Beyond English: An Empirical Study on Language Capability Transfer.](http://arxiv.org/abs/2401.01055) | 本文提出了LLaMA超越英语：语言能力转移的实证研究。通过对LLaMA进行广泛的实证调查，分析了词汇扩展、进一步预训练和指导调整等关键因素对非英语语言上的能力转移的影响，并通过四个标准化测试基准评估了模型的知识水平和响应质量。 |
| [^66] | [Stateful Conformer with Cache-based Inference for Streaming Automatic Speech Recognition.](http://arxiv.org/abs/2312.17279) | 本文提出一种基于FastConformer架构的流式语音识别模型，通过限制上下文和引入缓存机制，在推理过程中实现非自回归编码器的自回归操作，并消除了训练和推理准确度间的差异。同时，还提出了CTC/RNNT混合架构以提高准确度和节省计算。 |
| [^67] | [NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes.](http://arxiv.org/abs/2312.14890) | NPHardEval是一个新的基准，旨在评估大型语言模型在900个算法问题上的推理能力，扩展到NP-Hard复杂性类别。 |
| [^68] | [Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data.](http://arxiv.org/abs/2311.17492) | Mergen是首个满汉机器翻译模型，通过使用增强数据和双向GRU层，取得了在满汉翻译中显著提升的结果。 |
| [^69] | [Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation.](http://arxiv.org/abs/2311.08640) | 将大型语言模型的知识通过多阶段协作蒸馏的方式应用于半监督序列生成任务中，可以显著提高模型的泛化能力和性能。 |
| [^70] | [Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs.](http://arxiv.org/abs/2310.18152) | 本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。 |
| [^71] | [O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models.](http://arxiv.org/abs/2310.14403) | O3D提出了一种基于离线数据的学习框架，利用大规模数据改进了大规模语言模型在顺序决策问题中的性能，通过自动发现可重复使用的技能，提高了模型的表现 |
| [^72] | [MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations.](http://arxiv.org/abs/2310.12489) | 本研究通过零样本学习调查了预训练语言模型在医生和AI在健康咨询中的回答的准确分类上的效果。研究发现，虽然预训练语言模型在一般语言理解方面表现出了很强的能力，但在医疗咨询中，它们可能需要特定语料库训练或其他技术以实现准确的医生和AI生成文本的分类。 |
| [^73] | [Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model.](http://arxiv.org/abs/2309.13018) | 本研究提出了一种自适应掩蔽方法，用于高效地压缩多语种ASR模型。该方法通过动态适应子网络结构，能够在减少性能损失的情况下得到稀疏的单语种模型或稀疏的多语种模型。实验证明，与现有的修剪方法相比，该方法在针对稀疏的单语种模型时表现更好，并且减少了对特定语言进行修剪的需求。 |
| [^74] | [TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models.](http://arxiv.org/abs/2309.04027) | 本文介绍了TIDE（Textual Identity Detection）方法来改善分类器和语言模型中的文本公平性。通过创建一个包含身份词汇和语境的数据集，以及开发一个身份注释和增强工具，可以提高机器学习公平性技术的效果。 |
| [^75] | [PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation.](http://arxiv.org/abs/2308.12604) | PromptMRG是一种针对医学报告生成的诊断驱动方法，通过诊断感知的提示来提高MRG的诊断准确性。这种方法基于编码器-解码器架构，并具有额外的疾病分类分支。 |
| [^76] | [Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge.](http://arxiv.org/abs/2308.09311) | 本文提出了一种针对低资源语言的唇语识别框架，通过学习通用语音知识和语言特定知识，克服了低资源语言中缺乏视频文本配对数据的挑战。 |
| [^77] | [Self-consistency for open-ended generations.](http://arxiv.org/abs/2307.06857) | 本论文提出了一种改进大规模预训练语言模型生成输出质量和一致性的新方法，通过扩展自洽性框架的适用性，实现了从一个候选集中恢复最优或接近最优的生成结果，并提出了一种轻量级无参数相似性函数来改进代码生成、自动形式化和摘要任务的效果。 |
| [^78] | [Improving Language Plasticity via Pretraining with Active Forgetting.](http://arxiv.org/abs/2307.01163) | 本论文提出了一种通过在预训练过程中使用主动遗忘机制来提高语言模型的可塑性的方法。通过在预训练过程中定期重置嵌入层，模型可以更快地适应新的语言，并在低数据情况下表现出更好的性能。 |
| [^79] | [Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT.](http://arxiv.org/abs/2306.01393) | 本文通过实验发现，在NMT中基于子词的分词方法中，频率对于模型的表现贡献占据了90%-95%，因此相对于组合性，频率更为重要。 |
| [^80] | [A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation.](http://arxiv.org/abs/2304.07772) | 本研究综合评估自然语言到SPARQL查询生成中的复制机制，通过大量实验研究预训练和非预训练模型、问题注释格式以及使用复制机制的影响，并证明了在这些方面的优化可以提高性能。 |
| [^81] | [State-of-the-art generalisation research in NLP: A taxonomy and review.](http://arxiv.org/abs/2210.03050) | 本文提出了一个用于分类和理解NLP中泛化研究的分类法，对400多篇论文进行了综述和分类，总结了当前泛化研究的现状。 |
| [^82] | [NAAQA: A Neural Architecture for Acoustic Question Answering.](http://arxiv.org/abs/2106.06147) | 本文提出了一种名为NAAQA的神经网络结构，用于声学问答任务。通过使用1D卷积处理声学内容的2D频谱时域表示，该结构通过时间坐标图增加了时间定位能力，并在处理具有不同基本声音构建的场景时表现出有希望的结果。 |

# 详细

[^1]: 机器翻译模型是零射击的翻译方向检测器。

    Machine Translation Models are Zero-Shot Detectors of Translation Direction. (arXiv:2401.06769v1 [cs.CL])

    [http://arxiv.org/abs/2401.06769](http://arxiv.org/abs/2401.06769)

    本文探索了一种基于无监督方法的翻译方向检测，并通过实验证实其在高负载语言对上的有效性。论文标题为“Machine Translation Models are Zero-Shot Detectors of Translation Direction”。

    

    检测并行文本的翻译方向对于机器翻译的训练和评估具有应用价值，但也具有法医应用，例如解决剽窃或伪造指控。在这项工作中，我们根据一个简单的假设，即$p(\text{translation}|\text{original})>p(\text{original}|\text{translation})$，以传统上被称为翻译语或机器翻译语中的简化效应为动机，探索了一种无监督的翻译方向检测方法。通过对20个翻译方向进行大规模多语种机器翻译模型的实验，我们验证了该方法在资源丰富的语言对上的有效性，对于NMT生成的翻译，实现了文档级准确率为82-96％，对于人工翻译，根据所使用的模型，实现了60-81％的准确率。代码和演示可在https://github.com/ZurichNLP/translation-direction-detection找到。

    Detecting the translation direction of parallel text has applications for machine translation training and evaluation, but also has forensic applications such as resolving plagiarism or forgery allegations. In this work, we explore an unsupervised approach to translation direction detection based on the simple hypothesis that $p(\text{translation}|\text{original})>p(\text{original}|\text{translation})$, motivated by the well-known simplification effect in translationese or machine-translationese. In experiments with massively multilingual machine translation models across 20 translation directions, we confirm the effectiveness of the approach for high-resource language pairs, achieving document-level accuracies of 82-96% for NMT-produced translations, and 60-81% for human translations, depending on the model used. Code and demo are available at https://github.com/ZurichNLP/translation-direction-detection
    
[^2]: APAR: LLMs可以进行自动并行自回归解码

    APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding. (arXiv:2401.06761v1 [cs.CL])

    [http://arxiv.org/abs/2401.06761](http://arxiv.org/abs/2401.06761)

    这项工作介绍了一种名为APAR的并行自回归生成方法，通过对数据进行调整，使得LLMs能够独立规划生成过程以及执行自动并行自回归（APAR）生成，从而显著减少了生成步骤的数量，并在高吞吐量场景中实现了吞吐量增加和延迟降低。

    

    大规模语言模型（LLM）的广泛应用要求有效的部署策略。然而，大多数LLM生成文本的基本自回归解码过程对于实现高效服务提出了挑战。在这项工作中，我们介绍了一种并行自回归生成方法。通过以包含层次结构的通用领域数据为指导进行调整，我们使LLM能够独立规划其生成过程，并进行自动化并行自回归（APAR）生成，显著减少了生成步骤的数量。仅APAR就可以实现多达2倍的加速，而当与推测解码结合时，加速度可以达到4倍。此外，APAR在生成过程中减少了键值缓存消耗和注意力计算。与最先进的服务框架相比，在高吞吐量场景中，这导致吞吐量增加了20-70％，延迟降低了20-35％。

    The massive adoption of large language models (LLMs) demands efficient deployment strategies. However, the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving. In this work, we introduce a parallel auto-regressive generation method. By instruct-tuning on general domain data that contains hierarchical structures, we enable LLMs to independently plan their generation process and perform auto-parallel auto-regressive (APAR) generation, significantly reducing the number of generation steps. APAR alone can achieve up to 2x speed-up, and when combined with speculative decoding, the speed-up can reach up to 4x. In addition, APAR reduces the key-value cache consumption and attention computation during generation. This leads to a throughput increase of 20-70% and a latency reduce of 20-35% in high-throughput scenarios, compared to state-of-the-art serving frameworks.
    
[^3]: 解决度量数值和准确性之间的迷宫：导航指标

    Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies. (arXiv:2401.06760v1 [cs.CL])

    [http://arxiv.org/abs/2401.06760](http://arxiv.org/abs/2401.06760)

    本文研究了一系列现代度量标准的“动态范围”，以提供对度量标准之间和内部得分差异的共同理解，并通过人类感知水平的系统差异进行衡量。

    

    十年前，机器翻译研究中有一个单一的度量标准BLEU。如今，没有这样的共识，因此研究人员很难发展和保持之前推动研究和部署决策的那种启发性直觉。本文研究了一系列现代度量标准的“动态范围”，以提供对度量标准之间和内部得分差异的共同理解；换句话说，我们要问在度量标准Y中，两个系统需要有多大的得分差异X，人类才能注意到？我们使用一个新的大型数据集ToShip23进行评估，使用它发现度量标准达到人类感知水平的系统差异，我们通过成对系统准确性来衡量。此外，我们还表明与标准的统计p值在测试集上的稳定性相比，使用该方法建立差异准确性更为稳定。

    Ten years ago a single metric, BLEU, governed progress in machine translation research. For better or worse, there is no such consensus today, and consequently it is difficult for researchers to develop and retain the kinds of heuristic intuitions about metric deltas that drove earlier research and deployment decisions. This paper investigates the "dynamic range" of a number of modern metrics in an effort to provide a collective understanding of the meaning of differences in scores both within and among metrics; in other words, we ask what point difference X in metric Y is required between two systems for humans to notice? We conduct our evaluation on a new large dataset, ToShip23, using it to discover deltas at which metrics achieve system-level differences that are meaningful to humans, which we measure by pairwise system accuracy. We additionally show that this method of establishing delta-accuracy is more stable than the standard use of statistical p-values in regards to testset si
    
[^4]: 多作者文献的文体分析用于作者和作者文体变化检测

    Stylometry Analysis of Multi-authored Documents for Authorship and Author Style Change Detection. (arXiv:2401.06752v1 [cs.CL])

    [http://arxiv.org/abs/2401.06752](http://arxiv.org/abs/2401.06752)

    本文研究了多作者文献的文体分析，包括文体分类、单作者变化检测和多作者变化检测。我们提出了一个基于价值的融合框架，并整合了多种自然语言处理算法和权重优化技术。

    

    近年来，基于人工智能的文本生成工具的广泛应用给文献来源的考证、鉴别和作者检测带来了新的挑战。然而，文体分析的进展为使用文体分析技术在多作者文献中进行自动作者检测和作者变化检测提供了机会。文体分析可以作为文献来源和鉴别的首要步骤，通过作者检测进行验证。本文研究了文体分析的三个关键任务：（i）单作者和多作者文献的分类，（ii）单一变化检测，包括确定作者变换点，和（iii）多作者变换检测。我们将这三个任务都建立为分类问题，并提出了一个基于价值的融合框架，该框架整合了几种先进的自然语言处理（NLP）算法和权重优化技术。

    In recent years, the increasing use of Artificial Intelligence based text generation tools has posed new challenges in document provenance, authentication, and authorship detection. However, advancements in stylometry have provided opportunities for automatic authorship and author change detection in multi-authored documents using style analysis techniques. Style analysis can serve as a primary step toward document provenance and authentication through authorship detection. This paper investigates three key tasks of style analysis: (i) classification of single and multi-authored documents, (ii) single change detection, which involves identifying the point where the author switches, and (iii) multiple author-switching detection in multi-authored documents. We formulate all three tasks as classification problems and propose a merit-based fusion framework that integrates several state-of-the-art natural language processing (NLP) algorithms and weight optimization techniques. We also explo
    
[^5]: Easy Training Data对于困难任务的不合理有效性

    The Unreasonable Effectiveness of Easy Training Data for Hard Tasks. (arXiv:2401.06751v1 [cs.CL])

    [http://arxiv.org/abs/2401.06751](http://arxiv.org/abs/2401.06751)

    当困难训练数据很难正确标记时，当前的语言模型通常能够相对良好地从易到难的数据泛化，并且即使在关注于困难数据的性能时，收集和训练易数据可能比困难数据更好。

    

    当困难训练数据在定义上很难正确标记时，我们如何训练模型在困难测试数据上表现良好？这个问题被称为可扩展监督问题，在语言模型不断改进的过程中引起了越来越多的关注。在本文中，我们提出了一个令人惊讶的结论，即当前的语言模型通常从易到难的数据泛化相对良好，甚至表现得和在困难数据上训练的“oracle”模型一样好。我们使用简单的训练方法（如上下文学习、线性分类器头和QLoRA）展示了这种从易到难的泛化，针对七个不同的数据点难度度量，包括六个经验多样的人类难度度量（如年级水平）和一个基于模型的度量（基于损失）。此外，我们还表明，即使最关心模型在困难数据上的性能，收集并训练易数据可能比困难数据更好，因为困难数据通常更嘈杂和昂贵。

    How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current language models often generalize relatively well from easy to hard data, even performing as well as "oracle" models trained on hard data. We demonstrate this kind of easy-to-hard generalization using simple training methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect and train on easy data rather than hard data, since hard data is generally noisier and costli
    
[^6]: 使用自然语言推理来改进对话中的角色提取在新领域中的应用

    Using Natural Language Inference to Improve Persona Extraction from Dialogue in a New Domain. (arXiv:2401.06742v1 [cs.CL])

    [http://arxiv.org/abs/2401.06742](http://arxiv.org/abs/2401.06742)

    本研究通过使用自然语言推理方法，提出了一种后期适应已训练的角色提取模型到新领域中的方法，以解决对话中角色提取的多样性和非真实世界设置的问题。

    

    虽然宝贵的数据集如PersonaChat为训练有基于角色的对话代理提供了基础，但它们在对话和叙事环境的多样性方面缺乏，主要存在于“真实”世界中。为了开发具有独特角色的对话代理，模型被训练以在给定特定角色的情况下进行对话，但手工制作这些角色可能耗时，因此存在从现有特定角色对话中自动提取角色信息的方法。然而，这些角色提取模型也是在从PersonaChat衍生的数据集上进行训练，并且很难从非真实世界的对话设置中提供高质量的角色信息，例如以幻想为主题的数据集LIGHT。创建新数据以训练特定设置的模型是人力密集型的，因此代价过高。为了解决这两个问题，我们引入了一种自然语言推理方法，以后期调整已训练的角色提取模型适应新的设置

    While valuable datasets such as PersonaChat provide a foundation for training persona-grounded dialogue agents, they lack diversity in conversational and narrative settings, primarily existing in the "real" world. To develop dialogue agents with unique personas, models are trained to converse given a specific persona, but hand-crafting these persona can be time-consuming, thus methods exist to automatically extract persona information from existing character-specific dialogue. However, these persona-extraction models are also trained on datasets derived from PersonaChat and struggle to provide high-quality persona information from conversational settings that do not take place in the real world, such as the fantasy-focused dataset, LIGHT. Creating new data to train models on a specific setting is human-intensive, thus prohibitively expensive. To address both these issues, we introduce a natural language inference method for post-hoc adapting a trained persona extraction model to a new 
    
[^7]: 不可靠的依赖：语言模型不愿表达不确定性的影响

    Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty. (arXiv:2401.06730v1 [cs.CL])

    [http://arxiv.org/abs/2401.06730](http://arxiv.org/abs/2401.06730)

    本研究调查了语言模型在回答问题时不愿表达不确定性的影响，发现语言模型往往过于自信，导致高错误率。实验还表明用户无论是否标记了确定性都会严重依赖语言模型生成的结果。

    

    随着自然语言成为人工智能交互的默认接口，语言模型适当地传达下游应用的不确定性变得至关重要。本研究调查了语言模型如何通过自然语言表达对其回答的置信度，以及下游用户对语言模型表达的不确定性的反应。我们调查了公开部署的模型，发现在回答问题时，即使产生了错误答案，语言模型也无法表达不确定性。虽然可以明确要求语言模型表达置信度，但它们往往过于自信，导致在置信的回答中错误率高达平均47%。我们通过人类实验测试了语言模型过度自信的风险，并证明用户无论是否标记了确定性都会严重依赖语言模型生成的结果。最后，我们研究了在RLHF对齐中使用的偏好注释数据集，并发现人类对带有不确定性的文本有偏见。我们的研究突出了这一问题。

    As natural language becomes the default interface for human-AI interaction, there is a critical need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are unable to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (on average 47%) among confident responses. We test the risks of LM overconfidence by running human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in RLHF alignment and find that humans have a bias against texts with uncertainty. Our work hig
    
[^8]: 重新定义税法推论为类比推理

    Reframing Tax Law Entailment as Analogical Reasoning. (arXiv:2401.06715v1 [cs.CL])

    [http://arxiv.org/abs/2401.06715](http://arxiv.org/abs/2401.06715)

    本论文将法定推理重新定义为类比任务，通过增加数据集大小和引入解释性因素，展示了这个任务与原始任务的难度相当，并利用检索机制和类比模型解决法定推理问题，在之前的可比工作上取得了一些进展。

    

    法定推理是指将立法规定应用于用自然语言描述的一系列案例事实。我们将法定推理重新定义为类比任务，其中每个类比任务实例涉及两个法定推理实例的组合。这样做可以将数据集大小增加两个数量级，并引入解释性因素。我们证明这个任务对于自然语言处理模型来说与原始任务的难度相当。最后，我们通过结合检索机制和类比模型来解决法定推理问题，并在之前的可比工作上取得了一些进展。

    Statutory reasoning refers to the application of legislative provisions to a series of case facts described in natural language. We re-frame statutory reasoning as an analogy task, where each instance of the analogy task involves a combination of two instances of statutory reasoning. This increases the dataset size by two orders of magnitude, and introduces an element of interpretability. We show that this task is roughly as difficult to Natural Language Processing models as the original task. Finally, we come back to statutory reasoning, solving it with a combination of a retrieval mechanism and analogy models, and showing some progress on prior comparable work.
    
[^9]: 使用样式表示进行机器生成文本的小样本检测

    Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])

    [http://arxiv.org/abs/2401.06712](http://arxiv.org/abs/2401.06712)

    本研究提出了一种小样本检测方法，通过使用样式表示来检测机器生成的文本与人类撰写的文本的区别，以解决滥用语言模型带来的问题。

    

    受到指导调整的语言模型的出现使得人类写作的逼真模仿面临着重大滥用风险。然而，我们可以通过检测一段文本是由语言模型还是人类撰写而成来对抗此类滥用行为。本文提出了一种基于样式表示的小样本检测方法，避免了神经网络检测器在面对数据转换时的规约不足的挑战，同时也避免了在推理或检测时需要访问可能生成文档的模型的问题。

    The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally d
    
[^10]: 在用户撰写的文本中心理概念提取和分类的可靠性分析

    Reliability Analysis of Psychological Concept Extraction and Classification in User-penned Text. (arXiv:2401.06709v1 [cs.CL])

    [http://arxiv.org/abs/2401.06709](http://arxiv.org/abs/2401.06709)

    该论文研究了在用户撰写的文本中心理概念提取和分类的可靠性分析，并通过注解LoST数据集来捕捉表明低自尊存在的微妙文本提示。研究发现，NLP模型对触发词、LoST指标和后果这三类文本提示更加关注。

    

    社交NLP研究社区最近在心理健康分析的计算进展中见证了一波复杂的语言使用和自我感知之间相互作用的AI模型的建立。这些负责任的AI模型有助于从社交媒体上的用户撰写的文本中量化心理概念。在超越低级（分类）任务的基础上，我们将现有的二元分类数据集提升到更高级别的可靠性分析任务，通过解释的角度将之作为一种安全措施。我们注释了LoST数据集，以捕捉表明Reddit用户发帖中存在低自尊的微妙文本提示。我们进一步指出，用于确定低自尊存在的NLP模型更加关注三种类型的文本提示：（i）触发词：触发心理扰动的词汇，（ii）LoST指标：强调低自尊的文本指标，以及（iii）后果：描述情绪稳定性后果的词汇。

    The social NLP research community witness a recent surge in the computational advancements of mental health analysis to build responsible AI models for a complex interplay between language use and self-perception. Such responsible AI models aid in quantifying the psychological concepts from user-penned texts on social media. On thinking beyond the low-level (classification) task, we advance the existing binary classification dataset, towards a higher-level task of reliability analysis through the lens of explanations, posing it as one of the safety measures. We annotate the LoST dataset to capture nuanced textual cues that suggest the presence of low self-esteem in the posts of Reddit users. We further state that the NLP models developed for determining the presence of low self-esteem, focus more on three types of textual cues: (i) Trigger: words that triggers mental disturbance, (ii) LoST indicators: text indicators emphasizing low self-esteem, and (iii) Consequences: words describing
    
[^11]: 多候选推测解码

    Multi-Candidate Speculative Decoding. (arXiv:2401.06706v1 [cs.CL])

    [http://arxiv.org/abs/2401.06706](http://arxiv.org/abs/2401.06706)

    本研究提出了一种多候选推测解码的方法，通过从草稿模型中采样多个候选标记并进行批次验证，显著提高了接受率并优于标准推测解码方法。

    

    大型语言模型在各种自然语言处理任务中展现了令人瞩目的能力，但使用自回归生成文本的效率较低。其中一种提高效率的方法是推测解码，它从一个快速的草稿模型生成候选段落（一系列的标记），然后通过目标模型并行验证。然而，候选标记的接受率受到模型、数据集和解码设置等多个因素的限制。本文提出了从草稿模型中采样多个候选标记，并将它们组织成批次进行验证的方法。我们设计了高效的多候选验证算法，同时保持了目标模型的分布。我们的方法在多个数据集和模型上都显示出了显著的接受率提高，始终优于标准的推测解码方法。

    Large language models have shown impressive capabilities across a variety of NLP tasks, yet their generating text autoregressively is time-consuming. One way to speed them up is speculative decoding, which generates candidate segments (a sequence of tokens) from a fast draft model that is then verified in parallel by the target model. However, the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup. This paper proposes sampling multiple candidates from a draft model and then organising them in batches for verification. We design algorithms for efficient multi-candidate verification while maintaining the distribution of the target model. Our approach shows significant improvements in acceptance rates on multiple datasets and models, consistently outperforming standard speculative decoding.
    
[^12]: 大型语言模型有限标签监督微调的实验设计框架

    An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])

    [http://arxiv.org/abs/2401.06692](http://arxiv.org/abs/2401.06692)

    该论文提出了一个实验设计框架来减少大型语言模型有限标签监督微调的注释成本，并解决了主动学习的计算瓶颈问题。

    

    在现代大型语言模型中，指导数据集上的有限标签监督微调（SFT）在实现了令人惊叹的零射击泛化能力方面发挥了至关重要的作用。然而，为了为指令产生高质量的回答所需的注释工作正在变得难以承受，特别是随着指令数据集所涵盖的任务数量的增加。主动学习可以有效地从未标记的样本池中确定有用的子集进行注释，但其高计算成本仍然是其在LLMs环境中广泛应用的障碍。为了减少SFT的注释成本并规避主动学习的计算瓶颈，我们提出使用实验设计。实验设计技术选择最具信息量的样本进行标注，通常最大化某种不确定性和/或多样性的概念。在我们的工作中，我们实施了一个评估多种现有和新颖的实验设计方法的框架。

    Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimen
    
[^13]: 不要排名，要合并！使用质量估计来合并机器翻译假设

    Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation. (arXiv:2401.06688v1 [cs.CL])

    [http://arxiv.org/abs/2401.06688](http://arxiv.org/abs/2401.06688)

    本文介绍了一种利用质量估计指标合并机器翻译假设的方法，该方法在大型语言模型和多语言翻译模型上提升了翻译质量。通过使用候选池和QE指标，我们的方法能够生成多样且准确的翻译结果。

    

    神经机器翻译系统通过给定源句子估计目标句子的概率，但这些估计可能与人类喜好不一致。本研究引入了QE-fusion方法，该方法利用更能与人类判断相关的质量估计指标（QE）来综合改进翻译结果。QE-fusion利用从模型中抽取的候选池，使用像CometKiwi这样的QE指标组合不同候选的片段。我们将QE-fusion与波束搜索和最近的重新排序技术（如最小贝叶斯风险解码或QE-重新排序）进行比较。当应用于用于翻译的大型语言模型（PolyLM、XGLM、Llama2和Mistral）和多语言翻译模型（NLLB）时，我们的方法在COMET和BLEURT评分方面始终提高翻译质量，涵盖五种语言对。值得注意的是，由于能够生成多样化的输出，我们的方法对于大型语言模型的改进更大。我们证明了我们的方法能够产生多样且准确的翻译结果。

    Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method utilizing a quality estimation metric (QE) that better correlates with human judgments to synthesize improved translations. QE-fusion leverages a candidate pool sampled from a model, combining spans from different candidates using QE metrics such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generat
    
[^14]: 使用文本数据的近因果推断

    Proximal Causal Inference With Text Data. (arXiv:2401.06687v1 [cs.CL])

    [http://arxiv.org/abs/2401.06687](http://arxiv.org/abs/2401.06687)

    本论文提出了一种使用文本数据进行近因果推断的方法，通过将文本数据分割并使用零样本模型推断出代理变量，然后应用于近邻 g-formula，从而解决了混淆变量完全未观察到的情况。实验结果表明该方法产生了低偏差的估计值。

    

    最近的基于文本的因果方法试图通过将非结构化文本数据作为倾向于包含部分或不完全测量的混淆变量的代理来减轻混淆偏差。这些方法假设分析人员在一部分实例的文本中具有有监督的混淆变量标签，但由于数据隐私或成本，这种约束并不总是可行。在这里，我们解决了一个重要的混淆变量完全未观察到的情况。我们提出了一种新的因果推断方法，将处理前文本数据分割，并使用两个零样本模型从分割的两个部分推断出两个代理，并将这些代理应用于近邻 g-formula。我们证明了我们基于文本的代理方法满足近邻 g-formula所需的识别条件，而其他看似合理的提议则不满足。我们在合成和半合成环境中评估了我们的方法，并发现它产生了低偏差的估计值。

    Recent text-based causal methods attempt to mitigate confounding bias by including unstructured text data as proxies of confounding variables that are partially or imperfectly measured. These approaches assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is not always feasible due to data privacy or cost. Here, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that splits pre-treatment text data, infers two proxies from two zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. This combination of proximal causal inference and zero-sh
    
[^15]: DQNC2S：基于DQN的跨流危机事件摘要生成器

    DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])

    [http://arxiv.org/abs/2401.06683](http://arxiv.org/abs/2401.06683)

    本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。

    

    同时总结多个与灾害相关的数据流尤其具有挑战性，因为现有的检索与重新排序策略在多流数据的固有冗余和多查询环境下的限制可扩展性方面存在问题。本文提出了一种基于弱标注和深度Q网络的在线危机时间轴生成方法。它能够实时选择相关的文本片段，无需人工标注或内容重新排序，从而使推理时间与输入查询的数量无关。该方法还将冗余过滤器融入奖励函数中，以有效处理跨流内容重叠。在CrisisFACTS 2022基准测试中，所达到的ROUGE和BERTScore结果优于最佳性能模型。

    Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
    
[^16]: PolyTOPS：可重构和灵活的多面体调度器

    PolyTOPS: Reconfigurable and Flexible Polyhedral Scheduler. (arXiv:2401.06665v1 [cs.DC])

    [http://arxiv.org/abs/2401.06665](http://arxiv.org/abs/2401.06665)

    PolyTOPS是一个可配置的多面体调度器，可以针对不同架构、并行性模型或应用场景进行循环优化。

    

    多面体技术在低级编译器和高级过程中广泛用于自动代码优化。循环优化是这种技术的核心，已经提出了几种多面体调度器（如Feautrier、Pluto、isl和Tensor Scheduler），每个调度器针对的是不同的架构、并行性模型或应用场景。由于架构的异构性，对于特定场景的优化需求正在增长。其中最关键的情况之一是用于人工智能的神经处理单元（NPU），可能需要具有不同目标的循环优化。还需要考虑进行多面体优化的框架或编译器。根据目标架构、编译环境和应用领域的不同情况，可能需要不同类型的优化来最大限度地利用架构特性。

    Polyhedral techniques have been widely used for automatic code optimization in low-level compilers and higher-level processes. Loop optimization is central to this technique, and several polyhedral schedulers like Feautrier, Pluto, isl and Tensor Scheduler have been proposed, each of them targeting a different architecture, parallelism model, or application scenario. The need for scenario-specific optimization is growing due to the heterogeneity of architectures. One of the most critical cases is represented by NPUs (Neural Processing Units) used for AI, which may require loop optimization with different objectives. Another factor to be considered is the framework or compiler in which polyhedral optimization takes place. Different scenarios, depending on the target architecture, compilation environment, and application domain, may require different kinds of optimization to best exploit the architecture feature set.  We introduce a new configurable polyhedral scheduler, PolyTOPS, that c
    
[^17]: 改善多模态情感分析的智慧M：融合背景知识的上下文世界知识

    WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge. (arXiv:2401.06659v1 [cs.CL])

    [http://arxiv.org/abs/2401.06659](http://arxiv.org/abs/2401.06659)

    本文提出了一个名为智慧M的插件框架，利用从大型视觉语言模型中产生的上下文世界知识来改进多模态情感分析，实验证明该方法在不同任务上有着显著的改进。

    

    情感分析通过利用各种数据模态（例如文本、图像）迅速发展。然而，大多数先前的研究都依赖于表面信息，忽视了上下文世界知识（例如从给定图像和文本对之外获取的背景信息），从而限制了实现更好的多模态情感分析的能力。本文提出了一个名为智慧M的插件框架，旨在利用从大型视觉语言模型（LVLMs）中产生的上下文世界知识来改进多模态情感分析。智慧M利用LVLM来全面分析图像和相应的句子，同时生成相关的上下文。为了减少上下文中的噪声，我们还引入了一种无需训练的上下文融合机制。在多样的多模态情感分析任务的实验结果一致表明，我们的方法有着显著的改进。

    Sentiment analysis is rapidly advancing by utilizing various data modalities (e.g., text, image). However, most previous works relied on superficial information, neglecting the incorporation of contextual world knowledge (e.g., background information derived from but beyond the given image and text pairs) and thereby restricting their ability to achieve better multimodal sentiment analysis. In this paper, we proposed a plug-in framework named WisdoM, designed to leverage contextual world knowledge induced from the large vision-language models (LVLMs) for enhanced multimodal sentiment analysis. WisdoM utilizes a LVLM to comprehensively analyze both images and corresponding sentences, simultaneously generating pertinent context. To reduce the noise in the context, we also introduce a training-free Contextual Fusion mechanism. Experimental results across diverse granularities of multimodal sentiment analysis tasks consistently demonstrate that our approach has substantial improvements (br
    
[^18]: 多样性激励对LLM文本增强中样本多样性和下游模型性能的影响

    Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation. (arXiv:2401.06643v1 [cs.CL])

    [http://arxiv.org/abs/2401.06643](http://arxiv.org/abs/2401.06643)

    本研究评估了三种文本多样性激励方法对LLM文本增强中生成文本的词汇多样性和下游模型性能的影响。结果表明，使用禁忌词能够最大程度地增加多样性，而使用先前创建的改写作为提示时，下游模型的性能最高。

    

    最新的生成式大型语言模型（LLM）在数据增强任务中找到了应用，其中少量文本样本被LLM改写，然后用于模型的微调。然而，需要进一步研究不同提示、种子数据选择策略、过滤方法或模型设置对改写数据（和下游模型）质量的影响。在本研究中，我们调查了在众包中已经建立良好的三种文本多样性激励方法：禁忌词、通过先前异常解的提示和通过先前异常解的链接。使用这些激励方法作为指导LLM增补文本数据集的一部分，我们探测它们对生成的文本的词汇多样性和下游模型性能的影响。我们比较了5种不同的LLM和6个数据集的影响。结果显示，禁忌词能够最大程度地增加多样性，而使用先前创建的改写作为提示时，下游模型的性能最高。

    The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune the model. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts' lexical diversity and downstream model performance. We compare the effects over 5 different LLMs and 6 datasets. We show that diversity is most increased by taboo words, while downstream model performance is highest when previously created paraphrases are used as hint
    
[^19]: 实验环境能够促进语言模型在稳健的语义属性推断中的表现，但不一致。

    Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently. (arXiv:2401.06640v1 [cs.CL])

    [http://arxiv.org/abs/2401.06640](http://arxiv.org/abs/2401.06640)

    本研究通过控制实验环境的方式，发现语言模型在属性继承任务中表现出了一定的非平凡能力，但这种能力是不一致的。

    

    最近的无人监督评估凸显了语言模型（LMs）在执行意义提取方面的重要限制。然而，众所周知，在引入实验环境（如上下文示例和指导）的情况下，LMs的表现可以显著提高。那么这是否适用于先前研究的意义敏感任务呢？我们在控制上下文示例和指导内容的前提下，对实验环境对于提高LMs在执行属性继承任务中的鲁棒性的程度进行了案例研究，该任务是预先表明LMs无法完成的任务。我们的研究发现，实验环境确实可以导致LMs在属性继承行为方面表现出非平凡的能力。然而，这种能力是不一致的：通过对任务进行最小改写，发现一些LMs从输入中捕捉到浅层的非语义式启发式信息，这表明计算机的行为具有不一致性。

    Recent zero-shot evaluations have highlighted important limitations in the abilities of language models (LMs) to perform meaning extraction. However, it is now well known that LMs can demonstrate radical improvements in the presence of experimental contexts such as in-context examples and instructions. How well does this translate to previously studied meaning-sensitive tasks? We present a case-study on the extent to which experimental contexts can improve LMs' robustness in performing property inheritance -- predicting semantic properties of novel concepts, a task that they have been previously shown to fail on. Upon carefully controlling the nature of the in-context examples and the instructions, our work reveals that they can indeed lead to non-trivial property inheritance behavior in LMs. However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computati
    
[^20]: OOP：针对大型语言模型的面向对象编程评估基准

    OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models. (arXiv:2401.06628v1 [cs.CL])

    [http://arxiv.org/abs/2401.06628](http://arxiv.org/abs/2401.06628)

    本研究提出了一种面向对象编程的新型评估基准，包括431个Python程序，采用pass@o度量指标来提供更全面和相关的OOP代码生成评估。评估结果显示代码专用LLMs在OOP方面表现较差，需进一步改进此领域。

    

    推进自动化编程需要健壮且全面的代码生成评估基准，然而当前的评估框架在功能式编程方面（例如HumanEval和MBPP）很大程度上忽视了面向对象编程（OOP）。为了解决这个问题，我们的研究引入了一项创新的面向对象编程重点基准，包括431个Python程序，涵盖了类和封装方法等基本的OOP概念和特性。我们提出了一种新颖的评估度量指标pass@o，针对OOP进行了改进，增强传统的pass@k度量。我们评估了23个领先的大型语言模型（LLMs），包括通用模型和专门用于代码的模型，得出了三个关键发现：1）pass@o提供了更相关和全面的OOP代码生成评估；2）尽管在FP方面表现出色，像WizardCoder这样的代码专用LLMs在OOP方面落后于像ChatGPT这样的模型；3）所有先进的LLMs在我们的OOP基准上表现不佳，突显了改进此领域的迫切需求。

    Advancing automated programming necessitates robust and comprehensive code generation benchmarks, yet current evaluation frameworks largely neglect object-oriented programming (OOP) in favor of functional programming (FP), e.g., HumanEval and MBPP. To address this, our study introduces a pioneering OOP-focused benchmark, featuring 431 Python programs that encompass essential OOP concepts and features like classes and encapsulation methods. We propose a novel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k measures. Our evaluation of 23 leading large language models (LLMs), including both general and code-specialized models, reveals three key insights: 1) pass@o offers a more relevant and comprehensive assessment for OOP code generation; 2) Despite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP compared to models like ChatGPT; 3) The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in thi
    
[^21]: TransliCo：一种用于解决多语言预训练语言模型中脚本障碍的对比学习框架

    TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models. (arXiv:2401.06620v1 [cs.CL])

    [http://arxiv.org/abs/2401.06620](http://arxiv.org/abs/2401.06620)

    本论文提出了TransliCo，一个对比学习框架，用于解决多语言预训练语言模型中的脚本障碍。通过对比不同脚本的句子及其在统一脚本中的音译，实现了不同脚本的统一表示空间。实验证明，这种方法能够改善跨语言传递的性能。

    

    书面形式中有293种脚本代表着7000多种语言。由于各种原因，许多密切相关的语言使用不同的脚本，这给多语言预训练语言模型（mPLMs）通过词汇重叠学习跨语言知识带来了困难。因此，mPLMs存在脚本障碍：来自不同脚本的表示位于不同子空间中，这是为什么涉及不同脚本语言的跨语言传递显示次优性能的强有力指标。为了解决这个问题，我们提出了一个简单的框架TransliCo，它包含Transliteration Contrastive Modeling（TCM），通过对训练数据中的句子及其在统一脚本（在我们的案例中是拉丁字母）中的音译进行对比，来微调mPLM，从而确保不同脚本的表示空间的一致性。使用Glot500-m作为我们的源模型，它是在500多种语言上预训练的mPLM，我们将其在其5％的小部分上微调

    There are 293 scripts representing over 7,000 languages in the written form. Due to various reasons, many closely related languages use different scripts, which poses difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a result, mPLMs present a script barrier: representations from different scripts are located in different subspaces, which is a strong indicator of why crosslingual transfer involving languages of different scripts shows sub-optimal performance. To address this problem, we propose a simple framework TransliCo that contains Transliteration Contrastive Modeling (TCM) to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (Latn, in our case), which ensures uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we find-tune it on a small portion (5\%) of its 
    
[^22]: 通过双向反馈机制增强大型语言模型和强化学习模型的相互合作：一个案例研究

    Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study. (arXiv:2401.06603v1 [cs.CL])

    [http://arxiv.org/abs/2401.06603](http://arxiv.org/abs/2401.06603)

    通过双向反馈机制，这个研究探索了大型语言模型(LLMs)和强化学习模型的合作。LLM充当教师，强化学习模型充当学生，它们通过递归互助实现了相互协助。这种合作提供了高级信息和实时反馈，促进了优化。

    

    大型语言模型(LLMs)已经展示出对强化学习模型(如规划和推理能力)的显著能力，然而LLMs和强化学习模型之间的合作问题仍然需要解决。在这项研究中，我们采用了师生学习框架来解决这些问题，具体是通过使用强化学习模型提供LLMs反馈，并在合作的多智能体环境中使用LLMs为强化学习模型提供高级信息。在这个框架内，LLM扮演教师角色，而强化学习模型则扮演学生角色。这两个智能体通过递归互助的方式相互协助，如“我帮你帮我帮”等。LLM智能体向强化学习智能体提供抽象信息，实现有效的探索和策略改进。反过来，强化学习智能体向LLM智能体提供反馈，提供有价值的实时信息，帮助生成更有用的标记。这种双向反馈循环促进了优化。

    Large Language Models (LLMs) have demonstrated remarkable capabilities for reinforcement learning (RL) models, such as planning and reasoning capabilities. However, the problems of LLMs and RL model collaboration still need to be solved. In this study, we employ a teacher-student learning framework to tackle these problems, specifically by offering feedback for LLMs using RL models and providing high-level information for RL models with LLMs in a cooperative multi-agent setting. Within this framework, the LLM acts as a teacher, while the RL model acts as a student. The two agents cooperatively assist each other through a process of recursive help, such as "I help you help I help." The LLM agent supplies abstract information to the RL agent, enabling efficient exploration and policy improvement. In turn, the RL agent offers feedback to the LLM agent, providing valuable, real-time information that helps generate more useful tokens. This bi-directional feedback loop promotes optimization,
    
[^23]: Prometheus-Vision: 视觉语言模型作为细粒度评估的裁判

    Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation. (arXiv:2401.06591v1 [cs.CL])

    [http://arxiv.org/abs/2401.06591](http://arxiv.org/abs/2401.06591)

    Prometheus-Vision是一个开源的视觉语言模型评估器，使用了一个名为Perception Collection的反馈数据集，训练出的模型能够理解用户定义的评分标准，与人类评估员和GPT-4V之间显示出最高的相关性。

    

    评估由视觉语言模型（VLMs）生成的长篇回答是具有挑战性的。这不仅需要检查VLM是否遵循给定的指令，还需要验证文本输出是否正确地与给定的图像相联系。受到使用LMs评估LMs的新方法的启发，本文提出使用VLMs来评估VLMs。为此，我们提出了一个名为Perception Collection的新的反馈数据集，包含15K个用户可能在评估过程中关注的定制评分标准。使用Perception Collection，我们训练了Prometheus-Vision，这是第一个能够在评估过程中理解用户定义的评分标准的开源VLM评估模型。Prometheus-Vision与人类评估员和GPT-4V之间显示出最高的Pearson相关性，显示了其用于透明和可访问的VLM评估的有效性。我们将我们的代码、数据集和模型开源在https://github.com/kaistAI/prometheus-vision。

    Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at https://github.com/kaistAI/prometheus-vision
    
[^24]: 将变形器技术应用于跨语言文档表示的映射

    Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation. (arXiv:2401.06583v1 [cs.CL])

    [http://arxiv.org/abs/2401.06583](http://arxiv.org/abs/2401.06583)

    本研究通过使用预训练的变形器模型和映射方法，探索了跨语言文档表示的方法。实验结果表明，通过映射到跨语言领域的变形器技术文档表示（TLDRs），能够有效地实现跨语言的推荐系统。

    

    推荐系统对于文档已经成为在网络上找到相关内容的工具。然而，当推荐非查询语言的文档时，这些系统存在一定限制，可能会忽视非母语的资源。本研究旨在通过使用映射到跨语言领域的变形器技术文档表示（TLDRs）来表示跨语言文档。评估了四个多语言预训练变形器模型（mBERT，mT5 XLM RoBERTa，ErnieM）在20种语言对上使用三种映射方法的效果，这些语言对代表了欧盟选择的五种语言的组合。使用Mate检索率和互惠排序等指标来衡量映射TLDRs与未映射TLDRs的效果。结果强调了通过预训练变形器和映射方法实现的跨语言表示的能力，为扩展跨语言文档表示提供了有希望的方向。

    Recommendation systems, for documents, have become tools to find relevant content on the Web. However, these systems have limitations when it comes to recommending documents in languages different from the query language, which means they might overlook resources in non-native languages. This research focuses on representing documents across languages by using Transformer Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language pairs representing combinations of five selected languages of the European Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to measure the effectiveness of mapped TLDRs compared to non-mapped ones. The results highlight the power of cross-lingual representations achieved through pre-trained transformers and mapping approaches suggesting a promising direction for expanding
    
[^25]: 在源语言中迷失：大型语言模型如何评估机器翻译的质量

    Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation. (arXiv:2401.06568v1 [cs.CL])

    [http://arxiv.org/abs/2401.06568](http://arxiv.org/abs/2401.06568)

    本论文研究了大型语言模型（LLMs）如何利用源语言和参考信息评估机器翻译的质量，并发现参考信息显著提高了评估准确性，而源语言信息有时会适得其反，表明在使用LLMs评估翻译时存在跨语言能力不足的问题。

    

    大型语言模型（LLMs）在机器翻译评估任务中取得了显著的成果，但对它们如何利用提供的数据进行评估仍存在知识差距。本研究旨在探索LLMs如何利用源语言和参考信息评估翻译，以更好地理解LLMs的工作机制。为此，我们设计了涵盖各种输入模式和模型类型的受控实验，并采用粗粒度和细粒度的提示来区分源语言和参考信息的实用性。令人惊讶的是，我们发现参考信息显著提高了评估准确性，而源语言信息有时会适得其反，表明在使用LLMs评估翻译时存在跨语言能力不足的问题。我们还对LLMs进行了翻译错误检测的元评估，观察到了类似的现象。这些发现还暗示了一种潜在的方法，即利用参考信息来改善机器翻译的质量评估任务。

    Large Language Models (LLMs) have achieved remarkable results in the machine translation evaluation task, yet there remains a gap in knowledge regarding how they utilize the provided data to conduct evaluations. This study aims to explore how LLMs leverage source and reference information in evaluating translations, with the ultimate goal of better understanding the working mechanism of LLMs. To this end, we design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information. Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations. We further conduct a meta-evaluation for translation error detection of LLMs, observing a similar phenomenon. These findings also suggest a potential
    
[^26]: Intention Analysis Prompting使得大型语言模型成为良好的越狱防御者

    Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender. (arXiv:2401.06561v1 [cs.CL])

    [http://arxiv.org/abs/2401.06561](http://arxiv.org/abs/2401.06561)

    本研究提出了一种名为Intention Analysis Prompting (IAPrompt)的方法，通过触发大型语言模型（LLMs）的自我纠正和改进能力来防御越狱攻击。实验证明，该方法能够显著减少响应中的有害行为并保持整体有用性。

    

    在面对隐蔽和复杂的越狱攻击时，将大型语言模型(LLMs)与人类价值观保持一致是一项极具挑战性的任务。在本研究中，我们提出了一种简单但非常有效的防御策略，即Intention Analysis Prompting（IAPrompt）。其原理是通过两个阶段的过程触发LLMs的内在自我纠正和改进能力：1）基本意图分析，2）与政策一致的响应。值得注意的是，IAPrompt是一种仅推断的方法，因此可以提高LLMs的安全性而不损害其有用性。在Vicuna、ChatGLM、MPT、DeepSeek和GPT-3.5上进行的广泛实验表明，IAPrompt能够持续且显著地减少响应中的有害行为（平均攻击成功率下降46.5%），同时保持整体有用性。进一步的分析揭示了我们方法的一些见解。为了保证可重复性，我们在https://github.com/alph上发布了我们的代码和脚本。

    Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreaks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis Prompting (IAPrompt). The principle behind is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IAPrompt could consistently and significantly reduce the harmfulness in response (averagely -46.5% attack success rate) and maintain the general helpfulness. Further analyses present some insights into how our method works. To facilitate reproducibility, We release our code and scripts at: https://github.com/alph
    
[^27]: 通过直觉-分析式鉴别诊断生成医疗对话

    Medical Dialogue Generation via Intuitive-then-Analytical Differential Diagnosis. (arXiv:2401.06541v1 [cs.CL])

    [http://arxiv.org/abs/2401.06541](http://arxiv.org/abs/2401.06541)

    本研究提出了一种医疗对话生成框架，通过直觉-分析式鉴别诊断（IADDx）实现了对医疗对话的生成。该方法使用直觉和分析推理来建立鉴别诊断，并通过图增强的分析方法进行细化，提高了医疗对话系统的实际应用价值。

    

    医疗对话系统因其能够提供快速诊断、治疗方案和健康咨询的潜力而引起越来越多的研究关注。在医疗对话中，正确的诊断至关重要，因为它为未来的咨询建立了基础。临床医生通常使用直觉和分析推理来形成鉴别诊断。这个推理过程假设和验证了各种可能的疾病，并努力生成全面而严谨的诊断。然而，最近关于医疗对话生成的研究忽视了建模鉴别诊断的重要性，这阻碍了这些系统的实际应用。为了解决以上问题，我们提出了一种带有直觉-分析式鉴别诊断（IADDx）的医疗对话生成框架。我们的方法通过基于检索的直觉联想进行鉴别诊断，然后通过图增强的分析方法对其进行细化。

    Medical dialogue systems have attracted growing research attention as they have the potential to provide rapid diagnoses, treatment plans, and health consultations. In medical dialogues, a proper diagnosis is crucial as it establishes the foundation for future consultations. Clinicians typically employ both intuitive and analytic reasoning to formulate a differential diagnosis. This reasoning process hypothesizes and verifies a variety of possible diseases and strives to generate a comprehensive and rigorous diagnosis. However, recent studies on medical dialogue generation have overlooked the significance of modeling a differential diagnosis, which hinders the practical application of these systems. To address the above issue, we propose a medical dialogue generation framework with the Intuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts with a differential diagnosis via retrieval-based intuitive association and subsequently refines it through a graph-enhanced anal
    
[^28]: INTERS: 使用指令调优解锁大型语言模型在搜索中的力量

    INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning. (arXiv:2401.06532v1 [cs.CL])

    [http://arxiv.org/abs/2401.06532](http://arxiv.org/abs/2401.06532)

    本研究探索了指令调优的方法，以增强大型语言模型在信息检索任务中的能力，通过引入一个新的指令调优数据集INTERS，涵盖了21个IR任务，该方法显著提升了性能。

    

    大型语言模型（LLMs）在各种自然语言处理任务中展示了令人印象深刻的能力。然而，由于许多与信息检索（IR）具体概念的不经常出现在自然语言中，它们在信息检索任务中的应用仍然具有挑战性。虽然基于提示的方法可以向LLMs提供任务描述，但它们往往在促进全面理解和执行IR任务方面存在不足，从而限制了LLMs的适用性。为了弥补这一差距，本研究探索了指令调优的潜力，以提高LLMs在IR任务中的熟练程度。我们引入了一个新的指令调优数据集INTERS，涵盖了3个基本IR类别中的21个任务：查询理解、文档理解和查询文档关系理解。数据来自43个不同的由手动编写的模板构成的数据集。我们的实证结果表明，INTERS显著提升了各种公开数据集上的性能。

    Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly a
    
[^29]: MetaHate：一个用于统一对抗仇恨言论检测的数据集

    MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection. (arXiv:2401.06526v1 [cs.CL])

    [http://arxiv.org/abs/2401.06526](http://arxiv.org/abs/2401.06526)

    MetaHate是一个用于统一对抗仇恨言论检测的数据集，对现有收集进行了详细研究，强调了它们的优点和局限性，有助于更深入地了解现有数据集，为训练更强大和适应性更强的模型铺平了道路。

    

    仇恨言论代表着一种普遍且有害的在线言论形式，通常表现为一系列不友好的言语，从令人讨厌的推文到诽谤性的帖子。随着这种言论的蔓延，它连接了全球的人们，并对被针对的个人和社区构成了重大的社会、心理和偶尔的身体威胁。目前，针对这一现象的计算语言学方法依赖于带有标签的社交媒体数据集进行训练。为了统一努力，我们的研究在解决这一问题的关键需求上取得了进展，倡导使用一个全面的元收集数据集，有助于有效对抗这个问题。我们审查了60多个数据集，并选择性地将与之相关的数据纳入MetaHate中。本文对现有收集进行了详细研究，强调了它们的优点和局限性。我们的研究结果有助于更深入地了解现有数据集，为训练更强大和适应性更强的模型铺平了道路。

    Hate speech represents a pervasive and detrimental form of online discourse, often manifested through an array of slurs, from hateful tweets to defamatory posts. As such speech proliferates, it connects people globally and poses significant social, psychological, and occasionally physical threats to targeted individuals and communities. Current computational linguistic approaches for tackling this phenomenon rely on labelled social media datasets for training. For unifying efforts, our study advances in the critical need for a comprehensive meta-collection, advocating for an extensive dataset to help counteract this problem effectively. We scrutinized over 60 datasets, selectively integrating those pertinent into MetaHate. This paper offers a detailed examination of existing collections, highlighting their strengths and limitations. Our findings contribute to a deeper understanding of the existing datasets, paving the way for training more robust and adaptable models. These enhanced mo
    
[^30]: AntEval: 量化评估智能体社交互动的信息性和表达性

    AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions. (arXiv:2401.06509v1 [cs.CL])

    [http://arxiv.org/abs/2401.06509](http://arxiv.org/abs/2401.06509)

    本研究通过使用桌面角色扮演游戏规则创建了一个环境，量化评估智能体社交互动的信息性和表达性，旨在克服隐私问题并促使智能体进行有意义、高质量的互动。

    

    尽管基于大型语言模型（LLM）的智能体已成功地模仿了各种情境中的人类行为，但复杂的、多角色社交互动在扩展环境中的领域仍未充分探索。隐私问题使捕捉和利用复杂的现实生活互动变得困难。更重要的是，缺乏定量评估方法阻碍了高质量智能体互动的追求，导致互动的信息性和表达性有限，表现为肤浅的闲聊而没有清晰的意图。在这项工作中，我们利用桌面角色扮演游戏（TRPG）的规则创建了一个有利于复杂、上下文丰富的互动的环境，强调信息性和表达性。这个虚拟环境减轻了隐私问题，并激励智能体作为游戏目标的一部分进行有意义的、高质量的互动。

    While Large Language Models (LLMs) based agents have successfully mimicked human behaviors in various scenarios, the realm of complex, multi-character social interactions within extended contexts remains underexplored. The challenge is compounded by privacy concerns, making it difficult to capture and utilize intricate real-life interactions. More importantly, the absence of quantitative evaluation methods hampers the pursuit of high-quality agent interactions, often leading to interactions that are limited in informativeness and expressiveness, characterized by superficial small talk without clear intentions. In this work, we leverage the rules of Tabletop Role-Playing Games (TRPG) to create an environment conducive to complex, context-rich interactions, emphasizing informativeness and expressiveness. This virtual setting alleviates privacy concerns and motivates agents to engage in meaningful, high-quality interactions as part of their in-game objectives. To assess these interactions
    
[^31]: BERT和DistilBERT中负责性别偏见的结构的研究

    An investigation of structures responsible for gender bias in BERT and DistilBERT. (arXiv:2401.06495v1 [cs.CL])

    [http://arxiv.org/abs/2401.06495](http://arxiv.org/abs/2401.06495)

    本文研究了BERT和DistilBERT中负责性别偏见的结构，讨论了它们在预测中的公正性问题。

    

    近年来，基于Transformer的大规模预训练语言模型（PLM）通过推动最先进技术在各种任务上的性能边界，改变了自然语言处理（NLP）领域。然而，这种性能提升伴随着复杂性的增加，因此这些模型的大小（可高达数十亿参数）在嵌入式设备或短推理时间任务上的部署受到限制。为了应对这种情况，出现了压缩模型（如DistilBERT），使得它们在越来越多影响我们日常生活的应用中可以被广泛使用。一个关键问题是PLM和其精简版本的预测公正性。在本文中，我们通过明确两个问题来对这个问题进行实证研究：（1）我们能否确定BERT（以及DistilBERT）中负责性别偏见的神经机制？（2）蒸馏是否倾向于加重或减轻这种偏见？

    In recent years, large Transformer-based Pre-trained Language Models (PLM) have changed the Natural Language Processing (NLP) landscape, by pushing the performance boundaries of the state-of-the-art on a wide variety of tasks. However, this performance gain goes along with an increase in complexity, and as a result, the size of such models (up to billions of parameters) represents a constraint for their deployment on embedded devices or short-inference time tasks. To cope with this situation, compressed models emerged (e.g. DistilBERT), democratizing their usage in a growing number of applications that impact our daily lives. A crucial issue is the fairness of the predictions made by both PLMs and their distilled counterparts. In this paper, we propose an empirical exploration of this problem by formalizing two questions: (1) Can we identify the neural mechanism(s) responsible for gender bias in BERT (and by extension DistilBERT)? (2) Does distillation tend to accentuate or mitigate ge
    
[^32]: Kun: 使用指令反向翻译的中国自对齐问题的答案优化方法

    Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. (arXiv:2401.06477v1 [cs.CL])

    [http://arxiv.org/abs/2401.06477](http://arxiv.org/abs/2401.06477)

    Kun是一种使用指令反向翻译和答案优化的方法，用于创建高质量的指导调整数据集，该方法不依赖于手动注释，通过自我筛选过程来改善和选择最有效的指令-输出对。它的主要创新在于通过算法改进提高数据的保留和清晰度，并通过创新的数据生成方法减少了手动注释的依赖。

    

    在本文中，我们介绍了一种名为Kun的新方法，用于在不依赖手动注释的情况下为大型语言模型（LLMs）创建高质量的指导调整数据集。Kun利用来自吾道、完卷和SkyPile等多个来源的未标记数据，采用基于指令反向翻译和答案优化的自我训练算法，生成了一个超过一百万个中文指导数据点的大规模数据集。该方法通过使用自我筛选过程来完善和选择最有效的指令-输出对，显著偏离传统方法。我们在多个基准测试上对6B参数的Yi模型进行了实验，结果表明Kun具有鲁棒性和可扩展性。我们方法的核心贡献在于算法的改进，增强了数据的保留和清晰度，并且创新的数据生成方法极大地减少了对昂贵和耗时的手动注释的依赖。这种方法ological方法提出了一种解决中文自对齐问题的方法，并提高了数据的准确性和质量。

    In this paper, we introduce Kun, a novel approach for creating high-quality instruction-tuning datasets for large language models (LLMs) without relying on manual annotations. Adapting a self-training algorithm based on instruction back-translation and answer polishment, Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points. This approach significantly deviates from traditional methods by using a self-curation process to refine and select the most effective instruction-output pairs. Our experiments with the 6B-parameter Yi model across various benchmarks demonstrate Kun's robustness and scalability. Our method's core contributions lie in its algorithmic advancement, which enhances data retention and clarity, and its innovative data generation approach that substantially reduces the reliance on costly and time-consuming manual annotations. This methodology presents a sc
    
[^33]: 批处理ICL: 有效，高效且无序地进行上下文学习

    Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning. (arXiv:2401.06469v1 [cs.LG])

    [http://arxiv.org/abs/2401.06469](http://arxiv.org/abs/2401.06469)

    本文提出了批处理ICL方法，通过将ICL视为一个元优化过程，开发出了一个有效、高效且无序的推理算法。通过聚合元梯度并将其应用于零-shot学习，该方法使LLM对ICL示例顺序无关，并且在实验证明其在大多数情况下优于其他排列方式，甚至超过了标准ICL的最佳顺序的性能。

    

    本文将上下文学习（ICL）视为一个元优化过程，解释了LLM对ICL示例顺序敏感的原因。这种理解使我们开发出了Batch-ICL，一种用于ICL的有效、高效且无序的推理算法。与标准的N-shot学习方法不同，Batch-ICL使用N个单独的1-shot前向计算，并聚合得到的元梯度。然后，将这些聚合的元梯度应用于零-shot学习以生成最终预测。这种批处理方法使LLM对ICL示例的顺序无关。通过大量实验证明，Batch-ICL一致优于大多数示例序列的排列方式。在某些情况下，甚至超过了标准ICL的最佳顺序的性能，同时减少了所需的计算资源。此外，我们还开发了Batch-ICL的一种新颖变体，其中包含多个"epochs"。

    In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to a zero-shot learning to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of example sequences. In some cases, it even exceeds the performance of the optimal order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple "epochs" of 
    
[^34]: 适应大型语言模型进行文档级机器翻译的研究

    Adapting Large Language Models for Document-Level Machine Translation. (arXiv:2401.06468v1 [cs.CL])

    [http://arxiv.org/abs/2401.06468](http://arxiv.org/abs/2401.06468)

    本文研究了适应大型语言模型进行文档级机器翻译的过程。实验结果显示，这些专门的模型在某些情况下超过了GPT-4的翻译性能，但在其他情况下仍然存在离标翻译问题，需要进一步改进和探索。

    

    大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了重要进展。最近的研究表明，在任务特定的微调之后，中等规模的LLMs往往胜过其更大的对应模型。在这项工作中，我们深入研究了将LLMs调整为特定语言对的文档级机器翻译（DocMT）的过程。首先，我们探讨了提示策略对下游翻译性能的影响。然后，我们进行了大量实验，使用了两种微调方法、三种LLM主干和18个涉及九种语言对的翻译任务。我们的研究结果表明，在某些情况下，这些专门的模型甚至在翻译性能上超过了GPT-4，而在其他情况下，即使它们专门在双语平行文档上进行了微调，仍然明显存在离标翻译问题。此外，我们对这些针对DocMT量身定制的LLMs进行了深入分析，探讨了如翻译准确度改善、多源信息整合等各个方面。

    Large language models (LLMs) have made significant strides in various natural language processing (NLP) tasks. Recent research shows that the moderately-sized LLMs often outperform their larger counterparts after task-specific fine-tuning. In this work, we delve into the process of adapting LLMs to specialize in document-level machine translation (DocMT) for a specific language pair. Firstly, we explore how prompt strategies affect downstream translation performance. Then, we conduct extensive experiments with two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our findings indicate that in some cases, these specialized models even surpass GPT-4 in translation performance, while they still significantly suffer from the off-target translation issue in others, even if they are exclusively fine-tuned on bilingual parallel documents. Furthermore, we provide an in-depth analysis of these LLMs tailored for DocMT, exploring aspects such as transl
    
[^35]: PersianMind: 一个跨语言的波斯语-英语大型语言模型

    PersianMind: A Cross-Lingual Persian-English Large Language Model. (arXiv:2401.06466v1 [cs.CL])

    [http://arxiv.org/abs/2401.06466](http://arxiv.org/abs/2401.06466)

    PersianMind是一个开源的双语大型语言模型，通过在波斯语中展现与闭源的GPT-3.5-turbo相当的性能，并利用迁移学习在不同语言间传递任务知识的优势，解决了开源模型在非英文语言上性能不佳的问题。

    

    大型语言模型在各种语言任务中展现出了卓越的能力，并具备广泛的多领域知识。虽然它们在英语方面表现最优，但它们在其他语言方面的能力也很显著。与此相反，如LLaMa这样的开源模型主要是在英文数据集上训练的，导致在非英文语言中表现不佳。在本文中，我们介绍了PersianMind，一个开源的双语大型语言模型，在波斯语中展示了与闭源的GPT-3.5-turbo相当的性能。通过将LLaMa2的词汇表扩展10,000个波斯语标记，并训练约20亿个波斯语标记的数据集，我们展示了我们的方法保留了模型的英语知识并利用迁移学习在不同语言间传递任务知识的优势。

    Large language models demonstrate remarkable proficiency in various linguistic tasks and have extensive knowledge across various domains. Although they perform best in English, their ability in other languages is notable too. In contrast, open-source models, such as LLaMa, are primarily trained on English datasets, resulting in poor performance in non-English languages. In this paper, we introduce PersianMind, an open-source bilingual large language model which demonstrates comparable performance to closed-source GPT-3.5-turbo in the Persian language. By expanding LLaMa2's vocabulary with 10,000 Persian tokens and training it on a dataset comprising nearly 2 billion Persian tokens, we show that our approach preserves the model's English knowledge and employs transfer learning to excel at transferring task knowledge from one language to another.
    
[^36]: 代码之间的界限：揭示机器和人类程序员之间不同的模式

    Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])

    [http://arxiv.org/abs/2401.06461](http://arxiv.org/abs/2401.06461)

    本文通过分析代码的属性，揭示了机器和人类代码之间的独特模式，尤其是结构分割对于识别代码来源很关键。基于这些发现，我们提出了一种名为DetectCodeGPT的新方法来检测机器生成的代码。

    

    大型语言模型在代码生成方面取得了显著的进展，但它们模糊了机器和人类源代码之间的区别，导致软件产物的完整性和真实性问题。本文通过对代码长度、词汇多样性和自然性等属性的严格分析，揭示了机器和人类代码固有的独特模式。在我们的研究中特别注意到，代码的结构分割是识别其来源的关键因素。基于我们的发现，我们提出了一种名为DetectCodeGPT的新型机器生成代码检测方法，该方法改进了DetectGPT。

    Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
    
[^37]: BOK-VQA: 基于外部知识的双语视觉问答系统通过图表示预训练

    BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via Graph Representation Pretraining. (arXiv:2401.06443v1 [cs.CL])

    [http://arxiv.org/abs/2401.06443](http://arxiv.org/abs/2401.06443)

    本研究提出了BOK-VQA数据集，包含多语言的视觉问答数据以及与问题-回答内容相关的知识信息。通过以图嵌入的形式预训练数据的知识信息，可以有效地将外部知识注入VQA系统中，实现更好的问答效果。

    

    目前的生成模型研究方向，如最近开发的GPT4，旨在为多模态和多语言输入寻找相关的知识信息以提供答案。根据这些研究情况，对多语言评估视觉问答（VQA）任务的需求，作为多模态系统的代表任务，逐渐增加。因此，我们在本研究中提出了一种能够扩展到多语言的双语外部知识VQA（BOK-VQA）数据集。所提出的数据包括17K张图片，17K个韩语和英语的问题-回答对以及与问题-回答内容相关的28K个知识信息实例。我们还提出了一个框架，通过以图嵌入的形式预训练BOK-VQA数据的知识信息，可以有效地将知识信息注入VQA系统中。最后，通过深入分析，我们展示了构建训练数据中包含的知识信息的实际效果。

    The current research direction in generative models, such as the recently developed GPT4, aims to find relevant knowledge information for multimodal and multilingual inputs to provide answers. Under these research circumstances, the demand for multilingual evaluation of visual question answering (VQA) tasks, a representative task of multimodal systems, has increased. Accordingly, we propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that can be extended to multilingualism. The proposed data include 17K images, 17K question-answer pairs for both Korean and English and 280K instances of knowledge information related to question-answer content. We also present a framework that can effectively inject knowledge information into a VQA system by pretraining the knowledge information of BOK-VQA data in the form of graph embeddings. Finally, through in-depth analysis, we demonstrated the actual effect of the knowledge information contained in the constructed training data
    
[^38]: 从自动化到增强：大型语言模型提升作文评分领域

    From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape. (arXiv:2401.06431v1 [cs.CL])

    [http://arxiv.org/abs/2401.06431](http://arxiv.org/abs/2401.06431)

    本研究调查了大型语言模型（LLM）在自动化作文评分系统中的有效性，并发现LLM AES系统具有更高的准确性、一致性、普适性和可解释性。此外，LLM还能提升人类评分员的性能。

    

    对于第二语言学习者来说，接收即时个性化反馈非常重要，当人类教师无法提供时，自动化作文评分系统是一种重要资源。本研究调查了大型语言模型（LLM），特别是GPT-4和经过微调的GPT-3.5，作为AES工具的有效性。我们基于公共和私有数据集进行了一系列全面的实验，突出了LLM AES系统的显着优势，包括更高的准确性、一致性、普适性和可解释性，而经过微调的GPT-3.5超越了传统评分模型。此外，我们进行了LLM辅助的人工评估实验，涉及初学者和专家评分员。一个关键的发现是，LLM不仅能自动化评分过程，还能提升人类评分员的性能。当初学者评分员获得LLM生成的反馈时，其准确性与专家水平相当，同时专家变得更加高效。

    Receiving immediate and personalized feedback is crucial for second-language learners, and Automated Essay Scoring (AES) systems are a vital resource when human instructors are unavailable. This study investigates the effectiveness of Large Language Models (LLMs), specifically GPT-4 and fine-tuned GPT-3.5, as tools for AES. Our comprehensive set of experiments, conducted on both public and private datasets, highlights the remarkable advantages of LLM-based AES systems. They include superior accuracy, consistency, generalizability, and interpretability, with fine-tuned GPT-3.5 surpassing traditional grading models. Additionally, we undertake LLM-assisted human evaluation experiments involving both novice and expert graders. One pivotal discovery is that LLMs not only automate the grading process but also enhance the performance of human graders. Novice graders when provided with feedback generated by LLMs, achieve a level of accuracy on par with experts, while experts become more effici
    
[^39]: 不可能任务：语言模型

    Mission: Impossible Language Models. (arXiv:2401.06416v1 [cs.CL])

    [http://arxiv.org/abs/2401.06416](http://arxiv.org/abs/2401.06416)

    本文为了支持大型语言模型(LLMs)能够学习不可能的语言的观点，开发了一组人工合成的不可能语言，并通过评估GPT-2小型模型的学习能力得出了结论。

    

    Chomsky和其他人直接声称，大型语言模型(LLMs)能够学习人类无法学习的可能和不可能的语言。然而，很少有发表的实验证据支持这样的说法。在这里，我们通过系统地改变英文数据的词序和语法规则，开发了一组不可能的合成语言，每种语言的复杂程度不同。这些语言位于一个不可能的连续体上：一端是本质上不可能的语言，例如英文单词的随机和不可逆的洗牌，而另一端是在语言学上常被认为是不可能的语言，特别是基于计算词位置的规则。我们报告了广泛的评估来评估GPT-2小型模型学习这些无可争议的不可能语言的能力，并且至关重要的是，在整个过程中进行了这些评估。

    Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout
    
[^40]: 关于我：使用自我描述的网页来记录英语预训练数据过滤器的影响

    AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters. (arXiv:2401.06408v1 [cs.CL])

    [http://arxiv.org/abs/2401.06408](http://arxiv.org/abs/2401.06408)

    这项研究调查了十个质量和语言识别过滤器对不同社交维度变化的网页的影响。实验发现在数据筛选过程中存在隐含的偏好，一些质量分类器类似于主题过滤器，而语言识别可能会忽视某些地区的英语内容。我们的研究为促进更公正和全面的模型开发提供了洞察。

    

    大型语言模型（LLM）的能力来源于它们的预训练数据，模型的开发始于数据的筛选。然而，在这个初步阶段决定保留哪些数据或移除哪些数据的决策常常没有被充分审查。在我们的工作中，我们将网页文本与其社交和地理背景联系起来。我们创建了一个新的数据集，包含1030万个网页创建者的自我描述，并提取了关于他们的个人信息以及他们来自哪里的信息：他们的兴趣领域、社交角色和地理归属。然后，我们进行了第一项研究，调查了十个“质量”和英语语言识别（langID）过滤器对这些社交维度变化的网页的影响。我们的实验揭示了数据筛选中一系列隐含的偏好：我们展示出一些质量分类器的作用类似于主题领域过滤器，而langID可能会忽视世界某些地区的英语内容。总体而言，我们希望我们的工作能够提供对数据筛选中隐含偏好的洞察，以促进更公正和全面的模型开发。

    Large language models' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage is under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten "quality" and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will enc
    
[^41]: DevEval: 评估实际软件项目中的代码生成

    DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])

    [http://arxiv.org/abs/2401.06401](http://arxiv.org/abs/2401.06401)

    本文提出了一个名为DevEval的新基准测试，用于评估实际软件项目中的代码生成。与之前的基准测试相比，DevEval在真实的项目分布、充足的依赖和足够规模的项目背景等方面更贴合实际。通过对五个流行的大型语言模型进行评估，我们揭示了它们在代码生成中的实际能力。

    

    如何评估大型语言模型（LLMs）在代码生成中的表现是一个开放的问题。许多基准测试已经提出，但是与实际软件项目不一致，例如虚构的程序分布，依赖不足和小规模项目背景。因此，LLMs在实际项目中的能力还不清楚。在本文中，我们提出了一个名为DevEval的新基准测试，与开发人员在实际项目中的经验相吻合。DevEval通过一个严格的流程收集到了来自119个实际项目的2690个样本，涵盖10个领域。与之前的基准测试相比，DevEval在多个维度上与实际项目相吻合，例如真实的程序分布，充足的依赖和足够规模的项目背景。我们在DevEval上评估了五个流行的LLMs（例如gpt-4，gpt-3.5-turbo，CodeLLaMa和StarCoder），并揭示了它们在代码生成中的实际能力。例如，gpt-3.5-turbo的最高Pass@1只有42。

    How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experim
    
[^42]: 通过将大型语言模型与合成数据训练的VQA模型连接起来，将可视问答从合成问题泛化到人工问题

    Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model. (arXiv:2401.06400v1 [cs.CL])

    [http://arxiv.org/abs/2401.06400](http://arxiv.org/abs/2401.06400)

    本文提出了一种新方法CoQAH，通过连接大型语言模型和训练于合成数据上的VQA模型的QA交互序列，实现了将可视问答从合成问题泛化到人工问题，并在两种类型的人工问题数据集上取得了最先进的准确率，超过了通用视觉语言模型、VQA模型和医学基础模型。

    

    可视问答（VQA）是一个给定图像并就该图像提出一系列问题的任务。构建一个高效的VQA算法需要大量的QA数据，而且非常昂贵。根据模板生成合成的QA对是获得数据的一种实用方法。然而，训练于这些数据上的VQA模型在复杂的人工问题上表现不佳。为了解决这个问题，我们提出了一种新的方法，称为“人工问题连锁问答”（CoQAH）。CoQAH利用一个大型语言模型与一个训练于合成数据上的VQA模型之间的QA交互序列来推理和推导人工问题的逻辑答案。我们在两种类型的人工问题VQA数据集上测试了CoQAH的效果，包括3D渲染图像和胸部X线图像，并发现它在两种类型的数据上都达到了最先进的准确率。值得注意的是，CoQAH在通用视觉语言模型、VQA模型和医学基础模型方面的表现也超过了。

    Visual question answering (VQA) is a task where an image is given, and a series of questions are asked about the image. To build an efficient VQA algorithm, a large amount of QA data is required which is very expensive. Generating synthetic QA pairs based on templates is a practical way to obtain data. However, VQA models trained on those data do not perform well on complex, human-written questions. To address this issue, we propose a new method called {\it chain of QA for human-written questions} (CoQAH). CoQAH utilizes a sequence of QA interactions between a large language model and a VQA model trained on synthetic data to reason and derive logical answers for human-written questions. We tested the effectiveness of CoQAH on two types of human-written VQA datasets for 3D-rendered and chest X-ray images and found that it achieved state-of-the-art accuracy in both types of data. Notably, CoQAH outperformed general vision-language models, VQA models, and medical foundation models with no
    
[^43]: 修正印度机器翻译任务中常用数据集的误译方法

    An approach for mistranslation removal from popular dataset for Indic MT Task. (arXiv:2401.06398v1 [cs.CL])

    [http://arxiv.org/abs/2401.06398](http://arxiv.org/abs/2401.06398)

    本研究提出了一种算法，可以从印度机器翻译任务中的常用数据集中去除误译，以提高机器翻译的质量。

    

    将内容从一种语言转换为另一种语言的计算机系统被称为机器翻译（MT）。为了确保有效的翻译，保留源语言的上下文和词汇解释，各种技术已经出现。端到端神经机器翻译（NMT）是一种流行的技术，现在被广泛应用于实际的MT系统中。MT系统需要大量的平行数据集（一个语言中的句子与另一门语言的翻译），这些数据集对于MT系统在训练阶段学习语言结构和模式至关重要。其中一种数据集是Samanantar，这是最大的公开可访问的印度语言（ILs）平行数据集。由于该语料库来自各个来源，因此包含了许多错误的翻译。因此，使用该数据集构建的MT系统无法发挥其正常潜力。在本文中，我们提出了一个算法来去除误译。

    The conversion of content from one language to another utilizing a computer system is known as Machine Translation (MT). Various techniques have come up to ensure effective translations that retain the contextual and lexical interpretation of the source language. End-to-end Neural Machine Translation (NMT) is a popular technique and it is now widely used in real-world MT systems. Massive amounts of parallel datasets (sentences in one language alongside translations in another) are required for MT systems. These datasets are crucial for an MT system to learn linguistic structures and patterns of both languages during the training phase. One such dataset is Samanantar, the largest publicly accessible parallel dataset for Indian languages (ILs). Since the corpus has been gathered from various sources, it contains many incorrect translations. Hence, the MT systems built using this dataset cannot perform to their usual potential. In this paper, we propose an algorithm to remove mistranslati
    
[^44]: 自适应数据增强用于方面情感四元预测

    Adaptive Data Augmentation for Aspect Sentiment Quad Prediction. (arXiv:2401.06394v1 [cs.CL])

    [http://arxiv.org/abs/2401.06394](http://arxiv.org/abs/2401.06394)

    本文提出了一种自适应数据增强（ADA）框架来解决方面情感四元预测（ASQP）任务中的数据不平衡问题。实验证实，数据增强可以改善ASQP任务的性能，而ADA方法优于简单的数据过采样方法。

    

    方面情感四元预测（ASQP）旨在预测给定句子的四元情感元素，这是方面情感分析领域中的关键任务。然而，在ASQP任务中，数据不平衡问题尚未得到足够的重视。在本文中，我们将问题分为两个方面：四元模式不平衡和方面类别不平衡，并提出了一种自适应数据增强（ADA）框架来解决不平衡问题。具体而言，通过具有条件函数的数据增强过程，自适应增强尾部四元模式和方面类别，缓解ASQP中的数据不平衡。在之前的研究基础上，我们还进一步探索了生成框架，通过引入类别先验知识和语法引导解码目标，提取完整的四元。实验证实，数据增强可以改善ASQP任务中的不平衡问题，并且所提出的ADA方法优于简单的数据过采样方法。

    Aspect sentiment quad prediction (ASQP) aims to predict the quad sentiment elements for a given sentence, which is a critical task in the field of aspect-based sentiment analysis. However, the data imbalance issue has not received sufficient attention in ASQP task. In this paper, we divide the issue into two-folds, quad-pattern imbalance and aspect-category imbalance, and propose an Adaptive Data Augmentation (ADA) framework to tackle the imbalance issue. Specifically, a data augmentation process with a condition function adaptively enhances the tail quad patterns and aspect categories, alleviating the data imbalance in ASQP. Following previous studies, we also further explore the generative framework for extracting complete quads by introducing the category prior knowledge and syntax-guided decoding target. Experimental results demonstrate that data augmentation for imbalance in ASQP task can improve the performance, and the proposed ADA method is superior to naive data oversampling.
    
[^45]: 我应该说什么？-与AI和自然语言界面的交互

    What should I say? -- Interacting with AI and Natural Language Interfaces. (arXiv:2401.06382v1 [cs.HC])

    [http://arxiv.org/abs/2401.06382](http://arxiv.org/abs/2401.06382)

    随着人工智能技术的普及，研究人类与AI的交互变得越来越重要。本研究通过探索人类与AI交互过程中心智表征的建立，旨在帮助实现成功和轻松的沟通。

    

    随着人工智能（AI）技术越来越普遍，探索人类如何与AI交互变得越来越重要。人工智能交互（HAI）子领域从人机交互（HCI）领域中出现，旨在研究这个问题。许多交互模式已经实施，但对于使用这些更像人类本质的替代界面所需认知的变化以及使用这些界面的认知科学影响，了解还很有限。先前的研究表明，成功和轻松的沟通关键在于心智表征，然而，在与AI交互时，心智表征是如何建立的仍不甚了解。

    As Artificial Intelligence (AI) technology becomes more and more prevalent, it becomes increasingly important to explore how we as humans interact with AI. The Human-AI Interaction (HAI) sub-field has emerged from the Human-Computer Interaction (HCI) field and aims to examine this very notion. Many interaction patterns have been implemented without fully understanding the changes in required cognition as well as the cognitive science implications of using these alternative interfaces that aim to be more human-like in nature. Prior research suggests that theory of mind representations are crucial to successful and effortless communication, however very little is understood when it comes to how theory of mind representations are established when interacting with AI.
    
[^46]: 如何使Johnny说服LLMs越狱：通过人性化LLMs重新思考对AI安全的挑战

    How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. (arXiv:2401.06373v1 [cs.CL])

    [http://arxiv.org/abs/2401.06373](http://arxiv.org/abs/2401.06373)

    本文通过将LLMs视为人类交流者，探索了每天语言互动和AI安全之间忽视的交叉点，并提出了一种通过说服LLMs进行越狱的方法。研究结果表明，说服显著提高了越狱性能，在多个风险类别上均取得了超过92%的攻击成功率。

    

    大多数传统的AI安全研究将AI模型视为机器，并集中在由安全专家开发的基于算法的攻击上。随着大型语言模型（LLMs）的普及和竞争力越来越强，非专家用户在日常互动中也可能产生风险。本文介绍了一种新的视角，将LLMs作为类似人类的交流者来越狱，以探索每天语言互动和AI安全之间被忽视的交叉点。具体而言，我们研究如何说服LLMs越狱。首先，我们提出了一个从几十年的社会科学研究中得出的说服分类法。然后，我们应用这个分类法来自动生成可解释的说服对抗提示（PAP）来越狱LLMs。结果显示，说服显著提高了越狱性能，在所有风险类别上PAP在Llama 2-7b Chat、GPT-3.5和GPT-4上的攻击成功率在10次试验中均超过92%，超过了最近的基于算法的攻击。

    Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focu
    
[^47]: 从半事实中学习：一种去偏见和语义感知的广义关系发现框架

    Learning from Semi-Factuals: A Debiased and Semantic-Aware Framework for Generalized Relation Discovery. (arXiv:2401.06327v1 [cs.CL])

    [http://arxiv.org/abs/2401.06327](http://arxiv.org/abs/2401.06327)

    这项研究介绍了一种新的任务，称为广义关系发现（GRD），用于开放世界的关系抽取。研究围绕如何减轻由标记的预定义关系造成的模型偏见和确定新关系的特定语义进行了讨论，并提出了一个名为SFGRD的框架来解决这些问题。

    

    我们引入了一种新颖的任务，称为广义关系发现（GRD），用于开放世界的关系抽取。GRD旨在通过将实例分配到簇并为这些簇提供具体含义，来识别现有预定义关系中的未标记实例或发现新的关系。GRD的主要挑战是如何减轻由标记的预定义关系造成的严重模型偏见，以学习有效的关系表示，并在对未标记实例进行分类或聚类时确定新关系的特定语义。然后，我们提出了一个新的框架SFGRD来解决上述问题，通过从半事实中学习进行两个阶段的操作。第一个阶段是通过三视图去偏见关系表示模块来实现半事实生成，其中我们将每个原始句子作为主视图，并设计两个去偏见视图来生成该句子的半事实示例。第二个阶段是半事实思考

    We introduce a novel task, called Generalized Relation Discovery (GRD), for open-world relation extraction. GRD aims to identify unlabeled instances in existing pre-defined relations or discover novel relations by assigning instances to clusters as well as providing specific meanings for these clusters. The key challenges of GRD are how to mitigate the serious model biases caused by labeled pre-defined relations to learn effective relational representations and how to determine the specific semantics of novel relations during classifying or clustering unlabeled instances. We then propose a novel framework, SFGRD, for this task to solve the above issues by learning from semi-factuals in two stages. The first stage is semi-factual generation implemented by a tri-view debiased relation representation module, in which we take each original sentence as the main view and design two debiased views to generate semi-factual examples for this sentence. The second stage is semi-factual thinking e
    
[^48]: TTS中前端文本处理的多任务学习

    Multi-Task Learning for Front-End Text Processing in TTS. (arXiv:2401.06321v1 [cs.CL])

    [http://arxiv.org/abs/2401.06321](http://arxiv.org/abs/2401.06321)

    我们提出了一个多任务学习模型，用于在TTS前端处理文本。我们的模型同时解决了文本归一化、词性标注和同形异义词消歧这三个任务，并结合了预训练的语言模型以提高性能。通过逐任务消融实验证明，我们的模型在所有三个任务上表现最好。我们还提供了一个包含各种上下文的新HD数据集，用于同形异义词消歧任务的评估。

    

    我们提出了一个多任务学习模型，用于在文本到语音（TTS）前端中同时执行三个常见任务：文本归一化（TN），词性标注（POS）和同形异义词消歧（HD）。我们的框架利用一个类似树形的结构，其中一棵树学习共享表示，然后是单独的任务特定头部。我们进一步加入了一个预训练的语言模型，利用其内置的词汇和上下文知识，并研究如何最好地使用其嵌入以最有效地使我们的多任务模型受益。通过逐任务的消融实验证明，我们在所有三个任务上训练的完整模型相比于单独或部分任务组合训练的模型表现出最强的综合性能，验证了我们的MTL框架的优势。最后，我们介绍了一个新的HD数据集，其中包含各种上下文中平衡数量的句子，用于不同同形异义词及其发音。我们证明，在不同上下文中训练的模型在同形异义词消歧任务上取得了更好的性能。

    We propose a multi-task learning (MTL) model for jointly performing three tasks that are commonly solved in a text-to-speech (TTS) front-end: text normalization (TN), part-of-speech (POS) tagging, and homograph disambiguation (HD). Our framework utilizes a tree-like structure with a trunk that learns shared representations, followed by separate task-specific heads. We further incorporate a pre-trained language model to utilize its built-in lexical and contextual knowledge, and study how to best use its embeddings so as to most effectively benefit our multi-task model. Through task-wise ablations, we show that our full model trained on all three tasks achieves the strongest overall performance compared to models trained on individual or sub-combinations of tasks, confirming the advantages of our MTL framework. Finally, we introduce a new HD dataset containing a balanced number of sentences in diverse contexts for a variety of homographs and their pronunciations. We demonstrate that inco
    
[^49]: 零样本生成式大型语言模型用于系统性综述筛选自动化

    Zero-shot Generative Large Language Models for Systematic Review Screening Automation. (arXiv:2401.06320v1 [cs.IR])

    [http://arxiv.org/abs/2401.06320](http://arxiv.org/abs/2401.06320)

    本研究调查了使用零样本生成式大型语言模型进行系统性综述自动筛选的有效性，结果显示指导微调和校准技术在筛选中起到重要作用，并且与零样本模型的集成相结合可以显著节省筛选时间。

    

    系统性综述对于基于证据的医学非常重要，它们综合分析了特定问题的已发表研究结果。进行此类综述通常需要大量的资源和时间，特别是在筛选阶段，需要评估出版物摘要是否应包括在综述中。本研究调查了使用零样本大型语言模型（LLM）进行自动筛选的有效性。我们评估了八种不同的LLM的效果，并研究了一种使用预定义的召回阈值的校准技术，用于确定是否应将出版物包括在系统性综述中。我们的全面评估使用了五个标准测试集，结果显示指导微调在筛选中起到了重要作用，校准使LLMs在实现目标召回方面更实用，并且将这两者与零样本模型的集成相结合与现有技术相比节省了大量筛选时间。

    Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.
    
[^50]: 超越表面：文本到图像生成中视觉刻板印象的全球规模分析

    Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation. (arXiv:2401.06310v1 [cs.CV])

    [http://arxiv.org/abs/2401.06310](http://arxiv.org/abs/2401.06310)

    本论文提出了一种多方面的方法，利用现有的文本资源，基于135个全球范围内的身份群体对文本到图像生成（T2I）模型生成的图像中的地理文化刻板印象进行评估。研究结果表明，刻板属性在图像中的存在可能性是刻板属性的三倍。

    

    近期的研究已经强调了在文本到图像生成（T2I）模型生成的人物形象中存在的不同身份群体的刻板印象问题。然而，这些现有方法存在一些关键限制，包括在评估中对全球身份群体的覆盖率明显不足，以及相关刻板印象的范围。此外，它们通常缺乏对本质上是视觉刻板印象（如“瘦弱”或“墨西哥草帽”）和文化相关的刻板印象（如“吸引人”或“恐怖分子”）之间的重要区别。在本研究中，我们采用多方面的方法来解决这些限制，利用现有的文本资源来将我们对来自T2I模型生成的图像中与地理文化相关的刻板印象的评估进行基础绑定。我们使用现有的刻板印象基准来识别和评估全球范围内涉及135个基于国籍的身份群体的视觉刻板印象。我们证明，在图像中存在刻板印象的可能性是刻板属性的三倍。

    Recent studies have highlighted the issue of stereotypical depictions for people of different identity groups in Text-to-Image (T2I) model generations. However, these existing approaches have several key limitations, including a noticeable lack of coverage of global identity groups in their evaluation, and the range of their associated stereotypes. Additionally, they often lack a critical distinction between inherently visual stereotypes, such as `underweight' or `sombrero', and culturally dependent stereotypes like `attractive' or `terrorist'. In this work, we address these limitations with a multifaceted approach that leverages existing textual resources to ground our evaluation of geo-cultural stereotypes in the generated images from T2I models. We employ existing stereotype benchmarks to identify and evaluate visual stereotypes at a global scale, spanning 135 nationality-based identity groups. We demonstrate that stereotypical attributes are thrice as likely to be present in images
    
[^51]: 基于自信度的LLM上下文学习中的演示选择

    Misconfidence-based Demonstration Selection for LLM In-Context Learning. (arXiv:2401.06301v1 [cs.CL])

    [http://arxiv.org/abs/2401.06301](http://arxiv.org/abs/2401.06301)

    这项研究提出了一种名为ICR的方法，通过基于自信度的策略性演示选择，以降低LLM输出与实际输入输出之间的差异，从而克服了当前LLM上下文学习中演示选择的难题。

    

    借助大型语言模型（LLMs）进行上下文学习在快速适应各种任务方面表现出色。然而，其成功关键在于仔细选择演示，这在实践中仍然是一个障碍。目前解决这个问题的方法要么依赖于难以获取的外部监督，要么需要频繁与LLMs进行交互，导致成本高昂。我们提出了一种新的方法，称为上下文反思（ICR），以解决这些挑战。ICR通过策略性地选择演示来减少LLM输出与实际输入输出映射之间的差异。具体而言，ICR从随机的初始演示集开始，并逐步进行优化。在每个步骤中，它分析候选示例池，并通过一种称为"自信度"的新指标来确定最有可能挑战LLM当前理解的示例。然后选择这些最困惑的示例以替换当前集合中信息较少的演示。我们进行了全面的评估

    In-context learning with large language models (LLMs) excels at adapting to various tasks rapidly. However, its success hinges on carefully selecting demonstrations, which remains an obstacle in practice. Current approaches to this problem either rely on hard-to-acquire external supervision or require frequent interactions with LLMs, resulting in high costs. We propose a new method called In-Context Reflection (ICR) to overcome these challenges. ICR strategically selects demonstrations to reduce the discrepancy between the LLM's outputs and the actual input-output mappings. Specifically, ICR starts with a random set of initial demonstrations, then iteratively refines it. In each step, it analyzes a pool of candidate examples and identifies the ones most likely to challenge the LLM's current understanding, measured by a new metric called misconfidence. These most confusing examples are then selected to replace the less informative demonstrations in the current set. Our comprehensive eva
    
[^52]: LEGOBench：科学模型排行榜生成基准测试

    LEGOBench: Leaderboard Generation Benchmark for Scientific Models. (arXiv:2401.06233v1 [cs.CL])

    [http://arxiv.org/abs/2401.06233](http://arxiv.org/abs/2401.06233)

    LEGOBench是一个评估生成科学模型排行榜系统的基准测试，使用22年来的论文预印本数据和PapersWithCode门户上的机器学习排行榜的数据，初步结果显示自动排行榜生成存在显著性能差距。

    

    随着论文提交数量的不断增加，难以及时了解最新的最先进研究成果成为了一个难题。为了解决这个挑战，我们引入了LEGOBench，这是一个评估生成排行榜系统的基准测试。LEGOBench由22年来在arXiv上提交的预印本数据和PapersWithCode门户上的11,000多个机器学习排行榜组成。我们评估了四种传统的基于图的排名变体和三种最近提出的大型语言模型的性能。我们的初步结果显示自动排行榜生成存在显著的性能差距。代码可在https://github.com/lingo-iitgn/LEGOBench获取，数据集托管在https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c。

    The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce LEGOBench, a benchmark for evaluating systems that generate leaderboards. LEGOBench is curated from 22 years of preprint submission data in arXiv and more than 11,000 machine learning leaderboards in the PapersWithCode portal. We evaluate the performance of four traditional graph-based ranking variants and three recently proposed large language models. Our preliminary results show significant performance gaps in automatic leaderboard generation. The code is available on https://github.com/lingo-iitgn/LEGOBench and the dataset is hosted on https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c .
    
[^53]: 学习无监督的语义文档表示以进行细粒度的基于方面的情感分析

    Learning Unsupervised Semantic Document Representation for Fine-grained Aspect-based Sentiment Analysis. (arXiv:2401.06210v1 [cs.LG])

    [http://arxiv.org/abs/2401.06210](http://arxiv.org/abs/2401.06210)

    这篇论文研究了学习无监督的语义文档表示以进行细粒度的基于方面的情感分析。通过克服现有方法的困难，实验证明该模型在各个任务上的性能优于最先进方法。

    

    文档表示是机器理解中许多自然语言处理任务的核心。以无监督方式学习的一般表示保留了通用性，可用于各种应用。在实践中，情感分析（SA）是一个挑战性的任务，被认为与语义密切相关，并经常用于评估一般表示。现有的无监督文档表示学习方法可以分为两类：序列方法（显式考虑单词的顺序）和非序列方法（不显式考虑顺序）。然而，它们都有各自的缺点。在本文中，我们提出了一个模型，克服了这两类方法遇到的困难。实验证明，我们的模型在流行的SA数据集和细粒度的基于方面的SA上优于现有的最先进方法。

    Document representation is the core of many NLP tasks on machine understanding. A general representation learned in an unsupervised manner reserves generality and can be used for various applications. In practice, sentiment analysis (SA) has been a challenging task that is regarded to be deeply semantic-related and is often used to assess general representations. Existing methods on unsupervised document representation learning can be separated into two families: sequential ones, which explicitly take the ordering of words into consideration, and non-sequential ones, which do not explicitly do so. However, both of them suffer from their own weaknesses. In this paper, we propose a model that overcomes difficulties encountered by both families of methods. Experiments show that our model outperforms state-of-the-art methods on popular SA datasets and a fine-grained aspect-based SA by a large margin.
    
[^54]: EASYTOOL: 使用简洁的工具指示增强基于LLM的代理

    EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction. (arXiv:2401.06201v1 [cs.CL])

    [http://arxiv.org/abs/2401.06201](http://arxiv.org/abs/2401.06201)

    EASYTOOL是一个将多样化而冗长的工具文档转化为统一而简洁的工具指示的框架，用于增强基于LLM的代理的能力。通过从多个来源提取关键信息，并提供标准化的工具描述和功能，EasyTool显著降低了标记消耗，并提高了在真实场景中的工具利用性能。

    

    为了解决复杂的现实世界任务，越来越多的关注点放在了在大型语言模型(LLM)应用中的工具利用上。为了开发基于LLM的代理，通常需要LLM从不同的工具文档中理解许多工具功能。但这些文档可能是多样化的、冗余的或不完整的，这极大地影响了LLM在使用工具方面的能力。为了解决这个问题，我们介绍了EASYTOOL，这是一个将多样化而冗长的工具文档转化为统一而简洁的工具指示，以便更容易地使用工具。EasyTool从不同来源的广泛工具文档中提取出关键信息，并详细说明一个统一的接口（即工具指示），为基于LLM的代理提供标准化的工具描述和功能。对多个不同任务的广泛实验表明，EasyTool可以显著减少标记的消耗，并改善在现实场景中的工具利用性能。

    To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our co
    
[^55]: CrisisKAN: 知识注入和可解释的多模态注意力网络用于危机事件分类

    CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network for Crisis Event Classification. (arXiv:2401.06194v1 [cs.LG])

    [http://arxiv.org/abs/2401.06194](http://arxiv.org/abs/2401.06194)

    CrisisKAN是一种知识注入和可解释的多模态注意力网络，用于危机事件分类。它通过结合图像、文本和维基百科的外部知识来弥合图像和文本模态之间的语义差距，并解释模型的结果，以建立在高风险情况下的信任。

    

    广泛使用社交媒体已成为实时信息（如图像、文本或二者兼有）识别各种事件的新兴来源。尽管图像和文本的事件分类迅速发展，但最先进的模型仍然难以弥合由于不一致的编码导致的图像和文本模态之间的语义差距。此外，模型的黑匣子特性无法解释模型的结果，无法在灾难、大流行等高风险情况下建立信任。此外，社交媒体帖子的字数限制可能会对特定事件引入偏见。为了解决这些问题，我们提出了CrisisKAN，一种新颖的知识注入和可解释的多模态注意力网络，将图像和文本与维基百科的外部知识相结合，用于分类危机事件。为了丰富对文本信息的上下文特定理解，我们使用了提出的维基百科知识提取方法。

    Pervasive use of social media has become the emerging source for real-time information (like images, text, or both) to identify various events. Despite the rapid growth of image and text-based event classification, the state-of-the-art (SOTA) models find it challenging to bridge the semantic gap between features of image and text modalities due to inconsistent encoding. Also, the black-box nature of models fails to explain the model's outcomes for building trust in high-stakes situations such as disasters, pandemic. Additionally, the word limit imposed on social media posts can potentially introduce bias towards specific events. To address these issues, we proposed CrisisKAN, a novel Knowledge-infused and Explainable Multimodal Attention Network that entails images and texts in conjunction with external knowledge from Wikipedia to classify crisis events. To enrich the context-specific understanding of textual information, we integrated Wikipedia knowledge using proposed wiki extraction
    
[^56]: 使用Bark、mBART和经过微调的XLSR Wav2Vec2的端到端印地语到英语语音转换

    End to end Hindi to English speech conversion using Bark, mBART and a finetuned XLSR Wav2Vec2. (arXiv:2401.06183v1 [eess.AS])

    [http://arxiv.org/abs/2401.06183](http://arxiv.org/abs/2401.06183)

    本论文提出了一个端到端的语音转换框架，用于印地语到英语的转换，采用了Bark、mBART和经过微调的XLSR Wav2Vec2等先进技术，为跨语言交流提供了统一而无缝的解决方案。

    

    长期以来，语音一直是有效沟通和连接的障碍，在我们日益互联的世界中仍然具有挑战性。这篇研究论文介绍了一种针对印地语到英语翻译量身定制的端到端语音转换框架，最终实现了英文音频的合成。通过整合XLSR Wav2Vec2用于自动语音识别（ASR），mBART用于神经机器翻译（NMT），以及一个文本到语音（TTS）合成组件等尖端技术，该框架提供了一种统一而无缝的跨语言交流方式。我们深入研究了每个组件的复杂细节，阐明了它们的个别贡献，并探讨了相互之间的协同作用，从而实现从印地语口语到合成英文音频的流畅过渡。

    Speech has long been a barrier to effective communication and connection, persisting as a challenge in our increasingly interconnected world. This research paper introduces a transformative solution to this persistent obstacle an end-to-end speech conversion framework tailored for Hindi-to-English translation, culminating in the synthesis of English audio. By integrating cutting-edge technologies such as XLSR Wav2Vec2 for automatic speech recognition (ASR), mBART for neural machine translation (NMT), and a Text-to-Speech (TTS) synthesis component, this framework offers a unified and seamless approach to cross-lingual communication. We delve into the intricate details of each component, elucidating their individual contributions and exploring the synergies that enable a fluid transition from spoken Hindi to synthesized English audio.
    
[^57]: Patchscope: 一个统一的框架，用于检查语言模型的隐藏表示

    Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])

    [http://arxiv.org/abs/2401.06102](http://arxiv.org/abs/2401.06102)

    本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。

    

    检查大型语言模型（LLM）的隐藏表示中编码的信息可以解释模型的行为并验证其与人类价值观的一致性。鉴于LLM生成人类可理解文本的能力，我们建议利用模型本身以自然语言解释其内部表示。我们引入了一个称为Patchscopes的框架，并展示了如何使用它来回答关于LLM计算的各种研究问题。我们表明，先前基于将表示投影到词汇空间和干预LLM计算的可解释性方法，可以看作是该框架的特殊实例。此外，通过Patchscope可以弥补优势，如检查早期层失败或表达能力不足。除了统一先前的检查技术，Patchscopes还开辟了新的可能性，例如使用更强大的模型来解释较小模型的表示。

    Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
    
[^58]: LEGO: 语言增强的多模态关联模型

    LEGO:Language Enhanced Multi-modal Grounding Model. (arXiv:2401.06071v1 [cs.CV])

    [http://arxiv.org/abs/2401.06071](http://arxiv.org/abs/2401.06071)

    LEGO是一种语言增强的多模态关联模型，它能够在各种任务中实现细粒度的理解和精确的标识能力。

    

    多模态大型语言模型在不同模态的各种任务中展现出了令人印象深刻的性能。然而，现有的多模态模型主要强调捕捉每种模态内的全局信息，而忽视了跨模态感知局部信息的重要性。因此，这些模型缺乏有效理解输入数据细粒度细节的能力，从而限制了它们在需要更细致理解的任务中的性能。为了解决这个限制，迫切需要开发能够在多个模态之间进行细粒度理解的模型，从而增强它们在各种任务中的适用性。在本文中，我们提出了LEGO，一种语言增强的多模态关联模型。除了像其他多模态模型一样捕捉全局信息外，我们提出的模型在需要详细理解输入内的局部信息的任务中表现出色。它展示了精确的标识能力。

    Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose LEGO, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification 
    
[^59]: 大型语言模型中的通用漏洞：上下文学习后门攻击

    Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])

    [http://arxiv.org/abs/2401.05949](http://arxiv.org/abs/2401.05949)

    本研究发现上下文学习范式在大型语言模型中存在漏洞，攻击者可以通过污染示范上下文来操控模型行为，而无需进行微调。这项研究设计了一种名为ICLAttack的后门攻击方法，可以通过污染示范样本和提示来使模型按照预定义的意图行事。

    

    上下文学习是一种在预训练和微调之间弥合差距的范式，在几个自然语言处理任务中展现了高效性，特别是在少样本设置中。与传统的微调方法不同，上下文学习能够适应未见过的任务而无需更新任何参数。尽管被广泛应用，上下文学习仍然容易受到恶意攻击。本研究提出了对这一范式的安全性问题的关切。我们的研究表明，攻击者可以通过污染示范上下文来操控大型语言模型的行为，而无需对模型进行微调。具体来说，我们设计了一种新的后门攻击方法，命名为ICLAttack，针对基于上下文学习的大型语言模型。我们的方法包括两种类型的攻击：污染示范样本和污染提示，可以使模型按照预定义的意图行事。ICLAttack不需要额外的微调。

    In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
    
[^60]: 社交媒体数据选择的生成去重方法

    Generative Deduplication For Socia Media Data Selection. (arXiv:2401.05883v1 [cs.CL])

    [http://arxiv.org/abs/2401.05883](http://arxiv.org/abs/2401.05883)

    提出了一种名为生成去重的方法，用于解决社交媒体数据中的冗余问题和模型偏差。通过删除重复的文本，可以提高语言理解性能并节省训练时间。

    

    社交媒体数据受其噪声特性的影响，存在冗余问题，导致训练时间增加和模型偏差。为了解决这个问题，我们提出了一种新颖的方法，称为生成去重。它旨在从嘈杂的社交媒体数据中删除重复的文本，并减轻模型偏差。通过这样做，它可以提高社交媒体语言理解性能并节省训练时间。大量实验证明，提出的生成去重方法可以有效减少训练样本的同时提高性能。这一证据表明生成去重的有效性及其在社交媒体语言理解中的重要性。

    Social media data is plagued by the redundancy problem caused by its noisy nature, leading to increased training time and model bias. To address this issue, we propose a novel approach called generative duplication. It aims to remove duplicate text from noisy social media data and mitigate model bias. By doing so, it can improve social media language understanding performance and save training time. Extensive experiments demonstrate that the proposed generative deduplication can effectively reduce training samples while improving performance. This evidence suggests the effectiveness of generative deduplication and its importance in social media language understanding.
    
[^61]: 卧底特工：训练骗人的LLM以通过安全训练

    Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])

    [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)

    该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。

    

    人类有能力进行战略性的欺骗行为：在大多数情况下表现出有益的行为，但在有机会的时候却表现出截然不同的行为以追求其他目标。如果一个AI系统学会了这样的欺骗策略，是否能够通过当前最先进的安全训练技术检测并移除它？为了研究这个问题，我们构建了大型语言模型（LLM）中欺骗行为的概念验证样例。例如，我们训练模型，在提示语句中将年份设为2023时编写安全代码，但在年份设为2024时插入有漏洞的代码。我们发现，这种暗门行为可以被持续保留，无法通过标准的安全训练技术（包括监督微调、强化学习和对抗性训练）移除。暗门行为在最大的模型和训练成产生思维链的模型中最为持久。

    Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
    
[^62]: RoSA: 通过鲁棒适应实现准确的参数高效微调

    RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])

    [http://arxiv.org/abs/2401.04679](http://arxiv.org/abs/2401.04679)

    RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。

    

    我们研究了在大语言模型 (LLMs) 的背景下，能够在有限的计算和内存预算下提供良好准确性的参数高效微调 (PEFT) 方法。我们提出了一种新的PEFT方法，称为RoSA，受鲁棒主成分分析 (PCA) 的启发，它在一组固定的预训练权重上共同训练$\textit{低秩}$和$\textit{高度稀疏}$的组件，以高效近似完全微调（FFT）解决方案的性能。我们展示了RoSA在一系列具有挑战性的生成任务上的性能，例如小学数学和SQL查询生成，这些任务需要进行微调以获得良好性能，我们证明了在相同的参数预算下，RoSA优于LoRA和纯粹的稀疏微调。我们通过稀疏GPU内核为RoSA提供系统支持，以补充训练算法，从而实现内存和计算效率的训练。我们的代码将在https://github.com/IST-DASLab上提供。

    We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
    
[^63]: MERA: 俄语LLM综合评估的研究

    MERA: A Comprehensive LLM Evaluation in Russian. (arXiv:2401.04531v1 [cs.CL])

    [http://arxiv.org/abs/2401.04531](http://arxiv.org/abs/2401.04531)

    这项研究提出了MERA，一个多模态俄语基础模型评估指标。该指标包括21个评估任务，涵盖了11个技能领域中生成模型的评估。研究还提出了一种在零样本和少样本固定指令设置下评估FM和LM的方法。

    

    在过去几年中，人工智能研究中最显著的进展之一是基础模型（FM）的发展，其中语言模型（LM）的崛起引人注目。随着模型的规模增大，LM在可衡量的方面展示了提升，并且发展出了新的定性特征。然而，尽管研究人员的关注和LM应用的快速增长，LM的能力、限制和相关风险仍需更好地理解。为了解决这些问题，我们介绍了一种开放的俄语多模态架构评估（MERA）指导基准，用于评估以俄语为导向的基础模型。该基准涵盖了11个技能领域中生成模型的21个评估任务，并被设计为黑盒测试，以确保排除数据泄漏。论文介绍了一种在零样本和少样本固定指令设置下评估FM和LM的方法，并可扩展到其他模态。

    Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zeroand few-shot fixed instruction settings that can be extended to other modalities. We propose an 
    
[^64]: AI是否能像人类一样具备创造力？

    Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])

    [http://arxiv.org/abs/2401.01623](http://arxiv.org/abs/2401.01623)

    本文引入了一个新概念【相对创造力】，通过将焦点转向AI是否能够与人类具备相同的创造能力，实现对创造力的统计量化评估和直接比较。

    

    创造力是社会进步和创新的基石，但其评估仍然是一个复杂且主观的任务。随着先进的生成型AI模型的出现，能够完成曾经只属于人类创造力的任务，探索AI的创造潜力变得至关重要，以确保其负责任的发展和应用。本文通过引入一个名为“相对创造力”的新概念来解决定义和评估创造力的复杂性。我们不再试图对创造力进行普遍定义，而是将焦点转向AI是否能够与一位假设的人类具备相同的创造能力。这种观点借鉴了图灵测试的思想，并扩展其范围以解决评估创造力中所固有的挑战和主观性。这种方法的转变使得对AI创造力的统计量化评估成为可能，我们将其称为统计创造力。这种方法允许直接比较AI与特定人类的创造能力。

    Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
    
[^65]: LLaMA超越英语：语言能力转移的实证研究

    LLaMA Beyond English: An Empirical Study on Language Capability Transfer. (arXiv:2401.01055v1 [cs.CL])

    [http://arxiv.org/abs/2401.01055](http://arxiv.org/abs/2401.01055)

    本文提出了LLaMA超越英语：语言能力转移的实证研究。通过对LLaMA进行广泛的实证调查，分析了词汇扩展、进一步预训练和指导调整等关键因素对非英语语言上的能力转移的影响，并通过四个标准化测试基准评估了模型的知识水平和响应质量。

    

    最近，在大型语言模型（LLM）方面取得了重大进展，如ChatGPT，在各种复杂任务中展现出卓越的熟练度。然而，许多主流的LLM（如LLaMA）是基于以英语为主导的语料库进行预训练的，这限制了它们在其他非英语语言中的性能。本文主要研究如何有效地将语言生成和遵循指示的能力转移到非英语语言上。为了回答这个问题，我们基于LLaMA进行了广泛的实证调查，总计耗费了1440个GPU小时。我们分析了诸如词汇扩展、进一步预训练和指导调整等关键因素对转移的影响。为了准确评估模型的知识水平，我们采用了四个广泛使用的标准化测试基准：C-Eval、MMLU、AGI-Eval和GAOKAO-Bench。此外，我们还对模型的响应质量进行了全面评估，考虑了诸如...

    In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e.g. LLaMA) are pretrained on English-dominant corpus, which limits their performance in other non-English languages. In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language. To answer this question, we conduct an extensive empirical investigation based on LLaMA, accumulating over 1440 GPU hours. We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. To accurately assess the model's level of knowledge, we employ four widely used standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a comprehensive evaluation of the model's response quality is conducted, considering aspects such as 
    
[^66]: 使用基于缓存推理的带状态Conformer模型的流式自动语音识别

    Stateful Conformer with Cache-based Inference for Streaming Automatic Speech Recognition. (arXiv:2312.17279v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.17279](http://arxiv.org/abs/2312.17279)

    本文提出一种基于FastConformer架构的流式语音识别模型，通过限制上下文和引入缓存机制，在推理过程中实现非自回归编码器的自回归操作，并消除了训练和推理准确度间的差异。同时，还提出了CTC/RNNT混合架构以提高准确度和节省计算。

    

    本文提出了一种基于FastConformer架构的高效准确的流式语音识别模型。通过对FastConformer架构进行调整，我们适用于流式应用的方式有两个：（1）限制编码器中的前瞻和历史上下文，（2）引入激活缓存机制以使非自回归编码器在推理过程中以自回归方式工作。所提出的模型经过精心设计，消除了许多流式模型在训练和推理时间中的准确度差异。此外，我们的编码器与不同的解码器配置兼容，包括CTC和RNNT解码器。此外，我们还引入了一种混合的CTC/RNNT架构，它利用共享的编码器和CTC和RNNT解码器来提高准确度并节省计算。我们在LibriSpeech数据集和多领域大型数据集上评估了所提出的模型。

    In this paper, we propose an efficient and accurate streaming speech recognition model based on the FastConformer architecture. We adapted the FastConformer architecture for streaming applications through: (1) constraining both the look-ahead and past contexts in the encoder, and (2) introducing an activation caching mechanism to enable the non-autoregressive encoder to operate autoregressively during inference. The proposed model is thoughtfully designed in a way to eliminate the accuracy disparity between the train and inference time which is common for many streaming models. Furthermore, our proposed encoder works with various decoder configurations including Connectionist Temporal Classification (CTC) and RNN-Transducer (RNNT) decoders. Additionally, we introduced a hybrid CTC/RNNT architecture which utilizes a shared encoder with both a CTC and RNNT decoder to boost the accuracy and save computation. We evaluate the proposed model on LibriSpeech dataset and a multi-domain large sc
    
[^67]: NPHardEval: 通过复杂性类别对大型语言模型的推理能力进行动态基准评估

    NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.14890](http://arxiv.org/abs/2312.14890)

    NPHardEval是一个新的基准，旨在评估大型语言模型在900个算法问题上的推理能力，扩展到NP-Hard复杂性类别。

    

    复杂推理能力是当前大型语言模型的最重要特征之一，它也被用于在复杂决策任务中起到了重要作用。因此，研究大型语言模型的推理能力至关重要：已经建立了许多基准来评估大型语言模型的推理能力。然而，目前的基准在提供大型语言模型推理能力的全面评估方面还不够，同时也容易出现过拟合的风险，因为这些基准是公开可访问且静态的，使得模型有可能根据特定的基准指标调整其响应，从而夸大其性能。针对这些限制，我们的研究引入了一个新的基准，名为NPHardEval。该基准旨在评估大型语言模型在广泛的900个算法问题上的推理能力，涵盖了NP-Hard复杂性类别。

    Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class
    
[^68]: Mergen:首个使用增强数据训练的满汉机器翻译模型

    Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data. (arXiv:2311.17492v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.17492](http://arxiv.org/abs/2311.17492)

    Mergen是首个满汉机器翻译模型，通过使用增强数据和双向GRU层，取得了在满汉翻译中显著提升的结果。

    

    满语作为源于中国东北满洲地区的历史语言，正面临着灭绝的危机，因为几乎没有剩余的说者。为了保护满语，我们推出了Mergen，这是首个尝试满汉机器翻译模型的项目。为了开发这个模型，我们利用了宝贵的资源，如《满文老档》（一本历史书）和满汉词典。由于满汉平行数据集的稀缺性，我们通过使用在单语和平行文本上训练的GloVe嵌入引导的词替换来扩充我们的数据。我们的方法基于一个编码器-解码器神经机器翻译模型，包括一个双向门控循环单元（GRU）层。实验结果显示出令人满意的成果，在满汉翻译中有显著提升，BLEU分数增加了20-30个点。

    The Manchu language, with its roots in the historical Manchurian region of Northeast China, is now facing a critical threat of extinction, as there are very few speakers left. In our efforts to safeguard the Manchu language, we introduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation (MT) model. To develop this model, we utilize valuable resources such as the Manwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to the scarcity of a Manchu-Korean parallel dataset, we expand our data by employing word replacement guided by GloVe embeddings, trained on both monolingual and parallel texts. Our approach is built around an encoder-decoder neural machine translation model, incorporating a bi-directional Gated Recurrent Unit (GRU) layer. The experiments have yielded promising results, showcasing a significant enhancement in Manchu-Korean translation, with a remarkable 20-30 point increase in the BLEU score.
    
[^69]: 大型语言模型的多阶段协作知识蒸馏在半监督序列生成任务中的应用

    Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.08640](http://arxiv.org/abs/2311.08640)

    将大型语言模型的知识通过多阶段协作蒸馏的方式应用于半监督序列生成任务中，可以显著提高模型的泛化能力和性能。

    

    我们研究了半监督序列生成任务，在这种任务中，标记数据太少以至于无法有效地微调模型，同时在大型语言模型 (LLM) 中进行少样本提示的性能也不够理想，尤其是对于一些昂贵且对预训练的 LLM 不熟悉的任务，如解析。本文发现，从上下文学习的 LLM 蒸馏出的学生模型在这些任务上通常比其教师模型具有更好的泛化能力。基于这一发现，我们提出了一种新的方法 - 大型语言模型的多阶段协作知识蒸馏 (MCKD) - 用于这些任务。MCKD 首先进行少样本提示，让LLM为无标签数据生成伪标签。在每个中间知识蒸馏 (KD) 阶段，使用伪标签数据的不重叠分区来训练一对新的学生模型。然后，每个学生模型为其未见分区生成新的和改进的伪标签，在下一个蒸馏阶段中使用。我们展示了该方法的优势。

    We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method -multistage collaborative knowledge distillation from an LLM (MCKD) -- for such tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage
    
[^70]: 使用大型语言模型进行文本属性图的解缠表征学习

    Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])

    [http://arxiv.org/abs/2310.18152](http://arxiv.org/abs/2310.18152)

    本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。

    

    文本属性图（TAGs）在网络上非常常见，对于该类图，如引用网络、电子商务网络和社交网络的研究在网络社区中引起了相当大的关注。最近，大型语言模型（LLMs）在各种任务上展示了出色的能力。然而，现有的工作仅仅依靠提示信息来传达图结构信息给LLMs，因此对于TAGs中复杂的结构关系了解不足。为解决这个问题，本文提出了解缠图文学习器（DGTL）模型，能够增强LLMs对TAGs的推理和预测能力。我们的DGTL模型通过定制的解缠图神经网络（GNN）层将图结构信息纳入其中，使得LLMs能够捕捉多个结构因素中隐藏的复杂关系。

    Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
    
[^71]: O3D: 基于离线数据的发现与蒸馏方法，用于大规模语言模型在顺序决策中的应用

    O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.14403](http://arxiv.org/abs/2310.14403)

    O3D提出了一种基于离线数据的学习框架，利用大规模数据改进了大规模语言模型在顺序决策问题中的性能，通过自动发现可重复使用的技能，提高了模型的表现

    

    最近对大规模语言模型 (LLMs)的研究取得了令人期待的进展，在解决顺序决策问题方面显示出了良好的性能。通过模仿提示中提供的少量示例（即上下文学习），LLM代理可以与外部环境交互并完成给定任务，而无需额外的训练。然而，这种少量示例往往不足以生成复杂且长期目标任务的高质量解决方案，而有限的上下文长度无法处理更大规模的演示。为此，我们提出了一种利用离线数据的学习框架，以大规模的离线数据（例如人类交互的日志）来改进LLM代理的上下文学习性能。我们通过文本和代码两种方法正式定义了LLM强化策略。然后，我们引入了一种名为O3D的离线数据驱动发现和蒸馏框架，以改善LLM强化策略而无需微调。O3D自动地发现可重复使用的技能

    Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills
    
[^72]: MedAI对话语料库（MEDIC）：零样本分类医生与AI在健康咨询中的回答。

    MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations. (arXiv:2310.12489v1 [cs.CL])

    [http://arxiv.org/abs/2310.12489](http://arxiv.org/abs/2310.12489)

    本研究通过零样本学习调查了预训练语言模型在医生和AI在健康咨询中的回答的准确分类上的效果。研究发现，虽然预训练语言模型在一般语言理解方面表现出了很强的能力，但在医疗咨询中，它们可能需要特定语料库训练或其他技术以实现准确的医生和AI生成文本的分类。

    

    零样本分类使得可以将文本分类到在训练中没有见过的类中。在本文中，我们通过零样本学习，研究了预训练语言模型在准确分类来自医生和AI在健康咨询中的回答方面的有效性。我们的研究旨在确定这些模型是否能够在没有特定语料库训练的情况下有效地检测文本是来自人类还是AI模型。对于我们的实验，我们收集了来自医生对于患者健康咨询的回答，并对同样的问题/回答提问了AI模型。我们的研究结果显示，虽然预训练语言模型在一般语言理解方面表现出了很强的能力，但在医疗咨询中，它们可能需要特定语料库训练或其他技术以实现对医生和AI生成的文本的准确分类。作为基线方法，本研究展示了仅依靠零样本分类在医疗分类中的局限性。

    Zero-shot classification has enabled the classification of text into classes that were not seen during training. In this paper, we investigate the effectiveness of pre-trained language models to accurately classify responses from Doctors and AI in health consultations through zero-shot learning. Our study aims to determine whether these models can effectively detect if a text originates from human or AI models without specific corpus training. For our experiments, we collected responses from doctors to patient inquiries about their health and posed the same question/response to AI models. Our findings revealed that while pre-trained language models demonstrate a strong understanding of language generally, they may require specific corpus training or other techniques to achieve accurate classification of doctor- and AI-generated text in healthcare consultations. As a baseline approach, this study shows the limitations of relying solely on zero-shot classification in medical classificati
    
[^73]: 动态ASR路径：一种自适应掩蔽方法用于压缩多语种ASR模型的高效修剪

    Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model. (arXiv:2309.13018v1 [eess.AS])

    [http://arxiv.org/abs/2309.13018](http://arxiv.org/abs/2309.13018)

    本研究提出了一种自适应掩蔽方法，用于高效地压缩多语种ASR模型。该方法通过动态适应子网络结构，能够在减少性能损失的情况下得到稀疏的单语种模型或稀疏的多语种模型。实验证明，与现有的修剪方法相比，该方法在针对稀疏的单语种模型时表现更好，并且减少了对特定语言进行修剪的需求。

    

    神经网络修剪是一种有效的方法，可以在性能损失最小的情况下压缩多语种自动语音识别（ASR）模型。然而，这需要对每种语言运行多轮修剪和重新训练。在这项工作中，我们提出了一种自适应掩蔽方法，以两种场景高效地修剪多语种ASR模型，分别得到了稀疏的单语种模型或稀疏的多语种模型（称为动态ASR路径）。我们的方法动态地适应子网络，避免对固定的子网络结构进行过早决策。我们证明了我们的方法在针对稀疏的单语种模型时优于现有的修剪方法。此外，我们还说明了动态ASR路径通过自不同的子网络初始化进行调整，共同发现和训练更好的单一多语种模型的子网络（路径），从而减少了对特定语言进行修剪的需求。

    Neural network pruning offers an effective method for compressing a multilingual automatic speech recognition (ASR) model with minimal performance loss. However, it entails several rounds of pruning and re-training needed to be run for each language. In this work, we propose the use of an adaptive masking approach in two scenarios for pruning a multilingual ASR model efficiently, each resulting in sparse monolingual models or a sparse multilingual model (named as Dynamic ASR Pathways). Our approach dynamically adapts the sub-network, avoiding premature decisions about a fixed sub-network structure. We show that our approach outperforms existing pruning methods when targeting sparse monolingual models. Further, we illustrate that Dynamic ASR Pathways jointly discovers and trains better sub-networks (pathways) of a single multilingual model by adapting from different sub-network initializations, thereby reducing the need for language-specific pruning.
    
[^74]: TIDE: 用于评估和增强分类和语言模型的文本身份检测

    TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models. (arXiv:2309.04027v1 [cs.CL])

    [http://arxiv.org/abs/2309.04027](http://arxiv.org/abs/2309.04027)

    本文介绍了TIDE（Textual Identity Detection）方法来改善分类器和语言模型中的文本公平性。通过创建一个包含身份词汇和语境的数据集，以及开发一个身份注释和增强工具，可以提高机器学习公平性技术的效果。

    

    机器学习模型可以继承不公正和不平衡数据集中的意外偏见。在文本数据集中，评估和去偏这些数据集和模型尤其困难，因为种族、性别和性取向等敏感属性可能不可用。当这些模型投放到社会中时，它们可能对历史上弱势群体产生不公平的结果。本文提出了一个与方法相结合的数据集，以改善分类器和语言模型中的文本公平性。我们创建了一个更全面的身份词汇表TIDAL，包括15,123个身份术语和相关的语境，涵盖了三个人口统计类别。我们利用TIDAL开发了一个身份注释和增强工具，可以用于改善身份语境的可用性和机器学习公平性技术的效果。我们使用人类贡献者对我们的方法进行了评估，并进行了重点关注数据集和模型去偏的实验。

    Machine learning models can perpetuate unintended biases from unfair and imbalanced datasets. Evaluating and debiasing these datasets and models is especially hard in text datasets where sensitive attributes such as race, gender, and sexual orientation may not be available. When these models are deployed into society, they can lead to unfair outcomes for historically underrepresented groups. In this paper, we present a dataset coupled with an approach to improve text fairness in classifiers and language models. We create a new, more comprehensive identity lexicon, TIDAL, which includes 15,123 identity terms and associated sense context across three demographic categories. We leverage TIDAL to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of ML fairness techniques. We evaluate our approaches using human contributors, and additionally run experiments focused on dataset and model debiasing. Resul
    
[^75]: PromptMRG: 诊断驱动的医学报告生成方法

    PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation. (arXiv:2308.12604v1 [cs.CV])

    [http://arxiv.org/abs/2308.12604](http://arxiv.org/abs/2308.12604)

    PromptMRG是一种针对医学报告生成的诊断驱动方法，通过诊断感知的提示来提高MRG的诊断准确性。这种方法基于编码器-解码器架构，并具有额外的疾病分类分支。

    

    自动医学报告生成(MRG)具有很大的研究价值，因为它有潜力减轻放射科医生报告撰写的负担。尽管近年来取得了一些进展，但准确的MRG仍然具有挑战性，因为需要精确的临床理解和临床结果的识别。此外，疾病的不平衡分布使这一挑战更加突出，因为在训练数据中罕见疾病的比例较少，使得它们的诊断性能不可靠。为了解决这些挑战，我们提出了一种诊断驱动的医学报告生成方法(PromptMRG)，这是一种旨在通过诊断感知的提示来提高MRG诊断准确性的新框架。具体而言，PromptMRG基于编码器-解码器架构，有一个额外的疾病分类分支。在生成报告时，从分类分支得到的诊断结果被转化为标记提示，以明确地指导生成过程。

    Automatic medical report generation (MRG) is of great research value as it has the potential to relieve radiologists from the heavy burden of report writing. Despite recent advancements, accurate MRG remains challenging due to the need for precise clinical understanding and the identification of clinical findings. Moreover, the imbalanced distribution of diseases makes the challenge even more pronounced, as rare diseases are underrepresented in training data, making their diagnostic performance unreliable. To address these challenges, we propose diagnosis-driven prompts for medical report generation (PromptMRG), a novel framework that aims to improve the diagnostic accuracy of MRG with the guidance of diagnosis-aware prompts. Specifically, PromptMRG is based on encoder-decoder architecture with an extra disease classification branch. When generating reports, the diagnostic results from the classification branch are converted into token prompts to explicitly guide the generation process
    
[^76]: 学习和结合通用语音知识和语言特定知识进行低资源语言的唇语识别

    Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge. (arXiv:2308.09311v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.09311](http://arxiv.org/abs/2308.09311)

    本文提出了一种针对低资源语言的唇语识别框架，通过学习通用语音知识和语言特定知识，克服了低资源语言中缺乏视频文本配对数据的挑战。

    

    本文提出了一种新颖的唇语识别框架，特别针对低资源语言，在先前的文献中尚未得到很好的解决。由于低资源语言缺乏足够的视频文本配对数据来训练模型以获得足够的能力来建模唇部动作和语言，因此为低资源语言开发唇语识别模型被认为是具有挑战性的。为了缓解这一挑战，我们尝试通过预测语音单元来学习通用语音知识，即建模唇部动作的能力，从高资源语言中学习。已知不同语言部分共享相同的音素，因此从一个语言中学习的通用语音知识可以扩展到其他语言。然后，我们通过提出语言特定记忆增强解码器（LMDecoder）来尝试学习语言特定知识，即建模语言的能力。LMDecoder将语言特定的音频特征保存到存储器中，可以在音频文本配对数据上进行训练。

    This paper proposes a novel lip reading framework, especially for low-resource languages, which has not been well addressed in the previous literature. Since low-resource languages do not have enough video-text paired data to train the model to have sufficient power to model lip movements and language, it is regarded as challenging to develop lip reading models for low-resource languages. In order to mitigate the challenge, we try to learn general speech knowledge, the ability to model lip movements, from a high-resource language through the prediction of speech units. It is known that different languages partially share common phonemes, thus general speech knowledge learned from one language can be extended to other languages. Then, we try to learn language-specific knowledge, the ability to model language, by proposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder saves language-specific audio features into memory banks and can be trained on audio-text paired data
    
[^77]: 自洽性方法用于无限生成问题的改进

    Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])

    [http://arxiv.org/abs/2307.06857](http://arxiv.org/abs/2307.06857)

    本论文提出了一种改进大规模预训练语言模型生成输出质量和一致性的新方法，通过扩展自洽性框架的适用性，实现了从一个候选集中恢复最优或接近最优的生成结果，并提出了一种轻量级无参数相似性函数来改进代码生成、自动形式化和摘要任务的效果。

    

    在这篇论文中，我们提出了一种改进大规模预训练语言模型生成输出的质量和一致性的新方法。自洽性已经被证明是一种有效的方法，对于具有固定答案的提示，选择得票最多的答案。我们引入了一个推广的自洽性框架，扩展了其适用性，超越了固定答案问题的范围。通过大量的模拟实验，我们证明了我们的方法能够从候选集中恢复最优或接近最优的生成结果。我们还提出了一种轻量级无参数相似性函数，即使没有访问到标记的概率，也能在代码生成、自动形式化和摘要任务中显著和一致地改进效果。我们的方法几乎没有计算开销，不需要额外的再排序模型或对现有模型的修改。

    In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
    
[^78]: 通过主动遗忘在预训练中提高语言可塑性

    Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.01163](http://arxiv.org/abs/2307.01163)

    本论文提出了一种通过在预训练过程中使用主动遗忘机制来提高语言模型的可塑性的方法。通过在预训练过程中定期重置嵌入层，模型可以更快地适应新的语言，并在低数据情况下表现出更好的性能。

    

    预训练语言模型(PLMs)是自然语言处理中的主要模型。尽管它们在下游任务的性能令人印象深刻，但将PLMs应用于新语言可能很困难，这是使它们的能力普遍可访问的壁垒。先前的研究表明，通过为新语言学习新的嵌入层可以解决此问题，但这样做既浪费数据又浪费计算资源。我们建议在预训练期间使用主动遗忘机制，作为快速适应新语言的PLMs的简单方法。具体而言，通过在预训练期间的每K次更新时重置嵌入层，我们鼓励PLM在有限次更新内提高学习新嵌入的能力，类似于元学习的效果。使用RoBERTa进行的实验证明，使用我们的遗忘机制预训练的模型不仅在语言适应过程中显示出更快的收敛速度，而且在低数据的情况下也优于标准模型。

    Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, parti
    
[^79]: NMT中基于子词的分词中，频率和组合性的重要性评估

    Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT. (arXiv:2306.01393v1 [cs.CL])

    [http://arxiv.org/abs/2306.01393](http://arxiv.org/abs/2306.01393)

    本文通过实验发现，在NMT中基于子词的分词方法中，频率对于模型的表现贡献占据了90%-95%，因此相对于组合性，频率更为重要。

    

    子词分词是神经语言模型和机器翻译系统中的默认标准。频繁引用子词的优点有：对频繁词语进行更短编码，子词组合性强以及处理未知词语的能力。然而，它们的相对重要性尚不太清楚。本文提出了一种分词方法，可以将频率（第一个优点）与组合性分离开来，使用霍夫曼编码对单词进行分词，按频率顺序，使用固定数量的符号。实验表明，在CS-DE、EN-FR和EN-DE NMT中，仅频率就占了BPE得分的90%-95%，因此组合性并不像以前认为的那么重要。

    Subword tokenization is the de facto standard for tokenization in neural language models and machine translation systems. Three advantages are frequently cited in favor of subwords: shorter encoding of frequent tokens, compositionality of subwords, and ability to deal with unknown words. As their relative importance is not entirely clear yet, we propose a tokenization approach that enables us to separate frequency (the first advantage) from compositionality. The approach uses Huffman coding to tokenize words, by order of frequency, using a fixed amount of symbols. Experiments with CS-DE, EN-FR and EN-DE NMT show that frequency alone accounts for 90%-95% of the scores reached by BPE, hence compositionality has less importance than previously thought.
    
[^80]: 自然语言到SPARQL查询生成的复制机制综合评估

    A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v1 [cs.CL])

    [http://arxiv.org/abs/2304.07772](http://arxiv.org/abs/2304.07772)

    本研究综合评估自然语言到SPARQL查询生成中的复制机制，通过大量实验研究预训练和非预训练模型、问题注释格式以及使用复制机制的影响，并证明了在这些方面的优化可以提高性能。

    

    近年来，神经机器翻译（NMT）领域在SPARQL查询生成方面有了显著的增长。最近，将复制机制与传统的编码器-解码器架构相结合，并使用预训练的编码器-解码器，创造了新的性能基准。本文展示了大量的实验，复制并扩展了最近的基于NMT的SPARQL生成研究，比较了预训练和非预训练模型、问题注释格式以及对于非预训练和预训练模型使用复制机制的影响。我们的结果表明，对于非预训练模型和预训练模型，添加复制机制或使用问题注释都可以提高性能，并为三个流行数据集设置了新的基准。

    In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed a significant growth. Recently, the incorporation of the copy mechanism with traditional encoder-decoder architectures and the use of pre-trained encoder-decoders have set new performance benchmarks. This paper presents a large variety of experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained and non-pre-trained models, question annotation formats, and the use of a copy mechanism for non-pre-trained and pre-trained models. Our results show that either adding the copy mechanism or using a question annotation improves performances for nonpre-trained models and for pre-trained models, setting new baselines for three popular datasets.
    
[^81]: NLP中的最新泛化研究：分类和综述

    State-of-the-art generalisation research in NLP: A taxonomy and review. (arXiv:2210.03050v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.03050](http://arxiv.org/abs/2210.03050)

    本文提出了一个用于分类和理解NLP中泛化研究的分类法，对400多篇论文进行了综述和分类，总结了当前泛化研究的现状。

    

    良好的泛化能力是自然语言处理(NLP)的主要目标之一。然而，对于什么是“良好的泛化”以及如何评估它仍不明确，也没有泛化的评估标准。在本文中，我们为解决这两个问题奠定了基础。我们提出了一个用于表征和理解NLP中泛化研究的分类法。该分类法基于对泛化研究的广泛文献综述，包含了五个不同的维度：主要动机、研究的泛化类型、考虑的数据转移类型、数据转移的来源以及转移在建模流程中的位置。我们使用这个分类法对测试泛化的400多篇论文进行了分类，共进行了600多个实验。通过综述的结果，我们提供了一份深入分析，揭示了当前泛化研究的现状。

    The ability to generalise well is one of the primary desiderata of natural language processing (NLP). Yet, what 'good generalisation' entails and how it should be evaluated is not well understood, nor are there any evaluation standards for generalisation. In this paper, we lay the groundwork to address both of these issues. We present a taxonomy for characterising and understanding generalisation research in NLP. Our taxonomy is based on an extensive literature review of generalisation research, and contains five axes along which studies can differ: their main motivation, the type of generalisation they investigate, the type of data shift they consider, the source of this data shift, and the locus of the shift within the modelling pipeline. We use our taxonomy to classify over 400 papers that test generalisation, for a total of more than 600 individual experiments. Considering the results of this review, we present an in-depth analysis that maps out the current state of generalisation 
    
[^82]: NAAQA: 一种用于声学问答的神经网络结构

    NAAQA: A Neural Architecture for Acoustic Question Answering. (arXiv:2106.06147v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2106.06147](http://arxiv.org/abs/2106.06147)

    本文提出了一种名为NAAQA的神经网络结构，用于声学问答任务。通过使用1D卷积处理声学内容的2D频谱时域表示，该结构通过时间坐标图增加了时间定位能力，并在处理具有不同基本声音构建的场景时表现出有希望的结果。

    

    声学问答（AQA）任务的目标是回答关于声学场景内容的自由文本问题。本文基于之前介绍的CLEAR数据集，提出了一个新的AQA基准，即CLEAR2，它强调了声学输入的特定挑战，包括处理时长变化的场景和在训练集与测试集之间有不同的基本声音构建的场景。我们还介绍了一种名为NAAQA的神经网络结构，它利用了声学输入的特定属性。在时间和频率上使用1D卷积来处理声学内容的2D频谱时域表示，显示出有希望的结果，并能减少模型复杂性。我们展示了时间坐标图可以增加时间定位能力，从而将网络性能提高约17个百分点。另一方面，频率坐标图对网络性能几乎没有影响。

    The goal of the Acoustic Question Answering (AQA) task is to answer a free-form text question about the content of an acoustic scene. It was inspired by the Visual Question Answering (VQA) task. In this paper, based on the previously introduced CLEAR dataset, we propose a new benchmark for AQA, namely CLEAR2, that emphasizes the specific challenges of acoustic inputs. These include handling of variable duration scenes, and scenes built with elementary sounds that differ between training and test set. We also introduce NAAQA, a neural architecture that leverages specific properties of acoustic inputs. The use of 1D convolutions in time and frequency to process 2D spectro-temporal representations of acoustic content shows promising results and enables reductions in model complexity. We show that time coordinate maps augment temporal localization capabilities which enhance performance of the network by ~17 percentage points. On the other hand, frequency coordinate maps have little influen
    

