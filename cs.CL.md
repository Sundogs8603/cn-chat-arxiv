# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Open-source Frame Semantic Parsing.](http://arxiv.org/abs/2303.12788) | 本文介绍了一个开源Python库Frame Semantic Transformer，该库用于框架语义解析，并在易用性和性能方面实现接近最新水平，可通过在推理时使用FrameNet词汇单元来提高性能，并通过在训练期间使用文本数据增强来提高对真实世界数据的稳健性。 |
| [^2] | [Interpretable Bangla Sarcasm Detection using BERT and Explainable AI.](http://arxiv.org/abs/2303.12772) | 该论文提出了一种基于BERT的冷嘲热讽检测系统，可以有效地检测孟加拉语中的冷嘲热讽，对社交媒体分析等任务具有重要意义。 |
| [^3] | [Can we trust the evaluation on ChatGPT?.](http://arxiv.org/abs/2303.12767) | 本文讨论了ChatGPT评估中面临的数据污染挑战，通过倾向性检测任务阐述了这一问题，并探讨了如何在闭合且持续训练模型的时代确保模型评估的公平性。 |
| [^4] | [Comparing Trajectory and Vision Modalities for Verb Representation.](http://arxiv.org/abs/2303.12737) | 本文测试了使用2D图像和3D轨迹对动词语义表示的影响，发现2D视觉模态的表现与3D轨迹类似，挑战了传统的智慧。 |
| [^5] | [MultiModal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision Language Models.](http://arxiv.org/abs/2303.12734) | 本文提出了一个视觉和文本偏置基准MMBias，涵盖14个人口子群，并利用该基准评估了多个自我监督多模态模型（包括CLIP、ALBEF和ViLT），结果表明这些模型表现出偏向某些群体的有意义的偏见。同时，本文引入了一种针对大规模预先训练模型设计的去偏置方法作为后处理步骤，可以减轻偏差的影响，同时保证模型的性能。 |
| [^6] | [Integrating Image Features with Convolutional Sequence-to-sequence Network for Multilingual Visual Question Answering.](http://arxiv.org/abs/2303.12671) | 该论文针对VLSP2022-EVJVQA共享任务提出了一种基于卷积序列到序列网络的方法，在多语言视觉问答中将预训练的VQA模型和图像特征集成，获得了很好的效果。 |
| [^7] | [Evaluating the Role of Target Arguments in Rumour Stance Classification.](http://arxiv.org/abs/2303.12665) | 本文重新评估了目标论据在谣言立场分类中的作用，证明最先进的模型过于依赖表面信号，目标不独立的事件自然高发生率可能导致模型过度拟合。 |
| [^8] | [AfroDigits: A Community-Driven Spoken Digit Dataset for African Languages.](http://arxiv.org/abs/2303.12582) | AfroDigits是第一个发布的面向非洲语言的音频数字数据集，为非洲语言中的语音应用程序开辟了道路。 |
| [^9] | [RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation.](http://arxiv.org/abs/2303.12570) | RepoCoder是一个简单、通用和有效的框架，结合了一个基于相似性的检索器和预训练的代码语言模型，在库级别代码完成流程中实现了有效利用库级别信息进行代码完成和生成。 |
| [^10] | [MEGA: Multilingual Evaluation of Generative AI.](http://arxiv.org/abs/2303.12528) | 这项研究对 33 种语言中 8 个不同任务的生成 AI 进行了全面评估，比较了生成 LLMs 和非自回归模型的表现差异。 |
| [^11] | [Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding.](http://arxiv.org/abs/2303.12513) | 本文调查了视觉语言预训练对仅文本任务的表现是否有提高。作者提出了一套视觉语言理解任务，证明了多模态训练的文本编码器在视觉推理方面的优越性。 |
| [^12] | [Few-shot Multimodal Multitask Multilingual Learning.](http://arxiv.org/abs/2303.12489) | 提出了一种多阶段微调框架，针对少样本多模态多任务多语言学习，可以有效地利用迁移学习和少样本学习的优势，它采用一种通用的编码器-解码器骨架，并使用注意机制处理多模态信息和每个任务的语言特定的微调，在多个基准数据集上表现优异。 |
| [^13] | [GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering.](http://arxiv.org/abs/2303.12320) | GrapeQA是一种新方法，使用“重要实体图形增强”和“上下文感知节点剪枝”策略，以提高问答准确性和效率。 |
| [^14] | [Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization.](http://arxiv.org/abs/2303.12314) | 提出了一种自我监督元提示学习框架SUPMER，包括元梯度正则化，用于少样本泛化，通过锚定的元训练任务和基于课程的任务增强丰富了任务分布，解决了在少样本情况下良好初始化软提示和过拟合的问题。 |
| [^15] | [Exploring Turkish Speech Recognition via Hybrid CTC/Attention Architecture and Multi-feature Fusion Network.](http://arxiv.org/abs/2303.12300) | 本文提出了一种基于混合CTC/Attention架构和多特征融合网络的土耳其语音识别系统，实现了数据增强技术和特征提取器LSPC的优化。实验结果表明，该模型在土耳其语音语料库上取得良好表现。 |
| [^16] | [Evaluating Transformer Models and Human Behaviors on Chinese Character Naming.](http://arxiv.org/abs/2303.12294) | 本研究评估了一组 transformer 模型，在未知的中文字符命名任务中，这些模型表现得与人类很相似，能够很好地捕捉人类字符命名行为。 |
| [^17] | [A Unified Taxonomy of Deep Syntactic Relations.](http://arxiv.org/abs/2303.12220) | 本文提出了一组通用的语义角色标签，针对Universal Dependencies，并应用于四种语言的数据中。 |
| [^18] | [MAGVLT: Masked Generative Vision-and-Language Transformer.](http://arxiv.org/abs/2303.12208) | 本文提出 MGVLT 模型用于生成图像和文本序列，通过非自回归掩码预测实现了双向上下文编码和快速解码等特点。 |
| [^19] | [Understand Legal Documents with Contextualized Large Language Models.](http://arxiv.org/abs/2303.12135) | 本文介绍了针对 SemEval-2023 任务 6 开发的 Legal-BERT-HSLN 模型和 Legal-LUKE 模型，其中 Legal-BERT-HSLN 模型通过考虑句内和句间的上下文信息以预测修辞角色，Legal-LUKE 模型是具有法律上下文和实体知识的模型，以识别法律实体。模型相比基线模型更准确，能够解决在人口众多的国家处理法律文件的问题。 |
| [^20] | [Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense.](http://arxiv.org/abs/2303.12132) | 生成式语言模型的改进引起了公众广泛关注。它们广泛应用及其真实能力揭示了它们的潜在应用，但同时也带来了对其可能的恶意用途的担忧。本文旨在提供生成式大语言模型的简要概述以及在网络防御中的应用。 |
| [^21] | [Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation.](http://arxiv.org/abs/2303.12112) | 本论文提出一种新的图像标题评估指标PAC-S，可以更准确地评估图像和视频的标题，相比于现有的指标有更好的表现；源代码和训练模型已经公开。 |
| [^22] | [Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting.](http://arxiv.org/abs/2303.12057) | 本文展示了在零-shot学习环境下，大型语言模型可以用于评估政治家的意识形态，为我们更好地理解政治功能提供了有用的信息。 |
| [^23] | [EVA-02: A Visual Representation for Neon Genesis.](http://arxiv.org/abs/2303.11331) | EVA-02是一种基于Transformer的下一代视觉表征，具有重建强大且稳健特征的能力，并在各种代表性视觉任务中表现出色，同时使用的参数和计算预算显著较少。 |
| [^24] | [EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation.](http://arxiv.org/abs/2303.11117) | 本文提出了一种新的依赖性建模方法，由情感惯性和感染驱动（EmotionIC），用于在特征提取和分类级别上进行会话情感识别。设计了多项具体方法，包括身份掩码多头注意（IM-MHA）和基于对话门控循环单元(DialogGRU)，以抓取上下文信息，提高模型的性能。 |
| [^25] | [Character, Word, or Both? Revisiting the Segmentation Granularity for Chinese Pre-trained Language Models.](http://arxiv.org/abs/2303.10893) | 该论文提出了一种混合粒度中文BERT（MigBERT），同时考虑字符和单词，以更好地表现单词的语义。实验证明MigBERT在各种中文NLP任务上均可实现新的SOTA性能。 |
| [^26] | [COVID-19 event extraction from Twitter via extractive question answering with continuous prompts.](http://arxiv.org/abs/2303.10659) | 本文使用连续提示和抽取式问答的方法从Twitter中提取COVID-19事件，在COVID-19事件插槽中达到了5%以上的F1得分改进。 |
| [^27] | [ChatGPT Participates in a Computer Science Exam.](http://arxiv.org/abs/2303.09461) | ChatGPT在算法和数据结构考试中取得20.5分的成绩，表现出能在具有挑战性的大学考试中成功，但不能说明其对计算机科学有理解。 |
| [^28] | [UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation.](http://arxiv.org/abs/2303.08518) | UPRISE是一种通用的检索器，可自动为给定的零样本任务输入检索提示，从而提高大型语言模型的零样本评估。它通过跨任务和跨模型的实验展示了其通用性和潜力，同时表明具有减轻幻觉问题和提高LLM性能的能力。 |
| [^29] | [Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation.](http://arxiv.org/abs/2211.12824) | 本论文引入了一种新的任务，文本引导的视频补全，提出了多模态掩码视频生成方法，能够处理三种情况的视频补全。该方法在基准数据集上取得了最好的性能。 |
| [^30] | [VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval.](http://arxiv.org/abs/2211.12764) | 本论文提出了一种用于文本视频跨模态检索的协作提示调整（VoP）框架，与传统方法相比，VoP具有更少的可训练参数和更低的计算复杂度，同时实现了相似甚至更好的性能。 |
| [^31] | [Dual-Stream Transformer for Generic Event Boundary Captioning.](http://arxiv.org/abs/2207.03038) | 本文提出了一种双流Transformer的通用事件边界字幕生成方法，结合多个预训练模型和边界类型提示，以及单词级别的集成策略，实现了生成更人性化的字幕，并在GEBC测试集上取得了令人满意的结果。 |
| [^32] | [The Causal Structure of Semantic Ambiguities.](http://arxiv.org/abs/2206.06807) | 本文使用Gogioso和Pinzani在QPL 2021中提出的束理论模型，为语义歧义的两个特征（不同可能解释的联合可信度和某些词在过程中扮演更重要角色的因果结构）进行建模。通过对心理语言学文献中的歧义短语数据集进行分析，研究人员对人类对于这些歧义的判断进行了实证测量。 |
| [^33] | [Visual Spatial Reasoning.](http://arxiv.org/abs/2205.00363) | 本文介绍了一种名为Visual Spatial Reasoning（VSR）的数据集，其中包含超过10k个自然文本-图像配对，用于推理包括66种空间关系，研究发现目前的视觉和语言模型（VLMs）难以捕捉关系信息和较少关注物体的方向关系。 |

# 详细

[^1]: 开源框架语义解析

    Open-source Frame Semantic Parsing. (arXiv:2303.12788v1 [cs.CL])

    [http://arxiv.org/abs/2303.12788](http://arxiv.org/abs/2303.12788)

    本文介绍了一个开源Python库Frame Semantic Transformer，该库用于框架语义解析，并在易用性和性能方面实现接近最新水平，可通过在推理时使用FrameNet词汇单元来提高性能，并通过在训练期间使用文本数据增强来提高对真实世界数据的稳健性。

    

    虽然最近几年来框架语义解析的最新技术取得了显著进展，但对于终端用户来说，将最新模型应用于实践仍然很困难。为了解决这个问题，我们提出了Frame Semantic Transformer，这是一个开源Python库，可以在关注易用性的同时，在FrameNet 1.7上实现接近最新水平的性能。我们使用一个在Propbank和FrameNet示例上微调的T5模型作为基础，并通过在推理时使用FrameNet词汇单元为T5提供提示来提高性能。我们通过在训练期间使用文本数据增强来提高对真实世界数据的稳健性。

    While the state-of-the-art for frame semantic parsing has progressed dramatically in recent years, it is still difficult for end-users to apply state-of-the-art models in practice. To address this, we present Frame Semantic Transformer, an open-source Python library which achieves near state-of-the-art performance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model fine-tuned on Propbank and FrameNet exemplars as a base, and improve performance by using FrameNet lexical units to provide hints to T5 at inference time. We enhance robustness to real-world data by using textual data augmentations during training.
    
[^2]: 使用BERT和可解释AI的可解释孟加拉冷嘲热讽检测

    Interpretable Bangla Sarcasm Detection using BERT and Explainable AI. (arXiv:2303.12772v1 [cs.CL])

    [http://arxiv.org/abs/2303.12772](http://arxiv.org/abs/2303.12772)

    该论文提出了一种基于BERT的冷嘲热讽检测系统，可以有效地检测孟加拉语中的冷嘲热讽，对社交媒体分析等任务具有重要意义。

    

    正面的话或伴随负向动机的语句通常被定义为冷嘲热讽，而在当今社交媒体平台如Facebook、Twitter、Reddit等上广泛使用。最近，社交媒体平台上的活跃用户数量呈现爆炸性增长，这增强了需要一种基于自然语言处理的自动化系统来完成多项任务，如确定市场需求、情感分析、威胁检测等的需求。然而，由于冷嘲热讽通常意味着相反的意思，其检测经常是一个具有挑战性的问题，因此通过NLP模型进行数据意义提取变得更加复杂。因此，在过去的几年中，针对英语中的冷嘲热讽检测已经有大量研究，并且取得了显著的进步，但孟加拉语中的冷嘲热讽检测始终没有改善。本文提出了一种基于BERT模型的系统，在使用传统机器学习算法时只能达到99.60\%的情况下，可以实现冷嘲热讽的检测。

    A positive phrase or a sentence with an underlying negative motive is usually defined as sarcasm that is widely used in today's social media platforms such as Facebook, Twitter, Reddit, etc. In recent times active users in social media platforms are increasing dramatically which raises the need for an automated NLP-based system that can be utilized in various tasks such as determining market demand, sentiment analysis, threat detection, etc. However, since sarcasm usually implies the opposite meaning and its detection is frequently a challenging issue, data meaning extraction through an NLP-based model becomes more complicated. As a result, there has been a lot of study on sarcasm detection in English over the past several years, and there's been a noticeable improvement and yet sarcasm detection in the Bangla language's state remains the same. In this article, we present a BERT-based system that can achieve 99.60\% while the utilized traditional machine learning algorithms are only ca
    
[^3]: 我们能相信ChatGPT的评估吗？

    Can we trust the evaluation on ChatGPT?. (arXiv:2303.12767v1 [cs.CL])

    [http://arxiv.org/abs/2303.12767](http://arxiv.org/abs/2303.12767)

    本文讨论了ChatGPT评估中面临的数据污染挑战，通过倾向性检测任务阐述了这一问题，并探讨了如何在闭合且持续训练模型的时代确保模型评估的公平性。

    

    ChatGPT是第一个被广泛采纳的大型语言模型，展示出在多项自然语言任务中卓越的表现。但是，由于模型的闭合性以及通过强化学习和人类反馈不断更新，评估ChatGPT在不同问题领域的表现仍然具有挑战性。本文重点讨论了在ChatGPT的评估中存在的数据污染问题，并使用倾向性检测任务作为案例进行了说明。我们还讨论了如何在闭合和持续训练模型的时代，避免数据污染和确保公平的模型评估的挑战。

    ChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study of the task of stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.
    
[^4]: 比较轨迹和视觉模态对动词表示的影响

    Comparing Trajectory and Vision Modalities for Verb Representation. (arXiv:2303.12737v1 [cs.CV])

    [http://arxiv.org/abs/2303.12737](http://arxiv.org/abs/2303.12737)

    本文测试了使用2D图像和3D轨迹对动词语义表示的影响，发现2D视觉模态的表现与3D轨迹类似，挑战了传统的智慧。

    

    三维轨迹，即物体随时间的3D位置和旋转，被证明可以编码动词语义的关键方面（例如，roll和slide的含义）。然而，大多数NLP中的多模态模型使用2D图像作为世界的表示。考虑到3D空间在动词语义的形式模型中的重要性，我们预期这些2D图像会导致贫瘠的表示，无法捕捉到微妙的差异。本文在受控实验中直接测试了这个假设。我们训练了自监督的图像和轨迹编码器，然后评估它们学习区分动词概念的程度。与我们最初的预期相反，我们发现2D视觉模态的表现与3D轨迹类似。虽然还需要进一步研究这个问题，但我们的初步发现挑战了传统的智慧：更丰富的环境表示必然会导致更好的表示学习。

    Three-dimensional trajectories, or the 3D position and rotation of objects over time, have been shown to encode key aspects of verb semantics (e.g., the meanings of roll vs. slide). However, most multimodal models in NLP use 2D images as representations of the world. Given the importance of 3D space in formal models of verb semantics, we expect that these 2D images would result in impoverished representations that fail to capture nuanced differences in meaning. This paper tests this hypothesis directly in controlled experiments. We train self-supervised image and trajectory encoders, and then evaluate them on the extent to which each learns to differentiate verb concepts. Contrary to our initial expectations, we find that 2D visual modalities perform similarly well to 3D trajectories. While further work should be conducted on this question, our initial findings challenge the conventional wisdom that richer environment representations necessarily translate into better representation lea
    
[^5]: 多模态偏差：引入一种框架以评估视觉语言模型中的刻板印象偏差，超越性别和种族。

    MultiModal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision Language Models. (arXiv:2303.12734v1 [cs.CV])

    [http://arxiv.org/abs/2303.12734](http://arxiv.org/abs/2303.12734)

    本文提出了一个视觉和文本偏置基准MMBias，涵盖14个人口子群，并利用该基准评估了多个自我监督多模态模型（包括CLIP、ALBEF和ViLT），结果表明这些模型表现出偏向某些群体的有意义的偏见。同时，本文引入了一种针对大规模预先训练模型设计的去偏置方法作为后处理步骤，可以减轻偏差的影响，同时保证模型的性能。

    

    最近自主训练的突破为一类预先训练的视觉语言模型带来了新的机遇。虽然对多模态模型中的偏见进行了一些调查，但主要集中在性别和种族偏见上，对于其他相关群体，如宗教、国籍、性取向或残疾人群，给予的关注较少，这主要是由于缺乏适当的基准。我们通过提供一个称为MMBias的视觉和文本偏差基准来解决这一差距，包括约3800个图像和短语，涵盖14个人口子群。我们利用这个数据集来评估几个著名的自我监督多模态模型，包括CLIP、ALBEF和ViLT。我们的结果显示，这些模型表现出有意义的偏差，偏向某些群体。最后，我们引入了一种针对这种大规模预先训练模型设计的去偏置方法，可以作为后处理步骤应用于减轻偏差，同时保持模型性能。

    Recent breakthroughs in self supervised training have led to a new class of pretrained vision language models. While there have been investigations of bias in multimodal models, they have mostly focused on gender and racial bias, giving much less attention to other relevant groups, such as minorities with regard to religion, nationality, sexual orientation, or disabilities. This is mainly due to lack of suitable benchmarks for such groups. We seek to address this gap by providing a visual and textual bias benchmark called MMBias, consisting of around 3,800 images and phrases covering 14 population subgroups. We utilize this dataset to assess bias in several prominent self supervised multimodal models, including CLIP, ALBEF, and ViLT. Our results show that these models demonstrate meaningful bias favoring certain groups. Finally, we introduce a debiasing method designed specifically for such large pre-trained models that can be applied as a post-processing step to mitigate bias, while p
    
[^6]: 基于卷积序列到序列网络将图像特征集成到多语言视觉问答中

    Integrating Image Features with Convolutional Sequence-to-sequence Network for Multilingual Visual Question Answering. (arXiv:2303.12671v1 [cs.CV])

    [http://arxiv.org/abs/2303.12671](http://arxiv.org/abs/2303.12671)

    该论文针对VLSP2022-EVJVQA共享任务提出了一种基于卷积序列到序列网络的方法，在多语言视觉问答中将预训练的VQA模型和图像特征集成，获得了很好的效果。

    

    视觉问答(VQA)是一项要求计算机基于图像回答输入问题的任务。这个任务对人类来说很容易，但对计算机来说很具有挑战性。VLSP2022-EVJVQA共享任务在一个新发布的数据集UIT-EVJVQA中进行了多语言领域的VQA任务，其中问题和答案用三种不同语言编写：英语、越南语和日语。我们将这个挑战作为一个序列到序列的学习任务，通过将来自预训练的最先进的VQA模型和图像特征的提示与卷积序列到序列网络集成在一起来生成所需的答案。我们在公共测试集上获得了F1得分达到0.3442，在私有测试集上获得了0.4210的好成绩，并在比赛中取得了第三名。

    Visual Question Answering (VQA) is a task that requires computers to give correct answers for the input questions based on the images. This task can be solved by humans with ease but is a challenge for computers. The VLSP2022-EVJVQA shared task carries the Visual Question Answering task in the multilingual domain on a newly released dataset: UIT-EVJVQA, in which the questions and answers are written in three different languages: English, Vietnamese and Japanese. We approached the challenge as a sequence-to-sequence learning task, in which we integrated hints from pre-trained state-of-the-art VQA models and image features with Convolutional Sequence-to-Sequence network to generate the desired answers. Our results obtained up to 0.3442 by F1 score on the public test set, 0.4210 on the private test set, and placed 3rd in the competition.
    
[^7]: 评估目标论据在谣言立场分类中的作用

    Evaluating the Role of Target Arguments in Rumour Stance Classification. (arXiv:2303.12665v1 [cs.CL])

    [http://arxiv.org/abs/2303.12665](http://arxiv.org/abs/2303.12665)

    本文重新评估了目标论据在谣言立场分类中的作用，证明最先进的模型过于依赖表面信号，目标不独立的事件自然高发生率可能导致模型过度拟合。

    

    针对一组对话，立场分类旨在确定答复对给定目标的意见（例如同意或不同意）。在这项任务中，立场的目标预计是一个重要组成部分，是使其与情感分析不同的主要因素之一。然而，最近的一项研究表明，一个忽略目标的模型优于目标感知模型，表明在预测立场时目标并不有用。本文重新审视了这一现象，针对社交媒体上的谣言立场分类（RSC），其中目标是源推文中隐含的谣言故事。我们在测试数据中提出了对抗攻击，旨在评估模型的鲁棒性并评估数据在模型性能中的作用。结果显示，包括使用整个对话线程的方法在内的最先进的模型过于依赖表面信号。我们的假设是，目标不独立的事件自然高发生率可能导致模型过度拟合。

    Considering a conversation thread, stance classification aims to identify the opinion (e.g. agree or disagree) of replies towards a given target. The target of the stance is expected to be an essential component in this task, being one of the main factors that make it different from sentiment analysis. However, a recent study shows that a target-oblivious model outperforms target-aware models, suggesting that targets are not useful when predicting stance. This paper re-examines this phenomenon for rumour stance classification (RSC) on social media, where a target is a rumour story implied by the source tweet in the conversation. We propose adversarial attacks in the test data, aiming to assess the models robustness and evaluate the role of the data in the models performance. Results show that state-of-the-art models, including approaches that use the entire conversation thread, overly relying on superficial signals. Our hypothesis is that the naturally high occurrence of target-indepen
    
[^8]: AfroDigits：面向非洲语言的基于社区驱动的口语数字数据集

    AfroDigits: A Community-Driven Spoken Digit Dataset for African Languages. (arXiv:2303.12582v1 [cs.CL])

    [http://arxiv.org/abs/2303.12582](http://arxiv.org/abs/2303.12582)

    AfroDigits是第一个发布的面向非洲语言的音频数字数据集，为非洲语言中的语音应用程序开辟了道路。

    

    计算机语音技术的进步是显著的，但由于非洲语料库的匮乏，其在非洲语言中的集成仍然有限。为了解决这个问题，我们提出了AfroDigits，这是一个极简的、由社区驱动的非洲语言口语数字数据集，目前覆盖了38种非洲语言。作为AfroDigits实际应用的演示，我们使用Wav2Vec2.0-Large和XLS-R模型，在六种非洲语言[Igbo(ibo), Yoruba(yor), Rundi(run), Oshiwambo(kua), Shona(sna)和Oromo(gax)]上进行了音频数字分类实验。我们的实验揭示了混合非洲语料库在微调过程中的效果。AfroDigits是非洲语言中第一个发布的音频数字数据集，我们相信它将为面向非洲的语音应用程序开辟道路，如电话号码和街道地址的识别。我们在https:/ / afrodigits公开发布数据集和平台。

    The advancement of speech technologies has been remarkable, yet its integration with African languages remains limited due to the scarcity of African speech corpora. To address this issue, we present AfroDigits, a minimalist, community-driven dataset of spoken digits for African languages, currently covering 38 African languages. As a demonstration of the practical applications of AfroDigits, we conduct audio digit classification experiments on six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo (kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R models. Our experiments reveal a useful insight on the effect of mixing African speech corpora during finetuning. AfroDigits is the first published audio digit dataset for African languages and we believe it will, among other things, pave the way for Afro-centric speech applications such as the recognition of telephone numbers, and street numbers. We release the dataset and platform publicly at https:/
    
[^9]: RepoCoder：通过迭代检索和生成实现的代码存储库级别完成

    RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])

    [http://arxiv.org/abs/2303.12570](http://arxiv.org/abs/2303.12570)

    RepoCoder是一个简单、通用和有效的框架，结合了一个基于相似性的检索器和预训练的代码语言模型，在库级别代码完成流程中实现了有效利用库级别信息进行代码完成和生成。

    

    库级别代码完成任务是基于代码库更广阔上下文中继续编写未完成代码的过程。但是对于自动完成工具而言，很难利用散布在不同文件中的有用信息。我们提出了RepoCoder，这是一个简单、通用和有效的框架，可以应对这一挑战。它通过整合基于相似性的检索器和预训练的代码语言模型简化了库级别代码完成流程，从而允许有效利用库级别信息进行代码完成，并具有不同粒度层面的代码生成能力。此外，RepoCoder 还使用了一种新的迭代检索-生成模型，弥合了检索上下文和预期完成目标之间的差距。我们还提出了一个新的RepoEval基准测试，其中包含了最新和高质量真实世界的代码库，涵盖了行、API 调用和函数体完成场景。

    The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model, which allows for the effective utilization of repository-level information for code completion and grants the ability to generate code at various levels of granularity. Furthermore, RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges the gap between retrieval context and the intended completion target. We also propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion sce
    
[^10]: MEGA: 多语言生成人工智能的综合评估

    MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v1 [cs.CL])

    [http://arxiv.org/abs/2303.12528](http://arxiv.org/abs/2303.12528)

    这项研究对 33 种语言中 8 个不同任务的生成 AI 进行了全面评估，比较了生成 LLMs 和非自回归模型的表现差异。

    

    生成AI模型在许多自然语言处理任务（如语言理解、推理和语言生成）上具有令人印象深刻的性能。当今AI社区最重要的问题之一是关于这些模型的能力和限制，评估生成AI显然具有挑战性。大多数关于生成大型语言模型（LLMs）的研究都限于英语，不清楚这些模型在理解和生成其他语言方面的能力。我们提供了首个全面评估 8 项不同任务和 33 种语言的生成LLMs MEGA 的基准测试。我们还将生成LLMs的性能与这些任务上最先进的非自回归模型进行比较，以确定生成模型的表现如何与上一代LLMs相比。我们对模型的性能进行了彻底分析。

    Generative AI models have impressive performance on many Natural Language Processing tasks such as language understanding, reasoning and language generation. One of the most important questions that is being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative Large Language Models (LLMs) are restricted to English and it is unclear how capable these models are at understanding and generating other languages. We present the first comprehensive benchmarking of generative LLMs MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse tasks and 33 typologically diverse languages. We also compare the performance of generative LLMs to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of model
    
[^11]: BERT是否盲目？探索视觉语言预训练对视觉语言理解的影响。

    Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding. (arXiv:2303.12513v1 [cs.CV])

    [http://arxiv.org/abs/2303.12513](http://arxiv.org/abs/2303.12513)

    本文调查了视觉语言预训练对仅文本任务的表现是否有提高。作者提出了一套视觉语言理解任务，证明了多模态训练的文本编码器在视觉推理方面的优越性。

    

    大多数人使用视觉想象来理解和推理语言，但是像BERT这样的模型使用在仅包括文本的预训练过程中获取的知识来推理语言。在本文中，我们调查了视觉语言预训练是否可以提高在涉及隐含视觉推理的仅文本任务上的表现，重点是零样本探测方法。我们提出了一套用于探测文本编码器模型视觉推理能力的视觉语言理解（VLU）任务，以及各种非视觉自然语言理解（NLU）任务用于比较。我们还贡献了一种新型的零样本知识探测方法，Stroop probing，用于将像CLIP这样的模型应用于仅文本任务，而不需要像BERT模型的掩码语言建模头那样的预测头。我们证明了SOTA多模态训练的文本编码器在VLU任务上优于单模态训练的文本编码器，但在NLU任务上不及它们。

    Most humans use visual imagination to understand and reason about language, but models such as BERT reason about language using knowledge acquired during text-only pretraining. In this work, we investigate whether vision-and-language pretraining can improve performance on text-only tasks that involve implicit visual reasoning, focusing primarily on zero-shot probing methods. We propose a suite of visual language understanding (VLU) tasks for probing the visual reasoning abilities of text encoder models, as well as various non-visual natural language understanding (NLU) tasks for comparison. We also contribute a novel zero-shot knowledge probing method, Stroop probing, for applying models such as CLIP to text-only tasks without needing a prediction head such as the masked language modelling head of models like BERT. We show that SOTA multimodally trained text encoders outperform unimodally trained text encoders on the VLU tasks while being underperformed by them on the NLU tasks, lendin
    
[^12]: 少样本、多模态、多任务、多语言学习

    Few-shot Multimodal Multitask Multilingual Learning. (arXiv:2303.12489v1 [cs.LG])

    [http://arxiv.org/abs/2303.12489](http://arxiv.org/abs/2303.12489)

    提出了一种多阶段微调框架，针对少样本多模态多任务多语言学习，可以有效地利用迁移学习和少样本学习的优势，它采用一种通用的编码器-解码器骨架，并使用注意机制处理多模态信息和每个任务的语言特定的微调，在多个基准数据集上表现优异。

    

    少样本学习作为一种迁移学习范式，在数据受限的情况下已经得到了广泛的应用。然而，过去主要是在构建单模态单语言模型的背景下探讨了少样本学习。现有的少样本多任务学习领域的大部分文献都是在上下文学习的情况下进行的，需要手动生成提示作为输入，这导致了结果的差异，取决于手动提示工程的水平。此外，上下文学习会带来相当大的计算、内存、存储成本，最终导致高推理延迟，因为它涉及每次进行预测时都要通过模型运行所有提示的示例。相比之下，通过微调范式的迁移学习方法避免了上述问题，在任务的基础上以一次性的代价微调权重。然而，这种方法缺乏少样本多模态多任务学习的经验。在本文中，我们提出了一种多阶段微调框架，针对少样本多模态多任务多语言学习，可以有效地利用迁移学习和少样本学习的优势。我们的方法采用一种通用的编码器-解码器骨架，并使用注意机制处理多模态信息和每个任务的语言特定的微调。所提出的框架表现出了优秀的性能，在几个基准数据集上优于现有模型。

    While few-shot learning as a transfer learning paradigm has gained significant traction for scenarios with limited data, it has primarily been explored in the context of building unimodal and unilingual models. Furthermore, a significant part of the existing literature in the domain of few-shot multitask learning perform in-context learning which requires manually generated prompts as the input, yielding varying outcomes depending on the level of manual prompt-engineering. In addition, in-context learning suffers from substantial computational, memory, and storage costs which eventually leads to high inference latency because it involves running all of the prompt's examples through the model every time a prediction is made. In contrast, methods based on the transfer learning via the fine-tuning paradigm avoid the aforementioned issues at a one-time cost of fine-tuning weights on a per-task basis. However, such methods lack exposure to few-shot multimodal multitask learning. In this pap
    
[^13]: GrapeQA：增强问答功能的图形增强和剪枝方法

    GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering. (arXiv:2303.12320v1 [cs.CL])

    [http://arxiv.org/abs/2303.12320](http://arxiv.org/abs/2303.12320)

    GrapeQA是一种新方法，使用“重要实体图形增强”和“上下文感知节点剪枝”策略，以提高问答准确性和效率。

    

    常识问答方法结合了预先训练的语言模型（LM）的能力和知识图（KG）提供的推理。 典型方法从KG中收集与QA匹配的节点以形成工作图（WG），然后使用图神经网络（GNN）进行推理。这面临两个主要挑战：（i）很难从WG中捕获QA中的所有信息，（ii）WG包含一些来自KG的不相关节点。为了解决这些问题，我们提出了一个名为GrapeQA的算法以对WG进行两个简单的改进：（i）用于图形增强的重要实体（Prominent Entities）识别QA对当中相关文本块，并使用相应的潜在表示从LM进行增强；（ii）将不相关的节点剪枝。我们在OpenBookQA，CommonsenseQA和MedQA-USMLE上评估了结果，并发现GrapeQA显示出持续的改进，超过了其LM + KG前身（特别是QA-GNN）并获得了巨大的改进。

    Commonsense question-answering (QA) methods combine the power of pre-trained Language Models (LM) with the reasoning provided by Knowledge Graphs (KG). A typical approach collects nodes relevant to the QA pair from a KG to form a Working Graph (WG) followed by reasoning using Graph Neural Networks(GNNs). This faces two major challenges: (i) it is difficult to capture all the information from the QA in the WG, and (ii) the WG contains some irrelevant nodes from the KG. To address these, we propose GrapeQA with two simple improvements on the WG: (i) Prominent Entities for Graph Augmentation identifies relevant text chunks from the QA pair and augments the WG with corresponding latent representations from the LM, and (ii) Context-Aware Node Pruning removes nodes that are less relevant to the QA pair. We evaluate our results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows consistent improvements over its LM + KG predecessor (QA-GNN in particular) and large improveme
    
[^14]: 具有元梯度正则化的自监督元提示学习用于少样本泛化

    Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])

    [http://arxiv.org/abs/2303.12314](http://arxiv.org/abs/2303.12314)

    提出了一种自我监督元提示学习框架SUPMER，包括元梯度正则化，用于少样本泛化，通过锚定的元训练任务和基于课程的任务增强丰富了任务分布，解决了在少样本情况下良好初始化软提示和过拟合的问题。

    

    提示调整是一种参数有效的方法，它学习软提示并使冻结的语言模型执行特定的下游任务。尽管有效，但提示调整在少样本情况下一方面严重依赖于良好的软提示初始化。另一方面，它很容易导致过度拟合。现有的方法利用预训练或监督元学习来初始化软提示，但它们不能对未见下游任务进行数据有效的泛化。为了解决以上问题，本文提出了一种新的自我监督元提示学习框架，其中包括元梯度正则化，用于少样本泛化（SUPMER）。我们首先设计了一组自监督锚定的元训练任务，具有不同的任务格式，并通过基于课程的任务增强进一步丰富了任务分布。然后将一种新的元梯度正则化方法集成到元提示学习中。它元学习在少样本情况下如何转换原始梯度。

    Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily result in overfitting. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they cannot data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with meta-gradient Regularization for few-shot generalization (SUPMER). We first design a set of self-supervised anchor meta-training tasks with different task formats and further enrich the task distribution with curriculum-based task augmentation. Then a novel meta-gradient regularization method is integrated into meta-prompt learning. It meta-learns to transform the raw gradients during few
    
[^15]: 基于混合CTC/Attention架构和多特征融合网络的土耳其语音识别研究

    Exploring Turkish Speech Recognition via Hybrid CTC/Attention Architecture and Multi-feature Fusion Network. (arXiv:2303.12300v1 [cs.SD])

    [http://arxiv.org/abs/2303.12300](http://arxiv.org/abs/2303.12300)

    本文提出了一种基于混合CTC/Attention架构和多特征融合网络的土耳其语音识别系统，实现了数据增强技术和特征提取器LSPC的优化。实验结果表明，该模型在土耳其语音语料库上取得良好表现。

    

    近年来，基于深度学习的端到端语音识别技术迅速发展。由于缺乏土耳其语音数据，土耳其语音识别系统的性能较差。本文研究了一系列语音识别调优技术。首先，实验结果表明，当采用将速度扰动和噪声添加相结合的数据增强技术并将波束搜索宽度设置为16时，模型的性能最佳。其次，为充分利用有效特征信息、提高特征提取准确性，本文提出了一种新的特征提取器LSPC。并通过将LSPC和LiGRU网络相结合构成共享编码器结构实现模型压缩。最后，在以上两点基础上，提出了一个新的多特征融合网络。该网络提出了混合CTC/Attention架构，使模型兼具两者的优点，并有效地结合了不同的声学特征，比使用单一特征更具实验效果。实验结果表明，该模型在作者开发的土耳其语音语料库上取得了最先进的性能。

    In recent years, End-to-End speech recognition technology based on deep learning has developed rapidly. Due to the lack of Turkish speech data, the performance of Turkish speech recognition system is poor. Firstly, this paper studies a series of speech recognition tuning technologies. The results show that the performance of the model is the best when the data enhancement technology combining speed perturbation with noise addition is adopted and the beam search width is set to 16. Secondly, to maximize the use of effective feature information and improve the accuracy of feature extraction, this paper proposes a new feature extractor LSPC. LSPC and LiGRU network are combined to form a shared encoder structure, and model compression is realized. The results show that the performance of LSPC is better than MSPC and VGGnet when only using Fbank features, and the WER is improved by 1.01% and 2.53% respectively. Finally, based on the above two points, a new multi-feature fusion network is pr
    
[^16]: 评估变换器模型和人类行为在中文字符命名方面的表现。

    Evaluating Transformer Models and Human Behaviors on Chinese Character Naming. (arXiv:2303.12294v1 [cs.CL])

    [http://arxiv.org/abs/2303.12294](http://arxiv.org/abs/2303.12294)

    本研究评估了一组 transformer 模型，在未知的中文字符命名任务中，这些模型表现得与人类很相似，能够很好地捕捉人类字符命名行为。

    

    对于许多字母语言，已经提出了神经网络模型来解释人类的字素-音素映射过程。这些模型不仅成功地学习了字母字符串及其发音的对应关系，而且还捕捉了人类在短暂单词命名任务中的行为。本研究中，我们评估了一组变换器模型，并将它们的表现与人类在未知中文字符命名任务中的表现进行比较。我们发现，模型和人类的行为非常相似，它们在每个字符的准确度分布方面具有类似的准确度，并且在答案上有很大的重叠。此外，模型的答案与人类的答案高度相关。这些结果表明，变换器模型能够很好地捕捉人类的字符命名行为。

    Neural network models have been proposed to explain the grapheme-phoneme mapping process in humans for many alphabet languages. These models not only successfully learned the correspondence of the letter strings and their pronunciation, but also captured human behavior in nonce word naming tasks. How would the neural models perform for a non-alphabet language (e.g., Chinese) unknown character task? How well would the model capture human behavior? In this study, we evaluate a set of transformer models and compare their performances with human behaviors on an unknown Chinese character naming task. We found that the models and humans behaved very similarly, that they had similar accuracy distribution for each character, and had a substantial overlap in answers. In addition, the models' answers are highly correlated with humans' answers. These results suggested that the transformer models can well capture human's character naming behavior.
    
[^17]: 深层句法关系的统一分类

    A Unified Taxonomy of Deep Syntactic Relations. (arXiv:2303.12220v1 [cs.CL])

    [http://arxiv.org/abs/2303.12220](http://arxiv.org/abs/2303.12220)

    本文提出了一组通用的语义角色标签，针对Universal Dependencies，并应用于四种语言的数据中。

    

    本文分析了多个深层句法框架，旨在提出一组普适的语义角色标签。提案考虑了各种理论语言学视角，并重点关注了Meaning-Text Theory和Functional Generative Description框架。为了研究的目的，使用了来自四种语言的数据——西班牙语和加泰罗尼亚语（Taule等，2011），捷克语（Hajic等，2017）和英语（Hajic等，2012）。该提案针对Universal Dependencies（de Marneffe等，2021），并进一步意图将通用的语义角色标签应用于UD数据。

    This paper analyzes multiple deep-syntactic frameworks with the goal of creating a proposal for a set of universal semantic role labels. The proposal examines various theoretic linguistic perspectives and focuses on Meaning-Text Theory and Functional Generative Description frameworks.  For the purpose of this research, data from four languages is used -- Spanish and Catalan (Taule et al., 2011), Czech (Hajic et al., 2017), and English (Hajic et al., 2012). This proposal is oriented towards Universal Dependencies (de Marneffe et al., 2021) with a further intention of applying the universal semantic role labels to the UD data.
    
[^18]: MAGVLT: 带掩码的生成视觉语言变压器

    MAGVLT: Masked Generative Vision-and-Language Transformer. (arXiv:2303.12208v1 [cs.CV])

    [http://arxiv.org/abs/2303.12208](http://arxiv.org/abs/2303.12208)

    本文提出 MGVLT 模型用于生成图像和文本序列，通过非自回归掩码预测实现了双向上下文编码和快速解码等特点。

    

    虽然在多模态图像文本数据上的生成建模已经得到了广泛的发展，但仍有许多限制，例如仅生成一种模态的固定模型。在本文中，我们探讨了一种可以生成图像和文本序列的统一生成式视觉语言（VL）模型。特别地，我们提出了一种基于非自回归掩码预测的生成VL变压器，名为MAGVLT，并将其与自回归生成VL变压器（ARGVLT）进行了比较。与ARGVLT相比，所提出的MAGVLT实现了双向上下文编码，通过迭代细化的并行标记预测实现了快速解码，具有图像和文本填充等扩展编辑功能。为了从头开始严格训练我们的MAGVLT模型，我们结合了从图像到文本、从文本到图像、以及联合图像和文本的掩码预测任务。

    While generative modeling on multimodal image-text data has been actively developed with large-scale paired datasets, there have been limited attempts to generate both image and text data by a single model rather than a generation of one fixed modality conditioned on the other modality. In this paper, we explore a unified generative vision-and-language (VL) model that can produce both images and text sequences. Especially, we propose a generative VL transformer based on the non-autoregressive mask prediction, named MAGVLT, and compare it with an autoregressive generative VL transformer (ARGVLT). In comparison to ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast decoding by parallel token predictions in an iterative refinement, and extended editing capabilities such as image and text infilling. For rigorous training of our MAGVLT with image-text pairs from scratch, we combine the image-to-text, text-to-image, and joint image-and-text mask prediction tasks. Moreove
    
[^19]: 利用上下文化的大型语言模型理解法律文件

    Understand Legal Documents with Contextualized Large Language Models. (arXiv:2303.12135v1 [cs.CL])

    [http://arxiv.org/abs/2303.12135](http://arxiv.org/abs/2303.12135)

    本文介绍了针对 SemEval-2023 任务 6 开发的 Legal-BERT-HSLN 模型和 Legal-LUKE 模型，其中 Legal-BERT-HSLN 模型通过考虑句内和句间的上下文信息以预测修辞角色，Legal-LUKE 模型是具有法律上下文和实体知识的模型，以识别法律实体。模型相比基线模型更准确，能够解决在人口众多的国家处理法律文件的问题。

    

    在人口众多的国家，如印度，待处理的法律案件数量不断增加，这已成为一个重大问题。因此，开发有效的技术来处理和理解法律文件将非常有用。在本文中，我们介绍了我们针对 SemEval-2023 任务 6（Modi 等人，2023）所开发的理解法律文本系统。具体来说，我们首先开发了 Legal-BERT-HSLN 模型，该模型考虑了句内和句间的综合上下文信息，以预测修辞角色（子任务 A），然后训练出 Legal-LUKE 模型，该模型具有法律上下文化和实体感知能力，以识别法律实体（子任务 B）。我们的评估表明，我们设计的模型比基线模型更准确，如在子任务 B 中 F1 值提高了达 15.0%。我们在任务排行榜上取得了显著的表现，如 0.834 微平均 F1 值，并在子任务 A 中排名第 5。

    The growth of pending legal cases in populous countries, such as India, has become a major issue. Developing effective techniques to process and understand legal documents is extremely useful in resolving this problem. In this paper, we present our systems for SemEval-2023 Task 6: understanding legal texts (Modi et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that considers the comprehensive context information in both intra- and inter-sentence levels to predict rhetorical roles (subtask A) and then train a Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize legal entities (subtask B). Our evaluations demonstrate that our designed models are more accurate than baselines, e.g., with an up to 15.0% better F1 score in subtask B. We achieved notable performance in the task leaderboard, e.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.
    
[^20]: 生成式大语言模型的基础与在网络防御中的应用

    Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense. (arXiv:2303.12132v1 [cs.CL])

    [http://arxiv.org/abs/2303.12132](http://arxiv.org/abs/2303.12132)

    生成式语言模型的改进引起了公众广泛关注。它们广泛应用及其真实能力揭示了它们的潜在应用，但同时也带来了对其可能的恶意用途的担忧。本文旨在提供生成式大语言模型的简要概述以及在网络防御中的应用。

    

    生成式语言模型在2022年底和2023年初引起了广泛关注，尤其是引入了与用户期望的AI交互一致的模型（对话模型）。人们关注的焦点可以说是GPT3模型的这种改进——ChatGPT及其随后与辅助功能集成的应用，包括作为Microsoft Bing的搜索部分。尽管此前已经投入了大量研究进行开发，但它们在各种日常任务中的性能和适用性仍不清楚且狭窄。然而，他们的广泛应用并不需要技术专业知识，这在相当大的程度上是通过对话微调实现的，揭示了在实际环境中它们真实能力的范围。这引起了公众对其潜在应用的兴奋和对其能力及可能的恶意用途的担忧。本综述旨在提供生成式大语言模型的简要概述以及在网络防御中的应用。

    Generative Language Models gained significant attention in late 2022 / early 2023, notably with the introduction of models refined to act consistently with users' expectations of interactions with AI (conversational models). Arguably the focal point of public attention has been such a refinement of the GPT3 model -- the ChatGPT and its subsequent integration with auxiliary capabilities, including search as part of Microsoft Bing. Despite extensive prior research invested in their development, their performance and applicability to a range of daily tasks remained unclear and niche. However, their wider utilization without a requirement for technical expertise, made in large part possible through conversational fine-tuning, revealed the extent of their true capabilities in a real-world environment. This has garnered both public excitement for their potential applications and concerns about their capabilities and potential malicious uses. This review aims to provide a brief overview of th
    
[^21]: 基于正样本增强对比学习的图像视频标题评估

    Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v1 [cs.CV])

    [http://arxiv.org/abs/2303.12112](http://arxiv.org/abs/2303.12112)

    本论文提出一种新的图像标题评估指标PAC-S，可以更准确地评估图像和视频的标题，相比于现有的指标有更好的表现；源代码和训练模型已经公开。

    

    最近CLIP模型在很多跨模态任务上都非常有效，包括从视觉和语言结构中生成的标题评估。本文提出了一种新的基于对比度的图像标题评估指标配方，即正样本增强的对比度学习分数（PAC-S），以一种新颖的方式统一了对比度视觉-语义空间的学习和策展数据上生成的图像和文本的添加。跨越多个数据集的实验表明，我们的新指标在图像和视频上与人类判断的相关性最高，优于现有参考指标（如CIDEr和SPICE）和无参考指标（如CLIP-Score）。最后，我们考虑了流行的图像标题方法，并评估了采用不同跨模态特征的影响。我们的源代码和训练模型是公开的。

    The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly availa
    
[^22]: 大型语言模型可以在零-shot学习环境下用于评估政治家的意识形态

    Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting. (arXiv:2303.12057v1 [cs.CY])

    [http://arxiv.org/abs/2303.12057](http://arxiv.org/abs/2303.12057)

    本文展示了在零-shot学习环境下，大型语言模型可以用于评估政治家的意识形态，为我们更好地理解政治功能提供了有用的信息。

    

    大型语言模型（LLMs）中蕴含的大量知识可以为社会科学中的可观测性和测量问题提供新的解决方案。本文研究了其中一种模型在衡量立法者的潜在意识形态方面的效用，这有助于我们更好地理解塑造政策的政治功能，以及政治行为者代表其选民的方式。我们通过提示ChatGPT在两两比较中选择更自由派（或保守派）的参议员，将第116届美国国会的参议员按照自由派-保守派的光谱进行缩放。我们展示了LLM在重复迭代中产生了稳定的答案，没有产生幻觉，并且不仅仅是从单一来源中复制信息。这个新尺度与现有的自由派-保守派尺度（如NOMINATE）强相关，但也在几个重要方面存在差异，比如正确定位一些路径依赖和自由派派别的议员。

    The mass aggregation of knowledge embedded in large language models (LLMs) holds the promise of new solutions to problems of observability and measurement in the social sciences. We examine the utility of one such model for a particularly difficult measurement task: measuring the latent ideology of lawmakers, which allows us to better understand functions that are core to democracy, such as how politics shape policy and how political actors represent their constituents. We scale the senators of the 116th United States Congress along the liberal-conservative spectrum by prompting ChatGPT to select the more liberal (or conservative) senator in pairwise comparisons. We show that the LLM produced stable answers across repeated iterations, did not hallucinate, and was not simply regurgitating information from a single source. This new scale strongly correlates with pre-existing liberal-conservative scales such as NOMINATE, but also differs in several important ways, such as correctly placin
    
[^23]: EVA-02：新世纪福音战士的视觉表现

    EVA-02: A Visual Representation for Neon Genesis. (arXiv:2303.11331v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.11331](http://arxiv.org/abs/2303.11331)

    EVA-02是一种基于Transformer的下一代视觉表征，具有重建强大且稳健特征的能力，并在各种代表性视觉任务中表现出色，同时使用的参数和计算预算显著较少。

    

    我们发布EVA-02，这是一种基于Transformer的下一代视觉表征，经过预训练，通过掩蔽图像建模重建强大且稳健的特征，实现语言和视觉的对齐。使用更新的普通Transformer架构以及来自开放且易于访问的巨型CLIP视觉编码器的广泛预训练，EVA-02在各种代表性视觉任务方面表现优异，同时使用的参数和计算预算显著较少。值得注意的是，仅使用公开可访问的训练数据，具有304M参数的EVA-02在ImageNet-1K val集上实现了惊人的90.0微调top-1精度。此外，我们的EVA-02-CLIP在ImageNet-1K上的零样本top-1可达80.4，胜过了以前最大且最好的开源CLIP，仅使用了约1/6的参数和图像文本训练数据。我们提供了四种EVA-02变体，其模型大小各不相同，范围从6M到304M参数，均具有令人印象深刻的性能。

    We launch EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling. With an updated plain Transformer architecture as well as extensive pre-training from an open & accessible giant CLIP vision encoder, EVA-02 demonstrates superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets. Notably, using exclusively publicly accessible training data, EVA-02 with only 304M parameters achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set. Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on ImageNet-1K, outperforming the previous largest & best open-sourced CLIP with only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02 variants in various model sizes, ranging from 6M to 304M parameters, all with impressive
    
[^24]: EmotionIC：基于情感惯性和感染的依赖建模可用于对话中的情感识别

    EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation. (arXiv:2303.11117v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.11117](http://arxiv.org/abs/2303.11117)

    本文提出了一种新的依赖性建模方法，由情感惯性和感染驱动（EmotionIC），用于在特征提取和分类级别上进行会话情感识别。设计了多项具体方法，包括身份掩码多头注意（IM-MHA）和基于对话门控循环单元(DialogGRU)，以抓取上下文信息，提高模型的性能。

    

    最近，随着人机界面技术的进步和实施，对话中的情感识别（ERC）吸引了越来越多的关注。然而，以往的建模方法在全局和局部上下文依赖方面丢失了依赖信息的多样性，并且在分类级别不考虑上下文依赖关系。本文提出了一种新的依赖性建模方法，由情感惯性和感染驱动（EmotionIC），用于在特征提取和分类级别上进行会话情感识别。在特征提取级别，我们设计的身份掩码多头注意（IM-MHA）捕捉对话中基于身份的长距离上下文，以包含不同参与者的不同影响构建全局情感氛围，而设计的基于对话门控循环单元(DialogGRU)则聚合了二元对话的情感倾向，并应用于分类过程。

    Emotion Recognition in Conversation (ERC) has attracted growing attention in recent years as a result of the advancement and implementation of human-computer interface technologies. However, previous approaches to modeling global and local context dependencies lost the diversity of dependency information and do not take the context dependency into account at the classification level. In this paper, we propose a novel approach to dependency modeling driven by Emotional Inertia and Contagion (EmotionIC) for conversational emotion recognition at the feature extraction and classification levels. At the feature extraction level, our designed Identity Masked Multi-head Attention (IM-MHA) captures the identity-based long-distant context in the dialogue to contain the diverse influence of different participants and construct the global emotional atmosphere, while the devised Dialogue-based Gate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of dyadic dialogue is applied to
    
[^25]: 字符，词还是两者兼备？——重访中文预训练语言模型的分词粒度

    Character, Word, or Both? Revisiting the Segmentation Granularity for Chinese Pre-trained Language Models. (arXiv:2303.10893v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.10893](http://arxiv.org/abs/2303.10893)

    该论文提出了一种混合粒度中文BERT（MigBERT），同时考虑字符和单词，以更好地表现单词的语义。实验证明MigBERT在各种中文NLP任务上均可实现新的SOTA性能。

    

    预训练语言模型（PLMs）在各种自然语言处理任务中表现出了惊人的改进。大多数中文PLMs仅将输入文本视为一系列字符，并完全忽略单词信息。尽管整词遮盖可以缓解这个问题，但单词中的语义仍然无法良好地表现。在本文中，我们重新审视了中文PLMs的分词粒度，并提出了一种混合粒度的中文BERT（MigBERT），同时考虑字符和单词。为了实现这一点，我们设计了学习字符和单词级表示的目标函数。我们对各种中文NLP任务进行了广泛的实验，以评估现有的PLMs和所提出的MigBERT。实验结果表明，MigBERT在所有这些任务中均实现了新的SOTA性能。进一步的分析表明，单词在语义上比字符更丰富。更有趣的是，我们展示了MigBERT也可以在日语中工作。

    Pretrained language models (PLMs) have shown marvelous improvements across various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence of characters, and completely ignore word information. Although Whole Word Masking can alleviate this, the semantics in words is still not well represented. In this paper, we revisit the segmentation granularity of Chinese PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both characters and words. To achieve this, we design objective functions for learning both character and word-level representations. We conduct extensive experiments on various Chinese NLP tasks to evaluate existing PLMs as well as the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA performance on all these tasks. Further analysis demonstrates that words are semantically richer than characters. More interestingly, we show that MigBERT also works with Japanese. Our code and model have been released here~\footnote{htt
    
[^26]: 通过连续提示的抽取式问答从Twitter中提取COVID-19事件

    COVID-19 event extraction from Twitter via extractive question answering with continuous prompts. (arXiv:2303.10659v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.10659](http://arxiv.org/abs/2303.10659)

    本文使用连续提示和抽取式问答的方法从Twitter中提取COVID-19事件，在COVID-19事件插槽中达到了5%以上的F1得分改进。

    

    随着COVID-19席卷全球，社交媒体分析可以增强传统调查，评估该大流行的演变情况，并捕捉有助于卫生机构应对的消费者议论，一般涉及挖掘提到检测呈阳性或关于预防或治疗选项的讨论的披露事件。该论文将事件提取问题转化为抽取式问答，使用语言模型中的连续提示的最新进展。在共享任务测试数据集上，我们的方法使COVID-19事件槽的所有细节都能达到5%以上的微平均F1得分改进。我们的消融研究表明，连续提示对操作结果产生了重大影响。

    As COVID-19 ravages the world, social media analytics could augment traditional surveys in assessing how the pandemic evolves and capturing consumer chatter that could help healthcare agencies in addressing it. This typically involves mining disclosure events that mention testing positive for the disease or discussions surrounding perceptions and beliefs in preventative or treatment options. The 2020 shared task on COVID-19 event extraction (conducted as part of the W-NUT workshop during the EMNLP conference) introduced a new Twitter dataset for benchmarking event extraction from COVID-19 tweets. In this paper, we cast the problem of event extraction as extractive question answering using recent advances in continuous prompting in language models. On the shared task test dataset, our approach leads to over 5% absolute micro-averaged F1-score improvement over prior best results, across all COVID-19 event slots. Our ablation study shows that continuous prompts have a major impact on the 
    
[^27]: ChatGPT参加计算机科学考试

    ChatGPT Participates in a Computer Science Exam. (arXiv:2303.09461v1 [cs.CL])

    [http://arxiv.org/abs/2303.09461](http://arxiv.org/abs/2303.09461)

    ChatGPT在算法和数据结构考试中取得20.5分的成绩，表现出能在具有挑战性的大学考试中成功，但不能说明其对计算机科学有理解。

    

    我们要求ChatGPT参加“算法和数据结构”的本科计算机科学考试。我们评估了程序在整个考试中的表现，并将其答案手动复制到考试答题纸上，与其他200名学生一起进行匿名评分。我们发现ChatGPT勉强通过了考试，获得了40分中的20.5分。这个结果表明，ChatGPT确实可以在像大学考试这样的具有挑战性的任务中成功。同时，我们考试中的任务在结构上与其他在线可找到的考试卷、完成的作业问题和教学材料相似。因此，从这个实验中得出ChatGPT有任何计算机科学理解的结论是为时过早的。我们与ChatGPT的谈话记录可以在\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}上找到，整个评分考试在本文的附录中。

    We asked ChatGPT to participate in an undergraduate computer science exam on ''Algorithms and Data Structures''. We evaluated the program on the entire exam as posed to the students. We hand-copied its answers onto an exam sheet, which was subsequently graded in a blind setup alongside those of 200 participating students. We find that ChatGPT narrowly passed the exam, obtaining 20.5 out of 40 points. This impressive performance indicates that ChatGPT can indeed succeed in challenging tasks like university exams. At the same time, the tasks in our exam are structurally similar to those on other exams, solved homework problems, and teaching materials that can be found online. Therefore, it would be premature to conclude from this experiment that ChatGPT has any understanding of computer science. The transcript of our conversation with ChatGPT is available at \url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire graded exam is in the appendix of this paper.
    
[^28]: UPRISE: 通用提示检索以提高零样本评估

    UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. (arXiv:2303.08518v1 [cs.CL])

    [http://arxiv.org/abs/2303.08518](http://arxiv.org/abs/2303.08518)

    UPRISE是一种通用的检索器，可自动为给定的零样本任务输入检索提示，从而提高大型语言模型的零样本评估。它通过跨任务和跨模型的实验展示了其通用性和潜力，同时表明具有减轻幻觉问题和提高LLM性能的能力。

    

    大型语言模型因其出色的能力而受欢迎，但需要特定模型的微调或任务特定提示工程可能会阻碍其一般化。我们提出了UPRISE（通用提示检索以提高零样本评估），该方法调整了轻量级和多功能的检索器，以自动检索给定零样本任务输入的提示。具体而言，在跨任务和跨模型方案中展示了通用性：检索器针对各种任务进行微调，但在看不见的任务类型上进行测试；我们在一个小型冻结LLM——GPT-Neo-2.7B上调整检索器，但在不同规模的LLM上测试检索器，例如BLOOM-7.1B、OPT-66B和GPT3-175B。此外，我们展示了UPRISE在我们与ChatGPT的实验中减轻了幻觉问题，表明它有潜力改进甚至是最强的LLM。

    Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs.
    
[^29]: 讲述故事：通过多模态掩码视频生成统一文本引导的视频补全

    Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation. (arXiv:2211.12824v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.12824](http://arxiv.org/abs/2211.12824)

    本论文引入了一种新的任务，文本引导的视频补全，提出了多模态掩码视频生成方法，能够处理三种情况的视频补全。该方法在基准数据集上取得了最好的性能。

    

    在给定前几个静态帧的情况下生成一个视频是具有挑战性的，因为它需要以时间上的连贯性来预测合理的未来帧。除了视频预测之外，从最后一帧倒回或者在头尾之间进行插值也是至关重要的，但是它们很少被用于视频补全。由于仅凭几个帧的提示可能会有不同的结果，因此能够遵循自然语言来执行视频补全的系统可能会显著提高可控性。受此启发，我们引入了一种新的任务，文本引导的视频补全(TVC)，要求模型在指令的指导下从部分帧中生成视频。然后我们提出了多模态掩码视频生成(MMVG)来解决这个TVC任务。在训练期间，MMVG将视频帧离散为视觉令牌，并遮盖了大部分令牌以进行任意时间点的视频补全。在推理时，一个单独的MMVG模型可以处理TVC的所有三种情况，包括从前几个帧预测的视频，从最后一帧倒回的视频以及在头部和尾部之间进行插值。我们提出的方法在基准数据集上取得了最好的性能，与先前的视频补全方法相比，证明了我们的MMVG在文本引导的视频补全中的有效性。

    Generating a video given the first several static frames is challenging as it anticipates reasonable future frames with temporal coherence. Besides video prediction, the ability to rewind from the last frame or infilling between the head and tail is also crucial, but they have rarely been explored for video completion. Since there could be different outcomes from the hints of just a few frames, a system that can follow natural language to perform video completion may significantly improve controllability. Inspired by this, we introduce a novel task, text-guided video completion (TVC), which requests the model to generate a video from partial frames guided by an instruction. We then propose Multimodal Masked Video Generation (MMVG) to address this TVC task. During training, MMVG discretizes the video frames into visual tokens and masks most of them to perform video completion from any time point. At inference time, a single MMVG model can address all 3 cases of TVC, including video pred
    
[^30]: VoP：用于文本视频跨模态检索的协作提示调整

    VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval. (arXiv:2211.12764v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.12764](http://arxiv.org/abs/2211.12764)

    本论文提出了一种用于文本视频跨模态检索的协作提示调整（VoP）框架，与传统方法相比，VoP具有更少的可训练参数和更低的计算复杂度，同时实现了相似甚至更好的性能。

    

    许多最近的研究利用预训练的CLIP来通过使用额外的重模块来调整backbone从而实现文本视频跨模态检索，这不仅带来了大量的计算负担和更多的参数，也导致了上游模型的知识遗忘。本文提出了VoP：文本视频协作提示调整，以实现对文本视频检索任务的高效调整。所提出的VoP是一个端到端框架，具有引入视频和文本提示的能力，可视为具有仅0.1％可训练参数的强大基线。此外，基于视频的时空特征，我们开发了三种新型视频提示机制，以提高不同可训练参数规模的性能。VoP增强的基本思想是分别利用特定的可训练提示来模拟帧位置，帧上下文和层函数。大量实验表明，与完全微调相比，增强的VoP实现了相似甚至更好的效果，并且具有更少的可训练参数和更低的计算复杂度。

    Many recent studies leverage the pre-trained CLIP for text-video cross-modal retrieval by tuning the backbone with additional heavy modules, which not only brings huge computational burdens with much more parameters, but also leads to the knowledge forgetting from upstream models. In this work, we propose the VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the text-video retrieval task. The proposed VoP is an end-to-end framework with both video & text prompts introducing, which can be regarded as a powerful baseline with only 0.1% trainable parameters. Further, based on the spatio-temporal characteristics of videos, we develop three novel video prompt mechanisms to improve the performance with different scales of trainable parameters. The basic idea of the VoP enhancement is to model the frame position, frame context, and layer function with specific trainable prompts, respectively. Extensive experiments show that compared to full fine-tuning, the enhanced VoP achie
    
[^31]: 基于双流Transformer的通用事件边界字幕生成

    Dual-Stream Transformer for Generic Event Boundary Captioning. (arXiv:2207.03038v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.03038](http://arxiv.org/abs/2207.03038)

    本文提出了一种双流Transformer的通用事件边界字幕生成方法，结合多个预训练模型和边界类型提示，以及单词级别的集成策略，实现了生成更人性化的字幕，并在GEBC测试集上取得了令人满意的结果。

    

    本文介绍了我们参加CVPR2022通用事件边界字幕生成比赛的优胜解决方案。该任务要求字幕生成模型在给定视频边界周围能够理解瞬时状态变化，使其比传统视频字幕生成任务更具挑战性。文章提出了一种双流Transformer，改进了视频内容编码和字幕生成两个方面：(1)我们利用三个预训练模型从不同粒度提取视频特征。此外，我们利用边界类型作为提示，帮助模型生成字幕。(2)我们特别设计一个称为双流Transformer的模型，以学习区分性边界字幕表示。(3)为了生成与内容相关且更加人性化的字幕，我们通过设计一个单词级别的集成策略来改善描述质量。在GEBC测试集上前景不俗的结果证明了我们提出的方法的有效性。

    This paper describes our champion solution for the CVPR2022 Generic Event Boundary Captioning (GEBC) competition. GEBC requires the captioning model to have a comprehension of instantaneous status changes around the given video boundary, which makes it much more challenging than conventional video captioning task. In this paper, a Dual-Stream Transformer with improvements on both video content encoding and captions generation is proposed: (1) We utilize three pre-trained models to extract the video features from different granularities. Moreover, we exploit the types of boundary as hints to help the model generate captions. (2) We particularly design an model, termed as Dual-Stream Transformer, to learn discriminative representations for boundary captioning. (3) Towards generating content-relevant and human-like captions, we improve the description quality by designing a word-level ensemble strategy. The promising results on the GEBC test split demonstrate the efficacy of our proposed 
    
[^32]: 语义歧义的因果结构

    The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.06807](http://arxiv.org/abs/2206.06807)

    本文使用Gogioso和Pinzani在QPL 2021中提出的束理论模型，为语义歧义的两个特征（不同可能解释的联合可信度和某些词在过程中扮演更重要角色的因果结构）进行建模。通过对心理语言学文献中的歧义短语数据集进行分析，研究人员对人类对于这些歧义的判断进行了实证测量。

    

    歧义是自然语言现象，在不同的语法、语义和语用层面上发生。它得到了广泛的研究；例如，在心理语言学领域，我们有多种竞争性的研究人类消歧过程的方法。这些研究是经验性的，基于眼动跟踪等测量方法。本文首次尝试为语义歧义形式化这些进程，其中我们确定了两个特征：(1)不同可能解释之间的联合可信度，(2)根据某些词在过程中扮演更重要角色的因果结构。Gogioso和Pinzani在QPL 2021中提出的新型束理论确定因果性模型并对这些特征进行推理提供了工具。我们将这个理论应用于从心理语言学文献中提取的歧义短语数据集和我们使用Amazon的机械土耳其引擎收集的人类可信度判断中。我们测量了其中的因果关系、歧义水平等。

    Ambiguity is a natural language phenomenon occurring at different levels of syntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics, for instance, we have a variety of competing studies for the human disambiguation processes. These studies are empirical and based on eyetracking measurements. Here we take first steps towards formalizing these processes for semantic ambiguities where we identified the presence of two features: (1) joint plausibility degrees of different possible interpretations, (2) causal structures according to which certain words play a more substantial role in the processes. The novel sheaf-theoretic model of definite causality developed by Gogioso and Pinzani in QPL 2021 offers tools to model and reason about these features. We applied this theory to a dataset of ambiguous phrases extracted from Psycholinguistics literature and their human plausibility judgements collected by us using the Amazon Mechanical Turk engine. We measured the causal fr
    
[^33]: 视觉空间推理

    Visual Spatial Reasoning. (arXiv:2205.00363v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.00363](http://arxiv.org/abs/2205.00363)

    本文介绍了一种名为Visual Spatial Reasoning（VSR）的数据集，其中包含超过10k个自然文本-图像配对，用于推理包括66种空间关系，研究发现目前的视觉和语言模型（VLMs）难以捕捉关系信息和较少关注物体的方向关系。

    

    空间关系是人类认知的基本组成部分。然而，它们以各种方式用自然语言表达，先前的研究表明，目前的视觉和语言模型（VLMs）难以捕捉关系信息。在本文中，我们提出了一种名为Visual Spatial Reasoning（VSR）的数据集，其中包含超过10k个自然文本-图像配对，包括66种英语的空间关系（如：在下面，在前面和面对）。虽然使用了看似简单的注释格式，我们展示了数据集包括具有挑战性的语言现象，例如不同的参考框架。我们展示了人类和模型表现之间的巨大差距：人类准确率高达95%以上，而最先进的模型仅能达到70%左右。我们注意到VLM按关系表现的能力与训练示例数量几乎没有相关性，并且测试的模型通常无法识别涉及对象方向的关系。

    Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (such as: under, in front of, and facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: the human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs' by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.
    

