# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism.](http://arxiv.org/abs/2401.02954) | DeepSeek LLM是一个致力于以长期视野推进开源语言模型发展的项目，通过研究和应用扩展规律，成功创建了DeepSeek Chat模型，该模型在各种基准测试中表现出色，特别是在代码领域。 |
| [^2] | [Fast and Optimal Weight Update for Pruned Large Language Models.](http://arxiv.org/abs/2401.02938) | 本研究提出了一种基于交替方向乘积算法(ADMM)的快速且最优的修剪层权重更新算法，结合简单的迭代修剪掩码选择，在广泛的大型语言模型范围内实现了最先进的修剪性能。 |
| [^3] | [Towards ASR Robust Spoken Language Understanding Through In-Context Learning With Word Confusion Networks.](http://arxiv.org/abs/2401.02921) | 本研究介绍了一种通过利用ASR系统的lattice输出来提升口语语言理解鲁棒性的方法。实验结果表明，我们的方法可以弥合ASR假设和理想上限之间的性能差距，并提升在嘈杂语音转录下的SLU结果。 |
| [^4] | [Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task.](http://arxiv.org/abs/2401.02909) | 该论文介绍了一种精调的大型语言模型Bode，针对葡萄牙语提示的任务，该模型在分类任务中表现出令人满意的结果，并且可供免费用于研究或商业目的。 |
| [^5] | [MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance.](http://arxiv.org/abs/2401.02906) | MLLM-Protektor是一个即插即用的方法，用于保护多模态大型语言模型免受恶意攻击。通过解决图像与文本对齐的挑战，该方法提供了对MLLM的有效保护，防止其产生有害响应。 |
| [^6] | [AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models.](http://arxiv.org/abs/2401.02870) | 提出了一个用于塑造偏好和个性的代理框架（AFSPP），可以探索社交网络和主观意识对基于大语言模型的代理的偏好和个性形成的影响，并成功复制了几个人类个性实验的关键发现。 |
| [^7] | [Pheme: Efficient and Conversational Speech Generation.](http://arxiv.org/abs/2401.02839) | Pheme是一个高效和对话式语音生成模型，能够在实时情况下工作，不仅具备自然的语音生成能力，还能与大型语言模型结合使用。 |
| [^8] | [DocGraphLM: Documental Graph Language Model for Information Extraction.](http://arxiv.org/abs/2401.02823) | 本文介绍了一个名为DocGraphLM的框架，它将预训练的语言模型与图语义相结合，通过联合编码器架构表示文档，并使用一种新颖的链接预测方法重构文档图。实验证明，采用图特征可以在信息抽取和问题回答任务上取得一致的改进，并且在训练过程中加速了收敛过程。 |
| [^9] | [PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering.](http://arxiv.org/abs/2401.02797) | 本文提出了一种针对医学视觉问答的多模态大型语言模型的参数高效微调框架，并在公共数据集上进行了验证。实验证明，该模型在封闭式问题上比GPT-4v模型的准确率提高了26％。 |
| [^10] | [Large Language Models in Plant Biology.](http://arxiv.org/abs/2401.02789) | 大型语言模型在植物生物学中的应用。这些模型可以用于分析植物基因组数据，并提供强大的预测工具。 |
| [^11] | [From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models.](http://arxiv.org/abs/2401.02777) | 这项工作介绍了一种名为RAISE的架构，它将大型语言模型（LLMs）如GPT-4整合到对话代理中，通过引入双组件记忆系统来增强代理在多轮对话中的可控性和适应性。预liminary evaluations表明，RAISE在房地产销售领域具有优势，并具有广泛应用的潜力。 |
| [^12] | [Complex systems approach to natural language.](http://arxiv.org/abs/2401.02772) | 这篇综述从复杂性科学的角度探讨了自然语言的研究方法，包括词频和相关性分析，并发现了与复杂系统生成的信号相似的特征。 |
| [^13] | [Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding.](http://arxiv.org/abs/2401.02749) | 该论文提出了一种无需超参数的近似最小贝叶斯风险（AMBR）解码方法，用于更快地进行文本生成任务。该方法通过使用迭代消除法算法来解决中位数识别问题，以达到加速解码的目的。 |
| [^14] | [MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron Captioning.](http://arxiv.org/abs/2401.02744) | 本文提出了MAMI方法，利用多注意力互信息用于长序列神经元字幕生成，并通过不同类型的注意机制和多个注意力结果的汇聚进一步提升了MILAN方法的性能。 |
| [^15] | [German Text Embedding Clustering Benchmark.](http://arxiv.org/abs/2401.02709) | 本研究提出了一个评估不同领域中聚类德语文本嵌入性能的基准测试，并对预训练模型和聚类算法进行了初步分析。结果显示单语和多语模型表现强劲，缩减嵌入维度可以提高聚类效果。此外，持续预训练德语BERT模型可能对短文本性能有显著改善。所有代码和数据集均公开可用。 |
| [^16] | [Unsupervised hard Negative Augmentation for contrastive learning.](http://arxiv.org/abs/2401.02594) | 本论文提出了一种基于TF-IDF检索模型的无监督硬负样本增强方法UNA，通过替换句子中的术语生成合成的负样本，实验证明UNA方法能提高语义文本相似性任务的整体性能，并且与释义增强相结合可获得额外性能提升。 |
| [^17] | [Memory, Consciousness and Large Language Model.](http://arxiv.org/abs/2401.02509) | 该论文研究了大型语言模型（LLM）与图尔文的记忆理论之间的对应关系，并提出了意识可能是一种基于这种对应关系的新兴能力的猜想。 |
| [^18] | [AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets.](http://arxiv.org/abs/2401.01916) | 通过有针对性和持续的预训练，我们在天文学问题回答中扩展了AstroLLaMA，通过使用紧凑的LLaMA-2模型和专门的天文学语料库，我们实现了在专门主题理解方面的显著改进。我们还通过对特定领域的对话数据集进行微调，发布了带有聊天功能的AstroLLaMA。 |
| [^19] | [Patterns of Persistence and Diffusibility across World's Languages.](http://arxiv.org/abs/2401.01698) | 本研究通过构建大规模图表，探讨了合词和音韵跨语言相似性的语言原因，并支持了一项先前已有的假设。 |
| [^20] | [Can AI Be as Creative as Humans?.](http://arxiv.org/abs/2401.01623) | 本文引入了一个新概念【相对创造力】，通过将焦点转向AI是否能够与人类具备相同的创造能力，实现对创造力的统计量化评估和直接比较。 |
| [^21] | [LLM in a flash: Efficient Large Language Model Inference with Limited Memory.](http://arxiv.org/abs/2312.11514) | 本文提出了一种在有限内存条件下高效运行大型语言模型的方法，通过将模型参数存储在闪存中并按需传输到DRAM的方式来解决内存限制的挑战。该方法通过构建推理成本模型并优化数据传输和读取方式，引入了窗口化和行列绑定两种主要技术。 |
| [^22] | [Retrieval-Augmented Generation for Large Language Models: A Survey.](http://arxiv.org/abs/2312.10997) | 本综述论文调查了基于检索增强的大型语言模型的发展，包括三个主要范式：Naive RAG、Advanced RAG和Modular RAG。RAG通过整合外部数据库的知识，增强模型的准确性和可信度，并实现了持续更新知识和整合领域特定信息的功能。 |
| [^23] | [PromptBench: A Unified Library for Evaluation of Large Language Models.](http://arxiv.org/abs/2312.07910) | PromptBench是一个用于评估大型语言模型的统一库，包括了提示语构建、提示语工程、数据集和模型加载、对抗性提示攻击、动态评估协议和分析工具等组件，旨在促进原创研究和创建新的基准测试、部署下游应用以及设计新的评估协议。 |
| [^24] | [KwaiAgents: Generalized Information-seeking Agent System with Large Language Models.](http://arxiv.org/abs/2312.04889) | 本文介绍了 KwaiAgents，这是一个基于大型语言模型的通用信息搜索智能体系统。该系统能够利用语言模型作为认知核心，理解用户的查询，行为准则并参考外部文档，以提供高质量的知识和信息。 |
| [^25] | [Annotation Sensitivity: Training Data Collection Methods Affect Model Performance.](http://arxiv.org/abs/2311.14212) | 该研究发现训练数据收集方法对注释本身和下游模型性能产生影响。在对仇恨言论和冒犯性语言进行注释收集的实验中，发现注释工具的设计选择会对模型的性能产生明显差异。 |
| [^26] | [FlashDecoding++: Faster Large Language Model Inference on GPUs.](http://arxiv.org/abs/2311.01282) | FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。 |
| [^27] | [Code-Style In-Context Learning for Knowledge-Based Question Answering.](http://arxiv.org/abs/2309.04695) | 本论文提出了一种在上下文中学习编程风格的方法，用于解决基于知识的问答中生成逻辑表达式的格式错误问题。 |
| [^28] | [On the Efficacy of Sampling Adapters.](http://arxiv.org/abs/2307.03749) | 本研究提出了采样适配器，一种用于改善语言生成的技术，通过改变模型的采样分布来生成质量更高的文本。这种转变可以视为精确度和召回率的权衡，从而提高了期望文本的质量。 |
| [^29] | [DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.](http://arxiv.org/abs/2306.11698) | 这项工作提出了对GPT模型进行全面可信度评估，考虑了多个方面的风险，发现了以前未公开的威胁漏洞，例如对毒性输出和个人信息泄漏的易被误导性。 |
| [^30] | [Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations.](http://arxiv.org/abs/2305.07372) | 本文介绍一种交互机制，允许用户直接编辑一步步解释错误SQL以修复SQL错误，实验证明方法提高了31.6％的执行准确性。用户研究表明，该方法帮助用户以更少的时间和更高的信心解决了更多的SQL任务。 |
| [^31] | [mFACE: Multilingual Summarization with Factual Consistency Evaluation.](http://arxiv.org/abs/2212.10622) | 该论文提出了mFACE方法，通过利用事实一致性评估模型改进多语言摘要，采用数据过滤和控制生成两种方法来减少虚构信息，并在XLSum数据集的45种语言中进行了实验验证。 |
| [^32] | [Analyzing Wrap-Up Effects through an Information-Theoretic Lens.](http://arxiv.org/abs/2203.17213) | 本文通过分析封闭效应和信息论量之间的关系，发现先前上下文中的信息分布通常能够预测句子末尾和从句末尾的阅读时间（RT），从而支持了关于封闭效应涉及的几个先前的假设。 |

# 详细

[^1]: DeepSeek LLM：借助长期主义扩展开源语言模型

    DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. (arXiv:2401.02954v1 [cs.CL])

    [http://arxiv.org/abs/2401.02954](http://arxiv.org/abs/2401.02954)

    DeepSeek LLM是一个致力于以长期视野推进开源语言模型发展的项目，通过研究和应用扩展规律，成功创建了DeepSeek Chat模型，该模型在各种基准测试中表现出色，特别是在代码领域。

    

    开源大规模语言模型（LLM）的快速发展令人瞩目。然而，之前文献中描述的扩展规律得出了不同的结论，这对扩展LLM产生了负面影响。我们深入研究了扩展规律，并提出了我们独特的研究发现，以促进两种常用的开源配置（7B和67B）中大规模模型的扩展。在扩展规律的指导下，我们推出了DeepSeek LLM项目，致力于以长期视野推进开源语言模型的发展。为了支持预训练阶段，我们开发了一个数据集，目前包含2万亿个标记，并不断扩大。我们还在DeepSeek LLM基本模型上进行了监督微调（SFT）和直接偏好优化（DPO），从而创建了DeepSeek Chat模型。我们的评估结果表明，DeepSeek LLM 67B在各种基准测试中超过了LLaMA-2 70B，特别是在代码领域。

    The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code
    
[^2]: 快速且最优的修剪大型语言模型的权重更新。

    Fast and Optimal Weight Update for Pruned Large Language Models. (arXiv:2401.02938v1 [cs.CL])

    [http://arxiv.org/abs/2401.02938](http://arxiv.org/abs/2401.02938)

    本研究提出了一种基于交替方向乘积算法(ADMM)的快速且最优的修剪层权重更新算法，结合简单的迭代修剪掩码选择，在广泛的大型语言模型范围内实现了最先进的修剪性能。

    

    修剪大型语言模型(LLMs)是一个具有挑战性的任务，因为它们的规模庞大。主要困难在于修剪后的模型微调，这是为了恢复因删除权重而导致的性能损失。最近的方法要么完全忽略了微调，专注于高效的修剪标准，要么尝试逐层权重更新，保持每个层的行为。然而，即使是逐层权重更新对LLMs来说也可能代价高昂，之前的工作不得不采用各种近似方法。在我们的论文中，我们提出了一种基于交替方向乘积算法(ADMM)的快速且最优的修剪层权重更新算法。结合简单的迭代修剪掩码选择，我们的算法在广泛的LLMs范围内实现了最先进的修剪性能。代码可以在https://github.com/fmfi-compbio/admm-pruning获得。

    Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations.  In our paper, we propose a fast and optimal weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). Coupled with a simple iterative pruning mask selection, our algorithm achieves state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.
    
[^3]: 通过上下文学习与词混淆网实现ASR鲁棒的口语语言理解

    Towards ASR Robust Spoken Language Understanding Through In-Context Learning With Word Confusion Networks. (arXiv:2401.02921v1 [cs.CL])

    [http://arxiv.org/abs/2401.02921](http://arxiv.org/abs/2401.02921)

    本研究介绍了一种通过利用ASR系统的lattice输出来提升口语语言理解鲁棒性的方法。实验结果表明，我们的方法可以弥合ASR假设和理想上限之间的性能差距，并提升在嘈杂语音转录下的SLU结果。

    

    在口语语言理解领域，许多自然语言理解方法已经改进，将大型语言模型（LLM）与转录的语音进行了适配，而不是传统的书面文本。在现实场景中，自动语音识别（ASR）系统生成输出的转录假设，其中的错误会降低后续的SLU任务。我们提出了一种方法，利用ASR系统的lattice输出，而不仅仅依赖于顶部的假设，旨在包含语音模糊性并提升SLU的结果。我们进行了涵盖口语问答和意图分类的上下文学习实验，强调了LLM对嘈杂的语音转录的韧性，借助来自lattice的词混淆网络，弥合了使用顶部ASR假设和理想上限之间的SLU性能差距。此外，我们深入探讨了LLM对不同ASR性能的鲁棒性。

    In the realm of spoken language understanding (SLU), numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM, an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification, underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR perf
    
[^4]: 引入 Bode：针对基于葡萄牙语提示的任务的精调大型语言模型

    Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task. (arXiv:2401.02909v1 [cs.CL])

    [http://arxiv.org/abs/2401.02909](http://arxiv.org/abs/2401.02909)

    该论文介绍了一种精调的大型语言模型Bode，针对葡萄牙语提示的任务，该模型在分类任务中表现出令人满意的结果，并且可供免费用于研究或商业目的。

    

    大型语言模型（LLM）越来越多地为自然语言处理带来了进展。然而，低资源语言，即在各种NLP任务的数据集中缺乏广泛关注或现有数据集不够充分的语言（如葡萄牙语），已经从LLM中获得了几个好处，但程度上不及其他语言。在多语言数据集上训练的LLM通常难以令人满意地回应葡萄牙语提示，例如，在回应中出现代码切换。本文提出了一种针对葡萄牙语提示的精调LLaMA 2模型，命名为Bode，分为7B和13B两个版本。我们使用零样本方法和上下文学习评估了该模型在分类任务中的性能，并将其与其他LLM进行了比较。我们的主要贡献是提供了一个在葡萄牙语中具有令人满意结果的LLM，并且可供研究或商业目的免费使用。

    Large Language Models (LLMs) are increasingly bringing advances to Natural Language Processing. However, low-resource languages, those lacking extensive prominence in datasets for various NLP tasks, or where existing datasets are not as substantial, such as Portuguese, already obtain several benefits from LLMs, but not to the same extent. LLMs trained on multilingual datasets normally struggle to respond to prompts in Portuguese satisfactorily, presenting, for example, code switching in their responses. This work proposes a fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two versions: 7B and 13B. We evaluate the performance of this model in classification tasks using the zero-shot approach with in-context learning, and compare it with other LLMs. Our main contribution is to bring an LLM with satisfactory results in the Portuguese language, as well as to provide a model that is free for research or commercial purposes.
    
[^5]: MLLM-Protektor: 确保MLLM的安全性而不影响性能

    MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance. (arXiv:2401.02906v1 [cs.CR])

    [http://arxiv.org/abs/2401.02906](http://arxiv.org/abs/2401.02906)

    MLLM-Protektor是一个即插即用的方法，用于保护多模态大型语言模型免受恶意攻击。通过解决图像与文本对齐的挑战，该方法提供了对MLLM的有效保护，防止其产生有害响应。

    

    多模态大型语言模型(MLLM)的部署带来了一个独特的脆弱性：对视觉输入的恶意攻击易受攻击。我们探讨了保护MLLM免受此类攻击的新挑战。我们发现，图像作为一种“外语”在对齐过程中没有被考虑到，这可能使MLLM更容易产生有害的响应。不幸的是，与文本中所考虑的离散标记不同，图像信号的连续性质在对齐过程中带来了重大挑战，从而使覆盖可能情景变得困难。这种脆弱性加剧了一个事实，即开源的MLLM主要在有限的图像-文本对上进行了微调，远远小于广泛的文本预训练语料库，这使得MLLM在明确对齐调整过程中更容易遗忘其原始能力。为了应对这些挑战，我们介绍了MLLM-Protektor，一个即插即用的

    The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. We delve into the novel challenge of defending MLLMs against such attacks. We discovered that images act as a "foreign language" that is not considered during alignment, which can make MLLMs prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover the possible scenarios. This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play 
    
[^6]: AFSPP: 利用大语言模型塑造偏好和个性的代理框架

    AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models. (arXiv:2401.02870v1 [cs.MA])

    [http://arxiv.org/abs/2401.02870](http://arxiv.org/abs/2401.02870)

    提出了一个用于塑造偏好和个性的代理框架（AFSPP），可以探索社交网络和主观意识对基于大语言模型的代理的偏好和个性形成的影响，并成功复制了几个人类个性实验的关键发现。

    

    大语言模型的发展引入了一种新的方法来研究人类行为仿真。最近的研究利用基于大语言模型的代理创建了一个社会学研究环境，代理基于大语言模型的未过滤特征展示行为。然而，这些研究忽视了在类似人类环境中的迭代发展 - 人类的偏好和个性是复杂的，受到各种因素的影响，并且因环境和主观影响而不断变化。鉴于这一观察，我们提出了一个用于塑造偏好和个性的代理框架（AFSPP），探索社交网络和主观意识对基于大语言模型的代理的偏好和个性形成的多方面影响。通过AFSPP，我们首次成功复制了几个人类个性实验的关键发现。其他基于AFSPP的实验结果表明，计划制定...

    The evolution of Large Language Models (LLMs) has introduced a new paradigm for investigating human behavior emulation. Recent research has employed LLM-based Agents to create a sociological research environment, in which agents exhibit behavior based on the unfiltered characteristics of large language models. However, these studies overlook the iterative development within a human-like setting - Human preferences and personalities are complex, shaped by various factors and subject to ongoing change as a result of environmental and subjective influences. In light of this observation, we propose Agent Framework for Shaping Preference and Personality (AFSPP), exploring the multifaceted impact of social networks and subjective consciousness on LLM-based Agents' preference and personality formation. With AFSPP, we have, for the first time, successfully replicated several key findings from human personality experiments. And other AFSPP-based experimental results indicate that plan making, s
    
[^7]: Pheme: 高效和对话式语音生成

    Pheme: Efficient and Conversational Speech Generation. (arXiv:2401.02839v1 [eess.AS])

    [http://arxiv.org/abs/2401.02839](http://arxiv.org/abs/2401.02839)

    Pheme是一个高效和对话式语音生成模型，能够在实时情况下工作，不仅具备自然的语音生成能力，还能与大型语言模型结合使用。

    

    最近几年，语音生成取得了显著进展，现在已经实现了一次生成能力，往往几乎无法区分是否为真实的人声。将这样的语音生成进展与大型语言模型结合起来，可能会彻底改变各种应用。然而，某些应用，如辅助对话系统，需要自然和对话式语音生成工具，也需要在实时情况下运行效率高。当前的最先进模型如VALL-E和SoundStorm，由分层神经音频编解码器驱动，需要大型神经组件和大量训练数据才能良好运作。相比之下，MQTTS旨在构建更紧凑的对话式TTS模型，同时利用小规模真实对话式语音数据。然而，其自回归性质导致推理延迟高，从而限制了实时应用。

    In recent years, speech generation has seen remarkable progress, now achieving one-shot generation capability that is often virtually indistinguishable from real human voice. Integrating such advancements in speech generation with large language models might revolutionize a wide range of applications. However, certain applications, such as assistive conversational systems, require natural and conversational speech generation tools that also operate efficiently in real time. Current state-of-the-art models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs, require large neural components and extensive training data to work well. In contrast, MQTTS aims to build more compact conversational TTS models while capitalizing on smaller-scale real-life conversational speech data. However, its autoregressive nature yields high inference latency and thus limits its real-time usage. In order to mitigate the current limitations of the state-of-the-art TTS models while capitali
    
[^8]: DocGraphLM：信息抽取的文档图语言模型

    DocGraphLM: Documental Graph Language Model for Information Extraction. (arXiv:2401.02823v1 [cs.CL])

    [http://arxiv.org/abs/2401.02823](http://arxiv.org/abs/2401.02823)

    本文介绍了一个名为DocGraphLM的框架，它将预训练的语言模型与图语义相结合，通过联合编码器架构表示文档，并使用一种新颖的链接预测方法重构文档图。实验证明，采用图特征可以在信息抽取和问题回答任务上取得一致的改进，并且在训练过程中加速了收敛过程。

    

    在视觉丰富的文档理解(VrDU)方面取得的进展使得可以对具有复杂布局的文档进行信息抽取和问题回答成为可能。出现了两种架构的模式-受LLM启发的基于transformer的模型和图神经网络。在本文中，我们介绍了一种新颖的框架DocGraphLM，它将预训练的语言模型与图语义相结合。为了实现这一目标，我们提出了1)一种联合编码器架构来表示文档，以及2)一种新颖的链接预测方法来重构文档图。DocGraphLM使用一个收敛的联合损失函数来预测节点之间的方向和距离，该损失函数优先考虑邻域恢复并减轻远程节点检测。我们在三个最先进的数据集上的实验证明，采用图特征可以在信息抽取和问题回答任务上保持一致的改进。此外，我们报告说，尽管仅由构建而来，在训练过程中采用图特征可以加快收敛过程。

    Advances in Visually Rich Document Understanding (VrDU) have enabled information extraction and question answering over documents with complex layouts. Two tropes of architectures have emerged -- transformer-based models inspired by LLMs, and Graph Neural Networks. In this paper, we introduce DocGraphLM, a novel framework that combines pre-trained language models with graph semantics. To achieve this, we propose 1) a joint encoder architecture to represent documents, and 2) a novel link prediction approach to reconstruct document graphs. DocGraphLM predicts both directions and distances between nodes using a convergent joint loss function that prioritizes neighborhood restoration and downweighs distant node detection. Our experiments on three SotA datasets show consistent improvement on IE and QA tasks with the adoption of graph features. Moreover, we report that adopting the graph features accelerates convergence in the learning process during training, despite being solely constructe
    
[^9]: PeFoMed：针对医学视觉问答的多模态大型语言模型的参数高效微调

    PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])

    [http://arxiv.org/abs/2401.02797](http://arxiv.org/abs/2401.02797)

    本文提出了一种针对医学视觉问答的多模态大型语言模型的参数高效微调框架，并在公共数据集上进行了验证。实验证明，该模型在封闭式问题上比GPT-4v模型的准确率提高了26％。

    

    多模态大型语言模型（MLLM）在传统大型语言模型的能力上进行了进化扩展，使它们能够应对超越纯文本应用范围的挑战。它利用了先前编码在这些语言模型中的知识，从而增强了它们在多模态环境下的适用性和功能性。最近的研究探讨了将MLLMs适应为生成任务以解决医学视觉问答（Med-VQA）任务的自由形式答案的能力。在本文中，我们提出了一种针对Med-VQA应用特别定制的参数高效微调框架，并在公共基准数据集上进行了实证验证。为了准确衡量性能，我们进行了人工评估，结果显示我们的模型在封闭式问题的整体准确率上达到了81.9％，且其相对于GPT-4v模型的绝对准确率超过了26％。

    Multimodal large language models (MLLMs) represent an evolutionary expansion in the capabilities of traditional large language models, enabling them to tackle challenges that surpass the scope of purely text-based applications. It leverages the knowledge previously encoded within these language models, thereby enhancing their applicability and functionality in the reign of multimodal contexts. Recent works investigate the adaptation of MLLMs to predict free-form answers as a generative task to solve medical visual question answering (Med-VQA) tasks. In this paper, we propose a parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications, and empirically validate it on a public benchmark dataset. To accurately measure the performance, we employ human evaluation and the results reveal that our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions. The cod
    
[^10]: 植物生物学中的大型语言模型

    Large Language Models in Plant Biology. (arXiv:2401.02789v1 [q-bio.GN])

    [http://arxiv.org/abs/2401.02789](http://arxiv.org/abs/2401.02789)

    大型语言模型在植物生物学中的应用。这些模型可以用于分析植物基因组数据，并提供强大的预测工具。

    

    大型语言模型（LLM），如ChatGPT，已经席卷全球，并通过某些形式的图灵测试。然而，LLM不仅局限于人类语言，还可以分析DNA、蛋白质和基因表达等顺序数据。所得到的基础模型可以被重新利用来识别数据中的复杂模式，从而形成强大的多功能预测工具，能够解释细胞系统。本综述概述了不同类型的LLM，并展示了它们在生物学中的最新应用。考虑到LLM尚未在植物学界被接受，我们还介绍了如何将这些模型应用于植物王国。

    Large Language Models (LLMs), such as ChatGPT, have taken the world by storm and have passed certain forms of the Turing test. However, LLMs are not limited to human language and analyze sequential data, such as DNA, protein, and gene expression. The resulting foundation models can be repurposed to identify the complex patterns within the data, resulting in powerful, multi-purpose prediction tools able to explain cellular systems. This review outlines the different types of LLMs and showcases their recent uses in biology. Since LLMs have not yet been embraced by the plant community, we also cover how these models can be deployed for the plant kingdom.
    
[^11]: 从LLM到对话代理：具有大型语言模型微调的记忆增强架构

    From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. (arXiv:2401.02777v1 [cs.CL])

    [http://arxiv.org/abs/2401.02777](http://arxiv.org/abs/2401.02777)

    这项工作介绍了一种名为RAISE的架构，它将大型语言模型（LLMs）如GPT-4整合到对话代理中，通过引入双组件记忆系统来增强代理在多轮对话中的可控性和适应性。预liminary evaluations表明，RAISE在房地产销售领域具有优势，并具有广泛应用的潜力。

    

    本文介绍了RAISE（Scratchpad和Examples辅助推理和行为）,一种先进的架构，增强了将GPT-4等大型语言模型（LLMs）整合到对话代理中的能力。RAISE是ReAct框架的改进版本，包括一个双组件记忆系统，模仿人类的短期记忆和长期记忆，以保持对话的上下文和连续性。它包括了一个全面的代理构建情景，包括对话选择，场景提取，CoT完成和场景增强等阶段，最终导致LLMs的训练阶段。这种方法似乎提高了代理在复杂的多轮对话中的可控性和适应性。我们在房地产销售环境中的初步评估表明，RAISE相对于传统代理有一些优势，表明它在更广泛的应用中具有潜力。通过提供一个强大的框架来开发更具上下文感知和多功能的对话代理，这项工作为AI领域做出了贡献。

    This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile convers
    
[^12]: 自然语言的复杂系统方法

    Complex systems approach to natural language. (arXiv:2401.02772v1 [physics.soc-ph])

    [http://arxiv.org/abs/2401.02772](http://arxiv.org/abs/2401.02772)

    这篇综述从复杂性科学的角度探讨了自然语言的研究方法，包括词频和相关性分析，并发现了与复杂系统生成的信号相似的特征。

    

    这篇综述从复杂性科学的角度总结了研究自然语言所使用的主要方法论概念，并记录了它们在识别语言在书面表达中的通用特征和系统特定特征方面的适用性。涵盖了数量语言学中与复杂性相关的三个主要研究趋势。第一部分讨论了文本中的词频问题，并证明考虑标点符号可以恢复违反Zipf定律的最常见词语的比例。第二部分介绍了受时间序列分析启发的方法，用于研究书面文本中的各种相关性。相关时间序列是基于将文本分割为句子或连续标点之间的短语而生成的。结果表明，这些序列具有复杂系统生成的信号中常见的长程相关性或（多）分形特征。

    The review summarizes the main methodological concepts used in studying natural language from the perspective of complexity science and documents their applicability in identifying both universal and system-specific features of language in its written representation. Three main complexity-related research trends in quantitative linguistics are covered. The first part addresses the issue of word frequencies in texts and demonstrates that taking punctuation into consideration restores scaling whose violation in the Zipf's law is often observed for the most frequent words. The second part introduces methods inspired by time series analysis, used in studying various kinds of correlations in written texts. The related time series are generated on the basis of text partition into sentences or into phrases between consecutive punctuation marks. It turns out that these series develop features often found in signals generated by complex systems, like long-range correlations or (multi)fractal st
    
[^13]: 无需超参数的更快最小贝叶斯风险解码方法

    Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding. (arXiv:2401.02749v1 [cs.AI])

    [http://arxiv.org/abs/2401.02749](http://arxiv.org/abs/2401.02749)

    该论文提出了一种无需超参数的近似最小贝叶斯风险（AMBR）解码方法，用于更快地进行文本生成任务。该方法通过使用迭代消除法算法来解决中位数识别问题，以达到加速解码的目的。

    

    最小贝叶斯风险解码被证明是一种在广泛的文本生成任务中替代束搜索解码的强大方法。然而，MBR需要大量的时间来计算MBR目标，这使得该方法在许多需要响应时间至关重要的情况下不可行。最近提出了基于置信度的剪枝(CBP)方法来降低机器翻译任务中的推理时间。尽管已经证明它能显著减少计算量，但是它需要使用开发集进行超参数调优才能发挥作用。为此，我们提出了一种无需超参数的近似最小贝叶斯风险（AMBR）解码方法。AMBR基于以下观察得出：计算基于样本的MBR目标的问题是中位数识别问题。AMBR使用了迭代消除法（CSH）算法，这是迄今为止最好的近似算法。

    Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to beam search decoding for a wide range of text generation tasks. However, MBR requires a huge amount of time for inference to compute the MBR objective, which makes the method infeasible in many situations where response time is critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently been proposed to reduce the inference time in machine translation tasks. Although it is shown to significantly reduce the amount of computation, it requires hyperparameter tuning using a development set to be effective. To this end, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a hyperparameter-free method to run MBR decoding approximately. AMBR is derived from the observation that the problem of computing the sample-based MBR objective is the medoid identification problem. AMBR uses the Correlated Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best approximation algorithm to date
    
[^14]: MAMI: 多注意力互信息用于长序列神经元字幕生成

    MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron Captioning. (arXiv:2401.02744v1 [cs.AI])

    [http://arxiv.org/abs/2401.02744](http://arxiv.org/abs/2401.02744)

    本文提出了MAMI方法，利用多注意力互信息用于长序列神经元字幕生成，并通过不同类型的注意机制和多个注意力结果的汇聚进一步提升了MILAN方法的性能。

    

    神经元标记是一种可视化特定神经元对激活该神经元的模式的行为和反应的方法。神经元标记提取了深度神经网络中某些神经元捕获的特征信息，其中之一使用了编码器-解码器图像字幕生成方法。编码器可以是预训练的基于CNN的模型，解码器是基于RNN的模型用于文本生成。之前的工作，即MILAN（Mutual Information-guided Linguistic Annotation of Neuron），尝试使用修改后的Show, Attend, and Tell（SAT）模型进行神经元行为的可视化，在编码器中使用LSTM与Bahdanau注意力。MILAN在短序列神经元字幕生成方面表现出色，但在长序列神经元字幕生成方面表现不佳，因此在本文中，我们希望通过利用不同类型的注意力机制并额外添加若干注意力结果来进一步提升MILAN的性能。

    Neuron labeling is an approach to visualize the behaviour and respond of a certain neuron to a certain pattern that activates the neuron. Neuron labeling extract information about the features captured by certain neurons in a deep neural network, one of which uses the encoder-decoder image captioning approach. The encoder used can be a pretrained CNN-based model and the decoder is an RNN-based model for text generation. Previous work, namely MILAN (Mutual Information-guided Linguistic Annotation of Neuron), has tried to visualize the neuron behaviour using modified Show, Attend, and Tell (SAT) model in the encoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show great result on short sequence neuron captioning, but it does not show great result on long sequence neuron captioning, so in this work, we would like to improve the performance of MILAN even more by utilizing different kind of attention mechanism and additionally adding several attention result into one, 
    
[^15]: 德语文本嵌入聚类基准研究

    German Text Embedding Clustering Benchmark. (arXiv:2401.02709v1 [cs.CL])

    [http://arxiv.org/abs/2401.02709](http://arxiv.org/abs/2401.02709)

    本研究提出了一个评估不同领域中聚类德语文本嵌入性能的基准测试，并对预训练模型和聚类算法进行了初步分析。结果显示单语和多语模型表现强劲，缩减嵌入维度可以提高聚类效果。此外，持续预训练德语BERT模型可能对短文本性能有显著改善。所有代码和数据集均公开可用。

    

    本研究介绍了一个基准测试，用于评估在不同领域中聚类德语文本嵌入的性能。这个基准测试是由对聚类神经文本嵌入在需要对文本进行分组（如主题建模）的任务中的增加使用所驱动的，并且在现有基准测试中需要德语资源。我们对一系列预训练的单语和多语模型在不同聚类算法的结果上进行了初步分析。结果包括表现强劲的单语和多语模型。缩减嵌入的维度可以进一步提高聚类效果。此外，我们还进行了与德语BERT模型的持续预训练的实验，以评估这种额外训练的好处。我们的实验表明，对于短文本可能存在显著的性能改进。所有代码和数据集均可公开获取。

    This work introduces a benchmark assessing the performance of clustering German text embeddings in different domains. This benchmark is driven by the increasing use of clustering neural text embeddings in tasks that require the grouping of texts (such as topic modeling) and the need for German resources in existing benchmarks. We provide an initial analysis for a range of pre-trained mono- and multilingual models evaluated on the outcome of different clustering algorithms. Results include strong performing mono- and multilingual models. Reducing the dimensions of embeddings can further improve clustering. Additionally, we conduct experiments with continued pre-training for German BERT models to estimate the benefits of this additional training. Our experiments suggest that significant performance improvements are possible for short text. All code and datasets are publicly available.
    
[^16]: 无监督硬负样本增强的对比学习

    Unsupervised hard Negative Augmentation for contrastive learning. (arXiv:2401.02594v1 [cs.CL])

    [http://arxiv.org/abs/2401.02594](http://arxiv.org/abs/2401.02594)

    本论文提出了一种基于TF-IDF检索模型的无监督硬负样本增强方法UNA，通过替换句子中的术语生成合成的负样本，实验证明UNA方法能提高语义文本相似性任务的整体性能，并且与释义增强相结合可获得额外性能提升。

    

    我们提出了一种无监督的硬负样本增强（UNA）方法，该方法基于词频-逆文档频率（TF-IDF）检索模型生成合成的负样本。UNA使用TF-IDF分数确定句子中术语的感知重要性，并通过替换术语来生成负样本。我们的实验证明，使用UNA训练的模型在语义文本相似性任务上提高了整体性能。当将UNA与释义增强相结合时，还可获得额外的性能提升。进一步的结果表明，我们的方法与不同的骨干模型兼容。消融研究还支持在负样本增强中具有基于TF-IDF的控制选择。

    We present Unsupervised hard Negative Augmentation (UNA), a method that generates synthetic negative instances based on the term frequency-inverse document frequency (TF-IDF) retrieval model. UNA uses TF-IDF scores to ascertain the perceived importance of terms in a sentence and then produces negative samples by replacing terms with respect to that. Our experiments demonstrate that models trained with UNA improve the overall performance in semantic textual similarity tasks. Additional performance gains are obtained when combining UNA with the paraphrasing augmentation. Further results show that our method is compatible with different backbone models. Ablation studies also support the choice of having a TF-IDF-driven control on negative augmentation.
    
[^17]: 记忆、意识和大型语言模型

    Memory, Consciousness and Large Language Model. (arXiv:2401.02509v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.02509](http://arxiv.org/abs/2401.02509)

    该论文研究了大型语言模型（LLM）与图尔文的记忆理论之间的对应关系，并提出了意识可能是一种基于这种对应关系的新兴能力的猜想。

    

    随着认知科学和大型语言模型（LLM）的发展，这两个不同领域之间的联系越来越多地被揭示出来。在这些联系的基础上，我们提出了一个猜想，即LLM和图尔文的记忆理论之间存在一种对偶关系。我们确定了图尔文的协同引发（SEM）检索模型和LLM中观察到的新兴能力之间的潜在对应关系，为我们的猜想提供了支持证据。此外，我们推测意识可能被认为是这种对偶性的一种新兴能力形式。我们还讨论了其他意识理论如何与我们的研究相交叉。

    With the development in cognitive science and Large Language Models (LLMs), increasing connections have come to light between these two distinct fields. Building upon these connections, we propose a conjecture suggesting the existence of a duality between LLMs and Tulving's theory of memory. We identify a potential correspondence between Tulving's synergistic ecphory model (SEM) of retrieval and the emergent abilities observed in LLMs, serving as supporting evidence for our conjecture. Furthermore, we speculate that consciousness may be considered a form of emergent ability based on this duality. We also discuss how other theories of consciousness intersect with our research.
    
[^18]: AstroLLaMA-Chat: 使用对话和多样化数据集扩展AstroLLaMA

    AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets. (arXiv:2401.01916v1 [astro-ph.IM])

    [http://arxiv.org/abs/2401.01916](http://arxiv.org/abs/2401.01916)

    通过有针对性和持续的预训练，我们在天文学问题回答中扩展了AstroLLaMA，通过使用紧凑的LLaMA-2模型和专门的天文学语料库，我们实现了在专门主题理解方面的显著改进。我们还通过对特定领域的对话数据集进行微调，发布了带有聊天功能的AstroLLaMA。

    

    通过有针对性和持续的预训练，我们探索了在天文学问题回答中增强LLM性能的潜力。通过使用一个紧凑的7B参数的LLaMA-2模型，并且专注于一组经过筛选的天文学语料库，包括摘要、介绍和结论，我们在专门主题理解方面取得了显著的改进。虽然像GPT-4这样的通用LLMs在更广泛的问题回答场景中由于更强大的推理能力而表现出色，但我们的发现表明，有限资源的持续预训练仍然可以提高模型在专门主题上的性能。此外，我们提出了AstroLLaMA的扩展：在特定领域的对话数据集上对7B LLaMA模型进行微调，最终发布了适用于社区使用的具有聊天功能的AstroLLaMA。全面的定量基准测试正在进行中，并将在即将发布的完整论文中详细介绍。模型AstroLLaMA-Chat现已在...

    We explore the potential of enhancing LLM performance in astronomy-focused question-answering through targeted, continual pre-training. By employing a compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of astronomy corpus -- comprising abstracts, introductions, and conclusions -- we achieve notable improvements in specialized topic comprehension. While general LLMs like GPT-4 outperform in broader question-answering scenarios due to superior reasoning capabilities, our findings suggest that continual pre-training with limited resources can still enhance model performance on specialized topics. Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B LLaMA model on a domain-specific conversational dataset, culminating in the release of the chat-enabled AstroLLaMA for community use. Comprehensive quantitative benchmarking is currently in progress and will be detailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now available at
    
[^19]: 世界语言的持久性和扩散性模式

    Patterns of Persistence and Diffusibility across World's Languages. (arXiv:2401.01698v1 [cs.CL])

    [http://arxiv.org/abs/2401.01698](http://arxiv.org/abs/2401.01698)

    本研究通过构建大规模图表，探讨了合词和音韵跨语言相似性的语言原因，并支持了一项先前已有的假设。

    

    语言的相似性可能是由于遗传关系、区域接触、普遍性或偶然性。合词是一种未被充分研究的相似性类型，即使用一个词汇形式来表达多个意义。在我们的工作中，通过研究家族稳定性（持久性）和接触诱发变化（扩散性），我们揭示了合词和音韵跨语言相似性的语言原因。我们构建了涵盖1966种语言的大规模图，包括语义、家族、音韵和地理数据。我们通过研究语言学领域以前工作中的几个已建立的假设，并提出新的假设，展示了这一资源的潜力。我们的结果强烈支持语言学文献中一个之前已建立的假设，同时提供了对另一个假设的证据。我们的大规模资源为跨学科研究提供了更多的可能性，例如多语言自然语言处理和比较语言学。

    Language similarities can be caused by genetic relatedness, areal contact, universality, or chance. Colexification, i.e.~a type of similarity where a single lexical form is used to convey multiple meanings, is underexplored. In our work, we shed light on the linguistic causes of cross-lingual similarity in colexification and phonology, by exploring genealogical stability (persistence) and contact-induced change (diffusibility). We construct large-scale graphs incorporating semantic, genealogical, phonological and geographical data for 1,966 languages. We then show the potential of this resource, by investigating several established hypotheses from previous work in linguistics, while proposing new ones. Our results strongly support a previously established hypothesis in the linguistic literature, while offering contradicting evidence to another. Our large scale resource opens for further research across disciplines, e.g.~in multilingual NLP and comparative linguistics.
    
[^20]: AI是否能像人类一样具备创造力？

    Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])

    [http://arxiv.org/abs/2401.01623](http://arxiv.org/abs/2401.01623)

    本文引入了一个新概念【相对创造力】，通过将焦点转向AI是否能够与人类具备相同的创造能力，实现对创造力的统计量化评估和直接比较。

    

    创造力是社会进步和创新的基石，但其评估仍然是一个复杂且主观的任务。随着先进的生成型AI模型的出现，能够完成曾经只属于人类创造力的任务，探索AI的创造潜力变得至关重要，以确保其负责任的发展和应用。本文通过引入一个名为“相对创造力”的新概念来解决定义和评估创造力的复杂性。我们不再试图对创造力进行普遍定义，而是将焦点转向AI是否能够与一位假设的人类具备相同的创造能力。这种观点借鉴了图灵测试的思想，并扩展其范围以解决评估创造力中所固有的挑战和主观性。这种方法的转变使得对AI创造力的统计量化评估成为可能，我们将其称为统计创造力。这种方法允许直接比较AI与特定人类的创造能力。

    Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
    
[^21]: 闪存LLM：在有限内存下高效运行大型语言模型

    LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.11514](http://arxiv.org/abs/2312.11514)

    本文提出了一种在有限内存条件下高效运行大型语言模型的方法，通过将模型参数存储在闪存中并按需传输到DRAM的方式来解决内存限制的挑战。该方法通过构建推理成本模型并优化数据传输和读取方式，引入了窗口化和行列绑定两种主要技术。

    

    大型语言模型（LLM）在现代自然语言处理中起着至关重要的作用，在各种任务中表现出色。然而，它们庞大的计算和内存需求带来了挑战，特别是对于具有有限DRAM容量的设备而言。本文通过将模型参数存储在闪存中，并按需将其传输到DRAM的方式，解决了超过可用DRAM容量的LLM高效运行的挑战。我们的方法涉及构建一个考虑闪存特性的推理成本模型，引导我们在两个关键领域进行优化：减少从闪存传输的数据量，并以较大、更连续的块读取数据。在这个受硬件启发的框架内，我们引入了两个主要技术。首先，“窗口化”通过重复使用之前激活的神经元来策略性地减少数据传输，其次，“行列绑定”适应了闪存的顺序数据访问特点，

    Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, "windowing" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, 
    
[^22]: 基于检索增强的大型语言模型：一项调研

    Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.10997](http://arxiv.org/abs/2312.10997)

    本综述论文调查了基于检索增强的大型语言模型的发展，包括三个主要范式：Naive RAG、Advanced RAG和Modular RAG。RAG通过整合外部数据库的知识，增强模型的准确性和可信度，并实现了持续更新知识和整合领域特定信息的功能。

    

    大型语言模型（LLMs）展示了显著的能力，但面临幻觉、过时知识和非透明、不可追溯的推理过程等挑战。检索增强生成（RAG）已经成为一种有前途的解决方案，通过整合来自外部数据库的知识，增强模型的准确性和可信度，特别适用于知识密集型任务，并允许持续更新知识和整合领域特定信息。RAG将LLMs自身的知识与庞大、动态的外部数据库相结合，实现协同效应。本综述论文详细考察了RAG范式的发展，包括Naive RAG、Advanced RAG和Modular RAG。它详细审视了RAG框架的三个基本要素，包括检索、生成和增强技术。本文强调了嵌入在RAG框架中的最新技术。

    Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in e
    
[^23]: PromptBench：一个用于评估大型语言模型的统一库

    PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.07910](http://arxiv.org/abs/2312.07910)

    PromptBench是一个用于评估大型语言模型的统一库，包括了提示语构建、提示语工程、数据集和模型加载、对抗性提示攻击、动态评估协议和分析工具等组件，旨在促进原创研究和创建新的基准测试、部署下游应用以及设计新的评估协议。

    

    对大型语言模型（LLMs）的评估对于评估其性能和减轻潜在的安全风险至关重要。本文介绍了PromptBench，一个用于评估LLMs的统一库。它由几个关键组件组成，研究人员可以轻松使用和扩展：提示语构建、提示语工程、数据集和模型加载、对抗性提示攻击、动态评估协议和分析工具。PromptBench旨在成为一个开放、通用和灵活的代码库，以促进原创研究，创建新的基准测试、部署下游应用和设计新的评估协议。代码可在https://github.com/microsoft/promptbench上找到，并将持续支持。

    The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.
    
[^24]: KwaiAgents：基于大型语言模型的通用信息搜索智能体系统

    KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.04889](http://arxiv.org/abs/2312.04889)

    本文介绍了 KwaiAgents，这是一个基于大型语言模型的通用信息搜索智能体系统。该系统能够利用语言模型作为认知核心，理解用户的查询，行为准则并参考外部文档，以提供高质量的知识和信息。

    

    人类由于好奇心的驱使，不断探索和理解周围的世界，从而发明了各种工具来满足这种好奇心。尽管人类无法在大脑中处理和记忆大量信息，但在批判思维、规划、反思以及利用现有工具与世界进行交互和解释方面卓越出色，使其能够高效地寻找答案。最近大型语言模型（LLM）的进步表明，机器可能也具备类似于人类的能力，即使参数数量受限，也能展示强大的能力。在本文中，我们介绍了 KwaiAgents，这是一个基于LLM的通用信息搜索智能体系统。在 KwaiAgents 中，我们提出了一种利用LLM作为认知核心的智能体系统，它能够理解用户的查询、行为准则和参考外部文档。智能体还可以更新查询结果，与用户进行互动，并提供高质量的知识和信息。

    Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update an
    
[^25]: 注释敏感性：训练数据收集方法影响模型性能

    Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2311.14212](http://arxiv.org/abs/2311.14212)

    该研究发现训练数据收集方法对注释本身和下游模型性能产生影响。在对仇恨言论和冒犯性语言进行注释收集的实验中，发现注释工具的设计选择会对模型的性能产生明显差异。

    

    当训练数据由人工注释者收集时，注释工具的设计、给予注释者的指示、注释者的特征以及他们之间的互动都可能对训练数据产生影响。这项研究证明了创建注释工具时的设计选择也会影响基于得到的注释训练的模型。我们引入了"注释敏感性"这个术语，用来指代注释数据收集方法对注释本身以及下游模型性能和预测的影响。我们在五种实验条件下对仇恨言论和冒犯性语言进行注释收集，随机将注释者分配到不同条件下。然后，在每个得到的五个数据集上对BERT模型进行微调，并在每个条件的保留部分上评估模型性能。我们发现在以下方面条件之间存在明显差异：1）仇恨言论/冒犯性语言注释的比例，2）模型性能。

    When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. We collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. We then fine-tune BERT models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. We find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model per
    
[^26]: FlashDecoding++: 在GPU上加速大规模语言模型推理的更快算法

    FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])

    [http://arxiv.org/abs/2311.01282](http://arxiv.org/abs/2311.01282)

    FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。

    

    随着大规模语言模型在各个领域的重要性日益增加，加速语言模型推理仍然存在一些挑战未解决：(1) 同步部分softmax更新。softmax操作需要同步更新每个部分softmax结果，导致LLM中注意力计算的开销增加约20%。(2) 未充分利用扁平GEMM计算。在LLM推理中执行GEMM的矩阵形状是扁平的，导致在先前的设计中填充零后计算未充分利用，性能损失超过50%。(3) 静态数据流导致的性能损失。LLM中的内核性能取决于不同的输入数据特征、硬件配置等。单一和静态的数据流可能导致LLM推理中不同形状的GEMM的性能损失达到50.25%。我们提出了FlashDecoding++，一种快速支持主流LLM和硬件后端的LLM推理引擎。为了解决上述挑战，FlashDecoding++实现了以下目标：

    As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and >50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
    
[^27]: 在上下文中学习编程风格以解决基于知识的问答中的问题

    Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v1 [cs.CL])

    [http://arxiv.org/abs/2309.04695](http://arxiv.org/abs/2309.04695)

    本论文提出了一种在上下文中学习编程风格的方法，用于解决基于知识的问答中生成逻辑表达式的格式错误问题。

    

    目前，针对基于知识的问答(KBQA)的方法通常依赖复杂的训练技术和模型框架，导致在实际应用中存在许多限制。最近，大型语言模型(LLMs)中的上下文学习(ICL)能力的出现为KBQA提供了一种简单且无需训练的语义解析范式：给定少量问题及其标记的逻辑表达式作为演示示例，LLMs能够理解任务意图并为新问题生成逻辑表达式。然而，当前强大的LLMs在预训练过程中对逻辑表达式的了解很少，导致格式错误率较高。为了解决这个问题，我们提出了一种针对KBQA的代码风格上下文学习方法，将陌生逻辑表达式的生成过程转换为更为熟悉的代码生成过程。对三个主流数据集的实验结果表明，我们的方法显著减轻了生成逻辑表达式中的格式错误问题。

    Current methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic for
    
[^28]: 采样适配器的效能研究

    On the Efficacy of Sampling Adapters. (arXiv:2307.03749v1 [cs.CL])

    [http://arxiv.org/abs/2307.03749](http://arxiv.org/abs/2307.03749)

    本研究提出了采样适配器，一种用于改善语言生成的技术，通过改变模型的采样分布来生成质量更高的文本。这种转变可以视为精确度和召回率的权衡，从而提高了期望文本的质量。

    

    采样是从概率模型生成文本的常见策略，但标准的祖先采样往往导致文本不连贯或不符合语法。为了缓解这个问题，提出了各种修改模型采样分布的技术，如核心或top-k采样，并广泛应用于语言生成系统中。我们提出了一个统一的框架来理解这些技术，称之为采样适配器。采样适配器通常可以生成质量更高的文本，这引发了一个问题：从形式的角度来看，它们是如何改变语言生成模型的（子）词级分布的？为什么这些局部改变会导致更高质量的文本？我们认为，它们所强制执行的转变可以被视为精确度和召回率之间的权衡：虽然模型失去了产生某些字符串的能力，但对于期望的文本，其精确率提高了。尽管这种权衡在标准的距离度量中没有反映出来，但它确实对生成的文本质量起到了重要作用。

    Sampling is a common strategy for generating text from probabilistic models, yet standard ancestral sampling often results in text that is incoherent or ungrammatical. To alleviate this issue, various modifications to a model's sampling distribution, such as nucleus or top-k sampling, have been introduced and are now ubiquitously used in language generation systems. We propose a unified framework for understanding these techniques, which we term sampling adapters. Sampling adapters often lead to qualitatively better text, which raises the question: From a formal perspective, how are they changing the (sub)word-level distributions of language generation models? And why do these local changes lead to higher-quality text? We argue that the shift they enforce can be viewed as a trade-off between precision and recall: while the model loses its ability to produce certain strings, its precision rate on desirable text increases. While this trade-off is not reflected in standard metrics of dist
    
[^29]: DecodingTrust: GPT模型的全面可信度评估

    DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11698](http://arxiv.org/abs/2306.11698)

    这项工作提出了对GPT模型进行全面可信度评估，考虑了多个方面的风险，发现了以前未公开的威胁漏洞，例如对毒性输出和个人信息泄漏的易被误导性。

    

    生成预训练变压器（GPT）模型在其能力方面取得了令人兴奋的进展，引起了从从业者到公众的兴趣。然而，尽管关于GPT模型的可信度的文献仍然有限，从业者们提议将强大的GPT模型用于敏感应用，如医疗保健和金融领域，其中错误可能代价高昂。为此，本研究提出了对大型语言模型（重点放在GPT-4和GPT-3.5上）进行全面的可信度评估，考虑了多样的观点 - 包括有毒性、陈规偏见、对抗强度、超出分布的强度、对抗示范的强度、隐私、机器伦理和公平性。根据我们的评估，我们发现了以前未公开的可信度威胁漏洞。例如，我们发现GPT模型可以轻松被误导生成有毒和偏见的输出，并在训练数据和上下文中泄漏私人信息。

    Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance -- where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -- including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conv
    
[^30]: 通过可编辑的逐步解释实现交互式文本转SQL

    Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations. (arXiv:2305.07372v1 [cs.DB])

    [http://arxiv.org/abs/2305.07372](http://arxiv.org/abs/2305.07372)

    本文介绍一种交互机制，允许用户直接编辑一步步解释错误SQL以修复SQL错误，实验证明方法提高了31.6％的执行准确性。用户研究表明，该方法帮助用户以更少的时间和更高的信心解决了更多的SQL任务。

    

    关系数据库在大数据时代扮演着重要角色。然而，非专家很难完全释放关系数据库的分析能力，因为他们不熟悉SQL等数据库语言。许多技术已被提出自然语言自动生成SQL，但它们存在以下两个问题：（1）对于复杂查询它们仍会犯很多错误，（2）它们不提供一种灵活的方式，让非专家用户验证和改进不正确的查询。为了解决这些问题，我们引入了一种新的交互机制，允许用户直接编辑一步步解释错误SQL以修复SQL错误。在Spider基准测试上的实验证明，我们的方法至少比三种最先进方法在执行准确性方面提高了31.6％。24名参与者的用户研究进一步表明，我们的方法帮助用户以更少的时间和更高的信心解决了更多的SQL任务。

    Relational databases play an important role in this Big Data era. However, it is challenging for non-experts to fully unleash the analytical power of relational databases, since they are not familiar with database languages such as SQL. Many techniques have been proposed to automatically generate SQL from natural language, but they suffer from two issues: (1) they still make many mistakes, particularly for complex queries, and (2) they do not provide a flexible way for non-expert users to validate and refine the incorrect queries. To address these issues, we introduce a new interaction mechanism that allows users directly edit a step-by-step explanation of an incorrect SQL to fix SQL errors. Experiments on the Spider benchmark show that our approach outperforms three SOTA approaches by at least 31.6% in terms of execution accuracy. A user study with 24 participants further shows that our approach helped users solve significantly more SQL tasks with less time and higher confidence, demo
    
[^31]: mFACE:具有事实一致性评估的多语言摘要

    mFACE: Multilingual Summarization with Factual Consistency Evaluation. (arXiv:2212.10622v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10622](http://arxiv.org/abs/2212.10622)

    该论文提出了mFACE方法，通过利用事实一致性评估模型改进多语言摘要，采用数据过滤和控制生成两种方法来减少虚构信息，并在XLSum数据集的45种语言中进行了实验验证。

    

    近年来，由于预训练语言模型和大规模数据集的可用性，抽象摘要生成再次受到关注。尽管取得了有希望的结果，但当前模型仍然存在生成事实不一致的摘要的问题，降低了它们在实际应用中的效用。最近的一些努力尝试通过设计能够自动检测机器生成摘要中的事实不一致性的模型来解决这个问题。然而，他们仅专注于资源丰富的英语。在这项工作中，我们利用事实一致性评估模型来改进多语言摘要。我们探索了两种直观的方法来减少虚构信息，分别基于多语言NLI模型提供的信号，即数据过滤和控制生成。在XLSum数据集的45种语言中进行的实验结果显示，与强基线相比，在自动和人工评估中都获得了提升。

    Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on English, a language with abundant resources. In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate hallucinations based on the signal provided by a multilingual NLI model, namely data filtering and controlled generation. Experimental results in the 45 languages from the XLSum dataset show gains over strong baselines in both automatic and human evaluation.
    
[^32]: 通过信息论的角度分析封闭效应

    Analyzing Wrap-Up Effects through an Information-Theoretic Lens. (arXiv:2203.17213v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.17213](http://arxiv.org/abs/2203.17213)

    本文通过分析封闭效应和信息论量之间的关系，发现先前上下文中的信息分布通常能够预测句子末尾和从句末尾的阅读时间（RT），从而支持了关于封闭效应涉及的几个先前的假设。

    

    已经实施了许多阅读时间（RT）数据的分析，旨在更好地理解驱动阅读理解的认知过程。然而，由于所谓的“封闭效应”引入的混淆因素，通常会忽略句子末尾甚至从句末尾的单词的数据，这表现为这些单词的RT分布不均衡。因此，对参与这些封闭效应可能涉及的认知过程的理解是有限的。在这项工作中，我们试图通过研究封闭效应与词语和上下文惊讶值等信息论量之间的关系，进一步了解这些过程。我们发现先前上下文中的信息分布通常能够预测句子末尾和从句末尾的RT（而不能预测句中RT），这支持了关于封闭效应涉及的几个先前的假设。

    Numerous analyses of reading time (RT) data have been implemented -- all in an effort to better understand the cognitive processes driving reading comprehension. However, data measured on words at the end of a sentence -- or even at the end of a clause -- is often omitted due to the confounding factors introduced by so-called "wrap-up effects," which manifests as a skewed distribution of RTs for these words. Consequently, the understanding of the cognitive processes that might be involved in these wrap-up effects is limited. In this work, we attempt to learn more about these processes by examining the relationship between wrap-up effects and information-theoretic quantities, such as word and context surprisals. We find that the distribution of information in prior contexts is often predictive of sentence- and clause-final RTs (while not of sentence-medial RTs). This lends support to several prior hypotheses about the processes involved in wrap-up effects.
    

