# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Rewarding Chatbots for Real-World Engagement with Millions of Users.](http://arxiv.org/abs/2303.06135) | 本研究提出了一种奖励系统来训练优秀的聊天机器人，利用用户反馈数据去筛选输出来提高保留率，A/B测试表明该方法能提高68%的保留率。 |
| [^2] | [Susceptibility to Influence of Large Language Models.](http://arxiv.org/abs/2303.06074) | 两个研究发现，大型语言模型可以用于建模接受有影响力信息后的心理变化。 这些研究还发现，早期接触语句可能会增强后来的真实性测试评分，而且使用真相以外的属性和使用相同属性进行暴露和测试时表现出缺乏效应 |
| [^3] | [wav2vec and its current potential to Automatic Speech Recognition in German for the usage in Digital History: A comparative assessment of available ASR-technologies for the use in cultural heritage contexts.](http://arxiv.org/abs/2303.06026) | 本文研究了使用wav2vec2的ASR模型在德语文化遗产索引中的表现，并与商业云和专有服务进行了比较。目前可以实现90％以上的识别率，但这些数字很快就会降低，一旦录音具有受限的音频质量或使用非日常或过时的语言。 |
| [^4] | [Is In-hospital Meta-information Useful for Abstractive Discharge Summary Generation?.](http://arxiv.org/abs/2303.06002) | 本论文研究了电子健康记录系统中住院元信息对于出院摘要生成任务的有效性。研究表明，将编码元信息的模型与基本模型相比，可以显著提高摘要生成的ROUGE-1和BERTScore评分。 |
| [^5] | [Creation and evaluation of timelines for longitudinal user posts.](http://arxiv.org/abs/2303.05891) | 本文提出了一种方法将用户帖子分段到有意义的时间轴中，以提高手动注释的质量和成本，并提出了一个新颖的框架来评估时间轴。 |
| [^6] | [An Overview on Language Models: Recent Developments and Outlook.](http://arxiv.org/abs/2303.05759) | 本论文从五个方面介绍了传统语言模型和预训练语言模型，讨论了两者之间的关系，并展望了预训练时代语言建模的未来方向。 |
| [^7] | [MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling.](http://arxiv.org/abs/2303.05707) | 本文提出了一个高精度且内存高效的视频和语言理解模型MuLTI，它可以处理较长的序列，并且内存占用较少。MuLTI通过特征采样和注意力模块实现高效且有效的特征融合，引入了基于注意力的适配器和新的预训练任务来提高模型的性能。 |
| [^8] | [Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning.](http://arxiv.org/abs/2303.05670) | 本文研究了预训练的句子编码器中存在的带有陈规陋习的刻板印象，并比较了与之对应的文本蕴涵模型。研究发现，采用文本蕴涵的明确逻辑学习可以显著减少偏差，并提高社交社区的识别能力，而无需明确去偏见的过程。 |
| [^9] | [Open World Classification with Adaptive Negative Samples.](http://arxiv.org/abs/2303.05581) | 本文提出了一种基于自适应负样本的方法，旨在在培训阶段生成有效的合成开放类别样本，用于解决开放世界分类任务中的挑战。实证结果表明，使用辅助的一对多二进制分类器具有显着优势。 |
| [^10] | [CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network.](http://arxiv.org/abs/2303.03387) | 本文提出了一种名为CoSyn的机制，可以检测在线对话中的隐含仇恨言论。该机制首先对用户的上下文进行建模，然后使用新颖的上下文交互机制联合建模用户和对话上下文，证明了其有效性。 |
| [^11] | [Fillers in Spoken Language Understanding: Computational and Psycholinguistic Perspectives.](http://arxiv.org/abs/2301.10761) | 本文旨在调查在口语理解中的占位符（Fillers）的心理语言学和计算角度的研究。本文将这些视角以易于理解的方式呈现给口语理解和对话AI社区，并讨论每个领域的趋势和挑战。 |
| [^12] | [GPT-3-driven pedagogical agents for training children's curious question-asking skills.](http://arxiv.org/abs/2211.14228) | 本研究探索了使用大型语言模型自动化生产教育内容的效率，并使用自然语言来解释任务，以培养儿童的好奇心提问能力，结果表明使用该内容确实有效。 |
| [^13] | [BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets.](http://arxiv.org/abs/2211.09733) | 本文提出了一种基于BERT-Deep CNN的新冠肺炎推文情感分析的最新研究方法，旨在研究社交媒体上人们对疫情的情感，对于保障社会免受疫情影响具有重要意义。 |
| [^14] | [Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition.](http://arxiv.org/abs/2211.08233) | 本文提出了一种新的时间情感建模方法，可以从各种时间尺度学习多尺度的情感上下文表示，从而更好地适应情感变化。 |
| [^15] | [Large Language Models Are Human-Level Prompt Engineers.](http://arxiv.org/abs/2211.01910) | 本研究提出自动提示工程师(APE)来自动生成和选择自然语言指令，优化大型语言模型（LLMs）任务表现，在24个NLP任务的实验中，自动生成的说明明显优于先前的LLM基线，并实现了更好或可比较的性能 |
| [^16] | [Self-Adaptive Named Entity Recognition by Retrieving Unstructured Knowledge.](http://arxiv.org/abs/2210.07523) | 提出了一种通过检索非结构化文本来学习未被充分学习的实体的用法的自适应命名实体识别方法，在CrossNER数据集上的实验表明，比强基线表现高2.35个百分点 |
| [^17] | [ReAct: Synergizing Reasoning and Acting in Language Models.](http://arxiv.org/abs/2210.03629) | 本文探讨了使用LLMs以交替方式生成推理跟踪和任务特定的行动的方法，允许两者之间的更大协同作用。我们将我们的方法命名为ReAct，并将其应用于各种语言和决策任务，并展示其优于最先进的基线方法的效果，以及优于没有推理或行动组件的方法的人类可解释性和可信度提高。 |
| [^18] | [A Kind Introduction to Lexical and Grammatical Aspect, with a Survey of Computational Approaches.](http://arxiv.org/abs/2208.09012) | 本文介绍了模拟词汇和语法体向的计算方法以及必要的语言概念和术语的直观解释。特别地，我们描述了状态、目的性、习惯性、完成式和未完成式的概念，以及事件和情况类型的重要清单。 |
| [^19] | [Arabic aspect sentiment polarity classification using BERT.](http://arxiv.org/abs/2107.13290) | 本文使用BERT进行阿拉伯语态度情感极性分类，在三个不同的阿拉伯语数据集上实验，结果表明该BERT架构与简单的线性分类层超过了现有的最新工作，取得了89.51%的准确率。 |

# 详细

[^1]: 用于百万用户真实世界互动的聊天机器人的奖励系统

    Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v1 [cs.CL])

    [http://arxiv.org/abs/2303.06135](http://arxiv.org/abs/2303.06135)

    预训练的大型语言模型的出现导致了一系列的社交聊天机器人的部署，用于闲聊。虽然这些聊天机器人展示了语言能力和流利度，但它们并不一定引人入胜，有时候很难吸引用户。本研究调查了开发注重用户参与度的社交聊天机器人以增强保留率，特别是考察了使用人类反馈来高效开发极具吸引力的聊天机器人。所提出的方法利用从用户交互中收集的自动伪标签来训练一个奖励模型，在推理时可以用来拒绝聊天机器人模型产生的低分样本响应。引入了直观的评估指标，如平均对话长度 (MCL)，作为衡量部署聊天机器人的参与水平的代理。在Chai Research平台上，对每日新的10,000个聊天机器人用户组进行的A/B测试表明，这种方法将MCL提高了最多70％，这相当于将保留率从40％增加到68％。

    The emergence of pretrained large language models has led to the deployment of a range of social chatbots for chitchat. Although these chatbots demonstrate language ability and fluency, they are not guaranteed to be engaging and can struggle to retain users. This work investigates the development of social chatbots that prioritize user engagement to enhance retention, specifically examining the use of human feedback to efficiently develop highly engaging chatbots. The proposed approach uses automatic pseudo-labels collected from user interactions to train a reward model that can be used to reject low-scoring sample responses generated by the chatbot model at inference time. Intuitive evaluation metrics, such as mean conversation length (MCL), are introduced as proxies to measure the level of engagement of deployed chatbots. A/B testing on groups of 10,000 new daily chatbot users on the Chai Research platform shows that this approach increases the MCL by up to 70%, which translates to a
    
[^2]: 大型语言模型的影响易感性

    Susceptibility to Influence of Large Language Models. (arXiv:2303.06074v1 [cs.CL])

    [http://arxiv.org/abs/2303.06074](http://arxiv.org/abs/2303.06074)

    两个研究测试了一个假设：可以使用大型语言模型（LLM）来建模接受有影响力信息后的心理变化。第一项研究测试了一种通用的影响模式-虚假的真相效应（ITE），其中早期接触语句（例如通过评价其兴趣）会增强后来的真实性测试评分。使用在线实验从1000名人类参与者和1000个模拟参与者收集了数据，并使用工程提示和LLM完成了收集。每个参与者收集了64个评分，使用以下所有暴露-测试属性的组合：真相，兴趣，情感和重要性。人类参与者的结果重新确认了ITE，并在真相以外的属性和使用相同属性进行暴露和测试时表现出缺乏效应。LLM模拟参与者的结果也发现了相同的效应模式。第二项研究涉及一种特定的影响模式-通俗的新闻框架来激发

    Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to inc
    
[^3]: wav2vec及其在德语自动语音识别中的潜力：文化遗产场景下可用ASR技术的比较评估

    wav2vec and its current potential to Automatic Speech Recognition in German for the usage in Digital History: A comparative assessment of available ASR-technologies for the use in cultural heritage contexts. (arXiv:2303.06026v1 [eess.AS])

    [http://arxiv.org/abs/2303.06026](http://arxiv.org/abs/2303.06026)

    在本案例研究中，我们训练并发布了一个用于德语的最先进的开源自动语音识别（ASR）模型，以评估该技术在数字人文和文化遗产索引化的更大背景下的当前潜力。除了本文，我们还发布了基于wav2vec2的语音转文字模型，同时评估了我们的模型在与商用云和专有服务进行比较的历史录音语料库上的表现。虽然我们的模型取得了中等的结果，但我们发现专有云服务表现更好。正如我们的结果所显示的，目前可以实现90％以上的识别率，但这些数字很快就会降低，一旦录音具有受限的音频质量或使用非日常或过时的语言。德语中不同方言和口音的种类繁多是一个大问题。然而，本文强调了目前可用的识别质量是

    In this case study we trained and published a state-of-the-art open-source model for Automatic Speech Recognition (ASR) for German to evaluate the current potential of this technology for the use in the larger context of Digital Humanities and cultural heritage indexation. Along with this paper we publish our wav2vec2 based speech to text model while we evaluate its performance on a corpus of historical recordings we assembled compared against commercial cloud-based and proprietary services. While our model achieves moderate results, we see that proprietary cloud services fare significantly better. As our results show, recognition rates over 90 percent can currently be achieved, however, these numbers drop quickly once the recordings feature limited audio quality or use of non-every day or outworn language. A big issue is the high variety of different dialects and accents in the German language. Nevertheless, this paper highlights that the currently available quality of recognition is 
    
[^4]: 住院元信息对抽象出院摘要生成是否有用？

    Is In-hospital Meta-information Useful for Abstractive Discharge Summary Generation?. (arXiv:2303.06002v1 [cs.CL])

    [http://arxiv.org/abs/2303.06002](http://arxiv.org/abs/2303.06002)

    在病人住院期间，医生必须记录病人的日常观察结果，并在病人出院时将它们总结为一份简短的文档，称为“出院摘要”。自动化生成出院摘要可以大大减轻医生的负担，并最近得到研究界的关注。大多数以序列到序列架构为基础的出院摘要生成研究只关注住院记录作为输入。然而，电子健康记录（EHR）还具有丰富的结构化元数据（例如，医院、医生、疾病、住院时间等），可能会有所帮助。本文研究了医疗元信息对总结任务的有效性。我们从EHR系统获取了四种类型的元信息，并将每个元信息编码到序列到序列模型中。使用日本EHR，编码元信息的模型将ROUGE-1提高了最多4.45个点，BERTScore提高了3.77个点，超过了基本模型。

    During the patient's hospitalization, the physician must record daily observations of the patient and summarize them into a brief document called "discharge summary" when the patient is discharged. Automated generation of discharge summary can greatly relieve the physicians' burden, and has been addressed recently in the research community. Most previous studies of discharge summary generation using the sequence-to-sequence architecture focus on only inpatient notes for input. However, electric health records (EHR) also have rich structured metadata (e.g., hospital, physician, disease, length of stay, etc.) that might be useful. This paper investigates the effectiveness of medical meta-information for summarization tasks. We obtain four types of meta-information from the EHR systems and encode each meta-information into a sequence-to-sequence model. Using Japanese EHRs, meta-information encoded models increased ROUGE-1 by up to 4.45 points and BERTScore by 3.77 points over the vanilla 
    
[^5]: 纵向用户帖子时间轴的创建和评估

    Creation and evaluation of timelines for longitudinal user posts. (arXiv:2303.05891v1 [cs.CL])

    [http://arxiv.org/abs/2303.05891](http://arxiv.org/abs/2303.05891)

    越来越多的人对社交媒体中的用户生成内容感兴趣，特别是随时间推移的文本帖子。目前还没有一种一致的方式将用户帖子分段到有意义的时间轴中，以提高手动注释的质量和成本。在这里，我们提出了一组方法，用于将纵向用户帖子分段成可能包含用户行为变化有趣时刻的时间轴，基于他们的在线发布活动。我们还提出了一个新颖的框架来评估时间轴，并展示了其在两个不同社交媒体数据集背景下的适用性。最后，我们对排名靠前的时间轴的语言内容进行了讨论。

    There is increasing interest to work with user generated content in social media, especially textual posts over time. Currently there is no consistent way of segmenting user posts into timelines in a meaningful way that improves the quality and cost of manual annotation. Here we propose a set of methods for segmenting longitudinal user posts into timelines likely to contain interesting moments of change in a user's behaviour, based on their online posting activity. We also propose a novel framework for evaluating timelines and show its applicability in the context of two different social media datasets. Finally, we present a discussion of the linguistic content of highly ranked timelines.
    
[^6]: 语言模型综述：最新发展和展望

    An Overview on Language Models: Recent Developments and Outlook. (arXiv:2303.05759v1 [cs.CL])

    [http://arxiv.org/abs/2303.05759](http://arxiv.org/abs/2303.05759)

    语言建模研究文本串上的概率分布。这是自然语言处理中最基本的任务之一，已被广泛用于文本生成、语音识别、机器翻译等。传统语言模型（CLM）旨在以因果方式预测语言序列的概率。相比之下，预训练语言模型（PLMs）涵盖更广泛的概念，并可用于因果序列建模和下游应用的微调。PLMs具有自己的训练范式（通常是自我监督）并在现代NLP系统中充当基础模型。本综述论文从语言单元、结构、训练方法、评估方法和应用五个方面介绍了CLMs和PLMs，讨论了CLMs和PLMs之间的关系，并展望了预训练时代语言建模的未来方向。

    Language modeling studies the probability distributions over strings of texts. It is one of the most fundamental tasks in natural language processing (NLP). It has been widely used in text generation, speech recognition, machine translation, etc. Conventional language models (CLMs) aim to predict the probability of linguistic sequences in a causal manner. In contrast, pre-trained language models (PLMs) cover broader concepts and can be used in both causal sequential modeling and fine-tuning for downstream applications. PLMs have their own training paradigms (usually self-supervised) and serve as foundation models in modern NLP systems. This overview paper provides an introduction to both CLMs and PLMs from five aspects, i.e., linguistic units, structures, training methods, evaluation methods, and applications. Furthermore, we discuss the relationship between CLMs and PLMs and shed light on the future directions of language modeling in the pre-trained era.
    
[^7]: MuLTI: 使用多路径取样器和多项选择模型实现高效视频和语言理解

    MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling. (arXiv:2303.05707v1 [cs.CV])

    [http://arxiv.org/abs/2303.05707](http://arxiv.org/abs/2303.05707)

    视频与语言的理解在工业中具有各种应用，例如视频问答、文本-视频检索和多标签分类。现有的视频与语言理解方法通常采用重量级的多模态编码器和特征融合模块，它们消耗大量的GPU内存。特别是在工业应用中，处理密集的视频帧或长文本很困难。本文提出了一个高精度且内存高效的视频和语言理解模型MuLTI，通过特征采样和注意力模块实现了高效且有效的特征融合。因此，MuLTI可以处理较长的序列，并且内存占用较少。然后，我们引入了基于注意力的适配器到编码器中，通过微调浅层特征来提高模型的性能，同时消耗较少的GPU内存。最后，为了进一步提高模型的性能，我们引入了一项新的预训练任务

    Video-and-language understanding has a variety of applications in the industry, such as video question answering, text-video retrieval and multi-label classification. Existing video-and-language understanding methods generally adopt heavy multi-modal encoders and feature fusion modules, which consume large amounts of GPU memory. Especially, they have difficulty dealing with dense video frames or long text that are prevalent in industrial applications. In this paper, we propose MuLTI, a highly accurate and memory-efficient video-and-language understanding model that achieves efficient and effective feature fusion through feature sampling and attention modules. Therefore, MuLTI can handle longer sequences with limited GPU memory. Then, we introduce an attention-based adapter to the encoders, which finetunes the shallow features to improve the model's performance with low GPU memory consumption. Finally, to further improve the model's performance, we introduce a new pretraining task named
    
[^8]: 逻辑打破偏见：文本蕴涵缓解了带有陈规陋习的句子推理。

    Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning. (arXiv:2303.05670v1 [cs.CL])

    [http://arxiv.org/abs/2303.05670](http://arxiv.org/abs/2303.05670)

    由于预训练句子编码器的相似度学习目标，它们经常内化反映其训练语料库中存在的社会偏见的陈规陋习。本文描述了存在于流行句子表示模型中的几种关于不同社区的刻板印象。我们将这样的模型与学习各种下游语言理解任务的文本蕴涵模型进行比较。通过比较基于文本相似度的强预训练模型和文本蕴涵学习，我们得出结论：文本蕴涵的明确逻辑学习可以显著减少偏差，并提高社交社区的识别能力，而无需明确去偏见的过程。

    Due to their similarity-based learning objectives, pretrained sentence encoders often internalize stereotypical assumptions that reflect the social biases that exist within their training corpora. In this paper, we describe several kinds of stereotypes concerning different communities that are present in popular sentence representation models, including pretrained next sentence prediction and contrastive sentence representation models. We compare such models to textual entailment models that learn language logic for a variety of downstream language understanding tasks. By comparing strong pretrained models based on text similarity with textual entailment learning, we conclude that the explicit logic learning with textual entailment can significantly reduce bias and improve the recognition of social communities, without an explicit de-biasing process
    
[^9]: 自适应负样本的开放世界分类

    Open World Classification with Adaptive Negative Samples. (arXiv:2303.05581v1 [cs.CL])

    [http://arxiv.org/abs/2303.05581](http://arxiv.org/abs/2303.05581)

    开放世界分类是自然语言处理中具有关键实际意义和影响的任务。由于开放或未知类别数据仅在推断阶段显示，因此寻找具有适当决策边界以容纳已知类别的识别和开放类别的区分的模型具有挑战性。现有模型的性能受到训练阶段缺乏有效的开放类别数据或缺乏学习适当决策边界的良好机制的限制。我们提出了一种基于自适应负样本（ANS）的方法，旨在在培训阶段生成有效的合成开放类别样本，而不需要任何先前的知识或外部数据集。实证结果表明，使用辅助的一对多二进制分类器具有显着优势，这些分类器有效地利用了生成的负样本，并避免了复杂的阈值搜索阶段。

    Open world classification is a task in natural language processing with key practical relevance and impact. Since the open or {\em unknown} category data only manifests in the inference phase, finding a model with a suitable decision boundary accommodating for the identification of known classes and discrimination of the open category is challenging. The performance of existing models is limited by the lack of effective open category data during the training stage or the lack of a good mechanism to learn appropriate decision boundaries. We propose an approach based on \underline{a}daptive \underline{n}egative \underline{s}amples (ANS) designed to generate effective synthetic open category samples in the training stage and without requiring any prior knowledge or external datasets. Empirically, we find a significant advantage in using auxiliary one-versus-rest binary classifiers, which effectively utilize the generated negative samples and avoid the complex threshold-seeking stage in pr
    
[^10]: CoSyn：使用上下文协同作用的双曲网络检测在线对话中的隐含仇恨言论

    CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network. (arXiv:2303.03387v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03387](http://arxiv.org/abs/2303.03387)

    社交媒体用户进行在线对话的巨大增长也导致了仇恨言论的显著增长。大多数先前的工作集中于检测明确的仇恨言论，这是公开且利用令人讨厌的短语，但很少有关注检测通过间接或编码语言表示仇恨的隐含仇恨言论。在本文中，我们提出了CoSyn，这是一个用户和对话上下文协同作用的网络，用于检测在线对话树中的隐含仇恨言论。CoSyn首先使用新颖的双曲傅里叶注意机制和双曲图卷积网络对用户的个人历史和社交上下文进行建模。接下来，我们使用新的上下文交互机制在双曲空间中联合建模用户的个人上下文和对话上下文，该机制清晰地捕捉两者之间的相互作用，并对从两个上下文中检索的信息量进行独立评估。CoSyn表现出令人满意的性能，证明了我们模型的有效性。

    The tremendous growth of social media users interacting in online conversations has also led to significant growth in hate speech. Most of the prior works focus on detecting explicit hate speech, which is overt and leverages hateful phrases, with very little work focusing on detecting hate speech that is implicit or denotes hatred through indirect or coded language. In this paper, we present CoSyn, a user- and conversational-context synergized network for detecting implicit hate speech in online conversation trees. CoSyn first models the user's personal historical and social context using a novel hyperbolic Fourier attention mechanism and hyperbolic graph convolution network. Next, we jointly model the user's personal context and the conversational context using a novel context interaction mechanism in the hyperbolic space that clearly captures the interplay between the two and makes independent assessments on the amounts of information to be retrieved from both contexts. CoSyn perform
    
[^11]: 口语理解中的占位符：计算和心理语言学角度

    Fillers in Spoken Language Understanding: Computational and Psycholinguistic Perspectives. (arXiv:2301.10761v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10761](http://arxiv.org/abs/2301.10761)

    口语中的不流畅（即语言流的打断）是普遍存在的。填充语（“嗯”“啊”）是相对于其他类型的不流畅出现最频繁的。然而据我们所知，没有一个资源将影响口语理解的研究视角汇集起来，用于这些语音事件的处理。本文旨在以全面的方式调查各种角度；从考虑基础（心理）语言学理论，到他们在自动语音识别（ASR）和口语理解系统中的注释和考虑，最后再从生成角度进行研究。本文旨在以易于理解的方式向口语理解和对话AI社区介绍这些视角，并讨论前进的趋势和每个领域的挑战。

    Disfluencies (i.e. interruptions in the regular flow of speech), are ubiquitous to spoken discourse. Fillers ("uh", "um") are disfluencies that occur the most frequently compared to other kinds of disfluencies. Yet, to the best of our knowledge, there isn't a resource that brings together the research perspectives influencing Spoken Language Understanding (SLU) on these speech events. This aim of this article is to survey a breadth of perspectives in a holistic way; i.e. from considering underlying (psycho)linguistic theory, to their annotation and consideration in Automatic Speech Recognition (ASR) and SLU systems, to lastly, their study from a generation standpoint. This article aims to present the perspectives in an approachable way to the SLU and Conversational AI community, and discuss moving forward, what we believe are the trends and challenges in each area.
    
[^12]: 利用GPT-3驱动的教学代理培养儿童的好奇心提问能力

    GPT-3-driven pedagogical agents for training children's curious question-asking skills. (arXiv:2211.14228v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.14228](http://arxiv.org/abs/2211.14228)

    为了培养儿童提问好奇心，先前的研究探讨了设计特定的练习，依赖于提供语义和语言线索来帮助制定这样的问题。但尽管显示出教学效率，这种方法仍然有限，因为它依赖于手工生成所述提示，这可能是一个非常昂贵的过程。在这种情况下，我们建议利用自然语言处理领域(NLP)的进展，调查使用大型语言模型(LLM)自动生产好奇心提问(QA)培训的教学内容的效率。我们使用“基于提示”的方法来生成所述内容，该方法包括用自然文本向LLM解释任务。我们使用人类专家注释和与手工生成的内容进行比较来评估输出。结果确实表明了这种内容的相关性和有用性。我们还在小学进行了一项现场研究(75 ch

    In order to train children's ability to ask curiosity-driven questions, previous research has explored designing specific exercises relying on providing semantic and linguistic cues to help formulate such questions. But despite showing pedagogical efficiency, this method is still limited as it relies on generating the said cues by hand, which can be a very costly process. In this context, we propose to leverage advances in the natural language processing field (NLP) and investigate the efficiency of using a large language model (LLM) for automating the production of the pedagogical content of a curious question-asking (QA) training. We study generating the said content using the "prompt-based" method that consists of explaining the task to the LLM in natural text. We evaluate the output using human experts annotations and comparisons with hand-generated content. Results suggested indeed the relevance and usefulness of this content. We also conduct a field study in primary school (75 ch
    
[^13]: BERT-Deep CNN:新冠肺炎推文情感分析的最新研究

    BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets. (arXiv:2211.09733v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09733](http://arxiv.org/abs/2211.09733)

    社交媒体技术的快速发展加速了信息的自由流动。由于新冠病毒疾病(COVID-19)的爆发，对人口的社会和心理影响显著。新冠肺炎大流行是社交媒体平台上正在讨论的当前事件之一。为了保障社会免受这种大流行的影响，研究社交媒体上人们的情感至关重要。由于其特殊的特性，对推文等文本的情感分析仍然具有挑战性。情感分析是一种强大的文本分析工具，它可以自动检测和分析来自非结构化数据的意见和情绪。情感分析工具对来自各种来源的文本进行检查，包括电子邮件、调查、评论、社交媒体帖子和网络文章，从中提取含义。为了评估情感，使用自然语言处理(NLP)和机器学习技术，给实体分配权重。

    The free flow of information has been accelerated by the rapid development of social media technology. There has been a significant social and psychological impact on the population due to the outbreak of Coronavirus disease (COVID-19). The COVID-19 pandemic is one of the current events being discussed on social media platforms. In order to safeguard societies from this pandemic, studying people's emotions on social media is crucial. As a result of their particular characteristics, sentiment analysis of texts like tweets remains challenging. Sentiment analysis is a powerful text analysis tool. It automatically detects and analyzes opinions and emotions from unstructured data. Texts from a wide range of sources are examined by a sentiment analysis tool, which extracts meaning from them, including emails, surveys, reviews, social media posts, and web articles. To evaluate sentiments, natural language processing (NLP) and machine learning techniques are used, which assign weights to entit
    
[^14]: 时间建模的重要性：一种用于语音情感识别的新型时间情感建模方法

    Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition. (arXiv:2211.08233v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.08233](http://arxiv.org/abs/2211.08233)

    语音情感识别在通过语音信号推断人类情感和情感状态以提高人机交互方面起着至关重要的作用。而最近的研究主要集中在从手工特征中挖掘时空信息，我们探索如何从动态时间尺度模型语音情感的时间模式。为此，我们引入了一种新的用于语音情感识别的时间情感建模方法，称为时间感知的双向多尺度网络（TIM-Net），它从各种时间尺度学习多尺度的情感上下文表示。具体而言，TIM-Net首先采用时间感知块学习时间情感表示，然后从过去和未来集成补充信息以丰富情境表示，最后，融合多个时间尺度特征以更好地适应情感变化。在六个基准SER数据集上进行的广泛实验结果表明

    Speech emotion recognition (SER) plays a vital role in improving the interactions between humans and machines by inferring human emotion and affective states from speech signals. Whereas recent works primarily focus on mining spatiotemporal information from hand-crafted features, we explore how to model the temporal patterns of speech emotions from dynamic temporal scales. Towards that goal, we introduce a novel temporal emotional modeling approach for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net), which learns multi-scale contextual affective representations from various time scales. Specifically, TIM-Net first employs temporal-aware blocks to learn temporal affective representation, then integrates complementary information from the past and the future to enrich contextual representations, and finally, fuses multiple time scale features for better adaptation to the emotional variation. Extensive experimental results on six benchmark SER datasets demonstrate th
    
[^15]: 大型语言模型是与人类一样的提示工程师

    Large Language Models Are Human-Level Prompt Engineers. (arXiv:2211.01910v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01910](http://arxiv.org/abs/2211.01910)

    通过自然语言指令的条件语言模型，展示了作为通用计算机的惊人能力。然而，任务表现在很大程度上取决于用于引导模型的提示的质量，大多数有效的提示是由人类手工制作的。受经典程序综合和人类提示工程方法的启发，我们提出了用于自动指令生成和选择的自动提示工程师(APE)。在我们的方法中，我们将指令视为“程序”，通过在搜索由LLM提出的指令候选池上进行优化来最大化选择的得分函数。为了评估所选指令的质量，我们评估了另一个LLM在按照所选指令进行零射击时的性能。在24个NLP任务的实验中，我们自动生成的说明明显优于先前的LLM基线，并实现了更好或可比较的性能

    By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the 
    
[^16]: 通过检索非结构化知识进行自适应命名实体识别

    Self-Adaptive Named Entity Recognition by Retrieving Unstructured Knowledge. (arXiv:2210.07523v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07523](http://arxiv.org/abs/2210.07523)

    尽管命名实体识别（NER）可以帮助我们从文本中提取领域特定的实体（例如，音乐领域中的艺术家），但是在目标领域中创建大量的训练数据或结构化知识库以执行准确的NER是昂贵的。在这里，我们提出了自适应NER，它从非结构化文本中检索外部知识来学习未被充分学习的实体的用法。为了检索NER的有用知识，我们设计了一个有效的两阶段模型，使用不确定的实体作为查询来检索非结构化知识。我们的模型预测输入中的实体，然后找到预测不确定的实体。然后，它通过使用这些不确定的实体作为查询来检索知识，并将检索到的文本连接到原始输入来修正预测。在CrossNER数据集上的实验表明，我们的模型在F1度量中比强基线表现高2.35个百分点。

    Although named entity recognition (NER) helps us to extract domain-specific entities from text (e.g., artists in the music domain), it is costly to create a large amount of training data or a structured knowledge base to perform accurate NER in the target domain. Here, we propose self-adaptive NER, which retrieves external knowledge from unstructured text to learn the usages of entities that have not been learned well. To retrieve useful knowledge for NER, we design an effective two-stage model that retrieves unstructured knowledge using uncertain entities as queries. Our model predicts the entities in the input and then finds those of which the prediction is not confident. Then, it retrieves knowledge by using these uncertain entities as queries and concatenates the retrieved text to the original input to revise the prediction. Experiments on CrossNER datasets demonstrated that our model outperforms strong baselines by 2.35 points in F1 metric.
    
[^17]: ReAct: 在语言模型中协同推理和行动

    ReAct: Synergizing Reasoning and Acting in Language Models. (arXiv:2210.03629v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.03629](http://arxiv.org/abs/2210.03629)

    虽然大型语言模型在语言理解和交互式决策方面表现出了令人印象深刻的能力，但它们在推理和行动（例如行动计划生成）方面的能力主要作为单独的主题进行研究。在本文中，我们探讨了使用LLMs以交替方式生成推理跟踪和任务特定的行动的方法，允许两者之间的更大协同作用：推理跟踪有助于模型诱导、跟踪和更新行动计划，并处理异常，而行动允许它与外部来源进行交互，例如知识库或环境，以收集额外的信息。我们将我们的方法命名为ReAct，并将其应用于各种语言和决策任务，并展示其优于最先进的基线方法的效果，以及优于没有推理或行动组件的方法的人类可解释性和可信度提高。具体来说，在

    While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on 
    
[^18]: 词汇和语法体向的友善介绍以及计算方法综述

    A Kind Introduction to Lexical and Grammatical Aspect, with a Survey of Computational Approaches. (arXiv:2208.09012v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.09012](http://arxiv.org/abs/2208.09012)

    体向意味着事情的内部时间结构是如何呈现的。这包括情况是被描述为一种状态还是事件，情况是已结束还是正在进行，以及情况是整体还是关注于特定阶段。本文概述了模拟词汇和语法体向的计算方法以及必要的语言概念和术语的直观解释。特别地，我们描述了状态、目的性、习惯性、完成式和未完成式的概念，以及事件和情况类型的重要清单。我们认为，由于体向是语义的关键组成部分，特别是在精确报告情况的时间结构时，未来的NLP方法需要能够系统地处理和评估它，以实现人类水平的语言理解。

    Aspectual meaning refers to how the internal temporal structure of situations is presented. This includes whether a situation is described as a state or as an event, whether the situation is finished or ongoing, and whether it is viewed as a whole or with a focus on a particular phase. This survey gives an overview of computational approaches to modeling lexical and grammatical aspect along with intuitive explanations of the necessary linguistic concepts and terminology. In particular, we describe the concepts of stativity, telicity, habituality, perfective and imperfective, as well as influential inventories of eventuality and situation types. We argue that because aspect is a crucial component of semantics, especially when it comes to reporting the temporal structure of situations in a precise way, future NLP approaches need to be able to handle and evaluate it systematically in order to achieve human-level language understanding.
    
[^19]: 使用BERT进行阿拉伯语态度情感极性分类

    Arabic aspect sentiment polarity classification using BERT. (arXiv:2107.13290v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2107.13290](http://arxiv.org/abs/2107.13290)

    基于方面的情感分析(ABSA)是一种文本分析方法，它定义了与特定目标相关的某些方面上的意见极性。大多数ABSA研究都是用英语进行的，只有少量的阿拉伯语研究可用。大部分以前的阿拉伯语研究依赖于主要依赖于上下文无关词向量(例如word2vec)的深度学习模型，其中每个词都有一个独立于其上下文的固定表示。本文探讨了预训练语言模型(如BERT)的上下文嵌入的建模能力，并利用句子对输入处理阿拉伯语面向方面的情感极性分类任务。特别地，我们开发了一个简单但有效的基于BERT的神经基线来处理这个任务。根据在三个不同的阿拉伯语数据集上的实验结果，我们的BERT架构与简单的线性分类层超过了现有的最新工作。在阿拉伯国家的三个数据集上达到了89.51%的准确率。

    Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that defines the polarity of opinions on certain aspects related to specific targets. The majority of research on ABSA is in English, with a small amount of work available in Arabic. Most previous Arabic research has relied on deep learning models that depend primarily on context-independent word embeddings (e.g.word2vec), where each word has a fixed representation independent of its context. This article explores the modeling capabilities of contextual embeddings from pre-trained language models, such as BERT, and making use of sentence pair input on Arabic aspect sentiment polarity classification task. In particular, we develop a simple but effective BERT-based neural baseline to handle this task. Our BERT architecture with a simple linear classification layer surpassed the state-of-the-art works, according to the experimental results on three different Arabic datasets. Achieving an accuracy of 89.51% on the Arab
    

