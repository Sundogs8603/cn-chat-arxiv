# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Healthy AI: Large Language Models Need Therapists Too.](http://arxiv.org/abs/2304.00416) | SafeguardGPT框架使用心理治疗来纠正AI聊天机器人中的有害行为，改进与人类的对话质量，进而推进健康AI的发展。 |
| [^2] | [Automatic Authorship Attribution in the Work of Tirso de Molina.](http://arxiv.org/abs/2304.00363) | 本文在西班牙黄金时代剧场领域中应用AAA技术，通过聚类分析和距离度量方法证明了Tirso de Molina是五部传统喜剧的作者。 |
| [^3] | [When Crowd Meets Persona: Creating a Large-Scale Open-Domain Persona Dialogue Corpus.](http://arxiv.org/abs/2304.00350) | 该论文讨论了创建大规模开放领域角色扮演对话语料库的困难，并解决了众包和隐私等问题。 |
| [^4] | [What Does the Indian Parliament Discuss? An Exploratory Analysis of the Question Hour in the Lok Sabha.](http://arxiv.org/abs/2304.00235) | 本文对收集了1999年至2019年期间在印度议会下议院问题时间中讨论的问题和答案的数据集进行了探究性分析，并通过分析显示议会对话反映了不同时期的政治和社会经济紧张局势。 |
| [^5] | [Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers.](http://arxiv.org/abs/2304.00215) | 本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。 |
| [^6] | [FCC: Fusing Conversation History and Candidate Provenance for Contextual Response Ranking in Dialogue Systems.](http://arxiv.org/abs/2304.00180) | 本文提出了一种灵活的神经框架，可以将来自多个渠道的历史对话和候选来源上下文信息融合在一起，以改善多轮对话响应排序表现。 |
| [^7] | [Lego-Features: Exporting modular encoder features for streaming and deliberation ASR.](http://arxiv.org/abs/2304.00173) | 该论文介绍了一种称为Lego-Features的特征，将现有编码表示转换成模块化特征，而不修改预训练模型，在流式环境中也适用。这些特征性能强大，RNN-T或LAS解码器测试时保持高质量的下游性能，并足够丰富，可以代表两个通道决策中的第一遍预测。 |
| [^8] | [Practical Conformer: Optimizing size, speed and flops of Conformer for on-Device and cloud ASR.](http://arxiv.org/abs/2304.00171) | 本文介绍了一种优化的Conformer模型，它在大小、速度和FLOPS方面经过优化，可以在有限的设备上具有快速的推理速度。优化的Conformer可以轻松地并入级联编码器设置，并在更多资源可用时提高准确性。 |
| [^9] | [Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts.](http://arxiv.org/abs/2304.00121) | 通过分类法和数据集，本文提出一种有效的方法来解码学术手稿的端到端写作轨迹，以提供更细致的分析和反馈，并为大型语言模型提供在文档层面上的创造性和结构反馈。 |
| [^10] | [Enhancing Large Language Models with Climate Resources.](http://arxiv.org/abs/2304.00116) | 本研究利用最新的想法，将大型语言模型视为可以访问包含关于组织、机构和公司的最新和准确信息的代理，结合气候变化数据库和常规 Google 搜索，以提供更可靠和准确的信息。 |
| [^11] | [Extracting Thyroid Nodules Characteristics from Ultrasound Reports Using Transformer-based Natural Language Processing Methods.](http://arxiv.org/abs/2304.00115) | 本研究采用基于Transformer的自然语言处理方法，通过开发语料库和比较5种最先进的方法，成功提取了甲状腺结节超声报告中的关键特征，其中，GatorTron模型表现最佳。 |
| [^12] | [Dense Sparse Retrieval: Using Sparse Language Models for Inference Efficient Dense Retrieval.](http://arxiv.org/abs/2304.00114) | 本文研究了如何使用稀疏语言模型实现高效稠密检索，使用Tevatron 和MSMARCO、NQ和TriviaQA数据集，发现稀疏语言模型可以直接替换为原有模型，几乎不降低准确性，并且推理速度提高了最多4.3倍。 |
| [^13] | [Identifying Symptoms of Delirium from Clinical Narratives Using Natural Language Processing.](http://arxiv.org/abs/2304.00111) | 本论文使用自然语言处理技术，研究了从临床记录中提取谵妄症状的方法。经过比较，该研究发现GatorTron模型具有最佳效果。 |
| [^14] | [Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case.](http://arxiv.org/abs/2304.00025) | 本文提出了Alleviate，基于人工智能辅助电话医疗的虚拟助手，通过个性化护理来协助患有心理健康问题的患者，并协助临床医生更好地了解他们的患者。Alleviate利用临床有效的心理健康文本和数据库做出医学上合理和知情的决策。通过一系列案例研究，展示了Alleviate提供的基于人工智能的虚拟助手。 |
| [^15] | [SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis.](http://arxiv.org/abs/2304.00020) | 研究提出了一种利用多模态数据的半监督学习方法，命名为SemiMemes，主要应用于Memes的分析和注释过程。该方法在多个数据集中表现优异，并优于其他最新的多模态半监督学习和监督学习模型。 |
| [^16] | [On the Creativity of Large Language Models.](http://arxiv.org/abs/2304.00008) | 这篇论文探讨了大型语言模型的创造性问题，分析了与之相关的机器创造性的难点和易点，并重点分析了这些技术在创意产业中的社会影响。 |
| [^17] | [Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) Structural Exams?.](http://arxiv.org/abs/2303.18149) | 本论文探讨了AI聊天机器人能否通过工程基础（FE）和工程原理与实践（PE）考试，研究发现ChatGPT-4在FE考试中得分70.9％，在PE考试中得分46.2％，并且有望通过PE考试。 |
| [^18] | [Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning.](http://arxiv.org/abs/2303.17649) | 本文介绍了一种将英文GPT模型对齐到西班牙语的小封闭领域中的方法，该方法使用了奖励模型来改进答案的解码和生成，在问答任务中取得了良好的结果。 |
| [^19] | [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace.](http://arxiv.org/abs/2303.17580) | 用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。 |
| [^20] | [An Information Extraction Study: Take In Mind the Tokenization!.](http://arxiv.org/abs/2303.15100) | 该研究调查了从文本中提取信息时分词的影响，发现分词模式可以带来最先进的性能。同时，基于字符的模型也有希望，因此无分词的信息抽取模型是可行的。 |
| [^21] | [ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge.](http://arxiv.org/abs/2303.14070) | 本文介绍了一种在医学领域利用LLaMA模型微调的医疗聊天模型ChatDoctor。经过700多种疾病和其相应症状、药品和医疗检查的收集和处理，这种模型具有理解患者需求、提供建议和帮助的潜力。这些先进的语言模型集成到医疗保健中可以极大地改进医疗专业人员和患者的沟通方式。 |
| [^22] | [RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation.](http://arxiv.org/abs/2303.12570) | RepoCoder是一个简单、通用和有效的框架，结合了一个基于相似性的检索器和预训练的代码语言模型，在库级别代码完成流程中实现了有效利用库级别信息进行代码完成和生成。 |
| [^23] | [MEGA: Multilingual Evaluation of Generative AI.](http://arxiv.org/abs/2303.12528) | 这项研究对 33 种语言中 8 个不同任务的生成 AI 进行了全面评估，比较了生成 LLMs 和非自回归模型的表现差异。 |
| [^24] | [Block-wise Bit-Compression of Transformer-based Models.](http://arxiv.org/abs/2303.09184) | 本论文提出了一种用于Transformer的块位压缩方法，称为BBCT。该方法可以更细粒度地压缩整个Transformer，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。在GLUE数据集上测试结果表明，BBCT可以在大多数任务中实现少于1％的准确率下降。 |
| [^25] | [Truveta Mapper: A Zero-shot Ontology Alignment Framework.](http://arxiv.org/abs/2301.09767) | 提出了一个将无监督本体匹配或本体对齐视为翻译任务的新视角的Truveta Mapper框架，在零样本、统一和端到端的方式下执行多本体对齐。该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案，无需显式跨本体手动标注数据。 |
| [^26] | [Conditional Generation of Paired Antibody Chain Sequences through Encoder-Decoder Language Model.](http://arxiv.org/abs/2301.02748) | 本论文提出了pAbT5，这是一种能够采用编码器-解码器模型为蛋白质相互作用生成配对抗体链序列的语言模型。该模型可以准确地预测配对情况，并且能够在实验验证方面表现出最先进的无监督预测能力。 |
| [^27] | [MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology.](http://arxiv.org/abs/2301.02228) | 本文提出了一种基于医学知识增强的放射学语言-图像预训练方法，利用三元组提取模块和医学知识库查询实体翻译等技术，实现了医学实体之间的关系隐含以及医学诊断的能力。 |
| [^28] | [Highlighting Named Entities in Input for Auto-Formulation of Optimization Problems.](http://arxiv.org/abs/2212.13201) | 本文介绍了一种将线性规划单词问题转换为数学公式的方法。我们利用输入中的命名实体并增强输入以突出这些实体，从而实现了高准确度，赢得了NL4Opt竞赛生成赛道的第一名。 |
| [^29] | [Montague semantics and modifier consistency measurement in neural language models.](http://arxiv.org/abs/2212.04310) | 本文提出了一种用于测量神经语言模型合成行为的方法，并从形容词修饰名词短语的角度提出了三个新的合成行为测试。研究结果表明，当前的神经语言模型只在某种程度上符合预期的语言理论。 |
| [^30] | [Ham2Pose: Animating Sign Language Notation into Pose Sequences.](http://arxiv.org/abs/2211.13613) | 该论文提出了一种将HamNoSys符号转换为手语姿势序列的方法，使用变压器编码器建立文本和姿势间的有意义的表示，可用于不同手语之间的通用翻译。此外，提出了一种新的距离测量方法可以度量手语姿势序列之间的距离。 |
| [^31] | [Semantic Representations of Mathematical Expressions in a Continuous Vector Space.](http://arxiv.org/abs/2211.08142) | 该论文提出了一种在连续向量空间中表示数学表达式的方法，并使用序列到序列框架的编码器生成向量表示来捕捉数学语义，比自动编码器效果更好。 |
| [^32] | [Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs.](http://arxiv.org/abs/2210.13312) | 本文研究了现代NLP系统中社交智能和心理理论的问题。作者使用两个任务评估模型在理解社交互动意图和推断参与者心理状态和现实方面的能力，结果表明当前最大的语言模型GPT-3缺乏这种能力。这一发现强调了NLP系统在理解社交动态方面的当前局限性，并需要进一步研究。 |
| [^33] | [BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining.](http://arxiv.org/abs/2210.10341) | BioGPT是一种针对生物医学领域的生成式Transformer语言模型，它在多项生物医学自然语言处理任务中均表现出色，尤其在关系提取任务中表现尤为突出。 |
| [^34] | [MENLI: Robust Evaluation Metrics from Natural Language Inference.](http://arxiv.org/abs/2208.07316) | 本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。 |
| [^35] | [Large scale analysis of gender bias and sexism in song lyrics.](http://arxiv.org/abs/2208.02052) | 本文对377808首英文歌曲歌词进行大规模的自然语言处理分析，揭示了及时的性别歧视的增加以及不同性别表演者的语言偏见。 |
| [^36] | [DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing.](http://arxiv.org/abs/2207.08562) | 本文提出了一个双视图超关系知识图嵌入方法，其中DH-KG包含超关系实例视图和从实体层次抽象出的超关系本体视图，定义了链接预测和实体类型识别任务，并构建了两个DH-KG数据集JW44K-6K和HTDM。DHGE是一个基于GRAN编码器、HGNN和联合学习的DH-KG嵌入模型。 |
| [^37] | [Questions Are All You Need to Train a Dense Passage Retriever.](http://arxiv.org/abs/2206.10658) | ART是一种能够不使用标记数据进行训练的密集检索模型，并且只需要访问未配对的输入和输出。它使用一个新的文档检索自编码方案，通过问题重构进行检索训练，可以有效地进行无监督学习，并且可以将其合并到完整的Open QA系统中。 |
| [^38] | [Sparse*BERT: Sparse Models Generalize To New tasks and Domains.](http://arxiv.org/abs/2205.12452) | 本文研究了使用渐进非结构化幅值修剪进行修剪的模型如何在领域和任务之间进行转移。使用遮蔽语言模型进行预训练的被修剪模型能够在不进行广泛的超参数探索或专门方法的情况下转移到新的领域和任务。在生物医学NLP任务中，Sparse*BERT可以达到或超过BioBERT的性能。 |
| [^39] | [Improving Passage Retrieval with Zero-Shot Question Generation.](http://arxiv.org/abs/2204.07496) | 本论文提出了一种基于零样本问题生成模型的重新排序方法，适用于开放式问题回答中的段落检索，可以适用于任何检索方法，无需特定训练，可在多个数据集中提高检索准确性，可以帮助提高开放域问答的性能。 |
| [^40] | [Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding.](http://arxiv.org/abs/2203.05711) | 这个研究收集并发布了一个视频-语言故事数据集SYMON，用于推进多模态故事理解的发展。 |
| [^41] | [Structural invariants and semantic fingerprints in the "ego network" of words.](http://arxiv.org/abs/2203.00588) | 本文探究了词语的“自我网络”，通过分析Twitter上的异构数据集，发现词汇使用存在同心圆层次结构，各层大小呈规则性增长。 |

# 详细

[^1]: 迈向健康AI：大型语言模型也需要治疗师

    Towards Healthy AI: Large Language Models Need Therapists Too. (arXiv:2304.00416v1 [cs.AI])

    [http://arxiv.org/abs/2304.00416](http://arxiv.org/abs/2304.00416)

    SafeguardGPT框架使用心理治疗来纠正AI聊天机器人中的有害行为，改进与人类的对话质量，进而推进健康AI的发展。

    

    近期大型语言模型 (LLM) 的进展带来了功能强大的 AI 聊天机器人，能够参与自然且类似人类的对话。然而，这些聊天机器人可能具有潜在的危害性，表现出操纵、灌输虚假观念和自恋行为。我们定义健康AI为安全、可信和道德的AI。为了创造健康的AI系统，我们提出了SafeguardGPT框架，该框架使用心理治疗来纠正AI聊天机器人中的这些有害行为。该框架涉及四种类型的AI代理：聊天机器人、"用户"、"治疗师"和"评论家"。我们通过模拟社交对话的工作示例展示了SafeguardGPT的有效性。我们的结果表明，该框架能够改进AI聊天机器人和人类之间的对话质量。虽然未来仍需解决几个挑战和方向，但SafeguardGPT为改善AI聊天机器人与人类价值观之间的协调提供了一种有前途的方法。

    Recent advances in large language models (LLMs) have led to the development of powerful AI chatbots capable of engaging in natural and human-like conversations. However, these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to be safe, trustworthy and ethical. To create healthy AI systems, we present the SafeguardGPT framework that uses psychotherapy to correct for these harmful behaviors in AI chatbots. The framework involves four types of AI agents: a Chatbot, a "User," a "Therapist," and a "Critic." We demonstrate the effectiveness of SafeguardGPT through a working example of simulating a social conversation. Our results show that the framework can improve the quality of conversations between AI chatbots and humans. Although there are still several challenges and directions to be addressed in the future, SafeguardGPT provides a promising approach to improving the alignment between AI chatbots and human value
    
[^2]: Tirso de Molina作品中的自动作者归属研究

    Automatic Authorship Attribution in the Work of Tirso de Molina. (arXiv:2304.00363v1 [cs.CL])

    [http://arxiv.org/abs/2304.00363](http://arxiv.org/abs/2304.00363)

    本文在西班牙黄金时代剧场领域中应用AAA技术，通过聚类分析和距离度量方法证明了Tirso de Molina是五部传统喜剧的作者。

    

    自动作者归属 (AAA) 是在数字人文学领域应用工具和技术进行作者归属研究的结果。通过数量和统计的方法，这一学科可以对传统批评家处理了几个世纪的著名作者问题作出更进一步的结论，打开了对风格比较的新门径。本文旨在通过测试五部传统归属于西班牙剧作家Tirso de Molina (1579-1648) 的喜剧《La ninfa del cielo》、《El burlador de Sevilla》、《Tan largo me lo fiais》、《La mujer por fuerza》和《El condenado por desconfiado》的作者身份，证明这些工具和技术的潜力。为了达到这个目的，对由Tirso de Molina、Andres de Claramonte (c. 1560-1626)、Antonio Mira de Amescua (1577-1644)和Luis Velez de Guevara (1579-1644)的剧本构成的语料库进行了一些聚类分析的实验，使用了R语言中的Stylo包和四种距离度量方法。实验结果表明AAA在西班牙黄金时代剧院的作者归属研究中具有潜力，证明了Tirso de Molina在五部分析中的作者身份。

    Automatic Authorship Attribution (AAA) is the result of applying tools and techniques from Digital Humanities to authorship attribution studies. Through a quantitative and statistical approach this discipline can draw further conclusions about renowned authorship issues which traditional critics have been dealing with for centuries, opening a new door to style comparison. The aim of this paper is to prove the potential of these tools and techniques by testing the authorship of five comedies traditionally attributed to Spanish playwright Tirso de Molina (1579-1648): La ninfa del cielo, El burlador de Sevilla, Tan largo me lo fiais, La mujer por fuerza and El condenado por desconfiado. To accomplish this purpose some experiments concerning clustering analysis by Stylo package from R and four distance measures are carried out on a corpus built with plays by Tirso, Andres de Claramonte (c. 1560-1626), Antonio Mira de Amescua (1577-1644) and Luis Velez de Guevara (1579-1644). The results ob
    
[^3]: 当众包遇到角色扮演：创建一个大规模的开放领域角色扮演对话语料库

    When Crowd Meets Persona: Creating a Large-Scale Open-Domain Persona Dialogue Corpus. (arXiv:2304.00350v1 [cs.CL])

    [http://arxiv.org/abs/2304.00350](http://arxiv.org/abs/2304.00350)

    该论文讨论了创建大规模开放领域角色扮演对话语料库的困难，并解决了众包和隐私等问题。

    

    构建自然语言数据集需要小心，因为单词的语义容易受到微妙的文本变化或被注释概念的定义影响。这种趋势可以在生成任务（例如问答和对话生成）以及创建基于分类的语料库（例如主题分类或情感分析）中看到。开放领域会话涉及两个或多个众包工人自由谈论任何主题，收集这种数据尤其困难，原因是：1）由于隐私问题，数据集应该“创造”而不是“获得”；2）有偿创建这类对话可能与众包工作人员在现实世界中的行为不同。在本研究中，我们解决了创建一个大规模的开放领域角色扮演对话语料库时的这些问题，其中“角色扮演”意味着对话由固定角色的多个演员和来自未指定的群众的用户端工作者执行。

    Building a natural language dataset requires caution since word semantics is vulnerable to subtle text change or the definition of the annotated concept. Such a tendency can be seen in generative tasks like question-answering and dialogue generation and also in tasks that create a categorization-based corpus, like topic classification or sentiment analysis. Open-domain conversations involve two or more crowdworkers freely conversing about any topic, and collecting such data is particularly difficult for two reasons: 1) the dataset should be ``crafted" rather than ``obtained" due to privacy concerns, and 2) paid creation of such dialogues may differ from how crowdworkers behave in real-world settings. In this study, we tackle these issues when creating a large-scale open-domain persona dialogue corpus, where persona implies that the conversation is performed by several actors with a fixed persona and user-side workers from an unspecified crowd.
    
[^4]: 印度议会讨论了什么？Lok Sabha中问题时间的探索性分析。(arXiv:2304.00235v1 [cs.CL])

    What Does the Indian Parliament Discuss? An Exploratory Analysis of the Question Hour in the Lok Sabha. (arXiv:2304.00235v1 [cs.CL])

    [http://arxiv.org/abs/2304.00235](http://arxiv.org/abs/2304.00235)

    本文对收集了1999年至2019年期间在印度议会下议院问题时间中讨论的问题和答案的数据集进行了探究性分析，并通过分析显示议会对话反映了不同时期的政治和社会经济紧张局势。

    

    TCPD-IPD数据集是收集了1999年至2019年期间在印度议会下议院问题时间中讨论的问题和答案。虽然手动分析这么庞大的数据集很困难，但现代文本分析工具可以提供有效的导航手段。 在本文中，我们对数据集进行了探究性分析。具体而言，我们提供了有见地的语料库水平统计数据和对数据集的三个子集进行的详细分析。 在后一项分析中，重点是使用动态主题模型理解话题的时间演变。 我们观察到，议会对话确实反映了每个时期的政治和社会经济紧张局势。

    The TCPD-IPD dataset is a collection of questions and answers discussed in the Lower House of the Parliament of India during the Question Hour between 1999 and 2019. Although it is difficult to analyze such a huge collection manually, modern text analysis tools can provide a powerful means to navigate it. In this paper, we perform an exploratory analysis of the dataset. In particular, we present insightful corpus-level statistics and a detailed analysis of three subsets of the dataset. In the latter analysis, the focus is on understanding the temporal evolution of topics using a dynamic topic model. We observe that the parliamentary conversation indeed mirrors the political and socio-economic tensions of each period.
    
[^5]: 基于分层Transformer的关系路径和上下文归纳关系预测方法

    Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])

    [http://arxiv.org/abs/2304.00215](http://arxiv.org/abs/2304.00215)

    本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。

    

    在知识图谱中进行关系预测是一个重要的研究课题。现有的嵌入式方法主要依赖于转导设置，缺乏归纳能力，无法推广到新的实体上进行推理。本文提出了一种新方法，通过使用统一的分层Transformer框架，即REPORT，同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性，这种方法完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在实验中，REPORT表现优于所有基线方法，甚至在两个完全归纳的数据集的八个版本子集上也是如此。此外，REPORT能够将推理推广到训练和推理中没有公共实体的新实体上，并在基准数据集上实现了最先进的性能。

    Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
    
[^6]: FCC: 在对话系统中融合历史对话和候选来源进行上下文响应排序

    FCC: Fusing Conversation History and Candidate Provenance for Contextual Response Ranking in Dialogue Systems. (arXiv:2304.00180v1 [cs.CL])

    [http://arxiv.org/abs/2304.00180](http://arxiv.org/abs/2304.00180)

    本文提出了一种灵活的神经框架，可以将来自多个渠道的历史对话和候选来源上下文信息融合在一起，以改善多轮对话响应排序表现。

    

    对话中的响应排序在检索型会话系统中起着关键作用。在多轮对话中，为了捕捉对话的要点，上下文信息作为重要的知识至关重要。本文提出了一个灵活的神经框架，可以整合来自多个渠道的上下文信息。特别是针对当前任务，我们的方法是提供两个信息通道并行，即将与候选来源相关的 Conversational History（对话历史）和 Domain Knowledge（领域知识）融合在一起，作为上下文信息以提高多轮对话响应排序的表现。所提出的方法可以通用于将其他上下文目标任务的各种上下文特征纳入模块中。我们在广泛用于评估会话响应排名任务的 MSDialog 数据集上评估了我们的模型。实验结果显示，我们的框架显着优于其他最先进的方法。

    Response ranking in dialogues plays a crucial role in retrieval-based conversational systems. In a multi-turn dialogue, to capture the gist of a conversation, contextual information serves as essential knowledge to achieve this goal. In this paper, we present a flexible neural framework that can integrate contextual information from multiple channels. Specifically for the current task, our approach is to provide two information channels in parallel, Fusing Conversation history and domain knowledge extracted from Candidate provenance (FCC), where candidate responses are curated, as contextual information to improve the performance of multi-turn dialogue response ranking. The proposed approach can be generalized as a module to incorporate miscellaneous contextual features for other context-oriented tasks. We evaluate our model on the MSDialog dataset widely used for evaluating conversational response ranking tasks. Our experimental results show that our framework significantly outperform
    
[^7]: Lego-Features：导出模块化编码器特征用于流式和决策ASR

    Lego-Features: Exporting modular encoder features for streaming and deliberation ASR. (arXiv:2304.00173v1 [cs.CL])

    [http://arxiv.org/abs/2304.00173](http://arxiv.org/abs/2304.00173)

    该论文介绍了一种称为Lego-Features的特征，将现有编码表示转换成模块化特征，而不修改预训练模型，在流式环境中也适用。这些特征性能强大，RNN-T或LAS解码器测试时保持高质量的下游性能，并足够丰富，可以代表两个通道决策中的第一遍预测。

    

    在端到端的语音识别模型中，编码器和解码器之间不可避免地出现了一种紧密耦合的表达。我们建立在最近开始探索构建带有模块化编码表示的编码器的工作之上，这样不同模型的编码器和解码器可以在零样本的情况下进行拼接，而无需进行进一步的微调。虽然之前的研究只涉及全上下文语音模型，但我们也在流式环境中探索了这个问题。我们的框架建立在现有编码表示之上，将其转换为模块化特征，称为Lego-Features，而不修改预训练模型。这些特征在用不同的初始化重新训练模型时仍然可以相互替换。尽管稀疏，我们展示了Lego-Features在使用RNN-T或LAS解码器进行测试时具有强大的性能，保持高质量的下游性能。它们也足够丰富，可以代表两个通道决策中的第一遍预测。

    In end-to-end (E2E) speech recognition models, a representational tight-coupling inevitably emerges between the encoder and the decoder. We build upon recent work that has begun to explore building encoders with modular encoded representations, such that encoders and decoders from different models can be stitched together in a zero-shot manner without further fine-tuning. While previous research only addresses full-context speech models, we explore the problem in a streaming setting as well. Our framework builds on top of existing encoded representations, converting them to modular features, dubbed as Lego-Features, without modifying the pre-trained model. The features remain interchangeable when the model is retrained with distinct initializations. Though sparse, we show that the Lego-Features are powerful when tested with RNN-T or LAS decoders, maintaining high-quality downstream performance. They are also rich enough to represent the first-pass prediction during two-pass deliberatio
    
[^8]: 实用Conformer：优化设备和云ASR的Conformer大小，速度和FLOPS

    Practical Conformer: Optimizing size, speed and flops of Conformer for on-Device and cloud ASR. (arXiv:2304.00171v1 [cs.CL])

    [http://arxiv.org/abs/2304.00171](http://arxiv.org/abs/2304.00171)

    本文介绍了一种优化的Conformer模型，它在大小、速度和FLOPS方面经过优化，可以在有限的设备上具有快速的推理速度。优化的Conformer可以轻松地并入级联编码器设置，并在更多资源可用时提高准确性。

    

    Conformer模型维护大量的内部状态，其中绝大部分与自注意力层相关。在有限的内存带宽下，每次推理从内存中读取这些状态可能会减慢推理速度。在本文中，我们设计了一个优化的Conformer，它足够小，以满足设备的限制，并可以在TPU上快速推理。我们探索了各种想法来提高执行速度，包括用仅包含卷积的块替换较低的Conformer块，策略性地缩小体系结构，并利用RNNAttention-Performer。我们的优化Conformer可以轻松地并入级联编码器设置，允许第二遍解码器对其输出进行操作，并在更多资源可用时提高准确性。总的来说，我们发现这些优化可以将延迟降低6.8倍，且在质量上取得合理的折衷。通过级联第二次通过，我们展示了识别精度的完全实现。

    Conformer models maintain a large number of internal states, the vast majority of which are associated with self-attention layers. With limited memory bandwidth, reading these from memory at each inference step can slow down inference. In this paper, we design an optimized conformer that is small enough to meet on-device restrictions and has fast inference on TPUs. We explore various ideas to improve the execution speed, including replacing lower conformer blocks with convolution-only blocks, strategically downsizing the architecture, and utilizing an RNNAttention-Performer. Our optimized conformer can be readily incorporated into a cascaded-encoder setting, allowing a second-pass decoder to operate on its output and improve the accuracy whenever more resources are available. Altogether, we find that these optimizations can reduce latency by a factor of 6.8x, and come at a reasonable trade-off in quality. With the cascaded second-pass, we show that the recognition accuracy is completel
    
[^9]: 学术手稿的端到端写作轨迹解码

    Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts. (arXiv:2304.00121v1 [cs.CL])

    [http://arxiv.org/abs/2304.00121](http://arxiv.org/abs/2304.00121)

    通过分类法和数据集，本文提出一种有效的方法来解码学术手稿的端到端写作轨迹，以提供更细致的分析和反馈，并为大型语言模型提供在文档层面上的创造性和结构反馈。

    

    学术写作是一个复杂的领域，通常按照系统性的程序规划和制作既有合理性又富有创造性的作品。最近，大型语言模型（LLM）在文本生成和修订任务方面取得了相当的成功；然而，LLM仍然难以在文档层面上提供结构和创造性反馈，这对于学术写作至关重要。本文介绍了一个新的分类法，根据意图、写作者的行为及其所写数据的信息类型将学术写作行为分类，并提供了ManuScript数据集，其中包括我们分类法的简化版本及作家行为和意图等注释。我们的分类法包括三个级别，以追踪总体写作流程并识别嵌入在每个高级过程中的不同写作者活动。 ManuScript由1000多个语言表达特征组成，这些特征是通过对句子的全面解析和逻辑结构的注释而获得的。我们证明了我们的分类法能够有效地解码学术手稿的端到端写作轨迹，通过提供关于写作者意图、行为和使用信息类型的详细信息，实现对文档层面上更细致的分析和反馈。

    Scholarly writing presents a complex space that generally follows a methodical procedure to plan and produce both rationally sound and creative compositions. Recent works involving large language models (LLM) demonstrate considerable success in text generation and revision tasks; however, LLMs still struggle to provide structural and creative feedback on the document level that is crucial to academic writing. In this paper, we introduce a novel taxonomy that categorizes scholarly writing behaviors according to intention, writer actions, and the information types of the written data. We also provide ManuScript, an original dataset annotated with a simplified version of our taxonomy to show writer actions and the intentions behind them. Motivated by cognitive writing theory, our taxonomy for scientific papers includes three levels of categorization in order to trace the general writing flow and identify the distinct writer activities embedded within each higher-level process. ManuScript 
    
[^10]: 利用气候资源增强大型语言模型

    Enhancing Large Language Models with Climate Resources. (arXiv:2304.00116v1 [cs.CL])

    [http://arxiv.org/abs/2304.00116](http://arxiv.org/abs/2304.00116)

    本研究利用最新的想法，将大型语言模型视为可以访问包含关于组织、机构和公司的最新和准确信息的代理，结合气候变化数据库和常规 Google 搜索，以提供更可靠和准确的信息。

    

    大型语言模型 (LLM) 在生成各种主题的类人文本方面展示了其能力，显著地改变了人工智能的格局。然而，尽管它们具有令人印象深刻的能力，但 LLM 缺乏最近的信息，并且常常使用不够准确的语言，这在需要准确性的领域比如气候变化中可能会有害。在本研究中，我们利用最新的想法来利用 LLM 的潜力，将它们视为可以访问包含关于组织、机构和公司的最新和准确信息的数据库的代理。我们通过一个原型代理演示了我们的方法的有效性，该代理从 ClimateWatch (https://www.climatewatchdata.org/) 检索排放数据并利用常规 Google 搜索。通过将这些资源与 LLM 集成，我们的方法克服了与不精确语言相关的限制，在气候变化等领域提供了更可靠和准确的信息。

    Large language models (LLMs) have significantly transformed the landscape of artificial intelligence by demonstrating their ability in generating human-like text across diverse topics. However, despite their impressive capabilities, LLMs lack recent information and often employ imprecise language, which can be detrimental in domains where accuracy is crucial, such as climate change. In this study, we make use of recent ideas to harness the potential of LLMs by viewing them as agents that access multiple sources, including databases containing recent and precise information about organizations, institutions, and companies. We demonstrate the effectiveness of our method through a prototype agent that retrieves emission data from ClimateWatch (https://www.climatewatchdata.org/) and leverages general Google search. By integrating these resources with LLMs, our approach overcomes the limitations associated with imprecise language and delivers more reliable and accurate information in the cr
    
[^11]: 采用基于Transformer的自然语言处理方法从超声报告中提取甲状腺结节特征

    Extracting Thyroid Nodules Characteristics from Ultrasound Reports Using Transformer-based Natural Language Processing Methods. (arXiv:2304.00115v1 [cs.CL])

    [http://arxiv.org/abs/2304.00115](http://arxiv.org/abs/2304.00115)

    本研究采用基于Transformer的自然语言处理方法，通过开发语料库和比较5种最先进的方法，成功提取了甲状腺结节超声报告中的关键特征，其中，GatorTron模型表现最佳。

    

    甲状腺结节的超声特征是指导甲状腺癌评估的重要指标。然而，甲状腺结节的特征通常记录在临床叙述中，如超声报告中。先前的研究已经使用基于规则的NLP系统提取了一些限定数量(<9)的特征。在本研究中，多学科团队的NLP专家和甲状腺专家确定了对临床护理有重要意义的甲状腺结节特征，编写了注释指南，开发了一个语料库，并比较了5种最先进的基于transformer的NLP方法，包括BERT、RoBERTa、LongFormer、DeBERTa和GatorTron，从超声报告中提取甲状腺结节特征。我们的模型GatorTron，使用90亿个文本单词进行了训练，该模型是基于Transformer的大型语言模型，实现了最佳的严格和宽松F1-score分别为0.8851和0.9495，从超声报告中提取甲状腺结节特征。

    The ultrasound characteristics of thyroid nodules guide the evaluation of thyroid cancer in patients with thyroid nodules. However, the characteristics of thyroid nodules are often documented in clinical narratives such as ultrasound reports. Previous studies have examined natural language processing (NLP) methods in extracting a limited number of characteristics (<9) using rule-based NLP systems. In this study, a multidisciplinary team of NLP experts and thyroid specialists, identified thyroid nodule characteristics that are important for clinical care, composed annotation guidelines, developed a corpus, and compared 5 state-of-the-art transformer-based NLP methods, including BERT, RoBERTa, LongFormer, DeBERTa, and GatorTron, for extraction of thyroid nodule characteristics from ultrasound reports. Our GatorTron model, a transformer-based large language model trained using over 90 billion words of text, achieved the best strict and lenient F1-score of 0.8851 and 0.9495 for the extract
    
[^12]: 稠密稀疏检索：使用稀疏语言模型实现高效稠密检索

    Dense Sparse Retrieval: Using Sparse Language Models for Inference Efficient Dense Retrieval. (arXiv:2304.00114v1 [cs.IR])

    [http://arxiv.org/abs/2304.00114](http://arxiv.org/abs/2304.00114)

    本文研究了如何使用稀疏语言模型实现高效稠密检索，使用Tevatron 和MSMARCO、NQ和TriviaQA数据集，发现稀疏语言模型可以直接替换为原有模型，几乎不降低准确性，并且推理速度提高了最多4.3倍。

    

    基于向量的检索系统已成为学术和工业搜索应用的常见工具，因为它们提供了一种简单而可扩展的方式来利用文档和查询的上下文表示进行搜索。由于这些向量系统依赖于上下文语言模型，因此它们通常需要GPU的使用，这可能会很昂贵且难以管理。鉴于近年来引入稀疏性以提高推理效率的语言模型的最新进展，本文研究了如何使用稀疏语言模型进行稠密检索以提高推理效率。使用流行的检索库Tevatron和MSMARCO、NQ和TriviaQA数据集，我们发现稀疏语言模型可以直接替换为原有模型，几乎不降低准确性，并且推理速度提高了最多4.3倍。

    Vector-based retrieval systems have become a common staple for academic and industrial search applications because they provide a simple and scalable way of extending the search to leverage contextual representations for documents and queries. As these vector-based systems rely on contextual language models, their usage commonly requires GPUs, which can be expensive and difficult to manage. Given recent advances in introducing sparsity into language models for improved inference efficiency, in this paper, we study how sparse language models can be used for dense retrieval to improve inference efficiency. Using the popular retrieval library Tevatron and the MSMARCO, NQ, and TriviaQA datasets, we find that sparse language models can be used as direct replacements with little to no drop in accuracy and up to 4.3x improved inference speeds
    
[^13]: 使用自然语言处理从临床叙述中识别谵妄症状

    Identifying Symptoms of Delirium from Clinical Narratives Using Natural Language Processing. (arXiv:2304.00111v1 [cs.CL])

    [http://arxiv.org/abs/2304.00111](http://arxiv.org/abs/2304.00111)

    本论文使用自然语言处理技术，研究了从临床记录中提取谵妄症状的方法。经过比较，该研究发现GatorTron模型具有最佳效果。

    

    谵妄症状是一种急性的认知功能下降或波动，可能导致严重的不良后果。由于谵妄症状的短暂和多样性，它经常被忽视且未被编码到患者的电子病历中。自然语言处理是一种从临床叙述中提取医学概念的关键技术，在谵妄症状和后果的研究中显示出巨大的潜力。为了辅助谵妄症状的诊断和表型描述，我们组成了专家小组来分类不同的谵妄症状，编写注释指南，创建了包含多样化谵妄症状的数据集，并开发了从临床记录中提取谵妄症状的NLP方法。我们比较了包括2种常规领域模型（BERT和RoBERTa）和3种临床领域模型（BERT_MIMIC，RoBERTa_MIMIC和GatorTron）在内的5种最先进的转换器模型。GatorTron取得了最佳的结果。

    Delirium is an acute decline or fluctuation in attention, awareness, or other cognitive function that can lead to serious adverse outcomes. Despite the severe outcomes, delirium is frequently unrecognized and uncoded in patients' electronic health records (EHRs) due to its transient and diverse nature. Natural language processing (NLP), a key technology that extracts medical concepts from clinical narratives, has shown great potential in studies of delirium outcomes and symptoms. To assist in the diagnosis and phenotyping of delirium, we formed an expert panel to categorize diverse delirium symptoms, composed annotation guidelines, created a delirium corpus with diverse delirium symptoms, and developed NLP methods to extract delirium symptoms from clinical notes. We compared 5 state-of-the-art transformer models including 2 models (BERT and RoBERTa) from the general domain and 3 models (BERT_MIMIC, RoBERTa_MIMIC, and GatorTron) from the clinical domain. GatorTron achieved the best stri
    
[^14]: 演示Alleviate：基于人工智能辅助电话医疗的虚拟助手：以心理健康为例

    Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case. (arXiv:2304.00025v1 [cs.CL])

    [http://arxiv.org/abs/2304.00025](http://arxiv.org/abs/2304.00025)

    本文提出了Alleviate，基于人工智能辅助电话医疗的虚拟助手，通过个性化护理来协助患有心理健康问题的患者，并协助临床医生更好地了解他们的患者。Alleviate利用临床有效的心理健康文本和数据库做出医学上合理和知情的决策。通过一系列案例研究，展示了Alleviate提供的基于人工智能的虚拟助手。

    

    在疫情之后，基于人工智能的精神卫生护理支持变得愈发重要。为了提供足够的关怀，应对所面临挑战的广度和复杂性涉及：（a）个性化病人理解，（b）安全约束和医学验证的聊天机器人病人交互，以及（c）支持使用聊天机器人-病人交互进行基于反馈的设计的不断改进。我们提出了Alleviate，一个聊天机器人，旨在通过个性化护理来协助患有心理健康问题的患者，并协助临床医生更好地了解他们的患者。Alleviate利用一系列公开可用的临床有效的心理健康文本和数据库，从而使Alleviate能够做出医学上合理和知情的决策。此外，Alleviate的模块化设计和可解释性决策使其适合进行健壮和持续基于反馈的设计改进。在本文中，我们解释了Alleviate的不同模块以及它们如何共同为心理健康患者提供个性化的帮助。我们还通过一系列案例研究展示了Alleviate提供的基于人工智能的虚拟助手。

    After the pandemic, artificial intelligence (AI) powered support for mental health care has become increasingly important. The breadth and complexity of significant challenges required to provide adequate care involve: (a) Personalized patient understanding, (b) Safety-constrained and medically validated chatbot patient interactions, and (c) Support for continued feedback-based refinements in design using chatbot-patient interactions. We propose Alleviate, a chatbot designed to assist patients suffering from mental health challenges with personalized care and assist clinicians with understanding their patients better. Alleviate draws from an array of publicly available clinically valid mental-health texts and databases, allowing Alleviate to make medically sound and informed decisions. In addition, Alleviate's modular design and explainable decision-making lends itself to robust and continued feedback-based refinements to its design. In this paper, we explain the different modules of A
    
[^15]: SemiMemes：一种用于多模态Memes分析的半监督学习方法

    SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])

    [http://arxiv.org/abs/2304.00020](http://arxiv.org/abs/2304.00020)

    研究提出了一种利用多模态数据的半监督学习方法，命名为SemiMemes，主要应用于Memes的分析和注释过程。该方法在多个数据集中表现优异，并优于其他最新的多模态半监督学习和监督学习模型。

    

    社交媒体上Memes的普及性引发了分析其隐含含义、审查有害内容的需求。机器学习的Meme审查系统需要半监督学习解决方案，以利用互联网上大量未标记的Memes，并使注释过程变得更简单。此外，该方法需要利用多模态数据，因为Memes的含义通常来自图像和文本。该研究提出了一种多模态半监督学习方法，在两个数据集，即多媒体自动性别歧视识别和令人讨厌的Memes数据集上，优于其他多模态半监督和监督学习的最新模型。借鉴对比语言-图像预训练所获得的见解，这项研究引入了SemiMemes，一种新颖的训练方法，它结合了自编码器和分类任务

    The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to
    
[^16]: 关于大型语言模型的创造性研究

    On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])

    [http://arxiv.org/abs/2304.00008](http://arxiv.org/abs/2304.00008)

    这篇论文探讨了大型语言模型的创造性问题，分析了与之相关的机器创造性的难点和易点，并重点分析了这些技术在创意产业中的社会影响。

    

    大型语言模型(LLMs)正在颠覆人工智能的多个领域。其中最显著的应用之一是创作，例如诗歌或故事：生成的输出通常具有惊人的质量。但是，一个自然的问题是：LLMs真的可以被认为是创造性的吗？在本文中，我们首先通过创造性理论的角度分析了LLMs的发展，探讨了关键的未解决问题和挑战。然后，我们在与LLMs相关的机器创造性方面确定了一组“易”和“难”问题，并对其进行了讨论。最后，我们分析了这些技术在创意产业中的社会影响。

    Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
    
[^17]: AI聊天机器人是否能通过工程基础（FE）和工程原理与实践（PE）结构考试？

    Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) Structural Exams?. (arXiv:2303.18149v1 [cs.CL])

    [http://arxiv.org/abs/2303.18149](http://arxiv.org/abs/2303.18149)

    本论文探讨了AI聊天机器人能否通过工程基础（FE）和工程原理与实践（PE）考试，研究发现ChatGPT-4在FE考试中得分70.9％，在PE考试中得分46.2％，并且有望通过PE考试。

    

    在工程界，随着OpenAI ChatGPT-4和Google Bard的发布，聊天机器人技术近年来迅速发展。虽然这些聊天机器人被报道表现良好，甚至通过了各种标准化考试，包括医学和法律考试，但本论文探讨这些聊天机器人是否也能通过工程基础（FE）和工程原理与实践（PE）考试。我们使用多样化的土木和环境工程问题和情景来评估聊天机器人的性能，在FE和PE考试中常见。基于相关性、准确性和清晰度，分析了聊天机器人的响应，然后与National Council of Examiners for Engineering and Surveying (NCEES)的建议进行了比较。我们的报告显示，ChatGPT-4和Bard在FE考试中得分分别为70.9％和39.2％，在PE考试中得分分别为46.2％和41％。显然，目前版本的ChatGPT-4有可能通过PE考试，但在FE考试中成绩较低。

    The engineering community has recently witnessed the emergence of chatbot technology with the release of OpenAI ChatGPT-4 and Google Bard. While these chatbots have been reported to perform well and even pass various standardized tests, including medical and law exams, this forum paper explores whether these chatbots can also pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) exams. A diverse range of civil and environmental engineering questions and scenarios are used to evaluate the chatbots' performance, as commonly present in the FE and PE exams. The chatbots' responses were analyzed based on their relevance, accuracy, and clarity and then compared against the recommendations of the National Council of Examiners for Engineering and Surveying (NCEES). Our report shows that ChatGPT-4 and Bard, respectively scored 70.9% and 39.2% in the FE exam and 46.2% and 41% in the PE exam. It is evident that the current version of ChatGPT-4 could potentially
    
[^18]: 利用强化学习将一个中等大小的英文GPT模型对齐到西班牙语的小封闭领域中

    Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v1 [cs.CL])

    [http://arxiv.org/abs/2303.17649](http://arxiv.org/abs/2303.17649)

    本文介绍了一种将英文GPT模型对齐到西班牙语的小封闭领域中的方法，该方法使用了奖励模型来改进答案的解码和生成，在问答任务中取得了良好的结果。

    

    本文提出了一种方法，将原本用于开放领域的中等大小英文GPT模型，对齐到西班牙语的小封闭领域。该模型被精细调整用于问答任务。为了实现这一目标，我们还需要训练和实现另一个神经网络（我们称之为奖励模型），以评分并确定答案是否适用于给定的问题。该组件有助于改进系统回答的解码和生成。 BLEU和perplexity等数字度量标准被用于评估模型，同时也使用人类判断来比较解码技术与其他技术。最终，结果支持了所提出的方法，并确定使用奖励模型来对齐生成回答是可行的。

    In this paper, we propose a methodology to align a medium-sized GPT model, originally trained in English for an open domain, to a small closed domain in Spanish. The application for which the model is finely tuned is the question answering task. To achieve this we also needed to train and implement another neural network (which we called the reward model) that could score and determine whether an answer is appropriate for a given question. This component served to improve the decoding and generation of the answers of the system. Numerical metrics such as BLEU and perplexity were used to evaluate the model, and human judgment was also used to compare the decoding technique with others. Finally, the results favored the proposed method, and it was determined that it is feasible to use a reward model to align the generation of responses.
    
[^19]: HuggingGPT: 在HugingFace中使用ChatGPT及其伙伴解决AI任务

    HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])

    [http://arxiv.org/abs/2303.17580](http://arxiv.org/abs/2303.17580)

    用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。

    

    解决不同领域和模态的复杂AI任务是通向人工智能的关键步骤。本文提出了一个系统，利用大型语言模型（LLMs）作为控制器来管理现有的AI模型以解决AI任务，语言成为通用接口来赋能它。具体来说，我们使用ChatGPT作为任务规划工具，根据HuggingFace中可用的模型功能描述来选择模型，在选定AI模型的情况下执行每个子任务，并总结响应。

    Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
    
[^20]: 信息抽取研究：请注意分词！

    An Information Extraction Study: Take In Mind the Tokenization!. (arXiv:2303.15100v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.15100](http://arxiv.org/abs/2303.15100)

    该研究调查了从文本中提取信息时分词的影响，发现分词模式可以带来最先进的性能。同时，基于字符的模型也有希望，因此无分词的信息抽取模型是可行的。

    

    目前，使用字符而不是分词文本作为深度学习模型输入的优势和权衡的研究已经得到了很大的发展。新的无分词模型消除了传统的分词步骤，但它们的效率仍不清楚。此外，在序列标记任务中，分词的影响相对未被探索。为此，我们调查了从文档中提取信息时分词的影响，并对基于子词和基于字符的模型进行了比较研究和分析。具体而言，我们研究了从生物医学文本中进行信息抽取 (IE)。主要结果是双重的：分词模式可能会引入归纳偏差，从而产生最先进的性能。基于字符的模型产生有希望的结果，因此，转换为无分词的IE模型是可行的。

    Current research on the advantages and trade-offs of using characters, instead of tokenized text, as input for deep learning models, has evolved substantially. New token-free models remove the traditional tokenization step; however, their efficiency remains unclear. Moreover, the effect of tokenization is relatively unexplored in sequence tagging tasks. To this end, we investigate the impact of tokenization when extracting information from documents and present a comparative study and analysis of subword-based and character-based models. Specifically, we study Information Extraction (IE) from biomedical texts. The main outcome is twofold: tokenization patterns can introduce inductive bias that results in state-of-the-art performance, and the character-based models produce promising results; thus, transitioning to token-free IE models is feasible.
    
[^21]: ChatDoctor：使用医学领域知识在LLaMA模型上微调的医疗聊天模型

    ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v1 [cs.CL])

    [http://arxiv.org/abs/2303.14070](http://arxiv.org/abs/2303.14070)

    本文介绍了一种在医学领域利用LLaMA模型微调的医疗聊天模型ChatDoctor。经过700多种疾病和其相应症状、药品和医疗检查的收集和处理，这种模型具有理解患者需求、提供建议和帮助的潜力。这些先进的语言模型集成到医疗保健中可以极大地改进医疗专业人员和患者的沟通方式。

    

    最近，在一般领域中应用的大型语言模型（LLM），例如ChatGPT，已经表现出仿佛是人类讲话般的成功。然而，这样的语言模型并没有经过个别且仔细为医学领域学习，导致诊断准确度低且不能给出正确的医疗诊断、药品等建议。为了解决这个问题，我们收集了700多种疾病及其相应症状、推荐药品和所需医疗检查，然后生成了5K名医患的对话。通过微调医患对话模型，这些模型具有了理解患者需求、提供明智建议并在各种医疗相关领域提供宝贵帮助的巨大潜力。将这些先进的语言模型集成到医疗保健中，可以彻底改变医疗专业人员和患者的沟通方式，最终改善整体质量。

    Recent large language models (LLMs) in the general domain, such as ChatGPT, have shown remarkable success in following instructions and producing human-like responses. However, such language models have not been learned individually and carefully for the medical domain, resulting in poor diagnostic accuracy and inability to give correct recommendations for medical diagnosis, medications, etc. To address this issue, we collected more than 700 diseases and their corresponding symptoms, recommended medications, and required medical tests, and then generated 5K doctor-patient conversations. By fine-tuning models of doctor-patient conversations, these models emerge with great potential to understand patients' needs, provide informed advice, and offer valuable assistance in a variety of medical-related fields. The integration of these advanced language models into healthcare can revolutionize the way healthcare professionals and patients communicate, ultimately improving the overall quality 
    
[^22]: RepoCoder：通过迭代检索和生成实现的代码存储库级别完成

    RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])

    [http://arxiv.org/abs/2303.12570](http://arxiv.org/abs/2303.12570)

    RepoCoder是一个简单、通用和有效的框架，结合了一个基于相似性的检索器和预训练的代码语言模型，在库级别代码完成流程中实现了有效利用库级别信息进行代码完成和生成。

    

    库级别代码完成任务是基于代码库更广阔上下文中继续编写未完成代码的过程。但是对于自动完成工具而言，很难利用散布在不同文件中的有用信息。我们提出了RepoCoder，这是一个简单、通用和有效的框架，可以应对这一挑战。它通过整合基于相似性的检索器和预训练的代码语言模型简化了库级别代码完成流程，从而允许有效利用库级别信息进行代码完成，并具有不同粒度层面的代码生成能力。此外，RepoCoder 还使用了一种新的迭代检索-生成模型，弥合了检索上下文和预期完成目标之间的差距。我们还提出了一个新的RepoEval基准测试，其中包含了最新和高质量真实世界的代码库，涵盖了行、API 调用和函数体完成场景。

    The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model, which allows for the effective utilization of repository-level information for code completion and grants the ability to generate code at various levels of granularity. Furthermore, RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges the gap between retrieval context and the intended completion target. We also propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion sce
    
[^23]: MEGA: 多语言生成人工智能的综合评估

    MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v1 [cs.CL])

    [http://arxiv.org/abs/2303.12528](http://arxiv.org/abs/2303.12528)

    这项研究对 33 种语言中 8 个不同任务的生成 AI 进行了全面评估，比较了生成 LLMs 和非自回归模型的表现差异。

    

    生成AI模型在许多自然语言处理任务（如语言理解、推理和语言生成）上具有令人印象深刻的性能。当今AI社区最重要的问题之一是关于这些模型的能力和限制，评估生成AI显然具有挑战性。大多数关于生成大型语言模型（LLMs）的研究都限于英语，不清楚这些模型在理解和生成其他语言方面的能力。我们提供了首个全面评估 8 项不同任务和 33 种语言的生成LLMs MEGA 的基准测试。我们还将生成LLMs的性能与这些任务上最先进的非自回归模型进行比较，以确定生成模型的表现如何与上一代LLMs相比。我们对模型的性能进行了彻底分析。

    Generative AI models have impressive performance on many Natural Language Processing tasks such as language understanding, reasoning and language generation. One of the most important questions that is being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative Large Language Models (LLMs) are restricted to English and it is unclear how capable these models are at understanding and generating other languages. We present the first comprehensive benchmarking of generative LLMs MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse tasks and 33 typologically diverse languages. We also compare the performance of generative LLMs to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of model
    
[^24]: 基于块的变压器模型的位压缩

    Block-wise Bit-Compression of Transformer-based Models. (arXiv:2303.09184v1 [cs.CL])

    [http://arxiv.org/abs/2303.09184](http://arxiv.org/abs/2303.09184)

    本论文提出了一种用于Transformer的块位压缩方法，称为BBCT。该方法可以更细粒度地压缩整个Transformer，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。在GLUE数据集上测试结果表明，BBCT可以在大多数任务中实现少于1％的准确率下降。

    

    随着BERT、GPT-3和ChatGPT等近期基于Transformer的模型的流行，自然语言处理任务中取得了最先进的性能。然而，Transformer模型的巨大计算量、巨大的内存占用和高延迟是云计算中不可避免的挑战。为了解决这个问题，我们提出了BBCT方法，它是一种用于Transformer的块位压缩方法，无需重新训练。我们的方法实现了对整个Transformer的更细粒度的压缩，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。我们以高效BERT为案例，使用BBCT方法进行压缩。我们在General Language Understanding Evaluation(GLUE)数据集上的测试结果表明，BBCT在大多数任务中的准确度下降小于1％。

    With the popularity of the recent Transformer-based models represented by BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range of natural language processing tasks. However, the massive computations, huge memory footprint, and thus high latency of Transformer-based models is an inevitable challenge for the cloud with high real-time requirement. To tackle the issue, we propose BBCT, a method of block-wise bit-compression for transformer without retraining. Our method achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient BERT with the method of BBCT. Our benchmark test results on General Language Understanding Evaluation (GLUE) show that BBCT can achieve less than 1% accuracy drop in most tasks.
    
[^25]: Truveta Mapper：一个零样本本体映射框架

    Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09767](http://arxiv.org/abs/2301.09767)

    提出了一个将无监督本体匹配或本体对齐视为翻译任务的新视角的Truveta Mapper框架，在零样本、统一和端到端的方式下执行多本体对齐。该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案，无需显式跨本体手动标注数据。

    

    本文提出了一种将无监督本体匹配(Ontology Matching, OM)或本体对齐(Ontology Alignment, OA)视为翻译任务的新视角。将本体表示为图形，在源本体图中的节点到目标本体图中的路径之间进行翻译。所提出的Truveta Mapper (TM)框架利用多任务序列到序列转换器模型，在零样本、统一和端到端的方式下执行多本体对齐。多任务使模型能够通过迁移学习来隐含地学习不同本体之间的关系，无需任何显式的跨本体手动标注数据。这也使得该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案。模型仅在公开可用的文本语料库和内部本体数据上进行预训练和微调。该方案优于现有标准基准解决方案，如Edit-Similarity和MINTE+。

    In this paper, a new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task. Ontologies are represented as graphs, and the translation is performed from a node in the source ontology graph to a path in the target ontology graph. The proposed framework, Truveta Mapper (TM), leverages a multi-task sequence-to-sequence transformer model to perform alignment across multiple ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables the model to implicitly learn the relationship between different ontologies via transfer-learning without requiring any explicit cross-ontology manually labeled data. This also enables the formulated framework to outperform existing solutions for both runtime latency and alignment quality. The model is pre-trained and fine-tuned only on publicly available text corpus and inner-ontologies data. The proposed solution outperforms state-of-the-art approaches, Edit-Similari
    
[^26]: 基于编码器-解码器语言模型的配对抗体链序列条件生成

    Conditional Generation of Paired Antibody Chain Sequences through Encoder-Decoder Language Model. (arXiv:2301.02748v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2301.02748](http://arxiv.org/abs/2301.02748)

    本论文提出了pAbT5，这是一种能够采用编码器-解码器模型为蛋白质相互作用生成配对抗体链序列的语言模型。该模型可以准确地预测配对情况，并且能够在实验验证方面表现出最先进的无监督预测能力。

    

    蛋白质语言模型在序列、结构和功能预测方面具有很好的表现。然而，目前的蛋白质语言模型仅限于单一序列的编码器或解码器结构，而许多生物学环境涉及蛋白质相互作用。本文介绍了pAbT5，它采用基于T5的架构将抗体链配对建模为正向和反向翻译。我们展示了pAbT5通过序列生成准确地反映了链的配对。我们的蛋白质语言模型可以生成可变长度的序列，其下一个词语的预测概率与序列比对的位置特异性评分矩阵一致。像蛋白质语言模型中的其他研究一样，pAbT5在实验验证方面表现出最先进的无监督预测能力。据我们所知，pAbT5是第一个用于蛋白质相互作用的生成式编码器-解码器蛋白质语言模型。

    Protein language models (LMs) have been successful in sequence, structural and functional predictions. However, currently, protein LMs are limited to encoder- or decoder-only architectures for single sequences while many biological contexts involve protein-protein interactions. Here, we introduce pAbT5, which models antibody chain pairing as forward- and back-translations using a T5-based architecture. We show that pAbT5 accurately reflects chain pairing through sequence generation. Our protein LM generates variable-length sequences and its next-word prediction probability agrees with position-specific scoring matrix from sequence alignment. Like other works in protein LM, pAbT5 performs state-of-the-art unsupervised prediction on experimental measurements. To the best of our knowledge, pAbT5 is the first generative encoder-decoder protein LM for protein-protein interactions.
    
[^27]: MedKLIP：基于医学知识增强的放射学语言-图像预训练

    MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology. (arXiv:2301.02228v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.02228](http://arxiv.org/abs/2301.02228)

    本文提出了一种基于医学知识增强的放射学语言-图像预训练方法，利用三元组提取模块和医学知识库查询实体翻译等技术，实现了医学实体之间的关系隐含以及医学诊断的能力。

    

    本文考虑利用放射学日常实践中成对的图像-文字报告增强医学视觉语言预训练（VLP）中的领域特定知识。我们提出了以下贡献：首先，不同于直接处理原始报告的现有工作，我们采用三元组提取模块提取医学相关信息，避免语言语法中的不必要复杂性并增强监督信号；其次，我们提出一种新的三元组编码模块，通过查询知识库进行实体翻译，利用医学领域丰富的领域知识，并在语言嵌入空间中隐含地建立医学实体之间的关系；第三，我们提出使用基于Transformer的融合模型，在图像补丁级别上空间对齐实体描述和视觉信号，实现医学诊断的能力；第四，我们进行了全面的实验证明，以验证该方法的有效性。

    In this paper, we consider enhancing medical visual-language pre-training (VLP) with domain-specific knowledge, by exploiting the paired image-text reports from the radiological daily practice. In particular, we make the following contributions: First, unlike existing works that directly process the raw reports, we adopt a novel triplet extraction module to extract the medical-related information, avoiding unnecessary complexity from language grammar and enhancing the supervision signals; Second, we propose a novel triplet encoding module with entity translation by querying a knowledge base, to exploit the rich domain knowledge in medical field, and implicitly build relationships between medical entities in the language embedding space; Third, we propose to use a Transformer-based fusion model for spatially aligning the entity description with visual signals at the image patch level, enabling the ability for medical diagnosis; Fourth, we conduct thorough experiments to validate the eff
    
[^28]: 输入命名实体自动解析优化问题的方法研究

    Highlighting Named Entities in Input for Auto-Formulation of Optimization Problems. (arXiv:2212.13201v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.13201](http://arxiv.org/abs/2212.13201)

    本文介绍了一种将线性规划单词问题转换为数学公式的方法。我们利用输入中的命名实体并增强输入以突出这些实体，从而实现了高准确度，赢得了NL4Opt竞赛生成赛道的第一名。

    

    运筹学是将现实世界问题建模为数学优化问题来解决的。虽然解决数学系统的问题是由分析软件完成的，但将问题作为一组数学操作进行表达通常是由领域专家手动完成的。最近的机器学习方法显示出将文本问题描述转换为相应的数学公式的前景。本文提出了一种将线性规划单词问题转换为数学公式的方法。我们利用输入中的命名实体并增强输入以突出这些实体。我们的方法在NL4Opt竞赛的所有提交中获得了最高的准确性，获得了生成赛道的第一名。

    Operations research deals with modeling and solving real-world problems as mathematical optimization problems. While solving mathematical systems is accomplished by analytical software, formulating a problem as a set of mathematical operations has been typically done manually by domain experts. Recent machine learning methods have shown promise in converting textual problem descriptions to corresponding mathematical formulations. This paper presents an approach that converts linear programming word problems into mathematical formulations. We leverage the named entities in the input and augment the input to highlight these entities. Our approach achieves the highest accuracy among all submissions to the NL4Opt Competition, securing first place in the generation track.
    
[^29]: 蒙塔古语义与神经语言模型中的修饰一致性测量

    Montague semantics and modifier consistency measurement in neural language models. (arXiv:2212.04310v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.04310](http://arxiv.org/abs/2212.04310)

    本文提出了一种用于测量神经语言模型合成行为的方法，并从形容词修饰名词短语的角度提出了三个新的合成行为测试。研究结果表明，当前的神经语言模型只在某种程度上符合预期的语言理论。

    

    在最近几年中，分布式语言表示模型已经取得了巨大的成功。同时，可解释性的需求引发了人们对它们的本质属性和能力的质疑。尤其是，分布式模型在处理自然语言的组合现象时往往不一致，这对它们的安全性和公平性具有重要的影响。尽管如此，目前大多数有关合成性的研究只是针对改善它们在相似性任务上的表现。本研究采取了不同的方法，提出了一种用于测量当代语言模型组成性行为的方法。具体而言，我们关注形容词修饰名词短语中的形容词修饰现象。我们引入了三个灵感来自蒙塔古语意的合成行为测试。我们的实验结果表明，当前的神经语言模型只在某种程度上符合预期的语言理论。

    In recent years, distributional language representation models have demonstrated great practical success. At the same time, the need for interpretability has elicited questions on their intrinsic properties and capabilities. Crucially, distributional models are often inconsistent when dealing with compositional phenomena in natural language, which has significant implications for their safety and fairness. Despite this, most current research on compositionality is directed towards improving their performance on similarity tasks only. This work takes a different approach, and proposes a methodology for measuring compositional behavior in contemporary language models. Specifically, we focus on adjectival modifier phenomena in adjective-noun phrases. We introduce three novel tests of compositional behavior inspired by Montague semantics. Our experimental results indicate that current neural language models behave according to the expected linguistic theories to a limited extent only. This
    
[^30]: Ham2Pose：将手语符号转化成姿势序列的动画方法

    Ham2Pose: Animating Sign Language Notation into Pose Sequences. (arXiv:2211.13613v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13613](http://arxiv.org/abs/2211.13613)

    该论文提出了一种将HamNoSys符号转换为手语姿势序列的方法，使用变压器编码器建立文本和姿势间的有意义的表示，可用于不同手语之间的通用翻译。此外，提出了一种新的距离测量方法可以度量手语姿势序列之间的距离。

    

    将口语翻译成手语对于聋听社区之间的开放性交流至关重要。为了实现这一目标，我们提出了第一种将HamNoSys，一种词汇手语符号，转换为手语姿势序列的动画方法。由于HamNoSys是通用设计的，我们提出的方法提供了不受目标手语限制的通用解决方案。我们的方法使用变压器编码器逐渐生成姿势预测，同时考虑它们的空间和时间信息，为训练过程提供了弱监督，并且显示我们的方法在从部分和不准确的数据中进行学习时成功。此外，我们提供了一种新的距离测量方法，考虑缺失关键点，使用DTW-MJE来测量姿势序列之间的距离。我们使用AUTSL这个大规模手语数据集来验证它的正确性，并且展示它可以度量手语之间的距离。

    Translating spoken languages into Sign languages is necessary for open communication between the hearing and hearing-impaired communities. To achieve this goal, we propose the first method for animating a text written in HamNoSys, a lexical Sign language notation, into signed pose sequences. As HamNoSys is universal by design, our proposed method offers a generic solution invariant to the target Sign language. Our method gradually generates pose predictions using transformer encoders that create meaningful representations of the text and poses while considering their spatial and temporal information. We use weak supervision for the training process and show that our method succeeds in learning from partial and inaccurate data. Additionally, we offer a new distance measurement that considers missing keypoints, to measure the distance between pose sequences using DTW-MJE. We validate its correctness using AUTSL, a large-scale Sign language dataset, show that it measures the distance betw
    
[^31]: 连续向量空间中的数学表达式语义表示

    Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08142](http://arxiv.org/abs/2211.08142)

    该论文提出了一种在连续向量空间中表示数学表达式的方法，并使用序列到序列框架的编码器生成向量表示来捕捉数学语义，比自动编码器效果更好。

    

    数学符号在STEM文献中占据了很大一部分，但是，为公式找到语义表示仍然是一个具有挑战性的问题。由于数学符号是精确的，在字符微小变化时其含义会发生显著变化，因此适用于自然文本的方法并不一定适用于数学表达式。在这项工作中，我们描述了一种在连续向量空间中表示数学表达式的方法。我们使用一个序列到序列框架的编码器，训练其在视觉上不同但在数学上等价的表达式上生成向量表示（或嵌入）。我们将这种方法与自动编码器进行比较，并表明前者更能捕捉数学语义。最后，为了促进未来的研究，我们发布了一份等价的超越和代数表达式对的语料库。

    Mathematical notation makes up a large portion of STEM literature, yet, finding semantic representations for formulae remains a challenging problem. Because mathematical notation is precise, and its meaning changes significantly with small character shifts, the methods that work for natural text do not necessarily work well for mathematical expressions. In this work, we describe an approach for representing mathematical expressions in a continuous vector space. We use the encoder of a sequence-to-sequence architecture, trained on visually different but mathematically equivalent expressions, to generate vector representations (or embeddings). We compare this approach with an autoencoder and show that the former is better at capturing mathematical semantics. Finally, to expedite future research, we publish a corpus of equivalent transcendental and algebraic expression pairs.
    
[^32]: 神经心理理论？关于大型语言模型中社交智能局限的研究

    Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. (arXiv:2210.13312v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.13312](http://arxiv.org/abs/2210.13312)

    本文研究了现代NLP系统中社交智能和心理理论的问题。作者使用两个任务评估模型在理解社交互动意图和推断参与者心理状态和现实方面的能力，结果表明当前最大的语言模型GPT-3缺乏这种能力。这一发现强调了NLP系统在理解社交动态方面的当前局限性，并需要进一步研究。

    

    社交智能和心理理论（ToM）即理解所有参与者的不同心理状态、意图和反应的能力，使得人类能够有效地理解和应对日常社交互动。随着NLP系统在越来越复杂的社交场景中得到应用，理解社交动态的能力变得至关重要。本文从实证和理论的角度探讨了现代NLP系统中社交智能和心理理论这一开放性问题。我们使用两项任务：SocialIQa（Sap et al。，2019）和ToMi（Le et al。，2019）来衡量模型理解社交互动参与者意图和反应以及推断参与者心理状态和现实的能力。结果显示，当今最大的语言模型（GPT-3；Brown et al。，2020）缺乏这种社交智能。我们的研究结果强调了NLP系统在理解社交动态方面的当前局限性以及需要进一步研究这一领域的原因。

    Social intelligence and Theory of Mind (ToM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial. In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective. We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measures models' ability to understand intents and reactions of participants of social interactions, and ToMi (Le et al., 2019), which measures whether models can infer mental states and realities of participants of situations. Our results show that models struggle substantially at these Theory 
    
[^33]: BioGPT：针对生物医学文本生成和挖掘的生成式预训练Transformer模型

    BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10341](http://arxiv.org/abs/2210.10341)

    BioGPT是一种针对生物医学领域的生成式Transformer语言模型，它在多项生物医学自然语言处理任务中均表现出色，尤其在关系提取任务中表现尤为突出。

    

    预训练语言模型在自然语言方面已经取得了很大的成功，在生物医学领域也引起了越来越多的关注。在通用语言领域中，预训练语言模型的两个主要分支是BERT（及其变体）和GPT（及其变体）。已经有很多针对生物医学领域的预训练模型，如BioBERT和PubMedBERT，在多种判别式下游生物医学任务中取得了巨大成功，但缺乏生成能力限制了它们的应用范围。本文提出了一种针对生物医学领域的生成式Transformer语言模型BioGPT，并对其进行了广泛评估，展示了我们的模型在大多数任务上优于之前的模型。特别地，在BC5CDR、KD-DTI和DDI关系提取任务上，我们分别获得了44.98％，38.42％和40.76％的F1得分，以及在PubMedQA任务上的78.2％的准确率。

    Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA
    
[^34]: MENLI: 自然语言推理的鲁棒性评估指标

    MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.07316](http://arxiv.org/abs/2208.07316)

    本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。

    

    最近被提出的基于BERT的文本生成评估指标在标准基准测试中表现良好，但易受到对信息正确性的攻击。我们认为这部分原因是此类模型是基于语义相似性建模的。相反，我们提出一种基于自然语言推理（NLI）的鲁棒性评估指标，这种指标更适合建模。我们设计了一种基于偏好的对抗性攻击框架，并表明我们的NLI基础指标比最近的BERT基础指标更具鲁棒性。在标准基准测试中，我们的NLI基础指标优于现有的摘要评估指标，但低于SOTA MT指标。然而，在现有指标与我们的NLI指标相结合时，我们既获得了更高的对抗鲁棒性（15％-30％），又获得了标准基准测试中更高的质量指标（+5％至30％）。

    Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
    
[^35]: 歌词中的性别歧视和性别偏见的大规模分析

    Large scale analysis of gender bias and sexism in song lyrics. (arXiv:2208.02052v3 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2208.02052](http://arxiv.org/abs/2208.02052)

    本文对377808首英文歌曲歌词进行大规模的自然语言处理分析，揭示了及时的性别歧视的增加以及不同性别表演者的语言偏见。

    

    我们使用自然语言处理技术分析了“Two Million Song Database”语料库中377808首英文歌曲歌词，着重分析了五十年（1960-2010）间性别歧视的表达，以及对性别偏差的评测。通过使用一个性别歧视分类器，我们在较大的规模上识别了性别歧视歌词，远超前人用手动标注流行歌曲的小样本研究。此外，通过在歌曲歌词上学习的词嵌入来衡量关联，我们揭示了性别偏见。我们发现，尤其是由男性艺术家演唱的流行歌曲中的性别歧视内容在时间上呈逐渐增多的趋势。根据表演者的性别不同，歌曲还显示出不同的语言偏见，男性独唱艺术家的歌曲中包含更多和更强的偏见。这是第一次进行这种大规模的分析，为我们揭示了流行文化这一重要部分的语言用法。

    We employ Natural Language Processing techniques to analyse 377808 English song lyrics from the "Two Million Song Database" corpus, focusing on the expression of sexism across five decades (1960-2010) and the measurement of gender biases. Using a sexism classifier, we identify sexist lyrics at a larger scale than previous studies using small samples of manually annotated popular songs. Furthermore, we reveal gender biases by measuring associations in word embeddings learned on song lyrics. We find sexist content to increase across time, especially from male artists and for popular songs appearing in Billboard charts. Songs are also shown to contain different language biases depending on the gender of the performer, with male solo artist songs containing more and stronger biases. This is the first large scale analysis of this type, giving insights into language usage in such an influential part of popular culture.
    
[^36]: DHGE：双视图超关系知识图嵌入用于链接预测和实体类型

    DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2207.08562](http://arxiv.org/abs/2207.08562)

    本文提出了一个双视图超关系知识图嵌入方法，其中DH-KG包含超关系实例视图和从实体层次抽象出的超关系本体视图，定义了链接预测和实体类型识别任务，并构建了两个DH-KG数据集JW44K-6K和HTDM。DHGE是一个基于GRAN编码器、HGNN和联合学习的DH-KG嵌入模型。

    

    在知识图谱表示学习领域中，超关系事实由一个主三元组和几个辅助的属性-值描述组成，被认为比基于三元组的事实更全面和具体。然而，当前单视图的超关系知识图嵌入方法应用受到限制，因为它们弱化了表示实体之间亲属关系的分层结构。为了克服这一限制，我们提出了一个包含超关系实例视图和从实体层次抽象出的超关系本体视图的双视图超关系知识图谱结构（DH-KG）。本文首次在DH-KG上定义了链接预测和实体类型识别任务，并构建了两个DH-KG数据集，JW44K-6K，从维基数据中提取，和基于医学数据的HTDM。此外，我们提出了DHGE，一个基于GRAN编码器、HGNN和联合学习的DH-KG嵌入模型。

    In the field of representation learning on knowledge graphs (KGs), a hyper-relational fact consists of a main triple and several auxiliary attribute-value descriptions, which is considered more comprehensive and specific than a triple-based fact. However, currently available hyper-relational KG embedding methods in a single view are limited in application because they weaken the hierarchical structure that represents the affiliation between entities. To overcome this limitation, we propose a dual-view hyper-relational KG structure (DH-KG) that contains a hyper-relational instance view for entities and a hyper-relational ontology view for concepts that are abstracted hierarchically from the entities. This paper defines link prediction and entity typing tasks on DH-KG for the first time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding model based on GRAN encoders, HGNNs, and joint learnin
    
[^37]: 答案有解：无需标记数据来训练密集检索模型

    Questions Are All You Need to Train a Dense Passage Retriever. (arXiv:2206.10658v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.10658](http://arxiv.org/abs/2206.10658)

    ART是一种能够不使用标记数据进行训练的密集检索模型，并且只需要访问未配对的输入和输出。它使用一个新的文档检索自编码方案，通过问题重构进行检索训练，可以有效地进行无监督学习，并且可以将其合并到完整的Open QA系统中。

    

    我们引入了ART，一种新的语料库级自编码方法，用于训练密集检索模型，不需要任何标记的训练数据。在开放域任务中，如开放式问答（Open QA）中，密集检索是一个中心挑战，其中最先进的方法通常需要大量的监督数据集，具有自定义的硬负面挖掘和正面示例去噪声。与此相反，ART只需要访问未配对的输入和输出（例如，问题和潜在答案文件）。它使用一种新的文档检索自编码方案，其中（1）输入问题用于检索一组证据文档，（2）然后使用文档计算重构原始问题的概率。基于问题重构的检索训练可以有效地进行无监督学习，包括文档和问题编码器，可以稍后将其合并到完整的Open QA系统中，而不需要进一步进行微调。广泛的实验表明，ART获得了良好的性能。

    We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g. questions and potential answer documents). It uses a new document-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence documents, and (2) the documents are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both document and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtain
    
[^38]: 稀疏*BERT：稀疏模型能够泛化到新的任务和领域（翻译自arXiv:2205.12452v2 [cs.CL] UPDATED）

    Sparse*BERT: Sparse Models Generalize To New tasks and Domains. (arXiv:2205.12452v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12452](http://arxiv.org/abs/2205.12452)

    本文研究了使用渐进非结构化幅值修剪进行修剪的模型如何在领域和任务之间进行转移。使用遮蔽语言模型进行预训练的被修剪模型能够在不进行广泛的超参数探索或专门方法的情况下转移到新的领域和任务。在生物医学NLP任务中，Sparse*BERT可以达到或超过BioBERT的性能。

    

    大型语言模型已经成为大多数现代自然语言处理（NLP）系统的核心架构。这些模型可以在任务和领域之间始终提供卓越的准确性和鲁棒性，但其高计算开销可能会使推理变得困难和昂贵。为了使使用这些模型成本更低，近期的研究探讨了利用结构化和非结构化修剪、量化和蒸馏来提高推理速度并减小模型大小。本文研究了使用渐进非结构化幅值修剪进行修剪的模型如何在领域和任务之间进行转移。我们的实验表明，使用遮蔽语言模型进行预训练的被修剪模型能够在不进行广泛的超参数探索或专门方法的情况下转移到新的领域和任务。我们演示了我们的稀疏通用模型Sparse*BERT可以通过在非结构化生物医学文本上预训练压缩的架构而成为SparseBioBERT，并且在多种生物医学NLP任务中可以达到或超过BioBERT的性能。

    Large Language Models have become the core architecture upon which most modern natural language processing (NLP) systems build. These models can consistently deliver impressive accuracy and robustness across tasks and domains, but their high computational overhead can make inference difficult and expensive. To make using these models less costly, recent work has explored leveraging structured and unstructured pruning, quantization, and distillation to improve inference speed and decrease size. This paper studies how models pruned using Gradual Unstructured Magnitude Pruning can transfer between domains and tasks. Our experimentation shows that models that are pruned during pretraining using general domain masked language models can transfer to novel domains and tasks without extensive hyperparameter exploration or specialized approaches. We demonstrate that our general sparse model Sparse*BERT can become SparseBioBERT simply by pretraining the compressed architecture on unstructured bi
    
[^39]: 用零样本问题生成提高段落检索

    Improving Passage Retrieval with Zero-Shot Question Generation. (arXiv:2204.07496v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.07496](http://arxiv.org/abs/2204.07496)

    本论文提出了一种基于零样本问题生成模型的重新排序方法，适用于开放式问题回答中的段落检索，可以适用于任何检索方法，无需特定训练，可在多个数据集中提高检索准确性，可以帮助提高开放域问答的性能。

    

    我们提出了一种简单而有效的重新排序方法，以提高开放式问题回答中的段落检索。重新排序器使用零样本问题生成模型重新评分，该模型使用预训练的语言模型计算基于所检索到的段落的输入问题的概率。该方法可以应用于任何检索方法（如神经或基于关键字的检索），不需要任何域或任务特定的训练（因此，预计更好地推广到数据分布转移），并提供了查询和段落之间的丰富交叉注意（即必须解释问题中的每个令牌）。在多个开放域检索数据集的评估中，我们的重新排序器将强大的无监督检索模型中提高了6％-18％，并将强大的监督模型中的前20个段落检索的准确性提高高达12％。我们还通过简单地添加新的重新排序器来获得了新的开放域问答的最新结果。

    We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned on a retrieved passage. This approach can be applied on top of any retrieval method (e.g. neural or keyword-based), does not require any domain- or task-specific training (and therefore is expected to generalize better to data distribution shifts), and provides rich cross-attention between query and passage (i.e. it must explain every token in the question). When evaluated on a number of open-domain retrieval datasets, our re-ranker improves strong unsupervised retrieval models by 6%-18% absolute and strong supervised models by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new state-of-the-art results on full open-domain question answering by simply adding the new r
    
[^40]: 电影叙述摘要：一个用于故事理解的视频语言数据集

    Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.05711](http://arxiv.org/abs/2203.05711)

    这个研究收集并发布了一个视频-语言故事数据集SYMON，用于推进多模态故事理解的发展。

    

    尽管AI有了最近的进展，但故事理解仍然是一个未被充分研究的问题。我们收集、预处理并公开发布了一个视频语言故事数据集SYMON，其中包含5,193个流行电影和电视剧的视频摘要。SYMON捕捉了由人类创作者制作的面向人类观众的自然故事叙述视频。作为一个原型和自然故事数据集，SYMON具有高覆盖的多模态故事事件、丰富的心理状态描述和视觉和文本模态之间的大语义差距。我们建立了视频文本检索和电影摘要视频的零样本对齐的基准，展示了在故事理解中领域内数据的重要性。通过SYMON，我们希望为多模态故事理解的进展打下基础。

    Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SYMON), containing 5,193 video summaries of popular movies and TV series. SYMON captures naturalistic story-telling videos for human audience made by human creators. As a prototypical and naturalistic story dataset, SYMON features high coverage of multimodal story events, abundant mental-state descriptions, and large semantic gaps between the visual and the textual modalities. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data in story understanding. With SYMON, we hope to lay the groundwork for progress in multimodal story understanding.
    
[^41]: 词语的“自我网络”中的结构不变量与语义指纹

    Structural invariants and semantic fingerprints in the "ego network" of words. (arXiv:2203.00588v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2203.00588](http://arxiv.org/abs/2203.00588)

    本文探究了词语的“自我网络”，通过分析Twitter上的异构数据集，发现词汇使用存在同心圆层次结构，各层大小呈规则性增长。

    

    人类社交行为的认知模型表明，由于认知限制，人类会按照一种规则的结构组织社交关系。本文假设类似的规则性也存在于其他涉及语言产生的认知过程中。为了探究这一假设，我们分析了一个包含Twitter用户的异构数据集（包括普通用户和专业写手）。通过利用一种类似于揭示社交认知限制的方法，我们在结构和语义层面上发现了规则性。在结构层面上，我们发现一种称为“词语自我网络”的同心圆层次结构可以很好地捕捉个体组织所使用的词汇。这种结构中各层大小呈规则性增长（约2-3）。

    Well-established cognitive models coming from anthropology have shown that, due to the cognitive constraints that limit our "bandwidth" for social interactions, humans organize their social relations according to a regular structure. In this work, we postulate that similar regularities can be found in other cognitive processes, such as those involving language production. In order to investigate this claim, we analyse a dataset containing tweets of a heterogeneous group of Twitter users (regular users and professional writers). Leveraging a methodology similar to the one used to uncover the well-established social cognitive constraints, we find regularities at both the structural and semantic level. At the former, we find that a concentric layered structure (which we call ego network of words, in analogy to the ego network of social relationships) very well captures how individuals organise the words they use. The size of the layers in this structure regularly grows (approximately 2-3 
    

