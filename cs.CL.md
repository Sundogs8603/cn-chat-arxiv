# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [H2O Open Ecosystem for State-of-the-art Large Language Models.](http://arxiv.org/abs/2310.13012) | H2O推出了开放生态系统，旨在开发和测试最先进的大规模语言模型（LLMs），包括h2oGPT和H2O LLM Studio。这一开源项目提供了全面开放的替代方案，能够帮助推动人工智能的发展，使其更加可信赖和可访问。 |
| [^2] | [On the Representational Capacity of Recurrent Neural Language Models.](http://arxiv.org/abs/2310.12942) | 本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。 |
| [^3] | [A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models.](http://arxiv.org/abs/2310.12936) | 本研究对39个不同模型大小、训练目标、分词方法、训练数据领域和语言的预训练语言模型进行了全面研究，发现分词和模型目标等因素对MLMs学习的社会偏见具有重要影响。 |
| [^4] | [StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding.](http://arxiv.org/abs/2310.12874) | 本文通过构建故事级类比语料库StoryAnalogy评估了大型语言模型在识别和生成类比任务上的能力，发现这些任务对于句子嵌入模型和最近的大型语言模型来说都非常具有挑战性。同时，研究发现通过使用StoryAnalogy中的数据可以提高大型语言模型的类比生成质量。 |
| [^5] | [AgentTuning: Enabling Generalized Agent Abilities for LLMs.](http://arxiv.org/abs/2310.12823) | 本论文提出了AgentTuning，一种简单而通用的方法，可提升LLMs的代理能力，同时保持其通用能力。通过构建AgentInstruct数据集，并采用一种混合训练方法，作者成功地实现了提高LLMs代理能力的目标。 |
| [^6] | [Towards Real-World Streaming Speech Translation for Code-Switched Speech.](http://arxiv.org/abs/2310.12648) | 本文针对面向真实世界的混合语音流式翻译进行了研究，主要关注了流式设置和翻译到第三种语言的问题，并提出了一种基线模型进行了实验。 |
| [^7] | [Pretraining Language Models with Text-Attributed Heterogeneous Graphs.](http://arxiv.org/abs/2310.12580) | 本文提出了一个新的语言模型预训练框架，能够明确考虑到文本属性异构图中的拓扑和异构信息。通过优化语言模型和辅助的异构图神经网络，预测了文本属性异构图中的节点。同时，还设计了一个文本丰富性加权的节点抽样策略，以更好地利用文本信息。 |
| [^8] | [Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures.](http://arxiv.org/abs/2310.12074) | 本文介绍了一个名为IncidentAI的新数据集，用于安全预防。该数据集包含三个任务，并由高压气体保护管理领域的专家进行了注释。初步结果显示，使用自然语言处理技术有助于分析事故报告以预防未来的故障。这个数据集将促进NLP和事故管理领域的未来研究。 |
| [^9] | [Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education.](http://arxiv.org/abs/2310.12059) | 本研究评估了大型语言模型在越南普通教育中对多项选择题符号绑定能力的能力，并创建了一个新颖且高质量的数据集来评估语言模型的符号绑定能力。 |
| [^10] | [LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation.](http://arxiv.org/abs/2310.12020) | LoHoRavens是一个针对机器人桌面操作的长时程语言条件基准测试，涵盖颜色、大小、空间、算术和引用等各种推理方面。本研究还探索了在机器人执行过程中如何将观测反馈纳入到大型语言模型的闭环规划中的两种方法。 |
| [^11] | [Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences.](http://arxiv.org/abs/2310.11960) | 提出了一种名为快速多极化注意力的新型注意力机制，它使用分治策略将注意力的时间和内存复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。 |
| [^12] | [From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification.](http://arxiv.org/abs/2310.11878) | 本研究关注法律自然语言处理中人工标注的变异问题，通过收集一组律师对案件结果评估存在分歧的数据集，对这些分歧进行了研究，构建了一个两级分类体系，并发现分歧主要源于对法律背景的不明确描述。 |
| [^13] | [Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling.](http://arxiv.org/abs/2310.11772) | 本论文通过增强一致性建模的方式改进了长文档主题划分模型。具体地，通过引入主题感知的句子结构预测和对比语义相似性学习，该模型在捕捉语义一致性和主题划分之间的深层关系方面有了更好的表现。 |
| [^14] | [VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights.](http://arxiv.org/abs/2310.11368) | VECHR是一个专家注释的多标签数据集，用于欧洲人权法院漏洞类型的可解释和鲁棒分类。该数据集帮助识别脆弱性，并提供解释理由。结果显示了该任务的挑战性，模型与专家的一致性有限，模型在处理域外数据时鲁棒性也较低。 |
| [^15] | [VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System.](http://arxiv.org/abs/2310.11069) | VoxArabica是一个稳健的方言感知阿拉伯语音识别系统，通过开发和演示，实现了阿拉伯语方言识别和自动语音识别。该系统训练了各种模型用于不同方言的识别，并提供了多种功能的网络界面。 |
| [^16] | [Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT.](http://arxiv.org/abs/2310.10903) | ChatGPT在第二语言写作中扮演重要角色，能够有效地与语言学习者合作，在保持个人表达声音的同时提升写作能力。这项研究为ChatGPT在学术写作中的应用提供了重要的探索。 |
| [^17] | [BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys.](http://arxiv.org/abs/2310.10765) | 提出了一种新颖的方法BiomedJourney，通过指导学习多模态患者旅程，进行反事实生物医学图像生成。使用GPT-4处理图像报告生成疾病进展的自然语言描述，并训练潜在扩散模型。 |
| [^18] | [Theory of Mind for Multi-Agent Collaboration via Large Language Models.](http://arxiv.org/abs/2310.10701) | 本研究通过在多智能体合作游戏中评估基于大型语言模型的智能体，发现它们可以表现出协作行为和高级理论推理能力，并通过使用明确的信念状态表示来提高任务性能和理论推理准确性。 |
| [^19] | [Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation.](http://arxiv.org/abs/2310.10698) | 本文提出了一种“语义思维链”的方法，用于在代码生成中引入语义信息，并以此来实现更精细的代码理解和表示。 |
| [^20] | [In-Context Pretraining: Language Modeling Beyond Document Boundaries.](http://arxiv.org/abs/2310.10638) | 本论文提出了一种超越文档边界的上下文预训练方法，通过在相关文档序列上训练语言模型，鼓励模型进行跨文档的阅读和推理。该方法通过改变文档顺序并应用现有的预训练管道来实现。 |
| [^21] | [RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling.](http://arxiv.org/abs/2310.10567) | RegaVAE是一种基于变分自编码器的检索增强语言模型，它通过将文本编码到潜在空间中捕获当前和未来的信息，并使用潜在变量进行有效的信息聚合。 |
| [^22] | [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models.](http://arxiv.org/abs/2310.10378) | 本论文研究了多语言预训练语言模型中事实知识的跨语言一致性，提出了一种新的度量方法，并通过分析模型大小、语言配对等因素发现了影响一致性的因素。实验结果表明，增加模型大小可以提高准确性，但不会改善跨语言一致性。 |
| [^23] | [AdaLomo: Low-memory Optimization with Adaptive Learning Rate.](http://arxiv.org/abs/2310.10195) | AdaLomo是一种低内存优化方法，通过引入自适应学习率来改善大型语言模型优化器的性能，同时保持内存效率。 |
| [^24] | [In-Context Learning with Iterative Demonstration Selection.](http://arxiv.org/abs/2310.09881) | 这项研究提出了一种基于迭代示范选择的上下文学习方法，通过使用零样本链式思维推理来选择与测试样本不同但仍与之强相关的示范作为学习的上下文。 |
| [^25] | [Merging Experts into One: Improving Computational Efficiency of Mixture of Experts.](http://arxiv.org/abs/2310.09832) | 本文提出了一种名为“合并专家”的计算高效的方法，通过将计算成本降低到单个专家的水平来改进混合专家方法的计算效率，实验证明该方法显著提高了计算效率。 |
| [^26] | [Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents.](http://arxiv.org/abs/2310.09343) | 本论文提出了一种对话思路提炼的知识提炼框架，利用大型语言模型作为教师，并通过对齐过滤器选择性地提炼一致和有用的理由。同时，介绍了一个可靠的对话思路推理器，用于生成响应。 |
| [^27] | [A Zero-Shot Language Agent for Computer Control with Structured Reflection.](http://arxiv.org/abs/2310.08740) | 这种论文提出了一种零样本语言代理机制，它不需要专家示踪，并且通过自我反思和结构化思考管理来学习和改善计算机上的控制，表现出高效的推理能力。 |
| [^28] | [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models.](http://arxiv.org/abs/2310.08659) | 本论文提出了LoftQ：一种针对大型语言模型的LoRA精调感知量化框架。该框架同时对LLM进行量化，并为LoRA精调找到适当的低秩初始化，以缓解量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。 |
| [^29] | [Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation.](http://arxiv.org/abs/2310.08395) | 本文提出了一种使用思路链（CoT）对大型语言模型进行少样本知识库问题生成的方法，该方法将问题生成任务形式化为推理问题，并通过检索支持性逻辑形式和编写提示来实现生成过程。 |
| [^30] | [Fine-grained Conversational Decoding via Isotropic and Proximal Search.](http://arxiv.org/abs/2310.08130) | 本论文提出了一种细粒度的对话解码方法，通过各向同性和近端搜索（IPS）生成信息集中的语义回应，并在对话领域的评估中取得了优于现有方法的效果。 |
| [^31] | [Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models.](http://arxiv.org/abs/2310.07611) | 本论文通过提出自我改进的开源模型和一个新的排名指标（PeRFICS）来解决性能和成本之间的权衡问题。实验证明，所提出的方法可以在各种大小的开源模型中显著提高性能，并为开源LLMs的发展提供了一种成本优化的方法。 |
| [^32] | ["A Tale of Two Movements": Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction.](http://arxiv.org/abs/2310.07155) | 本论文提出了一种基于图的弱监督方法，用于识别和比较#BlackLivesMatter和#BlueLivesMatter运动相关推文中的观点。通过将文本转换为图，并根据作者的社交网络进行结构化预测，该方法能够在缺乏标注数据的情况下模拟运动中的观点。 |
| [^33] | [Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting.](http://arxiv.org/abs/2310.07081) | 本文通过引入检索增强和损失加权两种方法，成功改进了机器翻译系统对于惯用表达的翻译准确性。 |
| [^34] | [Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE.](http://arxiv.org/abs/2310.06918) | 本研究提出了一种聚焦-信息熵函数，利用硬负样本挖掘改进了对比学习句子嵌入的方法，并在各种基准测试中验证了其性能提升。 |
| [^35] | [Humans and language models diverge when predicting repeating text.](http://arxiv.org/abs/2310.06408) | 人类和语言模型在预测重复文本时存在分歧，虽然在文本片段第一次呈现时表现一致，但当记忆（或上下文学习）开始发挥作用时，表现快速分歧。研究发现这种分歧源于中间层的特定注意力头，并通过加入幂律近期偏好使模型更接近人类行为。 |
| [^36] | [Hexa: Self-Improving for Knowledge-Grounded Dialogue System.](http://arxiv.org/abs/2310.06404) | 本论文提出了一种自我提升的方法，用于改进知识驱动对话生成的中间步骤的生成性能。通过引入引导提示和修改损失函数的自举策略，提高了生成自动生成回答的多样性，并在各种基准数据集上实验证明了该方法的有效性。 |
| [^37] | [Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models.](http://arxiv.org/abs/2310.06374) | 本文通过系统分析了基于预训练语言模型的关键词生成任务中模型选择和解码策略的影响。并发现传统的模型选择智慧缺乏深度，并且在关键词生成中贪婪搜索的召回率较低。 |
| [^38] | [An Attribution Method for Siamese Encoders.](http://arxiv.org/abs/2310.05703) | 本文提出了一种适用于Siamese编码器的局部归因方法，通过将集成梯度原理推广到具有多个输入的模型，该方法能够解释句子转换器模型中重要的预测令牌对，主要集中在名词和动词上。 |
| [^39] | [Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance.](http://arxiv.org/abs/2310.05597) | 本文研究了语言模型是否能够学习类比推理的任务，并测试了几种学习方法。实验结果表明，模型能够通过少量数据学习类比推理，并在与人类基准进行比较后接近人类的表现水平。 |
| [^40] | [InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations.](http://arxiv.org/abs/2310.05592) | 本文介绍了一种名为InterroLang的对话式解释工具，通过对话界面帮助用户以情境化的方式探索具有解释的NLP模型和数据集，支持澄清和后续问题，并通过自然语言接口进行交互。研究中还提出了新的NLP操作，并在三个NLP任务上展示了其泛化能力。进行了用户研究评估工具的正确性、有用性和可模拟性。 |
| [^41] | [Establishing Trustworthiness: Rethinking Tasks and Model Evaluation.](http://arxiv.org/abs/2310.05442) | 重新思考自然语言处理中的任务和模型评估，以应对通用目标无关方法引发的挑战，并且关注在不断增加的实际应用中建立可信度的需求。 |
| [^42] | [Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization.](http://arxiv.org/abs/2310.05317) | 该论文提出了一种任务自适应分词的方法，通过优化分词过程来增强在心理健康领域中的长文本生成。实验证明，该方法在减少标记数量的情况下显著提高了生成性能，并且可与大型语言模型结合使用。 |
| [^43] | [Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems.](http://arxiv.org/abs/2310.05280) | 这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。 |
| [^44] | [DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models.](http://arxiv.org/abs/2310.05074) | DialCoT是一种对话引导的链式思维方法，用于在较小的语言模型中分解和探索推理路径。通过将复杂问题分解为简单的子问题，它降低了任务难度，并使用PPO算法优化模型的推理路径选择。 |
| [^45] | [Guideline Learning for In-context Information Extraction.](http://arxiv.org/abs/2310.05066) | 本文提出了一种用于上下文信息抽取的指南学习框架，通过反思性地学习和遵循指南，以更好地理解任务并提高性能。 |
| [^46] | [Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU.](http://arxiv.org/abs/2310.04928) | 通过对印度尼西亚历史、文化和语言进行的测试，我们发现大型语言模型只能通过印尼的中小学水平，对印尼地方语言和文化的了解有限。 |
| [^47] | [LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model.](http://arxiv.org/abs/2310.04445) | 本文提出了一种名为LoFT的方法，通过在与有害查询处于词汇-语义邻域的相似查询上进行代理模型的微调，来改善对大型语言模型的对抗攻击的可传递性。 |
| [^48] | [Evaluating Hallucinations in Chinese Large Language Models.](http://arxiv.org/abs/2310.03368) | 本研究评估了中文大型语言模型中的幻觉现象，通过建立HalluQA基准测试和使用GPT-4进行自动评估方法，发现18个模型的非幻觉率低于50%。研究分析了不同类型模型中的幻觉类型和原因。 |
| [^49] | [Conversational Health Agents: A Personalized LLM-Powered Agent Framework.](http://arxiv.org/abs/2310.02374) | 该论文介绍了一个基于LLM的会话式健康代理框架，旨在为代理赋予批判性思维、知识获取和问题解决能力，实现个性化的健康护理服务。该框架能够无缝集成医疗工具，实现多语言和多模态对话，并与多种用户数据分析工具连接。 |
| [^50] | [Improving Dialogue Management: Quality Datasets vs Models.](http://arxiv.org/abs/2310.01339) | 这项工作认为，对话管理器在达到最佳性能方面的主要问题在于数据集的质量，而不是采用的模型。研究发现最广泛使用的数据集中存在的错误是导致对话管理失败的主要原因。 |
| [^51] | [GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond.](http://arxiv.org/abs/2309.16583) | GPT-Fathom是一个用于评估大型语言模型的开源套件，它系统评估了10多个主要的语言模型，并提供了从GPT-3到GPT-4演化路径的宝贵见解。 |
| [^52] | [Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models.](http://arxiv.org/abs/2309.12075) | 本研究通过Benchmark测试，发现使用Prompt Tuning的预训练语言模型在多标签文本分类任务中具有较好的性能和计算效率。同时，提出了使用Trie搜索来解决生成标签匹配问题的限制。 |
| [^53] | [Towards Effective Disambiguation for Machine Translation with Large Language Models.](http://arxiv.org/abs/2309.11668) | 本文研究了大型语言模型(LLMs)在翻译歧义句子方面的能力，并通过上下文学习和歧义数据集微调提出了改进处理歧义的方法。实验证明，这些方法在多个语言方向上有着与最先进系统相当甚至超越的表现。这些研究为机器翻译的有效消歧提供了宝贵的见解。 |
| [^54] | [Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts.](http://arxiv.org/abs/2309.07430) | 本研究通过对八个大型语言模型在临床摘要任务上的领域适应方法实验进行了全面的定量评估，发现最佳适应的模型的摘要在完整性和正确性方面优于人类摘要。 |
| [^55] | [Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning.](http://arxiv.org/abs/2309.06553) | 这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。 |
| [^56] | [One Wide Feedforward is All You Need.](http://arxiv.org/abs/2309.01826) | 本文探讨了Transformer架构中前馈神经网络（FFN）的作用，发现尽管它占据了模型很大一部分的参数，但它是冗余的。通过移除解码器层的FFN并在编码器上共享单个FFN，我们能够显著减少参数数量并实现准确性和延迟上的显著提升。 |
| [^57] | [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models.](http://arxiv.org/abs/2308.13137) | OmniQuant是一种用于大型语言模型的全向校准量化技术，通过优化各种量化参数实现了良好的性能，并保持了计算效率。 |
| [^58] | [AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors.](http://arxiv.org/abs/2308.10848) | AgentVerse提出了一个多智能体框架，可以通过协同和动态调整合作团队的组成，实现超越单个智能体的性能。在合作任务中，该框架能够引发出群体内个体智能体之间的社会行为，从而提高多智能体团队的协作潜力。 |
| [^59] | [End-to-End Evaluation for Low-Latency Simultaneous Speech Translation.](http://arxiv.org/abs/2308.03415) | 本文提出了一个端到端的评估框架，用于评估低延迟语音翻译的各个方面。通过该框架，我们比较了不同方法的性能，并进行了全面的评估。 |
| [^60] | [Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models.](http://arxiv.org/abs/2308.01684) | "Baby's CoThought" 提出了一种利用大型语言模型重组数据训练紧凑语言模型的方法，经过评估发现，在10个语言学、NLU和问答任务中，BabyLM的表现超过RoBERTa-base超过3个点，展现出更好的上下文信息提取能力。 |
| [^61] | [Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes.](http://arxiv.org/abs/2307.15455) | 提出了一种基于Trie上下文增强的个性化查询自动补全算法，可以解决短前缀和未见前缀的问题，并有效利用历史查询的流行度信息。 |
| [^62] | [Large language models shape and are shaped by society: A survey of arXiv publication patterns.](http://arxiv.org/abs/2307.10700) | 大型语言模型的论文数量急剧增加，研究重点逐渐转向社会影响。与LLM相关的论文呈现持续增长的趋势，新发表关于LLM的作者更注重应用和社会影响。 |
| [^63] | [Zero-shot Query Reformulation for Conversational Search.](http://arxiv.org/abs/2307.09384) | 提出了一种零样本查询重构（ZeQR）框架，通过利用机器阅读理解任务的语言模型来解决对话搜索中的数据稀疏性、解释性不足和歧义的问题。 |
| [^64] | [Classifying Crime Types using Judgment Documents from Social Media.](http://arxiv.org/abs/2306.17020) | 本文提出了一种通过NLP处理方法的新的训练模型，通过生成新样本来平衡不均匀的数据集分布的缺陷，并使用预训练和微调来赋予模型对小数据集的良好泛化能力。 |
| [^65] | [Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers.](http://arxiv.org/abs/2306.14003) | 本文提出了一个弱监督的科技论文多标签分类框架FUTEX，该框架利用跨论文网络结构和各投稿内部分章节的层次结构，解决了在细粒度标签空间中将论文分类为研究主题和主题，可能有多个；应利用全文来补充论文标题和摘要以进行分类等挑战。 |
| [^66] | [GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning.](http://arxiv.org/abs/2306.13089) | 本研究提出了一种名为GIMLET的统一图文模型，用于在零样本设置下使用自然语言指令完成分子相关任务。我们解决了现有模型的指令处理不足和图形容量有限的问题，并证明了使用GIMLET能够增强图形特征的泛化能力。 |
| [^67] | [TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models.](http://arxiv.org/abs/2306.06815) | 本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。 |
| [^68] | [Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset.](http://arxiv.org/abs/2306.03030) | 该研究介绍了 CMExam 数据集，这是一个综合的、来自于中国国家医疗执业考试的数据集，为评估大型语言模型提供了一个标准化且客观的方法。在 CMExam 上，GPT-4 表现最好，这表明大型语言模型在医学领域有巨大潜力。 |
| [^69] | [LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers.](http://arxiv.org/abs/2305.18396) | 本文中，研究人员通过使用隐私计算友好的近似方法替换transformer架构中计算和通信密集的运算符，实现了大幅降低私有推断成本的效果，并在保持准确性的前提下实现了计算加速和通信开销降低。 |
| [^70] | [LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections.](http://arxiv.org/abs/2305.18287) | 本文首次展示了如何通过使用无标签图像集和自动生成的大量语言模型文本来缩小零样本分类器与有标签分类器之间的性能差距，从而在无需任何标签和配对的情况下提高了零样本分类器的性能。 |
| [^71] | [Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models.](http://arxiv.org/abs/2305.16426) | 本文通过研究标量副词，探究了预训练语言模型中“绝对”与“相对”词的表现，在涉及逻辑推理的NLP应用中面临挑战。 |
| [^72] | [Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model.](http://arxiv.org/abs/2305.16340) | 本文提出了一种分段循环Transformer（SRformer）来减少计算/内存成本，并使用RAF层处理跨段的信息，从而提高序列处理能力。 |
| [^73] | [Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy.](http://arxiv.org/abs/2305.15294) | 本文提出了一种被称为迭代检索-生成的方法，通过迭代地协同检索和生成，改善了检索增强的大型语言模型的性能。 |
| [^74] | [Meta-Learning Online Adaptation of Language Models.](http://arxiv.org/abs/2305.15076) | 提出了一种元学习方法，通过学习如何分配语言模型损失的权重，实现在在线微调过程中延长语言模型的有效时间，并提升问题回答能力。 |
| [^75] | [Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models.](http://arxiv.org/abs/2305.15074) | 这项研究提出了JEEBench，一个更具挑战性的基准数据集，用于评估大型语言模型的问题解决能力。通过评估各种模型，结果显示目前最好的模型在解决问题时存在代数操作错误、抽象概念转化不准确和难以检索相关概念等问题。 |
| [^76] | [AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models.](http://arxiv.org/abs/2305.15064) | 本文提出了一种名为AutoPlan的方法，利用大型语言模型（LLM）来指导代理完成复杂的交互式决策任务。通过将LLM提示与任务解决计划相结合并进行优化，AutoPlan在没有上下文演示的情况下，在ALFWorld上实现了与人类撰写的演示基线相当的成功率，并在HotpotQA上超过了8%。 |
| [^77] | [Is GPT-4 a Good Data Analyst?.](http://arxiv.org/abs/2305.15038) | 本研究探讨了GPT-4是否是一个优秀的数据分析师，并通过面对面比较研究的方式来回答这个问题。 |
| [^78] | [Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations.](http://arxiv.org/abs/2305.15035) | Self-ICL是一个简单的框架，通过自我生成示范的方式实现零样本上下文学习。它利用大型语言模型内在的能力来生成伪输入并预测伪标签，然后将这些伪输入和标签作为示范用于目标任务的学习。通过在23个难任务上的评估，Self-ICL在平均准确率和头对头比较方面表现优于其他零样本方法。 |
| [^79] | [ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories.](http://arxiv.org/abs/2305.15028) | 本论文提出了ImageNetVC数据集，用于对1000个ImageNet类别进行零次和少次远距离视觉常识评估。通过该评估，分析了目前大型语言模型和视觉增强模型对视觉常识知识的掌握程度，为丰富语言模型的视觉常识知识提供了见解。 |
| [^80] | [Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems.](http://arxiv.org/abs/2305.15017) | 该论文介绍了Calc-X和Calcformers，它们通过与符号系统的交互使语言模型在算术推理任务中表现更准确，提高了生成正确结果的准确性。 |
| [^81] | [Reasoning with Language Model is Planning with World Model.](http://arxiv.org/abs/2305.14992) | 本文介绍了一种新的大型语言模型推理框架RAP，通过构建内部的世界模型并模拟长期行动结果，从而使语言模型能够进行像人类大脑一样的有意识规划。 |
| [^82] | [GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP.](http://arxiv.org/abs/2305.14976) | 这项研究对ChatGPT在阿拉伯语自然语言处理领域进行了全面评估，发现尽管在英文上表现出色，但在阿拉伯语上的性能不如经过阿拉伯语微调的较小模型。 |
| [^83] | [Universal Self-Adaptive Prompting.](http://arxiv.org/abs/2305.14926) | 本研究通过介绍一种专门为零样本学习而设计的自动提示设计方法，解决了现有大型语言模型零样本性能较弱的问题。这种方法只需要少量无标签数据和一个推理模型，具有高度灵活性和通用性。 |
| [^84] | [Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory.](http://arxiv.org/abs/2305.14889) | 本文提出了一个基于测量理论的框架MetricEval，用于评估自然语言生成（NLG）的评估指标。通过该框架，可以量化指标的不确定性，并解决人工评估的效度结构混淆和基于LLM的指标的可靠性问题。 |
| [^85] | [Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification.](http://arxiv.org/abs/2305.14794) | 本文重新审视了基于种子匹配的伪标签生成方法，并通过简单的单词删除来缓解因规则注入的标签偏见而带来的影响，提高该方法的性能，其性能达到甚至超过最先进技术。 |
| [^86] | [Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via Adaptive Subnetwork Optimization.](http://arxiv.org/abs/2305.14760) | 本文提出了一种动态微调策略Bi-Drop来针对预训练语言模型在大规模微调时可能出现的过拟合现象。经过GLUE基准测试，Bi-Drop的性能优于其他微调方法，并对于多任务、多领域转移、数据不均衡和低资源情况下也表现出强大的鲁棒性。 |
| [^87] | [Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting.](http://arxiv.org/abs/2305.14755) | 本文研究提出了在文体改写的重写和评估阶段整合文本上下文的必要性，并通过few-shot prompting比较非上下文改写和上下文改写的效果。研究发现，自动度量指标不一定能反映出人类的偏好。 |
| [^88] | [ECHo: Event Causality Inference via Human-centric Reasoning.](http://arxiv.org/abs/2305.14740) | ECHo是一个基于人类中心推理的事件因果推断数据集，并提出了一个与CoT范式对齐的统一框架来评估当前AI系统的推理能力。 |
| [^89] | [Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection.](http://arxiv.org/abs/2305.14735) | 本文提出了一种基于异常值的方法，用于识别在毒性检测中受到伤害的人群，发现对于这些异常值，模型性能较差，他们面临的毒性更高。 |
| [^90] | [Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning.](http://arxiv.org/abs/2305.14711) | 本文研究了模型评估度量中的性别偏见对图像字幕任务的影响，并提出了替代方案以解决这一问题。 |
| [^91] | [You Are What You Annotate: Towards Better Models through Annotator Representations.](http://arxiv.org/abs/2305.14663) | 提出了一种方法，通过创建注释者和注释的矩阵表示以捕捉其特点，并在建模过程中使用它们来显著提高自然语言处理模型的性能，同时帮助民主化人工智能。 |
| [^92] | [COMET-M: Reasoning about Multiple Events in Complex Sentences.](http://arxiv.org/abs/2305.14617) | 提出了COMET-M，该模型可以推理复杂句子中多个事件之间的关系以及生成常识推断，并在35K个人类编写的推断上进行训练，相对于之前的COMET模型在生成多事件推断方面有显着的性能改进。 |
| [^93] | [MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems.](http://arxiv.org/abs/2305.14536) | MathDial是一个由实际教师和大型语言模型半合成生成的对话数据集，旨在解决自动对话辅导工具缺少高质量数据的问题。这个数据集关注通过引导学生使用问题来探索数学问题。 |
| [^94] | [NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders.](http://arxiv.org/abs/2305.14499) | NAIL是一种带有高效非自回归解码器的词汇检索指数模型，可与现有的预训练模型兼容，并且使用商品CPU提供服务。它可以捕捉Transformer交叉关注模型收益高达86％的方法，与BM25检索器结合使用匹配当前最先进的双编码器检索器的质量。 |
| [^95] | [Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment.](http://arxiv.org/abs/2305.14492) | 本论文提出了一种新方法来比较中美文化的社会规范，通过情境对齐和上下文学习提取社会规范。作者构建了一个高质量的数据集，通过测试发现该方法具有研究社会文化异同的潜力。 |
| [^96] | [Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA.](http://arxiv.org/abs/2305.14458) | 本研究引入了SALSA框架，对大型语言模型进行细粒度的文本简化评估，通过21种不同编辑类型，揭示了不同模型和人类文本简化的偏好和表现，并开发了LENS-SALSA指标用于自动简化度量。 |
| [^97] | [Automatic Model Selection with Large Language Models for Reasoning.](http://arxiv.org/abs/2305.14333) | 本论文提出了一种使用大语言模型来自动选择带有推理的模型的方法，通过结合自然语言和编程语言的优势，实现了显著的性能改进，并在多个数据集上取得了新的最先进结果。 |
| [^98] | [TalkUp: Paving the Way for Understanding Empowering Language.](http://arxiv.org/abs/2305.14326) | 本研究通过借鉴语言学和社会心理学的文献，探索了赋予力量语言的特征，并创建了一个名为TalkUp的数据集用于训练捕捉赋予力量和削弱力量语言的语言模型，并为探索含义的含蓄性、预设以及社会背景如何影响语言的意义提供了一个途径。 |
| [^99] | [LLM-powered Data Augmentation for Enhanced Cross-lingual Performance.](http://arxiv.org/abs/2305.14288) | 本文研究了利用LLMs进行数据增强以提升跨语言常识推理数据集性能的潜力，并通过评估展示了利用LLMs生成数据的总体优势，同时进行了人工评估，证明了生成的示例具有较高的自然性和逻辑连贯性。 |
| [^100] | [Query Rewriting for Retrieval-Augmented Large Language Models.](http://arxiv.org/abs/2305.14283) | 该论文介绍了一种新的框架，即查询重写-检索-阅读，用于检索增强的大型语言模型。通过关注搜索查询本身的适应性，采用可训练的重写器来更好地对齐查询与冻结模块，从而改善了检索的效果。实验结果表明该方法在知识密集任务上取得了显著的进展。 |
| [^101] | [Hierarchical Prompting Assists Large Language Model on Web Navigation.](http://arxiv.org/abs/2305.14257) | 这项研究提出了一种分层提示方法来解决大规模语言模型在处理复杂观察的交互决策任务中的困难。研究表明该方法在网络导航中的效果优于先前最先进的提示机制，具有广泛的适用性。 |
| [^102] | [Multilingual Large Language Models Are Not (Yet) Code-Switchers.](http://arxiv.org/abs/2305.14235) | 多语言大型语言模型表现出了强大的性能，但是在代码切换的语境中，它们仍然表现不佳。 |
| [^103] | [Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding.](http://arxiv.org/abs/2305.14232) | 本论文提出了一个名为SciMult的多任务对比学习框架，旨在共享不同科学文献理解任务之间的通用知识，同时防止任务特定技能相互干扰。 |
| [^104] | [CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models.](http://arxiv.org/abs/2305.14214) | 本研究介绍了一个跨56种不同语言的大规模数据集，用于评估语言模型在复合分词任务上的表现。研究发现，现有的语言模型在复合分词上表现不佳，特别是对于通过子词分词方式进行标记的单词。因此，研究还提出了一种新的方法，通过训练专门的模型来改进复合分词性能。 |
| [^105] | [CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation.](http://arxiv.org/abs/2305.14105) | 该论文提出了一个结合多种特征的通用框架，用于机器翻译中的上下文示例选择。通过学习一个回归模型，CTQ Scorer能够选择最优示例以提高翻译质量，并在多个语言对和语言模型上显著超过随机选择和单因素基线。 |
| [^106] | [Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model.](http://arxiv.org/abs/2305.13999) | 这项研究提出了一个统一的框架来分析稀疏前馈网络在预训练大型语言模型中的设计选择。在语言建模任务中，通过使用平均聚合隐藏状态的选择方法，相比现有的MoE架构，可以实现更低的困惑度。 |
| [^107] | [Condensing Multilingual Knowledge with Lightweight Language-Specific Modules.](http://arxiv.org/abs/2305.13993) | 本文介绍了一种使用轻量级的语言特定模块来压缩多语言知识的方法，通过生成低秩矩阵来构建语言特定模块，并使用Fuse Distillation技术将多个语言特定模块中的知识压缩到一个共享模块中，提高了推理和模型序列化的效率。 |
| [^108] | [Flexible Grammar-Based Constrained Decoding for Language Models.](http://arxiv.org/abs/2305.13971) | 本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。 |
| [^109] | [Aligning Large Language Models through Synthetic Feedback.](http://arxiv.org/abs/2305.13735) | 该论文提出了一种使用合成反馈对齐大型语言模型的新框架，几乎不需要人力成本，也不依赖于预先对齐的LLMs。其中，通过对尺寸和提示等不同因素的普通 LLMS的响应进行奖励建模，来模拟高质量的示范来训练监督策略，并进一步使用强化学习优化模型。 |
| [^110] | [EDIS: Entity-Driven Image Search over Multimodal Web Content.](http://arxiv.org/abs/2305.13631) | 这篇论文介绍了EDIS数据集，该数据集包括100万个多模态图像和文本配对，旨在鼓励开发实现跨模态信息融合和匹配的检索模型。 |
| [^111] | [Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks.](http://arxiv.org/abs/2305.13547) | 论文提出了一种基于自我进化学习的 Mixup 方法，用于文本分类的数据扩充，可以为模型训练生成更加适应和友好的伪样本，该方法可以降低模型的overconfidence。 |
| [^112] | [Look-back Decoding for Open-Ended Text Generation.](http://arxiv.org/abs/2305.13477) | Look-back是一种自由生成文本的改进算法，可以自动预测可能的重复短语和主题偏移，并删除可能导致失败的令牌，生成的文本更流畅和连贯。 |
| [^113] | [MAILEX: Email Event and Argument Extraction.](http://arxiv.org/abs/2305.13469) | 本文提出了针对邮件领域的事件抽取数据集\dataset，比较了序列标记和生成式端到端抽取的方法，结果表明该任务存在非连续共享触发器跨度、非命名实体参数和邮件会话历史等难点，未来需要更多研究。 |
| [^114] | [DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules.](http://arxiv.org/abs/2305.13406) | DADA是一种适用于多个方言，基于语言规则的动态聚合适配器，可为SAE训练的模型赋予多方言鲁棒性，同时针对特定方言变体进行适应，提供了一种可解释的方言适应性框架。 |
| [^115] | [Prompting is not a substitute for probability measurements in large language models.](http://arxiv.org/abs/2305.13264) | 元语言提示与直接概率测量相比，对于衡量大型语言模型的语言知识来说，元语言判断效果较差，并且随着提示查询偏离直接测量的概率，一致性变差。提示的负面结果不能作为缺乏特定语言概括的确凿证据。从闭源API迁移中，我们也会失去一定的价值。 |
| [^116] | [SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables.](http://arxiv.org/abs/2305.13186) | SCITAB是一个具有挑战性的评估数据集，包含1.2K个经验证的科学事实和相关的科学表格，要求进行组合推理和事实验证。对于最先进的模型来说，SCITAB提出了许多独特挑战，包括表格定位、事实歧义和组合推理。所有模型中，除了GPT-4之外，性能仅略高于随机猜测。提示技术如思维链对于在SCITAB上提升性能几乎没有作用。 |
| [^117] | [Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models.](http://arxiv.org/abs/2305.13085) | 本研究提出了DecoMT，一种基于分解提示法的机器翻译方法，用于处理相关语言之间的翻译任务。通过利用相关语言的单调对齐特性，我们的方法在多个语系上进行了验证，并取得了良好的翻译效果。 |
| [^118] | [Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture.](http://arxiv.org/abs/2305.12710) | 本研究提出了一种新颖的主动学习架构，通过整合解释生成模型和预测模型，并采用数据多样性的采样策略，支持低资源场景下专家对标签和解释的注释需求。 |
| [^119] | [Automated Few-shot Classification with Instruction-Finetuned Language Models.](http://arxiv.org/abs/2305.12576) | 基于自动调优语言模型的少样本分类方法AuT-Few消除了手工制作提示的需求，并在多个数据集上超越了当前最先进的少样本学习方法。 |
| [^120] | [Retrieving Texts based on Abstract Descriptions.](http://arxiv.org/abs/2305.12517) | 本研究针对语义检索问题，提出了一种基于摘要描述的文本检索模型，通过改进当前的文本嵌入方法，在标准最近邻搜索中取得了显著性能提升。 |
| [^121] | [Evaluating Open-QA Evaluation.](http://arxiv.org/abs/2305.12421) | 本研究侧重于评估开放式问答（Open-QA）任务的方法，引入了一个新的任务QA-Eval和数据集EVOUNA，通过人工评估方法来评估AI生成的答案的准确性。我们调查了与人工评估相关的方法，并讨论了当前方法的缺陷和改进方法。我们相信这对于未来的自动评估工具发展和研究具有价值。 |
| [^122] | [Re-visiting Automated Topic Model Evaluation with Large Language Models.](http://arxiv.org/abs/2305.12152) | 本文提出使用大型语言模型来评估主题模型输出，并使用自动化方法确定最佳主题数量。 |
| [^123] | [Reducing Sequence Length by Predicting Edit Operations with Large Language Models.](http://arxiv.org/abs/2305.11862) | 本文提出利用大型语言模型预测源文本中的编辑操作来减少序列长度的方法，从而减少计算成本，实验表明其达到了可比较的性能。 |
| [^124] | [HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models.](http://arxiv.org/abs/2305.11747) | 本文介绍了一个大型语言模型幻觉评估基准（HELMA），其为标准化和可靠的估算模型幻觉问题提供了一种方法，并使用ChatGPT进行了实证研究以表明其存在幻觉的风险并为鉴别和减轻模型幻觉问题提供了一种方法。 |
| [^125] | [Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens.](http://arxiv.org/abs/2305.11550) | 该论文引入了表征转移潜力（RTP）来衡量多语言神经机器翻译中的知识转移，发现多路并行重叠是关键特征，提出了一种新的训练方案，鼓励表征在语言之间更具不变性，并在多个数据和模型设置中提高了低资源和中资源语言的翻译质量。 |
| [^126] | [Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings.](http://arxiv.org/abs/2305.10786) | 提出了一种名为Ditto的简单有效的方法，其可以解决预训练语言模型中存在的各向异性问题，并在语义文本相似性任务中提高模型的性能。 |
| [^127] | [Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency.](http://arxiv.org/abs/2305.10713) | 本论文提出了一种新的度量--Prompt平坦度，可以优化语言提示选择，提高模型分类的准确性和样本效率，实验证明结合现有度量可以提高性能和样本效率。 |
| [^128] | [Learning the Visualness of Text Using Large Vision-Language Models.](http://arxiv.org/abs/2305.10434) | 该论文利用大型视觉语言模型如CLIP来检测文本的视觉性，并提出fine-tuning策略，将非视觉文本映射为NULL图像，匹配视觉文本与对应图像，以解锁在文本中嵌入相关图像的能力。 |
| [^129] | [Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.](http://arxiv.org/abs/2305.10163) | 本研究通过在ChatGPT中集成医学领域知识和启用少样本学习的新方法，在中国国家医学执业医师资格考试中取得成功，这为建立在自然语言处理技术和医学领域知识的创新应用提供了可能。 |
| [^130] | [Self-Supervised Sentence Compression for Meeting Summarization.](http://arxiv.org/abs/2305.07988) | 本论文提出了一种自监督学习的会议摘要框架SVB，通过三个过程压缩冗余内容并保留关键信息。其中，滑动窗口对话恢复与评分、通道重要性得分投票和相对位置分箱等算法用于实现这个框架。 |
| [^131] | [ZARA: Improving Few-Shot Self-Rationalization for Small Language Models.](http://arxiv.org/abs/2305.07355) | 本文提出了一种名为ZARA的方法，其可以通过将合理性判断问题转化为自然语言推理来自动构建伪平行数据进行自我训练，从而提高小型语言模型的少样本自我解释性能，实验结果表明ZARA在任务准确性和解释质量上都表现出SOTA水平。 |
| [^132] | [Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting.](http://arxiv.org/abs/2305.07004) | 该论文介绍了一种跨语言思维提示方法，名为XLT，用于提高LLMs的多语言能力。该方法能够显著提高各种多语言任务性能，并减少不同语言中任务性能的差距。 |
| [^133] | [Active Retrieval Augmented Generation.](http://arxiv.org/abs/2305.06983) | 本论文提出了一种主动检索增强生成的方法，与以往的方法相比，它在生成过程中更紧密地集成了主动检索和生成，并展示了在一组句子生成任务中的性能优势。 |
| [^134] | [Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good.](http://arxiv.org/abs/2305.05471) | 本文介绍了NLP4SGPAPERS数据集，通过对解决社会问题的论文进行分类、可持续发展目标映射、任务及方法的确定，使用最先进的NLP模型在整个ACL文集上进行处理，提供了一个可视化工作区，展示了NLP4SG领域的全貌。 |
| [^135] | [Expository Text Generation: Imitate, Retrieve, Paraphrase.](http://arxiv.org/abs/2305.03276) | 该论文介绍了一项名为“专业性文本生成”的任务，目的在于从知识来源自动生成准确、信息丰富的文档，研究者提出的IRP框架能够克服语言模型的缺点，实现了内容规划、事实选择 和改写步骤分别处理，使得生成的文档具有较高的质量。 |
| [^136] | [Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders.](http://arxiv.org/abs/2305.02996) | 本文提出了一种自适应锚点选择方法，可以在保持较小的计算成本的同时，实现与随机抽样锚点相当或者更好的k-NN召回性能。 |
| [^137] | [The Benefits of Label-Description Training for Zero-Shot Text Classification.](http://arxiv.org/abs/2305.02239) | 本文提出了标注描述训练的方法，在零样本分类中可以显著提高准确率，并能更鲁棒地处理分类任务。 |
| [^138] | [Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity.](http://arxiv.org/abs/2305.02176) | 本篇论文提出了一种新的分层稀疏激活Transformer模型，可以动态分配不同令牌的能力，解决了传统Mixture-of-experts模型参数低效的问题，实验表明该模型在多语言机器翻译任务中取得了较好效果。 |
| [^139] | [GPT-RE: In-context Learning for Relation Extraction using Large Language Models.](http://arxiv.org/abs/2305.02105) | 本文提出了GPT-RE，通过特定于任务的实体表示和使用金标签诱导的推理逻辑丰富演示，成功解决了大型语言模型在关系抽取方面的低相关性和倾向于错误分类的问题，成果在多个数据集上均超过现有基准，其中在三个数据集中的结果达到了最新研究成果。 |
| [^140] | [Towards Summarizing Multiple Documents with Hierarchical Relationships.](http://arxiv.org/abs/2305.01498) | 提出了一个新的数据集PeerSum用于生成科学论文的元评论，源文档具有显式层次结构的丰富文档间关系，提出了一种用于元评论生成的关系感知多任务模型Rammer。 |
| [^141] | [Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models.](http://arxiv.org/abs/2305.01219) | 本研究提出一种新颖有效的“ProAttack”方法来执行干净标签的后门攻击，使用的是提示本身作为触发器。该方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性，相比于现有的后门攻击方法有显著提升。 |
| [^142] | [Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.](http://arxiv.org/abs/2305.00118) | 通过名字填空成员推断查询，该研究考古了ChatGPT和GPT-4已知的图书，发现这些模型已经记忆了大量受版权保护的材料，这支持了一个使用已知训练数据的开放模型案例。 |
| [^143] | [Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens.](http://arxiv.org/abs/2304.11389) | 本文研究表明，基于Transformer的语言模型在观察约20亿个训练令牌后能够提供最佳拟合，其surprisal估计能力能够最好地预测人类阅读时间。在语言模型收敛时，较小的模型变体会出现“临界点”，使语言模型困惑度下降，从而导致较差的人类拟合度。 |
| [^144] | [Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling.](http://arxiv.org/abs/2304.09145) | 通过Outlier Suppression+框架的通道级移位和缩放操作，分析得到最优移位和缩放值，成功解决了量化Transformer语言模型中存在的不对称离群值问题，实现了接近浮点性能的结果。 |
| [^145] | [Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing.](http://arxiv.org/abs/2304.08315) | 本文调查了自然语言处理（NLP）领域的双重使用问题，提出了一份定制的双重使用定义，并讨论了当前的状况和可能的挑战。 |
| [^146] | [MEGA: Multilingual Evaluation of Generative AI.](http://arxiv.org/abs/2303.12528) | 这项研究对 33 种语言中 8 个不同任务的生成 AI 进行了全面评估，比较了生成 LLMs 和非自回归模型的表现差异。 |
| [^147] | [Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization.](http://arxiv.org/abs/2303.12314) | 提出了一种自我监督元提示学习框架SUPMER，包括元梯度正则化，用于少样本泛化，通过锚定的元训练任务和基于课程的任务增强丰富了任务分布，解决了在少样本情况下良好初始化软提示和过拟合的问题。 |
| [^148] | [Context-faithful Prompting for Large Language Models.](http://arxiv.org/abs/2303.11315) | 大型语言模型的准确提示对于解决上下文敏感的自然语言处理任务非常重要。本文通过研究两个方面的上下文准确性（知识冲突和预测的弃权），发现了基于观点的提示和虚构演示两种有效的方法，提高了大型语言模型的上下文准确性。 |
| [^149] | [WiCE: Real-World Entailment for Claims in Wikipedia.](http://arxiv.org/abs/2303.01432) | WiCE是一个细粒度的文本蕴含数据集，通过提取维基百科中的声明和证据配对构建。我们提出了自动声明分解策略，并展示了这种策略在多个数据集上的性能提升效果。此外，我们还发现现有模型无法解决数据集中的验证和检索问题。 |
| [^150] | [AI Chat Assistants can Improve Conversations about Divisive Topics.](http://arxiv.org/abs/2302.07268) | 该论文介绍了一个大型实验的结果，证明了使用人工智能工具可以改善关于分裂性话题的在线对话。他们通过使用一种大型语言模型实时提供基于证据的建议，帮助人们在对话中感受到理解的感觉。 |
| [^151] | [Towards Agile Text Classifiers for Everyone.](http://arxiv.org/abs/2302.06541) | 本文介绍和评估了敏捷文本分类的方法，通过使用小型、定向的数据集进行训练，可以为特定策略快速开发分类器，并通过提示调整大型语言模型来实现最先进的性能。这为支持更安全的在线交流的模型带来了范式转变。 |
| [^152] | [Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning.](http://arxiv.org/abs/2302.04858) | Re-ViLM是一种检索增强的视觉语言模型，通过从外部数据库中检索相关知识，减少了模型参数的数量，并且可以轻松适应新数据，用于零样本和少样本图像字幕生成任务。 |
| [^153] | [CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models.](http://arxiv.org/abs/2302.04012) | 这项工作提出了一种系统研究代码语言模型安全问题的方法，旨在评估和发现黑盒代码语言模型中的安全漏洞。 |
| [^154] | [Concept Algebra for Score-Based Conditional Models.](http://arxiv.org/abs/2302.03693) | 本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。 |
| [^155] | [The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study.](http://arxiv.org/abs/2302.03341) | 这项研究系统地研究了元数据对科学文献标签化的影响，并在19个领域中选择了三种代表性的多标签分类器进行实验。 |
| [^156] | [Using In-Context Learning to Improve Dialogue Safety.](http://arxiv.org/abs/2302.00871) | 本文研究了一种检索式方法来减少Chatbots回应中偏见和有毒内容的方法，通过使用上下文学习来引导模型生成更加安全的内容，该方法性能与强基线方法相媲美，还提出了一个可进一步提高响应安全性的重新排名程序。 |
| [^157] | [Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?.](http://arxiv.org/abs/2301.12609) | 知识蒸馏和标签平滑被认为是等价的方法，但实验证明它们对模型置信度的影响方向完全相反。知识蒸馏不仅传递知识，还传递了自信心。 |
| [^158] | [Differentially Private Natural Language Models: Recent Advances and Future Directions.](http://arxiv.org/abs/2301.09112) | 这篇论文对最近在NLP领域中的差分隐私深度学习模型的最新进展进行了系统综述，讨论了与标准差分隐私深度学习的不同之处和额外的挑战。 |
| [^159] | [Smooth Sailing: Improving Active Learning for Pre-trained Language Models with Representation Smoothness Analysis.](http://arxiv.org/abs/2212.11680) | 通过表示平滑度分析，我们改进了预训练语言模型中的主动学习方法，提出了一种无需验证集的早停技术，并发现任务适应对主动学习具有改进作用。这项工作在实际应用中证明了表示平滑度分析对于提高主动学习的有效性和实用性的重要性。 |
| [^160] | [ALCAP: Alignment-Augmented Music Captioner.](http://arxiv.org/abs/2212.10901) | 本文提出了一种基于对齐的音乐字幕生成器，通过对比学习显式学习音频和歌词的对应关系，并生成高质量的字幕，取得了两个音乐字幕数据集上的最新领先水平。 |
| [^161] | [A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models.](http://arxiv.org/abs/2212.09873) | 本研究比较了眼动数据、注释和语言模型对文本风格的显著性的影响，发现眼动数据在人类注释和基于模型的评分中起到了桥梁的作用。 |
| [^162] | [A Retrieve-and-Read Framework for Knowledge Graph Link Prediction.](http://arxiv.org/abs/2212.09724) | 这项研究提出了一种检索和阅读框架来解决现有知识图谱链接预测系统的局限性。通过首先检索相关子图上下文，然后使用高容量阅读器联合推理上下文和查询，该框架能够提供更有用的信息和更强大的表达能力。 |
| [^163] | [On Event Individuation for Document-Level Information Extraction.](http://arxiv.org/abs/2212.09702) | 提出了问题──事件个体化对于模板填充任务是否适用，通过注释研究和误差分析，我们发现这引发了对模板填充度量的有效性、任务数据集的质量以及模型学习能力的担忧。 |
| [^164] | [DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning.](http://arxiv.org/abs/2212.02851) | DiSTRICT是一种无需手工制作模板的通用上下文调优方法，通过检索相关的训练样本来微调对话状态跟踪模型，从而在各种零样本和少样本情况下使用更小的模型时获得了优于现有方法的性能。 |
| [^165] | [IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn Response Selection.](http://arxiv.org/abs/2212.00482) | 本研究提出了一种名为IRRGN的隐式关系推理图网络，用于解决多轮对话中的响应选择任务。该网络利用关系图卷积网络隐式提取话语之间以及话语和选项之间的依赖关系，并通过双重比较感知选项之间的差异，取得了显著的性能提升。 |
| [^166] | [Syntactic Substitutability as Unsupervised Dependency Syntax.](http://arxiv.org/abs/2211.16031) | 本文提出了一种新的无需语法规则的诱导方法，该方法基于语法可替代性，能够在定量和定性方面都有所提升。 |
| [^167] | [SciRepEval: A Multi-Format Benchmark for Scientific Document Representations.](http://arxiv.org/abs/2211.13308) | SciRepEval是第一个综合评估科学文献表示的全面基准，其中包括四种格式的 25 个任务。通过使用格式特定的控制代码和适配器，可以改进科学文献表示模型的泛化能力。 |
| [^168] | [Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks.](http://arxiv.org/abs/2211.12588) | 这篇论文提出了一种新的方法，即"思维程序"（PoT），通过将推理过程表达为一个程序，将计算与推理过程分离开来，以提升解决数值推理任务的性能。在多个数据集上的评估中，PoT相比最新方法CoT在性能上平均提高了约12％。 |
| [^169] | [[RE]VER: Learning Natural Language Representations for Verbalizing Entities and Relations.](http://arxiv.org/abs/2211.11093) | 本文提出了一种名为[RE]VER的系统，使用基于transformer的模型来学习实体和关系的自然语言表示，能够生成一个能够表示实体与其他实体关系的句子，相比于之前的最先进方法有了显著改进。 |
| [^170] | [CAPE: Corrective Actions from Precondition Errors using Large Language Models.](http://arxiv.org/abs/2211.09935) | CAPE是一种利用大型语言模型从前置错误中纠正行动的方法，提高了生成计划的质量，使具身代理能够执行更多任务，并改善了计划的正确性。 |
| [^171] | [Exploiting Contrastive Learning and Numerical Evidence for Improving Confusing Legal Judgment Prediction.](http://arxiv.org/abs/2211.08238) | 本文提出了一种利用对比学习和数字证据的方法改进混淆的法律判决预测。通过提出一种监督对比学习方法和利用数字证据预测处罚期限，成功地解决了区分分类错误和利用数字的问题。 |
| [^172] | [Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense.](http://arxiv.org/abs/2211.05895) | 该论文提出了一个多模态评估流程来测试模型对视觉场景、文本和相关知识的理解能力，并表明使用该数据进行训练可以提高模型在标准评估中的性能。 |
| [^173] | [Exploring Effective Distillation of Self-Supervised Speech Models for Automatic Speech Recognition.](http://arxiv.org/abs/2210.15631) | 本文探索了将HuBERT基于自监督学习的语音模型进行有效蒸馏，以改善自动语音识别的性能，特别是在低资源场景下。通过研究不同的学生模型结构，并引入判别损失，本研究提供了一种简单有效的算法来进行自监督语音模型的微型化。 |
| [^174] | [COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation.](http://arxiv.org/abs/2210.15500) | 本研究探讨了个性化解释生成中的反事实公平性问题，在解释生成中引入了一个通用框架以实现度量特定的反事实公平性，实验证明了方法的有效性。 |
| [^175] | [Unifying Data Perspectivism and Personalization: An Application to Social Norms.](http://arxiv.org/abs/2210.14531) | 本研究提出了一种应用于社会规范的个性化方法，通过分析一组标注者对社交媒体帖子的评价来预测社会规范的认知。实验证明，个性化方法在预测社会规范认知方面具有较好的效果，并在不同社交情境的子集中的表现有所差异。 |
| [^176] | [On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?.](http://arxiv.org/abs/2210.12770) | 本研究探讨了在临床自然语言处理任务中，将从一般领域或相关领域数据预训练的迁移学习模型微调到特定任务上的有效性。实验结果显示，微调的语言模型相对于从头开始学习的模型在命名实体识别任务上取得了更好的性能。 |
| [^177] | [InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions.](http://arxiv.org/abs/2210.07440) | 这篇论文提出了一种公平可解释的去偏方法，通过用户反馈在任务性能和偏差缓解之间实现更好和更公平的平衡。 |
| [^178] | [Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling.](http://arxiv.org/abs/2210.05261) | 本文提出了一种名为MixEncoder的新范式，用于高效的句对建模。该范式通过轻量级的交叉注意力机制实现了超过113倍的句对匹配加速，与更昂贵的交叉注意力模型相比，性能相当。 |
| [^179] | [A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting.](http://arxiv.org/abs/2208.08374) | 本文建立了一个计算接口，可以将非结构化语言策略翻译为可执行的目标和约束，并证明该模型在推断战略意图方面优于人类解释员。 |
| [^180] | [Meta-Referential Games to Learn Compositional Learning Behaviours.](http://arxiv.org/abs/2207.08012) | 本论文提出了一种元元反游戏学习的方法来解决组合学习行为的问题，通过解决绑定问题来支持人工智能代理展示组合学习行为的能力。 |
| [^181] | [PreBit -- A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin.](http://arxiv.org/abs/2206.00648) | 本文提出了一种利用多模态模型进行比特币极端价格波动预测的方法，将相关资产、技术指标和Twitter内容作为输入。通过使用预训练的金融词汇表的句级FinBERT嵌入，模型可以有效地捕捉推文中的内容，从而预测比特币的价格波动。 |
| [^182] | [Labeling Programs with Non-Programmers Indirectly via Active Examples: A Case Study with Text-to-SQL.](http://arxiv.org/abs/2205.12422) | 使用APEL框架，非程序员可以通过检查输入-输出示例间接选择复杂程序，从而实现对自然语言表达的注释，并且在重新注释文本到SQL数据集时达到了与专家相同的准确度，同时揭示了原始注释中的细微错误。 |
| [^183] | [Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds.](http://arxiv.org/abs/2205.01845) | 本文提出了一种带有未登录词种子的主题发现方法，将预训练语言模型和来自输入语料库的局部语义相结合，实验证明了该方法在主题连贯性、准确性和多样性方面的有效性。 |
| [^184] | [EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training.](http://arxiv.org/abs/2203.09313) | 本论文研究了开放领域中文对话系统的关键因素，包括数据质量控制、模型架构设计、训练方法和解码策略，并提出了具有28亿参数的EVA2.0模型，其在自动和人工评估中表现优秀，还讨论了该研究的局限性和未来的研究方向。 |
| [^185] | [Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification.](http://arxiv.org/abs/2202.05932) | 本文研究了零样本情况下的大规模多标签文本分类，提出了一种基于元数据引导的对比学习方法（MICoL）。实验结果表明该方法在两个大规模数据集上取得了良好的效果。 |
| [^186] | [Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval.](http://arxiv.org/abs/2112.05917) | 该论文提出了一个名为ENGINE的框架，用于实体感知的文章生成和检索。框架通过命名实体提取模块和实体感知机制，将命名实体明确地融入到语言模型中，以提高模型理解和生成文章的能力。 |
| [^187] | [RATE: Overcoming Noise and Sparsity of Textual Features in Real-Time Location Estimation.](http://arxiv.org/abs/2111.06515) | 本文提出了RATE算法，通过使用主题建模和整合其他特征，成功克服了实时位置估计中的文本特征噪声和稀疏性问题，并在实验中展现出优于其他基准方法的性能。 |
| [^188] | [MotifClass: Weakly Supervised Text Classification with Higher-order Metadata Information.](http://arxiv.org/abs/2111.04022) | 本文提出了一个名为MotifClass的框架，基于高阶元数据信息来进行弱监督的文本分类问题。通过建模文档和元数据之间的关系，利用图案描述元数据组合以捕捉高阶结构，并选择具有类别指示意义的图案实例进行分类。 |
| [^189] | [An Empirical Study of Compound PCFGs.](http://arxiv.org/abs/2103.02298) | C-PCFGs在无监督短语结构语法归纳方面取得了新的技术水平。数据显示，C-PCFGs数据利用效率高，能适应不同句子长度，并且能够充分利用句子级信息。然而，最佳配置不一定适用于形态丰富的语言。 |
| [^190] | [MATCH: Metadata-Aware Text Classification in A Large Hierarchy.](http://arxiv.org/abs/2102.07349) | MATCH是一个元数据感知的大型层次文本分类解决方案，利用了元数据和层次信息。它在预训练阶段将文本和元数据嵌入到同一空间，并利用全连接注意力捕捉它们之间的关系，同时通过不同的正则化方式利用标签层次结构。 |
| [^191] | [Hierarchical Metadata-Aware Document Categorization under Weak Supervision.](http://arxiv.org/abs/2010.13556) | 本论文研究了如何在弱监督下将标签层次、元数据和文本信号整合起来进行文档分类，并提出了HiMeCat框架，该框架可以同时建模类别依赖关系、元数据信息和文本语义，实现了在只有少量训练样本和元数据信息的情况下进行高效文档分类。 |
| [^192] | [Minimally Supervised Categorization of Text with Metadata.](http://arxiv.org/abs/2005.00624) | 本论文提出了MetaCat，一种使用元数据进行最小监督文本分类的框架。通过将生成模型应用于文本和元数据之间的关系，实现了在标签稀缺情况下的高效分类。 |
| [^193] | [Text as Environment: A Deep Reinforcement Learning Text Readability Assessment Model.](http://arxiv.org/abs/1912.05957) | 这是一种使用深度强化学习模型评估文本可读性的方法，通过使用硬注意力的主动推理技术和半监督信号来提高效率，并与其他先进模型进行比较。 |
| [^194] | [HiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories.](http://arxiv.org/abs/1910.07115) | HiGitClass是基于关键词的GitHub仓库的分层分类框架，用户只需提供带有关键词的标签层次结构作为监督。框架解决了多模式信号、监督稀缺性和监督格式不匹配等主要挑战。 |

# 详细

[^1]: H2O开放生态系统用于最先进的大规模语言模型

    H2O Open Ecosystem for State-of-the-art Large Language Models. (arXiv:2310.13012v1 [cs.CL])

    [http://arxiv.org/abs/2310.13012](http://arxiv.org/abs/2310.13012)

    H2O推出了开放生态系统，旨在开发和测试最先进的大规模语言模型（LLMs），包括h2oGPT和H2O LLM Studio。这一开源项目提供了全面开放的替代方案，能够帮助推动人工智能的发展，使其更加可信赖和可访问。

    

    大规模语言模型（LLMs）代表了人工智能的一项革命。然而，它们也带来了许多重大风险，例如存在偏见、私有、受版权保护或有害的文本。因此，我们需要开放、透明和安全的解决方案。我们介绍了一个完整的开源生态系统，用于开发和测试LLMs。该项目的目标是推动对封闭源方法的开放式替代方案。我们发布了h2oGPT，即从70亿到700亿参数的一系列精细调整的LLMs。我们还推出了H2O LLM Studio，这是一个框架和无代码GUI，专为使用最新的先进技术进行LLMs的高效精细调整、评估和部署而设计。我们的代码和模型在完全自由的Apache 2.0许可证下授权使用。我们相信开源语言模型有助于推动人工智能的发展，并使其更可访问和可信赖。演示网址为：https://gpt.h2o.ai/

    Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. The demo is available at: https://gpt.h2o.ai/
    
[^2]: 关于循环神经网络语言模型的表示能力的研究

    On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.12942](http://arxiv.org/abs/2310.12942)

    本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。

    

    本研究调查了基于循环神经网络(RNNs)的语言模型(LMs)的计算表达性。Siegelmann和Sontag(1992)曾经展示了具有有理权重和隐藏状态以及无限计算时间的RNNs是图灵完备的。然而，LMs不仅定义了字符串上的加权，还定义了(非加权)语言成员关系，对RNN LMs（RLMs）的计算能力分析应该反映这一点。我们将图灵完备性结果扩展到概率情况，展示了如何使用有理权重的RLM和无限计算时间来模拟任何概率图灵机(PTM)。由于在实践中，RLMs实时工作，每个时间步骤处理一个符号，因此我们将上述结果作为RLMs表达性的上界。我们还通过展示在实时计算限制下，这些模型可以模拟确定性实时有理PTMs来提供下界。

    This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
    
[^3]: 预训练遮蔽语言模型中社会偏见与任务表现的预测因素分析

    A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models. (arXiv:2310.12936v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.12936](http://arxiv.org/abs/2310.12936)

    本研究对39个不同模型大小、训练目标、分词方法、训练数据领域和语言的预训练语言模型进行了全面研究，发现分词和模型目标等因素对MLMs学习的社会偏见具有重要影响。

    

    先前的研究中报告了各种类型的预训练遮蔽语言模型(MLMs)存在社会偏见。然而，MLM与许多潜在因素相关，如模型大小、训练数据大小、训练目标、预训练数据的领域、分词和预训练语料中包含的语言等。目前尚不清楚这些因素中哪些影响了MLMs学习的社会偏见。为了研究模型因素与MLMs学习的社会偏见以及模型的下游任务表现之间的关系，我们对39个不同模型大小、训练目标、分词方法、训练数据领域和语言的预训练MLMs进行了全面研究。我们的结果揭示了在先前的文献中经常被忽视的重要因素，如分词或模型目标。

    Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship between model factors and the social biases learned by an MLM, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages. Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives.
    
[^4]: StoryAnalogy: 从大型语言模型中衍生出故事级类比以解开类比理解

    StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding. (arXiv:2310.12874v1 [cs.CL])

    [http://arxiv.org/abs/2310.12874](http://arxiv.org/abs/2310.12874)

    本文通过构建故事级类比语料库StoryAnalogy评估了大型语言模型在识别和生成类比任务上的能力，发现这些任务对于句子嵌入模型和最近的大型语言模型来说都非常具有挑战性。同时，研究发现通过使用StoryAnalogy中的数据可以提高大型语言模型的类比生成质量。

    

    故事之间的类比是自然语言理解中最关键的能力之一。本文通过构建一个规模巨大的故事级类比语料库StoryAnalogy来评估识别和生成类比的能力，该语料库包含来自不同领域的24K个故事对，并对来自扩展结构映射理论的两个相似性进行人工注释。我们设计了一系列在StoryAnalogy上的测试，首次评估了故事级类比的识别和生成。有趣的是，我们发现类比识别任务对于句子嵌入模型以及最近的大型语言模型（如ChatGPT和LLaMa）来说都非常具有挑战性，其中ChatGPT在多选题中只能达到约30%的准确率（对于人类而言，准确率超过85%）。最后，我们发现StoryAnalogy中的数据可以提高大型语言模型的类比生成质量，其中经过微调的FlanT5-xxl模型获得了可比较的性能。

    Analogy-making between narratives is one of the most critical abilities in natural language understanding. In this paper, we evaluate the ability to identify and generate analogy by building a first-of-its-kind large-scale story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are extremely challenging not only for the sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa, where ChatGPT only achieved around 30% accuracy in multiple-choice questions (> 85% accuracy for humans). Finally, we find that data in StoryAnalogy can improve LLMs analogy generation quality, where a fine-tuned FlanT5-xxl model yields comparable performanc
    
[^5]: AgentTuning: 为LLMs实现通用代理能力

    AgentTuning: Enabling Generalized Agent Abilities for LLMs. (arXiv:2310.12823v1 [cs.CL])

    [http://arxiv.org/abs/2310.12823](http://arxiv.org/abs/2310.12823)

    本论文提出了AgentTuning，一种简单而通用的方法，可提升LLMs的代理能力，同时保持其通用能力。通过构建AgentInstruct数据集，并采用一种混合训练方法，作者成功地实现了提高LLMs代理能力的目标。

    

    开放的大型语言模型（LLMs）在各种任务中具有出色的性能，极大地推动了LLMs的发展。然而，当它们作为代理在现实世界中应对复杂任务时，它们远不及ChatGPT和GPT-4等商业模型。这些代理任务将LLMs作为负责规划、记忆和工具利用的中央控制器，需要细粒度的提示方法和强大的LLMs才能达到令人满意的性能。虽然已经提出了许多提示方法来完成特定的代理任务，但缺乏研究专注于提高LLMs自身的代理能力而不损害其通用能力。在这项工作中，我们提出了AgentTuning，一种简单而通用的方法，可以提升LLMs的代理能力，同时保持其通用的LLM能力。我们构建了AgentInstruct，一个轻量级的指令调整数据集，其中包含高质量的交互轨迹。

    Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hy
    
[^6]: 面向现实世界的混合语音流式翻译的研究

    Towards Real-World Streaming Speech Translation for Code-Switched Speech. (arXiv:2310.12648v1 [cs.CL])

    [http://arxiv.org/abs/2310.12648](http://arxiv.org/abs/2310.12648)

    本文针对面向真实世界的混合语音流式翻译进行了研究，主要关注了流式设置和翻译到第三种语言的问题，并提出了一种基线模型进行了实验。

    

    混合语言（CS）即在一句话中混合使用不同语言，是通信中常见的现象，在许多自然语言处理（NLP）环境下可能具有挑战性。先前关于CS语音的研究在端到端语音翻译（ST）方面取得了有希望的结果，但仅限于离线场景，并且仅能翻译成源语中的一种语言（单语转录）。本文着重研究了两个尚未探索的面向现实世界的CS语音翻译领域：流式设置和翻译到第三种语言（即源语中未包含的语言）。为此，我们扩展了Fisher和Miami测试和验证数据集，包含了西班牙语和德语作为新的目标语言。利用这些数据，我们对离线和流式ST模型进行训练，并建立了之前提到的两个设置的基线结果。

    Code-switching (CS), i.e. mixing different languages in a single sentence, is a common phenomenon in communication and can be challenging in many Natural Language Processing (NLP) settings. Previous studies on CS speech have shown promising results for end-to-end speech translation (ST), but have been limited to offline scenarios and to translation to one of the languages present in the source (\textit{monolingual transcription}).  In this paper, we focus on two essential yet unexplored areas for real-world CS speech translation: streaming settings, and translation to a third language (i.e., a language not included in the source). To this end, we extend the Fisher and Miami test and validation datasets to include new targets in Spanish and German. Using this data, we train a model for both offline and streaming ST and we establish baseline results for the two settings mentioned earlier.
    
[^7]: 使用文本属性异构图进行语言模型的预训练

    Pretraining Language Models with Text-Attributed Heterogeneous Graphs. (arXiv:2310.12580v1 [cs.CL])

    [http://arxiv.org/abs/2310.12580](http://arxiv.org/abs/2310.12580)

    本文提出了一个新的语言模型预训练框架，能够明确考虑到文本属性异构图中的拓扑和异构信息。通过优化语言模型和辅助的异构图神经网络，预测了文本属性异构图中的节点。同时，还设计了一个文本丰富性加权的节点抽样策略，以更好地利用文本信息。

    

    在许多实际场景中（如学术网络、社交平台），不同类型的实体不仅与文本相关，还通过各种关系相连，这可以被抽象为文本属性异构图（Text-Attributed Heterogeneous Graphs，TAHGs）。

    In many real-world scenarios (e.g., academic networks, social platforms), different types of entities are not only associated with texts but also connected by various relationships, which can be abstracted as Text-Attributed Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs. In this paper, we present a new pretraining framework for LMs that explicitly considers the topological and heterogeneous information in TAHGs. Firstly, we define a context graph as neighborhoods of a target node within specific orders and propose a topology-aware pretraining task to predict nodes involved in the context graph by jointly optimizing an LM and an auxiliary heterogeneous graph neural network. Secondly, based on the observation that some nodes are text-rich while others have little text, we devise a tex
    
[^8]: 朝着更安全的操作：预防未来故障的高压气体事故专家参与数据集

    Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures. (arXiv:2310.12074v1 [cs.CL])

    [http://arxiv.org/abs/2310.12074](http://arxiv.org/abs/2310.12074)

    本文介绍了一个名为IncidentAI的新数据集，用于安全预防。该数据集包含三个任务，并由高压气体保护管理领域的专家进行了注释。初步结果显示，使用自然语言处理技术有助于分析事故报告以预防未来的故障。这个数据集将促进NLP和事故管理领域的未来研究。

    

    本文介绍了一个用于安全预防的新的IncidentAI数据集。与通常只包含一个任务的先前语料库不同，我们的数据集包括了三个任务：命名实体识别，因果关系提取和信息检索。该数据集由至少有六年高压气体保护管理实践经验的领域专家进行注释。我们验证了数据集在安全预防场景中的贡献。对三个任务的初步结果表明，自然语言处理技术有助于分析事故报告以避免未来的故障。该数据集便于NLP和事故管理社区的未来研究。数据集也可提供访问（IncidentAI数据集可在 https://github.com/Cinnamon/incident-ai-dataset 获取）。

    This paper introduces a new IncidentAI dataset for safety prevention. Different from prior corpora that usually contain a single task, our dataset comprises three tasks: named entity recognition, cause-effect extraction, and information retrieval. The dataset is annotated by domain experts who have at least six years of practical experience as high-pressure gas conservation managers. We validate the contribution of the dataset in the scenario of safety prevention. Preliminary results on the three tasks show that NLP techniques are beneficial for analyzing incident reports to prevent future failures. The dataset facilitates future research in NLP and incident management communities. The access to the dataset is also provided (the IncidentAI dataset is available at: https://github.com/Cinnamon/incident-ai-dataset).
    
[^9]: 评估大型语言模型在越南普通教育中对多项选择题符号绑定能力的能力

    Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education. (arXiv:2310.12059v1 [cs.CL])

    [http://arxiv.org/abs/2310.12059](http://arxiv.org/abs/2310.12059)

    本研究评估了大型语言模型在越南普通教育中对多项选择题符号绑定能力的能力，并创建了一个新颖且高质量的数据集来评估语言模型的符号绑定能力。

    

    本文评估了大型语言模型（LLMs）在零样本、一次性和少样本设置下，执行多项选择符号绑定（MCSB）用于多项选择题回答（MCQA）任务的能力。我们将重点放在越南语上，因为越南语中的挑战性MCQA数据集较英语少。现有的两个数据集，ViMMRC 1.0和ViMMRC 2.0，专注于文学问题。越南自然语言处理（NLP）领域的最新研究侧重于评估ChatGPT在2019年至2023年的越南国家高中毕业考试（VNHSGE）中的解决方案。然而，这些研究主要关注ChatGPT如何逐步解决VNHSGE。我们的目标是通过为数学、物理、化学和生物的LaTeX公式输入提供结构化指南，创建一个新颖且高质量的数据集。该数据集可用于评估LLMs和较小的语言模型（LMs）的MCSB能力，因为数据集要求使用严格的LaTeX样式进行输入。我们重点预测字符（A、B、C或

    In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or 
    
[^10]: LoHoRavens: 一项针对机器人桌面操作的长时程语言条件基准测试

    LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation. (arXiv:2310.12020v1 [cs.RO])

    [http://arxiv.org/abs/2310.12020](http://arxiv.org/abs/2310.12020)

    LoHoRavens是一个针对机器人桌面操作的长时程语言条件基准测试，涵盖颜色、大小、空间、算术和引用等各种推理方面。本研究还探索了在机器人执行过程中如何将观测反馈纳入到大型语言模型的闭环规划中的两种方法。

    

    体验式代理与大型语言模型的融合为体验式指导带来了显著的进展。特别是，大型语言模型的强大推理能力使得机器人能够在没有昂贵的注释演示的情况下进行长时程任务。然而，目前还缺乏用于测试语言条件机器人在各种场景中推理长时程能力的公共基准。为了填补这一空白，本研究聚焦于桌面操作任务，并发布了一个称为“LoHoRavens”的仿真基准测试，涵盖颜色、大小、空间、算术和引用等各种长时程推理方面。此外，对于使用大型语言模型进行长时程操作的任务，存在一个关键的模态过渡问题：如何在机器人执行过程中将观测反馈纳入到大型语言模型的闭环规划中，然而之前的研究对此进行的探索较少。我们研究了两种解决模态过渡问题的方法：标题生成和快照。

    The convergence of embodied agents and large language models (LLMs) has brought significant advancements to embodied instruction following. Particularly, the strong reasoning capabilities of LLMs make it possible for robots to perform long-horizon tasks without expensive annotated demonstrations. However, public benchmarks for testing the long-horizon reasoning capabilities of language-conditioned robots in various scenarios are still missing. To fill this gap, this work focuses on the tabletop manipulation task and releases a simulation benchmark, \textit{LoHoRavens}, which covers various long-horizon reasoning aspects spanning color, size, space, arithmetics and reference. Furthermore, there is a key modality bridging problem for long-horizon manipulation tasks with LLMs: how to incorporate the observation feedback during robot execution for the LLM's closed-loop planning, which is however less studied by prior work. We investigate two methods of bridging the modality gap: caption ge
    
[^11]: 快速多极化注意力：一种用于长序列的分治注意力机制

    Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v1 [cs.CL])

    [http://arxiv.org/abs/2310.11960](http://arxiv.org/abs/2310.11960)

    提出了一种名为快速多极化注意力的新型注意力机制，它使用分治策略将注意力的时间和内存复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。

    

    基于Transformer的模型已在许多领域取得了最先进的性能。然而，自注意力对于输入长度的二次复杂度限制了Transformer模型在长序列上的适用性。为了解决这个问题，我们提出了快速多极化注意力，一种使用分治策略来减少注意力时间和内存复杂度的新型注意力机制，将长度为n的序列的注意力复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。这种分层方法将查询、键和值分为O(log n)级的分辨率，较远距离的组群越来越大，并学习计算组群数量的权重。因此，以高效分层的方式在较低的分辨率中考虑远离彼此的标记之间的相互作用。快速多极化注意力的总体复杂度为O(n)或O(n log n)。

    Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}( \log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \
    
[^12]: 从不一致到洞察：对案例结果分类的理由数据集构建进行解析

    From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification. (arXiv:2310.11878v1 [cs.CL])

    [http://arxiv.org/abs/2310.11878](http://arxiv.org/abs/2310.11878)

    本研究关注法律自然语言处理中人工标注的变异问题，通过收集一组律师对案件结果评估存在分歧的数据集，对这些分歧进行了研究，构建了一个两级分类体系，并发现分歧主要源于对法律背景的不明确描述。

    

    在法律自然语言处理中，案例结果分类（COC）不仅需要准确性，还需要可信赖性和可解释性。现有的可解释COC研究仅限于由单个专家进行的注释。然而，众所周知，律师在对案件事实进行评估时可能存在分歧。因此，我们收集了一个新的数据集RAVE：欧洲人权法领域的理由变异，该数据集是从国际人权法领域的两位专家那里获得的，我们观察到他们之间存在弱一致性。我们研究了他们的分歧，并构建了一个两级任务无关的分类体系，同时补充了COC特定的子类别。据我们所知，这是法律自然语言处理领域首次关注人工标注的变异。我们定量评估了不同分类类别，并发现分歧主要源于对法律背景的不明确描述，这在COC元数据通常具有有限细粒度和噪声的情况下带来了挑战。我们进一步评估了SOTA COC模型在RAVE数据集上的可解释性，并观察到...

    In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of SOTA COC models on RAVE and observ
    
[^13]: 通过增强一致性建模来改进长文档主题划分模型

    Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling. (arXiv:2310.11772v1 [cs.CL])

    [http://arxiv.org/abs/2310.11772](http://arxiv.org/abs/2310.11772)

    本论文通过增强一致性建模的方式改进了长文档主题划分模型。具体地，通过引入主题感知的句子结构预测和对比语义相似性学习，该模型在捕捉语义一致性和主题划分之间的深层关系方面有了更好的表现。

    

    主题划分对于获取结构化的长文档和改善信息检索等下游任务至关重要。由于其能够自动从大量标注数据中探索主题转变的线索，最近的有监督神经模型极大地推动了长文档主题划分的发展，但对语义一致性和主题划分之间更深层次的关系尚未充分探索。因此，本文增强了有监督模型从结构和相似性两个方面捕捉一致性的能力，进一步提高了主题划分性能，包括主题感知的句子结构预测（TSSP）和对比语义相似性学习（CSSL）。具体而言，提出了TSSP任务，通过学习无序文档中相邻句子的原始关系，强制模型理解结构信息，该无序文档由同时破坏主题和

    Topic segmentation is critical for obtaining structured long documents and improving downstream tasks like information retrieval. Due to its ability of automatically exploring clues of topic shift from a large amount of labeled data, recent supervised neural models have greatly promoted the development of long document topic segmentation, but leaving the deeper relationship of semantic coherence and topic segmentation underexplored. Therefore, this paper enhances the supervised model's ability to capture coherence from both structure and similarity perspectives to further improve the topic segmentation performance, including the Topic-aware Sentence Structure Prediction (TSSP) and Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to force the model to comprehend structural information by learning the original relations of adjacent sentences in a disarrayed document, which is constructed by jointly disrupting the original document at the topic and 
    
[^14]: VECHR：欧洲人权法院漏洞类型的可解释和鲁棒分类数据集

    VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights. (arXiv:2310.11368v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.11368](http://arxiv.org/abs/2310.11368)

    VECHR是一个专家注释的多标签数据集，用于欧洲人权法院漏洞类型的可解释和鲁棒分类。该数据集帮助识别脆弱性，并提供解释理由。结果显示了该任务的挑战性，模型与专家的一致性有限，模型在处理域外数据时鲁棒性也较低。

    

    识别脆弱性对于了解和实施有针对性的支持以增强有需要的个人至关重要。这一点在欧洲人权法院尤为重要，法院将公约标准调整为满足实际个体需求，从而确保有效的人权保护。然而，脆弱性概念在欧洲人权法院仍然模糊不清，之前没有NLP研究涉及到这个问题。为了促进未来在这个领域的研究，我们提出了VECHR，一个新颖的专家注释的多标签数据集，包括脆弱性类型分类和解释理由。我们从预测和解释性的角度对VECHR上的最先进模型的性能进行了基准测试。我们的结果表明了这一任务的具有挑战性的特点，预测性能较低，并且模型和专家之间存在有限的一致性。此外，我们分析了这些模型处理域外数据的鲁棒性，并发现总体上受限。

    Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need. This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus ensures effective human rights protection. However, the concept of vulnerability remains elusive at the ECtHR and no prior NLP research has dealt with it. To enable future research in this area, we present VECHR, a novel expert-annotated multi-label dataset comprising of vulnerability type classification and explanation rationale. We benchmark the performance of state-of-the-art models on VECHR from both prediction and explainability perspectives. Our results demonstrate the challenging nature of the task with lower prediction performance and limited agreement between models and experts. Further, we analyze the robustness of these models in dealing with out-of-domain (OOD) data and observe overall limited per
    
[^15]: VoxArabica：一个稳健的方言感知阿拉伯语音识别系统

    VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System. (arXiv:2310.11069v1 [cs.CL])

    [http://arxiv.org/abs/2310.11069](http://arxiv.org/abs/2310.11069)

    VoxArabica是一个稳健的方言感知阿拉伯语音识别系统，通过开发和演示，实现了阿拉伯语方言识别和自动语音识别。该系统训练了各种模型用于不同方言的识别，并提供了多种功能的网络界面。

    

    阿拉伯语是一种复杂的语言，全球有超过4.5亿人口使用许多不同的方言和口音。由于语言的多样性和变化，为阿拉伯语构建一个稳健且通用的语音识别系统具有挑战性。在这项工作中，我们通过开发和演示一个名为VoxArabica的系统，解决了这个问题，用于方言识别(DID)和阿拉伯语自动语音识别(ASR)。我们在监督环境下训练了各种模型，例如HuBERT(DID)、Whisper和XLS-R(ASR)，用于阿拉伯语的DID和ASR任务。我们的DID模型被训练用于识别除了标准阿拉伯之外的17种不同的方言。我们在标准阿拉伯语(MSA)、埃及语、摩洛哥语和混合数据上微调我们的ASR模型。此外，对于ASR中的其他方言，我们提供了Whisper和MMS等不同模型的选择。我们将这些模型集成到一个单一的网络界面中，具有多样的功能，如音频录制、上传文件、模型选择和提出问题的选项。

    Arabic is a complex language with many varieties and dialects spoken by over 450 millions all around the world. Due to the linguistic diversity and variations, it is challenging to build a robust and generalized ASR system for Arabic. In this work, we address this gap by developing and demoing a system, dubbed VoxArabica, for dialect identification (DID) as well as automatic speech recognition (ASR) of Arabic. We train a wide range of models such as HuBERT (DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR tasks. Our DID models are trained to identify 17 different dialects in addition to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data. Additionally, for the remaining dialects in ASR, we provide the option to choose various models such as Whisper and MMS in a zero-shot setting. We integrate these models into a single web interface with diverse features such as audio recording, file upload, model selection, and the option to raise fl
    
[^16]: 新兴的AI辅助话语：ChatGPT与第二语言写作案例研究

    Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT. (arXiv:2310.10903v1 [cs.CL])

    [http://arxiv.org/abs/2310.10903](http://arxiv.org/abs/2310.10903)

    ChatGPT在第二语言写作中扮演重要角色，能够有效地与语言学习者合作，在保持个人表达声音的同时提升写作能力。这项研究为ChatGPT在学术写作中的应用提供了重要的探索。

    

    ChatGPT的快速普及引发了关于其对人类写作的影响的争议。在对写作标准下降的担忧中，本研究调查了ChatGPT在学术写作中的作用，特别是在语言学习者中的作用。采用案例研究方法，本研究考察了博士生凯灵（Kailing）在整个学术写作过程中如何使用ChatGPT的经验。研究采用活动理论作为理论框架来理解利用生成式AI工具进行写作的过程，研究数据包括半结构化访谈、写作样本和GPT日志的分析。结果表明，凯灵在各个写作阶段有效地与ChatGPT合作，同时保留自己独特的作者声音和主体性。这突显了ChatGPT等AI工具在增强语言学习者的学术写作能力时不会掩盖个人的真实性。本案例研究对ChatGPT在学术界中的使用进行了批判性的探讨。

    The rapid proliferation of ChatGPT has incited debates regarding its impact on human writing. Amid concerns about declining writing standards, this study investigates the role of ChatGPT in facilitating academic writing, especially among language learners. Using a case study approach, this study examines the experiences of Kailing, a doctoral student, who integrates ChatGPT throughout their academic writing process. The study employs activity theory as a lens for understanding writing with generative AI tools and data analyzed includes semi-structured interviews, writing samples, and GPT logs. Results indicate that Kailing effectively collaborates with ChatGPT across various writing stages while preserving her distinct authorial voice and agency. This underscores the potential of AI tools such as ChatGPT to enhance academic writing for language learners without overshadowing individual authenticity. This case study offers a critical exploration of how ChatGPT is utilized in the academi
    
[^17]: BiomedJourney: 指导学习多模态患者旅程中的反事实生物医学图像生成

    BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v1 [cs.CV])

    [http://arxiv.org/abs/2310.10765](http://arxiv.org/abs/2310.10765)

    提出了一种新颖的方法BiomedJourney，通过指导学习多模态患者旅程，进行反事实生物医学图像生成。使用GPT-4处理图像报告生成疾病进展的自然语言描述，并训练潜在扩散模型。

    

    随着自然语言指令图像编辑的指导学习取得了快速进展，如InstructPix2Pix，生物医学领域可以将这些方法应用于反事实图像生成，从而帮助区分因果结构和伪相关，并促进疾病进展建模的稳健图像解释。然而，通用的图像编辑模型并不适用于生物医学领域，反事实生物医学图像生成的研究还远未深入。在本文中，我们提出了一种新颖的方法BiomedJourney，通过指导学习多模态患者旅程，进行反事实生物医学图像生成。给定一个拍摄于不同时间点的两个生物医学图像的患者，我们使用GPT-4处理相应的图像报告，并生成疾病进展的自然语言描述。然后，使用生成的三元组（先前图像、进展描述、新图像）来训练一个潜在扩散模型。

    Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion mode
    
[^18]: 多智能体协作的大型语言模型理论

    Theory of Mind for Multi-Agent Collaboration via Large Language Models. (arXiv:2310.10701v1 [cs.CL])

    [http://arxiv.org/abs/2310.10701](http://arxiv.org/abs/2310.10701)

    本研究通过在多智能体合作游戏中评估基于大型语言模型的智能体，发现它们可以表现出协作行为和高级理论推理能力，并通过使用明确的信念状态表示来提高任务性能和理论推理准确性。

    

    大型语言模型在推理和规划方面取得了令人瞩目的成就，但它在多智能体协作方面的能力尚未得到深入探索。本研究通过对比多智能体强化学习和基于规划的基准方法，在多智能体合作文本游戏中评估了基于大型语言模型的智能体在理论推理任务上的表现。我们观察到基于大型语言模型的智能体出现了协作行为和高级理论推理能力的证据。我们的结果揭示了基于大型语言模型的智能体在长期规划上存在优化的局限性，以及对任务状态的错误认知。我们尝试使用明确的信念状态表示来缓解这些问题，并发现它可以提高大型语言模型智能体的任务性能和理论推理的准确性。

    While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
    
[^19]: 将代码语义与LLMs相结合：代码生成的语义思维链约束

    Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation. (arXiv:2310.10698v1 [cs.CL])

    [http://arxiv.org/abs/2310.10698](http://arxiv.org/abs/2310.10698)

    本文提出了一种“语义思维链”的方法，用于在代码生成中引入语义信息，并以此来实现更精细的代码理解和表示。

    

    大型语言模型（LLMs）在代码生成方面展示了出色的能力。然而，自动化代码生成仍然具有挑战性，因为它需要对自然语言需求和代码之间进行高级语义映射。大多数现有基于LLMs的代码生成方法依赖于仅使用解码器的因果语言模型，通常将代码仅视为纯文本标记，即将需求作为提示输入，并将代码作为一系列平面标记输出，可能会忽略源代码中固有的丰富语义特征。为了弥合这一差距，本文提出了一种“语义思维链”的方法，引入了代码的语义信息，称为SeCoT。我们的动机是源代码的语义信息（例如数据流和控制流）更精确地描述了程序执行行为、意图和功能。通过引导LLMs考虑和整合语义信息，我们可以实现对代码更精细的理解和表示，增强代码生成的质量。

    Large language models (LLMs) have showcased remarkable prowess in code generation. However, automated code generation is still challenging since it requires a high-level semantic mapping between natural language requirements and codes. Most existing LLMs-based approaches for code generation rely on decoder-only causal language models often treate codes merely as plain text tokens \ie feeding the requirements as a prompt input, and outputing code as flat sequence of tokens, potentially missing the rich semantic features inherent in source code. To bridge this gap, this paper proposes the "Semantic Chain-of-Thought" approach to intruduce semantic information of code, named SeCoT. Our motivation is that the semantic information of the source code (\eg data flow and control flow) describes more precise program execution behavior, intention and function. By guiding LLM consider and integrate semantic information, we can achieve a more granular understanding and representation of code, enhan
    
[^20]: 超越文档边界的上下文预训练：语言模型

    In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10638](http://arxiv.org/abs/2310.10638)

    本论文提出了一种超越文档边界的上下文预训练方法，通过在相关文档序列上训练语言模型，鼓励模型进行跨文档的阅读和推理。该方法通过改变文档顺序并应用现有的预训练管道来实现。

    

    目前，大型语言模型（LMs）通过预测给定文档前缀的标记来进行训练，从而能够直接进行长篇生成和提示式任务，这可以简化为文档完成。现有的预训练管道通过连接随机组合的短文档来训练LMs，以创建输入上下文，但前一个文档对于预测下一个文档没有提供任何信号。我们提出了一种新方法——上下文预训练，即在相关文档序列上预先训练语言模型，从而明确鼓励它们跨越文档边界进行阅读和推理。我们可以通过改变文档顺序，使每个上下文包含相关的文档，并直接应用现有的预训练管道来进行上下文预训练。然而，这个文档排序问题很具有挑战性。有数十亿个文档，我们希望在每个文档中最大化上下文相似性而不重复任何数据。

    Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
    
[^21]: RegaVAE：一种检索增强的高斯混合变分自编码器用于语言建模

    RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling. (arXiv:2310.10567v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10567](http://arxiv.org/abs/2310.10567)

    RegaVAE是一种基于变分自编码器的检索增强语言模型，它通过将文本编码到潜在空间中捕获当前和未来的信息，并使用潜在变量进行有效的信息聚合。

    

    检索增强的语言模型在解决语言模型中的过时信息和虚构现象等问题上表现出了潜力。然而，当前的研究面临两个主要问题：1）确定要检索的信息，2）在生成过程中有效地结合检索的信息。我们认为，有价值的检索信息不仅应与当前的源文本相关，还应考虑到未来的目标文本，因为语言模型是对未来令牌进行建模的。此外，我们提出，使用从紧凑潜在空间中派生的潜在变量进行聚合比使用受上下文长度限制和容易受噪声干扰的显式原始文本更有效。因此，我们引入了RegaVAE，这是一个基于变分自编码器（VAE）构建的检索增强的语言模型。它将文本语料库编码为潜在空间，从源文本和目标文本中捕获当前和未来的信息。此外，我们利用VAE初始化模型参数以提高生成能力。

    Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs). However, current research faces two main problems: 1) determining what information to retrieve, and 2) effectively combining retrieved information during generation. We argue that valuable retrieved information should not only be related to the current source text but also consider the future target text, given the nature of LMs that model future tokens. Moreover, we propose that aggregation using latent variables derived from a compact latent space is more efficient than utilizing explicit raw text, which is limited by context length and susceptible to noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model built upon the variational auto-encoder (VAE). It encodes the text corpus into a latent space, capturing current and future information from both source and target text. Additionally, we leverage the VAE to initialize the 
    
[^22]: 跨语言多语言模型中事实知识的跨语言一致性

    Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10378](http://arxiv.org/abs/2310.10378)

    本论文研究了多语言预训练语言模型中事实知识的跨语言一致性，提出了一种新的度量方法，并通过分析模型大小、语言配对等因素发现了影响一致性的因素。实验结果表明，增加模型大小可以提高准确性，但不会改善跨语言一致性。

    

    多语言大规模预训练语言模型（PLM）显示存储了大量的事实知识，但在不同语言之间存在较大的变化。为了确保不同语言背景的用户从同一个模型中获得一致的反馈，我们研究了各种多语言PLM中事实知识的跨语言一致性（CLC）。为此，我们提出了一种基于排序的一致性（RankC）度量，用于独立于准确性评估跨语言间的知识一致性。利用这个度量方法，我们对决定CLC的因素进行了深入分析，包括模型层面和语言对层面。在其他结果中，我们发现增加模型大小可以提高大多数语言中的事实探测准确性，但不能改善跨语言一致性。最后，我们通过模型编辑在PLMs中插入新的事实关联进行了一个CLC的案例研究。对一小部分事实进行了实验。

    Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
    
[^23]: AdaLomo: 低内存优化与自适应学习率

    AdaLomo: Low-memory Optimization with Adaptive Learning Rate. (arXiv:2310.10195v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10195](http://arxiv.org/abs/2310.10195)

    AdaLomo是一种低内存优化方法，通过引入自适应学习率来改善大型语言模型优化器的性能，同时保持内存效率。

    

    大型语言模型取得了显著的成功，但其庞大的参数规模需要大量的训练内存，从而设置了很高的门槛。尽管最近提出的低内存优化（LOMO）减少了内存占用，但其优化技术类似于随机梯度下降，对超参数敏感并展现出次优的收敛性，无法与当前大型语言模型优化器AdamW的性能相媲美。通过对Adam优化器进行经验分析，我们发现相对于动量来说，自适应学习率对于弥合差距更为关键。基于此，我们引入了带有自适应学习率的低内存优化（AdaLomo），为每个参数提供自适应学习率。为了保持内存效率，我们在优化器状态中采用非负矩阵分解来估计二阶矩。此外，我们建议使用分组更新规范。

    Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update norma
    
[^24]: 基于迭代示范选择的上下文学习

    In-Context Learning with Iterative Demonstration Selection. (arXiv:2310.09881v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.09881](http://arxiv.org/abs/2310.09881)

    这项研究提出了一种基于迭代示范选择的上下文学习方法，通过使用零样本链式思维推理来选择与测试样本不同但仍与之强相关的示范作为学习的上下文。

    

    受到规模的进展的推动，大型语言模型(LLMs)通过上下文学习(ICL)展示了强大的少样本学习能力。然而，ICL的性能已被证明对于示范的选择非常敏感。选择最合适的示范作为上下文仍然是一个持续挑战和一个未解决的问题。现有文献已经强调了选择那些与测试样本不同或语义相似性的示范的重要性，而忽视了最优示范选择维度是任务特定的事实。借鉴两个维度的优点，我们提出了迭代示范选择(IDS)。使用零样本链式思维推理(Zero-shot-CoT)，IDS迭代地选择那些与测试样本不同但仍与之强相关的示范作为ICL的示范。具体而言，IDS在示范选择之前将Zero-shot-CoT应用于测试样本。输出的推理路径是...

    Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Leveraging the merits of both dimensions, we propose Iterative Demonstration Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is 
    
[^25]: 合并专家：改进混合专家方法的计算效率

    Merging Experts into One: Improving Computational Efficiency of Mixture of Experts. (arXiv:2310.09832v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.09832](http://arxiv.org/abs/2310.09832)

    本文提出了一种名为“合并专家”的计算高效的方法，通过将计算成本降低到单个专家的水平来改进混合专家方法的计算效率，实验证明该方法显著提高了计算效率。

    

    将语言模型的规模扩大通常会带来NLP任务的显著进展，但往往会伴随着不断增长的计算成本。尽管稀疏的混合专家（MoE）可以通过激活每个输入的一个小子集参数（例如一个专家）来减少成本，但如果增加激活的专家数量，其计算将显著增加，限制了其实际效用。在本文中，我们首先展示了选择多个专家的优越性，然后提出了一种计算高效的方法，称为“合并专家”（MEO），将计算成本降低到单个专家的水平。广泛的实验表明，MEO显着提高了计算效率，例如，FLOPS从普通MoE的72.0G降低到28.6G（MEO）。此外，我们提出了一种基于标记的注意力模块，进一步增强了效率。

    Scaling the size of language models usually leads to remarkable advancements in NLP tasks. But it often comes with a price of growing computational cost. Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility. Can we retain the advantages of adding more experts without substantially increasing the computational costs? In this paper, we first demonstrate the superiority of selecting multiple experts and then propose a computation-efficient approach called \textbf{\texttt{Merging Experts into One}} (MEO), which reduces the computation cost to that of a single expert. Extensive experiments show that MEO significantly improves computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G (MEO). Moreover, we propose a token-level attention block that further enhances the ef
    
[^26]: 对常识感知对话代理的对话思路提炼

    Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents. (arXiv:2310.09343v1 [cs.CL])

    [http://arxiv.org/abs/2310.09343](http://arxiv.org/abs/2310.09343)

    本论文提出了一种对话思路提炼的知识提炼框架，利用大型语言模型作为教师，并通过对齐过滤器选择性地提炼一致和有用的理由。同时，介绍了一个可靠的对话思路推理器，用于生成响应。

    

    实现人类化的聊天机器人需要使用常识推理来有效地理解和回应对话中的隐含信息。然而，实现这样的连贯性和信息含量是一个非常困难的任务。即使对于大型语言模型（LLMs），在一个单跳内识别和聚合关键证据的任务也是具有相当大挑战性的。这个复杂性的原因是这样的证据分散在对话的多个轮次中，因此需要在多个跳中进行整合。因此，我们的焦点是促进对话上的多跳推理，即对话思路（CoT）推理。为此，我们提出了一个知识提炼框架，通过对齐过滤器选择性地提炼一致和有用的理由，利用LLMs作为不可靠的教师。我们进一步提出了DOCTOR，一个提供可靠的CoT理由以进行响应生成的对话思路推理器。

    Human-like chatbots necessitate the use of commonsense reasoning in order to effectively comprehend and respond to implicit information present within conversations. Achieving such coherence and informativeness in responses, however, is a non-trivial task. Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge. This complexity arises because such evidence is scattered across multiple turns in a conversation, thus necessitating integration over multiple hops. Hence, our focus is to facilitate such multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought (CoT) reasoning. To this end, we propose a knowledge distillation framework that leverages LLMs as unreliable teachers and selectively distills consistent and helpful rationales via alignment filters. We further present DOCTOR, a DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for response generation. We
    
[^27]: 一种用于计算机控制的零样本语言代理机制及其结构反思

    A Zero-Shot Language Agent for Computer Control with Structured Reflection. (arXiv:2310.08740v1 [cs.CL])

    [http://arxiv.org/abs/2310.08740](http://arxiv.org/abs/2310.08740)

    这种论文提出了一种零样本语言代理机制，它不需要专家示踪，并且通过自我反思和结构化思考管理来学习和改善计算机上的控制，表现出高效的推理能力。

    

    大型语言模型（LLMs）在计划和执行高级目标方面的能力不断增强，如在活动的计算机环境（例如MiniWoB ++）中。最近的研究通常要求模型通过监督学习或少/多样本提示从任务的跟踪示例中学习。在没有这些跟踪示例的情况下，一个代理机制如何能够自主学习并改善在计算机上的控制仍然是一个挑战，这限制了一个代理机构执行新任务的能力。我们通过零样本代理机制来解决这个问题，它不需要给定的专家示踪。我们的代理机制对于部分观察环境上的可执行行动进行规划，并通过自我反思和结构化思考管理来识别和学习错误，从而逐步推进任务。在MiniWoB ++的简单任务中，我们展示了我们的零样本代理机制往往胜过最近的SoTA，具有更高效的推理能力。对于更复杂的任务，我们的反思代理机制与先前的代理机制持平。

    Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We approach this problem with a zero-shot agent that requires no given expert traces. Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management. On the easy tasks of MiniWoB++, we show that our zero-shot agent often outperforms recent SoTAs, with more efficient reasoning. For tasks with more complexity, our reflective agent performs on par with prior 
    
[^28]: LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models

    LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])

    [http://arxiv.org/abs/2310.08659](http://arxiv.org/abs/2310.08659)

    本论文提出了LoftQ：一种针对大型语言模型的LoRA精调感知量化框架。该框架同时对LLM进行量化，并为LoRA精调找到适当的低秩初始化，以缓解量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。

    

    量化是为大型语言模型提供服务的不可或缺的技术，并最近被应用于LoRA精调中。本文关注在预训练模型上同时应用量化和LoRA精调的场景。在这种情况下，常常观察到完整精调和量化加LoRA精调方法之间在下游任务表现上存在一致的差距。为了解决这个问题，我们提出了LoftQ（LoRA-Fine-Tuning-aware Quantization）——一种新的量化框架，用于同时对LLM进行量化，并找到适当的低秩初始化来进行LoRA精调。这种初始化减轻了量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。我们在自然语言理解、问答、摘要和自然语言生成任务上评估了我们的方法。实验证明，我们的方法非常有效，在性能上优于现有的方法。

    Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
    
[^29]: 使用思路链进行大型语言模型的少样本知识库问题生成

    Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation. (arXiv:2310.08395v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.08395](http://arxiv.org/abs/2310.08395)

    本文提出了一种使用思路链（CoT）对大型语言模型进行少样本知识库问题生成的方法，该方法将问题生成任务形式化为推理问题，并通过检索支持性逻辑形式和编写提示来实现生成过程。

    

    知识库问答生成（KBQG）的任务是将逻辑形式转化为自然语言问题。由于大规模问题注释的昂贵成本，在低资源场景下急需开发KBQG方法。然而，当前方法在少样本问题生成中过于依赖注释数据的微调，这对于少样本问题生成并不合适。大型语言模型（LLM）的出现展示了它们在少样本任务中的印象力泛化能力。受到思路链（CoT）提示的启发，这是一种用于推理的上下文学习策略，我们将KBQG任务形式化为推理问题，其中一个完整问题的生成被分为一系列的子问题生成。我们提出的提示方法KQG-CoT首先从未标记数据池中检索支持性的逻辑形式，考虑逻辑形式的特征。然后，我们编写一个提示来明确推理链的生成过程。

    The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generati
    
[^30]: 通过各向同性和近端搜索实现细粒度对话解码

    Fine-grained Conversational Decoding via Isotropic and Proximal Search. (arXiv:2310.08130v1 [cs.CL])

    [http://arxiv.org/abs/2310.08130](http://arxiv.org/abs/2310.08130)

    本论文提出了一种细粒度的对话解码方法，通过各向同性和近端搜索（IPS）生成信息集中的语义回应，并在对话领域的评估中取得了优于现有方法的效果。

    

    通常采用通用的文本解码方法来进行对话回应生成。虽然采用了对话特定的编码方法可以提高生成的回应质量，但对话解码方法仍然未被充分探索。受到wu2023learning的启发，认为好的对话特征空间应遵循局部性和各向同性规则，我们提出了一种细粒度的对话解码方法，称为各向同性和近端搜索（IPS）。我们的方法旨在生成信息集中的语义回应，同时保持对上下文的信息量和区分度。实验证明，我们的方法在对话领域中的自动评估和人工评估指标上优于现有的解码策略。更深入的分析进一步证实了我们方法的有效性。

    General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by \citet{wu2023learning} that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed \textit{isotropic and proximal search (IPS)}. Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach.
    
[^31]: LLM的民主化：自我改进的开源模型中性能与成本的权衡探索

    Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models. (arXiv:2310.07611v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.07611](http://arxiv.org/abs/2310.07611)

    本论文通过提出自我改进的开源模型和一个新的排名指标（PeRFICS）来解决性能和成本之间的权衡问题。实验证明，所提出的方法可以在各种大小的开源模型中显著提高性能，并为开源LLMs的发展提供了一种成本优化的方法。

    

    私有LLMs的主导地位导致了受限的访问和提高的信息隐私问题。对于信息敏感和高容量应用程序来说，高性能的开源替代方案至关重要，但通常在性能方面落后。为了解决这一差距，我们提出了(1)一种无外部影响的非针对性迭代自我批评和自我改进的变体。(2)一种新颖的排名指标 - 性能、改进和推理成本得分(PeRFICS)，以考虑改进后的性能和成本来找到给定任务的最佳模型。我们的实验证明，不同大小的最先进开源模型从7B到65B，平均改善了8.2%的基准性能。令人惊讶的是，即使是内存占用极小的模型，比如Vicuna-7B，在整体上也有11.74%的改善，并在Vicuna基准测试的高创造力和开放性任务中提高了25.39%。Vicuna-13B更进一步，在改进后超越了ChatGPT。这项工作具有重要的创新和贡献，为开源LLMs的发展提供了一种成本优化的方法。

    The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. High-performing open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance. To address this gap, we propose (1) A untargeted variant of iterative self-critique and self-refinement devoid of external influence. (2) A novel ranking metric - Performance, Refinement, and Inference Cost Score (PeRFICS) - to find the optimal model for a given task considering refined performance and cost. Our experiments show that SoTA open source models of varying sizes from 7B - 65B, on average, improve 8.2% from their baseline performance. Strikingly, even models with extremely small memory footprints, such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39% improvement in high-creativity, open ended tasks on the Vicuna benchmark. Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement. This work has p
    
[^32]: "两个运动的故事": 使用弱监督的基于图的结构化预测方法鉴别和比较#BlackLivesMatter和#BlueLivesMatter运动相关的推文中的观点

    "A Tale of Two Movements": Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction. (arXiv:2310.07155v1 [cs.CL])

    [http://arxiv.org/abs/2310.07155](http://arxiv.org/abs/2310.07155)

    本论文提出了一种基于图的弱监督方法，用于识别和比较#BlackLivesMatter和#BlueLivesMatter运动相关推文中的观点。通过将文本转换为图，并根据作者的社交网络进行结构化预测，该方法能够在缺乏标注数据的情况下模拟运动中的观点。

    

    社交媒体已成为推动社会变革的重要因素，通过促进在线社会运动的形成。自动理解推动运动和反对运动声音的观点是一项具有挑战性的任务，因为很难获得带有注释的数据。我们提出了一种弱监督的基于图的方法，显式地对#BlakcLivesMatter相关推文中的观点进行建模。我们的方法利用了社会-语言学的数据表示。我们通过将文本分解为结构化元素并与作者的社交网络相连接将其转换为图形，然后使用结构化预测来识别观点。我们的方法使用了一小组标记示例。我们尝试使用大语言模型生成人工训练示例进行实验，并将其与手动注释进行比较，发现它们具有可比较的性能。我们使用人工注释的测试集进行定量和定性分析。我们的模型...

    Social media has become a major driver of social change, by facilitating the formation of online social movements. Automatically understanding the perspectives driving the movement and the voices opposing it, is a challenging task as annotated data is difficult to obtain. We propose a weakly supervised graph-based approach that explicitly models perspectives in #BackLivesMatter-related tweets. Our proposed approach utilizes a social-linguistic representation of the data. We convert the text to a graph by breaking it into structured elements and connect it with the social network of authors, then structured prediction is done over the elements for identifying perspectives. Our approach uses a small seed set of labeled examples. We experiment with large language models for generating artificial training examples, compare them to manual annotation, and find that it achieves comparable performance. We perform quantitative and qualitative analyses using a human-annotated test set. Our model
    
[^33]: 跨越门槛：通过检索增强和损失加权实现惯用机器翻译

    Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting. (arXiv:2310.07081v1 [cs.CL])

    [http://arxiv.org/abs/2310.07081](http://arxiv.org/abs/2310.07081)

    本文通过引入检索增强和损失加权两种方法，成功改进了机器翻译系统对于惯用表达的翻译准确性。

    

    惯用语在日常语言中很常见，但对于翻译人员来说常常是一个挑战，因为它们的意义不是由它们的部分的意义推导出来的。尽管取得了重大进展，机器翻译系统仍然难以翻译惯用表达。我们提供了惯用翻译和相关问题的简单描述。这使我们能够进行一个综合实验，揭示了基于transformer的机器翻译模型在正确地默认采用惯用翻译时的临界点。为了扩大多语资源，我们编制了一个包含法语、芬兰语和日语中包含惯用表达的大约4k个自然句子的数据集。为了改进对自然惯用语的翻译，我们引入了两种简单但有效的技术：在可能具有惯用性的句子上策略性地加权训练损失，并使用检索增强模型。这不仅使强大的预训练机器翻译模型在惯用句子上的准确性提高了最多13%，还改进了翻译的准确性。

    Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute a
    
[^34]: 用聚焦-信息熵改进对比学习句子嵌入

    Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE. (arXiv:2310.06918v1 [cs.CL])

    [http://arxiv.org/abs/2310.06918](http://arxiv.org/abs/2310.06918)

    本研究提出了一种聚焦-信息熵函数，利用硬负样本挖掘改进了对比学习句子嵌入的方法，并在各种基准测试中验证了其性能提升。

    

    SimCSE的最新成功极大地推进了句子表示的最新技术。然而，SimCSE的原始表达没有充分利用对比学习中硬负样本的潜力。本研究引入了一种无监督对比学习框架，将SimCSE与硬负样本挖掘相结合，旨在提高句子嵌入的质量。所提出的聚焦-信息熵函数在对比目标中引入了自适应调节项，降低与易负样本相关的损失，并鼓励模型关注于困难负样本。在各种STS基准测试上的实验结果表明，我们的方法在斯皮尔曼相关系数、表示对齐性和一致性方面改进了句子嵌入。

    The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that combines SimCSE with hard negative mining, aiming to enhance the quality of sentence embeddings. The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives. Experimentation on various STS benchmarks shows that our method improves sentence embeddings in terms of Spearman's correlation and representation alignment and uniformity.
    
[^35]: 人类和语言模型在预测重复文本时存在分歧

    Humans and language models diverge when predicting repeating text. (arXiv:2310.06408v1 [cs.CL])

    [http://arxiv.org/abs/2310.06408](http://arxiv.org/abs/2310.06408)

    人类和语言模型在预测重复文本时存在分歧，虽然在文本片段第一次呈现时表现一致，但当记忆（或上下文学习）开始发挥作用时，表现快速分歧。研究发现这种分歧源于中间层的特定注意力头，并通过加入幂律近期偏好使模型更接近人类行为。

    

    使用训练于下一个单词预测任务的语言模型已被证明能准确地模拟人类的单词预测和阅读速度。然而，与这些研究结果相反，我们展示了一种情况，在这种情况下，人类和语言模型的表现存在分歧。我们收集了人类对五个由重复文本片段组成的刺激的下一个单词预测的数据集。人类和GPT-2语言模型在文本片段的第一次呈现中预测结果高度一致，但当记忆（或上下文学习）开始发挥作用时，它们的表现迅速分歧。我们追踪了这种分歧的原因，发现它源于中间层的特定注意力头。向这些注意力头加入幂律近期偏好能够使模型的表现更接近人类。我们希望这个情景能够促进将语言模型更加接近人类行为的未来研究。

    Language models that are trained on the next-word prediction task have been shown to accurately model human behavior in word prediction and reading speed. In contrast with these findings, we present a scenario in which the performance of humans and LMs diverges. We collected a dataset of human next-word predictions for five stimuli that are formed by repeating spans of text. Human and GPT-2 LM predictions are strongly aligned in the first presentation of a text span, but their performance quickly diverges when memory (or in-context learning) begins to play a role. We traced the cause of this divergence to specific attention heads in a middle layer. Adding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans. We hope that this scenario will spur future work in bringing LMs closer to human behavior.
    
[^36]: Hexa: 知识驱动的对话系统的自我提升

    Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])

    [http://arxiv.org/abs/2310.06404](http://arxiv.org/abs/2310.06404)

    本论文提出了一种自我提升的方法，用于改进知识驱动对话生成的中间步骤的生成性能。通过引入引导提示和修改损失函数的自举策略，提高了生成自动生成回答的多样性，并在各种基准数据集上实验证明了该方法的有效性。

    

    知识驱动的对话生成中一种常见的做法是使用模块化的方法明确地利用中间步骤（如网络搜索、记忆检索）。然而，与对话响应相比，这些步骤的数据往往难以获取，因为在普通对话中无法观察到它们。为了填补这些数据的缺失，我们开发了一种自我提升方法，以改进中间步骤的生成性能，而不需要地面真实数据。具体而言，我们提出了一种新颖的引导提示和修改的损失函数的引导自动生成回答多样性的自举方法。通过在各种基准数据集上进行实验，我们经验证明我们的方法成功地利用了自我提升机制，在生成中间和最终回答方面改善了知识驱动对话生成任务的性能。

    A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
    
[^37]: 重新思考基于预训练序列到序列模型的关键词生成的模型选择和解码问题

    Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models. (arXiv:2310.06374v1 [cs.CL])

    [http://arxiv.org/abs/2310.06374](http://arxiv.org/abs/2310.06374)

    本文通过系统分析了基于预训练语言模型的关键词生成任务中模型选择和解码策略的影响。并发现传统的模型选择智慧缺乏深度，并且在关键词生成中贪婪搜索的召回率较低。

    

    关键词生成是自然语言处理中一个长期存在的任务，具有广泛的应用。序列到序列预训练语言模型的出现为关键词生成带来了革命性的改进，取得了令人期待的性能提升。然而，许多设计决策仍未被探索，并且常常是随意决策的。本文对基于预训练语言模型的关键词生成任务的模型选择和解码策略进行了系统分析。我们首先通过一个基于注意力的假设阐明了为什么序列到序列预训练语言模型适用于关键词生成。然后我们发现传统的选择序列到序列预训练语言模型的智慧缺乏深度：（1）仅增加模型大小或进行任务特定适应性调整并不是参数高效的；（2）尽管将领域内预训练与任务适应结合有利于关键词生成，但它在一定程度上会阻碍泛化能力。关于解码，我们证明贪婪搜索虽然在 F1 得分上表现出色，但在召回率方面落后于基于采样的方法。

    Keyphrase Generation (KPG) is a longstanding task in NLP with widespread applications. The advent of sequence-to-sequence (seq2seq) pre-trained language models (PLMs) has ushered in a transformative era for KPG, yielding promising performance improvements. However, many design decisions remain unexplored and are often made arbitrarily. This paper undertakes a systematic analysis of the influence of model selection and decoding strategies on PLM-based KPG. We begin by elucidating why seq2seq PLMs are apt for KPG, anchored by an attention-driven hypothesis. We then establish that conventional wisdom for selecting seq2seq PLMs lacks depth: (1) merely increasing model size or performing task-specific adaptation is not parameter-efficient; (2) although combining in-domain pre-training with task adaptation benefits KPG, it does partially hinder generalization. Regarding decoding, we demonstrate that while greedy search delivers strong F1 scores, it lags in recall compared with sampling-based
    
[^38]: Siamese编码器的归因方法

    An Attribution Method for Siamese Encoders. (arXiv:2310.05703v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05703](http://arxiv.org/abs/2310.05703)

    本文提出了一种适用于Siamese编码器的局部归因方法，通过将集成梯度原理推广到具有多个输入的模型，该方法能够解释句子转换器模型中重要的预测令牌对，主要集中在名词和动词上。

    

    尽管句子转换器等Siamese编码器模型取得了成功，但人们对它们关注的输入方面知之甚少。一个障碍是它们的预测不能归因于个别特征，因为它们比较的是两个输入而不是一个输入。本文通过将集成梯度原理推广到具有多个输入的模型，推导出一种适用于Siamese编码器的局部归因方法。该解决方案采用特征对归因的形式，并可将其简化为句子转换器的令牌-令牌矩阵。我们的方法涉及引入集成雅可比矩阵，并继承了集成梯度的优势形式特性：它考虑了模型的完整计算图，并确保收敛到实际预测结果。一项实验表明，在句子转换器中，很少的令牌对往往可以解释大部分的预测，并且它们主要集中在名词和动词上。然而，为了获得准确的预测，它需要关注大多数的令牌。

    Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The solution takes the form of feature-pair attributions, and can be reduced to a token-token matrix for STs. Our method involves the introduction of integrated Jacobians and inherits the advantageous formal properties of integrated gradients: it accounts for the model's full computation graph and is guaranteed to converge to the actual prediction. A pilot study shows that in an ST few token-pairs can often explain large fractions of predictions, and it focuses on nouns and verbs. For accurate predictions, it however needs to attend to the majorit
    
[^39]: 语言模型是否能够学习类比推理？研究训练目标和与人类表现的比较。

    Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance. (arXiv:2310.05597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05597](http://arxiv.org/abs/2310.05597)

    本文研究了语言模型是否能够学习类比推理的任务，并测试了几种学习方法。实验结果表明，模型能够通过少量数据学习类比推理，并在与人类基准进行比较后接近人类的表现水平。

    

    虽然类比是评估自然语言处理中词嵌入的常见方式，但研究类比推理是否是一种可以学习的任务也很有意义。本文测试了几种学习基本类比推理的方法，特别关注的是那些更符合人类类比推理评估标准的类比。我们的实验发现，模型能够在少量数据的情况下学习类比推理。此外，我们还将我们的模型与具有人类基准的数据集进行比较，并发现在训练后，模型接近人类的表现水平。

    While analogies are a common way to evaluate word embeddings in NLP, it is also of interest to investigate whether or not analogical reasoning is a task in itself that can be learned. In this paper, we test several ways to learn basic analogical reasoning, specifically focusing on analogies that are more typical of what is used to evaluate analogical reasoning in humans than those in commonly used NLP benchmarks. Our experiments find that models are able to learn analogical reasoning, even with a small amount of data. We additionally compare our models to a dataset with a human baseline, and find that after training, models approach human performance.
    
[^40]: InterroLang: 通过对话式解释探索NLP模型和数据集

    InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations. (arXiv:2310.05592v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05592](http://arxiv.org/abs/2310.05592)

    本文介绍了一种名为InterroLang的对话式解释工具，通过对话界面帮助用户以情境化的方式探索具有解释的NLP模型和数据集，支持澄清和后续问题，并通过自然语言接口进行交互。研究中还提出了新的NLP操作，并在三个NLP任务上展示了其泛化能力。进行了用户研究评估工具的正确性、有用性和可模拟性。

    

    最近发展的NLP可解释性方法以各种方式打开了黑箱，但是在这一努力中缺少的是提供对话界面的交互工具。这样的对话系统可以帮助用户以情境化的方式通过澄清或后续问题，并通过自然语言接口探索具有解释的数据集和模型。我们将对话式解释框架TalkToModel（Slack等，2022）改编为NLP领域，增加了自由文本解释等新的NLP特定操作，并展示了它在三个NLP任务（对话行为分类，问答，仇恨言论检测）上的泛化能力。为了识别用户的解释查询，我们评估了精调和少样本提示模型，并实施了一种新颖的基于适配器的方法。然后我们进行了两个用户研究：（1）评估对话的正确性和有用性，（2）可模拟性，即如何对象化用户对话。

    While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how object
    
[^41]: 建立可信度：重新思考任务和模型评估

    Establishing Trustworthiness: Rethinking Tasks and Model Evaluation. (arXiv:2310.05442v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05442](http://arxiv.org/abs/2310.05442)

    重新思考自然语言处理中的任务和模型评估，以应对通用目标无关方法引发的挑战，并且关注在不断增加的实际应用中建立可信度的需求。

    

    语言理解是一种多方面的认知能力，自然语言处理（NLP）社区一直在努力将其计算建模几十年。传统上，语言智能的多个方面被分隔成具有专门模型架构和相应评估协议的任务。随着大型语言模型（LLMs）的出现，社区目睹了一种通过生成模型驱动的通用目标无关方法的戏剧性转变。因此，传统的分隔式语言任务概念正在被打破，随之而来的是对评估和分析的越来越大的挑战。与此同时，LLMs正在更多现实世界的场景中进行部署，包括以前未预料到的零-shot设定，增加了对可信和可靠系统的需求。因此，我们认为是时候重新思考NLP中任务和模型评估的定义，并追求对语言的更全面的视角，将可信度放在首位。

    Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing tr
    
[^42]: 在心理健康领域中通过任务自适应分词来增强长文本生成

    Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05317](http://arxiv.org/abs/2310.05317)

    该论文提出了一种任务自适应分词的方法，通过优化分词过程来增强在心理健康领域中的长文本生成。实验证明，该方法在减少标记数量的情况下显著提高了生成性能，并且可与大型语言模型结合使用。

    

    我们提出了任务自适应分词作为一种方式，将生成流水线适应于下游任务的特定要求，并增强在心理健康领域的长文本生成。受认知科学的启发，我们的任务自适应分词器从多个结果中采样可变的分段，采样概率基于任务特定的数据进行优化。我们引入了一种构建专用词汇的策略，并介绍了一种词汇合并协议，可以将任务特定的标记整合到预训练模型的分词步骤中。通过对中英文心理问答任务进行广泛实验，我们发现我们的任务自适应分词方法在使用更少的标记的情况下带来了显著的生成性能提升，最高可达60%。初步实验表明，使用我们的分词方法与非常大的语言模型结合能够得到有希望的结果。

    We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
    
[^43]: 个性化随机鹦鹉更危险吗？评估对话系统中的人格偏见

    Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05280](http://arxiv.org/abs/2310.05280)

    这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。

    

    最近大型语言模型的发展使其能够按照自由形式的指令进行操作，包括在对话中模仿通用或特定人口群体的人格。通用人格指的是来自某一人口群体的个体（例如亚洲人），而特定人格可以是历史人物的实际姓名。虽然采用人格使对话系统更具吸引力和亲和力，但也存在潜在风险，可能通过与用户的交互而加剧社会偏见，进一步造成社会伤害。在本文中，我们系统地研究“人格偏见”，我们将其定义为有害对话模型行为对不同人格采用的敏感性。我们将人格偏见分为有害表达和有害认同两类，同时建立了一个全面的评估框架，以衡量五个方面的人格偏见：冒犯性、有毒延续、关怀、刻板印象的认同以及

    Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
    
[^44]: DialCoT遇到了PPO：在较小的语言模型中分解和探索推理路径

    DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models. (arXiv:2310.05074v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05074](http://arxiv.org/abs/2310.05074)

    DialCoT是一种对话引导的链式思维方法，用于在较小的语言模型中分解和探索推理路径。通过将复杂问题分解为简单的子问题，它降低了任务难度，并使用PPO算法优化模型的推理路径选择。

    

    链式思维（CoT）提示已经被证明有助于增强至少具有1000亿参数的大型语言模型（LLMs）的推理能力。然而，当应用于具有不到100亿参数的较小语言模型（SLMs）的推理任务时，它是无效甚至有害的。为了解决这个限制，我们引入了对话引导的链式思维（DialCoT），它采用对话格式生成中间推理步骤，引导模型朝着最终答案前进。此外，我们使用PPO算法优化模型的推理路径选择，进一步增强其推理能力。我们的方法与以前的方法相比具有几个优点。首先，我们通过将解决复杂推理问题的过程分解成一系列更简单的子问题，显著降低了任务的难度，使其更适合于较小的语言模型。其次，我们优化了模型的推理路径选择，使其更准确和高效。

    Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective or even detrimental when applied to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. To address this limitation, we introduce Dialogue-guided Chain-of-Thought (DialCoT) which employs a dialogue format to generate intermediate reasoning steps, guiding the model toward the final answer. Additionally, we optimize the model's reasoning path selection using the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Our method offers several advantages compared to previous approaches. Firstly, we transform the process of solving complex reasoning questions by breaking them down into a series of simpler sub-questions, significantly reducing the task difficulty and making it more suitable for SLMs. Secondly, we optimize the m
    
[^45]: 指南学习用于上下文信息抽取

    Guideline Learning for In-context Information Extraction. (arXiv:2310.05066v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05066](http://arxiv.org/abs/2310.05066)

    本文提出了一种用于上下文信息抽取的指南学习框架，通过反思性地学习和遵循指南，以更好地理解任务并提高性能。

    

    大型语言模型(LLMs)可以通过仅依赖任务指令和少量输入输出示例来执行新任务，而无需优化任何参数。这被称为上下文学习(ICL)。最近，上下文信息抽取(IE)在研究界引起了关注。然而，上下文IE的性能通常落后于最先进的有监督专家模型。我们强调了一个主要原因: 任务描述不明确。有限长度的上下文难以充分表达复杂的IE任务指令和各种边界情况，导致任务理解与人类出现不匹配。本文提出了一种用于上下文IE的指南学习(GL)框架，其反思性地学习并遵循指南。在学习阶段，GL基于少量错误案例自动合成一组指南，在推理阶段，GL检索有用的指南以实现更好的ICL。此外，我们提出了一种s

    Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans. In this paper, we propose a Guideline Learning (GL) framework for In-context IE which reflectively learns and follows guidelines. During the learning phrase, GL automatically synthesizes a set of guidelines based on a few error cases, and during inference, GL retrieves helpful guidelines for better ICL. Moreover, we propose a s
    
[^46]: 仅通过印尼中小学考试：对印度尼西亚IndoMMLU的全面测试的大型语言模型 (arXiv:2310.04928v2 [cs.CL]已更新)

    Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU. (arXiv:2310.04928v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.04928](http://arxiv.org/abs/2310.04928)

    通过对印度尼西亚历史、文化和语言进行的测试，我们发现大型语言模型只能通过印尼的中小学水平，对印尼地方语言和文化的了解有限。

    

    虽然大型语言模型 (LLMs) 经常在大规模多语种文本上进行预训练，但它们的推理能力和现实世界知识主要是基于英文数据集进行评估的。评估LLM在英文以外的能力变得越来越重要，但由于缺乏合适的数据集而受阻。在这项工作中，我们介绍了IndoMMLU，这是首个用于印尼文化和语言的多任务语言理解基准测试，包括印度尼西亚中小学到高等教育入学考试的问题。通过聘请专业教师，我们获得了14,981个问题，涵盖了64个任务和教育水平，并有46%的问题专注于评估对印尼语的熟练程度以及对印度尼西亚九个地方语言和文化的了解。我们的实证评估表明，GPT-3.5仅能通过印尼的中小学水平，对印尼地方语言和文化的知识有限。其他较小的模型，如BLOOMZ和Falcon，也有类似的表现。

    Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon per
    
[^47]: LoFT: 用于改进对大型语言模型的对抗攻击可传递性的本地代理微调

    LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model. (arXiv:2310.04445v1 [cs.CL])

    [http://arxiv.org/abs/2310.04445](http://arxiv.org/abs/2310.04445)

    本文提出了一种名为LoFT的方法，通过在与有害查询处于词汇-语义邻域的相似查询上进行代理模型的微调，来改善对大型语言模型的对抗攻击的可传递性。

    

    已经发现，大型语言模型（LLM）的对齐可以通过附加特制的攻击后缀和有害查询来规避，以引发有害的响应。为了对未知特征的私有目标模型进行攻击，可以使用公共模型作为代理来构建攻击，并将成功的攻击从公共代理传递到私有目标模型。攻击的成功率取决于代理模型能够多大程度上逼近私有模型。我们假设，对于攻击可传递性来说，只要代理能够在有害查询的词汇-语义邻域内逼近目标模型即可。因此，在本文中，我们提出了“本地微调（LoFT）”，即在与有害查询处于词汇-语义邻域的相似查询上进行代理模型的微调，以减小代理和目标模型之间的差异。首先，我们演示了三种促使私有目标模型变得易受攻击的方法。

    It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \emph{Local Fine-Tuning (LoFT)}, \textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obt
    
[^48]: 评估中文大型语言模型中的幻觉

    Evaluating Hallucinations in Chinese Large Language Models. (arXiv:2310.03368v1 [cs.CL])

    [http://arxiv.org/abs/2310.03368](http://arxiv.org/abs/2310.03368)

    本研究评估了中文大型语言模型中的幻觉现象，通过建立HalluQA基准测试和使用GPT-4进行自动评估方法，发现18个模型的非幻觉率低于50%。研究分析了不同类型模型中的幻觉类型和原因。

    

    本文介绍了一项名为HalluQA（中文幻觉问答）的基准测试，用于衡量中文大型语言模型中的幻觉现象。HalluQA包含450个经过精心设计的对抗性问题，涵盖多个领域，并考虑了中国历史文化、风俗和社会现象。在构建HalluQA过程中，我们考虑了两种幻觉类型：模仿性虚假和事实错误，并基于GLM-130B和ChatGPT构建对抗样本。为了评估，我们设计了一种使用GPT-4的自动评估方法来判断模型输出是否是幻觉。我们对24个大型语言模型进行了广泛的实验，包括ERNIE-Bot、Baichuan2、ChatGLM、Qwen、SparkDesk等。在这24个模型中，有18个的非幻觉率低于50%。这表明HalluQA具有很高的挑战性。我们分析了不同类型模型中主要的幻觉类型及其原因。

    In this paper, we establish a benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure the hallucination phenomenon in Chinese large language models. HalluQA contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account Chinese historical culture, customs, and social phenomena. During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on GLM-130B and ChatGPT. For evaluation, we design an automated evaluation method using GPT-4 to judge whether a model output is hallucinated. We conduct extensive experiments on 24 large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than 50%. This indicates that HalluQA is highly challenging. We analyze the primary types of hallucinations in different types of models and their causes. Add
    
[^49]: 会话式健康代理：个性化的基于LLM的代理框架

    Conversational Health Agents: A Personalized LLM-Powered Agent Framework. (arXiv:2310.02374v1 [cs.CL])

    [http://arxiv.org/abs/2310.02374](http://arxiv.org/abs/2310.02374)

    该论文介绍了一个基于LLM的会话式健康代理框架，旨在为代理赋予批判性思维、知识获取和问题解决能力，实现个性化的健康护理服务。该框架能够无缝集成医疗工具，实现多语言和多模态对话，并与多种用户数据分析工具连接。

    

    会话式健康代理（CHAs）是一种互动系统，旨在通过进行共情对话和处理多模态数据来增强个人健康护理服务。当前的CHAs，特别是那些利用大型语言模型（LLMs）的系统，主要关注对话，但往往缺乏全面的代理能力。这包括从可穿戴设备、全天候数据收集源和电子健康记录中获取个人用户的健康数据的能力，以及整合最新发布的健康见解，并与已建立的多模态数据分析工具连接。我们正在开发一个框架，通过赋予CHAs批判性思维、知识获取和问题解决能力来增强它们的能力。我们的CHA平台由LLMs驱动，无缝集成了医疗工具，实现了多语言和多模态对话，并与各种用户数据分析工具进行接口。我们展示了其在处理复杂医疗任务方面的熟练性。

    Conversational Health Agents (CHAs) are interactive systems designed to enhance personal healthcare services by engaging in empathetic conversations and processing multimodal data. While current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation, they often lack comprehensive agent capabilities. This includes the ability to access personal user health data from wearables, 24/7 data collection sources, and electronic health records, as well as integrating the latest published health insights and connecting with established multimodal data analysis tools. We are developing a framework to empower CHAs by equipping them with critical thinking, knowledge acquisition, and problem-solving abilities. Our CHA platform, powered by LLMs, seamlessly integrates healthcare tools, enables multilingual and multimodal conversations, and interfaces with a variety of user data analysis tools. We illustrate its proficiency in handling complex healthcare tasks, s
    
[^50]: 改进对话管理：质量数据集 vs 模型

    Improving Dialogue Management: Quality Datasets vs Models. (arXiv:2310.01339v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.01339](http://arxiv.org/abs/2310.01339)

    这项工作认为，对话管理器在达到最佳性能方面的主要问题在于数据集的质量，而不是采用的模型。研究发现最广泛使用的数据集中存在的错误是导致对话管理失败的主要原因。

    

    面向任务的对话系统(TODS)已经成为用户使用自然语言与机器和计算机交互的关键。其中一个关键组件是对话管理器，通过提供最佳响应将对话引导到用户的目标。先前的研究提出了基于规则的系统(RBS)、强化学习(RL)和监督学习(SL)作为正确对话管理的解决方案；换句话说，根据用户的输入选择最佳响应。然而，本研究认为，对话管理器未能达到最佳性能的主要原因在于数据集的质量，而不是迄今为止采用的模型；这意味着数据集错误，如错误标记，导致对话管理的大部分失败。为了证明这一假设，我们研究了最广泛使用的数据集Multiwoz 2.1和SGD中的主要错误。为此，我们设计了一个合成对话生成器以完全控制对话的生成过程。

    Task-oriented dialogue systems (TODS) have become crucial for users to interact with machines and computers using natural language. One of its key components is the dialogue manager, which guides the conversation towards a good goal for the user by providing the best possible response. Previous works have proposed rule-based systems (RBS), reinforcement learning (RL), and supervised learning (SL) as solutions for the correct dialogue management; in other words, select the best response given input by the user. However, this work argues that the leading cause of DMs not achieving maximum performance resides in the quality of the datasets rather than the models employed thus far; this means that dataset errors, like mislabeling, originate a large percentage of failures in dialogue management. We studied the main errors in the most widely used datasets, Multiwoz 2.1 and SGD, to demonstrate this hypothesis. To do this, we have designed a synthetic dialogue generator to fully control the am
    
[^51]: GPT-Fathom：评估大型语言模型以解析GPT-4及其后续版本的演化路径的基准测试

    GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v1 [cs.CL])

    [http://arxiv.org/abs/2309.16583](http://arxiv.org/abs/2309.16583)

    GPT-Fathom是一个用于评估大型语言模型的开源套件，它系统评估了10多个主要的语言模型，并提供了从GPT-3到GPT-4演化路径的宝贵见解。

    

    随着大型语言模型（LLMs）的快速进展，人们迫切需要一个全面的评估套件来评估它们的能力和局限性。现有的LLM排行榜通常参考其他论文中报告的得分，设置和提示不一致，这可能无意间鼓励选择有利的设置和提示以获得更好的结果。在这项工作中，我们引入了GPT-Fathom，这是一个基于OpenAI Evals构建的开源和可重复的LLM评估套件。我们在对齐的环境设置下系统评估了10多个主要的LLMs以及OpenAI的传统模型在20多个精选基准测试中的表现，涵盖了7个能力类别。我们对OpenAI早期模型的回顾性研究为我们揭示了从GPT-3到GPT-4的演化路径提供了宝贵的见解。目前，社区渴望了解GPT-3如何逐步改进到GPT-4，包括像添加代码数据是否提高了LLM的推理能力以及LLM能力的哪些方面等技术细节。

    With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capabili
    
[^52]: 使用Prompt调优的预训练语言模型加速主题投资

    Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models. (arXiv:2309.12075v1 [cs.CL])

    [http://arxiv.org/abs/2309.12075](http://arxiv.org/abs/2309.12075)

    本研究通过Benchmark测试，发现使用Prompt Tuning的预训练语言模型在多标签文本分类任务中具有较好的性能和计算效率。同时，提出了使用Trie搜索来解决生成标签匹配问题的限制。

    

    Prompt Tuning作为一种可扩展且成本效益高的方法，正在成为细调预训练语言模型（PLMs）的一种流行方法。本研究基于多标签文本分类任务对Prompt Tuning和基准方法的性能和计算效率进行了基准测试。将其应用于将公司分类为投资公司专有的行业分类法，以支持其主题投资策略。在多标签分类问题中，使用PLMs进行文本到文本分类经常被报告为优于使用分类头进行分类，但在每个标签由多个令牌组成的多标签分类问题中，存在一些限制：（a）生成的标签可能不匹配行业分类法中的任何标签；（b）在细调阶段，必须以任意顺序提供多个标签；（c）模型为每个标签提供二进制决策，而不是适当的置信度分数。通过应用Trie搜索来解决限制（a）。

    Prompt Tuning is emerging as a scalable and cost-effective method to fine-tune Pretrained Language Models (PLMs). This study benchmarks the performance and computational efficiency of Prompt Tuning and baseline methods on a multi-label text classification task. This is applied to the use case of classifying companies into an investment firm's proprietary industry taxonomy, supporting their thematic investment strategy. Text-to-text classification with PLMs is frequently reported to outperform classification with a classification head, but has several limitations when applied to a multi-label classification problem where each label consists of multiple tokens: (a) Generated labels may not match any label in the industry taxonomy; (b) During fine-tuning, multiple labels must be provided in an arbitrary order; (c) The model provides a binary decision for each label, rather than an appropriate confidence score. Limitation (a) is addressed by applying constrained decoding using Trie Search,
    
[^53]: 面向大型语言模型的机器翻译消歧效果研究

    Towards Effective Disambiguation for Machine Translation with Large Language Models. (arXiv:2309.11668v1 [cs.CL])

    [http://arxiv.org/abs/2309.11668](http://arxiv.org/abs/2309.11668)

    本文研究了大型语言模型(LLMs)在翻译歧义句子方面的能力，并通过上下文学习和歧义数据集微调提出了改进处理歧义的方法。实验证明，这些方法在多个语言方向上有着与最先进系统相当甚至超越的表现。这些研究为机器翻译的有效消歧提供了宝贵的见解。

    

    在机器翻译领域，解决语义歧义一直被认为是一个核心挑战。最近在歧义句子的翻译性能基准测试中，传统神经机器翻译系统的局限性暴露出来，无法捕捉到其中许多情况。大型语言模型(LLMs)已经成为一个有希望的替代方案，表现出与传统NMT模型相当的性能，并引入了控制目标输出的新范式。本文研究了LLMs在翻译包含多义词和稀有词义的歧义句子方面的能力，并提出了两种通过上下文学习和精心策划的歧义数据集微调来改进处理此类歧义的方法。实验证明，我们的方法在五个语言方向中有四个方向能够与DeepL和NLLB等最先进系统匹敌甚至超越。我们的研究为 effective disambiguation for machine translation 提供了有价值的见解。

    Resolving semantic ambiguity has long been recognised as a central challenge in the field of machine translation. Recent work on benchmarking translation performance on ambiguous sentences has exposed the limitations of conventional Neural Machine Translation (NMT) systems, which fail to capture many of these cases. Large language models (LLMs) have emerged as a promising alternative, demonstrating comparable performance to traditional NMT models while introducing new paradigms for controlling the target outputs. In this paper, we study the capabilities of LLMs to translate ambiguous sentences containing polysemous words and rare word senses. We also propose two ways to improve the handling of such ambiguity through in-context learning and fine-tuning on carefully curated ambiguous datasets. Experiments show that our methods can match or outperform state-of-the-art systems such as DeepL and NLLB in four out of five language directions. Our research provides valuable insights into effec
    
[^54]: 临床文本摘要: 大型语言模型的应用优于人类专家

    Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v1 [cs.CL])

    [http://arxiv.org/abs/2309.07430](http://arxiv.org/abs/2309.07430)

    本研究通过对八个大型语言模型在临床摘要任务上的领域适应方法实验进行了全面的定量评估，发现最佳适应的模型的摘要在完整性和正确性方面优于人类摘要。

    

    在临床工作中，浏览大量的文本数据并总结关键信息对临床医生的时间分配造成了很大的负担。尽管大型语言模型（LLMs）在自然语言处理（NLP）任务中展现了巨大的潜力，但它们在各种临床摘要任务中的效果尚未得到严格的检验。在本研究中，我们对八个LLMs进行了领域适应方法的实验，涵盖了六个数据集和四个不同的摘要任务：放射学报告、患者问题、病历记录和医患对话。我们进行了全面的定量评估，发现模型和适应方法之间存在权衡，并且在某些情况下，LLMs的最新进展可能不会带来改进的结果。此外，通过与六名医生进行的临床阅读者研究，我们发现最佳适应的LLM的摘要在完整性和正确性方面优于人类摘要。我们的进一步定性分析揭示了LLMs和人类在面对的共同挑战。

    Sifting through vast textual data and summarizing key information imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy across diverse clinical summarization tasks has not yet been rigorously examined. In this work, we employ domain adaptation methods on eight LLMs, spanning six datasets and four distinct summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not lead to improved results. Further, in a clinical reader study with six physicians, we depict that summaries from the best adapted LLM are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis delineates mutual challenges faced by both LLMs and
    
[^55]: 离线逆向强化学习下的提示评估与优化

    Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])

    [http://arxiv.org/abs/2309.06553](http://arxiv.org/abs/2309.06553)

    这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。

    

    最近，像ChatGPT这样的大型语言模型（LLM）的发展取得了显著的性能，通过利用人类专业知识。然而，充分揭示LLMs在复杂任务中的潜力需要在自然语言提示的广阔搜索空间中进行导航。虽然提示工程显示出潜力，但试错尝试中所需的人工设计提示和相关成本带来了重大挑战。关键是，提示优化的效率取决于昂贵的提示评估过程。本工作介绍了Prompt-OIRL，这是一种基于离线逆向强化学习的方法，旨在弥合有效提示评估和可负担性之间的差距。我们的方法利用专家评估的离线数据集，运用逆向强化学习获得一个针对离线、查询依赖型提示评估的奖励模型。Prompt-OIRL的优点是多方面的：它预测提示的性能，成本高效，生成易读的结果。

    The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
    
[^56]: 只需要一个宽度前馈神经网络

    One Wide Feedforward is All You Need. (arXiv:2309.01826v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.01826](http://arxiv.org/abs/2309.01826)

    本文探讨了Transformer架构中前馈神经网络（FFN）的作用，发现尽管它占据了模型很大一部分的参数，但它是冗余的。通过移除解码器层的FFN并在编码器上共享单个FFN，我们能够显著减少参数数量并实现准确性和延迟上的显著提升。

    

    Transformer架构有两个主要的非嵌入组件：注意力和前馈神经网络（FFN）。注意力捕捉到不考虑位置的单词之间的相互依赖关系，而FFN独立地对每个输入标记进行非线性转换。在这项工作中，我们探讨了FFN的作用，并发现尽管它占据了模型参数的相当大比例，但它是高度冗余的。具体来说，通过在解码器层移除FFN并在编码器上共享一个单一的FFN，我们能够显著减少参数的数量，只有轻微的准确性下降。最后，通过增加共享FFN的隐藏维度，我们将此架构缩小回原始大小，实现了准确性和延迟方面与原始Transformer Big相比的显著增益。

    The Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN non-linearly transforms each input token independently. In this work we explore the role of the FFN, and find that despite taking up a significant fraction of the model's parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a modest drop in accuracy by removing the FFN on the decoder layers and sharing a single FFN across the encoder. Finally we scale this architecture back to its original size by increasing the hidden dimension of the shared FFN, achieving substantial gains in both accuracy and latency with respect to the original Transformer Big.
    
[^57]: OmniQuant：用于大型语言模型的全向校准量化

    OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])

    [http://arxiv.org/abs/2308.13137](http://arxiv.org/abs/2308.13137)

    OmniQuant是一种用于大型语言模型的全向校准量化技术，通过优化各种量化参数实现了良好的性能，并保持了计算效率。

    

    大型语言模型（LLM）已经在自然语言处理任务中带来了革命性的变化。然而，它们的实际部署受到了其庞大的内存和计算需求的限制。虽然最近的后训练量化（PTQ）方法在减少内存占用和提高LLM的计算效率方面非常有效，但它们手工制定量化参数，导致性能较低并且不能处理极低位量化。为了解决这个问题，我们介绍了一种全向校准量化（OmniQuant）技术，用于LLMs，它在多种量化设置下实现了良好的性能，并通过高效优化各种量化参数来保持PTQ的计算效率。OmniQuant包含两个创新组件，包括可学习的权重剪裁（LWC）和可学习的等效变换（LET）。LWC通过优化剪裁阈值来调节权重的极值。与此同时，LET处理激活函数。

    Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activa
    
[^58]: AgentVerse: 促进多智能体协作和探索 emergent behaviors。

    AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. (arXiv:2308.10848v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10848](http://arxiv.org/abs/2308.10848)

    AgentVerse提出了一个多智能体框架，可以通过协同和动态调整合作团队的组成，实现超越单个智能体的性能。在合作任务中，该框架能够引发出群体内个体智能体之间的社会行为，从而提高多智能体团队的协作潜力。

    

    由大型语言模型（LLMs）赋能的自主智能体已经取得了显著进展，使它们能够在广泛的任务范围内进行泛化。然而，在现实世界的情境中，个体之间的合作经常需要以增强任务完成的效率和效果。因此，受到人类群体动力学的启发，我们提出了一个多智能体框架，可以作为一个大于其各个部分之和的系统进行协同和动态调整。我们的实验表明，这个框架能够有效地部署超过单个智能体的多智能体组，并取得更好的表现。此外，我们深入研究了在合作任务完成过程中，群体内个体智能体之间出现的社会行为的涌现。基于这些行为，我们讨论了一些可能的策略，以利用积极的行为，并减轻消极的行为，以提高多智能体组的协作潜力。我们的 \framework 框架的代码将很快公布。

    Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \framework will soon be
    
[^59]: 低延迟同时语音翻译的端到端评估

    End-to-End Evaluation for Low-Latency Simultaneous Speech Translation. (arXiv:2308.03415v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03415](http://arxiv.org/abs/2308.03415)

    本文提出了一个端到端的评估框架，用于评估低延迟语音翻译的各个方面。通过该框架，我们比较了不同方法的性能，并进行了全面的评估。

    

    近年来，低延迟语音翻译的挑战引起了研究界的广泛关注，许多出版物和共享任务也证明了这一点。因此，在实际场景中评估这些不同的方法非常重要。然而，目前只有系统的特定方面被评估，并且往往无法比较不同的方法。在这项工作中，我们提出了第一个在实际条件下执行和评估低延迟语音翻译各个方面的框架。评估是以端到端的方式进行的，包括音频的分段以及不同组成部分的运行时间。其次，我们使用该框架比较了不同的低延迟语音翻译方法。我们评估了具有修订输出选项的模型以及具有固定输出方法。此外，我们直接比较了最先进的级联系统和端到端系统。最后，该框架基于一个统一的度量来评估低延迟语音翻译性能，并提供了一个全面的评估结果。

    The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches.  In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components.  Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework all
    
[^60]: Baby's CoThought: 利用大型语言模型提高紧凑模型的推理能力

    Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models. (arXiv:2308.01684v1 [cs.CL])

    [http://arxiv.org/abs/2308.01684](http://arxiv.org/abs/2308.01684)

    "Baby's CoThought" 提出了一种利用大型语言模型重组数据训练紧凑语言模型的方法，经过评估发现，在10个语言学、NLU和问答任务中，BabyLM的表现超过RoBERTa-base超过3个点，展现出更好的上下文信息提取能力。

    

    大型语言模型(LLMs)在各种自然语言理解(NLU)任务中展示出出色的性能，主要是由于它们的上下文学习能力。我们提出的"CoThought"流水线利用LLMs的CoT提示，高效地训练较小的"baby"语言模型(BabyLMs)。我们使用GPT-3.5-turbo对少于100M大小的数据集进行重组，将其转化为面向任务的、可读性强的文本，类似于语言学习者的学校教材。然后，在这个重组后的数据集上，以RoBERTa(Liu等人，2019)的方式对BabyLM进行预训练。在4个基准测试中，我们的BabyLM在10个语言学、NLU和问答任务中的表现优于RoBERTa-base超过3个点，展现出更好的提取上下文信息的能力。这些结果表明，在小型的、由LLM重组的数据上预训练的紧凑语言模型能够更好地理解任务并实现

    Large Language Models (LLMs) demonstrate remarkable performance on a variety of Natural Language Understanding (NLU) tasks, primarily due to their in-context learning ability. This ability is utilized in our proposed "CoThought" pipeline, which efficiently trains smaller "baby" language models (BabyLMs) by leveraging the Chain of Thought (CoT) prompting of LLMs. Our pipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo, transforming it into task-oriented, human-readable texts that are comparable to the school texts for language learners. The BabyLM is then pretrained on this restructured dataset in a RoBERTa (Liu et al., 2019) fashion. In evaluations across 4 benchmarks, our BabyLM outperforms the RoBERTa-base in 10 linguistic, NLU, and question answering tasks by more than 3 points, showing superior ability to extract contextual information. These results suggest that compact LMs pretrained on small, LLM-restructured data can better understand tasks and achieve
    
[^61]: 基于Trie上下文增强的个性化查询自动补全算法，以提升对短前缀和未见前缀的支持

    Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes. (arXiv:2307.15455v1 [cs.CL])

    [http://arxiv.org/abs/2307.15455](http://arxiv.org/abs/2307.15455)

    提出了一种基于Trie上下文增强的个性化查询自动补全算法，可以解决短前缀和未见前缀的问题，并有效利用历史查询的流行度信息。

    

    查询自动补全(QAC)旨在为给定的查询前缀提供合理的补全建议。传统的QAC系统利用历史查询日志中的Trie数据结构来提供最受欢迎的补全建议。然而，对于任何QAC系统来说，有两种特定的场景很难处理：短前缀(本质上存在歧义)和未见前缀。最近，提出了个性化自然语言生成(NLG)模型，利用前一个会话的查询作为上下文来解决这两个挑战。然而，这样的NLG模型存在两个缺点：(1)前述会话查询可能与当前前缀的用户意图无关且包含噪声；(2)NLG模型无法直接融合历史查询的流行度。因此，我们提出了一种新颖的QAC算法Trie-NLG，该算法同时利用Trie中的流行度信息和前一个会话查询中的个性化信息。

    Query auto-completion (QAC) aims at suggesting plausible completions for a given query prefix. Traditionally, QAC systems have leveraged tries curated from historical query logs to suggest most popular completions. In this context, there are two specific scenarios that are difficult to handle for any QAC system: short prefixes (which are inherently ambiguous) and unseen prefixes. Recently, personalized Natural Language Generation (NLG) models have been proposed to leverage previous session queries as context for addressing these two challenges. However, such NLG models suffer from two drawbacks: (1) some of the previous session queries could be noisy and irrelevant to the user intent for the current prefix, and (2) NLG models cannot directly incorporate historical query popularity. This motivates us to propose a novel NLG model for QAC, Trie-NLG, which jointly leverages popularity signals from trie and personalization signals from previous session queries. We train the Trie-NLG model b
    
[^62]: 大型语言模型塑造并受到社会的影响：arXiv出版模式调查

    Large language models shape and are shaped by society: A survey of arXiv publication patterns. (arXiv:2307.10700v1 [cs.DL])

    [http://arxiv.org/abs/2307.10700](http://arxiv.org/abs/2307.10700)

    大型语言模型的论文数量急剧增加，研究重点逐渐转向社会影响。与LLM相关的论文呈现持续增长的趋势，新发表关于LLM的作者更注重应用和社会影响。

    

    大型语言模型的论文数量近年来呈急剧增加，这种变化对科学领域产生了戏剧性的影响，但目前还没有进行详细的文献计量分析。本文分析了CS和Stat arXiv上发布的388K篇论文，并重点关注2023年与2018-2022年之间发表模式的变化。我们分析了LLM论文的比例增加情况，得到了最多关注的与LLM相关的主题，撰写LLM论文的作者，作者的研究主题与背景的相关性，区分高被引用LLM论文的因素，以及国际合作的模式。我们展示了LLM研究越来越关注社会影响：在计算机与社会子arXiv上，与LLM相关的论文比例增加了18倍，新发表关于LLM的作者更倾向于关注应用和社会影响。LLM研究也受到社会动态的影响。

    There has been a steep recent increase in the number of large language model (LLM) papers, producing a dramatic shift in the scientific landscape which remains largely undocumented through bibliometric analysis. Here, we analyze 388K papers posted on the CS and Stat arXivs, focusing on changes in publication patterns in 2023 vs. 2018-2022. We analyze how the proportion of LLM papers is increasing; the LLM-related topics receiving the most attention; the authors writing LLM papers; how authors' research topics correlate with their backgrounds; the factors distinguishing highly cited LLM papers; and the patterns of international collaboration. We show that LLM research increasingly focuses on societal impacts: there has been an 18x increase in the proportion of LLM-related papers on the Computers and Society sub-arXiv, and authors newly publishing on LLMs are more likely to focus on applications and societal impacts than more experienced authors. LLM research is also shaped by social dyn
    
[^63]: 零样本对话搜索中的查询重构

    Zero-shot Query Reformulation for Conversational Search. (arXiv:2307.09384v1 [cs.IR])

    [http://arxiv.org/abs/2307.09384](http://arxiv.org/abs/2307.09384)

    提出了一种零样本查询重构（ZeQR）框架，通过利用机器阅读理解任务的语言模型来解决对话搜索中的数据稀疏性、解释性不足和歧义的问题。

    

    随着语音助手的普及，对话搜索在信息检索领域引起了更多的关注。然而，对话搜索中的数据稀疏性问题严重阻碍了监督式对话搜索方法的进展。因此，研究人员更加关注零样本对话搜索方法。然而，现有的零样本方法存在三个主要限制：它们不适用于所有的检索器，它们的有效性缺乏足够的解释性，并且他们无法解决因省略而导致的常见对话歧义。为了解决这些限制，我们引入了一种新颖的零样本查询重构（ZeQR）框架，该框架根据先前的对话上下文重构查询，而无需对话搜索数据的监督。具体来说，我们的框架利用了设计用于机器阅读理解任务的语言模型来明确解决两个常见的歧义：协调和省略。

    As the popularity of voice assistants continues to surge, conversational search has gained increased attention in Information Retrieval. However, data sparsity issues in conversational search significantly hinder the progress of supervised conversational search methods. Consequently, researchers are focusing more on zero-shot conversational search approaches. Nevertheless, existing zero-shot methods face three primary limitations: they are not universally applicable to all retrievers, their effectiveness lacks sufficient explainability, and they struggle to resolve common conversational ambiguities caused by omission. To address these limitations, we introduce a novel Zero-shot Query Reformulation (ZeQR) framework that reformulates queries based on previous dialogue contexts without requiring supervision from conversational search data. Specifically, our framework utilizes language models designed for machine reading comprehension tasks to explicitly resolve two common ambiguities: cor
    
[^64]: 使用社交媒体上的判决文件对犯罪类型进行分类

    Classifying Crime Types using Judgment Documents from Social Media. (arXiv:2306.17020v1 [cs.CL])

    [http://arxiv.org/abs/2306.17020](http://arxiv.org/abs/2306.17020)

    本文提出了一种通过NLP处理方法的新的训练模型，通过生成新样本来平衡不均匀的数据集分布的缺陷，并使用预训练和微调来赋予模型对小数据集的良好泛化能力。

    

    基于犯罪行为事实来确定犯罪类型的任务在社会科学中变得非常重要和有意义。但该领域面临的问题是，由于犯罪本身的性质，数据样本本身分布不均匀。同时，司法领域的数据集少有公开可用，无法产生用于直接训练的大型数据集。本文提出了一种通过NLP处理方法解决该问题的新的训练模型。我们首先提出了一个犯罪事实数据预处理模块(CFDPM)，通过生成新样本来平衡不均匀的数据集分布的缺陷。然后，我们使用一个大型开源数据集(CAIL-big)作为我们的预训练数据集，使用我们自己收集的一个小数据集进行微调，赋予模型对不熟悉的小数据集具有良好的泛化能力。同时，我们使用改进的Bert模型和动态遮蔽来改进模型。实验证明

    The task of determining crime types based on criminal behavior facts has become a very important and meaningful task in social science. But the problem facing the field now is that the data samples themselves are unevenly distributed, due to the nature of the crime itself. At the same time, data sets in the judicial field are less publicly available, and it is not practical to produce large data sets for direct training. This article proposes a new training model to solve this problem through NLP processing methods. We first propose a Crime Fact Data Preprocessing Module (CFDPM), which can balance the defects of uneven data set distribution by generating new samples. Then we use a large open source dataset (CAIL-big) as our pretraining dataset and a small dataset collected by ourselves for Fine-tuning, giving it good generalization ability to unfamiliar small datasets. At the same time, we use the improved Bert model with dynamic masking to improve the model. Experiments show that the 
    
[^65]: 全文科技论文的弱监督多标签分类

    Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers. (arXiv:2306.14003v1 [cs.CL])

    [http://arxiv.org/abs/2306.14003](http://arxiv.org/abs/2306.14003)

    本文提出了一个弱监督的科技论文多标签分类框架FUTEX，该框架利用跨论文网络结构和各投稿内部分章节的层次结构，解决了在细粒度标签空间中将论文分类为研究主题和主题，可能有多个；应利用全文来补充论文标题和摘要以进行分类等挑战。

    

    弱监督的科技论文分类依赖于分类描述而非人工标注样本建立分类器。已有的弱监督分类研究较少考虑到两个挑战：(1)在细粒度标签空间中将论文分类为研究主题和主题，可能有多个; (2)应利用全文来补充论文标题和摘要以进行分类。此外，应利用跨论文网络结构和各论文内部分章节的层次结构等结构信息。为解决这些挑战，本研究提出了FUTEX，该框架使用跨论文网络结构和各投稿内部分章节的层次结构。

    Instead of relying on human-annotated training samples to build a classifier, weakly supervised scientific paper classification aims to classify papers only using category descriptions (e.g., category names, category-indicative keywords). Existing studies on weakly supervised paper classification are less concerned with two challenges: (1) Papers should be classified into not only coarse-grained research topics but also fine-grained themes, and potentially into multiple themes, given a large and fine-grained label space; and (2) full text should be utilized to complement the paper title and abstract for classification. Moreover, instead of viewing the entire paper as a long linear sequence, one should exploit the structural information such as citation links across papers and the hierarchy of sections and paragraphs in each paper. To tackle these challenges, in this study, we propose FUTEX, a framework that uses the cross-paper network structure and the in-paper hierarchy structure to 
    
[^66]: GIMLET：一种用于基于指令分子零样本学习的统一图文模型

    GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])

    [http://arxiv.org/abs/2306.13089](http://arxiv.org/abs/2306.13089)

    本研究提出了一种名为GIMLET的统一图文模型，用于在零样本设置下使用自然语言指令完成分子相关任务。我们解决了现有模型的指令处理不足和图形容量有限的问题，并证明了使用GIMLET能够增强图形特征的泛化能力。

    

    分子属性预测近年来受到了广泛关注，但由于昂贵的实验造成的标签不足问题将是其主要瓶颈。为了缓解这个问题并更好地利用文本知识进行任务，本研究探讨了在零样本设置下使用自然语言指令完成分子相关任务的可行性。我们发现现有的分子-文本模型在这种情况下表现不佳，原因是处理指令不足以及图形容量有限。为了克服这些问题，我们提出了GIMLET，它统一了图形和文本数据的语言模型。通过采用广义位置嵌入，我们的模型被扩展以编码图形结构和指令文本，而无需额外的图形编码模块。GIMLET还在注意机制中解耦了图形的编码和任务指令，增强了跨新任务的图形特征的泛化能力。我们构建了一个数据集...

    Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset 
    
[^67]: TrojPrompt：基于黑盒方式的预训练语言模型木马攻击

    TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)

    [http://arxiv.org/abs/2306.06815](http://arxiv.org/abs/2306.06815)

    本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。

    

    Prompt学习被证明在提高预训练语言模型（PLM）适应性方面非常有效，超越了传统的微调范式，并在专为少样本学习场景量身定制的应用程序和API中展现了杰出的前景。但是，尽管prompt学习的API越来越受欢迎，但它们的安全问题仍未得到充分探索。本文在prompt学习的PLM API的特洛伊易感性方面进行了开创性研究。我们发现，离散提示，少样本和黑盒设置是几个关键挑战，限制了现有后门攻击的适用性。为了解决这些挑战，我们提出了TrojPrompt，这是一种自动的黑盒框架，可有效生成通用的和隐秘的触发器，并将特洛伊木马插入硬提示。具体而言，我们提出了一种API驱动的通用触发器发现算法，通过查询受害者PLM API，为各种输入生成通用触发器。

    Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
    
[^68]: 基于 CMExam 的大型语言模型评测——一份综合的中国医学考试数据集

    Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset. (arXiv:2306.03030v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.03030](http://arxiv.org/abs/2306.03030)

    该研究介绍了 CMExam 数据集，这是一个综合的、来自于中国国家医疗执业考试的数据集，为评估大型语言模型提供了一个标准化且客观的方法。在 CMExam 上，GPT-4 表现最好，这表明大型语言模型在医学领域有巨大潜力。

    

    最近大型语言模型 (LLM) 的进步已经改变了问答领域，然而，由于缺乏标准化和全面性的数据集，在医学领域对 LLM 进行评估具有挑战性。为了弥补这一空白，我们介绍了 CMExam，它来自中国国家医疗执业考试，由60,000多个选择题和模型推理解释的答案解析构成，可进行标准化和客观化的评估。为了深入分析 LLM，我们邀请医学专业人士对五个额外的问题逐个进行标注，包括疾病组、临床科室、医学学科、能力领域和难度级别。除了数据集外，我们还在 CMExam 上对代表性的 LLM 和 QA 算法进行了彻底的实验。结果表明，GPT-4 的准确度最高，为61.6％，加权 F1 分数为0.617。这些结果突显了大型语言模型在医学领域的巨大潜力以及标准化数据集对评估其性能的重要性。

    Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce CMExam, sourced from the Chinese National Medical Licensing Examination. CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great 
    
[^69]: LLM可以理解加密提示：面向隐私计算友好的Transformers

    LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v1 [cs.LG])

    [http://arxiv.org/abs/2305.18396](http://arxiv.org/abs/2305.18396)

    本文中，研究人员通过使用隐私计算友好的近似方法替换transformer架构中计算和通信密集的运算符，实现了大幅降低私有推断成本的效果，并在保持准确性的前提下实现了计算加速和通信开销降低。

    

    先前的研究尝试在服务器客户端环境中为基于transformer的大型语言模型 (LLMs) 构建私有推断框架，其中服务器持有模型参数，客户端输入私有数据进行推断。然而，当私有输入通过原始LLMs进行前向传播时，这些框架会产生显着的开销。在本文中，我们展示了通过用隐私计算友好的近似替换transformer架构中计算和通信密集的运算符可以大大降低私有推断成本，对模型性能的影响微乎其微。与最新的Iron（NeurIPS 2022）相比，我们的隐私计算友好的模型推断管道在计算上实现了$5 \times$的加速，在通信开销上实现了80\%的降低，同时几乎保持了相同的准确性。

    Prior works have attempted to build private inference frameworks for transformer-based large language models (LLMs) in a server-client setting, where the server holds the model parameters and the client inputs the private data for inference. However, these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs. In this paper, we show that substituting the computation- and communication-heavy operators in the transformer architecture with privacy-computing friendly approximations can greatly reduce the private inference costs with minor impact on model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our privacy-computing friendly model inference pipeline achieves a $5\times$ acceleration in computation and an 80\% reduction in communication overhead, while retaining nearly identical accuracy.
    
[^70]: LaFTer: 使用语言和无标签图像集进行无标签调整的零样本分类器

    LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections. (arXiv:2305.18287v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.18287](http://arxiv.org/abs/2305.18287)

    本文首次展示了如何通过使用无标签图像集和自动生成的大量语言模型文本来缩小零样本分类器与有标签分类器之间的性能差距，从而在无需任何标签和配对的情况下提高了零样本分类器的性能。

    

    最近，大规模预训练的视觉和语言（VL）模型在零样本视觉分类中取得了新的最先进（SOTA）的成果，实现了使用简单语言提示定义的潜在无限类别的开放词汇识别。然而，尽管取得了这些巨大的进展，这些零样本分类器的性能仍然不及经过监督微调训练的专用（封闭类别集）分类器的结果。在本文中，我们首次展示了如何在没有任何标签和任何配对的VL数据的情况下缩小这个差距，使用一个无标签图像集和使用大型语言模型（LLM）生成的一组描述感兴趣类别的文本，有效地替代那些类别的标记视觉实例。使用我们的无标签方法，我们能够在各种不同的情况下显著提高基本VL模型和其他当代方法和基准的零样本性能。

    Recently, large-scale pre-trained Vision and Language (VL) models have set a new state-of-the-art (SOTA) in zero-shot visual classification enabling open-vocabulary recognition of potentially unlimited set of categories defined as simple language prompts. However, despite these great advances, the performance of these zeroshot classifiers still falls short of the results of dedicated (closed category set) classifiers trained with supervised fine tuning. In this paper we show, for the first time, how to reduce this gap without any labels and without any paired VL data, using an unlabeled image collection and a set of texts auto-generated using a Large Language Model (LLM) describing the categories of interest and effectively substituting labeled visual instances of those categories. Using our label-free approach, we are able to attain significant performance improvements over the zero-shot performance of the base VL model and other contemporary methods and baselines on a wide variety of
    
[^71]: 预训练语言模型中的“绝对”与“相对”副词研究

    Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models. (arXiv:2305.16426v1 [cs.CL])

    [http://arxiv.org/abs/2305.16426](http://arxiv.org/abs/2305.16426)

    本文通过研究标量副词，探究了预训练语言模型中“绝对”与“相对”词的表现，在涉及逻辑推理的NLP应用中面临挑战。

    

    词义向量空间模型假设出现在相似语境中的词具有相似的含义。这些模型中，类似于主题关联但在逻辑力度上不同的词往往被视为语义上相似，这对涉及逻辑推理的NLP应用造成了普遍挑战。关于现代预训练语言模型（如BERT、RoBERTa和GPT-3）在逻辑任务上表现优异的报告存在混杂的情况。本文通过系统研究标量副词，这是一类具有强烈逻辑力度的词汇，推进了这一讨论。通过使用三项不同的任务（包括自然的社交媒体数据和构造的示例），我们调查了BERT、RoBERTa、GPT-2和GPT-3在这些常见词语方面是否展现出一般的、类人的知识。我们问：1）这些模型能否区分这三个语义类型中的差异？

    Vector space models of word meaning all share the assumption that words occurring in similar contexts have similar meanings. In such models, words that are similar in their topical associations but differ in their logical force tend to emerge as semantically close, creating well-known challenges for NLP applications that involve logical reasoning. Modern pretrained language models, such as BERT, RoBERTa and GPT-3 hold the promise of performing better on logical tasks than classic static word embeddings. However, reports are mixed about their success. In the current paper, we advance this discussion through a systematic study of scalar adverbs, an under-explored class of words with strong logical force. Using three different tasks, involving both naturalistic social media data and constructed examples, we investigate the extent to which BERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these common words. We ask: 1) Do the models distinguish amongst the three sema
    
[^72]: 分段循环Transformer:一种高效的序列到序列模型

    Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])

    [http://arxiv.org/abs/2305.16340](http://arxiv.org/abs/2305.16340)

    本文提出了一种分段循环Transformer（SRformer）来减少计算/内存成本，并使用RAF层处理跨段的信息，从而提高序列处理能力。

    

    Transformer在许多领域中表现出卓越的性能，包括语言和视觉。然而，随着序列长度的增加，它们的计算成本呈二次增长，使得它们在资源受限的应用中使用成为不可能。为了解决这个问题，我们的方法是将整个序列划分成若干段。然后使用具有循环结构的神经元来聚合跨段的信息，从而实现具有较低计算/内存成本的序列处理能力模型。为了验证这个想法，我们首先研究了使用局部Attention机制对单个段的影响。然后我们提出了一种分段循环Transformer（SRformer），它将分段Attention和循环Attention相结合。它使用循环accumulate and fire（RAF）层在相邻段之间处理信息。通过更新key的产品来补偿减少Attention窗口长度产生的误差。

    Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
    
[^73]: 用迭代检索-生成协同增强检索增强的大型语言模型

    Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. (arXiv:2305.15294v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15294](http://arxiv.org/abs/2305.15294)

    本文提出了一种被称为迭代检索-生成的方法，通过迭代地协同检索和生成，改善了检索增强的大型语言模型的性能。

    

    大型语言模型是强大的文本处理器和推理器，但仍然受到诸如过时知识和幻觉等限制，这需要将它们与现实世界连接起来。检索增强的大型语言模型已经引起了广泛的关注，以在外部知识上打下模型生成的基础。然而，检索器往往难以捕捉到复杂信息需求的相关性。最近的工作提出通过使大型语言模型积极参与检索来改善相关性建模，即通过生成来改善检索。在本文中，我们展示了一种我们称之为迭代检索-生成（Iter-RetGen）的方法，通过迭代方式协同检索和生成可以实现强大的性能。模型输出展示了完成任务所需的内容，因此为检索更相关的知识提供了信息上下文，进而有助于在下一次迭代中生成更好的输出。

    Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which 
    
[^74]: Meta-Learning Online Adaptation of Language Models. （arXiv:2305.15076v2 [cs.CL] 更新）

    Meta-Learning Online Adaptation of Language Models. (arXiv:2305.15076v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15076](http://arxiv.org/abs/2305.15076)

    提出了一种元学习方法，通过学习如何分配语言模型损失的权重，实现在在线微调过程中延长语言模型的有效时间，并提升问题回答能力。

    

    大型语言模型在其参数中编码了令人印象深刻的广泛世界知识。然而，静态语言模型中的知识很快过时，限制了模型的有效 "货架寿命"。虽然在线微调可以减轻这种退化，但我们发现简单地在一系列文档上进行微调会导致信息吸收水平较低。我们假设在线微调没有充分关注重要信息。也就是说，用于表示事实信息的重要标记的梯度信号被从本质上嘈杂的标记的梯度淹没，这表明动态的、上下文感知的学习率可能是有益的。因此，我们提出了学习如何增加权重的标记。我们元训练一个小的自回归模型，以在在线微调过程中重新调整每个标记的语言模型损失，其目标是最大化过时的基础问答模型在单个加权梯度之后对文档的问题回答能力。

    Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model's effective "shelf life." While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gr
    
[^75]: LLM们进步到了什么程度？一个挑战性的问题解决基准对大型语言模型

    Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. (arXiv:2305.15074v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15074](http://arxiv.org/abs/2305.15074)

    这项研究提出了JEEBench，一个更具挑战性的基准数据集，用于评估大型语言模型的问题解决能力。通过评估各种模型，结果显示目前最好的模型在解决问题时存在代数操作错误、抽象概念转化不准确和难以检索相关概念等问题。

    

    在过去的几年里，大型语言模型（LLMs）在现有的推理基准上的性能显著提高。为此，我们提出了JEEBench，一个更具挑战性的基准数据集，用于评估LLMs的问题解决能力。我们从高竞争的印度理工学院（IIT）JEE-Advanced考试中精选出了515个具有挑战性的预工程数学、物理和化学问题。在这个基准中，长期推理和深入领域知识的运用对问题的解决至关重要。我们对各种开源和专有模型进行了评估，结果显示，即使使用了自一致性、自我完善和思维链提示等技术，最高性能也不到40\%。最好的模型GPT-4的典型失败模式包括代数操作错误、将抽象概念准确地转化为数学方程以及无法检索相关的领域特定概念。我们还观察到，仅仅通过输入提示不能让模型成功解决问题。

    The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40\%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompti
    
[^76]: 自动规划：利用大型语言模型进行交互决策任务的自动规划

    AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models. (arXiv:2305.15064v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15064](http://arxiv.org/abs/2305.15064)

    本文提出了一种名为AutoPlan的方法，利用大型语言模型（LLM）来指导代理完成复杂的交互式决策任务。通过将LLM提示与任务解决计划相结合并进行优化，AutoPlan在没有上下文演示的情况下，在ALFWorld上实现了与人类撰写的演示基线相当的成功率，并在HotpotQA上超过了8%。

    

    最近的大型语言模型（LLM）对于在现实环境中进行决策具有很大的潜力。然而，由于LLM中预训练知识与实际环境规则之间的不匹配，LLM在复杂的决策任务中经常失败。现有的方法要么需要昂贵的梯度计算，要么需要耗时的上下文演示。在本文中，我们提出了一种名为AutoPlan的方法，用于引导基于LLM的代理完成交互式决策任务。AutoPlan通过迭代的经验收集和反思，将LLM提示与任务解决计划相结合，并进行优化。我们的实验证明，虽然未使用上下文演示，但AutoPlan在ALFWorld上实现了与人类撰写的演示基线相当的成功率，甚至在HotpotQA上超过了8%。代码可以在https://github.com/owaski/AutoPlan上找到。

    Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.
    
[^77]: GPT-4是一个优秀的数据分析师吗？

    Is GPT-4 a Good Data Analyst?. (arXiv:2305.15038v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15038](http://arxiv.org/abs/2305.15038)

    本研究探讨了GPT-4是否是一个优秀的数据分析师，并通过面对面比较研究的方式来回答这个问题。

    

    随着大型语言模型在许多领域和任务中展示了强大的能力，如上下文理解、代码生成、语言生成、数据叙述等，许多数据分析师可能担心他们的工作是否会被人工智能取代。这个有争议的话题引起了公众的极大关注。然而，我们仍处于意见分歧的阶段，没有任何明确的结论。基于此，我们在本研究中提出了“GPT-4是一个好的数据分析师吗？”的研究问题，并通过进行面对面的比较研究来回答这个问题。具体而言，我们将GPT-4视为一个数据分析师，使用来自各个领域的数据库进行端到端的数据分析。我们提出了一个框架来解决这些问题，通过精心设计GPT-4的提示语进行实验。我们还设计了几个任务特定的评估指标，以系统地比较几个专业人员之间的性能。

    As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by artificial intelligence (AI). This controversial topic has drawn great attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the research question of "is GPT-4 a good data analyst?" in this work and aim to answer it by conducting head-to-head comparative studies. In detail, we regard GPT-4 as a data analyst to perform end-to-end data analysis with databases from a wide range of domains. We propose a framework to tackle the problems by carefully designing the prompts for GPT-4 to conduct experiments. We also design several task-specific evaluation metrics to systematically compare the performance between several professional human
    
[^78]: 自我生成示范的零样本上下文学习(Self-ICL)

    Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations. (arXiv:2305.15035v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15035](http://arxiv.org/abs/2305.15035)

    Self-ICL是一个简单的框架，通过自我生成示范的方式实现零样本上下文学习。它利用大型语言模型内在的能力来生成伪输入并预测伪标签，然后将这些伪输入和标签作为示范用于目标任务的学习。通过在23个难任务上的评估，Self-ICL在平均准确率和头对头比较方面表现优于其他零样本方法。

    

    大型语言模型(LLMs)展示了在少量输入输出示范下适应目标任务的惊人的上下文学习(ICL)能力。为了更好地进行ICL，提出了不同的方法从现有的训练语料库中选择代表性的示范。然而，这种设置与现实世界的实践不一致，因为最终用户通常在没有访问示范池的情况下查询LLMs。在这项工作中，我们引入了自我生成示范的零样本上下文学习(Self-ICL) - 一个简单的框架，通过引导LLMs的内在能力来执行零样本ICL。给定一个测试输入，Self-ICL首先提示模型生成伪输入。然后，模型通过零样本提示为伪输入预测伪标签。最后，我们使用伪输入-标签对作为示范来进行测试输入的ICL。在23个BIG-Bench难任务的评估中，Self-ICL在平均准确率和头对头比较方面优于零样本基线。此外，通过零样本思维链路，Self-ICL实现了连续ICL。

    Large language models (LLMs) have exhibited striking in-context learning (ICL) ability to adapt to target tasks with a few input-output demonstrations. For better ICL, different methods are proposed to select representative demonstrations from existing training corpora. However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools. In this work, we introduce Self-ICL -- a simple framework which bootstraps LMs' intrinsic capabilities to perform zero-shot ICL. Given a test input, Self-ICL first prompts the model to generate pseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via zero-shot prompting. Finally, we perform ICL for the test input with the pseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard tasks shows Self-ICL outperforms zero-shot baselines on both average accuracy and head-to-head comparison. Moreover, with zero-shot chain-of-thought, Self-ICL achieves re
    
[^79]: ImageNetVC：在1000个ImageNet类别上进行零次和少次远距离视觉常识评估

    ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories. (arXiv:2305.15028v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15028](http://arxiv.org/abs/2305.15028)

    本论文提出了ImageNetVC数据集，用于对1000个ImageNet类别进行零次和少次远距离视觉常识评估。通过该评估，分析了目前大型语言模型和视觉增强模型对视觉常识知识的掌握程度，为丰富语言模型的视觉常识知识提供了见解。

    

    最近，大型语言模型（LLMs）已成为通用接口，对全面的视觉知识提出了重要要求。然而，目前的LLMs及其视觉增强型模型（VaLMs）在掌握视觉常识知识方面的水平仍不清楚。为了调查这个问题，我们提出了ImageNetVC，这是一个人工注释的数据集，专门用于对1000个ImageNet类别进行零次和少次远距离视觉常识评估。利用ImageNetVC，我们对单模态LLMs和VaLMs的基本视觉常识知识进行了基准评估。此外，我们分析了影响大规模模型视觉常识知识的因素，提供了丰富了视觉常识知识的语言模型发展的见解。我们的代码和数据集可在https://github.com/hemingkx/ImageNetVC上获取。

    Recently, Large Language Models (LLMs) have been serving as general-purpose interfaces, posing a significant demand for comprehensive visual knowledge. However, it remains unclear how well current LLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge. To investigate this, we propose ImageNetVC, a human-annotated dataset specifically designed for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet categories. Utilizing ImageNetVC, we benchmark the fundamental visual commonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze the factors affecting the visual commonsense knowledge of large-scale models, providing insights into the development of language models enriched with visual commonsense knowledge. Our code and dataset are available at https://github.com/hemingkx/ImageNetVC.
    
[^80]: Calc-X和Calcformers：通过与符号系统的交互增强算术推理的能力

    Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems. (arXiv:2305.15017v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15017](http://arxiv.org/abs/2305.15017)

    该论文介绍了Calc-X和Calcformers，它们通过与符号系统的交互使语言模型在算术推理任务中表现更准确，提高了生成正确结果的准确性。

    

    尽管在许多任务中表现出色，但语言模型在需要进行算术计算的任务中往往会产生事实错误。为了解决这个问题，我们创建了Calc-X，这是一个演示如何在推理链中正确使用计算器的数据集合。Calc-X适用于教导语言模型将计算任务转移到符号系统中。我们调查并统一了几个已有的推理链数据集，并提出了一个标准格式，结果是一个包含超过30万个需要进行算术推理的样本的标准数据集合。最后，我们使用新的Calc-X集合来训练我们称之为Calcformers的开源计算器模型，并展示这些模型相对于普通语言模型基线生成正确结果的准确性近乎翻倍。我们公开提供所有的Calc-X数据集、源代码和Calcformers模型。

    Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation. We address this deficiency by creating Calc-X, a collection of datasets that demonstrates the appropriate use of a calculator in reasoning chains. Calc-X is suitable for teaching language models to offload computations to a symbolic system. We survey and unify several existing chain-of-thought datasets into a proposed format, resulting in a standard collection of over 300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-X collection to train open-source calculator-using models we call Calcformers and show that these models approximately double the accuracy of generating correct results compared to vanilla language model baselines. We make all Calc-X datasets, source code and Calcformers models publicly available.
    
[^81]: 使用语言模型进行推理就是使用世界模型进行规划

    Reasoning with Language Model is Planning with World Model. (arXiv:2305.14992v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14992](http://arxiv.org/abs/2305.14992)

    本文介绍了一种新的大型语言模型推理框架RAP，通过构建内部的世界模型并模拟长期行动结果，从而使语言模型能够进行像人类大脑一样的有意识规划。

    

    大型语言模型(LLMs)展示了出色的推理能力，特别是在提示生成中间推理步骤时（例如思维链）。然而，LLMs在一些对人类来说容易的问题上仍然存在困难，例如在给定环境中生成执行任务的行动计划，或进行复杂的数学、逻辑和常识推理。这种不足源于LLMs缺乏一个内部的“世界模型”，用于预测世界的状态（例如环境状况、中间变量值）并模拟行动的长期结果。这使得LLMs无法像人类大脑那样进行有意识的规划，其中包括探索替代的推理路径、预测未来的状态和回报，并对现有的推理步骤进行迭代优化。为了克服这些限制，我们提出了一种新的LLM推理框架，即RAP（通过规划进行推理）。

    Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning $\textbf{(RAP)}$. RAP repurpo
    
[^82]: GPTAraEval: 对Arabic NLP上的ChatGPT进行全面评估

    GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP. (arXiv:2305.14976v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14976](http://arxiv.org/abs/2305.14976)

    这项研究对ChatGPT在阿拉伯语自然语言处理领域进行了全面评估，发现尽管在英文上表现出色，但在阿拉伯语上的性能不如经过阿拉伯语微调的较小模型。

    

    ChatGPT的出现标志着NLP领域的一次变革，尤其在许多英文基准测试中表现出色。然而，该模型在不同语言背景下的有效性仍然是未知领域。本研究旨在填补这一知识空白，重点评估ChatGPT在阿拉伯语言和方言上的能力。我们进行了一项大规模的自动化和人工评估研究，涵盖了44个不同的语言理解和生成任务，涉及60多个不同的数据集。据我们所知，这是对ChatGPT在Arabic NLP中进行的首次全面性性能分析。我们的研究结果表明，尽管在英语上表现出色，但ChatGPT在阿拉伯语上的性能始终不如经过阿拉伯语微调的较小模型。我们进一步对比了ChatGPT和GPT-4在现代标准阿拉伯语（MSA）和方言阿拉伯语（DA）上的性能，揭示了...

    ChatGPT's emergence heralds a transformative phase in NLP, particularly demonstrated through its excellent performance on many English benchmarks. However, the model's efficacy across diverse linguistic contexts remains largely uncharted territory. This work aims to bridge this knowledge gap, with a primary focus on assessing ChatGPT's capabilities on Arabic languages and dialectal varieties. Our comprehensive study conducts a large-scale automated and human evaluation of ChatGPT, encompassing 44 distinct language understanding and generation tasks on over 60 different datasets. To our knowledge, this marks the first extensive performance analysis of ChatGPT's deployment in Arabic NLP. Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic. We further undertake a meticulous comparison of ChatGPT and GPT-4's Modern Standard Arabic (MSA) and Dialectal Arabic (DA), unveiling th
    
[^83]: 通用的自适应提示

    Universal Self-Adaptive Prompting. (arXiv:2305.14926v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14926](http://arxiv.org/abs/2305.14926)

    本研究通过介绍一种专门为零样本学习而设计的自动提示设计方法，解决了现有大型语言模型零样本性能较弱的问题。这种方法只需要少量无标签数据和一个推理模型，具有高度灵活性和通用性。

    

    现代大型语言模型(LLMs)的标志是它们出色的零样本和少样本能力，通常通过提示的上下文学习(ICL)来实现。然而，尽管高度令人垂涎并且最为通用，LLMs的零样本性能通常较弱，因为缺乏引导并且难以在基于普通任务的情况下应用现有的自动提示设计方法，当地面真实标签不可用时。在这项研究中，我们通过提出通用自适应提示(USP)来解决这个问题，这是一种专门针对零样本学习的自动提示设计方法(同时兼容少样本学习)。USP只需要少量无标签数据和一个仅进行推理的LLM，非常灵活：为了实现通用提示，USP将可能的NLP任务归类为三种可能的任务类型之一，然后使用相应的选择器来选择最合适的查询和零样本模型生成的响应作为伪演示。

    A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot). Requiring only a small amount of unlabeled data and an inference-only LLM, USP is highly versatile: to achieve universal prompting, USP categorizes a possible NLP task into one of the three possible task types and then uses a corresponding selector to select the most suitable queries and zero-shot model-generated responses as pseudo-demonstration
    
[^84]: 评估评估指标：使用测量理论分析自然语言生成评估指标的框架

    Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory. (arXiv:2305.14889v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14889](http://arxiv.org/abs/2305.14889)

    本文提出了一个基于测量理论的框架MetricEval，用于评估自然语言生成（NLG）的评估指标。通过该框架，可以量化指标的不确定性，并解决人工评估的效度结构混淆和基于LLM的指标的可靠性问题。

    

    本文解决了自然语言生成（NLG）模型评估中的一个基本挑战--评估指标的设计和评估。我们意识到现有自动指标的局限性以及当前人工评估存在的误差，提出了MetricEval框架，该框架基于测量理论，教育测试设计的基础，用于概念化和评估NLG评估指标的可靠性和效度。该框架规范了测量误差的来源，并提供了基于实证数据评估评估指标的统计工具。借助我们的框架，可以量化指标的不确定性，以更好地解释结果。为了示范我们的框架在实践中的应用，我们分析了一组用于摘要的评估指标，并确定了人工评估中的效度结构混淆和基于LLM的指标中的可靠性相关问题。通过MetricEval，我们旨在推动评估指标的设计、评估和解释。

    We address a fundamental challenge in Natural Language Generation (NLG) model evaluation -- the design and evaluation of evaluation metrics. Recognizing the limitations of existing automatic metrics and noises from how current human evaluation was conducted, we propose MetricEval, a framework informed by measurement theory, the foundation of educational test design, for conceptualizing and evaluating the reliability and validity of NLG evaluation metrics. The framework formalizes the source of measurement error and offers statistical tools for evaluating evaluation metrics based on empirical data. With our framework, one can quantify the uncertainty of the metrics to better interpret the result. To exemplify the use of our framework in practice, we analyzed a set of evaluation metrics for summarization and identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics. Through MetricEval, we aim to promote the design, evaluation, and interp
    
[^85]: 省心学习变得领先：重新审视基于简单种子弱监督文本分类

    Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification. (arXiv:2305.14794v1 [cs.CL])

    [http://arxiv.org/abs/2305.14794](http://arxiv.org/abs/2305.14794)

    本文重新审视了基于种子匹配的伪标签生成方法，并通过简单的单词删除来缓解因规则注入的标签偏见而带来的影响，提高该方法的性能，其性能达到甚至超过最先进技术。

    

    近来，弱监督文本分类的研究主要集中在设计复杂的方法，将高层次的人类启发式方法转化为高质量的伪标签。在本文中，我们重新审视了基于种子匹配的方法，它是生成伪标签的最简单方法，我们展示了它的强大性能。我们表明种子匹配的有限性能很大程度上归因于种子匹配规则注入的标签偏差，这会阻止分类器学习可靠的置信度来选择高质量伪标签。有趣的是，简单地删除匹配输入文本中的种子词可以缓解标签偏差并帮助学习更好的置信度。随后，种子匹配的性能可以显著提高，使它达到或甚至超过最先进技术。此外，为了处理种子词不为人知的情况，我们建议简单地删除输入文本中的单词标记。

    Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show that its power was greatly underestimated. We show that the limited performance of seed matching is largely due to the label bias injected by the simple seed-match rule, which prevents the classifier from learning reliable confidence for selecting high-quality pseudo-labels. Interestingly, simply deleting the seed words present in the matched input texts can mitigate the label bias and help learn better confidence. Subsequently, the performance achieved by seed matching can be improved significantly, making it on par with or even better than the state-of-the-art. Furthermore, to handle the case when the seed words are not made known, we propose to simply delete the word tokens in the input tex
    
[^86]: Bi-Drop: 自适应子网络优化的预训练语言模型通用微调

    Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via Adaptive Subnetwork Optimization. (arXiv:2305.14760v1 [cs.CL])

    [http://arxiv.org/abs/2305.14760](http://arxiv.org/abs/2305.14760)

    本文提出了一种动态微调策略Bi-Drop来针对预训练语言模型在大规模微调时可能出现的过拟合现象。经过GLUE基准测试，Bi-Drop的性能优于其他微调方法，并对于多任务、多领域转移、数据不均衡和低资源情况下也表现出强大的鲁棒性。

    

    预训练语言模型在各种自然语言理解任务中已取得了显著成功。然而，如果训练集有限，对预训练模型的大规模微调容易出现过拟合，从而导致性能下降。本文提出了一种动态微调策略Bi-Drop来针对预训练语言模型的缺点。它利用dropout生成的各种子模型的梯度信息有选择性地更新模型参数。在GLUE基准测试中的实验表明，Bi-Drop的性能优于之前的微调方法，并在各种预训练模型中表现出对比纯微调更强的稳健性与通用性。此外，实证结果表明，在多任务、多领域转移、数据不均衡和低资源情况下，Bi-Drop的表现差异显著，具有强大的泛化能力和鲁棒性。

    Pretrained language models have achieved remarkable success in a variety of natural language understanding tasks. Nevertheless, finetuning large pretrained models on downstream tasks is susceptible to overfitting if the training set is limited, which will lead to diminished performance. In this work, we propose a dynamic fine-tuning strategy for pretrained language models called Bi-Drop. It utilizes the gradient information of various sub-models generated by dropout to update the model parameters selectively. Experiments on the GLUE benchmark show that Bi-Drop outperforms previous fine-tuning methods by a considerable margin, and exhibits consistent superiority over vanilla fine-tuning across various pretrained models. Furthermore, empirical results indicate that Bi-Drop yields substantial improvements in the multiple task or domain transfer, data imbalance, and low-resource scenarios, demonstrating superb generalization ability and robustness.
    
[^87]: 上下文模型及评估在文体改写中的必要性研究

    Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting. (arXiv:2305.14755v1 [cs.CL])

    [http://arxiv.org/abs/2305.14755](http://arxiv.org/abs/2305.14755)

    本文研究提出了在文体改写的重写和评估阶段整合文本上下文的必要性，并通过few-shot prompting比较非上下文改写和上下文改写的效果。研究发现，自动度量指标不一定能反映出人类的偏好。

    

    大多数现有的文体改写方法在句子级别操作，但是忽视文本更广泛的上下文可以导致改写结果是一般化、歧义和不连贯的。本文提出整合文本上下文到文体改写的重写和评估阶段，重点关注形式、毒性和情感转移任务。我们通过对 GPT-3.5 和 GPT NeoX 的 few-shot 提问比较重写的方法，并比较非上下文改写和上下文改写。我们的实验表明，人们通常更喜欢上下文改写，但自动度量指标（如 BLEU，sBERT）不是这样的。为弥合这种差距，我们提出通用自动度量指标的上下文融合版本，并证明这些更能反映人类偏好。总的来说，本文强调在文体改写的重写和评估阶段整合文本上下文的重要性。

    Most existing stylistic text rewriting methods operate on a sentence level, but ignoring the broader context of the text can lead to generic, ambiguous, and incoherent rewrites. In this paper, we propose the integration of preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting, focusing on formality, toxicity, and sentiment transfer tasks. We conduct a comparative evaluation of rewriting through few-shot prompting of GPT-3.5 and GPT NeoX, comparing non-contextual rewrites to contextual rewrites. Our experiments show that humans often prefer contextual rewrites over non-contextual ones, but automatic metrics (e.g., BLEU, sBERT) do not. To bridge this gap, we propose context-infused versions of common automatic metrics, and show that these better reflect human preferences. Overall, our paper highlights the importance of integrating preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting.
    
[^88]: ECHo: 基于人类中心推理的事件因果推断

    ECHo: Event Causality Inference via Human-centric Reasoning. (arXiv:2305.14740v1 [cs.AI])

    [http://arxiv.org/abs/2305.14740](http://arxiv.org/abs/2305.14740)

    ECHo是一个基于人类中心推理的事件因果推断数据集，并提出了一个与CoT范式对齐的统一框架来评估当前AI系统的推理能力。

    

    我们介绍了 ECHo，这是一个基于视觉和语言社交情境的事件因果推断诊断数据集。 ECHo利用从犯罪剧中收集的真实人类中心演绎信息，通过激发中间心灵理论（ToM）来弥合多模态推理的鸿沟，从而提高社交智能。我们提出了一个与Chain-of-Thought（CoT）范式对齐的统一框架，以评估当前AI系统的推理能力。这个ToM增强的CoT管道可以在 零-shot视觉和语言理解中包容和整合各种大型基础模型。利用这个框架，我们通过三个互补的基于人类中心的ECHo任务来审查先进的大型语言和多模态模型。进一步的分析表明，ECHo是一个具有挑战性的数据集，可以暴露推理中的不完善和不一致性。

    We introduce ECHo, a diagnostic dataset of event causality inference grounded in visual-and-linguistic social scenarios. ECHo employs real-world human-centric deductive information collected from crime drama, bridging the gap in multimodal reasoning towards higher social intelligence through the elicitation of intermediate Theory-of-Mind (ToM). We propose a unified framework aligned with the Chain-of-Thought (CoT) paradigm to assess the reasoning capability of current AI systems. This ToM-enhanced CoT pipeline can accommodate and integrate various large foundation models in zero-shot visual-and-linguistic understanding. With this framework, we scrutinize the advanced large language and multimodal models via three complementary human-centric ECHo tasks. Further analysis demonstrates ECHo as a challenging dataset to expose imperfections and inconsistencies in reasoning.
    
[^89]: 边缘聚焦：基于异常值的毒性检测中受损人群的识别

    Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])

    [http://arxiv.org/abs/2305.14735](http://arxiv.org/abs/2305.14735)

    本文提出了一种基于异常值的方法，用于识别在毒性检测中受到伤害的人群，发现对于这些异常值，模型性能较差，他们面临的毒性更高。

    

    衡量人工智能对边缘社区影响的标准方法是确定特定人口群体之间的性能差异。这些方法旨在解决针对弱势群体的伤害问题，但它们会掩盖由交叉子群或跨人口群体共享的伤害模式。相反，我们将“边缘”定义为具有远离“常态” 的人口属性的数据点，并度量针对这些异常值的伤害。我们提出了一种基于群体的性能差异指数（GPDI），以衡量数据集细分为子组对面临增加的伤害的识别程度。我们将我们的方法应用于检测毒性检测中的差异，并发现针对异常值的文本在所有类型的毒性检验中毒性更高，高达28％至86％。我们还发现，对于人口学异常值，模型性能始终较差，异常值和非异常值之间的错误差距高达10％。

    A standard method for measuring the impacts of AI on marginalized communities is to determine performance discrepancies between specified demographic groups. These approaches aim to address harms toward vulnerable groups, but they obscure harm patterns faced by intersectional subgroups or shared across demographic groups. We instead operationalize "the margins" as data points that are statistical outliers due to having demographic attributes distant from the "norm" and measure harms toward these outliers. We propose a Group-Based Performance Disparity Index (GPDI) that measures the extent to which a subdivision of a dataset into subgroups identifies those facing increased harms. We apply our approach to detecting disparities in toxicity detection and find that text targeting outliers is 28% to 86% more toxic for all types of toxicity examined. We also discover that model performance is consistently worse for demographic outliers, with disparities in error between outliers and non-outli
    
[^90]: 自动评估度量中的性别偏见：基于图像字幕的案例研究

    Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning. (arXiv:2305.14711v1 [cs.CL])

    [http://arxiv.org/abs/2305.14711](http://arxiv.org/abs/2305.14711)

    本文研究了模型评估度量中的性别偏见对图像字幕任务的影响，并提出了替代方案以解决这一问题。

    

    预训练模型评估度量在图像字幕等各种自然语言生成任务中已经证明具有强大的性能，并与人类判断高度相关。然而，它们对公平性的影响尚未被深入探讨——人们普遍认为预训练模型可能会编码社会偏见，并将其用于评估目的可能会无意中表现并潜在地放大偏见。本文针对图像字幕任务，对模型评估度量中的性别偏见进行了系统研究。具体而言，我们首先确定和量化了不同评估度量中关于职业、活动和物体概念的性别偏见。然后，我们展示了使用这些有偏见的度量带来的负面后果，比如在部署过程中偏向有偏见的生成模型，并通过强化学习向生成模型传播偏见。我们还提出了一个简单但有效的替代方案。

    Pretrained model-based evaluation metrics have demonstrated strong performance with high correlations with human judgments in various natural language generation tasks such as image captioning. Despite the impressive results, their impact on fairness is under-explored -- it is widely acknowledged that pretrained models can encode societal biases, and utilizing them for evaluation purposes may inadvertently manifest and potentially amplify biases. In this paper, we conduct a systematic study in gender biases of model-based evaluation metrics with a focus on image captioning tasks. Specifically, we first identify and quantify gender biases in different evaluation metrics regarding profession, activity, and object concepts. Then, we demonstrate the negative consequences of using these biased metrics, such as favoring biased generation models in deployment and propagating the biases to generation models through reinforcement learning. We also present a simple but effective alternative to r
    
[^91]: 你就是你的注释：通过注释者表示实现更好的模型

    You Are What You Annotate: Towards Better Models through Annotator Representations. (arXiv:2305.14663v1 [cs.CL])

    [http://arxiv.org/abs/2305.14663](http://arxiv.org/abs/2305.14663)

    提出了一种方法，通过创建注释者和注释的矩阵表示以捕捉其特点，并在建模过程中使用它们来显著提高自然语言处理模型的性能，同时帮助民主化人工智能。

    

    在自然语言处理（NLP）任务中，注释者的不一致是普遍存在的。存在多种原因导致注释者的不同意见，包括任务的主观性、困难案例、不明确的指南等等。我们的方法不是仅仅汇总注释的标签来获得数据注释，而是提议要明确考虑注释者特征，并在建模过程中利用它们。我们利用可学习的矩阵分别为注释者（注释者嵌入）和其注释（注释嵌入）创建表示。我们的方法通过使用少于1％的模型参数，在各种NLP基准测试中显著提高了模型性能。通过捕捉单个注释者的独特倾向和主观性，我们的嵌入有助于民主化人工智能，并确保人工智能模型包含多元化的观点。

    Annotator disagreement is ubiquitous in natural language processing (NLP) tasks. There are multiple reasons for such disagreements, including the subjectivity of the task, difficult cases, unclear guidelines, and so on. Rather than simply aggregating labels to obtain data annotations, we instead propose to explicitly account for the annotator idiosyncrasies and leverage them in the modeling process. We create representations for the annotators (annotator embeddings) and their annotations (annotation embeddings) with learnable matrices associated with each. Our approach significantly improves model performance on various NLP benchmarks by adding fewer than 1% model parameters. By capturing the unique tendencies and subjectivity of individual annotators, our embeddings help democratize AI and ensure that AI models are inclusive of diverse viewpoints.
    
[^92]: COMET-M: 在复杂句子中推理多个事件

    COMET-M: Reasoning about Multiple Events in Complex Sentences. (arXiv:2305.14617v1 [cs.CL])

    [http://arxiv.org/abs/2305.14617](http://arxiv.org/abs/2305.14617)

    提出了COMET-M，该模型可以推理复杂句子中多个事件之间的关系以及生成常识推断，并在35K个人类编写的推断上进行训练，相对于之前的COMET模型在生成多事件推断方面有显着的性能改进。

    

    理解说话者的意图通常涉及绘制常识推断，以推理未明确陈述的内容。在多事件句子中，需要基于上下文知识理解事件之间的关系。我们提出了COMET-M（Multi-Event），这是一个以事件为中心的常识模型，能够针对复杂句子内的目标事件生成常识推断。COMET-M是基于COMET（Bosselut et al.，2019）发展而来的，后者擅长为简单句子生成以事件为中心的推断，但在自然文本中普遍存在的多事件句子的复杂性方面表现不佳。为了克服这个限制，我们整理了一个包含35K个人类编写推断的多事件推断数据集。 我们在人类编写的推断上训练了COMET-M，并创建了使用自动标记示例的基线。实验结果表明，COMET-M在生成多事件推断方面相对于COMET具有显着的性能改进。此外，COMET-M成功预测了测试集中60％的复杂句子目标事件的常识推断。

    Understanding the speaker's intended meaning often involves drawing commonsense inferences to reason about what is not stated explicitly. In multi-event sentences, it requires understanding the relationships between events based on contextual knowledge. We propose COMET-M (Multi-Event), an event-centric commonsense model capable of generating commonsense inferences for a target event within a complex sentence. COMET-M builds upon COMET (Bosselut et al., 2019), which excels at generating event-centric inferences for simple sentences, but struggles with the complexity of multi-event sentences prevalent in natural text. To overcome this limitation, we curate a multi-event inference dataset of 35K human-written inferences. We trained COMET-M on the human-written inferences and also created baselines using automatically labeled examples. Experimental results demonstrate the significant performance improvement of COMET-M over COMET in generating multi-event inferences. Moreover, COMET-M succ
    
[^93]: MathDial: 一个以数学推理问题为基础的富含教育性属性的对话辅导数据集。

    MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems. (arXiv:2305.14536v1 [cs.CL])

    [http://arxiv.org/abs/2305.14536](http://arxiv.org/abs/2305.14536)

    MathDial是一个由实际教师和大型语言模型半合成生成的对话数据集，旨在解决自动对话辅导工具缺少高质量数据的问题。这个数据集关注通过引导学生使用问题来探索数学问题。

    

    尽管自动对话辅导工具在个性化和提高教育可及性方面具有巨大潜力，但由于缺乏足够大规模且高质量的数据集，这类系统的研究一直受到阻碍。然而，收集此类数据集仍然具有挑战性，因为录制辅导会引发隐私问题，而众包则会导致数据质量不足。为了解决这个问题，我们提出了一个框架，通过将真正的老师与大型语言模型(LLM)配对，构建常见学生错误的支架，来半合成地生成这些对话。在本文中，我们描述了使用这个框架来收集MathDial的最新工作，这是一个当前拥有约1.5k个基于多步数学词问题的辅导对话数据集。我们展示了我们的数据集展现出丰富的教育性属性，主要关注通过引导学生使用问题来探索问题。此外，我们概述了MathDial及其地面注释可用于微调...

    Although automatic dialogue tutors hold great potential in making education personalized and more accessible, research on such systems has been hampered by a lack of sufficiently large and high-quality datasets. However, collecting such datasets remains challenging, as recording tutoring sessions raises privacy concerns and crowdsourcing leads to insufficient data quality. To address this problem, we propose a framework to semi-synthetically generate such dialogues by pairing real teachers with a large language model (LLM) scaffolded to represent common student errors. In this paper, we describe our ongoing efforts to use this framework to collect MathDial, a dataset of currently ca. 1.5k tutoring dialogues grounded in multi-step math word problems. We show that our dataset exhibits rich pedagogical properties, focusing on guiding students using sense-making questions to let them explore problems. Moreover, we outline that MathDial and its grounding annotations can be used to finetune 
    
[^94]: NAIL: 带高效非自回归解码器的词汇检索指数

    NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders. (arXiv:2305.14499v1 [cs.CL])

    [http://arxiv.org/abs/2305.14499](http://arxiv.org/abs/2305.14499)

    NAIL是一种带有高效非自回归解码器的词汇检索指数模型，可与现有的预训练模型兼容，并且使用商品CPU提供服务。它可以捕捉Transformer交叉关注模型收益高达86％的方法，与BM25检索器结合使用匹配当前最先进的双编码器检索器的质量。

    

    神经文档重新排名器在精度方面非常有效。然而，最好的模型需要专用硬件进行服务，这是昂贵并且通常是不可行的。为了避免这种服务时间要求，我们提出一种捕捉Transformer交叉关注模型收益高达86％的方法，该方法使用只需要每个文档转换器FLOP的10-6％的词汇得分功能，并且可以使用商品CPU提供服务。当与BM25检索器结合使用时，此方法可以匹配现有的最先进的双编码器检索器的质量，该检索器仍需要加速器进行查询编码。我们将NAIL（带有语言模型的非自回归索引）引入为与最近的编码器-解码器和仅解码器大型语言模型（例如T5、GPT-3和PaLM）兼容的模型体系结构。该模型体系结构可以利用现有的预训练检查点，并可以微调以有效地构建不需要n

    Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this serving-time requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10-6% of the Transformer's FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-of-the art dual encoder retriever, that still requires an accelerator for query encoding. We introduce NAIL (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require n
    
[^95]: 通过情境对齐和可解释文本蕴含来比较社会文化规范的异同：以中国和美国文化为例

    Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment. (arXiv:2305.14492v1 [cs.CL])

    [http://arxiv.org/abs/2305.14492](http://arxiv.org/abs/2305.14492)

    本论文提出了一种新方法来比较中美文化的社会规范，通过情境对齐和上下文学习提取社会规范。作者构建了一个高质量的数据集，通过测试发现该方法具有研究社会文化异同的潜力。

    

    跨文化推理需要建立在相应文化背景下的准确规范基础之上。 然而，当前的社会规范计算模型主要关注美国社会。 本文提出了一种新的方法来发现和比较中国和美国文化的社会规范。 我们通过利用一个中文问答平台 - 知乎 - 和现有的SocialChemistry数据集作为不同文化维度的代理，通过情境对齐跨文化比较社会情境，使用上下文学习从文本中提取社会规范。我们将Chain-of-Thought提示嵌入到人工智能协作框架中，构建了一个高质量的数据集，其中包含3,069个跨中国和美国文化对齐的社会规范以及相应的自由文本解释。为了测试模型在跨文化社会规范方面的推理能力，我们提出了社会规范的可解释文本蕴含任务，其目标是预测一个文化中的社会规范的解释是否暗示另一个文化中的类似规范。我们使用这个任务来评估我们的方法，并发现我们的数据集提供了研究社会文化的异同的丰富机会。

    Designing systems that can reason across cultures requires that they are grounded in the norms of the contexts in which they operate. However, current research on developing computational models of social norms has primarily focused on American society. Here, we propose a novel approach to discover and compare descriptive social norms across Chinese and American cultures. We demonstrate our approach by leveraging discussions on a Chinese Q&A platform-Zhihu-and the existing SocialChemistry dataset as proxies for contrasting cultural axes, align social situations cross-culturally, and extract social norms from texts using in-context learning. Embedding Chain-of-Thought prompting in a human-AI collaborative framework, we build a high-quality dataset of 3,069 social norms aligned with social situations across Chinese and American cultures alongside corresponding free-text explanations. To test the ability of models to reason about social norms across cultures, we introduce the task of expl
    
[^96]: 在成功和失败之间跳舞：使用SALSA进行编辑级别的简化评估

    Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA. (arXiv:2305.14458v1 [cs.CL])

    [http://arxiv.org/abs/2305.14458](http://arxiv.org/abs/2305.14458)

    本研究引入了SALSA框架，对大型语言模型进行细粒度的文本简化评估，通过21种不同编辑类型，揭示了不同模型和人类文本简化的偏好和表现，并开发了LENS-SALSA指标用于自动简化度量。

    

    大型语言模型（例如GPT-3.5）可以产生高度评级的简化文本，但当前的人类评估方法未能提供对系统特定优势和劣势的清晰了解。为了解决这个限制，我们引入了SALSA，这是一个基于编辑的人类注释框架，可以实现整体和精细的文本简化评估。我们开发了21种基于语言学的编辑类型，涵盖了概念、句法和词汇简单性的所有成功和失败维度。使用SALSA，我们在700个简化案例上收集了12K个编辑注释，揭示了微调模型、少样本LLM和人类之间转化方法分布的差异，并发现GPT-3.5执行的高质量编辑比人类更多，但仍然存在频繁的错误。使用我们的精细注释，我们开发了LENS-SALSA，一种无参考自动简化度量，训练以直接从输入文本和提议的简化中预测句子和编辑级别质量分数。

    Large language models (e.g., GPT-3.5) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems' specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 12K edit annotations on 700 simplifications, revealing discrepancies in the distribution of transformation approaches performed by fine-tuned models, few-shot LLMs and humans, and finding GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our fine-grained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentence- an
    
[^97]: 使用大语言模型自动选择带有推理的模型

    Automatic Model Selection with Large Language Models for Reasoning. (arXiv:2305.14333v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14333](http://arxiv.org/abs/2305.14333)

    本论文提出了一种使用大语言模型来自动选择带有推理的模型的方法，通过结合自然语言和编程语言的优势，实现了显著的性能改进，并在多个数据集上取得了新的最先进结果。

    

    Chain-of-Thought（CoT）和Program-Aided Language Models（PAL）代表了两种不同的推理方法，各自具有自己的优势。CoT采用自然语言，具有灵活性和可解释性，而PAL利用编程语言，产生更结构化和严密的逻辑。我们引入了一种模型选择方法，通过使用大语言模型（LLM）动态选择它们之间的最佳方法来结合两者的优势。我们的理论分析强调了这种方法的可行性，经验结果进一步证实了这一点。我们提出的方法在八个推理数据集上与Codex、ChatGPT和GPT-4展示了显著的性能改进。此外，我们的方法与自一致性相辅相成；当整合在一起时，它可以进一步提高性能，同时显著降低计算成本。此外，我们在GSM8K和SVAMP上取得了新的最先进结果，分别达到96.8%和93.7%的准确率。我们的代码、数据和...

    Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We introduce a model selection method to combine the best of both worlds by employing a large language model (LLM) to dynamically select between them. Our theoretical analysis underscores the feasibility of this method, which is further corroborated by empirical results. Our proposed method demonstrates significant performance improvements across eight reasoning datasets with Codex, ChatGPT, and GPT-4. Additionally, our method is complementary to self-consistency; when integrated, it can further enhance performance while significantly reducing computation costs. Moreover, we achieve new state-of-the-art results on GSM8K and SVAMP, with respective accuracies of 96.8% and 93.7%. Our code, data and pr
    
[^98]: TalkUp：为理解赋予力量的语言铺平道路

    TalkUp: Paving the Way for Understanding Empowering Language. (arXiv:2305.14326v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14326](http://arxiv.org/abs/2305.14326)

    本研究通过借鉴语言学和社会心理学的文献，探索了赋予力量语言的特征，并创建了一个名为TalkUp的数据集用于训练捕捉赋予力量和削弱力量语言的语言模型，并为探索含义的含蓄性、预设以及社会背景如何影响语言的意义提供了一个途径。

    

    赋予力量的语言在教育、工作环境和医疗等许多现实世界的情境中都非常重要。尽管语言技术在这些情境中越来越普遍，但赋予力量在自然语言处理领域很少被研究，而且由于其隐性的特质，这一概念的实践化本质上具有挑战性。本研究借鉴了语言学和社会心理学的文献，探索了赋予力量的语言的特征。然后，我们通过众包的方式创建了一个新颖的 Reddit 帖子数据集，对这些帖子进行了赋予力量的标注，并分析了这些帖子对读者的赋予力量的原因以及发帖者和读者之间的社交关系。我们的初步分析表明，我们称之为TalkUp的数据集可以用来训练能够捕捉赋予力量和削弱力量语言的语言模型。更广泛地说，TalkUp为探索含义的含蓄性、预设以及社会背景如何影响语言的意义提供了一个途径。

    Empowering language is important in many real-world contexts, from education to workplace dynamics to healthcare. Though language technologies are growing more prevalent in these contexts, empowerment has seldom been studied in NLP, and moreover, it is inherently challenging to operationalize because of its implicit nature. This work builds from linguistic and social psychology literature to explore what characterizes empowering language. We then crowdsource a novel dataset of Reddit posts labeled for empowerment, reasons why these posts are empowering to readers, and the social relationships between posters and readers. Our preliminary analyses show that this dataset, which we call TalkUp, can be used to train language models that capture empowering and disempowering language. More broadly, TalkUp provides an avenue to explore implication, presuppositions, and how social context influences the meaning of language.
    
[^99]: 基于LLM的数据增强提升跨语言性能

    LLM-powered Data Augmentation for Enhanced Cross-lingual Performance. (arXiv:2305.14288v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14288](http://arxiv.org/abs/2305.14288)

    本文研究了利用LLMs进行数据增强以提升跨语言常识推理数据集性能的潜力，并通过评估展示了利用LLMs生成数据的总体优势，同时进行了人工评估，证明了生成的示例具有较高的自然性和逻辑连贯性。

    

    本文研究了利用大型语言模型（LLMs）对跨语言常识推理数据集进行数据增强的潜力，其中可用的训练数据非常有限。为了实现这一目标，我们利用了几个LLMs，包括Dolly-v2，StableVicuna，ChatGPT和GPT-4，来增强XCOPA，XWinograd和XStoryCloze三个数据集。随后，我们使用合成数据评估了小型多语言模型mBERT和XLMR的微调效果。我们比较了以英语和目标语言生成的数据进行训练的性能，以及翻译成英语的生成数据，揭示了利用LLMs生成数据的总体优势，例如在最佳情况下显著提高了13.4个准确率分数。此外，我们进行了人工评估，询问母语者评估不同语言之间生成的示例的自然性和逻辑连贯性。评估结果表明，像ChatG这样的LLMs可以产生较自然和逻辑连贯的示例。

    This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatG
    
[^100]: 检索增强的大型语言模型的查询重写

    Query Rewriting for Retrieval-Augmented Large Language Models. (arXiv:2305.14283v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14283](http://arxiv.org/abs/2305.14283)

    该论文介绍了一种新的框架，即查询重写-检索-阅读，用于检索增强的大型语言模型。通过关注搜索查询本身的适应性，采用可训练的重写器来更好地对齐查询与冻结模块，从而改善了检索的效果。实验结果表明该方法在知识密集任务上取得了显著的进展。

    

    大型语言模型（LLMs）在检索后阅读流程中充当强大的黑盒读者，在知识密集任务中取得了显著的进展。本研究从查询重写的角度引入了一个新的框架，即重新写入-检索-阅读，而不是之前的检索后阅读方法，用于检索增强的LLMs。与以往侧重于调整检索器或阅读器的研究不同，我们的方法关注的是搜索查询本身的适应性，因为输入文本与检索所需知识之间不可避免存在差距。我们首先提示LLM生成查询，然后使用网络搜索引擎来检索上下文。此外，为了更好地使查询与冻结模块对齐，我们提出了一个可训练的方案。采用一个小型语言模型作为可训练的重写器，以适应黑盒LLM阅读器。通过强化学习，使用LLM阅读器的反馈训练重写器。在Do任务数据集上进行了评估。

    Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on do
    
[^101]: 分层提示提升大规模语言模型在网络导航中的应用

    Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14257](http://arxiv.org/abs/2305.14257)

    这项研究提出了一种分层提示方法来解决大规模语言模型在处理复杂观察的交互决策任务中的困难。研究表明该方法在网络导航中的效果优于先前最先进的提示机制，具有广泛的适用性。

    

    大规模语言模型（LLMs）在处理交互决策任务中的复杂观察时遇到困难。为了解决这个问题，我们提出了一种简单的分层提示方法。不同于以往总是把\emph{完整}观察（例如网页）放到提示中的提示方法，我们提出首先构建一个与动作相关的\emph{压缩}和\emph{相关}的观察，并使用专门的\summ提示。然后，\actor提示根据总结的观察预测下一个动作。尽管我们的方法具有广泛的适用性，但我们尤其展示了它在复杂的网络导航领域的有效性，其中完整的观察通常包含冗余和无关信息。我们的方法在任务成功率上优于先前最先进的提示机制6.2\%，展示了其在具有长时间观察轨迹的交互决策任务中的潜力。

    Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the \emph{full} observation~(\eg a web page) to the prompt, we propose to first construct an action-aware observation which is more \emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actor prompt then predicts the next action based on the summarized observation. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanis by 6.2\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.
    
[^102]: 多语言大型语言模型尚无法进行代码切换

    Multilingual Large Language Models Are Not (Yet) Code-Switchers. (arXiv:2305.14235v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14235](http://arxiv.org/abs/2305.14235)

    多语言大型语言模型表现出了强大的性能，但是在代码切换的语境中，它们仍然表现不佳。

    

    多语言大型语言模型 (LLMs) 最近在各种任务中展示出了强大的能力，通过零-shot或少量-shot的提示方法展现出了最先进的性能。虽然已经有大量研究关于它们在单语任务中的能力，但是在代码切换 (CSW) 的语境中，即在一个话语中交替使用多种语言，它们的潜力的研究还相对较少。在本文中，我们对多个多语言LLM进行了全面的经验证实分析，将它们的性能进行了基准测试，包括情感分析、机器翻译、摘要和单词级语言识别等四个任务。我们的结果表明，尽管多语言LLMs在某些任务中通过零-shot或少量-shot的提示取得了有希望的效果，但与规模小得多的精调模型相比，它们仍然表现不佳。我们认为，目前LLMs中的"多语言能力"并不意味着具备代码切换的能力。

    Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current "multilingualism" in LLMs does not inherently imply proficiency with code-switching
    
[^103]: 为科学文献理解预训练多任务对比学习模型

    Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding. (arXiv:2305.14232v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14232](http://arxiv.org/abs/2305.14232)

    本论文提出了一个名为SciMult的多任务对比学习框架，旨在共享不同科学文献理解任务之间的通用知识，同时防止任务特定技能相互干扰。

    

    科学文献理解任务因其加速科学发现的潜力而受到关注。预训练语言模型通过对比学习在这些任务中显示出有效性。然而，跨多个异构任务共同利用预训练数据（例如，极限多标签论文分类、引文预测和文献搜索）仍然基本上未被探索。为弥合这一差距，我们提出了一个多任务对比学习框架SciMult，重点是促进不同科学文献理解任务之间的共享通用知识，同时防止任务特定技能相互干扰。具体来说，我们探索了两种技术-任务感知的特化和指令调整。前者采用了具有任务感知子层的多专家变压器架构；后者在输入文本之前添加了任务特定的指令以产生。

    Scientific literature understanding tasks have gained significant attention due to their potential to accelerate scientific discovery. Pre-trained language models (LMs) have shown effectiveness in these tasks, especially when tuned via contrastive learning. However, jointly utilizing pre-training data across multiple heterogeneous tasks (e.g., extreme multi-label paper classification, citation prediction, and literature search) remains largely unexplored. To bridge this gap, we propose a multi-task contrastive learning framework, SciMult, with a focus on facilitating common knowledge sharing across different scientific literature understanding tasks while preventing task-specific skills from interfering with each other. To be specific, we explore two techniques -task-aware specialization and instruction tuning. The former adopts a Mixture-of-Experts Transformer architecture with task-aware sub-layers; the latter prepends task-specific instructions to the input text so as to produce t
    
[^104]: CompoundPiece：评估和改进语言模型的复合分词性能

    CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models. (arXiv:2305.14214v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14214](http://arxiv.org/abs/2305.14214)

    本研究介绍了一个跨56种不同语言的大规模数据集，用于评估语言模型在复合分词任务上的表现。研究发现，现有的语言模型在复合分词上表现不佳，特别是对于通过子词分词方式进行标记的单词。因此，研究还提出了一种新的方法，通过训练专门的模型来改进复合分词性能。

    

    尽管许多语言都具有将两个或多个单词结合成复合词的过程，但以往的研究通常仅限于具有过度复合形成能力的语言（如德语、荷兰语），并且没有包含大量语言中复合和非复合词的公共数据集。在本研究中，我们系统地研究了广泛的复合分词任务，即将复合词拆分为其组成部分。我们首先通过介绍一个包含来自Wiktionary的255k个复合和非复合词的跨56种不同语言的数据集来解决数据缺口。然后，我们使用该数据集评估了一系列大型语言模型（LLMs）在复合分词任务上的表现。我们发现LLMs的性能较差，尤其是对于通过子词分词方式进行标记的单词。因此，我们引入了一种新的方法来训练专门用于复合分词的模型。所提出的两阶段方法依赖于完全自我监督的目标。

    While many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words across a large number of languages. In this work, we systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale. We first address the data gap by introducing a dataset of 255k compound and non-compound words across 56 diverse languages obtained from Wiktionary. We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization. We thus introduce a novel methodology to train dedicated models for decompounding. The proposed two-stage procedure relies on a fully self-supervised objective in the 
    
[^105]: CTQScorer: 结合多种特征进行上下文示例选择以用于机器翻译

    CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation. (arXiv:2305.14105v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14105](http://arxiv.org/abs/2305.14105)

    该论文提出了一个结合多种特征的通用框架，用于机器翻译中的上下文示例选择。通过学习一个回归模型，CTQ Scorer能够选择最优示例以提高翻译质量，并在多个语言对和语言模型上显著超过随机选择和单因素基线。

    

    当输入提示几个示例时（上下文学习），大型语言模型已经展示了在机器翻译上的能力。翻译质量取决于所选示例的各种特征，如其质量和相关性，但以往的工作主要集中在单个独立的特征上。在本文中，我们提出了一个通用框架，用于结合影响示例选择的不同特征。我们学习了一个回归模型CTQ Scorer（上下文翻译质量），它基于多种特征选择示例以最大化翻译质量。在多种语言对和语言模型上，我们证明CTQ Scorer显著超过了随机选择以及文献中报告的强单因素基线。相对于强BM25基线，我们还看到平均改善了2.5个COMET点。

    Large language models have demonstrated the capability to perform on machine translation when the input is prompted with a few examples (in-context learning). Translation quality depends on various features of the selected examples, such as their quality and relevance, but previous work has predominantly focused on individual features in isolation. In this paper, we propose a general framework for combining different features influencing example selection. We learn a regression model, CTQ Scorer (Contextual Translation Quality), that selects examples based on multiple features in order to maximize the translation quality. On multiple language pairs and language models, we show that CTQ Scorer helps significantly outperform random selection as well as strong single-factor baselines reported in the literature. We also see an improvement of over 2.5 COMET points on average with respect to a strong BM25 retrieval-based baseline.
    
[^106]: 迈向预训练大型语言模型中稀疏前馈网络的统一视角

    Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model. (arXiv:2305.13999v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13999](http://arxiv.org/abs/2305.13999)

    这项研究提出了一个统一的框架来分析稀疏前馈网络在预训练大型语言模型中的设计选择。在语言建模任务中，通过使用平均聚合隐藏状态的选择方法，相比现有的MoE架构，可以实现更低的困惑度。

    

    大型且稀疏的前馈层（S-FFN），如专家混合（MoE），已被证明在扩大Transformer模型规模以进行大型语言模型的预训练中非常有效。通过仅激活部分依赖于输入的FFN参数，S-FFN在保持训练和推理成本（以FLOPs计算）不变的同时提高了泛化性能。在本研究中，我们在稀疏神经记忆的通用概念框架下，分析了S-FFN的两个主要设计选择：内存块（即专家）大小和内存块选择方法。利用这个统一框架，我们比较了几种用于语言建模的S-FFN架构，并深入探讨了它们的相对效果和效率。我们发现一种更简单的选择方法 - Avg-K，通过均值聚合的隐藏状态来选择块，相比包括Switch Transformer（Fedus等，2021）和HashLaye在内的现有MoE架构，在语言模型预训练中实现了更低的困惑度。

    Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for \textit{pretraining} large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method -\textbf{\texttt{Avg-K}} that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLaye
    
[^107]: 使用轻量级的语言特定模块压缩多语言知识

    Condensing Multilingual Knowledge with Lightweight Language-Specific Modules. (arXiv:2305.13993v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13993](http://arxiv.org/abs/2305.13993)

    本文介绍了一种使用轻量级的语言特定模块来压缩多语言知识的方法，通过生成低秩矩阵来构建语言特定模块，并使用Fuse Distillation技术将多个语言特定模块中的知识压缩到一个共享模块中，提高了推理和模型序列化的效率。

    

    将语言特定（LS）模块纳入多语言机器翻译中是一种提升性能的有效方法。这种方法与专家混合（MoE）方法相似，因为它不会增加FLOPs。然而，由于全连接层中引入的全秩矩阵所带来的参数数量太多，这种方法在扩展到数百种语言（专家）时往往难以管理。在本文中，我们介绍了语言特定矩阵合成（LMS）方法。该方法通过从两个显著较小的矩阵生成低秩矩阵来构建LS模块以近似全秩矩阵。此外，我们使用Fuse Distillation（FD）技术将多个LS模块中的多语言知识压缩到一个共享模块中，以提高推理和模型序列化的效率。我们证明了我们的LMS方法在与同样数量的额外参数情况下显著优于之前的LS方法和MoE方法，例如1.73 BLE

    Incorporating language-specific (LS) modules is a proven method to boost performance in multilingual machine translation. This approach bears similarity to Mixture-of-Experts (MoE) because it does not inflate FLOPs. However, the scalability of this approach to hundreds of languages (experts) tends to be unmanageable due to the prohibitive number of parameters introduced by full-rank matrices in fully-connected layers. In this work, we introduce the Language-Specific Matrix Synthesis (LMS) method. This approach constructs LS modules by generating low-rank matrices from two significantly smaller matrices to approximate the full-rank matrix. Furthermore, we condense multilingual knowledge from multiple LS modules into a single shared module with the Fuse Distillation (FD) technique to improve the efficiency of inference and model serialization. We show that our LMS method significantly outperforms previous LS methods and MoE methods with the same amount of extra parameters, e.g., 1.73 BLE
    
[^108]: 基于语法约束的语言模型灵活解码技术

    Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])

    [http://arxiv.org/abs/2305.13971](http://arxiv.org/abs/2305.13971)

    本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。

    

    LLM在许多任务中展现出了惊人的少量样本表现，但在生成信息提取所需的复杂输出结构时仍存在困难。这个限制源于LLM在没有微调的情况下倾向于生成自由文本而不是遵循特定语法的精确结构。在本文中，我们提出在解码步骤中使用形式语法约束来丰富模型。在搜索过程中，只有符合语法产生规则的有效令牌能被考虑到。这样就强制只产生有效的序列。我们的框架非常通用和灵活，允许任何上下文无关语法(CFG)集成到我们的自定义约束beam搜索实现中。我们展示了许多NLP任务的输出可以被表示为形式语言，使它们适合在我们的框架中直接使用。对于输出空间取决于输入的任务，我们提出了基于输入的CFG，根据特定于输入的特征更新产生规则。实验证明了我们的方法在生成复杂输出结构方面的有效性，并在四个信息提取任务上实现了最先进的性能。

    LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
    
[^109]: 通过合成反馈对齐大型语言模型

    Aligning Large Language Models through Synthetic Feedback. (arXiv:2305.13735v1 [cs.CL])

    [http://arxiv.org/abs/2305.13735](http://arxiv.org/abs/2305.13735)

    该论文提出了一种使用合成反馈对齐大型语言模型的新框架，几乎不需要人力成本，也不依赖于预先对齐的LLMs。其中，通过对尺寸和提示等不同因素的普通 LLMS的响应进行奖励建模，来模拟高质量的示范来训练监督策略，并进一步使用强化学习优化模型。

    

    将大型语言模型(LLMs)与人类价值观对齐变得越来越重要，因为它能够提供复杂的LLMs控制，例如使它们按照特定的指令操作而不会产生有害反应。然而，这需要大量的人类示范和反馈。最近，开源模型试图通过提炼来自已对齐的LLMs（如InstructGPT或ChatGPT）的数据来复制对齐学习过程。虽然这个过程减少了人力成本，但是构建这些数据集对教师模型的依赖性很高。在这项工作中，我们提出了一个新的对齐学习框架，几乎不需要人类劳动，也不依赖于预先对齐的LLMs。首先，我们使用大小和提示等不同因素的普通LLMs的响应进行合成反馈的奖励建模(RM)。然后，我们使用RM模拟高质量的示范来训练监督策略，并进一步使用强化学习优化模型。

    Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs, e.g., making them follow given instructions while keeping them less toxic. However, it requires a significant amount of human demonstrations and feedback. Recently, open-sourced models have attempted to replicate the alignment learning process by distilling data from already aligned LLMs like InstructGPT or ChatGPT. While this process reduces human efforts, constructing these datasets has a heavy dependency on the teacher models. In this work, we propose a novel framework for alignment learning with almost no human labor and no dependency on pre-aligned LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM for simulating high-quality demonstrations to train a supervised policy and for further optimizing the model with reinforcement learning. Our 
    
[^110]: 基于实体的多模态网络内容图像搜索

    EDIS: Entity-Driven Image Search over Multimodal Web Content. (arXiv:2305.13631v1 [cs.CL])

    [http://arxiv.org/abs/2305.13631](http://arxiv.org/abs/2305.13631)

    这篇论文介绍了EDIS数据集，该数据集包括100万个多模态图像和文本配对，旨在鼓励开发实现跨模态信息融合和匹配的检索模型。

    

    为了在实际搜索应用中实现图像检索方法的实用性，需要在数据集规模、实体理解和多模态信息融合方面取得重大进展。

    Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce \textbf{E}ntity-\textbf{D}riven \textbf{I}mage \textbf{S}earch (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse tex
    
[^111]: 基于自我进化学习的 Mixup：增强少样本文本分类任务数据增强

    Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks. (arXiv:2305.13547v1 [cs.CL])

    [http://arxiv.org/abs/2305.13547](http://arxiv.org/abs/2305.13547)

    论文提出了一种基于自我进化学习的 Mixup 方法，用于文本分类的数据扩充，可以为模型训练生成更加适应和友好的伪样本，该方法可以降低模型的overconfidence。

    

    文本分类任务往往遇到有限标注数据的少样本场景，解决数据稀缺问题至关重要。使用 Mixup 进行数据扩充已经在各种文本分类任务中显示出有效性。然而，大多数 Mixup 方法并不考虑训练不同阶段的学习难度差异并产生带有 one hot 标签的新样本，导致模型过于自信。本文提出了一种基于自我进化学习（SE）的 Mixup 方法，用于文本分类的数据扩充，可以为模型训练生成更加适应和友好的伪样本。SE 关注模型的学习能力变化。为了减轻模型置信度，我们引入了一种新的实例标签平滑方法，该方法线性插值模型的输出和原始样本的 one-hot 标签，以生成新的软标签用于混合。通过实验分析，在提高分类准确率的同时，我们的方法可以降低模型的overconfidence。

    Text classification tasks often encounter few shot scenarios with limited labeled data, and addressing data scarcity is crucial. Data augmentation with mixup has shown to be effective on various text classification tasks. However, most of the mixup methods do not consider the varying degree of learning difficulty in different stages of training and generate new samples with one hot labels, resulting in the model over confidence. In this paper, we propose a self evolution learning (SE) based mixup approach for data augmentation in text classification, which can generate more adaptive and model friendly pesudo samples for the model training. SE focuses on the variation of the model's learning ability. To alleviate the model confidence, we introduce a novel instance specific label smoothing approach, which linearly interpolates the model's output and one hot labels of the original samples to generate new soft for label mixing up. Through experimental analysis, in addition to improving cla
    
[^112]: 自由生成文本的回溯解码算法

    Look-back Decoding for Open-Ended Text Generation. (arXiv:2305.13477v1 [cs.CL])

    [http://arxiv.org/abs/2305.13477](http://arxiv.org/abs/2305.13477)

    Look-back是一种自由生成文本的改进算法，可以自动预测可能的重复短语和主题偏移，并删除可能导致失败的令牌，生成的文本更流畅和连贯。

    

    给定一个前缀（上下文）时，自由生成就意味着解码出连贯的文本，不会突然偏离前面的主题和信息量多，不会出现不必要的重复。在本文中，我们提出了Look-back算法，一种改进的解码算法，它利用Kullback-Leibler散度来跟踪当前和历史解码步骤之间的分布距离。因此，Look-back可以自动预测可能的重复短语和主题偏移，并删除可能导致失败的令牌，将下一个令牌概率分布限制在可能的距离范围内，以便更好地符合历史。我们在文档延续和故事生成上执行解码实验，并证明Look-back能够生成更流畅和连贯的文本，在自动和人工评估中显著优于其他强的解码方法。

    Given a prefix (context), open-ended generation aims to decode texts that are coherent, which don't abruptly drift from previous topics, and informative, which don't suffer from undesired repetitions. In this paper, we propose Look-back, an improved decoding algorithm that leverages the Kullback-Leibler divergence to track the distribution distance between current and historical decoding steps. Thus Look-back can automatically predict potential repetitive phrase and topic drift, and remove tokens that may cause the failure modes, restricting the next token probability distribution within a plausible distance to the history. We perform decoding experiments on document continuation and story generation, and demonstrate that Look-back is able to generate more fluent and coherent text, outperforming other strong decoding methods significantly in both automatic and human evaluations.
    
[^113]: MAILEX: 邮件事件与参数抽取

    MAILEX: Email Event and Argument Extraction. (arXiv:2305.13469v1 [cs.CL])

    [http://arxiv.org/abs/2305.13469](http://arxiv.org/abs/2305.13469)

    本文提出了针对邮件领域的事件抽取数据集\dataset，比较了序列标记和生成式端到端抽取的方法，结果表明该任务存在非连续共享触发器跨度、非命名实体参数和邮件会话历史等难点，未来需要更多研究。

    

    本文提出了第一个数据集 \dataset，用于从邮件串中执行事件抽取。为此，我们首先提出了一个新的分类法，涵盖了邮件领域中的 10 种事件类型和 76 个参数。我们的最终数据集包括约 4K 封标记有约 9K 个事件实例的邮件。为了了解任务的挑战，我们进行了一系列实验，比较了两种常见的事件抽取方法，即序列标记和生成式端到端抽取（包括几率 GPT-3.5）。我们的结果表明，邮件事件抽取任务远未得到解决，因为存在诸多难点，例如提取非连续共享触发器跨度、提取非命名实体参数和建模邮件会话历史等。因此，我们的工作提出了未来在这个特定领域的事件抽取任务中需要进行更多研究的建议。

    In this work, we present the first dataset, \dataset, for performing event extraction from conversational email threads. To this end, we first proposed a new taxonomy covering 10 event types and 76 arguments in the email domain. Our final dataset includes $\sim$4K emails annotated with $\sim$9K event instances. To understand the task challenges, we conducted a series of experiments comparing two commonly-seen lines of approaches for event extraction, i.e., sequence labeling and generative end-to-end extraction (including few-shot GPT-3.5). Our results showed that the task of email event extraction is far from being addressed, due to challenges lying in, e.g., extracting non-continuous, shared trigger spans, extracting non-named entity arguments, and modeling the email conversational history. Our work thus suggests more investigations in this domain-specific event extraction task in the future.\footnote{The source code and dataset can be obtained from \url{https://github.com/salokr/Emai
    
[^114]: DADA: 基于语言规则的方言适应性动态聚合

    DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules. (arXiv:2305.13406v1 [cs.CL])

    [http://arxiv.org/abs/2305.13406](http://arxiv.org/abs/2305.13406)

    DADA是一种适用于多个方言，基于语言规则的动态聚合适配器，可为SAE训练的模型赋予多方言鲁棒性，同时针对特定方言变体进行适应，提供了一种可解释的方言适应性框架。

    

    现有的大型语言模型主要集中于标准美式英语（SAE），在应用于其他英语方言时表现往往较差。而现有的缓解方法针对单个目标方言的偏差，但假设了可以访问高精度的方言识别系统。方言之间的界限固有弹性，使得将语言划分为离散预定义的范畴更加困难。在本文中，我们提出了DADA（基于语言规则的方言适应性动态聚合），一种通过组合处理特定语言特征的适配器，为SAE训练的模型赋予多方言的鲁棒性的模块化方法。DADA的组合架构允许有针对性地适应特定方言变体，同时适应各种方言。我们展示了DADA对于单任务和指令微调语言模型都是有效的，提供了一种可扩展和可解释的框架来适应各种方言。

    Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic Aggregation), a modular approach to imbue SAE-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. The compositional architecture of DADA allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. We show that DADA is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting
    
[^115]: 提示不是大型语言模型中概率测量的替代方法

    Prompting is not a substitute for probability measurements in large language models. (arXiv:2305.13264v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13264](http://arxiv.org/abs/2305.13264)

    元语言提示与直接概率测量相比，对于衡量大型语言模型的语言知识来说，元语言判断效果较差，并且随着提示查询偏离直接测量的概率，一致性变差。提示的负面结果不能作为缺乏特定语言概括的确凿证据。从闭源API迁移中，我们也会失去一定的价值。

    

    提示现在是评估大型语言模型（LLM）语言知识的主要方法。而其他方法直接读取模型对字符串的概率分布，提示需要模型通过处理语言输入来访问这些内部信息，从而隐含地测试一种新型的紧急能力：元语言判断。在本研究中，我们比较元语言提示和直接概率测量作为衡量模型语言知识的方法。总体上，我们发现LLM的元语言判断不如直接从表示中派生的数量。此外，随着提示查询偏离直接测量下一个单词概率，一致性变差。我们的研究结果表明，依赖于元语言提示的负面结果不能作为LLM缺乏特定语言概括的确凿证据。我们的结果还突显了从闭源API迁移中所失去的价值。

    Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models' probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models' linguistic knowledge. Broadly, we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs 
    
[^116]: SCITAB: 一个对科学表格进行组合推理和事实验证的具有挑战性的基准测试

    SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables. (arXiv:2305.13186v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13186](http://arxiv.org/abs/2305.13186)

    SCITAB是一个具有挑战性的评估数据集，包含1.2K个经验证的科学事实和相关的科学表格，要求进行组合推理和事实验证。对于最先进的模型来说，SCITAB提出了许多独特挑战，包括表格定位、事实歧义和组合推理。所有模型中，除了GPT-4之外，性能仅略高于随机猜测。提示技术如思维链对于在SCITAB上提升性能几乎没有作用。

    

    当前的科学事实核查基准测试存在一些不足，例如来自众包审查的偏见和对基于文本的证据的过度依赖。我们介绍了SCITAB，一个具有挑战性的评估数据集，包含1.2K个经验证的科学事实，这些事实1）来源于真实的科学出版物，2）需要组合推理进行验证。这些事实与包含证据的科学表格进行配对，并进行了标记。通过广泛的评估，我们证明SCITAB对于最先进的模型，包括基于表格的预训练模型和大型语言模型，提出了重大挑战。除了GPT-4外，所有模型的性能仅略高于随机猜测。流行的提示技术，如思维链，对SCITAB的性能提升不大。我们的分析揭示了SCITAB提出的几个独特挑战，包括表格定位、事实的歧义性和组合推理。我们的代码和数据可供使用。

    Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are 
    
[^117]: 使用大型语言模型在相关语言之间进行机器翻译的分解提示法

    Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models. (arXiv:2305.13085v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13085](http://arxiv.org/abs/2305.13085)

    本研究提出了DecoMT，一种基于分解提示法的机器翻译方法，用于处理相关语言之间的翻译任务。通过利用相关语言的单调对齐特性，我们的方法在多个语系上进行了验证，并取得了良好的翻译效果。

    

    本研究探讨了相关语言之间的机器翻译，即在同一语系中具有词序和词汇相似性等语言特征的语言之间进行翻译。通过少量提示的机器翻译利用一小组翻译对示例来生成测试句子的翻译。这个过程要求模型学习如何生成翻译，同时确保令牌顺序保持一致，以产生流畅准确的翻译。我们提出，在相关语言中，机器翻译任务可以通过利用这种语言的单调对齐特性来简化。我们介绍了DecoMT，一种将翻译过程分解为一系列词块翻译的新颖提示方法。通过在多个相关语言对的不同语系上进行自动和人工评估，我们证明了我们提出的分解提示方法的有效性。

    This study investigates machine translation between related languages i.e., languages within the same family that share linguistic characteristics such as word order and lexical similarity. Machine translation through few-shot prompting leverages a small set of translation pair examples to generate translations for test sentences. This procedure requires the model to learn how to generate translations while simultaneously ensuring that token ordering is maintained to produce a fluent and accurate translation. We propose that for related languages, the task of machine translation can be simplified by leveraging the monotonic alignment characteristic of such languages. We introduce DecoMT, a novel approach of few-shot prompting that decomposes the translation process into a sequence of word chunk translations. Through automatic and human evaluation conducted on multiple related language pairs across various language families, we demonstrate that our proposed approach of decomposed prompt
    
[^118]: 超越标签：通过一种新的主动学习架构赋予人类标注者自然语言解释的能力

    Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture. (arXiv:2305.12710v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12710](http://arxiv.org/abs/2305.12710)

    本研究提出了一种新颖的主动学习架构，通过整合解释生成模型和预测模型，并采用数据多样性的采样策略，支持低资源场景下专家对标签和解释的注释需求。

    

    在现实世界中，领域专家（如医生）很少仅注释决策标签而不提供解释。然而，现有的低资源学习技术，如主动学习（AL），旨在支持人类标注者，却大多只关注标签而忽视数据点的自然语言解释。本研究提出了一种新颖的AL架构，以支持专家在低资源场景中对标签和解释注释的实际需求。我们的AL架构利用一个解释生成模型，根据人类解释生成解释，一个利用生成解释进行预测的预测模型，以及一个基于数据多样性的AL采样策略，从解释注释中受益。自动化和人工评估证明了将解释纳入AL采样的有效性，以及我们的AL架构改善了人工注释的效率和可信度。

    Real-world domain experts (e.g., doctors) rarely annotate only a decision label in their day-to-day workflow without providing explanations. Yet, existing low-resource learning techniques, such as Active Learning (AL), that aim to support human annotators mostly focus on the label while neglecting the natural language explanation of a data point. This work proposes a novel AL architecture to support experts' real-world need for label and explanation annotations in low-resource scenarios. Our AL architecture leverages an explanation-generation model to produce explanations guided by human explanations, a prediction model that utilizes generated explanations toward prediction faithfully, and a novel data diversity-based AL sampling strategy that benefits from the explanation annotations. Automated and human evaluations demonstrate the effectiveness of incorporating explanations into AL sampling and the improved human annotation efficiency and trustworthiness with our AL architecture. Add
    
[^119]: 基于自动调优语言模型的少样本分类方法

    Automated Few-shot Classification with Instruction-Finetuned Language Models. (arXiv:2305.12576v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12576](http://arxiv.org/abs/2305.12576)

    基于自动调优语言模型的少样本分类方法AuT-Few消除了手工制作提示的需求，并在多个数据集上超越了当前最先进的少样本学习方法。

    

    少样本学习的一类成功方法将语言模型与提示结合起来，这些提示是手工制作的任务描述，以补充数据样本。然而，为每个任务手工设计提示通常需要领域知识和大量猜测。我们观察到，在分类任务的背景下，经过指导调优的语言模型表现出了显著的提示鲁棒性，因此我们提出了一种名为AuT-Few的简单方法，以消除对手工制作提示的需求。该方法包括（i）一个从调优指令知识库中选择合适任务指令的指令检索模块，以及（ii）通过交叉验证生成两个不同的、语义有意义的类别描述和选择机制。在包括8个分类任务的12个数据集上，我们展示了AuT-Few优于当前最先进的少样本学习方法。此外，AuT-Few在数据集上是排名最高的方法。

    A particularly successful class of approaches for few-shot learning combines language models with prompts -- hand-crafted task descriptions that complement data samples. However, designing prompts by hand for each task commonly requires domain knowledge and substantial guesswork. We observe, in the context of classification tasks, that instruction finetuned language models exhibit remarkable prompt robustness, and we subsequently propose a simple method to eliminate the need for handcrafted prompts, named AuT-Few. This approach consists of (i) a prompt retrieval module that selects suitable task instructions from the instruction-tuning knowledge base, and (ii) the generation of two distinct, semantically meaningful, class descriptions and a selection mechanism via cross-validation. Over $12$ datasets, spanning $8$ classification tasks, we show that AuT-Few outperforms current state-of-the-art few-shot learning methods. Moreover, AuT-Few is the best ranking method across datasets on the
    
[^120]: 基于摘要描述的文本检索

    Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12517](http://arxiv.org/abs/2305.12517)

    本研究针对语义检索问题，提出了一种基于摘要描述的文本检索模型，通过改进当前的文本嵌入方法，在标准最近邻搜索中取得了显著性能提升。

    

    虽然针对文本的信息提取，指令优化的大型语言模型表现优异，但对于在大规模文档集合中定位符合给定描述的文本（语义检索）并不适用。基于嵌入向量的相似度搜索可以通过查询执行检索，但嵌入中的相似度定义不明确且不一致，并且对于许多用例来说都是次优的。那么，什么是有效检索的好的查询表示？我们确定了根据内容的摘要描述检索句子的明确定义且一致的任务。我们展示了当前文本嵌入的不足，并提出了一种替代模型，在标准最近邻搜索中的表现显著提升。该模型使用通过提示LLM获得的正负样本对进行训练。虽然很容易从LLM中获得训练材料，但LLM无法直接执行检索任务。

    While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
    
[^121]: 评估开放式问答评估

    Evaluating Open-QA Evaluation. (arXiv:2305.12421v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12421](http://arxiv.org/abs/2305.12421)

    本研究侧重于评估开放式问答（Open-QA）任务的方法，引入了一个新的任务QA-Eval和数据集EVOUNA，通过人工评估方法来评估AI生成的答案的准确性。我们调查了与人工评估相关的方法，并讨论了当前方法的缺陷和改进方法。我们相信这对于未来的自动评估工具发展和研究具有价值。

    

    本研究侧重于对开放式问答（Open-QA）任务的评估，该任务可以直接估计大型语言模型（LLMs）的事实性。目前的自动评估方法已显示出一定的局限性，表明人工评估仍然是最可靠的方法。我们引入了一个新的任务，即评估QA评估（QA-Eval）以及相应的数据集EVOUNA，旨在评估AI生成的答案与Open-QA中的标准答案之间的准确性。我们利用人工标注的结果来评估这些方法的性能。具体而言，本研究调查了那些与人工评估具有高度相关性的方法，认为它们更可靠。我们还讨论了当前方法的缺陷以及改进基于LLM的评估器的方法。我们相信，这个新的QA-Eval任务和相应的数据集EVOUNA将促进更有效的自动评估工具的开发，并对未来的研究具有价值。

    This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this a
    
[^122]: 用大语言模型重新审视自动主题模型评估

    Re-visiting Automated Topic Model Evaluation with Large Language Models. (arXiv:2305.12152v1 [cs.CL])

    [http://arxiv.org/abs/2305.12152](http://arxiv.org/abs/2305.12152)

    本文提出使用大型语言模型来评估主题模型输出，并使用自动化方法确定最佳主题数量。

    

    主题模型用于对大型文本集合进行分析。然而，自动评估主题模型输出和确定最佳主题数量一直是长期的挑战，目前尚无有效的自动化解决方案。本文提出使用大型语言模型来评估这种输出。我们发现，相比现有的自动度量标准，大型语言模型更能适当地评估出产生的主题，并且更能与人类判断相关。然后，我们研究能否利用大型语言模型自动确定最佳的主题数量。我们自动为文档分配标签，并选择具有最纯净标签的配置，以返回合理的最佳主题数量值。

    Topic models are used to make sense of large text collections. However, automatically evaluating topic model output and determining the optimal number of topics both have been longstanding challenges, with no effective automated solutions to date. This paper proposes using large language models to evaluate such output. We find that large language models appropriately assess the resulting topics, correlating more strongly with human judgments than existing automated metrics. We then investigate whether we can use large language models to automatically determine the optimal number of topics. We automatically assign labels to documents and choosing configurations with the most pure labels returns reasonable values for the optimal number of topics.
    
[^123]: 利用大语言模型预测编辑操作来减少序列长度

    Reducing Sequence Length by Predicting Edit Operations with Large Language Models. (arXiv:2305.11862v1 [cs.CL])

    [http://arxiv.org/abs/2305.11862](http://arxiv.org/abs/2305.11862)

    本文提出利用大型语言模型预测源文本中的编辑操作来减少序列长度的方法，从而减少计算成本，实验表明其达到了可比较的性能。

    

    大型语言模型在各种任务中展示出了卓越的性能，并获得了显着的关注。 LLMs也被用于本地序列转换任务，包括语法错误修正（GEC）和形式风格转换，在这些任务中，源文本中的大部分记号保持不变。 然而，生成所有目标记号是低效率的，因为目标记号的预测错误可能导致在预测后续记号时出现灾难，并且随着目标序列长度的增加，计算成本呈二次增长。 本文提出了一种预测本地序列转换任务的源文本的一组编辑操作的方法。 通过表示一个编辑操作的源文本跨度和更改的记号，我们可以减少目标序列的长度，从而减少推断的计算成本。 我们在编辑操作的监督数据上应用指导调优LLMs。 实验表明，所提出的方法达到了可比较的性能。

    Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, it is inefficient to generate all target tokens because a prediction error of a target token may cause a catastrophe in predicting subsequent tokens and because the computational cost grows quadratically with the target sequence length. This paper proposes to predict a set of edit operations for the source text for local sequence transduction tasks. Representing an edit operation with a span of the source text and changed tokens, we can reduce the length of the target sequence and thus the computational cost for inference. We apply instruction tuning for LLMs on the supervision data of edit operations. Experiments show that the proposed method achieves comparable performanc
    
[^124]: HELMA：大型语言模型的幻觉评估基准

    HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. (arXiv:2305.11747v1 [cs.CL])

    [http://arxiv.org/abs/2305.11747](http://arxiv.org/abs/2305.11747)

    本文介绍了一个大型语言模型幻觉评估基准（HELMA），其为标准化和可靠的估算模型幻觉问题提供了一种方法，并使用ChatGPT进行了实证研究以表明其存在幻觉的风险并为鉴别和减轻模型幻觉问题提供了一种方法。

    

    大型语言模型（LLM），如ChatGPT，容易生成幻觉，即与源内容冲突或无法通过事实知识进行验证的内容。为了了解LLMs会产生哪种类型的内容以及在多大程度上会产生幻觉，我们引入了Hallucination Evaluation for Large Language Models（HELMA）基准，这是一个包含大量生成的和人工注释的幻觉样本集，用于评估LLMs在识别和减轻幻觉方面的性能。为了生成这些样本，我们提出了一个基于ChatGPT的两步框架，即采样-过滤。具体来说，我们首先采用两种不同的采样方法基于指令生成幻觉样本，然后使用一个示例增强过滤方法选择最好的样本。此外，我们还聘请一些人工标注员来注释ChatGPT响应中的幻觉。经验证实，ChatGPT有一定的概率产生幻觉，并存在传播错误信息的潜在风险。我们提出的HELMA基准可作为识别和减轻LLMs幻觉问题的标准化可靠评估工具。

    Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, \ie content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HELMA) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing and alleviating hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, \ie sampling-then-filtering. Specifically, we first adopt two different sampling methods to generate hallucinated samples based on instructions, and then use an example-enhanced filtering method to select the best one. Furthermore, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT has some probabilities to generate hallucinations and exist
    
[^125]: 透过表征镜头看待多语言机器翻译中的知识转移

    Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens. (arXiv:2305.11550v1 [cs.CL])

    [http://arxiv.org/abs/2305.11550](http://arxiv.org/abs/2305.11550)

    该论文引入了表征转移潜力（RTP）来衡量多语言神经机器翻译中的知识转移，发现多路并行重叠是关键特征，提出了一种新的训练方案，鼓励表征在语言之间更具不变性，并在多个数据和模型设置中提高了低资源和中资源语言的翻译质量。

    

    我们认为，仅仅依靠翻译质量度量多语言神经机器翻译中的知识转移是不够的。为了支持这一观点，我们引入了表征转移潜力（RTP），它测量语言之间的表征相似度。我们展示了RTP可以测量正向和负向转移（干扰），并发现RTP与翻译质量变化强相关，表明确实存在转移。此外，我们研究了与转移相关的数据和语言特征，并发现多路并行重叠是一个重要但鲜有探索的特征。基于此，我们开发了一种新的训练方案，该方案使用辅助相似性损失，通过利用多路并行数据，鼓励表征在语言之间更具不变性。我们表明，我们的方法在多个数据和模型设置中提高了低资源和中资源语言的翻译质量。

    We argue that translation quality alone is not a sufficient metric for measuring knowledge transfer in multilingual neural machine translation. To support this claim, we introduce Representational Transfer Potential (RTP), which measures representational similarities between languages. We show that RTP can measure both positive and negative transfer (interference), and find that RTP is strongly correlated with changes in translation quality, indicating that transfer does occur. Furthermore, we investigate data and language characteristics that are relevant for transfer, and find that multi-parallel overlap is an important yet under-explored feature. Based on this, we develop a novel training scheme, which uses an auxiliary similarity loss that encourages representations to be more invariant across languages by taking advantage of multi-parallel data. We show that our method yields increased translation quality for low- and mid-resource languages across multiple data and model setups.
    
[^126]: Ditto: 一种改进句子嵌入的简洁有效方法

    Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings. (arXiv:2305.10786v1 [cs.CL])

    [http://arxiv.org/abs/2305.10786](http://arxiv.org/abs/2305.10786)

    提出了一种名为Ditto的简单有效的方法，其可以解决预训练语言模型中存在的各向异性问题，并在语义文本相似性任务中提高模型的性能。

    

    先前的研究诊断了由预训练语言模型（如BERT）产生的句子表示中存在的各向异性问题，没有进行微调。我们的分析揭示了BERT产生的句子嵌入存在偏向于非信息性单词的偏见，限制了在语义文本相似性（STS）任务中的性能。为了解决这种偏见问题，我们提出了一个简单有效的无监督方法——对角线注意力池化（Ditto），该方法使用基于模型的重要性估计权重单词，并计算预训练模型的单词表示的加权平均值作为句子嵌入。Ditto可以轻松地应用于任何预训练语言模型作为后处理操作。与先前的句子嵌入方法相比，Ditto不添加参数也不需要任何学习。实证评估表明，我们提出的Ditto可以缓解各向异性问题并提高在STS任务中各种预训练模型的性能。

    Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks. To address this bias, we propose a simple and efficient unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words with model-based importance estimations and computes the weighted average of word representations from pre-trained models as sentence embeddings. Ditto can be easily applied to any pre-trained language model as a postprocessing operation. Compared to prior sentence embedding approaches, Ditto does not add parameters nor requires any learning. Empirical evaluations demonstrate that our proposed Ditto can alleviate the anisotropy problem and improve various pre-trained models on STS tasks.
    
[^127]: 平坦度感知的Prompt选择能提高精度和样本效率

    Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency. (arXiv:2305.10713v1 [cs.CL])

    [http://arxiv.org/abs/2305.10713](http://arxiv.org/abs/2305.10713)

    本论文提出了一种新的度量--Prompt平坦度，可以优化语言提示选择，提高模型分类的准确性和样本效率，实验证明结合现有度量可以提高性能和样本效率。

    

    随着大型语言模型的能力不断增长，提示已成为访问它们的主要方式。这激发了自动选择有效语言提示策略的发展。本文介绍Prompt平坦度，一种量化语言提示预期效用的新度量。该度量受统计学习中的平坦度正则化启发，量化模型对其参数扰动的稳健性。我们提供该度量的理论基础及其与其他Prompt选择度量的关系，从而全面了解现有方法。从经验上讲，我们表明，将Prompt平坦度与现有度量结合使用可以提高性能和样本效率。在6个分类基准测试中，我们的度量优于以前的Prompt选择度量，平均精度提高5％，Pearson相关性提高10％。

    With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce prompt flatness, a new metric to quantify the expected utility of a language prompt. This metric is inspired by flatness regularization in statistical learning that quantifies the robustness of the model towards its parameter perturbations. We provide theoretical foundations for this metric and its relationship with other prompt selection metrics, providing a comprehensive understanding of existing methods. Empirically, we show that combining prompt flatness with existing metrics improves both performance and sample efficiency. Our metric outperforms the previous prompt selection metrics with an average increase of 5% in accuracy and 10% in Pearson correlation across 6 classification benchmarks.
    
[^128]: 利用大型视觉语言模型学习文本的视觉性

    Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v1 [cs.CL])

    [http://arxiv.org/abs/2305.10434](http://arxiv.org/abs/2305.10434)

    该论文利用大型视觉语言模型如CLIP来检测文本的视觉性，并提出fine-tuning策略，将非视觉文本映射为NULL图像，匹配视觉文本与对应图像，以解锁在文本中嵌入相关图像的能力。

    

    视觉文本会在人们的脑海中呈现图像，而非视觉文本则无法达到此效果。自动检测文本的视觉性将有助于在文本中嵌入相关图像。我们创建了一个数据集，包括3620个英语句子及其多个人类注释者提供的视觉性得分，并使用包含文本和视觉资产的文档来创建远程监督语料库，以评估文本的视觉性。

    Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will unlock the ability to augment text with relevant images, as neural text-to-image generation and retrieval models operate on the implicit assumption that the input text is visual in nature. We curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. Additionally, we use documents that contain text and visual assets to create a distantly supervised corpus of document text and associated images. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP that assume a one-to-one correspondence between text and image to the task of scoring text visualness from text input alone. Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images 
    
[^129]: 基于知识增强的生成预训练模型在中国医学执业医师资格考试上的应用研究

    Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])

    [http://arxiv.org/abs/2305.10163](http://arxiv.org/abs/2305.10163)

    本研究通过在ChatGPT中集成医学领域知识和启用少样本学习的新方法，在中国国家医学执业医师资格考试中取得成功，这为建立在自然语言处理技术和医学领域知识的创新应用提供了可能。

    

    生成式预训练模型（GPT），如ChatGPT，在各种自然语言处理任务中展现出了出色的性能。尽管ChatGPT已被整合到各个领域的工作流中以提高效率，但其微调过程的灵活性不足，阻碍了其在需要广泛领域专业知识和语义知识的领域，如医疗保健，的应用。在本文中，我们评估了ChatGPT在中国国家医学执业医师资格考试（CNMLE）中的表现，并提出了一种新的方法来改进ChatGPT，即从两个方面集成医学领域知识和启用少样本学习。通过使用简单但有效的检索方法，将医学背景知识提取为语义指令来指导ChatGPT的推断。类似地，相关的医疗问题被识别并作为演示输入给ChatGPT。实验结果表明，直接应用ChatGPT无法在CNMLE上获得合格分数（51分），只有基于知识增强训练的模型成功通过考试。

    Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., onl
    
[^130]: 自监督的句子压缩用于会议摘要

    Self-Supervised Sentence Compression for Meeting Summarization. (arXiv:2305.07988v1 [cs.CL])

    [http://arxiv.org/abs/2305.07988](http://arxiv.org/abs/2305.07988)

    本论文提出了一种自监督学习的会议摘要框架SVB，通过三个过程压缩冗余内容并保留关键信息。其中，滑动窗口对话恢复与评分、通道重要性得分投票和相对位置分箱等算法用于实现这个框架。

    

    传统总结模型往往无法捕捉到会议记录中的关键信息，因为会议语料库通常涉及多个参与者的冗长对话并充斥着冗余和琐碎的内容。为了解决这个问题，我们提出了SVB，一个有效且高效的会议摘要框架，通过三个过程“压缩”冗余而保留重要内容：滑动窗口对话恢复与评分、通道重要性得分投票和相对位置分箱。具体来说，在自监督范式下，滑动窗口评分旨在从多个视角评估每个标记的重要性。然后通过通道投票聚合这些评分。具有高评分的标记将被视为显着信息并标记为“anchers”。最后，为了使输入长度适合语言模型的长度限制，采用了相对位置分箱算法。

    The conventional summarization model often fails to capture critical information in meeting transcripts, as meeting corpus usually involves multiple parties with lengthy conversations and is stuffed with redundant and trivial content. To tackle this problem, we present SVB, an effective and efficient framework for meeting summarization that `compress' the redundancy while preserving important content via three processes: sliding-window dialogue restoration and \textbf{S}coring, channel-wise importance score \textbf{V}oting, and relative positional \textbf{B}ucketing. Specifically, under the self-supervised paradigm, the sliding-window scoring aims to rate the importance of each token from multiple views. Then these ratings are aggregated by channel-wise voting. Tokens with high ratings will be regarded as salient information and labeled as \textit{anchors}. Finally, to tailor the lengthy input to an acceptable length for the language model, the relative positional bucketing algorithm i
    
[^131]: ZARA：改进小型语言模型的少样本自理性

    ZARA: Improving Few-Shot Self-Rationalization for Small Language Models. (arXiv:2305.07355v1 [cs.CL])

    [http://arxiv.org/abs/2305.07355](http://arxiv.org/abs/2305.07355)

    本文提出了一种名为ZARA的方法，其可以通过将合理性判断问题转化为自然语言推理来自动构建伪平行数据进行自我训练，从而提高小型语言模型的少样本自我解释性能，实验结果表明ZARA在任务准确性和解释质量上都表现出SOTA水平。

    

    同时生成终端任务答案和自由文本解释的语言模型被称为自我解释模型。最近的研究通过使用有理据的例子来提示语言模型，展现了少样本自我解释性能显著提高的成果。然而，只有大规模语言模型才能受益于解释，而这些模型很难被获得。本文研究利用解释来提高少样本自我解释对小型语言模型的影响。我们首先重新探讨了解释和答案之间的关系。受到人类如何评估解释的隐含思考过程的启发，我们提出了一种新的方法ZARA，即理性答案对的零样本增强，通过将合理性判断问题转化为自然语言推理来自动构建伪平行数据进行自我训练。实验结果表明，在FEB基准测试中，ZARA在任务准确性和解释质量上都取得了SOTA的表现。

    Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Recent works demonstrate great performance gain for self-rationalization by few-shot prompting LMs with rationale-augmented exemplars. However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility. In this work, we explore the less-studied setting of leveraging explanations for small LMs to improve few-shot self-rationalization. We first revisit the relationship between rationales and answers. Inspired by the implicit mental process of how human beings assess explanations, we present a novel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to automatically construct pseudo-parallel data for self-training by reducing the problem of plausibility judgement to natural language inference. Experimental results show ZARA achieves SOTA performance on the FEB benchmark, for both the task accu
    
[^132]: LLM 不同语言的性能表现不一: 通过跨语言思维提示提高多语言能力

    Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting. (arXiv:2305.07004v1 [cs.CL])

    [http://arxiv.org/abs/2305.07004](http://arxiv.org/abs/2305.07004)

    该论文介绍了一种跨语言思维提示方法，名为XLT，用于提高LLMs的多语言能力。该方法能够显著提高各种多语言任务性能，并减少不同语言中任务性能的差距。

    

    大型语言模型(LLMs)展示了令人印象深刻的多语言能力，但它们的性能在不同语言之间具有显著差异。在这项工作中，我们介绍了一种简单但有效的方法，称为跨语言思维提示(XLT)，以系统地提高LLMs的多语言能力。具体而言，XLT是一个通用的模板提示，可以刺激跨语言和逻辑推理技能，以增强不同语言的任务性能。我们在涵盖高资源和低资源语言的7个典型推理、理解和生成任务的全面评估上进行了全面的评估。实验结果表明，XLT不仅显著提高了各种多语言任务的性能，而且还显著减少了不同语言中每个任务平均性能和最佳性能之间的差距。值得注意的是，XLT在算术推理和开放域问答方面带来了超过10个百分点的平均改进。

    Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages. We conduct comprehensive evaluations on 7 typical benchmarks related to reasoning, understanding, and generation tasks, covering both high-resource and low-resource languages. Experimental results show that XLT not only remarkably enhances the performance of various multilingual tasks but also significantly reduces the gap between the average performance and the best performance of each task in different languages. Notably, XLT brings over 10 points of average improvement in arithmetic reasoning and open-domain question-answeri
    
[^133]: 主动检索增强生成

    Active Retrieval Augmented Generation. (arXiv:2305.06983v1 [cs.CL])

    [http://arxiv.org/abs/2305.06983](http://arxiv.org/abs/2305.06983)

    本论文提出了一种主动检索增强生成的方法，与以往的方法相比，它在生成过程中更紧密地集成了主动检索和生成，并展示了在一组句子生成任务中的性能优势。

    

    尽管大型语言模型（LM）具有理解和生成语言的卓越能力，它们往往会产生虚假的和错误的输出。从外部知识资源中检索信息来增强LM是一种有前途的解决方案。大多数现有的检索增强的LM采用一种检索和生成的设置，仅基于输入一次检索信息。然而，在涉及生成长文本的更普遍的场景中，通过在生成过程中不断地收集信息是至关重要的。过去有一些检索信息，同时生成输出的努力，大多数都是使用前一个上下文作为查询，在固定的时间间隔内检索文档。在这项工作中，我们提供了一个主动检索增强生成的广义视图，即在整个生成过程中主动决定何时以及何时从哪里检索信息的方法。我们提出了前瞻性主动检索增强生成（FLARE），它通过允许生成器在每个步骤中主动查询检索组件来更紧密地集成主动检索和生成。FLARE在一组句子生成任务中优于以前的方法，展示了主动检索在整个生成过程中的好处。

    Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval-augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout the generation process is essential. There have been some past efforts to retrieve information multiple times while generating outputs, which mostly retrieve documents at fixed intervals using the previous context as queries. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval aug
    
[^134]: 超越善意：NLP用于社会公益的研究现状报告

    Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good. (arXiv:2305.05471v1 [cs.CL])

    [http://arxiv.org/abs/2305.05471](http://arxiv.org/abs/2305.05471)

    本文介绍了NLP4SGPAPERS数据集，通过对解决社会问题的论文进行分类、可持续发展目标映射、任务及方法的确定，使用最先进的NLP模型在整个ACL文集上进行处理，提供了一个可视化工作区，展示了NLP4SG领域的全貌。

    

    随着自然语言处理(NLP)的最新进展，越来越多的应用程序出现在各种用例中。在众多的NLP应用中，许多学术研究人员受到激励，希望通过工作具有积极的社会影响，符合NLP for Social Good (NLP4SG)的最新倡议。然而，研究人员并不总是清楚地了解自己的研究工作如何解决当今的重大社会问题。因此，在本文中，我们介绍NLP4SGPAPERS，这是一个具有三个相关任务的科学数据集，可以帮助识别NLP4SG论文，并通过以下几个方面对NLP4SG进行描述: (1)确定解决社会问题的论文，(2)将它们映射到相应的联合国可持续发展目标(SDGs)，以及(3)识别它们正在解决的任务和使用的方法。我们使用最先进的NLP模型，解决了每个任务，并将它们用于整个ACL文集，从而产生一个可视化工作区，为研究人员提供了对NLP4SG领域的鸟瞰图。

    With the recent advances in natural language processing (NLP), a vast number of applications have emerged across various use cases. Among the plethora of NLP applications, many academic researchers are motivated to do work that has a positive social impact, in line with the recent initiatives of NLP for Social Good (NLP4SG). However, it is not always obvious to researchers how their research efforts are tackling today's big social problems. Thus, in this paper, we introduce NLP4SGPAPERS, a scientific dataset with three associated tasks that can help identify NLP4SG papers and characterize the NLP4SG landscape by: (1) identifying the papers that address a social problem, (2) mapping them to the corresponding UN Sustainable Development Goals (SDGs), and (3) identifying the task they are solving and the methods they are using. Using state-of-the-art NLP models, we address each of these tasks and use them on the entire ACL Anthology, resulting in a visualization workspace that gives resear
    
[^135]: 专业性文本生成：模仿、检索、改写

    Expository Text Generation: Imitate, Retrieve, Paraphrase. (arXiv:2305.03276v1 [cs.CL])

    [http://arxiv.org/abs/2305.03276](http://arxiv.org/abs/2305.03276)

    该论文介绍了一项名为“专业性文本生成”的任务，目的在于从知识来源自动生成准确、信息丰富的文档，研究者提出的IRP框架能够克服语言模型的缺点，实现了内容规划、事实选择 和改写步骤分别处理，使得生成的文档具有较高的质量。

    

    专业性文档是向读者传达复杂信息的重要资源。尽管它们很有用，但手动编写专业性文献是耗时且劳动强度大的过程，需要对所涉领域有深入了解、精心规划内容、以及能够从多个来源综合信息的能力。为了减轻负担，我们介绍了专业性文本生成任务，它旨在从知识来源自动生成准确、信息丰富的专业性文档。我们通过开发IRP解决了这个任务，IRP是一个迭代框架，克服了语言模型的局限，分别处理了内容规划、事实选择和改写的步骤。通过对三个不同的数据集进行实验，我们证明IRP可以生成高质量的专业性文档，准确地向读者传递信息。

    Expository documents are vital resources for conveying complex information to readers. Despite their usefulness, writing expository documents by hand is a time-consuming and labor-intensive process that requires knowledge of the domain of interest, careful content planning, and the ability to synthesize information from multiple sources. To ease these burdens, we introduce the task of expository text generation, which seeks to automatically generate an accurate and informative expository document from a knowledge source. We solve our task by developing IRP, an iterative framework that overcomes the limitations of language models and separately tackles the steps of content planning, fact selection, and rephrasing. Through experiments on three diverse datasets, we demonstrate that IRP produces high-quality expository documents that accurately inform readers.
    
[^136]: 带有交叉编码器的CUR k-NN搜索的自适应锚定项选择

    Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders. (arXiv:2305.02996v1 [cs.IR])

    [http://arxiv.org/abs/2305.02996](http://arxiv.org/abs/2305.02996)

    本文提出了一种自适应锚点选择方法，可以在保持较小的计算成本的同时，实现与随机抽样锚点相当或者更好的k-NN召回性能。

    

    本文提出了一种自适应锚点选择方法，以改善ANNCUR模型中高前k项的逼近误差和召回率。该方法可以在保持较小的计算成本的同时，实现与随机抽样锚点相当或者更好的k-NN召回性能。

    Cross-encoder models, which jointly encode and score a query-item pair, are typically prohibitively expensive for k-nearest neighbor search. Consequently, k-NN search is performed not with a cross-encoder, but with a heuristic retrieve (e.g., using BM25 or dual-encoder) and re-rank approach. Recent work proposes ANNCUR (Yadav et al., 2022) which uses CUR matrix factorization to produce an embedding space for efficient vector-based search that directly approximates the cross-encoder without the need for dual-encoders. ANNCUR defines this shared query-item embedding space by scoring the test query against anchor items which are sampled uniformly at random. While this minimizes average approximation error over all items, unsuitably high approximation error on top-k items remains and leads to poor recall of top-k (and especially top-1) items. Increasing the number of anchor items is a straightforward way of improving the approximation error and hence k-NN recall of ANNCUR but at the cost o
    
[^137]: 标注描述训练在零样本文本分类中的好处

    The Benefits of Label-Description Training for Zero-Shot Text Classification. (arXiv:2305.02239v1 [cs.CL])

    [http://arxiv.org/abs/2305.02239](http://arxiv.org/abs/2305.02239)

    本文提出了标注描述训练的方法，在零样本分类中可以显著提高准确率，并能更鲁棒地处理分类任务。

    

    大型语言模型通过允许从训练数据中转移语义知识，提高了零样本文本分类的性能，本文提出了一种简单的方法，进一步提高零样本准确性。我们策划了一个小的微调数据集，旨在描述任务标签。与通常有文本标注标签的微调数据不同，我们的数据只是用语言描述标签，例如使用一些相关术语、词典/百科全书条目和短模板。我们的方法在各种主题和情感数据集上的准确性比零样本高15-17％绝对值。它还更具有零样本分类所需选择的鲁棒性，例如提示模型进行分类的模式以及从标签映射到模型词汇表中的令牌。此外，由于我们的数据仅描述标签但不使用输入文本，因此在其上微调的模型可以将分类的重点更专注于标签而不是文本。

    Large language models have improved zero-shot text classification by allowing the transfer of semantic knowledge from the training data in order to classify among specific label sets in downstream tasks. We propose a simple way to further improve zero-shot accuracies with minimal effort. We curate small finetuning datasets intended to describe the labels for a task. Unlike typical finetuning data, which has texts annotated with labels, our data simply describes the labels in language, e.g., using a few related terms, dictionary/encyclopedia entries, and short templates. Across a range of topic and sentiment datasets, our method is more accurate than zero-shot by 15-17% absolute. It is also more robust to choices required for zero-shot classification, such as patterns for prompting the model to classify and mappings from labels to tokens in the model's vocabulary. Furthermore, since our data merely describes the labels but does not use input texts, finetuning on it yields a model that p
    
[^138]: 向参数效率迈进：具有动态能力的分层稀疏激活Transformer

    Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity. (arXiv:2305.02176v1 [cs.CL])

    [http://arxiv.org/abs/2305.02176](http://arxiv.org/abs/2305.02176)

    本篇论文提出了一种新的分层稀疏激活Transformer模型，可以动态分配不同令牌的能力，解决了传统Mixture-of-experts模型参数低效的问题，实验表明该模型在多语言机器翻译任务中取得了较好效果。

    

    采用稀疏激活的Mixture-of-experts（MoE）模型已经证明在保持低每个令牌计算要求的同时显着增加参数的有效性。然而，最近的研究表明，MoE模型本质上是参数低效的，因为随着专家数量增加，性能的提高会变小。我们假设这种参数低效是所有专家具有相同能力的结果，这可能无法充分满足不同令牌或任务的不同复杂度要求，例如在多语言环境下，基于其资源水平的语言可能需要不同的能力。因此，我们提出了具有分层结构并可以为不同令牌分配动态能力的Stratified Mixture of Experts（SMoE）模型。我们证明SMoE的有效性，该模型在两个多语言机器翻译基准测试中表现优于多个最先进的MoE模型。

    Mixture-of-experts (MoE) models that employ sparse activation have demonstrated effectiveness in significantly increasing the number of parameters while maintaining low computational requirements per token. However, recent studies have established that MoE models are inherently parameter-inefficient as the improvement in performance diminishes with an increasing number of experts. We hypothesize this parameter inefficiency is a result of all experts having equal capacity, which may not adequately meet the varying complexity requirements of different tokens or tasks, e.g., in a multilingual setting, languages based on their resource levels might require different capacities. In light of this, we propose Stratified Mixture of Experts(SMoE) models, which feature a stratified structure and can assign dynamic capacity to different tokens. We demonstrate the effectiveness of SMoE on two multilingual machine translation benchmarks, where it outperforms multiple state-of-the-art MoE models. On
    
[^139]: GPT-RE: 利用大型语言模型进行上下文学习的关系抽取研究

    GPT-RE: In-context Learning for Relation Extraction using Large Language Models. (arXiv:2305.02105v1 [cs.CL])

    [http://arxiv.org/abs/2305.02105](http://arxiv.org/abs/2305.02105)

    本文提出了GPT-RE，通过特定于任务的实体表示和使用金标签诱导的推理逻辑丰富演示，成功解决了大型语言模型在关系抽取方面的低相关性和倾向于错误分类的问题，成果在多个数据集上均超过现有基准，其中在三个数据集中的结果达到了最新研究成果。

    

    尽管大型语言模型（例如GPT-3）有可能取得突破性成就，但它们在关系抽取方面仍远远落后于完全监督的基准（例如fine-tuned BERT）。  这是由于LLMs在RE方面存在两个主要缺点:(1)在上下文学习中，检索到的演示中实体和关系的相关性较低; (2)强烈倾向于错误地将NULL示例分类为其他预定义的标签。  本文提出了GPT-RE来弥合LLMs和完全监督的基准之间的差距。 GPT-RE通过（1）在演示检索中加入特定于任务的实体表示; （2）使用金标签诱导的推理逻辑丰富演示来成功解决上述问题。 我们在四个广泛使用的RE数据集上评估了GPT-RE，并观察到GPT-RE不仅改善了现有的GPT-3基准，而且改善了完全监督的基准。 具体而言，GPT-RE在四个数据集中的三个上都取得了最新的研究成果，证明了其在上下文学习关系抽取方面的有效性。

    In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE: (1) low relevance regarding entity and relation in retrieved demonstrations for in-context learning; and (2) the strong inclination to wrongly classify NULL examples into other pre-defined labels.  In this paper, we propose GPT-RE to bridge the gap between LLMs and fully-supervised baselines. GPT-RE successfully addresses the aforementioned issues by (1) incorporating task-specific entity representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE datasets, and observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE ach
    
[^140]: 旨在总结带有层次关系的多篇文档

    Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])

    [http://arxiv.org/abs/2305.01498](http://arxiv.org/abs/2305.01498)

    提出了一个新的数据集PeerSum用于生成科学论文的元评论，源文档具有显式层次结构的丰富文档间关系，提出了一种用于元评论生成的关系感知多任务模型Rammer。

    

    多数现存的多文档摘要(MDS)数据集缺少人工生成的、真实的(即非合成的)摘要或者带有显式文档间关系的源文档。为了增强MDS系统的能力，我们提出PeerSum，这是一个新颖的数据集，用于生成科学论文的元评论，其中元评论是对评论和相应讨论的高度概括且真实的摘要。这些源文档具有显式层次结构的丰富文档间关系，包括交叉引用和经常出现的冲突。鉴于很少有研究采用基于预训练语言模型的注意力操纵来将层次关系纳入MDS系统中，我们还提出了Rammer(关系感知多任务元评论生成器)，这是一种元评论生成模型，使用基于层次关系的稀疏注意力和多任务目标，可以预测多个度量值。

    Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
    
[^141]: 触发词作为后门攻击的触发器：检查语言模型的脆弱性

    Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])

    [http://arxiv.org/abs/2305.01219](http://arxiv.org/abs/2305.01219)

    本研究提出一种新颖有效的“ProAttack”方法来执行干净标签的后门攻击，使用的是提示本身作为触发器。该方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性，相比于现有的后门攻击方法有显著提升。

    

    基于提示的学习范例弥合了预训练和微调之间的差距，在几个NLP任务中取得了最先进的性能，尤其是在少样本情况下。尽管应用广泛，但基于提示的学习容易受到后门攻击。文本后门攻击旨在通过注入触发器并修改标签来在模型中引入有针对性的漏洞。然而，由于触发器的存在和毒瘤数据标注不正确等缺陷，这种攻击存在异常的自然语言表达。在本研究中，我们提出了一种新颖有效的“ProAttack”方法，基于提示来执行干净标签的后门攻击，使用的是提示本身作为触发器。我们的方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性。通过在丰富的资源和少样本文本语料库上的广泛实验，我们证明了ProAttack方法在保持干净数据一致性的同时显著优于现有的后门攻击方式。

    The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
    
[^142]: 聊天GPT/GPT-4已知的图书的考古学研究

    Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4. (arXiv:2305.00118v1 [cs.CL])

    [http://arxiv.org/abs/2305.00118](http://arxiv.org/abs/2305.00118)

    通过名字填空成员推断查询，该研究考古了ChatGPT和GPT-4已知的图书，发现这些模型已经记忆了大量受版权保护的材料，这支持了一个使用已知训练数据的开放模型案例。

    

    本研究使用名字填空成员推断查询，对ChatGPT和GPT-4已知的图书进行数据考古学推断，发现OpenAI模型已经记忆了大量受版权保护的材料，并且记忆的程度与这些书籍在网上出现的频率有关。这些模型记忆未知的书籍的能力使得文化分析的测量有效性评估变得复杂，因为它会污染测试数据。我们展示了这些模型在记忆的书籍上的表现远远优于未记忆的书籍在下游任务中的表现，这支持了一个使用已知训练数据的开放模型案例。

    In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query. We find that OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with which passages of those books appear on the web. The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks. We argue that this supports a case for open models whose training data is known.
    
[^143]: 基于Transformer的语言模型Surprisal最佳的预测人类阅读时间的训练令牌数约为20亿

    Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens. (arXiv:2304.11389v1 [cs.CL])

    [http://arxiv.org/abs/2304.11389](http://arxiv.org/abs/2304.11389)

    本文研究表明，基于Transformer的语言模型在观察约20亿个训练令牌后能够提供最佳拟合，其surprisal估计能力能够最好地预测人类阅读时间。在语言模型收敛时，较小的模型变体会出现“临界点”，使语言模型困惑度下降，从而导致较差的人类拟合度。

    

    最近的心理语言学研究对语言模型的质量与其surprisal估计能力预测人类阅读时间的关系得出了相互矛盾的结论，这可能是由于研究中的训练数据量和模型容量的巨大差距所致。本文旨在通过评估基于Transformer的语言模型变体的Surprisal估计能力，这些变体在训练数据量和模型容量方面有系统变化，以整合这些发现。结果表明，大多数具有现代模型容量的变体的Surprisal估计在观察约20亿个训练令牌后提供最佳拟合，此后它们开始偏离与人类期望相符的界限。此外，新训练的较小模型变体在收敛时显示出“临界点”，在此之后，语言模型困惑度的下降开始导致较差的人类拟合度。

    Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a 'tipping point' at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human
    
[^144]: Outlier Suppression+：通过等效和最优移位和缩放来准确量化大型语言模型

    Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. (arXiv:2304.09145v1 [cs.CL])

    [http://arxiv.org/abs/2304.09145](http://arxiv.org/abs/2304.09145)

    通过Outlier Suppression+框架的通道级移位和缩放操作，分析得到最优移位和缩放值，成功解决了量化Transformer语言模型中存在的不对称离群值问题，实现了接近浮点性能的结果。

    

    对Transformer语言模型进行量化面临着存在损害性离群值的重要难题。我们观察到这些离群值是不对称的并且集中在特定通道中。为了解决这个问题，我们提出了Outlier Suppression+框架。首先，我们引入了通道级别的移位和缩放操作来消除不对称表示并缩小有问题的通道。我们证明了这些操作可以无缝地迁移至后续模块而保持等效性。其次，我们量化分析了最优的移位和缩放值，考虑到下一层权重的不对称特性和量化误差。我们轻量级的框架可以在静态和标准的训练后量化设置下造成最小的性能降低。各种任务和模型的全面结果表明，我们的方法在小型模型和大型模型如GPT-2方面实现了接近浮点性能。

    Quantization of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are asymmetric and concentrated in specific channels. To address this issue, we propose the Outlier Suppression+ framework. First, we introduce channel-wise shifting and scaling operations to eliminate asymmetric presentation and scale down problematic channels. We demonstrate that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we quantitatively analyze the optimal values for shifting and scaling, taking into account both the asymmetric property and quantization errors of weights in the next layer. Our lightweight framework can incur minimal performance degradation under static and standard post-training quantization settings. Comprehensive results across various tasks and models reveal that our approach achieves near-floating-point performance on both small models
    
[^145]: 荆棘玫瑰：探究自然语言处理中的双重使用困境

    Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing. (arXiv:2304.08315v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08315](http://arxiv.org/abs/2304.08315)

    本文调查了自然语言处理（NLP）领域的双重使用问题，提出了一份定制的双重使用定义，并讨论了当前的状况和可能的挑战。

    

    双重使用是指有意将技术和科学成果用于有害目的的问题，在自然语言处理（NLP）领域尚未明确定义。然而，随着NLP技术的不断发展和在社会中的广泛应用，其内部运行方式变得越来越不透明。因此，理解双重使用的问题以及限制双重使用的潜在方法对于减少研究和开发的潜在危害至关重要。在本文中，我们对NLP研究人员和从业者进行了调查，以了解他们对该问题的深度理解和观点，并评估现有的支持情况。根据调查结果，我们为NLP社区提供了一份定制的双重使用定义。调查结果显示，大多数研究人员对他们的研究的潜在双重使用问题表示关切，但只采取有限的行动。基于调查结果，我们讨论了当前的状况和可能的挑战。

    Dual use, the intentional, harmful reuse of technology and scientific artefacts, is a problem yet to be well-defined within the context of Natural Language Processing (NLP). However, as NLP technologies continue to advance and become increasingly widespread in society, their inner workings have become increasingly opaque. Therefore, understanding dual use concerns and potential ways of limiting them is critical to minimising the potential harms of research and development. In this paper, we conduct a survey of NLP researchers and practitioners to understand the depth and their perspective of the problem as well as to assess existing available support. Based on the results of our survey, we offer a definition of dual use that is tailored to the needs of the NLP community. The survey revealed that a majority of researchers are concerned about the potential dual use of their research but only take limited action toward it. In light of the survey results, we discuss the current state and p
    
[^146]: MEGA: 多语言生成人工智能的综合评估

    MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v1 [cs.CL])

    [http://arxiv.org/abs/2303.12528](http://arxiv.org/abs/2303.12528)

    这项研究对 33 种语言中 8 个不同任务的生成 AI 进行了全面评估，比较了生成 LLMs 和非自回归模型的表现差异。

    

    生成AI模型在许多自然语言处理任务（如语言理解、推理和语言生成）上具有令人印象深刻的性能。当今AI社区最重要的问题之一是关于这些模型的能力和限制，评估生成AI显然具有挑战性。大多数关于生成大型语言模型（LLMs）的研究都限于英语，不清楚这些模型在理解和生成其他语言方面的能力。我们提供了首个全面评估 8 项不同任务和 33 种语言的生成LLMs MEGA 的基准测试。我们还将生成LLMs的性能与这些任务上最先进的非自回归模型进行比较，以确定生成模型的表现如何与上一代LLMs相比。我们对模型的性能进行了彻底分析。

    Generative AI models have impressive performance on many Natural Language Processing tasks such as language understanding, reasoning and language generation. One of the most important questions that is being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative Large Language Models (LLMs) are restricted to English and it is unclear how capable these models are at understanding and generating other languages. We present the first comprehensive benchmarking of generative LLMs MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse tasks and 33 typologically diverse languages. We also compare the performance of generative LLMs to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of model
    
[^147]: 具有元梯度正则化的自监督元提示学习用于少样本泛化

    Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])

    [http://arxiv.org/abs/2303.12314](http://arxiv.org/abs/2303.12314)

    提出了一种自我监督元提示学习框架SUPMER，包括元梯度正则化，用于少样本泛化，通过锚定的元训练任务和基于课程的任务增强丰富了任务分布，解决了在少样本情况下良好初始化软提示和过拟合的问题。

    

    提示调整是一种参数有效的方法，它学习软提示并使冻结的语言模型执行特定的下游任务。尽管有效，但提示调整在少样本情况下一方面严重依赖于良好的软提示初始化。另一方面，它很容易导致过度拟合。现有的方法利用预训练或监督元学习来初始化软提示，但它们不能对未见下游任务进行数据有效的泛化。为了解决以上问题，本文提出了一种新的自我监督元提示学习框架，其中包括元梯度正则化，用于少样本泛化（SUPMER）。我们首先设计了一组自监督锚定的元训练任务，具有不同的任务格式，并通过基于课程的任务增强进一步丰富了任务分布。然后将一种新的元梯度正则化方法集成到元提示学习中。它元学习在少样本情况下如何转换原始梯度。

    Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily result in overfitting. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they cannot data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with meta-gradient Regularization for few-shot generalization (SUPMER). We first design a set of self-supervised anchor meta-training tasks with different task formats and further enrich the task distribution with curriculum-based task augmentation. Then a novel meta-gradient regularization method is integrated into meta-prompt learning. It meta-learns to transform the raw gradients during few
    
[^148]: 大型语言模型的上下文准确提示

    Context-faithful Prompting for Large Language Models. (arXiv:2303.11315v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.11315](http://arxiv.org/abs/2303.11315)

    大型语言模型的准确提示对于解决上下文敏感的自然语言处理任务非常重要。本文通过研究两个方面的上下文准确性（知识冲突和预测的弃权），发现了基于观点的提示和虚构演示两种有效的方法，提高了大型语言模型的上下文准确性。

    

    大型语言模型（LLMs）编码了关于世界事实的参数化知识，在知识驱动的自然语言处理任务中表现出了非凡的性能。然而，它们对参数化知识的依赖可能导致它们忽视上下文线索，在上下文敏感的自然语言处理任务（例如知识获取任务）中做出错误的预测。本文旨在评估和增强LLMs在两个方面的上下文准确性：知识冲突和预测的弃权。我们证明可以通过精心设计的提示策略显著提高LLMs的准确性。特别是，我们确定了基于观点的提示和虚构演示是最有效的方法。基于观点的提示将上下文重新构造为讲述者的陈述，并询问讲述者的意见，而虚构演示使用包含错误事实的实例，以提高在知识冲突情况下的准确性。两种技术都不需要额外的训练。

    Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. W
    
[^149]: WiCE: 用于维基百科中声明的真实世界蕴含

    WiCE: Real-World Entailment for Claims in Wikipedia. (arXiv:2303.01432v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.01432](http://arxiv.org/abs/2303.01432)

    WiCE是一个细粒度的文本蕴含数据集，通过提取维基百科中的声明和证据配对构建。我们提出了自动声明分解策略，并展示了这种策略在多个数据集上的性能提升效果。此外，我们还发现现有模型无法解决数据集中的验证和检索问题。

    

    文本蕴含模型越来越多地应用于事实检查、问题回答中的预设验证或摘要评估等设置中。然而，这些应用与现有的蕴含数据集存在显著的领域转移，导致模型表现不佳。我们提出了WiCE，这是一个新的细粒度文本蕴含数据集，通过提取来自维基百科的自然声明和证据配对来构建。除了标准的声明级蕴含，WiCE还提供对声明子句的蕴含判断，以及支持每个子声明的最小证据句子子集。为了支持这一点，我们提出了一种使用GPT-3.5的自动声明分解策略，我们证明这种策略在测试时也能有效提高蕴含模型在多个数据集上的性能。最后，我们展示了我们数据集中真实声明涉及具有挑战性的验证和检索问题，现有模型无法解决。

    Textual entailment models are increasingly applied in settings like fact-checking, presupposition verification in question answering, or summary evaluation. However, these represent a significant domain shift from existing entailment datasets, and models underperform as a result. We propose WiCE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. In addition to standard claim-level entailment, WiCE provides entailment judgments over sub-sentence units of the claim, and a minimal subset of evidence sentences that support each subclaim. To support this, we propose an automatic claim decomposition strategy using GPT-3.5 which we show is also effective at improving entailment models' performance on multiple datasets at test time. Finally, we show that real claims in our dataset involve challenging verification and retrieval problems that existing models fail to address.
    
[^150]: AI聊天助手可改善关于分裂性话题的对话

    AI Chat Assistants can Improve Conversations about Divisive Topics. (arXiv:2302.07268v4 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.07268](http://arxiv.org/abs/2302.07268)

    该论文介绍了一个大型实验的结果，证明了使用人工智能工具可以改善关于分裂性话题的在线对话。他们通过使用一种大型语言模型实时提供基于证据的建议，帮助人们在对话中感受到理解的感觉。

    

    人类在线交流数量正在迅速增长。但是，社交媒体平台、消息应用程序和其他数字论坛上的基于文本的互动可能会产生分裂和冲突。这种有毒性增加了极化的程度，并且重要的是，侵蚀了多元化社会发展解决影响所有人的复杂社会问题的能力。学者和民间社会组织推动干预措施，使面对面的对话不那么具有分裂性或更具生产力，但将这些努力扩展至在线发生的许多话语是极具挑战性的。我们展示了一个大规模实验的结果，该实验证明了人工智能工具如何改善关于分裂性话题的在线对话。具体而言，我们采用大型语言模型实时提供基于证据的建议，以改善参与者在对话中感受到理解的感觉。我们发现这些建议确实可以帮助人们改善对话质量。

    A rapidly increasing amount of human conversation occurs online. But divisiveness and conflict can fester in text-based interactions on social media platforms, in messaging apps, and on other digital forums. Such toxicity increases polarization and, importantly, corrodes the capacity of diverse societies to develop efficient solutions to complex social problems that impact everyone. Scholars and civil society groups promote interventions that can make interpersonal conversations less divisive or more productive in offline settings, but scaling these efforts to the amount of discourse that occurs online is extremely challenging. We present results of a large-scale experiment that demonstrates how online conversations about divisive topics can be improved with artificial intelligence tools. Specifically, we employ a large language model to make real-time, evidence-based recommendations intended to improve participants' perception of feeling understood in conversations. We find that these
    
[^151]: 面向所有人的敏捷文本分类器

    Towards Agile Text Classifiers for Everyone. (arXiv:2302.06541v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.06541](http://arxiv.org/abs/2302.06541)

    本文介绍和评估了敏捷文本分类的方法，通过使用小型、定向的数据集进行训练，可以为特定策略快速开发分类器，并通过提示调整大型语言模型来实现最先进的性能。这为支持更安全的在线交流的模型带来了范式转变。

    

    基于文本的安全分类器广泛用于内容审核，并且越来越多地用于调整生成语言模型的行为，这是对数字助理和聊天机器人安全性的日益关注的话题。然而，不同的策略需要不同的分类器，并且安全策略本身也可以通过迭代和调整来改进。本文引入并评估了敏捷文本分类的方法，通过使用小型、定向的数据集进行训练，可以为特定策略快速开发分类器。在三个与安全相关的领域使用7个数据集，包括15个注释方案的实验中，我们的主要发现是：使用尽少80个示例的标记数据集，对大型语言模型（如PaLM 62B）进行提示调整，可以实现最先进的性能。我们认为，这为文本分类带来了范式转变，特别是为支持更安全的在线交流的模型。与试图收集数百万个示例以创建通用安全的模型相比，这种方法更加高效和灵活。

    Text-based safety classifiers are widely used for content moderation and increasingly to tune generative language model behavior - a topic of growing concern for the safety of digital assistants and chatbots. However, different policies require different classifiers, and safety policies themselves improve from iteration and adaptation. This paper introduces and evaluates methods for agile text classification, whereby classifiers are trained using small, targeted datasets that can be quickly developed for a particular policy. Experimenting with 7 datasets from three safety-related domains, comprising 15 annotation schemes, led to our key finding: prompt-tuning large language models, like PaLM 62B, with a labeled dataset of as few as 80 examples can achieve state-of-the-art performance. We argue that this enables a paradigm shift for text classification, especially for models supporting safer online discourse. Instead of collecting millions of examples to attempt to create universal safe
    
[^152]: Re-ViLM: 用于零样本和少样本图像字幕的检索增强视觉语言模型

    Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning. (arXiv:2302.04858v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.04858](http://arxiv.org/abs/2302.04858)

    Re-ViLM是一种检索增强的视觉语言模型，通过从外部数据库中检索相关知识，减少了模型参数的数量，并且可以轻松适应新数据，用于零样本和少样本图像字幕生成任务。

    

    在图像到文本生成任务中，使用预训练的语言模型（LMs）和视觉编码器（如Flamingo）相结合已经取得了最先进的结果。然而，这些模型将所有知识存储在其参数中，因此通常需要巨大的模型参数来建模丰富的视觉概念和丰富的文本描述。此外，它们在融合新数据方面效率低下，需要耗时的微调过程。在这项工作中，我们介绍了一种检索增强的视觉语言模型Re-ViLM，基于Flamingo构建，支持在零样本和上下文内少样本图像到文本生成任务中从外部数据库中检索相关知识。通过将某些知识明确存储在外部数据库中，我们的方法减少了模型参数的数量，并且可以通过简单更新数据库来轻松适应评估过程中的新数据。我们还构建了一种交错的图像和文本数据，以促进上下文内少样本生成任务。

    Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained the state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich textual descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations. By storing certain knowledge explicitly in the external database, our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in-context few-shot
    
[^153]: CodeLMSec基准：系统评估和发现黑盒代码语言模型中的安全漏洞

    CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models. (arXiv:2302.04012v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.04012](http://arxiv.org/abs/2302.04012)

    这项工作提出了一种系统研究代码语言模型安全问题的方法，旨在评估和发现黑盒代码语言模型中的安全漏洞。

    

    大型语言模型(LLMs)用于自动代码生成在几个编程任务中取得了突破。它们在竞赛级编程问题上的进展使它们成为AI辅助对编程的重要支柱，工具如GitHub Copilot已经成为数百万开发人员日常编程工作流程的一部分。这些模型的训练数据通常来自于互联网（例如开源存储库）并且可能含有缺陷和安全漏洞。这些未经消毒的训练数据可能导致语言模型学习这些漏洞并在代码生成过程中传播它们。尽管这些模型已经广泛评估了它们生成功能上正确程序的能力，但对这些模型的安全问题仍然缺乏全面的调查和基准测试。

    Large language models (LLMs) for automatic code generation have achieved breakthroughs in several programming tasks. Their advances in competition-level programming problems have made them an essential pillar of AI-assisted pair programming, and tools such as GitHub Copilot have emerged as part of the daily programming workflow used by millions of developers. The training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. While these models have been extensively assessed for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models.  In this work, we propose a method to systematically study the security issues of code l
    
[^154]: 基于分数的条件模型的概念代数

    Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03693](http://arxiv.org/abs/2302.03693)

    本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。

    

    本文研究了文本引导生成模型中学习表示的结构，重点关注基于分数的模型。我们聚焦于概念被编码为某种表示空间的子空间（或方向）的思想，并开发了这个思想的数学形式化。利用这个形式化方法，我们展示了有一个自然的表示选择具有这种性质，并且我们开发了一种简单的方法来识别与给定概念对应的表示部分。特别是，这使我们能够通过对表示的代数操作来操纵模型所表达的概念。我们使用稳定扩散在文本引导图像生成的示例中演示了这个思想。

    This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
    
[^155]: 元数据对科学文献标签化的影响：跨领域跨模型研究

    The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study. (arXiv:2302.03341v1 [cs.DL] CROSS LISTED)

    [http://arxiv.org/abs/2302.03341](http://arxiv.org/abs/2302.03341)

    这项研究系统地研究了元数据对科学文献标签化的影响，并在19个领域中选择了三种代表性的多标签分类器进行实验。

    

    随着科学出版物在网络上呈指数增长，迫切需要为每篇论文打上细粒度主题标签，以便研究人员可以跟踪自己感兴趣的研究领域，而不是淹没在整个文献中。科学文献的标签化不仅仅是一个纯粹的多标签文本分类任务，因为网络上的论文通常附带元数据信息，如会议、作者和参考文献，这些信息可以作为额外的信号来推断相关的标签。虽然已经有研究利用元数据进行学术论文分类，但它们通常限于一个或两个科学领域（如计算机科学和生物医学）和一个特定的模型。在这项工作中，我们系统地研究了元数据对19个领域科学文献标签化的影响。我们选择了三种代表性的多标签分类器（词袋模型、基于序列的模型和预训练语言模型），并探索了它们在不同领域的表现。

    Due to the exponential growth of scientific publications on the Web, there is a pressing need to tag each paper with fine-grained topics so that researchers can track their interested fields of study rather than drowning in the whole literature. Scientific literature tagging is beyond a pure multi-label text classification task because papers on the Web are prevalently accompanied by metadata information such as venues, authors, and references, which may serve as additional signals to infer relevant tags. Although there have been studies making use of metadata in academic paper classification, their focus is often restricted to one or two scientific fields (e.g., computer science and biomedicine) and to one specific model. In this work, we systematically study the effect of metadata on scientific literature tagging across 19 fields. We select three representative multi-label classifiers (i.e., a bag-of-words model, a sequence-based model, and a pre-trained language model) and explore t
    
[^156]: 使用上下文学习来提高对话安全性

    Using In-Context Learning to Improve Dialogue Safety. (arXiv:2302.00871v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.00871](http://arxiv.org/abs/2302.00871)

    本文研究了一种检索式方法来减少Chatbots回应中偏见和有毒内容的方法，通过使用上下文学习来引导模型生成更加安全的内容，该方法性能与强基线方法相媲美，还提出了一个可进一步提高响应安全性的重新排名程序。

    

    随着神经网络对话模型变得越来越成熟，人们开始关注这些系统的安全问题。这些系统很容易生成有毒内容，这些内容通常会延续社会偏见或刻板印象。本文提出了一种使用检索式方法来减少Chatbots回应中偏见和有毒内容的方法。我们采用上下文学习的方式来引导模型生成更加安全的内容。具体而言，当针对不安全的对话语境生成回答时，我们会从类似的对话语境中检索出安全回答的示例。我们发现，我们的方法不需要进行训练，就可以与强基线方法相媲美。例如，采用自动评估，我们发现我们最好的微调基线方法相较于我们的方法只能比“DiaSafety”中对不安全的对话语境生成安全回应多4.04%。最后，我们还提出了一种可进一步提高响应安全性的重新排名程序。

    While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.
    
[^157]: 知识蒸馏≈标签平滑：事实还是谬误？

    Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?. (arXiv:2301.12609v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12609](http://arxiv.org/abs/2301.12609)

    知识蒸馏和标签平滑被认为是等价的方法，但实验证明它们对模型置信度的影响方向完全相反。知识蒸馏不仅传递知识，还传递了自信心。

    

    最初被提出作为一种从一个模型向另一个模型进行知识传递的方法，一些最近的研究表明知识蒸馏(KD)实际上是一种正则化的形式。最强有力的支持来自于它与标签平滑(LS)方法的明显相似之处。本文通过比较它们所训练的模型的预测置信度，重新考察了这两种方法之间的等价关系。在涉及不同规模模型的四个文本分类任务的实验中，显示出：(a)在大多数设置中，KD和LS会完全相反地影响模型的置信度。(b) 在KD中，学生不仅继承知识，而且还从老师那里继承自信心，加强了传统的知识传递观点。

    Originally proposed as a method for knowledge transfer from one model to another, some recent studies have suggested that knowledge distillation (KD) is in fact a form of regularization. Perhaps the strongest support of all for this new perspective comes from its apparent similarities with label smoothing (LS). Here we re-examine this stated equivalence between the two methods by comparing the predictive confidences of the models they train. Experiments on four text classification tasks involving models of different sizes show that: (a) In most settings, KD and LS drive model confidence in completely opposite directions, and (b) In KD, the student inherits not only its knowledge but also its confidence from the teacher, reinforcing the classical knowledge transfer view.
    
[^158]: 不同ially私密的自然语言模型: 最新进展和未来方向。

    Differentially Private Natural Language Models: Recent Advances and Future Directions. (arXiv:2301.09112v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.09112](http://arxiv.org/abs/2301.09112)

    这篇论文对最近在NLP领域中的差分隐私深度学习模型的最新进展进行了系统综述，讨论了与标准差分隐私深度学习的不同之处和额外的挑战。

    

    深度学习的最新发展在各种自然语言处理（NLP）任务中取得了巨大成功。然而，这些应用可能涉及包含敏感信息的数据。因此，在保护敏感数据隐私的同时如何实现良好的性能是NLP中的一个重要挑战。为了保护隐私，差分隐私（DP）成为了隐私数据分析中防止重建攻击和防护潜在边缘知识的事实标准技术。近年来，DP在NLP模型（DP-NLP）方面已经从不同的角度进行了研究，值得进行全面的回顾。本文首次对DP深度学习模型在NLP中的最新进展进行了系统性的综述。具体而言，我们首先讨论了与标准DP深度学习相比，DP-NLP的一些差异和额外挑战。然后，我们调查了一些现有的DP-NLP工作，并从三个方面介绍了最新的发展：梯度pe

    Recent developments in deep learning have led to great success in various natural language processing (NLP) tasks. However, these applications may involve data that contain sensitive information. Therefore, how to achieve good performance while also protecting the privacy of sensitive data is a crucial challenge in NLP. To preserve privacy, Differential Privacy (DP), which can prevent reconstruction attacks and protect against potential side knowledge, is becoming a de facto technique for private data analysis. In recent years, NLP in DP models (DP-NLP) has been studied from different perspectives, which deserves a comprehensive review. In this paper, we provide the first systematic review of recent advances in DP deep learning models in NLP. In particular, we first discuss some differences and additional challenges of DP-NLP compared with the standard DP deep learning. Then, we investigate some existing work on DP-NLP and present its recent developments from three aspects: gradient pe
    
[^159]: 平稳航行：用表示平滑度分析改进预训练语言模型中的主动学习

    Smooth Sailing: Improving Active Learning for Pre-trained Language Models with Representation Smoothness Analysis. (arXiv:2212.11680v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11680](http://arxiv.org/abs/2212.11680)

    通过表示平滑度分析，我们改进了预训练语言模型中的主动学习方法，提出了一种无需验证集的早停技术，并发现任务适应对主动学习具有改进作用。这项工作在实际应用中证明了表示平滑度分析对于提高主动学习的有效性和实用性的重要性。

    

    主动学习（AL）方法旨在减少监督学习中的标注复杂性，以缓解昂贵的标注成本。最近的研究表明，将AL与大型预训练语言模型（PLM）结合使用具有益处，但往往忽视了影响AL效果的实际挑战。我们通过利用表示平滑度分析来解决这些挑战，以确保AL既有效又实用。首先，我们提出一种无需验证集的早停技术，在多个数据集和AL方法中对比随机抽样，观察到显著的改进。此外，我们发现任务适应改进了AL，而标准的短期微调在AL中并没有比随机抽样提供改进。我们的工作证明了表示平滑度分析在AL中的有用性，并引入了一种减少主动学习停止标准的方法。

    Developed to alleviate prohibitive labeling costs, active learning (AL) methods aim to reduce label complexity in supervised learning. While recent work has demonstrated the benefit of using AL in combination with large pre-trained language models (PLMs), it has often overlooked the practical challenges that hinder the effectiveness of AL. We address these challenges by leveraging representation smoothness analysis to ensure AL is feasible, that is, both effective and practicable. Firstly, we propose an early stopping technique that does not require a validation set -- often unavailable in realistic AL conditions -- and observe significant improvements over random sampling across multiple datasets and AL methods. Further, we find that task adaptation improves AL, whereas standard short fine-tuning in AL does not provide improvements over random sampling. Our work demonstrates the usefulness of representation smoothness analysis for AL and introduces an AL stopping criterion that reduce
    
[^160]: ALCAP: 基于对齐的音乐字幕生成器

    ALCAP: Alignment-Augmented Music Captioner. (arXiv:2212.10901v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2212.10901](http://arxiv.org/abs/2212.10901)

    本文提出了一种基于对齐的音乐字幕生成器，通过对比学习显式学习音频和歌词的对应关系，并生成高质量的字幕，取得了两个音乐字幕数据集上的最新领先水平。

    

    随着音乐流媒体平台用于音乐搜索和推荐的日益普及，需要新方法来解释音乐，同时考虑歌词和音频。然而，许多先前的研究关注于精细调整将音乐映射到字幕记号的编码器-解码器架构的各个组件，忽略了音频和歌词之间对应的潜在益处。本文提出了一种通过对比学习来显式学习多模态对齐的方法。通过学习音频-歌词的对应关系，使模型指导学习更好的跨模态一致性，从而生成高质量的字幕。我们提供了理论和经验结果，证明了所提出方法的优势，并在两个音乐字幕数据集上达到了新的状态-最先进水平。

    Growing popularity of streaming media platforms for music search and recommendations has led to a need for novel methods for interpreting music that take into account both lyrics and audio. However, many previous works focus on refining individual components of encoder-decoder architecture that maps music to caption tokens, ignoring the potential benefits of correspondence between audio and lyrics. In this paper, we propose to explicitly learn the multimodal alignment through contrastive learning. By learning audio-lyrics correspondence, the model is guided to learn better cross-modal consistency, thus generating high-quality captions. We provide both theoretical and empirical results demonstrating the advantage of the proposed method, and achieve new state-of-the-art on two music captioning datasets.
    
[^161]: 眼动数据、注释和语言模型的文本显著性风格的比较研究

    A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models. (arXiv:2212.09873v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09873](http://arxiv.org/abs/2212.09873)

    本研究比较了眼动数据、注释和语言模型对文本风格的显著性的影响，发现眼动数据在人类注释和基于模型的评分中起到了桥梁的作用。

    

    近年来，将眼动数据和其他人类语言处理的隐式测量方法引入自然语言处理（NLP）流程日益受到关注。人类语言处理数据中蕴含了对人类语言理解独特的洞察力，可以被语言模型所利用。然而，关于这些数据的性质以及如何在下游NLP任务中最好地利用它们仍然存在许多未解答的问题。在本文中，我们介绍了eyeStyliency，一个用于研究人类对文体文本（如礼貌性）进行处理的眼动数据集。我们开发了多种方法，利用收集的眼动数据对文本进行风格显著性评分。我们进一步研究了这种显著性数据与人类注释方法和基于模型的解释性指标的比较。我们发现，虽然眼动数据很独特，但它与人类注释和基于模型的重要性评分交集，为人类和机器之间的视角提供了可能的桥梁。

    There is growing interest in incorporating eye-tracking data and other implicit measures of human language processing into natural language processing (NLP) pipelines. The data from human language processing contain unique insight into human linguistic understanding that could be exploited by language models. However, many unanswered questions remain about the nature of this data and how it can best be utilized in downstream NLP tasks. In this paper, we present eyeStyliency, an eye-tracking dataset for human processing of stylistic text (e.g., politeness). We develop a variety of methods to derive style saliency scores over text using the collected eye dataset. We further investigate how this saliency data compares to both human annotation methods and model-based interpretability metrics. We find that while eye-tracking data is unique, it also intersects with both human annotations and model-based importance scores, providing a possible bridge between human- and machine-based perspecti
    
[^162]: 一种用于知识图谱链接预测的检索和阅读框架

    A Retrieve-and-Read Framework for Knowledge Graph Link Prediction. (arXiv:2212.09724v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09724](http://arxiv.org/abs/2212.09724)

    这项研究提出了一种检索和阅读框架来解决现有知识图谱链接预测系统的局限性。通过首先检索相关子图上下文，然后使用高容量阅读器联合推理上下文和查询，该框架能够提供更有用的信息和更强大的表达能力。

    

    知识图谱链接预测旨在根据知识图谱中的现有事实推断出新的事实。最近的研究表明，通过图神经网络（GNNs）使用节点的图邻域提供了比仅使用查询信息更有用的信息。传统的KG链接预测的GNNs遵循整个KG上的标准消息传递范式，这导致了冗余计算、节点表示的过度平滑以及限制了它们的表达能力。在大规模上，从整个KG中聚合有用的信息进行推理变得计算上昂贵。为了解决现有KG链接预测框架的局限性，我们提出了一种新颖的检索和阅读框架，该框架首先检索与查询相关的子图上下文，然后通过高容量阅读器联合推理上下文和查询。作为我们新框架的实例化的一部分，我们提出了一种基于Transformer的GNN作为r的新方法。

    Knowledge graph (KG) link prediction aims to infer new facts based on existing facts in the KG. Recent studies have shown that using the graph neighborhood of a node via graph neural networks (GNNs) provides more useful information compared to just using the query information. Conventional GNNs for KG link prediction follow the standard message-passing paradigm on the entire KG, which leads to superfluous computation, over-smoothing of node representations, and also limits their expressive power. On a large scale, it becomes computationally expensive to aggregate useful information from the entire KG for inference. To address the limitations of existing KG link prediction frameworks, we propose a novel retrieve-and-read framework, which first retrieves a relevant subgraph context for the query and then jointly reasons over the context and the query with a high-capacity reader. As part of our exemplar instantiation for the new framework, we propose a novel Transformer-based GNN as the r
    
[^163]: 论文信息提取中的事件个体化问题

    On Event Individuation for Document-Level Information Extraction. (arXiv:2212.09702v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09702](http://arxiv.org/abs/2212.09702)

    提出了问题──事件个体化对于模板填充任务是否适用，通过注释研究和误差分析，我们发现这引发了对模板填充度量的有效性、任务数据集的质量以及模型学习能力的担忧。

    

    随着信息提取系统在处理整个文件方面越来越熟练，传统的模板填充任务作为文件级信息提取的基准任务再次引起了人们的关注。在本文中，我们质疑了模板填充任务在这方面的适用性。我们认为该任务要求对事件个体化问题提供明确的答案——即区分不同的事件——而即使是人类专家在这个问题上也存在分歧。通过注释研究和误差分析，我们展示了这引发了对模板填充度量的有效性、任务数据集的质量以及模型学习能力的担忧。最后，我们考虑了可能的解决方案。

    As information extraction (IE) systems have grown more adept at processing whole documents, the classic task of template filling has seen renewed interest as benchmark for document-level IE. In this position paper, we call into question the suitability of template filling for this purpose. We argue that the task demands definitive answers to thorny questions of event individuation -- the problem of distinguishing distinct events -- about which even human experts disagree. Through an annotation study and error analysis, we show that this raises concerns about the usefulness of template filling metrics, the quality of datasets for the task, and the ability of models to learn it. Finally, we consider possible solutions.
    
[^164]: DiSTRICT: 通过Retriever驱动的上下文调优对话状态跟踪

    DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning. (arXiv:2212.02851v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.02851](http://arxiv.org/abs/2212.02851)

    DiSTRICT是一种无需手工制作模板的通用上下文调优方法，通过检索相关的训练样本来微调对话状态跟踪模型，从而在各种零样本和少样本情况下使用更小的模型时获得了优于现有方法的性能。

    

    对话状态跟踪(DST)是任务导向型对话系统的关键组件，它通过确定进行中对话中预定义槽位的值来表示用户意图。现有方法使用手工制作的模板和额外的槽位信息来对大型预训练语言模型进行微调和提示，并从对话上下文中引出槽位的值。然而，设计有效的提示需要大量手动工作和领域知识，限制了这些方法在新的领域和任务上的泛化能力。在这项工作中，我们提出了DiSTRICT，一种适用于DST的通用上下文调优方法，它通过检索与给定对话高度相关的训练样本来进行模型的微调，而无需使用手工制作的模板。在使用MultiWOZ基准数据集进行的实验中，DiSTRICT在各种零样本和少样本情况下使用较小的模型优于现有方法，从而为经常进行实际部署的实际情况提供了重要优势。

    Dialogue State Tracking (DST), a key component of task-oriented conversation systems, represents user intentions by determining the values of pre-defined slots in an ongoing dialogue. Existing approaches use hand-crafted templates and additional slot information to fine-tune and prompt large pre-trained language models and elicit slot values from the dialogue context. Significant manual effort and domain knowledge is required to design effective prompts, limiting the generalizability of these approaches to new domains and tasks. In this work, we propose DiSTRICT, a generalizable in-context tuning approach for DST that retrieves highly relevant training examples for a given dialogue to fine-tune the model without any hand-crafted templates. Experiments with the MultiWOZ benchmark datasets show that DiSTRICT outperforms existing approaches in various zero-shot and few-shot settings using a much smaller model, thereby providing an important advantage for real-world deployments that often 
    
[^165]: IRRGN: 一种用于多轮响应选择的隐式关系推理图网络

    IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn Response Selection. (arXiv:2212.00482v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.00482](http://arxiv.org/abs/2212.00482)

    本研究提出了一种名为IRRGN的隐式关系推理图网络，用于解决多轮对话中的响应选择任务。该网络利用关系图卷积网络隐式提取话语之间以及话语和选项之间的依赖关系，并通过双重比较感知选项之间的差异，取得了显著的性能提升。

    

    在多轮对话中，响应选择的任务是从所有候选项中找到最佳选项。为了提高模型的推理能力，先前的研究更关注使用明确的算法来建模话语之间的依赖关系，这些关系是确定性的、有限的和不灵活的。此外，很少有研究考虑到推理之前和之后选项之间的差异。在本文中，我们提出了一种隐式关系推理图网络来解决这些问题，它由话语关系推理器（URR）和选项双重比较器（ODC）组成。URR的目标是隐式提取话语之间以及话语和选项之间的依赖关系，并使用关系图卷积网络进行推理。ODC专注于通过双重比较感知选项之间的差异，从而排除噪声选项的干扰。在两个多轮对话推理基准数据集MuTual上进行的实验结果表明，提出的方法在响应选择任务上取得了显著的性能提升。

    The task of response selection in multi-turn dialogue is to find the best option from all candidates. In order to improve the reasoning ability of the model, previous studies pay more attention to using explicit algorithms to model the dependencies between utterances, which are deterministic, limited and inflexible. In addition, few studies consider differences between the options before and after reasoning. In this paper, we propose an Implicit Relational Reasoning Graph Network to address these issues, which consists of the Utterance Relational Reasoner (URR) and the Option Dual Comparator (ODC). URR aims to implicitly extract dependencies between utterances, as well as utterances and options, and make reasoning with relational graph convolutional networks. ODC focuses on perceiving the difference between the options through dual comparison, which can eliminate the interference of the noise options. Experimental results on two multi-turn dialogue reasoning benchmark datasets MuTual a
    
[^166]: 无监督依存语法的语法可替代性

    Syntactic Substitutability as Unsupervised Dependency Syntax. (arXiv:2211.16031v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.16031](http://arxiv.org/abs/2211.16031)

    本文提出了一种新的无需语法规则的诱导方法，该方法基于语法可替代性，能够在定量和定性方面都有所提升。

    

    语法是构成人类语言鲜明和组合性的潜在分层结构。本文进一步探究了依赖于语言模型注意力分布的句法依存关系表示的假设，并提出了一种新的无需语法规则的诱导方法。我们旨在模拟句法替代性这个更为普遍的关系，而不是严格按照标注架构建模依存关系。这种关系体现了事实，即句法依存关系两端的单词可以被同一句法范畴的单词所替换，从而定义了一组语法不变的句子。这些句子的表示被用作解析的基础。我们证明，我们的方法在定量和定性方面都有所提升，例如在长距离主谓一致性任务中实现了78.3％的召回率，而之前的无监督方法只有8.5％的召回率。

    Syntax is a latent hierarchical structure which underpins the robust and compositional nature of human language. In this work, we further explore the hypothesis that syntactic dependencies can be represented in the attention distributions of language models trained on text and propose a new method to induce these structures theory-agnostically. Instead of modeling syntactic relations as defined by annotation schemata, we model a more general property implicit in the definition of dependency relations, syntactic substitutability. This property captures the fact that the words at either end of a syntactic dependency can be substituted with words from the same syntactic category, defining a set of syntactically-invariant sentences whose representations are then used as the basis for parsing. We demonstrate that our method results in both qualitative and quantitative gains, for example achieving 78.3% recall on a long-distance subject-verb agreement task vs. 8.5% with a previous unsupervis
    
[^167]: SciRepEval：一个用于科学文献表示的多格式基准

    SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.13308](http://arxiv.org/abs/2211.13308)

    SciRepEval是第一个综合评估科学文献表示的全面基准，其中包括四种格式的 25 个任务。通过使用格式特定的控制代码和适配器，可以改进科学文献表示模型的泛化能力。

    

    学习的科学文献表示可以作为下游任务的有价值输入特征，无需进一步微调。然而，用于评估这些表示的现有基准未能捕捉到相关任务的多样性。为此，我们介绍了 SciRepEval，第一个用于训练和评估科学文献表示的全面基准。它包括四种格式的 25 个具有挑战性和现实性的任务，其中 11 个是新任务：分类、回归、排名和搜索。我们使用该基准来研究和改进科学文档表示模型的泛化能力。我们展示了最先进的模型如何在任务格式方面缺乏泛化性能，简单的多任务训练也不能改进它们。然而，一种新的方法，学习每个文档的多个嵌入，每个嵌入专门针对不同的格式，可以提高性能。我们尝试使用任务格式特定的控制代码和适配器。

    Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
    
[^168]: 思维促进程序：为解决数值推理任务将计算与推理分离开来

    Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. (arXiv:2211.12588v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.12588](http://arxiv.org/abs/2211.12588)

    这篇论文提出了一种新的方法，即"思维程序"（PoT），通过将推理过程表达为一个程序，将计算与推理过程分离开来，以提升解决数值推理任务的性能。在多个数据集上的评估中，PoT相比最新方法CoT在性能上平均提高了约12％。

    

    最近，在教授语言模型进行复杂数值推理任务的逐步推理方面取得了显著进展。思维链提示（CoT）是目前这些任务的最新方法。CoT使用语言模型来执行多步骤的推理和计算过程。为了将计算与推理分离开来，我们提出了"思维程序"（PoT），它使用语言模型（主要是Codex）将推理过程表达为一个程序。计算过程被委托给外部计算机执行生成的程序以得出答案。我们在五个数学问题数据集（GSM，AQuA，SVAMP，TabMWP，MultiArith）和三个金融问答数据集（FinQA，ConvFinQA，TATQA）上评估了PoT的表现，包括少样本和零样本设置。在少样本和零样本设置下，PoT的平均性能提升约为12％，在所有评估的数据集上。通过将PoT与自一致性组合，我们还提出了一种用于训练语言模型的方法，这种方法可以提高模型在数值推理任务上的性能。

    Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\% across all the evaluated datasets. By combining PoT with self-consistency de
    
[^169]: [RE]VER：学习自然语言表示以阐述实体和关系

    [RE]VER: Learning Natural Language Representations for Verbalizing Entities and Relations. (arXiv:2211.11093v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11093](http://arxiv.org/abs/2211.11093)

    本文提出了一种名为[RE]VER的系统，使用基于transformer的模型来学习实体和关系的自然语言表示，能够生成一个能够表示实体与其他实体关系的句子，相比于之前的最先进方法有了显著改进。

    

    实体及实体之间的关系是现实世界中至关重要的。人们通过理解实体和关系来了解世界。本文提出了一种名为[RE]VER的系统，通过使用基于transformer的模型来学习实体和关系的自然语言表示，并生成一个能够表示实体与其他实体关系的句子。我们在多个基准数据集上评估了我们的模型，并证明其相比于之前的最先进方法有了显著改进。

    Entities and relationships between entities are vital in the real world. Essentially, we understand the world by understanding entities and relations. For instance, to understand a field, e.g., computer science, we need to understand the relevant concepts, e.g., machine learning, and the relationships between concepts, e.g., machine learning and artificial intelligence. To understand a person, we should first know who he/she is and how he/she is related to others. To understand entities and relations, humans may refer to natural language descriptions. For instance, when learning a new scientific term, people usually start by reading its definition in dictionaries or encyclopedias. To know the relationship between two entities, humans tend to create a sentence to connect them. In this paper, we propose [RE]VER: A Unified Model for Verbalizing Entities and Relations. Specifically, we attempt to build a system that takes any entity or entity set as input and generates a sentence to repres
    
[^170]: CAPE: 使用大型语言模型从前置错误中纠正行动

    CAPE: Corrective Actions from Precondition Errors using Large Language Models. (arXiv:2211.09935v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.09935](http://arxiv.org/abs/2211.09935)

    CAPE是一种利用大型语言模型从前置错误中纠正行动的方法，提高了生成计划的质量，使具身代理能够执行更多任务，并改善了计划的正确性。

    

    从大型语言模型中提取常识知识为设计智能机器人提供了一种途径。现有的利用语言模型进行规划的方法在行动失败时无法恢复，并且通常只能尝试重新执行失败的行动，而无法解决错误的根本原因。我们提出了一种新颖的方法（CAPE），试图在规划过程中提出纠正前置条件错误的行动。CAPE通过利用少样本推理从行动前置条件中提高了生成计划的质量。我们的方法使得具身代理能够执行比基线方法更多的任务，同时确保语义正确性和最小化重新提示。在VirtualHome中，CAPE生成可执行的计划，并且相比SayCan，将人工标注的计划正确度指标从28.89%提高到49.63%。我们的改进也适用于一台配置了一组以语言为指定的技能和相关前置条件的波士顿动力公司的Spot机器人，其中CAPE提高了正确性。

    Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause.  We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctne
    
[^171]: 利用对比学习和数字证据改进混淆的法律判决预测

    Exploiting Contrastive Learning and Numerical Evidence for Improving Confusing Legal Judgment Prediction. (arXiv:2211.08238v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08238](http://arxiv.org/abs/2211.08238)

    本文提出了一种利用对比学习和数字证据的方法改进混淆的法律判决预测。通过提出一种监督对比学习方法和利用数字证据预测处罚期限，成功地解决了区分分类错误和利用数字的问题。

    

    鉴于一个法律案例的事实描述文本，法律判决预测（LJP）旨在预测案例的罪名、法律条款和处罚期限。LJP的一个核心问题是如何区分混淆的法律案例，其中只存在微妙的文本差异。以往的研究在使用标准的交叉熵分类损失无法区分不同的分类错误，并忽略了事实描述中的数字，用于预测处罚期限。为了解决这些问题，本文首先提出了一种基于moco的监督对比学习，以学习可区分的表示，并探索构建正例对的最佳策略，从而同时有利于LJP的三个子任务。其次，为了利用法律案例中的数字来预测某些案例的处罚期限，我们进一步增强了由预训练数值模型编码的提取的犯罪金额对事实描述的表示。对公开数据集进行了大量实验。

    Given the fact description text of a legal case, legal judgment prediction (LJP) aims to predict the case's charge, law article and penalty term. A core problem of LJP is how to distinguish confusing legal cases, where only subtle text differences exist. Previous studies fail to distinguish different classification errors with a standard cross-entropy classification loss, and ignore the numbers in the fact description for predicting the term of penalty. To tackle these issues, in this work, first, we propose a moco-based supervised contrastive learning to learn distinguishable representations, and explore the best strategy to construct positive example pairs to benefit all three subtasks of LJP simultaneously. Second, in order to exploit the numbers in legal cases for predicting the penalty terms of certain cases, we further enhance the representation of the fact description with extracted crime amounts which are encoded by a pre-trained numeracy model. Extensive experiments on public 
    
[^172]: 理解ME？用于细粒度视觉常识的多模态评估

    Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense. (arXiv:2211.05895v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.05895](http://arxiv.org/abs/2211.05895)

    该论文提出了一个多模态评估流程来测试模型对视觉场景、文本和相关知识的理解能力，并表明使用该数据进行训练可以提高模型在标准评估中的性能。

    

    视觉常识理解要求视觉语言模型不仅要理解图像和文本，还要在两者之间进行互相参考，以完全融合和理解所描述的视觉场景。最近，已经开发了各种方法，并在视觉常识基准上取得了高性能。然而，由于有限的评估数据资源，我们无法确定模型是否真正理解视觉场景和潜在的常识知识。为了提供深入的分析，我们提出了一个多模态评估（ME）流程，自动生成问题-答案对来测试模型对视觉场景、文本和相关知识的理解能力。我们还进一步展示，使用ME数据进行训练可以提高模型在标准VCR评估中的性能。最后，我们的深入分析和比较揭示了一些有趣的发现：（1）语义低级信息可以帮助学习高级信息，但不能完全解决语义理解问题。

    Visual commonsense understanding requires Vision Language (VL) models to not only understand image and text but also cross-reference in-between to fully integrate and achieve comprehension of the visual scene described. Recently, various approaches have been developed and have achieved high performance on visual commonsense benchmarks. However, it is unclear whether the models really understand the visual scene and underlying commonsense knowledge due to limited evaluation data resources. To provide an in-depth analysis, we present a Multimodal Evaluation (ME) pipeline to automatically generate question-answer pairs to test models' understanding of the visual scene, text, and related knowledge. We then take a step further to show that training with the ME data boosts the model's performance in standard VCR evaluation. Lastly, our in-depth analysis and comparison reveal interesting findings: (1) semantically low-level information can assist the learning of high-level information but not
    
[^173]: 自监督语音模型在自动语音识别中的有效蒸馏探索

    Exploring Effective Distillation of Self-Supervised Speech Models for Automatic Speech Recognition. (arXiv:2210.15631v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.15631](http://arxiv.org/abs/2210.15631)

    本文探索了将HuBERT基于自监督学习的语音模型进行有效蒸馏，以改善自动语音识别的性能，特别是在低资源场景下。通过研究不同的学生模型结构，并引入判别损失，本研究提供了一种简单有效的算法来进行自监督语音模型的微型化。

    

    近年来，自监督学习在语音处理领域取得了巨大的进展。自监督学习模型通常是在大量无标签数据上进行预训练，并且更喜欢使用大型模型来增加建模能力。然而，这可能由于过大的计算和存储成本而限制了其潜在应用。自监督学习模型的微型化已成为具有实用价值的重要研究方向。为此，我们探索了基于HuBERT的自监督语音模型在自动语音识别中的有效蒸馏。首先，为了建立一个强大的基准线，我们对不同的学生模型结构进行了全面的研究。在此基础上，作为对先前工作中广泛采用的回归损失的补充，引入了一种判别损失来增强HuBERT的蒸馏性能，特别是在低资源场景下。此外，我们设计了一种简单有效的算法来进行蒸馏。

    Recent years have witnessed great strides in self-supervised learning (SSL) on the speech processing. The SSL model is normally pre-trained on a great variety of unlabelled data and a large model size is preferred to increase the modeling capacity. However, this might limit its potential applications due to the expensive computation and memory costs introduced by the oversize model. Miniaturization for SSL models has become an important research direction of practical value. To this end, we explore the effective distillation of HuBERT-based SSL models for automatic speech recognition (ASR). First, in order to establish a strong baseline, a comprehensive study on different student model structures is conducted. On top of this, as a supplement to the regression loss widely adopted in previous works, a discriminative loss is introduced for HuBERT to enhance the distillation performance, especially in low-resource scenarios. In addition, we design a simple and effective algorithm to distil
    
[^174]: COFFEE: 可解释性推荐中个性化文本生成的反事实公平性

    COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation. (arXiv:2210.15500v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.15500](http://arxiv.org/abs/2210.15500)

    本研究探讨了个性化解释生成中的反事实公平性问题，在解释生成中引入了一个通用框架以实现度量特定的反事实公平性，实验证明了方法的有效性。

    

    随着语言模型越来越多地融入到我们的数字生活中，个性化文本生成（PTG）已成为一个具有广泛应用的重要组成部分。然而，用于PTG模型训练的用户编写文本中存在的偏见可能会无意中将不同水平的语言质量与用户的受保护属性关联起来。模型可以继承这种偏见，并在生成与用户的受保护属性相关的文本时延续不平等，导致在为用户提供服务时出现不公平的对待。在这项工作中，我们研究了个性化解释生成中PTG的公平性。我们首先讨论了生成的解释中的偏见及其公平性影响。为了促进公平性，我们引入了一个通用框架，以实现解释生成中的度量特定的反事实公平性。大量实验和人工评估表明了我们方法的有效性。

    As language models become increasingly integrated into our digital lives, Personalized Text Generation (PTG) has emerged as a pivotal component with a wide range of applications. However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate different levels of linguistic quality with users' protected attributes. The model can inherit the bias and perpetuate inequality in generating text w.r.t. users' protected attributes, leading to unfair treatment when serving users. In this work, we investigate fairness of PTG in the context of personalized explanation generation for recommendations. We first discuss the biases in generated explanations and their fairness implications. To promote fairness, we introduce a general framework to achieve measure-specific counterfactual fairness in explanation generation. Extensive experiments and human evaluations demonstrate the effectiveness of our method.
    
[^175]: 统一数据观点和个性化: 应用于社会规范的研究

    Unifying Data Perspectivism and Personalization: An Application to Social Norms. (arXiv:2210.14531v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.14531](http://arxiv.org/abs/2210.14531)

    本研究提出了一种应用于社会规范的个性化方法，通过分析一组标注者对社交媒体帖子的评价来预测社会规范的认知。实验证明，个性化方法在预测社会规范认知方面具有较好的效果，并在不同社交情境的子集中的表现有所差异。

    

    在语言处理任务中，不再使用单一的真实标签，近期研究着重于如何表示和预测一组标注者的标签。然而，对于标注者的信息往往很少或没有了解，或者标注者的数量很少。本研究分析了一组13k标注者对210k项社会规范的评价，并提供了一种将个性化方法应用于标注者建模的实验设计，比较了个性化方法对预测社会规范认知的有效性。我们进一步对不同社交情境的子集进行表现分析，这些情境根据冲突各方之间关系的亲密度而异，评估个性化方法在哪些方面的效果最好。

    Instead of using a single ground truth for language processing tasks, several recent studies have examined how to represent and predict the labels of the set of annotators. However, often little or no information about annotators is known, or the set of annotators is small. In this work, we examine a corpus of social media posts about conflict from a set of 13k annotators and 210k judgements of social norms. We provide a novel experimental setup that applies personalization methods to the modeling of annotators and compare their effectiveness for predicting the perception of social norms. We further provide an analysis of performance across subsets of social situations that vary by the closeness of the relationship between parties in conflict, and assess where personalization helps the most.
    
[^176]: 关于临床文本挖掘的跨领域预训练语言模型：在数据受限微调中它们表现如何？

    On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.12770](http://arxiv.org/abs/2210.12770)

    本研究探讨了在临床自然语言处理任务中，将从一般领域或相关领域数据预训练的迁移学习模型微调到特定任务上的有效性。实验结果显示，微调的语言模型相对于从头开始学习的模型在命名实体识别任务上取得了更好的性能。

    

    在自然语言处理领域，使用从一般或相关领域数据预训练的大型语言模型（LLMs）来将其微调到特定领域和任务上，并使用新任务中可用的有限资源进行微调，一直以来都是一个流行的实践。在本研究中，我们重新考虑了这种假设，并在临床自然语言处理领域进行了研究，具体是在药物及其相关属性的命名实体识别任务上。我们比较了从头开始学习的Transformer模型和通过微调BERT-based LLMs（包括BERT-base、BioBERT和ClinicalBERT）进行微调的模型。我们还对这些模型及其扩展模型与带有CRF层的连续学习进行了比较。我们使用n2c2-2018共享任务数据进行模型开发和评估。实验结果表明：1）CRF层对所有神经模型都起到了积极的影响；2）在使用宏平均F1对BIO-strict跨度级别进行评估时，微调的LLMs获得了0.83+的得分，而从头开始学习的TransformerCRF模型得分为0.78+，证明了微调模型的优势。

    Fine-tuning Large Language Models (LLMs) pre-trained from general or related domain data to a specific domain and task using a limited amount of resources available in the new task has been a popular practice in NLP fields. In this work, we re-visit this assumption, and carry out investigation in clinical NLP, specifically named-entity recognition on Drugs and their related Attributes. We compare Transformer models that are learned from scratch to fine-tuning BERT-based LLMs including BERT-base, BioBERT, and ClinicalBERT. We also investigate the comparison of such models and their extended models with a CRF layer for continuous learning. We use n2c2-2018 shared task data for model development and evaluations. The experimental outcomes show that 1) the CRF layer makes a difference for all neural models; 2) on BIO-strict span level evaluation using macro-average F1, while the fine-tuned LLMs achieved scores 0.83+, the TransformerCRF model learned from scratch achieved 0.78+ demonstrating
    
[^177]: InterFair: 通过自然语言反馈进行公平可解释预测的去偏方法

    InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions. (arXiv:2210.07440v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07440](http://arxiv.org/abs/2210.07440)

    这篇论文提出了一种公平可解释的去偏方法，通过用户反馈在任务性能和偏差缓解之间实现更好和更公平的平衡。

    

    传统的NLP模型去偏方法通常专注于隔离与敏感属性（如性别或种族）相关的信息。相反，我们认为一个有利的去偏方法应该“公平地”使用敏感信息，并且能够解释其使用原因，而不是盲目消除它。这种公平平衡往往是主观的，且在算法上实现起来具有挑战性。我们探索了两种交互设置，并使用冻结的预测模型展示了用户能够通过提供反馈来在任务性能和偏差缓解之间实现更好和更公平的平衡。在一种设置中，用户通过与测试示例的交互，进一步减小了解释中的偏差（5-8%），同时保持了相同的预测准确度。在另一种设置中，人类反馈能够从输入中将相关偏差和预测信息相分离，从而实现了更卓越的偏差缓解和改善的任务性能（4-5%）。

    Debiasing methods in NLP models traditionally focus on isolating information related to a sensitive attribute (e.g., gender or race). We instead argue that a favorable debiasing method should use sensitive information 'fairly,' with explanations, rather than blindly eliminating it. This fair balance is often subjective and can be challenging to achieve algorithmically. We explore two interactive setups with a frozen predictive model and show that users able to provide feedback can achieve a better and fairer balance between task performance and bias mitigation. In one setup, users, by interacting with test examples, further decreased bias in the explanations (5-8%) while maintaining the same prediction accuracy. In the other setup, human feedback was able to disentangle associated bias and predictive information from the input leading to superior bias mitigation and improved task performance (4-5%) simultaneously.
    
[^178]: 一次就够了：一种轻量级的用于快速句对建模的交叉注意力方法

    Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling. (arXiv:2210.05261v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.05261](http://arxiv.org/abs/2210.05261)

    本文提出了一种名为MixEncoder的新范式，用于高效的句对建模。该范式通过轻量级的交叉注意力机制实现了超过113倍的句对匹配加速，与更昂贵的交叉注意力模型相比，性能相当。

    

    基于Transformer的模型在句对建模任务（如答案选择和自然语言推理）上取得了巨大成功。这些模型通常在输入句对上执行交叉注意力，导致计算成本过高。最近的研究提出了双编码器和后期交互的架构以实现更快的计算。然而，交叉注意力的表达性和计算速度之间的平衡仍需更好的协调。因此，本文引入了一种新的范式MixEncoder用于高效的句对建模。MixEncoder包含了一种轻量级的交叉注意力机制，在编码查询时只进行一次，同时并行建模查询-候选交互。在四个任务上进行的大量实验表明，我们的MixEncoder可以将句对匹配加速超过113倍，同时实现与更昂贵的交叉注意力模型相当的性能。

    Transformer-based models have achieved great success on sentence pair modeling tasks, such as answer selection and natural language inference (NLI). These models generally perform cross-attention over input pairs, leading to prohibitive computational costs. Recent studies propose dual-encoder and late interaction architectures for faster computation. However, the balance between the expressive of cross-attention and computation speedup still needs better coordinated. To this end, this paper introduces a novel paradigm MixEncoder for efficient sentence pair modeling. MixEncoder involves a light-weight cross-attention mechanism. It conducts query encoding only once while modeling the query-candidate interaction in parallel. Extensive experiments conducted on four tasks demonstrate that our MixEncoder can speed up sentence pairing by over 113x while achieving comparable performance as the more expensive cross-attention models.
    
[^179]: 从非结构化语言中翻译战略意图的计算接口

    A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting. (arXiv:2208.08374v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.08374](http://arxiv.org/abs/2208.08374)

    本文建立了一个计算接口，可以将非结构化语言策略翻译为可执行的目标和约束，并证明该模型在推断战略意图方面优于人类解释员。

    

    许多现实世界的任务涉及到人类和AI系统协同完成任务的混合发起设定。尽管在通过语言精确指定代理完成任务的低级规范方面进行了大量工作，但之前的工作在解释人类指挥官的高级战略意图方面缺乏。从语言中解析战略意图将使自主系统能够根据用户的计划独立运行，而无需频繁的指导或指令。在本文中，我们构建了一个能够将非结构化语言策略转化为可行的目标和约束形式的计算接口。利用一个游戏环境，我们收集了1000多个示例的数据集，将语言策略映射到相应的目标和约束，并证明我们在这个数据集上训练的模型在推断战略意图方面显著优于人类解释员。

    Many real-world tasks involve a mixed-initiative setup, wherein humans and AI systems collaboratively perform a task. While significant work has been conducted towards enabling humans to specify, through language, exactly how an agent should complete a task (i.e., low-level specification), prior work lacks on interpreting the high-level strategic intent of the human commanders. Parsing strategic intent from language will allow autonomous systems to independently operate according to the user's plan without frequent guidance or instruction. In this paper, we build a computational interface capable of translating unstructured language strategies into actionable intent in the form of goals and constraints. Leveraging a game environment, we collect a dataset of over 1000 examples, mapping language strategies to the corresponding goals and constraints, and show that our model, trained on this dataset, significantly outperforms human interpreters in inferring strategic intent (i.e., goals an
    
[^180]: 元元反游戏学习组合学习行为

    Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.08012](http://arxiv.org/abs/2207.08012)

    本论文提出了一种元元反游戏学习的方法来解决组合学习行为的问题，通过解决绑定问题来支持人工智能代理展示组合学习行为的能力。

    

    人类利用组合性从过去的经验中推广到新颖的经验。我们假设我们的经验可以分解为基本的原子组件，这些组件可以以新颖的方式重新组合，以支持我们参与新颖经验的能力。我们将这视为学习以组合方式泛化的能力，并将利用这种能力的行为称为组合学习行为（CLBs）。学习CLBs的一个核心问题是解决绑定问题（BP）。尽管这是人类轻松完成的智能壮举，但对于现有技术的人工智能代理来说并非如此。因此，为了构建能够与人类合作的人工智能代理，我们建议开发一个新的基准来研究代理商通过解决BP的领域无关版本来展示CLBs的能力。我们受到指代游戏的语言涌现和基础架构框架的启发，提出了一个元学习扩展方案

    Human beings use compositionality to generalise from past experiences to novel experiences. We assume a separation of our experiences into fundamental atomic components that can be recombined in novel ways to support our ability to engage with novel experiences. We frame this as the ability to learn to generalise compositionally, and we will refer to behaviours making use of this ability as compositional learning behaviours (CLBs). A central problem to learning CLBs is the resolution of a binding problem (BP). While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents. Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extensio
    
[^181]: PreBit -- 一种利用Twitter FinBERT嵌入的多模态模型，用于比特币的极端价格波动预测。

    PreBit -- A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin. (arXiv:2206.00648v2 [q-fin.ST] UPDATED)

    [http://arxiv.org/abs/2206.00648](http://arxiv.org/abs/2206.00648)

    本文提出了一种利用多模态模型进行比特币极端价格波动预测的方法，将相关资产、技术指标和Twitter内容作为输入。通过使用预训练的金融词汇表的句级FinBERT嵌入，模型可以有效地捕捉推文中的内容，从而预测比特币的价格波动。

    

    比特币以其不断增长的受欢迎程度展示了自其诞生以来的极端价格波动性。这种波动性，加上其去中心化的性质，使比特币相对于更传统的资产更容易受到投机交易的影响。本文提出了一种用于预测极端价格波动的多模态模型。该模型将各种相关资产、技术指标以及Twitter内容作为输入。在一项深入研究中，我们探讨了来自大众对比特币的社交媒体讨论是否具有极端价格波动的预测能力。我们收集了从2015年到2021年每天包含关键词“比特币”的5,000条推文的数据集，称为PreBit，并将其在网上提供。在我们的混合模型中，我们使用在金融词汇表上预训练的句级FinBERT嵌入，以便以可理解的方式捕捉推文的全部内容并将其提供给模型。通过将这些嵌入与一种卷积层结合起来，我们可以提取推文的特征信息，并在模型中进行极端价格波动的预测。

    Bitcoin, with its ever-growing popularity, has demonstrated extreme price volatility since its origin. This volatility, together with its decentralised nature, make Bitcoin highly subjective to speculative trading as compared to more traditional assets. In this paper, we propose a multimodal model for predicting extreme price fluctuations. This model takes as input a variety of correlated assets, technical indicators, as well as Twitter content. In an in-depth study, we explore whether social media discussions from the general public on Bitcoin have predictive power for extreme price movements. A dataset of 5,000 tweets per day containing the keyword `Bitcoin' was collected from 2015 to 2021. This dataset, called PreBit, is made available online. In our hybrid model, we use sentence-level FinBERT embeddings, pretrained on financial lexicons, so as to capture the full contents of the tweets and feed it to the model in an understandable way. By combining these embeddings with a Convoluti
    
[^182]: 间接通过主动示例为非程序员添加标签程序：以Text-to-SQL为例的案例研究

    Labeling Programs with Non-Programmers Indirectly via Active Examples: A Case Study with Text-to-SQL. (arXiv:2205.12422v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12422](http://arxiv.org/abs/2205.12422)

    使用APEL框架，非程序员可以通过检查输入-输出示例间接选择复杂程序，从而实现对自然语言表达的注释，并且在重新注释文本到SQL数据集时达到了与专家相同的准确度，同时揭示了原始注释中的细微错误。

    

    非程序员能否使用复杂程序对自然语言表达进行注释以表示其含义？我们介绍了APEL，这是一个框架，在这个框架中，非程序员从由种子语义解析器（例如Codex）生成的候选程序中选择。由于他们无法理解候选程序，我们要求他们通过检查程序的输入-输出示例来间接选择。对于每个表达式，APEL主动搜索一个简单的输入，候选程序在这个输入上更倾向于产生不同的输出。然后，它只要求非程序员选择适当的输出，从而允许我们推断出哪个程序是正确的，可以用于调优解析器。作为第一个案例研究，我们招募了人类非程序员使用APEL重新注释SPIDER，一个文本到SQL的数据集。我们的方法实现了与原始专家注释者相同的注释准确度（75%），并揭示了原始注释中许多微小的错误。

    Can non-programmers annotate natural language utterances with complex programs that represent their meaning? We introduce APEL, a framework in which non-programmers select among candidate programs generated by a seed semantic parser (e.g., Codex). Since they cannot understand the candidate programs, we ask them to select indirectly by examining the programs' input-ouput examples. For each utterance, APEL actively searches for a simple input on which the candidate programs tend to produce different outputs. It then asks the non-programmers only to choose the appropriate output, thus allowing us to infer which program is correct and could be used to fine-tune the parser. As a first case study, we recruited human non-programmers to use APEL to re-annotate SPIDER, a text-to-SQL dataset. Our approach achieved the same annotation accuracy as the original expert annotators (75%) and exposed many subtle errors in the original annotations.
    
[^183]: 带有未登录词种子的主题发现

    Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds. (arXiv:2205.01845v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2205.01845](http://arxiv.org/abs/2205.01845)

    本文提出了一种带有未登录词种子的主题发现方法，将预训练语言模型和来自输入语料库的局部语义相结合，实验证明了该方法在主题连贯性、准确性和多样性方面的有效性。

    

    多年来，从文本语料库中发现潜在主题一直是研究的课题。许多现有的主题模型采用完全无监督的设置，由于它们无法利用用户指导，所以它们发现的主题可能不符合用户的特定兴趣。虽然存在利用用户提供的种子词来发现主题代表词的种子引导主题发现方法，但它们较少关注两个因素：(1)未登录词种子的存在和(2)预训练语言模型的能力。在本文中，我们将种子引导主题发现的任务推广到允许未登录词种子。我们提出了一个新的框架，名为SeeTopic，在其中PLM的通用知识和从输入语料库中学习的局部语义可以相互受益。在来自不同领域的三个真实数据集上的实验证明了SeeTopic在主题连贯性、准确性和多样性方面的有效性。

    Discovering latent topics from text corpora has been studied for decades. Many existing topic models adopt a fully unsupervised setting, and their discovered topics may not cater to users' particular interests due to their inability of leveraging user guidance. Although there exist seed-guided topic discovery approaches that leverage user-provided seeds to discover topic-representative terms, they are less concerned with two factors: (1) the existence of out-of-vocabulary seeds and (2) the power of pre-trained language models (PLMs). In this paper, we generalize the task of seed-guided topic discovery to allow out-of-vocabulary seeds. We propose a novel framework, named SeeTopic, wherein the general knowledge of PLMs and the local semantics learned from the input corpus can mutually benefit each other. Experiments on three real datasets from different domains demonstrate the effectiveness of SeeTopic in terms of topic coherence, accuracy, and diversity.
    
[^184]: EVA2.0：使用大规模预训练研究开放领域的中文对话系统

    EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training. (arXiv:2203.09313v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.09313](http://arxiv.org/abs/2203.09313)

    本论文研究了开放领域中文对话系统的关键因素，包括数据质量控制、模型架构设计、训练方法和解码策略，并提出了具有28亿参数的EVA2.0模型，其在自动和人工评估中表现优秀，还讨论了该研究的局限性和未来的研究方向。

    

    大规模预训练在构建开放领域的对话系统方面表现出色。然而，之前的研究主要关注发布的对话模型的对话性能展示和评估，忽视了一些关键因素对于强大的类人聊天机器人特别在中文场景下的影响。本文进行了大量实验证明这些未曾探索的因素，包括数据质量控制、模型架构设计、训练方法和解码策略。我们提出了EVA2.0，一个具有28亿参数的大规模预训练开放领域中文对话模型，并将我们的模型和代码公开可用。自动和人工评估表明，EVA2.0明显优于其他开源对应模型。我们还通过展示一些失败案例讨论了该研究的局限性，并提出了大规模中文开放领域对话系统的未来研究方向。

    Large-scale pre-training has shown remarkable performance in building open-domain dialogue systems. However, previous works mainly focus on showing and evaluating the conversational performance of the released dialogue model, ignoring the discussion of some key factors towards a powerful human-like chatbot, especially in Chinese scenarios. In this paper, we conduct extensive experiments to investigate these under-explored factors, including data quality control, model architecture designs, training approaches, and decoding strategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese dialogue model with 2.8 billion parameters, and will make our models and codes publicly available. Automatic and human evaluations show that EVA2.0 significantly outperforms other open-source counterparts. We also discuss the limitations of this work by presenting some failure cases and pose some future research directions on large-scale Chinese open-domain dialogue systems.
    
[^185]: 基于元数据引导的对比学习方法用于零样本多标签文本分类

    Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification. (arXiv:2202.05932v2 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2202.05932](http://arxiv.org/abs/2202.05932)

    本文研究了零样本情况下的大规模多标签文本分类，提出了一种基于元数据引导的对比学习方法（MICoL）。实验结果表明该方法在两个大规模数据集上取得了良好的效果。

    

    大规模多标签文本分类旨在将文档与其相关标签关联起来，并从一个大的候选集中选择标签。大多数现有的大规模多标签文本分类方法依赖于大量的人工标注训练数据，这些数据往往难以获取，并且在训练集中存在长尾标签分布（即许多标签只出现几次）。本文研究零样本情况下的大规模多标签文本分类，这不需要任何带有标签的注释文档，只依赖于标签的表面名称和描述。为了训练一个计算文档与标签之间相似度得分的分类器，我们提出了一种新颖的元数据引导的对比学习（MICoL）方法。与之前基于文本的对比学习技术不同，MICoL利用了广泛可用于Web上的文档元数据（例如作者，会议，研究论文的引用）来推导出相似的文档对。在两个大规模数据集上的实验结果表明：（1）

    Large-scale multi-label text classification (LMTC) aims to associate a document with its relevant labels from a large candidate set. Most existing LMTC approaches rely on massive human-annotated training data, which are often costly to obtain and suffer from a long-tailed label distribution (i.e., many labels occur only a few times in the training set). In this paper, we study LMTC under the zero-shot setting, which does not require any annotated documents with labels and only relies on label surface names and descriptions. To train a classifier that calculates the similarity score between a document and a label, we propose a novel metadata-induced contrastive learning (MICoL) method. Different from previous text-based contrastive learning techniques, MICoL exploits document metadata (e.g., authors, venues, and references of research papers), which are widely available on the Web, to derive similar document-document pairs. Experimental results on two large-scale datasets show that: (1)
    
[^186]: 展示、写作和检索: 实体感知的文章生成和检索

    Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval. (arXiv:2112.05917v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.05917](http://arxiv.org/abs/2112.05917)

    该论文提出了一个名为ENGINE的框架，用于实体感知的文章生成和检索。框架通过命名实体提取模块和实体感知机制，将命名实体明确地融入到语言模型中，以提高模型理解和生成文章的能力。

    

    文章理解是自然语言处理中的一个重要挑战，具有许多应用，例如文章生成或图像到文章的检索。以往的工作通常使用预训练语言模型对文章中的所有标记进行统一编码。然而，在许多应用中，如理解新闻报道，这些文章都基于现实世界的事件，可能引用了许多难以准确识别和预测的命名实体。为了解决这个挑战，我们提出了一个ENtity-aware article GeneratIoN and rEtrieval (ENGINE)框架，将命名实体明确地融入到语言模型中。ENGINE框架有两个主要组成部分：一个命名实体提取模块，用于从文章的元数据和嵌入图像中提取命名实体；一个实体感知机制，增强模型识别和预测实体名称的能力。我们在三个公共数据集GoodNews、VisualNews和WikiText上进行了实验。

    Article comprehension is an important challenge in natural language processing with many applications such as article generation or image-to-article retrieval. Prior work typically encodes all tokens in articles uniformly using pretrained language models. However, in many applications, such as understanding news stories, these articles are based on real-world events and may reference many named entities that are difficult to accurately recognize and predict by language models. To address this challenge, we propose an ENtity-aware article GeneratIoN and rEtrieval (ENGINE) framework, to explicitly incorporate named entities into language models. ENGINE has two main components: a named-entity extraction module to extract named entities from both metadata and embedded images associated with articles, and an entity-aware mechanism that enhances the model's ability to recognize and predict entity names. We conducted experiments on three public datasets: GoodNews, VisualNews, and WikiText, wh
    
[^187]: RATE: 克服实时位置估计中文本特征的噪声和稀疏性

    RATE: Overcoming Noise and Sparsity of Textual Features in Real-Time Location Estimation. (arXiv:2111.06515v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2111.06515](http://arxiv.org/abs/2111.06515)

    本文提出了RATE算法，通过使用主题建模和整合其他特征，成功克服了实时位置估计中的文本特征噪声和稀疏性问题，并在实验中展现出优于其他基准方法的性能。

    

    社交媒体用户的实时位置推断是一些空间应用（如本地搜索和事件检测）的基础。虽然推文文本是位置估计中最常用的特征，但大多数之前的工作都受到文本特征噪声或稀疏性的影响。本文旨在解决这两个问题。我们使用主题建模作为构建模块，以表征地理主题变化和词汇变化，从而不再直接使用“one-hot”编码向量。我们还结合通过Twitter流API提取的其他特征来克服噪声问题。实验结果表明，我们的RATE算法在地区分类的准确性和纬度经度回归的平均距离误差方面优于几种基准方法。

    Real-time location inference of social media users is the fundamental of some spatial applications such as localized search and event detection. While tweet text is the most commonly used feature in location estimation, most of the prior works suffer from either the noise or the sparsity of textual features. In this paper, we aim to tackle these two problems. We use topic modeling as a building block to characterize the geographic topic variation and lexical variation so that "one-hot" encoding vectors will no longer be directly used. We also incorporate other features which can be extracted through the Twitter streaming API to overcome the noise problem. Experimental results show that our RATE algorithm outperforms several benchmark methods, both in the precision of region classification and the mean distance error of latitude and longitude regression.
    
[^188]: MotifClass: 基于高阶元数据信息的弱监督文本分类

    MotifClass: Weakly Supervised Text Classification with Higher-order Metadata Information. (arXiv:2111.04022v3 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2111.04022](http://arxiv.org/abs/2111.04022)

    本文提出了一个名为MotifClass的框架，基于高阶元数据信息来进行弱监督的文本分类问题。通过建模文档和元数据之间的关系，利用图案描述元数据组合以捕捉高阶结构，并选择具有类别指示意义的图案实例进行分类。

    

    本文研究弱监督文本分类问题，旨在将文本文档分类为一组预定义的类别，仅使用类别表面名称，而无需提供任何标注的训练文档。目前大部分现有分类器利用每个文档中的文本信息。然而，在许多领域中，文档附带各种类型的元数据（例如作者、出处和研究论文的年份）。这些元数据及其组合可能作为强有力的类别指标，用于辅助文本分类。本文通过异构信息网络建模文档和元数据之间的关系，并通过图案描述元数据组合以有效捕捉网络中的高阶结构。我们提出了一个名为MotifClass的新框架，该框架（1）选择具有类别指示意义的图案实例，（2）检索和生成元数据的表示以进行文本分类。

    We study the problem of weakly supervised text classification, which aims to classify text documents into a set of pre-defined categories with category surface names only and without any annotated training document provided. Most existing classifiers leverage textual information in each document. However, in many domains, documents are accompanied by various types of metadata (e.g., authors, venue, and year of a research paper). These metadata and their combinations may serve as strong category indicators in addition to textual contents. In this paper, we explore the potential of using metadata to help weakly supervised text classification. To be specific, we model the relationships between documents and metadata via a heterogeneous information network. To effectively capture higher-order structures in the network, we use motifs to describe metadata combinations. We propose a novel framework, named MotifClass, which (1) selects category-indicative motif instances, (2) retrieves and gen
    
[^189]: 复合概率上下文无关文法的实证研究

    An Empirical Study of Compound PCFGs. (arXiv:2103.02298v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2103.02298](http://arxiv.org/abs/2103.02298)

    C-PCFGs在无监督短语结构语法归纳方面取得了新的技术水平。数据显示，C-PCFGs数据利用效率高，能适应不同句子长度，并且能够充分利用句子级信息。然而，最佳配置不一定适用于形态丰富的语言。

    

    复合概率上下文无关文法 (C-PCFGs) 最近在无监督短语结构语法归纳方面取得了新的技术水平。然而，由于基于图表的表示和推理的空间和时间复杂度较高，对C-PCFGs进行全面的研究存在困难。在这项工作中，我们依赖于一个快速的 C-PCFGs 实现，对\citet{kim-etal-2019-compound}的评估进行了补充。我们首先对英文树库进行了分析和修剪。我们的研究结果表明：(1)C-PCFGs具有高效利用数据的能力，在处理未见过的句子/成分长度时也能进行泛化；(2)C-PCFGs在生成终结符规则概率时充分利用了句子级信息。我们还对C-PCFGs进行了多语言评估。实验结果显示，经过在英文上调试的C-PCFGs的最佳配置并不总能适用于形态丰富的语言。

    Compound probabilistic context-free grammars (C-PCFGs) have recently established a new state of the art for unsupervised phrase-structure grammar induction. However, due to the high space and time complexities of chart-based representation and inference, it is difficult to investigate C-PCFGs comprehensively. In this work, we rely on a fast implementation of C-PCFGs to conduct an evaluation complementary to that of~\citet{kim-etal-2019-compound}. We start by analyzing and ablating C-PCFGs on English treebanks. Our findings suggest that (1) C-PCFGs are data-efficient and can generalize to unseen sentence/constituent lengths; and (2) C-PCFGs make the best use of sentence-level information in generating preterminal rule probabilities. We further conduct a multilingual evaluation of C-PCFGs. The experimental results show that the best configurations of C-PCFGs, which are tuned on English, do not always generalize to morphology-rich languages.
    
[^190]: MATCH: 元数据感知的大型层次文本分类

    MATCH: Metadata-Aware Text Classification in A Large Hierarchy. (arXiv:2102.07349v2 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2102.07349](http://arxiv.org/abs/2102.07349)

    MATCH是一个元数据感知的大型层次文本分类解决方案，利用了元数据和层次信息。它在预训练阶段将文本和元数据嵌入到同一空间，并利用全连接注意力捕捉它们之间的关系，同时通过不同的正则化方式利用标签层次结构。

    

    多标签文本分类是指将每个给定的文档分配给与标签集最相关的标签的问题。在现实世界的应用中，通常提供给定文档的元数据和标签的层次结构。然而，大多数现有研究只关注建模文本信息，对于利用元数据或层次结构信号的尝试较少，并非两者兼顾。在本文中，我们通过规范化元数据感知的大型标签层次结构下的文本分类问题（例如，具有数万个标签），填补了这一差距。为了解决这个问题，我们提出了MATCH解决方案——一个端到端框架，利用了元数据和层次信息。为了结合元数据，我们在相同空间中预训练文本和元数据的嵌入，并利用全连接注意力来捕捉它们之间的相互关系。为了利用标签层次结构，我们提出了不同的方式来正则化参数和...

    Multi-label text classification refers to the problem of assigning each given document its most relevant labels from the label set. Commonly, the metadata of the given documents and the hierarchy of the labels are available in real-world applications. However, most existing studies focus on only modeling the text information, with a few attempts to utilize either metadata or hierarchy signals, but not both of them. In this paper, we bridge the gap by formalizing the problem of metadata-aware text classification in a large label hierarchy (e.g., with tens of thousands of labels). To address this problem, we present the MATCH solution -- an end-to-end framework that leverages both metadata and hierarchy information. To incorporate metadata, we pre-train the embeddings of text and metadata in the same space and also leverage the fully-connected attentions to capture the interrelations between them. To leverage the label hierarchy, we propose different ways to regularize the parameters and
    
[^191]: 分层元数据感知的弱监督下文档分类

    Hierarchical Metadata-Aware Document Categorization under Weak Supervision. (arXiv:2010.13556v2 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2010.13556](http://arxiv.org/abs/2010.13556)

    本论文研究了如何在弱监督下将标签层次、元数据和文本信号整合起来进行文档分类，并提出了HiMeCat框架，该框架可以同时建模类别依赖关系、元数据信息和文本语义，实现了在只有少量训练样本和元数据信息的情况下进行高效文档分类。

    

    由于海量文本语料库中普遍存在层次化主题结构，将文档分类到给定的标签层次结构中具有直观吸引力。尽管相关研究在完全监督的分层文档分类中取得了令人满意的性能，但它们通常需要大量的人工标注训练数据，并且只利用文本信息。然而，在许多领域中，（1）标注非常昂贵，难以获取到很少的训练样本；（2）文档附带元数据信息。因此，本文研究了如何在弱监督下整合标签层次、元数据和文本信号进行文档分类。我们开发了HiMeCat，一个基于嵌入式生成框架，用于我们的任务。具体来说，我们提出了一种新颖的联合表示学习模块，可以同时建模类别依赖关系、元数据信息和文本语义，并引入了一种数据增强模块，用于分层合成。

    Categorizing documents into a given label hierarchy is intuitively appealing due to the ubiquity of hierarchical topic structures in massive text corpora. Although related studies have achieved satisfying performance in fully supervised hierarchical document classification, they usually require massive human-annotated training data and only utilize text information. However, in many domains, (1) annotations are quite expensive where very few training samples can be acquired; (2) documents are accompanied by metadata information. Hence, this paper studies how to integrate the label hierarchy, metadata, and text signals for document categorization under weak supervision. We develop HiMeCat, an embedding-based generative framework for our task. Specifically, we propose a novel joint representation learning module that allows simultaneous modeling of category dependencies, metadata information and textual semantics, and we introduce a data augmentation module that hierarchically synthesize
    
[^192]: 文本与元数据的最小监督分类

    Minimally Supervised Categorization of Text with Metadata. (arXiv:2005.00624v3 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2005.00624](http://arxiv.org/abs/2005.00624)

    本论文提出了MetaCat，一种使用元数据进行最小监督文本分类的框架。通过将生成模型应用于文本和元数据之间的关系，实现了在标签稀缺情况下的高效分类。

    

    文档分类是将主题标签分配给每个文档的基本任务，在各种应用程序中起着重要作用。尽管现有的传统监督文档分类研究取得了成功，但它们较少关注两个实际问题：（1）存在元数据：在许多领域，文本伴随着各种附加信息，例如作者和标签。这些元数据作为有力的主题指示器，应该被利用到分类框架中；（2）标签稀缺：在某些情况下，获得有标签的训练样本是昂贵的，需要只使用少量注释数据进行分类。鉴于这两个挑战，我们提出了MetaCat，一种使用元数据进行最小监督文本分类的框架。具体而言，我们开发了一个生成模型来描述单词、文档、标签和元数据之间的关系。根据生成模型的指导，我们将文本和元数据嵌入到分类框架中。

    Document categorization, which aims to assign a topic label to each document, plays a fundamental role in a wide variety of applications. Despite the success of existing studies in conventional supervised document classification, they are less concerned with two real problems: (1) the presence of metadata: in many domains, text is accompanied by various additional information such as authors and tags. Such metadata serve as compelling topic indicators and should be leveraged into the categorization framework; (2) label scarcity: labeled training samples are expensive to obtain in some cases, where categorization needs to be performed using only a small set of annotated data. In recognition of these two challenges, we propose MetaCat, a minimally supervised framework to categorize text with metadata. Specifically, we develop a generative process describing the relationships between words, documents, labels, and metadata. Guided by the generative model, we embed text and metadata into th
    
[^193]: 文本作为环境:一种深度强化学习的文本可读性评估模型

    Text as Environment: A Deep Reinforcement Learning Text Readability Assessment Model. (arXiv:1912.05957v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/1912.05957](http://arxiv.org/abs/1912.05957)

    这是一种使用深度强化学习模型评估文本可读性的方法，通过使用硬注意力的主动推理技术和半监督信号来提高效率，并与其他先进模型进行比较。

    

    评估文本的可读性可以显著促进信息的准确表达。文本可读性评估的制定涉及对文本的有意义的属性进行识别，而不论其长度。为了准确评估文本的可理解性，使用了复杂的特征和模型。尽管如此，高效评估文本可读性的问题相对较少研究。通过使用硬注意力的主动推理技术，提出的方法更有效地利用输入文本和计算资源。通过使用半监督信号，强化学习模型可以使用最少的文本来确定文本的可读性。将该模型与Weebit和剑桥考试等最先进的模型进行比较。

    Evaluating the readability of a text can significantly facilitate the precise expression of information in written form. The formulation of text readability assessment involves the identification of meaningful properties of the text regardless of its length. Sophisticated features and models are used to evaluate the comprehensibility of texts accurately. Despite this, the problem of assessing texts' readability efficiently remains relatively untouched. The efficiency of state-of-the-art text readability assessment models can be further improved using deep reinforcement learning models. Using a hard attention-based active inference technique, the proposed approach makes efficient use of input text and computational resources. Through the use of semi-supervised signals, the reinforcement learning model uses the minimum amount of text in order to determine text's readability. A comparison of the model on Weebit and Cambridge Exams with state-of-the-art models, such as the BERT text readab
    
[^194]: HiGitClass: 基于关键词的GitHub仓库的分层分类

    HiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories. (arXiv:1910.07115v2 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/1910.07115](http://arxiv.org/abs/1910.07115)

    HiGitClass是基于关键词的GitHub仓库的分层分类框架，用户只需提供带有关键词的标签层次结构作为监督。框架解决了多模式信号、监督稀缺性和监督格式不匹配等主要挑战。

    

    GitHub已经成为一个重要的代码分享和科学交流平台。由于可用的仓库数量庞大，需要基于主题进行搜索。尽管引入了主题标签功能，但大多数GitHub仓库都没有任何标签，限制了搜索和基于主题的分析的效用。本研究将自动仓库分类问题定位为基于关键词的分层分类。具体而言，用户只需提供带有关键词的标签层次结构作为监督。这种设置具有灵活性，能适应用户的需求，考虑到主题标签的不同粒度，并且需要较少的人力投入。我们确定了这个问题的三个关键挑战，即（1）多模式信号的存在；（2）监督稀缺性和偏见；（3）监督格式不匹配。鉴于这些挑战，我们提出了HiGitClass框架，包括三个模块：heterogeneou

    GitHub has become an important platform for code sharing and scientific exchange. With the massive number of repositories available, there is a pressing need for topic-based search. Even though the topic label functionality has been introduced, the majority of GitHub repositories do not have any labels, impeding the utility of search and topic-based analysis. This work targets the automatic repository classification problem as keyword-driven hierarchical classification. Specifically, users only need to provide a label hierarchy with keywords to supply as supervision. This setting is flexible, adaptive to the users' needs, accounts for the different granularity of topic labels and requires minimal human effort. We identify three key challenges of this problem, namely (1) the presence of multi-modal signals; (2) supervision scarcity and bias; (3) supervision format mismatch. In recognition of these challenges, we propose the HiGitClass framework, comprising of three modules: heterogeneou
    

