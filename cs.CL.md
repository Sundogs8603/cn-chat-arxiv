# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [NLP Verification: Towards a General Methodology for Certifying Robustness](https://arxiv.org/abs/2403.10144) | 本文尝试总结和评估由该领域迄今进展而形成的NLP验证流程的一般组成部分，贡献在于提出了将句子嵌入连续空间得到的可验证子空间的一般描述。 |
| [^2] | [MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder](https://arxiv.org/abs/2403.04626) | MedFLIP是一种用于医学分析的快速语言-图像预训练方法，通过引入SVD损失增强医学图像特征表示学习，验证了用语言可以提高零样本医学图像分析的性能。 |
| [^3] | [Learning to Use Tools via Cooperative and Interactive Agents](https://arxiv.org/abs/2403.03031) | 提出了一种ConAgents框架，通过合作和互动代理使大型语言模型能够学习使用工具，并引入了迭代校准方法，以解决单个代理执行多样化操作能力有限和自适应纠正错误困难的问题。 |
| [^4] | [Greed is All You Need: An Evaluation of Tokenizer Inference Methods](https://arxiv.org/abs/2403.01289) | 对 NLP 模型中常用的分词器进行了控制性分析，发现贪婪推理表现良好，而上下文感知分词器 SaGe 在形态对齐方面表现最优。 |
| [^5] | [Extreme Miscalibration and the Illusion of Adversarial Robustness](https://arxiv.org/abs/2402.17509) | 深度学习模型的对抗训练可能会造成对抗性强化学习的幻觉，研究表明通过测试时间温度缩放可以消除这种幻觉。 |
| [^6] | [Quantum linear algebra is all you need for Transformer architectures](https://arxiv.org/abs/2402.16714) | 本文研究了在容错性量子计算的视角下Transformer架构，展示了如何利用量子线性代数构建Transformer的关键组件。 |
| [^7] | [Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models](https://arxiv.org/abs/2402.15938) | 本文提出了一种通过LLMs输出分布进行污染检测的方法CDD，以及一种基于LLMs输出修正的可信评估方法TED，以应对大语言模型在数据污染和可信评估方面面临的挑战。 |
| [^8] | [Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement](https://arxiv.org/abs/2402.12146) | Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。 |
| [^9] | [II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering](https://arxiv.org/abs/2402.11058) | II-MMR提出了一种新颖的方法，用于识别和改进视觉问答中的多模态多跳推理，通过引入答案预测引导的CoT提示和知识三元组引导的提示，实现对不同推理情况的分析和识别。 |
| [^10] | [Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows](https://arxiv.org/abs/2402.09894) | 这项纵向研究调查了生成式AI工作流程的实用性和定制化程度，结果显示，在熟悉化阶段后，用户感知到的系统效用提高了。 |
| [^11] | [Best Arm Identification for Prompt Learning under a Limited Budget](https://arxiv.org/abs/2402.09723) | 这项工作提出了一种在提示学习中考虑有限预算约束的方法，通过建立提示学习和多臂赌博机中固定预算最佳臂识别之间的联系，提出了一个通用框架TRIPLE，通过利用聚类和嵌入思想实现了两个增强方法。 |
| [^12] | [API Pack: A Massive Multilingual Dataset for API Call Generation](https://arxiv.org/abs/2402.09615) | 这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成 |
| [^13] | [SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages](https://arxiv.org/abs/2402.08638) | SemRel2024是一个包含来自14种语言的语义文本相关性数据集合，通过该数据集合可以探索和量化语义相关性。这对于大型语言模型的能力和性能有重要的影响。这个数据集合涵盖了南非荷兰语、阿尔及利亚阿拉伯语、阿姆哈拉语、英语、豪萨语、印地语、印度尼西亚语、卢旺达语、马拉地语、摩洛哥阿拉伯语、现代标准阿拉伯语、旁遮普语、西班牙语和泰卢固语等14种语言。 |
| [^14] | [A Tale of Tails: Model Collapse as a Change of Scaling Laws](https://arxiv.org/abs/2402.07043) | 本文通过尺度律的视角，研究了AI模型大小增长和合成数据引入对模型性能的影响，发现模型可能会遭遇总体崩溃，并通过大规模实验证实了这一理论。 |
| [^15] | [Online Cascade Learning for Efficient Inference over Streams](https://arxiv.org/abs/2402.04513) | 这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。 |
| [^16] | [Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims](https://arxiv.org/abs/2402.03962) | 这篇论文讨论了在寻求人工通用智能的过程中，人们对于大型语言模型的类人特质过度归因的现象，并呼吁学术界在解读和传播关于AI研究的内容时要保持谨慎。 |
| [^17] | [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299) | 本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。 |
| [^18] | [One Size Does Not Fit All: Customizing Open-Domain Procedures](https://arxiv.org/abs/2311.09510) | 测试了多种简单的多LLM代理体系结构，发现两个LLM代理按顺序使用的简单结构表现最佳，一个编辑通用操作流程，另一个验证可执行性，在定制操作流程方面取得了显著效果，有望进一步探索多代理编辑体系结构的价值。 |
| [^19] | [Learning to Learn for Few-shot Continual Active Learning](https://arxiv.org/abs/2311.03732) | 提出了一种简单而高效的方法，Meta-Continual Active Learning，通过元学习和经验重播解决少样本持续主动学习中的任务混淆和灾难性遗忘，进一步结合文本增强来确保泛化。 |
| [^20] | [Mining experimental data from Materials Science literature with Large Language Models.](http://arxiv.org/abs/2401.11052) | 本研究评估了使用大型语言模型从材料科学文献中提取结构化信息的能力，并引入了一种新颖的方法来处理材料科学信息的复杂性。在命名实体识别和关系提取任务上，LLMs与传统模型相比表现有限，但在少数-shot提示下有一定的改进。 |
| [^21] | [Improving Text Embeddings with Large Language Models.](http://arxiv.org/abs/2401.00368) | 本文介绍了一种使用只用合成数据和少量训练步骤获取高质量文本嵌入的简单方法，并且在没有使用标记数据的情况下，在竞争激烈的文本嵌入基准上取得了强大的性能。 |
| [^22] | [The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation.](http://arxiv.org/abs/2312.09085) | 本研究研究了LLMs对误导信息的易受攻击性，特别是在说服性对话中。通过实验证明，LLMs在事实知识上的正确信念很容易被各种说服策略所操纵。 |
| [^23] | [Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers.](http://arxiv.org/abs/2310.02905) | 该论文提出了一种使用神经探测器和转换器优化指令的方法，以提高大型语言模型的性能。 |
| [^24] | [TRAM: Benchmarking Temporal Reasoning for Large Language Models.](http://arxiv.org/abs/2310.00835) | TRAM是一个用于评估大型语言模型的时间推理能力的基准评估。我们介绍了由十个数据集组成的TRAM基准评估，涵盖了事件的各种时间方面。尽管使用流行的语言模型进行广泛评估，结果表明这些模型在时间推理任务上仍然落后于人类。 |
| [^25] | [A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models.](http://arxiv.org/abs/2309.02691) | 这项研究提出了一个框架来研究视觉和语言模型中短语定位和任务性能之间的关系，并且通过验证实验发现了当代模型在短语定位和任务求解方面的不一致性。 |
| [^26] | [Learning to Model the World with Language.](http://arxiv.org/abs/2308.01399) | 本论文提出了一种通过语言学习对世界进行建模的方法，利用语言帮助代理器预测未来并进行行动。通过学习多模态世界模型，代理器可以预测未来的文本和图像表示，并在模型回滚中进行行动。 |
| [^27] | [Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers.](http://arxiv.org/abs/2305.15805) | 本研究提出了一种动态上下文剪枝方法，可以在保持模型表现力的同时，动态减少无效信息，提高模型的效率和可解释性。该技术可以应用于现有的预训练模型，并且可以通过简单的微调过程实现。 |
| [^28] | [LMs with a Voice: Spoken Language Modeling beyond Speech Tokens.](http://arxiv.org/abs/2305.15255) | SPECTRON是一个新颖的语音延续模型，通过利用预训练的语言模型和语音编码器进行端到端的训练来生成文本和语音输出，在语义内容和讲话者保护方面超越了现有的口语语言模型。 |

# 详细

[^1]: NLP验证：走向一种通用的用于认证鲁棒性的方法论

    NLP Verification: Towards a General Methodology for Certifying Robustness

    [https://arxiv.org/abs/2403.10144](https://arxiv.org/abs/2403.10144)

    本文尝试总结和评估由该领域迄今进展而形成的NLP验证流程的一般组成部分，贡献在于提出了将句子嵌入连续空间得到的可验证子空间的一般描述。

    

    深度神经网络在自然语言处理（NLP）领域取得了显著成功，确保它们的安全性和可靠性至关重要：在安全关键的情境中，这些模型必须对变化或攻击具有鲁棒性，并能对其输出给出保证。与计算机视觉不同，NLP缺乏一个统一的验证方法论，尽管近年来文献中取得了一些进展，但对于NLP验证的实用问题常常涉及不深。在本文中，我们尝试提炼和评估一个NLP验证流程的一般组成部分，该流程来源于迄今为止该领域的进展。我们的贡献有两方面：首先，我们给出了将句子嵌入连续空间得到的可验证子空间的一般描述。我们确定了可验证子空间的语义泛化技术挑战，并提出了一种有效处理的方法。

    arXiv:2403.10144v1 Announce Type: cross  Abstract: Deep neural networks have exhibited substantial success in the field of Natural Language Processing (NLP) and ensuring their safety and reliability is crucial: there are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Unlike Computer Vision, NLP lacks a unified verification methodology and, despite recent advancements in literature, they are often light on the pragmatical issues of NLP verification. In this paper, we make an attempt to distil and evaluate general components of an NLP verification pipeline, that emerges from the progress in the field to date. Our contributions are two-fold. Firstly, we give a general characterisation of verifiable subspaces that result from embedding sentences into continuous spaces. We identify, and give an effective method to deal with, the technical challenge of semantic generalisability of verified subspaces; and propose it a
    
[^2]: MedFLIP：医学视觉与语言自监督快速预训练与掩蔽自编码器

    MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder

    [https://arxiv.org/abs/2403.04626](https://arxiv.org/abs/2403.04626)

    MedFLIP是一种用于医学分析的快速语言-图像预训练方法，通过引入SVD损失增强医学图像特征表示学习，验证了用语言可以提高零样本医学图像分析的性能。

    

    在医学分析领域，广泛的研究探讨了掩蔽自编码器（MAEs）和多模态数据之间互相学习的潜力。然而，MAEs对跨模态学习的影响仍然是一个关键挑战。我们引入了MedFLIP，一种用于医学分析的快速语言-图像预训练方法。我们探索使用MAEs进行跨领域零样本学习，从而增强模型在医学诊断中常见的有限数据中学习的能力。我们验证了对图像进行掩蔽不会影响跨模态学习。此外，我们提出了SVD损失以增强医学图像特征的表示学习，旨在通过利用这类数据的结构复杂性来提高分类准确性。最后，我们验证了使用语言将提高医学图像分析的零样本性能。MedFLIP对掩蔽过程的扩展标志着该领域的进步。

    arXiv:2403.04626v1 Announce Type: cross  Abstract: Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the zero-shot performance for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering 
    
[^3]: 通过合作和互动代理学习使用工具

    Learning to Use Tools via Cooperative and Interactive Agents

    [https://arxiv.org/abs/2403.03031](https://arxiv.org/abs/2403.03031)

    提出了一种ConAgents框架，通过合作和互动代理使大型语言模型能够学习使用工具，并引入了迭代校准方法，以解决单个代理执行多样化操作能力有限和自适应纠正错误困难的问题。

    

    工具学习使大型语言模型（LLMs）作为代理人能够使用外部工具来扩展其功能。现有方法利用单个基于LLM的代理循环选择和执行工具，然后将结果合并到下一个动作预测中。然而，它们在处理复杂任务时仍然存在潜在的性能下降问题，原因是：（1）单个LLM的固有能力执行多样化操作受限，以及（2）在任务失败时难以自适应地纠正错误。为了缓解这些问题，我们提出了ConAgents，即合作和互动代理框架，将工具学习的工作流模块化为Grounding（基础）、Execution（执行）和Observing（观察）代理。我们还介绍了一种迭代校准（IterCali）方法，使代理能够根据来自工具环境的反馈对自己进行调整。在三个数据集上进行的实验证明了超过

    arXiv:2403.03031v1 Announce Type: new  Abstract: Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the super
    
[^4]: 贪婪是你所需要的一切：对分词推理方法的评估

    Greed is All You Need: An Evaluation of Tokenizer Inference Methods

    [https://arxiv.org/abs/2403.01289](https://arxiv.org/abs/2403.01289)

    对 NLP 模型中常用的分词器进行了控制性分析，发现贪婪推理表现良好，而上下文感知分词器 SaGe 在形态对齐方面表现最优。

    

    虽然 BPE 和 WordPiece 这样的子词分词器通常用于构建 NLP 模型的词汇表，但是将文本解码为这些词汇表中的一系列标记的方法通常未指定，或者不适合它们构建的方法。我们对四种不同算法和三种词汇量大小之间的七种分词器推理方法进行了控制性分析，我们在英语上为此构建了一个新颖的内在评估套件，结合了基于形态学、认知和信息理论的度量。我们展示了对于最常用的分词器，贪婪推理表现出人意料地良好；最近引入的上下文感知分词器 SaGe 在形态对齐方面优于所有其他方法。

    arXiv:2403.01289v1 Announce Type: new  Abstract: While subword tokenizers such as BPE and WordPiece are typically used to build vocabularies for NLP models, the method of decoding text into a sequence of tokens from these vocabularies is often left unspecified, or ill-suited to the method in which they were constructed. We provide a controlled analysis of seven tokenizer inference methods across four different algorithms and three vocabulary sizes, performed on a novel intrinsic evaluation suite we curated for English, combining measures rooted in morphology, cognition, and information theory. We show that for the most commonly used tokenizers, greedy inference performs surprisingly well; and that SaGe, a recently-introduced contextually-informed tokenizer, outperforms all others on morphological alignment.
    
[^5]: 极端失调与对抗鲁棒性的幻觉

    Extreme Miscalibration and the Illusion of Adversarial Robustness

    [https://arxiv.org/abs/2402.17509](https://arxiv.org/abs/2402.17509)

    深度学习模型的对抗训练可能会造成对抗性强化学习的幻觉，研究表明通过测试时间温度缩放可以消除这种幻觉。

    

    基于深度学习的自然语言处理（NLP）模型容易受到对抗攻击的影响，微小的扰动可能导致模型误分类。对抗训练（AT）经常被用来提升模型的鲁棒性。然而，我们发现了一个有趣的现象：有意或无意地失调模型会掩盖梯度，从而干扰对抗攻击搜索方法，导致表面上看似增加了鲁棒性。我们展示了这种观察到的鲁棒性增益是一种鲁棒性幻觉（IOR），并展示了对手如何执行各种形式的测试时间温度校准来抵消上述干扰，使对抗攻击能够找到对抗样本。因此，我们敦促NLP社区在其鲁棒性评估中纳入测试时间温度缩放，以确保观察到的任何增益都是真实的。

    arXiv:2402.17509v1 Announce Type: new  Abstract: Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify. Adversarial Training (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples. Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can 
    
[^6]: 量子线性代数是Transformer架构所需的一切

    Quantum linear algebra is all you need for Transformer architectures

    [https://arxiv.org/abs/2402.16714](https://arxiv.org/abs/2402.16714)

    本文研究了在容错性量子计算的视角下Transformer架构，展示了如何利用量子线性代数构建Transformer的关键组件。

    

    生成式机器学习方法如大型语言模型正在彻底改变文本和图像的创作。本文通过容错性量子计算的视角研究了Transformer架构。我们展示了如何准备self-attention矩阵的块编码，并结合量子子程序构建了Transformer中的重要组成部分。

    arXiv:2402.16714v1 Announce Type: cross  Abstract: Generative machine learning methods such as large-language models are revolutionizing the creation of text and images. While these models are powerful they also harness a large amount of computational resources. The transformer is a key component in large language models that aims to generate a suitable completion of a given partial sequence. In this work, we investigate transformer architectures under the lens of fault-tolerant quantum computing. The input model is one where pre-trained weight matrices are given as block encodings to construct the query, key, and value matrices for the transformer. As a first step, we show how to prepare a block encoding of the self-attention matrix, with a row-wise application of the softmax function using the Hadamard product. In addition, we combine quantum subroutines to construct important building blocks in the transformer, the residual connection, layer normalization, and the feed-forward neura
    
[^7]: 大语言模型的泛化或记忆：数据污染与可信评估

    Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models

    [https://arxiv.org/abs/2402.15938](https://arxiv.org/abs/2402.15938)

    本文提出了一种通过LLMs输出分布进行污染检测的方法CDD，以及一种基于LLMs输出修正的可信评估方法TED，以应对大语言模型在数据污染和可信评估方面面临的挑战。

    

    最近关于大语言模型（LLMs）令人印象深刻能力的说法通常是通过在开放获取的基准上进行评估来支持的。考虑到LLMs的训练数据的庞大规模和广泛来源，它可能明确或隐含地包含测试数据，导致LLMs更容易受到数据污染的影响。然而，由于训练数据的不透明性、模型的黑盒访问以及合成训练数据的快速增长，对于LLMs来说检测和减轻数据污染面临着重大挑战。在本文中，我们提出了CDD，即通过LLMs输出分布进行污染检测的CDD。CDD仅需要采样文本来检测数据污染，通过识别LLMs输出分布的峰值来进行检测。为了减轻评估中数据污染的影响，我们还提出了TED：基于LLMs输出修正的可信评估。

    arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
    
[^8]: Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    [https://arxiv.org/abs/2402.12146](https://arxiv.org/abs/2402.12146)

    Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。

    

    尽管大型语言模型（LLMs）在广泛任务中展现强大性能，但仍面临幻觉等可靠性挑战。先前的研究表明，像GPT-4这样高能力的LLMs在评估单个响应的可靠性方面是有效的，而较不具备能力的LLMs通常被调整来评估对相同查询的响应的相对可靠性。为了使较不具备能力的LLMs有效地评估单个响应的可靠性，我们提出了一种名为$\textit{Meta}$ $\textit{Ranking}$（MR）的新方法。与先前直接评估响应的方法不同，我们通过将目标查询-响应对与参考查询-响应对进行比较来实现判断。我们发现在推理任务的LLM响应的错误检测中，MR表现出显著的有效性，即使在没有微调的情况下，较不具备能力的LLMs也能胜过强基线。我们进一步证明MR可以被用

    arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
    
[^9]: II-MMR：在视觉问答中识别和改进多模态多跳推理

    II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering

    [https://arxiv.org/abs/2402.11058](https://arxiv.org/abs/2402.11058)

    II-MMR提出了一种新颖的方法，用于识别和改进视觉问答中的多模态多跳推理，通过引入答案预测引导的CoT提示和知识三元组引导的提示，实现对不同推理情况的分析和识别。

    

    视觉问答（VQA）通常涉及视觉和语言之间多样推理场景。然而，大多数先前的VQA研究仅关注评估模型的整体准确性，而没有在不同推理情况下对其进行评估。此外，一些最近的研究发现，传统的"CoT"提示无法有效生成VQA的推理，尤其是对于需要多跳推理的复杂场景。在本文中，我们提出了II-MMR，这是一个新颖的想法，用于识别和改进VQA中的多模态多跳推理。具体而言，II-MMR接受带有图像的VQA问题，并使用两种新颖的语言提示找到推理路径以获得答案：(i)答案预测引导的CoT提示，或者(ii)知识三元组引导的提示。然后，II-MMR分析这条路径，通过估计有多少跳和什么类型（即视觉或超出）来识别当前VQA基准中的不同推理情况。

    arXiv:2402.11058v1 Announce Type: cross  Abstract: Visual Question Answering (VQA) often involves diverse reasoning scenarios across Vision and Language (V&L). Most prior VQA studies, however, have merely focused on assessing the model's overall accuracy without evaluating it on different reasoning cases. Furthermore, some recent works observe that conventional Chain-of-Thought (CoT) prompting fails to generate effective reasoning for VQA, especially for complex scenarios requiring multi-hop reasoning. In this paper, we propose II-MMR, a novel idea to identify and improve multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA question with an image and finds a reasoning path to reach its answer using two novel language promptings: (i) answer prediction-guided CoT prompt, or (ii) knowledge triplet-guided prompt. II-MMR then analyzes this path to identify different reasoning cases in current VQA benchmarks by estimating how many hops and what types (i.e., visual or beyon
    
[^10]: 不仅仅是新颖性：关于AI工作流程的效用和定制化的纵向研究

    Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows

    [https://arxiv.org/abs/2402.09894](https://arxiv.org/abs/2402.09894)

    这项纵向研究调查了生成式AI工作流程的实用性和定制化程度，结果显示，在熟悉化阶段后，用户感知到的系统效用提高了。

    

    生成式AI为人们在日常任务中提供了新颖而令人印象深刻的能力。有许多AI工作流程通过将AI输出与人类互动相结合来解决真实而复杂的问题。尽管AI具有无可否认的吸引力，但在新鲜感消失后，生成式AI工作流程的实用性如何仍然不确定。此外，利用生成式AI构建的工具具有个性化和快速适应的潜力，但用户是否充分利用了个性化的可能性呢？我们进行了一项为期三周的纵向研究，共有12个用户，旨在了解科学传播中生成式AI工具的熟悉度和定制化程度。我们的研究发现，熟悉化阶段持续了4.3个会话，用户在这个阶段探索工作流程的功能以及他们发现哪些方面有用。在熟悉化后，系统的感知效用评分高于之前，表明了感知效用的提高。

    arXiv:2402.09894v1 Announce Type: cross  Abstract: Generative AI brings novel and impressive abilities to help people in everyday tasks. There are many AI workflows that solve real and complex problems by chaining AI outputs together with human interaction. Although there is an undeniable lure of AI, it's uncertain how useful generative AI workflows are after the novelty wears off. Additionally, tools built with generative AI have the potential to be personalized and adapted quickly and easily, but do users take advantage of the potential to customize? We conducted a three-week longitudinal study with 12 users to understand the familiarization and customization of generative AI tools for science communication. Our study revealed that the familiarization phase lasts for 4.3 sessions, where users explore the capabilities of the workflow and which aspects they find useful. After familiarization, the perceived utility of the system is rated higher than before, indicating that the perceived
    
[^11]: 有限预算下的迅速学习最佳臂识别

    Best Arm Identification for Prompt Learning under a Limited Budget

    [https://arxiv.org/abs/2402.09723](https://arxiv.org/abs/2402.09723)

    这项工作提出了一种在提示学习中考虑有限预算约束的方法，通过建立提示学习和多臂赌博机中固定预算最佳臂识别之间的联系，提出了一个通用框架TRIPLE，通过利用聚类和嵌入思想实现了两个增强方法。

    

    大型语言模型（LLMs）的显著指令跟随能力引发了对自动学习合适提示的兴趣。然而，虽然提出了许多有效的方法，但在学习过程中产生的成本（例如访问LLM和评估响应）尚未得到考虑。为克服这个限制，本工作在提示学习中明确引入了有限预算约束。为了开发有原则的解决方案，本研究在提示学习和多臂赌博机的固定预算最佳臂识别（BAI-FB）之间建立了一种新的联系。基于这种联系，提出了一个通用框架TRIPLE（用于提示学习的最佳臂识别），以系统地利用BAI-FB在提示学习中的力量。提示学习的独特特点进一步通过利用聚类和嵌入思想提出了TRIPLE的两个基于嵌入的增强方法。

    arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
    
[^12]: API Pack：一个用于API调用生成的大规模多语言数据集

    API Pack: A Massive Multilingual Dataset for API Call Generation

    [https://arxiv.org/abs/2402.09615](https://arxiv.org/abs/2402.09615)

    这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成

    

    我们介绍了API Pack，一个包含超过一百万个指令-API调用对的多语言数据集，旨在提高大型语言模型的API调用生成能力。通过实验，我们证明了API Pack在提升模型在这一特定任务上的效果的同时，保持其在一般编码方面的整体熟练程度。仅在20,000个Python实例上对CodeLlama-13B进行微调，其生成未见过的API调用的准确率比GPT-3.5和GPT-4分别高出10%和5%。扩展到100k个例子可以提高对训练期间未见过的新API的泛化能力。此外，实现了跨语言的API调用生成，而无需大量语言特定的数据。数据集、经过微调的模型和整体代码库可在https://github.com/anonymous_url上公开获取。

    arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
    
[^13]: SemRel2024: 14种语言的语义文本相关性数据集合

    SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages

    [https://arxiv.org/abs/2402.08638](https://arxiv.org/abs/2402.08638)

    SemRel2024是一个包含来自14种语言的语义文本相关性数据集合，通过该数据集合可以探索和量化语义相关性。这对于大型语言模型的能力和性能有重要的影响。这个数据集合涵盖了南非荷兰语、阿尔及利亚阿拉伯语、阿姆哈拉语、英语、豪萨语、印地语、印度尼西亚语、卢旺达语、马拉地语、摩洛哥阿拉伯语、现代标准阿拉伯语、旁遮普语、西班牙语和泰卢固语等14种语言。

    

    探索和量化语义相关性是语言表达的核心。它在各种自然语言处理任务中具有重要影响，包括为大型语言模型（LLM）的能力和性能提供洞察。虽然早期的自然语言处理研究主要集中在语义相似性上，往往是在英语语境中，但我们认为更广泛的语义相关性现象值得研究。本文介绍了SemRel，这是一个由母语为14种语言进行注释的新的语义相关性数据集合：南非荷兰语、阿尔及利亚阿拉伯语、阿姆哈拉语、英语、豪萨语、印地语、印度尼西亚语、卢旺达语、马拉地语、摩洛哥阿拉伯语、现代标准阿拉伯语、旁遮普语、西班牙语和泰卢固语。这些语言来自五个不同的语系，主要在非洲和亚洲使用，这些地区的自然语言处理资源相对较少。SemRel数据集中的每个实例都是与一个表示相关性得分的句子对相关联。

    Exploring and quantifying semantic relatedness is central to representing language. It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs). While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that repr
    
[^14]: 尾巴的故事：作为尺度律变化的模型崩溃

    A Tale of Tails: Model Collapse as a Change of Scaling Laws

    [https://arxiv.org/abs/2402.07043](https://arxiv.org/abs/2402.07043)

    本文通过尺度律的视角，研究了AI模型大小增长和合成数据引入对模型性能的影响，发现模型可能会遭遇总体崩溃，并通过大规模实验证实了这一理论。

    

    随着AI模型大小的增长，神经尺度律已成为预测大模型在扩容和原始（人类或自然）训练数据大小增加时改善的关键工具。然而，流行模型的广泛使用意味着在线数据和文本的生态系统将逐渐包含越来越多的合成数据。在本文中，我们问：当合成数据进入训练语料库时，尺度律会如何改变？未来的模型仍会改善，还是注定会完全崩溃（模型崩溃）？通过尺度律的视角，我们开发了一个模型崩溃的理论框架。我们发现了广泛的衰减现象，分析了尺度的丧失、与代数的变化尺度、技能的"遗忘"以及混合人类和合成数据时的理解能力。我们的理论通过对一个算术任务和文本生成的转换器进行大规模实验证实。

    As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation 
    
[^15]: 在流数据上进行高效推理的在线级联学习

    Online Cascade Learning for Efficient Inference over Streams

    [https://arxiv.org/abs/2402.04513](https://arxiv.org/abs/2402.04513)

    这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。

    

    大型语言模型 (LLM) 在回答关于数据流的复杂查询方面具有天然的优势，但是 LLM 推理的高计算成本使得它们在许多任务中不可行。我们提出了在线级联学习，这是首个解决这一挑战的方法。这里的目标是学习一个“级联”模型，从容量较低的模型（如逻辑回归器）开始，到强大的 LLM 结束，并配备一个决定在给定输入上使用哪个模型的推迟策略。我们将在线学习级联的任务公式化为一个模仿学习问题，并为该问题提供了无遗憾算法。在四个基准测试中的实验结果显示，我们的方法在准确性上与 LLM 相当，同时将推理成本削减了多达 90%，突显了它在流处理中的效能和适应能力。

    Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
    
[^16]: 位置论文：反对虚假的AI膨胀性声明

    Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims

    [https://arxiv.org/abs/2402.03962](https://arxiv.org/abs/2402.03962)

    这篇论文讨论了在寻求人工通用智能的过程中，人们对于大型语言模型的类人特质过度归因的现象，并呼吁学术界在解读和传播关于AI研究的内容时要保持谨慎。

    

    人类有一种倾向，看到周围的物体具有类似"人类"的特质。我们给汽车取名字，和宠物甚至家用电器交谈，仿佛它们能像其他人类一样理解我们。这种行为被称为拟人化，在机器学习（ML）领域也越来越受到关注，人们声称在大型语言模型（LLM）中能够察觉到类似于人类智能的特质。在这篇位置论文中，考虑到专业激励、人类偏见和一般方法论设置，我们讨论如何当前对于人工通用智能（AGI）的追求为将类人特质归因于LLM打开了滥觞之门。通过几个实验，我们证明在潜在空间中发现可解释为人类的模式并不应该是令人惊讶的结果。考虑到媒体对人工智能的普遍描写，我们呼吁学术界特别谨慎，并对学术诚信原则有额外的意识，在解读和传播关于AI研究的信息时要保持谨慎。

    Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI rese
    
[^17]: GUARD: 通过角色扮演生成自然语言越狱来测试大型语言模型遵循指南的合规性

    GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models

    [https://arxiv.org/abs/2402.03299](https://arxiv.org/abs/2402.03299)

    本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。

    

    发现绕过大型语言模型（LLM）的安全过滤和有害回应的"越狱"已经鼓励社区采取安全措施。其中一个主要的安全措施是在发布之前用越狱主动测试LLM。因此，这样的测试将需要一种能够大规模且高效地生成越狱的方法。本文在追随一种新颖而直观的策略下，以人类生成的方式来生成越狱。我们提出了一个角色扮演系统，将四种不同角色分配给用户LLM，以便协作生成新的越狱。此外，我们收集现有的越狱，并通过句子逐句进行聚类频率和语义模式的划分，将它们分成不同的独立特征。我们将这些特征组织成一个知识图，使其更易于访问和检索。我们的角色系统将利用这个知识图来生成新的越狱，证明了其有效性。

    The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
    
[^18]: 一刀切并非银弹：定制开放领域的操作流程

    One Size Does Not Fit All: Customizing Open-Domain Procedures

    [https://arxiv.org/abs/2311.09510](https://arxiv.org/abs/2311.09510)

    测试了多种简单的多LLM代理体系结构，发现两个LLM代理按顺序使用的简单结构表现最佳，一个编辑通用操作流程，另一个验证可执行性，在定制操作流程方面取得了显著效果，有望进一步探索多代理编辑体系结构的价值。

    

    如何操作的流程，比如如何种植花园，现在被数百万用户使用，但有时需要自定义以满足用户的特定需求，例如不使用杀虫剂种植花园。我们的目标是衡量和改进一个LLM执行此类定制的能力。我们的方法是测试几种简单的多LLM代理体系结构，用新的评估集CustomPlans对其进行定制，该数据集由200多个WikiHow操作流程组成，每个流程都有特定的定制需求。我们发现简单的具有两个LLM代理的体系结构表现最佳，它们按顺序使用，一个编辑通用操作流程，另一个验证其可执行性，明显优于端到端提示的LLM（10.5％绝对）。这表明LLM可以被合理有效地配置以进行操作流程的定制。这也表明，多代理编辑体系结构可能值得进一步探讨，以用于其他类型的定制。

    arXiv:2311.09510v2 Announce Type: replace  Abstract: How-to procedures, such as how to plant a garden, are now used by millions of users, but sometimes need customizing to meet a user's specific needs, e.g., planting a garden without pesticides. Our goal is to measure and improve an LLM's ability to perform such customization. Our approach is to test several simple multi-LLM-agent architectures for customization, as well as an end-to-end LLM, using a new evaluation set, called CustomPlans, of over 200 WikiHow procedures each with a customization need. We find that a simple architecture with two LLM agents used sequentially performs best, one that edits a generic how-to procedure and one that verifies its executability, significantly outperforming (10.5% absolute) an end-to-end prompted LLM. This suggests that LLMs can be configured reasonably effectively for procedure customization. This also suggests that multi-agent editing architectures may be worth exploring further for other custo
    
[^19]: 学习学习以进行少样本持久主动学习

    Learning to Learn for Few-shot Continual Active Learning

    [https://arxiv.org/abs/2311.03732](https://arxiv.org/abs/2311.03732)

    提出了一种简单而高效的方法，Meta-Continual Active Learning，通过元学习和经验重播解决少样本持续主动学习中的任务混淆和灾难性遗忘，进一步结合文本增强来确保泛化。

    

    持续学习旨在确保解决先前见过的任务的稳定性，同时展示对新领域的可塑性。最近在持续学习方面的进展主要局限于监督学习设置，尤其是在自然语言处理领域。在这项工作中，我们考虑了一种少样本持续主动学习（CAL）设置，其中标记数据不足，但未标记数据充足，但有限的注释预算。我们提出了一种简单而高效的方法，称为元持续主动学习。具体地，我们采用元学习和经验重播来解决任务之间的混淆和灾难性遗忘。我们进一步结合文本增强来确保泛化性能。我们在基准文本分类数据集上进行了大量实验，以验证所提方法的有效性，并分析了在少样本CAL设置中不同主动学习策略的影响。我们的实验结果表明

    arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t
    
[^20]: 使用大型语言模型从材料科学文献中挖掘实验数据

    Mining experimental data from Materials Science literature with Large Language Models. (arXiv:2401.11052v1 [cs.CL])

    [http://arxiv.org/abs/2401.11052](http://arxiv.org/abs/2401.11052)

    本研究评估了使用大型语言模型从材料科学文献中提取结构化信息的能力，并引入了一种新颖的方法来处理材料科学信息的复杂性。在命名实体识别和关系提取任务上，LLMs与传统模型相比表现有限，但在少数-shot提示下有一定的改进。

    

    本研究致力于评估先进的大型语言模型（LLMs），如GPT-3.5-Turbo、GPT-4和GPT-4-Turbo，在材料科学领域科学文档中提取结构化信息的能力。我们引入了一种新颖的方法，用于比较分析复杂的材料表达式，强调化学式的标准化，以解决材料科学信息评估中固有的复杂性。为此，我们主要关注信息提取的两个关键任务：（i）研究材料和物理性质的命名实体识别（NER）和（ii）这些实体之间的关系提取（RE）。LLMs在执行这些任务时的表现与基于BERT架构和基于规则的传统模型进行了基准测试。对于NER，LLMs在零-shot提示下无法超越基准线，并且仅在少数-shot提示下有少量改进。然而，对于...

    This study is dedicated to evaluating the capabilities of advanced large language models (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in the extraction of structured information from scientific documents within the field of materials science. We introduce a novel methodology for the comparative analysis of intricate material expressions, emphasising the standardisation of chemical formulas to tackle the complexities inherent in materials science information assessment. To this end, we primarily focus on two critical tasks of information extraction: (i) a named entity recognition (NER) of studied materials and physical properties and (ii) a relation extraction (RE) between these entities. The performance of LLMs in executing these tasks is benchmarked against traditional models based on the BERT architecture and rule-based approaches. For NER, LLMs fail to outperform the baseline with zero-shot prompting and exhibit only limited improvement with few-shot prompting. However, for 
    
[^21]: 用大型语言模型改善文本嵌入

    Improving Text Embeddings with Large Language Models. (arXiv:2401.00368v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.00368](http://arxiv.org/abs/2401.00368)

    本文介绍了一种使用只用合成数据和少量训练步骤获取高质量文本嵌入的简单方法，并且在没有使用标记数据的情况下，在竞争激烈的文本嵌入基准上取得了强大的性能。

    

    在本文中，我们介绍了一种新颖且简单的方法，仅使用合成数据和少于1k个训练步骤即可获得高质量的文本嵌入。与现有方法不同，现有方法往往依赖多阶段中间预训练，使用数十亿个弱监督文本对进行训练，然后再使用少量标记数据进行微调，我们的方法不需要构建复杂的训练流程，也不依赖于通常受任务多样性和语言覆盖范围限制的手动收集的数据集。我们利用专有的LLM来为近100种语言的数十万个文本嵌入任务生成多样的合成数据。然后，我们使用标准的对比损失在合成数据上微调开源的只有解码器的LLM。实验证明，我们的方法在竞争激烈的文本嵌入基准上取得了出色的性能，而且没有使用任何标记数据。此外，当与合成数据和标记数据的混合进行微调时，我们的模型创造了新的

    In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across nearly 100 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new
    
[^22]: 地球是扁平的，因为......：通过说服性对话研究LLMs对误导信息的信仰

    The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation. (arXiv:2312.09085v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.09085](http://arxiv.org/abs/2312.09085)

    本研究研究了LLMs对误导信息的易受攻击性，特别是在说服性对话中。通过实验证明，LLMs在事实知识上的正确信念很容易被各种说服策略所操纵。

    

    大型语言模型(LLMs)封装了大量知识，但仍然容易受到外部误导信息的攻击。现有研究主要在单轮对话中研究了这种易受攻击的行为。然而，在多轮对话中，特别是说服性对话中，信仰可以发生变化。因此，在本研究中，我们深入探讨了LLMs对说服性对话的易受攻击性，特别是对它们可以正确回答的事实问题。首先，我们整理了Farm（即事实到误导）数据集，其中包含与系统生成的说服性误导信息相匹配的事实问题。然后，我们开发了一个测试框架，以追踪LLMs在说服性对话中的信仰变化。通过大量实验，我们发现LLMs在事实知识上的正确信念很容易被各种说服策略所操纵。

    Large Language Models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies.
    
[^23]: 使用您的本能：使用神经探测器与转换器进行指令优化

    Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])

    [http://arxiv.org/abs/2310.02905](http://arxiv.org/abs/2310.02905)

    该论文提出了一种使用神经探测器和转换器优化指令的方法，以提高大型语言模型的性能。

    

    大型语言模型(LLMs)在各种应用中展示了出色的指令跟随能力，并取得了令人瞩目的表现。然而，LLMs的性能严重依赖于给予它们的指令，这些指令通常需要大量人力进行手动调整。最近的研究使用了高效的贝叶斯优化（BO）算法来自动优化给予黑盒LLMs的指令。然而，在优化高度复杂（例如高维）的目标函数时，如将指令映射到LLM性能的函数，BO通常表现不佳。这主要是由于BO使用的高斯过程（GP）模型的表达能力有限，该模型被用作BO的代理来建模目标函数。与此同时，已经多次证明神经网络（NNs），尤其是预训练的转换器，具有很强的表达能力，可以建模高度复杂的函数。因此，我们采用了一种神经探测器算法。

    Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algor
    
[^24]: TRAM：用于大型语言模型的时间推理基准评估

    TRAM: Benchmarking Temporal Reasoning for Large Language Models. (arXiv:2310.00835v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00835](http://arxiv.org/abs/2310.00835)

    TRAM是一个用于评估大型语言模型的时间推理能力的基准评估。我们介绍了由十个数据集组成的TRAM基准评估，涵盖了事件的各种时间方面。尽管使用流行的语言模型进行广泛评估，结果表明这些模型在时间推理任务上仍然落后于人类。

    

    时间推理对于理解自然语言中描述的事件的细微差别至关重要。以往对于这个主题的研究范围有限，缺乏标准化的基准评估，这导致不同研究间的评估结果不一致。本文介绍了一个称为TRAM的时间推理基准评估，由十个数据集组成，涵盖了事件的各种时间方面，如顺序、算术、频率和持续时间，旨在促进对大型语言模型（LLM）的时间推理能力的全面评估。我们在零样本学习和少样本学习场景中使用了流行的LLM，如GPT-4和Llama2进行了广泛评估。此外，我们还使用基于BERT的模型进行了基准评估。我们的研究结果表明，这些模型在时间推理任务上仍然落后于人类的表现。我们希望TRAM能够推动进一步提升时间推理的研究进展。

    Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the temporal reasoning capabilities of large language models (LLMs). We conduct an extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in both zero-shot and few-shot learning scenarios. Additionally, we employ BERT-based models to establish the baseline evaluations. Our findings indicate that these models still trail human performance in temporal reasoning tasks. It is our aspiration that TRAM will spur further progress in enhancing the temporal reason
    
[^25]: 视觉和语言模型中短语定位和任务表现的联合研究

    A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models. (arXiv:2309.02691v1 [cs.CL])

    [http://arxiv.org/abs/2309.02691](http://arxiv.org/abs/2309.02691)

    这项研究提出了一个框架来研究视觉和语言模型中短语定位和任务性能之间的关系，并且通过验证实验发现了当代模型在短语定位和任务求解方面的不一致性。

    

    在需要对视觉背景中的自然语言进行推理的任务中，关键是将单词和短语与图像区域联系起来。然而，即使通常预期以有助于泛化的方式解决任务，观察到当代模型中的这种定位也是复杂的。我们提出了一个框架来共同研究任务执行和短语定位，并提出了三个基准来研究两者之间的关系。我们的结果表明，当代模型在定位短语和解决任务的能力之间存在不一致性。我们展示了如何通过对定位标注进行强制性训练来解决这个问题，并分析了它所创建的动态。代码可在https://github.com/lil-lab/phrase_grounding获得。

    Key to tasks that require reasoning about natural language in visual contexts is grounding words and phrases to image regions. However, observing this grounding in contemporary models is complex, even if it is generally expected to take place if the task is addressed in a way that is conductive to generalization. We propose a framework to jointly study task performance and phrase grounding, and propose three benchmarks to study the relation between the two. Our results show that contemporary models demonstrate inconsistency between their ability to ground phrases and solve tasks. We show how this can be addressed through brute-force training on ground phrasing annotations, and analyze the dynamics it creates. Code and at available at https://github.com/lil-lab/phrase_grounding.
    
[^26]: 通过语言学习对世界建模

    Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])

    [http://arxiv.org/abs/2308.01399](http://arxiv.org/abs/2308.01399)

    本论文提出了一种通过语言学习对世界进行建模的方法，利用语言帮助代理器预测未来并进行行动。通过学习多模态世界模型，代理器可以预测未来的文本和图像表示，并在模型回滚中进行行动。

    

    为了与人类在世界中相互作用，代理器需要理解人们使用的多样化的语言类型，并将其与视觉世界关联起来，并基于语言行动。虽然当前的代理器可以通过任务奖励学习执行简单的语言指令，但我们的目标是建立可以利用传达一般知识、描述世界状态、提供互动反馈等多样化语言的代理器。我们的核心思想是语言帮助代理器预测未来：将会被观察到什么、世界将如何运行以及哪些情况将获得奖励。这个观点将语言理解与未来预测统一为一个强大的自监督学习目标。我们提出了Dynalang，一种学习多模态世界模型的代理器，它可以预测未来的文本和图像表示，并在想像的模型回滚中学习行动。与只使用语言预测动作的传统代理器不同，Dynalang通过过去的语言还可以获取丰富的语言理解能力。

    To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to 
    
[^27]: 动态上下文剪枝用于高效和可解释的自回归变换器

    Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. (arXiv:2305.15805v1 [cs.CL])

    [http://arxiv.org/abs/2305.15805](http://arxiv.org/abs/2305.15805)

    本研究提出了一种动态上下文剪枝方法，可以在保持模型表现力的同时，动态减少无效信息，提高模型的效率和可解释性。该技术可以应用于现有的预训练模型，并且可以通过简单的微调过程实现。

    

    大型语言模型中采用的自回归变换器难以扩展到长序列。尽管有几项工作试图减少它们的计算成本，但大多数LLM仍然在所有标记对之间采用注意层，从而产生二次成本。本研究提出了一种新方法，通过保留模型的表现力来动态修剪上下文信息，从而在推理过程中减少内存和计算要求。我们的方法使用可学习机制，在生成过程中确定哪些无关的标记可以从上下文中删除。通过这样做，我们的方法不仅解决了性能问题，而且增强了可解释性，为模型的决策过程提供了宝贵的洞察力。我们的技术可以通过简单的微调过程应用于现有的预训练模型，并且剪枝强度可以由稀疏度参数指定。

    Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity para
    
[^28]: 带有语音的LM：超越语音令牌的口语语言建模

    LMs with a Voice: Spoken Language Modeling beyond Speech Tokens. (arXiv:2305.15255v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15255](http://arxiv.org/abs/2305.15255)

    SPECTRON是一个新颖的语音延续模型，通过利用预训练的语言模型和语音编码器进行端到端的训练来生成文本和语音输出，在语义内容和讲话者保护方面超越了现有的口语语言模型。

    

    我们提出了SPECTRON，一种新颖的方法来适应预训练的语言模型（LM）以执行语音延续。通过利用预训练的语音编码器，我们的模型可以生成文本和语音输出，整个系统都在频谱图上进行端到端的训练。在频谱图领域训练整个模型相对于使用离散语音表示的现有级联方法简化了我们的语音延续系统。我们进一步展示了我们的方法在语义内容和讲话者保护方面超过了现有的口语语言模型，同时也从预先存在的模型中获得了知识传递的好处。我们的网站https://michelleramanovich.github.io/spectron/spectron上可以找到音频样本。

    We present SPECTRON, a novel approach to adapting pre-trained language models (LMs) to perform speech continuation. By leveraging pre-trained speech encoders, our model generates both text and speech outputs with the entire system being trained end-to-end operating directly on spectrograms. Training the entire model in the spectrogram domain simplifies our speech continuation system versus existing cascade methods which use discrete speech representations. We further show our method surpasses existing spoken language models both in semantic content and speaker preservation while also benefiting from the knowledge transferred from pre-existing models. Audio samples can be found in our website https://michelleramanovich.github.io/spectron/spectron
    

