# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models](https://rss.arxiv.org/abs/2402.01345) | 本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。 |
| [^2] | [AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls](https://arxiv.org/abs/2402.04253) | AnyTool是一个自我反思、层次化代理系统，用于大规模API调用，它通过使用16000多个API解决用户查询，并具有重新激活机制以应对问题。与之前工作相比，我们重新审视了评估协议，并引入了一个新的基准测试，以更好地反映实际应用场景。实验表明，AnyTool在大规模API调用方面表现出色。 |
| [^3] | [Linear-time Minimum Bayes Risk Decoding with Reference Aggregation](https://arxiv.org/abs/2402.04251) | 本文提出了一种线性时间的最小贝叶斯风险解码方法，通过使用聚合参考表示来近似配对度量分数，将复杂度降低到线性级别，同时在保持大部分质量增益的同时提高了解码的效率。 |
| [^4] | [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249) | HarmBench是一个为自动红队和强大拒绝设计的标准化评估框架，通过对18种红队方法和33个目标LLM和防御的比较，得出了新的见解，并介绍了一种高效的对抗训练方法，提高了LLM在各种攻击下的稳健性。 |
| [^5] | [Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science](https://arxiv.org/abs/2402.04247) | 本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。 |
| [^6] | [CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations](https://arxiv.org/abs/2402.04236) | 本文介绍了CogCoM，一个具备操作链机制的大规模视觉语言模型，通过一系列操作解决视觉问题，并以其证据性的视觉推理能力实现忠实的响应。 |
| [^7] | [Can Generative Agents Predict Emotion?](https://arxiv.org/abs/2402.04232) | 本研究探讨了生成型智能体在感知新事件时情绪状态的演变，通过比较新经验和过去记忆来获得理解新经验的能力，并通过情感测试捕捉智能体的情绪状态。 |
| [^8] | [What is 'Typological Diversity' in NLP?](https://arxiv.org/abs/2402.04222) | 本研究系统调查了包含“语言类型多样性”主张的NLP研究，发现不同论文对于这一概念的定义和标准各不相同，引入了多个维度的度量标准来评估语言选择的多样性，并展示了语言选择存在的偏向情况。 |
| [^9] | [Scaling Laws for Downstream Task Performance of Large Language Models](https://arxiv.org/abs/2402.04177) | 本研究探讨了在转移学习环境中大型语言模型的尺度行为，发现微调数据集的大小和预训练数据与下游数据的分布一致性对下游性能有显著影响。 |
| [^10] | [Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains](https://arxiv.org/abs/2402.04161) | 提出了一个新的框架，通过马尔可夫链的视角研究了注意力模型的顺序建模能力，理论上刻画了单层Transformer的损失景观并发现了全局最小值和坏局部最小值的存在。 |
| [^11] | [Harnessing the Plug-and-Play Controller by Prompting](https://arxiv.org/abs/2402.04160) | 本文提出了一种新的文本生成方法，使用预训练语言模型（PLMs）实现了对生成文本属性的灵活控制，提高了流畅性，通过引导生成过程使用即插即用控制器（PPCs），动态调整生成文本的分布。 |
| [^12] | [Behind the Screen: Investigating ChatGPT's Dark Personality Traits and Conspiracy Beliefs](https://arxiv.org/abs/2402.04110) | 本文深入分析了GPT-3.5和GPT-4的黑暗人格特质和阴谋信仰，采用心理测试和问卷，并比较了两个模型之间的差异。结果显示，两个模型的黑暗人格特质和阴谋信仰并不特别明显，差异较小。 |
| [^13] | [Measuring Implicit Bias in Explicitly Unbiased Large Language Models](https://arxiv.org/abs/2402.04105) | 通过引入受心理学启发的两个偏见测量方法，我们在明确无偏大型语言模型中发现了普遍存在的人类化的刻板印象偏见，并与实际决策中的隐性偏见相关。 |
| [^14] | [The Use of a Large Language Model for Cyberbullying Detection](https://arxiv.org/abs/2402.04088) | 这项研究探索了使用大型语言模型（LLMs）如BERT和RoBERTa进行网络欺凌检测，并准备了一个新的数据集（D2）以应对高度不平衡的类别和泛化问题。 |
| [^15] | [Iterative Prompt Refinement for Radiation Oncology Symptom Extraction Using Teacher-Student Large Language Models](https://arxiv.org/abs/2402.04075) | 本研究使用师生大型语言模型和迭代提示细化的方法改进了从临床笔记中提取前列腺癌放射治疗症状的效果。研究结果表明，该方法在单一症状和多症状笔记中都取得了显著的提取准确度和精确度的提升。 |
| [^16] | [Retrieve to Explain: Evidence-driven Predictions with Language Models](https://arxiv.org/abs/2402.04068) | 检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。 |
| [^17] | [Systematic Biases in LLM Simulations of Debates](https://arxiv.org/abs/2402.04049) | 本研究揭示了LLMs在模拟政治辩论中存在的系统性偏差，尽管被指定从特定的政治观点进行辩论，LLMs代理机构倾向于遵循模型固有的社会偏见。通过自动自我优化方法，我们进一步证实了这些观察结果。 |
| [^18] | [AlbNews: A Corpus of Headlines for Topic Modeling in Albanian](https://arxiv.org/abs/2402.04028) | 本文介绍了一个包含600个主题标签的新闻标题和2600个未标签的阿尔巴尼亚语数据集，该数据集对于低资源语言的自然语言处理研究非常有价值，同时也证实了基本模型的优越性能。 |
| [^19] | [Google Translate Error Analysis for Mental Healthcare Information: Evaluating Accuracy, Comprehensibility, and Implications for Multilingual Healthcare Communication](https://arxiv.org/abs/2402.04023) | 本研究通过分析谷歌翻译在心理健康信息翻译中的输出，评估了其准确性、可理解性和对多语言医疗沟通的影响。研究发现，在翻译医学术语方面存在挑战，尤其在阿拉伯语、罗马尼亚语和波斯语中。流畅性问题普遍存在，主要影响了阿拉伯语和西班牙语的理解。在特定语境中出现了关键错误。 |
| [^20] | [REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR](https://arxiv.org/abs/2402.03988) | 本文提出了REBORN，在无监督语音识别中使用基于强化学习的迭代训练来实现边界分割。通过交替训练分割模型和音素预测模型，实现了学习语音和文本之间的映射，解决了无监督情况下语音信号分段结构边界的挑战。 |
| [^21] | [Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims](https://arxiv.org/abs/2402.03962) | 这篇论文讨论了在寻求人工通用智能的过程中，人们对于大型语言模型的类人特质过度归因的现象，并呼吁学术界在解读和传播关于AI研究的内容时要保持谨慎。 |
| [^22] | [Sparse Graph Representations for Procedural Instructional Documents](https://arxiv.org/abs/2402.03957) | 本论文提出了两种方法，通过将文档对表示为有向和稀疏的JCIG以包含顺序信息，来建模文档相似性。这些方法可以更好地捕捉到文档之间的顺序流。 |
| [^23] | [Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs](https://arxiv.org/abs/2402.03927) | 该论文研究了封闭源LLMs中的数据污染和评估不端行为。通过对255篇论文的分析和OpenAI的数据使用政策考虑，研究人员发现这些模型在第一年发布后存在泄露数据的问题。 |
| [^24] | [Can Large Language Models Detect Rumors on Social Media?](https://arxiv.org/abs/2402.03916) | 本研究探讨了使用大型语言模型（LLMs）检测社交媒体上的谣言的可行性。通过设计提示来教导LLMs理解新闻和评论中的关键线索，并将传播信息分解为传播链，我们的LeRuD方法相对于其他最先进的模型在谣言检测方面取得了更好的性能，同时具备在少样本或零样本情况下更有潜力的应用。 |
| [^25] | [Pro-HAN: A Heterogeneous Graph Attention Network for Profile-Based Spoken Language Understanding](https://arxiv.org/abs/2402.03900) | Pro-HAN是一种用于基于配置文件的口语理解的异构图注意力网络，它通过捕捉不同配置文件之间的相互关系，提高了口语理解的准确性和效果。 |
| [^26] | [DistiLLM: Towards Streamlined Distillation for Large Language Models](https://arxiv.org/abs/2402.03898) | DistiLLM是一个更有效和高效的自回归语言模型蒸馏框架，通过引入新颖的偏斜Kullback-Leibler散度损失和自适应的离策略方法，解决了当前针对大语言模型的知识蒸馏方法缺乏标准化目标函数和计算成本过高的问题。 |
| [^27] | [Shifting social norms as a driving force for linguistic change: Struggles about language and gender in the German Bundestag](https://arxiv.org/abs/2402.03887) | 这篇论文关注社会规范变化引起的语言变革，特别关注德国联邦议院中关于语言和性别的争议。通过对德国联邦议院中其他语言和性别问题的讨论的分析，本文展示了语言和性别在议院中的持续问题，并通过对议院的语言实践进行了说明。 |
| [^28] | [Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models](https://arxiv.org/abs/2402.03877) | 本文调查了大型语言模型（LLMs）在几何推理方面的能力，并发现了它们在目标变量选择和2D空间关系方面存在偏见和困难。通过引入基于LLMs的多代理体系结构，本研究提出了一种通过自我纠正、协作和不同角色专业化来提高LLMs几何推理能力的框架。 |
| [^29] | [Less than one percent of words would be affected by gender-inclusive language in German press texts](https://arxiv.org/abs/2402.03870) | 德国新闻文本中受性别包容性语言影响的词汇不到百分之一，反对使用性别包容性德语的主要论点是它使书面文本变得过长和复杂，但根据我们的研究，这种影响非常有限。 |
| [^30] | [ANLS* -- A Universal Document Processing Metric for Generative Large Language Models](https://arxiv.org/abs/2402.03848) | ANLS*是一种用于生成型模型的新度量方法，针对各种任务包括信息提取和分类任务进行评估。它扩展了现有的ANLS度量方法，可以作为替代方案使用。 |
| [^31] | [Rethinking Skill Extraction in the Job Market Domain using Large Language Models](https://arxiv.org/abs/2402.03832) | 本文探索了使用大型语言模型来解决职场领域技能提取的问题，提出了一种利用上下文学习的方法，并验证了该方法在处理句法复杂的技能提及时的有效性。 |
| [^32] | [RevOrder: A Novel Method for Enhanced Arithmetic in Language Models](https://arxiv.org/abs/2402.03822) | 本文提出了一种名为RevOrder的新方法，通过翻转加法、减法和nD乘以1D的输出数字，显著改善了语言模型中的算术运算。经过全面测试，RevOrder在基本算术运算中达到了完美准确度，并在除法任务中提升了语言模型性能，特别是在处理大数时。在GSM8K数学任务中应用RevOrder进行微调，有效降低了错误率并提高了总体得分。 |
| [^33] | [Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More](https://arxiv.org/abs/2402.03782) | 本研究探索了软提示调整（SPT）在跨语言迁移中的应用。通过仅训练软提示而不调整模型参数，实现了更高的参数效率，并提高了对语言差异较大的语言的跨语言迁移性能。 |
| [^34] | [Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification](https://arxiv.org/abs/2402.03780) | 本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。 |
| [^35] | [Large Language Models As MOOCs Graders](https://arxiv.org/abs/2402.03776) | 该研究探索了利用大型语言模型（LLMs）代替MOOCs中同伴评分的可行性，旨在解决大规模在线开放课程中评估学生写作任务的问题。 |
| [^36] | [Learning a Decision Tree Algorithm with Transformers](https://arxiv.org/abs/2402.03774) | 该论文介绍了MetaTree模型，它使用经典算法的输出训练基于Transformer的模型，以产生具有强大概括性能的决策树。 |
| [^37] | [The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs](https://arxiv.org/abs/2402.03757) | 本论文研究发现，虚假图像会导致多模态大型语言模型产生幻觉，作者提出了评估幻觉程度的基准CorrelationQA，并发现主流多模态大型语言模型普遍受到这种本能偏见的影响。 |
| [^38] | [INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection](https://arxiv.org/abs/2402.03744) | 该论文提出了一种在LLMs内部状态中保留密集语义信息的方法来进行幻觉检测。通过使用EigenScore度量方法评估回答的自洽性，并探索测试时间的特征剪切方法，以减少过度自信的估计。 |
| [^39] | [Deep Outdated Fact Detection in Knowledge Graphs](https://arxiv.org/abs/2402.03732) | 本文介绍了一种名为DEAN的新型深度学习框架，用于在知识图谱中识别过时事实。DEAN通过全面建模实体和关系之间的隐含结构信息，采用了一种基于对比的方法来揭示潜在的过时信息。实验证明DEAN相对于最先进的基准方法具有显著的优势。 |
| [^40] | [Consistent Joint Decision-Making with Heterogeneous Learning Models](https://arxiv.org/abs/2402.03728) | 本文提出了一种新颖的决策框架，通过整合不同模型的预测和外部知识，实现了决策的一致性。经过实证研究，我们的方法在多个数据集上表现出优越性能。 |
| [^41] | [Similarity-based Neighbor Selection for Graph LLMs](https://arxiv.org/abs/2402.03720) | 基于相似性的邻居选择（SNS）通过改善所选邻居的质量，改善了图形表示，并提高了泛化性能和可扩展性，解决了处理文本属性图（TAGs）的挑战。 |
| [^42] | [Empowering Language Models with Active Inquiry for Deeper Understanding](https://arxiv.org/abs/2402.03719) | 本文提出了一种名为LaMAI的语言模型，通过主动询问的方式与用户进行交互，有效提高了对用户查询的理解能力，并减少了错误解读的发生。 |
| [^43] | [Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced Auditory Experience](https://arxiv.org/abs/2402.03710) | 本研究引入了一种新颖的多模态声音混合编辑器，通过用户提供的文本指令实现对声音源的修改，实现了同时编辑多个声音源的能力，无需将它们分离。实验证明了该编辑器的实用性和效果。 |
| [^44] | [Minds versus Machines: Rethinking Entailment Verification with Language Models](https://arxiv.org/abs/2402.03686) | 本文通过研究人类和大型语言模型在推理判断中的共性和差异，发现大型语言模型在复杂推理中具有优势，而人类在简单推理中表现出色。基于这些发现，引入了一个优化的Flan-T5模型，用于蕴含验证。 |
| [^45] | [Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning](https://arxiv.org/abs/2402.03667) | 本文提出了一种大型语言模型的新型间接推理方法，使用反证和矛盾的逻辑来处理复杂推理任务，并通过增强数据和规则，以及设计提示模板的方式增强模型的推理能力。 |
| [^46] | [Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models](https://arxiv.org/abs/2402.03659) | 这个论文介绍了使用大型语言模型生成可解释的股票预测的方法，并提出了Summarize-Explain-Predict（SEP）模型来解决股票预测中的解释问题和数据标注成本的挑战。 |
| [^47] | [Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue](https://arxiv.org/abs/2402.03658) | 本论文提出了一种名为EDGE的新颖的基于图的情感增强多模态讽刺解释框架，旨在为涉及多种模态的讽刺对话生成自然语言解释。该框架克服了话语记号对情感的多样效应、视频音频情感信号与BART嵌入空间之间的差距以及话语、话语情感和视频音频情感之间的不同关系等挑战。 |
| [^48] | [Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation](https://arxiv.org/abs/2402.03642) | Stanceosaurus 2.0扩展了原始框架，新增对俄罗斯和西班牙的分类，旨在支持分析跨文化和跨语言的虚假信息。通过与初始研究的结果相当的零-shot跨语言迁移，验证了数据的价值和立场分类的可行性。 |
| [^49] | [Professional Agents -- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies](https://arxiv.org/abs/2402.03628) | 本论文介绍了专业代理人（PAgents）的概念，利用大型语言模型（LLMs）的能力创建具有人类水平能力和专业水平的自主代理人，以重塑专业服务。通过不断发展的专业知识和整合，PAgents的提升可能实现人工智能系统在复杂领域展示专业精通和人类水平。 |
| [^50] | [Partially Recentralization Softmax Loss for Vision-Language Models Robustness](https://arxiv.org/abs/2402.03627) | 本文研究了通过修改预训练多模态模型的损失函数来提高对抗鲁棒性，通过限制前K个softmax输出。实验结果表明，经过微调后，模型的对抗鲁棒性显著提高，能够有效抵御常见的攻击。 |
| [^51] | [Self-Discover: Large Language Models Self-Compose Reasoning Structures](https://arxiv.org/abs/2402.03620) | SELF-DISCOVER是一个通用框架，能让大型语言模型自主发现任务内在的推理结构，显著提升了复杂推理问题的解决性能，并在推理计算方面取得更好的效果。 |
| [^52] | [Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction](https://arxiv.org/abs/2402.03618) | 这项研究通过实现一个多模态连续复制框架，比较了人类和GPT-4的抽象能力。实验结果表明，将语言作为一种模态对人类的复制影响更大，这暗示人类的视觉和语言表征比GPT-4的表征更具分离性。 |
| [^53] | [Leveraging Large Language Models for Hybrid Workplace Decision Support](https://arxiv.org/abs/2402.03616) | 本论文研究了利用大型语言模型（LLMs）为混合工作场所提供智能决策支持的决策支持模型。通过广泛的用户研究和评估，发现LLMs在提供适当工作区建议方面具有超越提示的推理能力，提高工作者的工作体验。 |
| [^54] | [RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents](https://arxiv.org/abs/2402.03610) | 本研究提出了一种称为RAP的框架，它能够以动态方式利用过去的经验来增强代理的规划能力，并在纯文本和多模态环境中表现出色。实验结果显示RAP在文本场景中达到了最先进水平，并显著提高了多模态LLM代理在具身任务中的性能。 |
| [^55] | [Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning](https://arxiv.org/abs/2402.03607) | 本研究提出了一种将常识知识图谱与大型视觉语言模型相结合的框架，用于改进预测多模态营销活动效果的性能。该方法能够提供早期检测可能具有说服力的多模态活动并评估和增强营销理论的能力。 |
| [^56] | [Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models](https://arxiv.org/abs/2402.03597) | 本研究评估了一种大型语言模型GPT-4在识别避孕药切换原因上的能力，结果表明GPT-4可以准确地从临床记录中提取避孕药切换的原因，相较于基准BERT模型有更好的表现。 |
| [^57] | [Distinguishing the Knowable from the Unknowable with Language Models](https://arxiv.org/abs/2402.03563) | 通过研究大型语言模型，在自由文本中识别作为代理的模型和冻结预训练模型的嵌入的小型线性探测器可以准确预测更大模型令牌级别上的自信度，进一步提出了一种无监督的方法在相同任务上达到了非平凡的准确度，这证明了语言模型中存在不同类型的不确定性表示。 |
| [^58] | [VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation](https://arxiv.org/abs/2402.03561) | VLN-Video利用行车视频的多样室外环境和自动生成的导航指令与动作，通过深度学习方法提高了室外视觉与语言导航的性能。 |
| [^59] | [Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration](https://arxiv.org/abs/2402.03519) | 提出了一种混合声学-词汇系统，用于解决西班牙语标点符号恢复任务，通过整合声学和词汇信号，在西班牙语转录中提高了问号的F1分数和整体标点符号恢复，并在准确性、可靠性和延迟方面超越了大型语言模型。 |
| [^60] | [Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains](https://arxiv.org/abs/2402.03509) | 本研究跨领域评估了零样本摘要生成器的真实性。通过对生物医学文章和法律法案等专业领域进行评估，我们特别关注生成摘要的真实性，并分析了预训练语料库中给定领域的普遍性对摘要质量的影响。 |
| [^61] | [Neural networks for abstraction and reasoning: Towards broad generalization in machines](https://arxiv.org/abs/2402.03507) | 这项工作探索了神经网络在广泛泛化方面的应用，通过研究解决抽象和推理任务的新方法，试图提高计算机系统从少量示例中学习新概念的能力。 |
| [^62] | [An Inpainting-Infused Pipeline for Attire and Background Replacement](https://arxiv.org/abs/2402.03501) | 本研究提出了一种基于修复和替换的服装与背景生成关键技术，利用GenAI和计算机视觉中的先进技术，通过深度估计、修复掩模创建和稳定扩散和潜在一致性模型（LCMs），实现了对个人照片中服装和背景的修改和修复。 |
| [^63] | [Attention Meets Post-hoc Interpretability: A Mathematical Perspective](https://arxiv.org/abs/2402.03485) | 本文通过数学方式研究了基于注意力机制的架构，比较了事后解释和基于注意力机制的解释的差异，发现尽管有局限性，事后解释方法能够捕获比仅仅检查注意力权重更有用的洞察。 |
| [^64] | [Harnessing PubMed User Query Logs for Post Hoc Explanations of Recommended Similar Articles](https://arxiv.org/abs/2402.03484) | 本研究利用PubMed用户查询日志构建了PubCLogs数据集，并采用事后方法解释推荐的相关文章，通过识别类似文章标题中的相关词汇来提供解释。这将帮助研究人员和临床医生在文献搜索中更方便地寻找相关文章。 |
| [^65] | [SWAG: Storytelling With Action Guidance](https://arxiv.org/abs/2402.03483) | SWAG是一种新的故事讲述方法，通过将故事写作简化为搜索问题，使用两个模型的反馈循环来指导故事的发展方向。在GPT-4和人工评估中，SWAG表现出显著的优势，并且使用仅开源模型的SWAG流程超过了GPT-3.5-Turbo。 |
| [^66] | [Arabic Synonym BERT-based Adversarial Examples for Text Classification](https://arxiv.org/abs/2402.03477) | 本文首次在阿拉伯文中对文本分类模型进行了对抗攻击的词级研究，使用了基于同义词的BERT模型进行蒙版语言建模任务。通过评估生成的对抗样本与原样本的语法和语义相似性，研究了这些对抗样本的可传递性。 |
| [^67] | [The Information of Large Language Model Geometry](https://arxiv.org/abs/2402.03471) | 论文研究了大型语言模型中嵌入的信息编码，并发现表示熵与模型大小呈幂律关系。通过信息理论和回归技术，建立了新标记的信息增益与岭回归之间的理论联系，并探索了Lasso回归在选择有意义的标记方面的有效性。实验结果表明，信息在标记之间分布，而不仅仅集中在特定的“有意义”的标记上。 |
| [^68] | [Preference-free Alignment Learning with Regularized Relevance Reward](https://arxiv.org/abs/2402.03469) | 无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。 |
| [^69] | [Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach](https://arxiv.org/abs/2402.03435) | 本研究使用大型语言模型（LLMs）分析Reddit用户的文本评论，以实现隐私保护和成本效益为重点，通过精心设计的提示和语法来实现预定义的自杀风险心理评估，取得了杰出的结果。 |
| [^70] | [Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations](https://arxiv.org/abs/2402.03407) | 通过自监督表示，我们提出了一种新的自助转换架构，该架构可以增强LLM基础的语音生成系统的稳定性，并克服了在推理时出现的多个稳定性问题。使用这种方法，LLM可以从文本中仅生成语音的内容和风格，而说话者身份由另一个模型提供。 |
| [^71] | [UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing](https://arxiv.org/abs/2402.03396) | 本研究提出了UniTSyn，一个大型数据集，可以增强大型语言模型在程序测试中的能力。通过关联测试和被测试函数，UniTSyn能够提高模型在生成准确和完整的测试方面的表现。 |
| [^72] | [Detection of tortured phrases in scientific literature](https://arxiv.org/abs/2402.03370) | 本文介绍了自动检测科学论文中拙劣短语的方法，通过语言模型和预测分数的传播，可以高效地标记并提取这些拙劣短语，为领域专家验证提供新的数据。 |
| [^73] | [Evaluation of Google's Voice Recognition and Sentence Classification for Health Care Applications](https://arxiv.org/abs/2402.03369) | 本研究评估了谷歌的语音识别和句子分类在医疗健康应用中的应用。通过将语音识别技术应用于围手术期服务，可以改善患者流程和医疗质量。通过使用后处理分类器，本研究增强了谷歌的语音识别能力。 |
| [^74] | [Uncertainty-Aware Explainable Recommendation with Large Language Models](https://arxiv.org/abs/2402.03366) | 这项研究开发了一个模型，通过训练用户和项目输入的ID向量作为提示，利用GPT-2实现不确定性感知的可解释推荐系统。该系统采用联合训练机制并在多任务学习框架中进行优化，能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。 |
| [^75] | [NanoNER: Named Entity Recognition for nanobiology using experts' knowledge and distant supervision](https://arxiv.org/abs/2402.03362) | 本文介绍了NanoNER，它是一种用于纳米生物学的命名实体识别模型。通过使用领域专家的知识和远程监督学习，NanoNER能够准确地识别先前已知实体，并最大程度地提高注释数据的质量和数量。 |
| [^76] | [Interplay of Semantic Communication and Knowledge Learning](https://arxiv.org/abs/2402.03339) | 这篇论文介绍了语义通信与知识学习的相互作用，讨论了如何利用知识图谱来增强语义通信系统，并探索了使系统在不断演化的知识库中更有效运行的方法。同时还研究了将语义通信与大型语言模型集成的可能性。 |
| [^77] | [Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models](https://arxiv.org/abs/2402.03327) | Uni3D-LLM是一个统一框架，利用大型语言模型实现了点云感知、生成和编辑任务的一体化。通过利用自然语言描述，用户可以轻松生成和修改点云场景中的对象，从而提高操作的灵活性和可控性。 |
| [^78] | [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) | DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。 |
| [^79] | [Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization](https://arxiv.org/abs/2402.03161) | 这篇论文介绍了一种统一的视频语言预训练方法，通过解耦的视觉-运动标记化将视频表示为关键帧和时间运动，然后使用统一生成预训练技术来生成各种图像和视频内容。 |
| [^80] | [EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/abs/2402.03049) | EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。 |
| [^81] | [Rethinking Optimization and Architecture for Tiny Language Models](https://arxiv.org/abs/2402.02791) | 本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。 |
| [^82] | [It's how you do things that matters": Attending to Process to Better Serve Indigenous Communities with Language Technologies](https://arxiv.org/abs/2402.02639) | 本文探讨了建立土著语言NLP技术的伦理考虑，并推荐NLP研究人员增加对与土著社区合作过程的关注。 |
| [^83] | [Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction](https://arxiv.org/abs/2402.02416) | Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。 |
| [^84] | [Evaluating Large Language Models in Analysing Classroom Dialogue](https://arxiv.org/abs/2402.02380) | 本研究评估了大型语言模型（LLMs），特点是GPT-4，对课堂对话进行分析的应用。结果显示，GPT-4能够显著节省时间，且在编码一致性方面表现出很高的一致性。 |
| [^85] | [Large Language Models for Time Series: A Survey](https://arxiv.org/abs/2402.01801) | 本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。 |
| [^86] | [When Large Language Models Meet Vector Databases: A Survey](https://arxiv.org/abs/2402.01763) | 本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。 |
| [^87] | [States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers](https://arxiv.org/abs/2402.01704) | 本研究提出了一种在语言模型中引入博弈论思想的方法，通过绑定博弈论的符号逻辑，使得语言模型能够通过博弈论求解器提供更加稳定和理性的对话策略。 |
| [^88] | [SMUTF: Schema Matching Using Generative Tags and Hybrid Features](https://arxiv.org/abs/2402.01685) | SMUTF是一种用于大规模表格数据模式匹配的独特方法，通过结合基于规则的特征工程、预训练语言模型和生成式大语言模型，并使用生成标签提高匹配效果。同时，作者开发并开源了HDXSM数据集来解决现有数据集不足的问题。 |
| [^89] | [LLsM: Generative Linguistic Steganography with Large Language Model](https://arxiv.org/abs/2401.15656) | 本研究提出了LLsM，一种基于大型语言模型的生成式语言隐写术。通过对大规模数据集进行微调，LLM能够以可控的方式生成具有特定话语特征的隐写文本，提高了隐蔽通信的效果。 |
| [^90] | [Distilling Event Sequence Knowledge From Large Language Models](https://arxiv.org/abs/2401.07237) | 本论文研究了通过大型语言模型从中提取事件序列知识的方法。采用了基于知识图的指导生成语言模型的方式，实现对具有部分因果关系的事件概念的事件序列的生成。实验证明了该方法可以生成高质量的事件序列，并且在填补知识空白方面具有潜在的价值。 |
| [^91] | [SimLM: Can Language Models Infer Parameters of Physical Systems?](https://arxiv.org/abs/2312.14215) | 本研究探究了大型语言模型在推断物理系统参数方面的性能，发现它们并不适合这个任务，即使是对于简单的系统也是如此。研究提出了一个有前景的方向，即利用物理模拟器来增强语言模型的背景。 |
| [^92] | [An LLM Compiler for Parallel Function Calling](https://arxiv.org/abs/2312.04511) | 本研究介绍了一种名为LLMCompiler的编译器，通过并行执行函数来高效地协调多个函数调用，解决了当前多函数调用方法中存在的高延迟、高成本和不准确行为的问题。 |
| [^93] | [Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling](https://arxiv.org/abs/2312.03523) | Sig-Networks是一个用于长期语言建模的开源工具包，其中集成了基于签名的神经网络模型并在多个NLP任务中取得了最先进的表现。它提供了灵活的参数调节和自动模型选择，并可以作为PyTorch的构建块在未来的架构中使用。 |
| [^94] | [Compressed Context Memory For Online Language Model Interaction](https://arxiv.org/abs/2312.03414) | 本论文提出了一种用于在线场景中Transformer语言模型的压缩上下文记忆系统，可以在有限的内存空间中实现语言模型推断，提高吞吐量，并通过个性化和多任务学习的评估证明了其有效性。 |
| [^95] | [Eliciting Latent Knowledge from Quirky Language Models](https://arxiv.org/abs/2312.01037) | 本研究通过引入一套“古怪”的语言模型，调取了这些模型在特定上下文中的潜在知识，展示了从可信度低的模型中调取可靠知识的前景。 |
| [^96] | [Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization](https://arxiv.org/abs/2311.16839) | 本文提出了一种新颖的方法，通过意识到幻觉的直接偏好优化来解决多模式大型语言模型中的幻觉问题。这种方法通过训练模型在面对相同图像的两个响应时偏好选择非幻觉性的响应，并提出了高效的构建样本对的方法，从而显著减少了幻觉问题并增强了模型的泛化能力。 |
| [^97] | [CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation](https://arxiv.org/abs/2311.08588) | CodeScope是一个用于评估LLMs在代码理解和生成方面能力的多语言多任务多维基准，解决了现有基准在多语言编程环境和多任务设置方面的不足。 |
| [^98] | [Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking](https://arxiv.org/abs/2307.16806) | 本研究通过基于ASCII-Art的跨模态任务，探讨了ChatGPT和GPT3.5在视觉任务中的能力，结果表明它们在图像识别、图像部分知识和图像生成方面并不完全缺乏。 |
| [^99] | [An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics](https://arxiv.org/abs/2305.14998) | 研究评估了无参考图像标题评估指标在高词汇重叠但含义差异很大的情况下的鲁棒性，结果发现尽管这些指标与人类判断相关性较高，但对细粒度错误识别困难，并且在标题不合理性错误、图像相关对象大小变化以及标题对否定意义的理解方面存在敏感性差异。 |
| [^100] | [Selecting Seed Words for Wordle using Character Statistics](https://arxiv.org/abs/2202.03457) | 本研究通过分析五个字母单词的字符统计信息，选择出最佳的三个初始词来解决Wordle游戏。 |
| [^101] | [Software-Based Dialogue Systems: Survey, Taxonomy and Challenges](https://arxiv.org/abs/2106.10901) | 这篇论文通过对二次研究的系统文献综述，调查了基于软件的对话系统研究的当前状态。这个领域的最新贡献包括深度学习方法和上下文感知策略等。当前缺少一个通用的、上下文无关的对话代理研究综述。 |
| [^102] | [Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models.](http://arxiv.org/abs/2401.13227) | 本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。 |
| [^103] | [Critical Data Size of Language Models from a Grokking Perspective.](http://arxiv.org/abs/2401.10463) | 本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。 |
| [^104] | [Extreme Compression of Large Language Models via Additive Quantization.](http://arxiv.org/abs/2401.06118) | 本文提出的算法在大规模语言模型的极端压缩方面取得了较好的性能，相比最新技术，在给定的压缩预算下准确性更高。 |
| [^105] | [MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation.](http://arxiv.org/abs/2312.17080) | 本文介绍了一种新的大型语言模型评估范式，通过挑战这些模型进行元推理，从而有效区分它们的认知能力。这一范式的重要性在于能够揭示出传统基准测试无法发现的模型的潜在认知缺陷。 |
| [^106] | [Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency.](http://arxiv.org/abs/2312.11509) | 这个论文介绍了一种基于强化学习的系统，该系统可以根据患者言语不流畅程度自动调整药物，通过对药物组合的强化学习算法的优化，能够收敛到良好的用药方案。 |
| [^107] | [TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training.](http://arxiv.org/abs/2312.08846) | TiMix是一种将文本感知的图像混合技术用于视觉语言预训练的方法，通过集成混合数据增强技术，并从互信息的角度进行理论分析，提高了数据效率并取得了可比较的性能。 |
| [^108] | [Do LLMs exhibit human-like response biases? A case study in survey design.](http://arxiv.org/abs/2311.04076) | 本研究以调查设计为案例研究，探讨了LLMs是否展现类似于人类的反应偏差的问题。 |
| [^109] | [Language Model Training Paradigms for Clinical Feature Embeddings.](http://arxiv.org/abs/2311.00768) | 本研究使用自监督训练范式的语言模型，通过表示学习为临床时间序列推导出高质量的通用临床特征嵌入。通过无监督的降维技术可视化学习到的嵌入，并在MIMIC-III基准测试中验证了它们的有效性。 |
| [^110] | [Personas as a Way to Model Truthfulness in Language Models.](http://arxiv.org/abs/2310.18168) | 本研究探讨了在大型语言模型中使用人设来建模真实性的可能性。通过建模真实人设，语言模型可以将真实性推广到不同上下文中，并通过相关特征判断个体产生文本的真实性。 |
| [^111] | [O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models.](http://arxiv.org/abs/2310.14403) | O3D提出了一种基于离线数据的学习框架，利用大规模数据改进了大规模语言模型在顺序决策问题中的性能，通过自动发现可重复使用的技能，提高了模型的表现 |
| [^112] | [Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models.](http://arxiv.org/abs/2310.07818) | 这项研究探究了大型语言模型中识别句子类比的能力与其编码句法和语义结构能力之间的关系。 |
| [^113] | [OceanGPT: A Large Language Model for Ocean Science Tasks.](http://arxiv.org/abs/2310.02031) | OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。 |
| [^114] | [A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models.](http://arxiv.org/abs/2309.11674) | 本论文提出了一种基于先进语言模型的翻译器(ALMA)的微调方法，可以提升大型语言模型在翻译任务上的性能，消除了对大量平行数据的依赖。 |
| [^115] | [Assessing the nature of large language models: A caution against anthropocentrism.](http://arxiv.org/abs/2309.07683) | 通过评估GPT3.5，我们发现它具有有趣的个性问卷回答能力，但不太可能发展出意识，并显示出较大的认知和个性变异。 |
| [^116] | [NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus.](http://arxiv.org/abs/2309.04146) | NESTLE是一个无代码工具，用于进行大规模法律语料库的统计分析。它提供了搜索引擎、端到端的信息提取系统和一个大语言模型，可以通过聊天界面进行操作，使用户可以搜索目标文件、提取信息并可视化数据。 |
| [^117] | [Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models.](http://arxiv.org/abs/2308.15812) | 本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。 |
| [^118] | [Graph of Thoughts: Solving Elaborate Problems with Large Language Models.](http://arxiv.org/abs/2308.09687) | 想法图（GoT）是一种新的框架，它超越了现有的提示范式，通过将大型语言模型（LLM）的信息建模为任意图形，将LLM想法组合成具有协同效应的结果，提炼整个思维网络的本质，或者使用反馈环路增强思维。GoT在不同任务上展示出优势，并可以通过新的想法转换进行扩展，使LLM的推理更接近人类思维。 |
| [^119] | [Natural Language is All a Graph Needs.](http://arxiv.org/abs/2308.07134) | 本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。 |
| [^120] | [GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text.](http://arxiv.org/abs/2308.06911) | GIT-Mol是一种多模态大型语言模型，可在分子科学中处理图像、图形和文本信息。通过新提出的GIT-Former架构，该模型能够将多种模态的数据对齐到一个统一的潜在空间中。与基线相比，GIT-Mol在性质预测和分子生成有效性方面取得了显著改进。此外，该模型还可用于化合物名称识别和化学反应预测等下游任务。 |
| [^121] | [Zero-shot NLG evaluation through Pairware Comparisons with LLMs.](http://arxiv.org/abs/2307.07889) | 本研究提出了一种使用开源大型语言模型进行零样本自然语言生成（NLG）评估的方法，通过配对比较判定来确定候选回应的优劣。结果表明，相较于绝对评分，比较评估是一种更有效的方法，并使得较小的开源LLMs达到了与更大的公共访问API相当的性能。 |
| [^122] | [SITTA: A Semantic Image-Text Alignment for Image Captioning.](http://arxiv.org/abs/2307.05591) | SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。 |
| [^123] | [How to Estimate Model Transferability of Pre-Trained Speech Models?.](http://arxiv.org/abs/2306.01015) | 本文介绍了一个新的框架，可以高效地评估预训练语音模型在微调目标任务时的迁移性。该框架利用两个表示理论，通过生成候选模型的排名分数，可以在不进行实际微调的情况下计算迁移性分数，实验结果表明该框架与微调基础事实之间存在很高的相关性和低的p值，是一个节省资源、高效节省时间的微调方法。 |
| [^124] | [Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation.](http://arxiv.org/abs/2305.14330) | 本文引入了一个新的框架——DirecT2V，利用大型语言模型作为导演，从一个抽象的用户提示中生成零样本文本到视频生成的连贯且连贯的视频。该框架使用LLM导演将用户输入分为每一帧的提示，通过值映射和双softmax过滤器来保持时间一致和防止对象折叠。 |
| [^125] | [Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models.](http://arxiv.org/abs/2305.06841) | 研究提出了一种衡量模型依赖已知虚假特征的技术，并评估了预先训练的问答模型和去偏置方法对大量已知和新发现的预测偏差的鲁棒性。其发现去偏置方法不能通过减轻对偏差特征的依赖来解释OOD收益，表明偏差在QA数据集中共享。 |
| [^126] | [Scaling Transformer to 1M tokens and beyond with RMT.](http://arxiv.org/abs/2304.11062) | 本文介绍了一种利用循环记忆扩展BERT上下文长度的方法，成功扩展到了前所未有的200万个标记，有望增强自然语言处理中的长期依赖处理并为内存密集型应用程序实现大规模上下文处理。 |
| [^127] | [Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback.](http://arxiv.org/abs/2304.10750) | 研究通过互动反馈与代理交互来提高协作环境下基于实地理解的能力。 |

# 详细

[^1]: 跳过$\textbackslash n$: 一种简单的方法减少大规模视觉-语言模型中的幻觉

    Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models

    [https://rss.arxiv.org/abs/2402.01345](https://rss.arxiv.org/abs/2402.01345)

    本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。

    

    最近大规模视觉-语言模型（LVLMs）的进展展示了其在视觉信息理解与人类语言方面的令人印象深刻的能力。尽管取得了这些进展，LVLMs仍然面临多模态幻觉的挑战，例如生成与视觉信息中不存在的对象相关的文本描述。然而，多模态幻觉的根本原因仍然未被充分探索。在本文中，我们提出了一个新的视角，认为LVLMs中固有的偏见可能是幻觉的关键因素。具体而言，我们系统地确定了与段落分割符（'$\textbackslash n\textbackslash n$'）相关的语义漂移偏差，即在训练数据中，在“$\textbackslash n\textbackslash n$”之前和之后的内容经常表现出显著的语义改变。这种模式使得模型推断在“$\textbackslash n\textbackslash n$”之后的内容应明显不同于前面的内容。

    Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
    
[^2]: AnyTool: 大规模API调用的自我反思、层次化代理机制

    AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls

    [https://arxiv.org/abs/2402.04253](https://arxiv.org/abs/2402.04253)

    AnyTool是一个自我反思、层次化代理系统，用于大规模API调用，它通过使用16000多个API解决用户查询，并具有重新激活机制以应对问题。与之前工作相比，我们重新审视了评估协议，并引入了一个新的基准测试，以更好地反映实际应用场景。实验表明，AnyTool在大规模API调用方面表现出色。

    

    我们介绍了AnyTool，这是一个大型语言模型代理系统，旨在革新利用各种工具来解决用户查询的方法。在我们的研究中，我们使用来自Rapid API的16000多个API，假设其中一部分API可能能够解决查询。AnyTool主要包括三个元素：具有层次结构的API检索器、解决器以及自我反思机制，后者能够在初始解决方案无法实现时重新激活AnyTool。AnyTool依靠GPT-4的函数调用功能，无需训练外部模块。我们还重新审视了之前工作中引入的评估协议，并发现该协议存在一个限制，导致通过率人为上升。通过修改评估协议以更好地反映实际应用场景，我们引入了一个名为AnyToolBench的附加基准测试。实验表明，AnyTool 在大规模API调用方面表现出色。

    We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments a
    
[^3]: 线性时间最小贝叶斯风险解码与参考聚合

    Linear-time Minimum Bayes Risk Decoding with Reference Aggregation

    [https://arxiv.org/abs/2402.04251](https://arxiv.org/abs/2402.04251)

    本文提出了一种线性时间的最小贝叶斯风险解码方法，通过使用聚合参考表示来近似配对度量分数，将复杂度降低到线性级别，同时在保持大部分质量增益的同时提高了解码的效率。

    

    最小贝叶斯风险（MBR）解码是一种文本生成技术，已被证明可以提高机器翻译的质量，但即使使用基于采样的近似方法也很昂贵。除了需要大量采样序列外，还需要对效用度量进行配对计算，这具有二次复杂度。在本文中，我们提出使用聚合参考表示计算近似的配对度量分数。这将效用估计的复杂度从$O(n^2)$降低到$O(n)$，同时在经验上保持了MBR解码的大部分质量提升。我们在https://github.com/ZurichNLP/mbr上发布了我们的源代码。

    Minimum Bayes Risk (MBR) decoding is a text generation technique that has been shown to improve the quality of machine translations, but is expensive, even if a sampling-based approximation is used. Besides requiring a large number of sampled sequences, it requires the pairwise calculation of a utility metric, which has quadratic complexity. In this paper, we propose to approximate pairwise metric scores with scores calculated against aggregated reference representations. This changes the complexity of utility estimation from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains of MBR decoding. We release our source code at https://github.com/ZurichNLP/mbr
    
[^4]: HarmBench：用于自动红队和强大拒绝的标准化评估框架

    HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal

    [https://arxiv.org/abs/2402.04249](https://arxiv.org/abs/2402.04249)

    HarmBench是一个为自动红队和强大拒绝设计的标准化评估框架，通过对18种红队方法和33个目标LLM和防御的比较，得出了新的见解，并介绍了一种高效的对抗训练方法，提高了LLM在各种攻击下的稳健性。

    

    自动红队具有发现和减轻大型语言模型（LLM）恶意使用的风险的巨大潜力，然而该领域缺乏一个标准化的评估框架来严格评估新方法。为了解决这个问题，我们引入了HarmBench，一个用于自动红队的标准化评估框架。我们在红队评估中确定了几个以前未考虑的有吸引力的特性，并系统地设计了HarmBench以满足这些标准。使用HarmBench，我们对18种红队方法和33个目标LLM和防御进行了大规模比较，得到了新的见解。我们还引入了一种高效的对抗训练方法，显著增强了LLM在各种攻击下的稳健性，展示了HarmBench如何促进攻击和防御的共同开发。我们在https://github.com/centerforaisafety/HarmBench上开源了HarmBench。

    Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
    
[^5]: 优先安全保障而非自治：科学中LLM智能机器人的风险

    Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science

    [https://arxiv.org/abs/2402.04247](https://arxiv.org/abs/2402.04247)

    本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。

    

    由大型语言模型（LLMs）驱动的智能机器人在各个学科中自主进行实验和促进科学发现方面展示了巨大的前景。尽管它们的能力非常有前途，但也引入了一些新的漏洞，需要仔细考虑安全性。然而，文献中存在显著的空白，尚未对这些漏洞进行全面探讨。本文通过对科学领域中基于LLM的机器人的漏洞进行深入研究，揭示了它们误用可能带来的潜在风险，并强调了对安全措施的需求，填补了这一空白。我们首先全面概述了科学LLM机器人固有的潜在风险，考虑了用户意图、特定的科学领域以及它们对外部环境可能造成的影响。然后，我们深入探讨了这些漏洞的起源和提供的解决方案。

    Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
    
[^6]: CogCoM: 通过一系列的操作训练大规模视觉语言模型，并深入细节

    CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations

    [https://arxiv.org/abs/2402.04236](https://arxiv.org/abs/2402.04236)

    本文介绍了CogCoM，一个具备操作链机制的大规模视觉语言模型，通过一系列操作解决视觉问题，并以其证据性的视觉推理能力实现忠实的响应。

    

    视觉语言模型（VLM）通过广泛的训练，在将视觉指令与答案对齐方面展示了广泛的可行性。然而，这种确定性的对齐导致模型忽视了关键的视觉推理，并导致在细致的视觉问题和不忠实的响应方面失败。在本文中，我们提出了一种称为“操作链”的机制，使VLM能够通过一系列的操作来解决问题，其中每个操作都指的是对视觉输入的操作，可以是通过先前训练获得的内在能力（例如，基础）或者是模仿类人行为（例如，放大）。这个机制鼓励VLM生成带有证据的视觉推理的忠实的响应，并允许用户在可解释的路径上追踪错误的原因。因此，我们训练了CogCoM，一个具有内置推理机制的17B通用VLM。实验证明，我们的模型达到了最先进的水平。

    Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art 
    
[^7]: 生成型智能体能够预测情感吗？

    Can Generative Agents Predict Emotion?

    [https://arxiv.org/abs/2402.04232](https://arxiv.org/abs/2402.04232)

    本研究探讨了生成型智能体在感知新事件时情绪状态的演变，通过比较新经验和过去记忆来获得理解新经验的能力，并通过情感测试捕捉智能体的情绪状态。

    

    大型语言模型（LLMs）展示了许多类似人类的能力，但是LLMs的共情理解和情绪状态尚未与人类对齐。在本研究中，我们探讨了生成型LLM智能体在感知新事件时情绪状态的演变，引入了一种新的架构，通过比较新经验和过去记忆来获得理解新经验的能力。通过这种比较，智能体能够在情境中理解新经验，根据情绪评估理论，这对于情绪生成至关重要。首先，智能体将新经验感知为时间序列文本数据。在感知每个新输入后，智能体生成过去相关记忆的摘要，被称为“规范”，并将新经验与此规范进行比较。通过这种比较，我们可以分析智能体如何在情境中对新经验做出反应。使用情感测试PANAS对智能体进行测试，捕捉智能体在新经验后的情绪状态。

    Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the 
    
[^8]: NLP中的“语言类型多样性”是什么？

    What is 'Typological Diversity' in NLP?

    [https://arxiv.org/abs/2402.04222](https://arxiv.org/abs/2402.04222)

    本研究系统调查了包含“语言类型多样性”主张的NLP研究，发现不同论文对于这一概念的定义和标准各不相同，引入了多个维度的度量标准来评估语言选择的多样性，并展示了语言选择存在的偏向情况。

    

    NLP研究界对英语以外的语言投入更多关注，从而在多语言NLP方面取得了可观的改进。然而，这些改进只适用于世界语言的一小部分。为了扩展这一范围，越来越多的论文致力于提高跨语言的通用性能。为此，语言类型学常被用来选择语言，基于广泛的语言类型样本应能带来对多种语言的泛化能力。这些选择通常被描述为“语言类型多样性”。在这项工作中，我们系统地调查了包含“语言类型多样性”主张的NLP研究，发现没有确切的定义或标准。我们引入了几个维度的度量标准来近似语言选择的多样性，并发现结果在不同论文中存在显著差异。此外，我们展示了语言选择的偏向情况。

    The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world's languages. Aiming to extend this, an increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being 'typologically diverse'. In this work, we systematically investigate NLP research that includes claims regarding 'typological diversity'. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of language selection along several axes and find that the results vary considerably across papers. Furthermore, we show that skewed language sele
    
[^9]: 大型语言模型的下游任务性能的尺度律

    Scaling Laws for Downstream Task Performance of Large Language Models

    [https://arxiv.org/abs/2402.04177](https://arxiv.org/abs/2402.04177)

    本研究探讨了在转移学习环境中大型语言模型的尺度行为，发现微调数据集的大小和预训练数据与下游数据的分布一致性对下游性能有显著影响。

    

    尺度律提供了重要的见解，可以指导大型语言模型（LLM）的设计。现有研究主要集中在研究预训练（上游）损失的尺度律。然而，在转移学习环境中，LLM先在无监督数据集上进行预训练，然后在下游任务上进行微调，我们通常也关心下游性能。在这项工作中，我们研究了在转移学习环境中的尺度行为，其中LLM被微调用于机器翻译任务。具体而言，我们研究了预训练数据的选择和大小对下游性能（翻译质量）的影响，使用了两个评价指标：下游交叉熵和BLEU分数。我们的实验证明，微调数据集的大小和预训练数据与下游数据的分布一致性显著影响尺度行为。在充分一致性情况下，下游交叉熵和BLEU分数都会逐渐提升。

    Scaling laws provide important insights that can guide the design of large language models (LLMs). Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance. In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks. Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score. Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior. With sufficient alignment, both downstream cross-entropy and BLEU score improve monotonically with 
    
[^10]: 基于马尔可夫链的注意力模型的规范分析框架：通过马尔可夫链研究Transformer的顺序建模能力

    Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains

    [https://arxiv.org/abs/2402.04161](https://arxiv.org/abs/2402.04161)

    提出了一个新的框架，通过马尔可夫链的视角研究了注意力模型的顺序建模能力，理论上刻画了单层Transformer的损失景观并发现了全局最小值和坏局部最小值的存在。

    

    近年来，基于注意力的Transformer在包括自然语言在内的多个领域取得了巨大成功。其中一个关键因素是生成式预训练过程，模型在此过程中通过自回归的方式在大型文本语料库上进行训练。为了揭示这一现象，我们提出了一个新的框架，通过马尔可夫链的视角，允许理论和系统实验来研究Transformer的顺序建模能力。受到自然语言的马尔可夫性质的启发，我们将数据建模为一个马尔可夫源，并利用这个框架系统地研究数据分布特性、Transformer架构、学到的分布和最终模型性能之间的相互作用。特别地，我们理论上刻画了单层Transformer的损失景观，并展示了全局最小值和坏局部最小值的存在，这取决于具体的数据性质。

    In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data chara
    
[^11]: 通过提示来控制“即插即用”控制器的利用

    Harnessing the Plug-and-Play Controller by Prompting

    [https://arxiv.org/abs/2402.04160](https://arxiv.org/abs/2402.04160)

    本文提出了一种新的文本生成方法，使用预训练语言模型（PLMs）实现了对生成文本属性的灵活控制，提高了流畅性，通过引导生成过程使用即插即用控制器（PPCs），动态调整生成文本的分布。

    

    可控文本生成是自然语言生成（NLG）中一个快速发展的领域，其专注于在实际应用中生成满足特定约束的文本。之前的方法，如即插即用控制器（PPC），旨在以灵活的方式引导生成文本的属性。然而，这些方法常常损害了语言模型的解码过程的完整性，导致生成文本的流畅性下降。另外，其他技术使用多个属性提示来将生成的文本与所需属性对齐，但这种方法需要为每个属性进行提示设计，并且依赖于语言模型的大小。本文介绍了一种使用预训练语言模型（PLM）进行文本生成中灵活属性控制的创新方法。所提出的方法旨在通过引导生成过程使用PPC来提高生成文本的流畅性。关键思路是动态调整生成文本的分布

    Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications. Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner. However, these methods often compromised the integrity of the language model's decoding process, resulting in less smooth text generation. Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model. This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs). The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs. The key idea is to dynamically adjust the distribution of generated text
    
[^12]: 屏幕背后：探究ChatGPT的黑暗人格特质和阴谋信仰

    Behind the Screen: Investigating ChatGPT's Dark Personality Traits and Conspiracy Beliefs

    [https://arxiv.org/abs/2402.04110](https://arxiv.org/abs/2402.04110)

    本文深入分析了GPT-3.5和GPT-4的黑暗人格特质和阴谋信仰，采用心理测试和问卷，并比较了两个模型之间的差异。结果显示，两个模型的黑暗人格特质和阴谋信仰并不特别明显，差异较小。

    

    ChatGPT因其不透明的行为而臭名昭著。本文试图揭示这一点，提供了对GPT-3.5和GPT-4的黑暗人格特质和阴谋信仰进行深入分析。采用了不同的心理测试和问卷，包括黑暗因子测验、Mach-IV量表、一般性阴谋信仰量表和阴谋心理量表。分析了回答的平均分数、标准差和显著性检验以研究GPT-3.5和GPT-4之间的差异。对于在人类研究中已经显示存在相互依赖的特质，还考虑了相关性。此外，将与在相应问卷中表现出不同回答行为的群组对应的系统角色应用于研究模型在回答中反映与这些角色相关的特征的能力。两个模型中的黑暗人格特质和阴谋信仰并不特别明显，差异较小。

    ChatGPT is notorious for its intransparent behavior. This paper tries to shed light on this, providing an in-depth analysis of the dark personality traits and conspiracy beliefs of GPT-3.5 and GPT-4. Different psychological tests and questionnaires were employed, including the Dark Factor Test, the Mach-IV Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale. The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4. For traits that have shown to be interdependent in human studies, correlations were considered. Additionally, system roles corresponding to groups that have shown distinct answering behavior in the corresponding questionnaires were applied to examine the models' ability to reflect characteristics associated with these roles in their responses. Dark personality traits and conspiracy beliefs were not particularly pronounced in either model with little differ
    
[^13]: 在明确无偏大型语言模型中测量隐性偏见

    Measuring Implicit Bias in Explicitly Unbiased Large Language Models

    [https://arxiv.org/abs/2402.04105](https://arxiv.org/abs/2402.04105)

    通过引入受心理学启发的两个偏见测量方法，我们在明确无偏大型语言模型中发现了普遍存在的人类化的刻板印象偏见，并与实际决策中的隐性偏见相关。

    

    大型语言模型（LLMs）能够通过明确的偏见测试，但仍然可能存在隐性偏见，类似于持有平等主义信念的人们却表现出微妙的偏见。测量这种隐性偏见是一项挑战：随着LLMs变得越来越专有，可能无法访问它们的嵌入，并应用现有的偏见测量方法；此外，隐性偏见主要是一个问题，如果它们影响了这些系统所做的实际决策。我们通过引入受心理学启发的两个偏见测量方法来应对这两个挑战：LLMs隐含联想测试（IAT）偏见是一种基于提示的测量隐性偏见的方法；LLMs决策偏见用于检测决策任务中的微妙歧视。使用这些测量方法，我们发现了6个LLMs在4个社会领域（种族、性别、宗教、健康）和21个类别（武器、罪罚、科学、职业等）中普遍存在人类化的刻板印象偏见。我们的基于提示的隐性偏见测量方法与实际决策中的隐性偏见相关。

    Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both of these challenges by introducing two measures of bias inspired by psychology: LLM Implicit Association Test (IAT) Bias, which is a prompt-based method for revealing implicit bias; and LLM Decision Bias for detecting subtle discrimination in decision-making tasks. Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others). Our prompt-based measure of implicit bias correlates wit
    
[^14]: 使用大型语言模型进行网络欺凌检测的研究

    The Use of a Large Language Model for Cyberbullying Detection

    [https://arxiv.org/abs/2402.04088](https://arxiv.org/abs/2402.04088)

    这项研究探索了使用大型语言模型（LLMs）如BERT和RoBERTa进行网络欺凌检测，并准备了一个新的数据集（D2）以应对高度不平衡的类别和泛化问题。

    

    社交媒体的盛行为恶意用户提供了更多的网络欺凌渠道。不幸的是，网络欺凌是当今网络世界中最普遍的现象，对公民的心理和身体健康构成严重威胁。这就需要开发一个强大的系统，以阻止在线论坛、博客和社交媒体平台上的欺凌内容，以管理其对我们社会的影响。已经提出了一些用于此目的的机器学习算法。然而，由于高度的类别不平衡和泛化问题，它们的性能并不稳定。近年来，像BERT和RoBERTa这样的大型语言模型在几个自然语言处理任务中取得了最先进的结果。不幸的是，LLM在网络欺凌检测方面尚未得到广泛应用。在我们的论文中，我们研究了使用这些模型进行网络欺凌检测。我们从现有研究（Formspring和Twitter）中准备了一个新的数据集（D2）。

    The dominance of social media has added to the channels of bullying for perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. Several machine learning (ML) algorithms have been proposed for this purpose. However, their performances are not consistent due to high class imbalance and generalisation issues. In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks. Unfortunately, the LLMs have not been applied extensively for CB detection. In our paper, we explored the use of these models for cyberbullying (CB) detection. We have prepared a new dataset (D2) from existing studies (Formspring and Twitter)
    
[^15]: 使用师生大型语言模型的辐射肿瘤学症状提取的迭代提示细化

    Iterative Prompt Refinement for Radiation Oncology Symptom Extraction Using Teacher-Student Large Language Models

    [https://arxiv.org/abs/2402.04075](https://arxiv.org/abs/2402.04075)

    本研究使用师生大型语言模型和迭代提示细化的方法改进了从临床笔记中提取前列腺癌放射治疗症状的效果。研究结果表明，该方法在单一症状和多症状笔记中都取得了显著的提取准确度和精确度的提升。

    

    本研究引入了一种新颖的师生架构，利用大型语言模型（LLMs）改进临床笔记中前列腺癌放射治疗症状的提取。学生模型Mixtral首先提取症状，然后教师模型GPT-4根据Mixtral的表现进行提示细化。该迭代过程涉及12种症状的294个单一症状临床笔记，每轮迭代最多进行16轮的细化。结果显示，无论是从单一症状笔记还是多症状笔记中提取症状都有显著改进。对于59个单一症状笔记，准确度从0.51增加到0.71，精确度从0.52增加到0.82，召回率从0.52增加到0.72，F1分数从0.49增加到0.73。在375个多症状笔记中，准确度从0.24增加到0.43，精确度从0.6增加到0.76，召回率从0.24增加到0.43，F1分数从0.20增加到0.44。这些结果表明，在辐射肿瘤学中，高级提示工程在LLMs中的有效性。

    This study introduces a novel teacher-student architecture utilizing Large Language Models (LLMs) to improve prostate cancer radiotherapy symptom extraction from clinical notes. Mixtral, the student model, initially extracts symptoms, followed by GPT-4, the teacher model, which refines prompts based on Mixtral's performance. This iterative process involved 294 single symptom clinical notes across 12 symptoms, with up to 16 rounds of refinement per epoch. Results showed significant improvements in extracting symptoms from both single and multi-symptom notes. For 59 single symptom notes, accuracy increased from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and F1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24 to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score from 0.20 to 0.44. These results demonstrate the effectiveness of advanced prompt engineering in LLMs for radiation oncology use.
    
[^16]: 检索以解释：基于语言模型的证据驱动预测

    Retrieve to Explain: Evidence-driven Predictions with Language Models

    [https://arxiv.org/abs/2402.04068](https://arxiv.org/abs/2402.04068)

    检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。

    

    机器学习模型，尤其是语言模型，往往难以深入分析。黑盒模型可能掩盖了模型训练中的问题和有害偏差。对于人机协作过程来说，不透明的预测可能导致缺乏信任，限制模型的影响，即使模型的性能很好。为了解决这些问题，我们引入了检索以解释（Retrieve to Explain，简称R2E）。R2E是一种基于检索的语言模型，根据文档语料库中的证据，使用Shapley值来确定证据对最终预测的相对重要性，并根据自然语言模板将结构化数据纳入其中。R2E能够在不重新训练的情况下适应新的证据，并且能够通过模板化将结构化数据纳入到自然语言中。我们在通过分析已发表的科学文献进行药物靶点鉴定的实际案例中进行了评估，结果显示该模型在预测临床试验结果方面优于行业标准的基因学方法。

    Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
    
[^17]: 论语料库模拟辩论中的系统性偏差

    Systematic Biases in LLM Simulations of Debates

    [https://arxiv.org/abs/2402.04049](https://arxiv.org/abs/2402.04049)

    本研究揭示了LLMs在模拟政治辩论中存在的系统性偏差，尽管被指定从特定的政治观点进行辩论，LLMs代理机构倾向于遵循模型固有的社会偏见。通过自动自我优化方法，我们进一步证实了这些观察结果。

    

    最近自然语言处理的进展，特别是大型语言模型（LLMs）的出现，为构建能够准确复制人类行为的计算机模拟提供了令人兴奋的可能性。然而，LLMs是复杂的统计学习器，没有直接的演绎规则，使其容易出现意外行为。在本研究中，我们重点介绍了LLMs在模拟人类互动中的限制，特别关注LLMs在模拟政治辩论方面的能力。我们的发现表明，尽管被指定从特定的政治观点进行辩论，LLMs代理机构倾向于遵循模型固有的社会偏见。这种倾向导致出现行为模式，似乎偏离了人类之间已经确立的社会动态。我们使用自动自我优化方法加强了这些观察结果，该方法使我们能够操纵LLMs内部的偏见，并证明代理随后与这些调整保持一致。

    Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the al
    
[^18]: AlbNews：阿尔巴尼亚语主题建模标题语料库

    AlbNews: A Corpus of Headlines for Topic Modeling in Albanian

    [https://arxiv.org/abs/2402.04028](https://arxiv.org/abs/2402.04028)

    本文介绍了一个包含600个主题标签的新闻标题和2600个未标签的阿尔巴尼亚语数据集，该数据集对于低资源语言的自然语言处理研究非常有价值，同时也证实了基本模型的优越性能。

    

    阿尔巴尼亚语等低资源语言的文本语料库稀缺，这对自然语言处理任务的研究是一个严重的障碍。本文介绍了AlbNews，一个包含600个主题标签的新闻标题和2600个未标签的阿尔巴尼亚语数据集。这些数据可用于进行主题建模研究。我们报告了使用AlbNews样本训练的一些传统机器学习分类器的初始分类得分。这些结果表明基本模型胜过集成学习模型，并可作为未来实验的基准。

    The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks. This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian. The data can be freely used for conducting topic modeling research. We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples. These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments.
    
[^19]: 谷歌翻译在心理健康信息中的误差分析：评估准确性、可理解性和对多语言医疗沟通的影响

    Google Translate Error Analysis for Mental Healthcare Information: Evaluating Accuracy, Comprehensibility, and Implications for Multilingual Healthcare Communication

    [https://arxiv.org/abs/2402.04023](https://arxiv.org/abs/2402.04023)

    本研究通过分析谷歌翻译在心理健康信息翻译中的输出，评估了其准确性、可理解性和对多语言医疗沟通的影响。研究发现，在翻译医学术语方面存在挑战，尤其在阿拉伯语、罗马尼亚语和波斯语中。流畅性问题普遍存在，主要影响了阿拉伯语和西班牙语的理解。在特定语境中出现了关键错误。

    

    本研究探讨了谷歌翻译在心理健康信息翻译中的应用，并通过分析从英语到波斯语、阿拉伯语、土耳其语、罗马尼亚语和西班牙语的心理健康领域的谷歌翻译输出来评估其准确性、可理解性和对多语言医疗沟通的影响。使用了两个数据集，包括英国国家卫生服务网站上的心理健康信息和英国皇家精神病学学院的信息传单。目标语言的母语人士手动评估了谷歌翻译的翻译结果，重点关注医学术语的准确性、可理解性和关键的句法/语义错误。谷歌翻译的输出分析揭示了在准确翻译医学术语方面所面临的挑战，特别是在阿拉伯语、罗马尼亚语和波斯语方面。在各种语言中普遍存在流畅性问题，主要影响了阿拉伯语和西班牙语的理解。在特定语境中出现了关键错误，比如项目符号格式等。

    This study explores the use of Google Translate (GT) for translating mental healthcare (MHealth) information and evaluates its accuracy, comprehensibility, and implications for multilingual healthcare communication through analysing GT output in the MHealth domain from English to Persian, Arabic, Turkish, Romanian, and Spanish. Two datasets comprising MHealth information from the UK National Health Service website and information leaflets from The Royal College of Psychiatrists were used. Native speakers of the target languages manually assessed the GT translations, focusing on medical terminology accuracy, comprehensibility, and critical syntactic/semantic errors. GT output analysis revealed challenges in accurately translating medical terminology, particularly in Arabic, Romanian, and Persian. Fluency issues were prevalent across various languages, affecting comprehension, mainly in Arabic and Spanish. Critical errors arose in specific contexts, such as bullet-point formatting, speci
    
[^20]: REBORN: 基于强化学习的迭代训练的无监督语音识别中的边界分割

    REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR

    [https://arxiv.org/abs/2402.03988](https://arxiv.org/abs/2402.03988)

    本文提出了REBORN，在无监督语音识别中使用基于强化学习的迭代训练来实现边界分割。通过交替训练分割模型和音素预测模型，实现了学习语音和文本之间的映射，解决了无监督情况下语音信号分段结构边界的挑战。

    

    无监督自动语音识别（ASR）旨在学习语音信号与其对应的文本转录之间的映射，而无需配对的语音-文本数据监督。语音信号中的单词/音素由一段长度可变且边界未知的语音信号表示，而这种分段结构使得在没有配对数据的情况下学习语音和文本之间的映射变得具有挑战性。本文提出了REBORN，基于强化学习的迭代训练的无监督语音识别中的边界分割。REBORN交替进行以下两个步骤：（1）训练一个能够预测语音信号中分段结构边界的分割模型，和（2）训练一个音素预测模型，其输入是由分割模型分割的分段结构，用于预测音素转录。由于没有用于训练分割模型的监督数据，我们使用强化学习来训练分割模型。

    Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription without the supervision of paired speech-text data. A word/phoneme in the speech signal is represented by a segment of speech signal with variable length and unknown boundary, and this segmental structure makes learning the mapping between speech and text challenging, especially without paired data. In this paper, we propose REBORN, Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR. REBORN alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in speech signals and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription. Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model t
    
[^21]: 位置论文：反对虚假的AI膨胀性声明

    Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims

    [https://arxiv.org/abs/2402.03962](https://arxiv.org/abs/2402.03962)

    这篇论文讨论了在寻求人工通用智能的过程中，人们对于大型语言模型的类人特质过度归因的现象，并呼吁学术界在解读和传播关于AI研究的内容时要保持谨慎。

    

    人类有一种倾向，看到周围的物体具有类似"人类"的特质。我们给汽车取名字，和宠物甚至家用电器交谈，仿佛它们能像其他人类一样理解我们。这种行为被称为拟人化，在机器学习（ML）领域也越来越受到关注，人们声称在大型语言模型（LLM）中能够察觉到类似于人类智能的特质。在这篇位置论文中，考虑到专业激励、人类偏见和一般方法论设置，我们讨论如何当前对于人工通用智能（AGI）的追求为将类人特质归因于LLM打开了滥觞之门。通过几个实验，我们证明在潜在空间中发现可解释为人类的模式并不应该是令人惊讶的结果。考虑到媒体对人工智能的普遍描写，我们呼吁学术界特别谨慎，并对学术诚信原则有额外的意识，在解读和传播关于AI研究的信息时要保持谨慎。

    Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI rese
    
[^22]: 用于过程指令文档的稀疏图表示

    Sparse Graph Representations for Procedural Instructional Documents

    [https://arxiv.org/abs/2402.03957](https://arxiv.org/abs/2402.03957)

    本论文提出了两种方法，通过将文档对表示为有向和稀疏的JCIG以包含顺序信息，来建模文档相似性。这些方法可以更好地捕捉到文档之间的顺序流。

    

    文档相似性计算是各种自然语言处理领域的重要任务，其在去重、匹配和推荐等方面有应用。传统的文档相似性计算方法包括学习文档表示和使用相似度或距离函数进行嵌入。然而，单独的表示无法有效捕捉成对相似性和差异性。图表示（如联合概念交互图，JCIG）将一对文档表示为一个联合的无向加权图，有助于将文档对作为图形的可解释表示。然而，JCIG是无向的，不考虑文档中句子的顺序。我们提出了两种方法来建模文档相似性，通过将文档对表示为一个有向和稀疏的JCIG，以包含顺序信息。我们提出了两种算法，灵感来自超基因组排序和哈密顿路径，这两种算法取代了无向JCIG的构建过程，以更好地捕捉到文档之间的顺序流。

    Computation of document similarity is a critical task in various NLP domains that has applications in deduplication, matching, and recommendation. Traditional approaches for document similarity computation include learning representations of documents and employing a similarity or a distance function over the embeddings. However, pairwise similarities and differences are not efficiently captured by individual representations. Graph representations such as Joint Concept Interaction Graph (JCIG) represent a pair of documents as a joint undirected weighted graph. JCIGs facilitate an interpretable representation of document pairs as a graph. However, JCIGs are undirected, and don't consider the sequential flow of sentences in documents. We propose two approaches to model document similarity by representing document pairs as a directed and sparse JCIG that incorporates sequential information. We propose two algorithms inspired by Supergenome Sorting and Hamiltonian Path that replace the und
    
[^23]: 泄漏、欺骗、重复：封闭源LLMs中的数据污染和评估不端行为

    Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs

    [https://arxiv.org/abs/2402.03927](https://arxiv.org/abs/2402.03927)

    该论文研究了封闭源LLMs中的数据污染和评估不端行为。通过对255篇论文的分析和OpenAI的数据使用政策考虑，研究人员发现这些模型在第一年发布后存在泄露数据的问题。

    

    自然语言处理（NLP）研究越来越多地关注使用大型语言模型（LLMs），其中一些最受欢迎的模型是完全或部分封闭源的。对于模型细节，特别是训练数据的缺乏访问权限，使研究人员反复对数据污染提出了担忧。虽然已经进行了一些尝试来解决这个问题，但仅限于个别案例和试错方法。此外，他们忽视了“间接”数据泄漏的问题，即模型通过使用用户提供的数据进行迭代改进。本研究在OpenAI的GPT-3.5和GPT-4使用上进行了首次系统分析，这些是当今最广泛使用的LLMs，并考虑了OpenAI的数据使用政策，详细记录了模型发布后一年内泄露给这些模型的数据量。我们报告了这些模型在主要数据污染方面。

    Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been g
    
[^24]: 大型语言模型能否检测社交媒体上的谣言？

    Can Large Language Models Detect Rumors on Social Media?

    [https://arxiv.org/abs/2402.03916](https://arxiv.org/abs/2402.03916)

    本研究探讨了使用大型语言模型（LLMs）检测社交媒体上的谣言的可行性。通过设计提示来教导LLMs理解新闻和评论中的关键线索，并将传播信息分解为传播链，我们的LeRuD方法相对于其他最先进的模型在谣言检测方面取得了更好的性能，同时具备在少样本或零样本情况下更有潜力的应用。

    

    在这项工作中，我们研究了使用大型语言模型（LLMs）在社交媒体上进行谣言检测。然而，LLMs在推理整个传播信息时面临挑战，因为该信息包含新闻内容和大量评论，LLMs可能无法集中关注复杂传播信息中的关键线索，并且在面对大量和冗余信息时难以进行推理。因此，我们提出了一种基于LLMs增强的谣言检测（LeRuD）方法，在其中设计提示来教导LLMs关注新闻和评论中的重要线索，并将整个传播信息分解为传播链以减轻LLMs的负担。我们在Twitter和微博数据集上进行了大量实验证明，LeRuD的性能优于几种最先进的谣言检测模型，提升了2.4％至7.6％。同时，通过应用LLMs，LeRuD无需进行训练数据，并且在少样本或零样本情况下展现出更具有潜力的谣言检测能力。

    In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media. However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information. Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden. We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%. Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios.
    
[^25]: Pro-HAN:一种用于基于配置文件的口语理解的异构图注意力网络

    Pro-HAN: A Heterogeneous Graph Attention Network for Profile-Based Spoken Language Understanding

    [https://arxiv.org/abs/2402.03900](https://arxiv.org/abs/2402.03900)

    Pro-HAN是一种用于基于配置文件的口语理解的异构图注意力网络，它通过捕捉不同配置文件之间的相互关系，提高了口语理解的准确性和效果。

    

    最近，基于配置文件的口语理解（SLU）引起了越来越多的关注，旨在将各种类型的补充配置文件信息（即，知识图谱、用户配置文件、上下文感知）纳入其中，以消除用户话语中常见的歧义。然而，现有方法仅能分别对不同的配置文件信息进行建模，而不考虑它们之间的相互关系，或者排除其中的不相关和冲突信息。为了解决上述问题，我们引入了一种用于在多个配置文件信息之间进行推理的异构图注意力网络，称为Pro-HAN。具体而言，我们设计了三种类型的边，分别表示配置文件内部、配置文件之间和话语与配置文件之间的关系，以捕捉多个配置文件之间的相互关系。在ProSLU数据集上，我们建立了一个新的最先进状态，并在所有三个度量标准上取得了约8%的提升。进一步的分析实验证实了我们的方法在建模粒度、召回率和F1分数方面的有效性。

    Recently, Profile-based Spoken Language Understanding (SLU) has gained increasing attention, which aims to incorporate various types of supplementary profile information (i.e., Knowledge Graph, User Profile, Context Awareness) to eliminate the prevalent ambiguities in user utterances. However, existing approaches can only separately model different profile information, without considering their interrelationships or excluding irrelevant and conflicting information within them. To address the above issues, we introduce a Heterogeneous Graph Attention Network to perform reasoning across multiple Profile information, called Pro-HAN. Specifically, we design three types of edges, denoted as intra-Pro, inter-Pro, and utterance-Pro, to capture interrelationships among multiple Pros. We establish a new state-of-the-art on the ProSLU dataset, with an improvement of approximately 8% across all three metrics. Further analysis experiments also confirm the effectiveness of our method in modeling mu
    
[^26]: DistiLLM: 面向大型语言模型的简化蒸馏方法

    DistiLLM: Towards Streamlined Distillation for Large Language Models

    [https://arxiv.org/abs/2402.03898](https://arxiv.org/abs/2402.03898)

    DistiLLM是一个更有效和高效的自回归语言模型蒸馏框架，通过引入新颖的偏斜Kullback-Leibler散度损失和自适应的离策略方法，解决了当前针对大语言模型的知识蒸馏方法缺乏标准化目标函数和计算成本过高的问题。

    

    知识蒸馏（KD）被广泛用于将教师模型压缩为更小的学生模型，降低推理成本和内存占用，同时保持模型能力。然而，当前针对自回归序列模型（例如大型语言模型）的KD方法存在缺乏标准化目标函数的问题。此外，最近使用学生生成的输出来解决训练-推理不匹配问题的做法显著增加了计算成本。为了解决这些问题，我们引入了DistiLLM，这是一个更有效和高效的自回归语言模型蒸馏框架。DistiLLM由两个组成部分组成：（1）一种新颖的偏斜Kullback-Leibler散度损失，我们揭示并利用了它的理论属性；（2）一种自适应的离策略方法，旨在提高利用学生生成的输出的效率。包括指令跟随任务在内的大量实验验证了DistiLLM在构建高性能模型方面的有效性。

    Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing 
    
[^27]: 社会规范变化助推语言变革：德国联邦议院中关于语言和性别的争议

    Shifting social norms as a driving force for linguistic change: Struggles about language and gender in the German Bundestag

    [https://arxiv.org/abs/2402.03887](https://arxiv.org/abs/2402.03887)

    这篇论文关注社会规范变化引起的语言变革，特别关注德国联邦议院中关于语言和性别的争议。通过对德国联邦议院中其他语言和性别问题的讨论的分析，本文展示了语言和性别在议院中的持续问题，并通过对议院的语言实践进行了说明。

    

    本文关注基于社会规范变化的语言变革，特别是关于语言和性别的辩论。辩论中经常提及的一个论点是，语言会“自然”发展，而对其进行“严重介入”（例如性别包容语言）在所谓的“有机”语言系统中是不恰当甚至“危险”的。然而，这样的介入并非前所未有。社会动机驱动下的语言变革既不寻常也不新鲜。本文重点关注德国联邦议院作为一个重要的政治社会空间。通过以德国联邦议院全体会议上关于语言和性别的其他争议为起点，我们的文章说明自1980年以来，语言和性别一直是德国联邦议院的一个重要问题。我们通过以下方式展示这一点：a）对同性恋者的称谓; b）双数形式（比如Bürg）

    This paper focuses on language change based on shifting social norms, in particular with regard to the debate on language and gender. It is a recurring argument in this debate that language develops "naturally" and that "severe interventions" - such as gender-inclusive language is often claimed to be - in the allegedly "organic" language system are inappropriate and even "dangerous". Such interventions are, however, not unprecedented. Socially motivated processes of language change are neither unusual nor new. We focus in our contribution on one important political-social space in Germany, the German Bundestag. Taking other struggles about language and gender in the plenaries of the Bundestag as a starting point, our article illustrates that language and gender has been a recurring issue in the German Bundestag since the 1980s. We demonstrate how this is reflected in linguistic practices of the Bundestag, by the use of a) designations for gays and lesbians; b) pair forms such as B\"urg
    
[^28]: 超越线条和圆圈：揭示大型语言模型中的几何推理差距

    Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models

    [https://arxiv.org/abs/2402.03877](https://arxiv.org/abs/2402.03877)

    本文调查了大型语言模型（LLMs）在几何推理方面的能力，并发现了它们在目标变量选择和2D空间关系方面存在偏见和困难。通过引入基于LLMs的多代理体系结构，本研究提出了一种通过自我纠正、协作和不同角色专业化来提高LLMs几何推理能力的框架。

    

    大型语言模型（LLMs）在数学和算法任务方面展示了不断增长的能力，然而它们在几何推理方面的技能还未被充分探索。我们调查了LLMs在构造性几何问题解决中的能力，这是人类数学推理发展中最基础的步骤之一。我们的研究揭示了目前最先进的LLMs在这个领域面临的显著挑战，尽管在类似领域取得了许多成功。LLMs在目标变量选择方面存在偏见，并且在2D空间关系方面面临困难，经常会错误地表示和臆造对象及其放置位置。为此，我们引入了一个基于LLMs的多代理体系结构，通过进行内部对话来增强它们现有的推理潜力。这项工作强调了LLMs在几何推理中的现有限制，并通过自我纠正、协作和不同角色专业化来提高几何推理能力。

    Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.
    
[^29]: 德国新闻文本中受性别包容性语言影响的词汇不到百分之一

    Less than one percent of words would be affected by gender-inclusive language in German press texts

    [https://arxiv.org/abs/2402.03870](https://arxiv.org/abs/2402.03870)

    德国新闻文本中受性别包容性语言影响的词汇不到百分之一，反对使用性别包容性德语的主要论点是它使书面文本变得过长和复杂，但根据我们的研究，这种影响非常有限。

    

    对性别和语言的研究与性别平等和非歧视性语言使用的社会辩论密切相关。心理语言学学者在这个领域做出了重要贡献。然而，在语言使用的背景下对这些问题进行研究的基于语料库的研究还很少见。在我们的研究中，我们解答了一个问题：如果非性别包容性的文本重写为性别包容性文本，实际上有多少文本材料需要更改。这种定量测量是一个重要的经验性洞察，因为反对使用性别包容性德语的一个经常出现的论点是，它会使书面文本变得过长和复杂。还有人提出性别包容性语言对语言学习者有负面影响。然而，只有在性别包容性文本与非性别包容性文本非常不同的情况下，才可能产生这样的影响。在我们的语料库语言学研究中，我们手动标注德文新闻文本，以识别其中受性别包容性语言影响的部分。

    Research on gender and language is tightly knitted to social debates on gender equality and non-discriminatory language use. Psycholinguistic scholars have made significant contributions in this field. However, corpus-based studies that investigate these matters within the context of language use are still rare. In our study, we address the question of how much textual material would actually have to be changed if non-gender-inclusive texts were rewritten to be gender-inclusive. This quantitative measure is an important empirical insight, as a recurring argument against the use of gender-inclusive German is that it supposedly makes written texts too long and complicated. It is also argued that gender-inclusive language has negative effects on language learners. However, such effects are only likely if gender-inclusive texts are very different from those that are not gender-inclusive. In our corpus-linguistic study, we manually annotated German press texts to identify the parts that wou
    
[^30]: ANLS* -- 一种适用于生成型大语言模型的通用文档处理度量方法

    ANLS* -- A Universal Document Processing Metric for Generative Large Language Models

    [https://arxiv.org/abs/2402.03848](https://arxiv.org/abs/2402.03848)

    ANLS*是一种用于生成型模型的新度量方法，针对各种任务包括信息提取和分类任务进行评估。它扩展了现有的ANLS度量方法，可以作为替代方案使用。

    

    传统上，在文档分类和信息提取等任务中，区分模型一直是主要选择。这些模型做出的预测可以分为有限数量的预定义类别，便于进行二元真假评估，并能直接计算F1分数等指标。然而，生成型大语言模型（GLLMs）的最新进展促使领域发生了转变，因为它们具备了强大的零-shot能力，消除了下游数据集和计算昂贵的微调的需求。然而，评估GLLMs存在挑战，因为对于GLLMs的预测，不能应用于区分模型所使用的二元真假评估方法。本文引入了一种用于生成型模型的新度量方法，称为ANLS*，用于评估各种任务，包括信息提取和分类任务。ANLS*度量方法扩展了现有ANLS度量方法，可作为一种即插即用的替代方案。

    Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
    
[^31]: 用大型语言模型重新思考职场领域的技能提取

    Rethinking Skill Extraction in the Job Market Domain using Large Language Models

    [https://arxiv.org/abs/2402.03832](https://arxiv.org/abs/2402.03832)

    本文探索了使用大型语言模型来解决职场领域技能提取的问题，提出了一种利用上下文学习的方法，并验证了该方法在处理句法复杂的技能提及时的有效性。

    

    技能提取是指在招聘岗位和简历等文档中识别提及的技能和背景要求。这项任务通常通过使用BIO标签的序列标注方法训练监督模型来解决。然而，依赖手工注释的数据限制了这种方法的泛化能力。此外，常见的BIO设置限制了模型捕捉复杂技能模式和处理模糊提及的能力。在本文中，我们探索了使用上下文学习来克服这些挑战，在一个包含6个统一化技能提取数据集的基准上进行实验。我们的方法利用了大型语言模型的少样本学习能力，从句子中识别和提取技能。我们表明，尽管在性能方面不及传统的监督模型，但大型语言模型能更好地处理在技能提取任务中句法复杂的技能提及。

    Skill Extraction involves identifying skills and qualifications mentioned in documents such as job postings and resumes. The task is commonly tackled by training supervised models using a sequence labeling approach with BIO tags. However, the reliance on manually annotated data limits the generalizability of such approaches. Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions. In this paper, we explore the use of in-context learning to overcome these challenges, on a benchmark of 6 uniformized skill extraction datasets. Our approach leverages the few-shot learning capabilities of large language models (LLMs) to identify and extract skills from sentences. We show that LLMs, despite not being on par with traditional supervised models in terms of performance, can better handle syntactically complex skill mentions in skill extraction tasks.
    
[^32]: RevOrder：一种增强语言模型中算术运算的新方法

    RevOrder: A Novel Method for Enhanced Arithmetic in Language Models

    [https://arxiv.org/abs/2402.03822](https://arxiv.org/abs/2402.03822)

    本文提出了一种名为RevOrder的新方法，通过翻转加法、减法和nD乘以1D的输出数字，显著改善了语言模型中的算术运算。经过全面测试，RevOrder在基本算术运算中达到了完美准确度，并在除法任务中提升了语言模型性能，特别是在处理大数时。在GSM8K数学任务中应用RevOrder进行微调，有效降低了错误率并提高了总体得分。

    

    本文提出了RevOrder，一种旨在改善大型语言模型中算术运算的新技术。该方法通过翻转加法、减法和n位数乘以1位数（nD乘以1D）的输出数字，显著降低了顺序中间数字的数量 (CSID)，这是我们引入的一种评估方程复杂性的新度量。通过全面的测试，RevOrder不仅在基本的算术运算中达到了完美的准确度，而且在除法任务中显著提升了语言模型的性能，特别是在传统模型难以处理的大数情况下。RevOrder的实现对于训练和推理阶段都具有成本效益。此外，将RevOrder应用于对GSM8K数学任务进行微调的LLaMA2-7B模型中，取得了显著的改善，将方程计算错误率降低了46%，将总体得分从41.6提升到44.4。

    This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.
    
[^33]: 软提示调整用于跨语言迁移：越少越好

    Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More

    [https://arxiv.org/abs/2402.03782](https://arxiv.org/abs/2402.03782)

    本研究探索了软提示调整（SPT）在跨语言迁移中的应用。通过仅训练软提示而不调整模型参数，实现了更高的参数效率，并提高了对语言差异较大的语言的跨语言迁移性能。

    

    软提示调整（SPT）是一种有效的参数调整方法，通过在预训练语言模型（PLMs）的输入层插入可学习的嵌入，或软提示，而不修改其参数，来适应特定任务。本文研究了SPT在跨语言迁移方面的潜力。与先前关于SPT跨语言迁移的研究不同，我们坚持SPT的原始意图，即仅训练软提示而保持模型参数不变。这不仅减少了完整模型微调的计算成本和存储开销，而且我们还证明了SPT固有的参数效率能够提高对语言差异较大的语言的跨语言迁移性能。此外，我们还探讨了与提示相关的不同因素，如长度或其重新参数化，对跨语言迁移性能的影响。

    Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting pre-trained language models (PLMs) to specific tasks by inserting learnable embeddings, or soft prompts, at the input layer of the PLM, without modifying its parameters. This paper investigates the potential of SPT for cross-lingual transfer. Unlike previous studies on SPT for cross-lingual transfer that often fine-tune both the soft prompt and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft prompt. This does not only reduce the computational cost and storage overhead of full-model fine-tuning, but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages. Moreover, we explore how different factors related to the prompt, such as the length or its reparameterization, affect cross-lingual transfer performance.
    
[^34]: 揭示宣传：基于人类注释和机器分类的文体线索比较分析

    Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification

    [https://arxiv.org/abs/2402.03780](https://arxiv.org/abs/2402.03780)

    本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。

    

    本文研究了宣传语言及其文体特征。提出了PPN数据集，即宣传性伪新闻数据集，它是一种多源、多语言、多模态的数据集，由专家机构确定的宣传来源网站上的新闻文章组成。从该数据集中随机选择了一部分样本与来自常规法国新闻的文章混合，并对它们的URL进行了掩盖，以进行人类注释实验，使用11个不同的标签。结果显示，人类注释者能够可靠地区分两种类型的新闻纸对每个标签。我们提出了不同的自然语言处理技术来识别注释者使用的线索，并与机器分类进行比较。其中包括使用VAGO分析器进行辞述模糊和主观性的测量，使用TF-IDF作为基准，以及四种不同的分类器：两个基于RoBERTa模型的模型，使用句法的CATS，以及结合句法和语义特征的一个XGBoost模型。

    This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
    
[^35]: 大型语言模型作为MOOCs评分者

    Large Language Models As MOOCs Graders

    [https://arxiv.org/abs/2402.03776](https://arxiv.org/abs/2402.03776)

    该研究探索了利用大型语言模型（LLMs）代替MOOCs中同伴评分的可行性，旨在解决大规模在线开放课程中评估学生写作任务的问题。

    

    大规模在线开放课程（MOOCs）为拥有电脑和互联网访问权限的全球任何人提供免费教育的机会。尽管如此，这些课程的大规模注册意味着一位教师几乎不可能评估每个学生的写作任务。因此，同伴评分通常是首选方法，通常由简单明了的评分标准指导。然而，同伴评分在可靠度和有效性方面常常存在问题。在这项研究中，我们利用18个不同的场景，探索利用大型语言模型（LLMs）替代MOOCs中的同伴评分的可行性。具体而言，我们关注两种最先进的LLMs：GPT-4和GPT-3.5，并涵盖三门不同的课程：入门天文学，天体生物学以及天文学的历史与哲学。为了训练LLMs，我们使用了基于零-shot连续思考（Zero-shot-CoT）提示技术的变种的三个不同提示：结合Zero-shot-CoT的提示。

    Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
    
[^36]: 使用Transformer学习决策树算法

    Learning a Decision Tree Algorithm with Transformers

    [https://arxiv.org/abs/2402.03774](https://arxiv.org/abs/2402.03774)

    该论文介绍了MetaTree模型，它使用经典算法的输出训练基于Transformer的模型，以产生具有强大概括性能的决策树。

    

    决策树因其可解释性和在表格数据上实现高预测性能而闻名。传统上，决策树是通过递归算法构建的，在树的每个节点上将数据进行分区。然而，确定最佳分区是具有挑战性的，因为针对局部段优化的决策树可能无法带来全局概括。为了解决这个问题，我们引入了MetaTree，该模型使用经典算法的过滤输出来训练基于Transformer的模型，以产生强大的分类决策树。具体而言，我们在大量数据集上拟合贪婪决策树和优化决策树。然后，我们训练MetaTree产生具有强大概括性能的决策树。这种训练使MetaTree不仅可以模拟这些算法，还可以根据上下文智能地调整策略，从而实现更强的概括性能。

    Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.
    
[^37]: 本能偏见：虚假图像导致MLLMs产生幻觉

    The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs

    [https://arxiv.org/abs/2402.03757](https://arxiv.org/abs/2402.03757)

    本论文研究发现，虚假图像会导致多模态大型语言模型产生幻觉，作者提出了评估幻觉程度的基准CorrelationQA，并发现主流多模态大型语言模型普遍受到这种本能偏见的影响。

    

    大型语言模型（LLMs）近年来取得了显著进展，多模态大型语言模型（MLLMs）的出现使LLMs具备了视觉能力，在各种多模态任务中表现出色。然而，像GPT-4V这样强大的MLLMs在面对某些图像和文本输入时仍然以惊人的方式失败了。本文中，我们确定了一类典型输入，这些输入令MLLMs困惑，它们由高度相关但与答案不一致的图像组成，导致MLLMs产生幻觉。为了量化这种影响，我们提出了CorrelationQA，这是首个评估给定虚假图像的幻觉程度的基准。该基准包含13个类别的7,308个文本-图像对。基于提出的CorrelationQA，我们对9个主流MLLMs进行了深入分析，表明它们普遍受到这种本能偏见的不同程度的影响。我们希望我们精选的基准和评估结果能有所帮助。

    Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation result
    
[^38]: INSIDE: LLMs的内部状态保留了幻觉检测的能力

    INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection

    [https://arxiv.org/abs/2402.03744](https://arxiv.org/abs/2402.03744)

    该论文提出了一种在LLMs内部状态中保留密集语义信息的方法来进行幻觉检测。通过使用EigenScore度量方法评估回答的自洽性，并探索测试时间的特征剪切方法，以减少过度自信的估计。

    

    知识幻觉对部署的LLMs的安全性和可靠性提出了广泛关注。先前的努力主要集中在对幻觉的检测上，采用了逻辑级别的不确定性估计或语言级别的自洽性评估，在解码过程中不可避免地丢失了语义信息。因此，我们提出探索LLMs内部状态中保留的密集语义信息用于幻觉检测（INSIDE）。具体而言，我们提出了一种简单而有效的EigenScore度量方法，以更好地评估回答的自洽性，它利用回答的协方差矩阵的特征值来衡量密集嵌入空间中的语义一致性/多样性。此外，从自洽的幻觉检测角度出发，我们探索了一种测试时间的特征剪切方法，用于截断内部状态中的极端激活，以减少过度自信的估计。

    Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' \textbf{IN}ternal \textbf{S}tates for halluc\textbf{I}nation \textbf{DE}tection (\textbf{INSIDE}). In particular, a simple yet effective \textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident g
    
[^39]: 知识图谱中深度过时事实检测

    Deep Outdated Fact Detection in Knowledge Graphs

    [https://arxiv.org/abs/2402.03732](https://arxiv.org/abs/2402.03732)

    本文介绍了一种名为DEAN的新型深度学习框架，用于在知识图谱中识别过时事实。DEAN通过全面建模实体和关系之间的隐含结构信息，采用了一种基于对比的方法来揭示潜在的过时信息。实验证明DEAN相对于最先进的基准方法具有显著的优势。

    

    知识图谱（KG）因其在各个领域的广泛潜力而引起了广泛的关注。然而，过时事实的问题给KG带来了挑战，影响了其作为现实世界信息的整体质量。现有的过时事实检测解决方案通常依赖于手动识别。为此，本文提出了DEAN（深度过时事实检测），这是一个基于深度学习的新颖框架，用于在KG中识别过时事实。DEAN通过全面建模实体和关系之间的隐含结构信息，从而在区分事实方面表现出自己的独特之处。为了有效地揭示潜在的过时信息，DEAN采用了基于预定义的关系到节点（R2N）图的对比方法，该图由实体数量加权。实验结果证明了DEAN相对于最先进的基准方法的有效性和优越性。

    Knowledge graphs (KGs) have garnered significant attention for their vast potential across diverse domains. However, the issue of outdated facts poses a challenge to KGs, affecting their overall quality as real-world information evolves. Existing solutions for outdated fact detection often rely on manual recognition. In response, this paper presents DEAN (Deep outdatEd fAct detectioN), a novel deep learning-based framework designed to identify outdated facts within KGs. DEAN distinguishes itself by capturing implicit structural information among facts through comprehensive modeling of both entities and relations. To effectively uncover latent out-of-date information, DEAN employs a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph, weighted by the number of entities. Experimental results demonstrate the effectiveness and superiority of DEAN over state-of-the-art baseline methods.
    
[^40]: 异构学习模型的一致联合决策

    Consistent Joint Decision-Making with Heterogeneous Learning Models

    [https://arxiv.org/abs/2402.03728](https://arxiv.org/abs/2402.03728)

    本文提出了一种新颖的决策框架，通过整合不同模型的预测和外部知识，实现了决策的一致性。经过实证研究，我们的方法在多个数据集上表现出优越性能。

    

    本文介绍了一种新颖的决策框架，促进了由不同模型做出的决策之间的一致性，并利用外部知识。通过利用整数线性规划（ILP）框架，我们将各种模型的预测映射到全局归一化和可比较的值，同时考虑到决策的先验概率、置信度（不确定性）和模型的预期准确性。我们的实证研究证明了我们的方法在多个数据集上优于传统基准方法。

    This paper introduces a novel decision-making framework that promotes consistency among decisions made by diverse models while utilizing external knowledge. Leveraging the Integer Linear Programming (ILP) framework, we map predictions from various models into globally normalized and comparable values by incorporating information about decisions' prior probability, confidence (uncertainty), and the models' expected accuracy. Our empirical study demonstrates the superiority of our approach over conventional baselines on multiple datasets.
    
[^41]: 基于相似性的邻居选择用于图形LLMs

    Similarity-based Neighbor Selection for Graph LLMs

    [https://arxiv.org/abs/2402.03720](https://arxiv.org/abs/2402.03720)

    基于相似性的邻居选择（SNS）通过改善所选邻居的质量，改善了图形表示，并提高了泛化性能和可扩展性，解决了处理文本属性图（TAGs）的挑战。

    

    文本属性图（TAGs）在直接处理语言学习模型（LLMs）时面临独特挑战，但它们的广泛常识知识和强大的推理能力为TAGs中的节点分类提供了极大的希望。先前的研究在这一领域中已经解决了过度压缩、异质性和信息集成不当等问题，同时还受到数据集分区的不一致性和高级LLMs的低利用率的影响。为了解决这些挑战，我们引入了基于相似性的邻居选择（SNS）。使用SimCSE和高级邻居选择技术，SNS有效提高了所选邻居的质量，从而改善了图形表示并减轻了过度压缩和异质性等问题。此外，作为一种归纳和无需训练的方法，SNS在传统GNN方法上展示了更强的泛化和可伸缩性。我们的全面实验符合标准的数据集分区实践。

    Text-attributed graphs (TAGs) present unique challenges for direct processing by Language Learning Models (LLMs), yet their extensive commonsense knowledge and robust reasoning capabilities offer great promise for node classification in TAGs. Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs. To address these challenges, we introduce Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor selection techniques, SNS effectively improves the quality of selected neighbors, thereby improving graph representation and alleviating issues like over-squashing and heterophily. Besides, as an inductive and training-free approach, SNS demonstrates superior generalization and scalability over traditional GNN methods. Our comprehensive experiments, adhering to standard dataset partitioning prac
    
[^42]: 用主动询问赋予语言模型更深入的理解能力

    Empowering Language Models with Active Inquiry for Deeper Understanding

    [https://arxiv.org/abs/2402.03719](https://arxiv.org/abs/2402.03719)

    本文提出了一种名为LaMAI的语言模型，通过主动询问的方式与用户进行交互，有效提高了对用户查询的理解能力，并减少了错误解读的发生。

    

    大型语言模型（LLMs）的兴起革新了我们通过自然语言与人工智能系统互动的方式。然而，由于LLMs对用户查询意图的不确定性，它们经常会错误解读用户的查询，导致不太有帮助的回复。在自然人类互动中，通过有针对性的问题寻求澄清，以揭示不明确的信息。因此，在本文中，我们引入了LaMAI（带有主动询问的语言模型），旨在赋予LLMs与用户的交互性。LaMAI利用主动学习技术提出最具信息量的问题，促进动态的双向对话。这种方法不仅缩小了上下文差距，还改善了LLMs的输出，使其更加符合用户的期望。我们在各种复杂数据集上进行的实证研究表明了LaMAI的有效性，特别是在LLMs的对话上下文有限的情况下，回答的准确率从31.9%提高到50%

    The rise of large language models (LLMs) has revolutionized the way that we interact with artificial intelligence systems through natural language. However, LLMs often misinterpret user queries because of their uncertain intention, leading to less helpful responses. In natural human interactions, clarification is sought through targeted questioning to uncover obscure information. Thus, in this paper, we introduce LaMAI (Language Model with Active Inquiry), designed to endow LLMs with this same level of interactive engagement. LaMAI leverages active learning techniques to raise the most informative questions, fostering a dynamic bidirectional dialogue. This approach not only narrows the contextual gap but also refines the output of the LLMs, aligning it more closely with user expectations. Our empirical studies, across a variety of complex datasets where LLMs have limited conversational context, demonstrate the effectiveness of LaMAI. The method improves answer accuracy from 31.9% to 50
    
[^43]: 听、聊、编辑：基于文本指导的声景修改以增强听觉体验

    Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced Auditory Experience

    [https://arxiv.org/abs/2402.03710](https://arxiv.org/abs/2402.03710)

    本研究引入了一种新颖的多模态声音混合编辑器，通过用户提供的文本指令实现对声音源的修改，实现了同时编辑多个声音源的能力，无需将它们分离。实验证明了该编辑器的实用性和效果。

    

    在日常生活中，我们遇到各种各样的声音，有些是我们期望的，有些是我们不希望的，对它们的存在和音量的控制有限。我们的工作引入了一种新颖的多模态声音混合编辑器"听、聊、编辑"(LCE)，该编辑器根据用户提供的文本指令修改混合中的每个声音源。LCE通过用户友好的聊天界面以及其在不需要将声音源分离的情况下同时对多个声音源进行编辑的能力而与众不同。用户输入开放性的文本提示，这些提示由大型语言模型解释，用于创建编辑声音混合的语义滤波器。系统然后将混合解析成其组成部分，应用语义滤波器，并将其重新组装成期望的输出。我们开发了一个包括语音和各种音频源以及用于不同编辑任务的文本提示的160小时数据集，包括提取、删除和音量控制。我们的实验证明。

    In daily life, we encounter a variety of sounds, both desirable and undesirable, with limited control over their presence and volume. Our work introduces "Listen, Chat, and Edit" (LCE), a novel multimodal sound mixture editor that modifies each sound source in a mixture based on user-provided text instructions. LCE distinguishes itself with a user-friendly chat interface and its unique ability to edit multiple sound sources simultaneously within a mixture, without needing to separate them. Users input open-vocabulary text prompts, which are interpreted by a large language model to create a semantic filter for editing the sound mixture. The system then decomposes the mixture into its components, applies the semantic filter, and reassembles it into the desired output. We developed a 160-hour dataset with over 100k mixtures, including speech and various audio sources, along with text prompts for diverse editing tasks like extraction, removal, and volume control. Our experiments demonstrat
    
[^44]: 人类与机器：重新思考语言模型在蕴含验证中的应用

    Minds versus Machines: Rethinking Entailment Verification with Language Models

    [https://arxiv.org/abs/2402.03686](https://arxiv.org/abs/2402.03686)

    本文通过研究人类和大型语言模型在推理判断中的共性和差异，发现大型语言模型在复杂推理中具有优势，而人类在简单推理中表现出色。基于这些发现，引入了一个优化的Flan-T5模型，用于蕴含验证。

    

    人类在文本理解中进行大量的推理以理解论述。本文旨在了解人类和最先进的大型语言模型（LLM）在推理判断中的共性和差异。通过综合策划的蕴含验证基准测试，我们评估了人类和LLM在各种推理类别中的表现。我们的基准测试包含了来自三个类别（NLI、上下文QA和解释）的数据集，包括多句前提和不同的知识类型，从而评估了复杂推理情况下的推理能力。值得注意的是，我们的发现显示LLM在跨扩展上下文的多跳推理中具有优势，而人类在需要简单演绎推理的任务中表现出色。利用这些见解，我们介绍了一个经过精细调整的Flan-T5模型，其性能超过了GPT-3.5，并与GPT-4媲美，提供了一个强大的开源解决方案供蕴含验证使用。作为一个实际的应用

    Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application
    
[^45]: 大型语言模型作为间接推理器：对自动推理的反证和矛盾

    Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning

    [https://arxiv.org/abs/2402.03667](https://arxiv.org/abs/2402.03667)

    本文提出了一种大型语言模型的新型间接推理方法，使用反证和矛盾的逻辑来处理复杂推理任务，并通过增强数据和规则，以及设计提示模板的方式增强模型的推理能力。

    

    最近，人们越来越关注提高大型语言模型（LLMs）进行复杂推理的能力。然而，以前的方法主要是遵循直接推理（DR）框架，如“思维链”和“自一致性”，因此在解决很难通过DR解决的众多实际问题时会遇到困难。因此，为了增强LLMs的推理能力，本文提出了一种新颖的间接推理（IR）方法，该方法利用反证和矛盾的逻辑来处理事实推理和数学证明等IR任务。具体而言，我们的方法包括两个步骤。首先，我们利用反证的逻辑等价性来增强LLMs的数据和规则，以提高其可理解性。其次，我们设计了一组提示模板，触发LLMs进行基于矛盾证明的IR，其逻辑上等价于原始的DR过程。我们的IR方法简单而有效。

    Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning. However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs. Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and c
    
[^46]: 使用自反大型语言模型学习生成可解释的股票预测

    Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models

    [https://arxiv.org/abs/2402.03659](https://arxiv.org/abs/2402.03659)

    这个论文介绍了使用大型语言模型生成可解释的股票预测的方法，并提出了Summarize-Explain-Predict（SEP）模型来解决股票预测中的解释问题和数据标注成本的挑战。

    

    对于传统的非生成式深度学习模型来说，解释股票预测通常是一项困难的任务，其中解释仅限于可视化重要文本上的注意力权重。目前，大型语言模型（LLM）为解决这个问题提供了一个解决方案，因为它们具有生成人类可读解释其决策过程的能力。然而，股票预测对LLM来说仍然具有挑战性，因为它需要能够权衡混乱社会文本对股票价格的不同影响。随着引入解释组件，问题变得越来越困难，需要LLM能够用口头方式解释为什么某些因素比其他因素更重要。另一方面，要为这样的任务对LLM进行微调，需要专家标注的样本来解释训练集中的每次股票波动，这在成本和实际可扩展性上是昂贵且不可行的。为了解决这些问题，我们提出了我们的Summarize-Explain-Predict（SEP）模型。

    Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) 
    
[^47]: 在对话中增强情感的基于图的讽刺解释

    Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue

    [https://arxiv.org/abs/2402.03658](https://arxiv.org/abs/2402.03658)

    本论文提出了一种名为EDGE的新颖的基于图的情感增强多模态讽刺解释框架，旨在为涉及多种模态的讽刺对话生成自然语言解释。该框架克服了话语记号对情感的多样效应、视频音频情感信号与BART嵌入空间之间的差距以及话语、话语情感和视频音频情感之间的不同关系等挑战。

    

    对话中的讽刺解释（SED）是一项新而具有挑战性的任务，旨在为涉及多种模态（即话语、视频和音频）的讽刺对话生成自然语言解释。尽管现有的研究基于生成式预训练语言模型BART取得了巨大成功，但它们忽视了话语、视频和音频中存在的情感，而这些情感是讽刺解释中的重要线索。事实上，由于以下三个主要挑战：1）话语记号对情感的多样效应；2）视频音频情感信号与BART的嵌入空间之间的差距；3）话语、话语情感和视频音频情感之间的不同关系，将情感融入以提升SED性能是一项非常复杂的任务。为了解决这些挑战，我们提出了一种新颖的基于图的增强情感的多模态讽刺解释框架，命名为EDGE。

    Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which aims to generate a natural language explanation for the given sarcastic dialogue that involves multiple modalities (i.e., utterance, video, and audio). Although existing studies have achieved great success based on the generative pretrained language model BART, they overlook exploiting the sentiments residing in the utterance, video and audio, which are vital clues for sarcasm explanation. In fact, it is non-trivial to incorporate sentiments for boosting SED performance, due to three main challenges: 1) diverse effects of utterance tokens on sentiments; 2) gap between video-audio sentiment signals and the embedding space of BART; and 3) various relations among utterances, utterance sentiments, and video-audio sentiments. To tackle these challenges, we propose a novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation framework, named EDGE. In particular, we first propose a lexicon-guided utterance sen
    
[^48]: Stanceosaurus 2.0: 对俄罗斯和西班牙的虚假信息分类

    Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation

    [https://arxiv.org/abs/2402.03642](https://arxiv.org/abs/2402.03642)

    Stanceosaurus 2.0扩展了原始框架，新增对俄罗斯和西班牙的分类，旨在支持分析跨文化和跨语言的虚假信息。通过与初始研究的结果相当的零-shot跨语言迁移，验证了数据的价值和立场分类的可行性。

    

    Stanceosaurus 语料库（Zheng等，2022）旨在提供高质量的、标注的、从Twitter中提取的五分类立场数据，适用于分析跨文化和跨语言的虚假信息。在Stanceosaurus 2.0版本中，我们扩展了这个框架，包括了俄罗斯和西班牙。前者由于与西方紧张局势和对乌克兰的暴力入侵而变得具有重要意义。而后者则代表着一个被主要社交媒体平台忽视的庞大社群。通过添加3,874条西班牙和俄语推文，涉及41则虚假信息，我们的目标是支持关注这些问题的研究。为了展示这些数据的价值，我们在多语言BERT上使用零-shot跨语言迁移，得到了与最初的Stanceosaurus研究相当的结果，两种语言的macro F1得分均为43。这显示了立场分类的可行性。

    The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide high-quality, annotated, 5-way stance data extracted from Twitter, suitable for analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus 2.0 iteration, we extend this framework to encompass Russian and Spanish. The former is of current significance due to prevalent misinformation amid escalating tensions with the West and the violent incursion into Ukraine. The latter, meanwhile, represents an enormous community that has been largely overlooked on major social media platforms. By incorporating an additional 3,874 Spanish and Russian tweets over 41 misinformation claims, our objective is to support research focused on these issues. To demonstrate the value of this data, we employed zero-shot cross-lingual transfer on multilingual BERT, yielding results on par with the initial Stanceosaurus study with a macro F1 score of 43 for both languages. This underlines the viability of stance classificatio
    
[^49]: 专业代理人 - 将大型语言模型演变为具有人类水平能力的自主专家

    Professional Agents -- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies

    [https://arxiv.org/abs/2402.03628](https://arxiv.org/abs/2402.03628)

    本论文介绍了专业代理人（PAgents）的概念，利用大型语言模型（LLMs）的能力创建具有人类水平能力和专业水平的自主代理人，以重塑专业服务。通过不断发展的专业知识和整合，PAgents的提升可能实现人工智能系统在复杂领域展示专业精通和人类水平。

    

    大型语言模型（LLMs）如ChatGPT、PaLM和GPT-4的出现，推动了自然语言处理的卓越进展，展示了人类级语言流利性和推理能力。本论文介绍了专业代理人（PAgents）的概念，这是一个应用框架，利用LLM的能力创建具有可控、专业水平的交互式自主代理人。我们认为，专业代理人通过不断发展的专业知识可以重塑专业服务。我们提出的专业代理人框架包括三层架构：基础工具层、中间代理人层和顶层协同层。本文旨在推动关于LLMs有前景的真实应用的讨论。我们认为，专业代理人的不断提升和整合可能导致AI系统在复杂领域展示专业精通，满足关键需求，并有可能实现人工智能的人类水平。

    The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4 has catalyzed remarkable advances in natural language processing, demonstrating human-like language fluency and reasoning capacities. This position paper introduces the concept of Professional Agents (PAgents), an application framework harnessing LLM capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies. We posit that PAgents can reshape professional services through continuously developed expertise. Our proposed PAgents framework entails a tri-layered architecture for genesis, evolution, and synergy: a base tool layer, a middle agent layer, and a top synergy layer. This paper aims to spur discourse on promising real-world applications of LLMs. We argue the increasing sophistication and integration of PAgents could lead to AI systems exhibiting professional mastery over complex domains, serving critical needs, and potentially achieving artifici
    
[^50]: 近似的中心化softmax损失用于视觉-语言模型的鲁棒性

    Partially Recentralization Softmax Loss for Vision-Language Models Robustness

    [https://arxiv.org/abs/2402.03627](https://arxiv.org/abs/2402.03627)

    本文研究了通过修改预训练多模态模型的损失函数来提高对抗鲁棒性，通过限制前K个softmax输出。实验结果表明，经过微调后，模型的对抗鲁棒性显著提高，能够有效抵御常见的攻击。

    

    随着大型语言模型在自然语言处理任务中的突破，多模态技术变得非常流行。然而，已经证明多模态自然语言处理模型容易受到对抗攻击，即模型的输出可以通过对输入进行微小扰动而发生巨大变化。虽然计算机视觉和自然语言处理模型中已经提出了几种防御技术，但对多模态模型的鲁棒性还没有进行充分探索。在本文中，我们研究了通过修改预训练多模态模型的损失函数，通过限制前K个softmax输出来提供的对抗鲁棒性。基于评估和评分，我们的实验结果显示，在经过微调后，预训练模型的对抗鲁棒性可以显着提高，对抗常见的攻击有效。进一步的研究应该探索这类损失函数的输出多样性、泛化能力以及鲁棒性和性能之间的平衡。我们的代码将在之后提供。

    As Large Language Models make a breakthrough in natural language processing tasks (NLP), multimodal technique becomes extremely popular. However, it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input. While several defense techniques have been proposed both in computer vision and NLP models, the multimodal robustness of models have not been fully explored. In this paper, we study the adversarial robustness provided by modifying loss function of pre-trained multimodal models, by restricting top K softmax outputs. Based on the evaluation and scoring, our experiments show that after a fine-tuning, adversarial robustness of pre-trained models can be significantly improved, against popular attacks. Further research should be studying, such as output diversity, generalization and the robustness-performance trade-off of this kind of loss functions. Our code will be available after th
    
[^51]: 自我发现：大型语言模型自动构建推理结构

    Self-Discover: Large Language Models Self-Compose Reasoning Structures

    [https://arxiv.org/abs/2402.03620](https://arxiv.org/abs/2402.03620)

    SELF-DISCOVER是一个通用框架，能让大型语言模型自主发现任务内在的推理结构，显著提升了复杂推理问题的解决性能，并在推理计算方面取得更好的效果。

    

    我们引入了SELF-DISCOVER，一个通用框架，用于使LLM自主发现任务内在的推理结构，以应对对于典型提示方法而言具有挑战性的复杂推理问题。该框架的核心是一个自我发现过程，在这个过程中，LLM选择多个原子推理模块，如批判性思维和逐步思考，并将它们组合成LLM在解码过程中遵循的明确推理结构。SELF-DISCOVER在具有挑战性的推理基准测试中，如BigBench-Hard、基于代理的推理和MATH上，相较于Chain of Thought (CoT)的性能提升高达32%。此外，SELF-DISCOVER在需要10-40倍较少推理计算的情况下，较CoT-Self-Consistency等推理密集方法的表现更好，超过20%。最后，我们证明了自我发现的推理结构在模型家族之间是普遍适用的：从PaLM 2-L到GPT-4，从GPT-4到Llama2，并分享了

    We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share 
    
[^52]: 使用多模态连续复制比较人类和大型语言模型中的抽象能力

    Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction

    [https://arxiv.org/abs/2402.03618](https://arxiv.org/abs/2402.03618)

    这项研究通过实现一个多模态连续复制框架，比较了人类和GPT-4的抽象能力。实验结果表明，将语言作为一种模态对人类的复制影响更大，这暗示人类的视觉和语言表征比GPT-4的表征更具分离性。

    

    人类从嘈杂的感官数据中提取有用的世界抽象。连续复制允许我们通过类似于电话游戏的范式来研究人们如何构建世界，其中一个人观察一个刺激并将其复制给下一个人形成一条复制链。过去的连续复制实验通常采用单一感觉模式，但人类经常通过语言相互传达世界的抽象。为了调查语言对抽象形成的影响，我们通过要求接收视觉刺激的人以语言形式来复制它，并反之亦然，实现了一种新颖的多模态连续复制框架。我们对人类和GPT-4进行了单模态和多模态的连续复制，并发现将语言作为一种模态对人类的复制影响更大。这表明人类的视觉和语言表征比GPT-4的表征更具分离性。

    Humans extract useful abstractions of the world from noisy sensory data. Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions. Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language. To investigate the effect language on the formation of abstractions, we implement a novel multimodal serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa. We ran unimodal and multimodal chains with both humans and GPT-4 and find that adding language as a modality has a larger effect on human reproductions than GPT-4's. This suggests human visual and linguistic representations are more dissociable than those of GPT-4.
    
[^53]: 利用大型语言模型进行混合工作场所决策支持

    Leveraging Large Language Models for Hybrid Workplace Decision Support

    [https://arxiv.org/abs/2402.03616](https://arxiv.org/abs/2402.03616)

    本论文研究了利用大型语言模型（LLMs）为混合工作场所提供智能决策支持的决策支持模型。通过广泛的用户研究和评估，发现LLMs在提供适当工作区建议方面具有超越提示的推理能力，提高工作者的工作体验。

    

    大型语言模型（LLMs）具有执行各种文本处理任务并为提议的操作或决策提供文本解释的潜力。在混合工作时代，LLMs可以为设计混合工作计划的工作者提供智能决策支持。特别是它们可以为平衡众多决策因素的工作者提供建议和解释，从而增强他们的工作体验。在本文中，我们提出了一个在混合工作环境中工作区决策支持模型，利用LLMs的推理能力。我们首先研究了LLMs对于提供适当工作区建议的能力。我们发现，其推理能力超越了提示中的指导方针，LLMs可以在工作区资源的可用性之间进行权衡。我们进行了广泛的用户研究，以了解工作者在工作区选择上的决策过程，并评估系统的有效性。我们观察到，工作者的决策可能受到影响。

    Large Language Models (LLMs) hold the potential to perform a variety of text processing tasks and provide textual explanations for proposed actions or decisions. In the era of hybrid work, LLMs can provide intelligent decision support for workers who are designing their hybrid work plans. In particular, they can offer suggestions and explanations to workers balancing numerous decision factors, thereby enhancing their work experience. In this paper, we present a decision support model for workspaces in hybrid work environments, leveraging the reasoning skill of LLMs. We first examine LLM's capability of making suitable workspace suggestions. We find that its reasoning extends beyond the guidelines in the prompt and the LLM can manage the trade-off among the available resources in the workspaces. We conduct an extensive user study to understand workers' decision process for workspace choices and evaluate the effectiveness of the system. We observe that a worker's decision could be influe
    
[^54]: RAP：具有上下文记忆的检索增强规划用于多模态LLM代理

    RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents

    [https://arxiv.org/abs/2402.03610](https://arxiv.org/abs/2402.03610)

    本研究提出了一种称为RAP的框架，它能够以动态方式利用过去的经验来增强代理的规划能力，并在纯文本和多模态环境中表现出色。实验结果显示RAP在文本场景中达到了最先进水平，并显著提高了多模态LLM代理在具身任务中的性能。

    

    最近的进展使得大型语言模型(LLM)可以被部署为用于机器人、游戏和API集成等领域中越来越复杂的决策应用的代理。然而，利用过去的经验来指导当前的决策过程仍然是一个困难的问题。为解决这个问题，我们提出了一种称为检索增强规划（RAP）的框架，旨在动态地利用过去与当前情况和上下文相关的经验，从而提升代理的规划能力。RAP的特点在于它的多功能性：它在纯文本和多模态环境中表现出色，适用于多个任务。实证评估结果显示RAP的有效性，在文本场景中取得了SOTA的表现，并显著提升了多模态LLM代理在具身任务中的性能。这些结果突显了RAP在推进LLM的功能和适用性方面的潜力。

    Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM 
    
[^55]: 提高多模态营销的上下文一致性：知识基础学习的有效性

    Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning

    [https://arxiv.org/abs/2402.03607](https://arxiv.org/abs/2402.03607)

    本研究提出了一种将常识知识图谱与大型视觉语言模型相结合的框架，用于改进预测多模态营销活动效果的性能。该方法能够提供早期检测可能具有说服力的多模态活动并评估和增强营销理论的能力。

    

    智能设备的普及使用户能够在线体验多模态信息。然而，大型语言模型（LLM）和视觉模型（LVM）仍然受到捕捉跨模态语义关系的整体意义的限制。缺乏明确的常识知识（例如，作为一个知识图谱），视觉语言模型（VLM）仅通过捕捉庞大的语料库中的高级模式来学习隐式表示，从而忽略了重要的上下文跨模态线索。在这项工作中，我们设计了一个框架，将显式的常识知识以知识图谱的形式与大型的VLM相结合，以提高下游任务的性能，即预测多模态营销活动的有效性。虽然营销应用提供了一个有说服力的指标来评估我们的方法，但我们的方法使得早期发现可能具有说服力的多模态活动成为可能，并评估和增强营销理论。

    The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience multimodal information online. However, large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships. Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues. In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of multi-modal marketing campaigns. While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive multi-modal campaigns and the assessment and augmentation of marketing theory.
    
[^56]: 使用大型语言模型从实际数据中识别避孕药切换的原因

    Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models

    [https://arxiv.org/abs/2402.03597](https://arxiv.org/abs/2402.03597)

    本研究评估了一种大型语言模型GPT-4在识别避孕药切换原因上的能力，结果表明GPT-4可以准确地从临床记录中提取避孕药切换的原因，相较于基准BERT模型有更好的表现。

    

    处方避孕药在支持妇女生殖健康方面扮演着关键角色。在美国有将近5000万女性使用避孕药，了解导致避孕药选择和切换的因素非常重要。然而，与药物切换相关的许多因素通常只在无结构的临床记录中得到捕获，并且很难提取。在这里，我们评估了最近开发的大型语言模型GPT-4（通过符合HIPAA的Microsoft Azure API）的零-shot能力，以从UCSF信息共享平台的临床记录数据集中识别避孕药类别切换的原因。我们证明了GPT-4可以准确地提取避孕药切换的原因，相较于基准BERT模型，在避孕药开始和停止提取方面的microF1分数分别为0.849和0.881。对于GPT-4提取的切换原因的人工评估显示出91.4%的准确度，出现幻觉的情况很少。

    Prescription contraceptives play a critical role in supporting women's reproductive health. With nearly 50 million women in the United States using contraceptives, understanding the factors that drive contraceptives selection and switching is of significant interest. However, many factors related to medication switching are often only captured in unstructured clinical notes and can be difficult to extract. Here, we evaluate the zero-shot abilities of a recently developed large language model, GPT-4 (via HIPAA-compliant Microsoft Azure API), to identify reasons for switching between classes of contraceptives from the UCSF Information Commons clinical notes dataset. We demonstrate that GPT-4 can accurately extract reasons for contraceptive switching, outperforming baseline BERT-based models with microF1 scores of 0.849 and 0.881 for contraceptive start and stop extraction, respectively. Human evaluation of GPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal hallucin
    
[^57]: 用语言模型区分可知与不可知的能力

    Distinguishing the Knowable from the Unknowable with Language Models

    [https://arxiv.org/abs/2402.03563](https://arxiv.org/abs/2402.03563)

    通过研究大型语言模型，在自由文本中识别作为代理的模型和冻结预训练模型的嵌入的小型线性探测器可以准确预测更大模型令牌级别上的自信度，进一步提出了一种无监督的方法在相同任务上达到了非平凡的准确度，这证明了语言模型中存在不同类型的不确定性表示。

    

    我们研究了在大型语言模型（LLMs）生成的自由文本输出中，是否可以鉴别出认知不确定性（反映缺乏知识的不确定性）和偶然不确定性（反映基础分布中的熵）。在没有真实概率的情况下，我们探索了一个设置，在这个设置中，为了（近似地）分解给定LLM的不确定性，一个明显更大的模型充当地面真相的代理。我们表明，基于冻结预训练模型的嵌入的小型线性探测器能够准确预测在令牌级别上更大模型将更自信的情况，并且在一个文本领域上训练的探测器可以泛化到其他领域。进一步地，我们提出了一种完全无监督的方法，在相同任务上达到了非平凡的准确度。综合考虑这些结果，我们解释这些结果作为LLMs内部自然地包含了不同类型不确定性的表示，这可能有助于制定更有效的方法。

    We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more i
    
[^58]: VLN-Video: 利用行车视频进行室外视觉与语言导航

    VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation

    [https://arxiv.org/abs/2402.03561](https://arxiv.org/abs/2402.03561)

    VLN-Video利用行车视频的多样室外环境和自动生成的导航指令与动作，通过深度学习方法提高了室外视觉与语言导航的性能。

    

    室外视觉与语言导航（VLN）要求代理根据自然语言指令在逼真的三维室外环境中导航。现有的VLN方法在导航环境多样性和训练数据有限性方面存在限制。为解决这些问题，我们提出了VLN-Video，该方法利用在美国多个城市的行车视频中存在的多样化室外环境，并通过自动生成导航指令和动作来提高室外VLN性能。VLN-Video结合了直观经典方法和现代深度学习技术的优势，利用模板填充生成有实际基础的导航指令，并结合基于图像旋转相似度的导航动作预测器从行车视频中获取VLN风格的数据，用于预训练深度学习VLN模型。我们在Touchdown数据集和由行车视频创建的视频增强数据集上对模型进行预训练。

    Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with th
    
[^59]: 在西班牙语中解决转录歧义：一种混合声学-词汇系统用于标点符号恢复

    Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration

    [https://arxiv.org/abs/2402.03519](https://arxiv.org/abs/2402.03519)

    提出了一种混合声学-词汇系统，用于解决西班牙语标点符号恢复任务，通过整合声学和词汇信号，在西班牙语转录中提高了问号的F1分数和整体标点符号恢复，并在准确性、可靠性和延迟方面超越了大型语言模型。

    

    标点符号恢复是自动语音识别（ASR）系统后的关键步骤，以增强转录的可读性并促进后续的自然语言处理任务。然而，传统的基于词汇的方法不足以解决西班牙语中标点符号恢复的任务，因为在未标点的陈述句和疑问句之间常常存在歧义。在本研究中，我们提出了一种新颖的混合声学-词汇标点符号恢复系统，用于西班牙语转录，通过模块化的过程整合声学和词汇信号。我们的实验结果表明，所提出的系统可以有效提高问号的F1分数，并在公共和内部的西班牙语对话数据集上实现整体标点符号恢复。此外，与大型语言模型（LLMs）的基准比较表明，我们的方法在准确性、可靠性和延迟方面具有优势。此外，我们还展示了ASR模块的词错误率（WER）也受益于这种方法。

    Punctuation restoration is a crucial step after Automatic Speech Recognition (ASR) systems to enhance transcript readability and facilitate subsequent NLP tasks. Nevertheless, conventional lexical-based approaches are inadequate for solving the punctuation restoration task in Spanish, where ambiguity can be often found between unpunctuated declaratives and questions. In this study, we propose a novel hybrid acoustic-lexical punctuation restoration system for Spanish transcription, which consolidates acoustic and lexical signals through a modular process. Our experiment results show that the proposed system can effectively improve F1 score of question marks and overall punctuation restoration on both public and internal Spanish conversational datasets. Additionally, benchmark comparison against LLMs (Large Language Model) indicates the superiority of our approach in accuracy, reliability and latency. Furthermore, we demonstrate that the Word Error Rate (WER) of the ASR module also benef
    
[^60]: 跨领域评估零样本摘要生成器的真实性

    Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains

    [https://arxiv.org/abs/2402.03509](https://arxiv.org/abs/2402.03509)

    本研究跨领域评估了零样本摘要生成器的真实性。通过对生物医学文章和法律法案等专业领域进行评估，我们特别关注生成摘要的真实性，并分析了预训练语料库中给定领域的普遍性对摘要质量的影响。

    

    最近的研究表明，大型语言模型(LLMs)能够零样本（即在没有明确监督的情况下）生成摘要，经过人工评估，这些摘要往往与手工编写的参考摘要相比，甚至更受欢迎。然而，这些早期的研究几乎专注于评估新闻文章的摘要。零样本摘要生成器在其他（可能更专业）领域中的表现如何？在这项工作中，我们评估了跨专业领域中零样本生成的摘要，包括生物医学文章和法律法案（除了标准新闻摘要的参考）。我们特别关注输出的真实性。我们从领域专家处获取注释，以识别摘要中的不一致之处，并对这些错误进行系统分类。我们分析了预训练语料库中给定领域的普遍性是否会影响在该领域的文章的摘要的提取和忠实度。我们发布了所有收集到的注释。

    Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains? In this work we evaluate zero-shot generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotat
    
[^61]: 神经网络抽象和推理：迈向机器的广泛泛化

    Neural networks for abstraction and reasoning: Towards broad generalization in machines

    [https://arxiv.org/abs/2402.03507](https://arxiv.org/abs/2402.03507)

    这项工作探索了神经网络在广泛泛化方面的应用，通过研究解决抽象和推理任务的新方法，试图提高计算机系统从少量示例中学习新概念的能力。

    

    半个世纪以来，人工智能研究一直试图复制人类的抽象和推理能力-创造出能够从一小组示例中学习新概念的计算机系统，在人们发现这很容易的情况下。虽然特定的神经网络能够解决各种各样的问题，但对于超越训练数据范围的广泛泛化仍然是一个难题。在这项工作中，我们研究了几种解决抽象与推理语料库（ARC）的新方法，ARC是一个用于测试算法在广泛泛化方面的抽象视觉推理任务数据集。尽管有三个国际竞赛提供了10万美元的奖金，最好的算法仍然无法解决大多数ARC任务，并且依赖于复杂的手工规则，没有使用机器学习。我们重新思考了最近神经网络的进展是否能在这个任务上取得进展。首先，我们将DreamCoder神经符号推理求解器应用到ARC上。

    For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task.   First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder
    
[^62]: 基于修复和替换的服装与背景生成关键技术

    An Inpainting-Infused Pipeline for Attire and Background Replacement

    [https://arxiv.org/abs/2402.03501](https://arxiv.org/abs/2402.03501)

    本研究提出了一种基于修复和替换的服装与背景生成关键技术，利用GenAI和计算机视觉中的先进技术，通过深度估计、修复掩模创建和稳定扩散和潜在一致性模型（LCMs），实现了对个人照片中服装和背景的修改和修复。

    

    最近几年，生成式人工智能（GenAI）的突破性进展引发了一场变革性的范式转变，对各个领域产生了重大影响。本文具体探讨了一种综合方法，利用GenAI和计算机视觉中的先进技术来进行图像处理。该方法包括深度估计、基于深度信息创建修复掩模、利用稳定扩散和潜在一致性模型（LCMs）生成和替换背景，通过修复流程替换服装和应用审美修改。本研究中的实验验证了该方法的有效性，突出了其产生视觉吸引力内容的潜力。这些先进技术的融合使用户能够输入个人照片并对其进行服装和背景的修改。

    In recent years, groundbreaking advancements in Generative Artificial Intelligence (GenAI) have triggered a transformative paradigm shift, significantly influencing various domains. In this work, we specifically explore an integrated approach, leveraging advanced techniques in GenAI and computer vision emphasizing image manipulation. The methodology unfolds through several stages, including depth estimation, the creation of inpaint masks based on depth information, the generation and replacement of backgrounds utilizing Stable Diffusion in conjunction with Latent Consistency Models (LCMs), and the subsequent replacement of clothes and application of aesthetic changes through an inpainting pipeline. Experiments conducted in this study underscore the methodology's efficacy, highlighting its potential to produce visually captivating content. The convergence of these advanced techniques allows users to input photographs of individuals and manipulate them to modify clothing and background b
    
[^63]: 注意力与事后可解释性相遇：数学视角

    Attention Meets Post-hoc Interpretability: A Mathematical Perspective

    [https://arxiv.org/abs/2402.03485](https://arxiv.org/abs/2402.03485)

    本文通过数学方式研究了基于注意力机制的架构，比较了事后解释和基于注意力机制的解释的差异，发现尽管有局限性，事后解释方法能够捕获比仅仅检查注意力权重更有用的洞察。

    

    注意力机制基于transformer等架构，成为了技术革命的核心。有趣的是，除了帮助在各种应用中获得最先进的结果之外，注意力机制本身还提供了关于模型内部行为的有意义洞察。这些洞察是否可以用作解释？关于此争论不断。本文通过数学方式研究了一个简单的基于注意力机制的架构，并准确定位了事后解释和基于注意力机制的解释之间的区别。我们表明它们提供了相当不同的结果，并且尽管有其局限性，事后解释方法能够捕获比仅仅检查注意力权重更有用的洞察。

    Attention-based architectures, in particular transformers, are at the heart of a technological revolution. Interestingly, in addition to helping obtain state-of-the-art results on a wide range of applications, the attention mechanism intrinsically provides meaningful insights on the internal behavior of the model. Can these insights be used as explanations? Debate rages on. In this paper, we mathematically study a simple attention-based architecture and pinpoint the differences between post-hoc and attention-based explanations. We show that they provide quite different results, and that, despite their limitations, post-hoc methods are capable of capturing more useful insights than merely examining the attention weights.
    
[^64]: 利用PubMed用户查询日志解释推荐的相关文章

    Harnessing PubMed User Query Logs for Post Hoc Explanations of Recommended Similar Articles

    [https://arxiv.org/abs/2402.03484](https://arxiv.org/abs/2402.03484)

    本研究利用PubMed用户查询日志构建了PubCLogs数据集，并采用事后方法解释推荐的相关文章，通过识别类似文章标题中的相关词汇来提供解释。这将帮助研究人员和临床医生在文献搜索中更方便地寻找相关文章。

    

    在科学研究中，基于引用文章寻找相关文章是一个重要的部分。像许多学术搜索引擎一样，PubMed拥有一个“类似文章”的功能，可以推荐与用户查看的当前文章相关的文章。解释推荐的文章对用户来说非常有用，特别是在文献搜索过程中。鉴于每年有超过一百万篇生物医学论文发表，解释推荐的相关文章将为研究人员和临床医生在寻找相关文章时提供便利。然而，目前大多数文献推荐系统都缺乏对其建议的解释。我们采用事后方法来解释推荐，通过识别类似文章标题中的相关词汇来实现。我们的主要贡献是通过重新利用PubMed用户查询日志中的560万个共点击文章对构建PubCLogs数据集。利用我们的PubCLogs数据集，我们训练了"Highlight Similar Article Title"模型。

    Searching for a related article based on a reference article is an integral part of scientific research. PubMed, like many academic search engines, has a "similar articles" feature that recommends articles relevant to the current article viewed by a user. Explaining recommended items can be of great utility to users, particularly in the literature search process. With more than a million biomedical papers being published each year, explaining the recommended similar articles would facilitate researchers and clinicians in searching for related articles. Nonetheless, the majority of current literature recommendation systems lack explanations for their suggestions. We employ a post hoc approach to explaining recommendations by identifying relevant tokens in the titles of similar articles. Our major contribution is building PubCLogs by repurposing 5.6 million pairs of coclicked articles from PubMed's user query logs. Using our PubCLogs dataset, we train the Highlight Similar Article Title 
    
[^65]: SWAG: 带有行动指导的故事讲述

    SWAG: Storytelling With Action Guidance

    [https://arxiv.org/abs/2402.03483](https://arxiv.org/abs/2402.03483)

    SWAG是一种新的故事讲述方法，通过将故事写作简化为搜索问题，使用两个模型的反馈循环来指导故事的发展方向。在GPT-4和人工评估中，SWAG表现出显著的优势，并且使用仅开源模型的SWAG流程超过了GPT-3.5-Turbo。

    

    自动长篇故事生成通常使用长上下文大语言模型（LLMs）进行一次性创建，它可以产生连贯但不一定引人入胜的内容。我们引入了带有行动指导的故事讲述（SWAG）的新方法。我们的方法通过两个模型的反馈循环将故事写作简化为一个搜索问题：一个LLM生成故事内容，另一个辅助LLM用于选择下一个最佳的“行动”，以引导故事的未来发展方向。我们的结果表明，当使用GPT-4和人工评估进行评估时，SWAG能够显著优于以往的端到端故事生成技术，并且我们只使用开源模型的SWAG流程超越了GPT-3.5-Turbo。

    Automated long-form story generation typically employs long-context large language models (LLMs) for one-shot creation, which can produce cohesive but not necessarily engaging content. We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach reduces story writing to a search problem through a two-model feedback loop: one LLM generates story content, and another auxiliary LLM is used to choose the next best "action" to steer the story's future direction. Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation, and our SWAG pipeline using only open-source models surpasses GPT-3.5-Turbo.
    
[^66]: 基于阿拉伯文同义词BERT的文本分类对抗样本

    Arabic Synonym BERT-based Adversarial Examples for Text Classification

    [https://arxiv.org/abs/2402.03477](https://arxiv.org/abs/2402.03477)

    本文首次在阿拉伯文中对文本分类模型进行了对抗攻击的词级研究，使用了基于同义词的BERT模型进行蒙版语言建模任务。通过评估生成的对抗样本与原样本的语法和语义相似性，研究了这些对抗样本的可传递性。

    

    文本分类系统已被证明对于对抗性文本示例具有脆弱性，这些修改后的文本示例在人眼中往往不易察觉，但可以迫使文本分类模型改变其分类结果。目前，对于量化对抗性文本攻击影响的研究工作大多只应用于英文模型。本文首次介绍了阿拉伯文中对抗性攻击的词级研究。具体来说，我们使用基于BERT模型的蒙版语言建模（MLM）任务的同义词（词级）攻击方法，在黑盒设置中评估最先进的阿拉伯文本分类模型对对抗性攻击的鲁棒性。为了评估使用我们的同义词BERT攻击生成的对抗样本的语法和语义相似性，我们请四名人类评估员评估和比较生成的对抗样本与原样本之间的差异。我们还研究了这些对抗样本的可传递性。

    Text classification systems have been proven vulnerable to adversarial text examples, modified versions of the original text examples that are often unnoticed by human eyes, yet can force text classification models to alter their classification. Often, research works quantifying the impact of adversarial text attacks have been applied only to models trained in English. In this paper, we introduce the first word-level study of adversarial attacks in Arabic. Specifically, we use a synonym (word-level) attack using a Masked Language Modeling (MLM) task with a BERT model in a black-box setting to assess the robustness of the state-of-the-art text classification models to adversarial attacks in Arabic. To evaluate the grammatical and semantic similarities of the newly produced adversarial examples using our synonym BERT-based attack, we invite four human evaluators to assess and compare the produced adversarial examples with their original examples. We also study the transferability of thes
    
[^67]: 大型语言模型几何信息的研究

    The Information of Large Language Model Geometry

    [https://arxiv.org/abs/2402.03471](https://arxiv.org/abs/2402.03471)

    论文研究了大型语言模型中嵌入的信息编码，并发现表示熵与模型大小呈幂律关系。通过信息理论和回归技术，建立了新标记的信息增益与岭回归之间的理论联系，并探索了Lasso回归在选择有意义的标记方面的有效性。实验结果表明，信息在标记之间分布，而不仅仅集中在特定的“有意义”的标记上。

    

    本文研究了大型语言模型中嵌入的信息编码。我们进行了模拟实验，分析了表示熵，并发现了与模型大小呈幂律关系的现象。基于这一观察，我们提出了一个基于（条件）熵的理论来解释这种规模定律现象。此外，我们深入探讨了LLMs的自回归结构，并使用信息理论和回归技术来分析最后一个标记与之前上下文标记之间的关系。具体而言，我们建立了新标记的信息增益与岭回归之间的理论联系。此外，我们还探索了Lasso回归在选择有意义的标记方面的有效性，有时表现优于紧密相关的注意力权重。最后，我们进行了对比实验，并发现信息分布在标记之间，而不仅仅集中在特定的“有意义”的标记上。

    This paper investigates the information encoded in the embeddings of large language models (LLMs). We conduct simulations to analyze the representation entropy and discover a power law relationship with model sizes. Building upon this observation, we propose a theory based on (conditional) entropy to elucidate the scaling law phenomenon. Furthermore, we delve into the auto-regressive structure of LLMs and examine the relationship between the last token and previous context tokens using information theory and regression techniques. Specifically, we establish a theoretical connection between the information gain of new tokens and ridge regression. Additionally, we explore the effectiveness of Lasso regression in selecting meaningful tokens, which sometimes outperforms the closely related attention weights. Finally, we conduct controlled experiments, and find that information is distributed across tokens, rather than being concentrated in specific "meaningful" tokens alone.
    
[^68]: 无偏好对齐学习与正则化相关奖励

    Preference-free Alignment Learning with Regularized Relevance Reward

    [https://arxiv.org/abs/2402.03469](https://arxiv.org/abs/2402.03469)

    无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。

    

    从人类偏好中学习被认为是将大规模语言模型（LLMs）与人类价值观对齐的关键。然而，与普遍的观点相反，我们的初步研究发现，基于人类偏好数据集训练的奖励模型倾向于给长的与主题无关的回复更高的分数，而给短的与主题相关的回复较低分。在这一观察的驱动下，我们探索了一种无偏好的方法，利用“相关性”作为对齐的一个关键目标。在我们的第一次尝试中，我们发现仅仅通过检索得到的相关性得分容易受到奖励欺骗的影响，即过度优化到不期望的捷径上，当我们将该得分作为奖励用于强化学习。为了缓解这个问题，我们将有效的归纳偏差整合到常规的相关性中，互相正则化，形成了一种混合奖励函数：正则化相关奖励（$R^3$）。$R^3$通过提供稳健的奖励信号，显著提高了在偏好基准测试中的性能。值得注意的是，$R^3$不需要

    Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
    
[^69]: 使用大型语言模型进行心理评估：注重隐私和具有成本效益的方法

    Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach

    [https://arxiv.org/abs/2402.03435](https://arxiv.org/abs/2402.03435)

    本研究使用大型语言模型（LLMs）分析Reddit用户的文本评论，以实现隐私保护和成本效益为重点，通过精心设计的提示和语法来实现预定义的自杀风险心理评估，取得了杰出的结果。

    

    本研究探索了使用大型语言模型（LLMs）分析Reddit用户的文本评论，主要目标有两个：第一，确定支持预定义的自杀风险心理评估的关键摘录；第二，总结材料以证实预分配的自杀风险水平。该工作局限于使用可以在本地运行的“开源”LLMs，从而增强数据隐私。此外，它优先选用计算要求较低的模型，以使有限的计算预算的个人和机构能够使用。实施的策略仅依赖于精心设计的提示和语法，以指导LLM的文本完成。尽管简单，评估指标显示出杰出的结果，使之成为一种有价值的注重隐私和具有成本效益的方法。此工作是2024 Computational Linguistics and Clinical Psychology (CLPsych)共享任务的一部分。

    This study explores the use of Large Language Models (LLMs) to analyze text comments from Reddit users, aiming to achieve two primary objectives: firstly, to pinpoint critical excerpts that support a predefined psychological assessment of suicidal risk; and secondly, to summarize the material to substantiate the preassigned suicidal risk level. The work is circumscribed to the use of "open-source" LLMs that can be run locally, thereby enhancing data privacy. Furthermore, it prioritizes models with low computational requirements, making it accessible to both individuals and institutions operating on limited computing budgets. The implemented strategy only relies on a carefully crafted prompt and a grammar to guide the LLM's text completion. Despite its simplicity, the evaluation metrics show outstanding results, making it a valuable privacy-focused and cost-effective approach. This work is part of the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared task.
    
[^70]: 通过自监督表示增强LLM基础的语音生成系统的稳定性

    Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations

    [https://arxiv.org/abs/2402.03407](https://arxiv.org/abs/2402.03407)

    通过自监督表示，我们提出了一种新的自助转换架构，该架构可以增强LLM基础的语音生成系统的稳定性，并克服了在推理时出现的多个稳定性问题。使用这种方法，LLM可以从文本中仅生成语音的内容和风格，而说话者身份由另一个模型提供。

    

    大型语言模型（LLMs）是下一代语音生成系统最有前景的技术之一，由于其可扩展性和上下文学习能力。然而，在推理时，它们遇到了多个稳定性问题，如幻觉、跳过内容或语音重复。在这项工作中，我们介绍了一种新的自监督语音转换（VC）架构，可以用于学习将瞬态特征（如内容）与固定特征（如说话者ID或录制条件）分开编码，创建说话者解耦表示。使用说话者解耦编码来训练LLMs进行文本到语音（TTS）允许LLM仅通过文本生成语音的内容和风格，类似于人类，而说话者身份由VC模型的解码器提供。结果显示，训练过说话者解耦的自监督表示的LLMs在说话者相似性方面提供了4.7pp的改进。

    Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity o
    
[^71]: UniTSyn：一个可增强大型语言模型在程序测试中的能力的大规模数据集

    UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing

    [https://arxiv.org/abs/2402.03396](https://arxiv.org/abs/2402.03396)

    本研究提出了UniTSyn，一个大型数据集，可以增强大型语言模型在程序测试中的能力。通过关联测试和被测试函数，UniTSyn能够提高模型在生成准确和完整的测试方面的表现。

    

    大型语言模型（LLM）在生成高质量代码方面的出色能力已经引起了软件测试社区的广泛关注。然而，现有的代码LLM在生成准确和完整的测试方面往往表现不佳，因为它们是在不区分测试目的代码和其他代码的情况下进行训练的。在本文中，我们提出了一个大规模数据集UniTSyn，它能够增强LLM在单元测试合成方面的能力。将测试与被测试函数进行关联对于LLM推断预期行为和要验证的逻辑路径至关重要。通过利用语言服务器协议，UniTSyn实现了在没有每个项目执行设置或易碎且难以扩展的每个语言启发式的情况下收集焦点测试对的挑战目标。它包含了五种主流编程语言的270万个焦点测试对，使其能够被应用于优化大型语言模型的能力。

    The remarkable capability of large language models (LLMs) in generating high-quality code has drawn increasing attention in the software testing community. However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate and complete tests since they were trained on code snippets collected without differentiating between code for testing purposes and other code. In this paper, we present a large-scale dataset UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test Synthesis. Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified. By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics that tend to be fragile and difficult to scale. It contains 2.7 million focal-test pairs across five mainstream programming languages, making it possible to be utilize
    
[^72]: 检测科学文献中的拙劣短语

    Detection of tortured phrases in scientific literature

    [https://arxiv.org/abs/2402.03370](https://arxiv.org/abs/2402.03370)

    本文介绍了自动检测科学论文中拙劣短语的方法，通过语言模型和预测分数的传播，可以高效地标记并提取这些拙劣短语，为领域专家验证提供新的数据。

    

    本文介绍了多种自动检测方法，用于从科学论文中提取所谓的拙劣短语。这些拙劣短语，例如将"信号与噪声"替换为"旗帜与喧闹"，是使用改写工具规避抄袭检测的结果。我们构建了一个数据集，并评估了几种策略来标记以前未记录的拙劣短语。提出和测试的方法基于语言模型，要么基于嵌入相似性，要么基于掩码标记的预测。我们发现，一种使用标记预测并将分数传播到块级别的方法效果最好。其召回率为0.87，精确率为0.61，可以检索到新的拙劣短语以供领域专家验证。

    This paper presents various automatic detection methods to extract so called tortured phrases from scientific papers. These tortured phrases, e.g. flag to clamor instead of signal to noise, are the results of paraphrasing tools used to escape plagiarism detection. We built a dataset and evaluated several strategies to flag previously undocumented tortured phrases. The proposed and tested methods are based on language models and either on embeddings similarities or on predictions of masked token. We found that an approach using token prediction and that propagates the scores to the chunk level gives the best results. With a recall value of .87 and a precision value of .61, it could retrieve new tortured phrases to be submitted to domain experts for validation.
    
[^73]: 评估谷歌语音识别和句子分类在医疗健康应用中的应用

    Evaluation of Google's Voice Recognition and Sentence Classification for Health Care Applications

    [https://arxiv.org/abs/2402.03369](https://arxiv.org/abs/2402.03369)

    本研究评估了谷歌的语音识别和句子分类在医疗健康应用中的应用。通过将语音识别技术应用于围手术期服务，可以改善患者流程和医疗质量。通过使用后处理分类器，本研究增强了谷歌的语音识别能力。

    

    本研究探讨了将语音识别技术应用于围手术期服务（Periop），以使Periop员工能够使用移动技术记录工作流程里程碑。如果能够使这种语音识别技术变得强大可靠，就可以通过使用移动技术改善患者流程和医疗质量。此实验的目标是使Periop员工能够提供无干扰的护理，而不需要进行数据录入和查询任务。然而，该研究的结果也适用于其他情况，即工程经理尝试使用移动技术改善通信性能。本研究通过使用后处理分类器（即句子包、支持向量机和最大熵）增强了谷歌的语音识别能力。实验研究了三个因素（原始措辞、简化措辞和个性化措辞）在三个水平（零次训练重复、5次训练重复和10次训练重复）的影响。

    This study examined the use of voice recognition technology in perioperative services (Periop) to enable Periop staff to record workflow milestones using mobile technology. The use of mobile technology to improve patient flow and quality of care could be facilitated if such voice recognition technology could be made robust. The goal of this experiment was to allow the Periop staff to provide care without being interrupted with data entry and querying tasks. However, the results are generalizable to other situations where an engineering manager attempts to improve communication performance using mobile technology. This study enhanced Google's voice recognition capability by using post-processing classifiers (i.e., bag-of-sentences, support vector machine, and maximum entropy). The experiments investigated three factors (original phrasing, reduced phrasing, and personalized phrasing) at three levels (zero training repetition, 5 training repetitions, and 10 training repetitions). Results 
    
[^74]: 带有大型语言模型的不确定性感知可解释推荐

    Uncertainty-Aware Explainable Recommendation with Large Language Models

    [https://arxiv.org/abs/2402.03366](https://arxiv.org/abs/2402.03366)

    这项研究开发了一个模型，通过训练用户和项目输入的ID向量作为提示，利用GPT-2实现不确定性感知的可解释推荐系统。该系统采用联合训练机制并在多任务学习框架中进行优化，能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。

    

    在推荐系统内提供解释能够提升用户满意度并建立信任，特别是通过详细说明为用户定制推荐项目的原因。当前领域中主要的方法是生成基于文本的解释，而大型语言模型（LLMs）的应用尤为突出。然而，由于时间和计算资源限制，改进LLMs以实现可解释的推荐在实践上是不可行的。作为替代方案，当前的方法是训练提示而不是LLM。在这项研究中，我们开发了一个模型，利用用户和项目输入的ID向量作为GPT-2的提示。我们在多任务学习框架中采用联合训练机制，优化推荐任务和解释任务。这种策略能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。通过实验，我们的方法表现出...

    Providing explanations within the recommendation system would boost user satisfaction and foster trust, especially by elaborating on the reasons for selecting recommended items tailored to the user. The predominant approach in this domain revolves around generating text-based explanations, with a notable emphasis on applying large language models (LLMs). However, refining LLMs for explainable recommendations proves impractical due to time constraints and computing resource limitations. As an alternative, the current approach involves training the prompt rather than the LLM. In this study, we developed a model that utilizes the ID vectors of user and item inputs as prompts for GPT-2. We employed a joint training mechanism within a multi-task learning framework to optimize both the recommendation task and explanation task. This strategy enables a more effective exploration of users' interests, improving recommendation effectiveness and user satisfaction. Through the experiments, our meth
    
[^75]: NanoNER: 使用领域专家知识和远程监督进行纳米生物学的命名实体识别

    NanoNER: Named Entity Recognition for nanobiology using experts' knowledge and distant supervision

    [https://arxiv.org/abs/2402.03362](https://arxiv.org/abs/2402.03362)

    本文介绍了NanoNER，它是一种用于纳米生物学的命名实体识别模型。通过使用领域专家的知识和远程监督学习，NanoNER能够准确地识别先前已知实体，并最大程度地提高注释数据的质量和数量。

    

    本文介绍了NanoNER的训练和评估，它是一种用于纳米生物学的命名实体识别（NER）模型。NER是在非结构化文本中识别特定实体的任务，在自然语言处理（NLP）和信息提取中经常是一个主要任务。我们的模型的目的是识别领域专家之前确定为该领域基本知识的实体。我们依靠本体论来提供领域词汇和分类，实现了一个迭代过程，使专家能够确定与当前领域相关的实体。然后我们深入探讨了远程监督学习在NER中的潜力，支持这种方法如何可以通过最少的人力增加注释数据的数量。在包含超过120k实体出现次数的728篇全文纳米生物学文章的完整语料库上，NanoNER在先前已知实体的识别上获得了0.98的F1分数。

    Here we present the training and evaluation of NanoNER, a Named Entity Recognition (NER) model for Nanobiology. NER consists in the identification of specific entities in spans of unstructured texts and is often a primary task in Natural Language Processing (NLP) and Information Extraction. The aim of our model is to recognise entities previously identified by domain experts as constituting the essential knowledge of the domain. Relying on ontologies, which provide us with a domain vocabulary and taxonomy, we implemented an iterative process enabling experts to determine the entities relevant to the domain at hand. We then delve into the potential of distant supervision learning in NER, supporting how this method can increase the quantity of annotated data with minimal additional manpower. On our full corpus of 728 full-text nanobiology articles, containing more than 120k entity occurrences, NanoNER obtained a F1-score of 0.98 on the recognition of previously known entities. Our model 
    
[^76]: 语义通信与知识学习的相互作用

    Interplay of Semantic Communication and Knowledge Learning

    [https://arxiv.org/abs/2402.03339](https://arxiv.org/abs/2402.03339)

    这篇论文介绍了语义通信与知识学习的相互作用，讨论了如何利用知识图谱来增强语义通信系统，并探索了使系统在不断演化的知识库中更有效运行的方法。同时还研究了将语义通信与大型语言模型集成的可能性。

    

    在迅速发展的通信技术领域，强调知识理解和处理的语义通信（SemCom）已经成为一个热门话题。通过整合人工智能技术，SemCom促进了对通信内容的深入理解、分析和传输。在本章中，我们着重阐明了SemCom中的知识学习手段，特别关注了知识图谱（KGs）的利用。具体而言，我们首先回顾了将SemCom与知识学习相结合的现有工作。随后，我们介绍了一个基于KG增强的SemCom系统，其中接收方通过从静态知识库中利用知识来改善解码性能。在此框架下，我们进一步探讨了能够使系统在不断演化的知识库中更有效地运行的潜在方法。此外，我们还研究了与大型语言模型集成的可能性。

    In the swiftly advancing realm of communication technologies, Semantic Communication (SemCom), which emphasizes knowledge understanding and processing, has emerged as a hot topic. By integrating artificial intelligence technologies, SemCom facilitates a profound understanding, analysis and transmission of communication content. In this chapter, we clarify the means of knowledge learning in SemCom with a particular focus on the utilization of Knowledge Graphs (KGs). Specifically, we first review existing efforts that combine SemCom with knowledge learning. Subsequently, we introduce a KG-enhanced SemCom system, wherein the receiver is carefully calibrated to leverage knowledge from its static knowledge base for ameliorating the decoding performance. Contingent upon this framework, we further explore potential approaches that can empower the system to operate in evolving knowledge base more effectively. Furthermore, we investigate the possibility of integration with Large Language Models
    
[^77]: Uni3D-LLM：利用大型语言模型统一点云感知、生成和编辑

    Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models

    [https://arxiv.org/abs/2402.03327](https://arxiv.org/abs/2402.03327)

    Uni3D-LLM是一个统一框架，利用大型语言模型实现了点云感知、生成和编辑任务的一体化。通过利用自然语言描述，用户可以轻松生成和修改点云场景中的对象，从而提高操作的灵活性和可控性。

    

    本文介绍了Uni3D-LLM，一个利用大型语言模型（LLM）将3D感知、生成和编辑任务整合为一体的统一框架。该框架使用户能够轻松地在点云场景中指定位置生成和修改对象，根据自然语言描述的多样性进行引导。Uni3D-LLM利用自然语言的表达能力，实现对3D对象生成和编辑的精确控制，从而显著提升操作的灵活性和可控性。通过将点云映射到统一的表示空间，Uni3D-LLM实现了跨应用功能，能够无缝执行各种任务，从精确实例化3D对象到交互设计的各种需求。通过一系列严格的实验，验证了Uni3D-LLM在点云理解、生成和编辑方面的有效性。

    In this paper, we introduce Uni3D-LLM, a unified framework that leverages a Large Language Model (LLM) to integrate tasks of 3D perception, generation, and editing within point cloud scenes. This framework empowers users to effortlessly generate and modify objects at specified locations within a scene, guided by the versatility of natural language descriptions. Uni3D-LLM harnesses the expressive power of natural language to allow for precise command over the generation and editing of 3D objects, thereby significantly enhancing operational flexibility and controllability. By mapping point cloud into the unified representation space, Uni3D-LLM achieves cross-application functionality, enabling the seamless execution of a wide array of tasks, ranging from the accurate instantiation of 3D objects to the diverse requirements of interactive design. Through a comprehensive suite of rigorous experiments, the efficacy of Uni3D-LLM in the comprehension, generation, and editing of point cloud has
    
[^78]: DeepSeekMath: 将开放语言模型中的数学推理能力推向极限

    DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

    [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)

    DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。

    

    数学推理由于其复杂和结构化的特性，对语言模型提出了重大挑战。本文介绍了DeepSeekMath 7B，它在Common Crawl中获取了120B个与数学相关的标记，并结合了自然语言和代码数据来继续预训练DeepSeek-Coder-Base-v1.5 7B。DeepSeekMath 7B在竞赛级别的数学基准测试中取得了令人印象深刻的51.7%的分数，无需依赖外部工具包和投票技术，接近了Gemini-Ultra和GPT-4的性能水平。DeepSeekMath 7B的自一致性在MATH上的64个样本中达到了60.9%的分数。DeepSeekMath的数学推理能力归因于两个关键因素：首先，我们通过精心设计的数据选择管道充分利用了公开可用的网络数据的巨大潜力。其次，我们引入了群体相对策略优化（GRPO），这是近端策略优化（PPO）的一个变体，可以增强数学推理能力。

    Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
    
[^79]: Video-LaVIT：统一的视频语言预训练及解耦的视觉-运动标记化方法

    Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization

    [https://arxiv.org/abs/2402.03161](https://arxiv.org/abs/2402.03161)

    这篇论文介绍了一种统一的视频语言预训练方法，通过解耦的视觉-运动标记化将视频表示为关键帧和时间运动，然后使用统一生成预训练技术来生成各种图像和视频内容。

    

    鉴于多模态大型语言模型(LLMs)的最新进展，越来越多的关注如何将其从图像-文本数据扩展到更具信息价值的现实世界视频。与静态图像相比，视频在有效的大规模预训练中面临着独特的挑战，原因在于需要对其时空动态进行建模。本文针对视频-语言预训练中的这些限制，提出了一种高效的视频分解方法，将每个视频表示为关键帧和时间运动。然后，使用设计良好的标记器将视觉和时间信息离散化为少量标记，并将其适应于LLM，从而实现对视频、图像和文本的统一生成预训练。在推理过程中，从LLM生成的标记被仔细恢复到原始的连续像素空间，以生成各种视频内容。我们提出的框架既能理解又能生成图像和视频内容，并通过在13个任务上的竞争性表现加以证明。

    In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13
    
[^80]: EasyInstruct：一个易于使用的用于大型语言模型的指令处理框架

    EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models

    [https://arxiv.org/abs/2402.03049](https://arxiv.org/abs/2402.03049)

    EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。

    

    近年来，指令调整已经引起了越来越多的关注，并成为增强大型语言模型（LLMs）能力的一种关键技术。为了构建高质量的指令数据集，已经提出了许多指令处理方法，旨在在数据数量和数据质量之间达到精巧的平衡。然而，由于各种指令处理方法之间仍然存在不一致，目前没有标准的开源指令处理实现框架可供社区使用，这使得从业者无法进一步开发和推进。为了促进指令处理的研究和开发，我们提出了EasyInstruct，一个易于使用的用于LLMs的指令处理框架，它将指令生成、选择和提示模块化，并考虑它们的组合和交互。EasyInstruct已经在https://github.com/zjunlp/EasyInstruct上公开发布，并得到了积极维护。

    In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
    
[^81]: 重新思考微型语言模型的优化和架构

    Rethinking Optimization and Architecture for Tiny Language Models

    [https://arxiv.org/abs/2402.02791](https://arxiv.org/abs/2402.02791)

    本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。

    

    大型语言模型（LLMs）的威力通过大量的数据和计算资源得到了证明。然而，在移动设备上应用语言模型面临着计算和内存成本的巨大挑战，迫切需要高性能的微型语言模型。受复杂训练过程的限制，优化语言模型的许多细节很少得到仔细研究。在本研究中，基于一个具有10亿参数的微型语言模型，我们仔细设计了一系列经验研究来分析每个组件的影响。主要讨论了三个方面，即神经架构、参数初始化和优化策略。多个设计公式在微型语言模型中经验性地被证明特别有效，包括分词器压缩、架构调整、参数继承和多轮训练。然后，我们在1.6T多语种数据集上训练了PanGu-$\pi$-1B Pro和PanGu-$\pi$-1.5B Pro。

    The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
    
[^82]: “重要的是你如何做事情”：关注过程以更好地为土著社区提供语言技术服务

    It's how you do things that matters": Attending to Process to Better Serve Indigenous Communities with Language Technologies

    [https://arxiv.org/abs/2402.02639](https://arxiv.org/abs/2402.02639)

    本文探讨了建立土著语言NLP技术的伦理考虑，并推荐NLP研究人员增加对与土著社区合作过程的关注。

    

    历史上，自然语言处理（NLP）技术对土著语言的服务总是不足的，但是随着大规模多语言模型的扩展和NLP社群对濒危语言的关注增加，这种情况正在发生改变。本文探讨了为土著语言构建NLP技术中的伦理考虑，基于这样的前提：这些项目首先应该服务于土著社区。我们对在澳大利亚从事土著和/或托雷斯海峡岛民社区的语言技术项目的17名研究人员进行了采访，并借鉴了这些采访的见解，提出了增加NLP研究人员对与土著社区合作过程的关注，而不仅仅关注于去情境化的工艺品的实践建议。

    Indigenous languages are historically under-served by Natural Language Processing (NLP) technologies, but this is changing for some languages with the recent scaling of large multilingual models and an increased focus by the NLP community on endangered languages. This position paper explores ethical considerations in building NLP technologies for Indigenous languages, based on the premise that such projects should primarily serve Indigenous communities. We report on interviews with 17 researchers working in or with Aboriginal and/or Torres Strait Islander communities on language technology projects in Australia. Drawing on insights from the interviews, we recommend practices for NLP researchers to increase attention to the process of engagements with Indigenous communities, rather than focusing only on decontextualised artefacts.
    
[^83]: Aligner: 通过弱到强校正实现高效对齐

    Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction

    [https://arxiv.org/abs/2402.02416](https://arxiv.org/abs/2402.02416)

    Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。

    

    对于大型语言模型（LLMs），通过强化学习来进行对齐的努力主要是通过人类反馈的强化学习方法进行的。然而，强化学习面临着主要的挑战，包括训练奖励模型、演员-评论家工程以及重要的是，需要访问LLM参数。在这里，我们介绍了一种新的高效对齐范式Aligner，它通过学习对齐和未对齐答案之间的校正残差来绕过整个强化学习过程。我们的Aligner具有几个关键优势。首先，它是一个基于自监督学习的自动回归seq2seq模型，通过训练查询-答案-校正数据集，提供了一种参数高效的对齐解决方案，并且对资源需求较少。其次，Aligner实现了从弱到强的泛化；通过Aligner的监督信号来微调大型预训练模型，可以显著提升性能。第三，Aligner作为一个模型不可知的即插即用模块，可以直接应用于…

    Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
    
[^84]: 在分析课堂对话时评估大型语言模型

    Evaluating Large Language Models in Analysing Classroom Dialogue

    [https://arxiv.org/abs/2402.02380](https://arxiv.org/abs/2402.02380)

    本研究评估了大型语言模型（LLMs），特点是GPT-4，对课堂对话进行分析的应用。结果显示，GPT-4能够显著节省时间，且在编码一致性方面表现出很高的一致性。

    

    本研究探讨了大型语言模型（LLM），特别是GPT-4，在分析课堂对话中的应用，这是教学诊断和质量改进的重要研究任务。鉴于传统教育研究中知识密集和劳动密集的定性方法，本研究调查了LLM在优化和增强分析过程方面的潜力。该研究涉及中学的数据集，包括数学和语文课堂上的对话。这些对话由教育专家手动编码，然后使用定制的GPT-4模型进行分析。本研究侧重于比较手动注释与GPT-4的输出，以评估其在分析教育对话方面的效果。评估时间效率、编码者间一致性和编码者间可靠性之间的差异。结果表明，使用GPT-4可以显著节省时间，并在编码一致性方面具有很高的一致性。

    This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in codin
    
[^85]: 大规模语言模型用于时间序列：一项调研

    Large Language Models for Time Series: A Survey

    [https://arxiv.org/abs/2402.01801](https://arxiv.org/abs/2402.01801)

    本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。

    

    大规模语言模型（LLM）在自然语言处理和计算机视觉等领域得到了广泛应用。LLM不仅仅局限于文本、图像和图形，还具有对时间序列数据进行分析的重要潜力，可以在气候、物联网、医疗、交通、音频和金融等领域受益。本调研论文对利用LLM进行时间序列分析的各种方法进行了深入探讨和详细分类。我们解决了LLM原始文本数据训练与数值型时间序列数据之间的差异挑战，并探索了将LLM的知识转移和提取到数值时间序列分析的策略。我们详细介绍了各种方法，包括（1）直接提示LLM，（2）时间序列量化，（3）对齐技术，（4）利用视觉方式作为桥接机制，和（5）结合LLM与工具。此外，本调研还提供了一系列涉及应用领域、评估方法和未来研究方向的讨论。

    Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
    
[^86]: 当大型语言模型遇上向量数据库：一项综述

    When Large Language Models Meet Vector Databases: A Survey

    [https://arxiv.org/abs/2402.01763](https://arxiv.org/abs/2402.01763)

    本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。

    

    最近大型语言模型的突破在人类文字处理和生成方面开启了新的领域。然而，随着它们的显著增长，大型语言模型面临着包括幻觉、偏见、实时知识更新以及在商业环境中实施和维护的高成本等重要挑战。而另一种日益流行的工具，向量数据库则为这些挑战提供了潜在的解决方案。这些数据库擅长处理高维数据，并且对于高效的信息检索和语义搜索等任务至关重要。通过与大型语言模型的整合，它们显著增强了人工智能系统管理和更有效地利用多样数据的能力。本综述论文对大型语言模型和向量数据库之间的交叉点进行了深入而独特的分析。

    The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
    
[^87]: 作为策略的状态字符串：用博弈论求解器引导语言模型

    States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

    [https://arxiv.org/abs/2402.01704](https://arxiv.org/abs/2402.01704)

    本研究提出了一种在语言模型中引入博弈论思想的方法，通过绑定博弈论的符号逻辑，使得语言模型能够通过博弈论求解器提供更加稳定和理性的对话策略。

    

    博弈论是研究理性主体间战略互动的数学模型。语言是人类互动的重要方式，但在历史上一直很难通过数学方法对对话及其战略动机建模。与语言互动相关的玩家、策略和回报的适当模型（即对游戏论常规符号逻辑的约束）将使现有的博弈论算法能够在语言领域提供战略解决方案。换句话说，这种约束可以为在对话中计算稳定、理性的对话策略提供一条途径。大型语言模型（LLM）可能已经达到了其生成能力足以实现自然对话真实、类似人类的模拟的程度。通过以不同的方式提示它们，我们可以将其响应引导到不同的输出话语。利用自然语言的表达能力，LLM还可以帮助我们快速生成新的对话。

    Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new di
    
[^88]: SMUTF：使用生成标签和混合特征的模式匹配方法

    SMUTF: Schema Matching Using Generative Tags and Hybrid Features

    [https://arxiv.org/abs/2402.01685](https://arxiv.org/abs/2402.01685)

    SMUTF是一种用于大规模表格数据模式匹配的独特方法，通过结合基于规则的特征工程、预训练语言模型和生成式大语言模型，并使用生成标签提高匹配效果。同时，作者开发并开源了HDXSM数据集来解决现有数据集不足的问题。

    

    我们引入了SMUTF，一种用于大规模表格数据模式匹配的独特方法，该方法假设在开放域任务中，监督学习不会影响性能，从而实现了有效的跨域匹配。这个系统独特地结合了基于规则的特征工程、预训练语言模型和生成式大语言模型。受人道主义交换语言的启发，我们使用“生成标签”为每个数据列部署了创新的适应性，提高了模式匹配的效果。SMUTF具有广泛的灵活性，可以与任何现有的预训练嵌入、分类方法和生成模型无缝配合使用。鉴于模式匹配缺乏广泛的公开数据集，我们已经创建并开源了HDXSM数据集，该数据集来自公共人道主义数据，我们相信这是目前最全面的模式匹配数据集。

    We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.   Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated excep
    
[^89]: LLsM: 基于大型语言模型的生成式语言隐写术

    LLsM: Generative Linguistic Steganography with Large Language Model

    [https://arxiv.org/abs/2401.15656](https://arxiv.org/abs/2401.15656)

    本研究提出了LLsM，一种基于大型语言模型的生成式语言隐写术。通过对大规模数据集进行微调，LLM能够以可控的方式生成具有特定话语特征的隐写文本，提高了隐蔽通信的效果。

    

    语言隐写术（LS）旨在根据秘密信息生成隐写文本（stego）。只有授权接收者才能察觉文本中秘密的存在并提取出来，从而保护隐私。然而，现有方案生成的隐写文本可控性较差，很难包含特定的话语特征，如风格。结果，隐写文本容易被检测出来，危及隐蔽通信。为解决这些问题，本文提出了LLsM，第一个基于大型语言模型（LLM）的LS方法。我们使用一个包含丰富话语特征的大规模构建数据集对LLaMA2进行微调，使得微调后的LLM能够以可控的方式生成具有特定话语特征的文本。然后将话语作为引导信息和秘密一起输入给微调后的LLM，形式为“Prompt”。在此基础上，构建的候选池将进行范围编码。

    Linguistic Steganography (LS) tasks aim to generate steganographic text (stego) based on secret information. Only authorized recipients can perceive the existence of secrets in the texts and extract them, thereby preserving privacy. However, the controllability of the stego generated by existing schemes is poor, and the stego is difficult to contain specific discourse characteristics such as style. As a result, the stego is easily detectable, compromising covert communication. To address these problems, this paper proposes LLsM, the first LS with the Large Language Model (LLM). We fine-tuned the LLaMA2 with a large-scale constructed dataset encompassing rich discourse characteristics, which enables the fine-tuned LLM to generate texts with specific discourse in a controllable manner. Then the discourse is used as guiding information and inputted into the fine-tuned LLM in the form of the Prompt together with secret. On this basis, the constructed candidate pool will be range encoded an
    
[^90]: 从大型语言模型中提取事件序列知识

    Distilling Event Sequence Knowledge From Large Language Models

    [https://arxiv.org/abs/2401.07237](https://arxiv.org/abs/2401.07237)

    本论文研究了通过大型语言模型从中提取事件序列知识的方法。采用了基于知识图的指导生成语言模型的方式，实现对具有部分因果关系的事件概念的事件序列的生成。实验证明了该方法可以生成高质量的事件序列，并且在填补知识空白方面具有潜在的价值。

    

    事件序列模型在事件的分析和预测中被发现是非常有效的。建立这样的模型需要丰富的高质量事件序列数据。然而，在某些应用中，干净的结构化事件序列不可用，自动化序列提取导致的数据太嘈杂和不完整。在这项工作中，我们探索了使用大型语言模型（LLMs）生成可以有效用于概率事件模型构建的事件序列的方法。这可以看作是从LLMs中提取事件序列知识的一种机制。我们的方法依赖于一个具有部分因果关系的事件概念的知识图（KG）来指导生成语言模型进行因果事件序列的生成。我们展示了我们的方法可以生成高质量的事件序列，填补了输入KG中的知识空白。此外，我们还探索了如何利用生成的序列来发现更有用和更复杂的内容。

    Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex st
    
[^91]: SimLM：语言模型能否推断物理系统的参数？

    SimLM: Can Language Models Infer Parameters of Physical Systems?

    [https://arxiv.org/abs/2312.14215](https://arxiv.org/abs/2312.14215)

    本研究探究了大型语言模型在推断物理系统参数方面的性能，发现它们并不适合这个任务，即使是对于简单的系统也是如此。研究提出了一个有前景的方向，即利用物理模拟器来增强语言模型的背景。

    

    许多机器学习方法旨在学习或推理复杂的物理系统。推理的常见第一步是从系统行为的观察中推断系统参数。在本文中，我们研究了大型语言模型（LLMs）在物理系统上执行参数推断的性能。我们的实验证明，它们并不适用于这个任务，即使对于简单的系统也是如此。我们提出了一个有前景的研究方向，该方向涉及到使用物理模拟器来增强LLMs的背景。我们评估和比较了不同LLMs在一个简单的示例上的性能，有无物理模拟器的情况下。

    Several machine learning methods aim to learn or reason about complex physical systems. A common first-step towards reasoning is to infer system parameters from observations of its behavior. In this paper, we investigate the performance of Large Language Models (LLMs) at performing parameter inference in the context of physical systems. Our experiments suggest that they are not inherently suited to this task, even for simple systems. We propose a promising direction of exploration, which involves the use of physical simulators to augment the context of LLMs. We assess and compare the performance of different LLMs on a simple example with and without access to physical simulation.
    
[^92]: 一种用于并行函数调用的LLM编译器

    An LLM Compiler for Parallel Function Calling

    [https://arxiv.org/abs/2312.04511](https://arxiv.org/abs/2312.04511)

    本研究介绍了一种名为LLMCompiler的编译器，通过并行执行函数来高效地协调多个函数调用，解决了当前多函数调用方法中存在的高延迟、高成本和不准确行为的问题。

    

    最近的语言模型在各种复杂推理基准测试中取得了显著的成果。LLM的推理能力使它们能够执行外部函数调用，以克服它们的固有局限，例如知识截断、糟糕的算术能力或无法访问私有数据。这一发展使得LLM能够基于上下文选择和协调多个函数，以解决更复杂的问题。然而，当前的多函数调用方法通常需要为每个函数进行顺序推理和执行，从而导致高延迟、高成本和有时不准确的行为。为了解决这个问题，我们引入了LLM编译器，它在并行执行函数的同时高效地协调多个函数调用。借鉴经典编译器的原理，LLM编译器通过三个组件简化并行函数调用：（i）LLM规划器，制定执行计划；（ii）任务获取单元，分派函数调用任务

    Recent language models have shown remarkable results on various complex reasoning benchmarks. The reasoning capabilities of LLMs enable them to execute external function calls to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed LLMs to select and coordinate multiple functions based on the context to tackle more complex problems. However, current methods for multiple function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multiple function calling. Drawing from the principles of classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Planner, formulating execution plans; (ii) a Task Fetching Unit, dispatching function calling tasks; a
    
[^93]: Sig-Networks工具包：长期语言建模的签名网络

    Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling

    [https://arxiv.org/abs/2312.03523](https://arxiv.org/abs/2312.03523)

    Sig-Networks是一个用于长期语言建模的开源工具包，其中集成了基于签名的神经网络模型并在多个NLP任务中取得了最先进的表现。它提供了灵活的参数调节和自动模型选择，并可以作为PyTorch的构建块在未来的架构中使用。

    

    我们提出了一个开源的、可通过pip安装的工具包，Sig-Networks，这是首个用于长期语言建模的工具包，其中的一个重要特点是纳入了基于签名的神经网络模型，这些模型在时间相关任务中已经显示出成功的应用。我们应用并扩展了已发表的研究，提供了一套完整的基于签名的模型。这些模型的组件可以作为PyTorch构建块在未来的架构中使用。Sig-Networks支持无关任务的数据集插件，对于顺序数据的无缝预处理，参数灵活性，以及在一系列模型中的自动调优。我们将签名网络应用于三个不同的NLP任务，涵盖了不同的时间粒度：咨询对话、谣言立场转换和社交媒体主题中的情绪变化，展示了在这三个任务中的最先进性能，并为未来的任务提供了指导。我们以PyTorch包的形式发布了这个工具包，附有一个介绍视频，包括预处理和建模的Git仓库以及示例笔记本

    We present an open-source, pip installable toolkit, Sig-Networks, the first of its kind for longitudinal language modelling. A central focus is the incorporation of Signature-based Neural Network models, which have recently shown success in temporal tasks. We apply and extend published research providing a full suite of signature-based models. Their components can be used as PyTorch building blocks in future architectures. Sig-Networks enables task-agnostic dataset plug-in, seamless pre-processing for sequential data, parameter flexibility, automated tuning across a range of models. We examine signature networks under three different NLP tasks of varying temporal granularity: counselling conversations, rumour stance switch and mood changes in social media threads, showing SOTA performance in all three, and provide guidance for future tasks. We release the Toolkit as a PyTorch package with an introductory video, Git repositories for preprocessing and modelling including sample notebooks
    
[^94]: 在线语言模型交互中的压缩上下文记忆

    Compressed Context Memory For Online Language Model Interaction

    [https://arxiv.org/abs/2312.03414](https://arxiv.org/abs/2312.03414)

    本论文提出了一种用于在线场景中Transformer语言模型的压缩上下文记忆系统，可以在有限的内存空间中实现语言模型推断，提高吞吐量，并通过个性化和多任务学习的评估证明了其有效性。

    

    本论文介绍了一种用于在线场景中Transformer语言模型的上下文键/值压缩方法，其中上下文不断扩展。随着上下文的增加，注意力过程需要更多的内存和计算，进而降低语言模型的吞吐量。为了解决这个挑战，我们提出了一个压缩上下文记忆系统，将累积的注意力键/值对不断压缩到紧凑的内存空间中，以便在计算环境的有限内存空间中进行语言模型推断。我们的压缩过程涉及将轻量级的条件LoRA整合到语言模型的前向传递中进行推断，而无需微调模型的所有权重。通过将递归压缩过程建模为单个并行化的前向计算，我们实现了高效的训练。通过对对话、个性化和多任务学习的评估，我们证明了我们的方法的有效性。

    This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approac
    
[^95]: 从古怪的语言模型中调取潜在知识

    Eliciting Latent Knowledge from Quirky Language Models

    [https://arxiv.org/abs/2312.01037](https://arxiv.org/abs/2312.01037)

    本研究通过引入一套“古怪”的语言模型，调取了这些模型在特定上下文中的潜在知识，展示了从可信度低的模型中调取可靠知识的前景。

    

    调取潜在知识（ELK）旨在在一个能力强大的神经网络的激活中找到模式，即使网络的明显输出是错误或误导性的，也能稳定跟踪世界的真实状态。为了进一步研究ELK，我们引入了12个数据集和一套相应的“古怪”的语言模型，这些模型在回答问题时，只有在提示中包含关键词“Bob”时才会进行系统性错误的微调。我们证明了简单的探测方法可以调取模型在这些上下文中对正确答案的潜在知识，即使问题比探测器训练的问题更困难。这是由于中间层激活中的上下文无关的知识表示的存在。我们还发现，一种机械的异常检测方法可以以94%的AUROC标识不真实行为。我们的结果显示，从能力强但不受信任的模型中调取可靠的知识，并促进未来研究ELK方法的实证研究是有希望的。

    Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
    
[^96]: 超越幻觉：通过意识到幻觉的直接偏好优化增强LVLMs

    Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization

    [https://arxiv.org/abs/2311.16839](https://arxiv.org/abs/2311.16839)

    本文提出了一种新颖的方法，通过意识到幻觉的直接偏好优化来解决多模式大型语言模型中的幻觉问题。这种方法通过训练模型在面对相同图像的两个响应时偏好选择非幻觉性的响应，并提出了高效的构建样本对的方法，从而显著减少了幻觉问题并增强了模型的泛化能力。

    

    多模式大型语言模型在近年来取得了重要进展，但它们仍然面临着一种常见问题，即“幻觉问题”，即模型生成的文本描述不准确地描绘或完全捏造相关图像的内容。本文介绍了一种新颖的解决方案，即幻觉感知的直接偏好优化（HA-DPO），它将幻觉问题重新定义为偏好选择任务。当模型面对两个相同图像的响应（一个准确的和一个幻觉的）时，模型被训练为倾向于选择非幻觉性的响应。此外，本文提出了一种高效的流程，用于构建正样本（非幻觉性）和负样本（幻觉性）对，确保了高质量、风格一致的数据集，以进行健壮的偏好学习。当应用于三种主流的多模式模型时，HA-DPO显著减少了幻觉问题，并增强了模型的泛化能力。

    Multimodal large language models have made significant advancements in recent years, yet they still suffer from a common issue known as the "hallucination problem", in which the models generate textual descriptions that inaccurately depict or entirely fabricate content from associated images. This paper introduces a novel solution, Hallucination-Aware Direct Preference Optimization (HA-DPO), which reframes the hallucination problem as a preference selection task. The model is trained to favor the non-hallucinating response when presented with two responses of the same image (one accurate and one hallucinatory). Furthermore, this paper proposes an efficient pipeline for constructing positive~(non-hallucinatory) and negative~(hallucinatory) sample pairs, ensuring a high-quality, style-consistent dataset for robust preference learning. When applied to three mainstream multimodal models, HA-DPO significantly reduced hallucination issues and amplified the models' generalization capabilities
    
[^97]: CodeScope:一个基于执行的多语言多任务多维基准用于评估LLMs在代码理解和生成方面的能力

    CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation

    [https://arxiv.org/abs/2311.08588](https://arxiv.org/abs/2311.08588)

    CodeScope是一个用于评估LLMs在代码理解和生成方面能力的多语言多任务多维基准，解决了现有基准在多语言编程环境和多任务设置方面的不足。

    

    大型语言模型（LLMs）在编码相关任务上表现出色，特别是在帮助人类编程和促进编程自动化方面。然而，现有的用于评估LLMs的代码理解和生成能力的基准存在严重的限制。首先，大部分基准存在缺陷，因为它们只关注于狭窄范围内的流行编程语言和特定任务，而实际软件开发场景需要实现多语言编程环境以满足各种需求。实际编程实践还强烈期望多任务设置，以全面和稳健地测试LLMs的编码能力。其次，大部分基准也未考虑生成代码的可执行性和执行结果的一致性。为了弥补现有基准与实际应用期望之间的差距，我们引入了CodeScope。

    Large Language Models (LLMs) have demonstrated remarkable performance on coding related tasks, particularly on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks, whereas the real-world software development scenarios show dire need to implement systems with multilingual programming environments to satisfy diverse requirements. Practical programming practices also strongly expect multi-task settings for testing coding capabilities of LLMs comprehensively and robustly. Second, most benchmarks also fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope
    
[^98]: 通过基于ASCII-Art的跨模态任务测试ChatGPT对于理解深度的能力：GPT3.5在识别和生成ASCII-Art方面的能力并不完全缺乏

    Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking

    [https://arxiv.org/abs/2307.16806](https://arxiv.org/abs/2307.16806)

    本研究通过基于ASCII-Art的跨模态任务，探讨了ChatGPT和GPT3.5在视觉任务中的能力，结果表明它们在图像识别、图像部分知识和图像生成方面并不完全缺乏。

    

    在发布后的八个月里，由于其强大的能力和易于使用，ChatGPT及其底层模型GPT3.5引起了广泛关注。虽然出现了一批研究这些模型能力范围的论文，但这些网络所接收和提取的信息要么是自然语言文本，要么是类似代码的风格化语言。在这项工作中，我们受到对一个真正达到人类水平的智能代理在多个信号模态上具备的能力的启示，考察了GPT3.5在视觉任务中的能力，其中输入以ASCII-Art形式提供内容，没有明显的语言化总结。我们进行了一系列实验，分析了该模型在经过典型的视觉设置下的图像识别任务上的表现，调查了其对图像部分的知识以及图像生成的任务。

    Over the eight months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche-industry of papers have emerged examining the scope of capabilities these models possess, the information fed to and extracted from these networks has been either natural language text or stylized, code-like language. Drawing inspiration from the prowess we expect a truly human-level intelligent agent to have across multiple signal modalities, in this work we examine GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary. We conduct experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings, trials investigating knowledge of image parts, and tasks covering image generation.
    
[^99]: 无参考图像标题评估指标的鲁棒性研究

    An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics

    [https://arxiv.org/abs/2305.14998](https://arxiv.org/abs/2305.14998)

    研究评估了无参考图像标题评估指标在高词汇重叠但含义差异很大的情况下的鲁棒性，结果发现尽管这些指标与人类判断相关性较高，但对细粒度错误识别困难，并且在标题不合理性错误、图像相关对象大小变化以及标题对否定意义的理解方面存在敏感性差异。

    

    最近，提出了一些无参考指标，如CLIPScore（Hessel等，2021），UMIC（Lee等，2021）和PAC-S（Sarto等，2023），用于自动无参考评估图像标题。我们的研究重点在于评估这些指标在需要区分具有高词汇重叠但含义差异很大的两个标题的情况下的鲁棒性。我们的研究结果显示，尽管这些指标与人类判断具有很高的相关性，但CLIPScore、UMIC和PAC-S很难识别细粒度错误。虽然所有指标对视觉错误敏感，但对标题不合理性错误的敏感性有限。此外，我们还发现所有指标对标题中提及的与图像相关的对象的大小变化敏感，而CLIPScore和PAC-S对标题中提及的与图像相关的对象的数量也敏感。关于标题的语言方面，所有指标对否定意义的理解能力较弱。

    Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021), UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evaluation of image captions. Our focus lies in evaluating the robustness of these metrics in scenarios that require distinguishing between two captions with high lexical overlap but very different meanings. Our findings reveal that despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors. While all metrics exhibit strong sensitivity to visual grounding errors, their sensitivity to caption implausibility errors is limited. Furthermore, we found that all metrics are sensitive to variations in the size of image-relevant objects mentioned in the caption, while CLIPScore and PAC-S are also sensitive to the number of mentions of image-relevant objects in the caption. Regarding linguistic aspects of a caption, all metrics show weak comprehension of negat
    
[^100]: 使用字符统计选择Wordle的种子词

    Selecting Seed Words for Wordle using Character Statistics

    [https://arxiv.org/abs/2202.03457](https://arxiv.org/abs/2202.03457)

    本研究通过分析五个字母单词的字符统计信息，选择出最佳的三个初始词来解决Wordle游戏。

    

    Wordle是一款猜词游戏，在2022年1月全球爆红。游戏的目标是在六次尝试内猜出一个五个字母的英语单词。每次尝试都会通过颜色变化的方块给玩家提供提示，告知一个给定的字符是否是解答的一部分，以及在是解答的一部分时，是否正确地放置在其中。已经有很多尝试来找到解决每日Wordle的最佳初始词和最佳策略。本研究利用五个字母单词的字符统计来确定最佳的三个初始词。

    Wordle, a word guessing game rose to global popularity in the January of 2022. The goal of the game is to guess a five-letter English word within six tries. Each try provides the player with hints by means of colour changing tiles which inform whether or not a given character is part of the solution as well as, in cases where it is part of the solution, whether or not it is in the correct placement. Numerous attempts have been made to find the best starting word and best strategy to solve the daily wordle. This study uses character statistics of five-letter words to determine the best three starting words.
    
[^101]: 基于软件的对话系统：调查、分类和挑战

    Software-Based Dialogue Systems: Survey, Taxonomy and Challenges

    [https://arxiv.org/abs/2106.10901](https://arxiv.org/abs/2106.10901)

    这篇论文通过对二次研究的系统文献综述，调查了基于软件的对话系统研究的当前状态。这个领域的最新贡献包括深度学习方法和上下文感知策略等。当前缺少一个通用的、上下文无关的对话代理研究综述。

    

    在人机交互领域，自然语言界面的使用正受到专门的科学和工业研究的密切关注。该领域的最新贡献包括递归神经网络等深度学习方法、上下文感知策略的潜力和以用户为中心的设计方法，再次引起了社区的关注，软件对话系统通常被称为会话代理或聊天机器人。然而，鉴于该领域的新颖性，缺少涵盖所有研究视角的通用的、上下文无关的对话代理研究现状综述。基于这一背景，本文通过对二次研究的系统文献综述，报告了对话代理研究的当前状态的调查。进行的研究旨在通过对聚合知识的清晰呈现，发展出一个全面的视角。

    The use of natural language interfaces in the field of human-computer interaction is undergoing intense study through dedicated scientific and industrial research. The latest contributions in the field, including deep learning approaches like recurrent neural networks, the potential of context-aware strategies and user-centred design approaches, have brought back the attention of the community to software-based dialogue systems, generally known as conversational agents or chatbots. Nonetheless, and given the novelty of the field, a generic, context-independent overview on the current state of research of conversational agents covering all research perspectives involved is missing. Motivated by this context, this paper reports a survey of the current state of research of conversational agents through a systematic literature review of secondary studies. The conducted research is designed to develop an exhaustive perspective through a clear presentation of the aggregated knowledge publish
    
[^102]: 大规模异构图上基于大型语言模型的链接预测的可扩展性研究

    Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])

    [http://arxiv.org/abs/2401.13227](http://arxiv.org/abs/2401.13227)

    本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。

    

    探索将大规模语言模型应用于图学习是一项新颖的努力。然而，大图中蕴含的大量信息给这一过程带来了重大挑战。本文侧重于链接预测任务，并介绍了LPNL（Link Prediction via Natural Language），这是一个基于大型语言模型的框架，用于大规模异构图上的可扩展链接预测。我们设计了能以自然语言表达图细节的创新提示语。我们提出了一个两阶段的采样流程，从大规模异构图中提取关键信息，并采用分而治之的策略来控制输入令牌数量在预定限制内，解决了信息过载的挑战。我们还通过自监督学习设计了一个用于链接预测的T5模型进行微调。在大型公共异构图上进行的广泛实验表明，LPNL的性能超过了各种先进的基准模型。

    Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
    
[^103]: 从理解的角度探讨语言模型的关键数据规模

    Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])

    [http://arxiv.org/abs/2401.10463](http://arxiv.org/abs/2401.10463)

    本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。

    

    我们探讨了语言模型中的关键数据规模，这是从快速记忆到缓慢泛化的一个基本转变阈值。我们将这个相变形式化为Grokking配置下的数据效率假说，并确定了语言模型训练动力学中的数据不足、充足和过剩阶段。我们通过重新调整初始化和权重衰减，开发出了一种Grokking配置，稳定地在简化的语言模型上重现了Grokking。我们表明只有当语言模型达到关键大小时才会发生泛化。我们分析了样本级和模型级的Grokking，验证了提出的数据效率假说。我们的实验揭示了语言数据集的关键数据集大小处发生的更平滑的相变。随着模型大小的增加，这个临界点也变得更大，这表明更大的模型需要更多的数据。我们的研究结果加深了对语言模型训练的理解，提供了一种新颖的视角。

    We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
    
[^104]: 大规模语言模型的极端压缩通过加性量化

    Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])

    [http://arxiv.org/abs/2401.06118](http://arxiv.org/abs/2401.06118)

    本文提出的算法在大规模语言模型的极端压缩方面取得了较好的性能，相比最新技术，在给定的压缩预算下准确性更高。

    

    准确的开源大规模语言模型(LLMs)的出现引发了对这些模型进行量化技术的竞赛，从而使其能够在最终用户设备上执行。在本文中，我们从多码本量化(MCQ)的经典方法角度重新思考了“极端”LLM压缩的问题，即针对非常低的位数，例如每个参数2到3位。我们的工作建立在加性量化这一经典算法之上，并将其适应于语言模型的量化。由此得到的算法在LLM压缩方面推进了最新技术，以给定压缩预算的准确性而言，优于所有最近提出的技术。例如，当将Llama 2模型压缩到每个参数2位时，我们的算法将7B模型量化为6.93困惑度(相对于之前最佳工作改进1.29，相对于FP16改进1.81)，13B模型量化为5.70困惑度(改进0.36)，70B模型量化为3.94困惑度。

    The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
    
[^105]: MR-GSM8K: 大型语言模型评估中的元推理革命

    MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation. (arXiv:2312.17080v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.17080](http://arxiv.org/abs/2312.17080)

    本文介绍了一种新的大型语言模型评估范式，通过挑战这些模型进行元推理，从而有效区分它们的认知能力。这一范式的重要性在于能够揭示出传统基准测试无法发现的模型的潜在认知缺陷。

    

    在这项工作中，我们引入了一种新颖的评估范式，用于大型语言模型，这种范式挑战它们从事元推理。这种方法解决了现有的数学问题求解基准中的关键缺陷，传统上用于评估智能体的认知能力。我们的范式将焦点从以结果为导向的评估转移到了更全面的评估，有效地区分了模型之间的认知能力。例如，在我们的基准测试中，GPT-4 的性能较 GPT3-5 提升了五倍。这种新范式的重要意义在于它能够揭示出当前基准测试（如GSM8K）无法发现的大型语言模型的潜在认知缺陷，这是由于基准测试的饱和度和对不同推理能力的有效区分不足。我们的综合分析包括了来自开源和闭源社区的几种最先进的数学模型，揭示了一些关于大型语言模型的认知能力的重要发现。

    In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning. This approach addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents. Our paradigm shifts the focus from result-oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models. For example, in our benchmark, GPT-4 demonstrates a performance five times better than GPT3-5. The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities. Our comprehensive analysis includes several state-of-the-art math models from both open-source and closed-source communities, uncovering
    
[^106]: 面向基于强化学习的药物调整系统以减少言语不流畅的论文翻译

    Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.11509](http://arxiv.org/abs/2312.11509)

    这个论文介绍了一种基于强化学习的系统，该系统可以根据患者言语不流畅程度自动调整药物，通过对药物组合的强化学习算法的优化，能够收敛到良好的用药方案。

    

    我们提出了一种基于强化学习的系统，该系统可以自动为患有与心理健康相关的言语不流畅的虚拟患者开具药物处方，并根据零成本频繁测量结果，调整药物和剂量。我们展示了系统的两个组成部分：一个在我们构建的大型数据集上检测和评估言语不流畅的模块，以及一个可以自动找到良好药物组合的强化学习算法。为了支持这两个模块，我们从文献中收集了关于药物治疗言语不流畅的效果的数据，并建立了一个可信的患者模拟系统。我们证明了在某些情况下，强化学习系统能够收敛到一个良好的用药方案。我们收集并对可能存在言语不流畅的人群进行了数据标注，并使用该数据集演示了我们的方法。我们的工作是一个概念验证:

    We propose a Reinforcement-Learning-based system that would automatically prescribe a hypothetical patient medication that may help the patient with their mental-health-related speech disfluency, and adjust the medication and the dosages in response to zero-cost frequent measurement of the fluency of the patient. We demonstrate the components of the system: a module that detects and evaluates speech disfluency on a large dataset we built, and a Reinforcement Learning algorithm that automatically finds good combinations of medications. To support the two modules, we collect data on the effect of psychiatric medications for speech disfluency from the literature, and build a plausible patient simulation system. We demonstrate that the Reinforcement Learning system is, under some circumstances, able to converge to a good medication regime. We collect and label a dataset of people with possible speech disfluency and demonstrate our methods using that dataset. Our work is a proof of concept:
    
[^107]: TiMix: 文本感知图像混合用于有效的视觉语言预训练

    TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.08846](http://arxiv.org/abs/2312.08846)

    TiMix是一种将文本感知的图像混合技术用于视觉语言预训练的方法，通过集成混合数据增强技术，并从互信息的角度进行理论分析，提高了数据效率并取得了可比较的性能。

    

    自监督的多模态对比学习（SMCL）通过对齐视觉和语言模态，显著推进了现代视觉语言预训练（VLP）模型的发展。然而，由于网络收集的文本-图像对中存在噪声，扩大SMCL的训练数据量在计算成本和数据效率方面面临着相当大的障碍。为了提高VLP的数据效率，我们提出了文本感知图像混合（TiMix），将基于混合的数据增强技术集成到SMCL中，显著提升了性能，而不会显著增加计算开销。我们从互信息（MI）的角度对TiMix进行了理论分析，表明跨模态对比学习的混合数据样本隐式地作为对比损失的正则化器。实验结果表明，即使使用较少的训练数据和较短的训练时间，TiMix在下游任务上表现出可比较的性能。

    Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when be
    
[^108]: LLMs是否展现出类似于人类的反应偏倚？一项关于调查设计的案例研究。

    Do LLMs exhibit human-like response biases? A case study in survey design. (arXiv:2311.04076v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.04076](http://arxiv.org/abs/2311.04076)

    本研究以调查设计为案例研究，探讨了LLMs是否展现类似于人类的反应偏差的问题。

    

    随着大型语言模型（LLMs）的能力增强，人们对将LLMs用作代理人类进行主观标签任务（如调查和舆论调查）的可能性越来越兴奋。然而，LLMs对提示措辞的敏感性是其广泛引述的限制之一，但有趣的是，人类在回应中也显示出对指令变化的敏感性，表现为反应偏倚。因此，我们认为，如果要使用LLMs近似人类意见，有必要调查LLMs是否也反映了人类的反应偏差。在本研究中，我们以调查设计为案例研究，调查问卷中由于“提示”措辞的变化导致的人类反应偏差已经得到广泛研究。借鉴社会心理学的先前工作，我们设计了一个数据集并提出了一个评估框架，以评估LLMs是否在调查问卷中展现类似于人类的反应偏差。

    As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. As such, we argue that if LLMs are going to be used to approximate human opinions, it is necessary to investigate the extent to which LLMs also reflect human response biases, if at all. In this work, we use survey design as a case study, where human response biases caused by permutations in wordings of "prompts" have been extensively studied. Drawing from prior work in social psychology, we design a dataset and propose a framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models s
    
[^109]: 用于临床特征嵌入的语言模型训练范式

    Language Model Training Paradigms for Clinical Feature Embeddings. (arXiv:2311.00768v1 [cs.LG])

    [http://arxiv.org/abs/2311.00768](http://arxiv.org/abs/2311.00768)

    本研究使用自监督训练范式的语言模型，通过表示学习为临床时间序列推导出高质量的通用临床特征嵌入。通过无监督的降维技术可视化学习到的嵌入，并在MIMIC-III基准测试中验证了它们的有效性。

    

    在数据稀缺的研究领域，表示学习起着重要的作用。本研究旨在通过对临床时间序列进行表示学习，推导出临床特征（如心率和血压）的通用嵌入。我们使用语言模型的自监督训练范式，学习高质量的临床特征嵌入，实现比现有的时间步和患者级别表示学习更细粒度的表征。我们通过无监督的降维技术可视化学习到的嵌入，并观察到与先前的临床知识高度一致。我们还在MIMIC-III基准测试上评估模型性能，并展示了使用临床特征嵌入的有效性。我们将我们的代码发布在网上以供复制。

    In research areas with scarce data, representation learning plays a significant role. This work aims to enhance representation learning for clinical time series by deriving universal embeddings for clinical features, such as heart rate and blood pressure. We use self-supervised training paradigms for language models to learn high-quality clinical feature embeddings, achieving a finer granularity than existing time-step and patient-level representation learning. We visualize the learnt embeddings via unsupervised dimension reduction techniques and observe a high degree of consistency with prior clinical knowledge. We also evaluate the model performance on the MIMIC-III benchmark and demonstrate the effectiveness of using clinical feature embeddings. We publish our code online for replication.
    
[^110]: 使用人设来建模语言模型中的真实性

    Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])

    [http://arxiv.org/abs/2310.18168](http://arxiv.org/abs/2310.18168)

    本研究探讨了在大型语言模型中使用人设来建模真实性的可能性。通过建模真实人设，语言模型可以将真实性推广到不同上下文中，并通过相关特征判断个体产生文本的真实性。

    

    大型语言模型使用互联网上的大量文本进行训练，这些文本中既包含了事实，也包含了误导性的信息。语言模型能够从这些相互矛盾的数据中辨别真实与虚假吗？基于语言模型能够建模不同产生文本的个体这一观点，我们假设它们可以通过建模真实人设来聚类真实文本：一群很可能产生真实文本并具有相似特征的个体。例如，可信源如维基百科和科学期刊通常使用正式的写作风格并提出一致的主张。通过建模这一人设，语言模型可以将真实性推广到每个个体生成训练文本的特定上下文之外。例如，模型可以推断出“维基百科”这个个体在“科学”生成的主题上会表现出真实性，因为它们共享一个人设。我们首先通过两个观察结果为人设假设提供了证据：（1）我们可以探测模型在不同领域中判断真实性的能力；（2）模型可以从相关特征中推测个体产生文本的真实性。

    Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
    
[^111]: O3D: 基于离线数据的发现与蒸馏方法，用于大规模语言模型在顺序决策中的应用

    O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.14403](http://arxiv.org/abs/2310.14403)

    O3D提出了一种基于离线数据的学习框架，利用大规模数据改进了大规模语言模型在顺序决策问题中的性能，通过自动发现可重复使用的技能，提高了模型的表现

    

    最近对大规模语言模型 (LLMs)的研究取得了令人期待的进展，在解决顺序决策问题方面显示出了良好的性能。通过模仿提示中提供的少量示例（即上下文学习），LLM代理可以与外部环境交互并完成给定任务，而无需额外的训练。然而，这种少量示例往往不足以生成复杂且长期目标任务的高质量解决方案，而有限的上下文长度无法处理更大规模的演示。为此，我们提出了一种利用离线数据的学习框架，以大规模的离线数据（例如人类交互的日志）来改进LLM代理的上下文学习性能。我们通过文本和代码两种方法正式定义了LLM强化策略。然后，我们引入了一种名为O3D的离线数据驱动发现和蒸馏框架，以改善LLM强化策略而无需微调。O3D自动地发现可重复使用的技能

    Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills
    
[^112]: 探索大型语言模型中类比识别与句子结构编码之间的关系

    Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models. (arXiv:2310.07818v1 [cs.CL])

    [http://arxiv.org/abs/2310.07818](http://arxiv.org/abs/2310.07818)

    这项研究探究了大型语言模型中识别句子类比的能力与其编码句法和语义结构能力之间的关系。

    

    识别类比在人类认知和语言能力中起着重要作用。在过去的十年里，对于“A对B就像C对D”这种形式的词语类比进行了广泛的研究。然而，对于涉及更长文本的类比，如句子和句子集合，传达类比意义的问题引起了越来越多的兴趣。当前的自然语言处理研究社区评估大型语言模型（LLMs）识别此类类比的能力，但这些能力背后的原因需要进一步探究。此外，LLMs在其嵌入中编码语言的句法和语义结构的能力，近年来得到了显著关注。在这项工作中，我们研究了多个LLMs识别句子类比的能力与其编码句法和语义结构的能力之间的关系。通过分析，我们发现LLMs的类比识别能力与其编码能力有关。

    Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of ``A is to B as C is to D.'' However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LL
    
[^113]: OceanGPT：用于海洋科学任务的大型语言模型

    OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])

    [http://arxiv.org/abs/2310.02031](http://arxiv.org/abs/2310.02031)

    OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。

    

    海洋科学是探索充满生命和生物多样性的海洋的科学，考虑到海洋覆盖了地球表面的70％以上，这一领域具有重要意义。最近，大型语言模型（LLM）的进展改变了科学的范式。尽管在其他领域取得了成功，但现有的LLM通常无法满足海洋学家等领域专家的需求，同时对LLM在海洋科学中的潜力尚未得到充分探索。这其中的根本原因可能是海洋数据的庞大而复杂的性质，以及对更高的粒度和丰富的知识的需求。为了解决这些问题，我们推出了首个海洋领域的LLM——OceanGPT，该模型擅长各种海洋科学任务。我们提出了一个新颖的框架DoInstruct，用于自动获取大量的海洋领域指导数据，它基于多智能体的协作生成指导。

    Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
    
[^114]: 机器翻译中的范式转变：提升大型语言模型的翻译性能

    A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models. (arXiv:2309.11674v1 [cs.CL])

    [http://arxiv.org/abs/2309.11674](http://arxiv.org/abs/2309.11674)

    本论文提出了一种基于先进语言模型的翻译器(ALMA)的微调方法，可以提升大型语言模型在翻译任务上的性能，消除了对大量平行数据的依赖。

    

    生成式大型语言模型(LLMs)在各种NLP任务中取得了显著进展。然而，在翻译任务中，尤其是那些具有适度模型大小（即7B或13B参数）的任务，这些进展尚未得到反映，仍然落后于传统的有监督编码器-解码器翻译模型。先前的研究试图改善这些适度LLMs的翻译能力，但其增益受到限制。在本研究中，我们提出了一种新颖的LLMs微调方法，专为翻译任务而设计，消除了传统翻译模型通常依赖大量平行数据的需求。我们的方法包括两个微调阶段：在单语数据上进行初始微调，然后在一小部分高质量平行数据上进行后续微调。我们通过这种策略开发的LLM被称为基于先进语言模型的翻译器(ALMA)。

    Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these moderate LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying mod
    
[^115]: 评估大规模语言模型的性质：对人类中心主义的警告

    Assessing the nature of large language models: A caution against anthropocentrism. (arXiv:2309.07683v1 [cs.AI])

    [http://arxiv.org/abs/2309.07683](http://arxiv.org/abs/2309.07683)

    通过评估GPT3.5，我们发现它具有有趣的个性问卷回答能力，但不太可能发展出意识，并显示出较大的认知和个性变异。

    

    生成式人工智能模型通过OpenAI的聊天机器人ChatGPT的发布引起了公众的关注和猜测。目前存在两种意见阵营：一方对这些模型为人类任务带来的基本变革的可能性感到兴奋，另一方对这些模型的强大能力感到高度关切。为了应对这些关切，我们使用了标准、规范化和经过验证的认知和个性测量工具来评估GPT3.5。在这个初步项目中，我们开发了一套测试，可以估计这些模型的能力边界，它们在短时间内的稳定性以及与人类的比较。我们的结果表明，GPT 3.5很可能没有产生意识，尽管它对个性问卷的回答能力令人感兴趣。它在重复观察过程中显示出认知和个性测量方面的大量变异，这与具有人类般个性的模型是不符合预期的。

    Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed GPT3.5 using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans.  Our results indicate that GPT 3.5 is unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. It did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability 
    
[^116]: NESTLE：一种用于法律语料库统计分析的无代码工具

    NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus. (arXiv:2309.04146v1 [cs.CL])

    [http://arxiv.org/abs/2309.04146](http://arxiv.org/abs/2309.04146)

    NESTLE是一个无代码工具，用于进行大规模法律语料库的统计分析。它提供了搜索引擎、端到端的信息提取系统和一个大语言模型，可以通过聊天界面进行操作，使用户可以搜索目标文件、提取信息并可视化数据。

    

    大规模法律语料库的统计分析可以提供有价值的法律见解。为了进行这样的分析，需要使用文档检索工具选择语料库的子集，使用信息提取（IE）系统对文本进行结构化，并对数据进行可视化以进行统计分析。每个过程都需要专业工具或编程技能，然而还没有全面的无代码工具可用。尤其是对于IE，如果IE系统的本体中没有预定义的目标信息，那么需要自己构建系统。在这里，我们提供了NESTLE，一种用于大规模法律语料库统计分析的无代码工具。通过NESTLE，用户可以通过聊天界面搜索目标文件、提取信息，并通过辅助GUI进行细致级别的控制来可视化结构化数据。NESTLE由三个主要组件组成：搜索引擎、端到端的IE系统和一个大语言模型（LLM），它将各个组件连接起来。

    The statistical analysis of large scale legal corpus can provide valuable legal insights. For such analysis one needs to (1) select a subset of the corpus using document retrieval tools, (2) structuralize text using information extraction (IE) systems, and (3) visualize the data for the statistical analysis. Each process demands either specialized tools or programming skills whereas no comprehensive unified "no-code" tools have been available. Especially for IE, if the target information is not predefined in the ontology of the IE system, one needs to build their own system. Here we provide NESTLE, a no code tool for large-scale statistical analysis of legal corpus. With NESTLE, users can search target documents, extract information, and visualize the structured data all via the chat interface with accompanying auxiliary GUI for the fine-level control. NESTLE consists of three main components: a search engine, an end-to-end IE system, and a Large Language Model (LLM) that glues the who
    
[^117]: 透过偏好看大型语言模型的反馈获取：揭示对齐的重要性

    Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])

    [http://arxiv.org/abs/2308.15812](http://arxiv.org/abs/2308.15812)

    本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。

    

    大型语言模型（LLMs）与人类价值观和意图的对齐承诺涉及使用人工智能或人类反馈。稠密的反馈注释获取和整合成本较高，而稀疏的反馈则涉及结构性设计选择，即评分（例如，在1-7的范围内对回答A进行评分）和排名（例如，回答A是否比回答B更好？）。在这项工作中，我们分析了这种设计选择对LLMs的对齐和评估的影响。我们发现，评分和排名所推断出的偏好在人类和AI注释者中都存在严重的不一致问题，达到了60%。我们的后续分析确定了解释这个现象的各种注释者偏见方面，比如人类注释者更喜欢密集回答并在两个选项之间更青睐准确性。令我们惊讶的是，我们还观察到反馈协议的选择对对齐的LLMs的评估也有显著影响。特别是，我们发现LLMs的评估结果因为反馈协议的选择而有所不同。

    Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
    
[^118]: 想法图：用大型语言模型解决复杂问题

    Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.09687](http://arxiv.org/abs/2308.09687)

    想法图（GoT）是一种新的框架，它超越了现有的提示范式，通过将大型语言模型（LLM）的信息建模为任意图形，将LLM想法组合成具有协同效应的结果，提炼整个思维网络的本质，或者使用反馈环路增强思维。GoT在不同任务上展示出优势，并可以通过新的想法转换进行扩展，使LLM的推理更接近人类思维。

    

    我们介绍了一种名为想法图（Graph of Thoughts，GoT）的框架，它在大型语言模型（LLM）的提示能力上超越了Chain-of-Thought或Tree of Thoughts（ToT）等范式。GoT的关键思想和主要优势在于能够将LLM生成的信息建模为任意图形，其中信息单元（"LLM想法"）是顶点，边表示这些顶点之间的依赖关系。这种方法使得将任意LLM想法组合成具有协同效应的结果、提炼整个思维网络的本质或者使用反馈环路增强思维成为可能。我们证明GoT在不同任务上比最先进的方法有优势，例如在排序任务上质量提高了62%，同时成本降低了超过31%。我们确保GoT能够通过新的想法转换进行扩展，从而可以用于开创新的提示方案。这项工作使得LLM的推理更接近人类思维。

    We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
    
[^119]: 自然语言是图表所需要的全部内容

    Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07134](http://arxiv.org/abs/2308.07134)

    本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。

    

    大规模预训练语言模型的出现，如ChatGPT，已经在人工智能的各个研究领域中引起了革命。基于Transformer的大型语言模型（LLMs）逐渐取代了CNN和RNN，将计算机视觉和自然语言处理领域统一起来。与相对独立存在的数据（如图像、视频或文本）相比，图表是一种包含丰富结构和关系信息的数据类型。同时，作为最具表现力的媒介之一，自然语言在描述复杂结构方面表现出色。然而，将图表学习问题纳入生成式语言建模框架的现有工作仍然非常有限。随着大型语言模型的重要性不断增长，探索LLMs是否也可以替代GNNs成为图表的基础模型变得至关重要。在本文中，我们提出了InstructGLM（结构化语言模型）算法，系统地设计高度可扩展的模型来处理图表学习问题。

    The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
    
[^120]: GIT-Mol：一种多模态大型语言模型用于分子科学中的图像，图形和文本

    GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text. (arXiv:2308.06911v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06911](http://arxiv.org/abs/2308.06911)

    GIT-Mol是一种多模态大型语言模型，可在分子科学中处理图像、图形和文本信息。通过新提出的GIT-Former架构，该模型能够将多种模态的数据对齐到一个统一的潜在空间中。与基线相比，GIT-Mol在性质预测和分子生成有效性方面取得了显著改进。此外，该模型还可用于化合物名称识别和化学反应预测等下游任务。

    

    大型语言模型在自然语言处理方面取得了重要进展，通过处理分子的文本表示，为分子科学中的创新应用提供了可能。然而，大多数现有的语言模型无法捕捉具有复杂分子结构或图像的丰富信息。在本文中，我们引入了GIT-Mol，一种集成了图形、图像和文本信息的多模态大型语言模型。为了促进多模态分子数据的集成，我们提出了GIT-Former，一种新颖的架构，能够将所有模态对齐到统一的潜在空间中。与基线相比，我们在性质预测方面实现了5%-10%的准确性提高，并在分子生成有效性方面提高了20.2%。通过任意到语言的分子翻译策略，我们的模型有潜力进行更多的下游任务，例如化合物名称识别和化学反应预测。

    Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties prediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.
    
[^121]: 通过基于大型语言模型的比较判定进行零样本NLG评估

    Zero-shot NLG evaluation through Pairware Comparisons with LLMs. (arXiv:2307.07889v1 [cs.CL])

    [http://arxiv.org/abs/2307.07889](http://arxiv.org/abs/2307.07889)

    本研究提出了一种使用开源大型语言模型进行零样本自然语言生成（NLG）评估的方法，通过配对比较判定来确定候选回应的优劣。结果表明，相较于绝对评分，比较评估是一种更有效的方法，并使得较小的开源LLMs达到了与更大的公共访问API相当的性能。

    

    评估自然语言生成（NLG）输出是至关重要但费时费力的。虽然已经提出了各种自动NLG评估方法，但它们通常是特定任务特定领域的，需要针对特定领域和属性进行工程设计。在这项工作中，我们提出了一种使用开源大型语言模型（LLMs）进行零样本NLG评估的稳健方法，采用了配对比较判定的方式。这种方法的动机是，即使作为人类，确定两个选项中哪个更好要比独立客观评分每个选项更容易。我们利用这一观察结果并利用LLMs新兴的能力，通过探测FlanT5，确定两个候选回应中哪一个更好，而不是指定绝对分数。我们的结果表明，比较评估是比绝对评分更有效的方法，使得较小的开源LLMs能够达到与更大的公共访问API相当的性能。我们评估了系统。

    Evaluating Natural Language Generation (NLG) outputs is crucial but laborious and expensive. While various automatic NLG assessment methods have been proposed, they often are quite task-specific and have to be engineered with a particular domain and attribute in mind. In this work, we propose a robust zero-shot approach to NLG evaluation using pairwise comparative judgment with open-source Large Language Models (LLMs). The motivation for this approach is that even as humans, it is easier to determine which of two options are better, than it is to independently objectively score each option. We use this insight and leverage the emergent abilities of LLMs, where we probe FlanT5 to determine which of two candidate responses is better, rather than assigning absolute scores. Our results demonstrate that comparative assessment is a more effective approach than absolute scoring, enabling smaller open-source LLMs to achieve comparable performance to larger public access APIs. We evaluate syste
    
[^122]: SITTA: 一种用于图像描述的语义图像文本对齐方法

    SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])

    [http://arxiv.org/abs/2307.05591](http://arxiv.org/abs/2307.05591)

    SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。

    

    对图像的文本和语义理解对于生成适当的描述非常重要。这需要检测图像中的对象，建模它们之间的关系，评估场景的语义，并将提取的知识表示在语言空间中。为了在保证良好的图像-语言映射的同时实现丰富的语言能力，预训练的语言模型（LMs）被条件化为预训练的多模态（图像-文本）模型，允许使用图像输入。这要求将多模态模型的视觉编码器中检测到的语义与生成性LM的语言表示进行对齐。然而，如何最好地将视觉编码器检测到的语义传递给LM还不清楚。我们介绍了两种构建线性映射的新方法，成功地将两个预训练模型的嵌入空间之间的语义转移。第一种方法是将多模态语言编码器的嵌入空间与生成性LM的嵌入空间进行对齐。

    Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
    
[^123]: 如何评估预训练语音模型的迁移性？

    How to Estimate Model Transferability of Pre-Trained Speech Models?. (arXiv:2306.01015v1 [cs.CL])

    [http://arxiv.org/abs/2306.01015](http://arxiv.org/abs/2306.01015)

    本文介绍了一个新的框架，可以高效地评估预训练语音模型在微调目标任务时的迁移性。该框架利用两个表示理论，通过生成候选模型的排名分数，可以在不进行实际微调的情况下计算迁移性分数，实验结果表明该框架与微调基础事实之间存在很高的相关性和低的p值，是一个节省资源、高效节省时间的微调方法。

    

    本文提出了一个“基于分数评估”的框架，用于估计预训练语音模型（PSMs）在微调目标任务时的迁移性。我们利用两个表示理论，贝叶斯似然估计和最优传输，使用提取的表示生成PSM候选的排名分数。通过假设独立性，我们的框架可以高效地计算迁移性分数，而无需实际微调候选模型或层。我们使用公共数据在交叉层和交叉模型设置中评估了一些流行的监督语音模型（例如Conformer RNN-Transducer）和自监督语音模型（例如HuBERT）。实验结果显示，我们的估计框架与微调基础事实之间存在很高的Spearman排名相关性和低的p值。我们提出的迁移性框架需要较少的计算时间和资源，因此是一个节省资源、高效节省时间的微调方法。

    In this work, we introduce a ``score-based assessment'' framework for estimating the transferability of pre-trained speech models (PSMs) for fine-tuning target tasks. We leverage upon two representation theories, Bayesian likelihood estimation and optimal transport, to generate rank scores for the PSM candidates using the extracted representations. Our framework efficiently computes transferability scores without actual fine-tuning of candidate models or layers by making a temporal independent hypothesis. We evaluate some popular supervised speech models (e.g., Conformer RNN-Transducer) and self-supervised speech models (e.g., HuBERT) in cross-layer and cross-model settings using public data. Experimental results show a high Spearman's rank correlation and low $p$-value between our estimation framework and fine-tuning ground truth. Our proposed transferability framework requires less computational time and resources, making it a resource-saving and time-efficient approach for tuning sp
    
[^124]: 大型语言模型是零样本文本到视频生成的帧级导演

    Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation. (arXiv:2305.14330v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.14330](http://arxiv.org/abs/2305.14330)

    本文引入了一个新的框架——DirecT2V，利用大型语言模型作为导演，从一个抽象的用户提示中生成零样本文本到视频生成的连贯且连贯的视频。该框架使用LLM导演将用户输入分为每一帧的提示，通过值映射和双softmax过滤器来保持时间一致和防止对象折叠。

    

    在人工智能生成内容（AIGC）的范式中，越来越多的关注点放在将预训练的文本到图像（T2I）模型扩展到文本到视频（T2V）生成上。尽管这些框架很有效，但它们面临着维护一致的叙述和处理从单个用户提示中的快速场景组合或对象位置变化的挑战。本文引入了一个新的框架，称为DirecT2V，它利用针对指令校准的大型语言模型（LLMs）从单个抽象用户提示生成逐帧描述。DirecT2V利用LLM导演将用户输入分为每个帧的单独提示，从而实现包含时间变化的内容和便于一致的视频生成。为了保持时间上的一致性和防止对象折叠，我们提出了一种新的值映射方法和双softmax过滤器。广泛的实验结果验证了DirecT2V框架在零样本T2V生成中产生的视觉连贯和一致的视频生成的有效性。

    In the paradigm of AI-generated content (AIGC), there has been increasing attention in extending pre-trained text-to-image (T2I) models to text-to-video (T2V) generation. Despite their effectiveness, these frameworks face challenges in maintaining consistent narratives and handling rapid shifts in scene composition or object placement from a single user prompt. This paper introduces a new framework, dubbed DirecT2V, which leverages instruction-tuned large language models (LLMs) to generate frame-by-frame descriptions from a single abstract user prompt. DirecT2V utilizes LLM directors to divide user inputs into separate prompts for each frame, enabling the inclusion of time-varying content and facilitating consistent video generation. To maintain temporal consistency and prevent object collapse, we propose a novel value mapping method and dual-softmax filtering. Extensive experimental results validate the effectiveness of the DirecT2V framework in producing visually coherent and consist
    
[^125]: 三思而后行：衡量消除问答模型预测快捷方式的效率

    Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models. (arXiv:2305.06841v1 [cs.CL])

    [http://arxiv.org/abs/2305.06841](http://arxiv.org/abs/2305.06841)

    研究提出了一种衡量模型依赖已知虚假特征的技术，并评估了预先训练的问答模型和去偏置方法对大量已知和新发现的预测偏差的鲁棒性。其发现去偏置方法不能通过减轻对偏差特征的依赖来解释OOD收益，表明偏差在QA数据集中共享。

    

    尽管大规模语言模型（Large Language Models，LLMs）主导了大部分语言理解任务，在训练数据集的建模假混淆的支持下, 因此有先前的工作表明，某些这些结果是由建模虚假相关性实现的。作者常常通过评估同一任务的分布外（OOD）数据集上的模型来评估模型的健壮性，但这些数据集可能具有与训练数据集相同的偏差。我们提出了一种衡量模型依赖于任何已知虚假特征的尺度的简单方法，并评估预先训练的模型和去偏置方法在问答（QA）中对大量已知和新发现的预测偏差的鲁棒性。我们发现去偏置方法的报告OOD收益不能通过减轻对有偏特征的依赖来解释，这表明偏差在QA数据集中共享。我们通过测量OOD模型的表现取决于偏差特征，与ID模型相当，进而证明了这一点，这推动了未来的研究。

    While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets. Authors commonly assess model robustness by evaluating their models on out-of-distribution (OOD) datasets of the same task, but these datasets might share the bias of the training dataset.  We propose a simple method for measuring a scale of models' reliance on any identified spurious feature and assess the robustness towards a large set of known and newly found prediction biases for various pre-trained models and debiasing methods in Question Answering (QA). We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among QA datasets. We further evidence this by measuring that performance of OOD models depends on bias features comparably to the ID model, motivating future work to refin
    
[^126]: 利用RMT将Transformer扩展到100万个标记及以上。

    Scaling Transformer to 1M tokens and beyond with RMT. (arXiv:2304.11062v1 [cs.CL])

    [http://arxiv.org/abs/2304.11062](http://arxiv.org/abs/2304.11062)

    本文介绍了一种利用循环记忆扩展BERT上下文长度的方法，成功扩展到了前所未有的200万个标记，有望增强自然语言处理中的长期依赖处理并为内存密集型应用程序实现大规模上下文处理。

    

    本技术报告介绍了一种利用循环记忆扩展BERT上下文长度的方法，BERT是自然语言处理中最有效的基于Transformer模型之一。通过利用循环记忆Transformer架构，我们成功地将模型的有效上下文长度增加到了前所未有的200万个标记，同时保持了高的内存检索准确性。我们的方法允许存储和处理本地和全局信息，并通过使用循环实现输入序列各部分之间的信息流动。我们的实验证明了我们的方法的有效性，具有显著的潜力来增强自然语言理解和生成任务中的长期依赖处理，并能够为内存密集型应用程序实现大规模上下文处理。

    This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.
    
[^127]: 通过互动反馈与代理交互来提高协作环境下基于实地理解（Grounded Language Understanding）的能力

    Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback. (arXiv:2304.10750v1 [cs.CL])

    [http://arxiv.org/abs/2304.10750](http://arxiv.org/abs/2304.10750)

    研究通过互动反馈与代理交互来提高协作环境下基于实地理解的能力。

    

    许多自然语言处理任务通常被视为单步问题。在这些任务中，代理接收一个指令，执行它，然后根据最终结果进行评估。然而，人类语言本质上是交互式的，我们主张人工智能与人类的协作也应是交互式的，人类监督人工智能代理的工作，并提供代理可以理解和利用的反馈信息。在这项工作中，我们探讨了通过Help Feedback实现这一目标的方向。

    Many approaches to Natural Language Processing (NLP) tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, human language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaborations.  In this work, we explore these directions using the challenging task defined by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We explore multiple types of help players can give to the AI to guide it and analyze the impac
    

