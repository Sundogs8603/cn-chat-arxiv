# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [PhonologyBench: Evaluating Phonological Skills of Large Language Models](https://arxiv.org/abs/2404.02456) | PhonologyBench是一个新颖的基准测试，旨在明确评估大型语言模型在英语中的音韵技能，展示了LLMs在没有语音数据情况下在PhonologyBench任务上表现出显著性能。 |
| [^2] | [Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models](https://arxiv.org/abs/2404.02124) | 通过大型语言模型探索数学多项选择题的自动生成干扰项，发现虽然LLMs可以生成一些数学上有效的干扰项，但在预测常见错误或误解方面表现不佳 |
| [^3] | [Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context](https://arxiv.org/abs/2404.02000) | 这项研究提出了首个仅在非洲语音上进行训练的自监督多语言语音模型，相比于常规方法，更高效并在ASR和LID任务中表现出竞争力。 |
| [^4] | [Octopus v2: On-device language model for super agent](https://arxiv.org/abs/2404.01744) | 该研究提出了一种新方法，通过将具有20亿参数的设备上模型赋予超越GPT-4的准确性和延迟性能，并将上下文长度缩减95％，从而解决了函数调用中的延迟和准确性问题。 |
| [^5] | [Prompt-prompted Mixture of Experts for Efficient LLM Generation](https://arxiv.org/abs/2404.01365) | 提出了一种名为GRIFFIN的训练-free MoE，能够在各种LLM模型中选择唯一的FF专家以实现高效生成。 |
| [^6] | [Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment](https://arxiv.org/abs/2404.01054) | 提出了Regularized Best-of-N (RBoN)，通过引入接近性项来减轻奖励欺骗，提高了算法在解码时与人类偏好对齐的效果。 |
| [^7] | [MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection](https://arxiv.org/abs/2403.14989) | 本文介绍了MasonTigers在SemEval-2024任务8上的表现分析，创新之处在于利用鉴别器Transformer模型的集成，结合句子Transformer和统计机器学习方法，以及在部分情况下采用零样本提示和针对FLAN-T5的微调。 |
| [^8] | [Attention-Driven Reasoning: Unlocking the Potential of Large Language Models](https://arxiv.org/abs/2403.14932) | 通过注意力机制优化，可以显著提高大型语言模型的推理能力，尤其对于非STEM问题。 |
| [^9] | [Unraveling the Mystery of Scaling Laws: Part I](https://arxiv.org/abs/2403.06563) | 确认缩放定律原则在模型预训练中的重要作用，揭示OpenAI原始缩放定律论文的不完整细节，并探究预测测试损失轨迹可靠公式的挑战 |
| [^10] | [Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset](https://arxiv.org/abs/2403.04460) | Pearl数据集利用了角色和知识增强的大型语言模型，提供了具体用户偏好，领域专业性和更相关的推荐。 |
| [^11] | [What Linguistic Features and Languages are Important in LLM Translation?](https://arxiv.org/abs/2402.13917) | Llama2模型在翻译中表现出准确度高，部分未见语言需要更大规模的模型来提升翻译质量，另外语言的句法相似性并非翻译质量的主要因素，某些语言即使数据少依然表现出强相关性。 |
| [^12] | [Multi-Word Tokenization for Sequence Compression](https://arxiv.org/abs/2402.09949) | 本论文介绍了一种名为MWT的多词标记器，通过将频繁出现的多词表达式表示为单个标记，突破了词边界的限制，从而实现更紧凑和高效的标记化，提高了性能并加速推理过程。 |
| [^13] | [Rethinking Machine Unlearning for Large Language Models](https://arxiv.org/abs/2402.08787) | 这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。 |
| [^14] | [Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models](https://arxiv.org/abs/2402.01740) | 这项研究研究了大型语言模型在选择对象时的偏见，发现偏见结构依赖于模型，对象类型调节了偏见的影响程度，导致列表中的第一个对象在输出中被过度呈现。 |
| [^15] | [Large Language Models for Mathematical Reasoning: Progresses and Challenges](https://arxiv.org/abs/2402.00157) | 大型语言模型(LLMs)在解决数学问题方面涉及了大量的数学问题类型和不同的数据集和设置。目前仍然存在一些挑战，需要进一步研究和解决。 |
| [^16] | [Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts.](http://arxiv.org/abs/2401.14295) | 这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。 |
| [^17] | [REE-HDSC: Recognizing Extracted Entities for the Historical Database Suriname Curacao.](http://arxiv.org/abs/2401.02972) | 本文介绍了REE-HDSC项目，旨在改进手写文本识别软件自动提取的命名实体的质量。通过六步处理流程，我们测试了该流程在处理库拉索民事登记处的19世纪和20世纪死亡证书时，日期提取具有高精度，人名提取的精度较低。我们提出了通过重新训练HTR模型、后处理和识别删除不正确的名字来提高人名提取精度的方法。 |
| [^18] | [ChipNeMo: Domain-Adapted LLMs for Chip Design.](http://arxiv.org/abs/2311.00176) | ChipNeMo通过领域自适应技术，实现了在工业芯片设计中大幅提升LLM性能，同时减小了模型尺寸，在工程助手、脚本生成和缺陷分析等方面具有良好表现。 |
| [^19] | [CapsFusion: Rethinking Image-Text Data at Scale.](http://arxiv.org/abs/2310.20550) | CapsFusion是一个先进的框架，通过利用大型语言模型整合和细化来自网络图像-文本对和合成字幕的信息，提供了更高质量、更可扩展的多模态预训练数据。 |
| [^20] | [LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud.](http://arxiv.org/abs/2309.17157) | LatticeGen是一个协作框架，通过将真实生成的文本与噪声混合并隐藏在格子中，以保护用户的隐私。实验证明，LatticeGen能够在面对强攻击时成功保护真实生成，超过50%的语义仍然隐藏。 |
| [^21] | [The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A".](http://arxiv.org/abs/2309.12288) | LLMs模型在训练中只能学习到"A是B"的结构，无法自动推广到"B是A"。这表明模型在逻辑推断上存在基本失败和训练集中模式的推广问题。 |
| [^22] | [Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?.](http://arxiv.org/abs/2309.08963) | 本研究评估了当前大型语言模型（LLMs）在生成复杂结构化数据方面的能力，并提出了一种结构感知的微调方法来改善这种能力。通过使用Struc-Bench和多个代表性的LLMs进行评估，发现了常见的格式错误和潜在改进的领域。通过应用结构感知微调方法，能够显著提高对自然语言约束的遵守程度。 |
| [^23] | [PROGrasp: Pragmatic Human-Robot Communication for Object Grasping.](http://arxiv.org/abs/2309.07759) | PROGrasp是一个实现物体抓取的人机交流系统，通过使用面向意图的多模态对话和答案解释模块，机器人能够根据用户的意图来识别和抓取目标物体。 |
| [^24] | [AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators.](http://arxiv.org/abs/2303.16854) | 本文提出了一种两步法，即“先解释再注释”，以使大型语言模型（LLMs）成为更好的众包标注器，首先为每个演示实例创建提示，随后利用这些提示提示LLM提供解释。 |

# 详细

[^1]: PhonologyBench：评估大型语言模型的音韵技能

    PhonologyBench: Evaluating Phonological Skills of Large Language Models

    [https://arxiv.org/abs/2404.02456](https://arxiv.org/abs/2404.02456)

    PhonologyBench是一个新颖的基准测试，旨在明确评估大型语言模型在英语中的音韵技能，展示了LLMs在没有语音数据情况下在PhonologyBench任务上表现出显著性能。

    

    音韵学是研究语音结构和发音规则的学科，是大型语言模型（LLM）研究中一个关键但经常被忽视的组成部分。LLMs在各种利用音韵学的下游应用中被广泛使用，如教育工具和诗歌生成。此外，LLMs可能会从训练数据中学习不完美的正字和音标形式之间的关联。因此，对LLMs的音韵技能进行基准测试至关重要。为此，我们提出了PhonologyBench，这是一个新颖的基准测试，包括三个诊断任务，旨在明确测试LLMs在英语中的音韵技能：形音转换、音节计数和押韵词生成。尽管没有访问语音数据，LLMs在PhonologyBench任务上表现出显著的性能。然而，我们观察到在押韵词生成和音节计数方面存在显著的17%和45%的差距， respectively, when...

    arXiv:2404.02456v1 Announce Type: cross  Abstract: Phonology, the study of speech's structure and pronunciation rules, is a critical yet often overlooked component in Large Language Model (LLM) research. LLMs are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation. Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data. Thus, it is imperative to benchmark the phonological skills of LLMs. To this end, we present PhonologyBench, a novel benchmark consisting of three diagnostic tasks designed to explicitly test the phonological skills of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation. Despite having no access to speech data, LLMs showcased notable performance on the PhonologyBench tasks. However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when 
    
[^2]: 通过大型语言模型探索数学多项选择题的自动生成干扰项

    Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models

    [https://arxiv.org/abs/2404.02124](https://arxiv.org/abs/2404.02124)

    通过大型语言模型探索数学多项选择题的自动生成干扰项，发现虽然LLMs可以生成一些数学上有效的干扰项，但在预测常见错误或误解方面表现不佳

    

    多项选择题在几乎所有教育层次中都是普遍存在的，因为它们易于管理、评分，并且是评估和实践中可靠的格式。其中最重要的方面之一是干扰项，即针对真实学生常见错误或误解而设计的不正确选项。目前，制作高质量干扰项的任务在很大程度上仍然是教师和学习内容设计者的劳动和耗时工作，这限制了可扩展性。在这项工作中，我们研究了在数学多项选择题领域中自动生成干扰项的任务，并探索了各种基于大型语言模型（LLM）的方法，从上下文学习到微调。我们使用真实数学多项选择题数据集进行了大量实验，发现虽然LLM可以生成一些数学上有效的干扰项，但它们在预测常见错误或误解方面表现不佳。

    arXiv:2404.02124v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconcept
    
[^3]: 非洲中心自监督预训练技术在撒哈拉以南地区的多语言语音表征中的应用

    Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context

    [https://arxiv.org/abs/2404.02000](https://arxiv.org/abs/2404.02000)

    这项研究提出了首个仅在非洲语音上进行训练的自监督多语言语音模型，相比于常规方法，更高效并在ASR和LID任务中表现出竞争力。

    

    我们提出了第一个仅在非洲语音上进行训练的自监督多语言语音模型。该模型从撒哈拉以南非洲地区讲话的21种语言和方言中学习了近60,000小时的未标记语音片段。在FLEURS-102数据集的SSA子集上，我们基于HuBERT$_{base}$ (0.09B) 架构的方法展现出了具有竞争力的结果，与FLEURS基准提出的w2v-bert-51 (0.6B) 预训练模型相比，在ASR下游任务中更加高效，使用的数据量少7倍，参数少6倍。此外，在LID下游任务中，我们的方法在准确率上超过FLEURS基线超过22%。

    arXiv:2404.02000v1 Announce Type: new  Abstract: We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA subset of the FLEURS-102 dataset, our approach based on a HuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed in the FLEURS benchmark, while being more efficient by using 7x less data and 6x less parameters. Furthermore, in the context of a LID downstream task, our approach outperforms FLEURS baselines accuracy by over 22\%.
    
[^4]: Octopus v2：用于超级代理的设备上语言模型

    Octopus v2: On-device language model for super agent

    [https://arxiv.org/abs/2404.01744](https://arxiv.org/abs/2404.01744)

    该研究提出了一种新方法，通过将具有20亿参数的设备上模型赋予超越GPT-4的准确性和延迟性能，并将上下文长度缩减95％，从而解决了函数调用中的延迟和准确性问题。

    

    语言模型在各种软件应用中展现出了高效性，特别是与自动工作流相关的任务。这些模型具有调用函数的关键能力，在创建AI代理时至关重要。尽管大规模语言模型在云环境中表现出色，但往往存在着隐私和成本方面的担忧。当前用于函数调用的设备上模型面临延迟和准确性问题。我们的研究提出了一种新方法，使具有20亿参数的设备上模型在准确性和延迟方面超越了GPT-4，并将上下文长度缩减了95%。与基于RAG的函数调用机制的Llama-7B相比，我们的方法将延迟提高了35倍。这种方法将延迟降低到适合在生产环境中的各种边缘设备上部署的水平上，符合性能要求。

    arXiv:2404.01744v1 Announce Type: new  Abstract: Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisite
    
[^5]: 基于提示的混合专家模型用于高效生成LLM

    Prompt-prompted Mixture of Experts for Efficient LLM Generation

    [https://arxiv.org/abs/2404.01365](https://arxiv.org/abs/2404.01365)

    提出了一种名为GRIFFIN的训练-free MoE，能够在各种LLM模型中选择唯一的FF专家以实现高效生成。

    

    随着基于transformer的大规模语言模型（LLMs）的发展，由于其出色的实用性，它们已被应用于许多领域，但在部署时存在相当大的计算成本。幸运的是，一些方法，如修剪或构建混合专家（MoE），旨在利用transformer前馈（FF）块中的稀疏性，以提高速度并降低内存需求。但是，这些技术在实践中可能非常昂贵和不灵活，因为它们通常需要训练或仅限于特定类型的架构。为了解决这个问题，我们引入了GRIFFIN，一种新颖的无需训练的MoE，它在序列级别为不同非ReLU激活函数的大量LLMs选择独特的FF专家以实现高效生成。这是可能的，因为我们关键观察到，许多经过训练的LLMs在序列中自然产生高度结构化的FF激活模式，这

    arXiv:2404.01365v1 Announce Type: cross  Abstract: With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which
    
[^6]: 正则化的最佳-N采样以减轻语言模型对齐中的奖励欺骗问题

    Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment

    [https://arxiv.org/abs/2404.01054](https://arxiv.org/abs/2404.01054)

    提出了Regularized Best-of-N (RBoN)，通过引入接近性项来减轻奖励欺骗，提高了算法在解码时与人类偏好对齐的效果。

    

    Best-of-N (BoN)采样与奖励模型已被证明是一种有效的策略，用于在解码时将大型语言模型(LLMs)与人类偏好对齐。然而，BoN采样容易受到奖励欺骗问题的影响。为了防止奖励欺骗，我们提出了一种名为Regularized Best-of-N (RBoN)的变体，通过在响应选择中结合接近性项来减轻奖励欺骗，类似于偏好学习技术。

    arXiv:2404.01054v1 Announce Type: cross  Abstract: Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially wh
    
[^7]: MasonTigers在SemEval-2024任务8上的表现分析：基于Transformer模型的机器生成文本检测

    MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection

    [https://arxiv.org/abs/2403.14989](https://arxiv.org/abs/2403.14989)

    本文介绍了MasonTigers在SemEval-2024任务8上的表现分析，创新之处在于利用鉴别器Transformer模型的集成，结合句子Transformer和统计机器学习方法，以及在部分情况下采用零样本提示和针对FLAN-T5的微调。

    

    本文介绍了MasonTigers参加SemEval-2024任务8的情况 - 多生成器、多领域和多语言的黑盒机器生成文本检测。该任务涵盖了二元人工撰写 vs. 机器生成文本分类（Track A）、多路机器生成文本分类（Track B）和人机混合文本检测（Track C）。我们的最佳方法主要利用鉴别器Transformer模型的集成，以及在特定情况下句子Transformer和统计机器学习方法。此外，对于Track A和B，还使用了零样本提示和对FLAN-T5的微调。

    arXiv:2403.14989v1 Announce Type: new  Abstract: This paper presents the MasonTigers entry to the SemEval-2024 Task 8 - Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection. The task encompasses Binary Human-Written vs. Machine-Generated Text Classification (Track A), Multi-Way Machine-Generated Text Classification (Track B), and Human-Machine Mixed Text Detection (Track C). Our best performing approaches utilize mainly the ensemble of discriminator transformer models along with sentence transformer and statistical machine learning approaches in specific cases. Moreover, zero-shot prompting and fine-tuning of FLAN-T5 are used for Track A and B.
    
[^8]: 专注驱动的推理:释放大型语言模型的潜力

    Attention-Driven Reasoning: Unlocking the Potential of Large Language Models

    [https://arxiv.org/abs/2403.14932](https://arxiv.org/abs/2403.14932)

    通过注意力机制优化，可以显著提高大型语言模型的推理能力，尤其对于非STEM问题。

    

    大型语言模型（LLMs）展示了卓越的能力，但它们的推理能力和基础机制仍不为人所了解。我们提出了一种通过注意力机制优化来增强LLMs推理能力的新方法，而无需额外的训练数据。我们确定了由非语义标记导致的注意力分布的低效率，并提出了一种算法来重新平衡偏斜分布，使模型能够抽象更加微妙的知识。我们的实验表明，推理能力得到了显着改进，特别是对于非STEM问题。我们深入探讨了注意力模式在LLMs推理中的作用，并提出了一种增强这些能力的方法，为更强大和多功能的语言模型铺平了道路。

    arXiv:2403.14932v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their reasoning abilities and underlying mechanisms remain poorly understood. We present a novel approach to enhance LLMs' reasoning through attention mechanism optimization, without additional training data. We identify inefficiencies in the attention distribution caused by non-semantic tokens and propose an algorithm to re-balance the skewed distribution, enabling the model to abstract more nuanced knowledge. Our experiments demonstrate significantly improved reasoning capabilities, particularly for non-STEM questions. We provide insights into the role of attention patterns in LLMs' reasoning and propose a method to enhance these abilities, paving the way for more powerful and versatile language models.
    
[^9]: 揭开缩放定律之谜：第一部分

    Unraveling the Mystery of Scaling Laws: Part I

    [https://arxiv.org/abs/2403.06563](https://arxiv.org/abs/2403.06563)

    确认缩放定律原则在模型预训练中的重要作用，揭示OpenAI原始缩放定律论文的不完整细节，并探究预测测试损失轨迹可靠公式的挑战

    

    缩放定律原则表明在模型大小、数据集大小和训练过程中使用的计算资源等变量之间存在幂定律相关性。这些原则在优化模型预训练的各个方面中起着至关重要的作用，最终有助于大型语言模型（如GPT-4、Llama和Gemini）的成功。然而，OpenAI的原始缩放定律论文并未披露推导精确缩放定律公式所必需的完整细节，他们的结论仅基于包含高达15亿参数的模型。尽管一些后续作品试图揭示这些细节并扩展到更大的模型，但它们经常忽略了重要因素的训练依赖性，如学习速率、上下文长度和批量大小，导致它们未能建立一个可靠的预测测试损失轨迹的公式。在本技术报告中，我们确认了缩放

    arXiv:2403.06563v1 Announce Type: cross  Abstract: Scaling law principles indicate a power-law correlation between loss and variables such as model size, dataset size, and computational resources utilized during training. These principles play a vital role in optimizing various aspects of model pre-training, ultimately contributing to the success of large language models such as GPT-4, Llama and Gemini. However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters. Though some subsequent works attempt to unveil these details and scale to larger models, they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for predicting the test loss trajectory. In this technical report, we confirm that the scaling 
    
[^10]: Pearl: 一项基于评论驱动的角色知识对话式推荐数据集

    Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset

    [https://arxiv.org/abs/2403.04460](https://arxiv.org/abs/2403.04460)

    Pearl数据集利用了角色和知识增强的大型语言模型，提供了具体用户偏好，领域专业性和更相关的推荐。

    

    arXiv:2403.04460v1 公告类型：新摘要：对话式推荐系统是一个新兴领域，尤其是随着大型语言模型（LLMs）的进步，使得对话输入的多样化推理引起了社区的越来越大的兴趣。尽管取得了进展，但该领域还有许多方面有待探索。目前可用的用于对话式推荐的公共数据集缺乏特定用户偏好和对推荐的解释，从而妨碍了高质量的推荐。为了解决这些挑战，我们提出了一种新颖的对话式推荐数据集，命名为PEARL，与角色和知识增强的LLM模拟器相结合。我们从真实评论中获得详细的角色和知识，并构建了一个超过57k对话的大规模数据集。我们的实验结果表明，PEARL中的话语包括更具体的用户偏好，显示了在目标领域的专业知识，并提供了更相关的推荐。

    arXiv:2403.04460v1 Announce Type: new  Abstract: Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relev
    
[^11]: LLM翻译中的语言特征和重要语言是什么？

    What Linguistic Features and Languages are Important in LLM Translation?

    [https://arxiv.org/abs/2402.13917](https://arxiv.org/abs/2402.13917)

    Llama2模型在翻译中表现出准确度高，部分未见语言需要更大规模的模型来提升翻译质量，另外语言的句法相似性并非翻译质量的主要因素，某些语言即使数据少依然表现出强相关性。

    

    arXiv：2402.13917v1 公告类型：跨领域 摘要：大型语言模型（LLMs）展示了在多个任务中具有强大能力，包括机器翻译。我们的研究重点在于评估Llama2的机器翻译能力，并探索翻译如何取决于其训练数据中的语言。我们的实验表明，7B Llama2模型对其所见的所有语言都可以获得超过10的BLEU分数，但并非总是对其未见的语言。对于这些未见语言，与使用聊天版本或添加少量数据相比，在模型规模上观察到的最大收益。此外，我们的语言距离分析显示，句法相似性并非始终是决定翻译质量的主要语言因素。有趣的是，我们发现在特定情况下，一些语言，尽管训练数据明显少于英语，却表现出与英语可比的强相关性。我们在这里的发现为研究提供了新的视角。

    arXiv:2402.13917v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the 
    
[^12]: 多词标记化用于序列压缩

    Multi-Word Tokenization for Sequence Compression

    [https://arxiv.org/abs/2402.09949](https://arxiv.org/abs/2402.09949)

    本论文介绍了一种名为MWT的多词标记器，通过将频繁出现的多词表达式表示为单个标记，突破了词边界的限制，从而实现更紧凑和高效的标记化，提高了性能并加速推理过程。

    

    大型语言模型在建模各种任务方面取得了极大成功。然而，这也意味着计算成本的大幅增加，限制了其在工业界的广泛应用。本论文介绍了一种名为MWT的多词标记器，通过将频繁出现的多词表达式表示为单个标记，突破了词边界的限制。MWT产生了更紧凑和高效的标记化结果，带来两个好处：（1）在固定序列长度和预算的情况下，提高了性能，因为能够更全面地覆盖输入数据；（2）由于能够减少序列长度而对性能几乎没有影响，从而实现更快速和更轻量的推理过程。我们的结果表明，MWT在较短的序列长度下更为稳健，从而可以通过早期序列截断实现重大加速。

    arXiv:2402.09949v1 Announce Type: new  Abstract: Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this pa005 per, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length and budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.
    
[^13]: 重新思考大型语言模型的机器消除技术

    Rethinking Machine Unlearning for Large Language Models

    [https://arxiv.org/abs/2402.08787](https://arxiv.org/abs/2402.08787)

    这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。

    

    我们研究了大型语言模型（LLM）领域的机器消除技术（MU），称为LLM消除技术。这个研究旨在消除不良数据的影响（例如敏感或非法信息）以及相关模型的能力，同时保持基本的知识生成的完整性，并不影响因果无关的信息。我们设想LLM消除技术将成为LLM生命周期管理中的关键要素，可能成为开发既安全、可靠又资源高效的生成式人工智能的基础，而无需进行完全重训练。我们从概念、方法、评估指标和应用等方面探索了LLM消除技术的研究领域。特别是，我们突出了现有LLM消除技术研究中经常被忽视的方面，例如消除范围、数据模型交互和多方面的有效性评估。

    arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
    
[^14]: 在认知负荷下的补偿性偏见：减少大型语言模型中的选择偏见

    Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models

    [https://arxiv.org/abs/2402.01740](https://arxiv.org/abs/2402.01740)

    这项研究研究了大型语言模型在选择对象时的偏见，发现偏见结构依赖于模型，对象类型调节了偏见的影响程度，导致列表中的第一个对象在输出中被过度呈现。

    

    大型语言模型（LLMs）如gpt-3.5-turbo和claude-instant-1.2在解释和执行语义任务方面变得非常重要。不幸的是，这些模型固有的偏见，类似于人类的认知偏见，会对它们的性能产生不利影响。其中一个受到影响最大的是从列表中进行对象选择，这是数字导航和决策制定中的基本操作。本研究重点检查这些偏见，并量化其对代表性列表选择任务的影响。通过进行一系列控制实验，我们操纵了温度、列表长度、对象身份、对象类型、提示复杂度和模型，以探索这些偏见。这使得我们能够孤立和测量这些偏见对选择行为的影响。我们的研究结果表明，偏见结构在很大程度上取决于模型，而对象类型调节了偏见影响的程度。由于存在较强的初现效应，列表中的第一个对象会在输出中被过度呈现。

    Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have become instrumental in interpreting and executing semantic-based tasks. Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance. Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we conducted a series of controlled experiments, manipulating temperature, list length, object identity, object type, prompt complexity, and model. This enabled us to isolate and measure the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproprotionately represented in ou
    
[^15]: 大型语言模型在数学推理中的应用：进展与挑战

    Large Language Models for Mathematical Reasoning: Progresses and Challenges

    [https://arxiv.org/abs/2402.00157](https://arxiv.org/abs/2402.00157)

    大型语言模型(LLMs)在解决数学问题方面涉及了大量的数学问题类型和不同的数据集和设置。目前仍然存在一些挑战，需要进一步研究和解决。

    

    数学推理是评估人类智能基本认知能力的基石。近年来，大型语言模型（LLMs）的发展引起了人们对自动解决数学问题的重视。然而，数学问题的类型非常广泛，LLM相关技术在不同数据集和设置下进行评估，使得如何判断这一新兴领域中的真正进展和障碍变得困难。本调查研究包括了以下四个关键方面：i）全面探索各种已经研究的数学问题及其相应数据集；ii）研究提出的解决数学问题的LLM技术的范围；iii）概述影响LLM在解决数学问题中的因素和关注点；iv）阐明仍然存在的挑战。

    Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges with
    
[^16]: 推理的拓扑学：揭秘思维链、树和图

    Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])

    [http://arxiv.org/abs/2401.14295](http://arxiv.org/abs/2401.14295)

    这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。

    

    自然语言处理（NLP）领域近年来取得了显著进展，特别是在通过创新的提示技术提高大型语言模型（LLM）性能方面。其中，与结构相结合的提示工程被视为一种有前途的范式，其设计如思维链、思维树或思维图等，通过结构指导整体LLM推理过程。通过大量实例的说明，这种范式显著增强了LLM在逻辑或数学推理、规划或创造性写作等各种任务中的能力。为了方便理解这个不断发展的领域并为未来的发展铺平道路，我们设计了一个有效和高效的LLM推理方案的通用蓝图。为此，我们对提示执行流程进行了深入分析，澄清并明确定义了不同的概念。然后我们建立第一个分类系统

    The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
    
[^17]: REE-HDSC: 识别历史数据库Suriname Curacao中提取的实体

    REE-HDSC: Recognizing Extracted Entities for the Historical Database Suriname Curacao. (arXiv:2401.02972v1 [cs.CL])

    [http://arxiv.org/abs/2401.02972](http://arxiv.org/abs/2401.02972)

    本文介绍了REE-HDSC项目，旨在改进手写文本识别软件自动提取的命名实体的质量。通过六步处理流程，我们测试了该流程在处理库拉索民事登记处的19世纪和20世纪死亡证书时，日期提取具有高精度，人名提取的精度较低。我们提出了通过重新训练HTR模型、后处理和识别删除不正确的名字来提高人名提取精度的方法。

    

    我们描述了REE-HDSC项目，并概述了我们努力提高手写文本识别（HTR）软件生成的文本自动提取的命名实体质量的工作。我们描述了一个六步处理流程，并通过处理库拉索民事登记处的19世纪和20世纪死亡证书进行测试。我们发现该流水线提取的日期具有高精度，但人名提取的精度较低。接下来，我们展示了如何通过重新训练含有名字的HTR模型、后处理以及识别和删除不正确的名字来改善名字提取的精度。

    We describe the project REE-HDSC and outline our efforts to improve the quality of named entities extracted automatically from texts generated by hand-written text recognition (HTR) software. We describe a six-step processing pipeline and test it by processing 19th and 20th century death certificates from the civil registry of Curacao. We find that the pipeline extracts dates with high precision but that the precision of person name extraction is low. Next we show how name precision extraction can be improved by retraining HTR models with names, post-processing and by identifying and removing incorrect names.
    
[^18]: ChipNeMo: 用于芯片设计的领域自适应LLMs

    ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])

    [http://arxiv.org/abs/2311.00176](http://arxiv.org/abs/2311.00176)

    ChipNeMo通过领域自适应技术，实现了在工业芯片设计中大幅提升LLM性能，同时减小了模型尺寸，在工程助手、脚本生成和缺陷分析等方面具有良好表现。

    

    ChipNeMo旨在探索大型语言模型（LLMs）在工业芯片设计中的应用。我们不直接使用商业或开源LLMs，而是采用以下领域自适应技术：定制分词器、领域自适应持续预训练、带有领域特定指令的监督微调（SFT）和领域自适应检索模型。我们在芯片设计的三个选定LLM应用上评估了这些方法：工程助手聊天机器人、EDA脚本生成以及缺陷摘要和分析。我们的结果显示，这些领域自适应技术使LLM在这三个应用中性能大幅提升，在各种设计任务上可以实现高达5倍的模型尺寸缩减，同时具有类似或更好的性能。我们的研究结果还表明，当前的结果和理想结果之间还有改进的空间。我们相信进一步的研究将有助于解决这个问题。

    ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigati
    
[^19]: CapsFusion: 重新思考大规模图像-文本数据

    CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.20550](http://arxiv.org/abs/2310.20550)

    CapsFusion是一个先进的框架，通过利用大型语言模型整合和细化来自网络图像-文本对和合成字幕的信息，提供了更高质量、更可扩展的多模态预训练数据。

    

    大规模多模态模型展示了在零样本情况下执行多样化多模态任务的显著泛化能力。大规模基于网络的图像-文本对在这一成功中起着根本性的贡献，但存在着过多的噪声。最近的研究使用由生成式字幕模型合成的替代字幕，并取得了显著的基准性能。然而，我们的实验证明，在使用合成字幕训练的模型中存在着显著的可扩展性不足和世界知识丧失问题，这些问题在其初始基准成功中大部分被掩盖了。经过进一步的研究，我们确定根本原因是现有合成字幕中过于简化的语言结构和缺乏知识细节。为了提供更高质量、更可扩展的多模态预训练数据，我们提出了CapsFusion，这是一个先进的框架，利用大型语言模型来整合和细化来自网络图像-文本对和合成字幕的信息。

    Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experi
    
[^20]: LatticeGen: 一种在云上进行隐私感知生成的协作框架，隐藏生成的文本在格子中

    LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud. (arXiv:2309.17157v1 [cs.CL])

    [http://arxiv.org/abs/2309.17157](http://arxiv.org/abs/2309.17157)

    LatticeGen是一个协作框架，通过将真实生成的文本与噪声混合并隐藏在格子中，以保护用户的隐私。实验证明，LatticeGen能够在面对强攻击时成功保护真实生成，超过50%的语义仍然隐藏。

    

    在当前的用户-服务器交互模式中，使用大型语言模型（LLM）进行提示生成的过程中，服务器完全控制着生成过程，这使得想要将生成的文本保留给自己的用户没有任何选择。我们提出了LatticeGen，一个协作框架，在该框架中，服务器仍然处理大部分计算任务，而用户控制采样操作。其核心思想是用户将真实生成序列与噪声标记混合，并隐藏在一个带噪声的格子中。考虑到来自假设恶意服务器的潜在攻击以及用户如何进行防御，我们提出了重复波束搜索攻击和混合噪声方案。在实验中，我们将LatticeGen应用于保护提示和生成。结果显示，虽然带噪声的格子会降低生成质量，但LatticeGen成功地在强攻击下显著保护了真实生成（超过50%的语义仍然隐藏）。

    In the current user-server interaction paradigm of prompted generation with large language models (LLM) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves. We propose LatticeGen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. Considering potential attacks from a hypothetically malicious server and how the user can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as 
    
[^21]: 翻转诅咒: 在大型语言模型中训练的"A是B"无法学习"B是A"

    The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])

    [http://arxiv.org/abs/2309.12288](http://arxiv.org/abs/2309.12288)

    LLMs模型在训练中只能学习到"A是B"的结构，无法自动推广到"B是A"。这表明模型在逻辑推断上存在基本失败和训练集中模式的推广问题。

    

    我们揭示了自回归大型语言模型（LLM）在泛化上的令人惊讶的失败。如果一个模型是基于"A是B"形式的句子进行训练，它不会自动推广到相反的方向"B是A"。这就是翻转诅咒。例如，如果一个模型是基于"Olaf Scholz是德国第九任总理"进行训练的，它不会自动能够回答问题"谁是德国第九任总理？"。此外，正确答案（"Olaf Scholz"）的可能性不会比随机名字更高。因此，模型在逻辑推断上存在基本失败，并且不会推广到它们训练集中的普遍模式（即如果出现"A是B"，则"B是A"更可能出现）。我们通过在虚构的陈述（如"Uriah Hawthorne是'Abyssal Melodies'的作曲家"）上对GPT-3和Llama-1进行微调，并展示它们无法正确回答"谁创作了'Abyssal Melodies'?"来提供翻转诅咒的证据。

    We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
    
[^22]: Struc-Bench：大型语言模型在生成复杂结构化数据方面表现得真的好吗？

    Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?. (arXiv:2309.08963v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.08963](http://arxiv.org/abs/2309.08963)

    本研究评估了当前大型语言模型（LLMs）在生成复杂结构化数据方面的能力，并提出了一种结构感知的微调方法来改善这种能力。通过使用Struc-Bench和多个代表性的LLMs进行评估，发现了常见的格式错误和潜在改进的领域。通过应用结构感知微调方法，能够显著提高对自然语言约束的遵守程度。

    

    尽管像GPT-4这样的大型语言模型（LLMs）非常强大，但它们在生成需要复杂结构化输出的任务上仍然有困难。在本研究中，我们评估了当前LLMs在生成复杂结构化数据方面的能力，并提出了一种结构感知的微调方法作为改进这种能力的解决方案。为了进行全面评估，我们提出了Struc-Bench，包括五个代表性的LLM（即GPT-NeoX 20B，GPT-3.5，GPT-4和Vicuna），并在我们精心构建的跨原始文本、HTML和LaTeX表的数据集上对它们进行评估。根据我们对当前模型性能的分析，我们确定了特定的常见格式错误和潜在改进的领域。为了解决复杂的格式要求，我们利用FormatCoT（思维链）从目标输出中生成格式指令。我们的实验证明，当将这种结构感知微调方法应用到LLaMA-7B上时，能够显著提高对自然语言约束的遵守程度。

    Despite the power of Large Language Models (LLMs) like GPT-4, they still struggle with tasks that require generating complex, structured outputs. In this study, we assess the capability of Current LLMs in generating complex structured data and propose a structure-aware fine-tuning approach as a solution to improve this ability. To perform a comprehensive evaluation, we propose Struc-Bench, include five representative LLMs (i.e., GPT-NeoX 20B, GPT-3.5, GPT-4, and Vicuna) and evaluate them on our carefully constructed datasets spanning raw text, HTML, and LaTeX tables. Based on our analysis of current model performance, we identify specific common formatting errors and areas of potential improvement. To address complex formatting requirements, we utilize FormatCoT (Chain-of-Thought) to generate format instructions from target outputs. Our experiments show that our structure-aware fine-tuning method, when applied to LLaMA-7B, significantly improves adherence to natural language constraint
    
[^23]: PROGrasp:实现物体抓取的人机交流

    PROGrasp: Pragmatic Human-Robot Communication for Object Grasping. (arXiv:2309.07759v1 [cs.CL])

    [http://arxiv.org/abs/2309.07759](http://arxiv.org/abs/2309.07759)

    PROGrasp是一个实现物体抓取的人机交流系统，通过使用面向意图的多模态对话和答案解释模块，机器人能够根据用户的意图来识别和抓取目标物体。

    

    交互式物体抓取(IOG)是通过人机自然语言交流识别和抓取目标物体的任务。当前IOG系统假定人类用户最初指定目标对象的类别(例如，瓶子)。受到语用学的启发，人类往往通过依赖上下文来传达意图以实现目标。我们引入了一项新的IOG任务，即实用IOG，并提出相应的数据集，即面向意图的多模态对话(IM-Dial)。在我们提出的任务场景中，首先给出一个面向意图的话语(例如，“我渴了”)。然后，机器人应通过与人类用户互动来识别目标对象。基于任务设置，我们提出了一个新的机器人系统，可以解释用户的意图并捡起目标对象，即实用物体抓取(PROGrasp)。PROGrasp通过结合视觉定位、问题提问、物体抓取以及最重要的，答案解释模块执行实用IOG。

    Interactive Object Grasping (IOG) is the task of identifying and grasping the desired object via human-robot natural language interaction. Current IOG systems assume that a human user initially specifies the target object's category (e.g., bottle). Inspired by pragmatics, where humans often convey their intentions by relying on context to achieve goals, we introduce a new IOG task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an intention-oriented utterance (e.g., "I am thirsty") is initially given to the robot. The robot should then identify the target object by interacting with a human user. Based on the task setup, we propose a new robotic system that can interpret the user's intention and pick up the target object, Pragmatic Object Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules for visual grounding, question asking, object grasping, and most importantly, answer interpre
    
[^24]: AnnoLLM：使大型语言模型成为更好的众包标注器

    AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators. (arXiv:2303.16854v1 [cs.CL])

    [http://arxiv.org/abs/2303.16854](http://arxiv.org/abs/2303.16854)

    本文提出了一种两步法，即“先解释再注释”，以使大型语言模型（LLMs）成为更好的众包标注器，首先为每个演示实例创建提示，随后利用这些提示提示LLM提供解释。

    

    许多自然语言处理（NLP）任务依赖于带标签的数据，以训练机器学习模型实现高性能。然而，数据注释可能是一个耗时且昂贵的过程，特别是当任务涉及大量数据或需要专业领域时。最近，GPT-3.5系列模型在各种NLP任务中展示出了令人瞩目的少样本和零样本能力。本文首先声称，大型语言模型（LLMs），如GPT-3.5，可以通过为它们提供充分的指导和演示示例来作为优秀的众包标注器。为了使LLMs成为更好的标注器，我们提出了一种两步方法，“先解释再注释”。更确切地说，我们首先为每个演示示例创建提示，随后利用这些提示提示LLM提供关于为什么对于特定示例选择了特定的基础真相回答/标签的解释。随后，我们构建了few-shot思维链。

    Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thoug
    

