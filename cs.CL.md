# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards](https://arxiv.org/abs/2402.18571) | 提出了方向偏好对齐（DPA）框架，通过多目标奖励模拟不同偏好配置，以实现用户相关的偏好控制。 |
| [^2] | [Approaching Human-Level Forecasting with Language Models](https://arxiv.org/abs/2402.18563) | 该研究探讨了使用语言模型（LMs）进行预测未来事件的能力，开发了一种检索增强型LM系统，通过在竞争性预测平台收集数据集，并在知识截止日期后评估系统性能，发现该系统能够准确预测未来事件并在某些情况下超越人类预测者。 |
| [^3] | [Implicit Bias of Next-Token Prediction](https://arxiv.org/abs/2402.18551) | 在基于梯度的优化器训练下的线性NTP模型中，确定了NTP可分离条件，并证明梯度下降能够实现其下界；同时证明了这些条件在过参数化时仍然成立。 |
| [^4] | [Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates](https://arxiv.org/abs/2402.18540) | 提出了“纯粹调优，安全测试”（PTST）原则，即在微调时不包含安全提示，但在测试时加入，可以显著减少LLMs中不安全行为的出现。 |
| [^5] | [RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval](https://arxiv.org/abs/2402.18510) | 本文研究了RNNs和Transformer在处理算法问题时的表现能力差距，发现RNNs存在关键瓶颈，即无法完美地从上下文中检索信息，导致无法像Transformer那样轻松解决需要这种能力的任务。 |
| [^6] | [Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification](https://arxiv.org/abs/2402.18502) | 本研究在公平性定义上引入了一个框架，探讨了LLM在公平性考虑下的生成公平结果的方法，包含对上下文学习配置和演示选择的探索。 |
| [^7] | [Language Models Represent Beliefs of Self and Others](https://arxiv.org/abs/2402.18496) | 通过神经激活线性解析语言模型中代理人观点下的信念状态，揭示了大型语言模型内部表述自我和他人信念，这对社会推理过程至关重要，并在多样社会推理任务中具有潜在的泛化能力。 |
| [^8] | [NewsQs: Multi-Source Question Generation for the Inquiring Mind](https://arxiv.org/abs/2402.18479) | NewsQs是一个为多个新闻文档提供问题-答案对的数据集，利用T5-Large模型fine-tune自动生成问题，通过对模型进行控制码微调，可以产生更多可接受的问题，释放高质量的数据集供未来基于查询的多文档总结研究使用。 |
| [^9] | [Meta-Task Prompting Elicits Embedding from Large Language Models](https://arxiv.org/abs/2402.18458) | 提出了一种新的无监督嵌入方法MetaEOL，通过元任务提示引导大型语言模型生成高质量句子嵌入，无需模型微调或特定任务工程，实验表明其在语义文本相似性测试和下游任务中表现出色 |
| [^10] | [HOP to the Next Tasks and Domains for Continual Learning in NLP](https://arxiv.org/abs/2402.18449) | 该方法HOP在连续学习中引入了三个方向以在自然语言处理中跨任务和领域进行学习。 |
| [^11] | [Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication](https://arxiv.org/abs/2402.18439) | 挑战了默认使用自然语言的做法，通过探索LLMs在推理和沟通中使用替代格式的实用性，实现了推理效率的提升和多智能体通信时标记使用量的显著减少。 |
| [^12] | [Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation](https://arxiv.org/abs/2402.18428) | 提出了一种新颖的通用协作学习方法DCMCL，在神经机器翻译中利用多样化建模上下文，将自回归（AR）和非自回归（NAR）模型作为合作者。 |
| [^13] | [Emotion Classification in Low and Moderate Resource Languages](https://arxiv.org/abs/2402.18424) | 通过跨语言情感分类器，在低和中等资源语言中实现情感分类，展示了两种迁移学习方法的有效性。 |
| [^14] | [Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?](https://arxiv.org/abs/2402.18419) | 通过问答任务，GPT能够验证医疗领域患者的PA请求，帮助卫生计划更快地做出决策。 |
| [^15] | [A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models](https://arxiv.org/abs/2402.18409) | 提出了一个新颖的评估基准，用于评估大型视觉语言模型的认知能力，发现LVLMs与人类之间存在较大的认知能力差距。 |
| [^16] | [Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models](https://arxiv.org/abs/2402.18397) | 通过分解提示方法，这项研究揭示了以英语为中心的大型语言模型在多语言任务上的语言结构理解能力，证实其在零次和少次迭代设置中的高效性和效率。 |
| [^17] | [The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA](https://arxiv.org/abs/2402.18385) | 本文介绍了在WSDM Cup 2024中获胜的方法，利用大型语言模型（LLMs）的卓越自然语言理解和生成能力，通过改编LLMs、设计混合训练策略、采用先进的文本嵌入模型、以及模型集成等多种技术，最终在会话式多文档问答方面取得了第一名。 |
| [^18] | [Tokenization Is More Than Compression](https://arxiv.org/abs/2402.18376) | 通过引入新的分词器PathPiece，研究者发现少量标记并不能导致更好的下游性能，这一结果对于 Tokenization 的有效性理解提出了质疑。 |
| [^19] | [VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models](https://arxiv.org/abs/2402.18374) | VerifiNER是一个后续验证框架，通过知识识别并修正现有命名实体识别方法的错误，以实现更忠实的预测。 |
| [^20] | [Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning](https://arxiv.org/abs/2402.18344) | 大型语言模型在常识推理中表现出高水平的能力，但由于信息丢失问题，提出了新方法RIDERS来解释和减轻有害CoT问题 |
| [^21] | [Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation](https://arxiv.org/abs/2402.18334) | Bonito是一种用于生成指令调优训练数据集的模型，通过将未注释的文本转换为特定任务训练数据，实现大型语言模型对用户专属数据的零shot任务适应，并显著提高了预训练和指令调整模型的平均性能。 |
| [^22] | [How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning](https://arxiv.org/abs/2402.18312) | 通过对LLMs进行机械性研究，我们发现模型在进行逐步推理时使用多个并行路径生成答案，同时存在功能性分歧。 |
| [^23] | [Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization](https://arxiv.org/abs/2402.18284) | 提出了一种自监督文本排序方法，利用近端策略优化对语言模型进行微调，消除了对人工注释员的需求，实验结果表明该方法训练的模型在各项得分方面明显优于基线 |
| [^24] | [Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient](https://arxiv.org/abs/2402.18281) | 通过研究梯度，将四种有效的对比损失集成到一个统一的范式中，以探究对比句子表示学习中各种对比损失达到卓越性能的共同特点。 |
| [^25] | [Exploration of Adapter for Noise Robust Automatic Speech Recognition](https://arxiv.org/abs/2402.18275) | 本文研究了适配器用于噪声鲁棒自动语音识别的探索，发现将适配器插入浅层可以获得更显著的效果，真实数据比模拟数据更有效，并且将适配器集成到基于语音增强的ASR系统中可以带来实质性改进。 |
| [^26] | [Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?](https://arxiv.org/abs/2402.18272) | 多智能体讨论提升了LLM的推理能力，但在有演示的情况下，单智能体LLM通过强提示几乎能达到与最佳讨论方法相同的性能。 |
| [^27] | [A Survey on Neural Question Generation: Methods, Applications, and Prospects](https://arxiv.org/abs/2402.18267) | 这项调查系统研究了神经问答生成（NQG）领域的进展，包括了背景概述、不同类别的方法、以及未来展望 |
| [^28] | [Retrieval-based Full-length Wikipedia Generation for Emergent Events](https://arxiv.org/abs/2402.18264) | 通过检索获取的Web来源信息，为新兴事件生成结构化的全长维基百科文档，避免大型语言模型在与最近发生事件相关的语料库上进行训练。 |
| [^29] | [Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding](https://arxiv.org/abs/2402.18262) | 提出了用于视觉丰富网页理解的分层多模态预训练方法，结合文本、结构和图像模态的交互来增强对网页的理解 |
| [^30] | [A BiRGAT Model for Multi-intent Spoken Language Understanding with Hierarchical Semantic Frames](https://arxiv.org/abs/2402.18258) | 提出了一种具有层次语义框架的多意图口语理解BiRGAT模型，在实验中通过多意图数据集MIVS，采用3层层次结构来解决多意图情况下的对齐和分配问题，并结合3路指针-生成器解码器，显著优于传统方案。 |
| [^31] | [Towards Generalist Prompting for Large Language Models by Mental Models](https://arxiv.org/abs/2402.18252) | 通过提出通用提示概念和创新的MeMo（心智模型）方法，实现大型语言模型在广泛任务范围内实现最佳性能，消除了手动选择和定制提示的需求。 |
| [^32] | [Learning or Self-aligning? Rethinking Instruction Fine-tuning](https://arxiv.org/abs/2402.18243) | 本研究揭示了指导微调的潜在机制，发现尝试通过指导微调学习额外世界知识往往难以产生积极影响，重点在于保持内部知识一致性。 |
| [^33] | [Prospect Personalized Recommendation on Large Language Model-based Agent Platform](https://arxiv.org/abs/2402.18240) | 提出了一种基于大型语言模型代理平台的个性化推荐系统Rec4Agentverse，强调代理项和代理推荐器之间的合作，促进个性化信息服务，提升信息交换，并展望了其演进为支持互动和信息交换的三个阶段 |
| [^34] | [CogBench: a large language model walks into a psychology lab](https://arxiv.org/abs/2402.18225) | CogBench提出了一个从七个认知心理学实验中衍生出十个行为指标的基准测试，为评估大型语言模型的行为提供了工具，研究发现模型大小和从人类反馈中学习的强化学习对性能改善和与人类行为一致具有重要作用。 |
| [^35] | [Improving Open-Ended Text Generation via Adaptive Decoding](https://arxiv.org/abs/2402.18223) | 引入自适应解码机制，通过置信度动态确定生成过程中的候选集，在故事生成任务中实现了更高的MAUVE和多样性，保持一定的连贯性，优于现有算法。 |
| [^36] | [LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History](https://arxiv.org/abs/2402.18216) | 本研究初步探讨了对话中任务切换对LLM模型干扰的影响，发现任务切换可能导致性能下降。 |
| [^37] | [DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity Recognition](https://arxiv.org/abs/2402.18209) | 丹麦NLP中命名实体识别领域的一个重要贡献是引入了DANSK数据集和DaCy 2.6.0模型，以解决现有模型在细粒度命名实体识别和跨领域泛化方面的限制。 |
| [^38] | [Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation](https://arxiv.org/abs/2402.18191) | 本文提出了一种聚类与排序方法（CaR），通过与专家偏好相一致的评分模型排名指令对，保留了数据集的多样性。 |
| [^39] | [Challenges in Pre-Training Graph Neural Networks for Context-Based Fake News Detection: An Evaluation of Current Strategies and Resource Limitations](https://arxiv.org/abs/2402.18179) | 将神经网络预训练与图神经网络在基于上下文的假新闻检测领域相结合，评估了不同预训练策略，并指出目前转移学习并未显著改进模型性能，主要问题在于缺乏大规模资源。 |
| [^40] | [MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery](https://arxiv.org/abs/2402.18169) | 提出了一种基于大型语言模型的MIKO框架，利用多模态和文本的协同作用揭示社交媒体用户的意图。 |
| [^41] | [Evaluating Quantized Large Language Models](https://arxiv.org/abs/2402.18158) | 该论文通过全面评估后训练量化对权重、激活和KV缓存的影响，以指导选择量化方法，并对11种模型系列进行了评估，覆盖了多种任务类型。 |
| [^42] | [From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs](https://arxiv.org/abs/2402.18157) | 本研究引入了一个新颖的工具调用管道，旨在控制大规模现实世界API，从而增强大型语言模型在复杂任务中的应用能力。 |
| [^43] | [Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models](https://arxiv.org/abs/2402.18154) | 通过信息流的视角解释并干预语言模型中的知识冲突，提出了一种名为Pruning Head via PatH的方法来缓解冲突 |
| [^44] | [Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation](https://arxiv.org/abs/2402.18150) | 本文提出了一种名为InFO-RAG的无监督信息细化训练方法，将大型语言模型在检索增强生成中的角色定义为“信息细化者”，帮助模型更好地整合检索信息以生成更加简洁、准确和完整的文本。 |
| [^45] | [Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis](https://arxiv.org/abs/2402.18145) | 提出了基于信息瓶颈梯度（IBG）解释框架，通过将单词嵌入调整为简明的内在维度来提高基于方面的情感分析模型的性能和可解释性。 |
| [^46] | [Cause and Effect: Can Large Language Models Truly Understand Causality?](https://arxiv.org/abs/2402.18139) | 本研究提出了一种名为CARE CA的新型架构，通过结合显式因果检测模块和反事实陈述、以及隐含因果检测模块，旨在增强大型语言模型对因果关系的理解能力。 |
| [^47] | [DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning](https://arxiv.org/abs/2402.18137) | 本文提出了 DecisionNCE 框架，通过隐式偏好学习实体多模态表示，实现了提取任务进展信息和与语言指令对齐的有效方法 |
| [^48] | [Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian](https://arxiv.org/abs/2402.18121) | 本研究评估了四种语言模型在未被充分探索的氨基酸语言中的适应性、有效性和局限性，并为未来自然语言处理的进展奠定了基础。 |
| [^49] | [Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?](https://arxiv.org/abs/2402.18120) | 通过探索多语言人类价值观念，我们证实了大型语言模型中存在多语言人类价值，并揭示了价值对齐能力可以被跨语言控制。 |
| [^50] | [UniVS: Unified and Universal Video Segmentation with Prompts as Queries](https://arxiv.org/abs/2402.18115) | UniVS提出了使用提示作为查询的统一视频分割架构，通过平均化前一帧中目标的提示特征来解码掩码，并引入了基于目标的提示交叉注意层，从而解决了统一视频分割模型的挑战。 |
| [^51] | [Small But Funny: A Feedback-Driven Approach to Humor Distillation](https://arxiv.org/abs/2402.18113) | 通过将大型语言模型分配为“教师”生成数据和“评论家”评估学生表现的双重角色，研究表明这种基于反馈的方法在幽默生成任务中取得了更高的性能。 |
| [^52] | [Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context](https://arxiv.org/abs/2402.18101) | 评估了一种日本背景下人类评估方法中最先进语法错误检测和校正模型的性能，在错误校正方面表现出较高的准确性和保守性，在错误检测方面表现出高精确度和调整召回率。 |
| [^53] | [Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models](https://arxiv.org/abs/2402.18099) | 提出了一种新型的Layer-wise Scalable Adapter策略MedLaSA，用于编辑医学大型语言模型，能精确修改医学知识并解释事实，解决了当前方法在医学知识特殊化和复杂性方面的困难。 |
| [^54] | [Polos: Multimodal Metric Learning from Human Feedback for Image Captioning](https://arxiv.org/abs/2402.18091) | 提出了一种使用多模态输入和基于人类反馈的框架训练的自动评估指标Polos，旨在有效开发图像字幕生成模型。 |
| [^55] | [On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction](https://arxiv.org/abs/2402.18061) | 本研究提出了一个新框架Clean-LaVe，旨在利用银标准数据来增强零样本分类性能。 |
| [^56] | [Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions](https://arxiv.org/abs/2402.18060) | 在回答医学问题方面，大型语言模型在处理具有挑战性的实际临床案例上的表现是关键，因此构建了两个结构化数据集进行评估。 |
| [^57] | [Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models](https://arxiv.org/abs/2402.18059) | 提出一种利用多目标优化方法的水印技术，通过轻量级网络生成特定令牌水印logits和分割比率，在保证检测性的同时提升了文本的语义完整性。 |
| [^58] | [Contextualizing Generated Citation Texts](https://arxiv.org/abs/2402.18054) | 生成引文文本时，不仅考虑引文本身，还要考虑整个上下文窗口，包括目标引文，以解决生成的引文忽视引文上下文焦点的问题。 |
| [^59] | [MEGAnno+: A Human-LLM Collaborative Annotation System](https://arxiv.org/abs/2402.18050) | MEGAnno+提供了一个人类-LLM协作标注系统，旨在通过人类和大语言模型（LLMs）的联合工作产生可靠且高质量的标签。 |
| [^60] | [Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension](https://arxiv.org/abs/2402.18048) | 本研究旨在通过使用局部内在维度来量化大型语言模型生成文本的真实性，实验证实了该方法的有效性。 |
| [^61] | [Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore](https://arxiv.org/abs/2402.18045) | 本文系统评估了多语言LLMs跨语言和地理区域的事实准确性，并发现英语在事实准确性和生成事实数量方面优于其他语言，同时多语言模型存在对来自西方大陆事实信息的偏见。 |
| [^62] | [Crisis talk: analysis of the public debate around the energy crisis and cost of living](https://arxiv.org/abs/2402.18043) | 本文通过分析能源危机和生活成本的公共辩论，识别了关键议题、新趋势、关键社会行为者以及他们在辩论中的角色，以及与这些议题和行为者相关的情绪。 |
| [^63] | [Datasets for Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2402.18041) | 本文全面探讨了大型语言模型数据集的不同类型、挑战和未来发展方向。 |
| [^64] | [ResLoRA: Identity Residual Mapping in Low-Rank Adaption](https://arxiv.org/abs/2402.18039) | ResLoRA提出了在训练中添加残余路径并在推断过程中消除这些额外路径的方法，实现了更好的结果，比LoRA更加高效。 |
| [^65] | [Corpus-Steered Query Expansion with Large Language Models](https://arxiv.org/abs/2402.18031) | 通过引入语料库引导的查询扩展（CSQE），结合大型语言模型的知识增强扩展，改善了查询与目标文档之间的相关性预测。 |
| [^66] | [Do Large Language Models Mirror Cognitive Language Processing?](https://arxiv.org/abs/2402.18023) | 本文提出了一种新颖方法，通过将大型语言模型（LLMs）的表示与人类认知信号联系起来，评估LLMs模拟认知语言处理的效果。 |
| [^67] | [A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems](https://arxiv.org/abs/2402.18013) | 这项调查综述了基于LLM的多轮对话系统的研究，并重点介绍了LLMs的应用和最新进展，对开放领域对话和任务导向对话系统进行了涵盖，并讨论了相关数据集和评估指标，以及未来研究方向和问题。 |
| [^68] | [Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization](https://arxiv.org/abs/2402.18005) | 通过人类元审阅者的情感整合框架，提出评估指标并在实验中验证，指导LLMs生成科学元审阅的逻辑被验证可行。 |
| [^69] | [FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization](https://arxiv.org/abs/2402.17985) | FlattenQuant方法通过展平张量中的大通道，实现了低比特每张量量化，降低了准确性损失 |
| [^70] | [M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding](https://arxiv.org/abs/2402.17983) | 这个模型是一个多模态、多任务、多教师的联合细粒度知识蒸馏模型，通过微妙协作令牌和实体表示，处理复杂的表单文档，引入新的损失函数改进知识蒸馏过程，在处理视觉复杂表单文档的结构和内容上表现出色。 |
| [^71] | [Collaborative decoding of critical tokens for boosting factuality of large language models](https://arxiv.org/abs/2402.17982) | 引入协同解码框架通过关键标记概念利用预训练模型中的高事实性，设计关键标记分类器进行模型选择，有效降低幻觉生成。 |
| [^72] | [All in a Single Image: Large Multimodal Models are In-Image Learners](https://arxiv.org/abs/2402.17971) | 这项研究引入了一种名为图片内学习（I$^2$L）的新型上下文学习机制，将演示示例、视觉线索和指令合并到一个图片中，以提升GPT-4V的能力，并通过整合图像处理、理解和推理的能力来取得多个优点 |
| [^73] | [An Iterative Associative Memory Model for Empathetic Response Generation](https://arxiv.org/abs/2402.17959) | 提出了一种用于共情响应生成的迭代关联记忆模型，采用二阶交互注意机制迭代地捕捉相关词语，实现准确、细致地理解话语。 |
| [^74] | [Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps](https://arxiv.org/abs/2402.17954) | 多语言语音模型在自动语音识别中表现出性别差距，且在不同语言中受益的性别群体各不相同。 |
| [^75] | [Gradient-Free Adaptive Global Pruning for Pre-trained Language Models](https://arxiv.org/abs/2402.17946) | 提出了自适应全局剪枝（AdaGP）框架，通过重新定义全局剪枝过程为可管理的协调子问题，实现对大型语言模型的资源高效优化，显著提高性能。 |
| [^76] | [Large Language Models on Tabular Data -- A Survey](https://arxiv.org/abs/2402.17944) | 该研究综述了大型语言模型在处理表格数据上的应用，包括关键技术、指标、数据集、模型和优化方法，为未来研究方向提供了启示。 |
| [^77] | [EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models](https://arxiv.org/abs/2402.17938) | EmMark是一种新型水印框架，用于保护嵌入式大型语言模型的知识产权，在鲁棒性和模型质量维持之间实现平衡，成功应对了IP盗窃风险，具有100%的水印提取成功率，并抵御了水印删除和伪造攻击。 |
| [^78] | [Acquiring Linguistic Knowledge from Multimodal Input](https://arxiv.org/abs/2402.17936) | 语言模型在获取语言知识时表现出较低的数据效率，研究发现这部分原因可能是由于缺乏多模态输入和接地。他们通过对FLAVA模型进行消融研究来验证假设，并试图通过多任务预训练来减少灾难性遗忘。 |
| [^79] | [Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures](https://arxiv.org/abs/2402.17934) | 提出了一种名为FLix的新型参数高效微调方法，适用于多任务多语言调整，通过关联每个独特数据集特征与其低秩权重更新参数，实现了更好的泛化能力和性能表现。 |
| [^80] | [Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning](https://arxiv.org/abs/2402.17930) | 本文介绍了一种名为合作语言引导逆向计划搜索（CLIPS）的贝叶斯代理架构，用于实现实用指令跟随和目标辅助，能够通过多模态贝叶斯推断，利用大型语言模型评估指令的可能性以实现实用目标达成成本最小化。 |
| [^81] | [LLM-Resistant Math Word Problem Generation via Adversarial Attacks](https://arxiv.org/abs/2402.17916) | 本研究提出了一种新方法，通过生成保留原问题结构难度但针对LLMs无解的对抗性示例，有效地降低了LLMs的数学问题解决能力。 |
| [^82] | [Extracting Lexical Features from Dialects via Interpretable Dialect Classifiers](https://arxiv.org/abs/2402.17914) | 通过可解释的方言分类器提取方言的词汇特征，成功识别了有助于方言变化的关键语言特定词汇特征。 |
| [^83] | [A Language Model based Framework for New Concept Placement in Ontologies](https://arxiv.org/abs/2402.17897) | 提出了一种基于语言模型的框架，用于将从文本中提取的新概念插入到本体中，在边搜索、边形成和增强、边选择三个步骤中分别利用神经方法，并在 SNOMED CT 本体和 MedMentions 实体链接基准上进行了评估 |
| [^84] | [Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents](https://arxiv.org/abs/2402.17896) | 提出了一个研究性问题数据集，其中包含非事实型、多透视的问题，能够挑战目前大型语言模型的表现。 |
| [^85] | [JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability](https://arxiv.org/abs/2402.17887) | JMLR通过联合训练信息检索系统和大型语言模型，在医学领域提高问题回答系统性能，降低计算资源需求，增强模型利用医疗知识进行推理和回答问题的能力。 |
| [^86] | [BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra](https://arxiv.org/abs/2402.17882) | BlendSQL是一个超集的SQLite，用于统一混合问题回答中的非结构化和结构化数据，尤其适用于包含多跳推理的任务，并且在使用更少令牌的情况下能够提高系统性能。 |
| [^87] | [Automated Statistical Model Discovery with Language Models](https://arxiv.org/abs/2402.17879) | 利用大型语言模型，提出了一种基于语言模型驱动的自动统计模型发现方法，不再需要定义特定领域模型语言或设计手工搜索程序。 |
| [^88] | [Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2402.17840) | 研究揭示了检索增强生成系统中的数据泄露风险，指出对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据，并设计了攻击对生产RAG模型GPTs造成数据存储泄漏。 |
| [^89] | [Stable LM 2 1.6B Technical Report](https://arxiv.org/abs/2402.17834) | 稳定LM 2 1.6B是语言模型系列中的新一代产品，在该报告中详细介绍了其数据、训练过程和性能评估，该模型在发布时是2B参数范围内最先进的开源模型之一，并提供了下载链接和性能对比数据。 |
| [^90] | [Prediction-Powered Ranking of Large Language Models](https://arxiv.org/abs/2402.17826) | 该研究提出了一种统计框架，可以衡量人类与模型偏好之间的不确定性，从而进行大型语言模型的预测排名。 |
| [^91] | [DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation](https://arxiv.org/abs/2402.17812) | DropBP提出了一种新颖的方式来加速大型语言模型的微调，通过在反向传播过程中随机丢弃层以减少计算成本同时保持准确性。 |
| [^92] | [TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space](https://arxiv.org/abs/2402.17811) | 本文提出了一种名为TruthX的方法，通过在真实空间中编辑大型语言模型的内部表示，有效提高了语言模型的真实性，实验证明在TruthfulQA基准测试中，TruthX平均提高了13种先进语言模型的真实性。 |
| [^93] | [A Surprising Failure? Multimodal LLMs and the NLVR Challenge](https://arxiv.org/abs/2402.17793) | 这项研究评估了多模LLMs在自然语言视觉推理任务NLVR上的性能表现，发现它们在需要组合和空间推理、对语义和系统性偏见具有鲁棒性的任务上表现不佳。 |
| [^94] | [Stepwise Self-Consistent Mathematical Reasoning with Large Language Models](https://arxiv.org/abs/2402.17786) | 提出了一种名为SSC-CoT的算法，通过选择中间步骤的策略和查询知识图来解决大型语言模型进行复杂数学推理时面临的挑战 |
| [^95] | [OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web](https://arxiv.org/abs/2402.17553) | OmniACT是一个针对代理生成可执行程序完成计算机任务能力的数据集和基准，超越了传统Web自动化，涵盖了各种桌面应用。 |
| [^96] | [Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?](https://arxiv.org/abs/2402.17493) | 通过评估临床大型语言模型在术后风险预测中的应用，研究探讨了使用不同训练策略的模型在围手术期护理中的潜在效果。 |
| [^97] | [Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder](https://arxiv.org/abs/2402.17433) | 通过Contrastive EEG-Text Masked Autoencoder（CET-MAE）和E2T-PTR框架，提出了一种新的模型和方法来增强基于EEG的语言解码。 |
| [^98] | [Dealing with Data for RE: Mitigating Challenges using NLP and Generative AI](https://arxiv.org/abs/2402.16977) | 数据在现代软件系统中扮演着不可或缺的角色，监管环境的变化、软件个性化需求的增长以及对治理的强调推动着大型企业采用自动化技术，并在AI为中心的系统中不断引入新的挑战和需求。 |
| [^99] | [If in a Crowdsourced Data Annotation Pipeline, a GPT-4](https://arxiv.org/abs/2402.16795) | 本文比较了 GPT-4 和 MTurk 管道的数据标注准确性，发现尽管 MTurk 采用了最佳实践，但 GPT-4 的准确率更高，并且结合 GPT-4 和众包标签使用聚合算法可以提高准确率。 |
| [^100] | [Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models](https://arxiv.org/abs/2402.16696) | 提出了一种决策感知和可泛化的工具使用框架，以帮助大型语言模型在操作工具时提高灵活性和泛化能力 |
| [^101] | [StructLM: Towards Building Generalist Models for Structured Knowledge Grounding](https://arxiv.org/abs/2402.16671) | StructLM系列模型基于全面指令调整数据集，超越了大多数任务特定模型，在结构化知识连接任务中取得了新的最先进成就。 |
| [^102] | [Aligning Large Language Models to a Domain-specific Graph Database](https://arxiv.org/abs/2402.16567) | 该论文提出了一种将大型语言模型对齐到特定领域的图数据库的方法，通过利用ChatGPT生成NL-GQL数据对并微调LLMs，实现了两者之间的对齐。 |
| [^103] | [LLM Inference Unveiled: Survey and Roofline Model Insights](https://arxiv.org/abs/2402.16363) | 本文提出了一个基于Roofline模型的框架，用于系统分析LLM推断技术，帮助识别部署中的瓶颈，并为更有效地部署LLM提供策略。 |
| [^104] | [A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction](https://arxiv.org/abs/2402.16278) | 提出了一种自匹配训练方法，通过两种本体嵌入模型捕获全局和局部信息，提高了概念子类预测的稳健性 |
| [^105] | [UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval](https://arxiv.org/abs/2402.16261) | 提出了一种UniRetriever框架，利用双编码器架构和两个损失约束实现了多任务候选者选择，适用于不同情境下的对话检索任务。 |
| [^106] | [MATHWELL: Generating Educational Math Word Problems at Scale](https://arxiv.org/abs/2402.15861) | 使用MATHWELL模型生成了迄今为止最大的英文数学应用题数据集，其中包含20,490个问题，经领域专家评分结果显示，MATHWELL的问题中具有可执行解决方案并符合所有标准的份额比其他选择高出40%，其中74%的可解决问题同时做到了准确和适当。 |
| [^107] | [Large Scale Generative AI Text Applied to Sports and Music](https://arxiv.org/abs/2402.15514) | 这项工作利用生成式人工智能模型将大规模多模数据转化为连贯流畅文本，首次推出了用于体育和音乐领域的AI评论系统，并取得了显著性能提升。 |
| [^108] | [Advancing Parameter Efficiency in Fine-tuning via Representation Editing](https://arxiv.org/abs/2402.15179) | RED通过表示编辑显著降低了可训练参数数量，实现了与完全参数微调和其他PEFT方法相当或更好的结果 |
| [^109] | [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](https://arxiv.org/abs/2402.14968) | 提出了一种通过后门增强对齐方法有效缓解微调越狱攻击，避免需要大量安全示例的低效问题。 |
| [^110] | [Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic](https://arxiv.org/abs/2402.14798) | 本文提出了一种一致且在理论上有根据的方法来注释分解蕴涵数据集，形成RDTE数据集，该数据集在解决何为有效的组合蕴涵的问题上有显著进展。 |
| [^111] | [OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement](https://arxiv.org/abs/2402.14658) | OpenCodeInterpreter是一种开源代码系统，集成了执行、人类反馈和动态代码细化的功能，并在关键基准测试中表现出色，甚至与GPT-4相媲美。 |
| [^112] | [Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation](https://arxiv.org/abs/2402.11894) | 本文提出自动化数据集更新策略，利用两种系统验证过的策略生成看不见和高质量的测试样本，以缓解数据泄漏问题并实现评估稳定性。 |
| [^113] | [Exploring Precision and Recall to assess the quality and diversity of LLMs](https://arxiv.org/abs/2402.10693) | 该研究提出了一种新的评估框架，将精度和召回率指标从图像生成转化为文本生成，细致评估了LLMs生成文本的质量和多样性，揭示了当前LLMs在生成任务中性能表现的重要见解。 |
| [^114] | [BitDelta: Your Fine-Tune May Only Be Worth One Bit](https://arxiv.org/abs/2402.10193) | BitDelta研究探讨了大型语言模型在微调过程中的信息冗余性，并提出了一种名为BitDelta的方法，可以将微调过程中添加的信息量化为一个比特，同时保持性能。这一发现对于多租户模型的服务和存储有重要意义，并可以显著降低GPU内存需求。 |
| [^115] | [A Closer Look at the Limitations of Instruction Tuning](https://arxiv.org/abs/2402.05119) | 本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。 |
| [^116] | [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838) | OLMo是一种真正开放的语言模型，旨在提供给研究社区强大的工具，以促进对语言模型科学的研究和创新。 |
| [^117] | [Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension](https://arxiv.org/abs/2312.10126) | 该论文介绍了一个人类评估框架，通过阅读理解问题来评估简化的文本是否保留了含义。 |
| [^118] | [Prompt Optimization via Adversarial In-Context Learning](https://arxiv.org/abs/2312.02614) | 提出了Adversarial In-Context Learning (adv-ICL)方法，通过生成器、鉴别器和提示修改器之间的对抗学习优化提示，在上下文学习中取得显着改进。 |
| [^119] | [ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions](https://arxiv.org/abs/2312.01661) | ChatGPT在生成预大学数学问题上的表现进行了评估，为填补现有教育问题生成模型的不足提供了新思路。 |
| [^120] | [BLT: Can Large Language Models Handle Basic Legal Text?](https://arxiv.org/abs/2311.09693) | 大型语言模型在处理基础法律文本方面表现不佳，但通过针对性微调，甚至较小的模型也能在测试中表现出色，提升了相关法律任务的表现。 |
| [^121] | [CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models](https://arxiv.org/abs/2311.09154) | Clean-Eval提出了一种清洁评估方法，通过LLM对污染数据进行释义和反向翻译，利用语义检测器过滤低质量样本，最终选择最佳候选，解决了大型语言模型评估中的数据污染问题。 |
| [^122] | [ChOiRe: Characterizing and Predicting Human Opinions with Chain of Opinion Reasoning](https://arxiv.org/abs/2311.08385) | ChOiRe是一个通过观点链推理表征和预测人类观点的框架，结合用户明确和隐式的个人角色特征，实现了对人类观点的预测。 |
| [^123] | [In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search](https://arxiv.org/abs/2311.07237) | 该研究提出了一个名为LINK的框架，能够系统性地生成长尾推理知识，从而更有效地评估LLMs在推理空间中的表现。 |
| [^124] | [Multilingual Jailbreak Challenges in Large Language Models](https://arxiv.org/abs/2310.06474) | 该研究揭示了大语言模型中存在的多语言越狱挑战，包括用户使用非英语提示绕过安全机制的非故意场景和恶意用户利用多语言提示恶意攻击LLMs的故意场景。 |
| [^125] | [Ask Again, Then Fail: Large Language Models' Vacillations in Judgement](https://arxiv.org/abs/2310.02174) | 目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。 |
| [^126] | [MiniLLM: Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2306.08543) | 本文提出了一种将大型语言模型的知识蒸馏到更小模型的方法，通过使用反向KLD替换标准KD方法中的前向KLD目标，有效避免了学生模型高估教师分布的低概率区域。 |
| [^127] | [Replacing Language Model for Style Transfer](https://arxiv.org/abs/2211.07343) | 提出了一种替换语言模型（RLM），结合了自回归模型的灵活性和非自回归模型的准确性，在文本风格转换中实现了更精确的生成控制。 |
| [^128] | [RealTime QA: What's the Answer Right Now?](https://arxiv.org/abs/2207.13332) | 该论文介绍了REALTIME QA，一个动态问答平台，挑战静态开放领域QA数据集假设，强调对实时信息的重要性，并展示了基于GPT-3的实时评估结果。 |
| [^129] | [A Call for Clarity in Beam Search: How It Works and When It Stops](https://arxiv.org/abs/2204.05424) | 提出了一个简单修改的耐心因子来改善束搜索解码算法，提高了强预训练模型在文本摘要和机器翻译任务上的性能。 |
| [^130] | [LegalDuet: Learning Effective Representations for Legal Judgment Prediction through a Dual-View Legal Clue Reasoning.](http://arxiv.org/abs/2401.15371) | LegalDuet是一种通过双视角法律线索推理模型，使用预训练语言模型学习定制嵌入空间来进行法律判决预测。该模型通过法律案例推理和法律基础推理两个推理链进行判决。在实验中，LegalDuet在CAIL2018数据集上表现出最先进的性能，并超过了基线模型。 |
| [^131] | [BIBench: Benchmarking Data Analysis Knowledge of Large Language Models.](http://arxiv.org/abs/2401.02982) | BIBench是一个旨在评估大型语言模型（LLMs）在商业智能（BI）数据分析领域中能力的综合基准测试，其通过测试模型在BI基础知识、应用知识和技术技能三个维度上的表现来进行评估。 |
| [^132] | [Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning.](http://arxiv.org/abs/2312.17267) | 提出了一种名为MVRE的新方法，通过将关系解耦为不同的视角，生成多视角关系表示，并利用预训练语言模型（PLMs）的能力来提高低资源关系抽取任务的性能。 |
| [^133] | [InstructCoder: Empowering Language Models for Code Editing.](http://arxiv.org/abs/2310.20329) | 本研究旨在探索使用大型语言模型（LLMs）进行代码编辑，并引入了InstructCoder数据集，该数据集包含多样性的代码编辑任务，为通用代码编辑提供支持。 |
| [^134] | [MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention.](http://arxiv.org/abs/2309.16639) | MindShift利用大型语言模型实现了基于心态的问题性智能手机使用干预，通过动态生成适应用户环境和心理状态的高质量说服内容来帮助用户解决问题性智能手机使用的困扰。 |
| [^135] | [Cure the headache of Transformers via Collinear Constrained Attention.](http://arxiv.org/abs/2309.08646) | 通过引入共线约束注意力（CoCA）结构，解决Transformer模型中的头痛问题，实现了出色的外推性能和提高的计算效率。 |
| [^136] | [A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models.](http://arxiv.org/abs/2309.02691) | 这项研究提出了一个框架来研究视觉和语言模型中短语定位和任务性能之间的关系，并且通过验证实验发现了当代模型在短语定位和任务求解方面的不一致性。 |
| [^137] | [Chinese Spelling Correction as Rephrasing Language Model.](http://arxiv.org/abs/2308.08796) | 本文提出了一种新颖的中文拼写纠错方法，通过改写语言建模来重新表达整个句子，而不是仅仅依赖错误模式进行字符级别标注，取得了最新的最优结果。 |
| [^138] | [Towards Understanding What Code Language Models Learned.](http://arxiv.org/abs/2306.11943) | 本研究探究了预先训练的代码语言模型的能力，证明其能够超越表面形式特征，学习精确而形式化定义的代码的计算语义。 |
| [^139] | [Questioning the Survey Responses of Large Language Models.](http://arxiv.org/abs/2306.07951) | 本文使用美国人口普查局建立的全美社区调查（ACS）评估了十几个不同大小的语言模型，发现小型模型具有显著的位置和标签偏差，而模型大小的增加能减轻这种偏差，但无法根据US群体或任何可识别的群体趋势进行调整。 |
| [^140] | [Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders.](http://arxiv.org/abs/2305.10734) | 本文提出了一种集成生成和预测信息的基于扩散的语音增强系统，其中两个语音增强模块在第一和最后一个扩散步骤中被融合，实验结果表明扩散评分估计可以从预测信息中受益并加快解码过程。 |
| [^141] | [PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization.](http://arxiv.org/abs/2305.06647) | 提出了一种新的短语级复制机制-PROM，可以增强对n-gram的注意力，用于具有预训练的零样本摘要生成，大大提高了摘要的质量和可信度。 |

# 详细

[^1]: 用于满足多样用户偏好的算术控制LLMs：具有多目标奖励的方向偏好对齐

    Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards

    [https://arxiv.org/abs/2402.18571](https://arxiv.org/abs/2402.18571)

    提出了方向偏好对齐（DPA）框架，通过多目标奖励模拟不同偏好配置，以实现用户相关的偏好控制。

    

    针对大型语言模型（LLMs）的精细控制仍然是一个重要挑战，阻碍了它们适应各种用户需求。本文提出了方向偏好对齐（DPA）框架，通过多目标奖励建模来表示多样化的偏好配置，将用户偏好建模为奖励空间中的方向（即单位向量）以实现用户相关的偏好控制。

    arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
    
[^2]: 用语言模型接近人类水平的预测能力

    Approaching Human-Level Forecasting with Language Models

    [https://arxiv.org/abs/2402.18563](https://arxiv.org/abs/2402.18563)

    该研究探讨了使用语言模型（LMs）进行预测未来事件的能力，开发了一种检索增强型LM系统，通过在竞争性预测平台收集数据集，并在知识截止日期后评估系统性能，发现该系统能够准确预测未来事件并在某些情况下超越人类预测者。

    

    预测未来事件对政策和决策制定至关重要。本研究探讨了语言模型(LMs)是否能够在竞争性人类预测者的水平上进行预测。为实现这一目标，我们开发了一种检索增强型LM系统，旨在自动搜索相关信息、生成预测和聚合预测。为了促进研究，我们收集了来自竞争性预测平台的大量问题数据集。在LM的知识截止日期之后发布的测试集下，我们评估了我们系统的端到端性能与人类预测的聚合之间的比较。平均而言，该系统接近于竞争预测者的聚合，并在某些情况下超越了它。我们的工作表明，利用LM来预测未来可能会提供准确的大规模预测，并有助于为机构决策提供信息。

    arXiv:2402.18563v1 Announce Type: cross  Abstract: Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.
    
[^3]: 隐性偏见的下一个标记预测

    Implicit Bias of Next-Token Prediction

    [https://arxiv.org/abs/2402.18551](https://arxiv.org/abs/2402.18551)

    在基于梯度的优化器训练下的线性NTP模型中，确定了NTP可分离条件，并证明梯度下降能够实现其下界；同时证明了这些条件在过参数化时仍然成立。

    

    下一个标记预测（NTP）是训练大型语言模型的首选范式，它涉及预测序列中的下一个标记。与传统的独热分类不同，在NTP中，多个具有不同频率的标记在给定上下文后继。本文将NTP训练框架化为跨不同上下文的交叉熵最小化，每个上下文都与有限词汇表中的稀疏经验概率向量相关联。然后，它探讨了以下问题：当NTP训练损失达到其下界（熵）时，基于梯度的优化器是否会对具有特定结构的解决方案存在偏见？具体地，对于使用梯度下降（GD）训练的线性NTP模型，我们做出以下贡献：首先，我们确定了数据上的NTP可分离条件，在这些条件下，GD能够达到其下界。我们还证明了这些条件在过参数化时仍成立。

    arXiv:2402.18551v1 Announce Type: cross  Abstract: Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that th
    
[^4]: 在微调后保持LLMs的对齐性:提示模板的关键作用

    Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates

    [https://arxiv.org/abs/2402.18540](https://arxiv.org/abs/2402.18540)

    提出了“纯粹调优，安全测试”（PTST）原则，即在微调时不包含安全提示，但在测试时加入，可以显著减少LLMs中不安全行为的出现。

    

    公共LLMs，如Llama 2-Chat，推动了LLM研究的巨大活动。这些模型经历了对齐性训练，被认为是安全的。最近，齐等人（2023年）报告称，即使是良性的微调（例如，在看似安全的数据集上）也可能导致模型产生不安全的行为。本文介绍了减轻这种对齐性丢失的方法和最佳实践。通过对几个聊天模型（Meta的Llama 2-Chat，Mistral AI的Mistral 7B Instruct v0.2和OpenAI的GPT-3.5 Turbo）进行广泛实验，本文发现微调和推理过程中使用的提示模板在保持安全对齐性方面起着至关重要的作用，并提出了“纯粹调优，安全测试”（PTST）原则 - 在测试时不使用安全提示进行模型微调，但在测试时包含它。对GSM8K，ChatDoctor和OpenOrca进行的微调实验表明，PTST显着减少了不安全行为的增加，甚至几乎消除了它们。

    arXiv:2402.18540v1 Announce Type: cross  Abstract: Public LLMs such as the Llama 2-Chat have driven huge activity in LLM research. These models underwent alignment training and were considered safe. Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the "Pure Tuning, Safe Testing" (PTST) principle -- fine-tune models without a safety prompt, but include it at test time. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost elimin
    
[^5]: RNNs还不是Transformer：在上下文检索中的关键瓶颈

    RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval

    [https://arxiv.org/abs/2402.18510](https://arxiv.org/abs/2402.18510)

    本文研究了RNNs和Transformer在处理算法问题时的表现能力差距，发现RNNs存在关键瓶颈，即无法完美地从上下文中检索信息，导致无法像Transformer那样轻松解决需要这种能力的任务。

    

    本文探讨循环神经网络（RNNs）和Transformer在解决算法问题时的表示能力差距。我们重点关注RNNs是否能在处理长序列时，通过Chain-of-Thought (CoT)提示，与Transformer的性能相匹配。我们的理论分析显示CoT可以改进RNNs，但无法弥补与Transformer之间的差距。关键瓶颈在于RNNs无法完全从上下文中检索信息，即使经过CoT的增强：对于几个明确或隐式需要这种能力的任务，如联想召回和确定图是否为树，我们证明RNNs表达能力不足以解决这些任务，而Transformer可以轻松解决。相反，我们证明采用增强RNNs上下文检索能力的技术，包括

    arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
    
[^6]: 少样本公平性：揭示LLM在公平意识分类中的潜力

    Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification

    [https://arxiv.org/abs/2402.18502](https://arxiv.org/abs/2402.18502)

    本研究在公平性定义上引入了一个框架，探讨了LLM在公平性考虑下的生成公平结果的方法，包含对上下文学习配置和演示选择的探索。

    

    使用大型语言模型（LLM）在各种下游应用中进行分类是至关重要的，尤其对于缺乏对模型进行微调所需的专业知识和资源的小型公司而言。 LLM中的公平性有助于确保包容性，基于种族、性别等因素实现平等代表，并促进负责任的AI部署。随着LLM的使用日益普及，评估LLM在考虑公平性时能否产生公平的结果至关重要。 在这项研究中，我们引入了一个框架，概述了与各种公平定义对齐的公平法规，每个定义都由不同程度的抽象调节。 我们探讨了基于RAG的上下文学习配置和选择上下文演示的程序，并将公平规则纳入其中。对不同LLM进行的实验表明，GPT-4提供

    arXiv:2402.18502v1 Announce Type: new  Abstract: Employing Large Language Models (LLM) in various downstream applications such as classification is crucial, especially for smaller companies lacking the expertise and resources required for fine-tuning a model. Fairness in LLMs helps ensure inclusivity, equal representation based on factors such as race, gender and promotes responsible AI deployment. As the use of LLMs has become increasingly prevalent, it is essential to assess whether LLMs can generate fair outcomes when subjected to considerations of fairness. In this study, we introduce a framework outlining fairness regulations aligned with various fairness definitions, with each definition being modulated by varying degrees of abstraction. We explore the configuration for in-context learning and the procedure for selecting in-context demonstrations using RAG, while incorporating fairness rules into the process. Experiments conducted with different LLMs indicate that GPT-4 delivers 
    
[^7]: 语言模型表达自我和他人信念

    Language Models Represent Beliefs of Self and Others

    [https://arxiv.org/abs/2402.18496](https://arxiv.org/abs/2402.18496)

    通过神经激活线性解析语言模型中代理人观点下的信念状态，揭示了大型语言模型内部表述自我和他人信念，这对社会推理过程至关重要，并在多样社会推理任务中具有潜在的泛化能力。

    

    理解和归因心理状态，即心灵理论（ToM），被视为人类社会推理的基本能力。虽然大型语言模型（LLMs）似乎具有某些ToM能力，但这些能力背后的机制仍然令人费解。在本研究中，我们发现通过语言模型的神经激活线性解码各个代理人观点下的信念状态是可能的，这表明存在自我的内部表述和他人信念的表示。通过操纵这些表征，我们观察到模型的ToM性能发生显著变化，突显了其在社会推理过程中的关键作用。此外，我们的发现还延伸到涉及不同因果推理模式的多样社会推理任务，暗示了这些表征的潜在泛化能力。

    arXiv:2402.18496v1 Announce Type: new  Abstract: Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.
    
[^8]: NewsQs: 多源问题生成助力探知心灵

    NewsQs: Multi-Source Question Generation for the Inquiring Mind

    [https://arxiv.org/abs/2402.18479](https://arxiv.org/abs/2402.18479)

    NewsQs是一个为多个新闻文档提供问题-答案对的数据集，利用T5-Large模型fine-tune自动生成问题，通过对模型进行控制码微调，可以产生更多可接受的问题，释放高质量的数据集供未来基于查询的多文档总结研究使用。

    

    我们提出了 NewsQs（news-cues），这是一个为多个新闻文档提供问题-答案对的数据集。为了创建 NewsQs，我们利用一个经过 T5-Large 模型在 News On the Web 语料库中的 FAQ 样式新闻文章上进行微调的模型，自动生成问题来扩充传统的多文档总结数据集。我们展示了使用控制码对模型进行微调可以产生更多被人类评价为可接受的问题，这是通过人工评估所测得的。我们使用与人类注释高度相关的 QNLI 模型来过滤我们的数据。我们发布了作为未来基于查询的多文档总结工作资源的高质量问题、答案和文档聚类的最终数据集。

    arXiv:2402.18479v1 Announce Type: new  Abstract: We present NewsQs (news-cues), a dataset that provides question-answer pairs for multiple news documents. To create NewsQs, we augment a traditional multi-document summarization dataset with questions automatically generated by a T5-Large model fine-tuned on FAQ-style news articles from the News On the Web corpus. We show that fine-tuning a model with control codes produces questions that are judged acceptable more often than the same model without them as measured through human evaluation. We use a QNLI model with high correlation with human annotations to filter our data. We release our final dataset of high-quality questions, answers, and document clusters as a resource for future work in query-based multi-document summarization.
    
[^9]: 使用元任务提示从大型语言模型中引出嵌入

    Meta-Task Prompting Elicits Embedding from Large Language Models

    [https://arxiv.org/abs/2402.18458](https://arxiv.org/abs/2402.18458)

    提出了一种新的无监督嵌入方法MetaEOL，通过元任务提示引导大型语言模型生成高质量句子嵌入，无需模型微调或特定任务工程，实验表明其在语义文本相似性测试和下游任务中表现出色

    

    在这项工作中，我们引入了一种新的无监督嵌入方法，即带显式单词限制的元任务提示（MetaEOL），用于从大型语言模型（LLMs）中生成高质量的句子嵌入，无需对模型进行微调或特定任务的工程。通过利用元任务提示，MetaEOL引导LLMs通过一系列精心设计的提示生成嵌入，这些提示涵盖了多个表示方面。我们全面的实验表明，从各种元任务平均得到的嵌入在语义文本相似性（STS）基准测试中表现出竞争力，并在下游任务中表现卓越，超越了对比训练模型。我们的发现提出了一种嵌入生成的新的扩展定律，为跨多种以句子为中心的场景中的嵌入提取提供了一种多才多艺、资源高效的方法。

    arXiv:2402.18458v1 Announce Type: new  Abstract: In this work, we introduce a new unsupervised embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse sentence-centric scenarios.
    
[^10]: HOP到自然语言处理中连续学习的下一个任务和领域

    HOP to the Next Tasks and Domains for Continual Learning in NLP

    [https://arxiv.org/abs/2402.18449](https://arxiv.org/abs/2402.18449)

    该方法HOP在连续学习中引入了三个方向以在自然语言处理中跨任务和领域进行学习。

    

    连续学习（CL）旨在通过转移先前问题中获得的知识来学习一系列问题（即任务和领域），同时避免遗忘过去的问题。与先前专注于特定用例中一个NLP任务或领域的CL方法不同，本文针对一个更通用的CL设置，从一个唯一的框架中学习一系列问题。我们的方法HOP通过沿三个方向解决CL问题来允许在任务和领域之间跳跃：（i）我们使用一组适配器将大型预训练模型推广到未见问题，（ii）我们计算嵌入表示分布上的高阶矩以区分不同任务和领域之间的独立和相关统计数据，（iii）我们通过为每个最终问题专门设计的辅助头处理这些丰富信息。我们在4个NLP应用程序，5个基准测试和...

    arXiv:2402.18449v1 Announce Type: cross  Abstract: Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones. Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework. Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem. Extensive experimental campaign on 4 NLP applications, 5 benchmarks and 
    
[^11]: 超越自然语言：LLM利用替代格式进行增强推理和沟通

    Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication

    [https://arxiv.org/abs/2402.18439](https://arxiv.org/abs/2402.18439)

    挑战了默认使用自然语言的做法，通过探索LLMs在推理和沟通中使用替代格式的实用性，实现了推理效率的提升和多智能体通信时标记使用量的显著减少。

    

    自然语言（NL）长期以来一直是人类认知和沟通的主要格式，因此，在大型语言模型（LLMs）的发展和应用中同样至关重要。然而，除了NL之外，LLMs在预训练过程中看到了各种非NL格式，如代码和逻辑表达式。NL作为LLMs的最佳格式，在单一LLM推理和多智能体通信中的地位尚未得到彻底审查。在这项工作中，我们挑战了默认使用NL，通过探索在这些上下文中使用非NL格式的效用。我们展示，允许LLMs在推理或沟通之前自主选择最合适的格式，可导致不同LLMs推理效率提高3.3至5.7％，并且在多智能体通信中最多可减少72.7％的标记使用，同时保持沟通效果。我们的综合分析进一步揭示了LLMs可以......

    arXiv:2402.18439v1 Announce Type: cross  Abstract: Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs c
    
[^12]: 利用协同学习在神经机器翻译中利用多样化建模上下文

    Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation

    [https://arxiv.org/abs/2402.18428](https://arxiv.org/abs/2402.18428)

    提出了一种新颖的通用协作学习方法DCMCL，在神经机器翻译中利用多样化建模上下文，将自回归（AR）和非自回归（NAR）模型作为合作者。

    

    自回归（AR）和非自回归（NAR）模型是神经机器翻译（NMT）的两种生成模型。AR模型以逐词方式预测令牌，可以有效捕捉真实翻译的分布。NAR模型通过提取双向上下文信息来预测令牌，可以提高推理速度，但它们会遭受性能下降。 以往的研究利用AR模型来改进NAR模型，通过减少训练数据的复杂性或通过NAR模型将全局信息融入AR模型。然而，这些方法仅利用单一类型模型的上下文信息，忽视了不同类型模型可以提供的上下文信息的多样性。在本文中，我们提出了一种新颖的通用协作学习方法，DCMCL，其中将AR和NAR模型视为合作者。

    arXiv:2402.18428v1 Announce Type: new  Abstract: Autoregressive (AR) and Non-autoregressive (NAR) models are two types of generative models for Neural Machine Translation (NMT). AR models predict tokens in a word-by-word manner and can effectively capture the distribution of real translations. NAR models predict tokens by extracting bidirectional contextual information which can improve the inference speed but they suffer from performance degradation. Previous works utilized AR models to enhance NAR models by reducing the training data's complexity or incorporating the global information into AR models by virtue of NAR models. However, those investigated methods only take advantage of the contextual information of a single type of model while neglecting the diversity in the contextual information that can be provided by different types of models. In this paper, we propose a novel generic collaborative learning method, DCMCL, where AR and NAR models are treated as collaborators instead 
    
[^13]: 低资源和中等资源语言中的情感分类

    Emotion Classification in Low and Moderate Resource Languages

    [https://arxiv.org/abs/2402.18424](https://arxiv.org/abs/2402.18424)

    通过跨语言情感分类器，在低和中等资源语言中实现情感分类，展示了两种迁移学习方法的有效性。

    

    能够分析全球范围内人们情绪状态是很重要的。全球有7100多种活跃语言，为每种语言构建情感分类是一项劳动密集型工作。特别是对于低资源和濒危语言，建立情感分类可能非常具有挑战性。我们提出了一种跨语言情感分类器，我们在资源丰富的语言（例如我们的工作中的英语）上训练情感分类器，并将学习迁移到低资源和中等资源的语言。我们比较并对比了从高资源语言到低资源或中等资源语言的两种迁移学习方法。一种方法将高资源语言的标注投影到低资源和中等资源语言的平行语料库中，另一种方法直接将高资源语言的学习迁移到其他语言。我们展示了我们的方法在6种语言上的有效性：Fa

    arXiv:2402.18424v1 Announce Type: cross  Abstract: It is important to be able to analyze the emotional state of people around the globe. There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive. Particularly for low-resource and endangered languages, building emotion classification can be quite challenging. We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \textit{English} in our work) and transfer the learning to low and moderate resource languages. We compare and contrast two approaches of transfer learning from a high-resource language to a low or moderate-resource language. One approach projects the annotation from a high-resource language to low and moderate-resource language in parallel corpora and the other one uses direct transfer from high-resource language to the other languages. We show the efficacy of our approaches on 6 languages: Fa
    
[^14]: 能否通过基于指南的自动问答来改善GPT的先前授权状态？

    Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?

    [https://arxiv.org/abs/2402.18419](https://arxiv.org/abs/2402.18419)

    通过问答任务，GPT能够验证医疗领域患者的PA请求，帮助卫生计划更快地做出决策。

    

    卫生保险公司有一个被称为先前授权（PA）的流程，这是一种卫生计划成本控制流程，要求医生和其他医疗专业人员在对患者执行特定程序之前必须事先获得卫生计划的批准，以便有资格获得支付覆盖。对卫生保险公司来说，批准医疗领域患者的PA请求是一项耗时且具有挑战性的任务。其中的一项关键挑战是验证请求是否符合某些标准，如年龄、性别等。在这项工作中，我们评估了GPT是否能验证大量关键因素，从而帮助卫生计划更快地做出决策。我们将其构建为一个问答任务，促使GPT从患者的电子健康记录中回答问题。我们尝试了不同的传统提示技术，同时还引入了我们自己的新颖提示技术。

    arXiv:2402.18419v1 Announce Type: cross  Abstract: Health insurance companies have a defined process called prior authorization (PA) which is a health plan cost-control process that requires doctors and other healthcare professionals to get clearance in advance from a health plan before performing a particular procedure on a patient in order to be eligible for payment coverage. For health insurance companies, approving PA requests for patients in the medical domain is a time-consuming and challenging task. One of those key challenges is validating if a request matches up to certain criteria such as age, gender, etc. In this work, we evaluate whether GPT can validate numerous key factors, in turn helping health plans reach a decision drastically faster. We frame it as a question answering task, prompting GPT to answer a question from patient electronic health record. We experiment with different conventional prompting techniques as well as introduce our own novel prompting technique. Mo
    
[^15]: 一个针对大型视觉语言模型图像推理和描述的认知评估基准

    A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models

    [https://arxiv.org/abs/2402.18409](https://arxiv.org/abs/2402.18409)

    提出了一个新颖的评估基准，用于评估大型视觉语言模型的认知能力，发现LVLMs与人类之间存在较大的认知能力差距。

    

    尽管大型视觉语言模型(LVLMs)近年来取得了成功，但它们很少受到全面的认知能力测试。受到人类认知测试中广泛使用的“偷饼干”任务的启发，我们提出了一个新颖的评估基准，利用具有丰富语义的图像评估LVLMs的高级认知能力。它定义了八种推理能力，并包括图像描述任务和视觉问答任务。我们对知名LVLMs进行的评估表明，在LVLMs和人类之间仍存在较大的认知能力差距。

    arXiv:2402.18409v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.
    
[^16]: 分解提示：揭示以英语为中心的大型语言模型中的多语言语言结构知识

    Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models

    [https://arxiv.org/abs/2402.18397](https://arxiv.org/abs/2402.18397)

    通过分解提示方法，这项研究揭示了以英语为中心的大型语言模型在多语言任务上的语言结构理解能力，证实其在零次和少次迭代设置中的高效性和效率。

    

    尽管英语在它们的训练数据中占主导地位，类似GPT-3和LLaMA这样以英语为中心的大型语言模型（LLMs）表现出卓越的多语言任务能力，这引发了关于其跨语言能力深度和性质的问题。本文引入了分解提示方法，用以探究这些LLMs在序列标注任务中的语言结构理解能力。与单一文本到文本提示不同，我们的方法为输入句子的每个令牌生成一个单独的提示，询问其语言标签。我们在38种语言的通用依赖词性标注数据集上评估了我们的方法，利用了以英语为中心和多语言LLMs。我们的研究结果表明，分解提示在零次和少次迭代设置下的效力和效率均超过了迭代提示基线。进一步的分析揭示了评估方法和他们之间的影响。

    arXiv:2402.18397v1 Announce Type: new  Abstract: Despite the predominance of English in their training data, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks. Diverging from the single text-to-text prompt, our method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Further analysis reveals the influence of evaluation methods and the us
    
[^17]: WSDM Cup 2024的第一名解决方案：利用大型语言模型进行会话式多文档问答

    The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA

    [https://arxiv.org/abs/2402.18385](https://arxiv.org/abs/2402.18385)

    本文介绍了在WSDM Cup 2024中获胜的方法，利用大型语言模型（LLMs）的卓越自然语言理解和生成能力，通过改编LLMs、设计混合训练策略、采用先进的文本嵌入模型、以及模型集成等多种技术，最终在会话式多文档问答方面取得了第一名。

    

    会话式多文档问答旨在根据检索到的文档以及上下文对话来回答特定问题。本文介绍了我们在WSDM Cup 2024“会话式多文档问答”挑战中的获胜方法，利用了大型语言模型（LLMs）卓越的自然语言理解和生成能力。我们首先将LLMs改编为此任务，然后设计了混合训练策略，充分利用领域内无标签数据。此外，采用了先进的文本嵌入模型来过滤掉潜在的不相关文档，并为模型集成设计和比较了几种方法。凭借所有这些技术，我们的解决方案最终在WSDM Cup 2024中排名第一，大大超越了竞争对手。源代码已发布在https://github.com/zhangzhao219/WSDM-Cup-2024。

    arXiv:2402.18385v1 Announce Type: new  Abstract: Conversational multi-doc question answering aims to answer specific questions based on the retrieved documents as well as the contextual conversations. In this paper, we introduce our winning approach for the "Conversational Multi-Doc QA" challenge in WSDM Cup 2024, which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs). We first adapt LLMs to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data. Moreover, an advanced text embedding model is adopted to filter out potentially irrelevant documents and several approaches are designed and compared for the model ensemble. Equipped with all these techniques, our solution finally ranked 1st place in WSDM Cup 2024, surpassing its rivals to a large extent. The source codes have been released at https://github.com/zhangzhao219/WSDM-Cup-2024.
    
[^18]: Tokenization超越了压缩

    Tokenization Is More Than Compression

    [https://arxiv.org/abs/2402.18376](https://arxiv.org/abs/2402.18376)

    通过引入新的分词器PathPiece，研究者发现少量标记并不能导致更好的下游性能，这一结果对于 Tokenization 的有效性理解提出了质疑。

    

    Tokenization是自然语言处理（NLP）任务中的基础步骤，它连接了原始文本和语言模型。现有的Tokenization方法，如字节对编码（Byte-Pair Encoding，BPE），源自数据压缩领域，并有人认为BPE的有效性源于其将文本压缩为相对较少的标记的能力。我们通过引入PathPiece来测试“更少的标记是否会导致更好的下游性能”这一假设，PathPiece是一种新的分词器，根据给定词汇将文档文本划分为最少数量的标记。通过广泛实验，我们发现这一假设并非成立，对有效Tokenization原因的理解产生了疑问。为了检查哪些其他因素起到作用，我们评估了Tokenization的所有三个阶段（预分词、词汇构造和分割）的设计决策，提供了关于设计的新见解。

    arXiv:2402.18376v1 Announce Type: cross  Abstract: Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of 
    
[^19]: VerifiNER: 通过大型语言模型基于知识驱动的推理增强的命名实体识别

    VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models

    [https://arxiv.org/abs/2402.18374](https://arxiv.org/abs/2402.18374)

    VerifiNER是一个后续验证框架，通过知识识别并修正现有命名实体识别方法的错误，以实现更忠实的预测。

    

    最近在特定领域命名实体识别（NER）中的方法，如生物医学NER，已经取得了显著进展。然而，它们仍然缺乏忠实性，会产生错误的预测。我们认为实体的知识可以在验证预测的正确性方面发挥作用。尽管知识的有用性，使用知识来纠正这些错误并不容易，因为知识本身并不直接指示出真实标签。因此，我们提出了VerifiNER，一个利用知识从现有NER方法中识别错误并将其修正为更忠实预测的后续验证框架。我们的框架利用大型语言模型的推理能力，在验证过程中充分基于知识和上下文信息。我们通过对生物医学数据集的广泛实验验证了VerifiNER的有效性。结果表明VerifiNER可以

    arXiv:2402.18374v1 Announce Type: new  Abstract: Recent approaches in domain-specific named entity recognition (NER), such as biomedical NER, have shown remarkable advances. However, they still lack of faithfulness, producing erroneous predictions. We assume that knowledge of entities can be useful in verifying the correctness of the predictions. Despite the usefulness of knowledge, resolving such errors with knowledge is nontrivial, since the knowledge itself does not directly indicate the ground-truth label. To this end, we propose VerifiNER, a post-hoc verification framework that identifies errors from existing NER methods using knowledge and revises them into more faithful predictions. Our framework leverages the reasoning abilities of large language models to adequately ground on knowledge and the contextual information in the verification process. We validate effectiveness of VerifiNER through extensive experiments on biomedical datasets. The results suggest that VerifiNER can su
    
[^20]: 专注于你的问题！解释和减轻常识推理中的有害CoT问题

    Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning

    [https://arxiv.org/abs/2402.18344](https://arxiv.org/abs/2402.18344)

    大型语言模型在常识推理中表现出高水平的能力，但由于信息丢失问题，提出了新方法RIDERS来解释和减轻有害CoT问题

    

    大型语言模型表现出高水平的常识推理能力，尤其是通过Chain-of-Thought（CoT）等增强方法。然而，我们发现这些类似CoT的方法导致了原本正确的答案变得错误的问题，我们将其定义为有害的CoT问题。为了解释和减轻这一问题，我们首先利用属性跟踪和因果跟踪方法来探究LLM在CoT推理过程中的内部工作机制。通过比较，我们证明了模型在生成推理或答案时存在来自问题的信息丢失现象在浅层注意力层中。基于探究结果，我们设计了一种名为RIDERS（Residual decodIng and sERial-position Swap）的新方法，从解码和序列位置的角度补偿模型中的信息亏缺。通过对多个常识推理基准的广泛实验，我们验证了

    arXiv:2402.18344v1 Announce Type: new  Abstract: Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate 
    
[^21]: 学习生成用于零shot任务适应的指令调优数据集

    Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation

    [https://arxiv.org/abs/2402.18334](https://arxiv.org/abs/2402.18334)

    Bonito是一种用于生成指令调优训练数据集的模型，通过将未注释的文本转换为特定任务训练数据，实现大型语言模型对用户专属数据的零shot任务适应，并显著提高了预训练和指令调整模型的平均性能。

    

    我们介绍了Bonito，这是一个开源模型，用于条件任务生成：将未注释的文本转换为用于指令调优的特定任务训练数据集。我们的目标是在用户专门的私人数据上实现大型语言模型的零shot任务适应。我们使用1.65M个示例的新大规模数据集训练Bonito，该数据集是通过将现有的指令调优数据集重新混合成元模板而创建的。数据集的元模板产生训练示例，其中输入是未注释的文本和任务属性，输出包括指令和响应。我们使用Bonito为七个专业领域的数据集生成合成任务，跨三种任务类型 -- 是非问答、抽取式问答和自然语言推理 -- 并调整语言模型。我们展示了Bonito显著改善了预训练和指令调整模型的平均性能。

    arXiv:2402.18334v1 Announce Type: new  Abstract: We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned mode
    
[^22]: 如何逐步思考：对思维链推理的机械理解

    How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning

    [https://arxiv.org/abs/2402.18312](https://arxiv.org/abs/2402.18312)

    通过对LLMs进行机械性研究，我们发现模型在进行逐步推理时使用多个并行路径生成答案，同时存在功能性分歧。

    

    尽管大型语言模型（LLMs）展示了出色的推理能力，通过思维链（CoT）提示，但对于促进CoT生成的模型内部机制仍存在缺乏理解的问题。本文从机械性的角度研究了LLMs中表现出CoT推理的神经子结构。通过对LLaMA-2 7B应用于虚构本体论的多步推理的分析，我们展示了LLMs为逐步推理部署了多个并行答案生成路径。这些并行路径提供了来自输入问题上下文以及生成的CoT的序贯答案。我们观察到LLMs中间层存在引人瞩目的功能分歧。初始一半的令牌表示仍然强烈偏向预训练先验，而后半部分突然被上下文所取代。这种内部相位转变在不同的功能协同中体现出来。

    arXiv:2402.18312v1 Announce Type: new  Abstract: Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a striking functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context taking over abruptly in the later half. This internal phase shift manifests in different functional co
    
[^23]: 众包是否让您破产了？使用近端策略优化对预训练语言模型进行成本效益微调

    Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization

    [https://arxiv.org/abs/2402.18284](https://arxiv.org/abs/2402.18284)

    提出了一种自监督文本排序方法，利用近端策略优化对语言模型进行微调，消除了对人工注释员的需求，实验结果表明该方法训练的模型在各项得分方面明显优于基线

    

    ChatGPT的广泛使用凸显了从人类反馈中进行强化学习的潜力。然而，其训练流程依赖于人工排序，这是一个资源密集型的过程。为了降低劳动成本，我们提出了一种自监督文本排序方法，用于应用近端策略优化来对语言模型进行微调，同时消除了对人工注释员的需求。我们的方法从概率抽样开始，鼓励语言模型为每个输入生成多样化的响应。然后，我们使用TextRank和ISODATA算法，基于语义对这些响应进行排序和聚类。随后，我们构建了一个奖励模型来学习排名并优化我们的生成策略。我们在三个任务上使用两个语言模型进行的实验结果表明，我们的方法训练的模型在BLEU、GLEU和METEOR得分方面明显优于基线。此外，我们的手动评估显示

    arXiv:2402.18284v1 Announce Type: cross  Abstract: Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation show
    
[^24]: 更好理解对比句子表示学习：梯度的统一范式

    Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient

    [https://arxiv.org/abs/2402.18281](https://arxiv.org/abs/2402.18281)

    通过研究梯度，将四种有效的对比损失集成到一个统一的范式中，以探究对比句子表示学习中各种对比损失达到卓越性能的共同特点。

    

    句子表示学习（SRL）是自然语言处理（NLP）中至关重要的任务，对照自监督学习（SSL）是目前主流方法。然而，其显著有效性背后的原因仍不清楚。特别是，在其他研究领域中，对比SSL在理论和实际表现上与非对比SSL（例如，对齐和一致性、Barlow Twins和VICReg）有相似之处。然而，在SRL中，对比SSL明显优于非对比SSL。因此，出现了两个问题：首先，是什么共同点使各种对比损失在SRL中取得了优越性能？其次，我们如何使非对比SSL（与对比SSL相似但在SRL中无效）变得有效？为了解决这些问题，我们从梯度的角度出发，发现四种有效的对比损失可以集成到一个统一的范式中

    arXiv:2402.18281v1 Announce Type: new  Abstract: Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, in other research fields, contrastive SSL shares similarities in both theory and practical performance with non-contrastive SSL (e.g., alignment & uniformity, Barlow Twins, and VICReg). However, in SRL, contrastive SSL outperforms non-contrastive SSL significantly. Therefore, two questions arise: First, what commonalities enable various contrastive losses to achieve superior performance in SRL? Second, how can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in SRL, effective? To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, whic
    
[^25]: 探索适配器用于噪声鲁棒自动语音识别

    Exploration of Adapter for Noise Robust Automatic Speech Recognition

    [https://arxiv.org/abs/2402.18275](https://arxiv.org/abs/2402.18275)

    本文研究了适配器用于噪声鲁棒自动语音识别的探索，发现将适配器插入浅层可以获得更显著的效果，真实数据比模拟数据更有效，并且将适配器集成到基于语音增强的ASR系统中可以带来实质性改进。

    

    适应鲁棒的自动语音识别（ASR）系统以解决未知噪声场景至关重要。将适配器集成到神经网络中已经成为一种强大的迁移学习技术。本文深入研究了基于适配器的噪声鲁棒ASR适应。我们使用了CHiME--4数据集进行实验。结果显示，在浅层插入适配器能够产生更显著的效果，在仅在浅层内部进行适应和在所有层之间进行适应之间没有显著差异。此外，模拟数据有助于系统改善其在实际噪声条件下的表现。然而，在数据量相同时，真实数据比模拟数据更有效。在适配器训练中，多条件训练仍然有效。此外，将适配器集成到基于语音增强的ASR系统中会带来显著的改进。

    arXiv:2402.18275v1 Announce Type: cross  Abstract: Adapting a robust automatic speech recognition (ASR) system to tackle unseen noise scenarios is crucial. Integrating adapters into neural networks has emerged as a potent technique for transfer learning. This paper thoroughly investigates adapter-based noise-robust ASR adaptation. We conducted the experiments using the CHiME--4 dataset. The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers. Besides, the simulated data helps the system to improve its performance under real noise conditions. Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data. Multi-condition training remains valid for adapter training. Furthermore, integrating adapters into speech enhancement-based ASR systems yields substantial improvements.
    
[^26]: 重新思考LLM推理的界限：多智能体讨论是关键吗？

    Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?

    [https://arxiv.org/abs/2402.18272](https://arxiv.org/abs/2402.18272)

    多智能体讨论提升了LLM的推理能力，但在有演示的情况下，单智能体LLM通过强提示几乎能达到与最佳讨论方法相同的性能。

    

    LLM讨论领域的最新进展表明，多智能体讨论提升了LLM的推理能力。在这项工作中，我们通过系统实验对这一说法进行重新评估，我们提出了一个新颖的小组讨论框架，丰富了讨论机制集合。有趣的是，我们的结果显示，一个带有强提示的单智能体LLM在广泛的推理任务和基本LLM中几乎可以达到与最佳现有讨论方法相同的性能。我们观察到，多智能体讨论仅在提示中没有演示时才优于单个智能体。进一步研究揭示了LLM在讨论过程中的常见交互机制。

    arXiv:2402.18272v1 Announce Type: cross  Abstract: Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.
    
[^27]: 关于神经问答生成的调查：方法、应用和前景

    A Survey on Neural Question Generation: Methods, Applications, and Prospects

    [https://arxiv.org/abs/2402.18267](https://arxiv.org/abs/2402.18267)

    这项调查系统研究了神经问答生成（NQG）领域的进展，包括了背景概述、不同类别的方法、以及未来展望

    

    在这项调查中，我们对神经问答生成（NQG）领域的进展进行了详细检查，这一领域利用神经网络技术从各种来源，如知识库、文本和图像中生成相关问题。调查从NQG背景概述开始，包括任务的问题制定、流行的基准数据集、已建立的评估指标和显著应用。然后，系统地将NQG方法分为三个主要类别：结构化NQG，利用有组织的数据源，非结构化NQG，专注于更松散结构的输入，如文本或视觉内容，以及混合NQG，利用多样的输入模式。这一分类后是对为每个类别量身定制的不同神经网络模型的深入分析，讨论它们固有的优势和潜在局限性。调查以展望未来结束。

    arXiv:2402.18267v1 Announce Type: cross  Abstract: In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications. It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking persp
    
[^28]: 基于检索的应急事件全长维基百科生成

    Retrieval-based Full-length Wikipedia Generation for Emergent Events

    [https://arxiv.org/abs/2402.18264](https://arxiv.org/abs/2402.18264)

    通过检索获取的Web来源信息，为新兴事件生成结构化的全长维基百科文档，避免大型语言模型在与最近发生事件相关的语料库上进行训练。

    

    在当今快节奏的世界中，迅速生成新兴事件全面准确的维基百科文档的需求日益重要且具有挑战性。然而，先前的维基百科生成工作往往未能满足现实需求。一些方法仅专注于生成完整维基百科文档的部分内容，而另一些则忽视了生成过程中忠实性的重要性，或未考虑预训练语料库的影响。本文中，我们模拟了一个真实世界场景，使用从网页来源检索的内容为新兴事件生成结构化的全长维基百科文档。为确保大型语言模型（LLMs）未经过基于最近发生事件的语料库训练，我们选择最近发生的事件并引入了一个新的基准 Wiki-GenBen，其中包含了309个事件及其对应的检索到的网页。

    arXiv:2402.18264v1 Announce Type: new  Abstract: In today's fast-paced world, the growing demand to quickly generate comprehensive and accurate Wikipedia documents for emerging events is both crucial and challenging. However, previous efforts in Wikipedia generation have often fallen short of meeting real-world requirements. Some approaches focus solely on generating segments of a complete Wikipedia document, while others overlook the importance of faithfulness in generation or fail to consider the influence of the pre-training corpus. In this paper, we simulate a real-world scenario where structured full-length Wikipedia documents are generated for emergent events using input retrieved from web sources. To ensure that Large Language Models (LLMs) are not trained on corpora related to recently occurred events, we select events that have taken place recently and introduce a new benchmark Wiki-GenBen, which consists of 309 events paired with their corresponding retrieved web pages for ge
    
[^29]: 用于视觉丰富网页理解的分层多模态预训练

    Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding

    [https://arxiv.org/abs/2402.18262](https://arxiv.org/abs/2402.18262)

    提出了用于视觉丰富网页理解的分层多模态预训练方法，结合文本、结构和图像模态的交互来增强对网页的理解

    

    具有丰富视觉内容的文档（如网页和扫描/数字化文档（图像，PDF等））的普及使得学术界和工业界对自动文档理解和信息提取产生了更大兴趣。本文引入了 WebLM，一个多模态预训练网络，旨在解决仅对网页中的文本和结构模态进行建模的局限性。WebLM将文档图像的分层结构集成到模态中，以增强对基于标记语言的文档的理解。

    arXiv:2402.18262v1 Announce Type: new  Abstract: The growing prevalence of visually rich documents, such as webpages and scanned/digital-born documents (images, PDFs, etc.), has led to increased interest in automatic document understanding and information extraction across academia and industry. Although various document modalities, including image, text, layout, and structure, facilitate human information retrieval, the interconnected nature of these modalities presents challenges for neural networks. In this paper, we introduce WebLM, a multimodal pre-training network designed to address the limitations of solely modeling text and structure modalities of HTML in webpages. Instead of processing document images as unified natural images, WebLM integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents. Additionally, we propose several pre-training tasks to model the interaction among text, structure, and image modalities eff
    
[^30]: 一种具有层次语义框架的多意图口语理解BiRGAT模型

    A BiRGAT Model for Multi-intent Spoken Language Understanding with Hierarchical Semantic Frames

    [https://arxiv.org/abs/2402.18258](https://arxiv.org/abs/2402.18258)

    提出了一种具有层次语义框架的多意图口语理解BiRGAT模型，在实验中通过多意图数据集MIVS，采用3层层次结构来解决多意图情况下的对齐和分配问题，并结合3路指针-生成器解码器，显著优于传统方案。

    

    先前对口语理解（SLU）的研究主要集中在单意图设置上，即每个输入话语仅包含一个用户意图。这种配置极大地限制了用户话语的表面形式和输出语义的容量。本文首先提出了一个从实际车载对话系统MIVS中收集的多意图数据集。目标语义框架组织成3层层次结构，以应对多意图情况下的对齐和分配问题。相应地，我们设计了一个BiRGAT模型来编码本体项目的层次结构，其主干是双重关系图注意网络。结合3路指针-生成器解码器，我们的方法在性能上大大优于传统的序列标注和基于分类的方案。

    arXiv:2402.18258v1 Announce Type: new  Abstract: Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent. This configuration significantly limits the surface form of user utterances and the capacity of output semantics. In this work, we first propose a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue System, called MIVS. The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases. Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational graph attention network. Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin.
    
[^31]: 通过心智模型实现大型语言模型的通用提示方法

    Towards Generalist Prompting for Large Language Models by Mental Models

    [https://arxiv.org/abs/2402.18252](https://arxiv.org/abs/2402.18252)

    通过提出通用提示概念和创新的MeMo（心智模型）方法，实现大型语言模型在广泛任务范围内实现最佳性能，消除了手动选择和定制提示的需求。

    

    大型语言模型（LLMs）在许多任务上展示出令人印象深刻的性能。然而，为了达到最佳性能，仍然需要特别设计的提示方法。这些方法要么依赖于需要一定领域知识的特定任务少量示例，要么被设计为简单，但只对少数类型任务表现良好。在这项工作中，我们尝试引入通用提示的概念，它的设计原则是在广泛任务范围内实现最佳或接近最佳的性能，同时消除了需要针对特定问题手动选择和定制提示的需求。此外，我们提出了MeMo（心智模型），这是一种简单设计的创新提示方法，能有效地实现通用提示的标准。MeMo将各种提示方法的核心精髓提炼为单个心智模型，并允许LLM自主选择。

    arXiv:2402.18252v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance on many tasks. However, to achieve optimal performance, specially designed prompting methods are still needed. These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks. In this work, we attempt to introduce the concept of generalist prompting, which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of prompts tailored to specific problems. Furthermore, we propose MeMo (Mental Models), an innovative prompting method that is simple-designed yet effectively fulfills the criteria of generalist prompting. MeMo distills the cores of various prompting methods into individual mental models and allows LLMs to autonomously select
    
[^32]: 学习还是自我调整？重新思考指导微调

    Learning or Self-aligning? Rethinking Instruction Fine-tuning

    [https://arxiv.org/abs/2402.18243](https://arxiv.org/abs/2402.18243)

    本研究揭示了指导微调的潜在机制，发现尝试通过指导微调学习额外世界知识往往难以产生积极影响，重点在于保持内部知识一致性。

    

    指导微调（IFT）是构建大型语言模型（LLM）中至关重要的阶段。先前的研究主要关注IFT在行为规范传递和额外世界知识学习中的作用。然而，对IFT潜在机制的理解仍然相当有限。本文设计了一个知识干预框架，以解耦IFT的潜在因素，从而实现对不同因素的个体分析。令人惊讶的是，我们的实验揭示，通过IFT试图学习额外的世界知识往往难以产生积极影响，甚至可能导致明显负面影响。此外，我们发现在IFT之前和之后保持内部知识一致性是实现成功IFT的关键因素。我们的研究结果揭示了IFT的潜在机制，并为最新和潜在未来的研究提供了有力支持。

    arXiv:2402.18243v1 Announce Type: new  Abstract: Instruction Fine-tuning~(IFT) is a critical phase in building large language models~(LLMs). Previous works mainly focus on the IFT's role in the transfer of behavioral norms and the learning of additional world knowledge. However, the understanding of the underlying mechanisms of IFT remains significantly limited. In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors. Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects. Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT. Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works.
    
[^33]: 基于大型语言模型的代理平台上的前景个性化推荐

    Prospect Personalized Recommendation on Large Language Model-based Agent Platform

    [https://arxiv.org/abs/2402.18240](https://arxiv.org/abs/2402.18240)

    提出了一种基于大型语言模型代理平台的个性化推荐系统Rec4Agentverse，强调代理项和代理推荐器之间的合作，促进个性化信息服务，提升信息交换，并展望了其演进为支持互动和信息交换的三个阶段

    

    新型代理导向信息系统，以GPT为例，促使我们审视信息系统基础设施，以支持代理级信息处理并适应基于大型语言模型（LLM）的代理的特征，如互动性。本研究展望了基于LLM代理平台的推荐系统的前景，并介绍了一种称为Rec4Agentverse的新型推荐范式，包括代理项和代理推荐器。Rec4Agentverse强调代理项和代理推荐器之间的合作，从而促进个性化信息服务，并增强信息交换，超越传统的用户-推荐器反馈循环。此外，我们展望了Rec4Agentverse的演进，并将其概念化为基于代理项、代理推荐器和用户之间互动和信息交换增强的三个阶段。

    arXiv:2402.18240v1 Announce Type: cross  Abstract: The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A pre
    
[^34]: CogBench: 一个大型语言模型步入心理实验室

    CogBench: a large language model walks into a psychology lab

    [https://arxiv.org/abs/2402.18225](https://arxiv.org/abs/2402.18225)

    CogBench提出了一个从七个认知心理学实验中衍生出十个行为指标的基准测试，为评估大型语言模型的行为提供了工具，研究发现模型大小和从人类反馈中学习的强化学习对性能改善和与人类行为一致具有重要作用。

    

    大型语言模型（LLMs）显著推动了人工智能领域的发展。然而，对它们进行全面评估仍然具有挑战性。我们认为，这部分是由于大多数基准测试中对性能指标的主要关注。本文介绍了CogBench，这是一个基准测试，包括从七个认知心理学实验中衍生的十个行为指标。这种新颖方法为表型化LLMs的行为提供了一个工具包。我们将CogBench应用于35个LLMs，得到丰富多样的数据集。我们使用统计多层建模技术分析这些数据，考虑到特定LLMs的微调版本之间的嵌套依赖关系。我们的研究突出了模型大小和从人类反馈中学习的强化学习在改善性能并与人类行为保持一致方面的关键作用。有趣的是，我们发现开源模型比专有模型更少风险，并且精细调

    arXiv:2402.18225v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-t
    
[^35]: 通过自适应解码改进开放式文本生成

    Improving Open-Ended Text Generation via Adaptive Decoding

    [https://arxiv.org/abs/2402.18223](https://arxiv.org/abs/2402.18223)

    引入自适应解码机制，通过置信度动态确定生成过程中的候选集，在故事生成任务中实现了更高的MAUVE和多样性，保持一定的连贯性，优于现有算法。

    

    当前语言模型根据概率分布逐标记解码文本，确定下一个标记的恰当候选者对于保证生成质量至关重要。本研究引入了自适应解码，一种机制使语言模型能够在生成过程中动态确定一个合理的候选集。具体来说，我们引入了一种基于熵的度量标准，称之为置信度，并将确定最佳候选集视为一个增加置信度的过程。通过利用置信度增加来评估将标记包含在候选集中的合理性，使模型能够自适应地确定最合适的候选集。实验结果表明，我们的方法在故事生成任务中实现了更高的MAUVE和多样性，并保持了一定的连贯性，突显了其优于现有算法的优越性。

    arXiv:2402.18223v1 Announce Type: new  Abstract: Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively. The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms. The code is available at https://github.
    
[^36]: LLM任务干扰：关于对话历史中任务切换影响的初步研究

    LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History

    [https://arxiv.org/abs/2402.18216](https://arxiv.org/abs/2402.18216)

    本研究初步探讨了对话中任务切换对LLM模型干扰的影响，发现任务切换可能导致性能下降。

    

    最近强大的指令调整的大型语言模型(LLMs)的出现，使得各种有用的对话人工智能(AI)系统已经部署在许多应用中。当用户提出问题时，这些AI系统成功地作为对话的一部分执行各种任务。为了提供某种记忆和上下文，这些方法通常将输出条件限制在整个对话历史上。尽管对对话历史的敏感性经常会导致在随后的任务中表现提高，但我们发现，实际上如果有任务切换，表现也可能受到负面影响。据我们所知，我们的工作首次尝试正式研究对话LLMs中由于对话历史中的任务切换而引起的任务干扰和干扰的脆弱性。我们在5个数据集上进行了实验，使用了流行的LLMs进行了15次任务切换，结果表明

    arXiv:2402.18216v1 Announce Type: new  Abstract: With the recent emergence of powerful instruction-tuned large language models (LLMs), various helpful conversational Artificial Intelligence (AI) systems have been deployed across many applications. When prompted by users, these AI systems successfully perform a wide range of tasks as part of a conversation. To provide some sort of memory and context, such approaches typically condition their output on the entire conversational history. Although this sensitivity to the conversational history can often lead to improved performance on subsequent tasks, we find that performance can in fact also be negatively impacted, if there is a task-switch. To the best of our knowledge, our work makes the first attempt to formalize the study of such vulnerabilities and interference of tasks in conversational LLMs caused by task-switches in the conversational history. Our experiments across 5 datasets with 15 task switches using popular LLMs reveal that 
    
[^37]: DANSK和DaCy 2.6.0：丹麦命名实体识别的领域泛化

    DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity Recognition

    [https://arxiv.org/abs/2402.18209](https://arxiv.org/abs/2402.18209)

    丹麦NLP中命名实体识别领域的一个重要贡献是引入了DANSK数据集和DaCy 2.6.0模型，以解决现有模型在细粒度命名实体识别和跨领域泛化方面的限制。

    

    命名实体识别是丹麦自然语言处理的基石之一，对于工业和研究中的语言技术应用至关重要。然而，丹麦的NER受到可用数据集的不足所限制。因此，目前没有模型能够进行细粒度的命名实体识别，也没有对跨数据集和领域的潜在泛化问题进行评估。为了缓解这些限制，本文介绍了：1）DANSK：一个提供高粒度标记以及在各种领域内模型评估的命名实体数据集；2）DaCy 2.6.0，其中包括三个可泛化的模型，并配有细粒度标注；以及3）对当前最先进模型在跨领域泛化能力方面的评估。对现有和新模型的评估揭示了值得在领域内解决的显着性能差异。

    arXiv:2402.18209v1 Announce Type: new  Abstract: Named entity recognition is one of the cornerstones of Danish NLP, essential for language technology applications within both industry and research. However, Danish NER is inhibited by a lack of available datasets. As a consequence, no current models are capable of fine-grained named entity recognition, nor have they been evaluated for potential generalizability issues across datasets and domains. To alleviate these limitations, this paper introduces: 1) DANSK: a named entity dataset providing for high-granularity tagging as well as within-domain evaluation of models across a diverse set of domains; 2) DaCy 2.6.0 that includes three generalizable models with fine-grained annotation; and 3) an evaluation of current state-of-the-art models' ability to generalize across domains. The evaluation of existing and new models revealed notable performance discrepancies across domains, which should be addressed within the field. Shortcomings of the
    
[^38]: 聚类与排序：通过专家定位质量估计实现保留多样性的指令选择

    Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation

    [https://arxiv.org/abs/2402.18191](https://arxiv.org/abs/2402.18191)

    本文提出了一种聚类与排序方法（CaR），通过与专家偏好相一致的评分模型排名指令对，保留了数据集的多样性。

    

    随着开源社区的贡献，涌现了大量指令调优（IT）数据。鉴于训练和评估模型需要大量资源分配，因此有必要采用高效的方法选择高质量的IT数据。然而，现有的指令数据选择方法存在一些限制，比如依赖脆弱的外部API、受GPT模型偏见影响，或减少所选指令数据集的多样性。在本文中，我们提出了一种面向工业的、与专家定位相吻合并保留多样性的指令数据选择方法：聚类与排序（CaR）。CaR分为两个步骤。第一步涉及使用与专家偏好很好对齐的评分模型对指令对进行排名（准确率达到84.25%）。第二步通过聚类过程保留数据集多样性。在我们的实验中，CaR选择了一个子集

    arXiv:2402.18191v1 Announce Type: new  Abstract: With contributions from the open-source community, a vast amount of instruction tuning (IT) data has emerged. Given the significant resource allocation required by training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data. However, existing methods for instruction data selection have limitations such as relying on fragile external APIs, being affected by biases in GPT models, or reducing the diversity of the selected instruction dataset. In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved instruction data selection method: Clustering and Ranking (CaR). CaR consists of two steps. The first step involves ranking instruction pairs using a scoring model that is well aligned with expert preferences (achieving an accuracy of 84.25%). The second step involves preserving dataset diversity through a clustering process.In our experiment, CaR selected a sub
    
[^39]: 针对基于上下文的假新闻检测预训练图神经网络的挑战：对当前策略和资源限制的评估

    Challenges in Pre-Training Graph Neural Networks for Context-Based Fake News Detection: An Evaluation of Current Strategies and Resource Limitations

    [https://arxiv.org/abs/2402.18179](https://arxiv.org/abs/2402.18179)

    将神经网络预训练与图神经网络在基于上下文的假新闻检测领域相结合，评估了不同预训练策略，并指出目前转移学习并未显著改进模型性能，主要问题在于缺乏大规模资源。

    

    神经网络的预训练最近在自然语言处理（NLP）领域彻底改变了局面，并且已经在计算机视觉领域证明了其有效性。同时，关于假新闻检测的进展主要受到基于上下文范式的驱动，其中来自社交媒体等不同类型信号形成类似图形结构，其中包含除新闻文章以外的上下文信息以进行分类。我们建议将这两个发展合并，通过在基于上下文的假新闻检测领域应用图神经网络（GNNs）的预训练。我们的实验评估了基于图的虚假信息检测的不同预训练策略，并证明了目前转移学习并未导致显着改进，优于从头开始训练模型。我们认为目前的主要问题是缺乏适用的大规模资源可以使用。

    arXiv:2402.18179v1 Announce Type: new  Abstract: Pre-training of neural networks has recently revolutionized the field of Natural Language Processing (NLP) and has before demonstrated its effectiveness in computer vision. At the same time, advances around the detection of fake news were mainly driven by the context-based paradigm, where different types of signals (e.g. from social media) form graph-like structures that hold contextual information apart from the news article to classify. We propose to merge these two developments by applying pre-training of Graph Neural Networks (GNNs) in the domain of context-based fake news detection. Our experiments provide an evaluation of different pre-training strategies for graph-based misinformation detection and demonstrate that transfer learning does currently not lead to significant improvements over training a model from scratch in the domain. We argue that a major current issue is the lack of suitable large-scale resources that can be used 
    
[^40]: MIKO：基于大型语言模型的社交媒体常识发现的多模态意图知识蒸馏

    MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery

    [https://arxiv.org/abs/2402.18169](https://arxiv.org/abs/2402.18169)

    提出了一种基于大型语言模型的MIKO框架，利用多模态和文本的协同作用揭示社交媒体用户的意图。

    

    社交媒体已经成为与他人联系、了解新闻、表达观点以及找到娱乐的无处不在的工具。然而，由于社交媒体帖子中意图的隐含性、需要跨模态理解文本和图像、以及存在标签、拼写错误和复杂缩写等嘈杂信息，理解社交媒体帖子背后的意图仍然具有挑战性。为了解决这些挑战，我们提出了MIKO，一个多模态意图知识蒸馏框架，它共同利用大型语言模型（LLM）和多模态大型语言模型（MLLM）来揭示用户的意图。具体来说，我们使用MLLM来解释图像，并使用LLM从文本中提取关键信息，最后再次指导LLM生成意图。通过将MIKO应用于公开可用的社交媒体数据集，我们构建了一个意图知识

    arXiv:2402.18169v1 Announce Type: new  Abstract: Social media has become a ubiquitous tool for connecting with others, staying updated with news, expressing opinions, and finding entertainment. However, understanding the intention behind social media posts remains challenging due to the implicitness of intentions in social media posts, the need for cross-modality understanding of both text and images, and the presence of noisy information such as hashtags, misspelled words, and complicated abbreviations. To address these challenges, we present MIKO, a Multimodal Intention Kowledge DistillatiOn framework that collaboratively leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to uncover users' intentions. Specifically, we use an MLLM to interpret the image and an LLM to extract key information from the text and finally instruct the LLM again to generate intentions. By applying MIKO to publicly available social media datasets, we construct an intention kno
    
[^41]: 评估量化大型语言模型

    Evaluating Quantized Large Language Models

    [https://arxiv.org/abs/2402.18158](https://arxiv.org/abs/2402.18158)

    该论文通过全面评估后训练量化对权重、激活和KV缓存的影响，以指导选择量化方法，并对11种模型系列进行了评估，覆盖了多种任务类型。

    

    后训练量化（PTQ）已经成为减少大型语言模型（LLMs）成本的一种有前景的技术，具体地，PTQ可以有效地减轻LLMs中的内存消耗并降低计算开销。为了满足各种场景下高效率和性能的要求，对量化LLMs进行全面评估是必要的，以指导量化方法的选择。本文通过评估PTQ对11个模型系列（包括OPT、LLaMA2、Falcon、Bloomz、Mistral、ChatGLM、Vicuna、LongChat、StableLM、Gemma和Mamba）的权重、激活和KV缓存的影响，范围从125M到180B，全面评估了这些因素。评估涵盖了五种类型的任务：基础NLP、突然出现的能力、可靠性、对话和长上下文任务。此外，我们还评估了最先进的量化方法，以展示它们的应用。

    arXiv:2402.18158v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their appli
    
[^42]: 从总结到行动：利用开放世界API增强复杂任务的大型语言模型

    From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs

    [https://arxiv.org/abs/2402.18157](https://arxiv.org/abs/2402.18157)

    本研究引入了一个新颖的工具调用管道，旨在控制大规模现实世界API，从而增强大型语言模型在复杂任务中的应用能力。

    

    人类与动物之间的区别在于人类具有使用和创造工具的独特能力。工具使人类能够克服生理限制，促进了宏伟文明的创造。类似地，将像大型语言模型（LLMs）这样的基础模型赋予学习外部工具使用能力可能是实现人工智能的关键一步。本领域先前的研究主要追求两种不同的方法来增强LLMs的工具调用能力。第一种方法强调构建用于模型微调的相关数据集。相反，第二种方法旨在通过上下文学习策略充分利用LLMs固有的推理能力。在这项工作中，我们介绍了一个旨在控制大型现实世界API的创新工具调用管道。这一管道反映了人类解决任务的过程，解决了c

    arXiv:2402.18157v1 Announce Type: new  Abstract: The distinction between humans and animals lies in the unique ability of humans to use and create tools. Tools empower humans to overcome physiological limitations, fostering the creation of magnificent civilizations. Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence. Previous studies in this field have predominantly pursued two distinct approaches to augment the tool invocation capabilities of LLMs. The first approach emphasizes the construction of relevant datasets for model fine-tuning. The second approach, in contrast, aims to fully exploit the inherent reasoning abilities of LLMs through in-context learning strategies. In this work, we introduce a novel tool invocation pipeline designed to control massive real-world APIs. This pipeline mirrors the human task-solving process, addressing c
    
[^43]: 切断头部终结冲突：解释和缓解语言模型中的知识冲突的机制

    Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models

    [https://arxiv.org/abs/2402.18154](https://arxiv.org/abs/2402.18154)

    通过信息流的视角解释并干预语言模型中的知识冲突，提出了一种名为Pruning Head via PatH的方法来缓解冲突

    

    最近，检索增强和工具增强展示了通过提供外部上下文来扩展语言模型（LMs）的内部记忆边界的显著能力。然而，内部记忆和外部上下文不可避免地发生冲突，导致LMs内部出现知识冲突。本文旨在通过信息流的视角解释知识冲突的机制，然后通过在关键点进行精确干预来缓解冲突。我们发现在后续层中有一些具有相反效果的注意力头，其中内存头可以从内部记忆中召回知识，而上下文头可以从外部上下文中检索知识。此外，我们揭示了LMs中知识冲突发生的关键点是内存头和上下文头整合不一致信息流的地方。受到这些见解的启发，我们提出了一种名为Pruning Head via PatH的新方法。

    arXiv:2402.18154v1 Announce Type: cross  Abstract: Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external context inevitably clash, leading to knowledge conflicts within LMs. In this paper, we aim to interpret the mechanism of knowledge conflicts through the lens of information flow, and then mitigate conflicts by precise interventions at the pivotal point. We find there are some attention heads with opposite effects in the later layers, where memory heads can recall knowledge from internal memory, and context heads can retrieve knowledge from external context. Moreover, we reveal that the pivotal point at which knowledge conflicts emerge in LMs is the integration of inconsistent information flows by memory heads and context heads. Inspired by the insights, we propose a novel method called Pruning Head via PatH 
    
[^44]: 大型语言模型的无监督信息细化训练用于检索增强生成

    Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation

    [https://arxiv.org/abs/2402.18150](https://arxiv.org/abs/2402.18150)

    本文提出了一种名为InFO-RAG的无监督信息细化训练方法，将大型语言模型在检索增强生成中的角色定义为“信息细化者”，帮助模型更好地整合检索信息以生成更加简洁、准确和完整的文本。

    

    检索增强生成（RAG）通过将来自检索的额外信息整合到大型语言模型（LLMs）中，从而增强其性能。然而，研究表明，LLMs在有效利用检索信息方面仍然面临挑战，有时会忽视或被错误引导。其关键原因在于LLMs的训练没有清晰地让LLMs学会如何利用具有不同质量的检索文本输入。本文提出了一个新颖的视角，将LLMs在RAG中的角色视为“信息细化者”，这意味着无论检索文本的正确性、完整性或有用性如何，LLMs都能一致地整合检索文本中的知识和模型参数，生成比检索文本更简洁、准确和完整的文本。为此，我们提出了一种名为InFO-RAG的信息细化训练方法，以无监督的方式优化LLMs用于RAG。

    arXiv:2402.18150v1 Announce Type: cross  Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG i
    
[^45]: 通过信息瓶颈学习内在维度，用于可解释的基于方面的情感分析

    Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis

    [https://arxiv.org/abs/2402.18145](https://arxiv.org/abs/2402.18145)

    提出了基于信息瓶颈梯度（IBG）解释框架，通过将单词嵌入调整为简明的内在维度来提高基于方面的情感分析模型的性能和可解释性。

    

    基于梯度的解释方法越来越多地用于解释自然语言处理（NLP）中的神经模型，因为其高保真度。这些方法通过一种规范函数使用维度级梯度值来确定单词级重要性，通常假定所有梯度维度具有相等的重要性。然而，在基于方面的情感分析（ABSA）的背景下，我们的初步研究表明只有特定的维度是相关的。为了解决这个问题，我们提出了基于信息瓶颈梯度（IBG）解释框架用于ABSA。该框架利用信息瓶颈将单词嵌入调整为简明的内在维度，保留关键特征并省略无关信息。全面的测试表明，我们的IBG方法通过识别与情感相关的特征，显著改善了模型的性能和可解释性。

    arXiv:2402.18145v1 Announce Type: new  Abstract: Gradient-based explanation methods are increasingly used to interpret neural models in natural language processing (NLP) due to their high fidelity. Such methods determine word-level importance using dimension-level gradient values through a norm function, often presuming equal significance for all gradient dimensions. However, in the context of Aspect-based Sentiment Analysis (ABSA), our preliminary research suggests that only specific dimensions are pertinent. To address this, we propose the Information Bottleneck-based Gradient (\texttt{IBG}) explanation framework for ABSA. This framework leverages an information bottleneck to refine word embeddings into a concise intrinsic dimension, maintaining essential features and omitting unrelated information. Comprehensive tests show that our \texttt{IBG} approach considerably improves both the models' performance and interpretability by identifying sentiment-aware features.
    
[^46]: 因果关系：大型语言模型真正理解因果关系吗？

    Cause and Effect: Can Large Language Models Truly Understand Causality?

    [https://arxiv.org/abs/2402.18139](https://arxiv.org/abs/2402.18139)

    本研究提出了一种名为CARE CA的新型架构，通过结合显式因果检测模块和反事实陈述、以及隐含因果检测模块，旨在增强大型语言模型对因果关系的理解能力。

    

    随着大型语言模型（LLMs）的兴起，理解它们在解读和解释语言所涉及的复杂因果关系的能力和局限性变得至关重要。目前的方法使用明确或隐含的因果推理，然而迫切需要一种统一的方法，将两者结合起来更有效地处理各种因果关系。本研究提出了一种新颖的架构，称为具有反事实分析的上下文感知推理增强（CARE CA）框架，以增强因果推理和可解释性。所提出的框架将 ConceptNet 和反事实陈述中的明确因果检测模块以及通过LLMs进行的隐含因果检测相结合。我们的框架通过一层反事实解释进一步突出LLMs对因果关系的理解。ConceptNet 中的知识提高了多

    arXiv:2402.18139v1 Announce Type: cross  Abstract: With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multi
    
[^47]: DecisionNCE: 通过隐式偏好学习实体多模态表示

    DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning

    [https://arxiv.org/abs/2402.18137](https://arxiv.org/abs/2402.18137)

    本文提出了 DecisionNCE 框架，通过隐式偏好学习实体多模态表示，实现了提取任务进展信息和与语言指令对齐的有效方法

    

    多模态预训练已被证明是自主机器人中表示学习的三大目标：1）提取局部和全局任务进展信息；2）强化视觉表示的时间一致性；3）捕获轨迹级语言基础的有效策略。大部分已有方法通过不同的目标来处理这些问题，往往导致次优解。本文提出了一个通用统一目标，可以同时从图像序列中提取有意义的任务进展信息，并将它们与语言指令无缝对齐。我们发现，通过隐式偏好，在视觉轨迹与其对应的语言指令相比不匹配对更好地对齐时，流行的 Bradley-Terry 模型可以通过适当的奖励重新参数化而变为表示学习。结果产生的 DecisionNCE 框架，类似于 InfoNC

    arXiv:2402.18137v1 Announce Type: cross  Abstract: Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNC
    
[^48]: 拯救英雄伊巴什的遗产：评估四种语言模型在氨基酸语言中的表现

    Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian

    [https://arxiv.org/abs/2402.18121](https://arxiv.org/abs/2402.18121)

    本研究评估了四种语言模型在未被充分探索的氨基酸语言中的适应性、有效性和局限性，并为未来自然语言处理的进展奠定了基础。

    

    本研究评估了四种前沿语言模型在未开发的氨基酸语言中的表现。通过评估，本研究审查了它们在文本生成、语义连贯性和上下文理解方面的适应性，有效性和局限性。揭示这些模型在低资源语言中的表现，为弥合语言差距开辟了道路。通过提供基准和理解挑战，为未来自然语言处理的进展奠定了基础，旨在提升语言模型在类似语言环境中的适用性，标志着语言技术包容性和进步的重要一步。

    arXiv:2402.18121v1 Announce Type: cross  Abstract: This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding. Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps. By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.
    
[^49]: 在大型语言模型中探索多语言人类价值观念：价值观齐整性、跨语言转移性和可控性是否具有一致性？

    Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?

    [https://arxiv.org/abs/2402.18120](https://arxiv.org/abs/2402.18120)

    通过探索多语言人类价值观念，我们证实了大型语言模型中存在多语言人类价值，并揭示了价值对齐能力可以被跨语言控制。

    

    先前在表示工程领域的研究揭示了LLMs在其表示空间中编码概念，主要围绕英语展开。在这项研究中，我们将这一理念扩展到多语境场景，深入探讨LLMs中的多语言人类价值概念。通过我们对7种人类价值、16种语言以及3个具有明显多语特性的LLM系列进行的全面探索，我们从实证角度证实了LLMs中存在多语言人类价值观念。对这些概念的跨语言分析进一步揭示了由于语言资源差异而产生的3个特征：跨语言不一致性、扭曲的语言关系以及高资源语言和低资源语言之间在人类价值概念方面的单向跨语言转移。此外，我们验证了通过利用主导语言作为信息源实现对LLMs价值观齐整性的跨语言控制的可行性。

    arXiv:2402.18120v1 Announce Type: new  Abstract: Prior research in representation engineering has revealed that LLMs encode concepts within their representation spaces, predominantly centered around English. In this study, we extend this philosophy to a multilingual scenario, delving into multilingual human value concepts in LLMs. Through our comprehensive exploration covering 7 types of human values, 16 languages and 3 LLM series with distinct multilinguality, we empirically substantiate the existence of multilingual human values in LLMs. Further cross-lingual analysis on these concepts discloses 3 traits arising from language resource disparities: cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts. Additionally, we validate the feasibility of cross-lingual control over value alignment capabilities of LLMs, leveraging the dominant language as a source 
    
[^50]: UniVS：使用提示作为查询的统一和通用视频分割

    UniVS: Unified and Universal Video Segmentation with Prompts as Queries

    [https://arxiv.org/abs/2402.18115](https://arxiv.org/abs/2402.18115)

    UniVS提出了使用提示作为查询的统一视频分割架构，通过平均化前一帧中目标的提示特征来解码掩码，并引入了基于目标的提示交叉注意层，从而解决了统一视频分割模型的挑战。

    

    尽管统一图像分割（IS）取得了最新进展，但开发统一的视频分割（VS）模型仍然是一项挑战。这主要是因为通用类别指定的VS任务需要检测所有对象并跟踪它们跨连续帧，而由提示引导的VS任务需要在整个视频中重新识别目标并使用视觉/文本提示，使得用相同架构处理不同任务变得困难。我们试图解决这些问题，并提出了一种新颖的统一VS架构，即UniVS，通过使用提示作为查询。UniVS将前一帧中目标的提示特征作为其初始查询平均化，以明确解码掩码，并在掩码解码器中引入基于目标的提示交叉注意层，以在内存池中整合提示特征。通过将先前帧中的实体的预测掩码作为它们的视觉提示，UniVS将不同的VS任务转换成通用的VS问题。

    arXiv:2402.18115v1 Announce Type: cross  Abstract: Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries. UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool. By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS ta
    
[^51]: 小而有趣：一种基于反馈的幽默精馏方法

    Small But Funny: A Feedback-Driven Approach to Humor Distillation

    [https://arxiv.org/abs/2402.18113](https://arxiv.org/abs/2402.18113)

    通过将大型语言模型分配为“教师”生成数据和“评论家”评估学生表现的双重角色，研究表明这种基于反馈的方法在幽默生成任务中取得了更高的性能。

    

    大型语言模型（LLMs）的出现揭示了有潜力的语言生成能力，特别是在执行诸如复杂推理和创意写作之类的任务方面。因此，通过模仿教师回答的方式进行精馏已经成为一种流行的技术，用于将LLMs中的知识转移到更易访问的小型语言模型（SLMs）中。虽然这对于简单任务效果很好，但在需要复杂语言理解和创造力的任务上存在实质性的表现差距，比如幽默生成。我们假设这种差距可能源自于创造性任务可能单凭模仿学习是很难的，并探讨一种涉及到教师额外指导的方法，能否产生更高的性能。为了解决这个问题，我们研究了将LLM分配双重角色的效果-作为生成数据的“教师”，以及作为评估学生表现的“评论家”。我们的实验结果表明，这种方法在幽默生成任务上确实产生了更高的性能。

    arXiv:2402.18113v1 Announce Type: cross  Abstract: The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a "teacher" generating data, as well as a "critic" evaluating the student's performance. Our ex
    
[^52]: 评估语法错误校正的有效性：在日本背景下的人类评估方法

    Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context

    [https://arxiv.org/abs/2402.18101](https://arxiv.org/abs/2402.18101)

    评估了一种日本背景下人类评估方法中最先进语法错误检测和校正模型的性能，在错误校正方面表现出较高的准确性和保守性，在错误检测方面表现出高精确度和调整召回率。

    

    在这项研究中，我们使用自动注释工具包ERRANT，评估了最先进的序列标记语法错误检测和校正模型（SeqTagger）在日本大学生写作样本上的表现。首先，我们将SeqTagger的性能与人类专家校正作为基准来评估错误校正的性能。然后采用人工标注方法来评估Seqtagger在错误检测方面的表现，使用写作数据集的一个子集。结果显示，在整个数据集上，错误校正的精确度为63.66%，召回率为20.19%。对于子集，在手动排除了语义和机械错误等不相关错误后，模型在错误检测方面显示出97.98%的调整精确度和42.98%的调整召回率，表明模型具有很高的准确性，但也表现出保守性。对模型未检测到的错误进行的主题分析揭示了限定词和冠词等问题。

    arXiv:2402.18101v1 Announce Type: new  Abstract: In this study, we evaluated the performance of the state-of-the-art sequence tagging grammar error detection and correction model (SeqTagger) using Japanese university students' writing samples. With an automatic annotation toolkit, ERRANT, we first evaluated SeqTagger's performance on error correction with human expert correction as the benchmark. Then a human-annotated approach was adopted to evaluate Seqtagger's performance in error detection using a subset of the writing dataset. Results indicated a precision of 63.66% and a recall of 20.19% for error correction in the full dataset. For the subset, after manual exclusion of irrelevant errors such as semantic and mechanical ones, the model shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for error detection, indicating the model's high accuracy but also its conservativeness. Thematic analysis on errors undetected by the model revealed that determiners and article
    
[^53]: 编辑医学大型语言模型的事实知识和解释能力

    Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models

    [https://arxiv.org/abs/2402.18099](https://arxiv.org/abs/2402.18099)

    提出了一种新型的Layer-wise Scalable Adapter策略MedLaSA，用于编辑医学大型语言模型，能精确修改医学知识并解释事实，解决了当前方法在医学知识特殊化和复杂性方面的困难。

    

    模型编辑旨在精确修改大型语言模型（LLMs）对特定知识的行为，同时保持不相关的知识不变。已经证明，这种方法在解决LLMs中的幻觉和过时问题方面是有效的。因此，它可以提高LLMs在许多关键领域（例如医学领域）中的应用，其中幻觉是不可容忍的。本文提出两项模型编辑研究，并在医学领域验证它们：（1）直接编辑医学事实知识和（2）编辑对事实的解释。同时，我们观察到当前的模型编辑方法在医学知识的特殊化和复杂性方面存在困难。因此，我们提出了MedLaSA，一种新型的适用于医学模型编辑的分层可扩展适配器策略。它采用因果追踪来识别神经元中知识的精确位置，然后将可扩展适配器引入到LLMs的密集层中。

    arXiv:2402.18099v1 Announce Type: cross  Abstract: Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LL
    
[^54]: Polos：从人类反馈学习的多模式度量用于图像字幕生成

    Polos: Multimodal Metric Learning from Human Feedback for Image Captioning

    [https://arxiv.org/abs/2402.18091](https://arxiv.org/abs/2402.18091)

    提出了一种使用多模态输入和基于人类反馈的框架训练的自动评估指标Polos，旨在有效开发图像字幕生成模型。

    

    建立一个与人类判断紧密对齐的自动评估指标对于有效开发图像字幕生成模型至关重要。最近的数据驱动指标表现出比传统指标如CIDEr更强的与人类判断相关性；然而，它们缺乏足够的能力来处理幻觉，并且跨各种图像和文本泛化部分是因为它们仅仅使用从与图像字幕生成评估无关的任务学习的嵌入计算标量相似性。在这项研究中，我们提出了Polos，一种用于图像字幕生成模型的监督自动评估指标。Polos从多模式输入中计算得分，使用一个并行特征提取机制，利用通过大规模对比学习训练的嵌入。为了训练Polos，我们引入了基于人类反馈的多模态度量学习（M$^2$LHF）框架，用于开发度量方法。

    arXiv:2402.18091v1 Announce Type: cross  Abstract: Establishing an automatic evaluation metric that closely aligns with human judgments is essential for effectively developing image captioning models. Recent data-driven metrics have demonstrated a stronger correlation with human judgments than classic metrics such as CIDEr; however they lack sufficient capabilities to handle hallucinations and generalize across diverse images and texts partially because they compute scalar similarities merely using embeddings learned from tasks unrelated to image captioning evaluation. In this study, we propose Polos, a supervised automatic evaluation metric for image captioning models. Polos computes scores from multimodal inputs, using a parallel feature extraction mechanism that leverages embeddings trained through large-scale contrastive learning. To train Polos, we introduce Multimodal Metric Learning from Human Feedback (M$^2$LHF), a framework for developing metrics based on human feedback. We co
    
[^55]: 关于在信息提取中利用银标准数据进行零样本分类任务的研究

    On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction

    [https://arxiv.org/abs/2402.18061](https://arxiv.org/abs/2402.18061)

    本研究提出了一个新框架Clean-LaVe，旨在利用银标准数据来增强零样本分类性能。

    

    在信息提取（IE）领域，监督分类方法的卓越性能严重依赖于大量的黄金标准数据。最近的零样本分类方法将任务转化为其他NLP任务（例如，文本蕴涵），并使用这些NLP任务的现成模型直接对测试数据进行推理，而无需使用大量的IE注释数据。这些方法的一个潜在有价值的副产品是大规模的银标准数据，即其他NLP任务的现成模型生成的伪标记数据。然而，对于这些数据的利用并没有进一步的研究。本文提出了一个新框架Clean-LaVe，旨在利用银标准数据来增强零样本分类性能。Clean-LaVe包括四个阶段：（1）获取银标准数据；（2）从银标准数据中识别相对干净的数据；（3）使用干净数据微调现成模型；

    arXiv:2402.18061v1 Announce Type: cross  Abstract: The superior performance of supervised classification methods in the information extraction (IE) area heavily relies on a large amount of gold standard data. Recent zero-shot classification methods converted the task to other NLP tasks (e.g., textual entailment) and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks. However, there is no further investigation into the use of these data. In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) Finetuning the off-the-shelf model using clea
    
[^56]: 在回答和解释具有挑战性的医学问题上对大型语言模型的基准测试

    Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions

    [https://arxiv.org/abs/2402.18060](https://arxiv.org/abs/2402.18060)

    在回答医学问题方面，大型语言模型在处理具有挑战性的实际临床案例上的表现是关键，因此构建了两个结构化数据集进行评估。

    

    LLMs在回答医学问题方面表现出色，例如通过医学执照考试。然而，大多数现有的基准测试依赖于委员会考试问题或一般医学问题，无法捕捉真实临床案例的复杂性。此外，缺乏答案的参考解释阻碍了对模型解释的评估，这对支持医生做出复杂的医疗决策至关重要。为解决这些挑战，我们构建了两个新数据集：JAMA临床挑战和Medbullets。JAMA临床挑战包含基于具有挑战性的临床案例的问题，而Medbullets包含类似USMLE Step 2&3风格的临床问题。两个数据集均以多项选择问题-回答任务的结构化形式呈现，每个问题都附有专家撰写的解释。我们使用各种提示在这两个数据集上评估了四个LLMs。实验表明

    arXiv:2402.18060v1 Announce Type: new  Abstract: LLMs have demonstrated impressive performance in answering medical questions, such as passing medical licensing examinations. However, most existing benchmarks rely on board exam questions or general medical questions, falling short in capturing the complexity of realistic clinical cases. Moreover, the lack of reference explanations for answers hampers the evaluation of model explanations, which are crucial to supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises USMLE Step 2&3 style clinical questions. Both datasets are structured as multiple-choice question-answering tasks, where each question is accompanied by an expert-written explanation. We evaluate four LLMs on the two datasets using various prompts. Experiments demonstrat
    
[^57]: 具有增强可检测性和语义连贯性的大型语言模型的特定令牌水印技术

    Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models

    [https://arxiv.org/abs/2402.18059](https://arxiv.org/abs/2402.18059)

    提出一种利用多目标优化方法的水印技术，通过轻量级网络生成特定令牌水印logits和分割比率，在保证检测性的同时提升了文本的语义完整性。

    

    大型语言模型生成高质量的响应，潜在地存在误导信息的问题，强调了通过区分人工智能生成和人类撰写的文本来加以规范的必要性。水印技术在这种情况下至关重要，它涉及在LLM推理阶段向文本中嵌入隐藏标记，而这对人类来说是不可感知的。然而，当前的水印算法面临着实现插入水印的可检测性和生成文本的语义完整性两方面的挑战，增强其中一个方面常常会损害另一个方面。为了克服这一问题，我们引入了一种新颖的多目标优化（MOO）方法，用于水印技术，利用轻量级网络生成特定令牌水印logits和分割比率。通过利用MOO来优化检测和语义目标函数，我们的方法同时实现了可检测性和语义完整性。实验结果表明，我们的方法在...

    arXiv:2402.18059v1 Announce Type: cross  Abstract: Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that ou
    
[^58]: 生成引文文本的情境化处理

    Contextualizing Generated Citation Texts

    [https://arxiv.org/abs/2402.18054](https://arxiv.org/abs/2402.18054)

    生成引文文本时，不仅考虑引文本身，还要考虑整个上下文窗口，包括目标引文，以解决生成的引文忽视引文上下文焦点的问题。

    

    arXiv:2402.18054v1 公告类型：新的 摘要：抽象引文文本生成通常被构建为一个填充任务，即使用一个序列到序列模型来训练生成一个引文，给出一个参考论文和目标周围的上下文窗口；生成的引文应该是有关参考论文的简要讨论，与引文上下文相关。然而，通过检查最近基于LED的引文生成系统，我们发现许多生成的引文是参考论文主要贡献的通用摘要，忽视了引文语境关注的不同主题。为了解决这个问题，我们提出对引文文本生成任务进行简单修改：生成目标不仅是引文本身，还包括整个上下文窗口，包括目标引文。这种方法可以轻松应用于任何抽象的引文生成系统，我们的实验结果表明，以这种方式训练更受人类读者和欢迎。

    arXiv:2402.18054v1 Announce Type: new  Abstract: Abstractive citation text generation is usually framed as an infilling task, where a sequence-to-sequence model is trained to generate a citation given a reference paper and the context window around the target; the generated citation should be a brief discussion of the reference paper as it relates to the citing context. However, examining a recent LED-based citation generation system, we find that many of the generated citations are generic summaries of the reference papers main contribution, ignoring the citation contexts focus on a different topic. To address this problem, we propose a simple modification to the citation text generation task: the generation target is not only the citation itself, but the entire context window, including the target citation. This approach can be easily applied to any abstractive citation generation system, and our experimental results show that training in this way is preferred by human readers and al
    
[^59]: MEGAnno+: 一个人类-LLM协作标注系统

    MEGAnno+: A Human-LLM Collaborative Annotation System

    [https://arxiv.org/abs/2402.18050](https://arxiv.org/abs/2402.18050)

    MEGAnno+提供了一个人类-LLM协作标注系统，旨在通过人类和大语言模型（LLMs）的联合工作产生可靠且高质量的标签。

    

    大语言模型（LLMs）可以比人类更快速、更便宜地为各种自然语言处理任务标记数据。尽管它们很强大，但LLMs可能在理解复杂、社会文化或领域特定上下文方面存在缺陷，这可能导致错误的注释。因此，我们提倡一种人类和LLMs共同合作的方法，以产生可靠且高质量的标签。我们提出MEGAnno+，这是一个人类-LLM协作的标注系统，提供有效的LLM代理和注释管理，便捷且稳固的LLM标注，以及人类对LLM标签的探索性验证。

    arXiv:2402.18050v1 Announce Type: new  Abstract: Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.
    
[^60]: 用局部内在维度表征大型语言模型生成的真实性

    Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension

    [https://arxiv.org/abs/2402.18048](https://arxiv.org/abs/2402.18048)

    本研究旨在通过使用局部内在维度来量化大型语言模型生成文本的真实性，实验证实了该方法的有效性。

    

    我们研究如何表征和预测从大型语言模型（LLMs）生成的文本的真实性，这在建立人类与LLMs之间的信任关系中起着至关重要的作用。我们建议通过研究内部激活并利用模型激活的局部内在维度（LID）来量化LLM的真实性。通过对四个问答（QA）数据集的实验，我们展示了我们提出的方法的有效性。此外，我们研究了LLMs中的内在维度及其与模型层、自回归语言建模以及LLMs的训练之间的关系，揭示了内在维度可以

    arXiv:2402.18048v1 Announce Type: new  Abstract: We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness ohttps://info.arxiv.org/help/prep#abstractsf our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be
    
[^61]: Multi-FAct: 使用FActScore评估多语言LLM的多区域知识

    Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore

    [https://arxiv.org/abs/2402.18045](https://arxiv.org/abs/2402.18045)

    本文系统评估了多语言LLMs跨语言和地理区域的事实准确性，并发现英语在事实准确性和生成事实数量方面优于其他语言，同时多语言模型存在对来自西方大陆事实信息的偏见。

    

    大型语言模型（LLMs）容易出现事实上的幻觉，生成与已知知识相矛盾的文本。尽管广泛研究了英语中的这一问题，但对于多语言LLMs知之甚少。本文系统评估了多语言LLMs跨语言和地理区域的事实准确性。我们引入了一个新颖的多语言事实评估流程，将FActScore（Min等，2023）改编为多样化语言。我们在九种语言上的分析显示，英语在事实准确性和生成事实数量方面始终表现优异。此外，多语言模型展现出对来自西方大陆的事实信息的偏见。这些发现突显了对改进多语言事实性评估的需求，并强调了LLMs的事实生成中的地理偏见。

    arXiv:2402.18045v1 Announce Type: new  Abstract: Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge. While extensive research has addressed this in English, little is known about multilingual LLMs. This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions. We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages. Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts. Furthermore, multilingual models demonstrate a bias towards factual information from Western continents. These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in LLMs' fact generation.
    
[^62]: 危机对话: 关于能源危机和生活成本的公共辩论分析

    Crisis talk: analysis of the public debate around the energy crisis and cost of living

    [https://arxiv.org/abs/2402.18043](https://arxiv.org/abs/2402.18043)

    本文通过分析能源危机和生活成本的公共辩论，识别了关键议题、新趋势、关键社会行为者以及他们在辩论中的角色，以及与这些议题和行为者相关的情绪。

    

    早在2020年代初期，英国和大部分欧洲受到能源危机的影响，这成为一个突出的媒体话题。它将能源依赖性和可持续发展、经济负担的公平分配以及生活成本、气候变化、风险和可持续性等问题汇集到一个公共辩论中。本文研究了围绕能源危机和生活成本的公共话语，以确定在这场辩论中如何调和这些关键且矛盾的问题，识别参与其中的社会行为者以及他们的角色。我们分析了从2014年1月至2023年3月检索的英国报纸文献语料库。我们应用了各种自然语言处理和数据可视化技术，以确定关键主题、新趋势、关键社会行为者及其在辩论中的角色，以及与这些行为者和主题相关的情绪。我们将自动化技术与手动技术相结合。

    arXiv:2402.18043v1 Announce Type: new  Abstract: A prominent media topic in the UK in the early 2020s is the energy crisis affecting the UK and most of Europe. It brings into a single public debate issues of energy dependency and sustainability, fair distribution of economic burdens and cost of living, as well as climate change, risk, and sustainability. In this paper, we investigate the public discourse around the energy crisis and cost of living to identify how these pivotal and contradictory issues are reconciled in this debate and to identify which social actors are involved and the role they play. We analyse a document corpus retrieved from UK newspapers from January 2014 to March 2023. We apply a variety of natural language processing and data visualisation techniques to identify key topics, novel trends, critical social actors, and the role they play in the debate, along with the sentiment associated with those actors and topics. We combine automated techniques with manual disco
    
[^63]: 大型语言模型数据集：一项全面调查

    Datasets for Large Language Models: A Comprehensive Survey

    [https://arxiv.org/abs/2402.18041](https://arxiv.org/abs/2402.18041)

    本文全面探讨了大型语言模型数据集的不同类型、挑战和未来发展方向。

    

    本文对大型语言模型（LLM）数据集进行了探索，这些数据集在LLMs的显着进展中起着至关重要的作用。数据集类似于维持和培育LLMs发展的根系基础架构。因此，检查这些数据集成为研究中的一个关键主题。为了解决当前缺乏对LLM数据集全面概述和彻底分析的问题，并获得有关它们当前状态和未来趋势的见解，本调查从五个角度整合和分类LLM数据集的基本方面：（1）预训练语料库；（2）指导微调数据集；（3）偏好数据集；（4）评估数据集；（5）传统自然语言处理（NLP）数据集。调查阐明了当前存在的挑战，并指出了未来研究的潜在途径。

    arXiv:2402.18041v1 Announce Type: cross  Abstract: This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a compre
    
[^64]: ResLoRA：低秩适应中的身份残差映射

    ResLoRA: Identity Residual Mapping in Low-Rank Adaption

    [https://arxiv.org/abs/2402.18039](https://arxiv.org/abs/2402.18039)

    ResLoRA提出了在训练中添加残余路径并在推断过程中消除这些额外路径的方法，实现了更好的结果，比LoRA更加高效。

    

    作为最流行的参数高效微调（PEFT）方法之一，低秩适应（LoRA）通常应用于微调大型语言模型（LLMs）。然而，在原始模型中由于长计算路径，在有效而迅速地更新LoRA块的权重方面存在挑战。为了解决这个问题，我们提出了ResLoRA，这是LoRA的改进框架。通过在训练过程中添加残余路径，并使用合并方法在推断过程中消除这些额外路径，我们的方法可以在较少的训练步骤内取得更好的结果，而与LoRA相比，不需要额外的可训练参数或推断成本。对 NLG、NLU 和文本到图像任务上的实验表明了我们方法的有效性。据我们所知，ResLoRA是首个将残余路径与LoRA结合的工作。我们方法的代码可在 https://github.com/microsoft/LMOps/tree/main/reslora 上获取。

    arXiv:2402.18039v1 Announce Type: cross  Abstract: As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at https://github.com/microsoft/LMOps/tree/main/reslora .
    
[^65]: 由大型语言模型引导的语料库查询扩展

    Corpus-Steered Query Expansion with Large Language Models

    [https://arxiv.org/abs/2402.18031](https://arxiv.org/abs/2402.18031)

    通过引入语料库引导的查询扩展（CSQE），结合大型语言模型的知识增强扩展，改善了查询与目标文档之间的相关性预测。

    

    最近的研究表明，由大型语言模型（LLMs）生成的查询扩展可以通过生成能够回答查询的假设文档而显着增强信息检索系统。然而，由于LLMs的有限内在知识，扩展与检索语料库之间的不对齐导致问题，如幻觉和过时信息。受伪相关反馈（PRF）启发，我们引入了语料库引导的查询扩展（CSQE）来促进嵌入在语料库中的知识的整合。CSQE利用LLMs的相关性评估能力系统地识别最初检索到的文档中的关键句。这些由语料库产生的文本随后与LLM知识增强扩展一起用于扩展查询，改善查询与目标文档之间的相关性预测。广泛的实验证明CSQE明显提高了信息检索结果的质量。

    arXiv:2402.18031v1 Announce Type: cross  Abstract: Recent studies demonstrate that query expansions generated by large language models (LLMs) can considerably enhance information retrieval systems by generating hypothetical documents that answer the queries as expansions. However, challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs. Inspired by Pseudo Relevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to promote the incorporation of knowledge embedded within the corpus. CSQE utilizes the relevance assessing capability of LLMs to systematically identify pivotal sentences in the initially-retrieved documents. These corpus-originated texts are subsequently used to expand the query together with LLM-knowledge empowered expansions, improving the relevance prediction between the query and the target documents. Extensive exp
    
[^66]: 大型语言模型是否反映认知语言处理？

    Do Large Language Models Mirror Cognitive Language Processing?

    [https://arxiv.org/abs/2402.18023](https://arxiv.org/abs/2402.18023)

    本文提出了一种新颖方法，通过将大型语言模型（LLMs）的表示与人类认知信号联系起来，评估LLMs模拟认知语言处理的效果。

    

    大型语言模型（LLMs）在文本理解和逻辑推理方面展现出卓越能力，甚至在许多认知任务中实现甚至超越人类水平的表现。由于LLMs是从人类语言认知的大量文本产出中训练出来的，自然而然地会问LLMs是否反映认知语言处理，或LLMs在多大程度上类似于认知语言处理。本文提出了一种新颖的方法，用于连接LLMs表征和人类认知信号，以评估LLMs如何有效地模拟认知语言处理。我们采用表征相似性分析（RSA）来衡量16种主流LLMs与大脑fMRI信号之间的对齐程度。我们在实验中探讨了各种因素（例如模型规模、对齐训练、指导附加）对LLM-大脑对齐的影响。实验结果表明，模型规模与正相关

    arXiv:2402.18023v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achiving or even surpassing human-level performance in numerous cognition tasks. As LLMs are trained from massive textual outputs of human language cognition, it is natural to ask whether LLMs mirror cognitive language processing. Or to what extend LLMs resemble cognitive language processing? In this paper, we propose a novel method that bridge between LLM representations and human cognition signals to evaluate how effectively LLMs simulate cognitive language processing. We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream LLMs and fMRI signals of the brain. We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such LLM-brain alignment. Experimental results indicate that model scaling is positively cor
    
[^67]: 基于LLM的多轮对话系统最新进展综述

    A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems

    [https://arxiv.org/abs/2402.18013](https://arxiv.org/abs/2402.18013)

    这项调查综述了基于LLM的多轮对话系统的研究，并重点介绍了LLMs的应用和最新进展，对开放领域对话和任务导向对话系统进行了涵盖，并讨论了相关数据集和评估指标，以及未来研究方向和问题。

    

    这项调查全面回顾了关于多轮对话系统的研究，特别侧重于基于大型语言模型（LLMs）的多轮对话系统。本文旨在（a）总结现有的LLMs和适应LLMs进行下游任务的方法；（b）详细阐述多轮对话系统的最新进展，涵盖基于LLM的开放领域对话（ODD）和任务导向对话（TOD）系统，以及数据集和评估指标；（c）讨论由于LLMs的发展和对多轮对话系统不断增加的需求而产生的未来重点和最新研究问题。

    arXiv:2402.18013v1 Announce Type: cross  Abstract: This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.
    
[^68]: 探索科学情感总结的多文档信息整合

    Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization

    [https://arxiv.org/abs/2402.18005](https://arxiv.org/abs/2402.18005)

    通过人类元审阅者的情感整合框架，提出评估指标并在实验中验证，指导LLMs生成科学元审阅的逻辑被验证可行。

    

    现代自然语言生成系统具有生成多个文档的合理摘要的能力；然而，现在尚不确定模型是否真正具有整合信息的能力来生成总结，尤其是对那些包含个人意见信息的源文档。为了使科学情感总结更加扎实，我们假设在同行评审中，人类元审阅者遵循情感整合的三层框架来撰写元审阅，并且这代表了在元审阅生成过程中总结科学情感的逻辑。通过人类注释，验证了这一框架。基于该框架，我们提出了评估指标来评估生成的元审阅的质量，并且在广泛实验中发现，当我们将其作为LLMs生成元审阅的提示时，情感整合框架的假设在经验上是行得通的。

    arXiv:2402.18005v1 Announce Type: cross  Abstract: Modern natural language generation systems with LLMs exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information. To make scientific sentiment summarization more grounded, we hypothesize that in peer review human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews and it represents the logic of summarizing scientific sentiments in meta-review generation. The framework is validated via human annotation. Based on the framework, we propose evaluation metrics to assess the quality of generated meta-reviews, and we find that the hypothesis of the sentiment consolidation framework works out empirically when we incorporate it as prompts for LLMs to generate meta-reviews in extensive experiments.
    
[^69]: FlattenQuant: 使用分张量量化打破大型语言模型推断计算限制

    FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization

    [https://arxiv.org/abs/2402.17985](https://arxiv.org/abs/2402.17985)

    FlattenQuant方法通过展平张量中的大通道，实现了低比特每张量量化，降低了准确性损失

    

    大型语言模型(LLMs)在各种任务中展现出领先的性能，然而，推断的延迟和LLMs的大GPU内存消耗限制了它们的部署性能。本文提出了一种名为FlattenQuant的方法，通过对张量中的大通道进行展平来显著降低张量的最大值，实现了低比特每张量量化，减小了准确性损失。

    arXiv:2402.17985v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance across various tasks. However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance. Recently, there have been some efficient attempts to quantize LLMs, yet inference with large batch size or long sequence still has the issue of being compute-bound. Fine-grained quantization methods have showcased their proficiency in achieving low-bit quantization for LLMs, while requiring FP16 data type for linear layer computations, which is time-consuming when dealing with large batch size or long sequence. In this paper, we introduce a method called FlattenQuant, which significantly reduces the maximum value of the tensor by flattening the large channels in the tensor, to achieve low bit per-tensor quantization with minimal accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits to achieve 48.29% 
    
[^70]: M3-VRD: 多模态多任务多教师视觉丰富表单文档理解

    M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding

    [https://arxiv.org/abs/2402.17983](https://arxiv.org/abs/2402.17983)

    这个模型是一个多模态、多任务、多教师的联合细粒度知识蒸馏模型，通过微妙协作令牌和实体表示，处理复杂的表单文档，引入新的损失函数改进知识蒸馏过程，在处理视觉复杂表单文档的结构和内容上表现出色。

    

    这篇论文提出了一个突破性的多模态、多任务、多教师联合细粒度知识蒸馏模型，用于视觉丰富的表单文档理解。该模型旨在通过促进令牌和实体表示之间的微妙相关性来利用细粒度和粗粒度级别的见解，解决表单文档固有的复杂性。此外，我们引入了新的跨细粒度和跨粗粒度损失函数，以进一步改进多教师知识蒸馏传递过程，呈现分布差距和对表单文档的统一理解。通过在公开可用的表单文档理解数据集上进行全面评估，我们提出的模型始终表现出色地优于现有基线，展示了其在处理复杂视觉表单文档的复杂结构和内容方面的功效。

    arXiv:2402.17983v1 Announce Type: new  Abstract: This paper presents a groundbreaking multimodal, multi-task, multi-teacher joint-grained knowledge distillation model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher knowledge distillation transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents.
    
[^71]: 增强大型语言模型事实性的关键标记协同解码

    Collaborative decoding of critical tokens for boosting factuality of large language models

    [https://arxiv.org/abs/2402.17982](https://arxiv.org/abs/2402.17982)

    引入协同解码框架通过关键标记概念利用预训练模型中的高事实性，设计关键标记分类器进行模型选择，有效降低幻觉生成。

    

    大型语言模型的常见训练流程包括预训练、微调和对齐阶段，产生相应的预训练模型和微调模型。微调和对齐模型表现出了改进的指令遵循和安全生成能力，然而它们在世界事实方面的能力受微调过程的影响。此外，在生成过程中常见的使用采样的做法也增加了幻觉的可能性。在本研究中，我们引入了一种协同解码框架，通过关键标记的概念利用预训练模型中的高事实性。我们首先设计了一个关键标记分类器来决定下一个标记使用哪个模型，然后使用不同的解码策略生成下一个标记。对不同模型和数据集进行的实验表明，我们的解码框架能够减少模型产生幻觉的情况。

    arXiv:2402.17982v1 Announce Type: new  Abstract: The most common training pipeline for large language models includes pretraining, finetuning and aligning phases, with their respective resulting models, such as the pretrained model and the finetuned model. Finetuned and aligned models show improved abilities of instruction following and safe generation, however their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination. In this work, we introduce a collaborative decoding framework to harness the high factuality within pretrained models through the concept of critical tokens. We first design a critical token classifier to decide which model to use for the next token, and subsequently generates the next token using different decoding strategies. Experiments with different models and datasets show that our decoding framework is able to reduce model hall
    
[^72]: 一张图片搞定：大型多模态模型是图片内学习者

    All in a Single Image: Large Multimodal Models are In-Image Learners

    [https://arxiv.org/abs/2402.17971](https://arxiv.org/abs/2402.17971)

    这项研究引入了一种名为图片内学习（I$^2$L）的新型上下文学习机制，将演示示例、视觉线索和指令合并到一个图片中，以提升GPT-4V的能力，并通过整合图像处理、理解和推理的能力来取得多个优点

    

    本文介绍了一种名为图片内学习（I$^2$L）的新型上下文学习（ICL）机制，将演示示例、视觉线索和指令合并到一张图片中，以增强GPT-4V的能力。与以往依赖将图像转换为文本或将视觉输入融入语言模型的方法不同，I$^2$L将所有信息整合到一张图片中，主要利用图像处理、理解和推理能力。这有几个优点：避免了对复杂图像的不准确文本描述，提供了在定位演示示例时的灵活性，减少了输入负担，并通过消除对多个图片和冗长文本的需求来避免超过输入限制。为了进一步结合不同ICL方法的优势，我们引入了一种自动策略，用于选择给定任务中数据示例的适当ICL方法。我们在MathVi上进行了实验

    arXiv:2402.17971v1 Announce Type: cross  Abstract: This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVi
    
[^73]: 一种用于共情响应生成的迭代关联记忆模型

    An Iterative Associative Memory Model for Empathetic Response Generation

    [https://arxiv.org/abs/2402.17959](https://arxiv.org/abs/2402.17959)

    提出了一种用于共情响应生成的迭代关联记忆模型，采用二阶交互注意机制迭代地捕捉相关词语，实现准确、细致地理解话语。

    

    共情响应生成是理解对话话语中的认知和情感状态，并生成恰当回应。心理学理论认为，理解情感和认知状态需要迭代地捕捉和理解对话话语之间的相关词语。然而，现有方法将对话话语视为长序列或独立话语来理解，往往会忽视它们之间的关联词语。为解决这一问题，我们提出了一种用于共情响应生成的迭代关联记忆模型（IAMM）。具体来说，我们采用一种新颖的二阶交互注意机制，迭代地捕捉对话话语、情境、对话历史和记忆模块（用于存储相关词语）之间的重要关联词语，从而准确而细致地理解话语。

    arXiv:2402.17959v1 Announce Type: new  Abstract: Empathetic response generation is to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Em
    
[^74]: 自动语音识别的多语言语音模型存在性别差距

    Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps

    [https://arxiv.org/abs/2402.17954](https://arxiv.org/abs/2402.17954)

    多语言语音模型在自动语音识别中表现出性别差距，且在不同语言中受益的性别群体各不相同。

    

    当前的语音识别方法使用多任务、多语言模型来进行诸如自动语音识别（ASR）的语音任务，使其适用于许多语言而不需要实质性的更改。然而，广泛的语言覆盖仍然可能掩盖语言内部存在的性别差距。本文系统评估多语言ASR系统在性别表现差距上的情况。在19种语言的三个数据集上使用两种流行模型，跨越七个语言家族，我们发现明显的性别差异。不过，不同语言中受益的群体各不相同。尽管在语音学变量（音高、说话速度等）上各群体间没有显著差异，但探索模型内部状态却揭示了探查性能和性别表现差距之间的负相关关系。即，在某种语言中更容易区分说话者性别，模型就更偏向于女性说话者。我们的结果显示组别差异

    arXiv:2402.17954v1 Announce Type: new  Abstract: Current voice recognition approaches use multi-task, multilingual models for speech tasks like Automatic Speech Recognition (ASR) to make them applicable to many languages without substantial changes. However, broad language coverage can still mask performance gaps within languages, for example, across genders. We systematically evaluate multilingual ASR systems on gendered performance gaps. Using two popular models on three datasets in 19 languages across seven language families, we find clear gender disparities. However, the advantaged group varies between languages. While there are no significant differences across groups in phonetic variables (pitch, speaking rate, etc.), probing the model's internal states reveals a negative correlation between probe performance and the gendered performance gap. I.e., the easier to distinguish speaker gender in a language, the more the models favor female speakers. Our results show that group dispar
    
[^75]: 无梯度自适应全局剪枝用于预训练语言模型

    Gradient-Free Adaptive Global Pruning for Pre-trained Language Models

    [https://arxiv.org/abs/2402.17946](https://arxiv.org/abs/2402.17946)

    提出了自适应全局剪枝（AdaGP）框架，通过重新定义全局剪枝过程为可管理的协调子问题，实现对大型语言模型的资源高效优化，显著提高性能。

    

    大型语言模型（LLMs）如LLaMA和GPT在自然语言处理中的转变性影响受到它们计算需求过高的限制。剪枝作为一种关键的压缩策略出现，引入稀疏性以增强内存和计算效率。然而，传统的全局剪枝对LLMs来说由于可扩展性问题而不实用，而本地剪枝，尽管效率高，却导致次优解决方案。为解决这些挑战，我们提出了自适应全局剪枝（AdaGP），这是一个重新定义全局剪枝处理为可管理的协调子问题的新框架，可以实现资源有效的全局最优化优化。AdaGP的方法将LLMs概念化为一系列模块化函数，并利用辅助变量进行问题分解，不仅便于在LLMs上实现实际应用，而且显示出显著的性能改进。

    arXiv:2402.17946v1 Announce Type: new  Abstract: The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, part
    
[^76]: 大型语言模型在表格数据上的应用--一项调查

    Large Language Models on Tabular Data -- A Survey

    [https://arxiv.org/abs/2402.17944](https://arxiv.org/abs/2402.17944)

    该研究综述了大型语言模型在处理表格数据上的应用，包括关键技术、指标、数据集、模型和优化方法，为未来研究方向提供了启示。

    

    大型语言模型在表格数据建模方面的应用取得了突破性进展，包括预测、表格数据合成、问答和表格理解等多种任务。每个任务都带来独特的挑战和机遇。然而，目前缺乏对该研究领域中关键技术、指标、数据集、模型和优化方法的全面审查。本调查旨在填补这一空白，总结并比较这些领域中的最新进展，提供对数据集、指标和方法论的全面调查和分类。它识别了现有文献中的优势、局限性、未开发领域和空白，同时为这一重要且快速发展的领域的未来研究方向提供了一些见解。它还提供了相关的代码和数据集引用。

    arXiv:2402.17944v1 Announce Type: new  Abstract: Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehen
    
[^77]: EmMark: 针对嵌入式量化大型语言模型的知识产权保护的鲁棒性水印

    EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models

    [https://arxiv.org/abs/2402.17938](https://arxiv.org/abs/2402.17938)

    EmMark是一种新型水印框架，用于保护嵌入式大型语言模型的知识产权，在鲁棒性和模型质量维持之间实现平衡，成功应对了IP盗窃风险，具有100%的水印提取成功率，并抵御了水印删除和伪造攻击。

    

    本文介绍了EmMark，一种新颖的水印框架，用于保护部署在资源受限边缘设备上的嵌入式大型语言模型的知识产权（IP）。为了解决恶意最终用户带来的IP盗窃风险，EmMark使所有者能够通过查询带有水印的模型权重并匹配插入的签名来验证所有权。EmMark的创新在于其战略性水印权重参数选择，确保了鲁棒性并保持模型质量。OPT和LLaMA-2系列模型的大量概念验证评估展示了EmMark的准确性，实现了在保留模型性能的同时，水印提取成功率达到100%。EmMark还展示了其对抗水印去除和篡改攻击的韧性。

    arXiv:2402.17938v1 Announce Type: cross  Abstract: This paper introduces EmMark,a novel watermarking framework for protecting the intellectual property (IP) of embedded large language models deployed on resource-constrained edge devices. To address the IP theft risks posed by malicious end-users, EmMark enables proprietors to authenticate ownership by querying the watermarked model weights and matching the inserted signatures. EmMark's novelty lies in its strategic watermark weight parameters selection, nsuring robustness and maintaining model quality. Extensive proof-of-concept evaluations of models from OPT and LLaMA-2 families demonstrate EmMark's fidelity, achieving 100% success in watermark extraction with model performance preservation. EmMark also showcased its resilience against watermark removal and forging attacks.
    
[^78]: 从多模态输入获取语言知识

    Acquiring Linguistic Knowledge from Multimodal Input

    [https://arxiv.org/abs/2402.17936](https://arxiv.org/abs/2402.17936)

    语言模型在获取语言知识时表现出较低的数据效率，研究发现这部分原因可能是由于缺乏多模态输入和接地。他们通过对FLAVA模型进行消融研究来验证假设，并试图通过多任务预训练来减少灾难性遗忘。

    

    与儿童相比，语言模型（LMs）在获取语言时显示出明显较低的数据效率。本文提交给BabyLM挑战（Warstadt 等人，2023），我们验证了这一数据效率差距部分是由于典型语言模型学习环境中缺乏多模态输入和接地引起的假设。尽管先前针对这一问题的研究发现，多模态训练甚至可能对仅语言表现造成伤害，我们推测这些发现可以归因于对标题数据微调导致复杂语言的灾难性遗忘。为验证我们的假设，我们在FLAVA（Singh 等人，2022）上进行消融研究，这是一个多模态视觉语言模型，独立地变化文本和视觉输入的数量，以量化在不同数据范围下视觉输入可以弥补多少文本数据（如果有的话）。我们旨在通过多任务预训练来限制灾难性遗忘。

    arXiv:2402.17936v1 Announce Type: new  Abstract: In contrast to children, language models (LMs) exhibit considerably inferior data efficiency when acquiring language. In this submission to the BabyLM Challenge (Warstadt et al., 2023), we test the hypothesis that this data efficiency gap is partly caused by a lack of multimodal input and grounding in the learning environment of typical language models. Although previous work looking into this question found that multimodal training can even harm language-only performance, we speculate that these findings can be attributed to catastrophic forgetting of complex language due to fine-tuning on captions data. To test our hypothesis, we perform an ablation study on FLAVA (Singh et al., 2022), a multimodal vision-and-language model, independently varying the volume of text and vision input to quantify how much text data (if any) can be offset by vision at different data scales. We aim to limit catastrophic forgetting through a multitask pretra
    
[^79]: 用特征化低秩混合进行多任务多语言模型适应

    Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures

    [https://arxiv.org/abs/2402.17934](https://arxiv.org/abs/2402.17934)

    提出了一种名为FLix的新型参数高效微调方法，适用于多任务多语言调整，通过关联每个独特数据集特征与其低秩权重更新参数，实现了更好的泛化能力和性能表现。

    

    预训练大型语言模型（LLMs）适应数十甚至数百种人类语言的各种下游任务在计算上是昂贵的。参数高效微调（PEFT）通过只调整少量参数显著减少了适应成本。然而，直接将像 LoRA（Hu 等人，2022）这样的 PEFT 方法应用于不同数据集混合可能导致性能次优，原因在于有限的参数容量和不同数据集之间的负面互相影响。在这项工作中，我们提出了特征化低秩混合（FLix），这是一种针对有效的多任务多语言调整的新型 PEFT 方法。FLix将每个独特数据集特征（例如数据集的语言或任务）与其自己的低秩权重更新参数相关联。通过为每个数据集组合特定于特征的参数，FLix能够适应多种数据集混合，并更好地泛化到未见数据集。我们的实验表明，FLix 可以在提供更好性能的同时显著减少适应成本。

    arXiv:2402.17934v1 Announce Type: cross  Abstract: Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning. FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads t
    
[^80]: 通过合作语言引导逆向规划实现实用指令跟随和目标辅助

    Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning

    [https://arxiv.org/abs/2402.17930](https://arxiv.org/abs/2402.17930)

    本文介绍了一种名为合作语言引导逆向计划搜索（CLIPS）的贝叶斯代理架构，用于实现实用指令跟随和目标辅助，能够通过多模态贝叶斯推断，利用大型语言模型评估指令的可能性以实现实用目标达成成本最小化。

    

    人们经常给出在缺乏进一步上下文的情况下意义模糊的指令，期望他们的行动或目标能消除不明确的意图。我们如何构建能够以灵活、与上下文相关的方式遵循这类指令的辅助代理呢？本文介绍了一种名为合作语言引导逆向计划搜索（CLIPS）的贝叶斯代理架构，用于实现实用指令跟随和目标辅助。我们的代理通过将人类建模为一个合作规划者，将共同计划与助手进行通信，然后通过动作和语言执行多模态贝叶斯推断，利用大型语言模型（LLMs）评估在假设计划下给出的指令的可能性。在获得这一后验分布后，我们的助手通过行动来最小化期望目标实现成本，使其能够实用地遵循含糊的指令，并即使在对指令不确定时也能提供有效的辅助。

    arXiv:2402.17930v1 Announce Type: new  Abstract: People often give instructions whose meaning is ambiguous without further context, expecting that their actions or goals will disambiguate their intentions. How can we build assistive agents that follow such instructions in a flexible, context-sensitive manner? This paper introduces cooperative language-guided inverse plan search (CLIPS), a Bayesian agent architecture for pragmatic instruction following and goal assistance. Our agent assists a human by modeling them as a cooperative planner who communicates joint plans to the assistant, then performs multimodal Bayesian inference over the human's goal from actions and language, using large language models (LLMs) to evaluate the likelihood of an instruction given a hypothesized plan. Given this posterior, our assistant acts to minimize expected goal achievement cost, enabling it to pragmatically follow ambiguous instructions and provide effective assistance even when uncertain about the g
    
[^81]: 通过对抗攻击实现抗LLM的数学问题生成

    LLM-Resistant Math Word Problem Generation via Adversarial Attacks

    [https://arxiv.org/abs/2402.17916](https://arxiv.org/abs/2402.17916)

    本研究提出了一种新方法，通过生成保留原问题结构难度但针对LLMs无解的对抗性示例，有效地降低了LLMs的数学问题解决能力。

    

    大型语言模型（LLMs）显著改变了教育领域。本文探讨了一种新的范例，生成对抗性示例，以确保公平评估，这些示例保留了原始问题的结构和难度，但LLMs无法解决。我们专注于数学应用领域的词问题，利用抽象语法树结构生成对抗示例，通过简单编辑问题中的数字值，导致LLMs产生错误答案。我们对各种开源和闭源LLMs进行实验，定量和定性地证明我们的方法显著降低了它们的数学问题解决能力。

    arXiv:2402.17916v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify
    
[^82]: 通过可解释的方言分类器提取方言的词汇特征

    Extracting Lexical Features from Dialects via Interpretable Dialect Classifiers

    [https://arxiv.org/abs/2402.17914](https://arxiv.org/abs/2402.17914)

    通过可解释的方言分类器提取方言的词汇特征，成功识别了有助于方言变化的关键语言特定词汇特征。

    

    识别一种语言的方言之间的语言差异通常需要专业知识和细致的人类分析。这主要是因为研究各种方言涉及到复杂性和微妙之处。我们提出了一种新颖的方法，通过利用可解释的方言分类器提取方言的区分性词汇特征，即使在没有人类专家的情况下。我们探索了事后和内在的解释性方法，对普通话、意大利语和低地萨克森语进行实验，并实验证明我们的方法成功地识别了有助于方言变化的关键语言特定词汇特征。

    arXiv:2402.17914v1 Announce Type: cross  Abstract: Identifying linguistic differences between dialects of a language often requires expert knowledge and meticulous human analysis. This is largely due to the complexity and nuance involved in studying various dialects. We present a novel approach to extract distinguishing lexical features of dialects by utilizing interpretable dialect classifiers, even in the absence of human experts. We explore both post-hoc and intrinsic approaches to interpretability, conduct experiments on Mandarin, Italian, and Low Saxon, and experimentally demonstrate that our method successfully identifies key language-specific lexical features that contribute to dialectal variations.
    
[^83]: 基于语言模型的本体论中新概念放置框架

    A Language Model based Framework for New Concept Placement in Ontologies

    [https://arxiv.org/abs/2402.17897](https://arxiv.org/abs/2402.17897)

    提出了一种基于语言模型的框架，用于将从文本中提取的新概念插入到本体中，在边搜索、边形成和增强、边选择三个步骤中分别利用神经方法，并在 SNOMED CT 本体和 MedMentions 实体链接基准上进行了评估

    

    我们研究了利用语言模型将从文本中提取的新概念插入本体的任务。我们探索了一个三步方法：边搜索，即找到要插入的候选位置集（即概念之间的包含关系），边形成和增强，利用本体结构生成和增强边候选，以及边选择，最终确定要放置的边。在所有步骤中，我们提出利用神经方法，其中应用基于嵌入的方法和对比学习，如BERT用于边搜索，采用基于BERT微调的多标签边交叉编码器，以及GPT系列、FLAN-T5 和 Llama 2 等大型语言模型（LLM）用于边选择。我们在使用 SNOMED CT 本体和 MedMentions 实体链接基准创建的最新数据集上评估了这些方法。

    arXiv:2402.17897v1 Announce Type: new  Abstract: We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our fram
    
[^84]: 研究性问题：LLM网络特工的多透视、分解问题数据集

    Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents

    [https://arxiv.org/abs/2402.17896](https://arxiv.org/abs/2402.17896)

    提出了一个研究性问题数据集，其中包含非事实型、多透视的问题，能够挑战目前大型语言模型的表现。

    

    现有的问答（QA）数据集对于大多数强大的大型语言模型（LLMs）来说不再具有挑战性。传统的QA基准如TriviaQA、NaturalQuestions、ELI5和HotpotQA主要研究明确指示了缺少哪些信息以及如何找到这些信息来回答问题的“已知未知s”。因此，对这些基准的优秀表现提供了一种虚假的安全感。自然语言处理（NLP）社区尚未满足的需求是一个非事实型、多透视问题的银行，涉及大量不明确的信息需求，即“未知的未知s”。我们声称可以在搜索引擎日志中找到这样的问题，这令人惊讶，因为大多数问答意图查询实际上是事实型的。我们展示了Researchy Questions，一个经过繁琐过滤以变为非事实型、“分解式”和多透视的搜索引擎查询数据集。我们展示了用户在这些问题上投入了大量“努力”，这种努力表现为信号

    arXiv:2402.17896v1 Announce Type: cross  Abstract: Existing question answering (QA) datasets are no longer challenging to most powerful Large Language Models (LLMs). Traditional QA benchmarks like TriviaQA, NaturalQuestions, ELI5 and HotpotQA mainly study ``known unknowns'' with clear indications of both what information is missing, and how to find it to answer the question. Hence, good performance on these benchmarks provides a false sense of security. A yet unmet need of the NLP community is a bank of non-factoid, multi-perspective questions involving a great deal of unclear information needs, i.e. ``unknown uknowns''. We claim we can find such questions in search engine logs, which is surprising because most question-intent queries are indeed factoid. We present Researchy Questions, a dataset of search engine queries tediously filtered to be non-factoid, ``decompositional'' and multi-perspective. We show that users spend a lot of ``effort'' on these questions in terms of signals lik
    
[^85]: JMLR：联合医疗LLM和检索训练以增强推理和专业问题回答能力

    JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability

    [https://arxiv.org/abs/2402.17887](https://arxiv.org/abs/2402.17887)

    JMLR通过联合训练信息检索系统和大型语言模型，在医学领域提高问题回答系统性能，降低计算资源需求，增强模型利用医疗知识进行推理和回答问题的能力。

    

    随着医疗数据的爆炸性增长和人工智能技术的快速发展，精准医学已经成为增强医疗服务质量和效率的关键。在这种背景下，大型语言模型（LLMs）在医疗知识获取和问题回答系统中发挥越来越重要的作用。为了进一步提高这些系统在医学领域的性能，我们介绍了一种创新方法，在微调阶段同时训练信息检索（IR）系统和LLM。我们称之为联合医疗LLM和检索训练（JMLR）的方法旨在克服传统模型在处理医学问题回答任务时面临的挑战。通过采用同步训练机制，JMLR减少了对计算资源的需求，并增强了模型利用医疗知识进行推理和回答问题的能力。

    arXiv:2402.17887v1 Announce Type: new  Abstract: With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering question
    
[^86]: BlendSQL：用于统一混合问题回答的可扩展方言在关系代数中

    BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra

    [https://arxiv.org/abs/2402.17882](https://arxiv.org/abs/2402.17882)

    BlendSQL是一个超集的SQLite，用于统一混合问题回答中的非结构化和结构化数据，尤其适用于包含多跳推理的任务，并且在使用更少令牌的情况下能够提高系统性能。

    

    许多现有端到端系统用于混合问题回答任务，往往可以归结为“提示-祈祷”范式，用户对中间推理步骤的控制和洞察受限。此外，由于许多基于Transformer的LLM模型的上下文大小限制，往往不合理期望完整的结构化和非结构化上下文适合于给定提示在零次示范环境中，更不用说几次提示环境中。我们介绍了BlendSQL，它是SQLite的一个超集，用作统一的方言，用于在非结构化和结构化数据之间编排推理。对于涉及多跳推理的混合问题回答任务，我们将完整的分解推理路线图编码为一个可解释的BlendSQL查询。值得注意的是，我们展示了BlendSQL可以扩展到大规模数据集，并在使用35%更少令牌的情况下改善端到端系统的性能。

    arXiv:2402.17882v1 Announce Type: new  Abstract: Many existing end-to-end systems for hybrid question answering tasks can often be boiled down to a "prompt-and-pray" paradigm, where the user has limited control and insight into the intermediate reasoning steps used to achieve the final result. Additionally, due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting. We introduce BlendSQL, a superset of SQLite to act as a unified dialect for orchestrating reasoning across both unstructured and structured data. For hybrid question answering tasks involving multi-hop reasoning, we encode the full decomposed reasoning roadmap into a single interpretable BlendSQL query. Notably, we show that BlendSQL can scale to massive datasets and improve the performance of end-to-end systems while using 35% fewer tokens. Our code
    
[^87]: 使用语言模型进行自动统计模型发现

    Automated Statistical Model Discovery with Language Models

    [https://arxiv.org/abs/2402.17879](https://arxiv.org/abs/2402.17879)

    利用大型语言模型，提出了一种基于语言模型驱动的自动统计模型发现方法，不再需要定义特定领域模型语言或设计手工搜索程序。

    

    统计模型发现涉及在受领域特定建模约束的广泛模型空间上进行具有挑战性的搜索。高效搜索这一空间需要具有建模和问题域人类专长的专业知识。受大型语言模型（LMs）领域知识和编程能力的启发，我们介绍了一种基于语言模型驱动的自动统计模型发现方法。我们将自动化流程置于Box的循环框架之内：LM在提出表示为概率程序的统计模型（充当建模者）之间迭代，并批判这些模型（充当领域专家）。通过利用LMs，我们不必定义一个领域特定的模型语言或设计手工搜索程序，这是先前系统的重要限制。我们在概率建模的三种常见设置中评估了我们的方法：在受限模型空间内搜索，搜索

    arXiv:2402.17879v1 Announce Type: cross  Abstract: Statistical model discovery involves a challenging search over a vast space of models subject to domain-specific modeling constraints. Efficiently searching over this space requires human expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, key restrictions of previous systems. We evaluate our method in three common settings in probabilistic modeling: searching within a restricted space of models, searching ove
    
[^88]: 遵循我的指示并说出真相：来自检索增强生成系统的可扩展数据提取

    Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems

    [https://arxiv.org/abs/2402.17840](https://arxiv.org/abs/2402.17840)

    研究揭示了检索增强生成系统中的数据泄露风险，指出对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据，并设计了攻击对生产RAG模型GPTs造成数据存储泄漏。

    

    检索增强生成（RAG）通过在测试时将外部知识纳入预训练模型，从而实现定制适应，提升了模型性能。本研究探讨了Retrieval-In-Context RAG语言模型（LMs）中的数据泄露风险。我们展示了当对使用指令调整的LMs构建的RAG系统进行提示注入时，对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据。这种漏洞存在于覆盖Llama2、Mistral/Mixtral、Vicuna、SOLAR、WizardLM、Qwen1.5和Platypus2等多种现代LMs的广泛范围内，并且随着模型规模的扩大，利用能力加剧。将研究扩展到生产RAG模型GPTs，我们设计了一种攻击，可以在对25个随机选择的定制GPTs施加最多2个查询时以100%成功率导致数据存储泄漏，并且我们能够以77,000字的书籍中的文本数据的提取率为41%，以及在含有1,569,00词的语料库中的文本数据的提取率为3%。

    arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
    
[^89]: 稳定LM 2 1.6B技术报告

    Stable LM 2 1.6B Technical Report

    [https://arxiv.org/abs/2402.17834](https://arxiv.org/abs/2402.17834)

    稳定LM 2 1.6B是语言模型系列中的新一代产品，在该报告中详细介绍了其数据、训练过程和性能评估，该模型在发布时是2B参数范围内最先进的开源模型之一，并提供了下载链接和性能对比数据。

    

    我们介绍了稳定LM 2 1.6B，这是我们语言模型系列的新一代产品。在这份技术报告中，我们详细介绍了导致StableLM 2 1.6B基础版本和指导调优版本的数据和训练过程。这两个模型的权重均可通过Hugging Face下载和使用。该报告包含了对这些模型的彻底评估，包括零 shot 和少 shot 基准测试，多语言基准测试，以及重点放在多轮对话的 MT 基准测试上。在发布本报告时，StableLM 2 1.6B是具有显著优势的 2B 参数范围内最先进的开源模型。鉴于其吸引人的小尺寸，我们还提供了在多种边缘设备上的吞吐量测试。此外，我们开源了几个量化检查点，并提供了它们与原始模型的性能指标比较。

    arXiv:2402.17834v1 Announce Type: new  Abstract: We introduce StableLM 2 1.6B, the first in a new generation of our language model series. In this technical report, we present in detail the data and training procedure leading to the base and instruction-tuned versions of StableLM 2 1.6B. The weights for both models are available via Hugging Face for anyone to download and use. The report contains thorough evaluations of these models, including zero- and few-shot benchmarks, multilingual benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of publishing this report, StableLM 2 1.6B was the state-of-the-art open model under 2B parameters by a significant margin. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.
    
[^90]: 大型语言模型的预测排名

    Prediction-Powered Ranking of Large Language Models

    [https://arxiv.org/abs/2402.17826](https://arxiv.org/abs/2402.17826)

    该研究提出了一种统计框架，可以衡量人类与模型偏好之间的不确定性，从而进行大型语言模型的预测排名。

    

    大型语言模型通常根据其与人类偏好的一致性水平进行排名--如果一个模型的输出更受人类偏好，那么它就比其他模型更好。本文提出了一种统计框架来弥合人类与模型偏好之间可能引入的不一致性。

    arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
    
[^91]: DropBP：通过丢弃反向传播加速大型语言模型的微调

    DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation

    [https://arxiv.org/abs/2402.17812](https://arxiv.org/abs/2402.17812)

    DropBP提出了一种新颖的方式来加速大型语言模型的微调，通过在反向传播过程中随机丢弃层以减少计算成本同时保持准确性。

    

    训练深度神经网络通常涉及正向和反向传播过程中的大量计算成本。传统的层次丢弃技术在训练过程中丢弃某些层以减少计算负担。然而，在正向传播过程中丢弃层会对训练过程产生不利影响，降低准确性。本文提出了DropBP，这是一种旨在减少计算成本同时保持准确性的新方法。DropBP在反向传播过程中随机丢弃层，不影响正向传播。此外，DropBP计算每个层的敏感性以分配适当的丢失率，从而稳定训练过程。DropBP旨在通过反向传播增强训练过程的效率，从而加速使用反向传播进行完全微调和参数高效微调。

    arXiv:2402.17812v1 Announce Type: cross  Abstract: Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropag
    
[^92]: TruthX: 通过在真实空间中编辑大型语言模型来减轻幻觉

    TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space

    [https://arxiv.org/abs/2402.17811](https://arxiv.org/abs/2402.17811)

    本文提出了一种名为TruthX的方法，通过在真实空间中编辑大型语言模型的内部表示，有效提高了语言模型的真实性，实验证明在TruthfulQA基准测试中，TruthX平均提高了13种先进语言模型的真实性。

    

    大型语言模型(LLMs)在各种任务中展现出了显著的能力。然而，它们有时会产生幻觉，特别是在它们可能生成不真实的回应，尽管拥有正确的知识的情况下。在本文中，我们提出了TruthX，一种用于在真实空间中编辑LLMs内部表示以获取其真实性的推断时间方法。TruthX利用自动编码器将LLM的表示分别映射到语义和真实潜在空间，并应用对比学习在真实空间中识别真实的编辑方向。在推断过程中，通过在真实空间中编辑LLM的内部表示，TruthX有效地增强了LLMs的真实性。实验证明，TruthX通过20%的平均值提高了13种先进LLMs在TruthfulQA基准测试中的真实性。进一步的分析表明，真实空间

    arXiv:2402.17811v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge. In this paper, we propose TruthX, an inference-time method to elicit the truthfulness of LLMs by editing their internal representations in truthful space. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLMs. Experiments show that TruthX effectively improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that the truthful space
    
[^93]: 一个令人惊讶的失败？多模LLMs和NLVR挑战

    A Surprising Failure? Multimodal LLMs and the NLVR Challenge

    [https://arxiv.org/abs/2402.17793](https://arxiv.org/abs/2402.17793)

    这项研究评估了多模LLMs在自然语言视觉推理任务NLVR上的性能表现，发现它们在需要组合和空间推理、对语义和系统性偏见具有鲁棒性的任务上表现不佳。

    

    这项研究评估了三种最先进的MLLMs——GPT-4V、Gemini Pro和开源模型IDEFICS——对于组合自然语言视觉推理任务NLVR的表现。NLVR要求模型根据一个人类书写的句子和一个合成图像来确定句子相对于图像的真假。尽管这些模型表现出强大的性能，我们观察到它们在NLVR上表现不佳，该任务旨在需要组合和空间推理，并且对语义和系统性偏见具有鲁棒性。

    arXiv:2402.17793v1 Announce Type: new  Abstract: This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and the open-source model IDEFICS -- on the compositional natural language vision reasoning task NLVR. Given a human-written sentence paired with a synthetic image, this task requires the model to determine the truth value of the sentence with respect to the image. Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases.
    
[^94]: 使用大型语言模型进行逐步自洽的数学推理

    Stepwise Self-Consistent Mathematical Reasoning with Large Language Models

    [https://arxiv.org/abs/2402.17786](https://arxiv.org/abs/2402.17786)

    提出了一种名为SSC-CoT的算法，通过选择中间步骤的策略和查询知识图来解决大型语言模型进行复杂数学推理时面临的挑战

    

    使用大型语言模型进行复杂数学推理是困难的，主要是由于多步推理过程的复杂性。该论文介绍了一种新的算法，名为Stepwise Self-Consistent Chain-of-Thought（SSC-CoT），用于解决这些问题。SSC-CoT利用选择基于不同推理链交集的中间步骤的策略，并通过查询包含相关领域知识的知识图来发现关键的中间步骤。

    arXiv:2402.17786v1 Announce Type: new  Abstract: Using Large Language Models for complex mathematical reasoning is difficult, primarily due to the complexity of multi-step reasoning. The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions. To address these issues, we introduce a novel algorithm, namely Stepwise Self-Consistent Chain-of-Thought (SSC-CoT). SSC-CoT employs a strategy of selecting intermediate steps based on the intersection of various reasoning chains. Additionally, SSC-CoT enables the model to discover critical intermediate steps by querying a knowledge graph comprising relevant domain knowledge. To validate SSC-CoT, we present a new dataset, TriMaster100, tailored for complex trigonometry problems. This dataset contains 100 questions, with each solution broken down into scored intermediate steps, facilitating a comprehensive evaluation of the mathematical reasoni
    
[^95]: OmniACT：用于启用桌面和Web多模式通用主动智能体的数据集和基准

    OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web

    [https://arxiv.org/abs/2402.17553](https://arxiv.org/abs/2402.17553)

    OmniACT是一个针对代理生成可执行程序完成计算机任务能力的数据集和基准，超越了传统Web自动化，涵盖了各种桌面应用。

    

    几十年来，人机交互从根本上一直是手动的。即使在今天，几乎所有在计算机上进行的高效工作都需要人类在每一步都提供输入。虚拟主动智能代表了自动化许多这些琐碎任务的一个激动人心的步骤。虚拟代理将使技术能力有限的用户能够充分利用计算机系统的各种可能性。它们还可以实现高效地简化许多计算机任务，从日历管理到复杂的旅行预订，减少人类干预。在这篇论文中，我们介绍了 OmniACT，这是一个用于评估代理生成可执行程序来完成计算机任务能力的首个数据集和基准。我们的范围超越了传统的Web自动化，涵盖了各种桌面应用。该数据集包含诸如"播放下一首歌"之类的基本任务，以及更为长期的任务

    arXiv:2402.17553v1 Announce Type: new  Abstract: For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such
    
[^96]: 为围手术期护理开具大型语言模型：预训练模型的正确剂量是多少？

    Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?

    [https://arxiv.org/abs/2402.17493](https://arxiv.org/abs/2402.17493)

    通过评估临床大型语言模型在术后风险预测中的应用，研究探讨了使用不同训练策略的模型在围手术期护理中的潜在效果。

    

    术后风险预测可以指导有效的围手术期护理管理和规划。我们旨在评估临床大型语言模型(LLMs)是否可以使用不同的训练策略预测术后风险。研究主要涉及2018年至2021年间来自Barnes Jewish医院系统的84,875份记录。方法在Beth Israel Deaconess的MIMIC数据集上进行了复制。两项研究的平均随访时间基于术后ICU住院时间小于7天。对于BJH数据集，结果包括30天死亡率、肺栓塞（PE）和肺炎。对BioGPT、ClinicalBERT和BioClinicalBERT实施了三种域自适应和微调策略：自监督目标；结合半监督微调的标签；以及通过多任务学习进行基础建模。模型性能使用接收器操作特征下的面积进行了比较。

    arXiv:2402.17493v1 Announce Type: new  Abstract: Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating ch
    
[^97]: 通过从预训练对比性EEG-文本蒙版自动编码器中转移的表示增强EEG到文本解码

    Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder

    [https://arxiv.org/abs/2402.17433](https://arxiv.org/abs/2402.17433)

    通过Contrastive EEG-Text Masked Autoencoder（CET-MAE）和E2T-PTR框架，提出了一种新的模型和方法来增强基于EEG的语言解码。

    

    从无创脑电图（EEG）重建自然语言具有很大的潜力，作为脑机接口（BCI）的语言解码技术。然而，基于EEG的语言解码仍处于初级阶段，面临诸多技术问题，如：1）缺乏一个能够有效整合跨模态（EEG和文本之间）自学习与EEG特征或文本序列的模内自重构的混合策略；2）未充分利用大型语言模型（LLMs）来增强基于EEG的语言解码。为解决上述问题，我们提出了对比性EEG-文本蒙版自动编码器（CET-MAE），这是一个通过专用的多流编码器在EEG和文本之间以及内部进行复合自监督学习的新型模型。此外，我们开发了一个名为E2T-PTR（使用预训练可转移表示进行EEG到文本解码）的框架，该框架利用预训练模组

    arXiv:2402.17433v1 Announce Type: new  Abstract: Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modul
    
[^98]: 处理数据以应对RE中的挑战：利用NLP和生成AI缓解挑战

    Dealing with Data for RE: Mitigating Challenges using NLP and Generative AI

    [https://arxiv.org/abs/2402.16977](https://arxiv.org/abs/2402.16977)

    数据在现代软件系统中扮演着不可或缺的角色，监管环境的变化、软件个性化需求的增长以及对治理的强调推动着大型企业采用自动化技术，并在AI为中心的系统中不断引入新的挑战和需求。

    

    在当今不断变化的商业环境中，企业面临着日益增多的挑战。这些挑战包括不断变化的监管环境、软件应用中个性化需求的增长以及对治理的强调。针对这些多方面的需求，大型企业一直在采用从优化核心业务流程到增强客户体验的自动化。实际上，人工智能（AI）已经成为现代软件系统的关键要素。在这种背景下，数据发挥着不可或缺的作用。基于监督学习且在工业规模上运行的以AI为中心的软件系统需要大量的训练数据才能有效运行。此外，引入生成AI导致对足够评估基准的需求不断增长。我们在这一领域的经验显示出，满足这些要求对于实现RE方面的成功至关重要。

    arXiv:2402.16977v1 Announce Type: cross  Abstract: Across the dynamic business landscape today, enterprises face an ever-increasing range of challenges. These include the constantly evolving regulatory environment, the growing demand for personalization within software applications, and the heightened emphasis on governance. In response to these multifaceted demands, large enterprises have been adopting automation that spans from the optimization of core business processes to the enhancement of customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a pivotal element of modern software systems. In this context, data plays an indispensable role. AI-centric software systems based on supervised learning and operating at an industrial scale require large volumes of training data to perform effectively. Moreover, the incorporation of generative AI has led to a growing demand for adequate evaluation benchmarks. Our experience in this field has revealed that the requirement 
    
[^99]: 如果在一个众包数据标注管道中，GPT-4

    If in a Crowdsourced Data Annotation Pipeline, a GPT-4

    [https://arxiv.org/abs/2402.16795](https://arxiv.org/abs/2402.16795)

    本文比较了 GPT-4 和 MTurk 管道的数据标注准确性，发现尽管 MTurk 采用了最佳实践，但 GPT-4 的准确率更高，并且结合 GPT-4 和众包标签使用聚合算法可以提高准确率。

    

    最近的研究表明GPT-4在数据标注准确性方面优于在线众包工作者，尤其是来自亚马逊机械土耳其（MTurk）的工作者。然而，这些研究因偏离标准众包实践并强调个别工作者的表现而受到批评，而不是整个数据标注过程。本文比较了GPT-4和一个道德且执行良好的MTurk管道，使用415名工作者标注了来自200篇学术文章的3,177个句段，使用了CODA-19方案。两个工作者界面产生了127,080个标签，然后通过八种标签聚合算法推断出最终的标签。我们的评估结果显示，尽管采用了最佳实践，MTurk管道的最高准确率为81.5%，而GPT-4达到了83.6%。有趣的是，当将GPT-4的标签与通过先进工作者界面收集的众包标签结合起来进行聚合时，8种算法中有2种实现了更高的准确率。

    arXiv:2402.16795v1 Announce Type: cross  Abstract: Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers' performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved 83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (
    
[^100]: 在大型语言模型中审慎行事：迈向决策感知和可泛化的工具使用

    Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models

    [https://arxiv.org/abs/2402.16696](https://arxiv.org/abs/2402.16696)

    提出了一种决策感知和可泛化的工具使用框架，以帮助大型语言模型在操作工具时提高灵活性和泛化能力

    

    工具增强的大型语言模型（LLM）在获取最新知识和缓解产生幻觉问题方面引起了广泛关注。当前，先进的闭源LLM（如ChatGPT）通过提示和上下文学习技术展示出令人惊讶的工具使用能力。为了增强开源LLM（如LLaMA）在操作工具方面的能力，当前的努力集中于基于模板驱动或基于标记触发的工具使用。然而，前者由于受到限制的工具交互，限制了LLM灵活地解决各种用户查询，而后者在使用新工具时限制了泛化能力，因为工具使用学习基于任务和工具特定的数据集。为了缓解这些问题，本文提出了一种决策感知和可泛化的工具使用框架（DEER）。具体而言，我们首先构建具有多个决策分支的工具使用样本。

    arXiv:2402.16696v1 Announce Type: new  Abstract: Tool-augmented large language models (LLMs) are attracting widespread attention when accessing up-to-date knowledge and alleviating hallucination issues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated surprising tool-usage capabilities through prompting and in-context learning techniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in manipulating tools, current efforts focus on either template-driven or token-triggered tool-usage. However, the former hampers LLMs' flexibility to address diverse user's queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets. To alleviate these concerns, in this paper, we propose a decision-aware and generalizable tool-usage framework (DEER). Specifically, we first construct the tool-usage samples with multiple decision branches 
    
[^101]: StructLM: 朝向构建结构化知识连接的通用模型

    StructLM: Towards Building Generalist Models for Structured Knowledge Grounding

    [https://arxiv.org/abs/2402.16671](https://arxiv.org/abs/2402.16671)

    StructLM系列模型基于全面指令调整数据集，超越了大多数任务特定模型，在结构化知识连接任务中取得了新的最先进成就。

    

    结构化数据源，如表格、图形和数据库，是普遍存在的知识源。尽管大型语言模型（LLM）在纯文本上表现出色，但它们在解释和利用结构化数据方面的能力仍然有限。我们的研究揭示了LLM在处理结构化数据方面的显着不足，例如，ChatGPT平均落后于最先进模型(SoTA)35%。为增强LLM中的结构化知识连接（SKG）能力，我们开发了一个包含110万个示例的全面指令调整数据集。利用这个数据集，我们训练了一系列基于Code-LLaMA架构的模型，称为StructLM，参数范围从7B到34B。我们的StructLM系列在18个评估数据集中有14个超越了特定任务的模型，并在7个SKG任务上确立了新的SoTA成就。此外，StructLM展现了卓越的泛化能力。

    arXiv:2402.16671v1 Announce Type: new  Abstract: Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalizat
    
[^102]: 将大型语言模型对齐到特定领域的图数据库

    Aligning Large Language Models to a Domain-specific Graph Database

    [https://arxiv.org/abs/2402.16567](https://arxiv.org/abs/2402.16567)

    该论文提出了一种将大型语言模型对齐到特定领域的图数据库的方法，通过利用ChatGPT生成NL-GQL数据对并微调LLMs，实现了两者之间的对齐。

    

    图数据库（Graph DB）被广泛应用于金融、社交网络和医药等各个领域。然而，将自然语言（NL）转换为图查询语言（GQL），通常称为NL2GQL，由于其固有复杂性和专业化特性而变得具有挑战性。一些方法试图利用大型语言模型（LLMs）来解决类似的任务，如文本转SQL。然而，在特定领域的NL2GQL任务中，缺乏特定领域的NL-GQL数据对使得难以建立LLMs和图数据库之间的对齐。为了解决这一挑战，我们提出了一个明确定义的流水线。具体地，我们利用ChatGPT基于给定的图数据库自我生成NL-GQL数据对。然后，我们使用创建的数据来对LLMs进行微调，从而实现LLMs与图数据库之间的对齐。此外，在推断过程中，我们提出了一种提取相关信息的方法。

    arXiv:2402.16567v1 Announce Type: new  Abstract: Graph Databases (Graph DB) are widely applied in various fields, including finance, social networks, and medicine. However, translating Natural Language (NL) into the Graph Query Language (GQL), commonly known as NL2GQL, proves to be challenging due to its inherent complexity and specialized nature. Some approaches have sought to utilize Large Language Models (LLMs) to address analogous tasks like text2SQL. Nevertheless, when it comes to NL2GQL taskson a particular domain, the absence of domain-specific NL-GQL data pairs makes it difficult to establish alignment between LLMs and the graph DB. To address this challenge, we propose a well-defined pipeline. Specifically, we utilize ChatGPT to create NL-GQL data pairs based on the given graph DB with self-instruct. Then, we use the created data to fine-tune LLMs, thereby achieving alignment between LLMs and the graph DB. Additionally, during inference, we propose a method that extracts relev
    
[^103]: LLM推断揭示：调查与Roofline模型见解

    LLM Inference Unveiled: Survey and Roofline Model Insights

    [https://arxiv.org/abs/2402.16363](https://arxiv.org/abs/2402.16363)

    本文提出了一个基于Roofline模型的框架，用于系统分析LLM推断技术，帮助识别部署中的瓶颈，并为更有效地部署LLM提供策略。

    

    高效大语言模型（LLM）推断领域正在迅速发展，提供了机遇和挑战的独特结合。虽然该领域已经扩展并充满活力，但至今还没有一个简明的框架来分析LLM推断的各种方法，以便清晰地理解这一领域。我们的调查不仅总结了当前研究现状，还基于Roofline模型引入了一个框架，用于系统分析LLM推断技术。这一框架能够帮助识别LLM部署中的瓶颈，并更深入地了解在实际设备上的实际方面，从而为部署LLM提供更有效的策略。此外，我们还系统地汇总了高效LLM推断的最新进展，涵盖关键领域，比如权重优化（如知识蒸馏和量化）。

    arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
    
[^104]: 一种使用注释嵌入模型的本体包含关系预测自匹配训练方法

    A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction

    [https://arxiv.org/abs/2402.16278](https://arxiv.org/abs/2402.16278)

    提出了一种自匹配训练方法，通过两种本体嵌入模型捕获全局和局部信息，提高了概念子类预测的稳健性

    

    最近，提出了一种在低维空间中表示实体的本体嵌入，用于本体完成。然而，用于概念子类预测的本体嵌入未解决类似和孤立实体的困难，并且未提取本体中注释公理的全局信息。本文提出了一种针对两种本体嵌入模型的自匹配训练方法：Inverted-index Matrix Embedding (InME) 和 Co-occurrence Matrix Embedding (CoME)。这两种嵌入通过每个单词在一组公理中出现的位置以及每个公理中单词的共现来捕获注释公理中的全局和局部信息。自匹配训练方法提高了概念子类预测的稳健性，当预测的超类与子类相似且孤立于本体中的其他实体时。

    arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
    
[^105]: UniRetriever：各种情境自适应对话检索的多任务候选者选择

    UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval

    [https://arxiv.org/abs/2402.16261](https://arxiv.org/abs/2402.16261)

    提出了一种UniRetriever框架，利用双编码器架构和两个损失约束实现了多任务候选者选择，适用于不同情境下的对话检索任务。

    

    对话检索是指以迭代和交互方式运行的信息检索系统，需要检索各种外部资源（如人设、知识甚至回应）以有效与用户交互并成功完成对话。为了提高效率和性能，我们提出了一个多任务框架，作为三个主要检索任务的通用检索器：人设选择、知识选择和回应选择。为此，我们设计了一个双编码器架构，包括一个情境自适应对话编码器和一个候选者编码器，旨在通过简单的点积关注长对话中的相关上下文并检索合适的候选者。此外，我们引入了两个损失约束以捕捉...

    arXiv:2402.16261v1 Announce Type: new  Abstract: Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture t
    
[^106]: MATHWELL: 在规模上生成教育数学应用题

    MATHWELL: Generating Educational Math Word Problems at Scale

    [https://arxiv.org/abs/2402.15861](https://arxiv.org/abs/2402.15861)

    使用MATHWELL模型生成了迄今为止最大的英文数学应用题数据集，其中包含20,490个问题，经领域专家评分结果显示，MATHWELL的问题中具有可执行解决方案并符合所有标准的份额比其他选择高出40%，其中74%的可解决问题同时做到了准确和适当。

    

    数学应用题在K-8教育中至关重要，但编写它们耗时且需要领域专业知识。我们认为语言模型可以通过自动生成规模化问题来支持K-8数学教育。为了教育性，生成的问题必须是1）可解决的，2）准确的，3）适当的。现有数据集未标记这些标准，因此不适合训练问题生成器。我们引入了MATHWELL，这是一个经过专家注释数据进行迭代微调的70B Llama-2模型，用于生成K-8数学应用题。借助MATHWELL，我们生成了迄今为止最大的英文应用题数据集，其中包含20,490个问题。经领域专家评分的3,484个问题发现，MATHWELL拥有比其他选择更高的可执行解决方案和满足所有标准的问题份额高出40％，其中74％的问题具有可解的、准确的和适当的解决方案。

    arXiv:2402.15861v1 Announce Type: new  Abstract: Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate.
    
[^107]: 大规模生成式人工智能文本在体育和音乐领域的应用

    Large Scale Generative AI Text Applied to Sports and Music

    [https://arxiv.org/abs/2402.15514](https://arxiv.org/abs/2402.15514)

    这项工作利用生成式人工智能模型将大规模多模数据转化为连贯流畅文本，首次推出了用于体育和音乐领域的AI评论系统，并取得了显著性能提升。

    

    我们解决了将媒体内容（包括评论和个性化新闻报道）扩展到全球大型体育和音乐活动的生产问题。我们的方法依赖生成式人工智能模型，将大量多模数据（例如视频、文章、实时比分、统计数据和资料）转换为连贯流畅的文本。基于这一方法，我们首次推出了一款人工智能评论系统，该系统被部署用于为2023年美国公开赛、温布尔登公开赛和大师赛的精彩片段制作自动化叙述。我们的解决方案还被扩展用于为ESPN梦幻橄榄球和格莱美奖音乐艺术家故事创造个性化内容。这些应用程序采用了相同的软件架构，实现了15倍的速度提升，平均Rouge-L为82.00，困惑度为6.6。

    arXiv:2402.15514v1 Announce Type: cross  Abstract: We address the problem of scaling up the production of media content, including commentary and personalized news stories, for large-scale sports and music events worldwide. Our approach relies on generative AI models to transform a large volume of multimodal data (e.g., videos, articles, real-time scoring feeds, statistics, and fact sheets) into coherent and fluent text. Based on this approach, we introduce, for the first time, an AI commentary system, which was deployed to produce automated narrations for highlight packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same vein, our solution was extended to create personalized content for ESPN Fantasy Football and stories about music artists for the Grammy awards. These applications were built using a common software architecture achieved a 15x speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our work was successfully deployed at the aforeme
    
[^108]: 通过表示编辑推进微调中的参数效率

    Advancing Parameter Efficiency in Fine-tuning via Representation Editing

    [https://arxiv.org/abs/2402.15179](https://arxiv.org/abs/2402.15179)

    RED通过表示编辑显著降低了可训练参数数量，实现了与完全参数微调和其他PEFT方法相当或更好的结果

    

    参数有效微调（PEFT）因其能够在仅更新可训练参数的一个小子集时达到竞争性结果而受到了重视。在解决这些挑战问题中，我们提出了一种新颖的微调神经模型的方法，称为表示编辑（RED），其扩放和偏置每一层产生的表示。与完全参数微调相比，RED将可训练参数数量降低了$25,700$倍，并与LoRA相比降低了32倍。值得注意的是，RED实现了与完全参数微调和其他PEFT方法相当或更好的结果。对不同架构和规模的模型进行了大量实验。

    arXiv:2402.15179v1 Announce Type: cross  Abstract: Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, includin
    
[^109]: 使用后门增强对齐来缓解微调越狱攻击

    Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment

    [https://arxiv.org/abs/2402.14968](https://arxiv.org/abs/2402.14968)

    提出了一种通过后门增强对齐方法有效缓解微调越狱攻击，避免需要大量安全示例的低效问题。

    

    尽管大型语言模型（LLMs）如GPT-4和Llama-2具有一般能力，但在满足特定业务需求和定制用例的复杂性时，仍然需要对其进行微调或自适应以满足需求。然而，这个过程不可避免地引入了新的安全威胁，特别是针对基于微调的越狱攻击（FJAttack），在这种情况下，将仅几个有害示例纳入微调数据集就可能显着地损害模型的安全性。虽然已经提出了一些潜在的防御方法，例如将安全示例纳入微调数据集以减少安全问题，但这些方法需要纳入大量的安全示例，效率低下。为了有效地针对FJAttack进行防御并只使用有限的安全示例，我们提出了一种灵感来自后门攻击概念的后门增强安全对齐方法。

    arXiv:2402.14968v1 Announce Type: cross  Abstract: Despite the general capabilities of Large Language Models (LLMs) like GPT-4 and Llama-2, these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the fine-tuning dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the fine-tuning dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In pa
    
[^110]: 利用非正式逻辑增强系统化分解的自然语言推理

    Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic

    [https://arxiv.org/abs/2402.14798](https://arxiv.org/abs/2402.14798)

    本文提出了一种一致且在理论上有根据的方法来注释分解蕴涵数据集，形成RDTE数据集，该数据集在解决何为有效的组合蕴涵的问题上有显著进展。

    

    当代语言模型为使用文本进行结构化推理提供了新的机会，例如在不依赖脆弱的形式逻辑的情况下构建和评估直观的、类似证明的文本蕴涵树。然而，沿着这个方向的进展受到一个长期以来缺乏明确的确定何为有效的组合蕴涵的清晰协议的阻碍。本文提出了一个一致且在理论上有根据的方法来注释分解蕴涵数据集，并评估其对基于LLM的文本推理的影响。我们发现，我们的结果数据集RDTE (Recognizing Decompositional Textual Entailment) 的内部一致性比先前的分解蕴涵数据集高得多（+9%），表明RDTE在长期存在的关于何为有效的组合蕴涵的问题上是一个重要的进步。

    arXiv:2402.14798v1 Announce Type: cross  Abstract: Contemporary language models enable new opportunities for structured reasoning with text, such as the construction and evaluation of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on LLM-based textual inference. We find that our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of for
    
[^111]: OpenCodeInterpreter：集成代码生成、执行和细化

    OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement

    [https://arxiv.org/abs/2402.14658](https://arxiv.org/abs/2402.14658)

    OpenCodeInterpreter是一种开源代码系统，集成了执行、人类反馈和动态代码细化的功能，并在关键基准测试中表现出色，甚至与GPT-4相媲美。

    

    大型语言模型的引入显著推动了代码生成的发展。然而，开源模型通常缺乏类似GPT-4 Code Interpreter这样的高级系统的执行能力和迭代细化能力。为了解决这一问题，我们介绍了OpenCodeInterpreter，这是一族旨在生成、执行和迭代细化代码的开源代码系统。通过Code-Feedback支持，该系统集成了执行和人类反馈，用于动态代码细化。我们对OpenCodeInterpreter在诸如HumanEval、MBPP以及它们来自EvalPlus的增强版本等关键基准上进行了全面评估，证实了其出色的性能。值得注意的是，OpenCodeInterpreter-33B在HumanEval和MBPP的平均值（以及其增强版本）上取得了83.2（76.4）的准确率，与GPT-4的84.2（76.2）紧密匹敌，并且通过合成hum

    arXiv:2402.14658v1 Announce Type: cross  Abstract: The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized hum
    
[^112]: 你见过我吗？自动化数据集更新以实现可靠及及时的评估

    Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation

    [https://arxiv.org/abs/2402.11894](https://arxiv.org/abs/2402.11894)

    本文提出自动化数据集更新策略，利用两种系统验证过的策略生成看不见和高质量的测试样本，以缓解数据泄漏问题并实现评估稳定性。

    

    由于大型语言模型（LLMs）的能力不断扩大和预训练数据的增加，LLMs面临着日益严重的评估挑战。一方面，数据泄漏问题导致对现有基准的过度估计。另一方面，定期手动整理数据集成本高昂。本文提出自动化数据集更新以实现可靠及及时的评估。基本思想是基于现有样本生成看不见的高质量测试样本以减轻泄漏问题。具体而言，我们提出了两种策略并进行了系统验证。第一种是模仿策略，利用LLMs创建类似现有样本的新样本，最大程度地保留原始数据集的风格。我们的实验表明它在多次实例中的评估稳定性以及在大多数情况下处理数据泄漏问题的有效性。

    arXiv:2402.11894v1 Announce Type: new  Abstract: Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing benchmarks. On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues. In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poo
    
[^113]: 探索精度和召回率以评估LLMs的质量和多样性

    Exploring Precision and Recall to assess the quality and diversity of LLMs

    [https://arxiv.org/abs/2402.10693](https://arxiv.org/abs/2402.10693)

    该研究提出了一种新的评估框架，将精度和召回率指标从图像生成转化为文本生成，细致评估了LLMs生成文本的质量和多样性，揭示了当前LLMs在生成任务中性能表现的重要见解。

    

    这篇论文介绍了一种针对大型语言模型（LLMs）如Llama-2和Mistral的新型评估框架，重点是将图像生成的精度和召回率指标转化为文本生成。这种方法允许对生成文本的质量和多样性进行细致评估，而无需对齐的语料库。通过对最先进的语言模型进行全面评估，研究揭示了它们在开放生成任务上的表现，这是传统基准无法充分捕捉的。研究结果突出了在模型利用人类反馈进行微调时，生成样本质量和多样性之间的权衡。这项工作扩展了基于分布的自然语言处理评估工具包，为当前LLMs在生成多样性和高质量文本方面面临的实际能力和挑战提供了见解。

    arXiv:2402.10693v1 Announce Type: new  Abstract: This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text.
    
[^114]: BitDelta：你的微调可能只有一个比特的价值

    BitDelta: Your Fine-Tune May Only Be Worth One Bit

    [https://arxiv.org/abs/2402.10193](https://arxiv.org/abs/2402.10193)

    BitDelta研究探讨了大型语言模型在微调过程中的信息冗余性，并提出了一种名为BitDelta的方法，可以将微调过程中添加的信息量化为一个比特，同时保持性能。这一发现对于多租户模型的服务和存储有重要意义，并可以显著降低GPU内存需求。

    

    大型语言模型（LLMs）通常在两个阶段进行训练：在大规模互联网数据集上进行预训练，然后在下游任务上进行微调。由于预训练的高计算需求，直觉上认为微调对模型的信息添加较少，因此更具有可压缩性。我们通过将微调模型的权重分解为预训练组件和额外的增量来探究这一假设。我们引入了一种简单的方法——BitDelta，成功地将这个增量量化为1比特而不影响性能。这一有趣的发现不仅突显了微调过程中添加的信息的潜在冗余性，而且对于多租户模型的服务和存储也具有重要影响。通过使用一个高精度的基础模型以及多个1比特的增量，BitDelta大大降低了GPU内存需求。

    arXiv:2402.10193v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requir
    
[^115]: 研究指令调整的局限性

    A Closer Look at the Limitations of Instruction Tuning

    [https://arxiv.org/abs/2402.05119](https://arxiv.org/abs/2402.05119)

    本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。

    

    指令调整（IT）是使用指令-回应对来训练大型语言模型（LLM）的过程，已成为将基础预训练LLM转化为开放领域对话代理的主要方法。虽然IT取得了显著的成功并广泛应用，但其局限性和不足仍未得到充分探讨。本文通过严格的实验和对LLM通过IT发生的变化的深入分析，揭示了IT的多种局限性。特别是，我们发现：（1）IT无法增强LLM的知识或技能。LoRA微调仅限于学习回应的启动和样式令牌，而全参数微调会导致知识退化。（2）从具有知识来源的IT数据集复制回应模式会导致回应质量下降。（3）全参数微调通过不准确地从IT数据集中获取概念上相似实例的标记，增加了错误生成的情况。

    Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
    
[^116]: OLMo: 加速语言模型科学

    OLMo: Accelerating the Science of Language Models

    [https://arxiv.org/abs/2402.00838](https://arxiv.org/abs/2402.00838)

    OLMo是一种真正开放的语言模型，旨在提供给研究社区强大的工具，以促进对语言模型科学的研究和创新。

    

    语言模型（LM）已广泛应用于自然语言处理研究和商业产品。随着商业重要性的增加，最强大的模型已经封闭起来，只能通过专有接口访问，其训练数据、架构和开发细节没有透露。考虑到这些细节对于科学研究这些模型的重要性，包括其偏见和潜在风险，我们认为研究社区有权访问强大而真正开放的LM。为此，本技术报告详细介绍了OLMo的首个版本，这是一种最先进、真正开放的语言模型，以及构建和研究语言建模科学的框架。与之前只发布模型权重和推理代码的努力不同，我们发布OLMo和整个框架，包括训练数据、训练和评估代码。我们希望这个发布能增强开放研究社区的能力，并激发更多的创新。

    Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspi
    
[^117]: 文本简化系统保留意义吗？通过阅读理解的人类评估

    Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension

    [https://arxiv.org/abs/2312.10126](https://arxiv.org/abs/2312.10126)

    该论文介绍了一个人类评估框架，通过阅读理解问题来评估简化的文本是否保留了含义。

    

    自动文本简化（TS）旨在自动化重新编写文本的过程，使其更易于人们阅读。 TS能否被使用的前提是，它应传达与原始文本意义一致的信息。然而，当前的TS评估协议评估系统输出的简易性和意义保留而不考虑输出句子出现在文档上下文中以及人们如何理解它们。在这项工作中，我们引入了一个人类评估框架，通过阅读理解问题来评估简化的文本是否保留了含义。使用这个框架，我们通过人类和九个自动系统进行了深入的人类评估。利用预训练知识的监督系统在自动可控TS系统中的阅读理解（RC）任务上取得了最高分。然而，即使是表现最佳的监督系统也在阅读理解任务中遇到了困难。

    arXiv:2312.10126v2 Announce Type: replace  Abstract: Automatic text simplification (TS) aims to automate the process of rewriting text to make it easier for people to read. A pre-requisite for TS to be useful is that it should convey information that is consistent with the meaning of the original text. However, current TS evaluation protocols assess system outputs for simplicity and meaning preservation without regard for the document context in which output sentences occur and for how people understand them. In this work, we introduce a human evaluation framework to assess whether simplified texts preserve meaning using reading comprehension questions. With this framework, we conduct a thorough human evaluation of texts by humans and by nine automatic systems. Supervised systems that leverage pre-training knowledge achieve the highest scores on the reading comprehension (RC) tasks amongst the automatic controllable TS systems. However, even the best-performing supervised system strugg
    
[^118]: 通过对抗性上下文学习优化提示

    Prompt Optimization via Adversarial In-Context Learning

    [https://arxiv.org/abs/2312.02614](https://arxiv.org/abs/2312.02614)

    提出了Adversarial In-Context Learning (adv-ICL)方法，通过生成器、鉴别器和提示修改器之间的对抗学习优化提示，在上下文学习中取得显着改进。

    

    我们提出了一种新方法，Adversarial In-Context Learning（adv-ICL），通过利用一个LLM作为生成器，另一个作为鉴别器，第三个作为提示修改器，来优化上下文学习（ICL）的提示。类似于传统的对抗性学习，adv-ICL被实现为生成器和鉴别器之间的双人博弈，其中生成器试图生成足够逼真的输出以欺骗鉴别器。 在每一轮中，给定由任务说明前缀和几个示例组成的输入，生成器产生一个输出。然后，鉴别器负责将生成器的输入-输出对分类为模型生成的还是真实数据。根据鉴别器损失，提示修改器提出了可能对生成器和鉴别器提示进行的编辑，并选择最大程度改善对抗损失的编辑。我们展示了adv-ICL相对于最先进的提示优化有显着改进。

    arXiv:2312.02614v2 Announce Type: replace-cross  Abstract: We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two-player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optim
    
[^119]: ChatGPT作为数学提问者？评估ChatGPT在生成预大学数学问题上的表现

    ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions

    [https://arxiv.org/abs/2312.01661](https://arxiv.org/abs/2312.01661)

    ChatGPT在生成预大学数学问题上的表现进行了评估，为填补现有教育问题生成模型的不足提供了新思路。

    

    数学提问对于评估学生的解决问题能力至关重要。由于手动创建这样的问题需要大量工作，因此人们已经探索了自动方法。现有的最先进模型依赖于微调策略，并且在生成涉及多步逻辑和算术推理的问题时很难。与此同时，诸如ChatGPT之类的大型语言模型(LLMs)在许多涉及逻辑和算术推理的NLP任务中表现出色。然而，它们在生成教育问题方面的应用尚未充分利用，特别是在数学领域。为了弥补这一差距，我们迈出了第一步，对ChatGPT在生成预大学数学问题方面进行了深入分析。我们的分析分为两个主要设置：基于上下文感知和不基于上下文感知。

    arXiv:2312.01661v2 Announce Type: replace-cross  Abstract: Mathematical questioning is crucial for assessing students problem-solving skills. Since manually creating such questions requires substantial effort, automatic methods have been explored. Existing state-of-the-art models rely on fine-tuning strategies and struggle to generate questions that heavily involve multiple steps of logical and arithmetic reasoning. Meanwhile, large language models(LLMs) such as ChatGPT have excelled in many NLP tasks involving logical and arithmetic reasoning. Nonetheless, their applications in generating educational questions are underutilized, especially in the field of mathematics. To bridge this gap, we take the first step to conduct an in-depth analysis of ChatGPT in generating pre-university math questions. Our analysis is categorized into two main settings: context-aware and context-unaware. In the context-aware setting, we evaluate ChatGPT on existing math question-answering benchmarks coverin
    
[^120]: BLT: 大型语言模型能处理基础法律文本吗？

    BLT: Can Large Language Models Handle Basic Legal Text?

    [https://arxiv.org/abs/2311.09693](https://arxiv.org/abs/2311.09693)

    大型语言模型在处理基础法律文本方面表现不佳，但通过针对性微调，甚至较小的模型也能在测试中表现出色，提升了相关法律任务的表现。

    

    我们发现像GPT-4、Claude和{PaLM 2}这样的最好的公开可用的LLM在处理基础法律文本方面表现不佳。我们引入了一个基准，其中包含律师和法律助理期望LLM零-shot处理的任务，比如查找证词文件的某一行或合同的某个子部分的文本。LLM在这个基准上的差劲表现对它们在法律实践中的可靠性提出了质疑。然而，针对这些任务进行微调甚至使一个较小的模型在我们的测试集上表现接近完美，并且还提升了相关法律任务的表现。这些结果表明，许多领域所需的简单行为在基础LLM中可能不存在，除非有领域专家的额外参与。

    arXiv:2311.09693v2 Announce Type: replace-cross  Abstract: We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM 2} currently perform poorly at basic legal text handling. We introduce a benchmark consisting of tasks that lawyers and paralegals would expect LLMs to handle zero-shot, such as looking up the text at a line of a witness deposition or at a subsection of a contract. LLMs' poor performance on this benchmark casts into doubt their reliability as-is for legal practice. However, fine-tuning for these tasks brings even a smaller model to near-perfect performance on our test set and also raises performance on a related legal task. These results suggest that many simple behaviors needed for a domain may not be present in foundational LLMs, without additional engagement from subject matter experts.
    
[^121]: CLEAN-EVAL：清洁评估污染大型语言模型

    CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models

    [https://arxiv.org/abs/2311.09154](https://arxiv.org/abs/2311.09154)

    Clean-Eval提出了一种清洁评估方法，通过LLM对污染数据进行释义和反向翻译，利用语义检测器过滤低质量样本，最终选择最佳候选，解决了大型语言模型评估中的数据污染问题。

    

    我们目前正处于各种大型语言模型（LLM）激烈竞争的时代，不断推动基准性能的边界。然而，由于潜在的数据污染，真正评估这些LLM的能力已经成为一个具有挑战性和关键性的问题，研究人员和工程师需要花费大量时间和精力下载和尝试这些受污染的模型。为了节省宝贵的时间，我们提出了一种新颖而有用的方法，Clean-Eval，它可以减轻数据污染问题，并以更整洁的方式评估LLM。Clean-Eval利用LLM对受污染数据进行释义和反向翻译，生成具有相同含义但不同表面形式的表达。然后使用语义检测器过滤生成的低质量样本，缩小候选集。最终从这个候选集中基于BLEURT得分选择最佳候选。

    arXiv:2311.09154v2 Announce Type: replace  Abstract: We are currently in an era of fierce competition among various large language models (LLMs) continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination, and it wastes dozens of time and effort for researchers and engineers to download and try those contaminated models. To save our precious time, we propose a novel and useful method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter the generated low-quality samples to narrow down this candidate set. The best candidate is finally selected from this set based on the BLEURT s
    
[^122]: ChOiRe：通过观点链推理表征和预测人类观点

    ChOiRe: Characterizing and Predicting Human Opinions with Chain of Opinion Reasoning

    [https://arxiv.org/abs/2311.08385](https://arxiv.org/abs/2311.08385)

    ChOiRe是一个通过观点链推理表征和预测人类观点的框架，结合用户明确和隐式的个人角色特征，实现了对人类观点的预测。

    

    将语言模型与人类观点对齐对于增强它们把握人类价值观、喜好和信仰至关重要。我们提出了ChOiRe，一个四步框架，用于预测人类观点，该框架不同地对待用户明确声明的个人角色（即人口统计或意识形态属性）和从用户历史观点推断出的隐式个人角色。ChOiRe包括：（i）一个语言模型分析用户明确的个人角色，以过滤出不相关的属性；（ii）语言模型将隐式人物观点排名成优先列表；（iii）观点链推理（CoO），其中语言模型顺序地分析明确的个人角色和最相关的隐式个人角色以执行观点预测；（iv）以及ChOiRe执行第（iii）步CoO多次，随着隐式个人角色列表不断增加来克服个人角色信息不足以推断最终结果。ChOiRe取得了新的成果。

    arXiv:2311.08385v3 Announce Type: replace  Abstract: Aligning language models (LMs) with human opinion is challenging yet vital to enhance their grasp of human values, preferences, and beliefs. We present ChOiRe, a four-step framework to predict human opinion which differentially models the user explicit personae (i.e. demographic or ideological attributes) that are manually declared, and implicit personae inferred from user historical opinions. ChOiRe consists of (i) an LM analyzing the user explicit personae to filter out irrelevant attributes; (ii) the LM ranking the implicit persona opinions into a preferential list; (iii) Chain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes the explicit personae and the most relevant implicit personae to perform opinion prediction; (iv) and where ChOiRe executes Step (iii) CoO multiple times with increasingly larger lists of implicit personae to overcome insufficient personae information to infer a final result. ChOiRe achieves new
    
[^123]: 在搜索长尾中：通过逻辑规则引导搜索系统性生成长尾推理知识

    In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search

    [https://arxiv.org/abs/2311.07237](https://arxiv.org/abs/2311.07237)

    该研究提出了一个名为LINK的框架，能够系统性地生成长尾推理知识，从而更有效地评估LLMs在推理空间中的表现。

    

    最先进的LLMs在诸如自然语言推理等推理任务上胜过人类。最近评估LLMs的研究指出，在来自低概率分布——即长尾的输入数据上表现大幅下降。因此，我们专注于系统生成涉及长尾推理知识的语句，以更有效地评估LLMs在推理空间中的表现。我们首先提出了一个新颖的框架Logic-Induced-Knowledge-Search（LINK），该框架生成基于符号规则模板的事实正确且长尾知识语句；LINK有效地生成长尾分布数据，零-shot提示的LLMs无法到达，并且在事实正确性方面优于零-shot GPT4达到5%。我们进一步使用LINK生成的数据构建了一个名为Logic-Induced-Long-Tail（LINT）的数据集，可用于评估长尾分布上的下游模型；LINT包含108K个知识条目。

    arXiv:2311.07237v2 Announce Type: replace-cross  Abstract: State-of-the-art LLMs outperform humans on reasoning tasks such as Natural Language Inference. Recent works evaluating LLMs note a marked performance drop on input data from the low-probability distribution, i.e., the longtail. Therefore, we focus on systematically generating statements involving long-tail inferential knowledge for more effective evaluation of LLMs in the reasoning space. We first propose a novel framework Logic-Induced- Knowledge-Search (LINK) that generates factually correct and long-tail knowledge statements grounded on symbolic rule templates; LINK effectively generates data in the longtail distribution that zero-shot prompted LLMs are unable to reach, and outperforms zero-shot GPT4 on factual correctness by 5%. We further use the data generated by LINK to construct a dataset Logic-Induced-Long-Tail (LINT) that can be used to evaluate downstream models on the long-tail distribution; LINT contains 108K knowl
    
[^124]: 大语言模型中的多语言越狱挑战

    Multilingual Jailbreak Challenges in Large Language Models

    [https://arxiv.org/abs/2310.06474](https://arxiv.org/abs/2310.06474)

    该研究揭示了大语言模型中存在的多语言越狱挑战，包括用户使用非英语提示绕过安全机制的非故意场景和恶意用户利用多语言提示恶意攻击LLMs的故意场景。

    

    虽然大语言模型(LLMs)在各种任务中表现出色，但它们存在潜在的安全问题，例如“越狱”问题，即恶意指令可能操纵LLMs表现出不良行为。尽管已经制定了几种预防措施来减轻与LLMs相关的潜在风险，但这些措施主要集中在英语上。本研究揭示了LLMs中存在的多语言越狱挑战，并考虑了两种潜在的风险场景：非故意和故意。非故意场景涉及用户使用非英语提示查询LLMs并无意中绕过安全机制，而故意场景涉及恶意用户将恶意指令与多语言提示结合起来，故意攻击LLMs。实验结果显示，在非故意场景中，不安全内容的比率增加了。

    arXiv:2310.06474v2 Announce Type: replace  Abstract: While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases
    
[^125]: 让循环的询问: 大型语言模型在判断中的摇摆

    Ask Again, Then Fail: Large Language Models' Vacillations in Judgement

    [https://arxiv.org/abs/2310.02174](https://arxiv.org/abs/2310.02174)

    目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。

    

    我们观察到目前的会话式语言模型在面对后续问题时往往在其判断上摇摆不定，即使原始判断是正确的。这种摇摆对于生成可靠回复和建立用户信任构成了重要挑战。为了全面评估这一问题，我们引入了一个后续问题机制以及两个度量标准来量化这种不一致性，确认了当前语言模型普遍存在这种情况。为了缓解这一问题，我们探讨了各种提示策略用于闭源模型；此外，我们开发了一个基于训练的框架Unwavering-FQ，通过合成高质量的偏好数据来教导语言模型保持其最初的正确判断。我们的实验结果验证了我们框架的有效性以及其增强模型通用能力的能力。

    arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
    
[^126]: MiniLLM：大型语言模型的知识蒸馏

    MiniLLM: Knowledge Distillation of Large Language Models

    [https://arxiv.org/abs/2306.08543](https://arxiv.org/abs/2306.08543)

    本文提出了一种将大型语言模型的知识蒸馏到更小模型的方法，通过使用反向KLD替换标准KD方法中的前向KLD目标，有效避免了学生模型高估教师分布的低概率区域。

    

    知识蒸馏（KD）是一种减少大型语言模型（LLMs）高计算需求的有前途的技术。然而，先前的KD方法主要应用于白盒分类模型或训练小模型来模仿如ChatGPT之类的黑盒模型API。如何有效地将白盒LLMs的知识蒸馏到小模型中仍未得到充分探讨，随着开源LLMs的蓬勃发展，这变得更为重要。在这项工作中，我们提出一种KD方法，将LLMs蒸馏到更小的语言模型。

    arXiv:2306.08543v2 Announce Type: replace-cross  Abstract: Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named Mi
    
[^127]: 替换语言模型用于文本风格转换

    Replacing Language Model for Style Transfer

    [https://arxiv.org/abs/2211.07343](https://arxiv.org/abs/2211.07343)

    提出了一种替换语言模型（RLM），结合了自回归模型的灵活性和非自回归模型的准确性，在文本风格转换中实现了更精确的生成控制。

    

    我们引入了一种称为替换语言模型（RLM）的序列到序列语言建模框架，用于文本风格转换（TST）。我们的方法自回归地将源句子的每个标记替换为具有类似含义但具有目标风格的文本片段。新的片段是通过非自回归掩蔽语言模型生成的，可以更好地保留替换标记的局部上下文含义。这种RLM生成方案汇集了自回归模型的灵活性和非自回归模型的准确性，弥合了句子级和词级风格转换方法之间的差距。为了更精确地控制生成风格，我们在RLM的隐藏表示上进行了标记级风格内容解缠。实证结果表明，与其他TST基线相比，RLM在真实文本数据集上的有效性。代码在https://github.com/Linear95/RLM。

    arXiv:2211.07343v2 Announce Type: replace  Abstract: We introduce replacing language model (RLM), a sequence-to-sequence language modeling framework for text style transfer (TST). Our method autoregressively replaces each token of the source sentence with a text span that has a similar meaning but in the target style. The new span is generated via a non-autoregressive masked language model, which can better preserve the local-contextual meaning of the replaced token. This RLM generation scheme gathers the flexibility of autoregressive models and the accuracy of non-autoregressive models, which bridges the gap between sentence-level and word-level style transfer methods. To control the generation style more precisely, we conduct a token-level style-content disentanglement on the hidden representations of RLM. Empirical results on real-world text datasets demonstrate the effectiveness of RLM compared with other TST baselines. The code is at https://github.com/Linear95/RLM.
    
[^128]: 实时问答系统：现在的答案是什么？

    RealTime QA: What's the Answer Right Now?

    [https://arxiv.org/abs/2207.13332](https://arxiv.org/abs/2207.13332)

    该论文介绍了REALTIME QA，一个动态问答平台，挑战静态开放领域QA数据集假设，强调对实时信息的重要性，并展示了基于GPT-3的实时评估结果。

    

    我们介绍了REALTIME QA，这是一个动态问答平台，定期发布问题并评估系统（本版本每周一次）。REALTIME QA询问当前世界，QA系统需要回答关于新事件或信息的问题。因此，它挑战了开放领域QA数据集中的静态、传统假设，并追求即时应用。我们基于大型预训练语言模型（包括GPT-3和T5）构建了强大的基线模型。我们的基准测试工作是一个持续的努力，本文呈现了过去一年的实时评估结果。我们的实验结果表明，GPT-3通常可以根据新检索的文档正确地更新其生成结果，突显了更新至关重要的信息检索的重要性。然而，我们发现在检索到的文档未提供足够信息以找到答案时，GPT-3往往会返回过时的答案。

    arXiv:2207.13332v2 Announce Type: replace  Abstract: We introduce REALTIME QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). REALTIME QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open-domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer
    
[^129]: 对束搜索在文本生成中的工作原理及停止时机的明确要求

    A Call for Clarity in Beam Search: How It Works and When It Stops

    [https://arxiv.org/abs/2204.05424](https://arxiv.org/abs/2204.05424)

    提出了一个简单修改的耐心因子来改善束搜索解码算法，提高了强预训练模型在文本摘要和机器翻译任务上的性能。

    

    使用束搜索进行文本生成在各种应用中已被证明成功。我们指出，尽管在文献中被大多数忽视，但常用的束解码实现（例如Hugging Face Transformers和fairseq）使用了先到先服务的启发式方法：它在时间步长上保留一组已完成的序列，并在此集合大小达到束大小时停止。基于这一发现，我们引入了一个耐心因子，对束解码实现进行了简单修改，使停止条件更一般化，并为搜索深度提供了灵活性。实证结果表明，调整这个耐心因子改善了强预训练模型在新闻文本摘要和各种语言对的机器翻译中的解码性能，而推理速度下降微不足道。我们的方法只修改了一行代码，因此可以很容易地在任何i中被纳入。

    arXiv:2204.05424v3 Announce Type: replace  Abstract: Text generation with beam search has proven successful in a wide range of applications. We point out that, though largely overlooked in the literature, the commonly-used implementation of beam decoding (e.g., Hugging Face Transformers and fairseq) uses a first come, first served heuristic: it keeps a set of already completed sequences over time steps and stops when the size of this set reaches the beam size. Based on this finding, we introduce a patience factor, a simple modification to this beam decoding implementation, that generalizes the stopping criterion and provides flexibility to the depth of search. Empirical results demonstrate that adjusting this patience factor improves decoding performance of strong pretrained models on news text summarization and machine translation over diverse language pairs, with a negligible inference slowdown. Our approach only modifies one line of code and can be thus readily incorporated in any i
    
[^130]: LegalDuet: 通过双视角法律线索推理学习有效的法律判决预测表示

    LegalDuet: Learning Effective Representations for Legal Judgment Prediction through a Dual-View Legal Clue Reasoning. (arXiv:2401.15371v1 [cs.CL])

    [http://arxiv.org/abs/2401.15371](http://arxiv.org/abs/2401.15371)

    LegalDuet是一种通过双视角法律线索推理模型，使用预训练语言模型学习定制嵌入空间来进行法律判决预测。该模型通过法律案例推理和法律基础推理两个推理链进行判决。在实验中，LegalDuet在CAIL2018数据集上表现出最先进的性能，并超过了基线模型。

    

    大多数现有的法律判决预测（LJP）模型侧重于发现刑事事实描述中的法律线索。然而，在现实场景中，专业法官不仅需要吸收过去判决的法律案例经验，还依赖于从专业法律知识中学到的专业法律基础推理。本文提出了一种名为LegalDuet的模型，该模型预训练语言模型以学习用于进行法律判决的定制嵌入空间。它提出了一种双视角法律线索推理机制，由两个推理链组成：1）法律案例推理，根据从类比/混淆的法律案例中学到的判决经验进行法律判决；2）法律基础推理，通过匹配刑事案件和法律决定之间的法律线索。实验证明，LegalDuet在CAIL2018数据集上实现了最先进的性能，并且超过了基线模型。

    Most existing Legal Judgment Prediction (LJP) models focus on discovering the legal triggers in the criminal fact description. However, in real-world scenarios, a professional judge not only needs to assimilate the law case experience that thrives on past sentenced legal judgments but also depends on the professional legal grounded reasoning that learned from professional legal knowledge. In this paper, we propose a LegalDuet model, which pretrains language models to learn a tailored embedding space for making legal judgments. It proposes a dual-view legal clue reasoning mechanism, which derives from two reasoning chains of judges: 1) Law Case Reasoning, which makes legal judgments according to the judgment experiences learned from analogy/confusing legal cases; 2) Legal Ground Reasoning, which lies in matching the legal clues between criminal cases and legal decisions. Our experiments show that LegalDuet achieves state-of-the-art performance on the CAIL2018 dataset and outperforms bas
    
[^131]: BIBench: 大型语言模型数据分析知识基准测试

    BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])

    [http://arxiv.org/abs/2401.02982](http://arxiv.org/abs/2401.02982)

    BIBench是一个旨在评估大型语言模型（LLMs）在商业智能（BI）数据分析领域中能力的综合基准测试，其通过测试模型在BI基础知识、应用知识和技术技能三个维度上的表现来进行评估。

    

    大型语言模型（LLMs）在各种任务中展示了令人印象深刻的能力。然而，它们在数据分析的专业领域中的熟练度和可靠性，特别是在以数据驱动思维为重点的领域中，仍然存在不确定性。为了填补这一差距，我们介绍了BIBench，这是一个全面的基准测试，旨在评估LLMs在商业智能（BI）的背景下的数据分析能力。BIBench通过三个维度评估LLMs：1）BI基础知识，评估模型的数值推理能力和对金融概念的熟悉程度；2）BI知识应用，确定模型快速理解文本信息并从多个视角生成分析问题的能力；3）BI技术技能，检查模型使用技术知识解决现实数据分析挑战的能力。BIBench包括11个子任务，涵盖分类、提取和生成三种任务类型。

    Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additi
    
[^132]: 通过多视角解耦学习改进低资源的基于提示的关系表示

    Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning. (arXiv:2312.17267v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.17267](http://arxiv.org/abs/2312.17267)

    提出了一种名为MVRE的新方法，通过将关系解耦为不同的视角，生成多视角关系表示，并利用预训练语言模型（PLMs）的能力来提高低资源关系抽取任务的性能。

    

    最近，使用预训练语言模型（PLMs）进行提示调整已经展示出了显著的关系抽取（RE）任务的增强能力。然而，在低资源场景中，即训练数据有限的情况下，由于对关系的表层理解，先前基于提示的方法可能仍然表现不佳，用于表示学习。为此，我们强调在低资源场景中学习高质量关系表示对于RE的重要性，并提出了一种新的基于提示的关系表示方法，名为MVRE（多视角关系抽取），以更好地利用PLMs的能力来改善低资源提示调整范式下的RE性能。具体而言，MVRE将每个关系解耦为不同的视角，以包含多视角的关系表示，以最大化关系推断过程中的似然性。此外，我们还设计了一个全局性的低领域任务学习策略，以进一步提高关系表示的质量。

    Recently, prompt-tuning with pre-trained language models (PLMs) has demonstrated the significantly enhancing ability of relation extraction (RE) tasks. However, in low-resource scenarios, where the available training data is scarce, previous prompt-based methods may still perform poorly for prompt-based representation learning due to a superficial understanding of the relation. To this end, we highlight the importance of learning high-quality relation representation in low-resource scenarios for RE, and propose a novel prompt-based relation representation method, named MVRE (\underline{M}ulti-\underline{V}iew \underline{R}elation \underline{E}xtraction), to better leverage the capacity of PLMs to improve the performance of RE within the low-resource prompt-tuning paradigm. Specifically, MVRE decouples each relation into different perspectives to encompass multi-view relation representations for maximizing the likelihood during relation inference. Furthermore, we also design a Global-Lo
    
[^133]: InstructCoder: 为代码编辑赋能的语言模型。

    InstructCoder: Empowering Language Models for Code Editing. (arXiv:2310.20329v1 [cs.CL])

    [http://arxiv.org/abs/2310.20329](http://arxiv.org/abs/2310.20329)

    本研究旨在探索使用大型语言模型（LLMs）进行代码编辑，并引入了InstructCoder数据集，该数据集包含多样性的代码编辑任务，为通用代码编辑提供支持。

    

    代码编辑涵盖了开发者日常处理的各种实用任务。尽管其相关性和实用性，但自动代码编辑仍然是深度学习模型演化中尚未充分探索的领域，部分原因是数据稀缺。在本研究中，我们探索了使用大型语言模型（LLMs）根据用户指令编辑代码的方法，涵盖了诸如注释插入，代码优化和代码重构等一系列隐含任务。为了实现这一目标，我们引入了InstructCoder，这是第一个专为通用代码编辑而设计的数据集，包含高多样性的代码编辑任务。该数据集包含超过114,000个指令-输入-输出三元组，并涵盖了多个不同的代码编辑场景。数据集通过一个迭代过程进行系统扩展，该过程从GitHub的提交中获取代码编辑数据作为种子任务。种子任务和生成的任务随后用于提示ChatGPT获取更多任务数据。

    Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of large language models (LLMs) to edit code based on user instructions, covering a broad range of implicit tasks such as comment insertion, code optimization, and code refactoring. To facilitate this, we introduce InstructCoder, the first dataset designed to adapt LLMs for general-purpose code editing, containing highdiversity code-editing tasks. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The dataset is systematically expanded through an iterative process that commences with code editing data sourced from GitHub commits as seed tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for more task data. Our exper
    
[^134]: MindShift: 利用大型语言模型进行基于心态的问题性智能手机使用干预

    MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention. (arXiv:2309.16639v1 [cs.CL])

    [http://arxiv.org/abs/2309.16639](http://arxiv.org/abs/2309.16639)

    MindShift利用大型语言模型实现了基于心态的问题性智能手机使用干预，通过动态生成适应用户环境和心理状态的高质量说服内容来帮助用户解决问题性智能手机使用的困扰。

    

    问题性智能手机使用对身体和心理健康有负面影响。尽管有大量的先前研究，现有的说服技巧不足以根据用户的身体环境和心理状态提供动态说服内容。我们首先进行了一项人为操作研究（N = 12）和一项访谈研究（N = 10），总结了问题性智能手机使用背后的心态：无聊、压力和惯性。这为我们设计了四种说服策略：理解、安抚、唤起和支持习惯。我们利用大型语言模型（LLMs）实现了有效说服内容的自动和动态生成。我们开发了一种新颖的LLM技术驱动的问题性智能手机使用干预技术MindShift。MindShift根据用户当下的身体环境、心态、应用使用行为、用户的目标与习惯作为输入，并生成具有适当说服策略的高质量和灵活的说服内容。我们进行了一项5-

    Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users' physical contexts and mental states. We first conduct a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leverage large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We develop MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users' in-the-moment physical contexts, mental states, app usage behaviors, users' goals & habits as input, and generates high-quality and flexible persuasive content with appropriate persuasion strategies. We conduct a 5-
    
[^135]: 通过共线约束注意力解决Transformer的头痛问题

    Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])

    [http://arxiv.org/abs/2309.08646](http://arxiv.org/abs/2309.08646)

    通过引入共线约束注意力（CoCA）结构，解决Transformer模型中的头痛问题，实现了出色的外推性能和提高的计算效率。

    

    随着基于大型语言模型的实际应用的快速进展，推断性能的外推变得在研究领域中变得越来越重要。在我们的研究中，我们发现了Transformer模型中的一个被之前忽视的异常行为，导致了最接近的标记之间的混乱，这些标记携带了最重要的信息。我们将这一发现称为“Transformer的头痛问题”。为了从根本上解决这个问题，我们引入了一种新的自注意结构，命名为Collinear Constrained Attention（CoCA）。这个结构可以无缝地与现有的推断、插值方法和其他针对传统Transformer模型设计的优化策略集成。我们在推断过程中实现了优秀的外推性能，即使是16到24倍的序列长度，而且没有对我们的模型进行任何微调。我们还增强了CoCA的计算和空间效率，以确保其实用性。我们计划...

    As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
    
[^136]: 视觉和语言模型中短语定位和任务表现的联合研究

    A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models. (arXiv:2309.02691v1 [cs.CL])

    [http://arxiv.org/abs/2309.02691](http://arxiv.org/abs/2309.02691)

    这项研究提出了一个框架来研究视觉和语言模型中短语定位和任务性能之间的关系，并且通过验证实验发现了当代模型在短语定位和任务求解方面的不一致性。

    

    在需要对视觉背景中的自然语言进行推理的任务中，关键是将单词和短语与图像区域联系起来。然而，即使通常预期以有助于泛化的方式解决任务，观察到当代模型中的这种定位也是复杂的。我们提出了一个框架来共同研究任务执行和短语定位，并提出了三个基准来研究两者之间的关系。我们的结果表明，当代模型在定位短语和解决任务的能力之间存在不一致性。我们展示了如何通过对定位标注进行强制性训练来解决这个问题，并分析了它所创建的动态。代码可在https://github.com/lil-lab/phrase_grounding获得。

    Key to tasks that require reasoning about natural language in visual contexts is grounding words and phrases to image regions. However, observing this grounding in contemporary models is complex, even if it is generally expected to take place if the task is addressed in a way that is conductive to generalization. We propose a framework to jointly study task performance and phrase grounding, and propose three benchmarks to study the relation between the two. Our results show that contemporary models demonstrate inconsistency between their ability to ground phrases and solve tasks. We show how this can be addressed through brute-force training on ground phrasing annotations, and analyze the dynamics it creates. Code and at available at https://github.com/lil-lab/phrase_grounding.
    
[^137]: 《中文拼写纠错作为改写语言模型》

    Chinese Spelling Correction as Rephrasing Language Model. (arXiv:2308.08796v1 [cs.CL])

    [http://arxiv.org/abs/2308.08796](http://arxiv.org/abs/2308.08796)

    本文提出了一种新颖的中文拼写纠错方法，通过改写语言建模来重新表达整个句子，而不是仅仅依赖错误模式进行字符级别标注，取得了最新的最优结果。

    

    本文研究了中文拼写纠错（CSC），旨在检测和纠正给定句子中的潜在拼写错误。目前最先进的方法将CSC视为序列标注任务，并在句子对上微调基于BERT的模型。然而，我们注意到在将一个字符标记为另一个字符的过程中存在一个关键缺陷，即纠正过程过于依赖错误。这与人类思维相反，人们根据句子的语义重新表达整个句子，而不仅仅是基于之前记忆的错误模式。这种违反直觉的学习过程导致机器拼写纠错的泛化能力和可迁移性受到限制。为了解决这个问题，我们提出了“改写语言建模”（ReLM），其中模型通过填充额外的位置来重新表达整个句子，而不是进行字符级别的标注。这种新颖的训练范式在微调后取得了最新的最优结果。

    This paper studies Chinese Spelling Correction (CSC), which aims to detect and correct potential spelling errors in a given sentence. Current state-of-the-art methods regard CSC as a sequence tagging task and fine-tune BERT-based models on sentence pairs. However, we note a critical flaw in the process of tagging one character to another, that the correction is excessively conditioned on the error. This is opposite from human mindset, where individuals rephrase the complete sentence based on its semantics, rather than solely on the error patterns memorized before. Such a counter-intuitive learning process results in the bottleneck of generalizability and transferability of machine spelling correction. To address this, we propose $Rephrasing Language Modeling$ (ReLM), where the model is trained to rephrase the entire sentence by infilling additional slots, instead of character-to-character tagging. This novel training paradigm achieves the new state-of-the-art results across fine-tuned 
    
[^138]: 探究代码语言模型所学习的内容

    Towards Understanding What Code Language Models Learned. (arXiv:2306.11943v1 [cs.SE])

    [http://arxiv.org/abs/2306.11943](http://arxiv.org/abs/2306.11943)

    本研究探究了预先训练的代码语言模型的能力，证明其能够超越表面形式特征，学习精确而形式化定义的代码的计算语义。

    

    预先训练的语言模型在各种自然语言任务中都非常有效，但有人认为它们的能力不足以完全学习语言的意义或理解语言。为了了解语言模型能够学习某种形式的意义的程度，我们研究它们捕捉代码语义的能力，超越表层频率和共现的限制。与以往研究模型语言特征的探究相比，我们在一种可以客观地、简单明了地评估模型学习语义能力的环境下研究预训练模型。本文研究了这样的模型是否能捕捉精确而形式化定义的代码的语义。通过对代码片段的操纵实验，我们展示了代码预先训练模型学习了代码的计算语义的强有力的表征，超越了代码表面特征。

    Pre-trained language models are effective in a variety of natural language tasks, but it has been argued their capabilities fall short of fully learning meaning or understanding language. To understand the extent to which language models can learn some form of meaning, we investigate their ability to capture semantics of code beyond superficial frequency and co-occurrence. In contrast to previous research on probing models for linguistic features, we study pre-trained models in a setting that allows for objective and straightforward evaluation of a model's ability to learn semantics. In this paper, we examine whether such models capture the semantics of code, which is precisely and formally defined. Through experiments involving the manipulation of code fragments, we show that code pre-trained models of code learn a robust representation of the computational semantics of code that goes beyond superficial features of form alone
    
[^139]: 对大型语言模型调查响应的质疑

    Questioning the Survey Responses of Large Language Models. (arXiv:2306.07951v1 [cs.CL])

    [http://arxiv.org/abs/2306.07951](http://arxiv.org/abs/2306.07951)

    本文使用美国人口普查局建立的全美社区调查（ACS）评估了十几个不同大小的语言模型，发现小型模型具有显著的位置和标签偏差，而模型大小的增加能减轻这种偏差，但无法根据US群体或任何可识别的群体趋势进行调整。

    

    随着大型语言模型的能力增强，研究人员开始以各种科学动机对这些模型进行调查。本文旨在通过美国人口普查局已经建立的全美社区调查（ACS），就模型的调查响应结果探究所能了解的内容。我们对十几个不同大小的模型进行了评估，这些模型的参数范围从几亿到一万亿不等，使用ACS的问题进行了数十万次的测试，系统地得出了两个主要模式。首先，小型模型存在明显的位置和标签偏差，例如偏向于采用标记为“A”的调查响应。随着模型尺寸的增加，A-偏差虽然有所减少，但也进展缓慢。其次，即使通过随机答案顺序来调整这种标记偏差，模型仍然不会趋向于美国人口统计数据或任何可识别的人口排序。相反，各种模型趋向于均匀随机化。

    As large language models increase in capability, researchers have started to conduct surveys of all kinds on these models with varying scientific motivations. In this work, we examine what we can learn from a model's survey responses on the basis of the well-established American Community Survey (ACS) by the U.S. Census Bureau. Evaluating more than a dozen different models, varying in size from a few hundred million to ten billion parameters, hundreds of thousands of times each on questions from the ACS, we systematically establish two dominant patterns. First, smaller models have a significant position and labeling bias, for example, towards survey responses labeled with the letter "A". This A-bias diminishes, albeit slowly, as model size increases. Second, when adjusting for this labeling bias through randomized answer ordering, models still do not trend toward US population statistics or those of any cognizable population. Rather, models across the board trend toward uniformly rando
    
[^140]: 基于扩散的语音增强方法与联合生成预测解码器

    Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders. (arXiv:2305.10734v1 [cs.SD])

    [http://arxiv.org/abs/2305.10734](http://arxiv.org/abs/2305.10734)

    本文提出了一种集成生成和预测信息的基于扩散的语音增强系统，其中两个语音增强模块在第一和最后一个扩散步骤中被融合，实验结果表明扩散评分估计可以从预测信息中受益并加快解码过程。

    

    近期研究了基于扩散的语音增强方法，但其解码过程非常耗时。解决方案之一是使用预测性语音增强系统估计增强特征，然后初始化解码过程。然而，这种两阶段方法忽略了预测性与扩散性语音增强之间的互补性。本文提出了一种集成这两个语音增强模块的统一系统。该系统编码生成和预测信息，然后应用生成和预测解码器，它们的输出被融合。具体地，两个语音增强模块在第一和最后一个扩散步骤中被融合：第一个步骤融合使用预测性语音增强来初始化扩散过程，以提高收敛速度；最后一个步骤融合将两个互补的语音增强输出组合起来，以提高语音增强性能。在Voice-Bank数据集上进行的实验表明，扩散评分估计可以从预测信息中获益并加快解码过程。

    Diffusion-based speech enhancement (SE) has been investigated recently, but its decoding is very time-consuming. One solution is to initialize the decoding process with the enhanced feature estimated by a predictive SE system. However, this two-stage method ignores the complementarity between predictive and diffusion SE. In this paper, we propose a unified system that integrates these two SE modules. The system encodes both generative and predictive information, and then applies both generative and predictive decoders, whose outputs are fused. Specifically, the two SE modules are fused in the first and final diffusion steps: the first step fusion initializes the diffusion process with the predictive SE for improving the convergence, and the final step fusion combines the two complementary SE outputs to improve the SE performance. Experiments on the Voice-Bank dataset show that the diffusion score estimation can benefit from the predictive information and speed up the decoding.
    
[^141]: PROM：基于预训练的短语级复制机制，用于抽象摘要生成

    PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization. (arXiv:2305.06647v1 [cs.CL])

    [http://arxiv.org/abs/2305.06647](http://arxiv.org/abs/2305.06647)

    提出了一种新的短语级复制机制-PROM，可以增强对n-gram的注意力，用于具有预训练的零样本摘要生成，大大提高了摘要的质量和可信度。

    

    基于预训练语言模型在抽象摘要生成方面的显著成就，复制机制通过提高事实性、稳定性和整体性能方面的改进证明其有助于这一进程。本文提出一种新的短语级复制机制-PROM，它增强了对n-gram的注意力，可以应用于具有预训练的零样本摘要生成中。PROM添加了一个指示器层，以明确从源中可以复制的n-gram中的令牌，并计算复制预测的辅助损失。实证研究表明，在基准微调中，PROM取得了显著改进。在零样本设置中，PROM用于对原始语料库进行自监督预训练，并在广泛的摘要数据集上提供了新的通用基线。进一步的分析表明，PROM执行更合理的复制并有助于保持忠实度。

    Based on the remarkable achievements of pre-trained language models in abstractive summarization, the copying mechanism has proved helpful by improving the factuality, stability, and overall performance. This work proposes PROM, a new PhRase-level cOpying Mechanism that enhances attention on n-grams, which can be applied to zero-shot summarization with pre-training. PROM adds an indicator layer to explicitly pick up tokens in n-gram that can be copied from the source, and calculates an auxiliary loss for the copying prediction. Empirical studies show that PROM makes significant improvements in fine-tuning on benchmarks. In zero-shot setting, PROM is utilized in the self-supervised pre-training on raw corpora and provides new general baselines on a wide range of summarization datasets. Further analysis shows that PROM performs more reasonable copying and contributes to faithfulness.
    

