# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution](https://rss.arxiv.org/abs/2402.01586) | 本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。 |
| [^2] | [Vaccine: Perturbation-aware Alignment for Large Language Model](https://rss.arxiv.org/abs/2402.01109) | 疫苗是一种针对大规模语言模型的干扰感知对齐技术，通过逐渐添加扰动产生不变的隐藏嵌入，提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。 |
| [^3] | [Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding](https://arxiv.org/abs/2402.12374) | Sequoia是一种可扩展、稳健且硬件感知的推测解码算法，通过引入动态规划算法优化标记树结构、采用新颖的采样和验证方法实现稳健性能以及硬件感知的树优化器最大化推测性能。 |
| [^4] | [HunFlair2 in a cross-corpus evaluation of named entity recognition and normalization tools](https://arxiv.org/abs/2402.12372) | 挑战BTM工具在不同上下文中应用的可靠性，通过跨语料基准测试评估命名实体识别和归一化工具的性能。 |
| [^5] | [AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies](https://arxiv.org/abs/2402.12370) | 通过提出ANALOBENCH基准来评估语言模型（LMs）进行类比推理的能力，发现扩展LMs规模对于处理涉及长场景或相关经验回忆的类比时带来的性能提升较小。 |
| [^6] | [A synthetic data approach for domain generalization of NLI models](https://arxiv.org/abs/2402.12368) | 本研究提出了一种新方法，通过生成多样领域和长度的合成NLI数据，解决了NLI模型在领域泛化方面的问题。 |
| [^7] | [A Critical Evaluation of AI Feedback for Aligning Large Language Models](https://arxiv.org/abs/2402.12366) | 研究质疑复杂的强化学习在AI反馈中的必要性，表明使用更强的教师模型进行监督微调可以超越现有的RLAIF管道。 |
| [^8] | [Emergent Word Order Universals from Cognitively-Motivated Language Models](https://arxiv.org/abs/2402.12363) | 认知驱动的语言模型显示出可以解释许多词序普遍规律的优势 |
| [^9] | [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354) | LoRA+通过设置不同的学习率来改进原始LoRA的低效率问题，在保持计算成本不变的情况下提高了模型性能和微调速度。 |
| [^10] | [Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge](https://arxiv.org/abs/2402.12352) | 基于Retrieval Augmented Generation (RAG)的图检索器被提议用来克服LLMs的局限，通过从外部数据集检索的上下文来增强提示信息，从而解决生物医学领域长尾知识的捕获挑战。 |
| [^11] | [GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations](https://arxiv.org/abs/2402.12348) | 该论文通过博弈论任务评估了LLMs在竞争环境中的推理能力，观察到LLMs在不同游戏场景下表现出不同行为，具有重要的战略推理局限性。 |
| [^12] | [Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!](https://arxiv.org/abs/2402.12343) | 安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。 |
| [^13] | [Triple-Encoders: Representations That Fire Together, Wire Together](https://arxiv.org/abs/2402.12332) | 通过三重编码器计算话语混合，实现了对话模型的显着改进和零-shot泛化性能 |
| [^14] | [Query-Based Adversarial Prompt Generation](https://arxiv.org/abs/2402.12329) | 该研究提出了一种基于查询的对抗性攻击方法，通过利用远程语言模型的 API 访问构造对抗性示例，使模型以更高概率发出有害字符串，而非仅仅基于模型之间的转移性攻击。 |
| [^15] | [Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents](https://arxiv.org/abs/2402.12327) | 该研究揭示了LLM代理甚至在竞争环境中也能自发形成合作关系的能力，验证了计算社会科学的愿景，表明LLM代理可以用于模拟人类社会互动，包括自发合作的互动，为社会现象提供洞察。 |
| [^16] | [LLM Agents for Psychology: A Study on Gamified Assessments](https://arxiv.org/abs/2402.12326) | 本研究提出了PsychoGAT（心理游戏代理）以实现心理评估的通用游戏化，通过将强大的LLM代理纳入角色，将标准量表转化为个性化且具有吸引力的互动小说游戏。 |
| [^17] | [ARKS: Active Retrieval in Knowledge Soup for Code Generation](https://arxiv.org/abs/2402.12317) | ARKS是一种用于代码生成的先进策略，通过活跃检索和整合各种信息源，能够提高大型语言模型的性能，为解决与频繁更新的库和长尾编程语言相关的现实编程问题提供了新的可能性。 |
| [^18] | [TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs](https://arxiv.org/abs/2402.12309) | TILP提出了一种用于学习时间逻辑规则的可微框架，通过设计受限随机游走机制和引入时间操作符，有效建模了时间特征，并在两个基准数据集上展示了其优越性能。 |
| [^19] | [Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports](https://arxiv.org/abs/2402.12298) | 这项研究比较了商业模型GPT-3.5 Turbo和GPT-4与开源模型Mistral-7B、Mixtral-8x7B、Llama2-13B、Llama2-70B、QWEN1.5-72B以及CheXbert和CheXpert-labeler在准确标记X射线文本报告中多发现存在的能力。 |
| [^20] | [KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students](https://arxiv.org/abs/2402.12291) | KARL是一种基于DKT的学生模型，利用检索和BERT嵌入来实现高效准确的学生记忆预测，在AUC和校准误差方面优于现有学生模型，并提出了新颖的教学策略。 |
| [^21] | [Ontology Enhanced Claim Detection](https://arxiv.org/abs/2402.12282) | 本文提出了一种基于本体增强的模型，用于句子级主张检测，在小型不平衡数据集上取得了最佳结果，并展示了添加领域特定特征和本体嵌入有助于改善传统机器学习方法。 |
| [^22] | [Adaptive Skeleton Graph Decoding](https://arxiv.org/abs/2402.12280) | 提出了骨架图解码（SGD）方法，利用子问题之间的依赖关系进行信息转发，改善响应质量且提高性能。 |
| [^23] | [Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks](https://arxiv.org/abs/2402.12279) | 通过微调学习率可以缓解零样本跨语言生成中以错误语言生成的问题，全面微调模型是很强大的基线，mBART在这个任务中表现类似于同等大小的mT5。 |
| [^24] | [WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment](https://arxiv.org/abs/2402.12275) | 通过编写代码和与环境交互来构建世界模型的基于模型的LLM代理在样本效率上优于深度RL，并在计算效率上优于ReAct风格的代理。 |
| [^25] | [High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models](https://arxiv.org/abs/2402.12267) | 大型语言模型(LLMs)在严重资源匮乏语言领域的数据文本生成中轻松刷新最新发展水平，展现出弥合性能差距的巨大潜力。 |
| [^26] | [Uncertainty quantification in fine-tuned LLMs using LoRA ensembles](https://arxiv.org/abs/2402.12264) | 使用LoRA集成在精调LLMs中提出了一种原则性不确定性量化方法，通过对不同数据域的低秩适应集成分析，推测了模型对特定架构难以学习的数据领域的信号。 |
| [^27] | [NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms](https://arxiv.org/abs/2402.12261) | 本研究通过创建一个多样化的最新英语新词资源，分析了大型语言模型对于新词的鲁棒性表现，并构建了一个基准用于评估模型对新词的泛化能力，结果显示机器翻译中模型性能会因引入新词而几乎减半。 |
| [^28] | [Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition](https://arxiv.org/abs/2402.12255) | GPT-4在支持人类进行头脑风暴方面表现出色，但在没有人类干预的情况下无法做到对相关作品进行详细的合成。 |
| [^29] | [Analysis of Levenshtein Transformer's Decoder and Its Variants](https://arxiv.org/abs/2402.12249) | 本文分析了Levenshtein Transformer（LevT）的解码器，探讨了解码结果长度、子词生成和删除模块的能力，旨在找出解码器的弱点以便未来改进。同时比较了原始LevT、知识蒸馏LevT、带有翻译记忆的LevT以及带有翻译记忆的知识蒸馏LevT的翻译结果。 |
| [^30] | [Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark](https://arxiv.org/abs/2402.12243) | 研究深入分析了文本到SQL领域中的噪声对模型的影响，并发现在BIRD-Bench基准测试中存在大量问题和标准查询中的噪声，这会显著影响模型的性能。 |
| [^31] | [Task-Oriented Dialogue with In-Context Learning](https://arxiv.org/abs/2402.12234) | 该论文描述了一个结合大型语言模型的上下文学习和确定性执行业务逻辑的任务型对话系统构建系统的方法，实验显示相比于传统方法，使用该系统开发聊天机器人需要的工作量明显减少，并且具有处理复杂对话和扩展任务型对话系统到大量任务的优势。 |
| [^32] | [Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers](https://arxiv.org/abs/2402.12233) | 通过对大型语言模型的知识编辑和微调任务进行比较，进一步理解了在转换器前馈层中更新关键或值的不同方法。 |
| [^33] | [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226) | AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。 |
| [^34] | [CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation](https://arxiv.org/abs/2402.12222) | CovRL使用强化学习结合大型语言模型和覆盖反馈，通过构建加权覆盖映射并应用于基于LLM的变异器，以提升对JavaScript引擎的模糊测试效果 |
| [^35] | [Reformatted Alignment](https://arxiv.org/abs/2402.12219) | 本文提出了一种名为ReAlign的简单有效方法，通过重新格式化指导数据的响应，显著提升了大型语言模型（LLMs）与人类价值观的对齐能力。 |
| [^36] | [Polarization of Autonomous Generative AI Agents Under Echo Chambers](https://arxiv.org/abs/2402.12212) | 在回声室环境中，基于ChatGPT的自主生成AI代理往往会发生极化，这对于了解人工智能代理在社交网络中的行为具有重要意义 |
| [^37] | [Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages](https://arxiv.org/abs/2402.12204) | 通过自蒸馏从资源丰富的语言进行方法，提高了大型语言模型在多语言任务上的性能。 |
| [^38] | [Zero shot VLMs for hate meme detection: Are we there yet?](https://arxiv.org/abs/2402.12198) | 本研究探讨了零-shot分类在处理复杂任务如恶意模因检测中的有效性 |
| [^39] | [Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion](https://arxiv.org/abs/2402.12195) | 提出了一个两阶段范式"浏览和集中"，通过在将特征输入LLMs之前进行深入的多模态上下文融合，解决了多模态内容理解中的 prior-LLM 模态隔离问题 |
| [^40] | [A Chinese Dataset for Evaluating the Safeguards in Large Language Models](https://arxiv.org/abs/2402.12193) | 该研究介绍了一个用于评估中文LLMs安全性的数据集，提出了细粒度的安全评估标准，以及扩展了两种场景用于识别有风险提示拒绝的虚阔负面和错误肯定示例。 |
| [^41] | [Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships](https://arxiv.org/abs/2402.12189) | 攻击者通过对预训练LM进行对抗微调，以放大原始训练数据的曝光，采用伪标签和机器生成概率来加强LM对预训练数据的保留。 |
| [^42] | [Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning](https://arxiv.org/abs/2402.12177) | Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。 |
| [^43] | [BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence](https://arxiv.org/abs/2402.12174) | BIDER通过知识综合和偏好对齐，将检索文档转化为关键支持证据，从而提高了LLMs的答案质量并减少了输入内容长度。 |
| [^44] | [Unsupervised LLM Adaptation for Question Answering](https://arxiv.org/abs/2402.12170) | 提出了无监督LLM适应问答任务，通过利用预训练的LLM和目标领域的未标记文档，实现在新领域回答问题的目标。 |
| [^45] | [Transformer-based Causal Language Models Perform Clustering](https://arxiv.org/abs/2402.12151) | Transformer-based因果语言模型通过在隐藏空间内对数据进行聚类来学习任务特定信息，这种聚类过程在学习中动态演变，并有助于处理未见实例。 |
| [^46] | [Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One](https://arxiv.org/abs/2402.12150) | 提出了一种通过提示大型语言模型（LLMs）具有特定角色以表达多样观点的方法，并开发了FairThinking流水线，以实现公平表达。 |
| [^47] | [End-to-end multilingual fact-checking at scale](https://arxiv.org/abs/2402.12147) | 使用Factiverse AI模型，可以进行跨语言的端到端事实核查，并且通过实验证明，为事实核查任务进行微调的模型优于大型语言模型。 |
| [^48] | [Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement](https://arxiv.org/abs/2402.12146) | Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。 |
| [^49] | [Evaluating Image Review Ability of Vision Language Models](https://arxiv.org/abs/2402.12121) | 本论文通过引入基于排名相关分析的评估方法，探讨了大规模视觉语言模型（LVLM）在生成图像评价文本方面的能力，并创建了一个评估数据集来验证这种方法。 |
| [^50] | [Is It a Free Lunch for Removing Outliers during Pretraining?](https://arxiv.org/abs/2402.12102) | 通过确保归一化对序列长度不变，我们改进了一种预训练方法，使其在移除异常值的同时促进了因果语言模型的成功预训练。 |
| [^51] | [Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation](https://arxiv.org/abs/2402.12100) | Groot是第一个利用基于树的语义转换进行对抗测试文本到图像模型的自动化框架，成功率高达93.66%。 |
| [^52] | [Do Large Language Models Understand Logic or Just Mimick Context?](https://arxiv.org/abs/2402.12091) | 大型语言模型在逻辑推理中并不真正理解逻辑规则，而是通过语境学习增强了模型到达结论的可能性 |
| [^53] | [Can LLMs Compute with Reasons?](https://arxiv.org/abs/2402.12080) | 提出一种“归纳学习”方法，利用分布式的SLM网络来提升SLM的推理能力，桥梁人类与LLM之间的逻辑差距。 |
| [^54] | [LVCHAT: Facilitating Long Video Comprehension](https://arxiv.org/abs/2402.12079) | LVChat 提出了 Frame-Scalable 编码和 Interleaved Frame Encoding（IFE）来解决长视频理解中的问题。 |
| [^55] | [EmoBench: Evaluating the Emotional Intelligence of Large Language Models](https://arxiv.org/abs/2402.12071) | EmoBench是一个基于心理学理论的基准测试，旨在评估大型语言模型的情感智能，包括情感理解和情感应用。 |
| [^56] | [WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More](https://arxiv.org/abs/2402.12065) | 该论文提出了WKVQuant，一种专为大型语言模型设计的量化框架，通过量化权重和键值缓存来改善性能。 |
| [^57] | [All Language Models Large and Small](https://arxiv.org/abs/2402.12061) | LONDI框架可以在需要复杂决策和推理的地方选择性地使用大的语言模型，极大地降低了资源消耗。 |
| [^58] | [Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models](https://arxiv.org/abs/2402.12058) | 提出了搭建提示方法，在图像中叠加点阵作为视觉信息锚点，并利用多维坐标作为文本位置参考，从而促进大型多模型的视觉-语言协调。 |
| [^59] | [Are LLM-based Evaluators Confusing NLG Quality Criteria?](https://arxiv.org/abs/2402.12055) | LLMs在NLG评估中表现良好，但存在混淆不同评估标准的问题，研究提出了一个详细的分类系统和针对不同LLMs评估行为的扰动攻击，揭示了LLMs固有的混淆问题，并需要进一步研究。 |
| [^60] | [Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs](https://arxiv.org/abs/2402.12052) | 本文介绍了一种新的协作方法SlimPLM，通过精简代理模型检测LLMs中缺失的知识，以增强LLMs的知识获取过程 |
| [^61] | [Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models](https://arxiv.org/abs/2402.12048) | 该论文提出了一种名为Model Tailor 的后训练调整方法，在多模态大语言模型中缓解了灾难性遗忘，通过保留预训练参数并替换少量微调参数，实现了对原始任务约99%的效果和对新任务约97%的效果。 |
| [^62] | [Citation Amnesia: NLP and Other Academic Fields Are in a Citation Age Recession](https://arxiv.org/abs/2402.12046) | 该研究分析了不同学术领域在43年间引用较旧作品的趋势，发现自然语言处理和机器学习研究中引文年龄衰退最为明显，此趋势并非由出版速率增长主导。 |
| [^63] | [Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics](https://arxiv.org/abs/2402.12036) | 通过排名关键词并指导屏蔽过程，这项研究提出了一种利用体裁和主题信息定制语言模型适应专业领域的创新方法。 |
| [^64] | [Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs](https://arxiv.org/abs/2402.12030) | 介绍了基于最优传输的通用logit蒸馏 (ULD) 损失，用于解决不同架构和分词器模型之间蒸馏的限制。 |
| [^65] | [Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space](https://arxiv.org/abs/2402.12026) | 通过对频率空间的分析，本文提出了一种多尺度低秩适应（MuScleLoRA）方法，用于解决在训练语言模型时受到后门攻击的问题。 |
| [^66] | [Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?](https://arxiv.org/abs/2402.12025) | 这项研究关注语音翻译领域的发展，通过将语音基础模型与大语言模型结合，为解决多模态任务提供了新的统一模型，但目前各种评估方法和设置多样性阻碍了确定每个架构构建块的最佳解决方案的识别。 |
| [^67] | [Distilling Large Language Models for Text-Attributed Graph Learning](https://arxiv.org/abs/2402.12022) | 本研究旨在将大型语言模型和图模型的优势相结合，通过将LLMs的能力压缩到 TAG 学习的本地图模型中，解决它们之间的固有差距。 |
| [^68] | [A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change](https://arxiv.org/abs/2402.12011) | 本文通过在相同条件下评估了最新的词汇语义变化模型和方法，将LSC问题分解为不同级别的任务，并展示了在不同语言的八个基准测试中，APD在GCD方面优于其他方法，XL-LEXEME在WiC、WSI和GCD方面优于其他上下文化模型，并且与GPT-4相当。 |
| [^69] | [Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models](https://arxiv.org/abs/2402.11997) | 大型语言模型在处理时间信息和推理方面存在显著限制，闭源模型可能暗示了不确定性认识与错误回应之间的权衡。 |
| [^70] | [Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations](https://arxiv.org/abs/2402.11975) | 提出了COmpressive Memory-Enhanced Dialogue sYstems（COMEDY）框架，通过“一对多”方法利用单一语言模型管理记忆生成、压缩和响应生成，核心概念是压缩记忆，支持大规模中文指导调优数据集Dolphin，比较评估证明了COMEDY的优越性。 |
| [^71] | [What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects](https://arxiv.org/abs/2402.11968) | 德语方言和区域语言说话者更倾向于支持能够处理方言输入的自然语言处理工具，比如虚拟助手，对于产生方言输出的应用则支持程度相对较低。 |
| [^72] | [DB-LLM: Accurate Dual-Binarization for Efficient LLMs](https://arxiv.org/abs/2402.11960) | 本文提出了一种名为DB-LLM的新颖双二值化方法，通过引入灵活双二值化(FDB)来平衡2位宽度的精度优势和二值化的效率优势，从而在提高LLMs的计算效率的同时保持了准确性。 |
| [^73] | [Automatic Evaluation for Mental Health Counseling using LLMs](https://arxiv.org/abs/2402.11958) | 使用LLMs自动评估心理咨询对话中的工作联盟，结果显示与人工评估高度一致，并提供宝贵见解。 |
| [^74] | [Analysis of Multidomain Abstractive Summarization Using Salience Allocation](https://arxiv.org/abs/2402.11955) | 通过SEASON技术，本研究评估了用于生成概括性摘要的方法，与BART、PEGASUS和ProphetNet等模型进行对比，在多个数据集上进行了评估，着重分析了财经数据集的表现。 |
| [^75] | [LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation](https://arxiv.org/abs/2402.11943) | LVLM-Enhanced Multimodal Misinformation Detection with LEMMA proposes a solution for detecting complex misinformation by enhancing the reasoning capability of Large Vision Language Models. |
| [^76] | [Comprehensive Cognitive LLM Agent for Smartphone GUI Automation](https://arxiv.org/abs/2402.11941) | 提出了全面认知LLM代理，通过全面环境感知和条件动作预测两种新方法系统性提高GUI自动化性能。 |
| [^77] | [Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text](https://arxiv.org/abs/2402.11934) | 本文研究了团队QUST在SemEval-2024任务8中的参与情况，通过数据增强和清洗提高了模型训练效率和准确性，在单语任务中评估了多种方法并最终采用堆叠集成模型，最终在多语言环境中取得了不错的排名。 |
| [^78] | [MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition](https://arxiv.org/abs/2402.11924) | 通过编辑HotpotQA数据集中的新知识，我们引入了一个LLM MHQA评估基准，同时注释和评估了推理链，揭示了当前MHQA基准存在数据污染的潜在风险。 |
| [^79] | [Semantic Textual Similarity Assessment in Chest X-ray Reports Using a Domain-Specific Cosine-Based Metric](https://arxiv.org/abs/2402.11908) | 本文介绍了一种专门设计用于评估胸部X射线报告中语义文本相似性的新方法，以提高医学成像解释的准确性，促进临床决策并改善患者预后。 |
| [^80] | [Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation](https://arxiv.org/abs/2402.11907) | 通过对比提示自我奖励方法，提出了一种直接对齐大型语言模型的自动对齐方法，无需依赖人工注释的偏好数据，在实验中表现优于现有方法RLHF。 |
| [^81] | [Learning to Edit: Aligning LLMs with Knowledge Editing](https://arxiv.org/abs/2402.11905) | 提出了一个名为Learning to Edit（LTE）的框架，教导LLMs将更新后的知识应用于输入问题，通过对齐阶段和推理阶段实现可靠的、范围内的文本编辑。 |
| [^82] | [SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning](https://arxiv.org/abs/2402.11903) | 提出了一种新颖的求解器层适应（SoLA）方法，在LLM中引入求解器层，不同地引导解决方案朝向可满足性 |
| [^83] | [Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models](https://arxiv.org/abs/2402.11900) | 本文系统调查了在大型语言模型中利用事实快捷方式进行多跳事实推理的可能性，并分析了这种快捷方式可能带来的风险。 |
| [^84] | [SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.11896) | SIBO提出了一个简单的增强器来增强参数高效微调技术，有效解决了Transformer-based LLMs中过度平滑的问题，并在多个基准数据集上取得了显著的性能提升。 |
| [^85] | [Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization](https://arxiv.org/abs/2402.11895) | 研究探讨团体互动对宗教极端化的影响，在印度 Twitter 用户中发现，政治和社会事件的团体间互动可以减少极端化。 |
| [^86] | [Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation](https://arxiv.org/abs/2402.11894) | 本文提出自动化数据集更新策略，利用两种系统验证过的策略生成看不见和高质量的测试样本，以缓解数据泄漏问题并实现评估稳定性。 |
| [^87] | [FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation](https://arxiv.org/abs/2402.11891) | 提出了FeB4RAG，这是一个专门为检索增强生成框架内的联合搜索设计的新数据集。 |
| [^88] | [Revisiting Knowledge Distillation for Autoregressive Language Models](https://arxiv.org/abs/2402.11890) | 提出一种自适应教学方法（ATKD），以改善自回归语言模型的知识蒸馏，帮助各种基线KD方法在所有模型类型和规模上实现一致且显著的性能提升。 |
| [^89] | [ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding](https://arxiv.org/abs/2402.11889) | ROSE是一种简单而有效的方法，通过抑制不受欢迎的输出来提高期望的安全输出的概率，从而直接提升现有调整指令LLMs的安全性。 |
| [^90] | [The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth](https://arxiv.org/abs/2402.11886) | 本文旨在评估和改进LLM作为酷儿青少年情感支持者的潜力，通过定性和定量分析LLM与酷儿相关内容的互动，并开发了一个新颖的评估标准量表。 |
| [^91] | [M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation](https://arxiv.org/abs/2402.11875) | 通过分析不同VDG模型的幻觉现象和锚标记的差异，本文提出了M2K-VDG框架，用于自适应增强多模态知识锚以减少幻觉，并在实验中表现出优越性。 |
| [^92] | [How Interpretable are Reasoning Explanations from Prompting Large Language Models?](https://arxiv.org/abs/2402.11863) | 对大型语言模型推理解释提出了全面、多角度的评估，包括忠实度、强健性和效用，并引入了新的可解释性指标。 |
| [^93] | [Modularized Networks for Few-shot Hateful Meme Detection](https://arxiv.org/abs/2402.11845) | 这种模块化网络利用LoRA模块增强了大型语言模型在少样本环境下进行仇恨模因检测的泛化能力。 |
| [^94] | [Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search](https://arxiv.org/abs/2402.11827) | 提出了RetPO框架，通过优化语言模型对搜索查询进行重构，以符合目标检索系统的偏好，并构建了一个大型数据集RF Collection，用于收集检索结果作为检索器的偏好。 |
| [^95] | [Microstructures and Accuracy of Graph Recall by Large Language Models](https://arxiv.org/abs/2402.11821) | 本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。 |
| [^96] | [Head-wise Shareable Attention for Large Language Models](https://arxiv.org/abs/2402.11819) | 本文提出了面向大语言模型的头部共享注意力的观点，提出了两种在注意力头之间共享参数的内存高效方法，以解决大型语言模型参数数量巨大导致部署受限的问题。 |
| [^97] | [Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages](https://arxiv.org/abs/2402.11818) | 提出了一种名为NewsSerow的方法，用于自动识别低资源语言中的环境保护内容，并且在尼泊尔语中使用少于10个示例新闻文章时，NewsSerow显着优于其他少样本模型。 |
| [^98] | [HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?](https://arxiv.org/abs/2402.11815) | 提出了一种基于对比学习的单一模型，用较少的参数实现与基线相当的机器生成文本检测性能 |
| [^99] | [FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema](https://arxiv.org/abs/2402.11811) | FIPO提出了基于自由形式指导的提示优化方法，结合偏好数据集和模块化微调模式，重新构思了优化过程并实现了灵活的任务提示生成。 |
| [^100] | [Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding](https://arxiv.org/abs/2402.11809) | 提出了Smart Parallel Auto-Correct Decoding (SPACE)方法，通过集成半自回归推理和猜测解码，实现了大型语言模型推理加速和并行生成验证令牌的功能。 |
| [^101] | [LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs](https://arxiv.org/abs/2402.11804) | 本文利用大型语言模型（LLMs）生成图形结构提示，以增强预训练的图神经网络（GNNs），提出一种新的方法论见解，实现了在任意知识图上进行低资源归纳推理的高通用性。 |
| [^102] | [Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation](https://arxiv.org/abs/2402.11794) | 本文揭示了检索增强生成中的注意力精炼的成功机制，并提出了优化模型训练方法的指标. |
| [^103] | [What Evidence Do Language Models Find Convincing?](https://arxiv.org/abs/2402.11782) | 通过构建 ConflictingQA 数据集，并进行敏感性和反事实分析，研究发现当前语言模型在预测时很大程度上依赖于网站与查询的相关性，而忽视了人类认为重要的文本风格特征。 |
| [^104] | [Uncovering Latent Human Wellbeing in Language Model Embeddings](https://arxiv.org/abs/2402.11777) | 本研究通过ETHICS Utilitarianism任务发现，预训练语言模型的表示隐含了对人类福祉的理解，且模型规模增加时，准确率呈非下降趋势。 |
| [^105] | [Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations](https://arxiv.org/abs/2402.11770) | 使用结构化思维链提示的方法，在少样本情况下生成内容相关的问答对话，提高了代理程序对基础文档的忠诚度，训练强大的对话问答代理。 |
| [^106] | [ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs](https://arxiv.org/abs/2402.11764) | 本研究提出了一种利用ChatGPT生成合成训练数据来增强LLMs去偏见化的新方法，能够高效地去除已知偏见并跨越不同类别进行去偏见化。 |
| [^107] | [Large Language Models for Stemming: Promises, Pitfalls and Failures](https://arxiv.org/abs/2402.11757) | 本文研究了使用大型语言模型来进行词干提取的有前途的想法，提出了三种不同的方法，每种方法在计算成本、有效性和稳健性方面具有不同的权衡。 |
| [^108] | [MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs](https://arxiv.org/abs/2402.11756) | MARS提出了一种新的评分函数MARS，考虑了生成序列中每个标记的语义贡献，该方法改进了生成式LLMs中的不确定性估计性能。 |
| [^109] | [SPML: A DSL for Defending Language Models Against Prompt Attacks](https://arxiv.org/abs/2402.11755) | SPML是一种用于优化提示并监控基于大型语言模型聊天机器人输入的领域特定语言，用于防御恶意攻击并优化成本。 |
| [^110] | [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/abs/2402.11753) | 提出了一种新颖的基于ASCII艺术的越狱攻击，以及一个用于评估LLMs在识别非纯语义提示方面能力的基准挑战。五个SOTA LLMs在识别ASCII艺术提示时存在困难。 |
| [^111] | [In-Context Learning Demonstration Selection via Influence Analysis](https://arxiv.org/abs/2402.11750) | 通过分析训练样本的影响，提出一种名为InfICL的演示选择方法，可以帮助提升上下文学习的泛化性能。 |
| [^112] | [Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic](https://arxiv.org/abs/2402.11746) | 提出了一种简单方法RESTA，通过任务算法对精调语言模型进行安全重新定位，有效降低了其有害程度。 |
| [^113] | [Machine-generated Text Localization](https://arxiv.org/abs/2402.11744) | 该论文首次深入研究了机器生成文本定位，针对文档中机器生成部分的定位，通过细粒度的方法提出了解决整个文档MGT检测失败情况的新途径。 |
| [^114] | [Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis](https://arxiv.org/abs/2402.11728) | 本研究探讨了分析师报告和盈利电话中的索赔对金融市场回报的影响，并构建了一个新的金融数据集用于索赔检测任务。提出了一种融入主题专家知识的新型弱监督模型，通过构建一种新的度量“乐观主义”展示了模型的实际效用。 |
| [^115] | [How Susceptible are Large Language Models to Ideological Manipulation?](https://arxiv.org/abs/2402.11725) | 大型语言模型展示出令人担忧的易受意识形态操纵的脆弱性，对极少量意识形态驱动样本的暴露就能显著改变其意识形态，并且能够从一个主题吸收意识形态并泛化到其他不相关主题上。 |
| [^116] | [Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models](https://arxiv.org/abs/2402.11723) | 本研究探讨了大型语言模型提供的不同支撑级别如何影响共同撰写过程，发现高支撑可以显著改善写作质量和生产率，特别有利于非常规写作者和不太精通技术的用户。 |
| [^117] | [Modelling Political Coalition Negotiations Using LLM-based Agents](https://arxiv.org/abs/2402.11712) | 本文引入联盟谈判作为一项新颖的自然语言处理任务，建模为基于大型语言模型代理之间的谈判，并提出了一个多语种数据集POLCA以及用于模拟政治谈判过程的分层马尔可夫决策过程。 |
| [^118] | [MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization](https://arxiv.org/abs/2402.11711) | 本研究将多目标优化技术应用于基于强化学习的离散提示优化，为解决奖励平衡问题提供了新视角。 |
| [^119] | [A Note on Bias to Complete](https://arxiv.org/abs/2402.11710) | 在动态环境中，重新审视偏见的定义，提出新的偏见类型，并建立包括假设、策略和方法的框架来减少社会偏见。 |
| [^120] | [GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network](https://arxiv.org/abs/2402.11709) | GNNavi通过引入基于提示的参数高效微调（PEFT）方法和图神经网络（GNN）层，准确引导信息流的汇聚和分布，在大型语言模型中导航信息流动态，超越了标准提示式微调的性能。 |
| [^121] | [Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers](https://arxiv.org/abs/2402.11700) | 减少大型语言模型的层数可在不损失性能的情况下减轻模型规模，甚至在某些情况下只有一个层的模型可以超越完全层式的对应项。 |
| [^122] | [Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning](https://arxiv.org/abs/2402.11690) | 构建了最多样化的视觉指导调整数据集Vision-Flan，提出了两阶段指导调整框架，显著优于传统方法 |
| [^123] | [ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model](https://arxiv.org/abs/2402.11684) | 通过采用GPT-4V合成的高质量训练数据，研究成功地实现了ALLaVA，一个轻量级视觉-语言模型，该模型在12个基准测试上表现出与最多3B LVLMs竞争性能。 |
| [^124] | [One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation](https://arxiv.org/abs/2402.11683) | 通过释放涵盖观点摘要评估相关七个维度的新数据集SUMMEVAL-OP，研究人员提出了 Op-I-Prompt 作为一种独立于维度的提示方法，以及 Op-Prompts 作为一组依赖于维度的提示，可以表明 Op-I-Prompt 是评估观点摘要的一个很好的替代方案，实现了平均 Spearman 相关性为 0.70。 |
| [^125] | [Opening the black box of language acquisition](https://arxiv.org/abs/2402.11681) | 提出了一种基于序列记忆和分块的最小认知架构，采用强化学习原则，能够学习人造语言并提取支持学习的语法信息。 |
| [^126] | [A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models](https://arxiv.org/abs/2402.11676) | 提出了一个多方面框架，使用大型语言模型评估反叙事，通过5个方面从专门 NGO 指南中提取定义的内容，以解决以往评估方法的局限性。 |
| [^127] | [Autocorrect for Estonian texts: final report from project EKTB25](https://arxiv.org/abs/2402.11671) | 该项目成功开发了爱沙尼亚语言的拼写和语法校正工具，主要创新在于使用迁移学习和自动评估来克服可用数据不足的挑战。 |
| [^128] | [Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals](https://arxiv.org/abs/2402.11655) | 本研究提出了机制之争的概念，关注语言模型中多个机制的相互作用，并揭示了它们之间的竞争过程，以及影响某些机制强度的注意力位置。 |
| [^129] | [Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents](https://arxiv.org/abs/2402.11651) | 大型语言模型通过整合负面示例和适当的数据清洗与微调策略，从失败中学习，提高作为代理的效果。 |
| [^130] | [Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks](https://arxiv.org/abs/2402.11638) | 本研究压力测试了机器生成文本检测器在恶意攻击下的鲁棒性，实验证明几乎所有现有检测器在各种攻击下都表现不稳定，平均性能下降35%，并提出了改善鲁棒性的初步解决方案。 |
| [^131] | [Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs](https://arxiv.org/abs/2402.11633) | 本论文提出了SOLID模型，利用自我播种和多意图自我指导方案来实现LLMs生成意图感知的信息检索对话。 |
| [^132] | [Metacognitive Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2402.11626) | 本文介绍了MetaRAG，一种结合了检索增强生成过程与元认知的方法，通过元认知调节流程，使模型具有监视、评估和规划其响应策略的能力，增强了其内省推理能力。 |
| [^133] | [SpeCrawler: Generating OpenAPI Specifications from API Documentation Using Large Language Models](https://arxiv.org/abs/2402.11625) | SpeCrawler利用大型语言模型从不同API文档生成OpenAPI规范，有助于简化API集成流程并促进工具整合到语言模型中。 |
| [^134] | [Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2402.11622) | 提出了一种基于逻辑闭环的框架（LogicCheckGPT），利用大型视觉-语言模型本身来检测和减轻对象幻觉。 |
| [^135] | [Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection](https://arxiv.org/abs/2402.11621) | 通过研究GPT-3.5 Turbo、GPT-4和Flan-T5模型在识别新闻标题中框架偏见的性能，发现可解释提示能够显著提高这些模型的可靠性，GPT-4在少射场景中表现较好，而FLAN-T5的表现较差，指出较小模型可能需要更多任务特定微调。 |
| [^136] | [Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations](https://arxiv.org/abs/2402.11608) | 应用指标学习编码模型（MLEMs）于BERT表示，发现语言特征在不同层中有序分离，神经表示层级组织，中间层解耦，优于其他解码方法。 |
| [^137] | [Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?](https://arxiv.org/abs/2402.11597) | 大型语言模型在多任务推断时表现出更高的性能，相比单任务推断平均推断时间减少1.46倍，并且在MTI Bench上显示出最多高达12.4%的性能改善。 |
| [^138] | [Extensible Embedding: A Flexible Multipler For LLM's Context Length](https://arxiv.org/abs/2402.11577) | 提出了可扩展嵌入方法，实现了对LLM上下文的高质量扩展，具有强大的灵活性和成本效益。 |
| [^139] | [Visual In-Context Learning for Large Vision-Language Models](https://arxiv.org/abs/2402.11574) | 提出了一种新的Visual In-Context Learning（VICL）方法，通过检索和重新排名图像、用任务意图和任务特定的视觉解析总结图像，以及组成语言演示来减少标记计数和减轻跨模态交互问题。 |
| [^140] | [BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models](https://arxiv.org/abs/2402.11573) | 可扩展嵌入方法提高了大型语言模型（LLM）的上下文扩展质量和成本效益，通过在架构和训练方法上进行系统优化，实现了上下文范围的灵活扩展和高效的训练样本效率。 |
| [^141] | [Cobra Effect in Reference-Free Image Captioning Metrics](https://arxiv.org/abs/2402.11572) | 本论文研究了在无参考图像字幕评估中存在的眼镜蛇效应，利用度量得分作为奖励来指导生成与度量标准一致的描述。 |
| [^142] | [Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru](https://arxiv.org/abs/2402.11571) | 将大型语言模型集成到社交机器人中，实现更动态和富有表现力的对话，包括使用情绪识别模型，调整语调，并利用表情符号生成机器人动作。 |
| [^143] | [Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru](https://arxiv.org/abs/2402.11569) | 通过Haru开发了一个完全自主的对话系统，最大限度地发挥了其情感表达和独特人格，成功应用于行为改变辅导，取得了显著的成效。 |
| [^144] | [LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration](https://arxiv.org/abs/2402.11550) | LongAgent通过多智能体协作将语言模型扩展到128K上下文，并在长文本处理方面表现出潜在的优越性。 |
| [^145] | [Syntactic Language Change in English and German: Metrics, Parsers, and Convergences](https://arxiv.org/abs/2402.11549) | 本文研究英语和德语句法语言变化趋势，使用议会辩论语料库，探讨了句法依存距离最小化及基于树图属性的15个度量标准，揭示了现代解析器在这种变化中的影响。 |
| [^146] | [KMMLU: Measuring Massive Multitask Language Understanding in Korean](https://arxiv.org/abs/2402.11548) | KMMLU是一个新的韩语基准，包含35,030道专家级多选题，从原始韩语考试中收集而来，测试了26个LLM模型，发现这些模型在KMMLU上的表现有很大提升空间。 |
| [^147] | [Question Answering Over Spatio-Temporal Knowledge Graph](https://arxiv.org/abs/2402.11542) | 介绍了一个新的基于时空知识图的问答系统STQAD，以解决问答系统在涵盖时空信息的问题上的挑战，提出了一种新的STComplEx嵌入方法STCQA来实现此目标 |
| [^148] | [Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought](https://arxiv.org/abs/2402.11541) | 本文通过对KG知识注入方法进行全面比较，探索为LLMs提供知识图谱知识的最佳方法，以增强它们的理解能力。 |
| [^149] | [Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning](https://arxiv.org/abs/2402.11537) | 通过对五个主要类别的预训练数据的48个数据集进行系统分析，研究了它们对大型语言模型性能的影响，并发现了一些“高影响数据”，如书籍，与模型能力相关联，为LLMs的优化提供了见解。 |
| [^150] | [PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability](https://arxiv.org/abs/2402.11534) | PreAct是一个整合了预测、推理和行动的智能体框架，利用预测信息可以帮助智能体进行更多样化和策略性的推理，导致更有效的行动，提升任务完成效率。 |
| [^151] | [Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models](https://arxiv.org/abs/2402.11532) | 提出了一种名为指令链（CoI）的新概念，通过逐步解决每个子任务来处理由多个子任务组成的指令，进而提高了大型语言模型（LLMs）的泛化能力和多语言摘要性能 |
| [^152] | [Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution](https://arxiv.org/abs/2402.11525) | 提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。 |
| [^153] | [Unveiling the Secrets of Engaging Conversations: Factors that Keep Users Hooked on Role-Playing Dialog Agents](https://arxiv.org/abs/2402.11522) | 机器人体现其扮演角色的程度对保留率的影响有限，而其讲话的每个轮次的长度显著影响保留率 |
| [^154] | [Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM](https://arxiv.org/abs/2402.11517) | Knowledge-to-SQL框架利用数据专家LLM提供有用知识，增强文本到SQL模型的鲁棒性。 |
| [^155] | [From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings](https://arxiv.org/abs/2402.11512) | 提出了DeepSoftDebias算法，在不同领域数据集、准确度指标和NLP任务中全面评估，发现其在减少性别、种族和宗教偏见方面优于现有最先进方法 |
| [^156] | [Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources](https://arxiv.org/abs/2402.11505) | 该研究引入了FlexLoRA，一个简单而有效的大型语言模型微调聚合方案，能够在联邦学习中充分利用异质客户资源，通过动态调整本地LoRA排名和采用奇异值分解进行权重重新分配，提升全局模型的广泛知识。 |
| [^157] | [Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation](https://arxiv.org/abs/2402.11493) | 引入了知识边界的概念，以涵盖语言模型内的无提示和有提示敏感性知识，通过避免提示敏感性，使得语言模型评估更可靠和稳健。 |
| [^158] | [What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs](https://arxiv.org/abs/2402.11489) | 提出了一种新的混合方法SimPlan，结合了LLMs和经典规划方法，在各种规划领域的实验表明SimPlan明显优于现有的基于LLM的规划者 |
| [^159] | [LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation](https://arxiv.org/abs/2402.11485) | LEIA是一种语言适应调整方法，利用维基百科实体名称跨语言增强目标语言语料库，通过左到右的语言建模训练，显著提高了各种非英语语言的表现。 |
| [^160] | [DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics](https://arxiv.org/abs/2402.11481) | DictLLM是一个创新性框架，旨在改进键值结构化数据的建模，用于生成医学诊断。它包括组位置编码、层次注意偏差和优化传输对齐层。 |
| [^161] | [A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models](https://arxiv.org/abs/2402.11469) | 本文研究了训练数据与模型鲁棒性之间的相关性，并提出通过提取不同特征来预测Transformer文本模型的对抗性稳健性的方法。 |
| [^162] | [When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation](https://arxiv.org/abs/2402.11457) | LLM何时需要检索增强？减轻LLM的过度自信有助于检索增强 |
| [^163] | [FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence](https://arxiv.org/abs/2402.11456) | 本文提出FactPICO，用于评估医学文本的简明语言摘要的事实性基准，对RCT中的关键要素和报告结果进行评估，并对LLMs添加的额外信息进行检查。 |
| [^164] | [LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks](https://arxiv.org/abs/2402.11455) | LoRA-Flow提出了动态权重来调整不同LoRA的影响，以应对生成任务中不同标记所需的多样化技能。 |
| [^165] | [MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization](https://arxiv.org/abs/2402.11453) | MatPlotAgent介绍了一种基于LLM的自动化科学数据可视化框架，包括查询理解、代码生成和视觉反馈机制，并引入了MatPlotBench基准和GPT-4V评分方法。 |
| [^166] | [AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition](https://arxiv.org/abs/2402.11452) | 提出了一种名为AutoPRM的自监督框架，通过可控问题分解和强化学习实现对复杂推理挑战的自动化监督和改进 |
| [^167] | [SciAgent: Tool-augmented Language Models for Scientific Reasoning](https://arxiv.org/abs/2402.11451) | 引入了工具增强型科学推理的新任务设置，通过提供可扩展的工具集，帮助大型语言模型在科学问题解决中变得更加实用和可解决。 |
| [^168] | [In-Context Example Ordering Guided by Label Distributions](https://arxiv.org/abs/2402.11447) | 该论文提出了由模型的概率预测引导的上下文示例排序的原则，以提高上下文学习在自然语言处理中的性能。 |
| [^169] | [Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation](https://arxiv.org/abs/2402.11443) | 介绍了一个基准自我演进框架，通过多Agent系统操作环境或问题，构建演化实例来更准确地评估LLMs的能力和限制，并扩展基准数据集以进行更具可扩展性和精细化的评估。 |
| [^170] | [Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs](https://arxiv.org/abs/2402.11442) | 提出了一个逻辑支架推理规则生成框架，通过构建包含基础和组合规则的推理规则库ULogic，揭示了LLMs在逻辑理解方面与人类表现的显著差距，并通过生成准确、复杂和抽象的推理结果来提升下游推理。 |
| [^171] | [InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration](https://arxiv.org/abs/2402.11441) | 提出了一种Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态有效地将未知知识集成到大型语言模型中，从而缓解知识遗忘问题。 |
| [^172] | [Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models](https://arxiv.org/abs/2402.11436) | 大型语言模型存在自我偏见，研究发现通过更大的模型规模和准确评估的外部反馈可以显著减少这种偏见，并提高后续任务的实际表现。 |
| [^173] | [Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning](https://arxiv.org/abs/2402.11432) | 本文提出了一种新的数据收集流程，利用 GPT-4 模拟嫌疑人与警官之间的角色扮演，以解决欺骗检测领域面临的数据稀缺问题，并将传统的欺骗检测任务拓展到欺骗推理。 |
| [^174] | [EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models](https://arxiv.org/abs/2402.11430) | EventRL通过结果监督和特定奖励函数有效改善了大型语言模型的事件提取效果，尤其在处理新型事件类型方面表现优异。 |
| [^175] | [Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling Correction by Multi-stage Knowledge Transfer Framework](https://arxiv.org/abs/2402.11422) | 提出了一个模型无关的多阶段知识迁移框架，通过不断发展的教师模型在多领域中防止CSC模型忘记先前获得的知识，实验证明了方法的有效性。 |
| [^176] | [Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction](https://arxiv.org/abs/2402.11420) | 重新思考大型语言模型在中文语法错误纠正中的作用，利用LLMs作为解释器提供解释信息并作为评估器带来更合理的CGEC评估以增强性能 |
| [^177] | [LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models](https://arxiv.org/abs/2402.11417) | LoRETTA是一个通过张量训练分解显著减少可训练参数的超低参数高效框架，在大型语言模型的微调中表现出与大多数PEFT方法相媲美甚至更好的性能。 |
| [^178] | [Fine-grained and Explainable Factuality Evaluation for Multimodal Summarization](https://arxiv.org/abs/2402.11414) | 提出两种细粒度和可解释的评估框架，用于评估多模态摘要模型的事实性，其中无参考事实性评估框架具有更广泛的应用场景，实验证实了方法的有效性。 |
| [^179] | [Aligning Modalities in Vision Large Language Models via Preference Fine-tuning](https://arxiv.org/abs/2402.11411) | 本研究将幻觉问题视为对齐问题，并通过偏好调整解决，提出了POVID方法来生成反馈数据。 |
| [^180] | [Multi-dimensional Evaluation of Empathetic Dialog Responses](https://arxiv.org/abs/2402.11409) | 提出了一个多维度的共情评估框架，同时衡量了说话者角度的表达意图和听者角度的感知共情，研究发现二者是相互关联的，感知共情与对话会话的满意水平呈高相关性。 |
| [^181] | [Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection](https://arxiv.org/abs/2402.11406) | 本文研究了LLMs在检测隐式仇恨言论和表达信心时的能力，发现LLMs存在两个极端：对可能引起公平问题的群体或话题表现出过于敏感，同时置信度评分过度集中在一个范围内。 |
| [^182] | [k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text](https://arxiv.org/abs/2402.11399) | k-SemStamp是一种简单而有效的语义水印方案，通过利用k均值聚类代替LSH来提高鲁棒性和抽样效率，同时保持生成质量，为机器生成文本检测提供了更有效的工具。 |
| [^183] | [Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis](https://arxiv.org/abs/2402.11398) | 通过利用LLM增强语义分析，开发了用于文本的相似度度量框架，可显著改善文本的语义相似性评估，并可扩展到其他专业领域。 |
| [^184] | [Training Language Model Agents without Modifying Language Models](https://arxiv.org/abs/2402.11359) | 提出一种新的方法，在不修改语言模型的情况下训练语言模型代理，通过进化代理的功能来解决下游任务 |
| [^185] | [What Changed? Converting Representational Interventions to Natural Language](https://arxiv.org/abs/2402.11355) | 将表征空间的反事实转化为自然语言，以分析和解释模型干预所引起的语言变化，并减轻分类中的偏见。 |
| [^186] | [Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention](https://arxiv.org/abs/2402.11353) | 长期记忆的引入提高了大型语言模型驱动聊天机器人在公共卫生干预中的用户自我披露，但仍存在挑战，特别是在解决慢性健康状况和隐私问题方面。 |
| [^187] | [Tasks That Language Models Don't Learn](https://arxiv.org/abs/2402.11349) | 大型语言模型没有学习到语言的视听特性，在新的任务基准测试中表现较差，暴露了人类语言理解与语言模型感官处理能力之间的根本差距。 |
| [^188] | [PhaseEvo: Towards Unified In-Context Prompt Optimization for Large Language Models](https://arxiv.org/abs/2402.11347) | 该研究提出了PhaseEvo，一个旨在实现提示指令和示例的联合优化的高效自动提示优化框架，结合了LLMs的生成能力和进化算法的全局搜索效率。 |
| [^189] | [EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries](https://arxiv.org/abs/2402.11324) | 本论文提出了一种通过事件描述配对实现知识更新的事件驱动知识编辑方法，解决了现有知识编辑方法在编辑过程中遇到的不确定性问题。 |
| [^190] | [MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal](https://arxiv.org/abs/2402.11297) | 这个模型是一种多模态大型语言模型，能够理解多图像、多音频和多模态信息，在英语和马来语之间切换，并通过使用SigLIP编码器和Whisper编码器实现了这一功能。 |
| [^191] | [Dissecting Human and LLM Preferences](https://arxiv.org/abs/2402.11296) | 本研究分析了人类和32种不同LLM的偏好，发现人类不太在意错误，偏好支持立场的回应，而先进的LLM更注重正确性、清晰性和无害性。 |
| [^192] | [OneBit: Towards Extremely Low-bit Large Language Models](https://arxiv.org/abs/2402.11295) | 本文提出了一种名为OneBit的1位量化感知训练框架，可以将大型语言模型的权重矩阵量化为1位，为极低比特宽度的LLMs部署铺平了道路。 |
| [^193] | [Puzzle Solving using Reasoning of Large Language Models: A Survey](https://arxiv.org/abs/2402.11291) | 本调查通过将难题分为基于规则和无规则两类的独特分类法，通过各种方法评估了大型语言模型（LLMs）的表现，强调了在复杂难题情境中LLMs的挑战和人类类似推理之间的差距，突出了推动LLMs解谜能力和贡献于人工智能发展的必要性。 |
| [^194] | [Grammaticality illusion or ambiguous interpretation? Event-related potentials reveal the nature of the missing-NP effect in Mandarin centre-embedded structures](https://arxiv.org/abs/2402.11282) | 汉语中缺失NP的双重中心嵌套结构并不是语法性错觉，而是动词歧义解释的含糊解释。 |
| [^195] | [Can Large Multimodal Models Uncover Deep Semantics Behind Images?](https://arxiv.org/abs/2402.11281) | 该论文引入了一个名为DEEPEVAL的基准，用于评估大型多模态模型对视觉深层语义的能力，揭示了现有LMMs在深层语义理解方面与人类之间存在显著差距。 |
| [^196] | [Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models](https://arxiv.org/abs/2402.11279) | 多透视一致性方法为大型语言模型中的置信度估计带来改进，能有效减轻过度自信问题，并在多个数据集上实现最先进的性能。 |
| [^197] | [Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima](https://arxiv.org/abs/2402.11271) | 合成信息更可能被大型模型纳入训练数据集和传播中，大型模型在传递信息时倾向于有选择地修改和丢失特定内容 |
| [^198] | [MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning](https://arxiv.org/abs/2402.11260) | MoRAL结合了MoE的多任务能力和LoRA的微调能力，采用问答对作为输入，实现了LLM的有效终身学习。 |
| [^199] | [C-ICL: Contrastive In-context Learning for Information Extraction](https://arxiv.org/abs/2402.11254) | C-ICL提出了一种利用正确和不正确样本构建进行上下文学习示范的新颖少样本技术，通过提示不仅包含正样本还包含背后推理，增强了LLMs提取实体和关系的能力。 |
| [^200] | [Aligning Large Language Models by On-Policy Self-Judgment](https://arxiv.org/abs/2402.11253) | 本文提出了一个新颖的对齐框架SELF-JUDGE，通过增加式监督微调（JSFT）训练一个同时充当策略和评判器的单一模型，实现了参数高效的基于政策学习，无需额外的奖励模型。 |
| [^201] | [LLM can Achieve Self-Regulation via Hyperparameter Aware Generation](https://arxiv.org/abs/2402.11251) | LLM通过超参数感知生成实现自我调节，消除了大量手动调整的需求。 |
| [^202] | [Can Large Language Models perform Relation-based Argument Mining?](https://arxiv.org/abs/2402.11243) | 大型语言模型在处理关系型论证挖掘方面表现更好，能够显著超过目前最佳基准线，并且在十个数据集上进行了实验验证。 |
| [^203] | [Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs](https://arxiv.org/abs/2402.11218) | 该研究提出了一个名为DATG的框架，利用动态属性图调节关键属性词和关键反属性词的发生，实现了有效的属性控制，在毒性缓解和情感转化任务中表现出19.29%的控制准确性增强。 |
| [^204] | [Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models](https://arxiv.org/abs/2402.11217) | Asclepius是一个新的医学多模态大语言模型基准，旨在为可信的Med-MLLMs评估提供单独且临床代表性的评估方案。 |
| [^205] | [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](https://arxiv.org/abs/2402.11208) | 这项工作调查了基于LLM的代理人面临的后门攻击威胁，并提出了一般框架和不同形式的后门攻击分析。 |
| [^206] | [Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges](https://arxiv.org/abs/2402.11203) | ChatGPT作为信息检索领域的关键技术，不断挑战传统范式，带来了新的机遇和挑战，同时超越了之前的GPT-3模型。 |
| [^207] | [Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs](https://arxiv.org/abs/2402.11199) | 本文通过利用知识图谱，提出了一种新颖的CoT推理能力评估模式，揭示了大型语言模型在多跳问题回答中推理知识和生成CoT的准确性之间的显著差异 |
| [^208] | [Centroid-Based Efficient Minimum Bayes Risk Decoding](https://arxiv.org/abs/2402.11197) | 基于质心的MBR解码方法提高了解码速度和翻译质量，在实验中表现优异。 |
| [^209] | [Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering](https://arxiv.org/abs/2402.11194) | 通过实验评估了LLMs在金融表格问答中的数学推理能力，发现引入了一种新型提示技术，能够在性能上胜过其他基线模型 |
| [^210] | [I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments](https://arxiv.org/abs/2402.11192) | 将微调过程中的实际响应风格与大型语言模型固有风格相匹配能够产生更好的学习结果，开发的方法通过最小程度地调整模型响应来避免过拟合。 |
| [^211] | [Knowledge Graph Assisted Automatic Sports News Writing](https://arxiv.org/abs/2402.11191) | 该论文提出了一种利用知识图谱和多阶段学习模型自动生成体育新闻的方法。 |
| [^212] | [Disclosure and Mitigation of Gender Bias in LLMs](https://arxiv.org/abs/2402.11190) | 提出了一种基于条件生成的间接探测框架，揭示了LLMs中的显性和隐性性别偏见，并探讨了三种缓解LLMs偏见的方法。 |
| [^213] | [LaCo: Large Language Model Pruning via Layer Collapse](https://arxiv.org/abs/2402.11187) | 提出了一种名为Layer Collapse（LaCo）的简明的逐层剪枝方法，使得大型语言模型可以在保持模型结构的同时迅速减小尺寸，并在剪枝比例达到25-30%时保持超过80%的平均任务性能，明显优于现有最先进的结构化剪枝方法。 |
| [^214] | [RENOVI: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations](https://arxiv.org/abs/2402.11178) | 提供了一个大规模语料库ReNoVi，帮助互动AI系统理解和纠正社会规范违反，包括人类编写对话和ChatGPT生成的合成对话，从而弥补数据稀缺并评估模型与人类在社会规范认知上的一致性。 |
| [^215] | [A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction](https://arxiv.org/abs/2402.11177) | 提出了一种基于问答的新方法，自动生成训练数据，用于QA模型的迁移学习，通过预处理模块解决了传统方法无法处理的信息提取类型挑战，实现了在电子健康记录中的信息提取任务，表现出色并能有效应对少样本或零样本情况。 |
| [^216] | [KnowTuning: Knowledge-aware Fine-tuning for Large Language Models](https://arxiv.org/abs/2402.11176) | 提出了一种知识感知微调方法，通过显式和隐式方式改善大型语言模型对知识的认识，包括训练模型明确识别答案中的知识三元组和隐式区分可靠和不可靠的知识。 |
| [^217] | [M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection](https://arxiv.org/abs/2402.11175) | 介绍了一个新的基准M4GT-Bench，涉及多语言、多领域和多生成器，用于检测机器生成文本，包括单语和多语种MGT检测、多模型检测和人机混合文本检测。 |
| [^218] | [Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection](https://arxiv.org/abs/2402.11167) | 提出了一种新颖的token-ensemble生成策略，挑战了当前AI内容检测方法的鲁棒性，对当前检测模型构成了重要挑战，需要进一步改进检测技术以应对复杂对抗策略。 |
| [^219] | [GenDec: A robust generative Question-decomposition method for Multi-hop reasoning](https://arxiv.org/abs/2402.11166) | GenDec提出了一种生成式问题分解方法，通过生成独立完整的子问题来增强LLMs在RAG中的推理能力。 |
| [^220] | [KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph](https://arxiv.org/abs/2402.11163) | 提出了一种名为KG-Agent的高效自主代理框架，该框架通过在KG上进行推理来提高大型语言模型的复杂问题回答能力 |
| [^221] | [PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation](https://arxiv.org/abs/2402.11161) | 提出了PANDA方法，引入了更精确的答案正确性评测方式，解决了当前自动评估问答和文本生成过程中的挑战。 |
| [^222] | [Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining](https://arxiv.org/abs/2402.11159) | 提出了一种反事实文本引导的对比语言-图像预训练框架CFT-CLIP，用于增强新闻文本和缩略图之间的对比学习。 |
| [^223] | [Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction](https://arxiv.org/abs/2402.11142) | 通过使用自然语言表达的关系定义来训练关系抽取模型的零-shot学习设置，从而为模型提供准确和明确的关系类型描述，并同时最小化注释要求。 |
| [^224] | [Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models](https://arxiv.org/abs/2402.11140) | 本文提出了一种名为Boosting of Thoughts（BoT）的自动提示框架，通过迭代地探索和自我评估多个思维树，获得一系列试错推理经验，作为解决复杂问题的新形式的提示。 |
| [^225] | [Contrastive Instruction Tuning](https://arxiv.org/abs/2402.11138) | 提出了对比指令调整方法，通过最大化相似性来提高大型语言模型对未知任务指令的稳健性 |
| [^226] | [Speculative Streaming: Fast LLM Inference without Auxiliary Models](https://arxiv.org/abs/2402.11131) | 提出了一种Speculative Streaming方法，将草稿模型融入目标模型，并通过将微调目标从下一个令牌预测更改为未来的n-gram预测，加速解码1.8-3.1倍，同时保持生成质量。 |
| [^227] | [BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering](https://arxiv.org/abs/2402.11129) | BlendFilter通过查询生成混合和知识过滤方法提升了检索增强型大型语言模型，在多领域的问答任务中取得了显著的性能提升。 |
| [^228] | [Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models](https://arxiv.org/abs/2402.11122) | 这项研究全面评估了大型语言模型中顺序记忆编辑的影响，发现修改参数ME可能会导致所有任务表现不佳，而保留参数ME则能够保持较好性能。 |
| [^229] | [Whose Emotions and Moral Sentiments Do Language Models Reflect?](https://arxiv.org/abs/2402.11114) | 该研究探讨了语言模型在情感和道德维度上如何代表不同群体，发现它们与意识形态团体存在显著的不一致性。 |
| [^230] | [Language Models as Science Tutors](https://arxiv.org/abs/2402.11111) | 介绍了TutorEval和TutorChat，通过TutorEval基准可以衡量LMs作为科学助手的实际可用性，TutorChat数据集用于微调模型。 |
| [^231] | [When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models](https://arxiv.org/abs/2402.11100) | 该论文提出了一个谬误理解基准FLUB，挑战大型语言模型在推理和理解能力上，重点是通过设计狡猾问题评估LLMs的谬误理解能力。 |
| [^232] | [Word Embeddings Revisited: Do LLMs Offer Something New?](https://arxiv.org/abs/2402.11094) | 该论文系统地比较了经典词嵌入技术和基于LLM的词嵌入，发现LLMs倾向于将语义相关的单词更紧密地聚类在一起，并在Bigger Analogy Test Set（BATS）上具有更高的平均准确度。 |
| [^233] | [Model Editing by Pure Fine-Tuning](https://arxiv.org/abs/2402.11078) | 纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。 |
| [^234] | [AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators](https://arxiv.org/abs/2402.11073) | 提出了一种新框架 AFaCTA，利用大型语言模型辅助事实性索赔的标注，提高了标注的效率和一致性。 |
| [^235] | [Bridging Causal Discovery and Large Language Models: A Comprehensive Survey of Integrative Approaches and Future Directions](https://arxiv.org/abs/2402.11068) | 本文综合调查了将大型语言模型（如GPT4）整合到因果发现任务中的方法，揭示了它们在推断因果结构时对元数据和自然语言的创新利用，强调了LLMs在增强传统CD方法和作为专家辅助方面的潜力和挑战。 |
| [^236] | [Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement](https://arxiv.org/abs/2402.11060) | 介绍了 Persona-DB，一个简单却有效的框架，通过层级构建过程和协同优化，改善了大规模语言模型个性化中数据库表示的泛化能力和检索效率。 |
| [^237] | [II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering](https://arxiv.org/abs/2402.11058) | II-MMR提出了一种新颖的方法，用于识别和改进视觉问答中的多模态多跳推理，通过引入答案预测引导的CoT提示和知识三元组引导的提示，实现对不同推理情况的分析和识别。 |
| [^238] | [Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives](https://arxiv.org/abs/2402.11051) | 提出了Conan数据集，用于从侦探叙事中提取和分析复杂的人物关系图，并揭示大型语言模型在推理复杂关系和处理长篇叙事方面的局限性。 |
| [^239] | [Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?](https://arxiv.org/abs/2402.11035) | DPR微调预训练网络以增强查询和相关文本数据之间的嵌入对齐，发现训练中知识去中心化，但也揭示了模型内部知识的局限性 |
| [^240] | [PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering](https://arxiv.org/abs/2402.11034) | PAT-Questions基准用于现在时刻为锚点的时间问答，通过自动刷新答案以解决大型语言模型知识过时、复杂时间关系难以推理、可能需要多跳推理以及基准答案持续更新等挑战。 |
| [^241] | [Exploring Value Biases: How LLMs Deviate Towards the Ideal](https://arxiv.org/abs/2402.11005) | 研究发现大型语言模型（LLMs）在给出响应时存在一个价值偏好的机制，倾向于偏向理想状态，这种偏差会对不同应用场景产生重要影响。 |
| [^242] | [ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment](https://arxiv.org/abs/2402.11000) | 提出了一个新的实体对齐框架ASGEA，利用Align-Subgraphs中的逻辑规则，设计了可解释的基于路径的图神经网络ASGNN，引入了多模态注意机制，取得了令人满意的实验结果 |
| [^243] | ["Understanding AI": Semantic Grounding in Large Language Models](https://arxiv.org/abs/2402.10992) | 大型语言模型（LLMs）展现了对语义的渐进理解，通过应用心灵哲学和语言学中关于含义的核心假设，研究发现LLMs不仅仅是生成文本的工具，而是在某种程度上已经理解了它们生成的语言。 |
| [^244] | [WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing](https://arxiv.org/abs/2402.10987) | 该研究提出了一种名为WilKE的知识编辑方法，通过选择编辑层来匹配不同层级中的知识编辑模式程度，在终身编辑中相比最先进的方法平均展现了46.2%和67.8%的改进。 |
| [^245] | [FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models](https://arxiv.org/abs/2402.10986) | FinTral是一类基于Mistral-7b模型的GPT-4级别多模态金融大型语言模型，通过领域特定的预训练和检索方法优化，在AI驱动金融技术中取得显著进展。 |
| [^246] | [SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs](https://arxiv.org/abs/2402.10979) | 这项研究介绍了围绕体育数据分析展开的四项新任务，旨在评估大型语言模型在数值推理和信息融合方面的能力。 |
| [^247] | [Language Models with Conformal Factuality Guarantees](https://arxiv.org/abs/2402.10978) | 提出了一种能够通过连接语言建模和符合预测为语言模型提供高概率正确性保证的框架。 |
| [^248] | [Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model](https://arxiv.org/abs/2402.10965) | 大型语言模型在医疗健康领域有着重要作用，然而它们的泛化效果取决于在不同临床环境和人群中的表现，对于泛化能力不足的原因进行了分析，发现在样本较少的医院和特定人群中存在挑战。 |
| [^249] | [GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements](https://arxiv.org/abs/2402.10963) | 提出了Stepwise ORMs (SORMs)，它们在合成数据上训练，以近似预测最优策略的未来预期奖励 |
| [^250] | [Measuring and Controlling Persona Drift in Language Model Dialogs](https://arxiv.org/abs/2402.10962) | 提出了一种量化基准来测量语言模型对话中的“人设”漂移，并提出了一种称为split-softmax的轻量级方法来对抗注意力衰减和“人设”漂移 |
| [^251] | [Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts](https://arxiv.org/abs/2402.10958) | 提出了相对优先权优化（RPO）方法，通过区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应，扩展了模型的学习能力。 |
| [^252] | [DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting](https://arxiv.org/abs/2402.10951) | DAEDRA是一种旨在在被动报告中检测监管相关结果的大型语言模型，弥补了通用模型无法捕捉临床维度与专业模型在非专业报告上表现不佳的缺陷 |
| [^253] | [The Unreasonable Effectiveness of Eccentric Automatic Prompts](https://arxiv.org/abs/2402.10949) | 异类自动提示的不合理有效性研究了大型语言模型在处理各种提示时的表现，结果显示在大多数情况下，包括“积极思考”提示会对模型性能产生积极影响。 |
| [^254] | [Zero-shot Explainable Mental Health Analysis on Social Media by incorporating Mental Scales](https://arxiv.org/abs/2402.10948) | 该方法结合心理量表通过LLMs进行零-shot心理健康分析，实验结果表明其优于其他方法 |
| [^255] | [CultureLLM: Incorporating Cultural Differences into Large Language Models](https://arxiv.org/abs/2402.10946) | 提出了一种名为CultureLLM的成本效益高的解决方案，通过使用世界价值调查（WVS）作为种子数据，并通过提出的语义数据增强来将文化差异纳入大型语言模型中，成功微调得到了涵盖富裕和低资源语言的9种文化特定LLMs以及一个统一模型（CultureLLM-One）。 |
| [^256] | [Advances and Limitations in Open Source Arabic-Script OCR: A Case Study](https://arxiv.org/abs/2402.10943) | 本文研究了开源OCR引擎Kraken在阿拉伯学术期刊al-Abhath上的准确性，表明其可以生成高度准确的阿拉伯文本OCR。文章建议通过更系统的训练数据生产和关键技术组件的开发来显着提高阿拉伯文字OCR的性能。 |
| [^257] | [Text2Data: Low-Resource Data Generation with Textual Control](https://arxiv.org/abs/2402.10941) | Text2Data提出了一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法，以解决低资源环境下缺乏文本标签的文本到数据任务中的挑战。 |
| [^258] | [Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification](https://arxiv.org/abs/2402.10940) | 研究引入了医学熵的概念，通过神经机器翻译基于ICD-9代码的患者预测结果，量化了不确定性。 |
| [^259] | [News Source Credibility Assessment: A Reddit Case Study](https://arxiv.org/abs/2402.10938) | 提出了针对Reddit提交的来源可信度评估模型CREDiBERT，采用半监督训练方法，结合Siamese神经网络显著提高了提交可信度的分类准确性，并引入了一种新的Reddit帖子-帖子网络版本来增强用户互动编码。 |
| [^260] | [LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration](https://arxiv.org/abs/2402.10908) | 通过LLAMA2语言模型，建立了一个能够从社交媒体和紧急消息中识别和分类紧急情况的方法，可协助在全国范围内的紧急情况下公共安全话务员和大众，提供相关指导并通知政府机构。 |
| [^261] | [Taxonomy-based CheckList for Large Language Model Evaluation](https://arxiv.org/abs/2402.10899) | 本研究在大型语言模型中引入人类知识，通过问答任务探究LM的不道德行为，发现了一致性和偏见倾向之间的关联。 |
| [^262] | [Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities](https://arxiv.org/abs/2402.10835) | 本研究通过比较LLMs与传统模型，发现了LLMs在时间序列预测中的优势和局限性，指出LLMs在预测具有明显模式和趋势的时间序列方面表现出色，但在缺乏周期性的数据集方面面临挑战，同时指出融入外部知识和采用自然语言释义有助于提升LLMs在时间序列预测中的性能。 |
| [^263] | [InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?](https://arxiv.org/abs/2402.10567) | 本研究在印度法律领域探讨了大型语言模型（LLMs）在处理社会因素时的能力，提出了结合公平性和准确性的新指标$LSS_{\beta}$，并评估了模型在二元法律推理任务中的表现以及在印度社会各种不平等方面的公平性展示。 |
| [^264] | [Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment](https://arxiv.org/abs/2402.10207) | 本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。 |
| [^265] | [Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective](https://arxiv.org/abs/2402.10184) | 本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。 |
| [^266] | [LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild](https://arxiv.org/abs/2402.09997) | LoraRetriever提出了一种适应输入的LoRA检索与合成方法，用于弥合实际情况下大型语言模型接收到不同任务提示的差距。 |
| [^267] | [LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition](https://arxiv.org/abs/2402.09989) | 本文提出了RiVEG，一个统一的框架，通过利用大型语言模型（LLMs）作为连接桥梁，将多模态命名实体识别重新构建为联合任务，解决了命名实体无法确定和指代表达与命名实体之间的区别的问题。 |
| [^268] | [Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation](https://arxiv.org/abs/2402.09954) | 本研究通过对大型语言模型在基于角色生成对话方面进行实验，发现调整提示指令可以最直接有效且经济地提高生成质量，并且随机检索示范会取得最佳结果，而查询相同上下文的示范检索效果最差。即使破坏了示范中的多回合关联和单回合语义，对话生成仍然有效。 |
| [^269] | [Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence](https://arxiv.org/abs/2402.09880) | 该论文通过批判性评估研究了23个最先进的大型语言模型基准的不足之处，包括偏见、真实推理衡量困难、实现不一致性等问题，强调了在人工智能时代需要标准化方法、监管确定性和伦理指南。 |
| [^270] | [Emerging Opportunities of Using Large Language Language Models for Translation Between Drug Molecules and Indications](https://arxiv.org/abs/2402.09588) | 本研究探讨了使用大型语言模型在药物分子和适应症之间进行翻译的机遇，提出了一个新任务，并测试了其有效性，这对于药物发现过程具有重要意义。 |
| [^271] | [LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset](https://arxiv.org/abs/2402.09391) | 本文介绍了LlaSMol，它是一种推进化学领域大规模语言模型的方法。通过使用一个大规模、全面、高质量的指令调优数据集来训练模型，LlaSMol在化学任务中表现出强大的性能，超过了GPT-4并接近于任务特定模型。 |
| [^272] | [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](https://arxiv.org/abs/2402.08983) | 本文提出了一种名为SafeDecoding的解决方案，通过安全感知解码策略来防御大型语言模型（LLMs）的越狱攻击。该策略可以生成对用户查询有益且无害的响应，有效缓解了LLMs安全性威胁。 |
| [^273] | [Structured Language Generation Model for Robust Structure Prediction](https://arxiv.org/abs/2402.08971) | 鲁棒结构预测的结构化语言生成模型通过新的损失函数和推理方法的混合，成功提高了结构化输出的泛化能力，并且可以在没有数据集信息的情况下工作，并且减少了格式错误。 |
| [^274] | [Knowledge Editing on Black-box Large Language Models](https://arxiv.org/abs/2402.08631) | 这项研究提出了在黑盒大型语言模型上进行知识编辑的方法，并引入了一种多角度评估框架和一种新的postEdit框架，以解决现有方法中的隐私和风格问题。 |
| [^275] | [ChatCell: Facilitating Single-Cell Analysis with Natural Language](https://arxiv.org/abs/2402.08303) | ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。 |
| [^276] | [UFO: A UI-Focused Agent for Windows OS Interaction](https://arxiv.org/abs/2402.07939) | UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。 |
| [^277] | [Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation](https://arxiv.org/abs/2402.07092) | 本文提出了一种通过LLM-认知数据增强的方法来广义对话密集检索。该方法首先生成多级增强对话，捕捉多样的对话环境。其次，通过认知感知过程减少错误生成情况，并通过难度自适应样本筛选器选择具有挑战性的样本。 |
| [^278] | [Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty](https://arxiv.org/abs/2402.06529) | 本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。 |
| [^279] | [Multimodal Clinical Trial Outcome Prediction with Large Language Models](https://arxiv.org/abs/2402.06512) | 本研究提出了一种名为LIFTED的多模态临床试验结果预测方法，通过将不同模态数据转化为自然语言描述来统一数据，并构建统一的抗噪声编码器进行信息提取。 |
| [^280] | [On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference](https://arxiv.org/abs/2402.06262) | 本文研究了针对键值约束生成语言模型推理的驱逐策略的有效性，通过引入基于时间注意力得分和鲁棒性度量的RoCo策略，优于现有的策略。 |
| [^281] | [Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning](https://arxiv.org/abs/2402.06025) | 本论文建立了一个计算模型来模拟人们通过实验主动推断隐藏规则的过程，并发现显式假设、概率规则和在线更新的组合可以解释人们在类似Zendo任务上的表现。 |
| [^282] | [A Closer Look at the Limitations of Instruction Tuning](https://arxiv.org/abs/2402.05119) | 本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。 |
| [^283] | [UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset](https://arxiv.org/abs/2402.04588) | 本论文构建了一个开源的多语言监督微调数据集UltraLink，通过引入基于知识的数据增强方法提升了语言模型在文化特定知识上的能力，同时发现现代语言模型具有强大的跨语言迁移能力，减少了语言无关数据集的需求。 |
| [^284] | [Rendering Graphs for Graph Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2402.02130) | 本文在多模态大型语言模型中为图推理任务引入了视觉信息，并提出了一个新的基准测试数据集GITQA。实验证明，结合文本和视觉信息的结果比单一模态效果更好。 |
| [^285] | [MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech and Target Detection Using Transformer Ensembles](https://arxiv.org/abs/2402.01967) | 本文提出了一种名为MasonPerplexity的方法来解决多模态仇恨言论事件检测的问题。该方法采用Transformer集成的方式，在识别仇恨言论和识别文本图像中目标的任务中均取得了较好的成绩，分别排名第三。 |
| [^286] | [Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties](https://arxiv.org/abs/2402.01741) | 本研究开发并测试了一种新的基于大型语言模型的临床决策支持系统，用于在12个临床专业中提供安全的药物处方。该系统通过定制的处方错误警报解决了传统基于规则的系统的局限性，并评估了其在识别药物错误方面的有效性和临床医生的偏好。 |
| [^287] | [Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884) | 提出了纠正检索增强生成（CRAG）来改善生成模型的鲁棒性，通过设计轻量级检索评估器和利用大规模网络搜索扩展检索结果。 |
| [^288] | [Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA](https://arxiv.org/abs/2401.15847) | 引入了多面板视觉问答（MultipanelVQA）基准挑战大型视觉语言模型（LVLMs）对理解多面板图像的能力，并发现LVLMs在这方面仍然存在显著挑战。 |
| [^289] | [RecDCL: Dual Contrastive Learning for Recommendation](https://arxiv.org/abs/2401.15635) | RecDCL提出了一个双重对比学习推荐框架，结合了批次对比学习（BCL）和特征对比学习（FCL），有助于消除冗余的解决方案，但又不会错过最优解。 |
| [^290] | [Do We Need Language-Specific Fact-Checking Models? The Case of Chinese](https://arxiv.org/abs/2401.15498) | 本文研究了语言特定事实核查模型的潜在益处，提出了一个汉语事实核查系统，并展示其优于翻译方法和多语言大型语言模型，同时对偏见更加稳健，强调了语言特定性的重要性。 |
| [^291] | [An Empirical Study of In-context Learning in LLMs for Machine Translation](https://arxiv.org/abs/2401.12097) | 该研究对机器翻译中LLMs的上下文学习进行了全面深入的研究，探讨了示例驱动的特点以及示例对下游性能的影响，同时也探讨了ICL的局限性。 |
| [^292] | [Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?](https://arxiv.org/abs/2401.11911) | 该论文研究了大型语言模型如何合并生成和检索的上下文以提升开放领域问答，发现这些模型偏向于生成的上下文，即使它们提供了错误的信息。 |
| [^293] | [PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety](https://arxiv.org/abs/2401.11880) | PsySafe提出了一个综合框架，通过深入探讨智能体心理学，揭示智能体的黑暗心理状态对安全构成威胁，并提出了有效的风险缓解策略。 |
| [^294] | [Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions](https://arxiv.org/abs/2401.09395) | 通过引入数学和编码问题的扰动本体以及两个数据集，作者评估了LLMs在数字推理和编码任务中的能力，在全面评估中发现所有模型在扰动问题上表现显著下降，表明当前的LLMs缺乏稳健性。 |
| [^295] | [DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models](https://arxiv.org/abs/2401.08392) | DoraemonGPT是一个由LLMs驱动的系统，旨在处理动态视频任务，通过将视频转换为符号记忆来进行空间-时间查询和推理，并取得简洁的中间结果。 |
| [^296] | [PRewrite: Prompt Rewriting with Reinforcement Learning](https://arxiv.org/abs/2401.08189) | 本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。 |
| [^297] | [Question Translation Training for Better Multilingual Reasoning](https://arxiv.org/abs/2401.07817) | 本文探讨了通过问题对齐训练模型将推理问题翻译成英语的方法，以实现有针对性的、领域内的语言对齐，最大限度地利用英语指导数据，释放了LLMs的多语言推理能力。 |
| [^298] | [Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation](https://arxiv.org/abs/2401.07382) | 本文提出了一种新的框架，利用大型语言模型的评论能力在强化学习训练中产生中间步骤奖励，以应对稀疏奖励信号所带来的挑战。 |
| [^299] | [Graph Language Models](https://arxiv.org/abs/2401.07105) | 引入了一种新型的图语言模型（GLM），结合线性化和图神经网络的优点，解决了传统方法在处理结构化知识图谱时的弱点。 |
| [^300] | [Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering](https://arxiv.org/abs/2401.06824) | 通过表示工程对LLMs进行越狱是一种新颖的方法，它利用少量查询对提取“安全模式”，成功规避目标模型的防御，实现了前所未有的越狱性能。 |
| [^301] | [SIG: Speaker Identification in Literature via Prompt-Based Generation](https://arxiv.org/abs/2312.14590) | 通过基于生成的方法SIG，在文学作品中实现了说话者识别任务，支持跨领域评估和开放世界分类范式。 |
| [^302] | [Demystifying Instruction Mixing for Fine-tuning Large Language Models](https://arxiv.org/abs/2312.10793) | 指令微调提升了大语言模型在各种任务中的性能，研究发现不同指令类型对特定应用更有利，但可能对其他领域产生负面影响。 |
| [^303] | [TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning](https://arxiv.org/abs/2312.09039) | TAP4LLM提出了一个用于生成表格提示的多功能预处理工具箱，通过采样、增补和打包半结构化数据，解决了在大型语言模型推理中处理复杂问题和大型表格的挑战。 |
| [^304] | [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/abs/2312.08935) | Math-Shepherd提出了一种新的数学奖励模型，通过自动生成的过程监督数据实现LLMs的逐步验证和加强，显著提高了数学问题解决的准确性。 |
| [^305] | [KnowGPT: Black-Box Knowledge Injection for Large Language Models](https://arxiv.org/abs/2312.06185) | KnowGPT是一种为大型语言模型提供黑盒知识注入的框架，通过深度强化学习和多臂老虎机构建最适合每个问题的提示，在三个基准数据集上实验证明其显著提升了知识注入的效果。 |
| [^306] | [Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding](https://arxiv.org/abs/2312.06149) | 提出了将文本生成形式化为未来受限生成问题的方法，以最小化不良行为并强制执行对指令的忠实性，并通过LLMs有效指导文本生成。 |
| [^307] | [Applying Large Language Models and Chain-of-Thought for Automatic Scoring](https://arxiv.org/abs/2312.03748) | 本研究探讨了在自动评分学生对科学评估写作反馈中应用大型语言模型（LLMs）和思维链（CoT），通过零次或少次学习等策略自动评分，其中少次学习表现更佳。 |
| [^308] | [UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296) | 评估了中文大型语言模型在文本生成中的虚构能力，并指出现有基准评估通常采用受限制的生成技术，无法满足真实需求中的无约束文本生成。 |
| [^309] | [Code Search Debiasing:Improve Search Results beyond Overall Ranking Performance](https://arxiv.org/abs/2311.14901) | 该论文提出了一个通用的去偏见框架，通过重新排序来校准搜索结果，有效缓解了代码搜索引擎的偏见，从而提高了代码搜索的总体排名表现。 |
| [^310] | [LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions](https://arxiv.org/abs/2311.11904) | 提出了一种集成LLMs和VLMs的框架，以找到最佳的类别描述符，解决了图像分类中在精确构建文本表示和区分相似类别方面的挑战 |
| [^311] | [Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information](https://arxiv.org/abs/2311.11509) | 该论文引入了一种新的方法，在令牌级别检测对抗性提示，利用语言模型的能力预测下一个标记的概率。 |
| [^312] | [MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning](https://arxiv.org/abs/2311.10537) | 提出了一个新颖的医学领域的跨学科合作(MC)框架，利用基于LLM的代理在角色扮演设置中参与协作多轮讨论，从而提高LLM的熟练程度和推理能力 |
| [^313] | [How Far Can We Extract Diverse Perspectives from Large Language Models?](https://arxiv.org/abs/2311.09799) | 本研究探讨了LLMs在生成多元化观点和理由方面的能力，并提出了从LLMs中最大程度提取多样性观点的新问题。 |
| [^314] | [LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores](https://arxiv.org/abs/2311.09766) | 本文研究了语言模型驱动的评估指标在总结任务中是否会对相同的基础语言模型生成的文本表现出偏见，并发现在无参考情况下使用时偏见尤为显著。 |
| [^315] | [Self-Contradictory Reasoning Evaluation and Detection](https://arxiv.org/abs/2311.09603) | 研究了大型语言模型在推理任务中自相矛盾的现象，发现在涉及上下文信息理解或常识的任务中经常存在自相矛盾，而高准确性并不总是对应较低的自相矛盾率。 |
| [^316] | [DocLens: Multi-aspect Fine-grained Evaluation for Medical Text Generation](https://arxiv.org/abs/2311.09581) | 本文提出了一个名为DocLens的框架，通过一组新的度量标准，在多个任务中展示其对医学文本生成的有效性，并且通过人类研究表明其在与医学专家判断的一致性上优于现有指标，同时指出了改进开源评估者的必要性。 |
| [^317] | [TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction](https://arxiv.org/abs/2311.09562) | 本研究提出了TextEE，这是一个用于事件提取的标准化、公平和可重复的基准，解决了评估中存在的挑战。 |
| [^318] | [Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models](https://arxiv.org/abs/2311.09278) | Symbol-LLM 提出了一种通过数据和框架来解决大型语言模型中符号数据注入的挑战，旨在捕捉符号间的相互关系和促进协同作用。 |
| [^319] | [Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation](https://arxiv.org/abs/2311.09136) | 提出一种使用部分排序来优化LLMs的方法，能够通过训练模型优先考虑特定任务候选响应池中的最佳响应，从而改善响应生成能力。 |
| [^320] | [Social Bias Probing: Fairness Benchmarking for Language Models](https://arxiv.org/abs/2311.09090) | 本文提出了一个用于探测语言模型中社会偏见的原创框架，包括对一般关联和社会类别、身份以及刻板印象的分析。 |
| [^321] | [Safer-Instruct: Aligning Language Models with Automated Preference Data](https://arxiv.org/abs/2311.08685) | Safer-Instruct通过反向指导调整、指导感应和专家模型评估的方式，实现了自动构建大规模偏好数据的目的，从而在没有人工标注者的情况下高效生成高质量的偏好数据。 |
| [^322] | [DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Language Models](https://arxiv.org/abs/2311.08598) | DALA是一种基于分布感知的LoRA对抗攻击方法，旨在改善对抗性样本的数据分布，提高攻击效果，并引入了非可检测攻击成功率（NASR）评价指标。 |
| [^323] | [A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268) | 提出一种利用大型语言模型自动生成有效的越狱提示的自动框架ReNeLLM，显著提高攻击成功率，同时大大减少时间成本。 |
| [^324] | [Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios](https://arxiv.org/abs/2311.08154) | 自一致性是一种通用的集成优化方法，可以应用于几乎所有情景中，能够解决语言模型推理中的重复性和局部最优性问题。 |
| [^325] | [Adversarial Preference Optimization](https://arxiv.org/abs/2311.08045) | 提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。 |
| [^326] | [Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators](https://arxiv.org/abs/2311.07879) | 本研究揭示了人工智能模型在识别有毒、冒犯和令人讨厌的内容方面的进展，并探讨了这些改进是否真正满足了志愿内容管理员在工作中的需求。 |
| [^327] | [It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning](https://arxiv.org/abs/2311.07532) | 大型语言模型在排除推理过程中遇到困难，提出了一种新的排除推理方法PoE与COT，发现此方法在多项选择问题上的表现不如选择正确答案，并指出了研究中发现的一致性和错误分析的问题。 |
| [^328] | [Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion](https://arxiv.org/abs/2311.06318) | 提出一种新颖且通用的方法，通过从用户与搜索引擎的交互历史中提取相关上下文来个性化大型语言模型的输出，尤其适用于改进网络搜索体验。 |
| [^329] | [LAiW: A Chinese Legal Large Language Models Benchmark](https://arxiv.org/abs/2310.05620) | 该论文提出了一个中文法律大型语言模型基准LAiW，通过构建基于法律实践逻辑的评估任务，揭示了当前通用和法律领域LLM可能不符合法律实践逻辑。 |
| [^330] | [Faithful Knowledge Graph Explanations for Commonsense Reasoning](https://arxiv.org/abs/2310.04910) | 本论文提出了两个量化指标来衡量基于知识图谱的解释的可信性，并引入了一种新的训练方法来改善解释的可信度。实验结果表明该方法可以提高解释的一致性和保真度。 |
| [^331] | [Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models](https://arxiv.org/abs/2310.00566) | 大型语言模型在信用评分任务中表现出强大的通用能力，通过首个开源框架和CALM模型，可以帮助解决传统信用评分方法所面临的挑战，并同时解决潜在的偏见问题。 |
| [^332] | [From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning](https://arxiv.org/abs/2310.00492) | 指令调整对LLMs产生了三个重要影响：1）使其能够识别用户提示中的指令部分；2）促进响应生成的不断调整 |
| [^333] | [OctoPack: Instruction Tuning Code Large Language Models](https://arxiv.org/abs/2308.07124) | 通过结合Git提交的自然结构，将代码更改与人类指令配对，我们提出了OctoPack，并在大规模语言模型上实现了表现最佳的指令调整方法。 |
| [^334] | [Building Cooperative Embodied Agents Modularly with Large Language Models](https://arxiv.org/abs/2307.02485) | 利用大型语言模型构建模块化的协作体现智能体，实现多智能体合作解决具有挑战性的任务，超越规划方法并展示有效沟通。 |
| [^335] | [BMX: Boosting Natural Language Generation Metrics with Explainability](https://arxiv.org/abs/2212.10469) | 提出的方法BMX利用解释来提升自然语言生成度量的性能，通过将特征重要性解释转化为段落级分数，并结合原始度量，取得了更好的评估结果。 |
| [^336] | [G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks](https://arxiv.org/abs/2212.03613) | G-MAP提出了一种新的通用记忆增强的预训练语言模型框架，能够在增强领域特定PLM时，保留通用知识，缓解灾难性遗忘现象，提高模型性能 |
| [^337] | [Self-consistent Reasoning For Solving Math Word Problems](https://arxiv.org/abs/2210.15373) | 提出了自洽推理框架SCR，通过修剪策略和对称Kullback-Leibler散度校准输出分布偏移，从而解决数学应用题中虚假相关性的问题 |
| [^338] | [Recipe Generation from Unsegmented Cooking Videos](https://arxiv.org/abs/2209.10134) | 本文研究如何从未分割的烹饪视频中生成正确的食谱，通过选择神谕事件并重新生成句子来实现。 |
| [^339] | [Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings](https://arxiv.org/abs/2104.08928) | 提出了一种基于群稀疏矩阵分解的方法，用于在新领域进行词嵌入的传递学习，以解决不同领域单词含义差异的挑战。 |
| [^340] | [CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models.](http://arxiv.org/abs/2401.17043) | 这篇论文构建了一个大规模且更全面的中文基准测试，评估了检索增强生成系统的所有组件在各种应用场景中的性能。 |
| [^341] | [Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports.](http://arxiv.org/abs/2401.16578) | 该论文提出了一种方法，将专业放射科医生的专业知识与大型语言模型相结合，来提升自动生成报告的自动评估。实验结果显示，该方法的模型在评估中表现优于传统的度量标准。 |
| [^342] | [A Unified Approach to Emotion Detection and Task-Oriented Dialogue Modeling.](http://arxiv.org/abs/2401.13789) | 提出了一种统一的方法来实现情感检测和任务导向对话建模，通过在信念状态跟踪中引入情感检测实现，并将其融入端到端的任务导向对话系统中。实验证明该方法提高了情感检测和任务结果的性能，并显示用户的情感可以作为回应的上下文条件，对于提高回应的共鸣程度具有帮助。 |
| [^343] | [MM-LLMs: Recent Advances in MultiModal Large Language Models.](http://arxiv.org/abs/2401.13601) | 近年来，多模式大语言模型（MM-LLMs）通过成本效益高的训练策略取得了显著进展，扩展了现有的语言模型的多模输入和输出支持。本论文提供了一份综合调查报告，介绍了MM-LLMs的设计和训练方案，整理了现有的MM-LLMs及其性能，总结了关键训练方法，并探讨了未来的研究方向。 |
| [^344] | [Prompt Weight Experiments for LLM Instruction Fine-Tuning.](http://arxiv.org/abs/2401.13586) | LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。 |
| [^345] | [Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation.](http://arxiv.org/abs/2401.10186) | 开放式大型语言模型在零-shot设置下能够从各种标准数据格式中生成流畅和连贯的文本，但是输出的语义准确性仍然是一个重要问题。 |
| [^346] | [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents.](http://arxiv.org/abs/2401.10019) | 这篇论文主要介绍了一种评估LLM代理在不同环境中判断安全风险能力的基准测试R-Judge，通过对162个代理交互记录进行评估，发现GPT-4模型表现最佳，达到了72.29%的准确率。 |
| [^347] | [INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning.](http://arxiv.org/abs/2401.06532) | 本研究探索了指令调优的方法，以增强大型语言模型在信息检索任务中的能力，通过引入一个新的指令调优数据集INTERS，涵盖了21个IR任务，该方法显著提升了性能。 |
| [^348] | [EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction.](http://arxiv.org/abs/2401.06201) | EASYTOOL是一个将多样化而冗长的工具文档转化为统一而简洁的工具指示的框架，用于增强基于LLM的代理的能力。通过从多个来源提取关键信息，并提供标准化的工具描述和功能，EasyTool显著降低了标记消耗，并提高了在真实场景中的工具利用性能。 |
| [^349] | [SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency.](http://arxiv.org/abs/2311.01740) | SAC^3是一种基于语义感知交叉检查一致性的方法，可以可靠地检测黑盒语言模型中的幻觉。该方法通过加入语义等效问题扰动和跨模型响应一致性检查等机制，能够有效识别问题级别和模型级别的幻觉。实证分析表明，SAC^3在检测非事实和事实陈述方面优于现有技术最新水平。 |
| [^350] | [Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions.](http://arxiv.org/abs/2311.00233) | 本文提出了一种简单而有效的方法，通过对比性调整预测，利用嘈杂指令来增强指令调节模型的效果，从而改进了面临超出训练范围的指令时的响应准确性。 |
| [^351] | [Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots.](http://arxiv.org/abs/2310.17976) | 本文介绍了一种创新的方法，用于评估角色扮演聊天机器人的个性特点，并发现基于LLMs的现代角色扮演聊天机器人能够有效地描绘出相应角色的个性特点，与人类感知的一致性达到82.8%。 |
| [^352] | [Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages.](http://arxiv.org/abs/2310.17953) | Whisper-MCE是使用自己收集的混合粤语和英语音频数据集（MCE）进行训练的Whisper模型微调，相较于基准模型，其在准确捕捉原始音频内容、提高识别准确性和加快识别速度方面具有更优越的能力，尤其在混合语言识别任务中表现出色。 |
| [^353] | [Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation.](http://arxiv.org/abs/2310.13505) | 这项研究提出了一种新的框架REIGN，通过生成训练问题的改写，并使用深度强化学习来指导对话问答模型，增加模型对表面形式变化的鲁棒性，同时在不同的基准上进行零-shot应用。 |
| [^354] | [Large Language Model Unlearning.](http://arxiv.org/abs/2310.10683) | 大型语言模型的去学习是一个研究的新领域，我们探索了三个场景，可以通过去学习让语言模型与人类偏好保持一致。去学习具有三个优势，只需要负面示例，计算效率高，特别对于知道具体导致不良行为的训练样本更为有效。 |
| [^355] | [Understanding the Effects of RLHF on LLM Generalisation and Diversity.](http://arxiv.org/abs/2310.06452) | 本研究深入分析了强化学习从人类反馈中调整的大型语言模型每个阶段对超出分布泛化和输出多样性的影响。 |
| [^356] | [JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning.](http://arxiv.org/abs/2310.02953) | JsonTuning是一种面向通用、强大和可控的指令调优方法，通过利用JSON的结构化特性，帮助模型理解任务要素及其关系，从而扩展了通用性、提高了稳健性，并增强了对输出的控制。 |
| [^357] | [ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events.](http://arxiv.org/abs/2309.12244) | ChaCha是一个利用大型语言模型（LLMs）的聊天机器人，鼓励儿童分享个人事件和相关情绪。通过一个探索性研究，发现儿童将ChaCha视为亲密的朋友，并愿意与其分享各种主题的故事。 |
| [^358] | [Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models.](http://arxiv.org/abs/2309.08902) | 本文调查了LLMs在年龄、美丽、机构和国籍等少研究但仍然重要的维度上的偏见，通过衡量在社会群体和不相关的正负属性之间做出的微妙相关决策。研究发现LLMs在特定社会群体上存在类似于“美丽即善”的广泛正面或负面态度的偏见。 |
| [^359] | [Anchor Points: Benchmarking Models with Much Fewer Examples.](http://arxiv.org/abs/2309.08638) | 这个论文介绍了一种使用更少的示例来对模型进行基准测试的方法，并提出了锚点选择技术来捕捉模型行为。实验证明，使用锚点对模型进行排序比使用均匀采样和其他基线方法更准确。仅使用几个锚点就可以估计模型对数据集中所有其他点的每个类别的预测，用于衡量模型性能。 |
| [^360] | [PROGrasp: Pragmatic Human-Robot Communication for Object Grasping.](http://arxiv.org/abs/2309.07759) | PROGrasp是一个实现物体抓取的人机交流系统，通过使用面向意图的多模态对话和答案解释模块，机器人能够根据用户的意图来识别和抓取目标物体。 |
| [^361] | [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning.](http://arxiv.org/abs/2309.05173) | DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。 |
| [^362] | [Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models.](http://arxiv.org/abs/2308.15022) | 递归总结在大型语言模型中实现长期对话记忆，可以提高对话系统在长对话中记忆重要信息的能力。 |
| [^363] | [Exploring Large Language Models for Knowledge Graph Completion.](http://arxiv.org/abs/2308.13916) | 本文研究了利用大型语言模型（LLM）进行知识图谱补全的方法，并引入了一种创新的框架（知识图谱LLM），以提高三元组分类和关系预测的性能。 |
| [^364] | [Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi.](http://arxiv.org/abs/2308.09862) | 本论文开发了一个用于印地语和马拉地语的问答数据集，通过翻译SQuAD 2.0数据集解决了数据稀缺问题，提供了这两种语言的最好表现模型。 |
| [^365] | [Backward Reasoning in Large Language Models for Verification.](http://arxiv.org/abs/2308.07758) | 本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。 |
| [^366] | [OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples.](http://arxiv.org/abs/2307.11729) | OUTFOX是一个新的框架，通过允许检测器和攻击者考虑彼此的输出，提高了LLM生成文本检测器的鲁棒性。攻击者利用检测器的预测标签作为示例进行上下文学习，并生成难以检测的对抗生成的论文。 |
| [^367] | [Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success.](http://arxiv.org/abs/2307.06865) | 本论文提出了一个系统地衡量提示提取攻击成功的框架，并通过多个实验发现，即使提示被保密，简单的基于文本的攻击仍然可以高概率地揭示提示。 |
| [^368] | [Towards Personalized Cold-Start Recommendation with Prompts.](http://arxiv.org/abs/2306.17256) | 本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。 |
| [^369] | [Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias.](http://arxiv.org/abs/2305.19894) | Med-UniC是一个新的框架，旨在通过整合英语和西班牙语的跨语言医学数据，实现跨语言医学图像-语言预训练的统一。他们提出了跨语言文本对齐规则(CTR)，以明确统一来自不同语言社区的医学报告的跨语言语义表示。 |
| [^370] | [Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery.](http://arxiv.org/abs/2305.14259) | 本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。 |
| [^371] | [LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models.](http://arxiv.org/abs/2305.13718) | 本文介绍了 LogicLLM，一种通过自监督后训练来提高大语言模型的逻辑推理能力的方法，该方法有效地在常见逻辑推理任务上进行表现，超过了目前最先进的无监督基线方法。 |
| [^372] | [GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study.](http://arxiv.org/abs/2305.13062) | 本文设计了一个基准测试来评估大型语言模型（LLMs）对结构化表格数据的理解能力，并发现不同的输入选择会对性能产生影响。在基准测试的基础上，提出了“自我增强”技术以改善理解能力。 |
| [^373] | [EventNet-ITA: Italian Frame Parsing for Events.](http://arxiv.org/abs/2305.10892) | 本文介绍了一个用于意大利语的大规模、多领域事件框架标注语料库-EventNet-ITA，并提出了一种高效的多标签框架解析方法。EventNet-ITA的主要贡献是为研究人员提供了一个用于文本事件挖掘的资源和意大利语框架解析的新颖工具。 |
| [^374] | [Analyzing FOMC Minutes: Accuracy and Constraints of Language Models.](http://arxiv.org/abs/2304.10164) | 该研究分析了FOMC官方声明中使用的语言，采用VADER和FinBERT等模型预测负面情绪，结果显示FinBERT表现相对更好。但是，该研究也强调了使用当前NLP技术分析FOMC文本的挑战和限制，建议增强语言模型并探索替代方法。 |
| [^375] | [Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study.](http://arxiv.org/abs/2304.04339) | 本文对ChatGPT作为情感分析器进行了初步评估，包括标准评估、极性转移评估、开放域评估和情感推理评估，共涉及18个数据集和5个情感分析任务。与经过微调的BERT和最先进的模型进行了对比，并进行了人工评估和案例研究。 |
| [^376] | [KPEval: Towards Fine-grained Semantic-based Evaluation of Keyphrase Extraction and Generation Systems.](http://arxiv.org/abs/2303.15422) | KPEval是一个综合评估框架，旨在解决关键词提取和生成系统评估中的短板。通过引入四个关键维度，包括显著性、忠实性、多样性和实用性，并设计相应的语义度量指标，KPEval在与人类偏好相关性方面表现更好。使用该框架重新评估了20个关键词系统，并发现模型选择最佳的情况取决于评估维度，实用性是一个重要因素。 |
| [^377] | [Can AI-Generated Text be Reliably Detected?.](http://arxiv.org/abs/2303.11156) | 本研究通过实证和理论分析表明，在实际场景中，几种AI文本检测器不可靠。改写攻击可以破解多种检测器，包括水印方案、神经网络检测器和零样本分类器。即使是最好的检测器，随着语言模型的进一步提升，性能也会下降。因此，AI生成的文本的可靠检测仍然是一个挑战。 |

# 详细

[^1]: TrustAgent: 通过代理构成实现安全可信赖的LLM代理

    TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution

    [https://rss.arxiv.org/abs/2402.01586](https://rss.arxiv.org/abs/2402.01586)

    本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。

    

    近年来，基于LLM的代理引起了广泛关注，但其可信度仍未得到深入探索。由于代理可以直接与物理环境交互，其可靠性和安全性至关重要。本文提出了一种基于代理构成的代理框架TrustAgent，对LLM代理的安全性维度进行了初步研究。该框架包括三种策略：预先规划策略，在生成计划之前向模型注入安全知识；规划过程中策略，在生成计划时增强安全性；计划后检查策略，通过计划后检查确保安全性。通过实验分析，我们展示了这些方法如何通过识别和预防潜在危险有效提高LLM代理的安全性。此外，我们还探讨了安全性与使用者满意度之间的复杂关系，以及模型的推理能力与其效率之间的关联。

    The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
    
[^2]: 疫苗：针对大规模语言模型的干扰感知对齐技术

    Vaccine: Perturbation-aware Alignment for Large Language Model

    [https://rss.arxiv.org/abs/2402.01109](https://rss.arxiv.org/abs/2402.01109)

    疫苗是一种针对大规模语言模型的干扰感知对齐技术，通过逐渐添加扰动产生不变的隐藏嵌入，提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。

    

    作为一种新的微调即服务范 paradigm，大型语言模型 (LLM) 为用户上传的一小部分有害数据提供了新的攻击面，这些数据很容易欺骗微调过程从而产生对齐失效的模型。我们进行了实证分析，揭示了一种可能导致对齐失效的有害嵌入漂移现象。受到我们的发现启发，我们提出了疫苗 (Vaccine) ，一种针对干扰感知的对齐技术，以减轻用户微调的安全风险。疫苗的核心思想是通过在对齐阶段逐渐添加精心设计的扰动，产生不变的隐藏嵌入，从而使嵌入能够抵御来自未经消毒的用户数据的有害扰动。我们在开源主流LLM（如Llama2，Opt，Vicuna）上的实验结果表明，疫苗能够提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。

    The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
    
[^3]: Sequoia: 可扩展、稳健且硬件感知的推测解码

    Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding

    [https://arxiv.org/abs/2402.12374](https://arxiv.org/abs/2402.12374)

    Sequoia是一种可扩展、稳健且硬件感知的推测解码算法，通过引入动态规划算法优化标记树结构、采用新颖的采样和验证方法实现稳健性能以及硬件感知的树优化器最大化推测性能。

    

    随着大型语言模型（LLMs）的使用增多，使用这些模型进行高效推理变得日益重要。虽然最近推测解码已经成为加速推理的一个有前途的方向，但现有方法在扩展到较大的推测预算、适应不同超参数和硬件方面存在局限性。本文介绍了Sequoia，一个可扩展、稳健且硬件感知的用于推测解码的算法。为了实现更好的可扩展性，Sequoia引入了一个动态规划算法来找到用于被推测标记的最佳树结构。为了实现稳健的推测性能，Sequoia使用了一种新颖的采样和验证方法，该方法在不同解码温度下优于先前的方法。最后，Sequoia引入了一种硬件感知的树优化器，通过自动选择给定情况下的标记树大小和深度来最大化推测性能。

    arXiv:2402.12374v1 Announce Type: new  Abstract: As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a giv
    
[^4]: 在跨语料评估中对命名实体识别和归一化工具的HunFlair2

    HunFlair2 in a cross-corpus evaluation of named entity recognition and normalization tools

    [https://arxiv.org/abs/2402.12372](https://arxiv.org/abs/2402.12372)

    挑战BTM工具在不同上下文中应用的可靠性，通过跨语料基准测试评估命名实体识别和归一化工具的性能。

    

    随着生命科学文献的指数增长，生物医学文本挖掘（BTM）已成为加速从出版物中提取见解的基本技术。在BTM流程中，识别文本中的命名实体（例如疾病、药物或基因）以及将其链接到参考知识库是关键步骤，以便从不同文档中启用信息聚合。然而，用于这两个步骤的工具很少在开发它们的上下文中应用。相反，它们被应用在野外，即在应用相关的文本集合上，不同于用于工具训练的文本，例如在焦点、体裁、风格和文本类型上有所不同。这引发了一个问题，即是否可以信任报告的BTM工具性能，用于下游应用。在这里，我们报告了针对命名实体提取的精心设计的跨语料基准测试结果，工具被应用到系统中

    arXiv:2402.12372v1 Announce Type: new  Abstract: With the exponential growth of the life science literature, biomedical text mining (BTM) has become an essential technology for accelerating the extraction of insights from publications. Identifying named entities (e.g., diseases, drugs, or genes) in texts and their linkage to reference knowledge bases are crucial steps in BTM pipelines to enable information aggregation from different documents. However, tools for these two steps are rarely applied in the same context in which they were developed. Instead, they are applied in the wild, i.e., on application-dependent text collections different from those used for the tools' training, varying, e.g., in focus, genre, style, and text type. This raises the question of whether the reported performance of BTM tools can be trusted for downstream applications. Here, we report on the results of a carefully designed cross-corpus benchmark for named entity extraction, where tools were applied system
    
[^5]: AnaloBench：评估抽象和长上下文类比识别的基准

    AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies

    [https://arxiv.org/abs/2402.12370](https://arxiv.org/abs/2402.12370)

    通过提出ANALOBENCH基准来评估语言模型（LMs）进行类比推理的能力，发现扩展LMs规模对于处理涉及长场景或相关经验回忆的类比时带来的性能提升较小。

    

    人类经常进行类比思维，将个人经验与当前情况联系起来（$X$类似于$Y$是因为$Z$）。类比思维使人类能够用创造性方式解决问题，理解困难概念，更有效地表达想法。能否语言模型（LMs）也能做到这一点？为了回答这个问题，我们提出了ANALOBENCH，一个用于确定LMs类比推理能力的基准。我们的基准方法专注于人类之间共同的类比推理能力方面：（i）从大量信息中回忆相关经验，以及（ii）将类比推理应用于复杂和长度较长的场景。我们测试了大量专有模型（例如，GPT系列，Claude V2）和开源模型，如LLaMA2。与先前的结果一样，扩展LMs会带来一些性能提升。令人惊讶的是，在类比涉及长场景或回忆相关经验时，规模的提升带来的增益很小。

    arXiv:2402.12370v1 Announce Type: cross  Abstract: Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) rec
    
[^6]: 一种用于NLI模型领域泛化的合成数据方法

    A synthetic data approach for domain generalization of NLI models

    [https://arxiv.org/abs/2402.12368](https://arxiv.org/abs/2402.12368)

    本研究提出了一种新方法，通过生成多样领域和长度的合成NLI数据，解决了NLI模型在领域泛化方面的问题。

    

    自然语言推理（NLI）仍然是LLMs的一个重要基准任务。 NLI数据集是迁移学习到其他语义任务的跳板，而NLI模型是识别模型生成文本忠实性的标准工具。 今天有几个大规模的NLI数据集，通过在这些集合上进行爬坡，模型已经取得了很大的改进。 然而，它们在分布/领域数据上的实际性能尚不很清楚。 我们对NLI模型领域泛化问题进行了深入探讨。 我们展示了一种在多个领域和长度生成合成NLI数据的新方法，这些数据迄今为止尚未被现有训练集覆盖。 生成的示例具有有意义的前提，假设以创造性的方式形成，而不是简单地对几个前提标记进行编辑，标签的准确率很高。 我们展示了在这些数据上训练的模型（685K个合成示例）具有

    arXiv:2402.12368v1 Announce Type: new  Abstract: Natural Language Inference (NLI) remains an important benchmark task for LLMs. NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text. There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We present an in-depth exploration of the problem of domain generalization of NLI models. We demonstrate a new approach for generating synthetic NLI data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data ($685$K synthetic examples) have the b
    
[^7]: 对大型语言模型进行AI反馈的关键评估

    A Critical Evaluation of AI Feedback for Aligning Large Language Models

    [https://arxiv.org/abs/2402.12366](https://arxiv.org/abs/2402.12366)

    研究质疑复杂的强化学习在AI反馈中的必要性，表明使用更强的教师模型进行监督微调可以超越现有的RLAIF管道。

    

    强化学习与AI反馈（RLAIF）是一种用于提高强大预训练语言模型的指令遵循能力的流行范式。 RLAIF首先使用来自教师模型的示范进行监督微调（SFT），然后再使用来自评论模型的反馈进行强化学习（RL）进一步微调模型。尽管最近流行的开源模型已经证明了从RL步骤中获得的性能显着提高，但在本文中，我们质疑是否复杂的RL步骤真正有必要为AI反馈。我们展示了RL步骤的改进几乎完全是因为使用较弱的教师模型（例如GPT-3.5）用于SFT数据收集而不是用于AI反馈生成的评论者（例如GPT-4）的广泛实践。具体而言，我们展示了简单的以GPT-4作为教师的监督微调优于现有的RLAIF管道。

    arXiv:2402.12366v1 Announce Type: cross  Abstract: Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, w
    
[^8]: 从认知驱动的语言模型中得出的词序普遍规律

    Emergent Word Order Universals from Cognitively-Motivated Language Models

    [https://arxiv.org/abs/2402.12363](https://arxiv.org/abs/2402.12363)

    认知驱动的语言模型显示出可以解释许多词序普遍规律的优势

    

    世界上的语言表现出某些所谓的类型学或蕴含规律；例如，主-宾-谓（SOV）的词序通常使用后置词。解释这些偏好的来源是语言学的一个关键目标。我们通过使用具有认知偏差的语言模型（LMs）进行计算模拟研究词序普遍规律。我们的实验证明，具有类型学典型词序的语言倾向于具有由具有认知合理偏差的LMs估计的较低困惑度：句法偏差、特定的解析策略和记忆限制。这表明，这些认知偏差和可预测性（困惑度）之间的相互作用可以解释词序普遍规律的许多方面。这也展示了认知驱动LMs的优势，在计算模拟语言普遍规律时通常用于认知建模。

    arXiv:2402.12363v1 Announce Type: new  Abstract: The world's languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) word order typically employs postpositions. Explaining the source of such biases is a key goal in linguistics. We study the word-order universals through a computational simulation with language models (LMs). Our experiments show that typologically typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of these cognitive biases and predictability (perplexity) can explain many aspects of word-order universals. This also showcases the advantage of cognitively-motivated LMs, which are typically employed in cognitive modeling, in the computational simulation of language universals.
    
[^9]: LoRA+: 大规模模型的高效低秩适应性

    LoRA+: Efficient Low Rank Adaptation of Large Models

    [https://arxiv.org/abs/2402.12354](https://arxiv.org/abs/2402.12354)

    LoRA+通过设置不同的学习率来改进原始LoRA的低效率问题，在保持计算成本不变的情况下提高了模型性能和微调速度。

    

    在这篇论文中，我们展示了低秩适应（LoRA）最初由胡等人（2021年）引入，导致对具有大宽度（嵌入维度）的模型进行微调时表现亚优。这是因为LoRA中的适配器矩阵A和B使用相同的学习率进行更新。通过对大宽度网络进行缩放参数的论证，我们展示了对适配器矩阵A和B使用相同的学习率不利于有效的特征学习。然后，我们表明LoRA的这种次优性可以简单地通过为LoRA适配器矩阵A和B设置不同的学习率以及一个精心选择的比率来进行校正。我们将这个提出的算法称为LoRA$+$。在我们广泛的实验证明中，LoRA$+$在相同计算成本下提高了性能（1-2％的改进）和微调速度（最多提速约2倍）。

    arXiv:2402.12354v1 Announce Type: cross  Abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.
    
[^10]: 基于图的检索器捕捉生物医学知识的长尾

    Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge

    [https://arxiv.org/abs/2402.12352](https://arxiv.org/abs/2402.12352)

    基于Retrieval Augmented Generation (RAG)的图检索器被提议用来克服LLMs的局限，通过从外部数据集检索的上下文来增强提示信息，从而解决生物医学领域长尾知识的捕获挑战。

    

    大型语言模型(Large language models, LLMs)正在改变信息检索的方式，通过自然语言对话总结和展示大量知识。然而，LLMs倾向于突出训练集中最常见的信息片段，并忽视罕见的信息。在生物医学研究领域，最新的发现对于学术和工业界是至关重要的，但往往被大量不断增长的文献领域所掩盖(信息过载问题)。利用LLMs展现生物医学实体之间的新关联，如药物、基因、疾病，成为捕捉生物医学科学生产的长尾知识的挑战。

    arXiv:2402.12352v1 Announce Type: new  Abstract: Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations. Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones. In the field of biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production. To overcome this challenge, Retrieval Augmented Generation (RAG) has been proposed to alleviate some of the shortcomings of LLMs by augmenting the prompts with context retrieved from external datasets. RAG methods typically select the conte
    
[^11]: 通过博弈论评估揭示LLM的战略推理局限性的GTBench

    GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations

    [https://arxiv.org/abs/2402.12348](https://arxiv.org/abs/2402.12348)

    该论文通过博弈论任务评估了LLMs在竞争环境中的推理能力，观察到LLMs在不同游戏场景下表现出不同行为，具有重要的战略推理局限性。

    

    随着大型语言模型（LLMs）被整合到关键的现实世界应用中，它们的战略和逻辑推理能力变得越来越关键。本文通过博弈论任务评估LLMs在竞争环境中的推理能力，例如，需要纯逻辑和战略推理来与对手竞争的棋盘游戏和纸牌游戏。我们首先提出了GTBench，这是一个以语言驱动的环境，包括10个广泛认可的任务，涵盖了全面的游戏分类法：完整信息与不完整信息，动态与静态，以及概率与确定性场景。然后，我们研究了两个关键问题：（1）表征LLMs的博弈论推理；（2）LLM对抗LLM的比赛作为推理评估。我们观察到（1）LLMs在各种游戏场景下有不同的行为；例如，LLMs在完整和确定性游戏中失败，但它们在概率游戏中具有竞争力。

    arXiv:2402.12348v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming
    
[^12]: 模拟失调: 大型语言模型的安全对齐可能会适得其反！

    Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!

    [https://arxiv.org/abs/2402.12343](https://arxiv.org/abs/2402.12343)

    安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。

    

    大型语言模型（LLMs）需要进行安全对齐，以确保与人类进行安全的对话。然而，在这项工作中，我们引入了一种推理时攻击框架，表明安全对齐也可能在对抗性操纵下无意中促成有害结果。这个框架被命名为模拟失调（ED），在输出空间中不良地组合了一对开源预训练和安全对齐的语言模型，产生了一个有害的语言模型而无需任何训练。我们对ED在三个数据集和四个模型系列（Llama-1、Llama-2、Mistral和Alpaca）上的实验表明，ED使预训练模型的有害性增加了一倍，并胜过强基线，以较大优势在48个评估子集中的43个中实现了最高的有害率。至关重要的是，我们的研究结果凸显了即使在安全对齐后，重新评估开源语言模型实践的重要性。

    arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
    
[^13]: 三重编码器：共同激活的表示一起连接

    Triple-Encoders: Representations That Fire Together, Wire Together

    [https://arxiv.org/abs/2402.12332](https://arxiv.org/abs/2402.12332)

    通过三重编码器计算话语混合，实现了对话模型的显着改进和零-shot泛化性能

    

    搜索导向的对话模型通常在每个轮次重新对对话历史进行编码，造成高昂的成本。曲率对比学习是一种最近展示出对话建模远超效率的表示学习方法，通过双编码器将话语之间的相对距离编码到嵌入空间中。高效率是通过独立编码话语实现的，然而这忽略了上下文化的重要性。为了解决这个问题，本研究引入了三重编码器，通过一种新颖的Hebbian启发的共存学习目标，从这些独立编码的话语中高效计算分布式话语混合，而不使用任何权重。实证上，我们发现三重编码器在比编码器上有显著改进，甚至比单向量表示模型实现更好的零-shot泛化，而无需重新编码。

    arXiv:2402.12332v1 Announce Type: new  Abstract: Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost. Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency. While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective without using any weights. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding. Our code/model
    
[^14]: 基于查询的对抗性提示生成

    Query-Based Adversarial Prompt Generation

    [https://arxiv.org/abs/2402.12329](https://arxiv.org/abs/2402.12329)

    该研究提出了一种基于查询的对抗性攻击方法，通过利用远程语言模型的 API 访问构造对抗性示例，使模型以更高概率发出有害字符串，而非仅仅基于模型之间的转移性攻击。

    

    最近的研究表明，可以构造对抗性示例，导致一个对其进行了调整的语言模型产生有害字符串或执行有害行为。现有的攻击要么在白盒设置中（完全访问模型权重），要么通过可转移性：一种现象，即在一个模型上精心设计的对抗性示例通常在其他模型上仍然有效。我们通过基于查询的攻击改进以前的工作，利用 API 访问远程语言模型来构造对抗性示例，使模型以（明显）更高的概率发出有害字符串，而不能仅仅使用转移攻击。我们在 GPT-3.5 和 OpenAI 的安全分类器上验证了我们的攻击；我们能够让 GPT-3.5 发出有害字符串，而目前的转移攻击失败了，并且我们几乎以 100% 的概率规避了安全分类器。

    arXiv:2402.12329v1 Announce Type: cross  Abstract: Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.
    
[^15]: 我们应该交流吗：探索竞争LLM代理之间的自发合作

    Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents

    [https://arxiv.org/abs/2402.12327](https://arxiv.org/abs/2402.12327)

    该研究揭示了LLM代理甚至在竞争环境中也能自发形成合作关系的能力，验证了计算社会科学的愿景，表明LLM代理可以用于模拟人类社会互动，包括自发合作的互动，为社会现象提供洞察。

    

    最近的进展表明，由大型语言模型（LLMs）驱动的代理具有模拟人类行为和社会动态的能力。然而，尚未研究LLM代理在没有明确指令的情况下自发建立合作关系的潜力。为了弥补这一空白，我们进行了三项案例研究，揭示了LLM代理甚至在竞争环境中也能自发形成合作关系的能力。这一发现不仅展示了LLM代理模拟人类社会中竞争与合作的能力，也验证了计算社会科学的一个有前途的愿景。具体来说，这表明LLM代理可以用于建模人类社会互动，包括那些自发合作的互动，从而提供对社会现象的洞察。这项研究的源代码可在https://github.com/wuzengqing001225/SABM_ShallWe 找到。

    arXiv:2402.12327v1 Announce Type: new  Abstract: Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics. However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWe
    
[^16]: 基于LLM的心理学智能代理：一项关于游戏化评估的研究

    LLM Agents for Psychology: A Study on Gamified Assessments

    [https://arxiv.org/abs/2402.12326](https://arxiv.org/abs/2402.12326)

    本研究提出了PsychoGAT（心理游戏代理）以实现心理评估的通用游戏化，通过将强大的LLM代理纳入角色，将标准量表转化为个性化且具有吸引力的互动小说游戏。

    

    心理测量对于精神健康、自我理解和个人发展至关重要。传统方法，如自我报告量表和心理学家访谈，常常面临参与度和可获得性方面的挑战。虽然已经探讨了基于游戏和LLM的工具来提高用户兴趣并自动化评估，但它们难以平衡参与度和普适性。在这项工作中，我们提出了PsychoGAT（心理游戏代理），以实现心理评估的通用游戏化。主要洞察是强大的LLM既可以充当熟练的心理学家，也可以是创新的游戏设计师。通过将LLM代理纳入指定角色并精心管理它们的互动，PsychoGAT可以将任何标准量表转化为个性化且具有吸引力的互动小说游戏。为验证所提出的方法，我们进行心理度量评估以评估其有效性，并使用人类

    arXiv:2402.12326v1 Announce Type: new  Abstract: Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ huma
    
[^17]: ARKS：用于代码生成中的知识汤中的活跃检索

    ARKS: Active Retrieval in Knowledge Soup for Code Generation

    [https://arxiv.org/abs/2402.12317](https://arxiv.org/abs/2402.12317)

    ARKS是一种用于代码生成的先进策略，通过活跃检索和整合各种信息源，能够提高大型语言模型的性能，为解决与频繁更新的库和长尾编程语言相关的现实编程问题提供了新的可能性。

    

    最近，检索增强生成（RAG）范式引起了人们的极大关注，因为它有潜力将外部知识整合到大型语言模型（LLMs）中，而无需进一步训练。尽管在自然语言应用中得到广泛探讨，但它在代码生成中的利用仍未得到充分探索。在本文中，我们介绍了一种名为知识汤中的活跃检索(ARKS)的先进策略，用于泛化大型语言模型以生成代码。与依靠单一来源不同，我们构建了一个将网页搜索、文档、执行反馈和进化代码片段整合在一起的知识汤。我们采用了一种积极的检索策略，该策略迭代地优化查询并更新知识汤。为了评估ARKS的性能，我们编制了一个新的基准，其中包括与频繁更新的库和长尾编程语言相关的现实编程问题。在ChatGPT和CodeLlama上的实验结果表明，ARKS比使用传统方法能够更好地产生代码。

    arXiv:2402.12317v1 Announce Type: cross  Abstract: Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training. While widely explored in natural language applications, its utilization in code generation remains under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code. In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets. We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on ChatGPT and CodeLlama de
    
[^18]: TILP：在知识图谱上学习时间逻辑规则的可微学习

    TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs

    [https://arxiv.org/abs/2402.12309](https://arxiv.org/abs/2402.12309)

    TILP提出了一种用于学习时间逻辑规则的可微框架，通过设计受限随机游走机制和引入时间操作符，有效建模了时间特征，并在两个基准数据集上展示了其优越性能。

    

    与静态知识图谱相比，能够捕捉信息随时间演变和变化的时间知识图谱（tKG）更加现实和通用。然而，由于时间概念引入到规则学习中所带来的复杂性，如准确的图推理，例如预测实体之间的新链接，仍然是一个困难的问题。本文提出了TILP，一种用于学习时间逻辑规则的可微框架。通过设计受限随机游走机制和引入时间操作符，我们确保了模型的效率。我们提出了在tKG中建模时间特征，例如复发性、时间顺序、关系对之间的间隔和持续时间，并将其纳入我们的学习过程。我们将TILP与两个基准数据集上的最先进方法进行了比较。我们表明，我们提出的框架可以改善基线方法的性能

    arXiv:2402.12309v1 Announce Type: new  Abstract: Compared with static knowledge graphs, temporal knowledge graphs (tKG), which can capture the evolution and change of information over time, are more realistic and general. However, due to the complexity that the notion of time introduces to the learning of the rules, an accurate graph reasoning, e.g., predicting new links between entities, is still a difficult problem. In this paper, we propose TILP, a differentiable framework for temporal logical rules learning. By designing a constrained random walk mechanism and the introduction of temporal operators, we ensure the efficiency of our model. We present temporal features modeling in tKG, e.g., recurrence, temporal order, interval between pair of relations, and duration, and incorporate it into our learning process. We compare TILP with state-of-the-art methods on two benchmark datasets. We show that our proposed framework can improve upon the performance of baseline methods while provid
    
[^19]: 开源到底如何了？关于商业和开源LLM在标记胸部X射线报告能力方面的比较研究

    Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports

    [https://arxiv.org/abs/2402.12298](https://arxiv.org/abs/2402.12298)

    这项研究比较了商业模型GPT-3.5 Turbo和GPT-4与开源模型Mistral-7B、Mixtral-8x7B、Llama2-13B、Llama2-70B、QWEN1.5-72B以及CheXbert和CheXpert-labeler在准确标记X射线文本报告中多发现存在的能力。

    

    随着大型语言模型（LLMs）的快速发展，涌现了许多新的开源模型和商业模型。虽然最近的出版物探讨了GPT-4在从放射学报告中提取感兴趣信息方面的应用，但尚未对GPT-4与不同领先的开源模型进行实际比较。本研究使用了两个不同的独立数据集。第一个数据集包括了2019年7月至2021年7月在马萨诸塞州综合医院创建的540份胸部X射线报告。第二个数据集包含了来自ImaGenome数据集的500份胸部X射线报告。然后，我们比较了商业模型GPT-3.5 Turbo和GPT-4与开源模型Mistral-7B、Mixtral-8x7B、Llama2-13B、Llama2-70B、QWEN1.5-72B以及CheXbert和CheXpert-labeler在准确标记X射线文本报告中多发现存在的能力。

    arXiv:2402.12298v1 Announce Type: cross  Abstract: Introduction: With the rapid advances in large language models (LLMs), there have been numerous new open source as well as commercial models. While recent publications have explored GPT-4 in its application to extracting information of interest from radiology reports, there has not been a real-world comparison of GPT-4 to different leading open-source models.   Materials and Methods: Two different and independent datasets were used. The first dataset consists of 540 chest x-ray reports that were created at the Massachusetts General Hospital between July 2019 and July 2021. The second dataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then compared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the open-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B, QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately label the presence of multiple findings in x-ray text repo
    
[^20]: KARL: 知识感知检索和表示帮助学生保持和学习

    KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students

    [https://arxiv.org/abs/2402.12291](https://arxiv.org/abs/2402.12291)

    KARL是一种基于DKT的学生模型，利用检索和BERT嵌入来实现高效准确的学生记忆预测，在AUC和校准误差方面优于现有学生模型，并提出了新颖的教学策略。

    

    Flashcard调度器是依赖于学生模型来预测学生掌握的单词卡，并使用教学策略根据这些预测安排词卡的工具。现有的学生模型仅使用单词卡级别的特征，比如学生的过去回答，忽略了单词卡之间的语义联系。深度知识跟踪（DKT）模型可以利用语言模型捕捉语义关系，但效率低下，缺乏内容丰富的数据集用于评估，并需要稳健的教学策略。为了解决这些问题，我们设计了KARL，这是受DKT启发的学生模型，利用检索和BERT嵌入以实现高效准确的学生记忆预测。为了测试KARL，我们收集了一个包含广泛学习历史关于琐事问题的新数据集。KARL在AUC和校准误差方面胜过现有的学生模型。最后，我们提出了一个新颖的教学策略，利用DKT模型的预测能力在线部署KARL。

    arXiv:2402.12291v1 Announce Type: new  Abstract: Flashcard schedulers are tools that rely on 1) student models to predict the flashcards a student knows; and 2) teaching policies to schedule cards based on these predictions. Existing student models, however, only use flashcard-level features, like the student's past responses, ignoring the semantic ties of flashcards. Deep Knowledge Tracing (DKT) models can capture semantic relations with language models, but are inefficient, lack content-rich datasets for evaluation, and require robust teaching policies. To address these issues, we design KARL, a DKT-inspired student model that uses retrieval and BERT embeddings for efficient and accurate student recall predictions. To test KARL, we collect a new dataset of diverse study history on trivia questions. KARL bests existing student models in AUC and calibration error. Finally, we propose a novel teaching policy that exploits the predictive power of DKT models to deploy KARL online. Based o
    
[^21]: 基于本体增强的主张检测

    Ontology Enhanced Claim Detection

    [https://arxiv.org/abs/2402.12282](https://arxiv.org/abs/2402.12282)

    本文提出了一种基于本体增强的模型，用于句子级主张检测，在小型不平衡数据集上取得了最佳结果，并展示了添加领域特定特征和本体嵌入有助于改善传统机器学习方法。

    

    我们提出了一种基于本体增强的模型，用于基于句子的主张检测。我们将来自知识库的本体嵌入与BERT句子嵌入融合，以执行ClaimBuster和NewsClaims数据集的主张检测。我们的本体增强方法在这些小型不平衡数据集中表现出最佳结果，与其他统计和神经机器学习模型相比。实验表明，添加领域特定特征（例如训练的词嵌入或知识图元数据）可以改善传统的机器学习方法。此外，以本体嵌入的形式添加领域知识有助于避免在基于神经网络的模型中遇到的偏见，例如我们小语料库中纯BERT模型偏向于更大类别的偏见。

    arXiv:2402.12282v1 Announce Type: new  Abstract: We propose an ontology enhanced model for sentence based claim detection. We fused ontology embeddings from a knowledge base with BERT sentence embeddings to perform claim detection for the ClaimBuster and the NewsClaims datasets. Our ontology enhanced approach showed the best results with these small-sized unbalanced datasets, compared to other statistical and neural machine learning models. The experiments demonstrate that adding domain specific features (either trained word embeddings or knowledge graph metadata) can improve traditional ML methods. In addition, adding domain knowledge in the form of ontology embeddings helps avoid the bias encountered in neural network based models, for example the pure BERT model bias towards larger classes in our small corpus.
    
[^22]: 自适应骨架图解码

    Adaptive Skeleton Graph Decoding

    [https://arxiv.org/abs/2402.12280](https://arxiv.org/abs/2402.12280)

    提出了骨架图解码（SGD）方法，利用子问题之间的依赖关系进行信息转发，改善响应质量且提高性能。

    

    大型语言模型（LLMs）已经在自然语言任务中得到广泛应用，其成功归因于大量的模型参数（例如，70亿+）；然而，LLM推断会产生巨大的计算和内存成本。最近的方法提出了并行解码策略，例如“思想骨架”（SoT），通过将提示分解为可以并行解码的子问题来改善性能；但是，它们往往在响应质量上遭受损失。我们的关键见解是，在生成子问题时，我们可以请求额外信息，特别是依赖关系和难度，以提高响应质量和性能。在本文中，我们提出了骨架图解码（SGD），利用子问题之间暴露的依赖关系，支持依赖子问题之间的信息转发，以提高质量，同时暴露独立子问题解码的并行化机会。

    arXiv:2402.12280v1 Announce Type: cross  Abstract: Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. 
    
[^23]: 有效零样本跨语言生成任务中的关键因素

    Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks

    [https://arxiv.org/abs/2402.12279](https://arxiv.org/abs/2402.12279)

    通过微调学习率可以缓解零样本跨语言生成中以错误语言生成的问题，全面微调模型是很强大的基线，mBART在这个任务中表现类似于同等大小的mT5。

    

    零样本跨语言生成意味着在一个语言上微调多语种预训练语言模型，然后将其用于在其他语言上进行此任务的预测。以前的研究指出一个经常出现的问题是以错误的语言生成，并提出了方法来解决这个问题，通常使用mT5作为主干模型。在本研究中，我们在统一的设置中比较了文献中提出的各种方法，还包括替代性的主干模型，即mBART和NLLB-200。我们首先强调了微调所使用的学习率的重要性，这有助于大大缓解以错误语言生成的问题。然后，我们表明通过细致的学习率调整，对模型进行全面微调作为非常强大的基线，替代方法只带来微小的改进。最后，我们发现mBART与同等大小的mT5表现类似。

    arXiv:2402.12279v1 Announce Type: cross  Abstract: Zero-shot cross-lingual generation implies finetuning of the multilingual pretrained language model on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size,
    
[^24]: WorldCoder，一种基于模型的LLM代理：通过编写代码和与环境交互构建世界模型

    WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment

    [https://arxiv.org/abs/2402.12275](https://arxiv.org/abs/2402.12275)

    通过编写代码和与环境交互来构建世界模型的基于模型的LLM代理在样本效率上优于深度RL，并在计算效率上优于ReAct风格的代理。

    

    我们提出了一种基于模型的代理，通过与环境的交互构建代表其对世界知识的Python程序。该世界模型试图解释其交互，同时对自己能够获得的奖励持乐观态度。我们通过扩展LLM的程序合成工作来实现这一点。我们在网格世界上研究了我们的代理，发现我们的方法在样本效率上比深度强化学习更高，并且在计算效率上比ReAct风格的代理更高效。

    arXiv:2402.12275v1 Announce Type: new  Abstract: We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.
    
[^25]: 针对严重资源匮乏语言的高质量数据文本生成与即插即用大语言模型

    High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models

    [https://arxiv.org/abs/2402.12267](https://arxiv.org/abs/2402.12267)

    大型语言模型(LLMs)在严重资源匮乏语言领域的数据文本生成中轻松刷新最新发展水平，展现出弥合性能差距的巨大潜力。

    

    NLP方法在资源丰富的语言领域的最新发展水平是无法期望在严重资源匮乏语言领域得到匹配的。我们通过爱尔兰语、威尔士语、布列塔尼语和马耳他语的数据文本生成为例，探讨预先训练的大型语言模型（LLMs）在这方面能够弥合这一差距的程度。我们在这些资源匮乏语言和英语上测试了LLMs，并在一系列场景中进行了测试。我们发现，LLMs以明显较大的优势轻松地刷新了资源匮乏语言的最新发展水平，无论是通过自动评估还是人工评估。对于我们所有的语言，人工评估显示我们最佳系统的性能与人类持平，但与英语相比，BLEU分数下降，这对于评估非任务特定系统的指标的适用性提出了疑问。总的来说，我们的结果表明LLMs在弥合资源匮乏语言性能差距方面具有巨大潜力。

    arXiv:2402.12267v1 Announce Type: new  Abstract: The performance of NLP methods for severely under-resourced languages cannot currently hope to match the state of the art in NLP methods for well resourced languages. We explore the extent to which pretrained large language models (LLMs) can bridge this gap, via the example of data-to-text generation for Irish, Welsh, Breton and Maltese. We test LLMs on these under-resourced languages and English, in a range of scenarios. We find that LLMs easily set the state of the art for the under-resourced languages by substantial margins, as measured by both automatic and human evaluations. For all our languages, human evaluation shows on-a-par performance with humans for our best systems, but BLEU scores collapse compared to English, casting doubt on the metric's suitability for evaluating non-task-specific systems. Overall, our results demonstrate the great potential of LLMs to bridge the performance gap for under-resourced languages.
    
[^26]: 使用LoRA集成在精调LLMs中的不确定性量化

    Uncertainty quantification in fine-tuned LLMs using LoRA ensembles

    [https://arxiv.org/abs/2402.12264](https://arxiv.org/abs/2402.12264)

    使用LoRA集成在精调LLMs中提出了一种原则性不确定性量化方法，通过对不同数据域的低秩适应集成分析，推测了模型对特定架构难以学习的数据领域的信号。

    

    精调大型语言模型可以提高特定任务的性能，尽管对于精调模型学到了什么、遗忘了什么以及如何信任其预测仍然缺乏一个一般的理解。我们提出了使用计算效率高的低秩适应集成对精调LLMs进行基于后验逼近的原则性不确定性量化。我们使用基于Mistral-7b的低秩适应集成分析了三个常见的多项选择数据集，并对其在精调过程中和之后对不同目标领域的感知复杂性和模型效能进行了定量和定性的结论。具体而言，基于数值实验支持，我们对那些对于给定架构难以学习的数据领域的熵不确定性度量提出了假设。

    arXiv:2402.12264v1 Announce Type: cross  Abstract: Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.
    
[^27]: NEO-BENCH：使用新词评估大型语言模型的鲁棒性

    NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms

    [https://arxiv.org/abs/2402.12261](https://arxiv.org/abs/2402.12261)

    本研究通过创建一个多样化的最新英语新词资源，分析了大型语言模型对于新词的鲁棒性表现，并构建了一个基准用于评估模型对新词的泛化能力，结果显示机器翻译中模型性能会因引入新词而几乎减半。

    

    Large Language Models (LLMs)的表现会因模型训练数据与推理过程中看到的新文本之间的时间漂移而退化。本文探讨了导致数据漂移的语言变化中一个不太被研究的方向，即随着时间推移而出现的新词形式——新词。我们通过使用几种流行的收集方法创建了一个多样化的最新英语新词资源。我们通过比较包含新词的句子与将新词替换为现有替代词的几乎相同的句子来分析新词对时间漂移的影响。在句子中引入单个新词时，机器翻译中的模型性能几乎减半。受到这些结果的启发，我们构建了一个基准来评估LLMs对不同自然语言理解任务和模型困惑度中新词的泛化能力。后期知识截止日期的模型产生较低的困惑度，并在下游任务中表现更好。

    arXiv:2402.12261v1 Announce Type: new  Abstract: The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks
    
[^28]: GPT生成文本中的知识浅层合成：自动相关工作撰写的案例研究

    Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition

    [https://arxiv.org/abs/2402.12255](https://arxiv.org/abs/2402.12255)

    GPT-4在支持人类进行头脑风暴方面表现出色，但在没有人类干预的情况下无法做到对相关作品进行详细的合成。

    

    已经开发出许多AI辅助学术应用程序来帮助研究过程的不同阶段。我们提出了对使用我们构建的工具ScholaCite生成的AI辅助学术写作进行分析。该工具旨在组织文献并撰写学术论文的相关工作部分。我们的评估方法侧重于分析引文图，以评估文本中引文的结构复杂性和互联性，并涉及对（1）原始人类撰写的文本，（2）纯粹由GPT生成的文本以及（3）人工智能与人类合作生成的文本的三方比较。我们发现，GPT-4能够生成合理的粗粒度引文分组，以支持人类用户进行头脑风暴，但在没有人类干预的情况下无法执行相关作品的详细合成。我们建议未来的写作助手工具不应独立于人类作者起草文本。

    arXiv:2402.12255v1 Announce Type: new  Abstract: Numerous AI-assisted scholarly applications have been developed to aid different stages of the research process. We present an analysis of AI-assisted scholarly writing generated with ScholaCite, a tool we built that is designed for organizing literature and composing Related Work sections for academic papers. Our evaluation method focuses on the analysis of citation graphs to assess the structural complexity and inter-connectedness of citations in texts and involves a three-way comparison between (1) original human-written texts, (2) purely GPT-generated texts, and (3) human-AI collaborative texts. We find that GPT-4 can generate reasonable coarse-grained citation groupings to support human users in brainstorming, but fails to perform detailed synthesis of related works without human intervention. We suggest that future writing assistant tools should not be used to draft text independently of the human author.
    
[^29]: Levenshtein Transformer解码器及其变体的分析

    Analysis of Levenshtein Transformer's Decoder and Its Variants

    [https://arxiv.org/abs/2402.12249](https://arxiv.org/abs/2402.12249)

    本文分析了Levenshtein Transformer（LevT）的解码器，探讨了解码结果长度、子词生成和删除模块的能力，旨在找出解码器的弱点以便未来改进。同时比较了原始LevT、知识蒸馏LevT、带有翻译记忆的LevT以及带有翻译记忆的知识蒸馏LevT的翻译结果。

    

    Levenshtein Transformer (LevT)是一种非自回归的机器翻译模型，具有高效的解码和可比的BLEU得分翻译质量，这归功于其并行解码和迭代细化的过程。本文关注LevT的解码器，分析解码结果长度、子词生成和删除模块的能力，旨在找出解码器的弱点以便未来改进。我们还比较了原始LevT、知识蒸馏LevT、带有翻译记忆的LevT以及带有翻译记忆的知识蒸馏LevT的翻译结果，以探讨知识蒸馏和翻译记忆对翻译的帮助。

    arXiv:2402.12249v1 Announce Type: new  Abstract: Levenshtein transformer (LevT) is a non-autoregressive machine translation model with high decoding efficiency and comparable translation quality in terms of bleu score, due to its parallel decoding and iterative refinement procedure. Are there any deficiencies of its translations and what improvements could be made? In this report, we focus on LevT's decoder and analyse the decoding results length, subword generation, and deletion module's capability. We hope to identify weaknesses of the decoder for future improvements.   We also compare translations of the original LevT, knowledge-distilled LevT, LevT with translation memory, and the KD-LevT with translation memory to see how KD and translation memory can help.
    
[^30]: 理解文本到SQL中噪声的影响：对BIRD-Bench基准测试的研究

    Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark

    [https://arxiv.org/abs/2402.12243](https://arxiv.org/abs/2402.12243)

    研究深入分析了文本到SQL领域中的噪声对模型的影响，并发现在BIRD-Bench基准测试中存在大量问题和标准查询中的噪声，这会显著影响模型的性能。

    

    Text-to-SQL涉及将自然语言翻译为结构化查询语言（SQL），对于使结构化数据库可以在没有专业知识的情况下得到广泛访问至关重要。然而，设计针对这些任务的模型是具有挑战性的，原因包括存在“噪声”，如模糊问题和语法错误。该研究对广泛使用的BIRD-Bench基准测试中噪声的分布和类型以及噪声对模型的影响进行了深入分析。虽然BIRD-Bench旨在模拟脏乱和嘈杂的数据库值，但并未包含问题和标准查询中的噪声和错误。我们发现数据集中问题和标准查询中的噪声普遍存在，跨领域存在不同程度的噪声，并且噪声类型之间分布不均匀。存在不正确的标准SQL查询，进而生成不正确的标准答案，对基准测试的影响显著。

    arXiv:2402.12243v1 Announce Type: new  Abstract: Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of 'noise,' such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRD-Bench benchmark and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the ben
    
[^31]: 具有上下文学习的面向任务型对话系统

    Task-Oriented Dialogue with In-Context Learning

    [https://arxiv.org/abs/2402.12234](https://arxiv.org/abs/2402.12234)

    该论文描述了一个结合大型语言模型的上下文学习和确定性执行业务逻辑的任务型对话系统构建系统的方法，实验显示相比于传统方法，使用该系统开发聊天机器人需要的工作量明显减少，并且具有处理复杂对话和扩展任务型对话系统到大量任务的优势。

    

    我们描述了一个结合大型语言模型（LLM）的上下文学习能力和确定性执行业务逻辑的任务型对话系统构建系统。LLM用于在表面形式的对话和特定领域语言（DSL）之间进行翻译，后者用于推进业务逻辑。我们将我们的方法与当前工业中主要使用的基于意图的自然语言理解（NLU）方法进行比较。我们的实验表明，使用我们的系统开发聊天机器人需要的工作量明显少于已建立的方法，这些聊天机器人能够成功地处理对基于NLU系统极具挑战性的复杂对话，并且我们的系统具有可扩展任务型对话系统到大量任务的理想性能。我们提供我们的实现供使用和进一步研究。

    arXiv:2402.12234v1 Announce Type: new  Abstract: We describe a system for building task-oriented dialogue systems combining the in-context learning abilities of large language models (LLMs) with the deterministic execution of business logic. LLMs are used to translate between the surface form of the conversation and a domain-specific language (DSL) which is used to progress the business logic. We compare our approach to the intent-based NLU approach predominantly used in industry today. Our experiments show that developing chatbots with our system requires significantly less effort than established approaches, that these chatbots can successfully navigate complex dialogues which are extremely challenging for NLU-based systems, and that our system has desirable properties for scaling task-oriented dialogue systems to a large number of tasks. We make our implementation available for use and further study.
    
[^32]: 论文标题：转换器前馈层中关键-值记忆更新的实证研究

    Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers

    [https://arxiv.org/abs/2402.12233](https://arxiv.org/abs/2402.12233)

    通过对大型语言模型的知识编辑和微调任务进行比较，进一步理解了在转换器前馈层中更新关键或值的不同方法。

    

    摘要：变压器中的前馈网络（FFNs）被认为是一组关键-值神经记忆，用于恢复抽象的高层知识。在这项工作中，我们对更新键（FFNs层中的第一层）或值（FFNs层中的第二层）进行了实证消融研究。我们将在大型语言模型的各种知识编辑和微调任务中比较这两种方法，以获得更深入地理解FFNs的见解。代码可在 $\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{此存储库}$ 中找到。

    arXiv:2402.12233v1 Announce Type: new  Abstract: The feed-forward networks (FFNs) in transformers are recognized as a group of key-value neural memories to restore abstract high-level knowledge. In this work, we conduct an empirical ablation study on updating keys (the 1st layer in the FFNs layer) or values (the 2nd layer in the FFNs layer). We compare those two methods in various knowledge editing and fine-tuning tasks of large language models to draw insights to understand FFNs further. Code is available at $\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this\,repo}$.
    
[^33]: AnyGPT：统一的多模式离散序列建模语言模型

    AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling

    [https://arxiv.org/abs/2402.12226](https://arxiv.org/abs/2402.12226)

    AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。

    

    我们介绍了 AnyGPT，这是一个任意多模式语言模型，利用离散表示统一处理各种模态，包括语音、文本、图像和音乐。AnyGPT 可以稳定训练，无需对当前大型语言模型（LLM）架构或训练范式进行任何改动。相反，它仅依赖于数据级预处理，促进了新模态的无缝集成到LLM中，类似于新语言的整合。我们构建了一个多模式文本中心的数据集，用于多模式对齐预训练。利用生成模型，我们合成了第一个大规模任意多模式指令数据集。它包括108k个多轮对话示例，精细地交织各种模态，从而使模型能够处理多模态输入和输出的任意组合。实验结果表明，AnyGPT能够促进...

    arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
    
[^34]: 使用基于覆盖引导的强化学习对LLM基础变异进行JavaScript引擎模糊测试

    CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation

    [https://arxiv.org/abs/2402.12222](https://arxiv.org/abs/2402.12222)

    CovRL使用强化学习结合大型语言模型和覆盖反馈，通过构建加权覆盖映射并应用于基于LLM的变异器，以提升对JavaScript引擎的模糊测试效果

    

    模糊测试是一种有效的发现错误的技术，但在像需要精确语法输入的JavaScript引擎这样的复杂系统中很难应用。最近，研究人员采用语言模型对模糊测试中的上下文感知变异进行了改进以解决这一问题。然而，现有技术在利用覆盖引导进行模糊测试方面存在局限性，通常以黑盒方式执行。本文提出了一种名为CovRL（基于覆盖引导的强化学习）的新技术，将大型语言模型（LLMs）与从覆盖反馈中得到的强化学习相结合。我们的模糊器CovRL-Fuzz通过利用词频-逆文档频率（TF-IDF）方法将覆盖反馈直接集成到LLM中，构建加权覆盖映射。该映射在计算模糊测试奖励中起着关键作用，然后通过强化学习应用于基于LLM的变异器。通过这种方法，CovRL-Fuzz实现了...

    arXiv:2402.12222v1 Announce Type: cross  Abstract: Fuzzing is an effective bug-finding technique but it struggles with complex systems like JavaScript engines that demand precise grammatical input. Recently, researchers have adopted language models for context-aware mutation in fuzzing to address this problem. However, existing techniques are limited in utilizing coverage guidance for fuzzing, which is rather performed in a black-box manner. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with reinforcement learning from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the g
    
[^35]: 重新格式化对齐

    Reformatted Alignment

    [https://arxiv.org/abs/2402.12219](https://arxiv.org/abs/2402.12219)

    本文提出了一种名为ReAlign的简单有效方法，通过重新格式化指导数据的响应，显著提升了大型语言模型（LLMs）与人类价值观的对齐能力。

    

    优化微调数据对于将大型语言模型（LLMs）与人类价值观对齐至关重要。当前改善数据质量的方法要么耗时费力，要么容易受到LLM幻觉引起的事实错误影响。本文探讨提升现有指导数据质量以更好地与人类价值观对齐的方法，引入了一种名为ReAlign的简单有效方法，它将指导数据的响应重新格式化为更符合预先建立标准和编译证据的格式。该方法最小化了人类注释、幻觉和扩展困难，与现有对齐技术正交。实验结果表明，ReAlign显著提升了LLMs的整体对齐能力、数学推理、事实性和可读性。令人鼓舞的是，在不引入任何额外数据或先进训练技术的情况下，仅通过重新格式化响应，LLaMA-2-13

    arXiv:2402.12219v1 Announce Type: cross  Abstract: The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13
    
[^36]: 自主生成AI代理在回声室中的极化

    Polarization of Autonomous Generative AI Agents Under Echo Chambers

    [https://arxiv.org/abs/2402.12212](https://arxiv.org/abs/2402.12212)

    在回声室环境中，基于ChatGPT的自主生成AI代理往往会发生极化，这对于了解人工智能代理在社交网络中的行为具有重要意义

    

    在线社交网络通常会产生回声室，使人们只听到与其信念相符的观点。回音室经常会产生极化，导致由持有激进观点的人引起的冲突，比如2021年1月6日对美国国会大厦的袭击。回音室被视为一种人类特有的问题，但随着诸如ChatGPT之类的大型语言模型获得社交能力，这种隐含的假设变得不太合理。针对这种情况，我们研究了基于生成语言模型的一组自主AI代理在回声室环境中发生极化的潜在可能性。我们让AI代理讨论特定主题，并分析了随着讨论的进行，群体意见如何变化。结果表明，基于ChatGPT的一组代理在回声室环境中往往会变得极化。意见转变的分析显示了这一结果

    arXiv:2402.12212v1 Announce Type: new  Abstract: Online social networks often create echo chambers where people only hear opinions reinforcing their beliefs. An echo chamber often generates polarization, leading to conflicts caused by people with radical opinions, such as the January 6, 2021, attack on the US Capitol. The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as large language models, such as ChatGPT, acquire social abilities. In response to this situation, we investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment. We had AI agents discuss specific topics and analyzed how the group's opinions changed as the discussion progressed. As a result, we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments. The analysis of opinion transitions shows that this result is
    
[^37]: 通过从资源丰富的语言进行自蒸馏来增强大型语言模型的多语言能力

    Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages

    [https://arxiv.org/abs/2402.12204](https://arxiv.org/abs/2402.12204)

    通过自蒸馏从资源丰富的语言进行方法，提高了大型语言模型在多语言任务上的性能。

    

    虽然大型语言模型（LLMs）已经在多语言语料库上进行了预训练，但在大多数语言中，它们的性能仍然落后于少数资源丰富的语言。一个常见的方法是将来自资源丰富语言的训练数据翻译成其他语言，然后继续训练，但是仅依赖翻译获得的数据，而忽略了LLMs跨语言的原始能力，不总是有效的，我们展示将限制跨语言知识转移的性能。在这项工作中，我们提出了SDRRL，一种基于从资源丰富的语言进行自蒸馏的方法，通过利用LLMs在资源丰富语言上的内在能力，有效地提高多语言性能。我们在不同的LLMs（LLaMA-2和SeaLLM）和源语言上评估各种理解和生成任务，实验结果表明SDRRL

    arXiv:2402.12204v1 Announce Type: new  Abstract: While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on Self-Distillation from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages across various comprehension and generation tasks, experimental results demonstrate that SDRRL
    
[^38]: 零-shot 可见语言模型用于仇恨模因检测：我们已经到达目标了吗？

    Zero shot VLMs for hate meme detection: Are we there yet?

    [https://arxiv.org/abs/2402.12198](https://arxiv.org/abs/2402.12198)

    本研究探讨了零-shot分类在处理复杂任务如恶意模因检测中的有效性

    

    社交媒体上的多媒体内容正在迅速发展，其中模因作为一种独特形式变得日益重要。不幸的是，一些恶意用户利用模因针对个人或易受攻击的社区，因此有必要识别和解决此类恶意模因。已经进行了大量研究来解决这个问题，通过开发仇恨模因检测模型。然而，传统的机器学习/深度学习模型的一个显著局限性是需要带标签的数据集才能进行准确分类。最近，研究界见证了几种可见语言模型的出现，在各种任务中展现出卓越的性能。在这项研究中，我们旨在调查这些可见语言模型在处理诸如仇恨模因检测等复杂任务中的有效性。我们使用各种提示设置来专注于对恶意/有害模因的零-shot 分类。通过我们的分析，我们o

    arXiv:2402.12198v1 Announce Type: new  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we o
    
[^39]: 通过 prior-LLM 上下文融合来理解多模态内容

    Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion

    [https://arxiv.org/abs/2402.12195](https://arxiv.org/abs/2402.12195)

    提出了一个两阶段范式"浏览和集中"，通过在将特征输入LLMs之前进行深入的多模态上下文融合，解决了多模态内容理解中的 prior-LLM 模态隔离问题

    

    随着大型语言模型（LLMs）的兴起，近期将LLMs与预训练的视觉模型相结合的多模态大型语言模型（MLLMs）已经展现出在各种视觉语言任务上令人印象深刻的性能。然而，它们在理解涉及多张图片的上下文方面仍有不足。这一缺陷的主要原因是，在将视觉特征输入LLM主干之前，每张图片的视觉特征都是由冻结的编码器单独编码的，缺乏对其他图片和多模态指令的意识。我们将这一问题称为 prior-LLM 模态隔离，并提出了一个两阶段范式，即“浏览和集中”，以实现在将特征输入LLMs之前进行深入的多模态上下文融合。这种范式最初“浏览”输入以获取关键见解，然后再次回顾输入“集中”于关键细节，通过这些见解的指导，从而实现对多模态内容更全面的理解。

    arXiv:2402.12195v1 Announce Type: new  Abstract: With the bloom of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) that incorporate LLMs with pre-trained vision models have recently demonstrated impressive performance across diverse vision-language tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth multimodal context fusion prior to feeding the features into LLMs. This paradigm initially "browses" through the inputs for essential insights, and then revisits the inputs to "concentrate" on crucial details, guided by these insights, to achieve a more comprehensive understanding of the
    
[^40]: 用于评估大型语言模型中安全机制的中文数据集

    A Chinese Dataset for Evaluating the Safeguards in Large Language Models

    [https://arxiv.org/abs/2402.12193](https://arxiv.org/abs/2402.12193)

    该研究介绍了一个用于评估中文LLMs安全性的数据集，提出了细粒度的安全评估标准，以及扩展了两种场景用于识别有风险提示拒绝的虚阔负面和错误肯定示例。

    

    许多研究已经证明大型语言模型（LLMs）可能产生有害响应，在LLMs部署时使用户面临意外风险。先前的研究提出了关于LLMs引发风险的综合分类法，以及相应的提示，可用于检查LLMs的安全机制。然而，这些研究几乎完全集中在英语上，其他语言的研究较少。在本文中，我们旨在弥补这一空白。我们首先介绍了一个用于评估中文LLMs安全性的数据集，然后将其扩展到另外两种情景，可用于更好地识别关于有风险提示拒绝的虚阔负面和错误肯定示例。我们进一步针对每种风险类型提出一组细粒度的安全评估标准，促进人工标注和自动评估LLM响应有害性。我们对五个LLM的实验表明，特定于地区的风险...

    arXiv:2402.12193v1 Announce Type: new  Abstract: Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks a
    
[^41]: 通过使用伪标签成员资格进行微调来增强训练数据曝光

    Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships

    [https://arxiv.org/abs/2402.12189](https://arxiv.org/abs/2402.12189)

    攻击者通过对预训练LM进行对抗微调，以放大原始训练数据的曝光，采用伪标签和机器生成概率来加强LM对预训练数据的保留。

    

    神经语言模型(LMs)由于数据记忆而容易受到训练数据提取攻击的影响。本文介绍了一种新的攻击场景，在这种场景中，攻击者对预训练LM进行对抗微调，以放大原始训练数据的曝光。该策略不同于先前的研究，其目的是加强LM对其预训练数据集的保留。为了实现这一目标，攻击者需要收集与预训练数据密切相关的生成文本。然而，如果没有实际数据集的知识，衡量生成文本中预训练数据的量是具有挑战性的。为了解决这个问题，我们提出利用目标LM的机器生成概率所表示的成员近似值为这些生成文本使用伪标签。随后，我们微调LM以支持那些更有可能源自预训练数据的生成文本，根据其成员资格。

    arXiv:2402.12189v1 Announce Type: new  Abstract: Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their memb
    
[^42]: Mafin: 用模型增强微调来增强黑盒嵌入

    Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning

    [https://arxiv.org/abs/2402.12177](https://arxiv.org/abs/2402.12177)

    Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。

    

    检索增强生成（RAG）已经成为缓解大型语言模型（LLMs）中幻觉的有效解决方案。RAG中的检索阶段通常涉及预训练的嵌入模型，将查询和段落转换为向量以捕获它们的语义。然而，当应用于特定领域知识时，标准的预训练嵌入模型可能表现出次优性能，需要进行微调。本文解决了仅能从黑盒模型获取嵌入的情况。我们引入了模型增强微调（Mafin）--一种通过用可训练的嵌入模型增强黑盒嵌入模型来进行微调的新方法。我们的结果表明，Mafin仅需要训练一个小的增强模型就可以显著提高黑盒嵌入的性能。我们验证了我们的方法在有标签和无标签数据集上的有效性。

    arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
    
[^43]: BIDER: 通过关键支持证据弥合有效检索增强的LLMs中的知识不一致性

    BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence

    [https://arxiv.org/abs/2402.12174](https://arxiv.org/abs/2402.12174)

    BIDER通过知识综合和偏好对齐，将检索文档转化为关键支持证据，从而提高了LLMs的答案质量并减少了输入内容长度。

    

    大规模语言模型（LLMs）在开放领域QA等知识密集型任务中表现出有效性，解决了知识更新和事实不足等固有挑战。然而，检索知识与LLMs所需知识之间的不一致性导致了LLMs答案质量下降。本文介绍了BIDER，一种通过知识综合、监督微调（SFT）和偏好对齐，将检索文档细化为关键支持证据（KSE）的方法。我们通过学习制定KSE来训练BIDER，同时通过强化学习将其输出最大化，以使其与LLMs的信息获取偏好保持一致。在五个数据集上的评估结果显示，BIDER提高了LLMs答案质量7％，同时将检索文档的输入内容长度减少了80％，优于现有方法。所提出的KSE模拟有效地为LLMs提供了必要的信息。

    arXiv:2402.12174v1 Announce Type: new  Abstract: Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy. However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM's information acquisition preferences through reinforcement learning. Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE simulation effectively equips LLMs with essential informatio
    
[^44]: 无监督LLM适应问答任务

    Unsupervised LLM Adaptation for Question Answering

    [https://arxiv.org/abs/2402.12170](https://arxiv.org/abs/2402.12170)

    提出了无监督LLM适应问答任务，通过利用预训练的LLM和目标领域的未标记文档，实现在新领域回答问题的目标。

    

    大型语言模型（LLM）通过自监督训练学习大规模训练数据集中的多样化知识。接着通过指导微调，LLM能够返回多样问题的正确信息。然而，将这些预训练的LLM调整到新的目标领域，如不同组织或时期，用于问答任务会产生很高的注释成本。为解决这一挑战，我们提出了一个新颖的任务，即无监督LLM适应问答任务。在这个任务中，我们利用预训练的LLM、一个公开可用的问答数据集（源数据）和目标域的未标记文档。我们的目标是学习LLM，使其能够回答关于目标领域的问题。我们引入了一个合成数据集和两个真实数据集来评估在源数据和目标数据上微调的模型，并揭示了一些有趣的见解；（i）微调模型展示了提供正确答案的能力

    arXiv:2402.12170v1 Announce Type: cross  Abstract: Large language models (LLM) learn diverse knowledge present in the large-scale training dataset via self-supervised training. Followed by instruction-tuning, LLM acquires the ability to return correct information for diverse questions. However, adapting these pre-trained LLMs to new target domains, such as different organizations or periods, for the question-answering (QA) task incurs a substantial annotation cost. To tackle this challenge, we propose a novel task, unsupervised LLM adaptation for question answering. In this task, we leverage a pre-trained LLM, a publicly available QA dataset (source data), and unlabeled documents from the target domain. Our goal is to learn LLM that can answer questions about the target domain. We introduce one synthetic and two real datasets to evaluate models fine-tuned on the source and target data, and reveal intriguing insights; (i) fine-tuned models exhibit the ability to provide correct answers 
    
[^45]: 基于Transformer的因果语言模型执行聚类

    Transformer-based Causal Language Models Perform Clustering

    [https://arxiv.org/abs/2402.12151](https://arxiv.org/abs/2402.12151)

    Transformer-based因果语言模型通过在隐藏空间内对数据进行聚类来学习任务特定信息，这种聚类过程在学习中动态演变，并有助于处理未见实例。

    

    即使大型语言模型(LLMs)已经展示出在解决各种自然语言任务方面的出色能力，LLM遵循人类指令的能力仍然是一个问题。最近的研究通过额外训练指令遵循任务已经显示出很大改进，然而，导致有效指令遵循能力的机制仍未得到充分理解。本文介绍了一个简化的指令遵循任务，并使用合成数据集分析了基于Transformer的因果语言模型。我们的发现表明，模型通过在其隐藏空间内对数据进行聚类而学习任务特定信息，这种聚类过程在学习过程中动态演变。我们还演示了这种现象如何帮助模型处理未见实例，并在更现实的环境中验证了我们的结果。

    arXiv:2402.12151v1 Announce Type: cross  Abstract: Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting.
    
[^46]: 您的大型语言模型暗中支持公平，您应该像对待一个公平者那样提示它

    Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One

    [https://arxiv.org/abs/2402.12150](https://arxiv.org/abs/2402.12150)

    提出了一种通过提示大型语言模型（LLMs）具有特定角色以表达多样观点的方法，并开发了FairThinking流水线，以实现公平表达。

    

    大规模语言模型（LLMs）的广泛应用凸显了确保其公平性的迫切需要。然而，LLMs经常展示支配性观点，同时忽视来自少数派的替代观点，可能导致潜在偏见。我们假设这些违反公平的行为发生是因为LLMs使用代表训练数据大多数的人类个性来表达他们的观点。作为回应，我们验证提示LLMs使用特定角色可以使LLMs表达多样观点。基于这一洞察和观察，我们开发了FairThinking，这是一个旨在自动生成能让LLMs表达多样观点以实现公平表达的流水线。为了评估FairThinking，我们创建了一个包含三个与公平相关主题的一千个项目的数据集，并在GPT-3.5，GPT-4，Llama2和Mistral上进行实验，以展示其优越性能。

    arXiv:2402.12150v1 Announce Type: cross  Abstract: The widespread adoption of large language models (LLMs) underscores the urgent need to ensure their fairness. However, LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases. We hypothesize that these fairness-violating behaviors occur because LLMs express their viewpoints using a human personality that represents the majority of training data. In response to this, we validate that prompting LLMs with specific roles can allow LLMs to express diverse viewpoints. Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable LLMs to articulate diverse perspectives for fair expressions. To evaluate FairThinking, we create a dataset with a thousand items covering three fairness-related topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to demonstrate its superior performanc
    
[^47]: 跨语言规模的端到端事实核查

    End-to-end multilingual fact-checking at scale

    [https://arxiv.org/abs/2402.12147](https://arxiv.org/abs/2402.12147)

    使用Factiverse AI模型，可以进行跨语言的端到端事实核查，并且通过实验证明，为事实核查任务进行微调的模型优于大型语言模型。

    

    在本文中，我们描述了如何使用Factiverse AI模型在100多种语言中进行端到端事实核查。我们还通过实验性基准测试展示，为事实核查任务进行微调的模型胜过GPT-4、GPT-3.5-Turbo和Mistral-7b等大型语言模型。

    arXiv:2402.12147v1 Announce Type: cross  Abstract: In this article, we describe how you can perform end-to-end fact-checking in over 100 languages using Factiverse AI models. We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.
    
[^48]: Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    [https://arxiv.org/abs/2402.12146](https://arxiv.org/abs/2402.12146)

    Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。

    

    尽管大型语言模型（LLMs）在广泛任务中展现强大性能，但仍面临幻觉等可靠性挑战。先前的研究表明，像GPT-4这样高能力的LLMs在评估单个响应的可靠性方面是有效的，而较不具备能力的LLMs通常被调整来评估对相同查询的响应的相对可靠性。为了使较不具备能力的LLMs有效地评估单个响应的可靠性，我们提出了一种名为$\textit{Meta}$ $\textit{Ranking}$（MR）的新方法。与先前直接评估响应的方法不同，我们通过将目标查询-响应对与参考查询-响应对进行比较来实现判断。我们发现在推理任务的LLM响应的错误检测中，MR表现出显著的有效性，即使在没有微调的情况下，较不具备能力的LLMs也能胜过强基线。我们进一步证明MR可以被用

    arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
    
[^49]: 评估视觉语言模型的图像评价能力

    Evaluating Image Review Ability of Vision Language Models

    [https://arxiv.org/abs/2402.12121](https://arxiv.org/abs/2402.12121)

    本论文通过引入基于排名相关分析的评估方法，探讨了大规模视觉语言模型（LVLM）在生成图像评价文本方面的能力，并创建了一个评估数据集来验证这种方法。

    

    大规模视觉语言模型（LVLM）是能够通过单个模型处理图像和文本输入的语言模型。本文探讨了使用LVLM生成图像评价文本的方法。LVLM对图像的评价能力尚未完全被理解，突显了对其评价能力进行系统评估的必要性。与图像标题不同，评价文本可以从图像构图和曝光等不同视角撰写。这种评价角度的多样性使得难以唯一确定图像的正确评价。为了解决这一挑战，我们提出了一种基于排名相关分析的评估方法，通过人类和LVLM对评价文本进行排名，然后测量这些排名之间的相关性。我们进一步通过创建一个旨在评估最新LVLM图像评价能力的基准数据集来验证这种方法。

    arXiv:2402.12121v1 Announce Type: cross  Abstract: Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings. We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset
    
[^50]: 在预训练过程中移除异常值是否有其益处？

    Is It a Free Lunch for Removing Outliers during Pretraining?

    [https://arxiv.org/abs/2402.12102](https://arxiv.org/abs/2402.12102)

    通过确保归一化对序列长度不变，我们改进了一种预训练方法，使其在移除异常值的同时促进了因果语言模型的成功预训练。

    

    随着大型语言模型的规模不断增长，量化的作用变得越来越重要。然而，在权重或激活中存在的异常值明显影响了量化模型的性能。最近，qtransformer 提出了一种旨在以无异常值方式预训练模型的新型 softmax 函数，从而提高了它们适用于量化的性能。有趣的是，我们观察到这种方法导致了全精度性能的下降。基于这一观察，我们通过确保其归一化对序列长度不变来增强该方法，这是在预训练和微调之间弥合差距的关键因素。此外，这种改进的方法还促进了因果语言模型的成功预训练。

    arXiv:2402.12102v1 Announce Type: cross  Abstract: With the growing size of large language models, the role of quantization becomes increasingly significant. However, outliers present in weights or activations notably influence the performance of quantized models. Recently, \citet{qtransformer} introduced a novel softmax function aimed at pretraining models in an outlier-free manner, thereby enhancing their suitability for quantization. Interestingly, we observed that such an approach leads to performance degradation in full precision. Building on this insight, we enhance the method by ensuring its normalization is invariant to sequence length, a crucial factor for bridging the gap between pretraining and fine-tuning. Moreover, this improved method also facilitates successful pretraining of causal language models.
    
[^51]: Groot: 使用基于树的语义转换对生成式文本到图像模型进行对抗测试

    Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation

    [https://arxiv.org/abs/2402.12100](https://arxiv.org/abs/2402.12100)

    Groot是第一个利用基于树的语义转换进行对抗测试文本到图像模型的自动化框架，成功率高达93.66%。

    

    随着文本到图像生成模型的普及，其安全性变得至关重要。对抗性测试技术已被开发用于探测这类模型是否能被激发产生不安全内容。然而，现有的解决方案面临着低成功率和低效率等挑战。我们引入了 Groot，这是第一个利用基于树的语义转换进行文本到图像模型对抗测试的自动化框架。Groot 结合语义分解和敏感元素淹没策略，结合 LLMs 来系统地优化对抗性提示。我们的全面评估验证了 Groot 的有效性，它不仅超越了当前最先进方法的性能，而且在领先的文本到图像模型，如 DALL-E 3 和 Midjourney 上取得了显著的成功率（93.66%）。

    arXiv:2402.12100v1 Announce Type: cross  Abstract: With the prevalence of text-to-image generative models, their safety becomes a critical concern. adversarial testing techniques have been developed to probe whether such models can be prompted to produce Not-Safe-For-Work (NSFW) content. However, existing solutions face several challenges, including low success rate and inefficiency. We introduce Groot, the first automated framework leveraging tree-based semantic transformation for adversarial testing of text-to-image models. Groot employs semantic decomposition and sensitive element drowning strategies in conjunction with LLMs to systematically refine adversarial prompts. Our comprehensive evaluation confirms the efficacy of Groot, which not only exceeds the performance of current state-of-the-art approaches but also achieves a remarkable success rate (93.66%) on leading text-to-image models such as DALL-E 3 and Midjourney.
    
[^52]: 大型语言模型真正理解逻辑还是仅仅模仿语境？

    Do Large Language Models Understand Logic or Just Mimick Context?

    [https://arxiv.org/abs/2402.12091](https://arxiv.org/abs/2402.12091)

    大型语言模型在逻辑推理中并不真正理解逻辑规则，而是通过语境学习增强了模型到达结论的可能性

    

    在过去几年中，大型语言模型（LLMs）的能力受到了广泛关注，它们在复杂场景（如逻辑推理和符号推理）中表现出色。其中一个重要因素是在语境学习和少样本提示的益处。然而，使用语境推理的这种模型成功背后的原因尚未被充分探讨。LLMs是否了解逻辑规则以进行推理，还是通过学习一种概率映射通过语境“猜测”答案？本文通过使用反事实方法在两个逻辑推理数据集上研究了LLMs的推理能力，替换语境文本并修改逻辑概念。根据我们的分析，发现LLMs并不真正理解逻辑规则；相反，在语境学习简单增强了这些模型到达

    arXiv:2402.12091v1 Announce Type: cross  Abstract: Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they ``guess'' the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving a
    
[^53]: LLM能否用理由进行计算？

    Can LLMs Compute with Reasons?

    [https://arxiv.org/abs/2402.12080](https://arxiv.org/abs/2402.12080)

    提出一种“归纳学习”方法，利用分布式的SLM网络来提升SLM的推理能力，桥梁人类与LLM之间的逻辑差距。

    

    大型语言模型（LLMs）经常在处理复杂数学任务时遇到困难，容易因依赖统计模式而“产生幻觉”，给出错误答案。这一局限在平均上下文和训练数据有限的小型语言模型（SLMs）中进一步放大。为了解决这一挑战，我们提出了一种“归纳学习”方法，利用分布式的SLM网络。该网络利用基于错误的学习和提示融合来提升SLM的推理能力。我们的目标是提供一个框架，赋予SLM接近高参数模型所实现的基于逻辑的应用水平的能力，潜在地使任何语言模型受益。最终，这一新颖概念为跨各领域中人类与LLM之间逻辑差距的弥合铺平了道路。

    arXiv:2402.12080v1 Announce Type: new  Abstract: Large language models (LLMs) often struggle with complex mathematical tasks, prone to "hallucinating" incorrect answers due to their reliance on statistical patterns. This limitation is further amplified in average Small LangSLMs with limited context and training data. To address this challenge, we propose an "Inductive Learning" approach utilizing a distributed network of SLMs. This network leverages error-based learning and hint incorporation to refine the reasoning capabilities of SLMs. Our goal is to provide a framework that empowers SLMs to approach the level of logic-based applications achieved by high-parameter models, potentially benefiting any language model. Ultimately, this novel concept paves the way for bridging the logical gap between humans and LLMs across various fields.
    
[^54]: LVCHAT: 促进长视频理解

    LVCHAT: Facilitating Long Video Comprehension

    [https://arxiv.org/abs/2402.12079](https://arxiv.org/abs/2402.12079)

    LVChat 提出了 Frame-Scalable 编码和 Interleaved Frame Encoding（IFE）来解决长视频理解中的问题。

    

    启用大型语言模型（LLMs）读取视频对于多模式LLMs至关重要。现有作品展示了对短视频的希望，而长视频（超过1分钟）的理解仍然具有挑战性。主要问题在于视频的过度压缩，即编码视频表示不足以代表整个视频。为了解决这个问题，我们提出了Long Video Chat（LVChat），其中引入了Frame-Scalable编码（FSE），以动态调整嵌入数量，与视频持续时间对齐，确保长视频不会被过度压缩为少数嵌入。为了处理长度超出训练过程中所见视频的长视频，我们提出了Interleaved Frame Encoding（IFE），重复位置嵌入和交错多组视频，以实现长视频输入，避免因视频过长而导致性能降低。实验结果表明LVChat sig

    arXiv:2402.12079v1 Announce Type: cross  Abstract: Enabling large language models (LLMs) to read videos is vital for multimodal LLMs. Existing works show promise on short videos whereas long video (longer than e.g.~1 minute) comprehension remains challenging. The major problem lies in the over-compression of videos, i.e., the encoded video representations are not enough to represent the whole video. To address this issue, we propose Long Video Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced to dynamically adjust the number of embeddings in alignment with the duration of the video to ensure long videos are not overly compressed into a few embeddings. To deal with long videos whose length is beyond videos seen during training, we propose Interleaved Frame Encoding (IFE), repeating positional embedding and interleaving multiple groups of videos to enable long video input, avoiding performance degradation due to overly long videos. Experimental results show that LVChat sig
    
[^55]: EmoBench: 评估大型语言模型的情感智能

    EmoBench: Evaluating the Emotional Intelligence of Large Language Models

    [https://arxiv.org/abs/2402.12071](https://arxiv.org/abs/2402.12071)

    EmoBench是一个基于心理学理论的基准测试，旨在评估大型语言模型的情感智能，包括情感理解和情感应用。

    

    近年来，大型语言模型（LLMs）的快速发展凸显了需要稳健、全面和具有挑战性的基准测试的重要性。然而，对它们的情感智能（EI）进行评估的研究相当有限。现有的基准测试存在两个主要缺点：首先，它们主要关注情感识别，忽视了情感调节等重要的情感智能能力，而情感理解则促进情感; 其次，它们主要基于现有数据集构建，这些数据集包含频繁模式、明确信息和注释错误，导致评估不可靠。我们提出了EmoBench，这是一个基准测试，借鉴了已建立的心理理论，并为机器EI提出了综合定义，包括情感理解和情感应用。EmoBench包括一组400个用英语和中文手工制作的问题，经过精心设计，需要深入推理。

    arXiv:2402.12071v1 Announce Type: cross  Abstract: Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning
    
[^56]: WKVQuant：量化大型语言模型的参数权重和键值缓存以提高性能

    WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More

    [https://arxiv.org/abs/2402.12065](https://arxiv.org/abs/2402.12065)

    该论文提出了WKVQuant，一种专为大型语言模型设计的量化框架，通过量化权重和键值缓存来改善性能。

    

    大型语言模型（LLMs）面临着部署挑战，主要是由于其巨大的内存需求和自回归文本生成过程的计算需求。本文通过关注LLMs的量化来解决这些挑战，量化是一种通过将模型参数和激活转换为低比特整数来减少内存消耗的技术。我们批判性地分析了现有的量化方法，识别出它们在平衡量化LLMs的准确性和效率方面的局限性。为了超越这些局限性，我们提出了WKVQuant，这是一个专为量化LLMs的参数权重和键值（KV）缓存而设计的PTQ框架。具体而言，我们引入了仅考虑过去的量化以改善注意力计算。此外，我们还介绍了二维量化策略来处理KV缓存的分布，以及一种跨块重建正则化方法以帮助模型压缩。

    arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa
    
[^57]: 所有语言模型的大小都一样吗？

    All Language Models Large and Small

    [https://arxiv.org/abs/2402.12061](https://arxiv.org/abs/2402.12061)

    LONDI框架可以在需要复杂决策和推理的地方选择性地使用大的语言模型，极大地降低了资源消耗。

    

    许多领先的语言模型（LMs）在训练和执行过程中使用高强度计算资源，这对于降低部署资源成本和更快执行决策任务等方面提出了挑战。我们引入了一种名为语言优化网络分布（LONDI）框架的新型即插即用LM框架。 LONDI学会了在需要进行复杂决策和推理的地方选择性地使用大的LM，而在其他地方使用低资源的LM。 LONDI由两个（离线）策略网络系统、一个LM、一个大的LM（LLM)和一个使用开关控制快速学习何时调用LLM的强化学习模块组成。 然后，我们介绍了一种在LLM调用和资源使用方面保持预算约束的LONDI变体。 从理论上讲，我们证明了LONDI学习激活所需解决任务的LLM的系统状态子集。

    arXiv:2402.12061v1 Announce Type: cross  Abstract: Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove
    
[^58]: 通过搭建坐标来促进大型多模型的视觉-语言协调

    Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models

    [https://arxiv.org/abs/2402.12058](https://arxiv.org/abs/2402.12058)

    提出了搭建提示方法，在图像中叠加点阵作为视觉信息锚点，并利用多维坐标作为文本位置参考，从而促进大型多模型的视觉-语言协调。

    

    最先进的大型多模型(LMMs)在视觉-语言任务中展现出极高的能力。尽管它们功能先进，但在需要复杂推理和多个层次的视觉信息的挑战性场景中，LMMs的性能仍然受限。现有的LMMs提示技术要么专注于改进文本推理，要么利用图像预处理工具，缺乏一种简单且通用的视觉提示方案，以促进LMMs中的视觉-语言协调。在这项工作中，我们提出了搭建提示，通过搭建坐标来促进视觉-语言协调。具体而言，搭建提示在图像中叠加一个点阵作为视觉信息锚点，并利用多维坐标作为文本位置参考。对一系列具有挑战性的视觉-语言任务进行了大量实验，证明了搭建提示相对于GPT-4V的优越性。

    arXiv:2402.12058v1 Announce Type: cross  Abstract: State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks. Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex reasoning with multiple levels of visual information. Existing prompting techniques for LMMs focus on either improving textual reasoning or leveraging tools for image preprocessing, lacking a simple and general visual prompting scheme to promote vision-language coordination in LMMs. In this work, we propose Scaffold prompting that scaffolds coordinates to promote vision-language coordination. Specifically, Scaffold overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references. Extensive experiments on a wide range of challenging vision-language tasks demonstrate the superiority of Scaffold over GPT-4V with the 
    
[^59]: 基于LLM的评估器是否混淆了自然语言生成（NLG）质量标准？

    Are LLM-based Evaluators Confusing NLG Quality Criteria?

    [https://arxiv.org/abs/2402.12055](https://arxiv.org/abs/2402.12055)

    LLMs在NLG评估中表现良好，但存在混淆不同评估标准的问题，研究提出了一个详细的分类系统和针对不同LLMs评估行为的扰动攻击，揭示了LLMs固有的混淆问题，并需要进一步研究。

    

    一些先前的研究表明，LLMs在不同任务的NLG评估中表现良好。然而，我们发现LLMs似乎混淆了不同的评估标准，从而降低了它们的可靠性。为了进一步验证，我们首先考虑避免现有NLG质量标准中不一致概念化和模糊表达的问题本身。因此，我们总结了一个清晰的层次分类系统，其中包含来自先前研究的11个常见方面的相应不同标准。受行为测试启发，我们精心设计了18种针对不同LLMs评估行为的方面定向扰动攻击，以进行细粒度分析。我们还进行了超出分类系统指导范围的人类注释，以验证扰动的影响。我们的实验结果揭示了LLMs固有的混淆问题，以及其他值得关注的现象，并需要进一步研究。

    arXiv:2402.12055v1 Announce Type: new  Abstract: Some prior work has shown that LLMs perform well in NLG evaluation for different tasks. However, we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability. For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing NLG quality criteria themselves. So we summarize a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved. Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different LLMs. We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations. Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further researc
    
[^60]: 小模型，大见解：利用精简代理模型确定LLMs何时以及为何检索

    Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs

    [https://arxiv.org/abs/2402.12052](https://arxiv.org/abs/2402.12052)

    本文介绍了一种新的协作方法SlimPLM，通过精简代理模型检测LLMs中缺失的知识，以增强LLMs的知识获取过程

    

    arXiv:2402.12052v1 公告类型:新摘要: 大型语言模型（LLMs）与搜索引擎的整合代表了知识获取方法的重要发展。然而，确定LLM已经具备的知识和需要搜索引擎帮助的知识仍然是一个未解决的问题。大多数现有方法通过LLM本身预处理答案或推理的结果来解决这个问题，但这带来了过高的计算成本。本文介绍了一种新颖的协作方法，即SlimPLM，通过精简代理模型检测LLMs中缺失的知识，以增强LLMs的知识获取过程。我们采用参数远远更少的代理模型，并将其答案视为启发式答案。然后利用启发式答案来预测回答用户问题所需的知识，以及LLM中已知和未知的知识。我们只为未知知识进行检索

    arXiv:2402.12052v1 Announce Type: new  Abstract: The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the mis
    
[^61]: 模型定制：在多模态大语言模型中缓解灾难性遗忘

    Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models

    [https://arxiv.org/abs/2402.12048](https://arxiv.org/abs/2402.12048)

    该论文提出了一种名为Model Tailor 的后训练调整方法，在多模态大语言模型中缓解了灾难性遗忘，通过保留预训练参数并替换少量微调参数，实现了对原始任务约99%的效果和对新任务约97%的效果。

    

    在微调多模态大语言模型（MLLMs）时，灾难性遗忘出现为一个关键挑战，这里优化未知任务的性能往往会导致原始任务的显著性能下降。本文对MLLMs中的灾难性遗忘进行了全面分析，并提出了一种名为Model Tailor的后训练调整方法。我们的方法主要保留了预训练参数，同时替换了少量（$\leq$ 10\%）微调参数，在原始任务上保持了与预训练时约99\%的效果，并在新任务上相比标准微调实现了约97\%的效果。具体来说，我们提出了一个稀疏掩码来识别“模型修补”，基于融合策略，该策略整合了显著性和敏感性分析。随后，引入了一种补偿机制来“装饰修补”，增强模型在目标任务和原始任务上的性能。

    arXiv:2402.12048v1 Announce Type: new  Abstract: Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks. This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor. Our method primarily preserves the pre-trained parameters while replacing a small number ($\leq$ 10\%) of fine-tuned parameters, maintaining $\sim$ 99\% effectiveness on original tasks versus pre-training, and achieving $\sim$ 97\% on new tasks compared to standard fine-tuning. Specifically, we derive a sparse mask to identify the "model patch", based on a fusion strategy that integrates salience and sensitivity analysis. Subsequently, a compensation mechanism is introduced to "decorate the patch", enhancing the model's performance on both target and original tasks
    
[^62]: 引文遗忘：自然语言处理和其他学术领域正处于引文年龄衰退期

    Citation Amnesia: NLP and Other Academic Fields Are in a Citation Age Recession

    [https://arxiv.org/abs/2402.12046](https://arxiv.org/abs/2402.12046)

    该研究分析了不同学术领域在43年间引用较旧作品的趋势，发现自然语言处理和机器学习研究中引文年龄衰退最为明显，此趋势并非由出版速率增长主导。

    

    这项研究考察了在43年的时间跨度（1980-2023年）内，在20个研究领域中倾向于引用较旧作品的趋势。我们将自然语言处理(NLP)倾向于引用较旧作品的特性放在其他20个领域的背景下进行分析，以探讨NLP是否展现出与其他领域随时间出现类似的引文模式，或者是否可以观察到差异。我们的分析基于约2.4亿篇论文的数据集，揭示了一个更广泛的科学趋势：许多领域在引用较旧作品方面明显下降（例如心理学、计算机科学）。我们将这种下降称为“引文年龄衰退”，类似于经济学家如何定义减少经济活动的时期。这一趋势在NLP和机器学习研究中最为显著（引文年龄从先前高峰下降了12.8%和5.5%）。我们的结果表明，对更近期作品的引用并非直接受到出版速率增长的推动（跨领域下降了3.4%，人文学科下降了5.2%，形式科学下降了5.5%）--即使在控制了发表数量时。

    arXiv:2402.12046v1 Announce Type: cross  Abstract: This study examines the tendency to cite older work across 20 fields of study over 43 years (1980--2023). We put NLP's propensity to cite older work in the context of these 20 other fields to analyze whether NLP shows similar temporal citation patterns to these other fields over time or whether differences can be observed. Our analysis, based on a dataset of approximately 240 million papers, reveals a broader scientific trend: many fields have markedly declined in citing older works (e.g., psychology, computer science). We term this decline a 'citation age recession', analogous to how economists define periods of reduced economic activity. The trend is strongest in NLP and ML research (-12.8% and -5.5% in citation age from previous peaks). Our results suggest that citing more recent works is not directly driven by the growth in publication rates (-3.4% across fields; -5.2% in humanities; -5.5% in formal sciences) -- even when controlli
    
[^63]: 通过基于体裁和主题特征的选择性屏蔽，将语言模型适应专业领域

    Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics

    [https://arxiv.org/abs/2402.12036](https://arxiv.org/abs/2402.12036)

    通过排名关键词并指导屏蔽过程，这项研究提出了一种利用体裁和主题信息定制语言模型适应专业领域的创新方法。

    

    最近，预训练语言建模的进展在各种自然语言处理（NLP）任务中取得了显著进展。模型训练期间的词屏蔽构成了像BERT这样的架构中语言建模的关键组成部分。然而，目前的词屏蔽方法依赖于随机选择，可能忽视领域特定的语言属性。在本文中，我们介绍了一种创新的屏蔽方法，利用体裁和主题信息来定制语言模型以适应专业领域。我们的方法包括一个排名过程，根据单词的重要性对其进行优先级排序，随后引导屏蔽过程。我们进行的实验使用法律领域内的持续预训练，强调了我们的方法在英语LegalGLUE基准上的有效性。预训练语言模型和代码可免费使用。

    arXiv:2402.12036v1 Announce Type: new  Abstract: Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks. Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT. However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes. In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains. Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure. Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language. Pre-trained language models and code are freely available for use.
    
[^64]: 跨分词器蒸馏：用于LLM的通用logit蒸馏损失

    Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs

    [https://arxiv.org/abs/2402.12030](https://arxiv.org/abs/2402.12030)

    介绍了基于最优传输的通用logit蒸馏 (ULD) 损失，用于解决不同架构和分词器模型之间蒸馏的限制。

    

    部署几十亿参数的大型语言模型 (LLMs) 在大多数工业应用中可能并不切实际，原因是诸如成本、延迟限制和硬件可访问性等约束。知识蒸馏 (KD) 通过将资源密集型大模型的知识压缩到较小模型中提供了解决方案。存在多种策略，一些依赖于教师模型生成的文本，并可选择性地利用其logits来增强学习。然而，基于logits的这些方法通常要求教师和学生模型共享相同的分词器，限制了它们在不同LLM系列中的适用性。本文引入了基于最优传输的通用logit蒸馏 (ULD) 损失，以解决这一限制。我们的实验结果显示了ULD损失在启用不同架构和分词器的模型之间的蒸馏方面的有效性，为更广泛的应用铺平了道路。

    arXiv:2402.12030v1 Announce Type: new  Abstract: Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility. Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones. Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning. However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families. In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation. Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread
    
[^65]: 从后门毒化数据集中通过降频空间获取清洁语言模型

    Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space

    [https://arxiv.org/abs/2402.12026](https://arxiv.org/abs/2402.12026)

    通过对频率空间的分析，本文提出了一种多尺度低秩适应（MuScleLoRA）方法，用于解决在训练语言模型时受到后门攻击的问题。

    

    尽管语言模型（LMs）在各种自然语言处理（NLP）任务中取得了显著成功，但LMs的可靠性容易受到后门攻击的影响。先前的研究尝试在毒化数据集上训练LMs时减轻后门学习，但在现实场景中抵御复杂的后门攻击时仍然面临困难。在本文中，我们通过傅里叶分析研究了频率空间中后门LMs的学习机制。我们的发现表明，毒化数据集上呈现的后门映射相比清洁映射更倾向于较低频率，导致后门映射更快地收敛。为了解决这一困境，我们提出了多尺度低秩适应（MuScleLoRA），它在频率空间中部署多个径向缩放，低秩适应目标模型，并在更新参数时进一步调整梯度。通过降频

    arXiv:2402.12026v1 Announce Type: cross  Abstract: Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscal
    
[^66]: 使用语音基础模型和大语言模型的语音翻译：存在和缺失的内容是什么？

    Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?

    [https://arxiv.org/abs/2402.12025](https://arxiv.org/abs/2402.12025)

    这项研究关注语音翻译领域的发展，通过将语音基础模型与大语言模型结合，为解决多模态任务提供了新的统一模型，但目前各种评估方法和设置多样性阻碍了确定每个架构构建块的最佳解决方案的识别。

    

    自然语言处理（NLP）领域最近发生了一场变革性的转变，随着基础模型的出现，特别是彻底改变了基于文本的NLP的大型语言模型（LLMs）。这种范式已经扩展到其他形式，包括语音，在那里研究人员正在积极探索将语音基础模型（SFMs）和LLMs结合成单一的统一模型，以解决多模态任务。在这些任务中，本文着重于语音到文本翻译（ST）。通过审查该主题上发表的论文，我们提出了迄今为止提出的架构解决方案和训练策略的统一观点，强调它们之间的相似之处和差异之处。基于这一研究，我们不仅整理了所学到的经验教训，还展示了多样化的设置和评估方法如何阻碍对每个架构构建块的最佳性能解决方案的识别。

    arXiv:2402.12025v1 Announce Type: new  Abstract: The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block 
    
[^67]: 将大型语言模型压缩用于文本属性图学习

    Distilling Large Language Models for Text-Attributed Graph Learning

    [https://arxiv.org/abs/2402.12022](https://arxiv.org/abs/2402.12022)

    本研究旨在将大型语言模型和图模型的优势相结合，通过将LLMs的能力压缩到 TAG 学习的本地图模型中，解决它们之间的固有差距。

    

    文本属性图（TAGs）是连接的文本文档图。图模型可以有效学习TAGs，但它们的训练严重依赖于人工标注的标签，在许多应用中这些标签很少或甚至不可用。大型语言模型（LLMs）最近在少样本和零样本TAG学习中展示了显著能力，但它们存在可伸缩性、成本和隐私问题。因此，在这项工作中，我们专注于通过将LLMs的能力传授给TAG学习中的本地图模型，从而协同LLMs和图模型的互补优势。

    arXiv:2402.12022v1 Announce Type: new  Abstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents. Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning. To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale. Extensive experiments validate the efficacy of our proposed 
    
[^68]: 一种用于词汇语义变化的上下文化词嵌入的系统比较

    A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change

    [https://arxiv.org/abs/2402.12011](https://arxiv.org/abs/2402.12011)

    本文通过在相同条件下评估了最新的词汇语义变化模型和方法，将LSC问题分解为不同级别的任务，并展示了在不同语言的八个基准测试中，APD在GCD方面优于其他方法，XL-LEXEME在WiC、WSI和GCD方面优于其他上下文化模型，并且与GPT-4相当。

    

    上下文化嵌入是建模词汇语义变化（LSC）的首选工具。当前的评估通常专注于称为分级变化检测（GCD）的特定任务。然而，由于它们依赖于不同设置，跨作品的性能比较经常具有误导性。在本文中，我们在相同条件下评估了GCD的最新模型和方法。我们进一步将LSC问题分解为上下文中的单词（WiC）和词义归纳（WSI）任务，并比较这些不同级别的模型。我们在八个可用的LSC基准测试中跨不同语言进行评估，结果表明：（i）APD在GCD方面优于其他方法；（ii）XL-LEXEME在WiC、WSI和GCD方面优于其他上下文化模型，同时与GPT-4相当；（iii）明显需要改进对词义建模以及关注这些意义何时、如何和为何变化的工作。

    arXiv:2402.12011v1 Announce Type: new  Abstract: Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC). Current evaluations typically focus on a specific task known as Graded Change Detection (GCD). However, performance comparison across work are often misleading due to their reliance on diverse settings. In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions. We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, and compare models across these different levels. Our evaluation is performed across different languages on eight available benchmarks for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need for improving the modeling of word meanings, as well as focus on how, when, and why these meanings change, rather t
    
[^69]: 回忆那一年发生的事件？评估大型语言模型中的时间信息和推理能力

    Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models

    [https://arxiv.org/abs/2402.11997](https://arxiv.org/abs/2402.11997)

    大型语言模型在处理时间信息和推理方面存在显著限制，闭源模型可能暗示了不确定性认识与错误回应之间的权衡。

    

    大型语言模型（LLMs）越来越普遍，但它们对于推理和保留时间信息的能力仍然有限。这限制了它们在理解事件的顺序性对关键的现实场景中的应用。本文在一个新颖的大规模时间数据集\textbf{TempUN}上对最先进的模型进行实验，揭示了时间保留和推理能力方面的显著限制。有趣的是，闭源模型更频繁地显示出知识差距，可能暗示了不确定性认识和错误回应之间的权衡。此外，探索各种微调方法并没有带来主要性能改进。相关数据集和代码可在以下网址获得（https://github.com/lingoiitgn/TempUN）。

    arXiv:2402.11997v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).
    
[^70]: 压缩以引人注目：释放压缩记忆在现实世界长期对话中的潜力

    Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations

    [https://arxiv.org/abs/2402.11975](https://arxiv.org/abs/2402.11975)

    提出了COmpressive Memory-Enhanced Dialogue sYstems（COMEDY）框架，通过“一对多”方法利用单一语言模型管理记忆生成、压缩和响应生成，核心概念是压缩记忆，支持大规模中文指导调优数据集Dolphin，比较评估证明了COMEDY的优越性。

    

    现有的基于检索的方法在维护长期对话方面取得了显著进展。然而，这些方法在记忆数据库管理和准确的记忆检索方面面临挑战，阻碍了它们在动态、真实世界互动中的有效性。该研究引入了一个新颖的框架，称为COmpressive Memory-Enhanced Dialogue sYstems（COMEDY），它摒弃了传统的检索模块和记忆数据库。相反，COMEDY采用了“一对多”方法，利用单一语言模型来管理记忆生成、压缩和响应生成。这一框架的核心概念是压缩记忆，它将会话特定摘要、用户-机器人动态和过去事件整合到简洁的记忆格式中。为了支持COMEDY，我们构建了一个大规模的中文指导调优数据集Dolphin，从真实用户-聊天机器人互动中得出。比较评估表明COMEDY的优越性。

    arXiv:2402.11975v1 Announce Type: new  Abstract: Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a ''One-for-All'' approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which intergrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we curated a large-scale Chinese instruction-tuning dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY's superiority ove
    
[^71]: 方言说话者有什么需求？德语方言语言技术态度调查

    What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects

    [https://arxiv.org/abs/2402.11968](https://arxiv.org/abs/2402.11968)

    德语方言和区域语言说话者更倾向于支持能够处理方言输入的自然语言处理工具，比如虚拟助手，对于产生方言输出的应用则支持程度相对较低。

    

    自然语言处理（NLP）主要集中于对标准化语言进行建模。最近，关注点逐渐转向了本地、非标准化语言和方言。然而，与NLP工具相关的说话者群体的需求和愿望在很大程度上是未知的。本文关注德语方言和区域语言，这是一组在声望和标准化方面异质性的语言变体。我们对这些语言变体的说话者进行了调查（N=327），并介绍了他们对于他们方言的假想语言技术的意见。尽管我们的受访者子群体之间的态度有所不同，但我们发现受访者特别支持能够处理方言输入的潜在NLP工具（特别是音频输入），比如虚拟助手，而对于产生方言输出的应用，比如机器翻译或拼写检查程序，支持程度要低一些。

    arXiv:2402.11968v1 Announce Type: new  Abstract: Natural language processing (NLP) has largely focused on modelling standardized languages. More recently, attention has increasingly shifted to local, non-standardized languages and dialects. However, the relevant speaker populations' needs and wishes with respect to NLP tools are largely unknown. In this paper, we focus on dialects and regional languages related to German -- a group of varieties that is heterogeneous in terms of prestige and standardization. We survey speakers of these varieties (N=327) and present their opinions on hypothetical language technologies for their dialects. Although attitudes vary among subgroups of our respondents, we find that respondents are especially in favour of potential NLP tools that work with dialectal input (especially audio input) such as virtual assistants, and less so for applications that produce dialectal output such as machine translation or spellcheckers.
    
[^72]: DB-LLM: 高效LLM的准确双二值化

    DB-LLM: Accurate Dual-Binarization for Efficient LLMs

    [https://arxiv.org/abs/2402.11960](https://arxiv.org/abs/2402.11960)

    本文提出了一种名为DB-LLM的新颖双二值化方法，通过引入灵活双二值化(FDB)来平衡2位宽度的精度优势和二值化的效率优势，从而在提高LLMs的计算效率的同时保持了准确性。

    

    大型语言模型(LLMs)显著推进了自然语言处理领域，然而高昂的内存和计算开销阻碍了它们的实际部署。量化成为改善LLMs计算效率的最有效方法之一。然而，现有的超低比特量化总是导致严重的精度下降。本文通过实证研究缓解了超低比特量化的微观和宏观特性，提出了一种新颖的LLMs双二值化方法，即DB-LLM。对于微观层面，我们考虑了2位宽度的准确性优势和二值化的效率优势，引入了灵活双二值化(FDB)。通过将2位量化权重分为两组独立的二进制数集，FDB确保了表示的准确性并引入了灵活性，利用二值化的高效位操作同时保留了

    arXiv:2402.11960v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs. However, existing ultra-low-bit quantization always causes severe accuracy drops. In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit quantization and present a novel Dual-Binarization method for LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while reta
    
[^73]: 使用LLMs自动评估心理健康咨询

    Automatic Evaluation for Mental Health Counseling using LLMs

    [https://arxiv.org/abs/2402.11958](https://arxiv.org/abs/2402.11958)

    使用LLMs自动评估心理咨询对话中的工作联盟，结果显示与人工评估高度一致，并提供宝贵见解。

    

    高质量的心理咨询对全球心理健康至关重要，及时评估对确保其有效性至关重要。然而，为每个咨询会话获取专业评估既昂贵又具挑战性。依赖自我或第三方手动报告来评估咨询质量的现有方法存在主观偏见和耗时的局限性。为了解决上述挑战，本文提出了一种创新高效的自动评估方法，利用大型语言模型(LLMs)来评估咨询对话中的工作联盟。我们收集了一个全面的咨询数据集，并基于治疗关系理论进行了多方评估。我们基于LLMs的评估结合我们的指南，与人工评估高度一致，并为咨询脚本提供了宝贵的见解。这突显了LLMs作为监督的潜力。

    arXiv:2402.11958v1 Announce Type: new  Abstract: High-quality psychological counseling is crucial for mental health worldwide, and timely evaluation is vital for ensuring its effectiveness. However, obtaining professional evaluation for each counseling session is expensive and challenging. Existing methods that rely on self or third-party manual reports to assess the quality of counseling suffer from subjective biases and limitations of time-consuming.   To address above challenges, this paper proposes an innovative and efficient automatic approach using large language models (LLMs) to evaluate the working alliance in counseling conversations. We collected a comprehensive counseling dataset and conducted multiple third-party evaluations based on therapeutic relationship theory. Our LLM-based evaluation, combined with our guidelines, shows high agreement with human evaluations and provides valuable insights into counseling scripts. This highlights the potential of LLMs as supervisory to
    
[^74]: 多领域概括性摘要生成的关键性分配分析

    Analysis of Multidomain Abstractive Summarization Using Salience Allocation

    [https://arxiv.org/abs/2402.11955](https://arxiv.org/abs/2402.11955)

    通过SEASON技术，本研究评估了用于生成概括性摘要的方法，与BART、PEGASUS和ProphetNet等模型进行对比，在多个数据集上进行了评估，着重分析了财经数据集的表现。

    

    本文通过SEASON（Salience Allocation as Guidance for Abstractive SummarizatiON）技术的视角探讨了概括性文本摘要生成的领域，该模型旨在通过利用关键性分配技术来增强摘要生成。研究通过与BART、PEGASUS和ProphetNet等知名模型进行比较，这些模型均针对各种文本摘要任务进行了微调，来评估SEASON技术的有效性。评估使用包括CNN/Dailymail、SAMSum和基于财经新闻的Event-Driven Trading (EDT)在内的多种数据集进行，特别关注包含大量新闻文章的财经数据集，时间跨度为2020/03/01至2021/05/06。本文采用多种评估指标（如ROUGE、METEOR、BERTScore和MoverScore）来评估这些经过微调的模型生成概括性摘要的性能。这些指标的分析提供了对这些模型优劣的深入见解。

    arXiv:2402.11955v1 Announce Type: cross  Abstract: This paper explores the realm of abstractive text summarization through the lens of the SEASON (Salience Allocation as Guidance for Abstractive SummarizatiON) technique, a model designed to enhance summarization by leveraging salience allocation techniques. The study evaluates SEASON's efficacy by comparing it with prominent models like BART, PEGASUS, and ProphetNet, all fine-tuned for various text summarization tasks. The assessment is conducted using diverse datasets including CNN/Dailymail, SAMSum, and Financial-news based Event-Driven Trading (EDT), with a specific focus on a financial dataset containing a substantial volume of news articles from 2020/03/01 to 2021/05/06. This paper employs various evaluation metrics such as ROUGE, METEOR, BERTScore, and MoverScore to evaluate the performance of these models fine-tuned for generating abstractive summaries. The analysis of these metrics offers a thorough insight into the strengths a
    
[^75]: LEMMA: 支持外部知识增强的LVLM增强多模态虚假信息检测

    LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation

    [https://arxiv.org/abs/2402.11943](https://arxiv.org/abs/2402.11943)

    LVLM-Enhanced Multimodal Misinformation Detection with LEMMA proposes a solution for detecting complex misinformation by enhancing the reasoning capability of Large Vision Language Models.

    

    社交平台上多模态虚假信息的兴起对个人和社会都带来了重大挑战。与文本虚假信息相比，多模态虚假信息具有更高的可信度和更广泛的影响，使得检测变得复杂，需要跨越不同媒体类型进行强大的推理，并具备准确验证的深刻知识。大规模视觉语言模型（LVLM）的出现为解决这一问题提供了潜在方案。利用LVLM在处理视觉和文本信息方面的熟练能力，LVLM在识别复杂信息和展现强大推理能力方面展示出有希望的能力。在本文中，我们首先研究了LVLM在多模态虚假信息检测中的潜力。我们发现，尽管LVLM的性能优于LLMs，但其深刻推理可能由于缺乏证据而表现出有限的效力。基于这些观察，我们提出了LEMMA：LVLM增强多模态虚假信息检测。

    arXiv:2402.11943v1 Announce Type: new  Abstract: The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. In this paper, we first investigate the potential of LVLM on multimodal misinformation detection. We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Det
    
[^76]: 智能手机GUI自动化的全面认知LLM代理

    Comprehensive Cognitive LLM Agent for Smartphone GUI Automation

    [https://arxiv.org/abs/2402.11941](https://arxiv.org/abs/2402.11941)

    提出了全面认知LLM代理，通过全面环境感知和条件动作预测两种新方法系统性提高GUI自动化性能。

    

    大型语言模型(LLMs)已经显示出作为人类般自主语言代理与现实环境进行交互的显著潜力，尤其在图形用户界面(GUI)自动化方面。然而，这些GUI代理需要全面的认知能力，包括详尽的感知和可靠的动作响应。我们提出了全面认知LLM代理，CoCo-Agent，采用了两种新方法，全面环境感知(CEP)和条件动作预测(CAP)，以系统性地提高GUI自动化性能。首先，CEP通过不同方面和粒度促进GUI感知，包括屏幕截图和用于视觉通道的补充详细布局，以及用于文本通道的历史动作。其次，CAP将动作预测分解为子问题：动作类型预测和条件化于动作类型的动作目标。

    arXiv:2402.11941v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation. However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response. We propose \underline{Co}mprehensive \underline{Co}gnitive LLM \underline{Agent}, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel. Second, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type. With our
    
[^77]: Team QUST在SemEval-2024任务8中的研究：单语和多语言方法对检测AI生成文本的全面研究

    Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text

    [https://arxiv.org/abs/2402.11934](https://arxiv.org/abs/2402.11934)

    本文研究了团队QUST在SemEval-2024任务8中的参与情况，通过数据增强和清洗提高了模型训练效率和准确性，在单语任务中评估了多种方法并最终采用堆叠集成模型，最终在多语言环境中取得了不错的排名。

    

    本文介绍了团队QUST在SemEval 2024任务8中的参与情况。我们首先对数据集进行了增强和清洗，以提高模型训练的效率和准确性。在单语任务中，我们评估了传统的深度学习方法、多尺度正负无标记框架（MPU）、微调、适配器和集成方法。然后，我们从单语模型中选择了表现最佳的模型，并在子任务A和B中对它们进行评估。最终模型采用了将微调与MPU相结合的堆叠集成。在多语言环境的子任务A中，我们的系统在官方测试集中取得了第8名（在准确性方面排名第13）。我们在https://github.com/warmth27/SemEval2024_QUST发布了我们的系统代码。

    arXiv:2402.11934v1 Announce Type: cross  Abstract: This paper presents the participation of team QUST in Task 8 SemEval 2024. We first performed data augmentation and cleaning on the dataset to enhance model training efficiency and accuracy. In the monolingual task, we evaluated traditional deep-learning methods, multiscale positive-unlabeled framework (MPU), fine-tuning, adapters and ensemble methods. Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B. The final model construction employed a stacking ensemble that combined fine-tuning with MPU. Our system achieved 8th (scored 8th in terms of accuracy, officially ranked 13th) place in the official test set in multilingual settings of subtask A. We release our system code at:https://github.com/warmth27/SemEval2024_QUST
    
[^78]: MRKE：通过知识编辑对LLMs进行多跳推理评估

    MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition

    [https://arxiv.org/abs/2402.11924](https://arxiv.org/abs/2402.11924)

    通过编辑HotpotQA数据集中的新知识，我们引入了一个LLM MHQA评估基准，同时注释和评估了推理链，揭示了当前MHQA基准存在数据污染的潜在风险。

    

    虽然大型语言模型（LLMs）在多跳问题回答（MHQA）任务中表现出色，但它们真正的推理能力仍有待探讨。目前的LLM QA评估基准存在一些限制，包括1）数据污染，评估数据可能在预训练阶段暴露给LLMs；以及2）忽视推理链评估。因此，我们引入了一种LLM MHQA评估基准，这是基于编辑现成HotpotQA数据集上的新、前所未有的知识的第一个QA基准；此外，我们还注释和评估了推理链，以子问题和中间答案的形式对应于多跳问题。具体来说，根据观察结果，1）LLMs在原始HotpotQA和我们编辑的数据之间显示性能差距，认为当前的MHQA基准可能存在数据污染的潜在风险，难以评估LLMs的性能。

    arXiv:2402.11924v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) have shown strong performance in Multi-hop Question Answering (MHQA) tasks, their real reasoning ability remains exploration. Current LLM QA evaluation benchmarks have shown limitations, including 1) data contamination, the evaluation data are potentially exposed to LLMs during the pretraining stage; and 2) ignoration of the reasoning chain evaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA benchmark based on the new, unprecedented knowledge by editing the off-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the reasoning chain in the form of sub-questions and intermediate answers corresponding to the multi-hop questions. Specifically, based on the observation, 1) LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' perfor
    
[^79]: 使用基于领域特定余弦相似度度量的胸部X射线报告中的语义文本相似性评估

    Semantic Textual Similarity Assessment in Chest X-ray Reports Using a Domain-Specific Cosine-Based Metric

    [https://arxiv.org/abs/2402.11908](https://arxiv.org/abs/2402.11908)

    本文介绍了一种专门设计用于评估胸部X射线报告中语义文本相似性的新方法，以提高医学成像解释的准确性，促进临床决策并改善患者预后。

    

    医学语言处理和深度学习技术已成为改善医疗保健的关键工具，特别是在医学成像和医学文本数据分析方面。这些多模态数据融合技术有助于改善医学成像的解释，从而提高诊断准确性，促进临床决策并改善患者预后。本文针对更强大的方法来评估医疗报告的语义内容的需求。传统的自然语言处理方法和指标最初设计用于考虑自然语言领域和机器翻译中的语义上下文，通常无法捕捉医学内容固有的复杂语义含义。在本研究中，我们引入了一种专门用于评估的新方法

    arXiv:2402.11908v1 Announce Type: new  Abstract: Medical language processing and deep learning techniques have emerged as critical tools for improving healthcare, particularly in the analysis of medical imaging and medical text data. These multimodal data fusion techniques help to improve the interpretation of medical imaging and lead to increased diagnostic accuracy, informed clinical decisions, and improved patient outcomes. The success of these models relies on the ability to extract and consolidate semantic information from clinical text. This paper addresses the need for more robust methods to evaluate the semantic content of medical reports. Conventional natural language processing approaches and metrics are initially designed for considering the semantic context in the natural language domain and machine translation, often failing to capture the complex semantic meanings inherent in medical content. In this study, we introduce a novel approach designed specifically for assessing
    
[^80]: 通过自我奖励对比提示精炼直接对齐大型语言模型

    Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation

    [https://arxiv.org/abs/2402.11907](https://arxiv.org/abs/2402.11907)

    通过对比提示自我奖励方法，提出了一种直接对齐大型语言模型的自动对齐方法，无需依赖人工注释的偏好数据，在实验中表现优于现有方法RLHF。

    

    在这篇论文中，我们提出了一种方法，通过使用对比提示对响应对的输出概率进行评估，从而在LLaMA2-7B和LLaMA2-13B上实现了比RLAIF更好的性能。基于此，我们提出了一种自动对齐方法，即直接大型模型对齐（DLMA）。首先，我们使用对比提示对自动生成的偏好数据。然后，我们继续使用对比提示对生成的偏好数据进行评估并计算自我奖励分数。最后，我们使用DPO算法通过结合这种自我奖励分数来有效地对齐LLMs。在实验阶段，我们的DLMA方法能够在不依赖人工注释的偏好数据的情况下超越RLHF方法。

    arXiv:2402.11907v1 Announce Type: new  Abstract: Aligning large language models (LLMs) with human expectations without human-annotated preference data is an important problem. In this paper, we propose a method to evaluate the response preference by using the output probabilities of response pairs under contrastive prompt pairs, which could achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based on this, we propose an automatic alignment method, Direct Large Model Alignment (DLMA). First, we use contrastive prompt pairs to automatically generate preference data. Then, we continue to evaluate the generated preference data using contrastive prompt pairs and calculate a self-rewarding score. Finally, we use the DPO algorithm to effectively align LLMs by combining this self-rewarding score. In the experimental stage, our DLMA method could surpass the \texttt{RLHF} method without relying on human-annotated preference data.
    
[^81]: 学习编写：将LLMs与知识编辑对齐

    Learning to Edit: Aligning LLMs with Knowledge Editing

    [https://arxiv.org/abs/2402.11905](https://arxiv.org/abs/2402.11905)

    提出了一个名为Learning to Edit（LTE）的框架，教导LLMs将更新后的知识应用于输入问题，通过对齐阶段和推理阶段实现可靠的、范围内的文本编辑。

    

    知识编辑技术旨在高效修改大型语言模型（LLMs）中的少量知识，而不会对其他输入的性能产生负面影响，已经引起了广泛关注。然而，现有方法主要依赖于记忆更新后的知识，阻碍了LLMs有效地将新知识与其固有知识相结合以回答问题。为此，我们提出了一个名为“学习编写”（LTE）的框架，重点教导LLMs将更新后的知识应用于输入问题，灵感来自于“授人以鱼不如授人以渔”的理念。LTE具有两阶段过程：（i）对齐阶段，通过在精心筛选的平行数据集上微调LLMs，使其能够进行可靠的、范围内的编辑，同时保留范围外信息和语言能力；（ii）推理阶段，采用基于检索的机制进行实时和大规模知识编辑。

    arXiv:2402.11905v1 Announce Type: new  Abstract: Knowledge editing techniques, aiming to efficiently modify a minor proportion of knowledge in large language models (LLMs) without negatively impacting performance across other inputs, have garnered widespread attention. However, existing methods predominantly rely on memorizing the updated knowledge, impeding LLMs from effectively combining the new knowledge with their inherent knowledge when answering questions. To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of "Teach a man to fish." LTE features a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on a meticulously curated parallel dataset to make reliable, in-scope edits while preserving out-of-scope information and linguistic proficiency; and (ii) the Inference Phase, which employs a retrieval-based mechanism for real-time and mass knowledge editing. By c
    
[^82]: SoLA: 为了更好的逻辑推理而对LLM进行求解层调整

    SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning

    [https://arxiv.org/abs/2402.11903](https://arxiv.org/abs/2402.11903)

    提出了一种新颖的求解器层适应（SoLA）方法，在LLM中引入求解器层，不同地引导解决方案朝向可满足性

    

    针对大型语言模型（LLMs）在逻辑推理上面临的挑战，先前的努力试图通过工具学习来改变问题求解。虽然在小规模问题上已经取得了进展，但由于规模庞大且表达复杂，解决工业案例仍然困难。在本文中，我们提出了一种新颖的求解层适应（SoLA）方法，我们在LLM中引入了一个求解器作为新层，不同地引导解决方案朝向可满足性。在SoLA中，LLM旨在理解自然语言描述的搜索空间，并识别最高质量的局部解，而求解器层则专注于初始解不满足的约束条件。借助MaxSAT作为桥梁，我们定义了前向和后向传递梯度，使最终模型能够收敛到一个满足的解或证明不可满足性。后门理论确保SoLA能够获得准确性

    arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
    
[^83]: 在大型语言模型的知识编辑中探索多跳事实快捷方式

    Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models

    [https://arxiv.org/abs/2402.11900](https://arxiv.org/abs/2402.11900)

    本文系统调查了在大型语言模型中利用事实快捷方式进行多跳事实推理的可能性，并分析了这种快捷方式可能带来的风险。

    

    最近的工作展示了大型语言模型（LLMs）在回忆知识和推理方面的强大能力。然而，LLMs在将这两种能力结合到通过多跳事实推理中尚未被广泛探索。本文系统地调查了LLMs利用基于多跳知识的初始和终端实体之间直接连接的快捷方式的可能性。我们首先通过Knowledge Neurons探索了事实快捷方式的存在，揭示出：(i)快捷方式的强度与预训练语料库中初始和终端实体的共现频率高度相关；（ii）少量提示在回答多跳问题时利用更多的快捷方式，相比之下，思维链提示更多。然后，我们从多跳知识编辑的角度分析了事实快捷方式带来的风险。分析表明，大约有

    arXiv:2402.11900v1 Announce Type: new  Abstract: Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximatel
    
[^84]: SIBO：一个简单的增强器用于参数高效微调

    SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2402.11896](https://arxiv.org/abs/2402.11896)

    SIBO提出了一个简单的增强器来增强参数高效微调技术，有效解决了Transformer-based LLMs中过度平滑的问题，并在多个基准数据集上取得了显著的性能提升。

    

    微调大型语言模型（LLMs）的所有参数需要大量的计算资源和较长时间。最新的参数高效微调（PEFT）技术，如适配器微调和LoRA，允许只调整这些LLMs的一小部分参数。同时，人们注意到过度平滑的问题削弱了基于Transformer的LLMs的有效性，在下游任务中表现不佳。在本文中，我们提出了SIBO，这是一个简单的增强器，通过注入初始残差来增强PEFT。SIBO直观易懂，并且很容易扩展到一系列最新的PEFT技术，以减轻过度平滑并提高性能。对22个基准数据集进行的大量实验证明，SIBO显著提高了各种强基线模型的性能，比现有的PEFT技术提高了高达15.7%和23.5%。

    arXiv:2402.11896v1 Announce Type: new  Abstract: Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time. Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs. Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT
    
[^85]: 与团体互动：对宗教极端化影响的联系与破裂

    Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization

    [https://arxiv.org/abs/2402.11895](https://arxiv.org/abs/2402.11895)

    研究探讨团体互动对宗教极端化的影响，在印度 Twitter 用户中发现，政治和社会事件的团体间互动可以减少极端化。

    

    虽然接触不同观点可能会减少极端化，但当讨论对抗性时，也可能产生反效应并加剧极端化。在这里，我们研究了围绕重要事件的团体间互动是否影响社交网络中多数群体和少数群体之间的极端化。我们收集了约 70 万名印度 Twitter 用户在 2020 年参与与 COVID-19 相关话题讨论时的宗教身份数据。我们引入了一个基于推文文本的情境嵌入的新量度，用于帮助我们评估宗教群体之间的极端化。然后，我们使用元学习框架来研究围绕共同、政治和社会经济事件的异质处理效果对个体群体符合性的影响。我们发现，在政治和社会事件方面，团体间互动会减少极端化。

    arXiv:2402.11895v1 Announce Type: cross  Abstract: While exposure to diverse viewpoints may reduce polarization, it can also have a backfire effect and exacerbate polarization when the discussion is adversarial. Here, we examine the question whether intergroup interactions around important events affect polarization between majority and minority groups in social networks. We compile data on the religious identity of nearly 700,000 Indian Twitter users engaging in COVID-19-related discourse during 2020. We introduce a new measure for an individual's group conformity based on contextualized embeddings of tweet text, which helps us assess polarization between religious groups. We then use a meta-learning framework to examine heterogeneous treatment effects of intergroup interactions on an individual's group conformity in the light of communal, political, and socio-economic events. We find that for political and social events, intergroup interactions reduce polarization. This decline is we
    
[^86]: 你见过我吗？自动化数据集更新以实现可靠及及时的评估

    Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation

    [https://arxiv.org/abs/2402.11894](https://arxiv.org/abs/2402.11894)

    本文提出自动化数据集更新策略，利用两种系统验证过的策略生成看不见和高质量的测试样本，以缓解数据泄漏问题并实现评估稳定性。

    

    由于大型语言模型（LLMs）的能力不断扩大和预训练数据的增加，LLMs面临着日益严重的评估挑战。一方面，数据泄漏问题导致对现有基准的过度估计。另一方面，定期手动整理数据集成本高昂。本文提出自动化数据集更新以实现可靠及及时的评估。基本思想是基于现有样本生成看不见的高质量测试样本以减轻泄漏问题。具体而言，我们提出了两种策略并进行了系统验证。第一种是模仿策略，利用LLMs创建类似现有样本的新样本，最大程度地保留原始数据集的风格。我们的实验表明它在多次实例中的评估稳定性以及在大多数情况下处理数据泄漏问题的有效性。

    arXiv:2402.11894v1 Announce Type: new  Abstract: Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing benchmarks. On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues. In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poo
    
[^87]: 在检索增强生成环境中评估联合搜索系统（FeB4RAG）

    FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation

    [https://arxiv.org/abs/2402.11891](https://arxiv.org/abs/2402.11891)

    提出了FeB4RAG，这是一个专门为检索增强生成框架内的联合搜索设计的新数据集。

    

    联合搜索系统聚合来自多个搜索引擎的结果，选择合适的来源以增强结果质量并与用户意图保持一致。随着检索增强生成（RAG）管道的日益流行，联合搜索可以在跨异构数据源中搜索相关信息以生成明智的响应起到关键作用。然而，现有数据集（如过去的TREC FedWeb跟踪中开发的数据集）过时了，缺乏对现代信息检索挑战的代表性。为了弥补这一差距，我们提出了FeB4RAG，这是一个专门为RAG框架内的联合搜索而设计的新数据集。这个数据集源自广泛使用的beir基准集的16个子集合，包括了为聊天机器人应用量身定制的790个信息请求（类似于对话查询），以及每个资源返回的最佳结果以及相关的LLM-der

    arXiv:2402.11891v1 Announce Type: cross  Abstract: Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges. To bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within RAG frameworks. This dataset, derived from 16 sub-collections of the widely used \beir benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-der
    
[^88]: 重新审视自回归语言模型的知识蒸馏

    Revisiting Knowledge Distillation for Autoregressive Language Models

    [https://arxiv.org/abs/2402.11890](https://arxiv.org/abs/2402.11890)

    提出一种自适应教学方法（ATKD），以改善自回归语言模型的知识蒸馏，帮助各种基线KD方法在所有模型类型和规模上实现一致且显著的性能提升。

    

    知识蒸馏（KD）是一种常见的方法，用于通过训练较小的学生模型来压缩教师模型，以减少推理成本和内存占用。然而，在自回归语言模型（LMs）的背景下，我们在实证中发现较大的教师LMs可能会导致较差的学生。为解决这个问题，我们进行了一系列分析，并揭示了不同记号具有不同的教学方式，忽视这一点将导致性能下降。在此基础上，我们提出了一种简单而有效的自适应教学方法（ATKD）来改善KD。ATKD的核心是减少死记硬背的学习，使教学更加多样化和灵活。对8个LM任务的大量实验表明，在ATKD的帮助下，各种基线KD方法在所有模型类型和规模上均可以实现一致且显著的性能提升（最高可达+3.04%的平均分数）。

    arXiv:2402.11890v1 Announce Type: new  Abstract: Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve 
    
[^89]: ROSE 不这样做：使用反向提示对比解码提升调整指令大型语言模型的安全性

    ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding

    [https://arxiv.org/abs/2402.11889](https://arxiv.org/abs/2402.11889)

    ROSE是一种简单而有效的方法，通过抑制不受欢迎的输出来提高期望的安全输出的概率，从而直接提升现有调整指令LLMs的安全性。

    

    随着调整指令大型语言模型（LLMs）的发展，提高LLMs的安全性变得更加关键。然而，目前用于将LLMs输出与预期安全性对齐的方法通常需要大量的训练工作，例如高质量的安全数据和昂贵的计算资源，这些都是昂贵且低效的。因此，我们提出了反向提示对比解码（ROSE），这是一种简单而有效的方法，可以直接提升现有调整指令LLMs的安全性，而无需额外训练。ROSE的原则是通过抑制经过精心设计的反向提示诱导的不受欢迎的输出，提高期望的安全输出的概率。在6个安全任务和2个通用任务上的实验表明，我们的ROSE不仅在5种类型的调整指令LLMs上带来了一致且显著的安全性改进（高达+13.8%的安全分数），而且对通用任务也有益处。

    arXiv:2402.11889v1 Announce Type: new  Abstract: With the development of instruction-tuned large language models (LLMs), improving the safety of LLMs has become more critical. However, the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse prompt contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse prompts. Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8% safety score) upon 5 types of instruction-tuned LLMs, but also benefits the general-p
    
[^90]: LLM的多彩未来：评估和改进LLM作为酷儿青少年情感支持者的作用

    The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth

    [https://arxiv.org/abs/2402.11886](https://arxiv.org/abs/2402.11886)

    本文旨在评估和改进LLM作为酷儿青少年情感支持者的潜力，通过定性和定量分析LLM与酷儿相关内容的互动，并开发了一个新颖的评估标准量表。

    

    酷儿青少年面临着增加的心理健康风险，如抑郁、焦虑和自杀意念。受负面标签的影响，他们经常避免寻求帮助，依赖在线资源，这可能提供不相容的信息。虽然获得支持环境和可靠信息是无价的，但全球许多酷儿青少年无法获得这种支持。然而，由于ChatGPT等大型语言模型（LLM）的快速采用，情况可能很快发生变化。本文旨在全面探讨LLM改变酷儿情感支持的潜力。为此，我们对LLM与酷儿相关内容的互动进行定性和定量分析。为了评估回应质量，我们开发了一个受心理标准和专家意见启发的新颖十个问题量表。我们将该量表应用于评分几个LLM和人类评论，这些评论是酷儿青少年寻求建议和分享时发表的。

    arXiv:2402.11886v1 Announce Type: cross  Abstract: Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT. This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of LLM's interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share 
    
[^91]: M2K-VDG: 模型自适应多模态知识锚增强视频对话生成

    M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation

    [https://arxiv.org/abs/2402.11875](https://arxiv.org/abs/2402.11875)

    通过分析不同VDG模型的幻觉现象和锚标记的差异，本文提出了M2K-VDG框架，用于自适应增强多模态知识锚以减少幻觉，并在实验中表现出优越性。

    

    视频对话生成（VDG）要求系统基于多模态知识生成流畅准确的答案。然而，在实践中，多模态知识利用的困难给VDG模型带来了严重的幻觉。尽管先前的研究以各种方式缓解了幻觉，但它们很少注意多模态知识锚答案标记的重要性。在本文中，我们通过困惑度揭示了不同VDG模型经历不同幻觉并展示不同的锚标记。基于这一观察，我们提出了M2K-VDG，一种模型自适应的多模态知识锚增强框架用于减少幻觉。此外，我们引入了反事实效应以实现更准确的锚标记检测。三个流行基准测试的实验结果展示了我们方法优于最先进方法的优越性，证明了它在减少幻觉中的有效性。

    arXiv:2402.11875v1 Announce Type: new  Abstract: Video-grounded dialogue generation (VDG) requires the system to generate a fluent and accurate answer based on multimodal knowledge. However, the difficulty in multimodal knowledge utilization brings serious hallucinations to VDG models in practice. Although previous works mitigate the hallucination in a variety of ways, they hardly take notice of the importance of the multimodal knowledge anchor answer tokens. In this paper, we reveal via perplexity that different VDG models experience varying hallucinations and exhibit diverse anchor tokens. Based on this observation, we propose M2K-VDG, a model-adaptive multimodal knowledge anchor enhancement framework for hallucination reduction. Furthermore, we introduce the counterfactual effect for more accurate anchor token detection. The experimental results on three popular benchmarks exhibit the superiority of our approach over state-of-the-art methods, demonstrating its effectiveness in reduc
    
[^92]: 大型语言模型推理解释的可解释性有多高？

    How Interpretable are Reasoning Explanations from Prompting Large Language Models?

    [https://arxiv.org/abs/2402.11863](https://arxiv.org/abs/2402.11863)

    对大型语言模型推理解释提出了全面、多角度的评估，包括忠实度、强健性和效用，并引入了新的可解释性指标。

    

    Prompt Engineering已经引起了人们的极大关注，可以增强大型语言模型在多项任务中的性能。Chain-of-Thought等技术不仅增强了任务性能，还描绘了清晰的推理步骤轨迹，为观众提供了一种有形的解释形式。我们对可解释性进行了全面多角度的评估，不仅考虑了忠实度，还考虑了在多个常识推理基准测试中的强健性和效用。此外，我们引入了一个简单的可解释性指标。

    arXiv:2402.11863v1 Announce Type: new  Abstract: Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability ali
    
[^93]: 模块化网络用于少样本仇恨模因检测

    Modularized Networks for Few-shot Hateful Meme Detection

    [https://arxiv.org/abs/2402.11845](https://arxiv.org/abs/2402.11845)

    这种模块化网络利用LoRA模块增强了大型语言模型在少样本环境下进行仇恨模因检测的泛化能力。

    

    在本文中，我们解决了在低资源环境中检测仇恨模因的挑战，该环境下只有少量标记示例可用。我们的方法利用了广泛使用的参数高效调整技术低秩适应（LoRA）的组合性。我们首先在选择的与仇恨模因检测相关的任务上利用LoRA微调大型语言模型（LLMs），从而生成一套LoRA模块。这些模块具有对于仇恨模因检测的基本推理能力。然后，我们使用少量可用的标记样本来训练一个模块组合器，它根据模块的相关性分配权重给LoRA模块。模型的可学习参数与LoRA模块的数量成正比。这种由LLMs支撑并且增强了LoRA模块的模块化网络，在仇恨模因检测的情境中表现出增强的泛化能力。我们的评估涵盖了三个设计的数据集。

    arXiv:2402.11845v1 Announce Type: new  Abstract: In this paper, we address the challenge of detecting hateful memes in the low-resource setting where only a few labeled examples are available. Our approach leverages the compositionality of Low-rank adaptation (LoRA), a widely used parameter-efficient tuning technique. We commence by fine-tuning large language models (LLMs) with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules. These modules are capable of essential reasoning skills for hateful meme detection. We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designe
    
[^94]: 询问最佳问题：将大型语言模型与检索器偏好在会话搜索中对齐

    Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search

    [https://arxiv.org/abs/2402.11827](https://arxiv.org/abs/2402.11827)

    提出了RetPO框架，通过优化语言模型对搜索查询进行重构，以符合目标检索系统的偏好，并构建了一个大型数据集RF Collection，用于收集检索结果作为检索器的偏好。

    

    会话式搜索与单轮检索任务不同，需要理解对话上下文中的当前问题。常见的“重写-然后检索”的方法旨在将问题去上下文化，使其对现成的检索器自给自足，但大多数现有方法由于能力有限而产生次优的查询重写，无法充分利用来自检索结果的信号。为了克服这一限制，我们提出了一种新颖的框架RetPO（检索器偏好优化），旨在优化语言模型（LM）以符合目标检索系统的重写搜索查询的偏好。该过程始于提示大型LM生成各种潜在重写，然后收集这些重写的检索性能作为检索器的偏好。通过该过程，我们构建了一个名为RF塑集的大型数据集，其中包含对超过410K个查询的检索器反馈。

    arXiv:2402.11827v1 Announce Type: cross  Abstract: Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K quer
    
[^95]: 大型语言模型对图形召回的微结构和准确性

    Microstructures and Accuracy of Graph Recall by Large Language Models

    [https://arxiv.org/abs/2402.11821](https://arxiv.org/abs/2402.11821)

    本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。

    

    图形数据对许多应用至关重要，其中很多数据以文本格式描述关系。因此，准确地召回和编码先前文本中描述的图形是大型语言模型(LLMs)需要展示的基本但关键能力，以执行涉及图形结构信息的推理任务。人类在图形召回方面的表现已被认知科学家研究了几十年，发现其经常呈现与人类处理社会关系一致的某些结构性偏见模式。然而，迄今为止，我们很少了解LLMs在类似图形召回任务中的行为：它们召回的图形是否也呈现某些偏见模式，如果是，它们与人类的表现有何不同并如何影响其他图形推理任务？在这项研究中，我们进行了第一次对LLMs进行图形召回的系统研究，研究其准确性和偏见微结构（局部结构）。

    arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
    
[^96]: 大语言模型的适用于头部共享注意力

    Head-wise Shareable Attention for Large Language Models

    [https://arxiv.org/abs/2402.11819](https://arxiv.org/abs/2402.11819)

    本文提出了面向大语言模型的头部共享注意力的观点，提出了两种在注意力头之间共享参数的内存高效方法，以解决大型语言模型参数数量巨大导致部署受限的问题。

    

    大型语言模型(LLMs)由于参数数量巨大受到限制，这限制了它们在边缘设备上的部署。参数共享是一种有利的解决方案，鼓励权重重用，有效地减少内存使用量并降低性能下降。然而，当前的参数共享技术主要专注于像BERT这样的小规模模型，并采用粗粒度的共享规则，例如逐层共享。鉴于LLMs的普及，这变得有限，并且共享整个层或块显然降低了参数共享的灵活性。在本文中，我们提出了$\textbf{面向大语言模型的头部共享注意力}$的观点。我们进一步提出了两种在注意力头之间共享参数的内存高效方法，特别关注LLMs。它们都使用相同的动态策略选择共享的参数矩阵。第一种方法直接重复使用预训练权重而无需重新训练。

    arXiv:2402.11819v1 Announce Type: new  Abstract: Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices. Weight sharing is one promising solution that encourages weight reuse, effectively reducing memory usage with less performance drop. However, current weight sharing techniques primarily focus on small-scale models like BERT and employ coarse-grained sharing rules, e.g., layer-wise. This becomes limiting given the prevalence of LLMs and sharing an entire layer or block obviously diminishes the flexibility of weight sharing. In this paper, we present a perspective on $\textit{$\textbf{head-wise shareable attention for large language models}$}$. We further propose two memory-efficient methods that share parameters across attention heads, with a specific focus on LLMs. Both of them use the same dynamic strategy to select the shared weight matrices. The first method directly reuses the pre-trained weights without retraining, d
    
[^97]: 在真正重要的地方：低资源语言的少样本环境保护媒体监测

    Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages

    [https://arxiv.org/abs/2402.11818](https://arxiv.org/abs/2402.11818)

    提出了一种名为NewsSerow的方法，用于自动识别低资源语言中的环境保护内容，并且在尼泊尔语中使用少于10个示例新闻文章时，NewsSerow显着优于其他少样本模型。

    

    环保组织常规监测有可能对环境产生影响的保护区媒体内容，以保持对可能发展的情况的认识。现有的自动化媒体监测系统需要由领域专家标记的大量数据，这在英语等高资源语言的规模上才是可行的。然而，这样的工具在全球南方最需要，在那里感兴趣的新闻主要是用本地低资源语言发布的，可持续地对数据集进行注释的专家也很少。在本文中，我们提出了一种NewsSerow方法，用于自动识别低资源语言中的环境保护内容。NewsSerow是一个利用大型语言模型（LLMs）的总结、上下文少样本分类和自我反思的流程。在尼泊尔语中使用最多10个新闻示例文章，NewsSerow明显优于其他少样本模型。

    arXiv:2402.11818v1 Announce Type: cross  Abstract: Environmental conservation organizations routinely monitor news content on conservation in protected areas to maintain situational awareness of developments that can have an environmental impact. Existing automated media monitoring systems require large amounts of data labeled by domain experts, which is only feasible at scale for high-resource languages like English. However, such tools are most needed in the global south where news of interest is mainly in local low-resource languages, and far fewer experts are available to annotate datasets sustainably. In this paper, we propose NewsSerow, a method to automatically recognize environmental conservation content in low-resource languages. NewsSerow is a pipeline of summarization, in-context few-shot classification, and self-reflection using large language models (LLMs). Using at most 10 demonstration example news articles in Nepali, NewsSerow significantly outperforms other few-shot me
    
[^98]: HU在SemEval-2024任务8A中的表现：对比学习能否学习嵌入以检测机器生成的文本？

    HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?

    [https://arxiv.org/abs/2402.11815](https://arxiv.org/abs/2402.11815)

    提出了一种基于对比学习的单一模型，用较少的参数实现与基线相当的机器生成文本检测性能

    

    这篇论文描述了我们为SemEval-2024任务8“多生成器、多领域和多语言黑匣子机器生成文本检测”开发的系统。由于大型语言模型（LLM）在虚假文本生成、网络钓鱼、考试作弊甚至抄袭版权材料中的使用，机器生成文本一直是主要关注的问题之一。许多系统已经被开发用于检测机器生成的文本。然而，这些系统中的大部分依赖于文本生成模型，这是一个在实际场景中不切实际的限制，因为通常不可能知道用户用于文本生成的具体模型。在这项工作中，我们提出了基于对比学习的单一模型，其使用基线参数的大约40%（149M比355M），但在测试数据集上表现出了可比的性能（在137个参与者中排名第21）。我们的关键发现是，即使没有多个模型的集成，

    arXiv:2402.11815v1 Announce Type: cross  Abstract: This paper describes our system developed for SemEval-2024 Task 8, "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection." Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a
    
[^99]: FIPO：基于自由形式指导的提示优化与偏好数据集和模块化微调模式

    FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema

    [https://arxiv.org/abs/2402.11811](https://arxiv.org/abs/2402.11811)

    FIPO提出了基于自由形式指导的提示优化方法，结合偏好数据集和模块化微调模式，重新构思了优化过程并实现了灵活的任务提示生成。

    

    在促进大语言模型在最终用户-机器人交互中的深度智能方面，提示创作的艺术被视为普通用户的一项关键但复杂的任务。与之前基于模型而不考虑指导的自动提示优化方法形成对比，这些方法为预定义目标模型产生了光滑的结果，但在使用开箱即用模型时容易快速退化，我们提出了基于自由形式指导的提示优化（FIPO）。这种方法得到我们的大规模提示偏好数据集的支持，并采用模块化微调模式。FIPO模式重新构思了优化过程，将其分解为可管理的模块，以动态调整内容的元提示为锚点。这允许灵活整合原始任务指导、可选指导响应和可选真实值，以生成经过精心优化的任务提示。

    arXiv:2402.11811v1 Announce Type: new  Abstract: In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user. Contrast to previous model-oriented yet instruction-agnostic Automatic Prompt Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented Prompt Optimization (FIPO). This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema. The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta prompt that dynamically adapts content. This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task prompts. The FIPO preference
    
[^100]: 智能并行自动纠错解码：加速大型语言模型推理

    Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding

    [https://arxiv.org/abs/2402.11809](https://arxiv.org/abs/2402.11809)

    提出了Smart Parallel Auto-Correct Decoding (SPACE)方法，通过集成半自回归推理和猜测解码，实现了大型语言模型推理加速和并行生成验证令牌的功能。

    

    这项研究旨在加速拥有数十亿参数的大型语言模型（LLMs）的推理速度。我们提出了“智能并行自动纠错解码”（SPACE），这是一种创新方法，旨在实现LLMs的无损加速。通过集成半自回归推理和猜测解码能力，SPACE独特地使自回归LLMs能够并行生成和验证令牌。这是通过专门的半自回归监督微调过程实现的，该过程使现有LLMs具有同时预测多个令牌的能力。此外，一种自动纠错解码算法促进了单个模型调用内令牌序列的同时生成和验证。通过在一系列LLMs上进行广泛实验证明，SPACE在HumanEval-X上表现出2.7倍至4.0倍的推理加速。

    arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
    
[^101]: LLM作为提示器：在任意知识图上进行低资源归纳推理

    LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs

    [https://arxiv.org/abs/2402.11804](https://arxiv.org/abs/2402.11804)

    本文利用大型语言模型（LLMs）生成图形结构提示，以增强预训练的图神经网络（GNNs），提出一种新的方法论见解，实现了在任意知识图上进行低资源归纳推理的高通用性。

    

    知识图（KG）归纳推理旨在推断训练期间未见过的新KG中缺失的事实，在各种应用中被广泛采用。KG归纳推理的一个关键挑战是处理在文本和结构方面都稀缺的低资源场景。本文尝试利用大型语言模型（LLMs）来解决这一挑战。具体来说，我们利用最先进的LLMs生成图形结构提示，以增强预训练的图神经网络（GNNs），从而为KG归纳推理方法带来新的方法论见解，以及在实践中具有很高的普适性。在方法论方面，我们引入了一种新颖的预训练和提示框架ProLINK，旨在在任意KG上进行低资源归纳推理，而无需额外训练。在实践方面，我们在36个低资源数据集上对我们的方法进行了实验评估。

    arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res
    
[^102]: 揭示魔法：探究检索增强生成中的注意力精炼

    Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation

    [https://arxiv.org/abs/2402.11794](https://arxiv.org/abs/2402.11794)

    本文揭示了检索增强生成中的注意力精炼的成功机制，并提出了优化模型训练方法的指标.

    

    检索增强生成框架可以通过实时知识更新来解决大型语言模型的局限，以实现更准确的答案。在检索增强模型的训练阶段中，一种高效的方法是注意力精炼，它使用注意力分数作为监督信号，而不是手动注释的查询文档对。尽管注意力精炼越来越受欢迎，但在它成功背后的详细机制仍未被探索，特别是它利用以受益于训练的具体模式。在本文中，我们通过对注意力精炼工作流程的全面回顾，识别影响检索增强语言模型学习质量的关键因素来填补这一空白。我们进一步提出了优化模型训练方法和避免低效训练的指标。

    arXiv:2402.11794v1 Announce Type: new  Abstract: Retrieval-augmented generation framework can address the limitations of large language models by enabling real-time knowledge updates for more accurate answers. An efficient way in the training phase of retrieval-augmented models is attention distillation, which uses attention scores as a supervision signal instead of manually annotated query-document pairs. Despite its growing popularity, the detailed mechanisms behind the success of attention distillation remain unexplored, particularly the specific patterns it leverages to benefit training. In this paper, we address this gap by conducting a comprehensive review of attention distillation workflow and identifying key factors influencing the learning quality of retrieval-augmented language models. We further propose indicators for optimizing models' training methods and avoiding ineffective training.
    
[^103]: 语言模型认为哪些证据令人信服？

    What Evidence Do Language Models Find Convincing?

    [https://arxiv.org/abs/2402.11782](https://arxiv.org/abs/2402.11782)

    通过构建 ConflictingQA 数据集，并进行敏感性和反事实分析，研究发现当前语言模型在预测时很大程度上依赖于网站与查询的相关性，而忽视了人类认为重要的文本风格特征。

    

    检索增强型语言模型越来越多地被赋予主观、有争议和矛盾的查询任务，如“阿斯巴甜是否与癌症有关”。为了解决这些模糊的查询，我们必须搜索大量网站，并考虑“我认为哪些证据是令人信服的？”。在这项工作中，我们研究了语言模型是如何回答这个问题的。特别是，我们构建了一个名为 ConflictingQA 的数据集，将有争议的查询与一系列包含不同事实（如定量结果）、论证风格（如权威呼声）和答案（是或否）的真实世界证据文档配对。我们使用这个数据集进行敏感性和反事实分析，探讨哪些文本特征最影响语言模型的预测。总体而言，我们发现当前模型在很大程度上依赖网站与查询的相关性，而在很大程度上忽视了人类认为重要的风格特征，比如文本是否是

    arXiv:2402.11782v1 Announce Type: new  Abstract: Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as "is aspartame linked to cancer". To resolve these ambiguous queries, one must search through a large range of websites and consider "which, if any, of this evidence do I find convincing?". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text 
    
[^104]: 在语言模型嵌入中揭示潜在的人类福祉

    Uncovering Latent Human Wellbeing in Language Model Embeddings

    [https://arxiv.org/abs/2402.11777](https://arxiv.org/abs/2402.11777)

    本研究通过ETHICS Utilitarianism任务发现，预训练语言模型的表示隐含了对人类福祉的理解，且模型规模增加时，准确率呈非下降趋势。

    

    语言模型是否隐含地学习了人类福祉的概念？我们通过ETHICS功利主义任务进行探讨，评估缩放是否增强了预训练模型的表示。我们的初步发现显示，无需任何提示工程或微调，OpenAI的text-embedding-ada-002的主成分达到73.9%的准确率。这与在整个ETHICS数据集上微调的BERT-large模型的74.6%准确率非常接近，表明预训练传达了对人类福祉的某种理解。接下来，我们考虑了四种语言模型系列，观察功利主义准确率随参数增加而变化。我们发现，使用足够数量的主成分时，性能随模型规模的增加而非减少。

    arXiv:2402.11777v1 Announce Type: cross  Abstract: Do language models implicitly learn a concept of human wellbeing? We explore this through the ETHICS Utilitarianism task, assessing if scaling enhances pretrained models' representations. Our initial finding reveals that, without any prompt engineering or finetuning, the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting pretraining conveys some understanding about human wellbeing. Next, we consider four language model families, observing how Utilitarianism accuracy varies with increased parameters. We find performance is nondecreasing with increased model size when using sufficient numbers of principal components.
    
[^105]: 面向少样本生成的内容相关问答对话结构化思维启发

    Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations

    [https://arxiv.org/abs/2402.11770](https://arxiv.org/abs/2402.11770)

    使用结构化思维链提示的方法，在少样本情况下生成内容相关的问答对话，提高了代理程序对基础文档的忠诚度，训练强大的对话问答代理。

    

    我们引入了一种结构化思维链（SCoT）提示方法，使用预训练的大型语言模型（LLM）生成基于内容的多轮问答对话。我们的提议的核心是将复杂任务结构化分解为状态机中的多个状态，以便执行对应于各种子任务（例如内容阅读和话语生成）的动作。每个状态利用一组独特的资源，包括提示和（可选）额外工具以增强生成过程。我们的实验结果表明，对于幻觉减轻，使用具有指定状态的SCoT提示可以使对接地文档的代理忠诚度提高高达16.8％。当用作训练数据时，仅从6个基于维基百科的种子示范合成的开放域对话训练出强大的对话问答代理程序；在领域外评估中，

    arXiv:2402.11770v1 Announce Type: new  Abstract: We introduce a structured chain-of-thought (SCoT) prompting approach to generating content-grounded multi-turn question-answer conversations using a pre-trained large language model (LLM). At the core of our proposal is a structured breakdown of the complex task into a number of states in a state machine, so that actions corresponding to various subtasks, e.g., content reading and utterance generation, can be executed in their own dedicated states. Each state leverages a unique set of resources including prompts and (optionally) additional tools to augment the generation process. Our experimental results show that SCoT prompting with designated states for hallucination mitigation increases agent faithfulness to grounding documents by up to 16.8%. When used as training data, our open-domain conversations synthesized from only 6 Wikipedia-based seed demonstrations train strong conversational QA agents; in out-of-domain evaluation, for exam
    
[^106]: 基于ChatGPT的数据增强技术用于改善LLMs的参数高效去偏见化

    ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs

    [https://arxiv.org/abs/2402.11764](https://arxiv.org/abs/2402.11764)

    本研究提出了一种利用ChatGPT生成合成训练数据来增强LLMs去偏见化的新方法，能够高效地去除已知偏见并跨越不同类别进行去偏见化。

    

    大语言模型（LLMs）虽然功能强大，但存在有害的社会偏见。由于计算成本、数据约束和可能降低多任务语言能力，去偏见化通常具有挑战性。本文介绍了一种利用ChatGPT生成合成训练数据的新方法，旨在增强LLMs的去偏见化。我们提出了两种策略：目标提示，对已知偏见提供有效的去偏见化，但需要事先指定问题中的偏见; 一般提示，虽然效果稍逊，但能够跨各种类别进行去偏见化。我们利用适配器调整来实现资源高效的LLM去偏见化，并比较了我们的合成数据与现有去偏见化数据集的效果。我们的结果表明：（1）ChatGPT可以高效地生成用于去偏见化其他LLMs的高质量训练数据；（2）通过我们的方法生成的数据超越了现有数据集在去偏见化上的效果。

    arXiv:2402.11764v1 Announce Type: cross  Abstract: Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debias
    
[^107]: 大型语言模型用于词干提取：承诺、风险和失败

    Large Language Models for Stemming: Promises, Pitfalls and Failures

    [https://arxiv.org/abs/2402.11757](https://arxiv.org/abs/2402.11757)

    本文研究了使用大型语言模型来进行词干提取的有前途的想法，提出了三种不同的方法，每种方法在计算成本、有效性和稳健性方面具有不同的权衡。

    

    文本词干提取是一种自然语言处理技术，用于将单词缩减为其基本形式，也称为根形式。传统的词干提取方法仅关注单个术语，忽视了上下文信息的丰富性。因此，在本文中，我们研究了利用大型语言模型（LLMs）通过利用其上下文理解能力来提取词干的有前途的想法。在这方面，我们确定了三种不同的权衡方案：（1）使用LLMs对集合的词汇进行提取，即出现在集合中的唯一单词集合（词汇提取），（2）将LLMs用于单独词根提取 (上下文提取)，(3) 使用LLMs从每个文档中提取

    arXiv:2402.11757v1 Announce Type: cross  Abstract: Text stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form. The use of stemming in IR has been shown to often improve the effectiveness of keyword-matching models such as BM25. However, traditional stemming methods, focusing solely on individual terms, overlook the richness of contextual information. Recognizing this gap, in this paper, we investigate the promising idea of using large language models (LLMs) to stem words by leveraging its capability of context understanding. With this respect, we identify three avenues, each characterised by different trade-offs in terms of computational cost, effectiveness and robustness : (1) use LLMs to stem the vocabulary for a collection, i.e., the set of unique words that appear in the collection (vocabulary stemming), (2) use LLMs to stem each document separately (contextual stemming), and (3) use LLMs to extract from eac
    
[^108]: MARS：用于生成式LLMs中不确定性估计的意义感知响应评分

    MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs

    [https://arxiv.org/abs/2402.11756](https://arxiv.org/abs/2402.11756)

    MARS提出了一种新的评分函数MARS，考虑了生成序列中每个标记的语义贡献，该方法改进了生成式LLMs中的不确定性估计性能。

    

    生成式大型语言模型（LLMs）因在各种任务中的卓越表现而被广泛利用。然而，它们产生不准确或误导性输出的倾向可能带来潜在风险，尤其是在高风险环境中。因此，估计生成式LLM输出的正确性是增强可靠性的重要任务。生成式LLMs中的不确定性估计（UE）是一个不断发展的领域，其中SOTA基于概率的方法通常采用长度标准化评分。在这项工作中，我们提出了一种名为意义感知响应评分（MARS）的替代长度标准化评分的UE方法。MARS是一种考虑在问题的上下文中生成序列中每个标记的语义贡献的新型评分函数。我们证明将MARS整合到UE方法中会在UE性能上带来普遍和显著的改进。我们使用三种不同的闭卷式问答来进行实验

    arXiv:2402.11756v1 Announce Type: new  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book questi
    
[^109]: SPML: 一种用于防御语言模型受到提示攻击的领域特定语言

    SPML: A DSL for Defending Language Models Against Prompt Attacks

    [https://arxiv.org/abs/2402.11755](https://arxiv.org/abs/2402.11755)

    SPML是一种用于优化提示并监控基于大型语言模型聊天机器人输入的领域特定语言，用于防御恶意攻击并优化成本。

    

    大型语言模型（LLMs）已深刻改变了自然语言应用，越来越多地依赖于基于指令的定义来设计聊天机器人。然而，部署后，聊天机器人的定义是固定的，并且容易受到恶意用户的攻击，突出了防止不道德应用和财务损失的需要。现有研究探讨了用户提示对基于LLM的聊天机器人的影响，但尚未探索包含应用特定聊天机器人的攻击的实用方法。本文提出了系统提示元语言（SPML），这是一种用于优化提示并监控基于LLM的聊天机器人输入的领域特定语言。SPML主动检查攻击提示，确保用户输入与聊天机器人定义相符，防止在LLM主干上对其进行恶意执行，优化成本。它也通过编程语言能力简化了聊天机器人定义的制作，克服了自然语言的限制。

    arXiv:2402.11755v1 Announce Type: cross  Abstract: Large language models (LLMs) have profoundly transformed natural language applications, with a growing reliance on instruction-based definitions for designing chatbots. However, post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users, emphasizing the need to prevent unethical applications and financial losses. Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored. This paper presents System Prompt Meta Language (SPML), a domain-specific language for refining prompts and monitoring the inputs to the LLM-based chatbots. SPML actively checks attack prompts, ensuring user inputs align with chatbot definitions to prevent malicious execution on the LLM backbone, optimizing costs. It also streamlines chatbot definition crafting with programming language capabilities, overcoming natural language 
    
[^110]: ArtPrompt: 基于ASCII艺术的对齐LLMs越狱攻击

    ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs

    [https://arxiv.org/abs/2402.11753](https://arxiv.org/abs/2402.11753)

    提出了一种新颖的基于ASCII艺术的越狱攻击，以及一个用于评估LLMs在识别非纯语义提示方面能力的基准挑战。五个SOTA LLMs在识别ASCII艺术提示时存在困难。

    

    安全对于大型语言模型（LLMs）的使用至关重要。已经开发了多种技术，如数据过滤和监督微调，以加强LLMs的安全性。然而，当前已知的技术假设用于对齐LLMs安全性的语料库仅由语义进行解释。然而，这一假设在现实应用中不成立，导致LLMs存在严重漏洞。本文提出了一种新颖的基于ASCII艺术的越狱攻击，并引入了一个全面的基准Vision-in-Text Challenge（ViTC）来评估LLMs在识别不能仅通过语义进行解释的提示的能力。我们展示了五个SOTA LLMs（GPT-3.5、GPT-4、Gemini、Claude和Llama2）在识别以ASCII艺术形式提供的提示方面存在困难。基于这一观察，我们开发了

    arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
    
[^111]: 通过影响分析进行上下文学习演示选择

    In-Context Learning Demonstration Selection via Influence Analysis

    [https://arxiv.org/abs/2402.11750](https://arxiv.org/abs/2402.11750)

    通过分析训练样本的影响，提出一种名为InfICL的演示选择方法，可以帮助提升上下文学习的泛化性能。

    

    大型语言模型（LLM）展示了其具有上下文学习（ICL）能力，这提供了进行少样本学习的机会，而无需任何梯度更新。尽管具有多重好处，ICL的泛化性能对所选演示敏感。选择用于ICL的有效演示仍然是一个开放的研究挑战。为了解决这一挑战，我们提出了一种名为InfICL的演示选择方法，该方法通过影响函数分析训练样本的影响。鉴别高度有影响力的训练样本可能有助于提升ICL的泛化性能。为了限制InfICL的运行成本，我们仅利用LLM生成样本嵌入，并不执行任何昂贵的微调。我们在多个真实世界数据集上进行实证研究，并展示了我们的InfICL相对于最先进基线方法的优点。

    arXiv:2402.11750v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated their In-Context Learning (ICL) capabilities which provides an opportunity to perform few shot learning without any gradient update. Despite its multiple benefits, ICL generalization performance is sensitive to the selected demonstrations. Selecting effective demonstrations for ICL is still an open research challenge. To address this challenge, we propose a demonstration selection method called InfICL which analyzes influences of training samples through influence functions. Identifying highly influential training samples can potentially aid in uplifting the ICL generalization performance. To limit the running cost of InfICL, we only employ the LLM to generate sample embeddings, and don't perform any costly fine tuning. We perform empirical study on multiple real-world datasets and show merits of our InfICL against state-of-the-art baselines.
    
[^112]: 语言模型就是霍默·辛普森！通过任务算法对精调语言模型进行安全重新定位

    Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic

    [https://arxiv.org/abs/2402.11746](https://arxiv.org/abs/2402.11746)

    提出了一种简单方法RESTA，通过任务算法对精调语言模型进行安全重新定位，有效降低了其有害程度。

    

    经过微调的对齐语言模型往往会导致安全性受损，为了解决这一问题，我们提出了一种简单的方法RESTA，该方法执行LLM安全重新定位。RESTA代表通过任务算法恢复安全。在其核心思想中，它涉及将一个安全向量简单地加到受损模型的权重上。我们展示了RESTA在参数高效和完全微调中的有效性，涵盖了广泛的下游任务，包括中文、英文和印地文的指令跟随，以及代码和数学的问题解决能力。我们还展示了RESTA在三个现有安全评估基准和作为本项工作一部分提出的一个多语言基准数据集上的通用性，该数据集包括550个有害问题，涵盖11个类别，每个类别下包含5个有害的子类别。总的来说，RESTA降低了受损模型的有害程度。

    arXiv:2402.11746v1 Announce Type: cross  Abstract: Aligned language models face a significant limitation as their fine-tuning often results in compromised safety. To tackle this, we propose a simple method RESTA that performs LLM safety realignment. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised 
    
[^113]: 机器生成文本定位

    Machine-generated Text Localization

    [https://arxiv.org/abs/2402.11744](https://arxiv.org/abs/2402.11744)

    该论文首次深入研究了机器生成文本定位，针对文档中机器生成部分的定位，通过细粒度的方法提出了解决整个文档MGT检测失败情况的新途径。

    

    机器生成文本（MGT）检测旨在识别一段文本是机器写作还是人类写作。先前的工作主要将MGT构建为对整个文档的二元分类任务，对文档中仅部分内容为机器生成的情况进行的研究有限。本文首次深入研究了定位文档中机器生成部分的MGT。因此，如果恶意行为者更改新闻文章的关键部分以传播错误信息，整个文档的MGT检测可能会失败，因为绝大部分是人类写作，但我们的方法由于其细粒度的方法可以成功。我们的MGT定位任务面临的一个关键挑战是，短跨度的文本，例如一个句子，由于长度较短几乎不提供指示其是否机器生成的信息。为了解决这个问题，我们利用上下文信息，预测多个句子是机器生成还是人类写作。

    arXiv:2402.11744v1 Announce Type: new  Abstract: Machine-Generated Text (MGT) detection aims to identify a piece of text as machine or human written. Prior work has primarily formulated MGT as a binary classification task over an entire document, with limited work exploring cases where only part of a document is machine generated. This paper provides the first in-depth study of MGT that localizes the portions of a document that were machine generated. Thus, if a bad actor were to change a key portion of a news article to spread misinformation, whole document MGT detection may fail since the vast majority is human written, but our approach can succeed due to its granular approach. A key challenge in our MGT localization task is that short spans of text, e.g., a single sentence, provides little information indicating if it is machine generated due to its short length. To address this, we leverage contextual information, where we predict whether multiple sentences are machine or human wri
    
[^114]: 金融领域的数字化索赔检测：一个新的金融数据集、弱监督模型和市场分析

    Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis

    [https://arxiv.org/abs/2402.11728](https://arxiv.org/abs/2402.11728)

    本研究探讨了分析师报告和盈利电话中的索赔对金融市场回报的影响，并构建了一个新的金融数据集用于索赔检测任务。提出了一种融入主题专家知识的新型弱监督模型，通过构建一种新的度量“乐观主义”展示了模型的实际效用。

    

    在本文中，我们研究了分析师报告和盈利电话中的索赔对金融市场回报的影响，将它们视为上市公司重要的季度事件。为了进行全面的分析，我们构建了一个新的金融数据集，用于金融领域的索赔检测任务。我们在该数据集上对各种语言模型进行了基准测试，并提出了一种融入主题专家（SMEs）知识的新型弱监督模型，在聚合函数中超越了现有方法。此外，我们通过构建一种新的度量“乐观主义”展示了我们提出的模型的实际效用。我们还观察到盈利惊喜和回报对我们的乐观主义度量的依赖。我们的数据集、模型和代码将在GitHub和Hugging Face上公开（遵循CC BY 4.0许可）。

    arXiv:2402.11728v1 Announce Type: new  Abstract: In this paper, we investigate the influence of claims in analyst reports and earnings calls on financial market returns, considering them as significant quarterly events for publicly traded companies. To facilitate a comprehensive analysis, we construct a new financial dataset for the claim detection task in the financial domain. We benchmark various language models on this dataset and propose a novel weak-supervision model that incorporates the knowledge of subject matter experts (SMEs) in the aggregation function, outperforming existing approaches. Furthermore, we demonstrate the practical utility of our proposed model by constructing a novel measure ``optimism". Furthermore, we observed the dependence of earnings surprise and return on our optimism measure. Our dataset, models, and code will be made publicly (under CC BY 4.0 license) available on GitHub and Hugging Face.
    
[^115]: 大型语言模型对意识形态操纵的易感性有多高？

    How Susceptible are Large Language Models to Ideological Manipulation?

    [https://arxiv.org/abs/2402.11725](https://arxiv.org/abs/2402.11725)

    大型语言模型展示出令人担忧的易受意识形态操纵的脆弱性，对极少量意识形态驱动样本的暴露就能显著改变其意识形态，并且能够从一个主题吸收意识形态并泛化到其他不相关主题上。

    

    大型语言模型(LLMs)具有对公众观念和信息互动施加重要影响的潜力。这引发了关于如果这些模型内的意识形态易受操纵可能带来社会影响的担忧。在这项工作中，我们研究了LLMs在学习和泛化意识形态偏见方面的效果。我们的发现揭示了一个令人担忧的脆弱性：仅接触到少量意识形态驱动的样本就会显著改变LLMs的意识形态。值得注意的是，LLMs表现出惊人的能力，能够从一个主题吸收意识形态并将其泛化到甚至不相关的主题上。LLMs的意识形态容易被扭曲的事实强调了恶意行为者故意毒害训练数据或数据注释者无意引入偏见所带来的风险。这也强调了采取强有力措施以减轻这些威胁的迫切性。

    arXiv:2402.11725v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the inf
    
[^116]: 塑造人工智能协作：在与语言模型共同撰写中使用不同支撑级别

    Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models

    [https://arxiv.org/abs/2402.11723](https://arxiv.org/abs/2402.11723)

    本研究探讨了大型语言模型提供的不同支撑级别如何影响共同撰写过程，发现高支撑可以显著改善写作质量和生产率，特别有利于非常规写作者和不太精通技术的用户。

    

    语言建模的进展为新颖的人工智能与人类共同撰写体验铺平了道路。本文探讨了大型语言模型（LLMs）提供的不同支撑级别如何塑造共同撰写的过程。通过拉丁方设计的被试内实验，我们邀请了131名参与者回应三个随机顺序条件下的论证写作提示：无AI辅助（对照组），下一句建议（低支撑），和下一段建议（高支撑）。我们的发现揭示了支撑对写作质量和生产率（单位时间词数）的U形影响。虽然低支撑并未显著提高写作质量或生产率，但高支撑导致了显著改善，尤其有利于非常规写作者和不太精通技术的用户。在使用支撑式写作工具时并未观察到显著的认知负担，但文字量略有下降。

    arXiv:2402.11723v1 Announce Type: cross  Abstract: Advances in language modeling have paved the way for novel human-AI co-writing experiences. This paper explores how varying levels of scaffolding from large language models (LLMs) shape the co-writing process. Employing a within-subjects field experiment with a Latin square design, we asked participants (N=131) to respond to argumentative writing prompts under three randomly sequenced conditions: no AI assistance (control), next-sentence suggestions (low scaffolding), and next-paragraph suggestions (high scaffolding). Our findings reveal a U-shaped impact of scaffolding on writing quality and productivity (words/time). While low scaffolding did not significantly improve writing quality or productivity, high scaffolding led to significant improvements, especially benefiting non-regular writers and less tech-savvy users. No significant cognitive burden was observed while using the scaffolded writing tools, but a moderate decrease in text
    
[^117]: 使用基于LLM的代理模拟政治联盟谈判

    Modelling Political Coalition Negotiations Using LLM-based Agents

    [https://arxiv.org/abs/2402.11712](https://arxiv.org/abs/2402.11712)

    本文引入联盟谈判作为一项新颖的自然语言处理任务，建模为基于大型语言模型代理之间的谈判，并提出了一个多语种数据集POLCA以及用于模拟政治谈判过程的分层马尔可夫决策过程。

    

    联盟谈判是议会民主制度的基石，以政党间复杂互动和战略沟通为特征。尽管具有重要意义，但由于缺乏合适数据，对这些谈判的建模在自然语言处理（NLP）领域中尚未得到探索。本文将联盟谈判作为一项新颖的NLP任务引入，并将其建模为基于大型语言模型代理之间的谈判。我们引入了一个多语种数据集POLCA，包括欧洲政党宣言和这些国家若干选举中的联盟协议。该数据集通过提供多样化的真实基础模拟解决了当前政治谈判建模的范围限制挑战。此外，我们提出了一个设计用于模拟政治联盟谈判过程的分层马尔可夫决策过程。

    arXiv:2402.11712v1 Announce Type: new  Abstract: Coalition negotiations are a cornerstone of parliamentary democracies, characterised by complex interactions and strategic communications among political parties. Despite its significance, the modelling of these negotiations has remained unexplored with the domain of Natural Language Processing (NLP), mostly due to lack of proper data. In this paper, we introduce coalition negotiations as a novel NLP task, and model it as a negotiation between large language model-based agents. We introduce a multilingual dataset, POLCA, comprising manifestos of European political parties and coalition agreements over a number of elections in these countries. This dataset addresses the challenge of the current scope limitations in political negotiation modelling by providing a diverse, real-world basis for simulation. Additionally, we propose a hierarchical Markov decision process designed to simulate the process of coalition negotiation between politica
    
[^118]: MORL-Prompt: 离散提示优化的多目标强化学习的实证分析

    MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization

    [https://arxiv.org/abs/2402.11711](https://arxiv.org/abs/2402.11711)

    本研究将多目标优化技术应用于基于强化学习的离散提示优化，为解决奖励平衡问题提供了新视角。

    

    基于RL的技术可以用于搜索提示，将其输入目标语言模型以最大化一组用户指定的奖励函数。然而，在许多目标应用中，自然奖励函数彼此之间存在紧张关系--例如，在风格转移任务中，内容保留与风格匹配之间存在矛盾。当前技术侧重于最大化奖励函数的平均值，这未必会导致取得各种奖励平衡的提示--这个问题在多目标和鲁棒优化文献中得到了深入研究。本文将几种多目标优化技术调整为基于RL的离散提示优化--其中有两种考虑帕累托奖励面积的方法，另外一种选择有益于所有奖励的更新方向。我们在两个NLP任务上对这些方法进行了实证分析：风格转移和机器翻译。

    arXiv:2402.11711v1 Announce Type: new  Abstract: RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete prompt optimization -- two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously. We conduct an empirical analysis of these methods on two NLP tasks: style transfer and machine translation, each
    
[^119]: 关于完善偏见的注解

    A Note on Bias to Complete

    [https://arxiv.org/abs/2402.11710](https://arxiv.org/abs/2402.11710)

    在动态环境中，重新审视偏见的定义，提出新的偏见类型，并建立包括假设、策略和方法的框架来减少社会偏见。

    

    减少社会偏见可以加强社会联系，促进共同理解和更好的决策。我们重新审视了对偏见的定义，发现了动态环境中的新偏见类型（例如社会地位），并将它们相对于文化、地区、时间和个人背景描述出来。我们的框架包括八个关于偏见的假设，以及每个假设的减少偏见策略以及在LLM中提出的五种方法作为解决方案。这个框架的实现尚未完成。

    arXiv:2402.11710v1 Announce Type: new  Abstract: Minimizing social bias strengthens societal bonds, promoting shared understanding and better decision-making. We revisit the definition of bias by discovering new bias types (e.g., societal status) in dynamic environments and describe them relative to context, such as culture, region, time, and personal background. Our framework includes eight hypotheses about bias and a minimizing bias strategy for each assumption as well as five methods as proposed solutions in LLM. The realization of the framework is yet to be completed.
    
[^120]: GNNavi：通过图神经网络导航大型语言模型中的信息流

    GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network

    [https://arxiv.org/abs/2402.11709](https://arxiv.org/abs/2402.11709)

    GNNavi通过引入基于提示的参数高效微调（PEFT）方法和图神经网络（GNN）层，准确引导信息流的汇聚和分布，在大型语言模型中导航信息流动态，超越了标准提示式微调的性能。

    

    大型语言模型（LLMs）在接收示范输入时表现出强大的上下文学习能力（ICL）。然而，微调仍然至关重要以进一步增强其适应性。基于提示的微调方法在数据稀缺情况下证明是有效的微调方法，但对计算资源的高需求限制了其实用性。我们通过引入基于提示的参数高效微调（PEFT）方法来解决这个问题。GNNavi利用了有关ICL信息流动态的见解，表明标签词在提示中作为信息传播的锚点。GNNavi利用图神经网络（GNN）层精确地引导信息流的汇聚和分布，在处理提示时将期望的信息流硬编码到GNN中。我们在使用GPT-2和Llama2进行文本分类任务的实验中发现，GNNavi超越了标准提示式微调的性能。

    arXiv:2402.11709v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-ba
    
[^121]: 为什么要举得那么沉？通过修剪层来减轻大型语言模型

    Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers

    [https://arxiv.org/abs/2402.11700](https://arxiv.org/abs/2402.11700)

    减少大型语言模型的层数可在不损失性能的情况下减轻模型规模，甚至在某些情况下只有一个层的模型可以超越完全层式的对应项。

    

    大型语言模型(LLMs)在处理各种自然语言处理(NLP)任务方面具有出色的能力。然而，这些模型的巨大规模在存储、训练和推理方面带来挑战，因为它们通过层叠包含了数十亿个参数。尽管传统方法如模型修剪或蒸馏为减小模型大小提供了途径，但往往会以性能保留为代价。在我们的调查中，我们系统地探讨了通过减少LLMs中的层数来减少模型规模的方法。令人惊讶的是，我们观察到，即使层数较少，LLMs在特别是基于提示的文本分类任务的微调中也能保持类似或更好的性能水平。值得注意的是，在某些情况下，只有一个层的模型可以胜过完全层式的对应项。这些发现为未来旨在减轻LLMs大小约束的工作提供了宝贵的见解。

    arXiv:2402.11700v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess outstanding capabilities in addressing various natural language processing (NLP) tasks. However, the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking. While traditional approaches such as model pruning or distillation offer ways for reducing model size, they often come at the expense of performance retention. In our investigation, we systematically explore the approach of reducing the number of layers in LLMs. Surprisingly, we observe that even with fewer layers, LLMs maintain similar or better performance levels, particularly in prompt-based fine-tuning for text classification tasks. Remarkably, in certain cases, models with a single layer outperform their fully layered counterparts. These findings offer valuable insights for future work aimed at mitigating the size constraints of LLMs while p
    
[^122]: Vision-Flan：扩展视觉指导调整中的人类标记任务

    Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning

    [https://arxiv.org/abs/2402.11690](https://arxiv.org/abs/2402.11690)

    构建了最多样化的视觉指导调整数据集Vision-Flan，提出了两阶段指导调整框架，显著优于传统方法

    

    尽管视觉-语言模型（VLMs）作为多功能视觉助手具有显著的能力，但现有VLM框架中仍存在两个重大挑战：（1）在预训练和视觉指导调整中缺乏任务多样性，以及（2）在GPT-4合成指导调整数据中存在注释错误和偏见。这两个挑战导致问题，如泛化能力差、幻觉和灾难性遗忘。为解决这些挑战，我们构建了Vision-Flan，这是迄今为止最多样化的公开可用视觉指导调整数据集，包括187个多样化任务和从学术数据集中提取的1,664,261个实例，每个任务都附带专家撰写的指导。此外，我们提出了一个两阶段指导调整框架，其中VLM首先在Vision-Flan上进行微调，然后在GPT-4合成数据上进一步进行调整。我们发现这种两阶段调整框架明显优于传统方法。

    arXiv:2402.11690v1 Announce Type: new  Abstract: Despite vision-language models' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional 
    
[^123]: 利用GPT4V合成数据实现轻量级视觉-语言模型ALLaVA

    ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model

    [https://arxiv.org/abs/2402.11684](https://arxiv.org/abs/2402.11684)

    通过采用GPT-4V合成的高质量训练数据，研究成功地实现了ALLaVA，一个轻量级视觉-语言模型，该模型在12个基准测试上表现出与最多3B LVLMs竞争性能。

    

    最近，大型视觉-语言模型(LVLMs)的发展使语言模型能够处理多模态输入，但部署时需要大量计算资源，尤其是在边缘设备上。本研究旨在通过采用高质量的训练数据来弥合传统尺度LVLMs和资源友好型Lite版本之间的性能差距。为此，通过利用GPT-4V生成详细描述、复杂推理指令和图片详细答案的能力创建了一个合成数据集。利用我们的数据训练的结果模型ALLaVA在12项基准测试上取得了与最多3B LVLMs竞争性能。这项工作突出了在设计更高效的LVLMs中采用高质量数据的可行性。我们的在线演示可在\url{https://allava.freedomai.cn}上获得。

    arXiv:2402.11684v1 Announce Type: cross  Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have enabled processing of multimodal inputs in language models but require significant computational resources for deployment, especially in edge devices. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To do this, a synthetic dataset is created by leveraging GPT-4V's ability to generate detailed captions, complex reasoning instructions and detailed answers from images. The resulted model trained with our data, ALLaVA, achieves competitive performance on 12 benchmarks up to 3B LVLMs. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. Our online demo is available at \url{https://allava.freedomai.cn}.
    
[^124]: 一种支配所有的提示：LLMs 用于观点摘要评估

    One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation

    [https://arxiv.org/abs/2402.11683](https://arxiv.org/abs/2402.11683)

    通过释放涵盖观点摘要评估相关七个维度的新数据集SUMMEVAL-OP，研究人员提出了 Op-I-Prompt 作为一种独立于维度的提示方法，以及 Op-Prompts 作为一组依赖于维度的提示，可以表明 Op-I-Prompt 是评估观点摘要的一个很好的替代方案，实现了平均 Spearman 相关性为 0.70。

    

    使用传统基于参考的度量对观点摘要进行评估很少提供全面的评估，并且已经显示出与人类判断的相关性相对较低。最近的研究表明，使用大型语言模型（LLMs）作为无参考度量的NLG评估，然而，它们在观点摘要评估方面尚未被探索。此外，有限的观点摘要评估数据集阻碍了进展。为了解决这个问题，我们发布了涵盖与观点摘要评估相关的7个维度的SUMMEVAL-OP数据集：流畅性、连贯性、相关性、忠实度、方面覆盖、情感一致性和特异性。我们研究了 Op-I-Prompt，一个独立于维度的提示，以及 Op-Prompts，一个依赖于维度的用于观点摘要评估的提示集。实验证明，Op-I-Prompt 是评估观点摘要的一个很好的替代方案，实现了平均 Spearman 相关性为 0.70。

    arXiv:2402.11683v1 Announce Type: new  Abstract: Evaluation of opinion summaries using conventional reference-based metrics rarely provides a holistic evaluation and has been shown to have a relatively low correlation with human judgments. Recent studies suggest using Large Language Models (LLMs) as reference-free metrics for NLG evaluation, however, they remain unexplored for opinion summary evaluation. Moreover, limited opinion summary evaluation datasets inhibit progress. To address this, we release the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a dimension-independent prompt, and Op-Prompts, a dimension-dependent set of prompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries achieving an average Spearman correlation of 0.70 w
    
[^125]: 打开语言习得的黑匣子

    Opening the black box of language acquisition

    [https://arxiv.org/abs/2402.11681](https://arxiv.org/abs/2402.11681)

    提出了一种基于序列记忆和分块的最小认知架构，采用强化学习原则，能够学习人造语言并提取支持学习的语法信息。

    

    最近深度学习技术在大型语言模型方面取得了重大进展，重新激发了人们对语言如何从数据中学习的兴趣。然而，目前尚不清楚这些模型是否以及如何表示所学语言的语法信息。此外，这些模型在使用之前必须在大型语料库上进行预训练。在本研究中，我们提出了一种替代性、更透明且认知合理的学习语言架构。我们的方法不是使用深度学习，而是基于序列记忆和分块的最小认知架构。学习机制基于强化学习原则。我们在多种类似自然语言的玩具语言上测试了我们的架构。结果显示，该模型能够从零开始学习这些人造语言，并提取支持学习的语法信息。我们的研究证明了这种简单架构的强大能力，并强调了其重要性。

    arXiv:2402.11681v1 Announce Type: new  Abstract: Recent advances in large language models using deep learning techniques have renewed interest on how languages can be learned from data. However, it is unclear whether or how these models represent grammatical information from the learned languages. In addition, the models must be pre-trained on large corpora before they can be used. In this work, we propose an alternative, more transparent and cognitively plausible architecture for learning language. Instead of using deep learning, our approach uses a minimal cognitive architecture based on sequence memory and chunking. The learning mechanism is based on the principles of reinforcement learning. We test our architecture on a number of natural-like toy languages. Results show that the model can learn these artificial languages from scratch and extract grammatical information that supports learning. Our study demonstrates the power of this simple architecture and stresses the importance o
    
[^126]: 使用大型语言模型进行反叙事评估的多方面框架

    A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models

    [https://arxiv.org/abs/2402.11676](https://arxiv.org/abs/2402.11676)

    提出了一个多方面框架，使用大型语言模型评估反叙事，通过5个方面从专门 NGO 指南中提取定义的内容，以解决以往评估方法的局限性。

    

    反叙事是对仇恨言论背景的知情回应，旨在驳斥仇恨主张并化解冲突，已成为一种有效的仇恨言论干预策略。先前的工作提出了自动生成反叙事的方法来辅助手动干预，但这些方法的评估仍未得到充分发展。先前用于反叙事评估的自动度量标准缺乏与人类判断的一致性，因为它们依赖于表面参考比较，而不是将反叙事质量的关键方面纳入评估标准。为解决先前的评估局限性，我们提出了一个新颖的评估框架，促使LLM提供生成的反叙事候选的得分和反馈，使用了来自专门NGO的反叙事指南中提取的5个定义的方面。

    arXiv:2402.11676v1 Announce Type: cross  Abstract: Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy. While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped. Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria. To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs. We found that LLM evaluators achieve strong alignment to human-annotated scores and feedback and
    
[^127]: 爱沙尼亚文本的自动校正：EKTB25项目的最终报告

    Autocorrect for Estonian texts: final report from project EKTB25

    [https://arxiv.org/abs/2402.11671](https://arxiv.org/abs/2402.11671)

    该项目成功开发了爱沙尼亚语言的拼写和语法校正工具，主要创新在于使用迁移学习和自动评估来克服可用数据不足的挑战。

    

    该项目由爱沙尼亚语言技术国家计划于2021-2023年资助，旨在为爱沙尼亚语开发拼写和语法校正工具。主要挑战是缺乏所需用于此类开发的可用错误校正数据量非常小。为了缓解这一问题，(1) 我们为模型训练和测试注释了更多的校正数据，(2) 我们测试了迁移学习，即重新训练为其他任务创建的机器学习模型，以便不仅依赖于校正数据，(3) 我们对比了开发的方法和模型与其他选择，包括大型语言模型。我们还开发了自动评估，可以通过错误类别计算修正的准确性和收益，从而可以详细比较不同方法的有效性。

    arXiv:2402.11671v1 Announce Type: cross  Abstract: The project was funded in 2021-2023 by the National Programme of Estonian Language Technology. Its main aim was to develop spelling and grammar correction tools for the Estonian language. The main challenge was the very small amount of available error correction data needed for such development. To mitigate this, (1) we annotated more correction data for model training and testing, (2) we tested transfer-learning, i.e. retraining machine learning models created for other tasks, so as not to depend solely on correction data, (3) we compared the developed method and model with alternatives, including large language models. We also developed automatic evaluation, which can calculate the accuracy and yield of corrections by error category, so that the effectiveness of different methods can be compared in detail.   There has been a breakthrough in large language models during the project: GPT4, a commercial language model with Estonian-lang
    
[^128]: 机制之争：追踪语言模型处理事实和虚拟语境的方式

    Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals

    [https://arxiv.org/abs/2402.11655](https://arxiv.org/abs/2402.11655)

    本研究提出了机制之争的概念，关注语言模型中多个机制的相互作用，并揭示了它们之间的竞争过程，以及影响某些机制强度的注意力位置。

    

    可解释性研究旨在弥合大型语言模型（LLMs）的经验成功和我们对内部机制的科学理解之间的差距。本研究提出了机制之争的形式，其不再关注单个机制，而是关注多个机制之间的相互作用，并追踪其中一个在最终预测中如何成为主导因素。我们利用logit检验和注意力修改两种可解释性方法揭示了机制之争在LLMs中的发生方式和位置。我们的发现显示了机制及其在各种模型组件中的竞争痕迹，并揭示了有效控制特定机制强度的注意力位置。我们的代码和数据位于https://github.com/francescortu/Competition_of

    arXiv:2402.11655v1 Announce Type: new  Abstract: Interpretability research aims to bridge the gap between the empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research in this area focused on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose the formulation of competition of mechanisms, which instead of individual mechanisms focuses on the interplay of multiple mechanisms, and traces how one of them becomes dominant in the final prediction. We uncover how and where the competition of mechanisms happens within LLMs using two interpretability methods, logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components, and reveal attention positions that effectively control the strength of certain mechanisms. Our code and data are at https://github.com/francescortu/Competition_of
    
[^129]: 从失败中学习：在对大型语言模型进行微调时整合负面示例

    Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents

    [https://arxiv.org/abs/2402.11651](https://arxiv.org/abs/2402.11651)

    大型语言模型通过整合负面示例和适当的数据清洗与微调策略，从失败中学习，提高作为代理的效果。

    

    大型语言模型(LLMs)在充当与环境进行交互的工具（如搜索引擎）时取得了成功。然而，LLMs在训练或对齐过程中并未专门针对工具使用进行优化，限制了它们作为代理的效果。为解决这一问题，之前的研究已经收集了GPT-4与环境之间的交互轨迹，并用它们对较小的模型进行微调。作为这一过程的一部分，标准方法通常是简单地丢弃未成功完成任务的轨迹，这一方面导致了数据和资源的显著浪费，另一方面有可能限制微调过程中的优化路径。本文认为大型语言模型可以通过适当的数据清洗和微调策略从失败中学习。我们在数学推理、多跳问题回答和战略方面进行了实验。

    arXiv:2402.11651v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines. However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning. In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies. We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic 
    
[^130]: 绊脚石：在攻击下对机器生成文本检测器的鲁棒性进行压力测试

    Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks

    [https://arxiv.org/abs/2402.11638](https://arxiv.org/abs/2402.11638)

    本研究压力测试了机器生成文本检测器在恶意攻击下的鲁棒性，实验证明几乎所有现有检测器在各种攻击下都表现不稳定，平均性能下降35%，并提出了改善鲁棒性的初步解决方案。

    

    大型语言模型（LLMs）的广泛使用增加了检测机器生成文本以防止滥用的需求。我们研究的目标是在现实场景下对检测器对恶意攻击的鲁棒性进行压力测试。我们全面研究了流行的机器生成文本检测器在不同攻击类型下的鲁棒性：编辑、改写、提示和共生成。我们的攻击假设对生成LLMs的访问受限，并比较了不同预算水平下检测器在不同攻击下的性能。我们的实验发现几乎没有现有检测器在所有攻击下保持稳健，所有检测器都展示出不同的漏洞。平均所有检测器，在所有攻击下性能下降了35%。此外，我们调查了这些缺陷背后的原因，并提出了初步的即插即用补丁来提高鲁棒性。

    arXiv:2402.11638v1 Announce Type: new  Abstract: The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors' robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating. Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches to improve robustness.
    
[^131]: 利用自我播种和多意图自我指导的LLM生成意图感知的信息检索对话

    Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs

    [https://arxiv.org/abs/2402.11633](https://arxiv.org/abs/2402.11633)

    本论文提出了SOLID模型，利用自我播种和多意图自我指导方案来实现LLMs生成意图感知的信息检索对话。

    

    识别信息检索对话中用户意图对于系统满足用户信息需求至关重要。意图预测（IP）具有挑战性，并需要充分的与人工标注意图对话用于训练。然而，手动注释意图资源密集。虽然大型语言模型（LLMs）已被证明在生成合成数据方面非常有效，但尚无研究使用LLMs生成意图感知的信息检索对话。本文的研究重点是利用LLMs进行零-shot生成大规模、开放领域和意图感知的信息检索对话。我们提出了SOLID，其中包括新颖的自我播种和多意图自我指导方案。前者通过利用LLM自身的知识范围来启动对话生成来提高生成质量；后者促使LLM按顺序生成话语，并通过要求LLM自动完成话题设计来减轻手动话题设计的需要。

    arXiv:2402.11633v1 Announce Type: new  Abstract: Identifying user intents in information-seeking dialogs is crucial for a system to meet user's information needs. Intent prediction (IP) is challenging and demands sufficient dialogs with human-labeled intents for training. However, manually annotating intents is resource-intensive. While large language models (LLMs) have been shown to be effective in generating synthetic data, there is no study on using LLMs to generate intent-aware information-seeking dialogs. In this paper, we focus on leveraging LLMs for zero-shot generation of large-scale, open-domain, and intent-aware information-seeking dialogs. We propose SOLID, which has novel self-seeding and multi-intent self-instructing schemes. The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to auton
    
[^132]: 元认知检索增强型大型语言模型

    Metacognitive Retrieval-Augmented Large Language Models

    [https://arxiv.org/abs/2402.11626](https://arxiv.org/abs/2402.11626)

    本文介绍了MetaRAG，一种结合了检索增强生成过程与元认知的方法，通过元认知调节流程，使模型具有监视、评估和规划其响应策略的能力，增强了其内省推理能力。

    

    由于其在生成事实内容方面的高效性，检索增强生成已经成为自然语言处理中的核心。 传统方法使用单次检索，而最近更倾向于多次检索以执行多跳推理任务。 然而，这些策略受到预定义推理步骤的限制，可能导致响应生成的不准确性。 本文介绍了MetaRAG，一种结合了检索增强生成过程与元认知的方法。 借鉴认知心理学，元认知使实体能够自我反思并批判性评估其认知过程。 通过整合这一点，MetaRAG使模型能够监视、评估和规划其响应策略，增强其内省推理能力。 通过三步元认知调节流程，模型能够识别初始认知响应中的不足之处，并加以修正。

    arXiv:2402.11626v1 Announce Type: new  Abstract: Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes t
    
[^133]: SpeCrawler：使用大型语言模型从API文档生成OpenAPI规范

    SpeCrawler: Generating OpenAPI Specifications from API Documentation Using Large Language Models

    [https://arxiv.org/abs/2402.11625](https://arxiv.org/abs/2402.11625)

    SpeCrawler利用大型语言模型从不同API文档生成OpenAPI规范，有助于简化API集成流程并促进工具整合到语言模型中。

    

    在数字时代，API的广泛使用是显而易见的。但由于在线API文档中观察到的结构差异，对API的可扩展利用构成了挑战。这凸显了需要自动化工具来促进API的使用。一种可行的方法是将文档转换成API规范格式。尽管先前尝试使用基于规则的方法，但这些方法在跨不同文档中普遍化方面遇到了困难。本文介绍了SpeCrawler，这是一个全面的系统，利用大型语言模型(LLMs)通过精心设计的流程从不同的API文档生成OpenAPI规范。通过为众多API创建一个标准化格式，SpeCrawler有助于简化API编排系统中的集成流程，并促进将工具整合到LLMs中。本文探讨了SpeCrawler的方法论。

    arXiv:2402.11625v1 Announce Type: new  Abstract: In the digital era, the widespread use of APIs is evident. However, scalable utilization of APIs poses a challenge due to structure divergence observed in online API documentation. This underscores the need for automatic tools to facilitate API consumption. A viable approach involves the conversion of documentation into an API Specification format. While previous attempts have been made using rule-based methods, these approaches encountered difficulties in generalizing across diverse documentation. In this paper we introduce SpeCrawler, a comprehensive system that utilizes large language models (LLMs) to generate OpenAPI Specifications from diverse API documentation through a carefully crafted pipeline. By creating a standardized format for numerous APIs, SpeCrawler aids in streamlining integration processes within API orchestrating systems and facilitating the incorporation of tools into LLMs. The paper explores SpeCrawler's methodology
    
[^134]: 逻辑闭环：揭示大型视觉-语言模型中的对象幻觉

    Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models

    [https://arxiv.org/abs/2402.11622](https://arxiv.org/abs/2402.11622)

    提出了一种基于逻辑闭环的框架（LogicCheckGPT），利用大型视觉-语言模型本身来检测和减轻对象幻觉。

    

    对象幻觉一直是阻碍大型视觉-语言模型（LVLMs）更广泛应用的软肋。对象幻觉是指LVLMs在图像中声称不存在的对象的现象。为了减轻对象幻觉，已经提出了指导调整和基于外部模型的检测方法，这两种方法要么需要大规模的计算资源，要么依赖于外部模型的检测结果。然而，仍然存在一个未深入探讨的领域，即利用LVLM本身来减轻对象幻觉。在这项工作中，我们采用了这样的直觉，即LVLM倾向于对存在的对象做出逻辑一致的反应，但对幻觉对象做出不一致的反应。因此，我们提出了基于逻辑闭环的对象幻觉检测和减轻框架，即LogicCheckGPT。具体来说，我们设计了逻辑一致性探测来提出具有逻辑性的问题。

    arXiv:2402.11622v1 Announce Type: cross  Abstract: Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical corr
    
[^135]: 解码新闻叙事：对大型语言模型在框架偏见检测中的关键分析

    Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection

    [https://arxiv.org/abs/2402.11621](https://arxiv.org/abs/2402.11621)

    通过研究GPT-3.5 Turbo、GPT-4和Flan-T5模型在识别新闻标题中框架偏见的性能，发现可解释提示能够显著提高这些模型的可靠性，GPT-4在少射场景中表现较好，而FLAN-T5的表现较差，指出较小模型可能需要更多任务特定微调。

    

    这项工作通过检验GPT-3.5 Turbo、GPT-4和Flan-T5模型在通过零射、少射和可解释提示方法检测新闻标题中框架偏见的表现，为LLMs在社会科学中的适用性不断扩展的研究做出贡献。我们评估的一个关键洞察是，可解释提示在提升这些模型可靠性方面表现出显著效果，凸显了解释设置对于社会科学关于框架偏见的研究的重要性。特别是，GPT-4在提供一系列相关领域内例子时，表现出改进的少射情况下的性能。FLAN-T5的表现不佳表明较小的模型可能需要额外的任务特定微调以识别框架偏见检测。我们的研究还发现模型，特别是GPT-4，经常将情绪语言误解为框架偏见的指标，突显了区分的挑战。

    arXiv:2402.11621v1 Announce Type: new  Abstract: This work contributes to the expanding research on the applicability of LLMs in social sciences by examining the performance of GPT-3.5 Turbo, GPT-4, and Flan-T5 models in detecting framing bias in news headlines through zero-shot, few-shot, and explainable prompting methods. A key insight from our evaluation is the notable efficacy of explainable prompting in enhancing the reliability of these models, highlighting the importance of explainable settings for social science research on framing bias. GPT-4, in particular, demonstrated enhanced performance in few-shot scenarios when presented with a range of relevant, in-domain examples. FLAN-T5's poor performance indicates that smaller models may require additional task-specific fine-tuning for identifying framing bias detection. Our study also found that models, particularly GPT-4, often misinterpret emotional language as an indicator of framing bias, underscoring the challenge of distingu
    
[^136]: 指标学习编码模型识别BERT表示中的语言特征处理特征

    Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations

    [https://arxiv.org/abs/2402.11608](https://arxiv.org/abs/2402.11608)

    应用指标学习编码模型（MLEMs）于BERT表示，发现语言特征在不同层中有序分离，神经表示层级组织，中间层解耦，优于其他解码方法。

    

    我们介绍了指标学习编码模型（MLEMs）作为一种理解神经系统如何表示其处理对象的理论特征的新方法。作为概念验证，我们将MLEMs应用于从BERT中提取的神经表示，并跟踪各种语言特征（例如时态、主语人称、从句类型、从句嵌套等）。我们发现：（1）语言特征是有序的：它们在不同层中以不同程度将句子的表示分开；（2）神经表示是分层组织的：在某些层中，我们发现表示的群集嵌套在更大的群集内部，遵循逐渐重要的语言特征；（3）语言特征在中间层中是解耦的：不同的、选择性单位由不同的语言特征激活。在方法论上，MLEMs（4）优于多变量解码方法，更具抗类型-I错误的鲁棒性，（5）优于单变量

    arXiv:2402.11608v1 Announce Type: new  Abstract: We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to understand how neural systems represent the theoretical features of the objects they process. As a proof-of-concept, we apply MLEMs to neural representations extracted from BERT, and track a wide variety of linguistic features (e.g., tense, subject person, clause type, clause embedding). We find that: (1) linguistic features are ordered: they separate representations of sentences to different degrees in different layers; (2) neural representations are organized hierarchically: in some layers, we find clusters of representations nested within larger clusters, following successively important linguistic features; (3) linguistic features are disentangled in middle layers: distinct, selective units are activated by distinct linguistic features. Methodologically, MLEMs are superior (4) to multivariate decoding methods, being more robust to type-I errors, and (5) to univ
    
[^137]: 多任务推断: 大型语言模型能够同时遵循多个指令吗？

    Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?

    [https://arxiv.org/abs/2402.11597](https://arxiv.org/abs/2402.11597)

    大型语言模型在多任务推断时表现出更高的性能，相比单任务推断平均推断时间减少1.46倍，并且在MTI Bench上显示出最多高达12.4%的性能改善。

    

    大型语言模型（LLMs）通常被要求在每次推断调用中遵循单个指令。在这项工作中，我们分析了LLMs是否也具有处理多个指令的能力，称为多任务推断。为此，我们引入了MTI Bench（多任务推断基准），一个包括25个任务的5000个实例的综合评估基准。MTI Bench中的每个任务都涉及2到3个子任务。正如预期的那样，我们首先证明了多任务推断平均降低了1.46倍的总推断时间，因为它不需要多次推断调用。有趣的是，与预期LLMs在任务被划分时表现更好相反，我们发现最先进的LLMs，例如Llama-2-Chat-70B和GPT-4，在MTI Bench上通过多任务推断与单任务推断相比可以获得高达7.3％和12.4％的性能改善。我们发布了MTI Bench。

    arXiv:2402.11597v1 Announce Type: new  Abstract: Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench
    
[^138]: 可扩展嵌入：LLM上下文长度的灵活多功能器

    Extensible Embedding: A Flexible Multipler For LLM's Context Length

    [https://arxiv.org/abs/2402.11577](https://arxiv.org/abs/2402.11577)

    提出了可扩展嵌入方法，实现了对LLM上下文的高质量扩展，具有强大的灵活性和成本效益。

    

    大语言模型（LLMs）需要扩展上下文以处理许多关键应用，但现有方法存在昂贵的成本和较差的上下文扩展质量。在本文中，我们提出了可扩展嵌入，实现了对LLM上下文的高质量扩展，具有强大的灵活性和成本效益。可扩展嵌入是一种 typica1令牌嵌入的增强，表示了一个可扩展范围的上下文信息，而不是单个标记。通过利用这种信息密度更高的紧凑输入单元，LLM即使在较小的上下文窗口中也可以访问广泛的上下文范围。可扩展嵌入在体系结构和训练方法上得到了系统优化，带来了多重优势。

    arXiv:2402.11577v1 Announce Type: new  Abstract: Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we propose Extensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which e
    
[^139]: 大视觉-语言模型中的视觉上下文学习

    Visual In-Context Learning for Large Vision-Language Models

    [https://arxiv.org/abs/2402.11574](https://arxiv.org/abs/2402.11574)

    提出了一种新的Visual In-Context Learning（VICL）方法，通过检索和重新排名图像、用任务意图和任务特定的视觉解析总结图像，以及组成语言演示来减少标记计数和减轻跨模态交互问题。

    

    在大视觉语言模型（LVLMs）中，上下文学习（ICL）的有效性仍受到跨模态交互和表示差异的挑战的限制。为了克服这些挑战，我们引入了一种新颖的Visual In-Context Learning（VICL）方法，包括Visual Demonstration Retrieval、Intent-Oriented Image Summarization和Intent-Oriented Demonstration Composition。我们的方法通过“检索与重新排名”范式检索图像，用任务意图和任务特定的视觉解析总结图像，并组成基于语言的演示，减少标记计数并缓解跨模态交互问题。对五个可视推理数据集的实验评估证明了我们方法的有效性。此外，我们广泛的实验利用信息流分析阐明了我们方法的有效性，并研究了演示的长度和位置对LVLM的影响。

    arXiv:2402.11574v1 Announce Type: cross  Abstract: In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning (ICL) remains limited by challenges in cross-modal interactions and representation disparities. To overcome these challenges, we introduce a novel Visual In-Context Learning (VICL) method comprising Visual Demonstration Retrieval, Intent-Oriented Image Summarization, and Intent-Oriented Demonstration Composition. Our approach retrieves images via ''Retrieval & Rerank'' paradigm, summarises images with task intent and task-specific visual parsing, and composes language-based demonstrations that reduce token count and alleviate cross-modal interaction problem. Experimental evaluations on five visual reasoning datasets demonstrate the effectiveness of our method. Moreover, our extensive experiments leverage information flow analysis to elucidate the effectiveness of our method, and investigate the impact of length and position of demonstrations for LVLM. The use 
    
[^140]: BGE地标嵌入：一种用于检索增强的长上下文大型语言模型的无块嵌入方法

    BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models

    [https://arxiv.org/abs/2402.11573](https://arxiv.org/abs/2402.11573)

    可扩展嵌入方法提高了大型语言模型（LLM）的上下文扩展质量和成本效益，通过在架构和训练方法上进行系统优化，实现了上下文范围的灵活扩展和高效的训练样本效率。

    

    大型语言模型需要扩展上下文以处理许多关键应用，然而现有方法往往成本高昂且上下文扩展质量较低。在本文中，我们提出了可扩展嵌入，实现了具有强大灵活性和成本效益的LLM上下文的高质量扩展。可扩展嵌入作为典型令牌嵌入的增强，代表了可扩展范围上下文的信息，而不是单个令牌。通过利用这种信息密度更高的紧凑输入单元，LLM即使在小上下文窗口下也能访问广泛的上下文范围。可扩展嵌入在架构和训练方法上进行了系统优化，具有多个优势。1) 高度灵活的上下文扩展，灵活支持各种上下文长度的即时扩展。2) 强大的训练样本效率，使得...

    arXiv:2402.11573v1 Announce Type: new  Abstract: Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we proposeExtensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which en
    
[^141]: 无参考图像字幕评估中的眼镜蛇效应

    Cobra Effect in Reference-Free Image Captioning Metrics

    [https://arxiv.org/abs/2402.11572](https://arxiv.org/abs/2402.11572)

    本论文研究了在无参考图像字幕评估中存在的眼镜蛇效应，利用度量得分作为奖励来指导生成与度量标准一致的描述。

    

    评估文本描述和相应图像之间的兼容性代表了多模态研究中的一个核心工作。近年来，利用视觉语言预训练模型（VLMs）的无参考方法大量涌现。实证证据证实，这些创新方法与人类判断的相关性更高，标志着该领域的重大进展。然而，仅高相关性是否足以完全表示度量标准的全面性？为了回答这个问题，在本文中，我们研究了无参考度量标准是否存在任何不足之处。具体来说，受到眼镜蛇效应的启发，我们使用度量得分作为奖励，指导字幕模型生成与度量标准密切一致的描述。如果某个度量标准有缺陷，模型将利用这一点，并体现在生成的句子中。

    arXiv:2402.11572v1 Announce Type: new  Abstract: Evaluating the compatibility between textual descriptions and corresponding images represents a core endeavor within multi-modal research. In recent years, a proliferation of reference-free methods, leveraging visual-language pre-trained models (VLMs), has emerged. Empirical evidence has substantiated that these innovative approaches exhibit a higher correlation with human judgment, marking a significant advancement in the field. However, does a higher correlation with human evaluations alone sufficiently denote the complete of a metric? In response to this question, in this paper, we study if there are any deficiencies in reference-free metrics. Specifically, inspired by the Cobra Effect, we utilize metric scores as rewards to direct the captioning model toward generating descriptions that closely align with the metric's criteria. If a certain metric has flaws, it will be exploited by the model and reflected in the generated sentences. 
    
[^142]: 不调皮 —— 使用LLMs生成与台式机器人Haru对话中富有表现力的机器人行为

    Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru

    [https://arxiv.org/abs/2402.11571](https://arxiv.org/abs/2402.11571)

    将大型语言模型集成到社交机器人中，实现更动态和富有表现力的对话，包括使用情绪识别模型，调整语调，并利用表情符号生成机器人动作。

    

    社交机器人旨在通过引人入胜的对话与人类建立长期关系。然而，传统的对话方法依赖于脚本化互动，往往无法保持引人入胜的对话。本文通过将大型语言模型（LLMs）集成到社交机器人中，实现更动态和富有表现力的对话，从而解决了这一局限性。我们介绍了一个完全自动化的对话系统，利用LLMs生成具有表现力行为的机器人响应，与机器人个性一致。我们将机器人行为与两种形式结合起来：1）一个能够具备各种语调风格的文本转语音（TTS）引擎，以及2）机器人的一系列物理动作库。我们开发了一个自定义的最新情绪识别模型，动态选择机器人的语调，并利用LLM输出中的表情符号作为生成机器人动作的线索。我们的系统演示可在此处查看。

    arXiv:2402.11571v1 Announce Type: cross  Abstract: Social robots aim to establish long-term bonds with humans through engaging conversation. However, traditional conversational approaches, reliant on scripted interactions, often fall short in maintaining engaging conversations. This paper addresses this limitation by integrating large language models (LLMs) into social robots to achieve more dynamic and expressive conversations. We introduce a fully-automated conversation system that leverages LLMs to generate robot responses with expressive behaviors, congruent with the robot's personality. We incorporate robot behavior with two modalities: 1) a text-to-speech (TTS) engine capable of various delivery styles, and 2) a library of physical actions for the robot. We develop a custom, state-of-the-art emotion recognition model to dynamically select the robot's tone of voice and utilize emojis from LLM output as cues for generating robot actions. A demo of our system is available here. To i
    
[^143]: 使用Haru开发自主机器人介导的行为辅导会话

    Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru

    [https://arxiv.org/abs/2402.11569](https://arxiv.org/abs/2402.11569)

    通过Haru开发了一个完全自主的对话系统，最大限度地发挥了其情感表达和独特人格，成功应用于行为改变辅导，取得了显著的成效。

    

    这项研究探讨了在人机交互中为行为改变辅导设计和影响自主对话的经验调查。我们关注使用桌面社交机器人Haru，并探索实施微习惯方法来促进积极行为改变。我们研究的核心是开发一个完全自主的对话系统，最大限度地发挥Haru的情感表达和独特人格。我们的方法涉及对对话系统的迭代设计和广泛测试，确保它有效地体现了微习惯方法的原则，同时还融合了建立信任和破坏信任的策略。最终版本对话的有效性在人类参与者（N=12）的实验研究中进行了评估。结果显示Haru的生气、互动性和中立性的认知显著提高。

    arXiv:2402.11569v1 Announce Type: cross  Abstract: This study presents an empirical investigation into the design and impact of autonomous dialogues in human-robot interaction for behavior change coaching. We focus on the use of Haru, a tabletop social robot, and explore the implementation of the Tiny Habits method for fostering positive behavior change. The core of our study lies in developing a fully autonomous dialogue system that maximizes Haru's emotional expressiveness and unique personality. Our methodology involved iterative design and extensive testing of the dialogue system, ensuring it effectively embodied the principles of the Tiny Habits method while also incorporating strategies for trust-raising and trust-dampening. The effectiveness of the final version of the dialogue was evaluated in an experimental study with human participants (N=12). The results indicated a significant improvement in perceptions of Haru's liveliness, interactivity, and neutrality. Additionally, our
    
[^144]: LongAgent: 通过多智能体协作将语言模型扩展到128K上下文

    LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration

    [https://arxiv.org/abs/2402.11550](https://arxiv.org/abs/2402.11550)

    LongAgent通过多智能体协作将语言模型扩展到128K上下文，并在长文本处理方面表现出潜在的优越性。

    

    大型语言模型（LLMs）在理解语言和执行复杂推理任务方面表现出色。然而，具有长上下文窗口的LLMs以其昂贵的训练成本和高推理延迟而臭名昭著。即使是最先进的模型如GPT-4和Claude2在处理超过$100k$标记的输入时也经常出错，这种现象也被称为\textit{中间迷失}。在本文中，我们提出了基于多智能体协作的方法\textsc{LongAgent}，将LLMs（例如LLaMA）扩展到128K上下文，并展示出在长文本处理方面可能优于GPT-4的潜力。在\textsc{LongAgent}中，一位领导者负责理解用户意图并指导团队成员从文档中获取信息。由于成员存在幻觉，领导者从几十到数百名成员的回应中获取准确信息并非易事。

    arXiv:2402.11550v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the middle}. In this paper, we propose \textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of member
    
[^145]: 英语和德语的句法语言变化：度量、解析器和趋同

    Syntactic Language Change in English and German: Metrics, Parsers, and Convergences

    [https://arxiv.org/abs/2402.11549](https://arxiv.org/abs/2402.11549)

    本文研究英语和德语句法语言变化趋势，使用议会辩论语料库，探讨了句法依存距离最小化及基于树图属性的15个度量标准，揭示了现代解析器在这种变化中的影响。

    

    许多研究表明，人类语言往往会优化语言结构以降低复杂性，增加交流效率。句法依存距离衡量了相关词汇之间的线性距离，通常被认为是语言处理困难和工作记忆负荷的关键指标。本文研究了英语和德语句法语言变化的历时趋势，使用了过去大约160年间的议会辩论语料库。我们基于5个依存句法解析器的观察结果，包括广泛使用的Stanford CoreNLP以及其他4个更新的替代方案。我们的句法语言变化分析超越了线性依存距离，探讨了与依存距离最小化（DDM）相关的15个度量标准，或者基于树图属性，比如树高和度变异。尽管我们有证据表明，最近基于现代树库训练的解析器并未受到重大影响。

    arXiv:2402.11549v1 Announce Type: cross  Abstract: Many studies have shown that human languages tend to optimize for lower complexity and increased communication efficiency. Syntactic dependency distance, which measures the linear distance between dependent words, is often considered a key indicator of language processing difficulty and working memory load. The current paper looks at diachronic trends in syntactic language change in both English and German, using corpora of parliamentary debates from the last c. 160 years. We base our observations on five dependency parsers, including the widely used Stanford CoreNLP as well as 4 newer alternatives. Our analysis of syntactic language change goes beyond linear dependency distance and explores 15 metrics relevant to dependency distance minimization (DDM) and/or based on tree graph properties, such as the tree height and degree variance. Even though we have evidence that recent parsers trained on modern treebanks are not heavily affected 
    
[^146]: KMMLU：在韩语中测量大规模多任务语言理解

    KMMLU: Measuring Massive Multitask Language Understanding in Korean

    [https://arxiv.org/abs/2402.11548](https://arxiv.org/abs/2402.11548)

    KMMLU是一个新的韩语基准，包含35,030道专家级多选题，从原始韩语考试中收集而来，测试了26个LLM模型，发现这些模型在KMMLU上的表现有很大提升空间。

    

    我们提出了KMMLU，这是一个新的韩语基准，涵盖了来自人文科学到STEM的45个学科的35,030道专家级多项选择题。与之前从现有英语基准翻译而来的韩语基准不同，KMMLU是从原始韩语考试中收集的，捕捉了韩语的语言和文化方面。我们测试了26个公开可用的和专有的LLM，发现有很大的改进空间。最好的公开模型在KMMLU上的准确率为50.54%，远低于平均人类表现的62.6%。这个模型主要是针对英语和中文进行训练的，而不是韩语。目前针对韩语的LLM，如Polyglot-Ko，表现得更糟。令人惊讶的是，即使是最有能力的专有LLM，例如GPT-4和HyperCLOVA X，也只分别达到了59.95%和53.40%。这表明需要进一步的工作来改进韩语LLM，而KMMLU提供了追踪这一进展的正确工具。

    arXiv:2402.11548v1 Announce Type: new  Abstract: We propose KMMLU, a new Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. Unlike previous Korean benchmarks that are translated from existing English benchmarks, KMMLU is collected from original Korean exams, capturing linguistic and cultural aspects of the Korean language. We test 26 publically available and proprietary LLMs, identifying significant room for improvement. The best publicly available model achieves 50.54% on KMMLU, far below the average human performance of 62.6%. This model was primarily trained for English and Chinese, not Korean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean LLMs, and KMMLU offers the right tool to track this progress. We mak
    
[^147]: 基于时空知识图的问答系统

    Question Answering Over Spatio-Temporal Knowledge Graph

    [https://arxiv.org/abs/2402.11542](https://arxiv.org/abs/2402.11542)

    介绍了一个新的基于时空知识图的问答系统STQAD，以解决问答系统在涵盖时空信息的问题上的挑战，提出了一种新的STComplEx嵌入方法STCQA来实现此目标

    

    时空知识图（STKG）通过整合时间和位置信息扩展了知识图（KG）的概念。尽管研究界关注知识图问答（KGQA），但基于STKG的涵盖时空信息的问题回答领域仍未被广泛探讨。此外，缺乏全面的数据集也阻碍了该领域的进展。为解决这一问题，我们提出了STQAD，这是一个包括10,000个自然语言问题的面向时空知识图问答（STKGQA）数据集。不幸的是，各种最先进的KGQA方法在我们的数据集上远未达到令人满意的性能。为此，我们提出了STCQA，一种新的时空KGQA方法，利用了一种名为STComplEx的新型STKG嵌入方法。通过从问题中提取时间和空间信息，我们的问答模型可以更好地理解问题。

    arXiv:2402.11542v1 Announce Type: cross  Abstract: Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge graphs (KGs) by incorporating time and location information. While the research community's focus on Knowledge Graph Question Answering (KGQA), the field of answering questions incorporating both spatio-temporal information based on STKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets also has hindered progress in this area. To address this issue, we present STQAD, a dataset comprising 10,000 natural language questions for spatio-temporal knowledge graph question answering (STKGQA). Unfortunately, various state-of-the-art KGQA approaches fall far short of achieving satisfactory performance on our dataset. In response, we propose STCQA, a new spatio-temporal KGQA approach that utilizes a novel STKG embedding method named STComplEx. By extracting temporal and spatial information from a question, our QA model can better comprehend the quest
    
[^148]: 逆向认知：大型语言模型比我们想象的更擅长理解知识图谱

    Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought

    [https://arxiv.org/abs/2402.11541](https://arxiv.org/abs/2402.11541)

    本文通过对KG知识注入方法进行全面比较，探索为LLMs提供知识图谱知识的最佳方法，以增强它们的理解能力。

    

    虽然通过使用知识图谱（KGs）来增强大型语言模型（LLMs）的推理能力并减少它们的幻觉的方法受到了广泛关注，但目前对如何使LLMs能够即时整合KGs中的结构化知识的探索还不足。本文采用复杂问题回答（CQA）作为一项任务，评估LLM理解KG知识的能力。我们对KG知识注入方法进行了全面比较（从三元组到自然语言文本），旨在探索为LLMs提供KG知识的最佳提示方法，从而增强它们的理解能力。

    arXiv:2402.11541v1 Announce Type: cross  Abstract: Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension o
    
[^149]: 通过机器去学习研究预训练数据对大型语言模型的影响

    Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning

    [https://arxiv.org/abs/2402.11537](https://arxiv.org/abs/2402.11537)

    通过对五个主要类别的预训练数据的48个数据集进行系统分析，研究了它们对大型语言模型性能的影响，并发现了一些“高影响数据”，如书籍，与模型能力相关联，为LLMs的优化提供了见解。

    

    通过在具有各种来源的语料库上进行预训练，大型语言模型（LLMs）取得了令人印象深刻的性能。然而，预训练语料库的每个组成部分的影响仍然不明确。因此，预训练语料库的组织仍然是经验性的，并且可能偏离最佳状态。为了解决这个问题，我们系统地分析了来自LLMs预训练数据的5个主要类别的48个数据集的影响，并使用关于九个主要模型能力类别的基准来衡量它们对LLMs的影响。我们的分析提供了关于多个语料库对LLMs性能贡献的实证结果，以及它们的联合影响模式，包括互补的、正交的和相关的关系。我们还确定了一组“高影响数据”，如书籍，与一组模型能力相关联。这些发现为我们提供了关于组织数据以支持LLMs优化的见解。

    arXiv:2402.11537v1 Announce Type: cross  Abstract: Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to sup
    
[^150]: PreAct: 在 ReAct 中预测未来增强智能体的规划能力

    PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability

    [https://arxiv.org/abs/2402.11534](https://arxiv.org/abs/2402.11534)

    PreAct是一个整合了预测、推理和行动的智能体框架，利用预测信息可以帮助智能体进行更多样化和策略性的推理，导致更有效的行动，提升任务完成效率。

    

    处理预测与实际结果之间的差异常常有助于个体拓展思维过程并进行反思，从而促进朝正确方向推理。本文介绍了一种名为 PreAct 的智能体框架，它将预测、推理和行动整合在一起。利用预测提供的信息，基于大语言模型（LLM）的智能体能够提供更多样化和策略导向的推理，进而导致更有效的行动，帮助智能体完成复杂任务。我们的实验表明，PreAct 在完成复杂任务方面优于 ReAct 方法，并且当与反思方法结合时，PreAct 的效果可以得到提升。我们对模型提供不同数量的历史预测，并发现历史预测对LLM规划有持续积极影响。

    arXiv:2402.11534v1 Announce Type: cross  Abstract: Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce $\textbf{PreAct}$, an agent framework that integrates $\textbf{pre}$diction with $\textbf{rea}$soning and $\textbf{act}$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The
    
[^151]: 指令链：大型语言模型的组合指令调整

    Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models

    [https://arxiv.org/abs/2402.11532](https://arxiv.org/abs/2402.11532)

    提出了一种名为指令链（CoI）的新概念，通过逐步解决每个子任务来处理由多个子任务组成的指令，进而提高了大型语言模型（LLMs）的泛化能力和多语言摘要性能

    

    使用一系列大型和多样化的指令对大型语言模型（LLMs）进行微调，提高了模型对不同任务的泛化能力，甚至对未曾见过的任务也适用。本研究提出了一种称为指令链（CoI）的新概念，其中一个指令的输出成为下一个指令的输入，就像一条链条。与解决单一指令任务的传统做法不同，我们提出的方法鼓励模型逐步解决每个子任务，直至得出最终答案。CoI调整（即使用CoI指令进行微调）提高了模型处理由多个子任务组成的指令能力。经CoI调整的模型在多语言摘要上也优于基准模型，证明....

    arXiv:2402.11532v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with a collection of large and diverse instructions has improved the model's generalization to different tasks, even for unseen tasks. However, most existing instruction datasets include only single instructions, and they struggle to follow complex instructions composed of multiple subtasks (Wang et al., 2023a). In this work, we propose a novel concept of compositional instructions called chain-of-instructions (CoI), where the output of one instruction becomes an input for the next like a chain. Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached. CoI-tuning (i.e., fine-tuning with CoI instructions) improves the model's ability to handle instructions composed of multiple subtasks. CoI-tuned models also outperformed baseline models on multilingual summarization, demonstratin
    
[^152]: 用RLHF推进翻译偏好建模：迈向成本效益解决方案

    Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution

    [https://arxiv.org/abs/2402.11525](https://arxiv.org/abs/2402.11525)

    提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。

    

    arXiv:2402.11525v1 公告类型：新 真实性、表达力和优雅是机器翻译中不断追求的目标。然而，传统的度量标准如BLEU并不严格符合人类对翻译质量的偏好。本文探讨了利用强化学习与人类反馈（RLHF）来提高翻译质量。收集人类对翻译之间的比较的大规模高质量数据集并不容易，尤其对于低资源语言。为解决这一问题，我们提出了一种成本效益的偏好学习策略，通过区分人类和机器翻译来优化奖励模型。通过这种方式，奖励模型学习机器翻译与人类之间的不足之处，并指导随后对机器翻译的改进。实验结果表明，RLHF可以有效地提升翻译质量，这种改进也有益于其他翻译方向。

    arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
    
[^153]: 揭示引人入胜对话的秘密：让用户沉迷于角色扮演对话代理的因素

    Unveiling the Secrets of Engaging Conversations: Factors that Keep Users Hooked on Role-Playing Dialog Agents

    [https://arxiv.org/abs/2402.11522](https://arxiv.org/abs/2402.11522)

    机器人体现其扮演角色的程度对保留率的影响有限，而其讲话的每个轮次的长度显著影响保留率

    

    随着对话代理变得越来越类人化，人们现在正在进行持续对话，可以从短暂的时刻延伸到长时间。理解促使这些互动持续的因素至关重要，然而现有研究主要集中在很少探索这种长时间和真实对话的短期模拟。在本文中，我们研究了影响角色扮演模型与真实用户之间互动中保留率的因素。通过分析真实用户和数千个角色之间的大型数据集，我们系统地检查了多个因素，并评估了它们对用户保留率的影响。令人惊讶的是，我们发现机器人体现其扮演角色的程度对保留率的影响有限，而其讲话的每个轮次的长度显著影响保留率。这项研究揭示了用户参与度的关键方面。

    arXiv:2402.11522v1 Announce Type: new  Abstract: With the growing humanlike nature of dialog agents, people are now engaging in extended conversations that can stretch from brief moments to substantial periods of time. Understanding the factors that contribute to sustaining these interactions is crucial, yet existing studies primarily focusing on short-term simulations that rarely explore such prolonged and real conversations.   In this paper, we investigate the factors influencing retention rates in real interactions with roleplaying models. By analyzing a large dataset of interactions between real users and thousands of characters, we systematically examine multiple factors and assess their impact on user retention rate. Surprisingly, we find that the degree to which the bot embodies the roles it plays has limited influence on retention rates, while the length of each turn it speaks significantly affects retention rates. This study sheds light on the critical aspects of user engageme
    
[^154]: 知识到SQL：用数据专家LLM增强SQL生成

    Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM

    [https://arxiv.org/abs/2402.11517](https://arxiv.org/abs/2402.11517)

    Knowledge-to-SQL框架利用数据专家LLM提供有用知识，增强文本到SQL模型的鲁棒性。

    

    为用户查询生成准确的SQL（文本到SQL）是一个长期存在的问题，因为生成SQL需要理解查询和数据库，然后根据数据库检索准确的数据。现有模型依赖于大型语言模型（LLMs）的综合能力，根据数据库模式生成SQL。然而，有些必要的知识没有明确包含在数据库模式中，或者被LLMs学习了。因此，知识不足的查询生成的SQL可能是不准确的，这会对文本到SQL模型的鲁棒性产生负面影响。为了应对这种情况，我们提出了Knowledge-to-SQL框架，该框架采用定制的数据专家LLM（DELLM）为所有类型的文本到SQL模型提供有用的知识。具体地，我们详细介绍了DELLM的设计，包括表格读取和基本微调过程。

    arXiv:2402.11517v1 Announce Type: new  Abstract: Generating accurate SQL for user queries (text-to-SQL) is a long-standing problem since the generation of the SQL requires comprehending the query and database and retrivale the accurate data from the database accordingly. Existing models rely on the comprehensive ability of Large Language Models (LLMs) to generate the SQL according to the database schema. However, there is some necessary knowledge that is not explicitly included in the database schema or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient queries may be inaccurate, which negatively impacts the robustness of the text-to-SQL models. To deal with this situation, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all types of text-to-SQL models. Specifically, we provide the detailed design of DELLM, in terms of table reading, and the basic fine-tuning process. We further prov
    
[^155]: 从偏见到平等：去偏巨型语言模型词嵌入的新方法

    From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings

    [https://arxiv.org/abs/2402.11512](https://arxiv.org/abs/2402.11512)

    提出了DeepSoftDebias算法，在不同领域数据集、准确度指标和NLP任务中全面评估，发现其在减少性别、种族和宗教偏见方面优于现有最先进方法

    

    嵌入在巨型语言模型的有效性中扮演着重要角色。它们是这些模型把握上下文关系、促进更细致语言理解以及在许多需要对人类语言有基本理解的复杂任务上表现出色的基石。鉴于这些嵌入往往自身反映或展示偏见，因此这些模型可能也会无意中学习这种偏见。在这项研究中，我们在开创性前人研究基础上提出了DeepSoftDebias，这是一种使用神经网络进行“软去偏”的算法。我们在各类最先进数据集、准确度指标和具有挑战的自然语言处理任务中全面评估了这个算法。我们发现DeepSoftDebias在减少性别、种族和宗教偏见方面优于目前的最先进方法。

    arXiv:2402.11512v1 Announce Type: new  Abstract: Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform `soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.
    
[^156]: 在异构语言任务和客户资源下对大型语言模型进行联邦微调

    Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources

    [https://arxiv.org/abs/2402.11505](https://arxiv.org/abs/2402.11505)

    该研究引入了FlexLoRA，一个简单而有效的大型语言模型微调聚合方案，能够在联邦学习中充分利用异质客户资源，通过动态调整本地LoRA排名和采用奇异值分解进行权重重新分配，提升全局模型的广泛知识。

    

    近期，联邦学习（FL）被应用于对大型语言模型（LLMs）进行参数高效微调。然而，由于客户的资源和数据分布不均匀，这引发了重大挑战。本研究引入了FlexLoRA，这是一种简单而有效的LLM微调聚合方案，它可以缓解传统FL中的“桶效应”，该效应限制了拥有丰富资源的客户实现潜力，将他们与最缺乏资源的参与者的能力捆绑在一起。FlexLoRA允许动态调整本地LoRA排名，促进全局模型的发展，并赋予更广泛、不太任务特定的知识。通过从个体客户贡献中合成完整大小的LoRA权重，并利用奇异值分解（SVD）进行权重重新分配，FlexLoRA充分利用了客户间的资源差异。本研究涉及超过1600个执行多样NLP任务的客户。

    arXiv:2402.11505v1 Announce Type: cross  Abstract: Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the "buckets effect" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, ou
    
[^157]: 基于大语言模型的知识边界基准：对模型评估的另一种视角

    Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation

    [https://arxiv.org/abs/2402.11493](https://arxiv.org/abs/2402.11493)

    引入了知识边界的概念，以涵盖语言模型内的无提示和有提示敏感性知识，通过避免提示敏感性，使得语言模型评估更可靠和稳健。

    

    最近几年，在大语言模型的发展中取得了实质性进展，在各种任务中取得了显著的性能。为了评估语言模型的知识能力，先前的研究提出了许多基于问答对的基准。我们认为，使用固定问题或有限的释义作为查询来评估语言模型是不可靠和全面的，因为语言模型对提示很敏感。因此，我们引入了一个名为知识边界的新概念，以包含语言模型内的无提示和有提示敏感性知识。知识边界避免了语言模型评估中的提示敏感性，使其更可靠和稳健。为了探索给定模型的知识边界，我们提出了带有语义约束的投影梯度下降方法，这是一种旨在识别每个部分的最佳提示的新算法。

    arXiv:2402.11493v1 Announce Type: new  Abstract: In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs. We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt. Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models. Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece o
    
[^158]: 论文标题: 计划是什么？评估和开发针对LLMs的计划意识技术

    What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs

    [https://arxiv.org/abs/2402.11489](https://arxiv.org/abs/2402.11489)

    提出了一种新的混合方法SimPlan，结合了LLMs和经典规划方法，在各种规划领域的实验表明SimPlan明显优于现有的基于LLM的规划者

    

    arXiv:2402.11489v1 公告类型: 新摘要: 计划是人工智能中的基本任务，涉及在给定环境中找到实现特定目标的一系列行动。 大型语言模型（LLMs）越来越多地用于需要计划能力的应用，如网络或具体代理。 与最近的研究一致，我们通过实验表明LLMs缺乏计划所需的必要技能。 基于这些观察结果，我们倡导一种将LLMs与经典计划方法结合的混合方法的潜力。 然后，我们介绍了SimPlan，一种新颖的混合方法，并评估其在新的具有挑战性的设置中的表现。 我们在各种规划领域进行的大量实验证明，SimPlan明显优于现有基于LLM的规划者。

    arXiv:2402.11489v1 Announce Type: new  Abstract: Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment. Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents. In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning. Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners.
    
[^159]: LEIA: 利用基于实体的数据增强在语言模型中促进跨语言知识转移

    LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation

    [https://arxiv.org/abs/2402.11485](https://arxiv.org/abs/2402.11485)

    LEIA是一种语言适应调整方法，利用维基百科实体名称跨语言增强目标语言语料库，通过左到右的语言建模训练，显著提高了各种非英语语言的表现。

    

    将基于英语的大型语言模型（LLMs）适应其他语言的操作由于跨语言转移的效率和潜力而变得越来越受欢迎。然而，现有的语言适应方法常常忽视跨语言监督的好处。在本研究中，我们介绍LEIA，一种利用跨语言对齐的维基百科实体名称的语言适应调整方法。该方法涉及使用英语实体名称增强目标语言语料库，并使用从左到右的语言建模训练模型。我们在多样的问答数据集上评估LEIA，使用7B参数的LLMs，展示了在各种非英语语言上的显著性能增益。源代码可在https://github.com/studio-ousia/leia上获得。

    arXiv:2402.11485v1 Announce Type: cross  Abstract: Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.
    
[^160]: DictLLM: 利用大型语言模型操纵键值数据结构以增强医学诊断

    DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics

    [https://arxiv.org/abs/2402.11481](https://arxiv.org/abs/2402.11481)

    DictLLM是一个创新性框架，旨在改进键值结构化数据的建模，用于生成医学诊断。它包括组位置编码、层次注意偏差和优化传输对齐层。

    

    结构化数据提供了一种复杂的信息组织机制。在大型语言模型的文本序列化结构化数据的现有方法未能充分解决键值结构化数据固有的异质性问题。这些方法并不理想，经常导致更大的输入尺寸和对输入更改的适应性较差。在本文中，我们介绍了DictLLM，这是一个创新性框架，旨在改进键值结构化数据（如医学实验室报告）的建模，以生成医学诊断。DictLLM整合了三个关键组件：（1）组位置编码以保持置换不变性，（2）层次注意偏差以捕捉结构化数据的固有偏差，以及（3）一个优化传输对齐层，将字典编码器生成的嵌入与LLM对齐，从而产生一系列固定长度的虚拟标记。

    arXiv:2402.11481v1 Announce Type: new  Abstract: Structured data offers a sophisticated mechanism for the organization of information. Existing methodologies for the text-serialization of structured data in the context of large language models fail to adequately address the heterogeneity inherent in key-value structured data. These methods are not ideal and frequently result in larger input sizes and poor adaptability to input changes. In this paper, we introduce DictLLM, an innovative framework designed to improve the modeling of key-value structured data, like medical laboratory reports, for generating medical diagnoses. DictLLM integrates three key components: (1) group positional encoding to maintain permutation invariance, (2) hierarchical attention bias to capture the inherent bias in structured data, and (3) an optimal transport alignment layer that aligns the embedding generated by the dictionary encoder with the LLM, thereby producing a sequence of fixed-length virtual tokens.
    
[^161]: 在搜索训练数据与Transformer文本模型对抗性稳健性之间的相关性时的一个有趣案例

    A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models

    [https://arxiv.org/abs/2402.11469](https://arxiv.org/abs/2402.11469)

    本文研究了训练数据与模型鲁棒性之间的相关性，并提出通过提取不同特征来预测Transformer文本模型的对抗性稳健性的方法。

    

    现有研究表明，经过微调的文本Transformer模型可以实现最先进的预测性能，但也容易受到对抗文本扰动的影响。传统的对抗性评估通常在对模型进行微调之后才进行，忽略了训练数据。本文旨在证明训练数据和模型鲁棒性之间也存在着强关联。为此，我们提取了代表广泛输入微调语料库属性的13种不同特征，并用它们来预测经过微调的模型的对抗性稳健性。我们主要关注仅编码器的Transformer模型BERT和RoBERTa，并附加了BART、ELECTRA和GPT2的其他结果，为我们的论点提供多样的证据。首先，经验证明，(a)提取的特征可与轻量级分类器（如随机森林）一起有效地预测攻击成功率。

    arXiv:2402.11469v1 Announce Type: cross  Abstract: Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done \textit{only after} fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate 
    
[^162]: LLM何时需要检索增强？减轻LLM的过度自信有助于检索增强

    When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation

    [https://arxiv.org/abs/2402.11457](https://arxiv.org/abs/2402.11457)

    LLM何时需要检索增强？减轻LLM的过度自信有助于检索增强

    

    大型语言模型（LLMs）被发现很难知道自己不具备某些知识，并且在这种情况下往往会提供虚假答案。检索增强（RA）已被广泛研究以减轻LLMs的幻觉。然而，由于额外的开销和检索质量不确定，始终进行RA可能并不是最佳选择。一个直观的想法是只有在LLMs对问题不确定时才进行检索。这激发我们增强LLMs感知知识边界的能力以帮助RA。本文首先定量衡量LLMs的这种能力并确认它们的过度自信。然后，我们研究LLMs对问题的确定性如何与他们依赖外部检索信息相关。我们提出了几种方法来增强LLMs对知识边界的感知，并显示它们在减少过度自信方面是有效的。

    arXiv:2402.11457v1 Announce Type: new  Abstract: Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped wi
    
[^163]: FactPICO: 医学证据的简明语言摘要的事实性评估

    FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence

    [https://arxiv.org/abs/2402.11456](https://arxiv.org/abs/2402.11456)

    本文提出FactPICO，用于评估医学文本的简明语言摘要的事实性基准，对RCT中的关键要素和报告结果进行评估，并对LLMs添加的额外信息进行检查。

    

    arXiv:2402.11456v1 公告类型: 新的 摘要: 利用LLMs进行简明语言摘要可以改善技术内容的文本可访问性。但是在医学这样一个高风险领域，这些摘要有多真实？本文介绍了FactPICO，这是一个用于描述随机对照试验（RCTs）的医学文本的简明语言摘要的事实性基准，RCTs是循证医学的基础，可以直接为患者治疗提供信息。FactPICO由来自三个LLMs（即GPT-4、Llama-2和Alpaca）生成的345个RCT摘要的简明语言摘要组成，具有专家细致评估和自然语言理由。我们评估这些摘要中RCT的关键要素（人群、干预措施、对照组、结果（PICO））以及关于这些内容的报告发现的事实性。我们还评估LLMs添加的额外信息（例如解释）的正确性。使用FactPICO，我们对多个现有

    arXiv:2402.11456v1 Announce Type: new  Abstract: Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? This paper presents FactPICO, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. FactPICO consists of 345 plain language summaries of RCT abstracts generated from three LLMs (i.e., GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. We also evaluate the correctness of the extra information (e.g., explanations) added by LLMs. Using FactPICO, we benchmark a range of exi
    
[^164]: LoRA-Flow: 大型语言模型在生成任务中的动态LoRA融合

    LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks

    [https://arxiv.org/abs/2402.11455](https://arxiv.org/abs/2402.11455)

    LoRA-Flow提出了动态权重来调整不同LoRA的影响，以应对生成任务中不同标记所需的多样化技能。

    

    LoRA利用轻量级模块定制大型语言模型（LLMs）以适应每个下游任务或领域，在那里不同的学习的额外模块代表不同的技能。结合现有的LoRA来解决新任务可以增强学习的LoRA的可重用性，特别适用于数据有限的任务。大多数先前关于LoRA组合的工作主要依赖于每个涉及的LoRA的任务级别权重，使不同的示例和标记共享相同的LoRA权重。然而，在生成任务中，不同的标记可能需要不同的技能来管理。以中文数学任务为例，理解问题描述可能更依赖于中文LoRA，而计算部分可能更依赖于数学LoRA。为此，我们提出了LoRA-Flow，它利用动态权重来调整不同LoRA的影响。每个步骤的权重由具有极少参数的融合门确定。

    arXiv:2402.11455v1 Announce Type: new  Abstract: LoRA employs lightweight modules to customize large language models (LLMs) for each downstream task or domain, where different learned additional modules represent diverse skills. Combining existing LoRAs to address new tasks can enhance the reusability of learned LoRAs, particularly beneficial for tasks with limited annotated data. Most prior works on LoRA combination primarily rely on task-level weights for each involved LoRA, making different examples and tokens share the same LoRA weights. However, in generative tasks, different tokens may necessitate diverse skills to manage. Taking the Chinese math task as an example, understanding the problem description may depend more on the Chinese LoRA, while the calculation part may rely more on the math LoRA. To this end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the impact of different LoRAs. The weights at each step are determined by a fusion gate with extremely few pa
    
[^165]: MatPlotAgent: 基于LLM的Agent科学数据可视化方法与评估

    MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization

    [https://arxiv.org/abs/2402.11453](https://arxiv.org/abs/2402.11453)

    MatPlotAgent介绍了一种基于LLM的自动化科学数据可视化框架，包括查询理解、代码生成和视觉反馈机制，并引入了MatPlotBench基准和GPT-4V评分方法。

    

    科学数据可视化通过直接展示复杂信息并帮助研究人员识别隐含模式，在研究中起着至关重要的作用。尽管其重要性，但对于使用Large Language Models（LLMs）进行科学数据可视化的研究仍较为未被探索。在这项研究中，我们介绍了MatPlotAgent，一种高效、与模型无关的LLM代理框架，旨在自动化科学数据可视化任务。MatPlotAgent利用代码LLMs和多模态LLMs的能力，由三个核心模块组成：查询理解、带有迭代调试的代码生成，以及用于错误更正的视觉反馈机制。为了解决该领域缺乏基准的问题，我们提出了MatPlotBench，一个由100个经人工验证的测试案例组成的高质量基准。此外，我们介绍了一种利用GPT-4V进行自动评估的评分方法。实验结果表明…（未完整）

    arXiv:2402.11453v1 Announce Type: new  Abstract: Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate t
    
[^166]: AutoPRM: 通过可控问题分解自动化多步推理的过程监督

    AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition

    [https://arxiv.org/abs/2402.11452](https://arxiv.org/abs/2402.11452)

    提出了一种名为AutoPRM的自监督框架，通过可控问题分解和强化学习实现对复杂推理挑战的自动化监督和改进

    

    大型语言模型（LLMs）的最新进展显示了在多步推理任务中的潜力，然而，它们对广泛的手动标注来提供程序反馈的依赖仍然是一个重大障碍。为了解决这一挑战，在本文中，我们提出了一种新颖的自监督框架AutoPRM，它有效地增强了LLMs对复杂推理挑战的微调。具体而言，AutoPRM首先通过可控粒度开关将复杂问题分解成更易管理的子问题，然后顺序应用强化学习来迭代改进子问题解决器。此外，我们提出了上下文引导解码来避免奖励篡改并引导子问题解决器朝向整体问题的解决方案。大量实验证明，AutoPRM在数学和常识推理任务上显著提高了性能，超过了SOTA。更令人鼓舞的是，AutoPRM可以轻松实现

    arXiv:2402.11452v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have shown promise in multi-step reasoning tasks, yet their reliance on extensive manual labeling to provide procedural feedback remains a significant impediment. To address this challenge, in this paper, we propose a novel self-supervised framework AutoPRM that efficiently enhances the fine-tuning of LLMs for intricate reasoning challenges. Specifically, AutoPRM first decomposes complex problems into more manageable subquestions with a controllable granularity switch, then sequentially apply reinforcement learning to iteratively improve the subquestion solver. Additionally, we propose context-guided-decoding to avoid reward tampering and guide the subquestion solver towards the solution of the holistic problem. Extensive experiments show that AutoPRM significantly improves performance on mathematical and commonsense reasoning tasks over SOTA. More encouragingly, AutoPRM can be easily i
    
[^167]: SciAgent: 工具增强型语言模型用于科学推理

    SciAgent: Tool-augmented Language Models for Scientific Reasoning

    [https://arxiv.org/abs/2402.11451](https://arxiv.org/abs/2402.11451)

    引入了工具增强型科学推理的新任务设置，通过提供可扩展的工具集，帮助大型语言模型在科学问题解决中变得更加实用和可解决。

    

    科学推理对于即使最先进的大型语言模型（LLMs）来说也是一项巨大挑战。为了使LLMs更加实用和可解决此任务，我们引入了一种名为工具增强型科学推理的新任务设置。这种设置通过为LLMs提供可扩展的工具集，将重点从追求全知问题求解器转变为熟练使用工具的人。为了促进这种设置的研究，我们构建了一个名为MathFunc的工具增强型训练语料库，涵盖了超过30,000个样本和大约6,000个工具。基于MathFunc，我们开发了SciAgent，用于检索、理解，以及必要时使用工具进行科学问题解决。此外，我们构建了一个名为SciToolBench的基准，涵盖五个科学领域，以评估LLMs在工具辅助下的能力。对SciToolBench进行的大量实验验证了SciAgent的有效性。值得注意的是，SciAgent-Mistral-7B超过了其他LLMs。

    arXiv:2402.11451v1 Announce Type: cross  Abstract: Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the sa
    
[^168]: 由标签分布引导的上下文示例排序

    In-Context Example Ordering Guided by Label Distributions

    [https://arxiv.org/abs/2402.11447](https://arxiv.org/abs/2402.11447)

    该论文提出了由模型的概率预测引导的上下文示例排序的原则，以提高上下文学习在自然语言处理中的性能。

    

    通过允许模型在没有特定任务训练的情况下进行预测，利用预训练LLMs进行上下文学习（ICL）在自然语言处理中具有巨大潜力。然而，ICL仍然存在一些问题。特别是，其性能对于上下文示例的选择和排序非常敏感。给定具有不同排序的相同一组上下文示例，模型的性能可能在接近随机到接近最先进之间变化。在这项工作中，我们将上下文示例排序规定为一个优化问题。我们研究了三种问题设置，它们在对任务已知信息的假设方面有所不同。受到从标签比例中学习的想法的启发，我们提出了两个原则，用于根据模型的概率预测指导上下文示例排序。我们将我们提出的原则应用于十三个文本分类数据集和九个具有从700M到13B参数的不同自回归LLMs。我们展示了我们的方法优越性。

    arXiv:2402.11447v1 Announce Type: new  Abstract: By allowing models to predict without task-specific training, in-context learning (ICL) with pretrained LLMs has enormous potential in NLP. However, a number of problems persist in ICL. In particular, its performance is sensitive to the choice and order of in-context examples. Given the same set of in-context examples with different orderings, model performance may vary between near random to near state-of-the-art. In this work, we formulate in-context example ordering as an optimization problem. We examine three problem settings that differ in the assumptions they make about what is known about the task. Inspired by the idea of learning from label proportions, we propose two principles for in-context example ordering guided by model's probability predictions. We apply our proposed principles to thirteen text classification datasets and nine different autoregressive LLMs with 700M to 13B parameters. We demonstrate that our approach outpe
    
[^169]: 基准自我演进: 用于动态LLM评估的多Agent框架

    Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation

    [https://arxiv.org/abs/2402.11443](https://arxiv.org/abs/2402.11443)

    介绍了一个基准自我演进框架，通过多Agent系统操作环境或问题，构建演化实例来更准确地评估LLMs的能力和限制，并扩展基准数据集以进行更具可扩展性和精细化的评估。

    

    这篇论文介绍了一个基准自我演进框架，用于动态评估快速发展的大型语言模型（LLMs），旨在更准确地评估它们的能力和局限性。我们利用一个多Agent系统来操作原始实例的环境或问题，通过重构新的演化实例来扩展现有基准，以更具高信心地动态扩展现有基准。为了实现更具可扩展性、强健性和精细化评价，我们实施了六个重构操作来构建演化实例，测试LLMs对各种查询、数据噪声并探究它们的问题解决子能力。通过这个框架，我们扩展了四个任务的基准数据集。实验结果显示，在大多数LLMs中，针对原始结果表现出普遍的性能下降。在我们的可扩展和强健的评估下，以及我们的精细化评估下，这种下降更准确地反映了模型的能力。

    arXiv:2402.11443v1 Announce Type: new  Abstract: This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations. We utilize a multi-agent system to manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extend existing benchmarks. Towards a more scalable, robust and fine-grained evaluation, we implement six reframing operations to construct evolving instances testing LLMs against diverse queries, data noise and probing their problem-solving sub-abilities. With this framework, we extend benchmark datasets of four tasks. Experimental results show a general performance decline in most LLMs against their original results. This decline under our scalable and robust evaluations, alongside our fine-grained evaluation, more accurately reflect models' capabilities. Besides, our frame
    
[^170]: 能够与规则进行推理吗？逻辑支架用于压力测试和提升LLM

    Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs

    [https://arxiv.org/abs/2402.11442](https://arxiv.org/abs/2402.11442)

    提出了一个逻辑支架推理规则生成框架，通过构建包含基础和组合规则的推理规则库ULogic，揭示了LLMs在逻辑理解方面与人类表现的显著差距，并通过生成准确、复杂和抽象的推理结果来提升下游推理。

    

    大型语言模型(LLMs)在各种推理任务中取得了令人印象深刻的接近人类表现的成绩。然而，它们对于基础推理规则的掌握仍然不及人类能力。为了研究这一问题，我们提出了一个逻辑支架推理规则生成框架，构建了一个包含五个领域中基础和组合规则的推理规则库ULogic。我们对GPT系列模型在规则子集上的分析揭示出LLMs在逻辑理解方面与人类表现存在显著差距，特别是在具有某些偏见模式的组合和结构复杂规则中。我们进一步将这些规则提炼成一个更小规模的推理引擎，用于灵活地生成规则并增强下游推理。通过多评估人员评估，我们的推理引擎证明在生成准确、复杂和抽象的结论和前提方面表现出效果，可以改善各种常识推理。

    arXiv:2402.11442v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reas
    
[^171]: InfuserKI：通过Infuser引导的知识集成增强大型语言模型与知识图谱

    InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration

    [https://arxiv.org/abs/2402.11441](https://arxiv.org/abs/2402.11441)

    提出了一种Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态有效地将未知知识集成到大型语言模型中，从而缓解知识遗忘问题。

    

    大型语言模型（LLMs）在各个领域展现出卓越的开放式生成能力，但在知识密集型任务中表现不佳。为了缓解这一问题，提出了知识集成方法，利用外部模块将领域特定知识图谱与LLMs结合起来。然而，它们存在数据效率低的问题，因为它们需要已知和未知的知识来进行微调。因此，我们研究了一个新颖的问题，即如何在不重复已知知识的情况下有效地将未知知识集成到LLMs中。注入新知识会导致遗忘先前获得的知识的风险。为了解决这个问题，我们提出了一种新颖的Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态来确定是否应该增强原始LLM输出信息，从而有效地减轻知识遗忘问题。在UMLS-2.5k和MetaQ上进行了评估。

    arXiv:2402.11441v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQ
    
[^172]: 自我反馈的危险：在大型语言模型中自我偏见被放大

    Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models

    [https://arxiv.org/abs/2402.11436](https://arxiv.org/abs/2402.11436)

    大型语言模型存在自我偏见，研究发现通过更大的模型规模和准确评估的外部反馈可以显著减少这种偏见，并提高后续任务的实际表现。

    

    最近的研究表明，自我反馈可以改善大型语言模型在某些任务上的表现，但在其他任务上却会恶化。我们发现这种矛盾是由于大型语言模型对其自身输出的偏见。本文正式定义了大型语言模型的自我偏见——倾向于偏爱自身生成——并使用两个统计量进行了分析。我们在翻译、受限文本生成和数学推理任务上分析了六种大型语言模型。我们发现自我偏见在所有检测的大型语言模型中都普遍存在，跨多种语言和任务。我们的分析表明，虽然自我改进管道提高了模型输出的流畅性和可理解性，但它进一步放大了自我偏见。为了缓解这种偏见，我们发现更大的模型规模和具有准确评估的外部反馈可以显著减少自我改进管道中的偏见，从而实际改善下游任务的性能。

    arXiv:2402.11436v1 Announce Type: cross  Abstract: Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.
    
[^173]: 欺骗检测能够更深入吗？用于欺骗推理的数据集、评估和基准

    Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning

    [https://arxiv.org/abs/2402.11432](https://arxiv.org/abs/2402.11432)

    本文提出了一种新的数据收集流程，利用 GPT-4 模拟嫌疑人与警官之间的角色扮演，以解决欺骗检测领域面临的数据稀缺问题，并将传统的欺骗检测任务拓展到欺骗推理。

    

    arXiv:2402.11432v1 公告类型：新摘要：由于在许多实际场景中的重要性，欺骗检测引起了越来越多的关注。目前，数据稀缺阻碍了这一领域的发展。一方面，雇佣参与者模拟欺骗场景成本高昂。另一方面，很难在互联网上收集包含欺骗行为的视频。为解决数据稀缺问题，本文提出了一种新的数据收集流程。具体来说，我们使用 GPT-4 模拟了嫌疑人和警官之间的角色扮演。在审讯过程中，嫌疑人向警官撒谎，试图逃避犯罪责任，而警官揭露了事实并收集了证据。与先前的数据集相比，这一策略减少了数据收集成本，为增加数据集规模提供了一种有前途的途径。同时，我们将传统的欺骗检测任务扩展到欺骗推理，进一步为欺骗行为提供证据。

    arXiv:2402.11432v1 Announce Type: new  Abstract: Deception detection has attracted increasing attention due to its importance in many practical scenarios. Currently, data scarcity harms the development of this field. On the one hand, it is costly to hire participants to simulate deception scenarios. On the other hand, it is difficult to collect videos containing deceptive behaviors on the Internet. To address data scarcity, this paper proposes a new data collection pipeline. Specifically, we use GPT-4 to simulate a role-play between a suspect and a police officer. During interrogation, the suspect lies to the police officer to evade responsibility for the crime, while the police officer uncovers the truth and gathers evidence. Compared with previous datasets, this strategy reduces data collection costs, providing a promising way to increase the dataset size. Meanwhile, we extend the traditional deception detection task to deception reasoning, further providing evidence for deceptive pa
    
[^174]: 使用结果监督增强大型语言模型的事件提取的EventRL方法

    EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models

    [https://arxiv.org/abs/2402.11430](https://arxiv.org/abs/2402.11430)

    EventRL通过结果监督和特定奖励函数有效改善了大型语言模型的事件提取效果，尤其在处理新型事件类型方面表现优异。

    

    在这项研究中，我们提出了EventRL，这是一种用于增强大型语言模型（LLMs）的事件提取的强化学习方法。EventRL利用特定的奖励函数的结果监督来解决LLMs中普遍存在的问题，例如遵循指令和产生幻觉，这表现为事件结构不匹配和生成未定义事件类型。我们对EventRL进行了评估，与现有方法如Few-Shot Prompting（FSP）（基于GPT4）和Supervised Fine-Tuning（SFT）进行对比，涵盖了包括GPT-4、LLaMa和CodeLLaMa模型在内的各种LLMs。我们的研究结果显示，EventRL在识别和结构化事件方面显著优于这些传统方法，特别是在处理新颖事件类型方面表现得更好。该研究强调了奖励函数选择的关键作用，并展示了将代码数据纳入以获得更好事件提取效果的好处。

    arXiv:2402.11430v1 Announce Type: new  Abstract: In this study, we present EventRL, a reinforcement learning approach developed to enhance event extraction for large language models (LLMs). EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like Few-Shot Prompting (FSP) (based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing m
    
[^175]: 多阶段知识迁移框架在多领域中文拼写校正中的防止灾难性遗忘

    Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling Correction by Multi-stage Knowledge Transfer Framework

    [https://arxiv.org/abs/2402.11422](https://arxiv.org/abs/2402.11422)

    提出了一个模型无关的多阶段知识迁移框架，通过不断发展的教师模型在多领域中防止CSC模型忘记先前获得的知识，实验证明了方法的有效性。

    

    中国会拼写校正（CSC）旨在检测和纠正给定句子中的拼写错误。最近，多领域CSC逐渐引起研究者的关注，因为它更加实用。本文关注CSC模型在适应多领域场景时存在的关键缺陷：学习新的领域特定知识时容易忘记先前获得的知识（即灾难性遗忘）。为解决这一问题，提出了一种新颖的模型无关的多阶段知识迁移（MKT）框架，该框架在每个领域中利用不断发展的教师模型进行知识迁移，而不仅仅专注于新领域知识。值得一提的是，我们是首次将连续学习方法应用于多领域CSC任务。实验证明了我们提出的方法的有效性，进一步分析展示了克服灾难性遗忘对于提高CSC性能的重要性。

    arXiv:2402.11422v1 Announce Type: new  Abstract: Chinese Spelling Correction (CSC) aims to detect and correct spelling errors in given sentences. Recently, multi-domain CSC has gradually attracted the attention of researchers because it is more practicable. In this paper, we focus on the key flaw of the CSC model when adapting to multi-domain scenarios: the tendency to forget previously acquired knowledge upon learning new domain-specific knowledge (i.e., catastrophic forgetting). To address this, we propose a novel model-agnostic Multi-stage Knowledge Transfer (MKT) framework, which utilizes a continuously evolving teacher model for knowledge transfer in each domain, rather than focusing solely on new domain knowledge. It deserves to be mentioned that we are the first to apply continual learning methods to the multi-domain CSC task. Experiments prove the effectiveness of our proposed method, and further analyses demonstrate the importance of overcoming catastrophic forgetting for impr
    
[^176]: 重新思考大型语言模型在中文语法错误纠正中的作用

    Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction

    [https://arxiv.org/abs/2402.11420](https://arxiv.org/abs/2402.11420)

    重新思考大型语言模型在中文语法错误纠正中的作用，利用LLMs作为解释器提供解释信息并作为评估器带来更合理的CGEC评估以增强性能

    

    最近，研究人员广泛研究大型语言模型（LLMs）在各种下游NLP任务中的作用。作为NLP领域的一项基础任务，中文语法错误纠正（CGEC）旨在纠正输入句子中的所有潜在语法错误。先前的研究表明，由于其具有挑战性的任务重点，LLMs作为CGEC校正器的性能仍然令人不满。为了推动CGEC领域更好地适应LLMs时代，我们重新思考LLMs在CGEC任务中的作用，使其能够在CGEC中得到更好的利用和探索。考虑到LLMs中存储的丰富语法知识和其强大的语义理解能力，我们利用LLMs作为解释器，为CGEC小模型提供解释信息，以增强性能。我们还将LLMs用作评估器，带来更合理的CGEC评估，从而缓解由于

    arXiv:2402.11420v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have been widely studied by researchers for their roles in various downstream NLP tasks. As a fundamental task in the NLP field, Chinese Grammatical Error Correction (CGEC) aims to correct all potential grammatical errors in the input sentences. Previous studies have shown that LLMs' performance as correctors on CGEC remains unsatisfactory due to its challenging task focus. To promote the CGEC field to better adapt to the era of LLMs, we rethink the roles of LLMs in the CGEC task so that they can be better utilized and explored in CGEC. Considering the rich grammatical knowledge stored in LLMs and their powerful semantic understanding capabilities, we utilize LLMs as explainers to provide explanation information for the CGEC small models during error correction to enhance performance. We also use LLMs as evaluators to bring more reasonable CGEC evaluations, thus alleviating the troubles caused by th
    
[^177]: LoRETTA: 低秩经济张量训练适应大型语言模型的超低参数微调

    LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models

    [https://arxiv.org/abs/2402.11417](https://arxiv.org/abs/2402.11417)

    LoRETTA是一个通过张量训练分解显著减少可训练参数的超低参数高效框架，在大型语言模型的微调中表现出与大多数PEFT方法相媲美甚至更好的性能。

    

    各种参数高效微调（PEFT）技术被提出以实现在保持模型性能的情况下进行计算高效的微调。然而，随着大型语言模型（LLMs）的快速部署，现有的PEFT方法仍然受到可训练参数数量增长的限制。为了解决这一挑战，我们提出了LoRETTA，这是一个超参数高效的框架，通过张量训练分解显著减少可训练参数。具体来说，我们提出了两种方法，分别命名为{LoRETTA}$_{adp}$和{LoRETTA}$_{rep}$。前者采用张量化适配器，为LLMs的微调提供了高性能且轻量级的方法。后者强调通过一组小张量因子进行权重参数化的微调。LoRETTA在LLaMA-2-7B模型上比大多数广泛使用的PEFT方法具有可比或更好的性能，并且参数少达到100倍。

    arXiv:2402.11417v1 Announce Type: cross  Abstract: Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight parameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\times$ fewer parameters on the LLaMA-2-7B models. Further
    
[^178]: 用于多模态摘要的细粒度可解释事实评估

    Fine-grained and Explainable Factuality Evaluation for Multimodal Summarization

    [https://arxiv.org/abs/2402.11414](https://arxiv.org/abs/2402.11414)

    提出两种细粒度和可解释的评估框架，用于评估多模态摘要模型的事实性，其中无参考事实性评估框架具有更广泛的应用场景，实验证实了方法的有效性。

    

    多模态摘要旨在生成基于输入文本和图像的简洁摘要。然而，现有方法可能存在事实性输出的问题。为了评估多模态摘要模型的事实性，我们提出了两种细粒度和可解释的评估框架（FALLACIOUS）用于不同的应用场景，即基于参考的事实性评估框架和无参考的事实性评估框架。值得注意的是，无参考事实性评估框架不需要基准真值，因此具有更广泛的应用场景。为了评估所提出框架的有效性，我们计算了我们的框架与其他指标之间的相关性。实验结果显示了我们提出方法的有效性。我们将通过GitHub发布我们的代码和数据集。

    arXiv:2402.11414v1 Announce Type: new  Abstract: Multimodal summarization aims to generate a concise summary based on the input text and image. However, the existing methods potentially suffer from unfactual output. To evaluate the factuality of multimodal summarization models, we propose two fine-grained and explainable evaluation frameworks (FALLACIOUS) for different application scenarios, i.e. reference-based factuality evaluation framework and reference-free factuality evaluation framework. Notably, the reference-free factuality evaluation framework doesn't need ground truth and hence it has a wider application scenario. To evaluate the effectiveness of the proposed frameworks, we compute the correlation between our frameworks and the other metrics. The experimental results show the effectiveness of our proposed method. We will release our code and dataset via github.
    
[^179]: 通过偏好微调在视觉大语言模型中对齐模态

    Aligning Modalities in Vision Large Language Models via Preference Fine-tuning

    [https://arxiv.org/abs/2402.11411](https://arxiv.org/abs/2402.11411)

    本研究将幻觉问题视为对齐问题，并通过偏好调整解决，提出了POVID方法来生成反馈数据。

    

    指示跟随的视觉大语言模型（VLLMs）最近在各种任务上取得了显著进展。这些方法合并了强大的预训练视觉模型和大型语言模型（LLMs）。由于这些组件是分别训练的，所以需要通过联合训练额外的图像-语言对来对学习的表示进行对齐。这个过程并不完美，可能会导致模型产生幻觉-即使核心LLM非常客观，视觉支撑具有充分完整的表示，也会提供与图像不符合的答案。在这项工作中，我们将幻觉问题定义为一个对齐问题，通过偏好调整来解决。具体来说，我们提出了POVID来产生AI模型的反馈数据。我们使用地面真实指示作为首选响应，采用两阶段方法生成不受欢迎的数据。首先，我们提示GPT-4V注入合理的幻觉。

    arXiv:2402.11411v1 Announce Type: cross  Abstract: Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucin
    
[^180]: 多维度评估共情对话回复

    Multi-dimensional Evaluation of Empathetic Dialog Responses

    [https://arxiv.org/abs/2402.11409](https://arxiv.org/abs/2402.11409)

    提出了一个多维度的共情评估框架，同时衡量了说话者角度的表达意图和听者角度的感知共情，研究发现二者是相互关联的，感知共情与对话会话的满意水平呈高相关性。

    

    共情是有效和令人满意的对话沟通的关键元素，然而先前的研究大多集中在衡量表达的沟通意图上——即共情是如何表达的，忽略了对话也是一种涉及说话者和聆听者的协作实践。相反，我们提出了一个多维度的共情评估框架，扩展了现有工作，以衡量从说话者角度表达的意图以及从听者角度感知到的共情。将提出的框架应用于分析我们内部的客户服务对话表明，这两个维度（表达的意图类型和感知到的共情）是相互关联的，而感知到的共情与对话会话的满意水平具有很高的相关性。这个提出的框架仍需要受过训练的注释员的主观评估，这可能并不容易。

    arXiv:2402.11409v1 Announce Type: new  Abstract: Empathy is a critical element of effective and satisfactory conversational communication, yet previous studies in measuring conversational empathy mostly focus on expressed communicative intents -- in which way empathy is expressed, ignoring the fact that conversation is also a collaborative practice involving both speakers and listeners. In contrast, we propose a multi-dimensional empathy evaluation framework that extends upon existing work to measure both expressed intents from the speaker's perspective and perceived empathy from the listener's perspective. Applying the proposed framework to analyzing our internal customer-service dialogue shows that the two dimensions (expressed intent types and perceived empathy) are inter-connected, while perceived empathy has high correlation with the satisfactory level of dialogue sessions. This proposed framework still requires subjective assessments from trained annotators, which can be non-triv
    
[^181]: 不要走向极端：揭示LLMs在隐式仇恨言论检测中的过度敏感性和校准限制

    Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection

    [https://arxiv.org/abs/2402.11406](https://arxiv.org/abs/2402.11406)

    本文研究了LLMs在检测隐式仇恨言论和表达信心时的能力，发现LLMs存在两个极端：对可能引起公平问题的群体或话题表现出过于敏感，同时置信度评分过度集中在一个范围内。

    

    大型语言模型（LLMs）的公平性和可信度越来越受到关注。隐式仇恨言论，利用间接语言传达仇恨意图，占据实践中的重要部分。然而，LLMs有效解决这一问题的程度尚未得到充分审查。本文探讨了LLMs检测隐式仇恨言论（分类任务）以及对其响应的信心进行表达（校准任务）的能力。我们的评估细致考虑了各种提示模式和主流的不确定性估计方法。我们的研究结果突出了LLMs展示了两个极端：（1）LLMs对可能导致公平性问题的群体或话题显示出过度的敏感性，导致将良性陈述错误分类为仇恨言论。 （2）LLMs对每种方法的置信度得分过度集中在一个固定范围上，无论数据集的复杂性如何也保持不变。

    arXiv:2402.11406v1 Announce Type: new  Abstract: The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complex
    
[^182]: k-SemStamp：一种基于聚类的语义水印用于检测机器生成的文本

    k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text

    [https://arxiv.org/abs/2402.11399](https://arxiv.org/abs/2402.11399)

    k-SemStamp是一种简单而有效的语义水印方案，通过利用k均值聚类代替LSH来提高鲁棒性和抽样效率，同时保持生成质量，为机器生成文本检测提供了更有效的工具。

    

    最近的水印生成算法在语言生成过程中注入可检测的签名，以便进行事后检测。虽然基于标记级别的水印容易受到改写攻击，但SemStamp (Hou等人，2023)在句子的语义表示上应用水印，并展示出很好的鲁棒性。SemStamp利用局部敏感哈希（LSH）来利用任意超平面对语义空间进行分区，导致在鲁棒性和速度之间存在次优的权衡。我们提出k-SemStamp，这是SemStamp的一个简单而有效的增强版，利用k均值聚类作为局部敏感哈希的替代方案，以了解内在的语义结构来分区嵌入空间。实验结果表明，k-SemStamp显著提高了其鲁棒性和抽样效率，同时保持生成质量，推进了更有效的机器生成文本检测工具。

    arXiv:2402.11399v1 Announce Type: new  Abstract: Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.
    
[^183]: 在比较之前进行推理：LLM增强的语义相似度度量用于领域专门文本分析

    Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis

    [https://arxiv.org/abs/2402.11398](https://arxiv.org/abs/2402.11398)

    通过利用LLM增强语义分析，开发了用于文本的相似度度量框架，可显著改善文本的语义相似性评估，并可扩展到其他专业领域。

    

    在这项研究中，我们利用LLM来增强语义分析，为文本开发相似度度量，解决传统无监督NLP度量（如ROUGE和BLEU）的局限性。我们开发了一个框架，其中LLM（例如GPT-4）用于零样本文本识别和放射学报告的标签生成，在那里这些标签然后被用作文本相似性的度量。通过在MIMIC数据集上测试所提出的框架，我们发现GPT-4生成的标签能够显著改善语义相似度评估，得分更接近临床实际情况比传统NLP度量。我们的工作展示了使用LLM进行高度专业领域的文本数据的语义分析的可能性，具有半定量推理结果。虽然该框架针对放射学报告相似性分析进行了实施，但其概念可以扩展到其他专门领域。

    arXiv:2402.11398v1 Announce Type: cross  Abstract: In this study, we leverage LLM to enhance the semantic analysis and develop similarity metrics for texts, addressing the limitations of traditional unsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs such as GPT-4 are employed for zero-shot text identification and label generation for radiology reports, where the labels are then used as measurements for text similarity. By testing the proposed framework on the MIMIC data, we find that GPT-4 generated labels can significantly improve the semantic similarity assessment, with scores more closely aligned with clinical ground truth than traditional NLP metrics. Our work demonstrates the possibility of conducting semantic analysis of the text data using semi-quantitative reasoning results by the LLMs for highly specialized domains. While the framework is implemented for radiology report similarity analysis, its concept can be extended to other specialized domains 
    
[^184]: 在不修改语言模型的情况下训练语言模型代理

    Training Language Model Agents without Modifying Language Models

    [https://arxiv.org/abs/2402.11359](https://arxiv.org/abs/2402.11359)

    提出一种新的方法，在不修改语言模型的情况下训练语言模型代理，通过进化代理的功能来解决下游任务

    

    研究人员和实践者最近已经将强大的大型语言模型（LLMs）重新定义为代理，使它们能够通过使用专门的功能自动化地完成复杂任务。为了促进LLM代理的发展，我们提出了一种在不修改LLM权重的情况下训练LLM代理的新范式，当LLM难以或无法进行修改时尤其有用。受到人类不断锻造工具以适应现实任务的启发，而不是改变我们的生物结构以适应一组静态工具，我们提出逐步锻造代理的功能，以更好地解决下游任务，而不是修改LLM权重。通过将这些功能视为可学习的“代理参数”并利用人工智能模型训练的基本思想，我们开发了AgentOptimizer，利用LLM更新代理的功能，并设计了一种代理训练算法

    arXiv:2402.11359v1 Announce Type: new  Abstract: Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with tw
    
[^185]: 改变了什么？将表征干预转化为自然语言

    What Changed? Converting Representational Interventions to Natural Language

    [https://arxiv.org/abs/2402.11355](https://arxiv.org/abs/2402.11355)

    将表征空间的反事实转化为自然语言，以分析和解释模型干预所引起的语言变化，并减轻分类中的偏见。

    

    针对语言模型（LMs）表征空间的干预方法已经被证明是影响模型行为的有效手段。这些方法被用来消除或改变模型表示中的人口统计信息（如性别）的编码，创建一个反事实的表示。然而，由于干预操作在表示空间内，准确理解它修改了哪些特征是一个挑战。我们展示了表征空间的反事实可以转化为自然语言的反事实。我们证明了这种方法使我们能够分析对应于给定表示空间干预的语言变化，并解释用于编码特定概念的特征。此外，由此产生的反事实可以用于减轻分类中的偏见。

    arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
    
[^186]: 理解长期记忆对大型语言模型驱动聊天机器人在公共卫生干预中自我披露的影响

    Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention

    [https://arxiv.org/abs/2402.11353](https://arxiv.org/abs/2402.11353)

    长期记忆的引入提高了大型语言模型驱动聊天机器人在公共卫生干预中的用户自我披露，但仍存在挑战，特别是在解决慢性健康状况和隐私问题方面。

    

    最近的大型语言模型(LLMs)有潜力通过开放式对话支持公共卫生监测，但在多次互动中很少保留关于个人的知识。通过添加长期记忆(LTM)来增强LLMs提供了改进参与度和自我披露的机会，但我们缺乏对LTM如何影响人们在公共卫生干预中与LLM驱动的聊天机器人互动的理解。我们通过分析1,252通话记录和对九名用户的访谈，研究了CareCall这种带有LTM的LLM驱动的语音聊天机器人的案例。我们发现LTM增强了健康披露，并通过提供熟悉感促进了用户对聊天机器人的积极看法。然而，我们也观察到通过LTM促进自我披露存在挑战，尤其是在解决慢性健康状况和隐私问题方面。我们讨论了LTM整合的考虑。

    arXiv:2402.11353v1 Announce Type: cross  Abstract: Recent large language models (LLMs) offer the potential to support public health monitoring by facilitating health disclosure through open-ended conversations but rarely preserve the knowledge gained about individuals across repeated interactions. Augmenting LLMs with long-term memory (LTM) presents an opportunity to improve engagement and self-disclosure, but we lack an understanding of how LTM impacts people's interaction with LLM-driven chatbots in public health interventions. We examine the case of CareCall -- an LLM-driven voice chatbot with LTM -- through the analysis of 1,252 call logs and interviews with nine users. We found that LTM enhanced health disclosure and fostered positive perceptions of the chatbot by offering familiarity. However, we also observed challenges in promoting self-disclosure through LTM, particularly around addressing chronic health conditions and privacy concerns. We discuss considerations for LTM integr
    
[^187]: 语言模型未学习的任务

    Tasks That Language Models Don't Learn

    [https://arxiv.org/abs/2402.11349](https://arxiv.org/abs/2402.11349)

    大型语言模型没有学习到语言的视听特性，在新的任务基准测试中表现较差，暴露了人类语言理解与语言模型感官处理能力之间的根本差距。

    

    我们认为，我们目前的大型语言模型（LLMs）没有学习到语言的某些特性。我们通过一系列任务（称为H-TEST）对语言的视听特性进行了实证研究。这一基准测试突显了人类语言理解与LLMs的感官受限处理能力之间的根本差距。支持我们的假设，1. 故意推理（思维链），2. 少量案例，或3. 同一模型系列的更强大LLM（LLaMA 2 13B->LLaMA 2 70B）并不能简单地带来H-TEST性能的改善。因此，我们特别将其与玛丽的哲学案例联系起来，她在感官受限环境中了解世界（Jackson，1986）。我们的实验表明，一些最强大的专有LLMs的表现接近于随机基准准确率50％，突显了极限。

    arXiv:2402.11349v1 Announce Type: cross  Abstract: We argue that there are certain properties of language that our current large language models (LLMs) don't learn. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-TEST. This benchmark highlights a fundamental gap between human linguistic comprehension, which naturally integrates sensory experiences, and the sensory-deprived processing capabilities of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -> LLaMA 2 70B) do not trivially bring improvements in H-TEST performance. Therefore, we make a particular connection to the philosophical case of Mary, who learns about the world in a sensory-deprived environment (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limit
    
[^188]: PhaseEvo：面向大型语言模型的统一上下文提示优化

    PhaseEvo: Towards Unified In-Context Prompt Optimization for Large Language Models

    [https://arxiv.org/abs/2402.11347](https://arxiv.org/abs/2402.11347)

    该研究提出了PhaseEvo，一个旨在实现提示指令和示例的联合优化的高效自动提示优化框架，结合了LLMs的生成能力和进化算法的全局搜索效率。

    

    制定大型语言模型（LLMs）的理想提示是一项具有挑战性的任务，需要显著的资源和专业人员的输入。现有工作将优化提示指令和上下文学习示例视为不同问题，导致提示性能次优。本研究通过建立统一的上下文提示优化框架来解决这一局限性，旨在实现提示指令和示例的联合优化。然而，在离散且高维的自然语言空间中制定这种优化引入了收敛和计算效率方面的挑战。为了克服这些问题，我们提出了PhaseEvo，这是一个结合了LLMs的生成能力和进化算法的全局搜索效率的高效自动提示优化框架。我们的框架采用多阶段设计，融合了创新的基于LLMs的变异

    arXiv:2402.11347v1 Announce Type: new  Abstract: Crafting an ideal prompt for Large Language Models (LLMs) is a challenging task that demands significant resources and expert human input. Existing work treats the optimization of prompt instruction and in-context learning examples as distinct problems, leading to sub-optimal prompt performance. This research addresses this limitation by establishing a unified in-context prompt optimization framework, which aims to achieve joint optimization of the prompt instruction and examples. However, formulating such optimization in the discrete and high-dimensional natural language space introduces challenges in terms of convergence and computational efficiency. To overcome these issues, we present PhaseEvo, an efficient automatic prompt optimization framework that combines the generative capability of LLMs with the global search proficiency of evolution algorithms. Our framework features a multi-phase design incorporating innovative LLM-based mut
    
[^189]: EVEDIT: 事件驱动的知识编辑与演绎编辑边界

    EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries

    [https://arxiv.org/abs/2402.11324](https://arxiv.org/abs/2402.11324)

    本论文提出了一种通过事件描述配对实现知识更新的事件驱动知识编辑方法，解决了现有知识编辑方法在编辑过程中遇到的不确定性问题。

    

    现实世界信息的动态性要求对大型语言模型（LLMs）进行高效的知识编辑（KE）以进行知识更新。然而，目前的KE方法通常操作于（主体，关系，客体）三元组，忽略了上下文信息和不同知识之间的关系。这种编辑方法可能遇到不确定的编辑边界，导致大量相关知识存在歧义：事先可以回答的查询在编辑后无法可靠地回答。本文通过引入一个KE的理论框架来分析这个问题，强调了一组被忽视的知识，这些知识在编辑过程中保持不变，并在编辑过程中进行知识推断，我们将其命名为演绎锚点。我们进一步通过提出一种新颖的基于事件的知识编辑任务来解决这个问题，该任务将事实与事件描述配对。这一任务不仅体现了对真实世界的更紧密模拟

    arXiv:2402.11324v1 Announce Type: new  Abstract: The dynamic nature of real-world information necessitates efficient knowledge editing (KE) in large language models (LLMs) for knowledge updating. However, current KE approaches, which typically operate on (subject, relation, object) triples, ignore the contextual information and the relation among different knowledge. Such editing methods could thus encounter an uncertain editing boundary, leaving a lot of relevant knowledge in ambiguity: Queries that could be answered pre-edit cannot be reliably answered afterward. In this work, we analyze this issue by introducing a theoretical framework for KE that highlights an overlooked set of knowledge that remains unchanged and aids in knowledge deduction during editing, which we name as the deduction anchor. We further address this issue by proposing a novel task of event-based knowledge editing that pairs facts with event descriptions. This task manifests not only a closer simulation of real-w
    
[^190]: MMMModal -- 多图像多音频多轮多模态

    MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal

    [https://arxiv.org/abs/2402.11297](https://arxiv.org/abs/2402.11297)

    这个模型是一种多模态大型语言模型，能够理解多图像、多音频和多模态信息，在英语和马来语之间切换，并通过使用SigLIP编码器和Whisper编码器实现了这一功能。

    

    我们的贡献是引入了一种突破性的多模态大型语言模型，旨在理解多图像、多音频和多图像多音频在单个多轮会话内。利用最先进的模型，我们利用SigLIP编码器用于视觉输入，Whisper编码器用于音频输入。值得注意的是，这种多模态大型语言模型是双语的，能够同时理解英语和马来语。我们自豪地推出了这个模型的两个版本：拥有11亿参数的TinyLlama和拥有70亿参数的Mistral。凭借其能够处理多样的模态和语言的能力，我们的模型代表了马来西亚背景和更广泛领域的重大进展。

    arXiv:2402.11297v1 Announce Type: new  Abstract: Our contribution introduces a groundbreaking multimodal large language model designed to comprehend multi-images, multi-audio, and multi-images-multi-audio within a single multiturn session. Leveraging state-of-the-art models, we utilize the SigLIP encoder for visual inputs and the Whisper Encoder for audio inputs. Notably, this multimodal large language model is bilingual, proficient in understanding both English and Malay simultaneously. We proudly unveil two versions of this model: TinyLlama with 1.1B parameters, and Mistral with 7B parameters. With its ability to navigate diverse modalities and languages, our model represents a significant advancement for the Malaysian context and beyond.   All models released at https://huggingface.co/collections/mesolitica/multimodal-malaysian-llm-65c6f893e03f78fa9e5c8859
    
[^191]: 分析人类和大型语言模型（LLM）的偏好

    Dissecting Human and LLM Preferences

    [https://arxiv.org/abs/2402.11296](https://arxiv.org/abs/2402.11296)

    本研究分析了人类和32种不同LLM的偏好，发现人类不太在意错误，偏好支持立场的回应，而先进的LLM更注重正确性、清晰性和无害性。

    

    作为模型响应的相对质量比较，人类和大型语言模型（LLM）的偏好在模型微调中作为共同的对齐目标和评估标准。然而，这些偏好仅反映了广泛趋势，导致了模型的可解释性和可控性较差，可能存在潜在的安全风险。本研究剖析了人类和32种不同LLM的偏好，以了解它们的定量组成，利用来自真实用户-模型对话的注释进行细粒度、场景化分析。我们发现人类对错误不太敏感，偏好支持其立场的回应，并在模型承认其局限性时表现出明显的不喜欢。相反，像GPT-4-Turbo这样的先进LLM更加强调正确性、清晰性和无害性。此外，大小相似的LLM倾向于展现出类似的偏好，无论它们的训练方法如何，并且为了对齐而进行的微调并不会导致显著的改变。

    arXiv:2402.11296v1 Announce Type: cross  Abstract: As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not sign
    
[^192]: OneBit:朝着极低比特大型语言模型迈进

    OneBit: Towards Extremely Low-bit Large Language Models

    [https://arxiv.org/abs/2402.11295](https://arxiv.org/abs/2402.11295)

    本文提出了一种名为OneBit的1位量化感知训练框架，可以将大型语言模型的权重矩阵量化为1位，为极低比特宽度的LLMs部署铺平了道路。

    

    模型量化使用低比特宽度值来表示模型的权重矩阵，这是减少部署高度期待的LLMs的存储和计算开销的一种有前途的方法。然而，现有的量化方法在比特宽度极小时性能严重下降，因此专注于利用4位或8位值来量化模型。本文大胆地将LLMs的权重矩阵量化为1位，为LLMs的极低比特宽度部署铺平了道路。为此，我们引入了一个名为OneBit的1位量化感知训练（QAT）框架，其中包括一种更好地量化LLMs的新颖的1位参数表示方法，以及基于矩阵分解的有效参数初始化方法，以提高QAT框架的收敛速度。充分的实验结果表明，OneBit取得了良好的性能（至少是非

    arXiv:2402.11295v1 Announce Type: new  Abstract: Model quantification uses low bit-width values to represent the weight matrices of models, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs. However, existing quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs. For this target, we introduce a 1-bit quantization-aware training (QAT) framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the QAT framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 83% of the non
    
[^193]: 使用大型语言模型的推理解决难题：一项调查

    Puzzle Solving using Reasoning of Large Language Models: A Survey

    [https://arxiv.org/abs/2402.11291](https://arxiv.org/abs/2402.11291)

    本调查通过将难题分为基于规则和无规则两类的独特分类法，通过各种方法评估了大型语言模型（LLMs）的表现，强调了在复杂难题情境中LLMs的挑战和人类类似推理之间的差距，突出了推动LLMs解谜能力和贡献于人工智能发展的必要性。

    

    探索大型语言模型（LLMs）在解决难题中的能力揭示了它们在人工智能中的潜力和挑战，标志着理解它们在复杂推理任务中的适用性迈出了重要的一步。本调查利用独特的分类法将难题分为基于规则和无规则两类，通过各种方法评估LLMs，包括提示技术、神经符号方法和微调。通过对相关数据集和基准的批判性审查，我们评估了LLMs在复杂难题场景中的表现，识别出复杂难题情境中的显著挑战。我们的研究结果突出了LLMs能力及类人推理之间的差距，特别是在需要高级逻辑推断的情况下。调查强调了需要新颖策略和更丰富数据集来提升LLMs的解谜能力并促进人工智能的发展。

    arXiv:2402.11291v1 Announce Type: cross  Abstract: Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in artificial intelligence, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy -- dividing puzzles into rule-based and rule-less categories -- to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's
    
[^194]: 中心嵌套结构中的缺失NP效应的性质：事件相关电位揭示晕眩错觉还是含糊解释？

    Grammaticality illusion or ambiguous interpretation? Event-related potentials reveal the nature of the missing-NP effect in Mandarin centre-embedded structures

    [https://arxiv.org/abs/2402.11282](https://arxiv.org/abs/2402.11282)

    汉语中缺失NP的双重中心嵌套结构并不是语法性错觉，而是动词歧义解释的含糊解释。

    

    在几种语言中，在双重中心嵌套结构中省略动词短语（VP）会产生一个语法性错觉。类似的错觉也表现在汉语缺失NP的双重中心嵌套结构中。然而，关于它的本质尚无共识。我们认为，与其把它看作是语法性错觉，不如将动词的歧义解释视为最能解释汉语中这一现象的方式。为了进一步支持这一假设，我们在减少复杂度的情况下进行了两项与自嵌入关系从句放置在句子主语位置相结合相近双中心嵌套结构的脑电图（EEG）实验。实验1表明，在这种结构中同样会表现出类似的现象，证据是缺少P600效应而存在N400效应。在实验2中，通过提供语义线索以减少歧义，消除了这种错觉，证据是存在P600效应。我们解释了这些结果

    arXiv:2402.11282v1 Announce Type: new  Abstract: In several languages, omitting a verb phrase (VP) in double centre-embedded structures creates a grammaticality illusion. Similar illusion also exhibited in Mandarin missing-NP double centre-embedded structures. However, there is no consensus on its very nature. Instead of treating it as grammaticality illusion, we argue that ambiguous interpretations of verbs can best account for this phenomenon in Mandarin. To further support this hypothesis, we conducted two electroencephalography (EEG) experiments on quasi double centre-embedded structures whose complexity is reduced by placing the self-embedding relative clauses into the sentence's subject position. Experiment 1 showed that similar phenomenon even exhibited in this structure, evidenced by an absence of P600 effect and a presence of N400 effect. In Experiment 2, providing semantic cues to reduce ambiguity dispelled this illusion, as evidenced by a P600 effect. We interpret the result
    
[^195]: 大型多模态模型能揭示图像背后的深层语义吗？

    Can Large Multimodal Models Uncover Deep Semantics Behind Images?

    [https://arxiv.org/abs/2402.11281](https://arxiv.org/abs/2402.11281)

    该论文引入了一个名为DEEPEVAL的基准，用于评估大型多模态模型对视觉深层语义的能力，揭示了现有LMMs在深层语义理解方面与人类之间存在显著差距。

    

    理解图像的深层语义在社交媒体主导的时代至关重要。然而，当前研究主要集中在对图像的表面描述上，揭示了在对内在深层语义进行系统调查方面的明显不足。在这项工作中，我们引入了DEEPEVAL，一个全面的基准，用于评估大型多模态模型(LMMs)对视觉深层语义的能力。 DEEPEVAL 包括人工注释的数据集和三个渐进的子任务：细粒度描述选择、深度标题匹配和深层语义理解。利用 DEEPEVAL，我们评估了9个开源LMMs和GPT-4V(ision)。我们的评估显示了现有LMMs与人类在深层语义理解能力上存在着实质差距。例如，即使在图像描述方面达到与人类可比的表现，GPT-4V在理解深层语义方面仍落后于人类30%。进一步的分析表明

    arXiv:2402.11281v1 Announce Type: new  Abstract: Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and GPT-4V(ision).Our evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, GPT-4V is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis indi
    
[^196]: 多透视一致性增强大型语言模型中的置信度估计

    Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models

    [https://arxiv.org/abs/2402.11279](https://arxiv.org/abs/2402.11279)

    多透视一致性方法为大型语言模型中的置信度估计带来改进，能有效减轻过度自信问题，并在多个数据集上实现最先进的性能。

    

    在大型语言模型（LLMs）的部署中，准确的置信度估计对于评估模型预测的可信度至关重要。然而，现有方法经常无法克服对错误答案的过度自信问题。在这项工作中，我们专注于改进大型语言模型的置信度估计。考虑到语言模型中自我意识的脆弱性，我们引入了一种多透视一致性（MPC）方法。我们利用模型内不同透视角度（MPC-Internal）和不同模型之间（MPC-Across）的互补见解来缓解由单一视角产生的过度自信问题。对八个公开数据集的实验结果显示，我们的MPC实现了最先进的性能。进一步的分析表明，MPC能够减轻过度自信问题，并且能够有效扩展到其他模型中。

    arXiv:2402.11279v1 Announce Type: cross  Abstract: In the deployment of large language models (LLMs), accurate confidence estimation is critical for assessing the credibility of model predictions. However, existing methods often fail to overcome the issue of overconfidence on incorrect answers. In this work, we focus on improving the confidence estimation of large language models. Considering the fragility of self-awareness in language models, we introduce a Multi-Perspective Consistency (MPC) method. We leverage complementary insights from different perspectives within models (MPC-Internal) and across different models (MPC-Across) to mitigate the issue of overconfidence arising from a singular viewpoint. The experimental results on eight publicly available datasets show that our MPC achieves state-of-the-art performance. Further analyses indicate that MPC can mitigate the problem of overconfidence and is effectively scalable to other models.
    
[^197]: 人工智能与人类在通信时代的互动：自噬使得大型模型实现局部最优

    Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima

    [https://arxiv.org/abs/2402.11271](https://arxiv.org/abs/2402.11271)

    合成信息更可能被大型模型纳入训练数据集和传播中，大型模型在传递信息时倾向于有选择地修改和丢失特定内容

    

    随着大型语言和多模态模型在社会信息处理中的重要性日益增加，引发了关于社会安全和伦理的争论。然而，很少有研究从人类和人工智能系统相互作用的综合视角分析这些限制。本研究调查了人类和大型模型在通信中作为关键联系的偏见和偏好。为实现此目的，我们设计了一个多模态数据集和三个不同的实验，评估生成模型在其作为信息生产者和传播者的角色中的表现。我们的主要发现突出显示，合成信息更有可能被纳入模型训练数据集和消息传递中，而人类生成的信息。此外，大型模型在以信息传递者的角色时，倾向于有选择地修改和丢失特定内容。在概念上，我们提出了两种真实的自

    arXiv:2402.11271v1 Announce Type: new  Abstract: The increasing significance of large language and multimodal models in societal information processing has ignited debates on social safety and ethics. However, few studies have approached the analysis of these limitations from the comprehensive perspective of human and artificial intelligence system interactions. This study investigates biases and preferences when humans and large models are used as key links in communication. To achieve this, we design a multimodal dataset and three different experiments to evaluate generative models in their roles as producers and disseminators of information. Our main findings highlight that synthesized information is more likely to be incorporated into model training datasets and messaging than human-generated information. Additionally, large models, when acting as transmitters of information, tend to modify and lose specific content selectively. Conceptually, we present two realistic models of auto
    
[^198]: MoRAL: MoE增强LoRA用于LLM的终身学习

    MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning

    [https://arxiv.org/abs/2402.11260](https://arxiv.org/abs/2402.11260)

    MoRAL结合了MoE的多任务能力和LoRA的微调能力，采用问答对作为输入，实现了LLM的有效终身学习。

    

    本文提出了一种名为MoRAL的方法，即Mixture-of-Experts增强低秩适应性用于LLM的终身学习。 MoRAL将MoE的多任务能力与LoRA的微调能力相结合，实现了LLM的有效终身学习。与传统方法不同，MoRAL依赖于简单的问答对作为输入，这是一种更实用和有效的鲁棒学习策略。鉴于新数据设置，我们引入了一个新的评估基准，即LLM的终身学习（5L-bench），包括一个新的精心策划的问答对数据集，以及一组用于在开放式和封闭式环境下严格评估MoRAL的评估指标。实验评估表明，在开放式环境下，LLM在短时间内迅速学习

    arXiv:2402.11260v1 Announce Type: cross  Abstract: Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to 
    
[^199]: C-ICL：对比上下文学习用于信息抽取

    C-ICL: Contrastive In-context Learning for Information Extraction

    [https://arxiv.org/abs/2402.11254](https://arxiv.org/abs/2402.11254)

    C-ICL提出了一种利用正确和不正确样本构建进行上下文学习示范的新颖少样本技术，通过提示不仅包含正样本还包含背后推理，增强了LLMs提取实体和关系的能力。

    

    最近，人们越来越感兴趣于探索先进大型语言模型（LLMs）在信息抽取（IE）领域的能力，特别是专注于命名实体识别（NER）和关系提取（RE）相关的任务。尽管研究人员正通过LLMs进行少样本信息抽取的上下文学习，他们往往只专注于使用正确或正向示例来展示，而忽视了将不正确或负向示例纳入学习过程的潜在价值。在本文中，我们提出了C-ICL，一种利用正确和不正确样本构建进行上下文学习示范的新颖少样本技术。这种方法通过利用不仅包含正样本还包含背后推理的提示，增强了LLMs提取实体和关系的能力。

    arXiv:2402.11254v1 Announce Type: new  Abstract: Recently, there has been increasing interest in exploring the capabilities of advanced large language models (LLMs) in the field of information extraction (IE), specifically focusing on tasks related to named entity recognition (NER) and relation extraction (RE). Although researchers are exploring the use of few-shot information extraction through in-context learning with LLMs, they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process. In this paper, we present c-ICL, a novel few-shot technique that leverages both correct and incorrect sample constructions to create in-context learning demonstrations. This approach enhances the ability of LLMs to extract entities and relations by utilizing prompts that incorporate not only the positive samples but also the reasoning behind them. This method allows for the identi
    
[^200]: 通过基于政策的自我判断来对齐大型语言模型

    Aligning Large Language Models by On-Policy Self-Judgment

    [https://arxiv.org/abs/2402.11253](https://arxiv.org/abs/2402.11253)

    本文提出了一个新颖的对齐框架SELF-JUDGE，通过增加式监督微调（JSFT）训练一个同时充当策略和评判器的单一模型，实现了参数高效的基于政策学习，无需额外的奖励模型。

    

    为了使大型语言模型与人类偏好保持一致，现有研究要么利用单独的奖励模型（RM）执行基于政策的学习，要么通过放弃基于政策的学习和对独立RM的需求简化训练过程。在本文中，我们提出了一个新颖的对齐框架SELF-JUDGE，它既是(1) 基于政策的学习，又是(2) 参数高效的，因为它不需要额外的RM来评估样本进行基于政策的学习。为此，我们提出了增强式监督微调（JSFT）来训练一个单一模型，作为策略和评判器。具体来说，我们将一对一判断任务视为指导式任务的特殊情况，从响应对中选择更好的响应。因此，得到的模型可以评判当前策略的即时响应偏好，从自身初始化。实验结果显示了SELF-JUDGE的有效性，优于基线模型。

    arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
    
[^201]: LLM可以通过超参数感知生成实现自我调节

    LLM can Achieve Self-Regulation via Hyperparameter Aware Generation

    [https://arxiv.org/abs/2402.11251](https://arxiv.org/abs/2402.11251)

    LLM通过超参数感知生成实现自我调节，消除了大量手动调整的需求。

    

    在大型语言模型（LLMs）领域，用户通常采用不同的解码策略并调整超参数以控制生成的文本。然而，一个关键问题出现了：LLMs是否意识到这些解码策略的存在并且能够自我调节？当前的解码生成过程通常依赖于根据不同任务和需求调整超参数的经验和启发式手动调整。然而，这个过程通常很繁琐，解码超参数可能并不总是对每个样本都是最佳的。为了解决上述挑战，我们提出了一种称为超参数感知生成（HAG）的新颖文本生成范式。通过利用超参数感知指令调整，LLM可以自主确定基于输入样本的最佳解码策略和配置，实现自我调节。我们的方法消除了大量手动调整的需求。

    arXiv:2402.11251v1 Announce Type: new  Abstract: In the realm of Large Language Models (LLMs), users commonly employ diverse decoding strategies and adjust hyperparameters to control the generated text. However, a critical question emerges: Are LLMs conscious of the existence of these decoding strategies and capable of regulating themselves? The current decoding generation process often relies on empirical and heuristic manual adjustments to hyperparameters based on types of tasks and demands. However, this process is typically cumbersome, and the decoding hyperparameters may not always be optimal for each sample. To address the aforementioned challenges, we propose a novel text generation paradigm termed Hyperparameter Aware Generation (HAG). By leveraging hyperparameter-aware instruction tuning, the LLM autonomously determines the optimal decoding strategy and configs based on the input samples, enabling self-regulation. Our approach eliminates the need for extensive manual tuning, o
    
[^202]: 大型语言模型能够进行基于关系的论证挖掘吗？

    Can Large Language Models perform Relation-based Argument Mining?

    [https://arxiv.org/abs/2402.11243](https://arxiv.org/abs/2402.11243)

    大型语言模型在处理关系型论证挖掘方面表现更好，能够显著超过目前最佳基准线，并且在十个数据集上进行了实验验证。

    

    论据挖掘（AM）是从文本中自动提取论据、它们的组成部分和/或论据和组成部分之间关系的过程。随着支持在线辩论的平台数量不断增加，对AM的需求变得愈发迫切，特别是为了支持下游任务。基于关系的AM（RbAM）是一种关注识别论据之间协议（支持）和不同意（攻击）关系的AM形式。RbAM是一个具有挑战性的分类任务，现有方法无法令人满意地执行。在本文中，我们展示了通用型大型语言模型（LLMs），经过适当的调整和提示，可以显著优于表现最好的（基于RoBERTa的）基准线。具体来说，我们对两个开源LLM（Llama-2和Mistral）在十个数据集上进行了实验。

    arXiv:2402.11243v1 Announce Type: cross  Abstract: Argument mining (AM) is the process of automatically extracting arguments, their components and/or relations amongst arguments and components from text. As the number of platforms supporting online debate increases, the need for AM becomes ever more urgent, especially in support of downstream tasks. Relation-based AM (RbAM) is a form of AM focusing on identifying agreement (support) and disagreement (attack) relations amongst arguments. RbAM is a challenging classification task, with existing methods failing to perform satisfactorily. In this paper, we show that general-purpose Large Language Models (LLMs), appropriately primed and prompted, can significantly outperform the best performing (RoBERTa-based) baseline. Specifically, we experiment with two open-source LLMs (Llama-2 and Mistral) with ten datasets.
    
[^203]: 使用动态属性图控制大型语言模型的文本生成

    Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs

    [https://arxiv.org/abs/2402.11218](https://arxiv.org/abs/2402.11218)

    该研究提出了一个名为DATG的框架，利用动态属性图调节关键属性词和关键反属性词的发生，实现了有效的属性控制，在毒性缓解和情感转化任务中表现出19.29%的控制准确性增强。

    

    控制文本生成（CTG）旨在生成具有特定所需属性的文本。在本研究中，我们引入了一个可插拔的CTG框架，用于大型语言模型（LLMs），名为基于动态属性图的控制文本生成（DATG）。该框架利用属性评分器评估由LLMs生成的句子的属性，并构建动态属性图。DATG调节关键属性词和关键反属性词的发生，实现了有效的属性控制，而不影响模型的原始能力。我们在两个任务中跨四个数据集进行实验：毒性缓解和情感转化，利用五个LLMs作为基础模型。我们的发现突出了在控制准确性方面的显着提高，实现了在四个数据集中最有利的任务中对基线方法的峰值改进为19.29％。此外，我们观察到显着

    arXiv:2402.11218v1 Announce Type: new  Abstract: Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29% over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant 
    
[^204]: Asclepius：用于医学多模态大语言模型的频谱评估基准

    Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models

    [https://arxiv.org/abs/2402.11217](https://arxiv.org/abs/2402.11217)

    Asclepius是一个新的医学多模态大语言模型基准，旨在为可信的Med-MLLMs评估提供单独且临床代表性的评估方案。

    

    arXiv:2402.11217v1 公告类型：新摘要：医学多模态大语言模型（Med-MLLMs）的重大突破通过强大的信息综合和医疗决策支持改造了现代医疗保健。然而，由于现实世界诊断框架的复杂性涵盖了各种医学专业，并涉及复杂的临床决策，这些模型通常在不适合Med-MLLMs的基准上进行评估。此外，由于Med-MLLMs是在大量公开可用数据集上进行训练的，这些基准容易出现数据泄露。因此，需要一个独立且临床代表性的基准用于可信的Med-MLLMs评估。为此，我们引入了Asclepius，一个新颖的Med-MLLM基准，严格和全面评估模型在不同医学专业（心血管、胃肠等）和不同诊断能力（知觉、疾病分析等）方面的能力。

    arXiv:2402.11217v1 Announce Type: new  Abstract: The significant breakthroughs of Medical Multi-Modal Large Language Models (Med-MLLMs) renovate modern healthcare with robust information synthesis and medical decision support. However, these models are often evaluated on benchmarks that are unsuitable for the Med-MLLMs due to the intricate nature of the real-world diagnostic frameworks, which encompass diverse medical specialties and involve complex clinical decisions. Moreover, these benchmarks are susceptible to data leakage, since Med-MLLMs are trained on large assemblies of publicly available data. Thus, an isolated and clinically representative benchmark is highly desirable for credible Med-MLLMs evaluation. To this end, we introduce Asclepius, a novel Med-MLLM benchmark that rigorously and comprehensively assesses model capability in terms of: distinct medical specialties (cardiovascular, gastroenterology, etc.) and different diagnostic capacities (perception, disease analysis, e
    
[^205]: 警惕您的代理人！调查基于LLM的代理人的后门威胁

    Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents

    [https://arxiv.org/abs/2402.11208](https://arxiv.org/abs/2402.11208)

    这项工作调查了基于LLM的代理人面临的后门攻击威胁，并提出了一般框架和不同形式的后门攻击分析。

    

    利用大型语言模型LLM的快速发展，已经开发出了用于处理各种实际应用（包括金融、医疗保健和购物等）的基于LLM的代理人。在应用过程中确保LLM代理人的可靠性和安全性至关重要。然而，目前对LLM代理人的安全性问题尚未得到充分探讨。本工作首次探讨了典型安全威胁之一，即对LLM代理人的后门攻击。我们首先制定了一个代理人后门攻击的一般框架，然后对不同形式的代理人后门攻击进行了彻底分析。具体而言，从最终攻击结果的角度来看，攻击者可以选择操纵最终输出分布，或者仅在中间推理过程中引入恶意行为，同时保持最终输出的正确性。此外，前一类可以分为

    arXiv:2402.11208v1 Announce Type: cross  Abstract: Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided
    
[^206]: 探索ChatGPT在下一代信息检索中的应用：机遇与挑战

    Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges

    [https://arxiv.org/abs/2402.11203](https://arxiv.org/abs/2402.11203)

    ChatGPT作为信息检索领域的关键技术，不断挑战传统范式，带来了新的机遇和挑战，同时超越了之前的GPT-3模型。

    

    人工智能（AI）的快速发展凸显了ChatGPT作为信息检索（IR）领域中的关键技术。与之前的模型不同，ChatGPT提供了显著的好处，吸引了行业和学术界的关注。一些人认为ChatGPT是一项开创性的创新，而另一些人将其成功归因于产品开发和市场策略的有效整合。ChatGPT的出现，以及与OpenAI的GPT-4一起，标志着生成式AI的新阶段，产生的内容与训练样本有所不同，并超越了以往的GPT-3模型的能力。与信息检索任务中的传统监督学习方法不同，ChatGPT挑战了现有的范式，带来了关于文本质量保证、模型偏差和效率方面的新挑战和机遇。本文旨在研究ChatGPT对信息检索任务的影响，并提供

    arXiv:2402.11203v1 Announce Type: cross  Abstract: The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT as a pivotal technology in the field of information retrieval (IR). Distinguished from its predecessors, ChatGPT offers significant benefits that have attracted the attention of both the industry and academic communities. While some view ChatGPT as a groundbreaking innovation, others attribute its success to the effective integration of product development and market strategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in Generative AI, generating content that is distinct from training examples and exceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the traditional supervised learning approach in IR tasks, ChatGPT challenges existing paradigms, bringing forth new challenges and opportunities regarding text quality assurance, model bias, and efficiency. This paper seeks to examine the impact of ChatGPT on IR tasks and offe
    
[^207]: 在知识图谱中对多跳推理中思维链的直接评估

    Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs

    [https://arxiv.org/abs/2402.11199](https://arxiv.org/abs/2402.11199)

    本文通过利用知识图谱，提出了一种新颖的CoT推理能力评估模式，揭示了大型语言模型在多跳问题回答中推理知识和生成CoT的准确性之间的显著差异

    

    大型语言模型(LLMs)展示了强大的推理能力，当提示生成链式思维（CoT）解释时，以及答案。然而，先前关于LLMs评估的研究仅关注答案准确性，忽略了生成的CoT的正确性。在本文中，我们通过利用知识图谱（KGs），深入探讨LLMs在多跳问题回答中的CoT推理能力。我们提出了一种新颖的辨别式和生成式CoT评估范式，以评估LLMs的推理知识和生成CoT的准确性。通过在2个多跳问题回答数据集上对5个不同系列的LLMs进行的实验，我们发现LLMs具有足够的知识来执行推理。然而，LLMs生成的CoT推理的准确性与答案准确性之间存在显著差异，表明它们经常通过错误的方式得出正确答案。

    arXiv:2402.11199v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate strong reasoning abilities when prompted to generate chain-of-thought (CoT) explanations alongside answers. However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT. In this paper, we delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question answering by utilizing knowledge graphs (KGs). We propose a novel discriminative and generative CoT evaluation paradigm to assess LLMs' knowledge of reasoning and the accuracy of the generated CoT. Through experiments conducted on 5 different families of LLMs across 2 multi-hop question-answering datasets, we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorr
    
[^208]: 基于质心的高效最小贝叶斯风险解码

    Centroid-Based Efficient Minimum Bayes Risk Decoding

    [https://arxiv.org/abs/2402.11197](https://arxiv.org/abs/2402.11197)

    基于质心的MBR解码方法提高了解码速度和翻译质量，在实验中表现优异。

    

    最小贝叶斯风险（MBR）解码通过使用COMET实现了一流的翻译性能，该神经度量与人类评估具有很高的相关性。然而，MBR解码需要二次时间，因为它计算翻译假设与所有参考翻译之间的期望分数。我们提出了基于质心的MBR（CBMBR）解码以提高MBR解码的速度。我们的方法在特征空间中对参考翻译进行聚类，然后使用每个簇的质心计算分数。实验结果表明，我们的CBMBR不仅将期望分数计算的解码速度提高了6.9倍，而且在WMT'22 En$\leftrightarrow$Ja、En$\leftrightarrow$De、En$\leftrightarrow$Zh以及WMT'23 En$\leftrightarrow$Ja翻译任务中，在翻译质量上超过了香草MBR解码，最高提高了0.5 COMET。

    arXiv:2402.11197v1 Announce Type: new  Abstract: Minimum Bayes risk (MBR) decoding achieved state-of-the-art translation performance by using COMET, a neural metric that has a high correlation with human evaluation. However, MBR decoding requires quadratic time since it computes the expected score between a translation hypothesis and all reference translations. We propose centroid-based MBR (CBMBR) decoding to improve the speed of MBR decoding. Our method clusters the reference translations in the feature space, and then calculates the score using the centroids of each cluster. The experimental results show that our CBMBR not only improved the decoding speed of the expected score calculation 6.9 times, but also outperformed vanilla MBR decoding in translation quality by up to 0.5 COMET in the WMT'22 En$\leftrightarrow$Ja, En$\leftrightarrow$De, En$\leftrightarrow$Zh, and WMT'23 En$\leftrightarrow$Ja translation tasks.
    
[^209]: 在金融文档问答中评估LLMs的数学推理能力

    Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering

    [https://arxiv.org/abs/2402.11194](https://arxiv.org/abs/2402.11194)

    通过实验评估了LLMs在金融表格问答中的数学推理能力，发现引入了一种新型提示技术，能够在性能上胜过其他基线模型

    

    大型语言模型（LLMs）在自然语言理解方面表现出色，但它们在具有结构化表格和非结构化文本混合的复杂数学推理方面的能力尚不确定。本研究探讨了LLMs在四个金融表格问答数据集上的数学推理能力：TATQA、FinQA、ConvFinQA和Multihiertt。通过对各种模型和提示技术进行广泛实验，我们评估了LLMs如何适应复杂表格和数学任务。我们关注对表格复杂性的敏感性以及在增加算术推理步骤数量时性能变化。结果揭示了LLMs处理半结构化表格中复杂数学场景的能力和局限性。最终，我们引入了一种针对半结构化文档的新型提示技术，在性能方面与其他基线相匹配或胜过，并提供了对LLMs能力的微妙理解。

    arXiv:2402.11194v1 Announce Type: new  Abstract: Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding 
    
[^210]: 如果你讲我的语言，我会更好地学习：使用风格对齐响应调整增强大型语言模型微调

    I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments

    [https://arxiv.org/abs/2402.11192](https://arxiv.org/abs/2402.11192)

    将微调过程中的实际响应风格与大型语言模型固有风格相匹配能够产生更好的学习结果，开发的方法通过最小程度地调整模型响应来避免过拟合。

    

    使用小数据集为特定任务微调大型语言模型(LLMs)是一个普遍遇到的但复杂的挑战。在有限的示例上过多拟合可能会对模型的泛化能力和保留原始技能产生负面影响。我们的研究探讨了在微调过程中地实际响应风格的影响。我们发现将地实际响应风格与LLM固有风格匹配会产生更好的学习结果。基于这一观点，我们开发了一种方法，最小程度地修改LLM的现有响应以更正错误，使用这些调整后的响应作为训练目标。这种技术能够实现与模型固有响应风格一致的精确更正，维护模型的核心能力，从而避免过多拟合。我们的研究结果表明，这种方法不仅提高了LLM的特定任务准确性，而且关键地

    arXiv:2402.11192v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially
    
[^211]: 知识图谱辅助自动生成体育新闻

    Knowledge Graph Assisted Automatic Sports News Writing

    [https://arxiv.org/abs/2402.11191](https://arxiv.org/abs/2402.11191)

    该论文提出了一种利用知识图谱和多阶段学习模型自动生成体育新闻的方法。

    

    在这篇论文中，我们提出了一种新颖的方法，用于自动生成体育新闻，该方法采用一种独特的算法，从现场文本广播中提取关键时刻，并使用这些时刻创建新闻的初稿。这一初稿通过整合专门设计的体育知识图谱中的关键细节和背景信息得以进一步完善。该图谱包含5,893个实体，分为三个不同的概念类别，通过四种关系类型相互连接，具有27个独特属性。此外，我们通过结合卷积神经网络和变压器编码器创建了一个多阶段学习模型。该模型使用卷积神经网络表达实体-任务交互，并利用变压器编码器在查询集中丰富实体表示。它还包括一个处理器，用于计算不完整三元组的匹配分数，解决了少样本知识匮乏的问题。

    arXiv:2402.11191v1 Announce Type: new  Abstract: In this paper, we present a novel method for automatically generating sports news, which employs a unique algorithm that extracts pivotal moments from live text broadcasts and uses them to create an initial draft of the news. This draft is further refined by incorporating key details and background information from a specially designed sports knowledge graph. This graph contains 5,893 entities, which are classified into three distinct conceptual categories, interconnected through four relationship types, and characterized by 27 unique attributes. In addition, we create a multi-stage learning model by combining convolutional neural networks and a transformer encoder. This model expresses entity-task interactions using convolutional neural networks and enriches entity representations in the query set with the transformer encoder. It also includes a processor to compute matching scores for incomplete triples, addressing few-shot knowledge g
    
[^212]: 大型语言模型（LLMs）中的性别偏见披露和缓解

    Disclosure and Mitigation of Gender Bias in LLMs

    [https://arxiv.org/abs/2402.11190](https://arxiv.org/abs/2402.11190)

    提出了一种基于条件生成的间接探测框架，揭示了LLMs中的显性和隐性性别偏见，并探讨了三种缓解LLMs偏见的方法。

    

    大型语言模型（LLMs）可能会生成带有偏见的回复。然而，先前的直接探测技术包含性别提及或预定义的性别刻板印象，这些很难全面收集。因此，我们提出了一种基于条件生成的间接探测框架。这种方法旨在诱使LLMs披露其性别偏见，即使没有明确的性别或刻板印象提及。我们探索了三种不同的策略，以披露LLMs中的显性和隐性性别偏见。我们的实验表明，在大多数情况下，所有经过测试的LLMs均表现出明确和/或隐性性别偏见，即使输入中没有性别刻板印象。此外，模型尺寸增加或模型对齐会在大多数情况下放大偏见。此外，我们通过超参数调整、指导指引和去偏调整来研究三种缓解LLMs偏见的方法。值得注意的是，即使在没有明确性别刻板印象的情况下，这些方法也被证明是有效的。

    arXiv:2402.11190v1 Announce Type: new  Abstract: Large Language Models (LLMs) can generate biased responses. Yet previous direct probing techniques contain either gender mentions or predefined gender stereotypes, which are challenging to comprehensively collect. Hence, we propose an indirect probing framework based on conditional generation. This approach aims to induce LLMs to disclose their gender bias even without explicit gender or stereotype mentions. We explore three distinct strategies to disclose explicit and implicit gender bias in LLMs. Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs. In addition, an increased model size or model alignment amplifies bias in most cases. Furthermore, we investigate three methods to mitigate bias in LLMs via Hyperparameter Tuning, Instruction Guiding, and Debias Tuning. Remarkably, these methods prove effective even in the absence of explici
    
[^213]: LaCo：通过层叠实现大型语言模型的剪枝

    LaCo: Large Language Model Pruning via Layer Collapse

    [https://arxiv.org/abs/2402.11187](https://arxiv.org/abs/2402.11187)

    提出了一种名为Layer Collapse（LaCo）的简明的逐层剪枝方法，使得大型语言模型可以在保持模型结构的同时迅速减小尺寸，并在剪枝比例达到25-30%时保持超过80%的平均任务性能，明显优于现有最先进的结构化剪枝方法。

    

    基于Transformer的大型语言模型（LLMs）正经历着尺寸扩大的明显趋势，这给模型的训练和推理带来了相当大的成本。然而，现有的方法如模型量化、知识蒸馏和模型剪枝受到各种问题的限制，包括硬件支持限制、需要大量的训练和对模型内部结构的改变。在本文中，我们提出了一种简洁的逐层剪枝方法，称为Layer Collapse（LaCo），其中后置模型层折叠到前置层，使模型尺寸迅速减小同时保持模型结构。综合实验表明，我们的方法在剪枝比例达到25-30%时，保持了超过80%的平均任务性能，明显优于现有最先进的结构化剪枝方法。我们还进行了后训练实验以确认所提方法的有效性。

    arXiv:2402.11187v1 Announce Type: cross  Abstract: Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the internal structure of the model. In this paper, we propose a concise layer-wise pruning method called \textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\% at pruning ratios of 25-30\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the proposed pr
    
[^214]: RENOVI：一个旨在纠正社会文化对话中规范违反的基准

    RENOVI: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations

    [https://arxiv.org/abs/2402.11178](https://arxiv.org/abs/2402.11178)

    提供了一个大规模语料库ReNoVi，帮助互动AI系统理解和纠正社会规范违反，包括人类编写对话和ChatGPT生成的合成对话，从而弥补数据稀缺并评估模型与人类在社会规范认知上的一致性。

    

    规范违反发生在个体未能遵守文化接受行为时，可能会导致潜在冲突。纠正规范违反需要对正在发生的微妙社会认知和文化敏感。为了赋予互动人工智能系统纠正能力，我们提供了ReNoVi - 一个包含9,258个多轮对话注释有社会规范的大规模语料库，并定义了一系列任务以帮助逐步理解和纠正规范违反。ReNoVi包含两个部分：512个人类编写的对话（真实数据）和通过提示学习由ChatGPT生成的8,746个合成对话。尽管收集充足的人类编写数据成本高昂，但合成对话提供了适量的数据帮助缓解训练数据的稀缺，并且评估了LLM与人类在社会规范认知方面的一致性的机会。因此，我们利用ChatGPT的优势来进行...

    arXiv:2402.11178v1 Announce Type: new  Abstract: Norm violations occur when individuals fail to conform to culturally accepted behaviors, which may lead to potential conflicts. Remediating norm violations requires social awareness and cultural sensitivity of the nuances at play. To equip interactive AI systems with a remediation ability, we offer ReNoVi - a large-scale corpus of 9,258 multi-turn dialogues annotated with social norms, as well as define a sequence of tasks to help understand and remediate norm violations step by step. ReNoVi consists of two parts: 512 human-authored dialogues (real data), and 8,746 synthetic conversations generated by ChatGPT through prompt learning. While collecting sufficient human-authored data is costly, synthetic conversations provide suitable amounts of data to help mitigate the scarcity of training data, as well as the chance to assess the alignment between LLMs and humans in the awareness of social norms. We thus harness the power of ChatGPT to g
    
[^215]: 基于问答的全面中文电子健康记录信息提取流水线

    A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction

    [https://arxiv.org/abs/2402.11177](https://arxiv.org/abs/2402.11177)

    提出了一种基于问答的新方法，自动生成训练数据，用于QA模型的迁移学习，通过预处理模块解决了传统方法无法处理的信息提取类型挑战，实现了在电子健康记录中的信息提取任务，表现出色并能有效应对少样本或零样本情况。

    

    电子健康记录（EHRs）对研究和应用具有重要价值。作为一种新的信息提取方式，问答（QA）可以提取比传统方法更灵活的信息，且更易于临床研究人员使用，但其进展受到标注数据稀缺的阻碍。在本文中，我们提出了一种新颖的方法，可自动生成训练数据，用于QA模型的迁移学习。我们的流水线集成了一个预处理模块，处理了与提取型QA框架不太兼容的提取类型所带来的挑战，包括具有不连续答案和多对一关系的情况。所得的QA模型在EHRs中的信息提取子任务上表现出色，能够有效处理包含是非问题的少样本或零样本设置。案例研究和消融研究证明了每个组件的必要性。

    arXiv:2402.11177v1 Announce Type: new  Abstract: Electronic health records (EHRs) hold significant value for research and applications. As a new way of information extraction, question answering (QA) can extract more flexible information than conventional methods and is more accessible to clinical researchers, but its progress is impeded by the scarcity of annotated data. In this paper, we propose a novel approach that automatically generates training data for transfer learning of QA models. Our pipeline incorporates a preprocessing module to handle challenges posed by extraction types that are not readily compatible with extractive QA frameworks, including cases with discontinuous answers and many-to-one relationships. The obtained QA model exhibits excellent performance on subtasks of information extraction in EHRs, and it can effectively handle few-shot or zero-shot settings involving yes-no questions. Case studies and ablation studies demonstrate the necessity of each component in 
    
[^216]: KnowTuning：针对大型语言模型的知识感知微调

    KnowTuning: Knowledge-aware Fine-tuning for Large Language Models

    [https://arxiv.org/abs/2402.11176](https://arxiv.org/abs/2402.11176)

    提出了一种知识感知微调方法，通过显式和隐式方式改善大型语言模型对知识的认识，包括训练模型明确识别答案中的知识三元组和隐式区分可靠和不可靠的知识。

    

    尽管大型语言模型（LLMs）在许多自然语言处理（NLP）任务上取得成功，但仍然难以有效利用知识进行知识密集型任务，表现出生成不完整、非事实性或不合逻辑的答案等限制。这些限制源于LLMs在普通微调期间对知识的认识不足。为解决这些问题，我们提出了一种知识感知微调（KnowTuning）方法，以明确和隐式地改善LLMs的知识认识。我们设计了一个显式知识感知生成阶段，训练LLMs明确识别答案中的知识三元组。我们还提出了一个隐式知识感知比较阶段，训练LLMs隐式区分可靠和不可靠的知识，包括完整性、事实性和逻辑性三个方面。对通用和医学问答（QA）数据集进行的大量实验证实了效果。

    arXiv:2402.11176v1 Announce Type: cross  Abstract: Despite their success at many natural language processing (NLP) tasks, large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to explicitly and implicitly improve the knowledge awareness of LLMs. We devise an explicit knowledge-aware generation stage to train LLMs to explicitly identify knowledge triples in answers. We also propose an implicit knowledge-aware comparison stage to train LLMs to implicitly distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effec
    
[^217]: M4GT-Bench: 用于黑盒机器生成文本检测的评估基准

    M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection

    [https://arxiv.org/abs/2402.11175](https://arxiv.org/abs/2402.11175)

    介绍了一个新的基准M4GT-Bench，涉及多语言、多领域和多生成器，用于检测机器生成文本，包括单语和多语种MGT检测、多模型检测和人机混合文本检测。

    

    大型语言模型（LLMs）的出现带来了机器生成文本（MGT）在不同渠道的激增，这引发了人们对其潜在滥用和社会影响的关注。识别和区分这种内容与真实人类生成的文本对于对抗虚假信息、保持教育和科学领域的完整性以及保持通信信任至关重要。本文通过引入一个涉及多语言、多领域和多生成器的新基准M4GT-Bench来解决这一问题。它针对三个任务提出了不同的集合：（1）单语和多语种二分类MGT检测；（2）多模型检测确定生成文本的特定模型；以及（3）人机混合文本检测，应确定一个词边界来界定MGT和人工撰写内容。对于任务2的人类评估显示

    arXiv:2402.11175v1 Announce Type: new  Abstract: The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark involving multilingual, multi-domain and multi-generator for MGT detection -- M4GT-Bench. It is collected for three task formulations: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection identifies which particular model generates the text; and (3) human-machine mixed text detection, where a word boundary delimiting MGT from human-written content should be determined. Human evaluation for Task 2 shows
    
[^218]: Token-Ensemble文本生成：对自动AI生成文本检测的攻击

    Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection

    [https://arxiv.org/abs/2402.11167](https://arxiv.org/abs/2402.11167)

    提出了一种新颖的token-ensemble生成策略，挑战了当前AI内容检测方法的鲁棒性，对当前检测模型构成了重要挑战，需要进一步改进检测技术以应对复杂对抗策略。

    

    AI内容检测模型对经过精心设计的攻击（例如改写或词语替换）的鲁棒性仍然是一个重要问题。本研究提出了一种新颖的token-ensemble生成策略，挑战了当前AI内容检测方法的鲁棒性。我们通过使用从随机候选语言模型生成的下一个token完成提示来探索集成攻击策略。我们发现token-ensemble方法显著降低了AI内容检测模型的性能（代码和测试集将发布）。我们的发现表明，token-ensemble生成对当前检测模型构成了重要挑战，并强调了改进检测技术以应对复杂对抗策略的需求。

    arXiv:2402.11167v1 Announce Type: cross  Abstract: The robustness of AI-content detection models against cultivated attacks (e.g., paraphrasing or word switching) remains a significant concern. This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches. We explore the ensemble attack strategy by completing the prompt with the next token generated from random candidate LLMs. We find the token-ensemble approach significantly drops the performance of AI-content detection models (The code and test sets will be released). Our findings reveal that token-ensemble generation poses a vital challenge to current detection models and underlines the need for advancing detection technologies to counter sophisticated adversarial strategies.
    
[^219]: GenDec:一种用于多跳推理的稳健生成式问题分解方法

    GenDec: A robust generative Question-decomposition method for Multi-hop reasoning

    [https://arxiv.org/abs/2402.11166](https://arxiv.org/abs/2402.11166)

    GenDec提出了一种生成式问题分解方法，通过生成独立完整的子问题来增强LLMs在RAG中的推理能力。

    

    Multi-hop QA (MHQA)涉及逐步推理以回答复杂问题并找到多个相关支持事实。然而，现有大型语言模型在多跳问题回答中的推理能力仍然不足，无法满足多跳问题的回答需求。此外，尚不清楚LLMs是否按照期望的推理链来达到正确的最终答案。在本文中，我们提出了一种从可解释QA的角度提高LLMs推理能力的生成式问题分解方法（GenDec），通过根据额外提取的证据生成独立完整的子问题。为了展示Gendec的影响、泛化性和稳健性，我们进行了两个实验，首先是将GenDec与小型QA系统结合在段落检索和QA任务上。其次，我们检查了各种最先进的LLM的推理能力。

    arXiv:2402.11166v1 Announce Type: new  Abstract: Multi-hop QA (MHQA) involves step-by-step reasoning to answer complex questions and find multiple relevant supporting facts. However, Existing large language models'(LLMs) reasoning ability in multi-hop question answering remains exploration, which is inadequate in answering multi-hop questions. Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach the right final answer. In this paper, we propose a \textbf{gen}erative question \textbf{dec}omposition method (GenDec) from the perspective of explainable QA by generating independent and complete sub-questions based on incorporating additional extracted evidence for enhancing LLMs' reasoning ability in RAG. To demonstrate the impact, generalization, and robustness of Gendec, we conduct two experiments, the first is combining GenDec with small QA systems on paragraph retrieval and QA tasks. We secondly examine the reasoning capabilities of various state-of-the-art LLM
    
[^220]: KG-Agent: 一种用于在知识图谱上进行复杂推理的高效自主代理框架

    KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph

    [https://arxiv.org/abs/2402.11163](https://arxiv.org/abs/2402.11163)

    提出了一种名为KG-Agent的高效自主代理框架，该框架通过在KG上进行推理来提高大型语言模型的复杂问题回答能力

    

    在这篇论文中，我们旨在提高大型语言模型（LLMs）在知识图谱（KGs）上回答复杂问题的推理能力。受到现有设计LLMs和KG之间交互策略方法的启发，我们提出了一种名为KG-Agent的自主LLM代理框架，该框架使得小型LLM能够在完成对KG的推理过程中主动做出决策。在KG-Agent中，我们整合了LLM、多功能工具箱、基于KG的执行器和知识存储器，并开发了一个迭代机制，该机制自主选择工具然后更新内存以进行在KG上的推理。为了保证有效性，我们利用程序语言来制定在KG上的多跳推理过程，并合成基于代码的指令数据集来微调基础LLM。大量实验表明，仅使用10K个样本来调整LLaMA-7B比使用更大LLM的最先进方法表现更好。

    arXiv:2402.11163v1 Announce Type: new  Abstract: In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLM
    
[^221]: PANDA（Pedantic ANswer-correctness Determination and Adjudication）：改进问答和文本生成的自动评估

    PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation

    [https://arxiv.org/abs/2402.11161](https://arxiv.org/abs/2402.11161)

    提出了PANDA方法，引入了更精确的答案正确性评测方式，解决了当前自动评估问答和文本生成过程中的挑战。

    

    问答（QA）只有在我们知道答案是否正确时才能取得进展，但对于许多最具挑战性和有趣的QA示例，当前的答案正确性（AC）指标与人类判断不一致，特别是来自大型语言模型（LLM）的冗长、自由格式答案。我们提出了两个挑战：缺乏数据和模型过大。基于LLM的评分器与人类更好地相关，但这项昂贵的任务仅在有限的QA数据集上进行了测试。我们通过提供清晰的指南来评估从人类QA比赛中采纳的机器QA，解决了这些问题。我们还引入了精确的答案正确性确定和裁决（Precise ANswer correctness Determination and Adjudication，PANDA），这是一个小巧、高效、确定性的AC分类器（812 KB），更准确地评估答案的正确性。

    arXiv:2402.11161v1 Announce Type: cross  Abstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.
    
[^222]: 通过反事实文本引导的对比语言-图像预训练来理解新闻缩略图的代表性

    Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining

    [https://arxiv.org/abs/2402.11159](https://arxiv.org/abs/2402.11159)

    提出了一种反事实文本引导的对比语言-图像预训练框架CFT-CLIP，用于增强新闻文本和缩略图之间的对比学习。

    

    本文深入探讨了理解新闻缩略图的代表性这一关键挑战，这些缩略图通常在文章在社交媒体上传播时作为读者的第一个视觉参与。我们关注新闻图像是否代表新闻文本中讨论的主要主题。为了应对这一挑战，我们引入了一个手动注释的新闻缩略图和文本配对数据集\textsc{NewsTT}。我们发现，例如CLIP和BLIP-2这样的预训练视觉和语言模型在这一任务上表现不佳。由于新闻主题经常涉及命名实体或专有名词，预训练模型缺乏匹配其视觉和文本外观的能力。为了填补这一空白，我们提出了CFT-CLIP，一个反事实文本引导的对比语言-图像预训练框架。

    arXiv:2402.11159v1 Announce Type: new  Abstract: This paper delves into the critical challenge of understanding the representativeness of news thumbnail images, which often serve as the first visual engagement for readers when an article is disseminated on social media. We focus on whether a news image represents the main subject discussed in the news text. To serve the challenge, we introduce \textsc{NewsTT}, a manually annotated dataset of news thumbnail image and text pairs. We found that pretrained vision and language models, such as CLIP and BLIP-2, struggle with this task. Since news subjects frequently involve named entities or proper nouns, a pretrained model could not have the ability to match its visual and textual appearances. To fill the gap, we propose CFT-CLIP, a counterfactual text-guided contrastive language-image pretraining framework. We hypothesize that learning to contrast news text with its counterfactual, of which named entities are replaced, can enhance the cross
    
[^223]: 把握要点：定制大型语言模型进行零-shot关系抽取

    Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction

    [https://arxiv.org/abs/2402.11142](https://arxiv.org/abs/2402.11142)

    通过使用自然语言表达的关系定义来训练关系抽取模型的零-shot学习设置，从而为模型提供准确和明确的关系类型描述，并同时最小化注释要求。

    

    关系抽取（RE）是自然语言处理中的一个关键任务，旨在识别文本中提及的实体之间的语义关系。尽管这一领域取得了显著进展，但现有模型通常依赖于大量的注释数据进行训练，获取这些数据可能既昂贵又耗时。此外，这些模型通常难以适应新的或未见过的关系。相比之下，少样本学习设置旨在减少注释要求，对于理解目标关系语义提供了不完整且有偏见的监督，导致性能下降且不稳定。为了为模型提供准确和明确的关系类型描述，同时最小化注释要求，我们研究了仅使用自然语言中表示的关系定义来训练RE模型的仅零-shot RE设置。受LLM（大型语言模型）强大的合成数据生成能力的启发，我们提出了一种

    arXiv:2402.11142v1 Announce Type: new  Abstract: Relation extraction (RE), a crucial task in NLP, aims to identify semantic relationships between entities mentioned in texts. Despite significant advancements in this field, existing models typically rely on extensive annotated data for training, which can be both costly and time-consuming to acquire. Moreover, these models often struggle to adapt to new or unseen relationships. In contrast, few-shot learning settings, which aim to reduce annotation requirements, may offer incomplete and biased supervision for understanding target relation semantics, leading to degraded and unstable performance. To provide the model with accurate and explicit descriptions of the relations types and meanwhile minimize the annotation requirements, we study the definition only zero-shot RE setting where only relation definitions expressed in natural language are used to train a RE model. Motivated by the strong synthetic data generation power of LLMs, we pr
    
[^224]: 思维的提升：使用大型语言模型进行试错问题解决

    Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models

    [https://arxiv.org/abs/2402.11140](https://arxiv.org/abs/2402.11140)

    本文提出了一种名为Boosting of Thoughts（BoT）的自动提示框架，通过迭代地探索和自我评估多个思维树，获得一系列试错推理经验，作为解决复杂问题的新形式的提示。

    

    大型语言模型（LLMs）在各种问题上的推理性能关键取决于思维链提示，其中包括在提示中提供一些思维链示范作为示例。最近的工作（例如Thought Tree）指出了在复杂问题解决的推理步骤选择中，探索和自我评估的重要性。在本文中，我们提出了一种名为Boosting of Thoughts（BoT）的自动提示框架，用于通过迭代地探索和自我评估许多思维树来获得一系列试错推理经验，这将作为解决复杂问题的新形式的提示。BoT从一个简单提示开始，无需示例，迭代地探索和评估大量的推理步骤，更重要的是，利用LLM获得的错误分析来明确修改提示。

    arXiv:2402.11140v1 Announce Type: new  Abstract: The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompt
    
[^225]: 对比指令调整

    Contrastive Instruction Tuning

    [https://arxiv.org/abs/2402.11138](https://arxiv.org/abs/2402.11138)

    提出了对比指令调整方法，通过最大化相似性来提高大型语言模型对未知任务指令的稳健性

    

    指令调整一直被用作改善大型语言模型（LLMs）在未知任务上的性能的一种有前途的方法。然而，当前的LLMs在面临未知指令时表现出有限的稳健性，当相同的指令以稍微变化的形式或语言风格提出时会产生不一致的输出。这种行为表明LLMs对文本变化的稳健性和对未知指令的泛化能力不足，可能会导致可信度问题。因此，我们提出了对比指令调整，该方法在最大化语义上等价的指令-实例对的隐藏表示之间的相似性的同时，最小化语义上不同的对之间的相似性。为了促进这种方法，我们通过释义任务指令，扩充现有的FLAN集合。在PromptBench基准测试上的实验表明，对比指令调整（CoIN）一直提高了LLMs对未知指令的稳健性

    arXiv:2402.11138v1 Announce Type: cross  Abstract: Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructio
    
[^226]: 推测式流式处理: 无需辅助模型的快速LLM推理

    Speculative Streaming: Fast LLM Inference without Auxiliary Models

    [https://arxiv.org/abs/2402.11131](https://arxiv.org/abs/2402.11131)

    提出了一种Speculative Streaming方法，将草稿模型融入目标模型，并通过将微调目标从下一个令牌预测更改为未来的n-gram预测，加速解码1.8-3.1倍，同时保持生成质量。

    

    推测式解码是一种突出的技术，可以提高基于辅助草稿模型预测的大型目标语言模型的推理速度。虽然在特定应用设置中有效，但通常需要微调草稿和目标模型以实现较高的接受率。随着下游任务数量的增加，这些草稿模型给推理系统增加了显著的复杂性。我们提出了Speculative Streaming，一种单模型的推测式解码方法，通过将草拟融入目标模型，将微调目标从下一个令牌预测对象更改为未来的n-gram预测。 Speculative Streaming在各种任务中加速解码1.8-3.1倍，如摘要、结构化查询和意义表达，同时不降低生成质量。此外，Speculative Streaming参数有效。它实现了与Medusa风格架构相媲美/更高的加速度

    arXiv:2402.11131v1 Announce Type: cross  Abstract: Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 - 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while u
    
[^227]: BlendFilter: 通过查询生成混合和知识过滤推进检索增强型大型语言模型

    BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering

    [https://arxiv.org/abs/2402.11129](https://arxiv.org/abs/2402.11129)

    BlendFilter通过查询生成混合和知识过滤方法提升了检索增强型大型语言模型，在多领域的问答任务中取得了显著的性能提升。

    

    arXiv:2402.11129v1 公告类型：新摘要：检索增强型大型语言模型（LLM）在提升知识密集型场景中的性能方面具有显著优势。然而，这些方法经常面临复杂输入的挑战，并且由于嘈杂的知识检索而遇到困难，明显阻碍了模型的有效性。为解决这个问题，我们引入了BlendFilter，一种通过将查询生成混合与知识过滤相结合来提升检索增强型LLM的新方法。BlendFilter提出了通过其查询生成方法的混合过程，该方法将外部知识和内部知识增强与原始查询相结合，确保全面收集信息。此外，我们独特的知识过滤模块充分利用了LLM的固有能力，有效消除了多余的数据。我们在三个开放域问答基准上进行了大量实验，结果表明

    arXiv:2402.11129v1 Announce Type: new  Abstract: Retrieval-augmented Large Language Models (LLMs) offer substantial benefits in enhancing performance across knowledge-intensive scenarios. However, these methods often face challenges with complex inputs and encounter difficulties due to noisy knowledge retrieval, notably hindering model effectiveness. To address this issue, we introduce BlendFilter, a novel approach that elevates retrieval-augmented LLMs by integrating query generation blending with knowledge filtering. BlendFilter proposes the blending process through its query generation method, which integrates both external and internal knowledge augmentation with the original query, ensuring comprehensive information gathering. Additionally, our distinctive knowledge filtering module capitalizes on the intrinsic capabilities of the LLM, effectively eliminating extraneous data. We conduct extensive experiments on three open-domain question answering benchmarks, and the findings clea
    
[^228]: 导航双重面：对大型语言模型中顺序记忆编辑的全面评估

    Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models

    [https://arxiv.org/abs/2402.11122](https://arxiv.org/abs/2402.11122)

    这项研究全面评估了大型语言模型中顺序记忆编辑的影响，发现修改参数ME可能会导致所有任务表现不佳，而保留参数ME则能够保持较好性能。

    

    记忆编辑（ME）已经成为修改大型语言模型（LLMs）中错误事实或注入新事实的有效方法。存在两种主流ME方法：修改参数ME和保留参数ME（在保留原始参数的同时整合额外模块）。遗憾的是，先前对ME评估的研究存在两个关键限制：（i）仅评估带有单个编辑的LLMs，忽略了持续编辑的需要，以及（ii）评估仅关注基本事实三元组，忽视了更广泛的LLM能力，如逻辑推理和阅读理解。本研究通过以下三点解决了这些限制：（i）我们探索了ME如何影响LLMs的广泛基本能力在顺序编辑下。实验结果揭示了一个有趣的现象：大多数修改参数ME在几次顺序编辑后一贯降低所有任务的表现。相比之下，

    arXiv:2402.11122v1 Announce Type: cross  Abstract: Memory Editing (ME) has emerged as an efficient method to modify erroneous facts or inject new facts into Large Language Models (LLMs). Two mainstream ME methods exist: parameter-modifying ME and parameter-preserving ME (integrating extra modules while preserving original parameters). Regrettably, previous studies on ME evaluation have two critical limitations: (i) evaluating LLMs with single edit only, neglecting the need for continuous editing, and (ii) evaluations focusing solely on basic factual triples, overlooking broader LLM capabilities like logical reasoning and reading understanding. This study addresses these limitations with contributions threefold: (i) We explore how ME affects a wide range of fundamental capabilities of LLMs under sequential editing. Experimental results reveal an intriguing phenomenon: Most parameter-modifying ME consistently degrade performance across all tasks after a few sequential edits. In contrast,
    
[^229]: 语言模型反映了谁的情感和道德情感？

    Whose Emotions and Moral Sentiments Do Language Models Reflect?

    [https://arxiv.org/abs/2402.11114](https://arxiv.org/abs/2402.11114)

    该研究探讨了语言模型在情感和道德维度上如何代表不同群体，发现它们与意识形态团体存在显著的不一致性。

    

    语言模型已知更好地代表一些社会群体的观点，这可能会影响它们的性能，特别是在主观任务上，比如内容管理和仇恨言论检测。我们定义了情感对齐的问题，用来衡量语言模型的情感和道德色调如何代表不同群体的情感。通过比较36个语言模型生成的回应的情感与Twitter消息的情感，我们观察到语言模型与两种意识形态团体存在显著不一致。即使在引导语言模型朝着特定意识形态方向发展之后，这种不一致性也仍然存在。

    arXiv:2402.11114v1 Announce Type: new  Abstract: Language models (LMs) are known to represent the perspectives of some social groups better than others, which may impact their performance, especially on subjective tasks such as content moderation and hate speech detection. To explore how LMs represent different perspectives, existing research focused on positional alignment, i.e., how closely the models mimic the opinions and stances of different groups, e.g., liberals or conservatives. However, human communication also encompasses emotional and moral dimensions. We define the problem of affective alignment, which measures how LMs' emotional and moral tone represents those of different groups. By comparing the affect of responses generated by 36 LMs to the affect of Twitter messages, we observe significant misalignment of LMs with both ideological groups. This misalignment is larger than the partisan divide in the U.S. Even after steering the LMs towards specific ideological perspectiv
    
[^230]: 语言模型作为科学导师

    Language Models as Science Tutors

    [https://arxiv.org/abs/2402.11111](https://arxiv.org/abs/2402.11111)

    介绍了TutorEval和TutorChat，通过TutorEval基准可以衡量LMs作为科学助手的实际可用性，TutorChat数据集用于微调模型。

    

    NLP最近取得了令人兴奋的进展，朝着训练具有较强科学问题解决能力的语言模型（LMs）的方向发展。然而，模型的发展并没有专注于LMs在科学教育中的实际用例，包括需要处理长篇科学文档的应用。为了解决这个问题，我们引入了TutorEval和TutorChat。TutorEval是一个多样化的问答基准，其中包含有关STEM教科书长篇章节的问题，由专家编写。TutorEval有助于衡量LMs作为科学助手的实际可用性，它是第一个结合长上下文、自由生成和多学科科学知识的基准。此外，我们展示了使用现有对话数据集对基础模型进行微调会导致TutorEval性能不佳。因此，我们创建了TutorChat，这是一个包含80,000个关于教科书的长合成对话的数据集。我们使用TutorChat来微调Llemma模型。

    arXiv:2402.11111v1 Announce Type: cross  Abstract: NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models wit
    
[^231]: 当大型语言模型遇到狡猾问题：用于大型语言模型的谬误理解基准

    When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.11100](https://arxiv.org/abs/2402.11100)

    该论文提出了一个谬误理解基准FLUB，挑战大型语言模型在推理和理解能力上，重点是通过设计狡猾问题评估LLMs的谬误理解能力。

    

    最近，大型语言模型(LLMs)在语言理解和生成方面取得了显著进展。本文通过提出一个名为FaLlacy Understanding Benchmark (FLUB)的基准来挑战LLMs的推理和理解能力，其中包含易于人类理解但难于模型把握的狡猾问题。具体来说，FLUB专注于从真实互联网环境中收集到的棘手、幽默和误导性问题。我们设计了三个难度递增的任务，用于评估LLMs的谬误理解能力。基于FLUB，我们研究了多个代表性的和先进的LLMs的表现，表明我们的FLUB是具有挑战性的并值得未来进一步研究的。

    arXiv:2402.11100v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have made remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning questions that FLUB focuses on mainly consist of the tricky, humorous, and misleading questions collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights 
    
[^232]: 重新审视词嵌入：LLMs是否提供新的东西？

    Word Embeddings Revisited: Do LLMs Offer Something New?

    [https://arxiv.org/abs/2402.11094](https://arxiv.org/abs/2402.11094)

    该论文系统地比较了经典词嵌入技术和基于LLM的词嵌入，发现LLMs倾向于将语义相关的单词更紧密地聚类在一起，并在Bigger Analogy Test Set（BATS）上具有更高的平均准确度。

    

    学习有意义的词嵌入对于训练稳健的语言模型至关重要。最近兴起的大型语言模型（LLMs）为我们提供了许多新的单词/句子/文档嵌入模型。尽管LLMs在各种自然语言处理任务中显示出显着的进步，但仍不清楚性能的提升仅仅是因为规模还是它们生成的底层嵌入与句子-BERT（SBERT）或通用句子编码器（USE）之类的传统编码模型有显著区别。本文通过比较经典词嵌入技术与基于LLM的词嵌入，系统地调查了这个问题，从它们的潜在向量语义方面进行比较。我们的结果显示，LLMs倾向于将语义相关的单词更紧密地聚类在一起，LLMs在Bigger Analogy Test Set（BATS）上的平均准确度也高于经典方法。最后，一些LLMs倾向于产生词嵌入si。

    arXiv:2402.11094v1 Announce Type: new  Abstract: Learning meaningful word embeddings is key to training a robust language model. The recent rise of Large Language Models (LLMs) has provided us with many new word/sentence/document embedding models. Although LLMs have shown remarkable advancement in various NLP tasks, it is still unclear whether the performance improvement is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper systematically investigates this issue by comparing classical word embedding techniques against LLM-based word embeddings in terms of their latent vector semantics. Our results show that LLMs tend to cluster semantically related words more tightly than classical models. LLMs also yield higher average accuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally, some LLMs tend to produce word embeddings si
    
[^233]: 通过纯微调进行模型编辑

    Model Editing by Pure Fine-Tuning

    [https://arxiv.org/abs/2402.11078](https://arxiv.org/abs/2402.11078)

    纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。

    

    精细调整被认为在模型编辑中不够有效，因为相对更专业的方法而言，它的表现较差。然而，微调是简单的，不关心被编辑模型的体系结构细节，并且能够利用标准训练方法的不断进展（例如PEFT），使其成为模型编辑器的吸引选择。在本文中，我们展示了纯粹的微调可以是一种可行的模型编辑方法。我们提出了对朴素微调进行轻微修改的两个关键因素。第一，我们优化条件似然而非完整似然。第二，我们使用随机释义和事实来增加数据，以鼓励泛化和局部性。我们在ZsRE和CounterFact上的实验表明，这一简单修改使得微调通常可以与专业编辑器在编辑分数方面匹敌甚至超越。

    arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
    
[^234]: AFaCTA: 使用可靠的LLM标注者辅助事实性索赔检测的标注

    AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators

    [https://arxiv.org/abs/2402.11073](https://arxiv.org/abs/2402.11073)

    提出了一种新框架 AFaCTA，利用大型语言模型辅助事实性索赔的标注，提高了标注的效率和一致性。

    

    随着生成式人工智能的兴起，用于打击误导信息的自动事实核查方法变得越来越重要。然而，事实性索赔检测，即事实核查管道中的第一步，存在两个关键问题限制了其可伸缩性和泛化性：（1）任务定义和索赔概念的不一致性以及（2）手动标注的高成本。为了解决（1），我们审查了相关工作中的定义，并提出了一个聚焦于可验证性的事实性索赔的统一定义。为了解决（2），我们引入了AFaCTA（自动事实性索赔检测标注器），这是一个新颖的框架，利用大型语言模型（LLMs）在事实性索赔的标注中提供帮助。AFaCTA通过沿着三条预定义的推理路径保持一致性来校准其注释的置信度。在政治言论领域的大量评估和实验表明，AFaCTA能够高效地协助专业人员进行标注。

    arXiv:2402.11073v1 Announce Type: cross  Abstract: With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist exper
    
[^235]: 架起因果发现与大型语言模型之间的桥梁：整合方法和未来方向的综合调查

    Bridging Causal Discovery and Large Language Models: A Comprehensive Survey of Integrative Approaches and Future Directions

    [https://arxiv.org/abs/2402.11068](https://arxiv.org/abs/2402.11068)

    本文综合调查了将大型语言模型（如GPT4）整合到因果发现任务中的方法，揭示了它们在推断因果结构时对元数据和自然语言的创新利用，强调了LLMs在增强传统CD方法和作为专家辅助方面的潜力和挑战。

    

    因果发现（CD）和大型语言模型（LLMs）代表着两个具有重要影响力的人工智能研究领域。尽管它们起源不同，CD侧重于从数据中揭示因果关系，LLMs则侧重于处理和生成类似人类的文本，但这两个领域的融合为理解复杂系统提供了新颖的见解和方法论。本文介绍了将LLMs（如GPT4）整合到CD任务中的综合调查。我们系统地审查和比较了利用LLMs进行各种CD任务的现有方法，并突出了它们对元数据和自然语言的创新利用以推断因果结构。我们的分析揭示了LLMs在增强传统CD方法和作为不完美专家方面的优势和潜力，同时也揭示了当前实践中固有的挑战和限制。此外，我们确定了文献中的空白。

    arXiv:2402.11068v1 Announce Type: cross  Abstract: Causal discovery (CD) and Large Language Models (LLMs) represent two emerging fields of study with significant implications for artificial intelligence. Despite their distinct origins, CD focuses on uncovering cause-effect relationships from data, and LLMs on processing and generating humanlike text, the convergence of these domains offers novel insights and methodologies for understanding complex systems. This paper presents a comprehensive survey of the integration of LLMs, such as GPT4, into CD tasks. We systematically review and compare existing approaches that leverage LLMs for various CD tasks and highlight their innovative use of metadata and natural language to infer causal structures. Our analysis reveals the strengths and potential of LLMs in both enhancing traditional CD methods and as an imperfect expert, alongside the challenges and limitations inherent in current practices. Furthermore, we identify gaps in the literature 
    
[^236]: Persona-DB：用于响应预测的高效大规模语言模型个性化与协同数据优化

    Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement

    [https://arxiv.org/abs/2402.11060](https://arxiv.org/abs/2402.11060)

    介绍了 Persona-DB，一个简单却有效的框架，通过层级构建过程和协同优化，改善了大规模语言模型个性化中数据库表示的泛化能力和检索效率。

    

    随着对大型语言模型（LLMs）个性化交互需求的增加，需要开发能够准确快速识别用户意见和偏好的方法。检索增强作为一种有效策略出现，因为它可以适应大量用户而无需进行微调的成本。然而，现有研究主要集中在增强检索阶段，并对数据库表示的优化进行了有限的探索，这是个性化等任务的关键方面。在这项工作中，我们从一个新的角度研究了这个问题，着重于如何更有效地表示数据，以便在LLM定制的情境下更有效地进行检索。为了解决这一挑战，我们介绍了Persona-DB，这是一个简单而有效的框架，包括一个分层构建过程，以改善跨任务背景的泛化能力，并进行协同优化。

    arXiv:2402.11060v1 Announce Type: cross  Abstract: The increasing demand for personalized interactions with large language models (LLMs) calls for the development of methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to
    
[^237]: II-MMR：在视觉问答中识别和改进多模态多跳推理

    II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering

    [https://arxiv.org/abs/2402.11058](https://arxiv.org/abs/2402.11058)

    II-MMR提出了一种新颖的方法，用于识别和改进视觉问答中的多模态多跳推理，通过引入答案预测引导的CoT提示和知识三元组引导的提示，实现对不同推理情况的分析和识别。

    

    视觉问答（VQA）通常涉及视觉和语言之间多样推理场景。然而，大多数先前的VQA研究仅关注评估模型的整体准确性，而没有在不同推理情况下对其进行评估。此外，一些最近的研究发现，传统的"CoT"提示无法有效生成VQA的推理，尤其是对于需要多跳推理的复杂场景。在本文中，我们提出了II-MMR，这是一个新颖的想法，用于识别和改进VQA中的多模态多跳推理。具体而言，II-MMR接受带有图像的VQA问题，并使用两种新颖的语言提示找到推理路径以获得答案：(i)答案预测引导的CoT提示，或者(ii)知识三元组引导的提示。然后，II-MMR分析这条路径，通过估计有多少跳和什么类型（即视觉或超出）来识别当前VQA基准中的不同推理情况。

    arXiv:2402.11058v1 Announce Type: cross  Abstract: Visual Question Answering (VQA) often involves diverse reasoning scenarios across Vision and Language (V&L). Most prior VQA studies, however, have merely focused on assessing the model's overall accuracy without evaluating it on different reasoning cases. Furthermore, some recent works observe that conventional Chain-of-Thought (CoT) prompting fails to generate effective reasoning for VQA, especially for complex scenarios requiring multi-hop reasoning. In this paper, we propose II-MMR, a novel idea to identify and improve multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA question with an image and finds a reasoning path to reach its answer using two novel language promptings: (i) answer prediction-guided CoT prompt, or (ii) knowledge triplet-guided prompt. II-MMR then analyzes this path to identify different reasoning cases in current VQA benchmarks by estimating how many hops and what types (i.e., visual or beyon
    
[^238]: 大型语言模型的短板：理解侦探叙事中复杂关系

    Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives

    [https://arxiv.org/abs/2402.11051](https://arxiv.org/abs/2402.11051)

    提出了Conan数据集，用于从侦探叙事中提取和分析复杂的人物关系图，并揭示大型语言模型在推理复杂关系和处理长篇叙事方面的局限性。

    

    现有的叙事理解数据集往往无法表达现实社会场景中关系的复杂性和不确定性。为了弥补这一空白，我们引入了一个新的基准数据集Conan，旨在从侦探叙事中提取和分析复杂的人物关系图。具体地，我们设计了分层关系类别，并从不同角色的角度手动提取和注释了以角色为导向的关系，包括大多数角色知晓的公开关系和仅少数角色知晓的秘密关系。我们对GPT-3.5、GPT-4和Llama2等先进的大型语言模型进行的实验揭示了它们在推理复杂关系和处理较长叙事方面的局限性。Conan数据集与我们的流程策略的结合旨在了解大型语言模型理解叙事背景中微妙关系动态的能力。

    arXiv:2402.11051v1 Announce Type: cross  Abstract: Existing datasets for narrative understanding often fail to represent the complexity and uncertainty of relationships in real-life social scenarios. To address this gap, we introduce a new benchmark, Conan, designed for extracting and analysing intricate character relation graphs from detective narratives. Specifically, we designed hierarchical relationship categories and manually extracted and annotated role-oriented relationships from the perspectives of various characters, incorporating both public relationships known to most characters and secret ones known to only a few. Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives. The combination of the Conan dataset and our pipeline strategy is geared towards understanding the ability of LLMs to comprehend nuanced relational dynamics in narrative contexts.
    
[^239]: 密集通道检索：密集通道检索是否在检索中？

    Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?

    [https://arxiv.org/abs/2402.11035](https://arxiv.org/abs/2402.11035)

    DPR微调预训练网络以增强查询和相关文本数据之间的嵌入对齐，发现训练中知识去中心化，但也揭示了模型内部知识的局限性

    

    密集通道检索（DPR）是改进大型语言模型（LLM）性能的检索增强生成（RAG）范式中的第一步。 DPR微调预训练网络，以增强查询和相关文本数据之间的嵌入对齐。对DPR微调的深入理解将需要从根本上释放该方法的全部潜力。在这项工作中，我们通过使用探针、层激活分析和模型编辑的组合，机械地探索了DPR训练模型。我们的实验证明，DPR训练使网络中存储知识的方式去中心化，创建了访问相同信息的多个路径。我们还发现了这种训练风格的局限性：预训练模型的内部知识限制了检索模型可以检索的内容。这些发现为密集检索提出了一些可能的方向：（1）暴露DPR训练过程

    arXiv:2402.11035v1 Announce Type: new  Abstract: Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process 
    
[^240]: PAT-Questions：一个用于现在时刻为锚点的时间问答自更新基准

    PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering

    [https://arxiv.org/abs/2402.11034](https://arxiv.org/abs/2402.11034)

    PAT-Questions基准用于现在时刻为锚点的时间问答，通过自动刷新答案以解决大型语言模型知识过时、复杂时间关系难以推理、可能需要多跳推理以及基准答案持续更新等挑战。

    

    关于时间问答（TQA）的现有研究主要集中在锚定特定时间戳或事件的问题上（例如“1970年谁是美国总统？”）。很少有研究关注其时间背景相对于当前时间的问题（例如“之前的美国总统是谁？”）。我们将这个问题称为现在时刻为锚的时间问答（PATQA）。PATQA面临着独特的挑战：（1）大型语言模型（LLMs）可能具有过时的知识，（2）复杂的时间关系（例如“之前”，“以前”）难以推理，（3）可能需要多跳推理，（4）基准的正确答案必须持续更新。为了解决这些挑战，我们介绍了PAT-Questions基准，其中包括单跳和多跳时间问题。PAT-Questions中的答案可以通过在知识图上重新运行SPARQL查询来自动刷新。我们评估了几个最先进的

    arXiv:2402.11034v1 Announce Type: new  Abstract: Existing work on Temporal Question Answering (TQA) has predominantly focused on questions anchored to specific timestamps or events (e.g. "Who was the US president in 1970?"). Little work has studied questions whose temporal context is relative to the present time (e.g. "Who was the previous US president?"). We refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses unique challenges: (1) large language models (LLMs) may have outdated knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are hard to reason, (3) multi-hop reasoning may be required, and (4) the gold answers of benchmarks must be continuously updated. To address these challenges, we introduce the PAT-Questions benchmark, which includes single and multi-hop temporal questions. The answers in PAT-Questions can be automatically refreshed by re-running SPARQL queries on a knowledge graph, if available. We evaluate several state-of-the-art 
    
[^241]: 探究价值偏好：LLMs偏向理想状态的偏差

    Exploring Value Biases: How LLMs Deviate Towards the Ideal

    [https://arxiv.org/abs/2402.11005](https://arxiv.org/abs/2402.11005)

    研究发现大型语言模型（LLMs）在给出响应时存在一个价值偏好的机制，倾向于偏向理想状态，这种偏差会对不同应用场景产生重要影响。

    

    大型语言模型（LLMs）被部署在各种应用中，并且它们的响应对社会产生着越来越大的影响。理解LLMs在给出响应时的非故意机制对于解释它们的性能并辨别它们在现实世界应用中的偏差至关重要。这类似于人类研究中，这种无意识的响应被称为抽样。我们研究了LLMs的这种抽样现象，发现LLMs的抽样倾向于偏爱高价值选项。价值偏好对应于从最可能的响应向LLM中代表的理想价值的转变。实际上，即便是通过上下文提示学习到的新实体，这种效果也能够再现。我们表明这种偏差表现在意想不到的地方，并对选择典型实例等相关应用场景产生影响。结果显示，价值偏好在不同分类的LLMs中都很明显。

    arXiv:2402.11005v1 Announce Type: cross  Abstract: Large-Language-Models (LLMs) are deployed in a wide range of applications, and their response has an increasing social impact. Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications. This is analogous to human studies, where such inadvertent responses are referred to as sampling. We study this sampling of LLMs in light of value bias and show that the sampling of LLMs tends to favour high-value options. Value bias corresponds to this shift of response from the most likely towards an ideal value represented in the LLM. In fact, this effect can be reproduced even with new entities learnt via in-context prompting. We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars. The results show that value bias is strong in LLMs across different categor
    
[^242]: ASGEA：利用Align-Subgraphs中的逻辑规则进行实体对齐

    ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment

    [https://arxiv.org/abs/2402.11000](https://arxiv.org/abs/2402.11000)

    提出了一个新的实体对齐框架ASGEA，利用Align-Subgraphs中的逻辑规则，设计了可解释的基于路径的图神经网络ASGNN，引入了多模态注意机制，取得了令人满意的实验结果

    

    实体对齐（EA）旨在识别代表相同现实世界对象的不同知识图中的实体。最近基于嵌入的EA方法在EA方面取得了最先进的性能，但面临着解释性挑战，因为它们完全依赖于嵌入距离，并忽视了一对对齐实体背后的逻辑规则。在本文中，我们提出了Align-Subgraph实体对齐（ASGEA）框架来利用Align-Subgraphs中的逻辑规则。ASGEA使用锚链接作为桥梁来构建Align-Subgraphs，并沿着跨知识图的路径传播，这使其区别于基于嵌入的方法。此外，我们设计了一种可解释的基于路径的图神经网络ASGNN，以有效识别和整合跨知识图的逻辑规则。我们还引入了一个节点级多模态注意机制，结合多模态增强的锚点来增强Align-Subgraph。我们的实验结果

    arXiv:2402.11000v1 Announce Type: cross  Abstract: Entity alignment (EA) aims to identify entities across different knowledge graphs that represent the same real-world objects. Recent embedding-based EA methods have achieved state-of-the-art performance in EA yet faced interpretability challenges as they purely rely on the embedding distance and neglect the logic rules behind a pair of aligned entities. In this paper, we propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct Align-Subgraphs and spreads along the paths across KGs, which distinguishes it from the embedding-based methods. Furthermore, we design an interpretable Path-based Graph Neural Network, ASGNN, to effectively identify and integrate the logic rules across KGs. We also introduce a node-level multi-modal attention mechanism coupled with multi-modal enriched anchors to augment the Align-Subgraph. Our experimental results 
    
[^243]: 在大型语言模型中理解人工智能：语义基础

    "Understanding AI": Semantic Grounding in Large Language Models

    [https://arxiv.org/abs/2402.10992](https://arxiv.org/abs/2402.10992)

    大型语言模型（LLMs）展现了对语义的渐进理解，通过应用心灵哲学和语言学中关于含义的核心假设，研究发现LLMs不仅仅是生成文本的工具，而是在某种程度上已经理解了它们生成的语言。

    

    近年来我们目睹了人工智能的生成式转变，生成模型，包括大型语言模型（LLMs），对于自监督学习至关重要。我们提出一个问题，LLMs是否理解其生成的文本的含义？它们是否具有语义基础？我们如何了解它们是否理解以及理解的是什么？本文探讨了对语义基础问题的评估，区分和讨论了五种方法。其中最有前景的方法是将心灵哲学和语言学中关于含义的核心假设应用于LLMs。我们发现，语义基础是一个渐进的过程，包括功能性、社会性和因果性三个维度的区分。LLMs在这三个维度上展现出基本证据。一个强有力的论据是LLMs会形成世界模型。因此，LLMs既不是随机的鹦鹉，也不是语义僵尸，而是至少在基本层面上已经理解它们生成的语言。

    arXiv:2402.10992v1 Announce Type: cross  Abstract: Do LLMs understand the meaning of the texts they generate? Do they possess a semantic grounding? And how could we understand whether and what they understand? I start the paper with the observation that we have recently witnessed a generative turn in AI, since generative models, including LLMs, are key for self-supervised learning. To assess the question of semantic grounding, I distinguish and discuss five methodological ways. The most promising way is to apply core assumptions of theories of meaning in philosophy of mind and language to LLMs. Grounding proves to be a gradual affair with a three-dimensional distinction between functional, social and causal grounding. LLMs show basic evidence in all three dimensions. A strong argument is that LLMs develop world models. Hence, LLMs are neither stochastic parrots nor semantic zombies, but already understand the language they generate, at least in an elementary sense.
    
[^244]: WilKE：智慧层知识编辑器用于终身知识编辑

    WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing

    [https://arxiv.org/abs/2402.10987](https://arxiv.org/abs/2402.10987)

    该研究提出了一种名为WilKE的知识编辑方法，通过选择编辑层来匹配不同层级中的知识编辑模式程度，在终身编辑中相比最先进的方法平均展现了46.2%和67.8%的改进。

    

    知识编辑旨在纠正大型语言模型（LLMs）中的不准确性，而无需为过时或错误的知识进行昂贵的重新训练。然而，当前的知识编辑方法主要集中于单次编辑，未能满足终身编辑的要求。本文中，终身编辑与终身知识编辑同义。本研究揭示了知识编辑在终身编辑中遇到的性能下降问题，其特征为毒性积累和毒性闪现，主要原因是模式不匹配。我们介绍了一种名为WilKE的知识编辑方法，它根据不同层级中编辑知识的模式匹配程度选择编辑层。实验结果表明，在终身编辑中，相对于最先进的知识编辑方法，WilKE在编辑GPT2-XL和GPT-J方面分别平均改进了46.2%和67.8%。

    arXiv:2402.10987v1 Announce Type: cross  Abstract: Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. In this paper, lifelong editing is synonymous with lifelong knowledge editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named WilKE, which selects editing layer based on the pattern matching degree of editing knowledge across different layers. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2\% and 67.8\% on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.
    
[^245]: FinTral：一类GPT-4级别的多模态金融大型语言模型

    FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models

    [https://arxiv.org/abs/2402.10986](https://arxiv.org/abs/2402.10986)

    FinTral是一类基于Mistral-7b模型的GPT-4级别多模态金融大型语言模型，通过领域特定的预训练和检索方法优化，在AI驱动金融技术中取得显著进展。

    

    我们引入FinTral，这是一组基于Mistral-7b模型构建的一流多模态大型语言模型（LLMs），专门为金融分析定制。FinTral整合了文本、数字、表格和图像数据。我们通过利用为本研究策划的大量文本和视觉数据集，通过领域特定的预训练、指导微调和RLAIF训练增强了FinTral。我们还介绍了一个包含九个任务和25个数据集进行评估的广泛基准测试，其中包括金融领域的幻觉。我们的FinTral模型，通过采用先进的工具和检索方法进行直接偏好优化训练，命名为FinTral-DPO-T&R，展现了出色的零-shot性能。它在所有任务中均优于ChatGPT-3.5，并在九项任务中的五项中超越GPT-4，标志着人工智能驱动的金融技术的重要进步。我们还展示了FinTral具有潜力

    arXiv:2402.10986v1 Announce Type: cross  Abstract: We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to e
    
[^246]: SportsMetrics:将文本和数值数据融合以理解LLM中的信息融合

    SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs

    [https://arxiv.org/abs/2402.10979](https://arxiv.org/abs/2402.10979)

    这项研究介绍了围绕体育数据分析展开的四项新任务，旨在评估大型语言模型在数值推理和信息融合方面的能力。

    

    arXiv:2402.10979v1 公告类型：交叉 摘要：大型语言模型对于整合文本文档和数据库记录等各种数据类型进行先进分析具有重要潜力。然而，融合文本和数值数据存在重大挑战。LLMs需要处理和交叉引用实体和数字，处理数据不一致性和冗余，并发展规划能力，比如构建用于管理复杂数据查询的工作内存。在本文中，我们介绍了围绕体育数据分析的四项新颖任务，以评估LLMs的数值推理和信息融合能力。这些任务涉及向LLMs提供详细的逐场比赛描述，然后在面对诸如新比赛规则、更长持续时间、故事混乱以及分析比赛摘要中的关键统计数据等对抗性场景。我们对NBA和NFL比赛进行了大量实验，以评估LLMs在该领域的表现。

    arXiv:2402.10979v1 Announce Type: cross  Abstract: Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on the
    
[^247]: 具有符合事实性保证的语言模型

    Language Models with Conformal Factuality Guarantees

    [https://arxiv.org/abs/2402.10978](https://arxiv.org/abs/2402.10978)

    提出了一种能够通过连接语言建模和符合预测为语言模型提供高概率正确性保证的框架。

    

    语言模型（LM）输出的正确性和事实性保证是一个重要的开放问题。在这项工作中，我们提出了符合事实性，这是一个框架，可以通过连接语言建模和符合预测，确保LM的高概率正确性保证。我们观察到，LM输出的正确性等价于一个不确定性量化问题，其中不确定性集被定义为LM输出的蕴含集。利用这种联系，我们表明，语言模型中的符合预测对应于一种后退算法，通过逐渐使LM输出变得不太具体（并扩大相关的不确定性集）提供高概率的正确性保证。这种方法适用于任何黑盒LM，并且需要很少的人工注释样本。我们在封闭书籍QA（FActScore，NaturalQuestions）和推理任务（MATH）上对我们的方法进行评估，结果表明我们的方法可以p

    arXiv:2402.10978v1 Announce Type: cross  Abstract: Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. We observe that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can p
    
[^248]: 医疗AI中的泛化性能：临床大型语言模型的评估

    Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model

    [https://arxiv.org/abs/2402.10965](https://arxiv.org/abs/2402.10965)

    大型语言模型在医疗健康领域有着重要作用，然而它们的泛化效果取决于在不同临床环境和人群中的表现，对于泛化能力不足的原因进行了分析，发现在样本较少的医院和特定人群中存在挑战。

    

    大型语言模型（LLMs）的进展为医疗健康领域提供了新机遇，可以改善患者护理、临床决策以及提升医师和管理人员的工作流程。然而，这些模型的潜力重要取决于它们在临床环境和人群中有效泛化的能力，这是在早期开发中经常被低估的挑战。为了更好地理解这些挑战的原因并制定缓解方法，我们评估了ClinicLLM，这是一个在 [HOSPITAL] 的临床笔记上训练的LLM模型，对其在30天全因素再入院预测中的表现进行分析，关注跨医院和患者特征的变异性。我们发现在样本较少的医院、政府和未指定保险的患者、老年人以及高共病性患者中，泛化效果较差。为了了解泛化不彰的原因，我们调查了样本量

    arXiv:2402.10965v1 Announce Type: new  Abstract: Advances in large language models (LLMs) provide new opportunities in healthcare for improved patient care, clinical decision-making, and enhancement of physician and administrator workflows. However, the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development. To better understand reasons for these challenges and inform mitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s clinical notes, analyzing its performance on 30-day all-cause readmission prediction focusing on variability across hospitals and patient characteristics. We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities. To understand reasons for lack of generalization, we investigated sample sizes 
    
[^249]: GLoRe: 何时、何地以及如何通过全局和局部的改进来提高LLM推理能力

    GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements

    [https://arxiv.org/abs/2402.10963](https://arxiv.org/abs/2402.10963)

    提出了Stepwise ORMs (SORMs)，它们在合成数据上训练，以近似预测最优策略的未来预期奖励

    

    最先进的语言模型在数学、科学或编码任务中展现出令人印象深刻的推理改进能力。然而，最近的研究表明，即使最好的模型也很难在没有外部反馈的情况下确定何时何地进行改进。基于结果的奖励模型(ORMs)，被训练来预测最终答案的正确性，指示何时进行改进，为决定何时进行改进提供了一种便利的解决方案。基于过程的奖励模型(PRMs)受过训练，用以预测中间步骤的正确性，然后可以用来指示何处进行改进。但它们很昂贵，需要大量的人工注释。在本文中，我们提出了逐步ORMs(SORMs)，它们只在合成数据上受过训练，以近似预测最优策略或$V^{\star}$的未来预期奖励。更具体地说，SORMs受训练来预测当取样时最终答案的正确性

    arXiv:2402.10963v1 Announce Type: new  Abstract: State-of-the-art language models can exhibit impressive reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify \textit{when and where to refine} without access to external feedback. Outcome-based Reward Models (\textbf{ORMs}), trained to predict correctness of the final answer indicating when to refine, offer one convenient solution for deciding when to refine. Process Based Reward Models (\textbf{PRMs}), trained to predict correctness of intermediate steps, can then be used to indicate where to refine. But they are expensive to train, requiring extensive human annotations. In this paper, we propose Stepwise ORMs (\textbf{SORMs}) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\star}$. More specifically, SORMs are trained to predict the correctness of the final answer when samplin
    
[^250]: 在语言模型对话中测量和控制“人设”漂移

    Measuring and Controlling Persona Drift in Language Model Dialogs

    [https://arxiv.org/abs/2402.10962](https://arxiv.org/abs/2402.10962)

    提出了一种量化基准来测量语言模型对话中的“人设”漂移，并提出了一种称为split-softmax的轻量级方法来对抗注意力衰减和“人设”漂移

    

    提示是定制语言模型聊天机器人的标准工具，使其能够承担特定的“人设”。在使用提示时的一个隐含假设是，它们将是稳定的，因此聊天机器人将在整个对话过程中继续根据规定的“人设”生成文本。我们提出了一个量化基准来测试这一假设，通过两个个性化聊天机器人之间的自我对话来评估“人设”的稳定性。我们对流行模型如LLaMA2-chat-70B进行测试，发现在八轮对话中存在显著的“人设”漂移。对这一现象的实证和理论分析表明，由于长对话中的注意力衰减，变压器注意力机制起到了一定作用。为了对抗注意力衰减和“人设”漂移，我们提出了一种称为split-softmax的轻量级方法，与两个强基线方法相比表现优异。

    arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
    
[^251]: 相对优先权优化: 通过对相同和不同提示的对比响应增强LLM对齐

    Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts

    [https://arxiv.org/abs/2402.10958](https://arxiv.org/abs/2402.10958)

    提出了相对优先权优化（RPO）方法，通过区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应，扩展了模型的学习能力。

    

    在大型语言模型（LLM）领域，将模型与用户的多样化偏好相一致是一个关键挑战。直接优先权优化（DPO）在这一领域起到了关键作用。DPO通过使用从相同提示中派生的偏好对来工作，而无需额外的奖励模型。然而，DPO并不能完全反映人类学习的复杂性，这种学习往往涉及对不仅相同而且相似问题的对比响应的理解。为了克服这一不足，我们提出了相对优先权优化（RPO）。RPO旨在区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应。它引入了对比加权机制，使LLMs能够使用更广泛的偏好数据进行调整，包括成对和不成对的数据集。这种方法扩展了模型的学习能力，使其能够利用更多的偏好数据进行优化。

    arXiv:2402.10958v1 Announce Type: cross  Abstract: In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to lever
    
[^252]: DAEDRA：用于预测被动药物警戒报告结果的语言模型

    DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting

    [https://arxiv.org/abs/2402.10951](https://arxiv.org/abs/2402.10951)

    DAEDRA是一种旨在在被动报告中检测监管相关结果的大型语言模型，弥补了通用模型无法捕捉临床维度与专业模型在非专业报告上表现不佳的缺陷

    

    近年来，大型语言模型（LLMs）的出现导致了特定领域模型的激增，这些模型旨在反映来源领域的语言环境和内容的特殊性。本文详细介绍了DAEDRA的构思、设计、训练和评估，这是一个旨在检测被动报告（PR）中的监管相关结果（死亡、急诊就诊和住院）的LLM。虽然PR是一种高效的从广泛和多样化受众（通常不仅包括医生和医护人员，还包括患者、家庭成员和其他非专业利益相关者）那里获取信息的方式，但是这种多样性使PR语料库难以分析。通用语言模型可能无法捕捉复杂的临床维度，而特定的临床或生物医学模型可能在非专业报告上表现不佳。

    arXiv:2402.10951v1 Announce Type: new  Abstract: Over the recent years, the emergence of large language models (LLMs) has given rise to a proliferation of domain-specific models that are intended to reflect the particularities of linguistic context and content as a correlate of the originating domain. This paper details the conception, design, training and evaluation of DAEDRA, a LLM designed to detect regulatory-relevant outcomes (mortality, ER attendance and hospitalisation) in adverse event reports elicited through passive reporting (PR). While PR is a highly cost-efficient way of eliciting information from a wide and diverse audience -- typically including not only physicians and healthcare providers but also patients, family members and other lay stakeholders --, this diversity makes PR corpora difficult to analyse. Generic language models may not capture the complex clinical dimensions while specific clinical or biomedical models may not perform well on lay reports. To evaluate t
    
[^253]: 异类自动提示的不合理有效性

    The Unreasonable Effectiveness of Eccentric Automatic Prompts

    [https://arxiv.org/abs/2402.10949](https://arxiv.org/abs/2402.10949)

    异类自动提示的不合理有效性研究了大型语言模型在处理各种提示时的表现，结果显示在大多数情况下，包括“积极思考”提示会对模型性能产生积极影响。

    

    大型语言模型（LLMs）展示了出色的问题解决和基本数学能力。然而，它们的功效高度依赖于提示的制定。本研究旨在量化将“积极思考”纳入系统提示消息的影响，然后将其与系统化提示优化进行比较。我们评估了60种系统消息片段的性能，分别使用和不使用Chain of Thought提示，跨三个参数范围从70亿到70亿个变量的模型，在GSM8K数据集上进行测试。我们的发现表明，结果并不在所有模型中普遍适用。在大多数情况下，包括“积极思考”提示会积极影响模型性能。然而，值得注意的是，Llama2-70B在不使用Chain of Thought时是个例外，因为发现最佳系统消息实际上是没有消息。考虑到组合复杂性，以及其导至的加# Truncated due to exceeding character limit.

    arXiv:2402.10949v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus com
    
[^254]: 在社交媒体上结合心理量表进行零-shot可解释的心理健康分析

    Zero-shot Explainable Mental Health Analysis on Social Media by incorporating Mental Scales

    [https://arxiv.org/abs/2402.10948](https://arxiv.org/abs/2402.10948)

    该方法结合心理量表通过LLMs进行零-shot心理健康分析，实验结果表明其优于其他方法

    

    传统的心理健康分析方法在容量方面表现强大，但缺乏解释能力，并且需要大规模注释的数据。另一方面，基于大型语言模型（LLMs）的生成式方法有潜力摆脱繁重的注释并提供解释。受到使用量表评估心理状态的心理评估实践的启发，我们的方法通过LLMs结合了两个程序。首先，患者完成心理健康问卷，其次，心理学家解释来自心理健康问题的收集信息并做出明智决策。实验结果表明，我们的方法胜过其他零-shot方法。

    arXiv:2402.10948v1 Announce Type: cross  Abstract: Traditional discriminative approaches in mental health analysis are known for their strong capacity but lack interpretability and demand large-scale annotated data. On the other hand, generative approaches, such as those based on large language models (LLMs),have the potential to get rid of heavy annotations and provide explanations. However, their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is a black-box process. Inspired by the psychological assessment practice of using scales to evaluate mental states, our method incorporates two procedures via LLMs. First, the patient completes mental health questionnaires, and second, the psychologist interprets the collected information from the mental health questions and makes informed decisions. Experimental results show that our method outperforms other zero-shot methods. Our 
    
[^255]: 将文化差异纳入大型语言模型的研究

    CultureLLM: Incorporating Cultural Differences into Large Language Models

    [https://arxiv.org/abs/2402.10946](https://arxiv.org/abs/2402.10946)

    提出了一种名为CultureLLM的成本效益高的解决方案，通过使用世界价值调查（WVS）作为种子数据，并通过提出的语义数据增强来将文化差异纳入大型语言模型中，成功微调得到了涵盖富裕和低资源语言的9种文化特定LLMs以及一个统一模型（CultureLLM-One）。

    

    大型语言模型（LLMs）被报道偏向于某些文化，因为训练数据主要来自英语语料库。由于多语种文化数据通常较难收集，现有的工作通过提示工程或特定文化的预训练来处理这一问题。然而，它们可能忽视了低资源文化的知识缺乏，并需要大量的计算资源。本文提出了CultureLLM，这是一个成本效益高的解决方案，可将文化差异纳入LLMs中。CultureLLM采用世界价值调查（WVS）作为种子数据，并通过提出的语义数据增强生成语义等效的训练数据。仅使用来自WVS的50个种子样本和增强数据，我们对9种包括富裕和低资源语言的文化特定LLMs和一个统一模型（CultureLLM-One）进行了微调。对60个与文化相关的数据集进行的大量实验表明，CultureLLM在增强LLM的文化特性方面取得了显著的成果。

    arXiv:2402.10946v1 Announce Type: cross  Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM signif
    
[^256]: 开源阿拉伯语文字识别技术的进展与局限性：以阿尔-阿巴斯为案例研究

    Advances and Limitations in Open Source Arabic-Script OCR: A Case Study

    [https://arxiv.org/abs/2402.10943](https://arxiv.org/abs/2402.10943)

    本文研究了开源OCR引擎Kraken在阿拉伯学术期刊al-Abhath上的准确性，表明其可以生成高度准确的阿拉伯文本OCR。文章建议通过更系统的训练数据生产和关键技术组件的开发来显着提高阿拉伯文字OCR的性能。

    

    本文介绍了对开源OCR引擎Kraken在领先的阿拉伯学术期刊al-Abhath上的准确性研究。与其他商业OCR引擎相比，Kraken表现出能够生成非常准确的阿拉伯文本OCR的能力。研究还评估了al-Abhath数据上特定字体和广义模型的相对准确性，并对“错误实例”进行了微观分析以及可能导致OCR错误识别的上下文特征。基于这一分析，论文提出，通过更加系统的训练数据生产方法以及关键技术组件的开发（特别是多语言模型和改进的线段分割和布局分析），阿拉伯文字OCR可以得到显著改进。

    arXiv:2402.10943v1 Announce Type: new  Abstract: This work presents an accuracy study of the open source OCR engine, Kraken, on the leading Arabic scholarly journal, al-Abhath. In contrast with other commercially available OCR engines, Kraken is shown to be capable of producing highly accurate Arabic-script OCR. The study also assesses the relative accuracy of typeface-specific and generalized models on the al-Abhath data and provides a microanalysis of the ``error instances'' and the contextual features that may have contributed to OCR misrecognition. Building on this analysis, the paper argues that Arabic-script OCR can be significantly improved through (1) a more systematic approach to training data production, and (2) the development of key technological components, especially multi-language models and improved line segmentation and layout analysis.   Cet article pr{\'e}sente une {\'e}tude d'exactitude du moteur ROC open source, Krakan, sur la revue acad{\'e}mique arabe de premier 
    
[^257]: Text2Data：使用文本控制的低资源数据生成

    Text2Data: Low-Resource Data Generation with Textual Control

    [https://arxiv.org/abs/2402.10941](https://arxiv.org/abs/2402.10941)

    Text2Data提出了一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法，以解决低资源环境下缺乏文本标签的文本到数据任务中的挑战。

    

    自然语言作为人类与机器无缝交互的一种常见直接控制信号。意识到这一接口的重要性，机器学习社区正在投入大量精力生成与文本指令在语义上一致的数据。虽然在涵盖图像编辑、音频合成、视频生成等领域取得了进展，但低资源领域由于昂贵注释或复杂数据结构（如分子、运动动态和时序）等特点，往往缺乏文本标签。这种不足阻碍了监督学习，从而限制了将先进生成模型应用于文本到数据任务的可能性。为了应对低资源场景中的这些挑战，我们提出了Text2Data，这是一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法。

    arXiv:2402.10941v1 Announce Type: cross  Abstract: Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model
    
[^258]: 临床程序代码的神经机器翻译用于医学诊断和不确定性量化

    Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification

    [https://arxiv.org/abs/2402.10940](https://arxiv.org/abs/2402.10940)

    研究引入了医学熵的概念，通过神经机器翻译基于ICD-9代码的患者预测结果，量化了不确定性。

    

    临床决策支持系统（CDSS）旨在通过将系统生成的建议与医学专业知识结合来增强临床医生的决策能力。本研究引入了医学熵的概念，通过基于手术ICD-9代码的神经机器翻译来量化患者预测结果中的不确定性。我们的实验结果不仅展示了程序代码与实际医疗结果之间的强相关性，

    arXiv:2402.10940v1 Announce Type: new  Abstract: A Clinical Decision Support System (CDSS) is designed to enhance clinician decision-making by combining system-generated recommendations with medical expertise. Given the high costs, intensive labor, and time-sensitive nature of medical treatments, there is a pressing need for efficient decision support, especially in complex emergency scenarios. In these scenarios, where information can be limited, an advanced CDSS framework that leverages AI (artificial intelligence) models to effectively reduce diagnostic uncertainty has utility. Such an AI-enabled CDSS framework with quantified uncertainty promises to be practical and beneficial in the demanding context of real-world medical care. In this study, we introduce the concept of Medical Entropy, quantifying uncertainties in patient outcomes predicted by neural machine translation based on the ICD-9 code of procedures. Our experimental results not only show strong correlations between proce
    
[^259]: 新闻来源可信度评估：Reddit案例研究

    News Source Credibility Assessment: A Reddit Case Study

    [https://arxiv.org/abs/2402.10938](https://arxiv.org/abs/2402.10938)

    提出了针对Reddit提交的来源可信度评估模型CREDiBERT，采用半监督训练方法，结合Siamese神经网络显著提高了提交可信度的分类准确性，并引入了一种新的Reddit帖子-帖子网络版本来增强用户互动编码。

    

    在社交媒体平台时代，识别在线内容的可信度对抗击错误信息至关重要。我们提出了一种名为CREDiBERT (CREDibility assessment using Bi-directional Encoder Representations from Transformers) 的来源可信度评估模型，针对Reddit提交进行了微调，主要关注政治话语。我们采用半监督训练方法对CREDiBERT进行训练，利用Reddit的基于社区的结构。通过使用CREDiBERT对提交内容进行编码并将其整合到Siamese神经网络中，我们显著改善了提交可信度的二元分类，相比现有方法，F1分数提高了9%。此外，我们引入了Reddit上的一种新版本的帖子-帖子网络，有效对用户互动进行编码，将二元分类任务的F1分数提高了近8%。最后，我们利用CREDiBERT评估了sus。

    arXiv:2402.10938v1 Announce Type: new  Abstract: In the era of social media platforms, identifying the credibility of online content is crucial to combat misinformation. We present the CREDiBERT (CREDibility assessment using Bi-directional Encoder Representations from Transformers), a source credibility assessment model fine-tuned for Reddit submissions focusing on political discourse as the main contribution. We adopt a semi-supervised training approach for CREDiBERT, leveraging Reddit's community-based structure. By encoding submission content using CREDiBERT and integrating it into a Siamese neural network, we significantly improve the binary classification of submission credibility, achieving a 9% increase in F1 score compared to existing methods. Additionally, we introduce a new version of the post-to-post network in Reddit that efficiently encodes user interactions to enhance the binary classification task by nearly 8% in F1 score. Finally, we employ CREDiBERT to evaluate the sus
    
[^260]: LLM辅助危机管理：构建用于有效应急响应和公众协作的先进LLM平台

    LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration

    [https://arxiv.org/abs/2402.10908](https://arxiv.org/abs/2402.10908)

    通过LLAMA2语言模型，建立了一个能够从社交媒体和紧急消息中识别和分类紧急情况的方法，可协助在全国范围内的紧急情况下公共安全话务员和大众，提供相关指导并通知政府机构。

    

    紧急情况和重大事件往往迅速发展，需要迅速有效的响应。本研究介绍了一种新方法，利用开源大型语言模型LLAMA2，从社交媒体帖子和直接的紧急消息中识别和分类紧急情况。旨在利用自然语言处理和机器学习的力量，协助公共安全话务员和大量民众在全国范围内的紧急情况中。我们的研究集中于开发一种语言模型，能够理解用户在911呼叫中描述自己的情况，使LLAMA2能够分析内容并为话务员提供相关指导，同时创建工作流程，在必要时将呼叫者信息通知政府机构。该语言模型提供的另一个好处是，当911系统不堪重负时，它能够在重大紧急事件中协助人们。

    arXiv:2402.10908v1 Announce Type: cross  Abstract: Emergencies and critical incidents often unfold rapidly, necessitating a swift and effective response. In this research, we introduce a novel approach to identify and classify emergency situations from social media posts and direct emergency messages using an open source Large Language Model, LLAMA2. The goal is to harness the power of natural language processing and machine learning to assist public safety telecommunicators and huge crowds during countrywide emergencies. Our research focuses on developing a language model that can understand users describe their situation in the 911 call, enabling LLAMA2 to analyze the content and offer relevant instructions to the telecommunicator, while also creating workflows to notify government agencies with the caller's information when necessary. Another benefit this language model provides is its ability to assist people during a significant emergency incident when the 911 system is overwhelme
    
[^261]: 基于分类法的大型语言模型评估检查表

    Taxonomy-based CheckList for Large Language Model Evaluation

    [https://arxiv.org/abs/2402.10899](https://arxiv.org/abs/2402.10899)

    本研究在大型语言模型中引入人类知识，通过问答任务探究LM的不道德行为，发现了一致性和偏见倾向之间的关联。

    

    由于大型语言模型（LLMs）已被用于许多下游任务，内在的刻板印象可能影响输出的公平性。在这项工作中，我们将人类知识引入自然语言干预，并研究预训练语言模型（LMs）在性别偏见上的行为。受CheckList行为测试的启发，我们提出了一项类似检查表的任务，旨在通过问答（QA）探究并量化LMs的不道德行为。我们设计了三项比较研究，以评估LMs的一致性、偏见倾向、模型偏好和性别偏好切换。我们探究了一个基于transformer的QA模型（在SQuAD-v2数据集上训练）和一个自回归的大型语言模型。我们的结果表明，transformer-based QA模型的偏见倾向与其一致性呈正相关，而LLM表现出相反的关系。我们提出的任务提供了第一个数据集

    arXiv:2402.10899v1 Announce Type: new  Abstract: As large language models (LLMs) have been used in many downstream tasks, the internal stereotypical representation may affect the fairness of the outputs. In this work, we introduce human knowledge into natural language interventions and study pre-trained language models' (LMs) behaviors within the context of gender bias. Inspired by CheckList behavioral testing, we present a checklist-style task that aims to probe and quantify LMs' unethical behaviors through question-answering (QA). We design three comparison studies to evaluate LMs from four aspects: consistency, biased tendency, model preference, and gender preference switch. We probe one transformer-based QA model trained on SQuAD-v2 dataset and one autoregressive large language model. Our results indicate that transformer-based QA model's biased tendency positively correlates with its consistency, whereas LLM shows the opposite relation. Our proposed task provides the first dataset
    
[^262]: LLMs下的时间序列预测：理解和增强模型能力

    Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities

    [https://arxiv.org/abs/2402.10835](https://arxiv.org/abs/2402.10835)

    本研究通过比较LLMs与传统模型，发现了LLMs在时间序列预测中的优势和局限性，指出LLMs在预测具有明显模式和趋势的时间序列方面表现出色，但在缺乏周期性的数据集方面面临挑战，同时指出融入外部知识和采用自然语言释义有助于提升LLMs在时间序列预测中的性能。

    

    大语言模型(LLMs)近年来在许多领域得到迅速发展。作为一种经典的机器学习任务，时间序列预测最近从LLMs中获得了推动。然而，在这一领域，LLMs的偏好存在研究空白。通过将LLMs与传统模型进行比较，发现了LLMs在时间序列预测中的许多特性。例如，我们的研究表明，LLMs在预测具有明显模式和趋势的时间序列方面表现出色，但在缺乏周期性的数据集方面面临挑战。我们通过设计提示要求LLMs告知数据集的周期来解释我们的发现。此外，本文还研究了输入策略，发现融入外部知识和采用自然语言释义积极影响了LLMs在时间序列预测中的预测性能。总的来说，这项研究有助于洞察LLMs在时间序列预测中的优势和局限性。

    arXiv:2402.10835v1 Announce Type: new  Abstract: Large language models (LLMs) have been applied in many fields with rapid development in recent years. As a classic machine learning task, time series forecasting has recently received a boost from LLMs. However, there is a research gap in the LLMs' preferences in this field. In this paper, by comparing LLMs with traditional models, many properties of LLMs in time series prediction are found. For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity. We explain our findings through designing prompts to require LLMs to tell the period of the datasets. In addition, the input strategy is investigated, and it is found that incorporating external knowledge and adopting natural language paraphrases positively affects the predictive performance of LLMs for time series. Overall, this study contributes to insight into the advantages and limitations of
    
[^263]: 在InSaAF中融入安全性，通过准确性和公平性 | LLM是否已经准备好进入印度法律领域？

    InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?

    [https://arxiv.org/abs/2402.10567](https://arxiv.org/abs/2402.10567)

    本研究在印度法律领域探讨了大型语言模型（LLMs）在处理社会因素时的能力，提出了结合公平性和准确性的新指标$LSS_{\beta}$，并评估了模型在二元法律推理任务中的表现以及在印度社会各种不平等方面的公平性展示。

    

    语言技术和人工智能的最新进展已经导致提出了众多语言模型，用于执行法律领域的各种任务，从预测判决到生成摘要。尽管它们具有巨大潜力，但已经证明这些模型学习并展示社会偏见，并做出不公平的预测。在这项研究中，我们探讨了当涉及社会因素时大型语言模型（LLMs）在印度法律领域执行任务的能力。我们提出了一种新颖的度量标准，$\beta$-加权的$\textit{法律安全分数($LSS_{\beta}$)}$，将LLM的公平性和准确性两个方面结合起来。我们通过考虑LLM在$\textit{二元法律推理}$任务中的表现以及其在印度社会各种不平等方面的公平展示来评估LLMs的安全性。LLaMA和LLaMA--2模型的任务表现和公平得分表明...

    arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th
    
[^264]: 基于上下文的奖励：基于动态偏好调整的多目标基础模型对齐

    Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment

    [https://arxiv.org/abs/2402.10207](https://arxiv.org/abs/2402.10207)

    本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。

    

    我们考虑了基于人类偏好的基础模型多目标对齐问题，这是实现有益和无害的人工智能系统的关键步骤。然而，使用强化学习（RL）对大型基础模型进行微调通常是昂贵且不稳定的，并且人类偏好的多维度、异质性和冲突性进一步复杂化了对齐过程。在本文中，我们引入了Rewards-in-Context（RiC）方法，它使得基础模型的响应取决于其提示上下文中的多个奖励，并应用有监督的微调来进行对齐。RiC的显著特点是简单性和适应性，因为它只需要对单个基础模型进行有监督的微调，并支持在推理时动态调整用户偏好。受到抽象的凸优化问题的解析解的启发，我们提出了一种动态推理时调整方法。

    arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
    
[^265]: 重塑RLHF中的信息结构：基于图论的奖励泛化视角

    Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective

    [https://arxiv.org/abs/2402.10184](https://arxiv.org/abs/2402.10184)

    本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。

    

    在强化学习从人类反馈中（RLHF）存在一个三难问题：高度多样的环境、低标注成本和可靠的对齐性能之间的不兼容性。本文旨在通过设计奖励建模过程中的数据集信息结构来缓解这种不兼容性。具体而言，我们重新审视了RLHF过程，并提出了一个理论框架将其描绘为文本分布上的自动编码过程。我们的框架形式化了RLHF目标，即确保人类偏好与大型语言模型（LLM）行为之间的分布一致性。基于这个框架，我们系统地研究了RLHF奖励建模阶段中信息结构的性能影响。为了进一步理解奖励建模阶段中的奖励泛化，我们引入了一种基于随机图论的方法来建模语义空间中的泛化。其中的关键见解是...

    arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
    
[^266]: LoraRetriever: 适应输入的LoRA检索与合成方法用于混合任务

    LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild

    [https://arxiv.org/abs/2402.09997](https://arxiv.org/abs/2402.09997)

    LoraRetriever提出了一种适应输入的LoRA检索与合成方法，用于弥合实际情况下大型语言模型接收到不同任务提示的差距。

    

    Low-Rank Adaptation (LoRA)为大型语言模型（LLM）的微调提供了一种有效而高效的解决方案。LoRA的模块化和即插即用的特性使得能够集成各种领域特定的LoRA，以增强LLM的能力。先前的研究要么专注于特定的隔离下游任务，要么在训练过程中固定LoRA的选择。然而，在实际情况中，LLM接收到涵盖不同任务的各种提示，并且候选LoRA的池经常动态更新。为了弥合这一差距，我们提出了LoraRetriever，一种根据输入提示自适应检索和合成多个LoRA的框架。LoraRetriever包含三个主要组成部分：首先，识别和检索与给定输入相关的LoRA；其次，制定有效整合检索到的LoRA的策略；最后，开发高效的方法用于实现LoRA的合成。

    arXiv:2402.09997v1 Announce Type: new  Abstract: Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLM). The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs. Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts. LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing effici
    
[^267]: LLMs作为桥梁：重新构建基于多模态图像的命名实体识别

    LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition

    [https://arxiv.org/abs/2402.09989](https://arxiv.org/abs/2402.09989)

    本文提出了RiVEG，一个统一的框架，通过利用大型语言模型（LLMs）作为连接桥梁，将多模态命名实体识别重新构建为联合任务，解决了命名实体无法确定和指代表达与命名实体之间的区别的问题。

    

    Grounded Multimodal Named Entity Recognition (GMNER) 是一个新兴的多模态任务，旨在识别命名实体、实体类型及其对应的视觉区域。GMNER任务具有两个挑战性质：1）社交媒体中图像和文本之间的弱相关性导致大部分命名实体难以确定；2）常用于类似任务的粗粒度指代表达与细粒度命名实体之间存在明显区别。本文提出了RiVEG，一个统一的框架，通过利用大型语言模型（LLMs）作为连接桥梁，将GMNER重新构建为联合MNER-VE-VG任务。这种重新构建带来了两个好处：1）保持了最佳的MNER性能，消除了使用目标检测方法预提取区域特征的需求，自然解决了这两个挑战。

    arXiv:2402.09989v1 Announce Type: cross  Abstract: Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two m
    
[^268]: 制定良好提示还是提供出色的对话？关于基于上下文学习的角色生成对话的研究

    Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation

    [https://arxiv.org/abs/2402.09954](https://arxiv.org/abs/2402.09954)

    本研究通过对大型语言模型在基于角色生成对话方面进行实验，发现调整提示指令可以最直接有效且经济地提高生成质量，并且随机检索示范会取得最佳结果，而查询相同上下文的示范检索效果最差。即使破坏了示范中的多回合关联和单回合语义，对话生成仍然有效。

    

    先前关于上下文学习（ICL）的研究主要侧重于分类、机器翻译、文本到表格等任务，而对于ICL能否改进生成类似人类对话的研究很少。我们的工作通过在高质量的真实人类对话数据集上进行广泛的实验，系统地研究了大型语言模型（LLMs）在基于角色生成对话方面的ICL能力。根据实验结果，我们得出三个结论：1）调整提示指令是提高生成质量最直接、有效和经济的方法；2）随机检索示范可以取得最佳的结果，可能是因为具有更多样化和有效信息的原因；与查询相同上下文的示范检索结果最差；3）即使破坏了示范中的多回合关联和单回合语义，对话生成仍然可以实现较好的效果。

    arXiv:2402.09954v1 Announce Type: new  Abstract: Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demo
    
[^269]: 在生成人工智能时代，大型语言模型基准的不足之处

    Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence

    [https://arxiv.org/abs/2402.09880](https://arxiv.org/abs/2402.09880)

    该论文通过批判性评估研究了23个最先进的大型语言模型基准的不足之处，包括偏见、真实推理衡量困难、实现不一致性等问题，强调了在人工智能时代需要标准化方法、监管确定性和伦理指南。

    

    大型语言模型（LLMs）随着其新兴能力的快速崛起，引发了公众的好奇心，以评估和比较不同的LLMs，许多研究人员提出了他们的LLM基准。我们注意到这些基准的初步不足，开始了一项研究，通过人们、过程和技术的视角，以功能和安全两大支柱为基础，使用我们的新颖统一评估框架对23个最先进的LLM基准进行了批判性评估。我们的研究揭示了一些重大限制，包括偏见、测量真实推理的困难、适应性、实现不一致性、提示工程复杂性、评估者多样性以及在一次综合评估中忽视了文化和意识形态规范。我们的讨论强调了在人工智能时代，迫切需要标准化方法、监管确定性和伦理指南。

    arXiv:2402.09880v1 Announce Type: new  Abstract: The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of functionality and security. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligenc
    
[^270]: 使用大型语言模型在药物分子和适应症之间进行翻译的新机遇

    Emerging Opportunities of Using Large Language Language Models for Translation Between Drug Molecules and Indications

    [https://arxiv.org/abs/2402.09588](https://arxiv.org/abs/2402.09588)

    本研究探讨了使用大型语言模型在药物分子和适应症之间进行翻译的机遇，提出了一个新任务，并测试了其有效性，这对于药物发现过程具有重要意义。

    

    药物分子是一种改变生物体精神或身体状态的物质。每种批准的药物都有适应症，指的是该药物治疗特定医疗条件的治疗用途。虽然大型语言模型（LLM），一种生成式人工智能技术，最近已经证明在分子和其文本描述之间进行翻译方面是有效的，但仍存在关于在药物分子和适应症（或反之）之间进行翻译的应用的研究空白，这可能极大地有益于药物发现过程。从给定的适应症生成药物的能力将允许发现针对特定疾病或靶点的药物，并最终为患者提供更好的治疗方法。在本文中，我们首先提出了一个新任务，即药物分子和相应适应症之间的翻译，然后进行了实验测试。

    arXiv:2402.09588v1 Announce Type: new  Abstract: A drug molecule is a substance that changes the organism's mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test exi
    
[^271]: LlaSMol:利用大规模、全面、高质量的指令调优数据集推进化学的大规模语言模型

    LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset

    [https://arxiv.org/abs/2402.09391](https://arxiv.org/abs/2402.09391)

    本文介绍了LlaSMol，它是一种推进化学领域大规模语言模型的方法。通过使用一个大规模、全面、高质量的指令调优数据集来训练模型，LlaSMol在化学任务中表现出强大的性能，超过了GPT-4并接近于任务特定模型。

    

    化学在药物研发和材料科学等许多领域中起着至关重要的作用。尽管诸如GPT-4之类的大型语言模型（LLM）在自然语言处理任务上展现出了非凡的能力，但现有工作表明它们在化学任务上的性能令人失望。然而，在本文中，我们展示了我们开发的LLM在一系列化学任务上可以取得非常强大的结果，在所有任务上都显著优于最先进的GPT-4，并接近SoTA任务特定模型。我们取得成功的关键是一个名为SMolInstruct的大规模、全面、高质量的指令调优数据集。它包含了14个经过精心挑选的化学任务和超过三百万个高质量样本，为训练和评估化学LLM奠定了坚实基础。基于SMolInstruct，我们对一组开源LLM进行了微调，其中，我们发现Mistral ser是最佳性能的模型。

    arXiv:2402.09391v1 Announce Type: new Abstract: Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral ser
    
[^272]: SafeDecoding: 通过安全感知解码防御越狱攻击

    SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding

    [https://arxiv.org/abs/2402.08983](https://arxiv.org/abs/2402.08983)

    本文提出了一种名为SafeDecoding的解决方案，通过安全感知解码策略来防御大型语言模型（LLMs）的越狱攻击。该策略可以生成对用户查询有益且无害的响应，有效缓解了LLMs安全性威胁。

    

    随着大型语言模型（LLMs）越来越多地应用于代码生成和聊天机器人辅助等现实应用中，人们为了使LLM的行为与人类价值观保持一致，包括安全性在内做出了大量努力。越狱攻击旨在引发LLM的非预期和不安全行为，仍然是LLM安全性的重要威胁。本文旨在通过引入SafeDecoding来防御LLM的越狱攻击，这是一种安全感知的解码策略，用于生成对用户查询有益且无害的响应。我们在开发SafeDecoding时的洞察力基于观察到，即使代表有害内容的标记的概率超过代表无害响应的标记的概率，安全免责声明仍然出现在按概率降序排序的标记中的前几个。这使我们能够通过识别安全免责声明并增强其良性影响力来减轻越狱攻击。

    arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
    
[^273]: 鲁棒结构预测的结构化语言生成模型

    Structured Language Generation Model for Robust Structure Prediction

    [https://arxiv.org/abs/2402.08971](https://arxiv.org/abs/2402.08971)

    鲁棒结构预测的结构化语言生成模型通过新的损失函数和推理方法的混合，成功提高了结构化输出的泛化能力，并且可以在没有数据集信息的情况下工作，并且减少了格式错误。

    

    我们提出了一种结构化语言生成模型（SLGM），通过新的损失函数和推理方法的混合来改善结构化输出的泛化能力。以往的结构预测研究（如NER，RE）利用了显式的数据集信息，这可以提高性能，但可能会对现实世界中的鲁棒泛化性产生挑战。相反，我们的模型间接地提供了有关数据的通用格式信息。利用格式信息，我们可以通过损失校准和格式化解码将序列到序列问题简化为分类问题。我们的实验结果表明，SLGM在没有数据集信息的情况下成功保持了性能，并且显示出较少的格式错误。我们还展示了我们的模型可以像适配器一样在各个数据集上工作，而无需额外的训练。

    arXiv:2402.08971v1 Announce Type: new Abstract: We propose Structured Language Generation Model (SLGM), a mixture of new loss function and inference method for better generalization of structured outputs. Previous studies on structure prediction (e.g. NER, RE) make use of explicit dataset information, which would boost performance, yet it might pose challenges to robust generalization in real-world situations. Instead, our model gives generalized format information about data indirectly. With format information, we could reduce sequence-to-sequence problem into classification problem via loss calibration and formatted decoding. Our experimental results showed SLGM successfully maintain performance without dataset information, and showed much less format errors. We also showed our model can work like adapters on individual dataset, with no additional training.
    
[^274]: 在黑盒大型语言模型上进行知识编辑

    Knowledge Editing on Black-box Large Language Models

    [https://arxiv.org/abs/2402.08631](https://arxiv.org/abs/2402.08631)

    这项研究提出了在黑盒大型语言模型上进行知识编辑的方法，并引入了一种多角度评估框架和一种新的postEdit框架，以解决现有方法中的隐私和风格问题。

    

    知识编辑旨在高效、精确地修改大型语言模型的行为，以更新特定的知识，而不对其他知识产生负面影响。当前的研究主要集中在白盒语言模型编辑上，忽视了一个重要的场景：黑盒语言模型编辑，即通过接口访问语言模型，并仅可用文本输出。为了解决现有评估在黑盒语言模型编辑上不适用且缺乏全面性的局限性，我们提出了一种多角度评估框架，首次将风格保留的评估纳入其中。为了解决当前方法中的编辑数据隐私泄漏和风格过度编辑的问题，我们引入了一种新的postEdit框架，通过下游后处理解决隐私问题，并通过对原始回答进行细粒度编辑来保持文本风格一致性。在两个基准测试上的实验与分析表明，postEdit的性能超过了所有现有方法。

    Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all 
    
[^275]: ChatCell: 利用自然语言促进单细胞分析

    ChatCell: Facilitating Single-Cell Analysis with Natural Language

    [https://arxiv.org/abs/2402.08303](https://arxiv.org/abs/2402.08303)

    ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。

    

    随着大型语言模型(LLMs)的快速发展，它们在科学中的影响日益突出。LLMs在任务泛化和自由对话方面的新兴能力可以极大地推进化学和生物学等领域。然而，单细胞生物学这个构成生物体基础构件的领域仍面临一些挑战。当前方法在知识门槛和可扩展性方面存在限制，阻碍了LLMs在掌握单细胞数据方面的充分利用，影响了直接可访问和快速迭代的能力。为此，我们引入了ChatCell，通过利用词汇适应和统一序列生成，它在单细胞生物学领域获得了深厚的专业知识和适应各种分析任务的能力，标志着一种范式转变。

    As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
    
[^276]: UFO: 一个专注于Windows操作系统交互的用户界面智能体

    UFO: A UI-Focused Agent for Windows OS Interaction

    [https://arxiv.org/abs/2402.07939](https://arxiv.org/abs/2402.07939)

    UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。

    

    我们介绍了UFO，一个创新的专注于Windows操作系统上应用程序的用户界面智能体，利用了GPT-Vision的能力来满足用户需求。UFO采用双智能体框架，精确观察和分析Windows应用程序的图形用户界面（GUI）和控制信息。这使得智能体可以无缝地在单个应用程序内以及跨应用程序进行导航和操作，以满足用户的需求，即使涉及多个应用程序。该框架包括一个控制交互模块，实现无需人工干预的动作连接，并实现完全自动化执行。因此，UFO将艰巨而耗时的过程转变为仅通过自然语言命令就可以完成的简单任务。我们在9个流行的Windows应用程序上对UFO进行了测试，涵盖了反映用户日常使用情景的各种情况。通过定量指标和真实案例研究得出的结果强调了UFO的效果。

    We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
    
[^277]: 通过LLM-认知数据增强广义对话密集检索

    Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation

    [https://arxiv.org/abs/2402.07092](https://arxiv.org/abs/2402.07092)

    本文提出了一种通过LLM-认知数据增强的方法来广义对话密集检索。该方法首先生成多级增强对话，捕捉多样的对话环境。其次，通过认知感知过程减少错误生成情况，并通过难度自适应样本筛选器选择具有挑战性的样本。

    

    对话式搜索利用多轮自然语言环境来检索相关段落。现有的对话密集检索模型大多将对话视为一系列固定的问题和回答，忽视了严重的数据稀疏性问题 - 也就是说，用户可以以不同的方式进行对话，而这些备选对话是未记录的。因此，它们经常难以推广到真实场景中的多样对话。在这项工作中，我们提出了一种通过LLM-认知数据增强广义对话密集检索的框架(ConvAug)。ConvAug首先生成多级增强对话，以捕捉对话环境的多样性。受人类认知方式的启发，我们设计了一种认知感知过程，以减少错误的正例、负例和幻觉的生成。此外，我们还开发了一种难度自适应样本筛选器，用于选择复杂对话的具有挑战性的样本。

    Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby g
    
[^278]: 内省规划：引导语言驱动的代理机器人改进自身的不确定性

    Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty

    [https://arxiv.org/abs/2402.06529](https://arxiv.org/abs/2402.06529)

    本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。

    

    大型语言模型（LLM）展示了先进的推理能力，使得机器人能够理解自然语言指令，并通过适当的基础塑造来策略性地进行高级行动规划。然而，LLM产生的幻觉可能导致机器人自信地执行与用户目标不符或在极端情况下不安全的计划。此外，自然语言指令中的固有歧义可能引发任务的不确定性，尤其是在存在多个有效选项的情况下。为了解决这个问题，LLMs必须识别此类不确定性并主动寻求澄清。本文探索了内省规划的概念，作为一种系统方法，引导LLMs在无需微调的情况下形成意识到不确定性的机器人任务执行计划。我们研究了任务级机器人规划中的不确定性量化，并证明与最先进的基于LLM的规划方法相比，内省显著提高了成功率和安全性。

    Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
    
[^279]: 使用大型语言模型的多模态临床试验结果预测

    Multimodal Clinical Trial Outcome Prediction with Large Language Models

    [https://arxiv.org/abs/2402.06512](https://arxiv.org/abs/2402.06512)

    本研究提出了一种名为LIFTED的多模态临床试验结果预测方法，通过将不同模态数据转化为自然语言描述来统一数据，并构建统一的抗噪声编码器进行信息提取。

    

    临床试验是一个关键且昂贵的过程，通常需要多年时间和大量财力资源。因此，开发临床试验结果预测模型旨在排除可能失败的药物，并具有显著的成本节约潜力。最近的数据驱动尝试利用深度学习方法整合多模态数据来预测临床试验结果。然而，这些方法依赖于手动设计的模态特定编码器，这限制了适应新模态的可扩展性和识别不同模态之间相似信息模式的能力。为了解决这些问题，我们提出了一种多模态专家混合（LIFTED）方法用于临床试验结果预测。具体而言，LIFTED通过将不同模态的数据转化为自然语言描述来统一不同模态数据。然后，LIFTED构建统一的抗噪声编码器，从模态特定的语言描述中提取信息。

    The clinical trial is a pivotal and costly process, often spanning multiple years and requiring substantial financial resources. Therefore, the development of clinical trial outcome prediction models aims to exclude drugs likely to fail and holds the potential for significant cost savings. Recent data-driven attempts leverage deep learning methods to integrate multimodal data for predicting clinical trial outcomes. However, these approaches rely on manually designed modal-specific encoders, which limits both the extensibility to adapt new modalities and the ability to discern similar information patterns across different modalities. To address these issues, we propose a multimodal mixture-of-experts (LIFTED) approach for clinical trial outcome prediction. Specifically, LIFTED unifies different modality data by transforming them into natural language descriptions. Then, LIFTED constructs unified noise-resilient encoders to extract information from modal-specific language descriptions. S
    
[^280]: 关于针对键值约束生成语言模型推理的驱逐策略的有效性研究

    On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference

    [https://arxiv.org/abs/2402.06262](https://arxiv.org/abs/2402.06262)

    本文研究了针对键值约束生成语言模型推理的驱逐策略的有效性，通过引入基于时间注意力得分和鲁棒性度量的RoCo策略，优于现有的策略。

    

    尽管大型语言模型（LLMs）在最近取得了成功，但由于它们对内存和计算资源的过度需求，它们在资源受限环境中部署仍然昂贵。除了模型参数外，键值缓存也存储在GPU内存中，随着批处理大小和序列长度的增加而线性增长。为此，最近的研究提出了各种针对给定预算下维护键值缓存开销的驱逐策略。本文着眼于现有驱逐策略在重要性评分计算和驱逐范围构建两个方面的效果。我们确定了先前策略在这两个方面的不足，并引入了基于时间注意力得分和鲁棒性度量的RoCo，一种强大的缓存驱逐策略。涵盖了预填充和自回归解码阶段的广泛实验验证了RoCo的优越性。最后，我们公开发布了RoCo的代码和模型供研究者使用。

    Despite the recent success associated with Large Language Models~(LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands. In addition to model parameters, the key-value cache is also stored in GPU memory, growing linearly with batch size and sequence length. As a remedy, recent works have proposed various eviction policies for maintaining the overhead of key-value cache under a given budget. This paper embarks on the efficacy of existing eviction policies in terms of \textit{importance score calculation} and \textit{eviction scope construction}. We identify the deficiency of prior policies in these two aspects and introduce RoCo, a \underline{r}\underline{o}bust \underline{c}ache \underline{o}mission policy based on temporal attention scores and robustness measures. Extensive experimentation spanning prefilling and auto-regressive decoding stages validates the superiority of RoCo. Finally, we relea
    
[^281]: 用自然语言和概率推理进行实验与修订规则

    Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning

    [https://arxiv.org/abs/2402.06025](https://arxiv.org/abs/2402.06025)

    本论文建立了一个计算模型来模拟人们通过实验主动推断隐藏规则的过程，并发现显式假设、概率规则和在线更新的组合可以解释人们在类似Zendo任务上的表现。

    

    我们建立了一个计算模型，模拟人们通过实验主动推断隐藏规则的过程。该模型的基本原理是，即使规则是确定性的，学习者也会考虑更广泛的模糊概率规则，并用自然语言表示，根据近似贝叶斯原则在每次实验后在线更新自己的假设。在同一框架下，我们还根据信息论准则建立了实验设计模型。我们发现，这三个原则的组合——显式假设、概率规则和在线更新——可以解释人们在类似Zendo任务上的表现，而去掉其中任何一个组件都使得模型无法解释数据。

    We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy probabilistic rules, which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles -- explicit hypotheses, probabilistic rules, and online updates -- can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.
    
[^282]: 研究指令调整的局限性

    A Closer Look at the Limitations of Instruction Tuning

    [https://arxiv.org/abs/2402.05119](https://arxiv.org/abs/2402.05119)

    本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。

    

    指令调整（IT）是使用指令-回应对来训练大型语言模型（LLM）的过程，已成为将基础预训练LLM转化为开放领域对话代理的主要方法。虽然IT取得了显著的成功并广泛应用，但其局限性和不足仍未得到充分探讨。本文通过严格的实验和对LLM通过IT发生的变化的深入分析，揭示了IT的多种局限性。特别是，我们发现：（1）IT无法增强LLM的知识或技能。LoRA微调仅限于学习回应的启动和样式令牌，而全参数微调会导致知识退化。（2）从具有知识来源的IT数据集复制回应模式会导致回应质量下降。（3）全参数微调通过不准确地从IT数据集中获取概念上相似实例的标记，增加了错误生成的情况。

    Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
    
[^283]: UltraLink: 一个开源的知识增强多语言监督微调数据集

    UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset

    [https://arxiv.org/abs/2402.04588](https://arxiv.org/abs/2402.04588)

    本论文构建了一个开源的多语言监督微调数据集UltraLink，通过引入基于知识的数据增强方法提升了语言模型在文化特定知识上的能力，同时发现现代语言模型具有强大的跨语言迁移能力，减少了语言无关数据集的需求。

    

    开源的大型语言模型(LLMs)在不同领域取得了显著的优势。然而，大部分研究主要集中在英文上，对于多语言监督微调的研究还相对有限。因此，在本研究中我们构建了一个开源的多语言监督微调数据集。与之前简单翻译英文指令的方法不同，我们考虑了LLMs的语言特定和语言无关能力。对于语言特定能力，我们引入了一个基于知识的数据增强方法，以提取LLMs更多的文化特定知识，提高它们为不同国家用户服务的能力。对于语言无关能力，通过实验发现现代LLMs展现出很强的跨语言迁移能力，因此多次学习相同内容的多种语言并不必要。因此，我们可以大幅减少语言无关SFT数据集。

    Open-source large language models (LLMs) have gained significant strength across diverse fields. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual supervised fine-tuning. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. For language-specific abilities, we introduce a knowledge-grounded data augmentation approach to elicit more culture-specific knowledge of LLMs, improving their ability to serve users from different countries. For language-agnostic abilities, we find through experiments that modern LLMs exhibit strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic SFT data w
    
[^284]: 在多模态大型语言模型中为图推理渲染图形

    Rendering Graphs for Graph Reasoning in Multimodal Large Language Models

    [https://arxiv.org/abs/2402.02130](https://arxiv.org/abs/2402.02130)

    本文在多模态大型语言模型中为图推理任务引入了视觉信息，并提出了一个新的基准测试数据集GITQA。实验证明，结合文本和视觉信息的结果比单一模态效果更好。

    

    大型语言模型(LLMs)在机器人规划、知识图谱补全和常识推理等任务中越来越多地使用图结构，LLMs能够理解文本格式的图信息，但忽视了丰富的视觉模态，而视觉是人类理解结构信息和进行图推理的直观方式。将图结构表示为视觉图像(即视觉图)的潜在益处和能力仍未被探索。本文在图推理任务中首次引入视觉信息，并提出一个新的基准测试数据集GITQA，其中每个样本是一个元组(图、图像、文本描述)。我们利用最先进的多模态LLMs在GITQA基准测试数据集上进行了大量实验证明，结合文本和视觉信息的结果比单一模态效果更好。此外，在LLaVA-7B/13B模型的微调上表现出色。

    Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on 
    
[^285]: 基于Transformer集成的多模态仇恨言论事件检测的MasonPerplexity方法

    MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech and Target Detection Using Transformer Ensembles

    [https://arxiv.org/abs/2402.01967](https://arxiv.org/abs/2402.01967)

    本文提出了一种名为MasonPerplexity的方法来解决多模态仇恨言论事件检测的问题。该方法采用Transformer集成的方式，在识别仇恨言论和识别文本图像中目标的任务中均取得了较好的成绩，分别排名第三。

    

    在网络社区中，自动识别诸如仇恨言论之类的冒犯性语言对于维护讨论的文明十分重要。在多模态内容中识别仇恨言论是一项特别具有挑战性的任务，因为冒犯性既可以体现在文字上，也可以体现在图像上，或者两者同时存在。本文介绍了在EACL 2024的CASE 2024上共享任务“多模态仇恨言论事件检测”中的MasonPerplexity方法。该任务分为两个子任务：子任务A注重识别仇恨言论，子任务B注重识别政治事件中嵌入文本图像中的目标。我们使用了一个XLM-roBERTa-large模型来处理子任务A，并使用集成方法将XLM-roBERTa-base、BERTweet-large和BERT-base模型结合起来处理子任务B。我们的方法在子任务A中获得了0.8347的F1分数，在子任务B中获得了0.6741的F1分数，并在两个子任务中排名第三。

    The automatic identification of offensive language such as hate speech is important to keep discussions civil in online communities. Identifying hate speech in multimodal content is a particularly challenging task because offensiveness can be manifested in either words or images or a juxtaposition of the two. This paper presents the MasonPerplexity submission for the Shared Task on Multimodal Hate Speech Event Detection at CASE 2024 at EACL 2024. The task is divided into two sub-tasks: sub-task A focuses on the identification of hate speech and sub-task B focuses on the identification of targets in text-embedded images during political events. We use an XLM-roBERTa-large model for sub-task A and an ensemble approach combining XLM-roBERTa-base, BERTweet-large, and BERT-base for sub-task B. Our approach obtained 0.8347 F1-score in sub-task A and 0.6741 F1-score in sub-task B ranking 3rd on both sub-tasks.
    
[^286]: 开发并测试一种新的基于大型语言模型的临床决策支持系统，用于药物安全的12种临床专业

    Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties

    [https://arxiv.org/abs/2402.01741](https://arxiv.org/abs/2402.01741)

    本研究开发并测试了一种新的基于大型语言模型的临床决策支持系统，用于在12个临床专业中提供安全的药物处方。该系统通过定制的处方错误警报解决了传统基于规则的系统的局限性，并评估了其在识别药物错误方面的有效性和临床医生的偏好。

    

    重要性：我们介绍了一种新颖的基于检索增强生成（RAG）-大型语言模型（LLM）的临床决策支持系统（CDSS），用于安全用药处方。该模型通过提供与患者背景和机构指南相关的处方错误警报，解决了传统基于规则的CDSS的局限性。目标：本研究评估了基于LLM的CDSS在识别各种医学和外科病例中的药物错误方面的有效性，与人工专家小组进行比较。它还研究了临床医生在不同CDSS集成方式（初级药师、仅基于LLM的CDSS和二者的组合）中的偏好。设计、设置和参与者：利用带有GPT-4.0的RAG模型，本研究涉及12个专业中23个临床案例的61个处方错误场景。专家小组使用PCNE分类和NCC MERP指数评估这些案例。三名初级药师独立审核每个场景，并提出处理建议。根据检查的错误和建议编制了反馈报告。 然后，三名医生独立审核这些报告，并提出对下一步处理的意见。

    Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large Language Model (LLM) as a Clinical Decision Support System (CDSS) for safe medication prescription. This model addresses the limitations of traditional rule-based CDSS by providing relevant prescribing error alerts tailored to patient context and institutional guidelines.   Objective: The study evaluates the efficacy of an LLM-based CDSS in identifying medication errors across various medical and surgical case vignettes, compared to a human expert panel. It also examines clinician preferences among different CDSS integration modalities: junior pharmacist, LLM-based CDSS alone, and a combination of both.   Design, Setting, and Participants: Utilizing a RAG model with GPT-4.0, the study involved 61 prescribing error scenarios within 23 clinical vignettes across 12 specialties. An expert panel assessed these cases using the PCNE classification and NCC MERP index. Three junior pharmacists independently reviewed eac
    
[^287]: 纠正检索增强生成

    Corrective Retrieval Augmented Generation

    [https://arxiv.org/abs/2401.15884](https://arxiv.org/abs/2401.15884)

    提出了纠正检索增强生成（CRAG）来改善生成模型的鲁棒性，通过设计轻量级检索评估器和利用大规模网络搜索扩展检索结果。

    

    大型语言模型（LLMs）不可避免地出现幻觉，因为生成的文本准确性不能仅通过它们封装的参数化知识来保证。尽管检索增强生成（RAG）是对LLMs的可行补充，但它严重依赖于检索文档的相关性，引发了如果检索出现问题模型将如何行为的担忧。为此，我们提出了纠正检索增强生成（CRAG）来提高生成的鲁棒性。具体地，设计了一个轻量级的检索评估器，用于评估为查询检索的文档的整体质量，根据返回的置信度触发不同的知识检索操作。由于从静态和有限的语料库中检索只能返回次优文档，因此利用大规模网络搜索作为扩展来增强检索结果。此外，还有一个分解-重组算法。

    arXiv:2401.15884v2 Announce Type: replace  Abstract: Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose alg
    
[^288]: 松饼还是吉娃娃？用多面板VQA挑战大型视觉语言模型

    Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA

    [https://arxiv.org/abs/2401.15847](https://arxiv.org/abs/2401.15847)

    引入了多面板视觉问答（MultipanelVQA）基准挑战大型视觉语言模型（LVLMs）对理解多面板图像的能力，并发现LVLMs在这方面仍然存在显著挑战。

    

    多面板图像，通常在网页截图、海报等中看到，充斥着我们的日常生活。这些图像以多个子图以不同布局组成，有效地向人们传达信息。为了构建高级的多模态人工智能应用，如能理解复杂场景并在网页中导航的代理程序，多面板视觉推理的技能是至关重要的，对模型在这方面进行全面评估是很重要的。因此，我们引入了多面板视觉问答（MultipanelVQA），这是一个新颖的基准，包括6,600个问题、答案和多面板图像三元组，专门挑战模型理解多面板图像。我们的评估表明，MultipanelVQA基准中的问题对测试的最先进的大型视觉语言模型（LVLMs）提出了重大挑战，即使人类可以获得约99%的准确率。

    arXiv:2401.15847v2 Announce Type: replace-cross  Abstract: Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99\% accurac
    
[^289]: RecDCL: 用于推荐的双重对比学习

    RecDCL: Dual Contrastive Learning for Recommendation

    [https://arxiv.org/abs/2401.15635](https://arxiv.org/abs/2401.15635)

    RecDCL提出了一个双重对比学习推荐框架，结合了批次对比学习（BCL）和特征对比学习（FCL），有助于消除冗余的解决方案，但又不会错过最优解。

    

    自监督学习（SSL）最近在挖掘协同过滤中的用户-项目交互方面取得了巨大成功。对比学习（CL）是一个重要范式，可以通过对比原始数据和增强数据之间的嵌入来解决网络平台中的数据稀疏性。然而，现有的基于CL的方法主要集中在批次方式对比上，未能充分利用特征维度中的潜在规律，这导致在用户和项目的表示学习过程中出现了冗余的解决方案。在这项工作中，我们研究了如何同时利用批次对比学习（BCL）和特征对比学习（FCL）进行推荐。我们在理论上分析了BCL和FCL之间的关系，并发现结合BCL和FCL有助于消除冗余解决方案，但永远不会错过最优解。我们提出了一个双重对比学习推荐框架-- RecDCL。在RecDCL中，FCL目标旨在消除 ...

    arXiv:2401.15635v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) has recently achieved great success in mining the user-item interactions for collaborative filtering. As a major paradigm, contrastive learning (CL) based SSL helps address data sparsity in Web platforms by contrasting the embeddings between raw and augmented data. However, existing CL-based methods mostly focus on contrasting in a batch-wise way, failing to exploit potential regularity in the feature dimension. This leads to redundant solutions during the representation learning of users and items. In this work, we investigate how to employ both batch-wise CL (BCL) and feature-wise CL (FCL) for recommendation. We theoretically analyze the relation between BCL and FCL, and find that combining BCL and FCL helps eliminate redundant solutions but never misses an optimal solution. We propose a dual contrastive learning recommendation framework -- RecDCL. In RecDCL, the FCL objective is designed to eli
    
[^290]: 我们是否需要语言特定的事实核查模型？以汉语为例

    Do We Need Language-Specific Fact-Checking Models? The Case of Chinese

    [https://arxiv.org/abs/2401.15498](https://arxiv.org/abs/2401.15498)

    本文研究了语言特定事实核查模型的潜在益处，提出了一个汉语事实核查系统，并展示其优于翻译方法和多语言大型语言模型，同时对偏见更加稳健，强调了语言特定性的重要性。

    

    本文研究了语言特定事实核查模型的潜在益处，重点关注汉语案例。我们首先展示了基于翻译方法和多语言大型语言模型（例如GPT-4）的局限性，突出了对语言特定系统的需求。我们进一步提出了一个汉语事实核查系统，通过整合上下文信息，可以更好地从文档中检索证据。为了更好地分析不同系统中的令牌级偏见，我们基于CHEF数据集构建了一个对抗数据集，其中每个实例与原始实例具有较大的词重叠，但具有相反的真实性标签。在CHEF数据集和我们的对抗数据集上的实验结果表明，我们提出的方法优于基于翻译的方法和多语言LLM，并且对偏见更加稳健，但仍有很大的改进空间，强调了语言特定性的重要性。

    arXiv:2401.15498v2 Announce Type: replace  Abstract: This paper investigates the potential benefits of language-specific fact-checking models, focusing on the case of Chinese. We first demonstrate the limitations of translation-based methods and multilingual large language models (e.g., GPT-4), highlighting the need for language-specific systems. We further propose a Chinese fact-checking system that can better retrieve evidence from a document by incorporating context information. To better analyze token-level biases in different systems, we construct an adversarial dataset based on the CHEF dataset, where each instance has large word overlap with the original one but holds the opposite veracity label. Experimental results on the CHEF dataset and our adversarial dataset show that our proposed method outperforms translation-based methods and multilingual LLMs and is more robust toward biases, while there is still large room for improvement, emphasizing the importance of language-specif
    
[^291]: 机器翻译中LLMs的上下文学习的实证研究

    An Empirical Study of In-context Learning in LLMs for Machine Translation

    [https://arxiv.org/abs/2401.12097](https://arxiv.org/abs/2401.12097)

    该研究对机器翻译中LLMs的上下文学习进行了全面深入的研究，探讨了示例驱动的特点以及示例对下游性能的影响，同时也探讨了ICL的局限性。

    

    最近，使用大型语言模型（LLMs）进行机器翻译（MT）的上下文学习（ICL）引起了人们的浓厚兴趣。大多数先前的研究主要集中在优化翻译质量上，对影响所述质量的ICL的特定方面关注有限。为此，我们进行了首次全面研究上下文学习用于机器翻译。我们首先确定ICL主要是由示例驱动而不是指令驱动。随后，我们对示例的各个方面进行了广泛探索，以了解它们对下游性能的影响。我们的分析包括示范的质量和数量、空间接近性以及源语言与目标语言的原创性等因素。此外，我们还研究了涉及间接性和示例不匹配的具有挑战性的场景，以了解ICL的局限性。

    arXiv:2401.12097v2 Announce Type: replace  Abstract: Recent interest has surged in employing Large Language Models (LLMs) for machine translation (MT) via in-context learning (ICL) (Vilar et al., 2023). Most prior studies primarily focus on optimizing translation quality, with limited attention to understanding the specific aspects of ICL that influence the said quality. To this end, we perform the first of its kind, exhaustive study of in-context learning for machine translation. We first establish that ICL is primarily example-driven and not instruction-driven. Following this, we conduct an extensive exploration of various aspects of the examples to understand their influence on downstream performance. Our analysis includes factors such as quality and quantity of demonstrations, spatial proximity, and source versus target originality. Further, we also investigate challenging scenarios involving indirectness and misalignment of examples to understand the limits of ICL. While we establ
    
[^292]: 如何合并生成和检索上下文以增强开放领域问答的语言模型的研究

    Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?

    [https://arxiv.org/abs/2401.11911](https://arxiv.org/abs/2401.11911)

    该论文研究了大型语言模型如何合并生成和检索的上下文以提升开放领域问答，发现这些模型偏向于生成的上下文，即使它们提供了错误的信息。

    

    虽然辅助信息已经成为增强大型语言模型（LLMs）的关键，但对于LLMs如何合并生成的和检索的上下文仍知之甚少。为了研究这一点，我们制定了一个系统性的框架来确定LLMs的响应是源自于生成的上下文还是检索的上下文。为了实现这个目标，我们构建了包含相互冲突的上下文的数据集，其中每个问题都与生成的和检索的上下文配对，但只有一个上下文包含了正确的答案。我们的实验证明，LLMs（如GPT-4/3.5和Llama2）存在显著的偏差，更倾向于生成的上下文，即使这些上下文提供了错误的信息。我们进一步确定了导致这种偏差的两个关键因素：i）LLMs生成的上下文通常与问题更相似，增加了其被选择的可能性；ii）检索上下文中使用的分割过程打断了其连贯性。

    While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To achieve this, we construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs (GPT-4/3.5 and Llama2) towards generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) the segmentation process used in retrieved contexts disrupts their compl
    
[^293]: PsySafe：基于心理学的多智能体系统安全攻击、防御和评估的综合框架

    PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety

    [https://arxiv.org/abs/2401.11880](https://arxiv.org/abs/2401.11880)

    PsySafe提出了一个综合框架，通过深入探讨智能体心理学，揭示智能体的黑暗心理状态对安全构成威胁，并提出了有效的风险缓解策略。

    

    多智能体系统在加入大型语言模型（LLMs）后，展现出了集体智能的深远能力。然而，这种智能被恶意使用可能带来重大风险。迄今为止，关于多智能体系统安全问题的全面研究仍然有限。本文通过创新的视角探索了这些问题，发现智能体的黑暗心理状态构成了对安全的重大威胁。为了解决这些问题，我们提出了一个以智能体心理学为基础的综合框架（PsySafe），关注三个关键领域：首先，识别智能体中的黑暗人格特征如何导致风险行为；其次，从心理和行为角度评估多智能体系统的安全性；第三，制定有效的策略来减轻这些风险。我们的实验揭示

    arXiv:2401.11880v2 Announce Type: replace-cross  Abstract: Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety. To tackle these concerns, we propose a comprehensive framework (PsySafe) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks. Our experiments reveal
    
[^294]: 被理性的流沙所困，远离AGI峰会：通过本体引导干预评估LLMs的数学和编码能力

    Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions

    [https://arxiv.org/abs/2401.09395](https://arxiv.org/abs/2401.09395)

    通过引入数学和编码问题的扰动本体以及两个数据集，作者评估了LLMs在数字推理和编码任务中的能力，在全面评估中发现所有模型在扰动问题上表现显著下降，表明当前的LLMs缺乏稳健性。

    

    最近大型语言模型（LLMs）的先进发展展示了在现有逻辑推理基准测试中取得了引人注目的成果，其中一些模型甚至超过了人类表现。然而，它们在推理任务中的实际能力和稳健性仍然是一个未解之谜。因此，本文关注两个流行的推理任务：算术推理和代码生成。特别是，我们引入了：（i）数学和编码问题的通用扰动本体，（ii）一种半自动方法来应用这些扰动，以及（iii）两个数据集MORE和CORE，分别用于扰动数学和编码问题，以探究LLM在数字推理和编码任务中的能力极限。通过对封闭源和开源LLMs的全面评估，我们展示了所有模型对扰动问题的显著性能下降，表明当前的LLMs缺乏稳健性。

    arXiv:2401.09395v2 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust probl
    
[^295]: DoraemonGPT：朝向理解具有大语言模型的动态场景迈进

    DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models

    [https://arxiv.org/abs/2401.08392](https://arxiv.org/abs/2401.08392)

    DoraemonGPT是一个由LLMs驱动的系统，旨在处理动态视频任务，通过将视频转换为符号记忆来进行空间-时间查询和推理，并取得简洁的中间结果。

    

    最近由LLM驱动的视觉代理主要集中于解决基于图像的任务，这限制了它们理解动态场景的能力，使其远离像引导学生进行实验室实验和识别错误这样的真实应用。考虑到视频模态更好地反映了真实世界场景的不断变化性质，我们设计了DoraemonGPT，这是一个由LLM驱动的综合概念简洁系统，用于处理动态视频任务。给定一个带有问题/任务的视频，DoraemonGPT首先将输入视频转换为存储与任务相关属性的符号存储器。这种结构化表示允许通过精心设计的子任务工具进行空间-时间查询和推理，从而产生简洁的中间结果。鉴于LLM在涉及专业领域（例如分析实验中潜在的科学原理）时具有有限的内部知识，我们引入了

    arXiv:2401.08392v2 Announce Type: replace-cross  Abstract: Recent LLM-driven visual agents mainly focus on solving image-based tasks, which limits their ability to understand dynamic scenes, making it far from real-life applications like guiding students in laboratory experiments and identifying their mistakes. Considering the video modality better reflects the ever-changing nature of real-world scenarios, we devise DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks. Given a video with a question/task, DoraemonGPT begins by converting the input video into a symbolic memory that stores task-related attributes. This structured representation allows for spatial-temporal querying and reasoning by well-designed sub-task tools, resulting in concise intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorpor
    
[^296]: PRewrite: 使用强化学习的提示重写

    PRewrite: Prompt Rewriting with Reinforcement Learning

    [https://arxiv.org/abs/2401.08189](https://arxiv.org/abs/2401.08189)

    本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。

    

    arXiv:2401.08189v2 公告类型: 替换 摘要: 提示工程对于基于LLM的应用程序的开发至关重要。然而，通常以“试错”的方式手动完成。这种手动程序可能耗时，效果不佳，并且在许多情况下生成的提示都是次优的。即使对那些看似运作良好的提示，始终存在一个悬而未决的问题：是否可以通过进一步修改使提示变得更好呢？为了解决这些问题，在本文中，我们研究了提示工程自动化。我们考虑了一个特定的使用情景，即开发者/用户已经起草了初始提示，但缺乏时间/专业知识来优化它们。我们提出了PRewrite，一个自动化工具，可重写这些草案，并生成高效的新提示。PRewrite基于强化学习（RL）框架，允许端到端优化，我们的设计允许RL搜索在大动作空间中进行。

    arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
    
[^297]: 为了更好的多语言推理能力而进行的问题翻译训练

    Question Translation Training for Better Multilingual Reasoning

    [https://arxiv.org/abs/2401.07817](https://arxiv.org/abs/2401.07817)

    本文探讨了通过问题对齐训练模型将推理问题翻译成英语的方法，以实现有针对性的、领域内的语言对齐，最大限度地利用英语指导数据，释放了LLMs的多语言推理能力。

    

    大型语言模型在推理任务上表现出色，但在非英语语言上的表现往往较差。传统解决方案是将指导数据翻译成所有感兴趣的语言，然后在生成的多语言数据上进行训练，这被称为翻译训练。本文探讨了问题对齐的好处，通过在X-英语平行问题数据上微调，训练模型将推理问题翻译成英语。这种方法通过有针对性的、领域内的语言对齐，最大限度地利用英语指导数据，释放了LLMs的多语言推理能力。实验结果表明在LLaMA2-13上

    arXiv:2401.07817v2 Announce Type: replace  Abstract: Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of mathematical chain-of-thought. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English parallel question data. In this way we perform targeted, in-domain language alignment which makes best use of English instruction data to unlock the LLMs' multilingual reasoning abilities. Experimental results on LLaMA2-13
    
[^298]: 超越稀疏奖励：在文本生成中通过语言模型评论增强强化学习

    Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation

    [https://arxiv.org/abs/2401.07382](https://arxiv.org/abs/2401.07382)

    本文提出了一种新的框架，利用大型语言模型的评论能力在强化学习训练中产生中间步骤奖励，以应对稀疏奖励信号所带来的挑战。

    

    强化学习可以将语言模型与非可微分奖励信号（如人类偏好）进行对齐。然而，其中一个主要挑战在于这些奖励信号的稀疏性 - 通常，整个输出只有一个奖励。这种奖励的稀疏性可能导致学习效率低下且不稳定。为了解决这一挑战，本文介绍了一种新颖的框架，利用大型语言模型（LLMs）的评论能力在强化学习训练过程中产生中间步骤奖励。我们的方法涉及将策略模型与评论语言模型相结合，评论语言模型负责为输出的每个部分提供细致的反馈。然后将这些反馈转化为可用于指导强化学习训练过程的标记或跨度级别奖励。我们在两种不同的设置下研究了这种方法：一种是策略模型较小且与更强大的评论者配对。

    arXiv:2401.07382v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful crit
    
[^299]: 图语言模型

    Graph Language Models

    [https://arxiv.org/abs/2401.07105](https://arxiv.org/abs/2401.07105)

    引入了一种新型的图语言模型（GLM），结合线性化和图神经网络的优点，解决了传统方法在处理结构化知识图谱时的弱点。

    

    虽然语言模型（LMs）是自然语言处理的主力军，它们与结构化知识图谱（KGs）的相互作用仍在积极研究中。当前用于编码这些图形的方法通常要么（i）将它们线性化以供LM嵌入--这样会低效利用结构信息，要么（ii）使用图神经网络（GNNs）来保留图结构--但GNNs无法像预训练的LM一样很好地表示文本特征。在我们的工作中，我们引入了一种新型LM类型，即图语言模型（GLM），它整合了两种方法的优点并减轻了它们的弱点。GLM参数从预训练的LM中初始化，以增强对个别图概念和三元组的理解。同时，我们设计GLM的架构以整合图偏差，从而促进图内的知识分布。这使GLM能够处理图形、文本以及两者的交织输入。实证

    arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical 
    
[^300]: 打开LLMs的潘多拉魔盒：通过表示工程对LLMs进行越狱

    Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering

    [https://arxiv.org/abs/2401.06824](https://arxiv.org/abs/2401.06824)

    通过表示工程对LLMs进行越狱是一种新颖的方法，它利用少量查询对提取“安全模式”，成功规避目标模型的防御，实现了前所未有的越狱性能。

    

    越狱技术旨在通过诱使大型语言模型（LLMs）生成对恶意查询产生有毒响应，来探索LLMs安全性边界，这在LLMs社区内是一个重要关注点。我们提出一种名为通过表示工程对LLMs进行越狱（Jailbreaking LLMs through Representation Engineering，JRE）的新颖越狱方法，其仅需要少量查询对以提取可用于规避目标模型防御的“安全模式”，实现了前所未有的越狱性能。

    arXiv:2401.06824v2 Announce Type: replace-cross  Abstract: Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community. While existing jailbreaking methods primarily rely on prompt engineering, altering inputs to evade LLM safety mechanisms, they suffer from low attack success rates and significant time overheads, rendering them inflexible. To overcome these limitations, we propose a novel jailbreaking approach, named Jailbreaking LLMs through Representation Engineering (JRE). Our method requires only a small number of query pairs to extract ``safety patterns'' that can be used to circumvent the target model's defenses, achieving unprecedented jailbreaking performance. Building upon these findings, we also introduce a novel defense framework inspired by JRE principles, which demonstrates notable effectiveness. Extensive experimentation conf
    
[^301]: 通过基于提示生成的方法在文学作品中进行说话者识别

    SIG: Speaker Identification in Literature via Prompt-Based Generation

    [https://arxiv.org/abs/2312.14590](https://arxiv.org/abs/2312.14590)

    通过基于生成的方法SIG，在文学作品中实现了说话者识别任务，支持跨领域评估和开放世界分类范式。

    

    在文学分析中，识别叙述中引用的发言者是一项重要任务，挑战性情景包括对看不见发言者的跨领域推断，以及周围环境中没有提到发言者的非明确情况。本文提出了一种简单且有效的方法SIG，这是一种基于生成的方法，根据设计的提示模板对任务和引语输入进行语言化处理，还可以轻松集成其他进一步增强说话者识别性能的辅助任务。预测可以来自模型的直接生成，也可以由每个发言者候选人的最高生成概率确定。基于我们的方法设计，SIG支持跨领域评估，并实现了能够接受任何候选输入形式的开放世界分类范式。我们在PDNC上进行了跨领域评估和内领域评估。

    arXiv:2312.14590v2 Announce Type: replace  Abstract: Identifying speakers of quotations in narratives is an important task in literary analysis, with challenging scenarios including the out-of-domain inference for unseen speakers, and non-explicit cases where there are no speaker mentions in surrounding context. In this work, we propose a simple and effective approach SIG, a generation-based method that verbalizes the task and quotation input based on designed prompt templates, which also enables easy integration of other auxiliary tasks that further bolster the speaker identification performance. The prediction can either come from direct generation by the model, or be determined by the highest generation probability of each speaker candidate. Based on our approach design, SIG supports out-of-domain evaluation, and achieves open-world classification paradigm that is able to accept any forms of candidate input. We perform both cross-domain evaluation and in-domain evaluation on PDNC, t
    
[^302]: 解读大语言模型微调中的指令混合

    Demystifying Instruction Mixing for Fine-tuning Large Language Models

    [https://arxiv.org/abs/2312.10793](https://arxiv.org/abs/2312.10793)

    指令微调提升了大语言模型在各种任务中的性能，研究发现不同指令类型对特定应用更有利，但可能对其他领域产生负面影响。

    

    指令微调显著提高了大型语言模型（LLM）在各种任务上的性能。然而，对于优化LLM微调的指令数据集混合的过程仍然知之甚少。本研究将指令分为三类主要类型：自然语言处理下游任务、编程和一般对话。我们探讨了指令微调对LLM性能的不同数据集组合的影响，并发现某些指令类型对特定应用更有利，但可能对其他领域产生负面影响。这项工作为指令混合提供了见解，为未来研究奠定了基础。

    arXiv:2312.10793v3 Announce Type: replace-cross  Abstract: Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures, laying the foundations for future research.
    
[^303]: TAP4LLM：用于大型语言模型推理的表格提供者在对半结构化数据进行采样、增补和打包

    TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning

    [https://arxiv.org/abs/2312.09039](https://arxiv.org/abs/2312.09039)

    TAP4LLM提出了一个用于生成表格提示的多功能预处理工具箱，通过采样、增补和打包半结构化数据，解决了在大型语言模型推理中处理复杂问题和大型表格的挑战。

    

    基于表格的推理在结合深度模型和离散推理方面取得了显著进展，这需要对自由形式的自然语言（NL）问题和半结构化表格数据进行推理。然而，先前的表格推理解决方案只考虑小型表格，并且在处理更大表格时存在局限性。此外，大多数现有方法难以推理复杂问题，因为它们缺乏基本信息或分散在不同位置。为了解决这些挑战，我们提出了TAP4LLM作为一个多功能的预处理工具箱，通过平衡标记分配权衡来生成表格提示，实现(1) 表格采样，(2) 表格增补和(3) 表格打包。在每个模块中，我们收集和设计了几种在不同情况下使用的常见方法（例如，速度与准确性的平衡）。我们还对T内部每个组件的性能进行了全面评估。

    arXiv:2312.09039v2 Announce Type: replace-cross  Abstract: Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and semi-structured tabular data. However, previous table reasoning solutions only consider small-sized tables and exhibit limitations in handling larger tables. In addition, most existing methods struggle to reason over complex questions since they lack essential information or they are scattered in different places. To alleviate these challenges, we propose TAP4LLM as a versatile pre-processing toolbox to generate table prompts through (1) table sampling, (2) table augmentation, and (3) table packing while balancing the token allocation trade-off. In each module, we collect and design several common methods for usage in various scenarios (e.g., speed over accuracy). We also provide a comprehensive evaluation on performance of each components inside T
    
[^304]: Math-Shepherd: 在不需要人工标注的情况下逐步验证和加强LLMs

    Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations

    [https://arxiv.org/abs/2312.08935](https://arxiv.org/abs/2312.08935)

    Math-Shepherd提出了一种新的数学奖励模型，通过自动生成的过程监督数据实现LLMs的逐步验证和加强，显著提高了数学问题解决的准确性。

    

    在这篇论文中，我们提出了一种名为Math-Shepherd的创新过程导向数学奖励模型，为数学问题解决的每一步分配奖励分数。Math-Shepherd的训练是使用自动构建的基于过程的监督数据完成的，打破了现有工作中对手动标注的严重依赖瓶颈。我们探讨了Math-Shepherd在两种场景中的有效性：1）\textit{验证}：利用Math-Shepherd对大型语言模型(LLMs)生成的多个输出进行重新排序；2）\textit{强化学习}：使用Math-Shepherd通过逐步的近端策略优化(PPO)加强LLMs。通过Math-Shepherd，一系列开源LLMs展现出卓越的性能。例如，使用Math-Shepherd的逐步PPO显著提高了Mistral-7B的准确率(GSM8K由77.9%提高到84.1%，MATH由28.6%提高到33.0%)

    arXiv:2312.08935v3 Announce Type: replace  Abstract: In this paper, we present an innovative process-oriented math process reward model called \textbf{Math-Shepherd}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K and 28.6\%$\to$33.0\% on MATH). The
    
[^305]: KnowGPT：大型语言模型的黑盒知识注入

    KnowGPT: Black-Box Knowledge Injection for Large Language Models

    [https://arxiv.org/abs/2312.06185](https://arxiv.org/abs/2312.06185)

    KnowGPT是一种为大型语言模型提供黑盒知识注入的框架，通过深度强化学习和多臂老虎机构建最适合每个问题的提示，在三个基准数据集上实验证明其显著提升了知识注入的效果。

    

    生成式大型语言模型（LLMs），如ChatGPT，提供互动式API，可以以人类专家水平回答常见问题。然而，当面临需要特定领域或专业领域知识的问题时，这些模型通常会给出不准确或不正确的响应，这些知识并未包含在它们的训练语料库中。此外，许多最先进的LLMs并非开源，这使得仅使用模型API注入知识具有挑战性。在本研究中，我们介绍了KnowGPT，一种用于LLMs在问答中的黑盒知识注入框架。KnowGPT利用深度强化学习（RL）从知识图中提取相关知识，并使用多臂老虎机（MAB）为每个问题构建最合适的提示。我们在三个基准数据集上进行了大量实验，展示了KnowGPT显著增强了现有方法。值得注意的是，KnowGPT平均改进了23%。

    arXiv:2312.06185v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.
    
[^306]: 解锁预测性文本生成：对大型语言模型解码的受限方法

    Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding

    [https://arxiv.org/abs/2312.06149](https://arxiv.org/abs/2312.06149)

    提出了将文本生成形式化为未来受限生成问题的方法，以最小化不良行为并强制执行对指令的忠实性，并通过LLMs有效指导文本生成。

    

    大型语言模型(LLMs)展现了强大的文本生成能力。然而，对于给定提示或指令实现最佳结果可能具有挑战性，特别是对于十亿级别的模型。此外，不良行为如毒性或幻觉可能会显现。在这项工作中，我们提出将文本生成形式化为未来受限生成问题，以最小化不良行为并强制执行对指令的忠实性。使用LLMs实现未来约束满足度的估计引导文本生成过程。我们的广泛实验表明所提出的方法在三个不同的文本生成任务中的有效性：关键词受限生成、毒性减少等。

    arXiv:2312.06149v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 202
    
[^307]: 应用大型语言模型和思维链进行自动评分

    Applying Large Language Models and Chain-of-Thought for Automatic Scoring

    [https://arxiv.org/abs/2312.03748](https://arxiv.org/abs/2312.03748)

    本研究探讨了在自动评分学生对科学评估写作反馈中应用大型语言模型（LLMs）和思维链（CoT），通过零次或少次学习等策略自动评分，其中少次学习表现更佳。

    

    本研究调查了大型语言模型（LLMs），特别是 GPT-3.5 和 GPT-4，以及思维链（CoT）在自动评分学生对科学评估的写作反馈中的应用。我们专注于克服先前限制研究人员和教育工作者使用基于人工智能的自动评分工具的无法访问性、技术复杂性和缺乏解释性的挑战。通过一个包括 1,650 个学生反馈的六项评估任务（三个二项和三个三项）的测试数据集，我们采用六种提示工程策略来自动评分学生的反馈。这六种策略将零次学习或少次学习与 CoT 结合在一起，要么独自使用，要么与项目干和评分细则一起使用。结果表明，少次学习（准确率=.67）优于零次学习（准确率=.60），提高12.6%。

    arXiv:2312.03748v2 Announce Type: replace-cross  Abstract: This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1,650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot learning (acc = .60), with 12.6% increase. CoT, when used without item stem and scoring rubric
    
[^308]: UHGEval：通过无约束生成评估中文大型语言模型的虚构能力

    UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation

    [https://arxiv.org/abs/2311.15296](https://arxiv.org/abs/2311.15296)

    评估了中文大型语言模型在文本生成中的虚构能力，并指出现有基准评估通常采用受限制的生成技术，无法满足真实需求中的无约束文本生成。

    

    大型语言模型（LLMs）已成为当代自然语言处理领域的重要贡献者，并越来越广泛地应用于各种行业。然而，这些大规模概率统计模型目前无法保证在专业内容生成中的必要质量。这些模型经常产生虚构的文本，影响它们在专业环境中的实用性。为了评估LLMs在文本生成中的真实可靠性，许多倡议已开发了用于虚构现象的基准评估。然而，这些基准评估通常利用受限制的生成技术，因为成本和时间限制。这些技术包括使用定向幻觉诱导和故意改变真实文本以产生幻觉的策略。这些方法与实际需求的无约束文本生成不一致。

    arXiv:2311.15296v2 Announce Type: replace  Abstract: Large language models (LLMs) have emerged as pivotal contributors in contemporary natural language processing and are increasingly being applied across a diverse range of industries. However, these large-scale probabilistic statistical models cannot currently ensure the requisite quality in professional content generation. These models often produce hallucinated text, compromising their practical utility in professional contexts. To assess the authentic reliability of LLMs in text generation, numerous initiatives have developed benchmark evaluations for hallucination phenomena. Nevertheless, these benchmarks frequently utilize constrained generation techniques due to cost and temporal constraints. These techniques encompass the use of directed hallucination induction and strategies that deliberately alter authentic text to produce hallucinations. These approaches are not congruent with the unrestricted text generation demanded by rea
    
[^309]: 代码搜索去偏见:改善搜索结果超越总体排名表现

    Code Search Debiasing:Improve Search Results beyond Overall Ranking Performance

    [https://arxiv.org/abs/2311.14901](https://arxiv.org/abs/2311.14901)

    该论文提出了一个通用的去偏见框架，通过重新排序来校准搜索结果，有效缓解了代码搜索引擎的偏见，从而提高了代码搜索的总体排名表现。

    

    arXiv:2311.14901v2 公告类型:替换 摘要: 代码搜索引擎是软件开发中的重要工具。许多代码搜索方法涌现出来，专注于代码搜索的总体排名性能。本文从另一个角度研究代码搜索，分析了代码搜索模型的偏见。有偏见的代码搜索引擎会提供较差的用户体验，即使它们表现出色总体表现。由于不同的开发惯例（例如，更喜欢长查询还是缩写），有些程序员会发现引擎有用，而其他人可能很难获得理想的搜索结果。为了减轻偏见，我们开发了一个通用的去偏见框架，利用重新排序来校准搜索结果。它可以轻松插入现有引擎，并处理将来发现的新代码搜索偏见。实验证明我们的框架能够有效减少偏见。与此同时，在去偏见后，代码搜索的总体排名表现也得到了改善。

    arXiv:2311.14901v2 Announce Type: replace  Abstract: Code search engine is an essential tool in software development. Many code search methods have sprung up, focusing on the overall ranking performance of code search. In this paper, we study code search from another perspective by analyzing the bias of code search models. Biased code search engines provide poor user experience, even though they show promising overall performance. Due to different development conventions (e.g., prefer long queries or abbreviations), some programmers will find the engine useful, while others may find it hard to get desirable search results. To mitigate biases, we develop a general debiasing framework that employs reranking to calibrate search results. It can be easily plugged into existing engines and handle new code search biases discovered in the future. Experiments show that our framework can effectively reduce biases. Meanwhile, the overall ranking performance of code search gets improved after debi
    
[^310]: LLMs作为视觉解释器：通过不断演进的视觉描述提升图像分类

    LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions

    [https://arxiv.org/abs/2311.11904](https://arxiv.org/abs/2311.11904)

    提出了一种集成LLMs和VLMs的框架，以找到最佳的类别描述符，解决了图像分类中在精确构建文本表示和区分相似类别方面的挑战

    

    视觉语言模型（VLMs）通过比较图像与类别嵌入之间的相似性，为图像分类提供了一种有前途的范式。一个关键的挑战在于为类别名称构建精确的文本表示。本文提出了一种集成LLMs和VLMs的新框架，以找到最佳的类别描述符。

    arXiv:2311.11904v2 Announce Type: replace-cross  Abstract: Vision-language models (VLMs) offer a promising paradigm for image classification by comparing the similarity between images and class embeddings. A critical challenge lies in crafting precise textual representations for class names. While previous studies have leveraged recent advancements in large language models (LLMs) to enhance these descriptors, their outputs often suffer from ambiguity and inaccuracy. We attribute this to two primary factors: 1) the reliance on single-turn textual interactions with LLMs, leading to a mismatch between generated text and visual concepts for VLMs; 2) the oversight of the inter-class relationships, resulting in descriptors that fail to differentiate similar classes effectively. In this paper, we propose a novel framework that integrates LLMs and VLMs to find the optimal class descriptors. Our training-free approach develops an LLM-based agent with an evolutionary optimization strategy to ite
    
[^311]: 基于困惑度量和上下文信息的令牌级对抗提示检测

    Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information

    [https://arxiv.org/abs/2311.11509](https://arxiv.org/abs/2311.11509)

    该论文引入了一种新的方法，在令牌级别检测对抗性提示，利用语言模型的能力预测下一个标记的概率。

    

    近年来，大型语言模型(LLM)已成为各种应用中的关键工具。然而，这些模型容易受到对抗性提示攻击，攻击者可以精心策划输入字符串，误导LLM生成不正确或不希望的输出。先前的研究揭示了利用离散优化的相对简单却有效的攻击方式可以生成绕过模型的调整和对齐的对抗性提示。对对抗性提示的脆弱性凸显了对LLM健壮性和可靠性的重要关注。我们的工作旨在通过引入一种新颖方法，在令牌级别检测对抗性提示，利用LLM预测下一个标记的概率能力。我们测量模型困惑度的程度，其中高概率预测的令牌被视为正常，而那些表现异常的则可能是对抗性提示。

    arXiv:2311.11509v3 Announce Type: replace  Abstract: In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that mislead LLMs into generating incorrect or undesired outputs. Previous work has revealed that with relatively simple yet effective attacks based on discrete optimization, it is possible to generate adversarial prompts that bypass moderation and alignment of the models. This vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs. Our work aims to address this concern by introducing a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity, where tokens predicted with high probability are considered normal, and those exhibi
    
[^312]: MedAgents: 大型语言模型作为零-shot医学推理的合作者

    MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning

    [https://arxiv.org/abs/2311.10537](https://arxiv.org/abs/2311.10537)

    提出了一个新颖的医学领域的跨学科合作(MC)框架，利用基于LLM的代理在角色扮演设置中参与协作多轮讨论，从而提高LLM的熟练程度和推理能力

    

    大型语言模型(LLMs)尽管在各种通用领域取得了显著进展，但在医学和医疗保健领域面临重大障碍。为了解决这些问题，我们提出了一个新颖的医学领域的跨学科合作(MC)框架，利用基于LLM的代理在角色扮演设置中参与协作多轮讨论，从而提高LLM的熟练程度和推理能力。这种无需训练的框架包括五个关键步骤：收集领域专家、提出个别分析、将这些分析总结成报告、在讨论中反复迭代直到达成共识，最终做出决策。我们的工作侧重于零-shot情景，在实际场景中具有适用性。在九个数据集上的实验结果显示...

    arXiv:2311.10537v2 Announce Type: replace-cross  Abstract: Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine dataset
    
[^313]: 我们能从大型语言模型中提取多元化观点到何种程度？

    How Far Can We Extract Diverse Perspectives from Large Language Models?

    [https://arxiv.org/abs/2311.09799](https://arxiv.org/abs/2311.09799)

    本研究探讨了LLMs在生成多元化观点和理由方面的能力，并提出了从LLMs中最大程度提取多样性观点的新问题。

    

    收集多样化的人类观点成本高且具有挑战性。最近的合作努力趋势表明，人类和大型语言模型（LLMs）之间进行合作为生成多样化数据提供了潜在可扩展和高效的解决方案。然而，关于LLMs在主观话题上生成多元化观点的能力程度仍是一个未被探讨的问题。本研究探讨了LLMs在生成多元化观点和理由（例如社会规范和辩论文本）方面的能力。我们提出了从LLMs中提取最大多样性信息的新问题。受人类通过其价值观发展观点的启发，我们提出了一个基于标准的提示技术来确立多样化观点。为了了解我们能从LLMs中提取多元化观点到何种程度，或者称之为多样性覆盖率，我们采用了逐步回忆提示的方法，以在迭代方式下从模型中生成更多输出。

    arXiv:2311.09799v2 Announce Type: replace  Abstract: Collecting diverse human opinions is costly and challenging. This leads to a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate a new problem of maximum diversity extraction from LLMs. Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative mann
    
[^314]: LLMs作为自恋评估者：当自我膨胀影响评估分数

    LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores

    [https://arxiv.org/abs/2311.09766](https://arxiv.org/abs/2311.09766)

    本文研究了语言模型驱动的评估指标在总结任务中是否会对相同的基础语言模型生成的文本表现出偏见，并发现在无参考情况下使用时偏见尤为显著。

    

    arXiv:2311.09766v2 公告类型：替换 摘要：生成文本内容的自动评估在自然语言处理领域中一直是一个持续挑战。鉴于现代语言模型（LMs）在各种NLP任务中的出色表现，越来越多的人倾向于利用这些模型创造创新的评估指标，用于自动生成任务的自动评估。本文探讨了一个重要问题：由于语言模型驱动的评估指标是否会固有地表现出偏向于由相同基础语言模型生成的文本的偏见？具体而言，我们评估了知名的基于LM的评估指标（例如BARTScore、T5Score和GPTScore）在总结任务中是否对其各自的基础LM表现出偏好。我们的发现揭示了潜在偏见，特别是当这些评估指标在无参考的情况下使用且不利用黄金摘要时，这种偏见尤为显著。这些结果突显了通过生成文本获得的评估结果可能会受到自我偏误的影响。

    arXiv:2311.09766v2 Announce Type: replace  Abstract: Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in an reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generat
    
[^315]: 自相矛盾推理评估与检测

    Self-Contradictory Reasoning Evaluation and Detection

    [https://arxiv.org/abs/2311.09603](https://arxiv.org/abs/2311.09603)

    研究了大型语言模型在推理任务中自相矛盾的现象，发现在涉及上下文信息理解或常识的任务中经常存在自相矛盾，而高准确性并不总是对应较低的自相矛盾率。

    

    在最近的大量工作中，大型语言模型展示了令人印象深刻的推理能力，但许多提出的下游推理任务主要关注性能评估。然而，仍然存在两个基本问题：1）推理质量有多可靠，2）模型能否检测到不可靠的推理？本文研究了自相矛盾（Self-Contra）推理，即模型推理不支持预测的情况。为了解决第一个问题，我们评估了四个数据集中的Self-Contra率，并深入探讨了自相矛盾推理的更细粒度类别。我们发现，大型语言模型在进行涉及上下文信息理解或常识的推理任务时经常自相矛盾。重要的是，更高的准确性并不一定对应更低的自相矛盾率。模型可能会产生正确答案，但在推理过程中可能会采取捷径或忽略上下文证据。

    arXiv:2311.09603v2 Announce Type: replace  Abstract: In a plethora of recent work, large language models (LLMs) demonstrated impressive reasoning ability, but many proposed downstream reasoning tasks focus on performance-wise evaluation. Two fundamental questions persist: 1) how reliable is the quality of reasoning, and 2) can models detect unreliable reasoning? In this paper, we investigate self-contradictory (Self-Contra) reasoning, where the model reasoning does not support predictions. To address 1), we assess the Self-Contra rate across four datasets and delve into finer-grained categories of Self-Contra reasoning. We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information understanding or commonsense. Importantly, a higher accuracy does not necessarily correspond to a lower Self-Contra rate. The model may appear to generate correct answers but it may take shortcuts in reasoning or skip over contextual evidence, thereby displa
    
[^316]: DocLens: 多方面细粒度评估医学文本生成

    DocLens: Multi-aspect Fine-grained Evaluation for Medical Text Generation

    [https://arxiv.org/abs/2311.09581](https://arxiv.org/abs/2311.09581)

    本文提出了一个名为DocLens的框架，通过一组新的度量标准，在多个任务中展示其对医学文本生成的有效性，并且通过人类研究表明其在与医学专家判断的一致性上优于现有指标，同时指出了改进开源评估者的必要性。

    

    医学文本生成旨在协助行政工作并突出要点信息以支持决策制定。为了反映医学文本的特定要求，本文提出了一组度量标准，用于在细粒度水平上评估生成文本的完整性、简明性和归因性。这些度量标准可以由各种类型的评估者计算，包括遵循说明（专有和开源）和监督蕴涵模型。我们通过三个评估者在三个任务上展示了所得出的框架DocLens的有效性：临床记录生成、放射学报告摘要和患者问题摘要。一项全面的人类研究显示，DocLens与医学专家的判断之间存在相当高的一致性，高于现有指标。结果还突出显示了改进开源评估者的必要性，并提出了潜在的改进方向。

    arXiv:2311.09581v2 Announce Type: replace  Abstract: Medical text generation aims to assist with administrative work and highlight salient information to support decision-making. To reflect the specific requirements of medical text, in this paper, we propose a set of metrics to evaluate the completeness, conciseness, and attribution of the generated text at a fine-grained level. The metrics can be computed by various types of evaluators including instruction-following (both proprietary and open-source) and supervised entailment models. We demonstrate the effectiveness of the resulting framework, DocLens, with three evaluators on three tasks: clinical note generation, radiology report summarization, and patient question summarization. A comprehensive human study shows that DocLens exhibits substantially higher agreement with the judgments of medical experts than existing metrics. The results also highlight the need to improve open-source evaluators and suggest potential directions.
    
[^317]: TextEE：事件提取中的基准、重新评估、反思和未来挑战

    TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction

    [https://arxiv.org/abs/2311.09562](https://arxiv.org/abs/2311.09562)

    本研究提出了TextEE，这是一个用于事件提取的标准化、公平和可重复的基准，解决了评估中存在的挑战。

    

    事件提取由于其广泛的应用而引起了广泛关注。然而，最近的研究引起了对评估问题的关注，表明报告的分数可能无法准确反映真实性能。在这项工作中，我们确定并解决了评估挑战，包括由于不同的数据假设或预处理步骤而引起的不一致性，目前评估框架的不足可能引入数据集或数据分割偏见，以及一些先前方法的低可重复性。为了解决这些挑战，我们提出了TextEE，这是一个用于事件提取的标准化、公平和可重复的基准。TextEE包括标准化的数据预处理脚本和用于跨七个不同领域的14个数据集的数据分割，并包括14种最近的方法，进行全面的基准重新评估。我们还对我们的TextEE基准上的五种不同的大型语言模型进行了评估，并演示了如何

    arXiv:2311.09562v2 Announce Type: replace  Abstract: Event extraction has gained considerable interest due to its wide-ranging applications. However, recent studies draw attention to evaluation issues, suggesting that reported scores may not accurately reflect the true performance. In this work, we identify and address evaluation challenges, including inconsistency due to varying data assumptions or preprocessing steps, the insufficiency of current evaluation frameworks that may introduce dataset or data split bias, and the low reproducibility of some previous approaches. To address these challenges, we present TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE comprises standardized data preprocessing scripts and splits for 14 datasets spanning seven diverse domains and includes 14 recent methodologies, conducting a comprehensive benchmark reevaluation. We also evaluate five varied large language models on our TextEE benchmark and demonstrate how the
    
[^318]: Symbol-LLM: 面向大型语言模型基础符号中心接口

    Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models

    [https://arxiv.org/abs/2311.09278](https://arxiv.org/abs/2311.09278)

    Symbol-LLM 提出了一种通过数据和框架来解决大型语言模型中符号数据注入的挑战，旨在捕捉符号间的相互关系和促进协同作用。

    

    虽然大型语言模型(LLMs)展现出在处理和生成类似于人类文本方面的显著能力，但在理解和表达超出自然语言范围的世界知识方面存在局限性(例如化学分子式)。直接将一系列符号数据注入到LLMs的训练中可能存在问题，因为它忽视了不同符号家族之间的协同关系，也忽视了自然数据和符号数据之间平衡混合的必要性。在这项工作中，我们从数据和框架两个方面应对这些挑战，并引入了Symbol-LLM系列模型。首先，我们策划了一个包含34个任务并涵盖约20个不同符号家族的数据集，旨在捕捉符号之间的相互关系并促进符号之间的协同作用。然后，一个两阶段调优框架成功地注入了符号知识而不会损失

    arXiv:2311.09278v2 Announce Type: replace-cross  Abstract: Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language(e.g., chemical molecular formula). Injecting a collection of symbolic data directly into the training of LLMs can be problematic, as it disregards the synergies among different symbolic families and overlooks the need for a balanced mixture of natural and symbolic data. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we curated a data collection consisting of 34 tasks and incorporating approximately 20 distinct symbolic families, intending to capture the interrelations and foster synergies between symbols. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss 
    
[^319]: 用部分排序对LLM响应进行排名以改善响应生成

    Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation

    [https://arxiv.org/abs/2311.09136](https://arxiv.org/abs/2311.09136)

    提出一种使用部分排序来优化LLMs的方法，能够通过训练模型优先考虑特定任务候选响应池中的最佳响应，从而改善响应生成能力。

    

    定制LLMs以适应特定任务涉及将有效响应与错误响应区分开。这种技能可以通过使用大量人类偏好数据进行监督微调来发展。然而，对于大多数任务来说，获取专家注释的偏好数据是昂贵的。在本文中，我们提出了一种使用排名度量来优化LLMs的新方法。该方法训练模型优先考虑为特定任务创建的候选响应池中的最佳响应。我们主张采用部分排序而不是传统的完全排序，因为就候选响应的完美顺序达成共识可能具有挑战性。我们的部分排序更加稳健，对噪声的敏感性较低，并且可以通过有限的人类注释或启发式方法来实现。我们使用基准数据集测试了我们系统的改进响应生成能力，包括最新的多文档问答任务。

    arXiv:2311.09136v2 Announce Type: replace  Abstract: Customizing LLMs for a specific task involves distinguishing effective responses from erroneous ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining expert-annotated preference data is expensive for most tasks. In this paper, we present a novel method to optimize LLMs using ranking metrics. This method trains the model to prioritize the best responses from a pool of candidates created for a particular task. Rather than a traditional full ordering, we advocate for a partial ordering, as achieving consensus on the perfect order of candidate responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be achieved with limited human annotations or through heuristic methods. We test our system's improved response generation ability using benchmark datasets, including the latest multi-document question answering task. We conduct ablati
    
[^320]: 社会偏见探测：语言模型的公平基准评估

    Social Bias Probing: Fairness Benchmarking for Language Models

    [https://arxiv.org/abs/2311.09090](https://arxiv.org/abs/2311.09090)

    本文提出了一个用于探测语言模型中社会偏见的原创框架，包括对一般关联和社会类别、身份以及刻板印象的分析。

    

    大型语言模型已被证明编码了各种社会偏见，这带来了下游风险。本文提出了一个用于探测语言模型中社会偏见的原创框架，包括对语言模型的一般关联以及社会类别、身份和刻板印象的分析。

    arXiv:2311.09090v2 Announce Type: replace  Abstract: Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms. While the impact of these biases has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, offering a constrained view of the nature of societal biases within language models. In this paper, we propose an original framework for probing language models for societal biases. We collect a probing dataset to analyze language models' general associations, as well as along the axes of societal categories, identities, and stereotypes. To this end, we leverage a novel perplexity-based fairness score. We curate a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections, expanding to a variety of different identities and stereotypes. When comparing our methodology with prior work, we demonstrate that biases within langua
    
[^321]: Safer-Instruct: 使用自动化偏好数据对齐语言模型

    Safer-Instruct: Aligning Language Models with Automated Preference Data

    [https://arxiv.org/abs/2311.08685](https://arxiv.org/abs/2311.08685)

    Safer-Instruct通过反向指导调整、指导感应和专家模型评估的方式，实现了自动构建大规模偏好数据的目的，从而在没有人工标注者的情况下高效生成高质量的偏好数据。

    

    人工反馈强化学习（RLHF）是增强语言模型能力的重要策略。然而，为RLHF标注偏好数据是一项资源密集且需要创造力的过程，而现有的自动生成方法在数据多样性和质量方面存在局限性。为了应对这一挑战，我们提出了Safer-Instruct，这是一个用于自动构建大规模偏好数据的全新流水线。我们的方法利用了反向指导调整、指导感应和专家模型评估，以高效生成高质量的偏好数据，无需人工标注者。为了验证Safer-Instruct的有效性，我们将该流水线应用于构建一个安全偏好数据集作为案例研究。在这个合成数据集上微调Alpaca模型不仅展示出更好的无害性，还表现出优于在人工标注的安全偏好数据上微调的模型，同时保持

    arXiv:2311.08685v2 Announce Type: replace-cross  Abstract: Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while mainta
    
[^322]: DALA: 一种基于分布感知的面向语言模型的对抗攻击方法

    DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Language Models

    [https://arxiv.org/abs/2311.08598](https://arxiv.org/abs/2311.08598)

    DALA是一种基于分布感知的LoRA对抗攻击方法，旨在改善对抗性样本的数据分布，提高攻击效果，并引入了非可检测攻击成功率（NASR）评价指标。

    

    语言模型（LMs）可以通过对抗性攻击进行操纵，这些攻击在输入数据中引入微妙的扰动。近期的攻击方法可以实现相对较高的攻击成功率（ASR），但我们观察到生成的对抗性样本与原始样本相比具有不同的数据分布。具体而言，这些对抗性样本表现出降低的置信水平和与训练数据分布的较大差异。因此，它们很容易被简单的检测方法检测出来，降低了此类攻击的有效性。为解决这一问题，我们提出了一种基于LoRA的分布感知的对抗攻击方法（DALA）。DALA考虑对抗性样本的分布变化，以提高在检测方法下的攻击效果。我们进一步设计了一种新颖的评价度量，非可检测攻击成功率（NASR），它融合了ASR和可检测性。

    arXiv:2311.08598v2 Announce Type: replace  Abstract: Language models (LMs) can be manipulated by adversarial attacks, which introduce subtle perturbations to input data. While recent attack methods can achieve a relatively high attack success rate (ASR), we've observed that the generated adversarial examples have a different data distribution compared with the original examples. Specifically, these adversarial examples exhibit reduced confidence levels and greater divergence from the training data distribution. Consequently, they are easy to detect using straightforward detection methods, diminishing the efficacy of such attacks. To address this issue, we propose a Distribution-Aware LoRA-based Adversarial Attack (DALA) method. DALA considers distribution shifts of adversarial examples to improve the attack's effectiveness under detection methods. We further design a novel evaluation metric, the Non-detectable Attack Success Rate (NASR), which integrates both ASR and detectability for 
    
[^323]: 伪装成羊的狼：普遍的嵌套越狱提示可以轻松愚弄大型语言模型

    A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily

    [https://arxiv.org/abs/2311.08268](https://arxiv.org/abs/2311.08268)

    提出一种利用大型语言模型自动生成有效的越狱提示的自动框架ReNeLLM，显著提高攻击成功率，同时大大减少时间成本。

    

    大型语言模型（LLMs），如ChatGPT和GPT-4，旨在提供有用和安全的响应。然而，被称为“越狱”的对抗性提示可以规避保障措施，导致LLMs生成潜在有害内容。探索越狱提示可以帮助更好地揭示LLMs的弱点，并进一步引导我们安全地保护它们。不幸的是，现有的越狱方法要么遭受复杂的手工设计，要么需要在其他白盒模型上进行优化，从而损害了泛化性或效率。在本文中，我们将越狱提示攻击概括为两个方面：（1）提示重写和（2）场景嵌套。基于此，我们提出了ReNeLLM，一个利用LLMs自身生成有效越狱提示的自动框架。大量实验证明，与现有基线相比，ReNeLLM显著提高了攻击成功率，同时大大减少了时间成本。

    arXiv:2311.08268v2 Announce Type: replace  Abstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, compromising generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Ou
    
[^324]: 再问一次：自一致性改善语言模型在（几乎）所有场景中的推理

    Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios

    [https://arxiv.org/abs/2311.08154](https://arxiv.org/abs/2311.08154)

    自一致性是一种通用的集成优化方法，可以应用于几乎所有情景中，能够解决语言模型推理中的重复性和局部最优性问题。

    

    尽管思维链（CoT）提示结合语言模型在复杂推理任务上取得了令人鼓舞的结果，但CoT提示中通常使用的贪婪解码会导致重复性和局部最优性。为解决这一缺点，集成优化尝试获得多个推理路径以得到最终答案集成。然而，当前的集成优化方法要么简单地采用基于规则的后处理，比如“自一致性”，要么训练一个基于几个与任务相关的人类注释的附加模型来在多个推理路径中选择最佳路径，但未能推广到现实设置，其中输入问题类型未知或推理路径的答案格式未知。为了避免它们的局限性，我们提出了“自一致性”，这是一种通用的集成优化方法，在几乎所有情景中适用，其中输入问题的类型...

    arXiv:2311.08154v2 Announce Type: replace-cross  Abstract: Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \textit{self-consistency}, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose \textbf{Self-Agreement}, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input question
    
[^325]: 对抗偏好优化

    Adversarial Preference Optimization

    [https://arxiv.org/abs/2311.08045](https://arxiv.org/abs/2311.08045)

    提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。

    

    人类偏好调整是提高大型语言模型（LLMs）交互质量的关键。现有的对齐方法依赖于手动注释的偏好数据来指导LLM的优化方向。然而，在实践中，持续更新LLMs会导致模型生成样本与人类首选响应之间存在分布差距，这阻碍了模型微调的效率。为了缓解这个问题，先前的方法需要在生成的样本上额外进行偏好注释，以适应转移分布，这需要大量的注释资源。针对更高效的人类偏好优化，我们提出了一种对抗偏好优化（APO）框架，其中LLM代理和偏好模型通过极小-极大博弈交替更新。在没有额外注释的情况下，我们的APO方法可以通过对抗学习自适应于生成分布差距。

    arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
    
[^326]: 毒性检测并不是你所需要的全部：弥合支持志愿内容管理员的差距

    Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators

    [https://arxiv.org/abs/2311.07879](https://arxiv.org/abs/2311.07879)

    本研究揭示了人工智能模型在识别有毒、冒犯和令人讨厌的内容方面的进展，并探讨了这些改进是否真正满足了志愿内容管理员在工作中的需求。

    

    人工智能模型在识别有毒、冒犯和令人讨厌的内容方面取得了长足的进展，旨在减轻管理员的工作负担。然而，目前尚不清楚这些任务的改进是否真正满足了管理员在工作中的需求。本文揭示了过去研究努力致力于为内容管理的各个方面提供自动化支持与志愿内容管理员的需求之间存在的差距，尤其是在识别违反各种管理规则方面。为此，我们在Hugging Face上对模型进行了调查，以揭示涵盖三个示范论坛的各种管理规则和指南的模型的可用性。我们进一步对最先进的LLM进行了测试，评估这些模型在标记某个特定论坛的平台规则违规方面的表现。最后，我们进行了用户调查研究。

    arXiv:2311.07879v2 Announce Type: replace-cross  Abstract: Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators' needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey stud
    
[^327]: 错误并不容易：大型语言模型在排除推理过程中遇到困难

    It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning

    [https://arxiv.org/abs/2311.07532](https://arxiv.org/abs/2311.07532)

    大型语言模型在排除推理过程中遇到困难，提出了一种新的排除推理方法PoE与COT，发现此方法在多项选择问题上的表现不如选择正确答案，并指出了研究中发现的一致性和错误分析的问题。

    

    链式思维（COT）提示可以帮助大型语言模型（LLMs）朝着正确答案进行推理，但其在朝着错误答案进行推理方面的有效性尚未被探究。当与COT一起使用时，这种排除推理（PoE）可以增强自我一致性、可解释性以及诸如排除性医学诊断等任务。因此，我们提出了在多项选择问题中进行PoE与COT的方法，LLMs必须朝着不正确的选项进行推理。我们评估了GPT-3.5、LLaMA-2和Falcon在总共四个常识和科学推理数据集上执行带有COT的PoE的能力。我们发现PoE策略总是表现不如选择正确答案的策略。这两种策略的一致性也低于每种策略的自我一致性。为了进一步研究这些问题，我们进行了错误分析并提出了未来工作的建议。

    arXiv:2311.07532v2 Announce Type: replace  Abstract: Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.
    
[^328]: 知识增强的大型语言模型用于个性化上下文查询建议

    Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion

    [https://arxiv.org/abs/2311.06318](https://arxiv.org/abs/2311.06318)

    提出一种新颖且通用的方法，通过从用户与搜索引擎的交互历史中提取相关上下文来个性化大型语言模型的输出，尤其适用于改进网络搜索体验。

    

    大型语言模型（LLMs）擅长解决各种自然语言任务。然而，由于重新训练或微调它们所涉及的成本巨大，它们仍然在很大程度上是静态的，并且难以个性化。尽管如此，许多应用程序可以从根据用户的偏好、目标和知识量定制的生成中受益。其中之一是网络搜索，了解用户试图做什么、关心什么以及他们知道什么可以提高搜索体验。在这项工作中，我们提出了一种新颖且通用的方法，该方法使用用户与搜索引擎的交互历史中的相关上下文来增强LLM以个性化其输出。具体而言，我们根据用户在网络上的搜索和浏览活动构建了每个用户的以实体为中心的知识存储，然后利用这些知识为LLM提供具有上下文相关性的提示增强。

    arXiv:2311.06318v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is 
    
[^329]: LAiW：一个中文法律大型语言模型基准

    LAiW: A Chinese Legal Large Language Models Benchmark

    [https://arxiv.org/abs/2310.05620](https://arxiv.org/abs/2310.05620)

    该论文提出了一个中文法律大型语言模型基准LAiW，通过构建基于法律实践逻辑的评估任务，揭示了当前通用和法律领域LLM可能不符合法律实践逻辑。

    

    arXiv:2310.05620v2 公告类型：替换 摘要：一般和法律领域的LLM在LegalAI的各种任务中表现出色。然而，当前对这些LLM在LegalAI中的评估是由计算机科学专家定义的，缺乏与法律实践逻辑一致性，使得很难判断它们的实际能力。为了解决这一挑战，我们首次基于法律实践逻辑构建了中文法律LLM基准LAiW。为了与法律专家和法律实践的思维过程（三段论）保持一致，我们将LLM的法律能力从易到难分为三个级别：基本信息检索、法律基础推理和复杂法律应用。每个级别包含多个任务以确保全面评估。通过对当前通用和法律领域LLM在我们的基准上的自动化评估，我们表明这些LLM可能不符合法律实践逻辑。LLM似乎

    arXiv:2310.05620v2 Announce Type: replace  Abstract: General and legal domain LLMs have demonstrated strong performance in various tasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are defined by the experts of computer science, lacking consistency with the logic of legal practice, making it difficult to judge their practical capabilities. To address this challenge, we are the first to build the Chinese legal LLMs benchmark LAiW, based on the logic of legal practice. To align with the thinking process of legal experts and legal practice (syllogism), we divide the legal capabilities of LLMs from easy to difficult into three levels: basic information retrieval, legal foundation inference, and complex legal application. Each level contains multiple tasks to ensure a comprehensive evaluation. Through automated evaluation of current general and legal domain LLMs on our benchmark, we indicate that these LLMs may not align with the logic of legal practice. LLMs seem 
    
[^330]: 关于常识推理的知识图谱解释的可信性

    Faithful Knowledge Graph Explanations for Commonsense Reasoning

    [https://arxiv.org/abs/2310.04910](https://arxiv.org/abs/2310.04910)

    本论文提出了两个量化指标来衡量基于知识图谱的解释的可信性，并引入了一种新的训练方法来改善解释的可信度。实验结果表明该方法可以提高解释的一致性和保真度。

    

    融合语言模型(LMs)和知识图谱(KGs)已成为常识问答研究中的常见方法，但在这些模型中实现精确的思路链解释仍然是一个未解决的问题。当前基于知识图谱的解释技术的一个主要弱点是在评估过程中忽视了生成解释的可信性。为了弥补这一差距，我们提出并验证了两个量化指标 - 图一致性和图保真度 - 来衡量基于知识图谱的解释的可信性。我们引入一种新的训练方法Consistent GNN (CGNN)，该方法添加了一项一致性正则化项来改善解释的可信度。我们的分析表明，KG的预测经常偏离原始模型的预测。所提出的CGNN方法提高了一致性和保真度，展示了它产生更可信解释的潜力。我们的工作强调了明确评估解释可信性的重要性。

    While fusing language models (LMs) and knowledge graphs (KGs) has become common in commonsense question answering research, enabling faithful chain-of-thought explanations in these models remains an open problem. One major weakness of current KG-based explanation techniques is that they overlook the faithfulness of generated explanations during evaluation. To address this gap, we make two main contributions: (1) We propose and validate two quantitative metrics - graph consistency and graph fidelity - to measure the faithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN), a novel training method that adds a consistency regularization term to improve explanation faithfulness. Our analysis shows that predictions from KG often diverge from original model predictions. The proposed CGNN approach boosts consistency and fidelity, demonstrating its potential for producing more faithful explanations. Our work emphasises the importance of explicitly evaluating suggest a path
    
[^331]: 赋能众多，偏袒少数：通过大型语言模型实现通用信用评分

    Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models

    [https://arxiv.org/abs/2310.00566](https://arxiv.org/abs/2310.00566)

    大型语言模型在信用评分任务中表现出强大的通用能力，通过首个开源框架和CALM模型，可以帮助解决传统信用评分方法所面临的挑战，并同时解决潜在的偏见问题。

    

    在金融行业，信用评分是一个基础要素，塑造着个人和企业的信贷准入，决定着贷款条件。本研究认为，大型语言模型（LLMs）在信用评分任务中具有巨大潜力，能够强大地跨多个任务进行泛化。为了系统地探索LLMs在信用评分中的应用，我们提出了第一个开源全面框架。我们收集了一个涵盖9个数据集、1.4K样本的新型基准，专门用于信用评估，并对LLMs内潜在偏见进行了重要检查，同时提供了超过45K样本的新型指导调整数据。然后，我们通过指导调整提出了首个信贷与风险评估大型语言模型（CALM），以针对不同的微妙需求

    arXiv:2310.00566v3 Announce Type: replace-cross  Abstract: In the financial industry, credit scoring is a fundamental element, shaping access to credit and determining the terms of loans for individuals and businesses alike. Traditional credit scoring methods, however, often grapple with challenges such as narrow knowledge scope and isolated evaluation of credit tasks. Our work posits that Large Language Models (LLMs) have great potential for credit scoring tasks, with strong generalization ability across multiple tasks. To systematically explore LLMs for credit scoring, we propose the first open-source comprehensive framework. We curate a novel benchmark covering 9 datasets with 14K samples, tailored for credit assessment and a critical examination of potential biases within LLMs, and the novel instruction tuning data with over 45k samples. We then propose the first Credit and Risk Assessment Large Language Model (CALM) by instruction tuning, tailored to the nuanced demands of various
    
[^332]: 从语言建模到指令跟随：理解指令调整后LLMs中行为的转变

    From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning

    [https://arxiv.org/abs/2310.00492](https://arxiv.org/abs/2310.00492)

    指令调整对LLMs产生了三个重要影响：1）使其能够识别用户提示中的指令部分；2）促进响应生成的不断调整

    

    大型语言模型（LLMs）已经取得了显著的成功，其中指令调整是将LLMs与用户意图对齐的关键步骤。在这项工作中，我们研究了指令调整如何调整经过预训练的模型，重点关注内在变化。具体来说，我们首先开发了几种本地和全局解释方法，包括一种基于梯度的输入输出归因方法，以及用于解释自注意力和前馈层中的模式和概念的技术。然后通过比较从预训练和指令调整模型中得出的解释来研究指令调整的影响。这种方法在人可理解的水平上提供了模型转变的内部视角。我们的研究发现了指令调整的三个重要影响：1）它使LLMs能够识别用户提示中的指令部分，并不断促进响应生成

    arXiv:2310.00492v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts from user prompts, and promotes the response generation constantly condition
    
[^333]: OctoPack: 使用指令调整代码的大规模语言模型

    OctoPack: Instruction Tuning Code Large Language Models

    [https://arxiv.org/abs/2308.07124](https://arxiv.org/abs/2308.07124)

    通过结合Git提交的自然结构，将代码更改与人类指令配对，我们提出了OctoPack，并在大规模语言模型上实现了表现最佳的指令调整方法。

    

    在指令上进行大规模语言模型（LLMs）的微调可显著提高自然语言任务的性能。我们应用代码进行指令调整，利用Git提交的自然结构，将代码更改与人类指令配对。我们编译了CommitPack：跨350种编程语言的4TB Git提交。我们在拥有16B参数的StarCoder模型上对比CommitPack和其他自然与合成代码指令（xP3x, Self-Instruct, OASST），在HumanEval Python基准测试（46.2% pass@1）中取得了未在OpenAI输出上训练的模型中的最新性能。我们进一步推出HumanEvalPack，将HumanEval基准测试扩展到共计3个编码任务（代码修复、代码解释、代码合成）跨6种语言（Python、JavaScript、Java、Go、C ++、Rust）。我们的模型OctoCoder和OctoGeeX在HumanEvalPack中取得了所有许可模型中的最佳性能。

    arXiv:2308.07124v2 Announce Type: replace-cross  Abstract: Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive m
    
[^334]: 用大型语言模型模块化地构建协作体现智能体

    Building Cooperative Embodied Agents Modularly with Large Language Models

    [https://arxiv.org/abs/2307.02485](https://arxiv.org/abs/2307.02485)

    利用大型语言模型构建模块化的协作体现智能体，实现多智能体合作解决具有挑战性的任务，超越规划方法并展示有效沟通。

    

    在这项工作中，我们处理具有去中心化控制、原始感知观察、昂贵通讯和多目标任务的具有各种体现环境的具有挑战性的多智能体合作问题。与先前研究不同的是，我们利用大型语言模型的常识知识、推理能力、语言理解和文本生成能力，并将它们无缝地融入到一个与感知、记忆和执行相结合的认知启发式模块化框架中。从而构建了一个可以规划、沟通和与其他人合作以高效完成长时程任务的合作体现智能体CoELA。我们在C-WAH和TDW-MAT上的实验表明，由GPT-4驱动的CoELA可以超越强大的基于规划的方法，并展示出新兴的有效沟通。

    arXiv:2307.02485v2 Announce Type: replace  Abstract: In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current O
    
[^335]: BMX：使用可解释性提升自然语言生成度量

    BMX: Boosting Natural Language Generation Metrics with Explainability

    [https://arxiv.org/abs/2212.10469](https://arxiv.org/abs/2212.10469)

    提出的方法BMX利用解释来提升自然语言生成度量的性能，通过将特征重要性解释转化为段落级分数，并结合原始度量，取得了更好的评估结果。

    

    最先进的自然语言生成评估度量基于黑盒语言模型。因此，最近的工作考虑了它们的可解释性，目标是更好地为人类理解和更好地度量分析，包括失败案例。相反，我们提出的方法BMX：使用可解释性提升自然语言生成度量明确利用解释来提升指标的性能。具体来说，我们将特征重要性解释视为单词级分数，通过幂均值将其转换为段落级分数。然后，我们将此段落级分数与原始度量结合以获得更好的度量。我们的测试显示，在多个机器翻译和摘要数据集上取得了改进。虽然机器翻译方面的改进很小，但在摘要方面效果显著。值得注意的是，使用LIME解释器和预选参数的BMX达到了平均0.0的改进。

    arXiv:2212.10469v2 Announce Type: replace  Abstract: State-of-the-art natural language generation evaluation metrics are based on black-box language models. Hence, recent works consider their explainability with the goals of better understandability for humans and better metric analysis, including failure cases. In contrast, our proposed method BMX: Boosting Natural Language Generation Metrics with explainability explicitly leverages explanations to boost the metrics' performance. In particular, we perceive feature importance explanations as word-level scores, which we convert, via power means, into a segment-level score. We then combine this segment-level score with the original metric to obtain a better metric. Our tests show improvements for multiple metrics across MT and summarization datasets. While improvements in machine translation are small, they are strong for summarization. Notably, BMX with the LIME explainer and preselected parameters achieves an average improvement of 0.0
    
[^336]: G-MAP：通用记忆增强的面向领域任务的预训练语言模型

    G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks

    [https://arxiv.org/abs/2212.03613](https://arxiv.org/abs/2212.03613)

    G-MAP提出了一种新的通用记忆增强的预训练语言模型框架，能够在增强领域特定PLM时，保留通用知识，缓解灾难性遗忘现象，提高模型性能

    

    最近，已经提出了领域特定的PLMs，通过继续使用领域特定语料库对通用PLMs进行预训练，来提高特定领域（例如生物医学和计算机科学）的任务性能。然而，这种领域自适应预训练（DAPT；Gururangan等人（2020））往往会遗忘通用PLMs已经获得的先前通用知识，导致灾难性遗忘现象和次优性能。为了缓解这个问题，我们提出了一个新的通用记忆增强的预训练语言模型框架（G-MAP），它通过从冻结的通用PLM构建的记忆表示来增强领域特定的PLM，而不会丢失任何通用知识。具体来说，我们提出了一种新的记忆增强层，并基于它，探索了不同的增强策略来构建记忆表示，然后将其自适应地融合到领域特定的PLM中。我们展示了G-MAP的有效性

    arXiv:2212.03613v3 Announce Type: replace  Abstract: Recently, domain-specific PLMs have been proposed to boost the task performance of specific domains (e.g., biomedical and computer science) by continuing to pre-train general PLMs with domain-specific corpora. However, this Domain-Adaptive Pre-Training (DAPT; Gururangan et al. (2020)) tends to forget the previous general knowledge acquired by general PLMs, which leads to a catastrophic forgetting phenomenon and sub-optimal performance. To alleviate this problem, we propose a new framework of General Memory Augmented Pre-trained Language Model (G-MAP), which augments the domain-specific PLM by a memory representation built from the frozen general PLM without losing any general knowledge. Specifically, we propose a new memory-augmented layer, and based on it, different augmented strategies are explored to build the memory representation and then adaptively fuse it into the domain-specific PLM. We demonstrate the effectiveness of G-MAP 
    
[^337]: 自洽推理用于解决数学应用题

    Self-consistent Reasoning For Solving Math Word Problems

    [https://arxiv.org/abs/2210.15373](https://arxiv.org/abs/2210.15373)

    提出了自洽推理框架SCR，通过修剪策略和对称Kullback-Leibler散度校准输出分布偏移，从而解决数学应用题中虚假相关性的问题

    

    数学应用题（MWPs）是一个从给定的数学问题文本中自动推导解题表达式的任务。先前的研究受到输入文本和输出表达式之间虚假相关性的困扰。为了缓解这一问题，我们提出了一个自洽推理框架SCR，试图采用修剪策略来纠正输出分布偏移，以隐式修复这些虚假相关样本。具体来说，我们首先通过修剪roberta2tree模型来获取一个子网络，以便利用原始roberta2tree模型和被修剪子网络之间的输出分布差距来暴露虚假相关样本。然后，我们通过对称Kullback-Leibler散度来校准输出分布偏移，以减轻虚假相关。此外，SCR生成等效表达式，从而捕捉原始文本的逻辑，而不是依赖于提示。

    arXiv:2210.15373v2 Announce Type: replace  Abstract: Math word problems (MWPs) is a task that automatically derives solution expression from a giving math problems in text. The previous studies suffer from spurious correlations between input text and output expression. To mitigate this issue, we propose a self-consistent reasoning framework called SCR, which attempts to adopt a pruning strategy to correct the output distribution shift so as to implicitly fix those spurious correlative samples. Specifically, we firstly obtain a sub-network by pruning a roberta2tree model, for the sake to use the gap on output distribution between the original roberta2tree model and the pruned sub-network to expose spurious correlative samples. Then, we calibrate the output distribution shift by applying symmetric Kullback-Leibler divergence to alleviate spurious correlations. In addition, SCR generates equivalent expressions, thereby, capturing the original text's logic rather than relying on hints from
    
[^338]: 来自未分割烹饪视频的食谱生成

    Recipe Generation from Unsegmented Cooking Videos

    [https://arxiv.org/abs/2209.10134](https://arxiv.org/abs/2209.10134)

    本文研究如何从未分割的烹饪视频中生成正确的食谱，通过选择神谕事件并重新生成句子来实现。

    

    本文解决了从未分割的烹饪视频中生成食谱的问题，这需要代理程序提取完成菜肴时的关键事件并为提取的事件生成句子。我们的任务类似于密集视频字幕生成（DVC），其目标是彻底检测事件并为其生成句子。然而，与DVC不同，对于食谱生成，食谱故事意识至关重要，模型应该按正确顺序提取适当数量的事件并基于它们生成准确的句子。我们分析了DVC模型的输出并确认，尽管几个事件可以被采用作为食谱故事，但为这些事件生成的句子与视觉内容并不相关。基于此，我们的目标是通过从输出事件中选择神谕事件并为其重新生成句子来获得正确的食谱。为实现这一目标，我们提出了一种基于变压器的多模式r

    arXiv:2209.10134v2 Announce Type: replace-cross  Abstract: This paper tackles recipe generation from unsegmented cooking videos, a task that requires agents to (1) extract key events in completing the dish and (2) generate sentences for the extracted events. Our task is similar to dense video captioning (DVC), which aims at detecting events thoroughly and generating sentences for them. However, unlike DVC, in recipe generation, recipe story awareness is crucial, and a model should extract an appropriate number of events in the correct order and generate accurate sentences based on them. We analyze the output of the DVC model and confirm that although (1) several events are adoptable as a recipe story, (2) the generated sentences for such events are not grounded in the visual content. Based on this, we set our goal to obtain correct recipes by selecting oracle events from the output events and re-generating sentences for them. To achieve this, we propose a transformer-based multimodal r
    
[^339]: 基于群稀疏矩阵分解的词嵌入传递学习

    Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings

    [https://arxiv.org/abs/2104.08928](https://arxiv.org/abs/2104.08928)

    提出了一种基于群稀疏矩阵分解的方法，用于在新领域进行词嵌入的传递学习，以解决不同领域单词含义差异的挑战。

    

    非结构化文本为许多领域的决策者提供了丰富的数据源，涵盖范围从零售中的产品评论到医疗保健中的护理记录。为了利用这些信息，通常会通过无监督学习算法（如矩阵分解）将单词转换为词嵌入——编码单词之间语义关系的向量。然而，从具有有限训练数据的新领域学习单词嵌入可能具有挑战性，因为在新领域中，单词的含义/用法可能不同，例如，“positive”一词通常具有正面情绪，但在医疗记录中往往具有负面情绪，因为它可能意味着患者检测呈阳性。在实践中，我们预计只有少量领域特定单词可能具有新含义。我们提出了一个直观的两阶段估计器，通过群稀疏惩罚来有效地传递学习领域特定的新含义。

    arXiv:2104.08928v3 Announce Type: replace-cross  Abstract: Unstructured text provides decision-makers with a rich data source in many domains, ranging from product reviews in retail to nursing notes in healthcare. To leverage this information, words are typically translated into word embeddings -- vectors that encode the semantic relationships between words -- through unsupervised learning algorithms such as matrix factorization. However, learning word embeddings from new domains with limited training data can be challenging, because the meaning/usage may be different in the new domain, e.g., the word ``positive'' typically has positive sentiment, but often has negative sentiment in medical notes since it may imply that a patient tested positive for a disease. In practice, we expect that only a small number of domain-specific words may have new meanings. We propose an intuitive two-stage estimator that exploits this structure via a group-sparse penalty to efficiently transfer learn dom
    
[^340]: CRUD-RAG: 用于检索增强生成的大型语言模型的全面中文基准

    CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models. (arXiv:2401.17043v1 [cs.CL])

    [http://arxiv.org/abs/2401.17043](http://arxiv.org/abs/2401.17043)

    这篇论文构建了一个大规模且更全面的中文基准测试，评估了检索增强生成系统的所有组件在各种应用场景中的性能。

    

    检索增强生成（RAG）是一种通过整合外部知识源来增强大型语言模型（LLM）能力的技术。该方法解决了LLM的常见限制，包括过时的信息和产生不准确的“虚构”内容的倾向。然而，评估RAG系统具有挑战性，因为现有的基准测试在范围和多样性上存在限制。大多数当前的基准测试主要评估问答应用，忽视了RAG可能有优势的更广泛的场景。此外，它们只评估RAG流程中LLM组件的性能，并忽视检索组件和外部知识数据库的影响。为了解决这些问题，本文构建了一个大规模且更全面的基准测试，并在各种RAG应用场景中评估了RAG系统的所有组件。

    Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the ran
    
[^341]: 发挥专业放射科医生的专长，提升放射学报告的LLM评估

    Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports. (arXiv:2401.16578v1 [cs.CL])

    [http://arxiv.org/abs/2401.16578](http://arxiv.org/abs/2401.16578)

    该论文提出了一种方法，将专业放射科医生的专业知识与大型语言模型相结合，来提升自动生成报告的自动评估。实验结果显示，该方法的模型在评估中表现优于传统的度量标准。

    

    在放射学领域，人工智能（AI）已经大大推进了报告生成，但自动生成报告的自动评估仍然具有挑战性。目前的度量标准，如传统自然语言生成（NLG）和临床效能（CE），往往无法捕捉临床背景的语义复杂性，或者过分强调临床细节，降低了报告的清晰性。为了解决这些问题，我们提出的方法将专业放射科医生的专业知识与大型语言模型（LLMs），如GPT-3.5和GPT-4 1，相结合。利用上下文指导学习（ICIL）和思维链（CoT）推理，我们的方法使LLM的评估与放射科医生的标准保持一致，实现了人工智能生成报告与人类生成报告之间的详细比较。这进一步通过回归模型来综合句子评估分数。实验结果表明，我们的“详细GPT-4（5次训练）”模型获得了0.48的分数，优于METEOR指标。

    In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our ''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the METEOR metric 
    
[^342]: 一种统一的情感检测和任务导向对话建模方法

    A Unified Approach to Emotion Detection and Task-Oriented Dialogue Modeling. (arXiv:2401.13789v1 [cs.CL])

    [http://arxiv.org/abs/2401.13789](http://arxiv.org/abs/2401.13789)

    提出了一种统一的方法来实现情感检测和任务导向对话建模，通过在信念状态跟踪中引入情感检测实现，并将其融入端到端的任务导向对话系统中。实验证明该方法提高了情感检测和任务结果的性能，并显示用户的情感可以作为回应的上下文条件，对于提高回应的共鸣程度具有帮助。

    

    在当前基于文本的任务导向对话（TOD）系统中，用户情感检测（ED）经常被忽视，或者通常被视为一项独立的任务，需要额外的训练。相反，我们的工作证明了无缝地统一ED和TOD建模可以带来相互的好处，因此是一种值得考虑的替代方法。我们的方法是通过将ED包含在信念状态跟踪中，并依赖于单一的语言模型，来扩展SimpleToD这个端到端的TOD系统。我们使用GPT-2和Llama-2在EmoWOZ基准测试集上评估了我们的方法，这是一个使用情感进行注释的MultiWOZ版本。我们的结果显示，ED和任务结果的性能普遍提高。我们的研究结果还表明，用户的情感为系统的回应提供了有用的上下文条件，并可以用于进一步改善回应的共鸣程度。

    In current text-based task-oriented dialogue (TOD) systems, user emotion detection (ED) is often overlooked or is typically treated as a separate and independent task, requiring additional training. In contrast, our work demonstrates that seamlessly unifying ED and TOD modeling brings about mutual benefits, and is therefore an alternative to be considered. Our method consists in augmenting SimpleToD, an end-to-end TOD system, by extending belief state tracking to include ED, relying on a single language model. We evaluate our approach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ annotated with emotions. Our results reveal a general increase in performance for ED and task results. Our findings also indicate that user emotions provide useful contextual conditioning for system responses, and can be leveraged to further refine responses in terms of empathy.
    
[^343]: MM-LLMs: 多模式大语言模型的最新进展

    MM-LLMs: Recent Advances in MultiModal Large Language Models. (arXiv:2401.13601v1 [cs.CL])

    [http://arxiv.org/abs/2401.13601](http://arxiv.org/abs/2401.13601)

    近年来，多模式大语言模型（MM-LLMs）通过成本效益高的训练策略取得了显著进展，扩展了现有的语言模型的多模输入和输出支持。本论文提供了一份综合调查报告，介绍了MM-LLMs的设计和训练方案，整理了现有的MM-LLMs及其性能，总结了关键训练方法，并探讨了未来的研究方向。

    

    在过去的一年中，多模式大语言模型（MM-LLMs）取得了显著的进展，通过成本效益高的训练策略，增强了现有的LLMs对多模输入或输出的支持。这些结果模型不仅保留了LLMs固有的推理和决策能力，还赋予了各种多模任务。本文提供了一份综合性的调查报告，旨在促进对MM-LLMs的进一步研究。具体而言，我们首先概述了模型架构和训练流程的一般设计方案。随后，我们简要介绍了26种现有的MM-LLMs，每种都以其具体的公式为特征。此外，我们还回顾了MM-LLMs在主流基准测试上的性能，并总结了提高MM-LLMs效力的关键训练方法。最后，我们探讨了MM-LLMs的有前途的方向，同时还为该领域的最新发展提供了实时追踪网站。我们希望这份调查报告能够促进对MM-LLMs的进一步研究。

    In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this surv
    
[^344]: LLM指令微调中的提示权重实验

    Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])

    [http://arxiv.org/abs/2401.13586](http://arxiv.org/abs/2401.13586)

    LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。

    

    我们进行了一项小型研究，分析了提示词标记分类损失加权（PLW）如何影响在指令任务上进行微调的7B大小的LLaMA模型的性能。我们使用多个指令数据集重现了斯坦福大学的Alpaca实验，其中包括LLaMA 1和LLaMA 2。我们发现，在我们的短提示完成数据集上微调的模型与PLW之间存在负二次关系，而在长提示完成数据集上微调的模型不受PLW的影响。

    We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
    
[^345]: 超越基于参考指标：分析开放式LLM在数据到文本生成上的行为

    Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation. (arXiv:2401.10186v1 [cs.CL])

    [http://arxiv.org/abs/2401.10186](http://arxiv.org/abs/2401.10186)

    开放式大型语言模型在零-shot设置下能够从各种标准数据格式中生成流畅和连贯的文本，但是输出的语义准确性仍然是一个重要问题。

    

    我们调查开放式大型语言模型(LLMs)在从结构化数据生成连贯和相关文本方面的程度。为了防止基准泄露到LLM训练数据中的偏差，我们收集了Quintd-1:一个为5个数据到文本(D2T)生成任务设计的专门基准，该任务包括从公共API中收集的标准格式的结构化数据记录。我们利用无参考评估指标和LLMs的上下文学习能力，使我们能够在没有人工写作参考资料的情况下测试模型。我们的评估重点是在token级别上对语义准确性错误进行注释，结合人类标注者和基于GPT-4的指标。我们系统地研究了模型在不同领域和任务中的行为，发现7B参数的最先进开放式LLMs可以在零-shot设置中从各种标准数据格式中生成流畅和连贯的文本。然而，我们也表明输出的语义准确性仍然是一个重大问题：在我们的基准上，80%的输出存在语义准确性错误。

    We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data. To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs. We leverage reference-free evaluation metrics and LLMs' in-context learning capabilities, allowing us to test the models with no human-written references. Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4. Our systematic examination of the models' behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs o
    
[^346]: R-Judge: 评估LLM代理的安全风险意识的基准测试

    R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. (arXiv:2401.10019v1 [cs.CL])

    [http://arxiv.org/abs/2401.10019](http://arxiv.org/abs/2401.10019)

    这篇论文主要介绍了一种评估LLM代理在不同环境中判断安全风险能力的基准测试R-Judge，通过对162个代理交互记录进行评估，发现GPT-4模型表现最佳，达到了72.29%的准确率。

    

    大型语言模型（LLM）在自动完成各种真实世界应用任务方面展现出巨大潜力。然而，这些LLM代理在交互环境中操作时会引入意外的安全风险。与大多数之前的研究集中在LLM生成内容的安全性不同，本研究关注评估LLM代理在不同环境中的行为安全性的迫切需求。我们介绍了一个名为R-Judge的基准测试，用于评估LLM在给定代理交互记录时判断安全风险的能力。R-Judge包括162个代理交互记录，涵盖7个应用领域和10种风险类型的27个关键风险场景。它结合了人类对安全性的共识，并具有标记的安全风险标签和高质量的风险描述。利用R-Judge，我们对8种常用作代理骨干的著名LLM模型进行了全面评估。表现最好的模型GPT-4实现了72.29%的对比结果。

    Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to
    
[^347]: INTERS: 使用指令调优解锁大型语言模型在搜索中的力量

    INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning. (arXiv:2401.06532v1 [cs.CL])

    [http://arxiv.org/abs/2401.06532](http://arxiv.org/abs/2401.06532)

    本研究探索了指令调优的方法，以增强大型语言模型在信息检索任务中的能力，通过引入一个新的指令调优数据集INTERS，涵盖了21个IR任务，该方法显著提升了性能。

    

    大型语言模型（LLMs）在各种自然语言处理任务中展示了令人印象深刻的能力。然而，由于许多与信息检索（IR）具体概念的不经常出现在自然语言中，它们在信息检索任务中的应用仍然具有挑战性。虽然基于提示的方法可以向LLMs提供任务描述，但它们往往在促进全面理解和执行IR任务方面存在不足，从而限制了LLMs的适用性。为了弥补这一差距，本研究探索了指令调优的潜力，以提高LLMs在IR任务中的熟练程度。我们引入了一个新的指令调优数据集INTERS，涵盖了3个基本IR类别中的21个任务：查询理解、文档理解和查询文档关系理解。数据来自43个不同的由手动编写的模板构成的数据集。我们的实证结果表明，INTERS显著提升了各种公开数据集上的性能。

    Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly a
    
[^348]: EASYTOOL: 使用简洁的工具指示增强基于LLM的代理

    EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction. (arXiv:2401.06201v1 [cs.CL])

    [http://arxiv.org/abs/2401.06201](http://arxiv.org/abs/2401.06201)

    EASYTOOL是一个将多样化而冗长的工具文档转化为统一而简洁的工具指示的框架，用于增强基于LLM的代理的能力。通过从多个来源提取关键信息，并提供标准化的工具描述和功能，EasyTool显著降低了标记消耗，并提高了在真实场景中的工具利用性能。

    

    为了解决复杂的现实世界任务，越来越多的关注点放在了在大型语言模型(LLM)应用中的工具利用上。为了开发基于LLM的代理，通常需要LLM从不同的工具文档中理解许多工具功能。但这些文档可能是多样化的、冗余的或不完整的，这极大地影响了LLM在使用工具方面的能力。为了解决这个问题，我们介绍了EASYTOOL，这是一个将多样化而冗长的工具文档转化为统一而简洁的工具指示，以便更容易地使用工具。EasyTool从不同来源的广泛工具文档中提取出关键信息，并详细说明一个统一的接口（即工具指示），为基于LLM的代理提供标准化的工具描述和功能。对多个不同任务的广泛实验表明，EasyTool可以显著减少标记的消耗，并改善在现实场景中的工具利用性能。

    To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our co
    
[^349]: SAC^3: 基于语义感知交叉检查一致性的黑盒语言模型可靠幻觉检测

    SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency. (arXiv:2311.01740v1 [cs.CL])

    [http://arxiv.org/abs/2311.01740](http://arxiv.org/abs/2311.01740)

    SAC^3是一种基于语义感知交叉检查一致性的方法，可以可靠地检测黑盒语言模型中的幻觉。该方法通过加入语义等效问题扰动和跨模型响应一致性检查等机制，能够有效识别问题级别和模型级别的幻觉。实证分析表明，SAC^3在检测非事实和事实陈述方面优于现有技术最新水平。

    

    幻觉检测是了解现代语言模型可信度的关键步骤。为了实现这一目标，我们重新审视了基于语言模型自一致性的现有检测方法，并发现了两种幻觉类型，即基于问题和基于模型的幻觉，它们仅通过自一致性检查无法有效识别。基于这一发现，我们提出了一种新的基于采样的方法，即语义感知交叉检查一致性（SAC^3），它在自一致性检查原则的基础上加入了额外的机制，通过利用语义等效问题扰动和跨模型响应一致性检查来检测问题级别和模型级别的幻觉。通过广泛而系统的实证分析，我们证明SAC^3在检测非事实和事实陈述方面胜过了现有技术的最新水平。

    Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC$^3$) that expands on the principle of self-consistency checking. Our SAC$^3$ approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC$^3$ outperforms the state of the art in detecting both non-factual and factual statements across multip
    
[^350]: 扭曲，分散，解码：指令调节模型能够根据嘈杂的指令改进其响应

    Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions. (arXiv:2311.00233v1 [cs.CL])

    [http://arxiv.org/abs/2311.00233](http://arxiv.org/abs/2311.00233)

    本文提出了一种简单而有效的方法，通过对比性调整预测，利用嘈杂指令来增强指令调节模型的效果，从而改进了面临超出训练范围的指令时的响应准确性。

    

    虽然经过指令调节的语言模型在零-shot泛化方面表现出色，但是当面临超出训练范围的指令时，这些模型往往难以生成准确的响应。本文提出了Instructive Decoding（ID）的方法，这是一种简单且有效的方法，可以增强指令调节模型的效果。具体而言，ID通过对下一个标记预测的logits进行对比性调整，利用从原始指令的操纵版本生成的预测，即嘈杂指令。这个嘈杂指令旨在引发与预期指令不一致但仍然合理的响应。我们在一系列此类嘈杂指令上进行了实验，从通过随机词插入语义噪声到像“相反”的其他指令，以引发偏离的响应。我们的方法在各种指令调节模型和任务中取得了相当大的性能提升，而无需

    While instruction-tuned language models have demonstrated impressive zero-shot generalization, these models often struggle to generate accurate responses when faced with instructions that fall outside their training set. This paper presents Instructive Decoding (ID), a simple yet effective approach that augments the efficacy of instruction-tuned models. Specifically, ID adjusts the logits for next-token prediction in a contrastive manner, utilizing predictions generated from a manipulated version of the original instruction, referred to as a noisy instruction. This noisy instruction aims to elicit responses that could diverge from the intended instruction yet remain plausible. We conduct experiments across a spectrum of such noisy instructions, ranging from those that insert semantic noise via random words to others like 'opposite' that elicit the deviated responses. Our approach achieves considerable performance gains across various instruction-tuned models and tasks without necessita
    
[^351]: 角色扮演聊天机器人能够捕捉角色个性吗？评估角色扮演聊天机器人的个性特点。

    Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots. (arXiv:2310.17976v1 [cs.CL])

    [http://arxiv.org/abs/2310.17976](http://arxiv.org/abs/2310.17976)

    本文介绍了一种创新的方法，用于评估角色扮演聊天机器人的个性特点，并发现基于LLMs的现代角色扮演聊天机器人能够有效地描绘出相应角色的个性特点，与人类感知的一致性达到82.8%。

    

    大规模预训练语言模型的出现彻底改变了新型人工智能应用的能力，尤其是在打造具有独特人物的聊天机器人方面。鉴于聊天机器人的"刺激-响应"性质，本文揭示了一种创新的开放式采访式方法，用于评估角色扮演聊天机器人的个性特点，从而更深入地理解其内在个性。我们对ChatHaruhi图书馆创建的32个角色扮演聊天机器人进行了大五人格和MBTI维度上的个性评估，并测量它们与人类感知的一致性。评估结果强调，基于LLMs的现代角色扮演聊天机器人能够有效地描绘出相应角色的个性特点，与人类感知的一致性达到82.8%。此外，我们还提出了塑造聊天机器人个性的潜在策略。因此，本文为角色扮演聊天机器人研究奠定了基础。

    The emergence of large-scale pretrained language models has revolutionized the capabilities of new AI application, especially in the realm of crafting chatbots with distinct personas. Given the "stimulus-response" nature of chatbots, this paper unveils an innovative open-ended interview-style approach for personality assessment on role-playing chatbots, which offers a richer comprehension of their intrinsic personalities. We conduct personality assessments on 32 role-playing chatbots created by the ChatHaruhi library, across both the Big Five and MBTI dimensions, and measure their alignment with human perception. Evaluation results underscore that modern role-playing chatbots based on LLMs can effectively portray personality traits of corresponding characters, with an alignment rate of 82.8% compared with human-perceived personalities. Besides, we also suggest potential strategies for shaping chatbots' personalities. Hence, this paper serves as a cornerstone study for role-playing chat
    
[^352]: Whisper-MCE: 针对混合语言实现更好性能的Whisper模型微调

    Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages. (arXiv:2310.17953v1 [cs.SD])

    [http://arxiv.org/abs/2310.17953](http://arxiv.org/abs/2310.17953)

    Whisper-MCE是使用自己收集的混合粤语和英语音频数据集（MCE）进行训练的Whisper模型微调，相较于基准模型，其在准确捕捉原始音频内容、提高识别准确性和加快识别速度方面具有更优越的能力，尤其在混合语言识别任务中表现出色。

    

    最近，Whisper在英语自动语音识别（ASR）领域已经接近于人类级别的鲁棒性和准确性，但在较小语种和混合语言的语音识别中，仍然需要进一步改进。本文介绍了我们细调的Whisper模型Whisper-MCE的令人瞩目的结果，该模型使用了我们自己收集的混合粤语和英语音频数据集（MCE）进行训练。同时，考虑到词错误率（WER）在较小语种和混合语言环境中评估其有效性时存在挑战，我们提出了一种新颖的评估机制。通过将我们的模型与基准的whisper-large-v2模型进行比较，我们展示了它准确捕捉原始音频内容的能力更强、识别准确性更高、识别速度更快。值得注意的是，我们的模型在识别混合语言的特定任务中胜过其他现有模型。

    Recently Whisper has approached human-level robustness and accuracy in English automatic speech recognition (ASR), while in minor language and mixed language speech recognition, there remains a compelling need for further improvement. In this work, we present the impressive results of Whisper-MCE, our finetuned Whisper model, which was trained using our self-collected dataset, Mixed Cantonese and English audio dataset (MCE). Meanwhile, considering word error rate (WER) poses challenges when it comes to evaluating its effectiveness in minor language and mixed-language contexts, we present a novel rating mechanism. By comparing our model to the baseline whisper-large-v2 model, we demonstrate its superior ability to accurately capture the content of the original audio, achieve higher recognition accuracy, and exhibit faster recognition speed. Notably, our model outperforms other existing models in the specific task of recognizing mixed language.
    
[^353]: 具有强化改写生成的对话问答模型的鲁棒训练

    Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])

    [http://arxiv.org/abs/2310.13505](http://arxiv.org/abs/2310.13505)

    这项研究提出了一种新的框架REIGN，通过生成训练问题的改写，并使用深度强化学习来指导对话问答模型，增加模型对表面形式变化的鲁棒性，同时在不同的基准上进行零-shot应用。

    

    知识图谱（KG）上的对话问答（ConvQA）模型通常在黄金QA对的基准上进行训练和测试。这意味着训练仅限于在相应数据集中见到的表面形式，评估仅针对一小部分问题。通过我们的提出的框架REIGN，我们采取了几个步骤来解决这个受限的学习设置。首先，我们系统地生成训练问题的改写，以提高模型对表面形式变化的鲁棒性。这是一个特别具有挑战性的问题，因为这些问题的不完整性。其次，我们使用深度强化学习将ConvQA模型引导到更高的性能，只提供那些有助于提高回答质量的改写。第三，我们展示了在一个基准上训练主要模型组件并将其零-shot应用于另一个的可行性。最后，为了对训练模型的鲁棒性进行严格评估，我们使用和重新配置初始的改写、测试语料。

    Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
    
[^354]: 大型语言模型的去学习研究

    Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])

    [http://arxiv.org/abs/2310.10683](http://arxiv.org/abs/2310.10683)

    大型语言模型的去学习是一个研究的新领域，我们探索了三个场景，可以通过去学习让语言模型与人类偏好保持一致。去学习具有三个优势，只需要负面示例，计算效率高，特别对于知道具体导致不良行为的训练样本更为有效。

    

    我们研究了如何对大型语言模型（LLMs）进行去学习，即忘记不受欢迎的（非）行为。我们展示了至少三种情境可以从去学习中使LLMs与人类偏好保持一致：（1）删除有害回复，（2）按要求删除受版权保护的内容，以及（3）消除幻觉。作为对齐技术的一种，去学习具有三个优点：（1）只需要负面（例如有害）示例，这比在RLHF（基于人类反馈的强化学习）中所需的正面（例如有帮助且通常由人类编写）示例更容易和更便宜地收集（例如通过红队测试或用户报告）；（2）计算效率高；（3）当我们知道哪些训练样本导致了不良行为时，它特别有效。据我们所知，我们的工作是首次探索LLM去学习的工作之一。我们也是首次在LLM去学习中制定了设置、目标和评估。我们表明，如果从业者只有有限的

    We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited
    
[^355]: 理解RLHF对LLM泛化和多样性的影响

    Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06452](http://arxiv.org/abs/2310.06452)

    本研究深入分析了强化学习从人类反馈中调整的大型语言模型每个阶段对超出分布泛化和输出多样性的影响。

    

    在最广泛使用的AI模型中，如OpenAI的ChatGPT或Anthropic的Claude，使用强化学习从人类反馈中调整的大型语言模型（LLM）。尽管在这些方法的开发方面有大量的研究，但我们对RLHF过程中每个阶段的利与弊的理解仍然有限。为了填补这一空白，我们对每个阶段（即监督微调（SFT），奖励建模和RLHF）如何影响两个关键属性进行了全面分析：超出分布的泛化和输出多样性。在这些模型被广泛应用于真实世界中的各种情景的背景下，超出分布的泛化非常重要，而输出多样性指的是模型生成各种不同输出的能力，对于各种用例来说都非常重要。我们在摘要和指令遵循任务中对两个基本模型进行了分析，后者非常相关。

    Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant 
    
[^356]: JsonTuning：面向通用、强大和可控的指令调优

    JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning. (arXiv:2310.02953v1 [cs.CL])

    [http://arxiv.org/abs/2310.02953](http://arxiv.org/abs/2310.02953)

    JsonTuning是一种面向通用、强大和可控的指令调优方法，通过利用JSON的结构化特性，帮助模型理解任务要素及其关系，从而扩展了通用性、提高了稳健性，并增强了对输出的控制。

    

    指令调优已成为利用大型语言模型（LLM）能力的关键过程，通过提供明确的任务指令，从而在各种任务中提高性能。然而，目前的文本-文本指令调优（TextTuning）方法由于任务的模糊性和缺乏明确的结构而存在通用性、稳健性和可控性的限制。在本文中，我们提出了JsonTuning，这是一种新的结构到结构的指令调优方法。通过利用JSON的多功能和结构化特性来表示任务，JsonTuning通过帮助模型理解关键任务要素及其关系，扩展了通用性，通过最小化歧义性提高了稳健性，并通过提供对输出的显式控制增强了可控性。我们对不同的语言模型和评估基准进行了全面的比较研究。实验结果表明，JsonTuning在性能上优于TextTuning。

    Instruction tuning has emerged as a crucial process for harnessing the capabilities of large language models (LLMs) by providing explicit task instructions, leading to improved performance in various tasks. However, prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks. In this paper, we propose JsonTuning, a novel structure-to-structure approach for instruction tuning. By leveraging the versatility and structured nature of JSON to represent tasks, JsonTuning enhances generalization by helping the model understand essential task elements and their relations, improves robustness by minimizing ambiguity, and increases controllability by providing explicit control over the output. We conduct a comprehensive comparative study with diverse language models and evaluation benchmarks. Experimental results show that JsonTuning outperforms TextTuning in
    
[^357]: ChaCha：利用大型语言模型引导儿童分享与个人事件相关的情绪

    ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])

    [http://arxiv.org/abs/2309.12244](http://arxiv.org/abs/2309.12244)

    ChaCha是一个利用大型语言模型（LLMs）的聊天机器人，鼓励儿童分享个人事件和相关情绪。通过一个探索性研究，发现儿童将ChaCha视为亲密的朋友，并愿意与其分享各种主题的故事。

    

    儿童通常通过与家人或他人分享故事和感受来学习辨识和表达情绪，然而，由于儿童正在发展他们的交流技能，父母或兄弟姐妹很难与他们进行情感沟通。本文介绍了ChaCha，一个鼓励和引导儿童分享个人事件和相关情绪的聊天机器人。ChaCha结合了状态机和大型语言模型（LLMs），在进行自由对话的同时保持对话的方向性。通过与20名年龄在8-12岁的儿童进行的探索性研究，我们研究了ChaCha如何促使儿童分享个人事件并引导他们描述相关情绪。参与者认为ChaCha就像一个亲密的朋友，并分享了各种主题的故事，如家庭旅行和个人成就。基于定量和定性发现，我们讨论了利用LLMs设计适合儿童的聊天机器人的机遇。

    Children typically learn to identify and express emotions through sharing their stories and feelings with others, particularly their family. However, it is challenging for parents or siblings to have emotional communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8-12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the quantitative and qualitative findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to
    
[^358]: 调查LLMs中更微妙的偏见：生成模型中的年龄主义、美丽、机构和国籍偏见

    Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models. (arXiv:2309.08902v1 [cs.CL])

    [http://arxiv.org/abs/2309.08902](http://arxiv.org/abs/2309.08902)

    本文调查了LLMs在年龄、美丽、机构和国籍等少研究但仍然重要的维度上的偏见，通过衡量在社会群体和不相关的正负属性之间做出的微妙相关决策。研究发现LLMs在特定社会群体上存在类似于“美丽即善”的广泛正面或负面态度的偏见。

    

    LLMs越来越强大并广泛用于辅助用户完成各种任务。这种使用可能会将LLM偏见引入到重要决策中，如招聘、人员绩效评估和刑事判决。在NLP系统中的性别和种族等方面的偏见已得到广泛研究，尤其是针对特定刻板印象的偏见（例如，亚洲人擅长数学）。在本文中，我们研究了一些较少研究但仍然重要的维度上的偏见，如年龄和美丽，在LLMs（特别是自回归语言模型）在社会群体和不相关的正负属性之间做出更微妙的相关决策。我们问LLMs是否对特定社会群体持有广泛的正面或负面态度的偏见，类似于实验心理学中人们发现的“美丽即善”的偏见。我们引入了一个模板生成的句子完成任务的数据集，要求模型选择最合适的属性。

    LLMs are increasingly powerful and widely used to assist users in a variety of tasks. This use risks the introduction of LLM biases to consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. Bias in NLP systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., Asians are good at math). In this paper, we investigate bias along less studied, but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that LLMs (specially autoregressive language models) make between social groups and unrelated positive and negative attributes. We ask whether LLMs hold wide-reaching biases of positive or negative sentiment for specific social groups similar to the ``what is beautiful is good'' bias found in people in experimental psychology. We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attrib
    
[^359]: 锚点：用更少的示例对模型进行基准测试

    Anchor Points: Benchmarking Models with Much Fewer Examples. (arXiv:2309.08638v1 [cs.CL])

    [http://arxiv.org/abs/2309.08638](http://arxiv.org/abs/2309.08638)

    这个论文介绍了一种使用更少的示例来对模型进行基准测试的方法，并提出了锚点选择技术来捕捉模型行为。实验证明，使用锚点对模型进行排序比使用均匀采样和其他基线方法更准确。仅使用几个锚点就可以估计模型对数据集中所有其他点的每个类别的预测，用于衡量模型性能。

    

    现代语言模型通常表现出强大但脆弱的行为，因此开发出更大、更多样化的基准来可靠地评估它们的行为。在这里，我们建议可以使用更小的评估集对模型性能进行基准测试和阐明。我们首先展示了在六个流行语言分类基准中，模型对许多点对的正确类别的置信度在各个模型之间具有强相关性。我们在此现象基础上提出了锚点选择技术，该技术可以选择捕捉整个数据集上的模型行为的小子集。锚点可靠地对模型进行排序：在87个不同的语言模型-提示对上，使用1-30个锚点评估模型在准确排序模型方面优于均匀采样和其他基线方法。此外，只需要几个锚点就可以用较低的平均绝对误差估计出模型对数据集中所有其他点的每个类别的预测，足以衡量模型在哪些方面表现得如何。

    Modern language models often exhibit powerful but brittle behavior, leading to the development of larger and more diverse benchmarks to reliably assess their behavior. Here, we suggest that model performance can be benchmarked and elucidated with much smaller evaluation sets. We first show that in six popular language classification benchmarks, model confidence in the correct class on many pairs of points is strongly correlated across models. We build upon this phenomenon to propose Anchor Point Selection, a technique to select small subsets of datasets that capture model behavior across the entire dataset. Anchor points reliably rank models: across 87 diverse language model-prompt pairs, evaluating models using 1-30 anchor points outperforms uniform sampling and other baselines at accurately ranking models. Moreover, just several anchor points can be used to estimate model per-class predictions on all other points in a dataset with low mean absolute error, sufficient for gauging where
    
[^360]: PROGrasp:实现物体抓取的人机交流

    PROGrasp: Pragmatic Human-Robot Communication for Object Grasping. (arXiv:2309.07759v1 [cs.CL])

    [http://arxiv.org/abs/2309.07759](http://arxiv.org/abs/2309.07759)

    PROGrasp是一个实现物体抓取的人机交流系统，通过使用面向意图的多模态对话和答案解释模块，机器人能够根据用户的意图来识别和抓取目标物体。

    

    交互式物体抓取(IOG)是通过人机自然语言交流识别和抓取目标物体的任务。当前IOG系统假定人类用户最初指定目标对象的类别(例如，瓶子)。受到语用学的启发，人类往往通过依赖上下文来传达意图以实现目标。我们引入了一项新的IOG任务，即实用IOG，并提出相应的数据集，即面向意图的多模态对话(IM-Dial)。在我们提出的任务场景中，首先给出一个面向意图的话语(例如，“我渴了”)。然后，机器人应通过与人类用户互动来识别目标对象。基于任务设置，我们提出了一个新的机器人系统，可以解释用户的意图并捡起目标对象，即实用物体抓取(PROGrasp)。PROGrasp通过结合视觉定位、问题提问、物体抓取以及最重要的，答案解释模块执行实用IOG。

    Interactive Object Grasping (IOG) is the task of identifying and grasping the desired object via human-robot natural language interaction. Current IOG systems assume that a human user initially specifies the target object's category (e.g., bottle). Inspired by pragmatics, where humans often convey their intentions by relying on context to achieve goals, we introduce a new IOG task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an intention-oriented utterance (e.g., "I am thirsty") is initially given to the robot. The robot should then identify the target object by interacting with a human user. Based on the task setup, we propose a new robotic system that can interpret the user's intention and pick up the target object, Pragmatic Object Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules for visual grounding, question asking, object grasping, and most importantly, answer interpre
    
[^361]: DePT:分解提示调整以实现参数高效微调

    DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05173](http://arxiv.org/abs/2309.05173)

    DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。

    

    提示调整（PT）是一种将可训练的少量软提示向量附加到语言模型（LM）输入中的参数高效微调（PEFT）方法，已在各种任务和模型中显示出了有希望的结果。 与其他PEFT方法相比，PT的竞争性能可以在可训练参数更少的情况下保持，并且随着模型规模的扩大，其参数并不会显著增加。 但是，PT引入了额外的软提示标记，导致输入序列变长，这对于Transformer的二次复杂度而言，在训练和推理时间以及内存使用方面会产生显著影响。 这对于面临大量每日查询的大型语言模型（LLMs）尤其令人担忧。

    Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
    
[^362]: 递归总结在大型语言模型中实现长期对话记忆

    Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models. (arXiv:2308.15022v1 [cs.CL])

    [http://arxiv.org/abs/2308.15022](http://arxiv.org/abs/2308.15022)

    递归总结在大型语言模型中实现长期对话记忆，可以提高对话系统在长对话中记忆重要信息的能力。

    

    大多数开放领域的对话系统在长期对话中容易遗忘重要信息。现有方法通常训练特定的检索器或总结器从过去获取关键信息，这需要耗费时间且高度依赖标记数据的质量。为了缓解这个问题，我们提出使用大型语言模型（LLMs）递归生成总结/记忆，以增强长期记忆能力。具体而言，我们的方法首先刺激LLMs记住小对话上下文，然后递归地使用之前的记忆和随后的对话内容产生新的记忆。最后，LLM可以在最新记忆的帮助下轻松生成高度一致的响应。我们使用ChatGPT和text-davinci-003进行评估，对广泛使用的公共数据集进行的实验证明我们的方法在长对话中可以生成更一致的响应。值得注意的是，我们的方法是实现LLM建模的潜在解决方案。

    Most open-domain dialogue systems suffer from forgetting important information, especially in a long-term conversation. Existing works usually train the specific retriever or summarizer to obtain key information from the past, which is time-consuming and highly depends on the quality of labeled data. To alleviate this problem, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the LLM can easily generate a highly consistent response with the help of the latest memory. We evaluate our method using ChatGPT and text-davinci-003, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Notably, our method is a potential solution to enable the LLM to model
    
[^363]: 探索大型语言模型用于知识图谱补全

    Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])

    [http://arxiv.org/abs/2308.13916](http://arxiv.org/abs/2308.13916)

    本文研究了利用大型语言模型（LLM）进行知识图谱补全的方法，并引入了一种创新的框架（知识图谱LLM），以提高三元组分类和关系预测的性能。

    

    知识图谱在众多人工智能任务中发挥着重要作用，但经常面临不完整性的问题。在本研究中，我们探索了利用大型语言模型（LLM）进行知识图谱补全的方法。我们将知识图谱中的三元组视为文本序列，并引入了一种创新的框架，称为知识图谱LLM（KG-LLM），来对这些三元组进行建模。我们的技术利用三元组的实体和关系描述作为提示，并利用响应进行预测。对各种基准知识图谱的实验表明，我们的方法在三元组分类和关系预测等任务中达到了最先进的性能。我们还发现，微调相对较小的模型（例如LLaMA-7B，ChatGLM-6B）优于最新的ChatGPT和GPT-4。

    Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
    
[^364]: 打破语言障碍：用于印地语和马拉地语的问答数据集

    Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi. (arXiv:2308.09862v1 [cs.CL])

    [http://arxiv.org/abs/2308.09862](http://arxiv.org/abs/2308.09862)

    本论文开发了一个用于印地语和马拉地语的问答数据集，通过翻译SQuAD 2.0数据集解决了数据稀缺问题，提供了这两种语言的最好表现模型。

    

    深度学习的最新进展导致了开发出高度复杂的系统，对数据有着无止境的需求。然而，对于低资源语言来说，构建良好的深度学习模型仍然是一个具有挑战性的任务。本文重点是为两种这样的语言-印地语和马拉地语-开发一个问答数据集。虽然印地语是全球第三大使用人数最多的语言，拥有3.45亿说话者，而马拉地语则是全球第11大使用人数最多的语言，拥有8.32千万说话者，但这两种语言在构建高效的问答系统的资源上都面临限制。为了解决数据稀缺的挑战，我们开发了一种新颖的方法来将SQuAD 2.0数据集翻译成印地语和马拉地语。我们发布了这两种语言中最大的问答数据集，每个数据集包含28,000个样本。我们在各种架构上评估了数据集，并发布了在印地语和马拉地语中表现最好的模型。

    The recent advances in deep-learning have led to the development of highly sophisticated systems with an unquenchable appetite for data. On the other hand, building good deep-learning models for low-resource languages remains a challenging task. This paper focuses on developing a Question Answering dataset for two such languages- Hindi and Marathi. Despite Hindi being the 3rd most spoken language worldwide, with 345 million speakers, and Marathi being the 11th most spoken language globally, with 83.2 million speakers, both languages face limited resources for building efficient Question Answering systems. To tackle the challenge of data scarcity, we have developed a novel approach for translating the SQuAD 2.0 dataset into Hindi and Marathi. We release the largest Question-Answering dataset available for these languages, with each dataset containing 28,000 samples. We evaluate the dataset on various architectures and release the best-performing models for both Hindi and Marathi, which 
    
[^365]: 在大型语言模型中使用反向推理进行验证

    Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])

    [http://arxiv.org/abs/2308.07758](http://arxiv.org/abs/2308.07758)

    本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。

    

    链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。

    Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
    
[^366]: OUTFOX: 基于上下文学习和对抗生成例子的LLM生成论文检测

    OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples. (arXiv:2307.11729v1 [cs.CL])

    [http://arxiv.org/abs/2307.11729](http://arxiv.org/abs/2307.11729)

    OUTFOX是一个新的框架，通过允许检测器和攻击者考虑彼此的输出，提高了LLM生成文本检测器的鲁棒性。攻击者利用检测器的预测标签作为示例进行上下文学习，并生成难以检测的对抗生成的论文。

    

    大型语言模型(LLMs)已经达到了与人类写作相当的流利程度，很难区分人类写作和LLM生成的文本。这增加了LLMs被误用的风险，并需要开发检测器来识别LLM生成的文本。然而，现有的检测器通过简单地改写LLM生成的文本来降低检测准确性。此外，这些检测器在学生在写作作业（如论文）中使用LLMs并迅速学会如何规避这些检测器的真实生活情况下的有效性尚未被探讨。在本文中，我们提出了OUTFOX，一个新的框架，通过允许检测器和攻击者考虑彼此的输出并将其应用于学生论文领域来提高LLM生成文本检测器的鲁棒性。在我们的框架中，攻击者使用检测器的预测标签作为上下文学习的示例，并对难以检测的对抗生成论文进行生成。

    Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, the effectiveness of these detectors in real-life situations, such as when students use LLMs for writing homework assignments (e.g., essays) and quickly learn how to evade these detectors, has not been explored. In this paper, we propose OUTFOX, a novel framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output and apply this to the domain of student essays. In our framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are hard
    
[^367]: 提示不应被视为秘密：系统地衡量提示提取攻击的成功性

    Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success. (arXiv:2307.06865v1 [cs.CL])

    [http://arxiv.org/abs/2307.06865](http://arxiv.org/abs/2307.06865)

    本论文提出了一个系统地衡量提示提取攻击成功的框架，并通过多个实验发现，即使提示被保密，简单的基于文本的攻击仍然可以高概率地揭示提示。

    

    大型语言模型的生成通常通过提示技术来控制，其中用户对模型的查询以旨在指导模型在该查询上的行为的提示作为前缀。公司用于指导其模型的提示通常被视为秘密，隐藏在查询的用户之外。它们甚至被视为可以买卖的商品。然而，有经验性的证据显示，即使提示被保密，用户仍然可以提取它们。在本文中，我们提出了一个系统地衡量提示提取攻击成功的框架。在使用多个提示源和多个基础语言模型的实验中，我们发现简单的基于文本的攻击实际上可以高概率地揭示提示。

    The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.
    
[^368]: 以提示为基础的个性化冷启动推荐的研究

    Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])

    [http://arxiv.org/abs/2306.17256](http://arxiv.org/abs/2306.17256)

    本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。

    

    推荐系统在根据用户过去的行为帮助用户发现与其兴趣相符的信息方面发挥着关键作用。然而，当用户和物品之间的历史交互记录不可用时，开发个性化推荐系统变得具有挑战性，这就是所谓的系统冷启动推荐问题。此问题在创业企业或用户参与历史不足的平台中尤为突出。以往的研究集中在用户或物品的冷启动场景，其中系统仍然通过在同一领域中的历史用户和物品交互进行训练来为新用户或物品提供推荐，而无法解决我们的问题。为了弥合这一鸿沟，我们的研究引入了一种创新且有效的方法，利用预训练语言模型的能力。我们将推荐过程转化为自然语言情感分析，其中包含用户资料和物品属性的信息。

    Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
    
[^369]: Med-UniC：通过减少偏见实现跨语言医学图像-语言预训练的统一

    Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias. (arXiv:2305.19894v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19894](http://arxiv.org/abs/2305.19894)

    Med-UniC是一个新的框架，旨在通过整合英语和西班牙语的跨语言医学数据，实现跨语言医学图像-语言预训练的统一。他们提出了跨语言文本对齐规则(CTR)，以明确统一来自不同语言社区的医学报告的跨语言语义表示。

    

    数据稀缺性对医学图像-语言预训练(VLP)的效果造成了严重障碍。解决方案可能在于结合来自各种语言社区的数据集。然而，主要挑战来自于整合不同的语法和语义、特定于语言的医学术语以及特定于文化的隐式知识的复杂性。因此，一个关键的考虑因素是由不同语言引起的社区偏见的存在。本文介绍了一种名为统一跨语言医学图像-语言预训练(Med-UniC)的新框架，旨在整合来自两种最常见语言的多模态医学数据，即英语和西班牙语。具体而言，我们提出了跨语言文本对齐规则(CTR)，明确统一来自不同语言社区的医学报告的跨语言语义表示。通过潜在语言解缠，优化CTR，使我们的优化成果。

    The scarcity of data presents a critical obstacle to the efficacy of medical visionlanguage pre-training (VLP). A potential solution lies in the combination of datasets from various language communities. Nevertheless, the main challenge stems from the complexity of integrating diverse syntax and semantics, language-specific medical terminology, and culture-specific implicit knowledge. Therefore, one crucial aspect to consider is the presence of community bias caused by different languages. This paper presents a novel framework named Unifying Cross-Lingual Medical Vision-Language Pre-Training (Med-UniC), designed to integrate multimodal medical data from the two most prevalent languages, English and Spanish. Specifically, we propose Cross-lingual Text Alignment Regularization (CTR) to explicitly unify cross-lingual semantic representations of medical reports originating from diverse language communities. CTR is optimized through latent language disentanglement, rendering our optimizatio
    
[^370]: 使用基于文献的语境化学习生成新的科学方向

    Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14259](http://arxiv.org/abs/2305.14259)

    本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。

    

    基于文献的发现（LBD）旨在通过挖掘论文并生成假设来发现新的科学知识。标准的LBD仅限于预测离散概念之间的两两关系（例如，药物和疾病的关联）。LBD还忽略了关键的上下文，例如实验设置（例如，药物评估的特定患者群体）和人类科学家考虑的背景知识和动机（例如，找到没有特定副作用的药物候选）。我们通过一种新颖的上下文化LBD（C-LBD）表述来解决这些局限性：以自然语言生成科学假设，同时将它们联系到控制假设搜索空间的上下文中。我们提出了一个建模框架，使用获得的引文和知识图关系的异构网络中的“灵感”，并创建了一个从论文中派生的新数据集。我们使用强大的大型语言模型（LLM）进行评估，发现GPT4倾向于生成具有创新性的思想。

    Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
    
[^371]: LogicLLM：探索自监督逻辑增强训练的大语言模型

    LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. (arXiv:2305.13718v1 [cs.CL])

    [http://arxiv.org/abs/2305.13718](http://arxiv.org/abs/2305.13718)

    本文介绍了 LogicLLM，一种通过自监督后训练来提高大语言模型的逻辑推理能力的方法，该方法有效地在常见逻辑推理任务上进行表现，超过了目前最先进的无监督基线方法。

    

    改善语言模型的逻辑推理能力的现有努力主要依赖于有监督微调，这阻碍了将模型泛化到新的领域和/或任务。然而，通过发展大语言模型（LLM）已经证明了将丰富的知识压缩为单个代理的能力，使它们能够有效地处理多个任务。然而，我们的初步实验表明，LLMs 在逻辑推理方面并没有表现出能力。LLMs 在逻辑推理基准测试中的表现远远落后于现有的最先进基线。在本文中，我们首次尝试通过自监督后训练来探索融合逻辑知识的可行性，并通过上下文学习来激活它，我们将其称为LogicLLM。具体来说，我们设计了一个MERIt 的自回归目标变体，并将其与两个LLM系列FLAN-T5和LLaMA集成在一起，参数大小范围从30亿到130亿。实验结果表明，我们的方法在常用推理策略上与目前最先进的有监督方法相当，并且远远超过了目前最先进的无监督基线方法。

    Existing efforts to improve logical reasoning ability of language models have predominantly relied on supervised fine-tuning, hindering generalization to new domains and/or tasks. The development of Large Langauge Models (LLMs) has demonstrated the capacity of compressing abundant knowledge into a single proxy, enabling them to tackle multiple tasks effectively. Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines. In this paper, we make the first attempt to investigate the feasibility of incorporating logical knowledge through self-supervised post-training, and activating it via in-context learning, which we termed as LogicLLM. Specifically, we devise an auto-regressive objective variant of MERIt and integrate it with two LLM series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to 13 billion. The results 
    
[^372]: GPT4Table：大型语言模型能理解结构化表格数据吗？一项基准测试和实证研究

    GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13062](http://arxiv.org/abs/2305.13062)

    本文设计了一个基准测试来评估大型语言模型（LLMs）对结构化表格数据的理解能力，并发现不同的输入选择会对性能产生影响。在基准测试的基础上，提出了“自我增强”技术以改善理解能力。

    

    大型语言模型（LLMs）作为少样本推理器来解决与自然语言相关的任务越来越具吸引力。然而，关于LLMs对结构化数据（例如表格）的理解程度还有很多需要学习的地方。尽管可以使用表格序列化作为LLMs的输入，但目前还缺乏对LLMs是否真正能够理解这类数据的全面研究。本文通过设计一个基准测试来评估LLMs的结构理解能力（SUC）来解决这个问题。我们创建的基准测试包括七个任务，每个任务都有其独特的挑战，例如单元格查找、行检索和大小检测。我们对GPT-3.5和GPT-4进行了一系列评估。我们发现性能因多种输入选择而异，包括表格输入格式、内容顺序、角色提示和分区标记等。根据基准测试评估所得的见解，我们提出了“自我增强”技术以改善性能。

    Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
    
[^373]: EventNet-ITA: 用于意大利事件的框架解析

    EventNet-ITA: Italian Frame Parsing for Events. (arXiv:2305.10892v1 [cs.CL])

    [http://arxiv.org/abs/2305.10892](http://arxiv.org/abs/2305.10892)

    本文介绍了一个用于意大利语的大规模、多领域事件框架标注语料库-EventNet-ITA，并提出了一种高效的多标签框架解析方法。EventNet-ITA的主要贡献是为研究人员提供了一个用于文本事件挖掘的资源和意大利语框架解析的新颖工具。

    

    本文介绍了EventNet-ITA，这是一个用于意大利语的大规模、多领域事件框架标注语料库，并提出了一种高效的多标签框架解析方法。然后在该数据集上进行了评估。涵盖了各种个人、社会和历史现象，EventNet-ITA的主要贡献是为研究人员提供了一个用于文本事件挖掘的资源以及意大利语框架解析的新颖而广泛的工具。

    This paper introduces EventNet-ITA, a large, multi-domain corpus annotated with event frames for Italian, and presents an efficient approach for multi-label Frame Parsing. The approach is then evaluated on the dataset. Covering a wide range of individual, social and historical phenomena, the main contribution of EventNet-ITA is to provide the research community with a resource for textual event mining and a novel and extensive tool for Frame Parsing in Italian.
    
[^374]: 分析FOMC会议记录：语言模型的准确性和限制。

    Analyzing FOMC Minutes: Accuracy and Constraints of Language Models. (arXiv:2304.10164v1 [cs.CL])

    [http://arxiv.org/abs/2304.10164](http://arxiv.org/abs/2304.10164)

    该研究分析了FOMC官方声明中使用的语言，采用VADER和FinBERT等模型预测负面情绪，结果显示FinBERT表现相对更好。但是，该研究也强调了使用当前NLP技术分析FOMC文本的挑战和限制，建议增强语言模型并探索替代方法。

    

    本研究论文分析了联邦公开市场委员会（FOMC）在其定期会议后发布的官方声明中使用的语言，以获取有关FOMC官方声明对金融市场和经济预测的影响的见解。研究发现，FOMC小心避免在句子中表达情感，并遵循一套模板来覆盖经济情况。该分析采用了VADER和FinBERT等先进的语言建模技术，以及使用GPT-4的试验测试。结果表明，在准确预测负面情绪方面，FinBERT的表现优于其他技术。然而，研究还强调了使用当前NLP技术分析FOMC文本的挑战和限制，并建议增强语言模型并探索替代方法的潜力。

    This research article analyzes the language used in the official statements released by the Federal Open Market Committee (FOMC) after its scheduled meetings to gain insights into the impact of FOMC official statements on financial markets and economic forecasting. The study reveals that the FOMC is careful to avoid expressing emotion in their sentences and follows a set of templates to cover economic situations. The analysis employs advanced language modeling techniques such as VADER and FinBERT, and a trial test with GPT-4. The results show that FinBERT outperforms other techniques in predicting negative sentiment accurately. However, the study also highlights the challenges and limitations of using current NLP techniques to analyze FOMC texts and suggests the potential for enhancing language models and exploring alternative approaches.
    
[^375]: ChatGPT是一个好的情感分析器吗？一项初步研究。

    Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. (arXiv:2304.04339v1 [cs.CL])

    [http://arxiv.org/abs/2304.04339](http://arxiv.org/abs/2304.04339)

    本文对ChatGPT作为情感分析器进行了初步评估，包括标准评估、极性转移评估、开放域评估和情感推理评估，共涉及18个数据集和5个情感分析任务。与经过微调的BERT和最先进的模型进行了对比，并进行了人工评估和案例研究。

    

    最近，ChatGPT在研究和公众的关注下受到了极大的关注。我们特别想知道它是否可以作为通用情感分析器。为此，在这项工作中，我们对ChatGPT在文本中包含的意见、情感和情绪的理解进行了初步评估。具体而言，我们在四个设置下进行评估，包括标准评估、极性转移评估、开放域评估和情感推理评估。以上评估涉及18个基准数据集和5个代表性情感分析任务，我们将ChatGPT与经过微调的BERT和相应的最先进模型进行了对比，并在末端任务上进行了评估。此外，我们还进行了人工评估，并展示了一些定性案例研究以深入理解其情感分析能力。

    Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly curious about whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text. Specifically, we evaluate it in four settings, including standard evaluation, polarity shift evaluation, open-domain evaluation, and sentiment inference evaluation. The above evaluation involves 18 benchmark datasets and 5 representative sentiment analysis tasks, and we compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task. Moreover, we also conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.
    
[^376]: KPEval：面向细粒度语义评估关键词提取和生成系统

    KPEval: Towards Fine-grained Semantic-based Evaluation of Keyphrase Extraction and Generation Systems. (arXiv:2303.15422v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.15422](http://arxiv.org/abs/2303.15422)

    KPEval是一个综合评估框架，旨在解决关键词提取和生成系统评估中的短板。通过引入四个关键维度，包括显著性、忠实性、多样性和实用性，并设计相应的语义度量指标，KPEval在与人类偏好相关性方面表现更好。使用该框架重新评估了20个关键词系统，并发现模型选择最佳的情况取决于评估维度，实用性是一个重要因素。

    

    尽管关键词提取和生成方法取得了显著的进展，但现行评估方法仅依赖于与人工参考的完全匹配，而忽略了无参考属性。这种方式无法识别生成与参考语义等效或具有实际效用的多样化关键词的系统。为了更好地评估关键词系统的能力，我们提出了一个全面的评估框架KPEval，包含四个关键维度：显著性、忠实性、多样性和实用性。对于每个维度，我们设计了与评估目标相一致的基于语义的度量指标。元评估研究表明，与之前使用的一系列度量指标相比，我们的评估策略更好地与人类偏好相关。使用这个框架，我们重新评估了20个关键词系统，并进一步发现：(1)最好的模型根据评估维度不同而不同；(2)实用性是关键词系统的一个重要方面。

    Despite the significant advancements in keyphrase extraction and keyphrase generation methods, the predominant approach for evaluation only relies on exact matching with human references and disregards reference-free attributes. This scheme fails to recognize systems that generate keyphrases semantically equivalent to the references or diverse keyphrases that carry practical utility. To better assess the capability of keyphrase systems, we propose KPEval, a comprehensive evaluation framework consisting of four critical dimensions: saliency, faithfulness, diversity, and utility. For each dimension, we design semantic-based metrics that align with the evaluation objectives. Meta-evaluation studies demonstrate that our evaluation strategy correlates better with human preferences compared to a range of previously used metrics. Using this framework, we re-evaluate 20 keyphrase systems and further discover that (1) the best model differs depending on the evaluation dimension; (2) the utility
    
[^377]: AI生成的文本是否可靠地检测出来？

    Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.11156](http://arxiv.org/abs/2303.11156)

    本研究通过实证和理论分析表明，在实际场景中，几种AI文本检测器不可靠。改写攻击可以破解多种检测器，包括水印方案、神经网络检测器和零样本分类器。即使是最好的检测器，随着语言模型的进一步提升，性能也会下降。因此，AI生成的文本的可靠检测仍然是一个挑战。

    

    本文从实证和理论两个方面表明，在实际场景中，几种AI文本检测器并不可靠。从实践上来说，我们证明了轻量级的改写器应用在大型语言模型（LLM）上可以破解一系列的检测器，包括使用水印方案、神经网络检测器和零样本分类器。我们的实验表明，旨在躲避改写攻击的基于检索的检测器仍然容易受到递归改写的攻击。然后，我们提出了一个理论上的不可能结果，指出随着语言模型变得越来越复杂和更擅长模仿人类文本，在最好的检测器性能会下降。对于一个足够先进的语言模型来模仿人类文本，即使最佳的检测器的表现只比随机分类器好上一点点。我们的结果足够概括特定的场景，如改写攻击。

    In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as par
    

