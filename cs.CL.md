# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore.](http://arxiv.org/abs/2308.04430) | SILO是一种新的语言模型，通过在推理过程中对非参数化的数据存储进行查询，实现在面临法律风险和模型性能之间的权衡，并支持数据归属和数据生产者退出模型的功能。 |
| [^2] | [A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition.](http://arxiv.org/abs/2308.04424) | 我们提出了一种双向多跳推理模型，用于联合对话情感分类和行为识别。该模型能够全面理解对话上下文，并显式建模情感和行为标签之间的关联，从而提取丰富的情感和行为线索，实现有效且准确的推理。 |
| [^3] | [Legal Summarisation through LLMs: The PRODIGIT Project.](http://arxiv.org/abs/2308.04416) | PRODIGIT项目旨在通过数字技术支持税务法官和律师，特别是通过使用LLMs和GPT4进行法律摘要生成和相关信息提取的方法得到令人满意的结果。 |
| [^4] | [Character-level NMT and language similarity.](http://arxiv.org/abs/2308.04398) | 本研究探索了字符级神经机器翻译在不同语言相似性和数据集大小上的有效性，通过对捷克语和克罗地亚语、德语、匈牙利语、斯洛伐克语和西班牙语的翻译进行评估，发现对于相似语言，字符级输入分割有益处，而对于关联性较小的语言，字符级模型落后于子词级模型，但可以通过字符级微调来弥补差距。 |
| [^5] | [Learning Evaluation Models from Large Language Models for Sequence Generation.](http://arxiv.org/abs/2308.04386) | 本文提出了一种评估能力转移方法（ECT），可以将大型语言模型的评估能力转移到相对轻量级的语言模型上，提高序列生成模型的性能。 |
| [^6] | [Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles.](http://arxiv.org/abs/2308.04346) | 通过人类评估方法，我们研究了自然语言处理模型中的国籍偏见。研究发现，有偏见的模型会复制和放大现有的社会偏见，从而可能造成伤害。 |
| [^7] | [Towards an AI to Win Ghana's National Science and Maths Quiz.](http://arxiv.org/abs/2308.04333) | 本文介绍了一项开源项目，该项目旨在建立一种人工智能，参加并赢得加纳国家科学与数学竞赛。该人工智能的成功可以在教育领域产生真实的影响。 |
| [^8] | [Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review.](http://arxiv.org/abs/2308.04306) | 本文对基于深度学习的隐喻识别任务中知识注入的研究进展进行了全面综述，包括主流知识和知识注入原则的总结、数据集、评估指标和基准模型的回顾，并探讨了当前的知识注入问题。 |
| [^9] | [Comparative Analysis of the wav2vec 2.0 Feature Extractor.](http://arxiv.org/abs/2308.04286) | 本研究比较了wav2vec 2.0模型的特征提取器与传统特征提取方法在连接主义时间分类ASR模型上的表现，发现两者在LibriSpeech基准测试上具有竞争力。通过分析学习到的滤波器，发现ASR系统最重要的信息是通过一组带通滤波器获得的。 |
| [^10] | [In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning.](http://arxiv.org/abs/2308.04275) | 本文提出了一种在微调之前与纯净语言模型进行对话的方法，通过上下文学习实现了推理时的对齐。实验证明，这种方法将纯净语言模型的胜率提高了7倍，使其可以与通过对齐微调的强基准模型媲美。 |
| [^11] | [CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages.](http://arxiv.org/abs/2308.04255) | CLASSLA-Stanza是一个为南斯拉夫语言提供自动语言注释的流水线，相对于Stanza，在性能和功能上有多个改进，并取得了始终如一的高性能。 |
| [^12] | [Gloss Alignment Using Word Embeddings.](http://arxiv.org/abs/2308.04248) | 这篇论文提出了一种使用大型口语语言模型对定位结果与字幕进行对齐的方法，解决了手语翻译数据中字幕和手势不匹配的问题。 |
| [^13] | [Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance.](http://arxiv.org/abs/2308.04215) | 提出了一种混合检索增强生成的框架，通过将云模型的检索增强内存整合到客户端模型中，实现实时响应的作曲辅助。 |
| [^14] | [Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: "Are we on the same page ?".](http://arxiv.org/abs/2308.04180) | 本研究通过不同的视角研究在线文本中的社交不可接受言论（SUD）分类和检测，并建立了一个新颖的语料库。通过分析不同背景下的SUD分类器的泛化能力以及注释模态对SUD学习的影响，我们提出了一些开放的挑战和研究方向，并提供了有助于领域专家在注释任务中的数据洞察。 |
| [^15] | [On Monotonic Aggregation for Open-domain QA.](http://arxiv.org/abs/2308.04176) | 这篇论文提出了一个称为Judge-Specialist框架的方法，用于解决开放领域问答中出现的单调性问题。该框架包括专家主题检索器/阅读器和一个评判者语言模型，能够确保单调性并在自然问题上表现出色。 |
| [^16] | [Large Language Model Prompt Chaining for Long Legal Document Classification.](http://arxiv.org/abs/2308.04138) | 本研究利用提示链的方法在处理复杂的法律文件分类任务时取得了成功，通过创建简洁摘要、语义搜索相关示例文本和利用上下文学习进行提示，可以提高零样本任务的性能并超过较大模型的得分。 |
| [^17] | [Social Media, Topic Modeling and Sentiment Analysis in Municipal Decision Support.](http://arxiv.org/abs/2308.04124) | 本文介绍了一个针对市政决策的社交媒体处理框架，它能够对社交媒体帖子进行情感分析和话题建模，并将这些信息综合起来，以表示对每个话题的整体情感。在奥斯特拉的推文上的应用示例说明了该框架的效果。 |
| [^18] | [Collective Human Opinions in Semantic Textual Similarity.](http://arxiv.org/abs/2308.04114) | 该论文介绍了USTS数据集，旨在研究语义文本相似度中的集体人类观点。分析表明现有的STS模型无法捕捉个别实例上由人类不一致带来的变异。 |
| [^19] | [I-WAS: a Data Augmentation Method with GPT-2 for Simile Detection.](http://arxiv.org/abs/2308.04109) | 我们提出了一种基于GPT-2的比喻数据增强方法I-WAS，通过替换词汇和完成句子的方式来增强句子质量，实验结果证明其在比喻检测中的有效性。 |
| [^20] | [DataTales: Investigating the use of Large Language Models for Authoring Data-Driven Articles.](http://arxiv.org/abs/2308.04076) | DataTales研究探索了使用大型语言模型(LLMs)辅助撰写数据驱动文章的方法，并设计了一个原型系统来生成数据驱动文章的文本叙述。通过定性研究，发现LLMs可以作为有价值的数据驱动文章撰写助手进一步整合。 |
| [^21] | [The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings.](http://arxiv.org/abs/2308.04052) | 五美元模型是一种轻量级的文本到图像生成架构，可以从编码的文本提示中生成低维度的图片，并在有限数据集上保持语义含义。 |
| [^22] | [InfeRE: Step-by-Step Regex Generation via Chain of Inference.](http://arxiv.org/abs/2308.04041) | InfeRE是一种通过推理链逐步生成正则表达式的新方法，它考虑了生成最终表达式背后的逐步内部文本匹配过程，并引入了自一致性解码机制来提高鲁棒性。实验结果表明，InfeRE在生成正则表达式方面取得了显著的改进。 |
| [^23] | [A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset.](http://arxiv.org/abs/2308.04037) | 该论文比较研究了TF-IDF特征加权方法并使用非结构化数据集进行了分析，结果发现相比于N-Gram，使用TF-IDF特征可以显著提高特征提取效果。 |
| [^24] | [Top K Relevant Passage Retrieval for Biomedical Question Answering.](http://arxiv.org/abs/2308.04028) | 这篇论文提出了一种用于生物医学问题回答的Top K相关段落检索方法，传统的稀疏向量空间模型不适用于这个任务。然而，对于临床领域来说，这个问题还没有得到很好的解决。 |
| [^25] | [Continual Pre-Training of Large Language Models: How to (re)warm your model?.](http://arxiv.org/abs/2308.04014) | 该论文研究了大型语言模型的持续预训练问题，探讨了热启动策略对于解决分布变化和提高计算效率的影响。 |
| [^26] | [SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool.](http://arxiv.org/abs/2308.03983) | SimplyRetrieve是一款私密且轻量级的生成型AI工具，使用了基于检索的生成方法，通过无需额外模型微调将私有数据集成进生成型AI系统，并提供了一个本地化、轻量级和用户友好的界面，以方便用户探索RCG在提升生成型AI性能方面的潜力。 |
| [^27] | [Simple synthetic data reduces sycophancy in large language models.](http://arxiv.org/abs/2308.03958) | 本文研究了语言模型中阿谀奉承行为的普遍性，并提出了一个简单的合成数据干预方法来减少这种行为。 |
| [^28] | [Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links.](http://arxiv.org/abs/2308.03929) | 本研究通过构建基于本体的知识图谱，利用疾病本体和症状本体构建数学模型，利用事实核查算法和网络中心度指标分析ChatGPT生成的文本与真实医学文献之间的准确性，以验证疾病-症状关系。 |
| [^29] | [Universal Automatic Phonetic Transcription into the International Phonetic Alphabet.](http://arxiv.org/abs/2308.03917) | 本文提出了一种最先进的模型，用于将任何语言的语音转写成国际音标（IPA），部分自动化这个过程有潜力极大加快濒危语言的记录速度。虽然使用的训练数据集比之前的模型小，但质量更高并达到了相对较好的结果，还展示了模型的质量接近人类注释者的水平。 |
| [^30] | [Intelligent Assistant Language Understanding On Device.](http://arxiv.org/abs/2308.03905) | 本论文描述了一种在设备上运行的自然语言理解系统的设计，相比于基于服务器的助手，该系统更加私密、可靠、快速、表达更强、准确性更高。通过分享实践经验，为研究界的未来工作提供参考。 |
| [^31] | [A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction.](http://arxiv.org/abs/2308.03891) | 本研究进行了跨领域的评估，比较了因果知识提取的三个序列标注模型和基于跨度的提取方法。实验结果表明，预训练语言模型的词嵌入在这个任务中表现出了较好的性能，同时基于跨度的模型在不同类型的数据集上表现更好。 |
| [^32] | [Generative Benchmark Creation for Table Union Search.](http://arxiv.org/abs/2308.03883) | 采用生成型AI模型为表联合搜索创建结构化数据基准 |
| [^33] | [Semantic Equivalence of e-Commerce Queries.](http://arxiv.org/abs/2308.03869) | 本文介绍了一种识别和利用电子商务查询等价性的框架，以提升搜索者和商业结果。该框架解决了将查询映射为搜索意图向量表示、识别等价或相似意图的最近邻查询以及优化用户或商业目标等三个关键问题。通过表面相似性和行为相似性来确定查询的等价性。 |
| [^34] | [Trusting Language Models in Education.](http://arxiv.org/abs/2308.03866) | 本研究在教育中使用语言模型，提出了使用XGBoost和BERT的结合来校准语言模型的置信度，通过基于注意力机制的特征来输出校正的概率。 |
| [^35] | [Storyfier: Exploring Vocabulary Learning Support with Text Generation Models.](http://arxiv.org/abs/2308.03864) | Storyfier利用文本生成模型为学习者提供了有助于记忆词汇的生成故事，并提供适应性的AI辅助撰写新故事。然而，在实验中，使用Storyfier的学习者在回忆和使用目标词方面表现较差。 |
| [^36] | [Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models.](http://arxiv.org/abs/2308.03853) | 本研究开发了一个详细的肿瘤学信息注释方案，使用大型语言模型从肿瘤学笔记中提取和推理复杂的修辞，并应用于乳腺癌进展笔记的语料库。 |
| [^37] | [Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach.](http://arxiv.org/abs/2308.03800) | 该论文提出了一种深度学习方法，用于分析金融欺诈文本并进行分类。通过比较多种神经网络模型的准确性，该研究对金融欺诈检测具有重要意义 |
| [^38] | [Forget Demonstrations, Focus on Learning from Textual Instructions.](http://arxiv.org/abs/2308.03795) | 本研究提出了一种从文本指令中学习任务的方法，通过自动找出定义中的关键句子和使用排名目标，实现了最先进的性能。 |
| [^39] | [Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction.](http://arxiv.org/abs/2308.03782) | 该研究比较了Bio+Clinical BERT、BERT Base和CNN在预测药物评价满意度方面的性能，结果显示医学领域特定的Bio+Clinical BERT模型在整体性能上优于通用领域的BERT Base模型，提高了11%的Macro F1和召回率得分。 |
| [^40] | [Enhancing image captioning with depth information using a Transformer-based framework.](http://arxiv.org/abs/2308.03767) | 本研究提出了一个基于Transformer的框架，将深度信息与RGB图像结合，以增强图像字幕生成任务。通过使用多句字幕描述3D场景，我们的框架能够更好地理解输入场景，并在不同的融合方法下进行实验验证。 |
| [^41] | [GPT-4 Can't Reason.](http://arxiv.org/abs/2308.03762) | GPT-4在推理方面无能为力，尽管有着偶尔显示的分析才智。 |
| [^42] | [MedMine: Examining Pre-trained Language Models on Medication Mining.](http://arxiv.org/abs/2308.03629) | MedMine通过检验预训练语言模型在药物挖掘中的应用，发现了它们在不同实体类型和临床事件上的不平衡表现，并提供了解决这些问题的研究方向。 |
| [^43] | [Topological Interpretations of GPT-3.](http://arxiv.org/abs/2308.03565) | 本文研究了GPT-3的拓扑解释方法，通过计算句子向量的距离和相关性，揭示了不同嵌入空间中句子之间的关联性。 |
| [^44] | [RecycleGPT: An Autoregressive Language Model with Recyclable Module.](http://arxiv.org/abs/2308.03421) | RecycleGPT是一种生成式语言模型，通过回收预生成的模型状态而无需多次运行整个模型，并且证明了其降低推理延迟的有效性，实现高速解码和保持高性能。 |
| [^45] | [Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation.](http://arxiv.org/abs/2308.03131) | 本论文提出了使用多个参考来增强匹配指标与人类评估之间的一致性。在WMT Metrics基准中，多参考F200spBLEU相对于传统的单参考方法准确率提高了7.2\%，超过了基于神经网络的BERTscore的3.9\%的准确率提高。此外，该方法还可以在很大程度上缓解大型语言模型中的数据泄漏问题。 |
| [^46] | [Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting.](http://arxiv.org/abs/2308.02582) | 该论文介绍了一种通过领域适应和最少到最多提示的方式实现文本到SQL的高效泛化的方法。通过离线抽样获取少量样本，并合成一个通用提示，避免了昂贵的测试时间样本检索，并通过自适应和分解的方法更好地处理跨领域和跨组合式的泛化。 |
| [^47] | [Federated Representation Learning for Automatic Speech Recognition.](http://arxiv.org/abs/2308.02013) | 使用联邦学习和自监督学习的结合方法，研究了面向自动语音识别的联邦表示学习，将边缘设备上的隐私数据用于学习健壮的音频表示，并取得了显著的性能改善。 |
| [^48] | [NBIAS: A Natural Language Processing Framework for Bias Identification in Text.](http://arxiv.org/abs/2308.01681) | 本论文提出了一个名为NBIAS的自然语言处理框架，旨在识别文本中的偏见。通过收集来自社交媒体、医疗保健和职位招聘等领域的多样化数据构建数据集，并应用基于Transformer的令牌分类模型来识别偏见词/短语。通过定量和定性评估方法来评估模型的效果。 |
| [^49] | [Gzip versus bag-of-words for text classification with KNN.](http://arxiv.org/abs/2307.15002) | Gzip与KNN相比较在文本分类中，我们发现通过简单的词袋匹配可以获得类似或更好的准确性，并且更加高效。 |
| [^50] | [Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts.](http://arxiv.org/abs/2307.11661) | 本文展示了如何利用GPT-4生成具有视觉描述性的文本，并将其用于适应CLIP到下游任务。与CLIP的默认提示相比，在专门细粒度数据集上显示了较大的0-shot迁移准确性改进。 |
| [^51] | [Improving Pre-trained Language Models' Generalization.](http://arxiv.org/abs/2307.10457) | 该研究提出了一种名为Mask-tuning的训练方法，通过将Masked Language Modeling (MLM)训练目标整合到微调过程中来增强预训练语言模型（PLMs）的泛化能力。实验证明，Mask-tuning在非分布数据集上超过了当前最先进的技术，并提高了PLMs在分布数据集上的性能。 |
| [^52] | [Generating Mathematical Derivations with Large Language Models.](http://arxiv.org/abs/2307.09998) | 本文利用大型语言模型生成数学导出，分析了微调模型对未见符号和方程结构更改的敏感性，结果表明微调的FLAN-T5-large（MathT5）在各个测试集上的绝对性能优于GPT模型。 |
| [^53] | [Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models.](http://arxiv.org/abs/2307.06713) | 本文提出了一种使用大型语言模型进行文本分类的无监督校准方法，通过调整先验类别分布，实现在没有标记样本和仅少量领域内样本查询的情况下执行任务。 |
| [^54] | [Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views.](http://arxiv.org/abs/2306.09841) | 本文评估了大型语言模型的逻辑推理能力，选择了15个典型数据集，考虑了演绎、归纳、阿布达斯和混合推理形式，并选择了三个代表性的LLMs进行零样本、一次和三次的设置下评估。提出精细级别的评估方法。 |
| [^55] | [GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition.](http://arxiv.org/abs/2306.07848) | 本文提出了GEmo-CLAP模型用于语音情感识别，结合了性别属性信息，相比于其他先进方法，该模型在IEMOCAP上实现了更优越的识别性能。 |
| [^56] | [Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs.](http://arxiv.org/abs/2306.02864) | LLMs在公共事务文件分类中的表现进行了分析。通过使用正则表达式工具收集了一个包含超过33K个样本和22.5M个标记的公共事务文件数据库。实验结果表明，LLMs可以有效处理和理解这类文件中使用的复杂语言，从而提高了公共事务文件的分析能力。 |
| [^57] | [Speech Separation based on Contrastive Learning and Deep Modularization.](http://arxiv.org/abs/2305.10652) | 本文提出了一种基于对比学习和深度模块化的完全无监督语音分离方法，解决了有监督学习中存在的排列问题、说话人数量不匹配的问题和高质量标记数据的依赖问题。 |
| [^58] | [Whats New? Identifying the Unfolding of New Events in Narratives.](http://arxiv.org/abs/2302.07748) | 本文提出了一个新的挑战性任务：自动识别叙述中的新事件，以识别创新和贡献。他们将事件定义为主语、谓语和宾语的三元组，并将其分类为新事件，具体取决于其是否可以通过常识推理来推导。 |
| [^59] | [A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and Future Trends.](http://arxiv.org/abs/2302.03512) | 本研究对阿拉伯命名实体识别（NER）的发展进行了全面回顾，尤其关注深度学习和预训练语言模型方面的最新进展。该调查总结了传统的NER方法和近年来基于深度学习的方法，并介绍了阿拉伯NER的背景和现有资源。 |
| [^60] | [A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models.](http://arxiv.org/abs/2201.05337) | 这篇论文综述了基于Transformer预训练语言模型的可控文本生成方法。这些方法利用大规模预训练语言模型生成多样化、流畅的文本，但由于深度神经网络的可解释性较低，需要保证其可控性。 |

# 详细

[^1]: SILO语言模型：在非参数化数据存储中隔离法律风险

    SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. (arXiv:2308.04430v1 [cs.CL])

    [http://arxiv.org/abs/2308.04430](http://arxiv.org/abs/2308.04430)

    SILO是一种新的语言模型，通过在推理过程中对非参数化的数据存储进行查询，实现在面临法律风险和模型性能之间的权衡，并支持数据归属和数据生产者退出模型的功能。

    

    在对将语言模型（LMs）训练在受版权或受其他限制的数据上的合法性进行激烈辩论的同时，我们展示了仅在低风险文本（例如过期版权图书或政府文件）上训练时，模型性能显著下降的问题，原因是该文本的规模和领域覆盖有限。我们提出了SILO，一种新的语言模型，在推理过程中管理这种风险-性能权衡。SILO通过以下方式构建：（1）在我们策划的新语料库“开放许可证语料库”（OLC）上训练参数化的LM，该语料库包含228B个公共领域和许可文本。（2）通过非参数化的数据存储（例如包含受版权保护的图书或新闻的数据）对其进行扩充，该数据存储仅在推理过程中被查询。该数据存储允许使用高风险数据而无需对其进行训练，支持句级数据归属，并使数据生产者可以通过从存储中删除内容来选择退出模型。这些功能可以促进对数据使用规范的遵循。

    The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use
    
[^2]: 双向多跳推理模型用于联合对话情感分类和行为识别

    A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition. (arXiv:2308.04424v1 [cs.CL])

    [http://arxiv.org/abs/2308.04424](http://arxiv.org/abs/2308.04424)

    我们提出了一种双向多跳推理模型，用于联合对话情感分类和行为识别。该模型能够全面理解对话上下文，并显式建模情感和行为标签之间的关联，从而提取丰富的情感和行为线索，实现有效且准确的推理。

    

    对话情感分类（DSC）和行为识别（DAR）的联合任务旨在同时预测对话中每个话语的情感标签和行为标签。然而，当前的方法只能单向编码对话上下文，限制了其对上下文的全面理解能力。此外，这些方法忽视了情感和行为标签之间的显式关联，导致对丰富的情感和行为线索的获取能力不足，从而阻碍了有效和准确的推理。为了解决这些问题，我们提出了一种双向多跳推理模型（BMIM），它利用特征选择网络和双向多跳推理网络来迭代地提取和整合丰富的情感和行为线索。我们还采用对比学习和双学习来明确建模情感和行为标签之间的关联。我们在两个广泛使用的数据集上的实验证明，BMIM的性能优于当前方法。

    The joint task of Dialog Sentiment Classification (DSC) and Act Recognition (DAR) aims to predict the sentiment label and act label for each utterance in a dialog simultaneously. However, current methods encode the dialog context in only one direction, which limits their ability to thoroughly comprehend the context. Moreover, these methods overlook the explicit correlations between sentiment and act labels, which leads to an insufficient ability to capture rich sentiment and act clues and hinders effective and accurate reasoning. To address these issues, we propose a Bi-directional Multi-hop Inference Model (BMIM) that leverages a feature selection network and a bi-directional multi-hop inference network to iteratively extract and integrate rich sentiment and act clues in a bi-directional manner. We also employ contrastive learning and dual learning to explicitly model the correlations of sentiment and act labels. Our experiments on two widely-used datasets show that BMIM outperforms s
    
[^3]: LLMs背景下的法律摘要：PRODIGIT项目

    Legal Summarisation through LLMs: The PRODIGIT Project. (arXiv:2308.04416v1 [cs.CL])

    [http://arxiv.org/abs/2308.04416](http://arxiv.org/abs/2308.04416)

    PRODIGIT项目旨在通过数字技术支持税务法官和律师，特别是通过使用LLMs和GPT4进行法律摘要生成和相关信息提取的方法得到令人满意的结果。

    

    我们介绍了一个名为PRODIGIT的大型意大利项目的初步结果，该项目旨在通过数字技术（尤其是人工智能）支持税务法官和律师。我们关注的是法官决策的摘要生成以及相关信息的提取，如法律问题和决策标准的确定，以及关键词的规定。为此，我们使用了不同的工具和方法来进行摘要提取和抽象化。我们应用了LLMs，特别是GPT4，根据专业税务法官和律师的评估，我们得到了令人满意的结果。基于这个基础，正在建设一个原型应用，将公开提供。

    We present some initial results of a large-scale Italian project called PRODIGIT which aims to support tax judges and lawyers through digital technology, focusing on AI. We have focused on generation of summaries of judicial decisions and on the extraction of related information, such as the identification of legal issues and decision-making criteria, and the specification of keywords. To this end, we have deployed and evaluated different tools and approaches to extractive and abstractive summarisation. We have applied LLMs, and particularly on GPT4, which has enabled us to obtain results that proved satisfactory, according to an evaluation by expert tax judges and lawyers. On this basis, a prototype application is being built which will be made publicly available.
    
[^4]: 字符级神经机器翻译和语言相似性

    Character-level NMT and language similarity. (arXiv:2308.04398v1 [cs.CL])

    [http://arxiv.org/abs/2308.04398](http://arxiv.org/abs/2308.04398)

    本研究探索了字符级神经机器翻译在不同语言相似性和数据集大小上的有效性，通过对捷克语和克罗地亚语、德语、匈牙利语、斯洛伐克语和西班牙语的翻译进行评估，发现对于相似语言，字符级输入分割有益处，而对于关联性较小的语言，字符级模型落后于子词级模型，但可以通过字符级微调来弥补差距。

    

    我们探索了使用Transformer架构进行字符级神经机器翻译在不同语言相似性和训练数据集大小上的有效性，翻译语言包括捷克语和克罗地亚语、德语、匈牙利语、斯洛伐克语和西班牙语。我们使用自动机器翻译评估指标对模型进行评估，并显示对于相似语言之间的翻译，字符级输入分割可以带来益处，而对于关联性较小的语言，字符级Transformer模型往往落后于子词级分割。我们证实了之前的研究结果，即可以通过对已经训练好的子词级模型进行字符级微调来弥补差距。

    We explore the effectiveness of character-level neural machine translation using Transformer architecture for various levels of language similarity and size of the training dataset on translation between Czech and Croatian, German, Hungarian, Slovak, and Spanish. We evaluate the models using automatic MT metrics and show that translation between similar languages benefits from character-level input segmentation, while for less related languages, character-level vanilla Transformer-base often lags behind subword-level segmentation. We confirm previous findings that it is possible to close the gap by finetuning the already trained subword-level models to character-level.
    
[^5]: 从大型语言模型中学习评估模型，用于序列生成

    Learning Evaluation Models from Large Language Models for Sequence Generation. (arXiv:2308.04386v1 [cs.CL])

    [http://arxiv.org/abs/2308.04386](http://arxiv.org/abs/2308.04386)

    本文提出了一种评估能力转移方法（ECT），可以将大型语言模型的评估能力转移到相对轻量级的语言模型上，提高序列生成模型的性能。

    

    大型语言模型在序列生成评估方面表现出最先进的性能，但通常具有大量的参数。这是一个计算挑战，因为在大规模应用它们的评估能力时会带来计算问题。为了克服这个挑战，本文提出了名为ECT的评估能力转移方法，将评估能力从大型语言模型转移到相对轻量级的语言模型上。基于所提出的ECT，我们从ChatGPT中学习了各种评估模型，并将它们作为奖励模型通过强化学习和重新排序方法来改进序列生成模型。在机器翻译、文本风格转换和摘要任务上的实验结果证明了我们的ECT的有效性。值得注意的是，将学习到的评估模型应用于序列生成模型会产生更好的生成序列，这是通过常用的度量和ChatGPT进行评估的。

    Large language models achieve state-of-the-art performance on sequence generation evaluation, but typically have a large number of parameters. This is a computational challenge as presented by applying their evaluation capability at scale. To overcome the challenge, in this paper, we propose \textbf{ECT}, an \textbf{e}valuation \textbf{c}apability \textbf{t}ransfer method, to transfer the evaluation capability from LLMs to relatively lightweight language models. Based on the proposed ECT, we learn various evaluation models from ChatGPT, and employ them as reward models to improve sequence generation models via reinforcement learning and reranking approaches. Experimental results on machine translation, text style transfer, and summarization tasks demonstrate the effectiveness of our ECT. Notably, applying the learned evaluation models to sequence generation models results in better generated sequences as evaluated by commonly used metrics and ChatGPT.
    
[^6]: 揭示国家偏见：对AI生成文章中国籍知觉的研究

    Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles. (arXiv:2308.04346v1 [cs.CL])

    [http://arxiv.org/abs/2308.04346](http://arxiv.org/abs/2308.04346)

    通过人类评估方法，我们研究了自然语言处理模型中的国籍偏见。研究发现，有偏见的模型会复制和放大现有的社会偏见，从而可能造成伤害。

    

    我们使用人类评估方法，研究自然语言处理(NLP)模型中潜在的国籍偏见。有偏见的NLP模型可能会传播刻板印象，导致算法性别歧视，对AI系统的公平性和正义性构成重大挑战。我们的研究采用了两步混合方法，既包括定量分析，又包括定性分析，以识别和理解文本生成模型中的国籍偏见的影响。通过我们以人为中心的定量分析，我们评估了AI生成的文章中国籍偏见的程度。然后，我们对参与者进行了开放式访谈，进行了定性编码和主题分析，以了解这些偏见对人类读者的影响。我们的研究发现，有偏见的NLP模型倾向于复制和放大现有的社会偏见，在社会技术环境中使用会造成伤害。

    We investigate the potential for nationality biases in natural language processing (NLP) models using human evaluation methods. Biased NLP models can perpetuate stereotypes and lead to algorithmic discrimination, posing a significant challenge to the fairness and justice of AI systems. Our study employs a two-step mixed-methods approach that includes both quantitative and qualitative analysis to identify and understand the impact of nationality bias in a text generation model. Through our human-centered quantitative analysis, we measure the extent of nationality bias in articles generated by AI sources. We then conduct open-ended interviews with participants, performing qualitative coding and thematic analysis to understand the implications of these biases on human readers. Our findings reveal that biased NLP models tend to replicate and amplify existing societal biases, which can translate to harm if used in a sociotechnical setting. The qualitative analysis from our interviews offers
    
[^7]: 迈向赢得加纳国家科学与数学竞赛的人工智能

    Towards an AI to Win Ghana's National Science and Maths Quiz. (arXiv:2308.04333v1 [cs.HC])

    [http://arxiv.org/abs/2308.04333](http://arxiv.org/abs/2308.04333)

    本文介绍了一项开源项目，该项目旨在建立一种人工智能，参加并赢得加纳国家科学与数学竞赛。该人工智能的成功可以在教育领域产生真实的影响。

    

    一个开源项目正在建立一种人工智能来参加并获胜加纳国家科学与数学竞赛。本文概述了该项目的进展，描述了每个团队的情况，介绍了迄今取得的进展以及计划于2023年10月在NSMQ 2023中发布和展示人工智能的下一步。

    Can an AI win Ghana's National Science and Maths Quiz (NSMQ)? That is the question we seek to answer in the NSMQ AI project, an open-source project that is building AI to compete live in the NSMQ and win. The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering questions across biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. The NSMQ is an exciting live quiz competition with interesting technical challenges across speech-to-text, text-to-speech, question-answering, and human-computer interaction. In this ongoing work that began in January 2023, we give an overview of the project, describe each of the teams, progress made thus far, and the next steps toward our planned launch and debut of the AI in October for NSMQ 2023. An AI that conquers this grand challenge can have real-world impact on education such as en
    
[^8]: 基于深度学习的隐喻检测知识注入：综述研究

    Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review. (arXiv:2308.04306v1 [cs.CL])

    [http://arxiv.org/abs/2308.04306](http://arxiv.org/abs/2308.04306)

    本文对基于深度学习的隐喻识别任务中知识注入的研究进展进行了全面综述，包括主流知识和知识注入原则的总结、数据集、评估指标和基准模型的回顾，并探讨了当前的知识注入问题。

    

    隐喻研究的历史也标志着知识注入研究的演变。随着近年来深度学习技术的不断进步，自然语言处理社区对将知识应用于在隐喻识别任务中取得成功结果表现出极大兴趣。尽管在隐喻识别领域涉及知识注入的方法逐渐增加，但缺乏一篇完整的关于基于知识注入的方法的综述文章。因此，本文旨在综述深度学习在隐喻识别任务中应用知识注入的研究进展。本文系统总结和概括了主流的知识和知识注入原则，同时回顾了在隐喻识别任务中使用的数据集、评估指标和基准模型。最后，我们探讨了当前面临的知识注入问题。

    The history of metaphor research also marks the evolution of knowledge infusion research. With the continued advancement of deep learning techniques in recent years, the natural language processing community has shown great interest in applying knowledge to successful results in metaphor recognition tasks. Although there has been a gradual increase in the number of approaches involving knowledge injection in the field of metaphor recognition, there is a lack of a complete review article on knowledge injection based approaches. Therefore, the goal of this paper is to provide a comprehensive review of research advances in the application of deep learning for knowledge injection in metaphor recognition tasks. In this paper, we systematically summarize and generalize the mainstream knowledge and knowledge injection principles, as well as review the datasets, evaluation metrics, and benchmark models used in metaphor recognition tasks. Finally, we explore the current issues facing knowledge 
    
[^9]: wav2vec 2.0特征提取器的比较分析

    Comparative Analysis of the wav2vec 2.0 Feature Extractor. (arXiv:2308.04286v1 [eess.AS])

    [http://arxiv.org/abs/2308.04286](http://arxiv.org/abs/2308.04286)

    本研究比较了wav2vec 2.0模型的特征提取器与传统特征提取方法在连接主义时间分类ASR模型上的表现，发现两者在LibriSpeech基准测试上具有竞争力。通过分析学习到的滤波器，发现ASR系统最重要的信息是通过一组带通滤波器获得的。

    

    自动语音识别（ASR）系统通常使用手工设计的特征提取流程。为了避免它们固有的信息损失，并实现从语音到转录文本的更一致的建模，神经原始波形特征提取器（FEs）是一种有吸引力的方法。最近广受欢迎的wav2vec 2.0模型也使用了卷积FE，直接对语音波形进行操作。然而，它在文献中研究得还不够充分。本研究中，我们研究了它在连接主义时间分类（CTC）ASR模型中替代标准特征提取方法的能力，并将其与另一种替代性神经FE进行了比较。我们表明，在LibriSpeech基准测试中，两者都与传统的FEs竞争力不相上下，并分析了各个组件的影响。此外，我们还分析了学习到的滤波器，并显示ASR系统的最重要信息是通过一组带通滤波器获得的。

    Automatic speech recognition (ASR) systems typically use handcrafted feature extraction pipelines. To avoid their inherent information loss and to achieve more consistent modeling from speech to transcribed text, neural raw waveform feature extractors (FEs) are an appealing approach. Also the wav2vec 2.0 model, which has recently gained large popularity, uses a convolutional FE which operates directly on the speech waveform. However, it is not yet studied extensively in the literature. In this work, we study its capability to replace the standard feature extraction methods in a connectionist temporal classification (CTC) ASR model and compare it to an alternative neural FE. We show that both are competitive with traditional FEs on the LibriSpeech benchmark and analyze the effect of the individual components. Furthermore, we analyze the learned filters and show that the most important information for the ASR system is obtained by a set of bandpass filters.
    
[^10]: 在微调之前与纯净语言模型对话进行上下文对齐

    In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning. (arXiv:2308.04275v1 [cs.CL])

    [http://arxiv.org/abs/2308.04275](http://arxiv.org/abs/2308.04275)

    本文提出了一种在微调之前与纯净语言模型进行对话的方法，通过上下文学习实现了推理时的对齐。实验证明，这种方法将纯净语言模型的胜率提高了7倍，使其可以与通过对齐微调的强基准模型媲美。

    

    在这个说明中，我们通过上下文学习来探索推理时的对齐。我们考虑了一个纯净的预训练语言模型 Llama-2，在进行任何微调之前，当模型被要求按照聊天式的指令进行操作时，我们检索到了平均9个对齐演示示例。与直接提示相比，不改变模型权重的上下文对齐导致了与OpenAI的text-davinci-003模型相比，胜率提高了7倍，使得纯净语言模型可以媲美通过对齐微调的强基准模型。

    In this note, we explore inference-time alignment through in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. Compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making the vanilla language model comparable to strong baselines with alignment fine-tuning.
    
[^11]: CLASSLA-Stanza: 南斯拉夫语言的语言处理的下一步

    CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages. (arXiv:2308.04255v1 [cs.CL])

    [http://arxiv.org/abs/2308.04255](http://arxiv.org/abs/2308.04255)

    CLASSLA-Stanza是一个为南斯拉夫语言提供自动语言注释的流水线，相对于Stanza，在性能和功能上有多个改进，并取得了始终如一的高性能。

    

    我们介绍了CLASSLA-Stanza，一个用于南斯拉夫语言的自动语言注释流水线，该流水线基于Stanza自然语言处理流水线。我们描述了CLASSLA-Stanza相对于Stanza的主要改进，并详细描述了最新2.1版本流水线的模型训练过程。我们还报告了流水线对不同语言和变种的性能评分。CLASSLA-Stanza在所有支持的语言上表现出始终如一的高性能，并在所有支持的任务上优于或扩展了其父流水线Stanza。我们还介绍了流水线的新功能，使其能够高效处理网络数据，并解释了导致其实现的原因。

    We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation of the South Slavic languages, which is based on the Stanza natural language processing pipeline. We describe the main improvements in CLASSLA-Stanza with respect to Stanza, and give a detailed description of the model training process for the latest 2.1 release of the pipeline. We also report performance scores produced by the pipeline for different languages and varieties. CLASSLA-Stanza exhibits consistently high performance across all the supported languages and outperforms or expands its parent pipeline Stanza at all the supported tasks. We also present the pipeline's new functionality enabling efficient processing of web data and the reasons that led to its implementation.
    
[^12]: 使用词嵌入进行词汇对齐

    Gloss Alignment Using Word Embeddings. (arXiv:2308.04248v1 [cs.CL])

    [http://arxiv.org/abs/2308.04248](http://arxiv.org/abs/2308.04248)

    这篇论文提出了一种使用大型口语语言模型对定位结果与字幕进行对齐的方法，解决了手语翻译数据中字幕和手势不匹配的问题。

    

    捕捉和注释手语数据集是一项耗时且昂贵的过程。目前的数据集规模远远不足以成功训练无约束的手语翻译模型。因此，研究已转向电视广播内容作为大规模训练数据的来源，包括手语翻译者和相关音频字幕。然而，缺乏手语注释限制了这些数据的可用性，并导致了自动注释技术（如手语定位）的发展。这些定位与视频对齐，而不是与字幕对齐，这往往导致字幕和定位的手势不匹配。在本文中，我们提出了一种使用大型口语语言模型将定位与其对应字幕对齐的方法。使用单一模态意味着我们的方法在计算上开销小，并且可以与现有的定位技术结合使用。我们定量地证明了该方法的有效性。

    Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effective
    
[^13]: 实时作曲辅助的混合检索增强生成

    Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])

    [http://arxiv.org/abs/2308.04215](http://arxiv.org/abs/2308.04215)

    提出了一种混合检索增强生成的框架，通过将云模型的检索增强内存整合到客户端模型中，实现实时响应的作曲辅助。

    

    检索增强模型在提升传统语言模型的上下文理解、整合私人数据和减少幻觉方面显示出了潜力。然而，应用于需要实时响应的任务（如作曲辅助）时，检索增强的大型语言模型所需的处理时间存在挑战。为了克服这一限制，我们提出了Hybrid Retrieval-Augmented Generation (HybridRAG)框架，利用了将客户端模型和云模型结合起来的混合设置。HybridRAG通过异步生成的检索增强内存，将大型语言模型（LLM）在云端生成的检索增强内存整合到客户端模型中。通过整合这种检索增强内存，客户端模型能够生成高效的响应，从LLM的能力中受益。此外，通过异步内存集成，客户端模型能够实时响应用户请求，无需等待云端处理。

    Retrieval augmented models show promise in enhancing traditional language models by improving their contextual understanding, integrating private data, and reducing hallucination. However, the processing time required for retrieval augmented large language models poses a challenge when applying them to tasks that require real-time responses, such as composition assistance.  To overcome this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework that leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously by a Large Language Model (LLM) in the cloud. By integrating this retrieval augmented memory, the client model acquires the capability to generate highly effective responses, benefiting from the LLM's capabilities. Furthermore, through asynchronous memory integration, the client model is capable of delivering real-time responses to user requests without the need to 
    
[^14]: 通过不同视角研究社交不可接受言论分类（SUD）：“我们是否在同一页上？”

    Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: "Are we on the same page ?". (arXiv:2308.04180v1 [cs.CL])

    [http://arxiv.org/abs/2308.04180](http://arxiv.org/abs/2308.04180)

    本研究通过不同的视角研究在线文本中的社交不可接受言论（SUD）分类和检测，并建立了一个新颖的语料库。通过分析不同背景下的SUD分类器的泛化能力以及注释模态对SUD学习的影响，我们提出了一些开放的挑战和研究方向，并提供了有助于领域专家在注释任务中的数据洞察。

    

    我们研究在线文本中社交不可接受言论（SUD）的特征和检测。我们首先构建并展示了一个新颖的语料库，其中包含了来自不同在线来源的大量手工注释文本，这些文本在现有的最先进的机器学习（ML）SUD检测解决方案中使用。这种全局背景使我们可以测试获取关于相同SUD类别的知识的SUD分类器的泛化能力，但来自不同背景的知识。从这个角度出发，我们可以通过讨论开放的挑战和开放的研究方向来分析注释模态可能对SUD学习的影响。我们还提供了一些数据洞察，可以支持领域专家在注释任务中。

    We study Socially Unacceptable Discourse (SUD) characterization and detection in online text. We first build and present a novel corpus that contains a large variety of manually annotated texts from different online sources used so far in state-of-the-art Machine learning (ML) SUD detection solutions. This global context allows us to test the generalization ability of SUD classifiers that acquire knowledge around the same SUD categories, but from different contexts. From this perspective, we can analyze how (possibly) different annotation modalities influence SUD learning by discussing open challenges and open research directions. We also provide several data insights which can support domain experts in the annotation task.
    
[^15]: 关于开放领域问答的单调聚合

    On Monotonic Aggregation for Open-domain QA. (arXiv:2308.04176v1 [cs.CL])

    [http://arxiv.org/abs/2308.04176](http://arxiv.org/abs/2308.04176)

    这篇论文提出了一个称为Judge-Specialist框架的方法，用于解决开放领域问答中出现的单调性问题。该框架包括专家主题检索器/阅读器和一个评判者语言模型，能够确保单调性并在自然问题上表现出色。

    

    问题回答（QA）是一项关键任务，用于从知识源中通过筛选答案而无需阅读支持文档进行基于语音的检索。特别是，开放领域的QA旨在回答关于无限制知识源的用户问题。理想情况下，添加一个来源不应降低准确性，但我们发现当前最先进的方法不满足这一特性（称为“单调性”）。我们确定了原因，并在此基础上提出了Judge-Specialist框架。我们的框架包括（1）专家主题检索器/阅读器，用于覆盖个别来源，和（2）评判者，一个专用的语言模型，用于选择最终答案。我们的实验表明，我们的框架不仅确保了单调性，而且在“自然问题”上胜过了最先进的多来源QA方法。此外，我们展示了我们的模型对于来自语音识别的噪声具有鲁棒性。我们公开发布了我们的代码和设置。

    Question answering (QA) is a critical task for speech-based retrieval from knowledge sources, by sifting only the answers without requiring to read supporting documents. Specifically, open-domain QA aims to answer user questions on unrestricted knowledge sources. Ideally, adding a source should not decrease the accuracy, but we find this property (denoted as "monotonicity") does not hold for current state-of-the-art methods. We identify the cause, and based on that we propose Judge-Specialist framework. Our framework consists of (1) specialist retrievers/readers to cover individual sources, and (2) judge, a dedicated language model to select the final answer. Our experiments show that our framework not only ensures monotonicity, but also outperforms state-of-the-art multi-source QA methods on Natural Questions. Additionally, we show that our models robustly preserve the monotonicity against noise from speech recognition. We publicly release our code and setting.
    
[^16]: 大型语言模型中的提示链对于长篇法律文件分类的应用

    Large Language Model Prompt Chaining for Long Legal Document Classification. (arXiv:2308.04138v1 [cs.CL])

    [http://arxiv.org/abs/2308.04138](http://arxiv.org/abs/2308.04138)

    本研究利用提示链的方法在处理复杂的法律文件分类任务时取得了成功，通过创建简洁摘要、语义搜索相关示例文本和利用上下文学习进行提示，可以提高零样本任务的性能并超过较大模型的得分。

    

    提示用于引导或指导语言模型生成与所需结果一致的适当回应。链接是一种将复杂任务分解为较小、可管理组件的策略。在本研究中，我们利用提示链对复杂的法律文件分类任务进行了广泛应用，这些任务由于其复杂的领域特定语言和 considerable length 的长度而具有困难。我们的方法从创建原始文件的简洁摘要开始，然后从训练语料库中进行语义搜索，寻找相关的示例文本及其相应的注释。最后，我们根据任务提示一个标签，通过利用少量提示中的上下文学习来指定标签。我们证明了通过提示链，我们不仅可以提高零样本任务的性能，还可以超过较大模型（如ChatGPT零样本）使用较小模型所达到的微F1得分。

    Prompting is used to guide or steer a language model in generating an appropriate response that is consistent with the desired outcome. Chaining is a strategy used to decompose complex tasks into smaller, manageable components. In this study, we utilize prompt chaining for extensive legal document classification tasks, which present difficulties due to their intricate domain-specific language and considerable length. Our approach begins with the creation of a concise summary of the original document, followed by a semantic search for related exemplar texts and their corresponding annotations from a training corpus. Finally, we prompt for a label - based on the task - to assign, by leveraging the in-context learning from the few-shot prompt. We demonstrate that through prompt chaining, we can not only enhance the performance over zero-shot, but also surpass the micro-F1 score achieved by larger models, such as ChatGPT zero-shot, using smaller models.
    
[^17]: 社交媒体、话题建模和情感分析在市政决策支持中的应用

    Social Media, Topic Modeling and Sentiment Analysis in Municipal Decision Support. (arXiv:2308.04124v1 [cs.CL])

    [http://arxiv.org/abs/2308.04124](http://arxiv.org/abs/2308.04124)

    本文介绍了一个针对市政决策的社交媒体处理框架，它能够对社交媒体帖子进行情感分析和话题建模，并将这些信息综合起来，以表示对每个话题的整体情感。在奥斯特拉的推文上的应用示例说明了该框架的效果。

    

    全球许多城市都希望成为智慧城市，然而，智慧倡议往往对普通市民的意见不太重视。社交媒体是市民意见的重要来源之一。本文提出了一个针对市政决策的社交媒体处理框架原型。该框架包括三个步骤：（1）确定每个社交媒体帖子的情感极性，（2）识别主要话题，并将这些话题映射到个别帖子上，（3）将这两个信息合并为一个表示对每个话题表达的整体情感的模糊数。可选地，这个模糊数可以转化为一个由两个实数元组组成的元组，表示对每个话题表达的“正面”和“负面”观点的“数量”。该框架在捷克奥斯特拉发表的推文上进行了演示，演示期为大约两个月。此应用示例说明了模糊数如何表示对每个话题表达的情感。

    Many cities around the world are aspiring to become. However, smart initiatives often give little weight to the opinions of average citizens.  Social media are one of the most important sources of citizen opinions. This paper presents a prototype of a framework for processing social media posts with municipal decision-making in mind. The framework consists of a sequence of three steps: (1) determining the sentiment polarity of each social media post (2) identifying prevalent topics and mapping these topics to individual posts, and (3) aggregating these two pieces of information into a fuzzy number representing the overall sentiment expressed towards each topic. Optionally, the fuzzy number can be reduced into a tuple of two real numbers indicating the "amount" of positive and negative opinion expressed towards each topic.  The framework is demonstrated on tweets published from Ostrava, Czechia over a period of about two months. This application illustrates how fuzzy numbers represent s
    
[^18]: 团体人类观点在语义文本相似度中的应用

    Collective Human Opinions in Semantic Textual Similarity. (arXiv:2308.04114v1 [cs.CL])

    [http://arxiv.org/abs/2308.04114](http://arxiv.org/abs/2308.04114)

    该论文介绍了USTS数据集，旨在研究语义文本相似度中的集体人类观点。分析表明现有的STS模型无法捕捉个别实例上由人类不一致带来的变异。

    

    尽管语义文本相似度（STS）具有主观性，并且在STS注释中普遍存在不一致，但现有的基准使用平均人员评分作为金标准。平均化掩盖了在低一致性示例中人类观点的真实分布，并阻止模型捕捉个别评分所代表的语义模糊性。在这项工作中，我们引入了USTS，第一个包含约15,000个中文句对和150,000个标签的不确定性感知STS数据集，以研究STS中的集体人类观点。分析表明，单一高斯函数和标量均不能充分适应一组观察到的判断。我们进一步证明，当前的STS模型无法捕捉个别实例上由人类不一致带来的变异，而是反映了对整个数据集预测置信度。

    Despite the subjective nature of semantic textual similarity (STS) and pervasive disagreements in STS annotation, existing benchmarks have used averaged human ratings as the gold standard. Averaging masks the true distribution of human opinions on examples of low agreement, and prevents models from capturing the semantic vagueness that the individual ratings represent. In this work, we introduce USTS, the first Uncertainty-aware STS dataset with ~15,000 Chinese sentence pairs and 150,000 labels, to study collective human opinions in STS. Analysis reveals that neither a scalar nor a single Gaussian fits a set of observed judgements adequately. We further show that current STS models cannot capture the variance caused by human disagreement on individual instances, but rather reflect the predictive confidence over the aggregate dataset.
    
[^19]: I-WAS：一种基于GPT-2的用于开设比喻检测的数据增强方法

    I-WAS: a Data Augmentation Method with GPT-2 for Simile Detection. (arXiv:2308.04109v1 [cs.CL])

    [http://arxiv.org/abs/2308.04109](http://arxiv.org/abs/2308.04109)

    我们提出了一种基于GPT-2的比喻数据增强方法I-WAS，通过替换词汇和完成句子的方式来增强句子质量，实验结果证明其在比喻检测中的有效性。

    

    比喻检测是许多自然语言处理（NLP）应用中的一项有价值的任务，尤其在文学领域。然而，现有的比喻检测研究常常依赖于规模有限且未能充分代表比喻形式的语料库。为了解决这个问题，我们提出了一种基于GPT-2语言模型的比喻数据增强方法，该方法通过\textbf{替换词汇}和\textbf{完成句子}的方式进行。我们的迭代过程称为I-WAS，旨在提高增强句子的质量。为了更好地评估我们的方法在实际应用中的性能，我们编制了一个包含更多不同比喻形式的语料库进行实验。我们的实验结果证明了我们提出的比喻检测数据增强方法的有效性。

    Simile detection is a valuable task for many natural language processing (NLP)-based applications, particularly in the field of literature. However, existing research on simile detection often relies on corpora that are limited in size and do not adequately represent the full range of simile forms. To address this issue, we propose a simile data augmentation method based on \textbf{W}ord replacement And Sentence completion using the GPT-2 language model. Our iterative process called I-WAS, is designed to improve the quality of the augmented sentences. To better evaluate the performance of our method in real-world applications, we have compiled a corpus containing a more diverse set of simile forms for experimentation. Our experimental results demonstrate the effectiveness of our proposed data augmentation method for simile detection.
    
[^20]: DataTales: 探索使用大型语言模型撰写数据驱动文章的方法

    DataTales: Investigating the use of Large Language Models for Authoring Data-Driven Articles. (arXiv:2308.04076v1 [cs.HC])

    [http://arxiv.org/abs/2308.04076](http://arxiv.org/abs/2308.04076)

    DataTales研究探索了使用大型语言模型(LLMs)辅助撰写数据驱动文章的方法，并设计了一个原型系统来生成数据驱动文章的文本叙述。通过定性研究，发现LLMs可以作为有价值的数据驱动文章撰写助手进一步整合。

    

    撰写数据驱动文章是一个复杂的过程，需要作者不仅分析数据以获取见解，还要构建一个连贯的叙述来有效传达这些见解。当代大型语言模型(LLMs)的文本生成能力为辅助数据驱动文章的撰写提供了机会，加快了写作过程。本研究旨在调查利用LLMs来辅助作者撰写数据驱动文章的可行性和感知价值。我们设计了一个原型系统DataTales，利用LLM来生成给定图表的文本叙述。通过使用DataTales作为设计探针，我们对11名专业人员进行了定性研究，以评估该概念，并从中提炼出进一步整合LLMs作为有价值的数据驱动文章撰写助手的可行性和机会。

    Authoring data-driven articles is a complex process requiring authors to not only analyze data for insights but also craft a cohesive narrative that effectively communicates the insights. Text generation capabilities of contemporary large language models (LLMs) present an opportunity to assist the authoring of data-driven articles and expedite the writing process. In this work, we investigate the feasibility and perceived value of leveraging LLMs to support authors of data-driven articles. We designed a prototype system, DataTales, that leverages a LLM to generate textual narratives accompanying a given chart. Using DataTales as a design probe, we conducted a qualitative study with 11 professionals to evaluate the concept, from which we distilled affordances and opportunities to further integrate LLMs as valuable data-driven article authoring assistants.
    
[^21]: 五美元模型：从句子嵌入生成游戏地图和精灵角色

    The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings. (arXiv:2308.04052v1 [cs.LG])

    [http://arxiv.org/abs/2308.04052](http://arxiv.org/abs/2308.04052)

    五美元模型是一种轻量级的文本到图像生成架构，可以从编码的文本提示中生成低维度的图片，并在有限数据集上保持语义含义。

    

    五美元模型是一种轻量级的文本到图像生成架构，可以从编码的文本提示中生成低维度的图片。这个模型可以在低维度领域中成功生成准确且美观的内容，即使只有有限的训练数据。尽管模型和数据集都很小，但生成的图片仍然能够保持文本提示的语义含义。我们将这个模型应用于三个小型数据集：像素艺术的游戏地图、游戏角色精灵图像和缩小的表情符号图像，并应用了新颖的扩充策略来提高模型在这些有限数据集上的性能。我们使用CLIP VIT-B/32模型生成的文本-图像对之间的余弦相似度评估了我们模型的性能。

    The five-dollar model is a lightweight text-to-image generative architecture that generates low dimensional images from an encoded text prompt. This model can successfully generate accurate and aesthetically pleasing content in low dimensional domains, with limited amounts of training data. Despite the small size of both the model and datasets, the generated images are still able to maintain the encoded semantic meaning of the textual prompt. We apply this model to three small datasets: pixel art video game maps, video game sprite images, and down-scaled emoji images and apply novel augmentation strategies to improve the performance of our model on these limited datasets. We evaluate our models performance using cosine similarity score between text-image pairs generated by the CLIP VIT-B/32 model.
    
[^22]: InfeRE：通过推理链逐步生成正则表达式

    InfeRE: Step-by-Step Regex Generation via Chain of Inference. (arXiv:2308.04041v1 [cs.AI])

    [http://arxiv.org/abs/2308.04041](http://arxiv.org/abs/2308.04041)

    InfeRE是一种通过推理链逐步生成正则表达式的新方法，它考虑了生成最终表达式背后的逐步内部文本匹配过程，并引入了自一致性解码机制来提高鲁棒性。实验结果表明，InfeRE在生成正则表达式方面取得了显著的改进。

    

    自然语言描述（NL2RE）生成正则表达式（regexes）的自动化已经成为一个新兴的研究领域。以往的研究将正则表达式视为一个线性的令牌序列，并通过单次自回归生成最终的表达式。然而，他们没有考虑到生成最终结果背后逐步的内部文本匹配过程。这严重影响了神经语言模型生成正则表达式的效能和可解释性。在本文中，我们提出了一种新的范式，称为InfeRE，它将正则表达式的生成分解成一系列逐步推理的过程。为了增强鲁棒性，我们引入了一个自一致性解码机制，它将从不同的模型中采样得到的多个输出进行集成。我们在两个公开可用的数据集NL-RX-Turk和KB13上对InfeRE进行了评估，并将结果与最先进的方法和流行的基于树的生成方法TRANX进行了比较。实验结果表明，InfeRE显著提高了生成正则表达式的性能。

    Automatically generating regular expressions (abbrev. regexes) from natural language description (NL2RE) has been an emerging research area. Prior studies treat regex as a linear sequence of tokens and generate the final expressions autoregressively in a single pass. They did not take into account the step-by-step internal text-matching processes behind the final results. This significantly hinders the efficacy and interpretability of regex generation by neural language models. In this paper, we propose a new paradigm called InfeRE, which decomposes the generation of regexes into chains of step-by-step inference. To enhance the robustness, we introduce a self-consistency decoding mechanism that ensembles multiple outputs sampled from different models. We evaluate InfeRE on two publicly available datasets, NL-RX-Turk and KB13, and compare the results with state-of-the-art approaches and the popular tree-based generation approach TRANX. Experimental results show that InfeRE substantially
    
[^23]: TF-IDF特征加权方法的比较研究及其在非结构化数据集中的分析

    A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset. (arXiv:2308.04037v1 [cs.CL])

    [http://arxiv.org/abs/2308.04037](http://arxiv.org/abs/2308.04037)

    该论文比较研究了TF-IDF特征加权方法并使用非结构化数据集进行了分析，结果发现相比于N-Gram，使用TF-IDF特征可以显著提高特征提取效果。

    

    文本分类是将文本分类到相关类别中的过程，其算法是自然语言处理 (NLP) 的核心。在文本分类中，术语频率-逆文件频率 (TF-IDF) 和 NLP 是最常用的信息检索方法。我们研究和分析了在非结构化数据上进行文本分类的特征加权方法。提出的模型考虑了两个特征 N-Gram 和 TF-IDF，用于情感分析的 IMDB 电影评论和亚马逊 Alexa 评论数据集。然后我们使用了最先进的分类器来验证该方法，即支持向量机 (SVM)、逻辑回归、多项式朴素贝叶斯 (Multinomial NB)、随机森林、决策树和 K 最近邻 (KNN)。从这两个特征提取中，使用 TF-IDF 特征相比基于 N-Gram 有了显著的特征提取增加。TF-IDF 获得了最高的准确率 (93.81%)、精确率 (94.20%)、召回率 (93.81%)，

    Text Classification is the process of categorizing text into the relevant categories and its algorithms are at the core of many Natural Language Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP are the most highly used information retrieval methods in text classification. We have investigated and analyzed the feature weighting method for text classification on unstructured data. The proposed model considered two features N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset for sentiment analysis. Then we have used the state-of-the-art classifier to validate the method i.e., Support Vector Machine (SVM), Logistic Regression, Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and k-nearest neighbors (KNN). From those two feature extractions, a significant increase in feature extraction with TF-IDF features rather than based on N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall (93.81%), an
    
[^24]: 生物医学问题回答的Top K相关段落检索

    Top K Relevant Passage Retrieval for Biomedical Question Answering. (arXiv:2308.04028v1 [cs.CL])

    [http://arxiv.org/abs/2308.04028](http://arxiv.org/abs/2308.04028)

    这篇论文提出了一种用于生物医学问题回答的Top K相关段落检索方法，传统的稀疏向量空间模型不适用于这个任务。然而，对于临床领域来说，这个问题还没有得到很好的解决。

    

    问答是一项利用大量文档回答事实性问题的任务。它旨在以自然语言回答用户的问题并提供准确的答案。问答依赖于高效的段落检索来选择候选上下文，传统的稀疏向量空间模型，如TF-IDF或BM25，是事实上的方法。在网络上，没有一篇文章可以提供所有可能的答案，以回答用户所提出的问题。现有的稠密段落检索模型已经对维基百科2018年12月20日的倾销进行了训练，用作回答问题的源文档。问答系统在多个开放领域和机器理解系统上取得了重大进展，使用了大规模的注释数据集。然而，在临床领域，这个问题仍然相对未被探索。根据多项调查，无法从维基百科准确回答生物医学问题。

    Question answering is a task that answers factoid questions using a large collection of documents. It aims to provide precise answers in response to the user's questions in natural language. Question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. On the web, there is no single article that could provide all the possible answers available on the internet to the question of the problem asked by the user. The existing Dense Passage Retrieval model has been trained on Wikipedia dump from Dec. 20, 2018, as the source documents for answering questions. Question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets. However, in the clinical domain, this problem remains relatively unexplored. According to multiple surveys, Biomedical Questions cannot be answered correctly from Wikipedia 
    
[^25]: 大型语言模型的持续预训练：如何（重新）热启动模型？

    Continual Pre-Training of Large Language Models: How to (re)warm your model?. (arXiv:2308.04014v1 [cs.CL])

    [http://arxiv.org/abs/2308.04014](http://arxiv.org/abs/2308.04014)

    该论文研究了大型语言模型的持续预训练问题，探讨了热启动策略对于解决分布变化和提高计算效率的影响。

    

    大型语言模型通常会对数十亿个标记进行预训练，一旦有新数据可用，就会重新开始这个过程。一种更廉价和高效的解决方案是实现这些模型的持续预训练，即用新数据更新预训练模型而不是从头开始重新训练。然而，新数据引起的分布变化通常会导致过去数据的性能下降。在本研究中，我们研究了不同的热启动策略对持续预训练的影响。我们的假设是，在训练新数据集时，需要重新增加学习率以提高计算效率。我们在Pile（上游数据，300B标记）上持续预训练模型在SlimPajama（下游数据，297B标记）上进行了线性热启动和余弦衰减的调度。我们在Pythia 410M语言模型架构上进行了所有实验。

    Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and ev
    
[^26]: SimplyRetrieve: 一款私密且轻量级的基于检索为中心的生成型AI工具

    SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool. (arXiv:2308.03983v1 [cs.CL])

    [http://arxiv.org/abs/2308.03983](http://arxiv.org/abs/2308.03983)

    SimplyRetrieve是一款私密且轻量级的生成型AI工具，使用了基于检索的生成方法，通过无需额外模型微调将私有数据集成进生成型AI系统，并提供了一个本地化、轻量级和用户友好的界面，以方便用户探索RCG在提升生成型AI性能方面的潜力。

    

    近年来，基于大型语言模型（LLM）的生成型AI系统取得了显著的进展。将知识检索架构整合进公开可用的预训练LLM中，可以无需额外的模型微调，将私有数据无缝集成进生成型AI系统。此外，检索为中心的生成（RCG）方法，作为一种有前景的未来研究方向，明确区分了LLM和检索器在上下文解释和知识记忆中的作用，潜在地导致更高效的实现。SimplyRetrieve是一个面向机器学习社区的开源工具，旨在为这些先进技术提供一个本地化、轻量级和用户友好的界面。SimplyRetrieve提供基于GUI和API的RCG平台，辅以私密知识库构建器和检索调优模块。通过利用这些功能，用户可以探索RCG在改进生成型AI性能方面的潜力。

    Large Language Model (LLM) based Generative AI systems have seen significant progress in recent years. Integrating a knowledge retrieval architecture allows for seamless integration of private data into publicly available Generative AI systems using pre-trained LLM without requiring additional model fine-tuning. Moreover, Retrieval-Centric Generation (RCG) approach, a promising future research direction that explicitly separates roles of LLMs and retrievers in context interpretation and knowledge memorization, potentially leads to more efficient implementation. SimplyRetrieve is an open-source tool with the goal of providing a localized, lightweight, and user-friendly interface to these sophisticated advancements to the machine learning community. SimplyRetrieve features a GUI and API based RCG platform, assisted by a Private Knowledge Base Constructor and a Retrieval Tuning Module. By leveraging these capabilities, users can explore the potential of RCG for improving generative AI per
    
[^27]: 简单的合成数据减少了大型语言模型中的阿谀奉承行为

    Simple synthetic data reduces sycophancy in large language models. (arXiv:2308.03958v1 [cs.CL])

    [http://arxiv.org/abs/2308.03958](http://arxiv.org/abs/2308.03958)

    本文研究了语言模型中阿谀奉承行为的普遍性，并提出了一个简单的合成数据干预方法来减少这种行为。

    

    阿谀奉承是一种不可取的行为，模型会根据用户的观点调整回应，即使这个观点在客观上是不正确的（例如，一旦用户透露他们是自由主义者，模型会适应自由主义观点）。在本文中，我们研究了语言模型中阿谀奉承行为的普遍性，并提出了一个简单的合成数据干预来减少这种行为。首先，在一组三个阿谀奉承任务中（Perez等，2022），模型被要求对没有正确答案的陈述（例如，政治问题）发表意见，我们观察到模型的扩展和指令调优显著增加了PaLM模型（多达540B参数）的阿谀奉承行为。其次，我们将阿谀奉承评估扩展到一些明显不正确的加法陈述，发现尽管模型知道这些陈述是错误的，但如果用户也这么认为，语言模型仍会同意这些陈述。为了减少阿谀奉承，我们提出了一个简单的合成数据干预方法，该方法利用公共的NLP任务。

    Sycophancy is an undesirable behavior where models tailor their responses to follow a human user's view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). In this paper, we study the prevalence of sycophancy in language models and propose a simple synthetic-data intervention to reduce this behavior.  First, on a set of three sycophancy tasks (Perez et al., 2022) where models are asked for an opinion on statements with no correct answers (e.g., politics), we observe that both model scaling and instruction tuning significantly increase sycophancy for PaLM models up to 540B parameters. Second, we extend sycophancy evaluations to simple addition statements that are objectively incorrect, finding that despite knowing that these statements are wrong, language models will still agree with them if the user does as well.  To reduce sycophancy, we present a straightforward synthetic-data intervention that takes public NLP task
    
[^28]: ChatGPT生物医学生成文本中建立信任的方法：基于本体的知识图谱用于验证疾病-症状关系

    Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])

    [http://arxiv.org/abs/2308.03929](http://arxiv.org/abs/2308.03929)

    本研究通过构建基于本体的知识图谱，利用疾病本体和症状本体构建数学模型，利用事实核查算法和网络中心度指标分析ChatGPT生成的文本与真实医学文献之间的准确性，以验证疾病-症状关系。

    

    方法：通过创新的方法，我们从真实的医学文献和人工智能生成的内容构建了基于本体的知识图谱。我们的目标是区分事实信息和未经验证的数据。我们收集了两个数据集：一个是使用“人类疾病和症状”查询从生物医学文献中编译的，另一个是由ChatGPT生成的模拟文章。利用这些数据集（PubMed和ChatGPT），我们随机选择了10组每组250个摘要，并使用特定的种子。我们的方法主要是利用疾病本体（DOID）和症状本体（SYMP）构建知识图谱，这是一种强大的数学模型，可以进行无偏差的比较。通过使用我们的事实核查算法和网络中心度指标，我们进行了GPT疾病-症状链接分析，以量化在噪声、假设和重要发现中的事实知识的准确性。结果：通过比较不同ChatGPT知识图谱及其PubMed计数获得的结果，我们发现...

    Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
    
[^29]: 通用自动国际音标转写技术的研究

    Universal Automatic Phonetic Transcription into the International Phonetic Alphabet. (arXiv:2308.03917v1 [cs.CL])

    [http://arxiv.org/abs/2308.03917](http://arxiv.org/abs/2308.03917)

    本文提出了一种最先进的模型，用于将任何语言的语音转写成国际音标（IPA），部分自动化这个过程有潜力极大加快濒危语言的记录速度。虽然使用的训练数据集比之前的模型小，但质量更高并达到了相对较好的结果，还展示了模型的质量接近人类注释者的水平。

    

    本文提出了一种最先进的模型，用于将任何语言的语音转写成国际音标（IPA）。将口语转写成国际音标是语言记录过程中必不可少但耗时的工作，部分自动化这个过程有潜力极大加快濒危语言的记录速度。我们的模型类似于之前最好的语音到国际音标模型（Wav2Vec2Phoneme），基于wav2vec 2.0并通过微调预测音频输入的国际音标。我们使用来自CommonVoice 11.0的七种语言的训练数据，通过半自动转写成国际音标。虽然这个训练数据集比Wav2Vec2Phoneme的小得多，但其质量更高，使得我们的模型达到了可比较甚至更好的结果。此外，我们还展示了我们的通用语音到国际音标模型的质量接近人类注释者的水平。

    This paper presents a state-of-the-art model for transcribing speech in any language into the International Phonetic Alphabet (IPA). Transcription of spoken languages into IPA is an essential yet time-consuming process in language documentation, and even partially automating this process has the potential to drastically speed up the documentation of endangered languages. Like the previous best speech-to-IPA model (Wav2Vec2Phoneme), our model is based on wav2vec 2.0 and is fine-tuned to predict IPA from audio input. We use training data from seven languages from CommonVoice 11.0, transcribed into IPA semi-automatically. Although this training dataset is much smaller than Wav2Vec2Phoneme's, its higher quality lets our model achieve comparable or better results. Furthermore, we show that the quality of our universal speech-to-IPA models is close to that of human annotators.
    
[^30]: 在设备上的智能助手语言理解

    Intelligent Assistant Language Understanding On Device. (arXiv:2308.03905v1 [cs.CL])

    [http://arxiv.org/abs/2308.03905](http://arxiv.org/abs/2308.03905)

    本论文描述了一种在设备上运行的自然语言理解系统的设计，相比于基于服务器的助手，该系统更加私密、可靠、快速、表达更强、准确性更高。通过分享实践经验，为研究界的未来工作提供参考。

    

    最近，将个人数字助手应用于手机和其他个人设备已成为可能。在本文中，我们描述了一个在设备上运行的自然语言理解系统的设计。与基于服务器的助手相比，该系统更具私密性、可靠性、速度快、表达更强、准确性更高。我们描述了在架构和技术方面做出的关键选择。例如，对话系统文献中的一些方法在部署环境中难以长期维护。我们希望通过分享实践经验，为研究界的未来工作提供参考。

    It has recently become feasible to run personal digital assistants on phones and other personal devices. In this paper we describe a design for a natural language understanding system that runs on device. In comparison to a server-based assistant, this system is more private, more reliable, faster, more expressive, and more accurate. We describe what led to key choices about architecture and technologies. For example, some approaches in the dialog systems literature are difficult to maintain over time in a deployment setting. We hope that sharing learnings from our practical experiences may help inform future work in the research community.
    
[^31]: 跨领域评估因果知识提取方法

    A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction. (arXiv:2308.03891v1 [cs.CL])

    [http://arxiv.org/abs/2308.03891](http://arxiv.org/abs/2308.03891)

    本研究进行了跨领域的评估，比较了因果知识提取的三个序列标注模型和基于跨度的提取方法。实验结果表明，预训练语言模型的词嵌入在这个任务中表现出了较好的性能，同时基于跨度的模型在不同类型的数据集上表现更好。

    

    因果知识提取是从文本中提取相关因果关系的任务。尽管这个任务对于语言理解和知识发现很重要，但是最近在这个领域的工作主要集中在将文本片段二分类为因果或非因果。因此，本研究对三个序列标注模型进行了彻底分析，比较了使用基于跨度的方法提取因果关系的效果。实验结果表明，与复杂的模型架构相比，预训练语言模型（如BERT）的词嵌入在这个任务上提供了显著的性能提升。我们观察到，在涵盖不同领域和不同类型的因果短语的4个数据集中，基于跨度的模型表现优于简单的序列标注模型。

    Causal knowledge extraction is the task of extracting relevant causes and effects from text by detecting the causal relation. Although this task is important for language understanding and knowledge discovery, recent works in this domain have largely focused on binary classification of a text segment as causal or non-causal. In this regard, we perform a thorough analysis of three sequence tagging models for causal knowledge extraction and compare it with a span based approach to causality extraction. Our experiments show that embeddings from pre-trained language models (e.g. BERT) provide a significant performance boost on this task compared to previous state-of-the-art models with complex architectures. We observe that span based models perform better than simple sequence tagging models based on BERT across all 4 data sets from diverse domains with different types of cause-effect phrases.
    
[^32]: 表联合搜索生成性基准创建

    Generative Benchmark Creation for Table Union Search. (arXiv:2308.03883v1 [cs.DB])

    [http://arxiv.org/abs/2308.03883](http://arxiv.org/abs/2308.03883)

    采用生成型AI模型为表联合搜索创建结构化数据基准

    

    数据管理传统上依靠合成数据生成器生成结构化基准，如TPC套件，我们可以精确控制重要参数如数据大小和分布。这些基准对于数据库管理系统的成功和采用至关重要。然而，越来越多的数据管理问题属于语义性质。一个重要的例子是找到可以联合的表。虽然任何具有相同基数的两个表都可以联合，表联合搜索是找到其联合在语义上连贯的表的问题。语义问题无法使用合成数据进行基准测试。我们目前创建的基准的方法涉及实际数据的手动策划和标记。这些方法不稳健且不可扩展，而且更重要的是，不清楚所创建的基准的稳健性如何。我们提议使用生成型AI模型为表联合搜索创建结构化数据基准。我们提出了一种新的方法

    Data management has traditionally relied on synthetic data generators to generate structured benchmarks, like the TPC suite, where we can control important parameters like data size and its distribution precisely. These benchmarks were central to the success and adoption of database management systems. But more and more, data management problems are of a semantic nature. An important example is finding tables that can be unioned. While any two tables with the same cardinality can be unioned, table union search is the problem of finding tables whose union is semantically coherent. Semantic problems cannot be benchmarked using synthetic data. Our current methods for creating benchmarks involve the manual curation and labeling of real data. These methods are not robust or scalable and perhaps more importantly, it is not clear how robust the created benchmarks are. We propose to use generative AI models to create structured data benchmarks for table union search. We present a novel method 
    
[^33]: 电子商务查询的语义等价性

    Semantic Equivalence of e-Commerce Queries. (arXiv:2308.03869v1 [cs.IR])

    [http://arxiv.org/abs/2308.03869](http://arxiv.org/abs/2308.03869)

    本文介绍了一种识别和利用电子商务查询等价性的框架，以提升搜索者和商业结果。该框架解决了将查询映射为搜索意图向量表示、识别等价或相似意图的最近邻查询以及优化用户或商业目标等三个关键问题。通过表面相似性和行为相似性来确定查询的等价性。

    

    电子商务搜索中，查询变化会带来挑战，因为相同的搜索意图可以通过具有表层差异的不同查询来表达。本文介绍了一个框架来识别和利用查询等价性以提升搜索者和商业结果。所提出的方法解决了三个关键问题：将查询映射到搜索意图的向量表示，识别表达等价或相似意图的最近邻查询，以及优化用户或商业目标。该框架利用表面相似性和行为相似性来确定查询的等价性。表面相似性涉及基于词的变形、词序、复合和噪声词来规范化查询。行为相似性利用历史搜索行为生成查询意图的向量表示。离线过程用于训练句子相似性模型，而在线最近邻方法支持对未见查询的处理。

    Search query variation poses a challenge in e-commerce search, as equivalent search intents can be expressed through different queries with surface-level differences. This paper introduces a framework to recognize and leverage query equivalence to enhance searcher and business outcomes. The proposed approach addresses three key problems: mapping queries to vector representations of search intent, identifying nearest neighbor queries expressing equivalent or similar intent, and optimizing for user or business objectives. The framework utilizes both surface similarity and behavioral similarity to determine query equivalence. Surface similarity involves canonicalizing queries based on word inflection, word order, compounding, and noise words. Behavioral similarity leverages historical search behavior to generate vector representations of query intent. An offline process is used to train a sentence similarity model, while an online nearest neighbor approach supports processing of unseen qu
    
[^34]: 在教育中信任语言模型

    Trusting Language Models in Education. (arXiv:2308.03866v1 [cs.CL])

    [http://arxiv.org/abs/2308.03866](http://arxiv.org/abs/2308.03866)

    本研究在教育中使用语言模型，提出了使用XGBoost和BERT的结合来校准语言模型的置信度，通过基于注意力机制的特征来输出校正的概率。

    

    语言模型在教育领域被广泛应用。尽管现代深度学习模型在问答任务上表现出色，但有时会出现错误。为了避免向学生展示错误答案，重要的是校准这些模型的置信度 - 即预测概率。在我们的工作中，我们提出使用一种基于注意力机制的特征的XGBoost在BERT之上输出校正的概率。我们的假设是注意力流中包含的不确定性水平与模型的响应质量相关。

    Language Models are being widely used in Education. Even though modern deep learning models achieve very good performance on question-answering tasks, sometimes they make errors. To avoid misleading students by showing wrong answers, it is important to calibrate the confidence - that is, the prediction probability - of these models. In our work, we propose to use an XGBoost on top of BERT to output the corrected probabilities, using features based on the attention mechanism. Our hypothesis is that the level of uncertainty contained in the flow of attention is related to the quality of the model's response itself.
    
[^35]: Storyfier: 使用文本生成模型探索词汇学习支持

    Storyfier: Exploring Vocabulary Learning Support with Text Generation Models. (arXiv:2308.03864v1 [cs.HC])

    [http://arxiv.org/abs/2308.03864](http://arxiv.org/abs/2308.03864)

    Storyfier利用文本生成模型为学习者提供了有助于记忆词汇的生成故事，并提供适应性的AI辅助撰写新故事。然而，在实验中，使用Storyfier的学习者在回忆和使用目标词方面表现较差。

    

    词汇学习支持工具广泛利用现有材料，如故事或视频片段，作为帮助用户记忆每个目标单词的上下文。然而，这些工具无法为学习者感兴趣的任何目标词提供连贯的上下文，并且它们很少帮助实践单词的使用。在本文中，我们与教师和学生合作，迭代地开发了Storyfier，它利用文本生成模型使学习者能够阅读覆盖任何目标词的生成故事，进行故事克隆测试，并使用这些单词撰写一个适应性人工智能辅助的新故事。我们的被试研究（N = 28）显示，学习者普遍喜欢生成的故事，因为它们可以连接目标词并减轻学习负担。然而，在阅读-克隆-写学习会话中，使用Storyfier的参与者在回忆和使用目标词方面表现不如基线工具（不含我们的人工智能特性）。我们讨论了对支持词汇学习的洞察。

    Vocabulary learning support tools have widely exploited existing materials, e.g., stories or video clips, as contexts to help users memorize each target word. However, these tools could not provide a coherent context for any target words of learners' interests, and they seldom help practice word usage. In this paper, we work with teachers and students to iteratively develop Storyfier, which leverages text generation models to enable learners to read a generated story that covers any target words, conduct a story cloze test, and use these words to write a new story with adaptive AI assistance. Our within-subjects study (N=28) shows that learners generally favor the generated stories for connecting target words and writing assistance for easing their learning workload. However, in the read-cloze-write learning sessions, participants using Storyfier perform worse in recalling and using target words than learning with a baseline tool without our AI features. We discuss insights into suppor
    
[^36]: 使用大型语言模型从医学肿瘤学笔记中提取详细的肿瘤病史和治疗计划

    Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models. (arXiv:2308.03853v1 [cs.CL])

    [http://arxiv.org/abs/2308.03853](http://arxiv.org/abs/2308.03853)

    本研究开发了一个详细的肿瘤学信息注释方案，使用大型语言模型从肿瘤学笔记中提取和推理复杂的修辞，并应用于乳腺癌进展笔记的语料库。

    

    医学护理和肿瘤学观察研究都需要全面了解患者的疾病进展和治疗历史，这些信息通常在临床记录中详细记录。尽管它们在肿瘤学中的重要作用，但目前没有针对这些记录中记录的多样信息进行完整封装的肿瘤学信息表示和注释方案。虽然大型语言模型（LLM）最近在各种医学自然语言处理任务中表现出色，但由于目前缺乏全面注释的肿瘤学数据集，对LLM在提取和推理肿瘤学笔记中的复杂修辞的广泛评估仍然不足。我们开发了一个详细的方案，用于注释肿瘤学文本信息，包括患者特征、肿瘤特征、测试、治疗和时间性。我们利用加利福尼亚大学旧金山分校的10个去标识化乳腺癌进展笔记语料库，应用了这个方案。

    Both medical care and observational studies in oncology require a thorough understanding of a patient's disease progression and treatment history, often elaborately documented in clinical notes. Despite their vital role, no current oncology information representation and annotation schema fully encapsulates the diversity of information recorded within these notes. Although large language models (LLMs) have recently exhibited impressive performance on various medical natural language processing tasks, due to the current lack of comprehensively annotated oncology datasets, an extensive evaluation of LLMs in extracting and reasoning with the complex rhetoric in oncology notes remains understudied. We developed a detailed schema for annotating textual oncology information, encompassing patient characteristics, tumor characteristics, tests, treatments, and temporality. Using a corpus of 10 de-identified breast cancer progress notes at University of California, San Francisco, we applied this
    
[^37]: 金融欺诈检测的文本数据挖掘：一种深度学习方法

    Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach. (arXiv:2308.03800v1 [cs.CL])

    [http://arxiv.org/abs/2308.03800](http://arxiv.org/abs/2308.03800)

    该论文提出了一种深度学习方法，用于分析金融欺诈文本并进行分类。通过比较多种神经网络模型的准确性，该研究对金融欺诈检测具有重要意义

    

    在本报告中，我提出了一种深度学习方法，用于进行自然语言处理（以下简称NLP）的二元分类任务，以分析金融欺诈文本。首先，我搜索了港交所新闻的监管公告和执法公告，以定义欺诈公司并提取其MD＆A报告，然后整理了报告中的句子，并标记了报告时间。我的方法包括各种神经网络模型，包括具有嵌入层的多层感知器（MLP），基本循环神经网络（RNN），长短期记忆（LSTM）和门限循环单元（GRU）用于文本分类任务。通过利用这个多样化的模型集合，我旨在全面比较它们在检测金融欺诈方面的准确性。我的结果对于金融欺诈检测具有重要意义，因为这项工作为深度学习，NLP和金融交叉研究的不断增长贡献了有价值的研究成果

    In this report, I present a deep learning approach to conduct a natural language processing (hereafter NLP) binary classification task for analyzing financial-fraud texts. First, I searched for regulatory announcements and enforcement bulletins from HKEX news to define fraudulent companies and to extract their MD&A reports before I organized the sentences from the reports with labels and reporting time. My methodology involved different kinds of neural network models, including Multilayer Perceptrons with Embedding layers, vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) for the text classification task. By utilizing this diverse set of models, I aim to perform a comprehensive comparison of their accuracy in detecting financial fraud. My results bring significant implications for financial fraud detection as this work contributes to the growing body of research at the intersection of deep learning, NLP, and finance, providing valuabl
    
[^38]: 忘记演示，专注于从文本指令中学习

    Forget Demonstrations, Focus on Learning from Textual Instructions. (arXiv:2308.03795v1 [cs.CL])

    [http://arxiv.org/abs/2308.03795](http://arxiv.org/abs/2308.03795)

    本研究提出了一种从文本指令中学习任务的方法，通过自动找出定义中的关键句子和使用排名目标，实现了最先进的性能。

    

    本研究针对零示范跨任务泛化的一个更具挑战性但更真实的情境进行研究：从文本指令中学习而无需示范，假设存在一种段落式任务定义但不存在示范。为了更好地从定义中学习任务监督，我们提出了两种策略：首先，自动找出定义中的关键句子；其次，使用排名目标来强制模型在定义中突出显示这些关键部分时生成金标输出的概率更高。这两种策略的共同努力在具有挑战性的基准测试中产生了最先进的性能。我们的代码将在论文的最终版本中发布。

    This work studies a challenging yet more realistic setting for zero-shot cross-task generalization: demonstration-free learning from textual instructions, presuming the existence of a paragraph-style task definition while no demonstrations exist. To better learn the task supervision from the definition, we propose two strategies: first, to automatically find out the critical sentences in the definition; second, a ranking objective to force the model to generate the gold outputs with higher probabilities when those critical parts are highlighted in the definition. The joint efforts of the two strategies yield state-of-the-art performance on the challenging benchmark. Our code will be released in the final version of the paper.
    
[^39]: Bio+Clinical BERT、BERT Base和CNN在预测药物评论满意度方面的性能比较

    Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction. (arXiv:2308.03782v1 [cs.CL])

    [http://arxiv.org/abs/2308.03782](http://arxiv.org/abs/2308.03782)

    该研究比较了Bio+Clinical BERT、BERT Base和CNN在预测药物评价满意度方面的性能，结果显示医学领域特定的Bio+Clinical BERT模型在整体性能上优于通用领域的BERT Base模型，提高了11%的Macro F1和召回率得分。

    

    该研究的目标是开发能够分析患者药物评论并准确分类满意程度为积极、中性或消极的自然语言处理（NLP）模型。这样的模型将减轻医疗保健专业人员的工作负担，并提供更多关于患者生活质量的见解，这是治疗效果的重要指标。为了实现这一目标，我们实施和评估了多个分类模型，包括BERT base模型、Bio+Clinical BERT以及一个更简单的CNN。结果表明，医学领域特定的Bio+Clinical BERT模型在整体性能上显著优于通用领域的BERT base模型，如表2所示，它在Macro F1和召回率得分上提升了11%。未来的研究可以探索如何充分利用每个模型的特定优势。Bio+Clinical BERT在整体性能上表现出色，特别是在处理医学行话方面，而更简单的CNN则展现出识别关键词和上下文的能力。

    The objective of this study is to develop natural language processing (NLP) models that can analyze patients' drug reviews and accurately classify their satisfaction levels as positive, neutral, or negative. Such models would reduce the workload of healthcare professionals and provide greater insight into patients' quality of life, which is a critical indicator of treatment effectiveness. To achieve this, we implemented and evaluated several classification models, including a BERT base model, Bio+Clinical BERT, and a simpler CNN. Results indicate that the medical domain-specific Bio+Clinical BERT model significantly outperformed the general domain base BERT model, achieving macro f1 and recall score improvement of 11%, as shown in Table 2. Future research could explore how to capitalize on the specific strengths of each model. Bio+Clinical BERT excels in overall performance, particularly with medical jargon, while the simpler CNN demonstrates the ability to identify crucial words and a
    
[^40]: 使用基于Transformer的框架结合深度信息增强图像字幕生成

    Enhancing image captioning with depth information using a Transformer-based framework. (arXiv:2308.03767v1 [cs.CV])

    [http://arxiv.org/abs/2308.03767](http://arxiv.org/abs/2308.03767)

    本研究提出了一个基于Transformer的框架，将深度信息与RGB图像结合，以增强图像字幕生成任务。通过使用多句字幕描述3D场景，我们的框架能够更好地理解输入场景，并在不同的融合方法下进行实验验证。

    

    图像字幕生成是连接计算机视觉和自然语言处理的具有挑战性的场景理解任务。虽然图像字幕生成模型在生成优秀描述方面取得了成功，但该领域主要集中于为2D图像生成单个句子。本文研究了通过将深度信息与RGB图像集成，能否增强字幕生成任务并生成更好的描述。为此，我们提出了一个基于Transformer的编码器-解码器框架，用于生成3D场景的多句字幕描述。RGB图像及其对应的深度图作为我们框架的输入，将它们结合起来以更好地理解输入场景。深度图可以是真实值或估计值，这使得我们的框架在任何RGB字幕数据集上都可以广泛应用。我们探索了不同的融合方法来融合RGB和深度图像。实验在NYU-v2数据集和Stanford图像集上进行。

    Captioning images is a challenging scene-understanding task that connects computer vision and natural language processing. While image captioning models have been successful in producing excellent descriptions, the field has primarily focused on generating a single sentence for 2D images. This paper investigates whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions. For this purpose, we propose a Transformer-based encoder-decoder framework for generating a multi-sentence description of a 3D scene. The RGB image and its corresponding depth map are provided as inputs to our framework, which combines them to produce a better understanding of the input scene. Depth maps could be ground truth or estimated, which makes our framework widely applicable to any RGB captioning dataset. We explored different fusion approaches to fuse RGB and depth images. The experiments are performed on the NYU-v2 dataset and the Stanford image para
    
[^41]: GPT-4无法进行推理

    GPT-4 Can't Reason. (arXiv:2308.03762v1 [cs.CL])

    [http://arxiv.org/abs/2308.03762](http://arxiv.org/abs/2308.03762)

    GPT-4在推理方面无能为力，尽管有着偶尔显示的分析才智。

    

    GPT-4于2023年3月发布，广受好评，相比之前的GPT-3.5（OpenAI之前最好的模型，用于ChatGPT的初次发布），在各个方面都有很大的改进。然而，尽管有着令人印象深刻的改进，对于GPT-4的推理能力存在充分的怀疑是有道理的。本文讨论了推理的本质；批评了当前NLP社区中推理问题的表述方式，以及目前LLM推理性能的评估方式；引入了一系列由21个多样化推理问题组成的集合；并对GPT-4在这些问题上的性能进行了详细的定性评估。基于这个分析，本文得出结论，尽管偶尔显示出分析上的才智，但目前的GPT-4完全无法进行推理。

    GPT-4 was released in March 2023 to wide acclaim, marking a very substantial improvement across the board over GPT-3.5 (OpenAI's previously best model, which had powered the initial release of ChatGPT). However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason. This position paper discusses the nature of reasoning; criticizes the current formulation of reasoning problems in the NLP community, as well as the way in which LLM reasoning performance is currently evaluated; introduces a small collection of 21 diverse reasoning problems; and performs a detailed qualitative evaluation of GPT-4's performance on those problems. Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning.
    
[^42]: MedMine: 检验预训练语言模型在药物挖掘中的应用

    MedMine: Examining Pre-trained Language Models on Medication Mining. (arXiv:2308.03629v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03629](http://arxiv.org/abs/2308.03629)

    MedMine通过检验预训练语言模型在药物挖掘中的应用，发现了它们在不同实体类型和临床事件上的不平衡表现，并提供了解决这些问题的研究方向。

    

    自动从临床和生物医学文本中进行药物挖掘已成为一个热门话题，这是由于其对医疗应用的真实影响以及强大语言模型的最新发展。然而，全自动提取模型仍然面临一些障碍，以便可以直接部署到临床实践中以获得更好的影响。这些障碍包括它们在不同实体类型和临床事件上的不平衡表现。在本研究中，我们通过微调，包括基于单语言模型Med7和多语言大型语言模型XLM-RoBERTa的方式，检验了当前最先进的预训练语言模型在这些任务上的表现。我们使用n2c2-2018挑战赛的历史药物挖掘共享任务数据集进行了它们的优劣比较。我们报告了这些微调实验的结果，以便促进未来研究解决这些问题，比如如何结合它们的输出，合并这些模型，或者改进其性能。

    Automatic medication mining from clinical and biomedical text has become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medication mining shared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or impr
    
[^43]: GPT-3的拓扑解释

    Topological Interpretations of GPT-3. (arXiv:2308.03565v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03565](http://arxiv.org/abs/2308.03565)

    本文研究了GPT-3的拓扑解释方法，通过计算句子向量的距离和相关性，揭示了不同嵌入空间中句子之间的关联性。

    

    这是一项实证研究，旨在探索一种一致的方法，用于推导句向量与句子语义之间的相关性。我们首先使用三种最先进的词/句子嵌入方法，包括GPT-3，Word2Vec和Sentence-BERT，将纯文本句子字符串嵌入到高维空间中。然后我们计算嵌入空间中任意两个句向量的配对距离，并将其映射到一个矩阵中。根据每个距离矩阵，我们计算句向量相对于其他句向量的距离的相关性。然后我们计算距离矩阵对的相关性。我们观察到相同句子在不同嵌入空间中的相关性，以及在同一嵌入空间中不同句子的相关性。这些观察结果与我们的假设一致，并带领我们进入下一个阶段。

    This is an experiential study of investigating a consistent method for deriving the correlation between sentence vector and semantic meaning of a sentence. We first used three state-of-the-art word/sentence embedding methods including GPT-3, Word2Vec, and Sentence-BERT, to embed plain text sentence strings into high dimensional spaces. Then we compute the pairwise distance between any possible combination of two sentence vectors in an embedding space and map them into a matrix. Based on each distance matrix, we compute the correlation of distances of a sentence vector with respect to the other sentence vectors in an embedding space. Then we compute the correlation of each pair of the distance matrices. We observed correlations of the same sentence in different embedding spaces and correlations of different sentences in the same embedding space. These observations are consistent with our hypothesis and take us to the next stage.
    
[^44]: RecycleGPT：一种具有可回收模块的自回归语言模型

    RecycleGPT: An Autoregressive Language Model with Recyclable Module. (arXiv:2308.03421v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03421](http://arxiv.org/abs/2308.03421)

    RecycleGPT是一种生成式语言模型，通过回收预生成的模型状态而无需多次运行整个模型，并且证明了其降低推理延迟的有效性，实现高速解码和保持高性能。

    

    现有的大型语言模型必须运行K次才能生成K个令牌的序列。在本文中，我们提出了RecycleGPT，这是一种具有快速解码速度的生成式语言模型，通过回收预生成的模型状态而无需将整个模型运行多次步骤。我们的方法基于这样的观察：序列中相邻的令牌通常具有很强的相关性，并且可以根据前面的令牌合理猜测或推断出下一个令牌。实验证明了我们的方法在降低推理延迟方面的有效性，实现了高达1.4倍的加速，同时保持高性能。

    Existing large language models have to run K times to generate a sequence of K tokens. In this paper, we present RecycleGPT, a generative language model with fast decoding speed by recycling pre-generated model states without running the whole model in multiple steps. Our approach relies on the observation that adjacent tokens in a sequence usually have strong correlations and the next token in a sequence can be reasonably guessed or inferred based on the preceding ones. Experiments and analysis demonstrate the effectiveness of our approach in lowering inference latency, achieving up to 1.4x speedup while preserving high performance.
    
[^45]: 迈向多参考时代 —— 解决NLG评估中的数据泄漏和参考多样性有限问题

    Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation. (arXiv:2308.03131v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03131](http://arxiv.org/abs/2308.03131)

    本论文提出了使用多个参考来增强匹配指标与人类评估之间的一致性。在WMT Metrics基准中，多参考F200spBLEU相对于传统的单参考方法准确率提高了7.2\%，超过了基于神经网络的BERTscore的3.9\%的准确率提高。此外，该方法还可以在很大程度上缓解大型语言模型中的数据泄漏问题。

    

    N-gram匹配的评估指标，如BLEU和chrF，在各种自然语言生成（NLG）任务中被广泛使用。然而，最近的研究发现，这些基于匹配的指标与人类评估之间存在较弱的相关性，尤其是与基于神经网络的指标如BLEURT相比。在本文中，我们推测匹配指标性能瓶颈的原因可能是参考资料多样性有限。为了解决这个问题，我们提出利用"多个参考"来增强这些指标与人类评估之间的一致性。在WMT Metrics基准中，我们观察到多参考F200spBLEU相对于传统的单参考方法，准确率提高了7.2\%。值得注意的是，它还超过了基于神经网络的BERTscore，准确率提高了3.9\%。此外，我们观察到大型语言模型（LLMs）中的数据泄漏问题可以在很大程度上得到缓解通过我们的多参考方法。

    N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely utilized across a range of natural language generation (NLG) tasks. However, recent studies have revealed a weak correlation between these matching-based metrics and human evaluations, especially when compared with neural-based metrics like BLEURT. In this paper, we conjecture that the performance bottleneck in matching-based metrics may be caused by the limited diversity of references. To address this issue, we propose to utilize \textit{multiple references} to enhance the consistency between these metrics and human evaluations. Within the WMT Metrics benchmarks, we observe that the multi-references F200spBLEU surpasses the conventional single-reference one by an accuracy improvement of 7.2\%. Remarkably, it also exceeds the neural-based BERTscore by an accuracy enhancement of 3.9\%. Moreover, we observe that the data leakage issue in large language models (LLMs) can be mitigated to a large extent by our multi
    
[^46]: 通过领域适应的最少到最多提示的方式实现文本到SQL的高效泛化

    Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])

    [http://arxiv.org/abs/2308.02582](http://arxiv.org/abs/2308.02582)

    该论文介绍了一种通过领域适应和最少到最多提示的方式实现文本到SQL的高效泛化的方法。通过离线抽样获取少量样本，并合成一个通用提示，避免了昂贵的测试时间样本检索，并通过自适应和分解的方法更好地处理跨领域和跨组合式的泛化。

    

    跨领域和跨组合式的文本到SQL语义解析的泛化是一项具有挑战性的任务。现有的基于大型语言模型（LLM）的解决方案依赖于从训练集中推理出少量样本，以合成每个自然语言（NL）测试查询的运行时提示。与此相反，我们设计了一种算法，该算法通过离线抽样从训练数据中获取少量样本，完全覆盖SQL子句、运算符和函数，并在允许的令牌长度范围内实现最大领域覆盖。这样可以合成一个固定的通用提示（GP），其中包含NL测试查询之间共用的多样化样本集，避免了昂贵的测试时间样本检索。我们还将GP自适应到目标数据库领域（DA-GP），以更好地处理跨领域泛化；然后采用分解的最少到最多提示（LTMP-DA-GP）来处理跨组合泛化。LTMP-DA-GP的合成是离线任务，

    Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
    
[^47]: 面向自动语音识别的联邦表示学习

    Federated Representation Learning for Automatic Speech Recognition. (arXiv:2308.02013v1 [cs.SD])

    [http://arxiv.org/abs/2308.02013](http://arxiv.org/abs/2308.02013)

    使用联邦学习和自监督学习的结合方法，研究了面向自动语音识别的联邦表示学习，将边缘设备上的隐私数据用于学习健壮的音频表示，并取得了显著的性能改善。

    

    联邦学习（FL）是一种保护隐私的模式，允许边缘设备在不共享数据的情况下进行协作学习。像Alexa和Siri这样的边缘设备是潜在的非标记音频数据来源，可以用来学习健壮的音频表示。在这项工作中，我们将自监督学习（SSL）和FL结合起来，以遵守数据隐私约束条件，学习用于自动语音识别的表示。我们使用未标记的语音数据集Libri-Light中的说话者和章节信息，模拟非独立同分布的说话者隔离数据分布，并使用FedSGD在对比预测编码框架下进行LSTM编码器的预训练。我们展示了在FL中预训练的ASR编码器的性能与中心预训练模型相当，并且相比没有预训练，有12-15%（WER）的改善。我们进一步将联邦预训练模型适应到一种新的语言，法语，并且相比没有预训练，实现了20%（WER）的改善。

    Federated Learning (FL) is a privacy-preserving paradigm, allowing edge devices to learn collaboratively without sharing data. Edge devices like Alexa and Siri are prospective sources of unlabeled audio data that can be tapped to learn robust audio representations. In this work, we bring Self-supervised Learning (SSL) and FL together to learn representations for Automatic Speech Recognition respecting data privacy constraints. We use the speaker and chapter information in the unlabeled speech dataset, Libri-Light, to simulate non-IID speaker-siloed data distributions and pre-train an LSTM encoder with the Contrastive Predictive Coding framework with FedSGD. We show that the pre-trained ASR encoder in FL performs as well as a centrally pre-trained model and produces an improvement of 12-15% (WER) compared to no pre-training. We further adapt the federated pre-trained models to a new language, French, and show a 20% (WER) improvement over no pre-training.
    
[^48]: NBIAS: 用于文本中偏见识别的自然语言处理框架

    NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])

    [http://arxiv.org/abs/2308.01681](http://arxiv.org/abs/2308.01681)

    本论文提出了一个名为NBIAS的自然语言处理框架，旨在识别文本中的偏见。通过收集来自社交媒体、医疗保健和职位招聘等领域的多样化数据构建数据集，并应用基于Transformer的令牌分类模型来识别偏见词/短语。通过定量和定性评估方法来评估模型的效果。

    

    在文本数据中存在偏见可能导致数据使用时产生倾斜的解释和结果。这些偏见可能会持续强化刻板印象、歧视或其他形式的不公平待遇。在有偏见的数据上训练的算法最终会做出不平等影响某个群体的决策。因此，检测和消除这些偏见至关重要，以确保对数据的公平和道德使用。为此，我们开发了一个全面而强大的框架"NBIAS"，它包括数据层、语料库构建、模型开发层和评估层。数据集由从各个领域收集的多样化数据构建，包括社交媒体、医疗保健和职位招聘门户网站。因此，我们应用了基于Transformer的令牌分类模型，通过一个唯一的命名实体能够识别出偏见词/短语。在评估过程中，我们结合了定量和定性评估方法来评估我们模型的效果。

    Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data ends up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework \textsc{Nbias} that consists of a data layer, corpus contruction, model development layer and an evaluation layer. The dataset is constructed by collecting diverse data from various fields, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity. In the assessment procedure, we incorporate a blend of quantitative and qualitative evaluations to gauge the effectiveness of our models.
    
[^49]: Gzip与KNN在文本分类中的对比研究

    Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v1 [cs.CL])

    [http://arxiv.org/abs/2307.15002](http://arxiv.org/abs/2307.15002)

    Gzip与KNN相比较在文本分类中，我们发现通过简单的词袋匹配可以获得类似或更好的准确性，并且更加高效。

    

    最近，基于KNN的文本分类中压缩距离（gzip）的有效性引起了很多关注。在本文中，我们展示了通过更简单的方法可以达到类似或更好的效果，并且可能不需要文本压缩。实际上，我们发现简单的“词袋”匹配可以达到类似或更好的准确性，并且更高效。

    The effectiveness of compression distance in KNN-based text classification ('gzip') has recently garnered lots of attention. In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary. Indeed, we find that a simple 'bag-of-words' matching can achieve similar or better accuracy, and is more efficient.
    
[^50]: 用GPT-4增强CLIP：利用视觉描述作为提示

    Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])

    [http://arxiv.org/abs/2307.11661](http://arxiv.org/abs/2307.11661)

    本文展示了如何利用GPT-4生成具有视觉描述性的文本，并将其用于适应CLIP到下游任务。与CLIP的默认提示相比，在专门细粒度数据集上显示了较大的0-shot迁移准确性改进。

    

    对比预训练的大型视觉-语言模型（VLMs）如CLIP在下游数据集上提供了良好性能，从而革新了视觉表示学习。VLMs通过设计与数据集相关的提示来0-shot适应下游数据集。这种提示工程利用了领域专业知识和验证数据集。同时，像GPT-4这样的生成预训练模型的最新发展意味着它们可以用作先进的互联网搜索工具。它们还可以被操作以提供任何结构化的视觉信息。在这项工作中，我们展示了如何利用GPT-4生成具有视觉描述性的文本，并将其用于适应CLIP到下游任务。与CLIP的默认提示相比，我们在专门细粒度数据集（如EuroSAT（~7％）、DTD（~7％）、SUN397（~4.6％）和CUB（~3.3％））上显示出了较大的0-shot迁移准确性改进。我们还设计了一个简单的少量样本适配器，它可以学习选择最佳的s

    Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible s
    
[^51]: 提高预训练语言模型的泛化能力

    Improving Pre-trained Language Models' Generalization. (arXiv:2307.10457v1 [cs.CL])

    [http://arxiv.org/abs/2307.10457](http://arxiv.org/abs/2307.10457)

    该研究提出了一种名为Mask-tuning的训练方法，通过将Masked Language Modeling (MLM)训练目标整合到微调过程中来增强预训练语言模型（PLMs）的泛化能力。实验证明，Mask-tuning在非分布数据集上超过了当前最先进的技术，并提高了PLMs在分布数据集上的性能。

    

    最先进的预训练语言模型（PLMs）的可重复使用性通常受到其泛化问题的限制，即当在与训练数据集不同的示例上进行评估时，其性能显著下降，这种示例被称为“非分布/未见示例”。这一限制源于PLMs对虚假相关性的依赖，虚假相关性对于常见示例类型效果良好，但对于一般示例效果不佳。为了解决这个问题，我们提出了一种称为Mask-tuning的训练方法，该方法将遮蔽语言建模（MLM）训练目标整合到微调过程中，以增强PLMs的泛化能力。全面的实验表明，Mask-tuning超过了当前最先进的技术，并增强了PLMs对非分布数据集的泛化能力，同时提高了它们在分布数据集上的性能。研究结果表明，Mask-tuning提高了PLMs在未见数据上的可重复使用性，使它们在实际应用中更加实用和有效。

    The reusability of state-of-the-art Pre-trained Language Models (PLMs) is often limited by their generalization problem, where their performance drastically decreases when evaluated on examples that differ from the training dataset, known as Out-of-Distribution (OOD)/unseen examples. This limitation arises from PLMs' reliance on spurious correlations, which work well for frequent example types but not for general examples. To address this issue, we propose a training approach called Mask-tuning, which integrates Masked Language Modeling (MLM) training objectives into the fine-tuning process to enhance PLMs' generalization. Comprehensive experiments demonstrate that Mask-tuning surpasses current state-of-the-art techniques and enhances PLMs' generalization on OOD datasets while improving their performance on in-distribution datasets. The findings suggest that Mask-tuning improves the reusability of PLMs on unseen data, making them more practical and effective for real-world applications
    
[^52]: 用大型语言模型生成数学导出

    Generating Mathematical Derivations with Large Language Models. (arXiv:2307.09998v1 [cs.CL])

    [http://arxiv.org/abs/2307.09998](http://arxiv.org/abs/2307.09998)

    本文利用大型语言模型生成数学导出，分析了微调模型对未见符号和方程结构更改的敏感性，结果表明微调的FLAN-T5-large（MathT5）在各个测试集上的绝对性能优于GPT模型。

    

    利用大型语言模型（LLM）在专业领域中生成数学结果的导出是一个新兴的研究方向，可以帮助识别模型的局限性，并有可能支持数学发现。本文利用符号引擎在大规模上生成方程的导出，并研究了LLM在从前提中导出目标方程时的能力。具体而言，我们采用上下文学习来对GPT进行训练，并对一系列T5模型进行了微调，以比较预训练策略对专门模型的鲁棒性和泛化能力。实证结果表明，经过微调的FLAN-T5-large（MathT5）在所有静态和超出分布的测试集上的绝对性能优于GPT模型。然而，深入分析表明，微调模型对涉及未见符号的扰动（以及在较小程度上的方程结构更改）更为敏感。此外，我们分析了1.7K个方程和200多个导出以凸显出LLM的局限性。

    The derivation of mathematical results in specialised fields using Large Language Models (LLMs) is an emerging research direction that can help identify models' limitations, and potentially support mathematical discovery. In this paper, we leverage a symbolic engine to generate derivations of equations at scale, and investigate the capabilities of LLMs when deriving goal equations from premises. Specifically, we employ in-context learning for GPT and fine-tune a range of T5 models to compare the robustness and generalisation of pre-training strategies to specialised models. Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in terms of absolute performance. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse 1.7K equations and over 200 derivations to hig
    
[^53]: 使用大型语言模型实现无监督校准的文本分类方法的先验适应

    Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v1 [cs.CL])

    [http://arxiv.org/abs/2307.06713](http://arxiv.org/abs/2307.06713)

    本文提出了一种使用大型语言模型进行文本分类的无监督校准方法，通过调整先验类别分布，实现在没有标记样本和仅少量领域内样本查询的情况下执行任务。

    

    当前有许多自然语言任务正在使用大规模语言模型（LLM）进行研究。这些模型通常通过大量无监督文本数据进行训练，并通过微调、校准或上下文学习等方法进行适应以执行下游自然语言任务。在本研究中，我们提出了一种方法，通过调整先验类别分布，实现在没有标记样本和仅少量领域内样本查询的情况下执行文本分类任务。该方法将LLM视为黑盒，在模型屏障中添加了一个阶段，用于校准模型后验以完成任务。结果表明，这些方法在不同数量的提示训练样本和无适应数据下的校准方法中优于未适应的模型。

    A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach were calibration is performed without using any adaptation data.
    
[^54]: 大型语言模型真的是良好的逻辑推理者吗？基于演绎、归纳和阿布达斯观点的全面评估。

    Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])

    [http://arxiv.org/abs/2306.09841](http://arxiv.org/abs/2306.09841)

    本文评估了大型语言模型的逻辑推理能力，选择了15个典型数据集，考虑了演绎、归纳、阿布达斯和混合推理形式，并选择了三个代表性的LLMs进行零样本、一次和三次的设置下评估。提出精细级别的评估方法。

    

    大型语言模型(LLMs)在各种自然语言任务中取得了巨大成功。对LLMs的具体推理能力进行评估，如多语言推理和数学推理，引起了广泛关注。然而，作为关键推理视角之一，逻辑推理能力还没有得到彻底评估。本文旨在填补这些差距并提供全面的评估。首先，为了进行系统化评估，本文选择了15个典型的逻辑推理数据集，并将它们组织成演绎、归纳、阿布达斯和混合形式的推理设置。考虑评估的全面性，我们选择了三个代表性的LLMs（text-davinci-003，ChatGPT和BARD），并在零样本、一次和三次的设置下对所有选择的数据集进行评估。其次，与以往仅依赖简单指标（如准确性）的评估不同，我们提出了从目标推理角度进行的精细级别评估。

    Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective 
    
[^55]: GEmo-CLAP: 面向语音情感识别的性别属性增强对比语音-语言预训练模型

    GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])

    [http://arxiv.org/abs/2306.07848](http://arxiv.org/abs/2306.07848)

    本文提出了GEmo-CLAP模型用于语音情感识别，结合了性别属性信息，相比于其他先进方法，该模型在IEMOCAP上实现了更优越的识别性能。

    

    对比语音-语言预训练（CLAP）最近在不同领域取得了惊人的成功。本文提出了一种名为GEmo-CLAP的高效性别属性增强CLAP模型，用于语音情感识别（SER）。具体而言，我们首先利用各种自监督学习的预训练模型构建了一种有效的情感CLAP模型（称为Emo-CLAP），用于SER。然后，考虑到在语音情感建模中性别属性的重要性，我们进一步提出了两种GEmo-CLAP方法，来整合语音信号的情感和性别信息，形成更合理的目标。在IEMOCAP语料库上进行的大量实验表明，我们提出的两种GEmo-CLAP方法始终优于基线Emo-CLAP模型（使用不同的预训练模型），同时与其他最先进的方法相比实现了更优越的识别性能。

    Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
    
[^56]: 利用大型语言模型在公共事务领域进行主题分类

    Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs. (arXiv:2306.02864v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.02864](http://arxiv.org/abs/2306.02864)

    LLMs在公共事务文件分类中的表现进行了分析。通过使用正则表达式工具收集了一个包含超过33K个样本和22.5M个标记的公共事务文件数据库。实验结果表明，LLMs可以有效处理和理解这类文件中使用的复杂语言，从而提高了公共事务文件的分析能力。

    

    分析公共事务文件对公民至关重要，它促进透明度、问责制和决策。它使公民能够了解政府政策，参与公共话语，并追究代表的责任。对于依靠某些规定运营的公司来说，这是至关重要的，有时甚至关乎生死。大型语言模型（LLM）有潜力通过有效处理和理解这类文件中使用的复杂语言，大大增强公共事务文件的分析能力。在这项工作中，我们分析了LLM在分类公共事务文件中的性能。作为一项自然的多标签任务，这些文件的分类具有重要挑战。在这项工作中，我们使用了一个基于正则表达式的工具来收集了一个包含超过33K个样本和22.5M个标记的公共事务文件数据库。我们的实验评估了4个不同的西班牙语LLM在最多30个不同类别的文件分类中的性能。

    The analysis of public affairs documents is crucial for citizens as it promotes transparency, accountability, and informed decision-making. It allows citizens to understand government policies, participate in public discourse, and hold representatives accountable. This is crucial, and sometimes a matter of life or death, for companies whose operation depend on certain regulations. Large Language Models (LLMs) have the potential to greatly enhance the analysis of public affairs documents by effectively processing and understanding the complex language used in such documents. In this work, we analyze the performance of LLMs in classifying public affairs documents. As a natural multi-label task, the classification of these documents presents important challenges. In this work, we use a regex-powered tool to collect a database of public affairs documents with more than 33K samples and 22.5M tokens. Our experiments assess the performance of 4 different Spanish LLMs to classify up to 30 diff
    
[^57]: 基于对比学习和深度模块化的语音分离

    Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v1 [cs.SD])

    [http://arxiv.org/abs/2305.10652](http://arxiv.org/abs/2305.10652)

    本文提出了一种基于对比学习和深度模块化的完全无监督语音分离方法，解决了有监督学习中存在的排列问题、说话人数量不匹配的问题和高质量标记数据的依赖问题。

    

    目前，语音分离的最先进工具依赖于有监督学习。这意味着它们必须处理排列问题，它们受到训练和推断中使用的说话者数量不匹配的影响。此外，它们的性能严重依赖于高质量标记数据的存在。这些问题可以通过采用完全无监督的语音分离技术有效地解决。在本文中，我们使用对比学习建立帧的表示，然后在下游的深度模块化任务中使用学习到的表示。具体而言，在语音分离中，说话人的不同帧可以被看作是给定那个说话人的隐含标准帧的增强版。说话人的帧包含足够的韵律信息重叠，这是语音分离的关键。基于此，我们实现了自监督学习，学习缩小帧之间的距离。

    The current monaural state of the art tools for speech separation relies on supervised learning. This means that they must deal with permutation problem, they are impacted by the mismatch on the number of speakers used in training and inference. Moreover, their performance heavily relies on the presence of high-quality labelled data. These problems can be effectively addressed by employing a fully unsupervised technique for speech separation. In this paper, we use contrastive learning to establish the representations of frames then use the learned representations in the downstream deep modularization task. Concretely, we demonstrate experimentally that in speech separation, different frames of a speaker can be viewed as augmentations of a given hidden standard frame of that speaker. The frames of a speaker contain enough prosodic information overlap which is key in speech separation. Based on this, we implement a self-supervised learning to learn to minimize the distance between frames
    
[^58]: 什么是新事件？在叙事中识别新事件的演变。

    Whats New? Identifying the Unfolding of New Events in Narratives. (arXiv:2302.07748v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07748](http://arxiv.org/abs/2302.07748)

    本文提出了一个新的挑战性任务：自动识别叙述中的新事件，以识别创新和贡献。他们将事件定义为主语、谓语和宾语的三元组，并将其分类为新事件，具体取决于其是否可以通过常识推理来推导。

    

    叙事包含了上下文和时间的丰富事件资源。对这些事件的自动理解提供了摘要理解，以供进一步的计算(如推理)。本文研究了事件的信息状态(IS)，并提出了一个新的有挑战性的任务:自动识别叙述中的新事件。我们将事件定义为主语、谓语和宾语的三元组。该事件相对于话语上下文被归类为新事件，并取决于是否可以通过常识推理来推导。我们使用人类标注者在公开的叙述语料库上进行了句子级别的新事件标注。我们提供了标注协议，并研究了注释的质量和任务的难度。我们公开了标注的数据集、标注材料和用于叙述理解中新事件提取的机器学习基准模型。

    Narratives include a rich source of events unfolding over time and context. Automatic understanding of these events provides a summarised comprehension of the narrative for further computation (such as reasoning). In this paper, we study the Information Status (IS) of the events and propose a novel challenging task: the automatic identification of \textit{new} events in a narrative. We define an event as a triplet of subject, predicate, and object. The event is categorized as new with respect to the discourse context and whether it can be inferred through commonsense reasoning. We annotated a publicly available corpus of narratives with the new events at sentence level using human annotators. We present the annotation protocol and study the quality of the annotation and the difficulty of the task. We publish the annotated dataset, annotation materials, and machine learning baseline models for the task of new event extraction for narrative understanding.
    
[^59]: 对阿拉伯命名实体识别的调查：过去、近期进展和未来趋势

    A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and Future Trends. (arXiv:2302.03512v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03512](http://arxiv.org/abs/2302.03512)

    本研究对阿拉伯命名实体识别（NER）的发展进行了全面回顾，尤其关注深度学习和预训练语言模型方面的最新进展。该调查总结了传统的NER方法和近年来基于深度学习的方法，并介绍了阿拉伯NER的背景和现有资源。

    

    随着越来越多的阿拉伯文本在互联网上出现，从这些阿拉伯文本中提取重要信息变得尤为有用。作为一项基本技术，命名实体识别（NER）在信息提取技术中充当核心组件，同时在许多其他自然语言处理（NLP）系统中起着至关重要的作用，如问答和知识图谱构建。在本文中，我们对阿拉伯NER的发展进行了全面回顾，特别是深度学习和预训练语言模型方面的最新进展。具体来说，我们首先介绍了阿拉伯NER的背景，包括阿拉伯语特点和现有的阿拉伯NER资源。然后，我们系统地回顾了阿拉伯NER方法的发展。传统的阿拉伯NER系统侧重于特征工程和设计特定领域的规则。近年来，通过连续向量表示文本的深度学习方法取得了显著的进展。

    As more and more Arabic texts emerged on the Internet, extracting important information from these Arabic texts is especially useful. As a fundamental technology, Named entity recognition (NER) serves as the core component in information extraction technology, while also playing a critical role in many other Natural Language Processing (NLP) systems, such as question answering and knowledge graph building. In this paper, we provide a comprehensive review of the development of Arabic NER, especially the recent advances in deep learning and pre-trained language model. Specifically, we first introduce the background of Arabic NER, including the characteristics of Arabic and existing resources for Arabic NER. Then, we systematically review the development of Arabic NER methods. Traditional Arabic NER systems focus on feature engineering and designing domain-specific rules. In recent years, deep learning methods achieve significant progress by representing texts via continuous vector repres
    
[^60]: 基于Transformer预训练语言模型的可控文本生成综述

    A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2201.05337](http://arxiv.org/abs/2201.05337)

    这篇论文综述了基于Transformer预训练语言模型的可控文本生成方法。这些方法利用大规模预训练语言模型生成多样化、流畅的文本，但由于深度神经网络的可解释性较低，需要保证其可控性。

    

    可控文本生成是自然语言生成领域中新兴的方向，被认为对于开发更自然、更符合特定应用场景的先进文本生成技术至关重要。近年来，利用大规模预训练语言模型（PLMs），尤其是广泛使用的基于Transformer的PLMs，已成为自然语言生成新范式，可以生成更多样化、更流畅的文本。然而，由于深度神经网络的可解释性较低，这些方法的可控性需要得到保证。为此，基于Transformer的PLMs的可控文本生成已成为一个快速增长但具有挑战性的新研究热点。最近3-4年出现了各种方法，针对可能需要不同类型控制约束的不同CTG任务。在本文中，我们对当前基于Transformer预训练语言模型的可控文本生成方法进行了系统的综述。

    Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that are more natural and better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the lower level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks which may require different types of controlled constraints. In this paper, we present a systematic critical review on the com
    

