# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Can MLLMs Perform Text-to-Image In-Context Learning?](https://rss.arxiv.org/abs/2402.01293) | 本论文探索了将上下文学习扩展到多模态的文本到图像转换任务，并提出了第一个T2I-ICL基准数据集CoBSAT。研究发现MLLMs在解决T2I-ICL问题时面临着多模态和图像生成的固有复杂性，并通过微调和思维链提示等策略取得了显著改进。 |
| [^2] | [Topic-based Watermarks for LLM-Generated Text](https://arxiv.org/abs/2404.02138) | 提出了一种新的基于主题的水印算法，旨在解决当前水印方案的局限性，为区分LLM生成的文本和人类生成的文本提供了新的思路。 |
| [^3] | [Octopus v2: On-device language model for super agent](https://arxiv.org/abs/2404.01744) | 该研究提出了一种新方法，通过将具有20亿参数的设备上模型赋予超越GPT-4的准确性和延迟性能，并将上下文长度缩减95％，从而解决了函数调用中的延迟和准确性问题。 |
| [^4] | [Scaling Properties of Speech Language Models](https://arxiv.org/abs/2404.00685) | 通过研究语音语言模型的尺度特性，可以预测其语言性能的扩展速度，与文本-based大型语言模型相比，语音语言模型的语言性能扩展速度慢三个数量级。 |
| [^5] | [Large Language Models Are Unconscious of Unreasonability in Math Problems](https://arxiv.org/abs/2403.19346) | 本文研究了大型语言模型在解决数学问题中对不合理性的反应，设计了不合理数学问题基准以及关键计算和结论提示模板，提升了它们在错误检测和修正方面的能力。 |
| [^6] | [Towards Explainability in Legal Outcome Prediction Models](https://arxiv.org/abs/2403.16852) | 先例是促进法律NLP模型可解释性的一种自然方式，我们提出了一种新颖的方法来识别法律结果预测模型使用的先例，并发现模型预测结果的能力不错，但其使用先例的方式与人类法官不同。 |
| [^7] | [Computational Sentence-level Metrics Predicting Human Sentence Comprehension](https://arxiv.org/abs/2403.15822) | 本研究引入了创新方法，使用多语言大型语言模型计算句子级度量，并证明这些度量能够高度准确地预测人类句子阅读速度，为未来整合LLMs和认知科学研究提供了有前景的方向。 |
| [^8] | [Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models](https://arxiv.org/abs/2403.14633) | 本文调查了大型语言模型中是否存在社会经济偏见，引入了一个新的数据集SilverSpoon，并评估了这种偏见的程度以及随着模型大小的变化。 |
| [^9] | [AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models](https://arxiv.org/abs/2403.13269) | AFLoRA是一种自适应冻结低秩调整方法，通过逐步冻结投影矩阵来提高性能，减少计算量，并提供对GLUE基准测试的最先进表现。 |
| [^10] | [ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text](https://arxiv.org/abs/2403.09131) | ProSwitch通过知识引导的指令微调，在专业和非专业风格之间生成文本，并在专业性评估和质量评估方面表现出优越性。 |
| [^11] | [Gemma: Open Models Based on Gemini Research and Technology](https://arxiv.org/abs/2403.08295) | Gemma是基于Gemini研究和技术所构建的开放模型系列，在语言理解、推理和安全性等方面表现出色，负责任地发布这些大型语言模型对于提高前沿模型的安全性至关重要。 |
| [^12] | [Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing](https://arxiv.org/abs/2403.07175) | 本文重建了ROME，提供了更稳定的r-ROME实现，解决了顺序模型编辑过程中的模型崩溃问题。 |
| [^13] | [No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks](https://arxiv.org/abs/2403.06249) | ICE-PIXIU模型将中文和英文金融分析统一，通过整合多种任务和数据集提升双语金融建模的效果。 |
| [^14] | [Can Large Language Models Automatically Score Proficiency of Written Essays?](https://arxiv.org/abs/2403.06149) | 本研究旨在测试大型语言模型在分析和评分书面作文方面的能力，通过对两种流行的LLMs进行实验，设计不同提示并在ASAP数据集上进行实验，揭示了有趣的观察结果。 |
| [^15] | [Large Language Models are In-Context Molecule Learners](https://arxiv.org/abs/2403.04197) | 提出了上下文分子适应（ICMA）范式，允许LLMs通过上下文示例学习分子-文本对齐，解决了在分子-标题翻译任务中对LLMs的挑战。 |
| [^16] | [A Measure for Transparent Comparison of Linguistic Diversity in Multilingual NLP Data Sets](https://arxiv.org/abs/2403.03909) | 本文提出了一种评估数据集语言多样性的方法，通过与参考语言样本进行对比，利用适合比较度量集合的Jaccard指数的版本来最大程度地促进长期内的多语言多样性。 |
| [^17] | [Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective](https://arxiv.org/abs/2403.01373) | 本文评估了大型视觉语言模型中的数字幻觉问题，并提出一种一致性训练方法，可使数字幻觉得到显著改善 |
| [^18] | [Cause and Effect: Can Large Language Models Truly Understand Causality?](https://arxiv.org/abs/2402.18139) | 本研究提出了一种名为CARE CA的新型架构，通过结合显式因果检测模块和反事实陈述、以及隐含因果检测模块，旨在增强大型语言模型对因果关系的理解能力。 |
| [^19] | [Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?](https://arxiv.org/abs/2402.18120) | 通过探索多语言人类价值观念，我们证实了大型语言模型中存在多语言人类价值，并揭示了价值对齐能力可以被跨语言控制。 |
| [^20] | [Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese](https://arxiv.org/abs/2402.17302) | LLM使用在印尼语方面能够生成具有足够知识的问题，但在巽他语方面表现不佳，突显中低资源语言之间的性能差距。 |
| [^21] | [MATHWELL: Generating Educational Math Word Problems at Scale](https://arxiv.org/abs/2402.15861) | 使用MATHWELL模型生成了迄今为止最大的英文数学应用题数据集，其中包含20,490个问题，经领域专家评分结果显示，MATHWELL的问题中具有可执行解决方案并符合所有标准的份额比其他选择高出40%，其中74%的可解决问题同时做到了准确和适当。 |
| [^22] | [APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2402.14866) | APTQ提出了针对大型语言模型的注意力感知后训练混合精度量化方法，在保持模型性能的同时，超越了先前的量化方法，并在零-shot任务上达到了最先进的准确率 |
| [^23] | [Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering](https://arxiv.org/abs/2402.14320) | Triad框架利用了基于多角色LLM代理来解决知识库问答问题，通过代理的不同角色分别处理KBQA子任务，合作完成KBQA任务，并在多个基准数据集上表现出色。 |
| [^24] | [Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding](https://arxiv.org/abs/2402.11809) | 提出了Smart Parallel Auto-Correct Decoding (SPACE)方法，通过集成半自回归推理和猜测解码，实现了大型语言模型推理加速和并行生成验证令牌的功能。 |
| [^25] | [Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents](https://arxiv.org/abs/2402.11651) | 大型语言模型通过整合负面示例和适当的数据清洗与微调策略，从失败中学习，提高作为代理的效果。 |
| [^26] | [From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings](https://arxiv.org/abs/2402.11512) | 提出了DeepSoftDebias算法，在不同领域数据集、准确度指标和NLP任务中全面评估，发现其在减少性别、种族和宗教偏见方面优于现有最先进方法 |
| [^27] | [Multi-dimensional Evaluation of Empathetic Dialog Responses](https://arxiv.org/abs/2402.11409) | 提出了一个多维度的共情评估框架，同时衡量了说话者角度的表达意图和听者角度的感知共情，研究发现二者是相互关联的，感知共情与对话会话的满意水平呈高相关性。 |
| [^28] | [Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability](https://arxiv.org/abs/2402.10688) | 通过整体可解释性框架，本文提出了打开大型语言模型黑匣子的方法，包括自下而上的机械解释和自上而下的表示工程视角，有助于深入理解和应用LLMs的行为和机制。 |
| [^29] | [Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs](https://arxiv.org/abs/2402.07938) | 本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介，通过对自然文本输入进行彻底分析，实现智能和响应式用户体验。 |
| [^30] | [ANLS* -- A Universal Document Processing Metric for Generative Large Language Models](https://arxiv.org/abs/2402.03848) | ANLS*是一种用于生成型模型的新度量方法，针对各种任务包括信息提取和分类任务进行评估。它扩展了现有的ANLS度量方法，可以作为替代方案使用。 |
| [^31] | [PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction](https://arxiv.org/abs/2310.18463) | 我们提出了PeTailor，这是一个基于检索的框架，通过使用定制的分块评分器从预先构建的分块数据库中检索相关文档，并将检索到的信息集成到大型语言模型（LLM）的输入中，以改进生物医学三元组提取的效果。 |
| [^32] | [Explicitly Representing Syntax Improves Sentence-to-layout Prediction of Unexpected Situations.](http://arxiv.org/abs/2401.14212) | 本文研究了句子到布局预测任务中的语法表示对模型性能的影响。实验结果显示，显式表示语法增强了模型对意外情况的预测能力，但对于未在训练集中出现的句子结构仍存在困难。 |
| [^33] | [Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment.](http://arxiv.org/abs/2401.10768) | 本文提出了一种称为知识一致性对齐（KCA）的方法，通过减少训练数据中外部知识和预训练语料库中内在知识之间的不一致性，从而缓解了大型语言模型产生幻觉的问题。实验结果表明，KCA方法在多个基准测试中取得了优异的性能。 |
| [^34] | [PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering.](http://arxiv.org/abs/2401.02797) | 本文提出了一种针对医学视觉问答的多模态大型语言模型的参数高效微调框架，并在公共数据集上进行了验证。实验证明，该模型在封闭式问题上比GPT-4v模型的准确率提高了26％。 |
| [^35] | [Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code.](http://arxiv.org/abs/2311.07989) | 这篇论文系统地回顾了代码处理方面的语言模型的最新进展，涵盖了50多个模型、30多个评估任务、170多个数据集和700多个相关工作。它突出了代码建模从统计模型和RNN到预训练的Transformer和LLM之间的历史转变，并讨论了代码特定的特性和关键挑战。 |
| [^36] | [Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism.](http://arxiv.org/abs/2311.01041) | 本文提出了一种学会拒绝（L2R）的简单而有效的解决方案，通过引入拒绝机制，使大型语言模型（LLMs）能够识别和拒绝难以回答的问题，从而提高模型的可控性和可靠性。 |
| [^37] | [Large Language Models as Generalizable Policies for Embodied Tasks.](http://arxiv.org/abs/2310.17722) | 本研究展示了大型语言模型(LLMs)可以被适应为具有普适性的机器人任务策略。通过我们的方法LLaRP，我们成功将预训练的冻结LLM用于接收指令和视觉输入，并在环境中直接输出动作。我们的实验结果表明LLaRP不仅对任务指令的复杂改写具有鲁棒性，而且可以推广到需要新颖最优行为的新任务。在大量未见任务中，LLaRP表现出了显著的成功率提升，并且我们提供了一个新的基准测试(Language Rearrangement)来促进进一步的研究。 |
| [^38] | [Automatic Macro Mining from Interaction Traces at Scale.](http://arxiv.org/abs/2310.07023) | 本文介绍了一种基于大型语言模型的新方法，用于从移动交互轨迹中自动提取语义上有意义的宏任务。通过多项研究和实验证明了该方法的有效性和提取的宏任务的实用性。 |
| [^39] | [A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models.](http://arxiv.org/abs/2309.10003) | 本文提出了一种使用概率从语言模型中获得的自信息来测量专利权要求范围的新方法。该方法通过计算要求的发生概率和自信息来评估要求的信息量，进而反映出要求的范围。研究结果表明，不同类型的语言模型对范围测量的影响不同，最简单的模型可以将范围度量简化为单词或字符计数的倒数。此方法在九个系列的专利权要求上进行了验证，结果表明各系列的要求范围逐渐减小。 |
| [^40] | [X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs.](http://arxiv.org/abs/2309.08873) | X-PARADE是第一个跨语言段落级别信息分歧的数据集，通过将来源于不同语言的维基百科页面上的段落对齐，标注者评估了目标语言段落与源语言段落之间的信息是否相同、新的或者可以推断，为解决这个问题提供了一个全面的数据集。 |
| [^41] | [How Good Are Large Language Models at Out-of-Distribution Detection?.](http://arxiv.org/abs/2308.10261) | 本文通过对大型语言模型进行实证调查，探索了分布外检测的能力。作者发现了LLM在分布外检测方面的差异，并采用了新的生成式微调方法，提高了模型的性能。 |
| [^42] | [WebArena: A Realistic Web Environment for Building Autonomous Agents.](http://arxiv.org/abs/2307.13854) | WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。 |
| [^43] | [Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step.](http://arxiv.org/abs/2306.14050) | 本研究提出了符号化思维链提炼（SCoTD）方法，该方法使用从大型教师模型中抽样出的合理化解释来训练数量级更小的学生模型，从而实现小模型也能受益于思维链提示，提升模型性能。 |
| [^44] | [RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning.](http://arxiv.org/abs/2305.14502) | 本文提出了一种 RetICL 的方法来优化地选择用于上下文学习模型的示例。此方法利用强化学习框架将序列示例选择问题作为马尔科夫决策过程，并且优化选择为使任务表现最佳的组合。 |
| [^45] | [Iterated learning and communication jointly explain efficient color naming systems.](http://arxiv.org/abs/2305.10154) | 本论文通过结合迭代学习和交流的文化进化模型，展示这个模型能够在神经网络中实现并收敛于高效的颜色命名系统，进一步证明了语义系统反映效率压力的观点。 |
| [^46] | [Controlling High-Dimensional Data With Sparse Input.](http://arxiv.org/abs/2303.09446) | 本论文提出了一种新的控制机制，即将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。通过实验，此方法表现出了效率、鲁棒性和保真性，即使只有少量的输入数值。 |

# 详细

[^1]: MLLMs能否进行上下文学习的文本到图像转换？

    Can MLLMs Perform Text-to-Image In-Context Learning?

    [https://rss.arxiv.org/abs/2402.01293](https://rss.arxiv.org/abs/2402.01293)

    本论文探索了将上下文学习扩展到多模态的文本到图像转换任务，并提出了第一个T2I-ICL基准数据集CoBSAT。研究发现MLLMs在解决T2I-ICL问题时面临着多模态和图像生成的固有复杂性，并通过微调和思维链提示等策略取得了显著改进。

    

    从大型语言模型（LLMs）发展到多模式大型语言模型（MLLMs）推动了将上下文学习（ICL）扩展到多模式的研究。现有的研究主要集中在图像到文本的ICL上。然而，文本到图像的ICL（T2I-ICL）具有独特的特性和潜在的应用，但仍然少有研究。为了填补这个空白，我们正式定义了T2I-ICL任务，并提出了CoBSAT，第一个包含十个任务的T2I-ICL基准数据集。利用我们的数据集评估了六个最先进的MLLMs，我们发现MLLMs在解决T2I-ICL问题时面临着相当大的困难。我们确定了多模态和图像生成的固有复杂性是主要挑战。为了克服这些挑战，我们探索了微调和思维链提示等策略，并取得了显著的改进。我们的代码和数据集可以在\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}上获得。

    The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
    
[^2]: 基于主题的LLM生成文本的水印

    Topic-based Watermarks for LLM-Generated Text

    [https://arxiv.org/abs/2404.02138](https://arxiv.org/abs/2404.02138)

    提出了一种新的基于主题的水印算法，旨在解决当前水印方案的局限性，为区分LLM生成的文本和人类生成的文本提供了新的思路。

    

    大型语言模型（LLMs）的最新进展导致了生成的文本输出与人类生成的文本相似度难以分辨。水印算法是潜在工具，通过在LLM生成的输出中嵌入可检测的签名，可以区分LLM生成的文本和人类生成的文本。然而，当前的水印方案在已知攻击下缺乏健壮性。此外，考虑到LLM每天生成数万个文本输出，水印算法需要记忆每个输出才能让检测正常工作，这是不切实际的。本文针对当前水印方案的局限性，提出了针对LLMs的“基于主题的水印算法”概念。

    arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
    
[^3]: Octopus v2：用于超级代理的设备上语言模型

    Octopus v2: On-device language model for super agent

    [https://arxiv.org/abs/2404.01744](https://arxiv.org/abs/2404.01744)

    该研究提出了一种新方法，通过将具有20亿参数的设备上模型赋予超越GPT-4的准确性和延迟性能，并将上下文长度缩减95％，从而解决了函数调用中的延迟和准确性问题。

    

    语言模型在各种软件应用中展现出了高效性，特别是与自动工作流相关的任务。这些模型具有调用函数的关键能力，在创建AI代理时至关重要。尽管大规模语言模型在云环境中表现出色，但往往存在着隐私和成本方面的担忧。当前用于函数调用的设备上模型面临延迟和准确性问题。我们的研究提出了一种新方法，使具有20亿参数的设备上模型在准确性和延迟方面超越了GPT-4，并将上下文长度缩减了95%。与基于RAG的函数调用机制的Llama-7B相比，我们的方法将延迟提高了35倍。这种方法将延迟降低到适合在生产环境中的各种边缘设备上部署的水平上，符合性能要求。

    arXiv:2404.01744v1 Announce Type: new  Abstract: Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisite
    
[^4]: 语音语言模型的尺度特性

    Scaling Properties of Speech Language Models

    [https://arxiv.org/abs/2404.00685](https://arxiv.org/abs/2404.00685)

    通过研究语音语言模型的尺度特性，可以预测其语言性能的扩展速度，与文本-based大型语言模型相比，语音语言模型的语言性能扩展速度慢三个数量级。

    

    语音语言模型（SLM）旨在从原始音频中学习语言，而无需文本资源。尽管取得了显著进展，我们当前的模型表现出弱的句法和语义能力。然而，如果神经语言模型的尺度特性对语音模态成立，这些能力将随着训练所使用的计算量增加而提高。本文利用模型的尺度行为来估计我们当前方法将产生具有文本-based大型语言模型（LLMs）英语熟练度的SLM的尺度。

    arXiv:2404.00685v1 Announce Type: cross  Abstract: Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost
    
[^5]: 大型语言模型在数学问题中对不合理性毫无意识

    Large Language Models Are Unconscious of Unreasonability in Math Problems

    [https://arxiv.org/abs/2403.19346](https://arxiv.org/abs/2403.19346)

    本文研究了大型语言模型在解决数学问题中对不合理性的反应，设计了不合理数学问题基准以及关键计算和结论提示模板，提升了它们在错误检测和修正方面的能力。

    

    大型语言模型(LLMs)展示了在解决数学问题方面的巨大能力。然而，当给出包含不合理错误的问题时，它们倾向于产生幻觉。在本文中，我们研究了LLMs在面对不合理数学问题时的行为，并进一步探讨了它们解决这些问题的潜力。首先，我们构建了不合理数学问题(UMP)基准来检查LLMs的错误检测能力。实验证明，LLMs能够检测到不合理错误，但仍然在生成非幻觉内容方面失败。为了改善它们的错误检测和修正能力，我们进一步设计了一种称为关键计算和结论(CCC)的战略提示模板。通过CCC，LLMs可以更好地自我评估并检测数学问题中的不合理错误，使它们在实际应用场景中更可靠和安全。

    arXiv:2403.19346v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors. In this paper, we study the behavior of LLMs when faced with unreasonable math problems and further explore their potential to address these problems. First, we construct the Unreasonable Math Problem (UMP) benchmark to examine the error detection ability of LLMs. Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content. In order to improve their ability of error detection and correction, we further design a strategic prompt template called Critical Calculation and Conclusion(CCC). With CCC, LLMs can better self-evaluate and detect unreasonable errors in math questions, making them more reliable and safe in practical application scenarios.
    
[^6]: 在法律结果预测模型中迈向可解释性

    Towards Explainability in Legal Outcome Prediction Models

    [https://arxiv.org/abs/2403.16852](https://arxiv.org/abs/2403.16852)

    先例是促进法律NLP模型可解释性的一种自然方式，我们提出了一种新颖的方法来识别法律结果预测模型使用的先例，并发现模型预测结果的能力不错，但其使用先例的方式与人类法官不同。

    

    当前的法律结果预测模型 - 法律NLP的基本组成部分 - 不能解释其推理过程。然而，为了在现实世界中应用这些模型，人类法律主体需要能够理解它们的决策。在普通法案例中，法律从业者通过参考被称为先例的过去案例法律推理到案件结果。我们认为，先例因此成为促进法律NLP模型可解释性的一种自然方式。在本文中，我们提出了一种新颖的方法来识别法律结果预测模型使用的先例。此外，通过制定法律先例的分类法，我们能够比较人类法官和我们的模型在他们依赖的不同类型先例方面的差异。我们发现，虽然模型学会了合理地预测结果，但它们使用的先例方式不同于人类法官。

    arXiv:2403.16852v1 Announce Type: cross  Abstract: Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand their decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and our models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.
    
[^7]: 计算句子级度量预测人类句子理解

    Computational Sentence-level Metrics Predicting Human Sentence Comprehension

    [https://arxiv.org/abs/2403.15822](https://arxiv.org/abs/2403.15822)

    本研究引入了创新方法，使用多语言大型语言模型计算句子级度量，并证明这些度量能够高度准确地预测人类句子阅读速度，为未来整合LLMs和认知科学研究提供了有前景的方向。

    

    计算心理语言学的研究大多集中在单词处理上。本研究引入了创新方法，使用多语言大型语言模型计算句子级度量。开发的度量包括句子意外性和句子相关性，然后经过测试和比较以验证它们是否可以预测人类如何跨语言整体理解句子。这些度量提供了重要的可解释性，并在预测人类句子阅读速度方面取得了很高的准确性。我们的结果表明，这些计算的句子级度量在预测和阐明读者在理解整体句子时遇到的处理困难方面异常有效，可跨越多种语言。它们出色的性能和泛化能力为未来在整合LLMs和认知科学方面的研究提供了一个有前途的途径。

    arXiv:2403.15822v1 Announce Type: new  Abstract: The majority of research in computational psycholinguistics has concentrated on the processing of words. This study introduces innovative methods for computing sentence-level metrics using multilingual large language models. The metrics developed sentence surprisal and sentence relevance and then are tested and compared to validate whether they can predict how humans comprehend sentences as a whole across languages. These metrics offer significant interpretability and achieve high accuracy in predicting human sentence reading speeds. Our results indicate that these computational sentence-level metrics are exceptionally effective at predicting and elucidating the processing difficulties encountered by readers in comprehending sentences as a whole across a variety of languages. Their impressive performance and generalization capabilities provide a promising avenue for future research in integrating LLMs and cognitive science.
    
[^8]: 出身富贵？探讨大型语言模型中的社会经济偏见

    Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models

    [https://arxiv.org/abs/2403.14633](https://arxiv.org/abs/2403.14633)

    本文调查了大型语言模型中是否存在社会经济偏见，引入了一个新的数据集SilverSpoon，并评估了这种偏见的程度以及随着模型大小的变化。

    

    社会经济偏见在社会中加剧了不公平现象，根据个人经济和社会背景影响获取机会和资源的机会。这一普遍问题持续地延续了系统性的不平等，阻碍了作为一个社会追求包容性进步。在本文中，我们调查了大型语言模型中是否存在社会经济偏见。为此，我们引入了一个新的数据集（SilverSpoon），包含3000个样本，展示了牵涉到弱势群体由于他们的处境而实施道德模糊行为的假设情景，并问这种行为是否在道德上成立。此外，这个数据集具有双重标记方案，并由属于社会经济两端的人进行了注释。使用SilverSpoon，我们评估了大型语言模型中表现出的社会经济偏见程度以及该程度如何随模型大小变化。

    arXiv:2403.14633v1 Announce Type: cross  Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. W
    
[^9]: AFLoRA: 自适应冻结低秩调整在大型模型参数高效微调中的应用

    AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models

    [https://arxiv.org/abs/2403.13269](https://arxiv.org/abs/2403.13269)

    AFLoRA是一种自适应冻结低秩调整方法，通过逐步冻结投影矩阵来提高性能，减少计算量，并提供对GLUE基准测试的最先进表现。

    

    我们提出了一种新颖的参数高效微调（PEFT）方法，称为自适应冻结低秩调整（AFLoRA）。具体地，对于每个预训练的冻结权重张量，我们添加一个可训练的低秩矩阵并行路径，即下投影和上投影矩阵，每个矩阵后面跟着一个特征变换向量。基于一种新颖的冻结分数，我们在微调过程中逐步冻结这些投影矩阵，以减少计算量并减轻过拟合。我们的实验结果表明，我们可以在GLUE基准测试中获得最先进的性能，平均改善高达0.85％，同时可减少高达9.5倍的平均可训练参数。在运行时间方面，与类似的PEFT备选方案相比，AFLoRA可以提供高达1.86倍的改进。除了我们方法的实际效用之外，我们还提供了关于训练

    arXiv:2403.13269v1 Announce Type: new  Abstract: We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the train
    
[^10]: ProSwitch：知识引导的语言模型微调，生成专业和非专业风格的文本

    ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text

    [https://arxiv.org/abs/2403.09131](https://arxiv.org/abs/2403.09131)

    ProSwitch通过知识引导的指令微调，在专业和非专业风格之间生成文本，并在专业性评估和质量评估方面表现出优越性。

    

    大语言模型（LLMs）在各种语言应用中表现出有效性，包括文本摘要和可控文本生成。然而，关于它们通过微调在不同风格间切换的能力的研究仍未被充分探讨。本研究聚焦于文本专业性，并引入了一种新颖的方法，名为ProSwitch，通过知识引导的指令微调，使语言模型具备生成专业和非专业回复的能力。ProSwitch分为三个阶段：数据准备，用于收集领域知识和训练语料库；指令微调，用于优化带有多种指令格式的语言模型；全面评估，用于评估生成文本的专业性区分能力和基于参考的质量。 ProSwitch相对于通用和专门语言模型的比较分析显示了我们的方法的优越性。

    arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
    
[^11]: Gemma：基于Gemini研究和技术的开放模型

    Gemma: Open Models Based on Gemini Research and Technology

    [https://arxiv.org/abs/2403.08295](https://arxiv.org/abs/2403.08295)

    Gemma是基于Gemini研究和技术所构建的开放模型系列，在语言理解、推理和安全性等方面表现出色，负责任地发布这些大型语言模型对于提高前沿模型的安全性至关重要。

    

    本文介绍了Gemma，这是一个基于Gemini模型研究和技术构建的轻量级、最先进的开放模型系列。Gemma模型在语言理解、推理和安全性等学术基准上表现出色。我们发布了两个规模的模型（20亿和70亿参数），并提供了预训练和微调的检查点。Gemma在18个基于文本的任务中，有11个任务优于类似规模的开放模型，并对模型的安全性和责任方面进行了全面评估，同时详细描述了模型开发过程。我们相信负责任地发布大型语言模型对于提高前沿模型的安全性，并实现下一波大型语言模型创新至关重要。

    arXiv:2403.08295v1 Announce Type: cross  Abstract: This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.
    
[^12]: 重建ROME: 解决顺序模型编辑过程中的模型崩溃问题

    Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing

    [https://arxiv.org/abs/2403.07175](https://arxiv.org/abs/2403.07175)

    本文重建了ROME，提供了更稳定的r-ROME实现，解决了顺序模型编辑过程中的模型崩溃问题。

    

    最近关于使用Rank-One Model Editing (ROME)进行模型编辑的研究表明，有一些事实表明该算法无法进行编辑而不破坏模型。这些编辑以前被称为禁用编辑。这些禁用编辑会导致立即模型崩溃，并限制了ROME用于顺序编辑的使用。在本文中，我们做出了两个主要贡献。首先，我们展示了在使用CounterFact数据集进行编辑时，ROME仅在此时发生模型崩溃，并在使用zsRE数据集时不会发生。其次，我们发现禁用编辑是ROME原始实现的产物。通过本文，我们提供了一个更稳定的实现ROME，我们将其称为r-ROME，并展示我们在使用ROME进行大规模顺序编辑时不再观察到模型崩溃。

    arXiv:2403.07175v1 Announce Type: cross  Abstract: Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we make two main contributions. Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset. Secondly, we find that disabling edits are an artifact of the original implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME.
    
[^13]: 没有孤岛语言:统一中英文金融大型语言模型、指导数据和基准

    No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks

    [https://arxiv.org/abs/2403.06249](https://arxiv.org/abs/2403.06249)

    ICE-PIXIU模型将中文和英文金融分析统一，通过整合多种任务和数据集提升双语金融建模的效果。

    

    随着大型语言模型（LLM）的发展显著推动了金融分析，但它们的应用主要局限在单一语言领域，未充分开发中英文双语能力的潜力。为弥合这一鸿沟，我们引入 ICE-PIXIU，无缝融合 ICE-INTENT 模型和 ICE-FLARE 双语金融分析基准。ICE-PIXIU 独特地整合了一系列中文任务，以及翻译和原始英文数据集，丰富了双语金融建模的广度和深度。它提供了对多种模型变体的无限访问权限，一个包含多种跨语言和多模态指导数据的大量编译，以及一个具有专家注释的评估基准，包括 10 个 NLP 任务，20 个双语专用任务，共计1,185k 数据集。我们的彻底评估强调了将这些双语数据集纳入的优势，尤其在t

    arXiv:2403.06249v1 Announce Type: cross  Abstract: While the progression of Large Language Models (LLMs) has notably propelled financial analysis, their application has largely been confined to singular language realms, leaving untapped the potential of bilingual Chinese-English capacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating the ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis. ICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated and original English datasets, enriching the breadth and depth of bilingual financial modeling. It provides unrestricted access to diverse model variants, a substantial compilation of diverse cross-lingual and multi-modal instruction data, and an evaluation benchmark with expert annotations, comprising 10 NLP tasks, 20 bilingual specific tasks, totaling 1,185k datasets. Our thorough evaluation emphasizes the advantages of incorporating these bilingual datasets, especially in t
    
[^14]: 大型语言模型能否自动评分写作文章的能力？

    Can Large Language Models Automatically Score Proficiency of Written Essays?

    [https://arxiv.org/abs/2403.06149](https://arxiv.org/abs/2403.06149)

    本研究旨在测试大型语言模型在分析和评分书面作文方面的能力，通过对两种流行的LLMs进行实验，设计不同提示并在ASAP数据集上进行实验，揭示了有趣的观察结果。

    

    虽然在过去50年中提出了几种方法来解决自动评分作文（AES）的问题，但在效果方面仍有许多不足之处。大型语言模型（LLMs）是基于Transformer的模型，在各种任务上展示了非凡的能力。本文测试了LLMs的能力，鉴于它们强大的语言知识，来分析和有效评分书面作文。我们对两种流行的LLMs进行了实验，分别是ChatGPT和Llama。我们旨在检查这些模型是否能够完成这项任务，以及它们在两个层面上的表现如何，即在整体上和在个体写作特征上。我们利用提示工程策略设计了四个不同的提示，以发挥它们在这项任务中的最大潜力。我们在ASAP数据集上进行的实验揭示了几个有趣的观察结果。

    arXiv:2403.06149v1 Announce Type: cross  Abstract: Although several methods were proposed to address the problem of automated essay scoring (AES) in the last 50 years, there is still much to desire in terms of effectiveness. Large Language Models (LLMs) are transformer-based models that demonstrate extraordinary capabilities on various tasks. In this paper, we test the ability of LLMs, given their powerful linguistic knowledge, to analyze and effectively score written essays. We experimented with two popular LLMs, namely ChatGPT and Llama. We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait. We utilized prompt-engineering tactics in designing four different prompts to bring their maximum potential to this task. Our experiments conducted on the ASAP dataset revealed several interesting observations. First, choosing the right prompt depend
    
[^15]: 大规模语言模型是上下文分子学习器

    Large Language Models are In-Context Molecule Learners

    [https://arxiv.org/abs/2403.04197](https://arxiv.org/abs/2403.04197)

    提出了上下文分子适应（ICMA）范式，允许LLMs通过上下文示例学习分子-文本对齐，解决了在分子-标题翻译任务中对LLMs的挑战。

    

    大型语言模型（LLMs）在生物化学任务中表现出色，尤其是分子标题翻译任务，旨在弥合分子和自然语言文本之间的差距。然而，先前在适应LLMs到分子-标题翻译任务中的方法需要额外的领域特定预训练阶段，存在分子和文本空间之间的弱对齐，或对LLMs的规模有严格要求。为了解决这些挑战，我们提出了上下文分子适应（ICMA），作为一种新的范例，允许LLMs通过上下文示例学习分子-文本对齐，通过上下文分子调整。具体而言，ICMA包括以下三个阶段：跨模态检索、检索后排序和上下文分子调整。

    arXiv:2403.04197v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Addi
    
[^16]: 用于透明比较多语言自然语言处理数据集语言多样性的度量方法

    A Measure for Transparent Comparison of Linguistic Diversity in Multilingual NLP Data Sets

    [https://arxiv.org/abs/2403.03909](https://arxiv.org/abs/2403.03909)

    本文提出了一种评估数据集语言多样性的方法，通过与参考语言样本进行对比，利用适合比较度量集合的Jaccard指数的版本来最大程度地促进长期内的多语言多样性。

    

    类型学多样化的基准越来越多地被创建来追踪多语言自然语言处理中取得的进展。这些数据集的语言多样性通常被测量为样本中包含的语言或语言家族的数量，但这些度量并未考虑所包含语言的结构特性。在本文中，我们提出将数据集的语言多样性与参考语言样本进行评估，作为最大程度地促进长期内的语言多样性的一种方式。我们将语言表示为特征集，并应用适用于比较度量集合的Jaccard指数的版本。除了从类型学数据库中提取的特征之外，我们还提出一种自动的基于文本的度量方法，可以作为克服手动收集特征中已知的数据稀疏问题的手段。我们的多样性评分在语言特征方面是可解释的，并且可以识别语言类型

    arXiv:2403.03909v1 Announce Type: new  Abstract: Typologically diverse benchmarks are increasingly created to track the progress achieved in multilingual NLP. Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages. In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run. We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures. In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features. Our diversity score is interpretable in terms of linguistic features and can identify the types of lang
    
[^17]: 评估和减少大规模视觉语言模型中的数字幻觉：一种一致性视角

    Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective

    [https://arxiv.org/abs/2403.01373](https://arxiv.org/abs/2403.01373)

    本文评估了大型视觉语言模型中的数字幻觉问题，并提出一种一致性训练方法，可使数字幻觉得到显著改善

    

    大型视觉语言模型已经展示出在处理文本和视觉内容相关挑战方面的显著功效。然而，这些模型容易出现各种幻觉。本文关注一种新形式的幻觉，称为数字幻觉，指的是模型未能准确识别图像中物体的数量的情况。我们建立了一个数据集，并采用评估指标评估数字幻觉，揭示了这一问题在主流大型视觉语言模型(LVLMs)中的明显普遍性。此外，我们从两个相关视角深入分析了数字幻觉，考察了内在和外在的不一致问题。我们认为这种不一致性是数字幻觉的一个原因，并提出了一种一致性训练方法作为减轻此类幻觉的手段，该方法取得了8%的平均改进。

    arXiv:2403.01373v1 Announce Type: new  Abstract: Large vision language models have demonstrated remarkable efficacy in addressing challenges related to both textual and visual content. Nevertheless, these models are susceptible to various hallucinations. In this paper, we focus on a new form of hallucination, specifically termed as number hallucination, which denotes instances where models fail to accurately identify the quantity of objects in an image. We establish a dataset and employ evaluation metrics to assess number hallucination, revealing a pronounced prevalence of this issue across mainstream large vision language models (LVLMs). Additionally, we delve into a thorough analysis of number hallucination, examining inner and outer inconsistency problem from two related perspectives. We assert that this inconsistency is one cause of number hallucination and propose a consistency training method as a means to alleviate such hallucination, which achieves an average improvement of 8\%
    
[^18]: 因果关系：大型语言模型真正理解因果关系吗？

    Cause and Effect: Can Large Language Models Truly Understand Causality?

    [https://arxiv.org/abs/2402.18139](https://arxiv.org/abs/2402.18139)

    本研究提出了一种名为CARE CA的新型架构，通过结合显式因果检测模块和反事实陈述、以及隐含因果检测模块，旨在增强大型语言模型对因果关系的理解能力。

    

    随着大型语言模型（LLMs）的兴起，理解它们在解读和解释语言所涉及的复杂因果关系的能力和局限性变得至关重要。目前的方法使用明确或隐含的因果推理，然而迫切需要一种统一的方法，将两者结合起来更有效地处理各种因果关系。本研究提出了一种新颖的架构，称为具有反事实分析的上下文感知推理增强（CARE CA）框架，以增强因果推理和可解释性。所提出的框架将 ConceptNet 和反事实陈述中的明确因果检测模块以及通过LLMs进行的隐含因果检测相结合。我们的框架通过一层反事实解释进一步突出LLMs对因果关系的理解。ConceptNet 中的知识提高了多

    arXiv:2402.18139v1 Announce Type: cross  Abstract: With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multi
    
[^19]: 在大型语言模型中探索多语言人类价值观念：价值观齐整性、跨语言转移性和可控性是否具有一致性？

    Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?

    [https://arxiv.org/abs/2402.18120](https://arxiv.org/abs/2402.18120)

    通过探索多语言人类价值观念，我们证实了大型语言模型中存在多语言人类价值，并揭示了价值对齐能力可以被跨语言控制。

    

    先前在表示工程领域的研究揭示了LLMs在其表示空间中编码概念，主要围绕英语展开。在这项研究中，我们将这一理念扩展到多语境场景，深入探讨LLMs中的多语言人类价值概念。通过我们对7种人类价值、16种语言以及3个具有明显多语特性的LLM系列进行的全面探索，我们从实证角度证实了LLMs中存在多语言人类价值观念。对这些概念的跨语言分析进一步揭示了由于语言资源差异而产生的3个特征：跨语言不一致性、扭曲的语言关系以及高资源语言和低资源语言之间在人类价值概念方面的单向跨语言转移。此外，我们验证了通过利用主导语言作为信息源实现对LLMs价值观齐整性的跨语言控制的可行性。

    arXiv:2402.18120v1 Announce Type: new  Abstract: Prior research in representation engineering has revealed that LLMs encode concepts within their representation spaces, predominantly centered around English. In this study, we extend this philosophy to a multilingual scenario, delving into multilingual human value concepts in LLMs. Through our comprehensive exploration covering 7 types of human values, 16 languages and 3 LLM series with distinct multilinguality, we empirically substantiate the existence of multilingual human values in LLMs. Further cross-lingual analysis on these concepts discloses 3 traits arising from language resource disparities: cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts. Additionally, we validate the feasibility of cross-lingual control over value alignment capabilities of LLMs, leveraging the dominant language as a source 
    
[^20]: LLM能否生成与文化相关的常识性问答数据？印尼和巽他语的案例研究

    Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese

    [https://arxiv.org/abs/2402.17302](https://arxiv.org/abs/2402.17302)

    LLM使用在印尼语方面能够生成具有足够知识的问题，但在巽他语方面表现不佳，突显中低资源语言之间的性能差距。

    

    大型语言模型(LLMs)越来越多地被用于生成合成数据以训练和评估模型。然而，目前尚不清楚它们是否能够生成一个融入语言中知识和文化细微差别的高质量问答(QA)数据集，尤其是对于资源匮乏的语言。在这项研究中，我们调查了使用LLMs生成印尼语和巽他语文化相关常识性问答数据集的有效性。为此，我们使用包括LLMs和人类标注者在内的各种方法为这些语言创建数据集。我们的实验表明，目前性能最佳的LLM，GPT-4 Turbo，能够生成印尼语中具有足够知识的问题，但在巽他语中却不行，突出了中低资源语言之间的性能差距。我们还在我们生成的数据集上对各种LLMs进行基准测试，发现它们在......

    arXiv:2402.17302v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators. Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages. We also benchmark various LLMs on our generated datasets and find that they perform better on 
    
[^21]: MATHWELL: 在规模上生成教育数学应用题

    MATHWELL: Generating Educational Math Word Problems at Scale

    [https://arxiv.org/abs/2402.15861](https://arxiv.org/abs/2402.15861)

    使用MATHWELL模型生成了迄今为止最大的英文数学应用题数据集，其中包含20,490个问题，经领域专家评分结果显示，MATHWELL的问题中具有可执行解决方案并符合所有标准的份额比其他选择高出40%，其中74%的可解决问题同时做到了准确和适当。

    

    数学应用题在K-8教育中至关重要，但编写它们耗时且需要领域专业知识。我们认为语言模型可以通过自动生成规模化问题来支持K-8数学教育。为了教育性，生成的问题必须是1）可解决的，2）准确的，3）适当的。现有数据集未标记这些标准，因此不适合训练问题生成器。我们引入了MATHWELL，这是一个经过专家注释数据进行迭代微调的70B Llama-2模型，用于生成K-8数学应用题。借助MATHWELL，我们生成了迄今为止最大的英文应用题数据集，其中包含20,490个问题。经领域专家评分的3,484个问题发现，MATHWELL拥有比其他选择更高的可执行解决方案和满足所有标准的问题份额高出40％，其中74％的问题具有可解的、准确的和适当的解决方案。

    arXiv:2402.15861v1 Announce Type: new  Abstract: Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate.
    
[^22]: APTQ: 针对大型语言模型的注意力感知后训练混合精度量化

    APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models

    [https://arxiv.org/abs/2402.14866](https://arxiv.org/abs/2402.14866)

    APTQ提出了针对大型语言模型的注意力感知后训练混合精度量化方法，在保持模型性能的同时，超越了先前的量化方法，并在零-shot任务上达到了最先进的准确率

    

    大型语言模型（LLMs）极大地推动了自然语言处理范式。然而，高计算负载和巨大的模型尺寸对在边缘设备上部署构成了巨大挑战。为此，我们提出了针对LLMs的APTQ（Attention-aware Post-Training Mixed-Precision Quantization），该方法不仅考虑了每层权重的二阶信息，而且首次考虑了注意力输出对整个模型的非线性影响。我们利用Hessian迹作为混合精度量化的敏感度度量，确保经过理性的精度降低能保持模型性能。实验表明，APTQ超越了先前的量化方法，在C4数据集中以平均4位宽度获得5.22困惑度，几乎等效于全精度。此外，APTQ在LLaMa-7B和LLaMa-1中以平均3.8位宽度达到了68.24％和70.48％的最先进零-shot准确率。

    arXiv:2402.14866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-1
    
[^23]: Triad: 一个利用基于多角色LLM代理的框架来解决知识库问答问题

    Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering

    [https://arxiv.org/abs/2402.14320](https://arxiv.org/abs/2402.14320)

    Triad框架利用了基于多角色LLM代理来解决知识库问答问题，通过代理的不同角色分别处理KBQA子任务，合作完成KBQA任务，并在多个基准数据集上表现出色。

    

    最近基于LLM代理的进展在各种任务中展现出了令人期待的结果。然而，它们在回答知识库中问题的运用仍然鲜为人知。使用传统方法来实现KBQA系统具有挑战性，因为缺乏特定任务训练数据以及创建以任务为中心的模型结构的复杂性。在本文中，我们提出了Triad，一个利用具有三个角色的LLM代理的统一框架来进行KBQA任务。代理被分配三个角色来处理不同的KBQA子任务：作为掌握各种子任务的通才，作为选择候选者的决策者，以及作为回答带有知识的问题的顾问。我们的KBQA框架在四个阶段中执行，涉及代理的多重角色的协作。我们使用三个基准数据集评估了我们框架的性能，结果显示我们的框架胜过了

    arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
    
[^24]: 智能并行自动纠错解码：加速大型语言模型推理

    Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding

    [https://arxiv.org/abs/2402.11809](https://arxiv.org/abs/2402.11809)

    提出了Smart Parallel Auto-Correct Decoding (SPACE)方法，通过集成半自回归推理和猜测解码，实现了大型语言模型推理加速和并行生成验证令牌的功能。

    

    这项研究旨在加速拥有数十亿参数的大型语言模型（LLMs）的推理速度。我们提出了“智能并行自动纠错解码”（SPACE），这是一种创新方法，旨在实现LLMs的无损加速。通过集成半自回归推理和猜测解码能力，SPACE独特地使自回归LLMs能够并行生成和验证令牌。这是通过专门的半自回归监督微调过程实现的，该过程使现有LLMs具有同时预测多个令牌的能力。此外，一种自动纠错解码算法促进了单个模型调用内令牌序列的同时生成和验证。通过在一系列LLMs上进行广泛实验证明，SPACE在HumanEval-X上表现出2.7倍至4.0倍的推理加速。

    arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
    
[^25]: 从失败中学习：在对大型语言模型进行微调时整合负面示例

    Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents

    [https://arxiv.org/abs/2402.11651](https://arxiv.org/abs/2402.11651)

    大型语言模型通过整合负面示例和适当的数据清洗与微调策略，从失败中学习，提高作为代理的效果。

    

    大型语言模型(LLMs)在充当与环境进行交互的工具（如搜索引擎）时取得了成功。然而，LLMs在训练或对齐过程中并未专门针对工具使用进行优化，限制了它们作为代理的效果。为解决这一问题，之前的研究已经收集了GPT-4与环境之间的交互轨迹，并用它们对较小的模型进行微调。作为这一过程的一部分，标准方法通常是简单地丢弃未成功完成任务的轨迹，这一方面导致了数据和资源的显著浪费，另一方面有可能限制微调过程中的优化路径。本文认为大型语言模型可以通过适当的数据清洗和微调策略从失败中学习。我们在数学推理、多跳问题回答和战略方面进行了实验。

    arXiv:2402.11651v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines. However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning. In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies. We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic 
    
[^26]: 从偏见到平等：去偏巨型语言模型词嵌入的新方法

    From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings

    [https://arxiv.org/abs/2402.11512](https://arxiv.org/abs/2402.11512)

    提出了DeepSoftDebias算法，在不同领域数据集、准确度指标和NLP任务中全面评估，发现其在减少性别、种族和宗教偏见方面优于现有最先进方法

    

    嵌入在巨型语言模型的有效性中扮演着重要角色。它们是这些模型把握上下文关系、促进更细致语言理解以及在许多需要对人类语言有基本理解的复杂任务上表现出色的基石。鉴于这些嵌入往往自身反映或展示偏见，因此这些模型可能也会无意中学习这种偏见。在这项研究中，我们在开创性前人研究基础上提出了DeepSoftDebias，这是一种使用神经网络进行“软去偏”的算法。我们在各类最先进数据集、准确度指标和具有挑战的自然语言处理任务中全面评估了这个算法。我们发现DeepSoftDebias在减少性别、种族和宗教偏见方面优于目前的最先进方法。

    arXiv:2402.11512v1 Announce Type: new  Abstract: Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform `soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.
    
[^27]: 多维度评估共情对话回复

    Multi-dimensional Evaluation of Empathetic Dialog Responses

    [https://arxiv.org/abs/2402.11409](https://arxiv.org/abs/2402.11409)

    提出了一个多维度的共情评估框架，同时衡量了说话者角度的表达意图和听者角度的感知共情，研究发现二者是相互关联的，感知共情与对话会话的满意水平呈高相关性。

    

    共情是有效和令人满意的对话沟通的关键元素，然而先前的研究大多集中在衡量表达的沟通意图上——即共情是如何表达的，忽略了对话也是一种涉及说话者和聆听者的协作实践。相反，我们提出了一个多维度的共情评估框架，扩展了现有工作，以衡量从说话者角度表达的意图以及从听者角度感知到的共情。将提出的框架应用于分析我们内部的客户服务对话表明，这两个维度（表达的意图类型和感知到的共情）是相互关联的，而感知到的共情与对话会话的满意水平具有很高的相关性。这个提出的框架仍需要受过训练的注释员的主观评估，这可能并不容易。

    arXiv:2402.11409v1 Announce Type: new  Abstract: Empathy is a critical element of effective and satisfactory conversational communication, yet previous studies in measuring conversational empathy mostly focus on expressed communicative intents -- in which way empathy is expressed, ignoring the fact that conversation is also a collaborative practice involving both speakers and listeners. In contrast, we propose a multi-dimensional empathy evaluation framework that extends upon existing work to measure both expressed intents from the speaker's perspective and perceived empathy from the listener's perspective. Applying the proposed framework to analyzing our internal customer-service dialogue shows that the two dimensions (expressed intent types and perceived empathy) are inter-connected, while perceived empathy has high correlation with the satisfactory level of dialogue sessions. This proposed framework still requires subjective assessments from trained annotators, which can be non-triv
    
[^28]: 打开大型语言模型的黑匣子：整体可解释性的两个视角

    Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability

    [https://arxiv.org/abs/2402.10688](https://arxiv.org/abs/2402.10688)

    通过整体可解释性框架，本文提出了打开大型语言模型黑匣子的方法，包括自下而上的机械解释和自上而下的表示工程视角，有助于深入理解和应用LLMs的行为和机制。

    

    随着大型语言模型(LLMs)变得越来越强大，人们对潜在伤害(如毒性、不公平和幻觉)的担忧威胁到用户的信任。通过模型对齐确保LLMs与人类价值观的有益契合因此至关重要，但具有挑战性，需要对LLMs的行为和机制有更深入的理解。我们提出通过一个涵盖互补的自下而上和自上而下视角的整体解释框架来打开LLMs的黑匣子。自下而上视角由机械解释能力实现，侧重于组件功能和训练动态。自上而下视角利用表示工程通过隐藏表示分析行为。在本文中，我们回顾了周围关于机械解释能力和表示工程的情况，总结了方法，讨论了限制和应用，并概述了将这些技术用于达到的未来挑战。

    arXiv:2402.10688v1 Announce Type: new  Abstract: As large language models (LLMs) grow more powerful, concerns around potential harms like toxicity, unfairness, and hallucination threaten user trust. Ensuring beneficial alignment of LLMs with human values through model alignment is thus critical yet challenging, requiring a deeper understanding of LLM behaviors and mechanisms. We propose opening the black box of LLMs through a framework of holistic interpretability encompassing complementary bottom-up and top-down perspectives. The bottom-up view, enabled by mechanistic interpretability, focuses on component functionalities and training dynamics. The top-down view utilizes representation engineering to analyze behaviors through hidden representations. In this paper, we review the landscape around mechanistic interpretability and representation engineering, summarizing approaches, discussing limitations and applications, and outlining future challenges in using these techniques to achiev
    
[^29]: 大型语言用户界面：由LLMs驱动的语音交互用户界面

    Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs

    [https://arxiv.org/abs/2402.07938](https://arxiv.org/abs/2402.07938)

    本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介，通过对自然文本输入进行彻底分析，实现智能和响应式用户体验。

    

    最近大型语言模型的快速发展展示了其在逻辑推理和理解方面的卓越能力。这些新发现的能力引发了新一代软件的诞生，正如它们在工业界无数应用中所展示的那样。本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介。通过对自然文本输入进行彻底分析，一个经过精心设计的LLM引擎可以理解用户的需求，分类最有可能的应用程序，识别所需的UI组件，并随后执行用户期望的操作。这种集成可以将静态UI系统发展成高度动态和可适应的解决方案，引入智能和响应式用户体验的新领域。这样的框架可以从根本上改变用户完成日常任务的方式，极大提升用户体验。

    The recent meteoric advancements in large language models have showcased a remarkable capacity for logical reasoning and comprehension. These newfound capabilities have opened the door to a new generation of software, as has been made obvious through the innumerable ways they are being applied in the industry. This research focuses on harnessing and guiding the upgraded power of LLMs to construct a framework that can serve as an intermediary between a user and their user interface. By comprehending a user's needs through a thorough analysis of natural textual inputs, an effectively crafted LLM engine can classify the most likely available application, identify the desired UI component and subsequently execute the user's expected actions. This integration can evolve static UI systems into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences. Such a framework can fundamentally shift how users accomplish daily tasks, skyrocket e
    
[^30]: ANLS* -- 一种适用于生成型大语言模型的通用文档处理度量方法

    ANLS* -- A Universal Document Processing Metric for Generative Large Language Models

    [https://arxiv.org/abs/2402.03848](https://arxiv.org/abs/2402.03848)

    ANLS*是一种用于生成型模型的新度量方法，针对各种任务包括信息提取和分类任务进行评估。它扩展了现有的ANLS度量方法，可以作为替代方案使用。

    

    传统上，在文档分类和信息提取等任务中，区分模型一直是主要选择。这些模型做出的预测可以分为有限数量的预定义类别，便于进行二元真假评估，并能直接计算F1分数等指标。然而，生成型大语言模型（GLLMs）的最新进展促使领域发生了转变，因为它们具备了强大的零-shot能力，消除了下游数据集和计算昂贵的微调的需求。然而，评估GLLMs存在挑战，因为对于GLLMs的预测，不能应用于区分模型所使用的二元真假评估方法。本文引入了一种用于生成型模型的新度量方法，称为ANLS*，用于评估各种任务，包括信息提取和分类任务。ANLS*度量方法扩展了现有ANLS度量方法，可作为一种即插即用的替代方案。

    Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
    
[^31]: PeTailor：通过定制的分块评分器改进生物医学三元组提取的大型语言模型

    PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction

    [https://arxiv.org/abs/2310.18463](https://arxiv.org/abs/2310.18463)

    我们提出了PeTailor，这是一个基于检索的框架，通过使用定制的分块评分器从预先构建的分块数据库中检索相关文档，并将检索到的信息集成到大型语言模型（LLM）的输入中，以改进生物医学三元组提取的效果。

    

    生物医学三元组提取系统旨在自动提取生物医学实体和实体之间的关系。虽然当前的统一信息提取模型展示了最先进的性能，但在理解复杂生物医学句子中实体之间的关系方面面临挑战。此外，缺乏高质量的生物医学三元组提取数据集阻碍了稳健的三元组提取系统的开发进展。为了解决这些挑战，我们提出了一种新颖的适用于生物医学三元组提取的基于检索的框架，名为PeTailor，它使用一种新颖的定制分块评分器从我们预先构建的多样分块数据库中显式地检索相关文档，并将检索到的信息集成到大型语言模型（LLM）的输入中，为输入的句子生成相应的三元组（头实体，关系，尾实体）。此外，我们还提供了GM-CIHT，一种专家标注的生物医学三元组提取数据集，该数据集支持了我们的方法的实验评估。

    Biomedical triple extraction systems aim to automatically extract biomedical entities and relations between entities. While current unified information extraction models showcase state-of-the-art performance, they face challenges in understanding relationships between entities within intricate biomedical sentences. Furthermore, the absence of a high-quality biomedical triple extraction dataset impedes the progress in developing robust triple extraction systems. To tackle these challenges, we propose a novel retrieval-based framework for biomedical triple extraction, namely PeTailor, which explicitly retrieves the relevant document from our pre-built diverse chunk database using a novel tailored chunk scorer and integrates the retrieved information into the input of a Large Language Model (LLM) to generate the corresponding triple (head entity, relation, tail entity) for the input sentence. Additionally, we present GM-CIHT, an expert-annotated biomedical triple extraction dataset that c
    
[^32]: 显式表示语法改进了意外情况下的句子到布局预测

    Explicitly Representing Syntax Improves Sentence-to-layout Prediction of Unexpected Situations. (arXiv:2401.14212v1 [cs.CL])

    [http://arxiv.org/abs/2401.14212](http://arxiv.org/abs/2401.14212)

    本文研究了句子到布局预测任务中的语法表示对模型性能的影响。实验结果显示，显式表示语法增强了模型对意外情况的预测能力，但对于未在训练集中出现的句子结构仍存在困难。

    

    在自然语言句子中识别视觉实体并将它们排列在二维空间布局中，需要对语言和空间的组合理解。布局预测任务在文本到图像合成中非常有价值，因为它允许对图像进行局部和受控的修复。通过比较性研究表明，我们可以从隐式或显式编码句子语法的语言表示中预测布局，如果句子提到的实体关系与训练中看到的类似。为了测试组合理解能力，我们收集了一个由语法正确的句子和布局组成的测试集，描述了训练过程中可能未曾见过的实体和关系组合。在这个测试集上的性能显著下降，表明当前模型依赖于训练数据中的相关性，并且在理解输入句子的结构方面存在困难。我们提出了一种新的结构损失函数，更好地强化了句子结构。

    Recognizing visual entities in a natural language sentence and arranging them in a 2D spatial layout require a compositional understanding of language and space. This task of layout prediction is valuable in text-to-image synthesis as it allows localized and controlled in-painting of the image. In this comparative study it is shown that we can predict layouts from language representations that implicitly or explicitly encode sentence syntax, if the sentences mention similar entity-relationships to the ones seen during training. To test compositional understanding, we collect a test set of grammatically correct sentences and layouts describing compositions of entities and relations that unlikely have been seen during training. Performance on this test set substantially drops, showing that current models rely on correlations in the training data and have difficulties in understanding the structure of the input sentences. We propose a novel structural loss function that better enforces th
    
[^33]: 缓解大型语言模型的幻觉问题：通过知识一致性对齐

    Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment. (arXiv:2401.10768v1 [cs.CL])

    [http://arxiv.org/abs/2401.10768](http://arxiv.org/abs/2401.10768)

    本文提出了一种称为知识一致性对齐（KCA）的方法，通过减少训练数据中外部知识和预训练语料库中内在知识之间的不一致性，从而缓解了大型语言模型产生幻觉的问题。实验结果表明，KCA方法在多个基准测试中取得了优异的性能。

    

    虽然大型语言模型在对齐后在各种任务上表现出色，但它们仍可能产生与上下文或世界知识自信矛盾的响应，这被称为“幻觉”现象。本文展示了通过减少训练数据中的外部知识与预训练语料库中继承的内在知识之间的不一致性，可以缓解对齐中的幻觉问题。具体而言，我们引入了一种新颖的知识一致性对齐（KCA）方法，该方法通过根据外部知识自动制定考试来评估大型语言模型的理解能力。对于包含知识不一致性的数据，KCA实施了几种简单而高效的处理策略。我们通过使用不同背景和规模的大型语言模型在六个基准测试中展示了所提出的KCA方法在缓解幻觉方面的卓越性能。

    While Large Language Models (LLMs) have proven to be exceptional on a variety of tasks after alignment, they may still produce responses that contradict the context or world knowledge confidently, a phenomenon known as ``hallucination''. In this paper, we demonstrate that reducing the inconsistency between the external knowledge encapsulated in the training data and the intrinsic knowledge inherited in the pretraining corpus could mitigate hallucination in alignment. Specifically, we introduce a novel knowledge consistent alignment (KCA) approach, which involves automatically formulating examinations based on external knowledge for accessing the comprehension of LLMs. For data encompassing knowledge inconsistency, KCA implements several simple yet efficient strategies for processing. We illustrate the superior performance of the proposed KCA approach in mitigating hallucinations across six benchmarks using LLMs of different backbones and scales. Furthermore, we confirm the correlation 
    
[^34]: PeFoMed：针对医学视觉问答的多模态大型语言模型的参数高效微调

    PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])

    [http://arxiv.org/abs/2401.02797](http://arxiv.org/abs/2401.02797)

    本文提出了一种针对医学视觉问答的多模态大型语言模型的参数高效微调框架，并在公共数据集上进行了验证。实验证明，该模型在封闭式问题上比GPT-4v模型的准确率提高了26％。

    

    多模态大型语言模型（MLLM）在传统大型语言模型的能力上进行了进化扩展，使它们能够应对超越纯文本应用范围的挑战。它利用了先前编码在这些语言模型中的知识，从而增强了它们在多模态环境下的适用性和功能性。最近的研究探讨了将MLLMs适应为生成任务以解决医学视觉问答（Med-VQA）任务的自由形式答案的能力。在本文中，我们提出了一种针对Med-VQA应用特别定制的参数高效微调框架，并在公共基准数据集上进行了实证验证。为了准确衡量性能，我们进行了人工评估，结果显示我们的模型在封闭式问题的整体准确率上达到了81.9％，且其相对于GPT-4v模型的绝对准确率超过了26％。

    Multimodal large language models (MLLMs) represent an evolutionary expansion in the capabilities of traditional large language models, enabling them to tackle challenges that surpass the scope of purely text-based applications. It leverages the knowledge previously encoded within these language models, thereby enhancing their applicability and functionality in the reign of multimodal contexts. Recent works investigate the adaptation of MLLMs to predict free-form answers as a generative task to solve medical visual question answering (Med-VQA) tasks. In this paper, we propose a parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications, and empirically validate it on a public benchmark dataset. To accurately measure the performance, we employ human evaluation and the results reveal that our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions. The cod
    
[^35]: 统一自然语言处理和软件工程视角：代码语言模型综述

    Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code. (arXiv:2311.07989v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.07989](http://arxiv.org/abs/2311.07989)

    这篇论文系统地回顾了代码处理方面的语言模型的最新进展，涵盖了50多个模型、30多个评估任务、170多个数据集和700多个相关工作。它突出了代码建模从统计模型和RNN到预训练的Transformer和LLM之间的历史转变，并讨论了代码特定的特性和关键挑战。

    

    在这项工作中，我们系统地回顾了最近在代码处理中的语言模型方面的进展，涵盖了50多个模型、30多个评估任务、170多个数据集和700多个相关工作。我们将代码处理模型分为通用语言模型（如GPT系列）和专门在代码上预训练的模型，通常具有专门的目标。我们讨论了这些模型之间的关系和差异，并突出了代码建模从统计模型和RNN到预训练的Transformer和LLM之间的历史转变，这正是NLP也经历的过程。我们还讨论了代码特定的特性，如AST、CFG和单元测试，以及它们在训练代码语言模型中的应用，并确定了该领域中的关键挑战和潜在的未来方向。我们将这份综述保持开放，并在GitHub上更新，网址为https://github.com/codefuse-ai/Awesome-Code-LLM。

    In this work we systematically review the recent advancements in code processing with language models, covering 50+ models, 30+ evaluation tasks, 170+ datasets, and 700+ related works. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also discuss code-specific features such as AST, CFG, and unit tests, along with their application in training code language models, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.
    
[^36]: 学会拒绝：通过知识范围限制和拒绝机制使大型语言模型更可控和可靠

    Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism. (arXiv:2311.01041v1 [cs.CL])

    [http://arxiv.org/abs/2311.01041](http://arxiv.org/abs/2311.01041)

    本文提出了一种学会拒绝（L2R）的简单而有效的解决方案，通过引入拒绝机制，使大型语言模型（LLMs）能够识别和拒绝难以回答的问题，从而提高模型的可控性和可靠性。

    

    大型语言模型（LLMs）展示了令人印象深刻的语言理解和生成能力，使它们能够回答各个领域的广泛问题。然而，这些模型并不完美，经常产生含有错误或错误信息的回答。这些不准确性，通常称为幻觉，使得LLMs在许多场景中不可靠甚至不可用。本文的重点是在LLMs中缓解幻觉问题，特别是在问答环境中。我们探索了一种拒绝机制，指导LLMs拒绝回答具有挑战性的问题以避免错误。我们提出了一个简单而有效的解决方案Learn to Refuse (L2R)，它将拒绝机制纳入到LLMs中，使其能够识别和拒绝那些它们难以回答的问题。为了实现这一点，我们利用结构化知识库来表示所有LLMs所需要的知识。

    Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM
    
[^37]: 大型语言模型作为具有普适性的机器人任务策略

    Large Language Models as Generalizable Policies for Embodied Tasks. (arXiv:2310.17722v1 [cs.LG])

    [http://arxiv.org/abs/2310.17722](http://arxiv.org/abs/2310.17722)

    本研究展示了大型语言模型(LLMs)可以被适应为具有普适性的机器人任务策略。通过我们的方法LLaRP，我们成功将预训练的冻结LLM用于接收指令和视觉输入，并在环境中直接输出动作。我们的实验结果表明LLaRP不仅对任务指令的复杂改写具有鲁棒性，而且可以推广到需要新颖最优行为的新任务。在大量未见任务中，LLaRP表现出了显著的成功率提升，并且我们提供了一个新的基准测试(Language Rearrangement)来促进进一步的研究。

    

    我们展示了大型语言模型(LLMs)可以被调整为适用于机器人视觉任务的普适性策略。我们的方法被称为大型语言模型强化学习策略(LLaRP)，它将预训练的冻结的LLM调整为接收文本指令和视觉自我中心观测作为输入，并直接在环境中输出动作。通过强化学习，我们训练LLaRP通过与环境的交互来看和行动。我们展示了LLaRP对任务指令的复杂改写具有鲁棒性，并且可以推广到需要新颖最优行为的新任务。特别地，在1,000个未见任务中，它的成功率达到了42%，是其他常见学习基线或零样本应用的1.7倍成功率。最后，为了帮助社区研究以语言为条件的、大规模多任务的机器人AI问题，我们发布了一个新的基准测试(Language Rearrangement)，包括150,000个训练任务和1,000个测试任务，用于语言为条件的重新排列。

    We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangem
    
[^38]: 从大规模交互轨迹中自动挖掘宏任务

    Automatic Macro Mining from Interaction Traces at Scale. (arXiv:2310.07023v1 [cs.HC])

    [http://arxiv.org/abs/2310.07023](http://arxiv.org/abs/2310.07023)

    本文介绍了一种基于大型语言模型的新方法，用于从移动交互轨迹中自动提取语义上有意义的宏任务。通过多项研究和实验证明了该方法的有效性和提取的宏任务的实用性。

    

    宏任务是我们日常手机活动的构建块任务（例如，“登录”或“预定航班”）。有效地提取宏任务对于理解移动交互和实现任务自动化至关重要。然而，这些宏任务在大规模情况下很难提取，因为它们可以由多个步骤组成，同时又隐藏在应用的编程组件中。在本文中，我们介绍了一种基于大型语言模型（LLMs）的新方法，以自动从随机和用户策划的移动交互轨迹中提取语义上有意义的宏任务。我们的方法产生的宏任务自动标记了自然语言描述，并且可以完全执行。为了检验提取的质量，我们进行了多项研究，包括用户评估、与人工策划任务的比较分析以及对这些宏任务的自动执行。这些实验和分析显示了我们方法的有效性以及提取的宏任务在不同的任务中的有用性。

    Macros are building block tasks of our everyday smartphone activity (e.g., "login", or "booking a flight"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of the app. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. To examine the quality of extraction, we conduct multiple studies, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses show the effectiveness of our approach and the usefulness of extracted macros in various dow
    
[^39]: 基于语言模型的概率测量专利权要求范围的新方法

    A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])

    [http://arxiv.org/abs/2309.10003](http://arxiv.org/abs/2309.10003)

    本文提出了一种使用概率从语言模型中获得的自信息来测量专利权要求范围的新方法。该方法通过计算要求的发生概率和自信息来评估要求的信息量，进而反映出要求的范围。研究结果表明，不同类型的语言模型对范围测量的影响不同，最简单的模型可以将范围度量简化为单词或字符计数的倒数。此方法在九个系列的专利权要求上进行了验证，结果表明各系列的要求范围逐渐减小。

    

    本文提出了一种将专利权要求的范围测量为该要求所包含的自信息的倒数的方法。这种方法基于信息论，基于一个假设，即罕见的概念比平常的概念更具信息量，因为它更令人惊讶。自信息是从该要求的发生概率计算得出的，其中概率是根据语言模型计算的。本文考虑了五个语言模型，从最简单的模型（每个单词或字符均从均匀分布中抽取）到中等模型（使用平均词或字符频率），再到一个大型语言模型（GPT2）。有趣的是，最简单的语言模型将范围度量减少为单词或字符计数的倒数，这是先前作品中已经使用的度量标准。该方法应用于九个系列的针对不同发明的专利权要求，其中每个系列的要求范围逐渐减小。

    This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
    
[^40]: X-PARADE: 跨语言文本蕴含和段落之间的信息分歧

    X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs. (arXiv:2309.08873v1 [cs.CL])

    [http://arxiv.org/abs/2309.08873](http://arxiv.org/abs/2309.08873)

    X-PARADE是第一个跨语言段落级别信息分歧的数据集，通过将来源于不同语言的维基百科页面上的段落对齐，标注者评估了目标语言段落与源语言段落之间的信息是否相同、新的或者可以推断，为解决这个问题提供了一个全面的数据集。

    

    理解两段文本是否传达相同的信息是自然语言处理中许多子问题的目标，包括文本蕴含和事实核查。当这两段文本处于不同的语言时，这个问题变得更加复杂。在这里，我们介绍了X-PARADE（跨语言段落级别的分歧和蕴含分析），这是第一个跨语言段落级别信息分歧的数据集。标注者在目标语言上以跨度级别标注段落，并与源语言中的相应段落进行评估，表示给定的信息是否相同、新的，或者新的但可以推断。这个概念与跨语言自然语言推理建立了联系。对齐的段落来自不同语言的维基百科页面，反映了实际观察到的信息分歧。凭借我们的数据集，我们研究了一系列解决这个问题的方法，包括经典的机器翻译中的令牌对齐。

    Understanding when two pieces of text convey the same information is a goal touching many subproblems in NLP, including textual entailment and fact-checking. This problem becomes more complex when those two pieces of text are in different languages. Here, we introduce X-PARADE (Cross-lingual Paragraph-level Analysis of Divergences and Entailments), the first cross-lingual dataset of paragraph-level information divergences. Annotators label a paragraph in a target language at the span level and evaluate it with respect to a corresponding paragraph in a source language, indicating whether a given piece of information is the same, new, or new but can be inferred. This last notion establishes a link with cross-language NLI. Aligned paragraphs are sourced from Wikipedia pages in different languages, reflecting real information divergences observed in the wild. Armed with our dataset, we investigate a diverse set of approaches for this problem, including classic token alignment from machine 
    
[^41]: 大型语言模型在分布外检测方面有多好？

    How Good Are Large Language Models at Out-of-Distribution Detection?. (arXiv:2308.10261v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10261](http://arxiv.org/abs/2308.10261)

    本文通过对大型语言模型进行实证调查，探索了分布外检测的能力。作者发现了LLM在分布外检测方面的差异，并采用了新的生成式微调方法，提高了模型的性能。

    

    分布外（OOD）检测在提高机器学习（ML）模型的可靠性方面起着至关重要的作用。大型语言模型（LLM）的出现在ML社区引起了范式转变，展示了它们在各种自然语言处理任务中的出色能力。尽管现有的研究已经以BERT、RoBERTa和GPT-2等相对小规模的Transformer模型探索了OOD检测，但在规模、预训练目标和推理范式方面的明显差异引发了对这些发现在LLM中的适用性的质疑。本文在LLMs领域进行了开创性的实证调查，重点关注7B到65B大小的LLaMA系列。我们对常用的OOD检测器进行了全面评估，审查了它们在零梯度和微调场景下的性能。值得注意的是，我们将之前的判别式的内部分布微调改为生成式微调，使LLM的预训练目标与之一致。

    Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning (ML) models. The emergence of large language models (LLMs) has catalyzed a paradigm shift within the ML community, showcasing their exceptional capabilities across diverse natural language processing tasks. While existing research has probed OOD detection with relative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark differences in scales, pre-training objectives, and inference paradigms call into question the applicability of these findings to LLMs. This paper embarks on a pioneering empirical investigation of OOD detection in the domain of LLMs, focusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly-used OOD detectors, scrutinizing their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLM
    
[^42]: WebArena: 一个用于构建自主智能体的真实网络环境

    WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])

    [http://arxiv.org/abs/2307.13854](http://arxiv.org/abs/2307.13854)

    WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。

    

    随着生成式人工智能的进展，通过自然语言指令进行日常任务的自主智能体的潜力逐渐显现。然而，当前的智能体主要是在简化的合成环境中创建和测试的，严重限制了现实世界场景的表示能力。在本文中，我们构建了一个高度逼真且可复现的智能体指令和控制环境。具体而言，我们关注在网站上执行任务的智能体，我们创建了一个包含来自四个常见领域的完全功能网站的环境，分别是电子商务、社交论坛讨论、协同软件开发和内容管理。我们的环境使用工具（如地图）和外部知识库（如用户手册）来鼓励像人类一样解决任务。在我们的环境基础上，我们发布了一组重点评估任务完成功能正确性的基准任务。我们基准任务具有多样性和长远的视野，并且被设计为鼓励智能体进行更深层次的任务理解和解决。

    With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
    
[^43]: 符号化思维链提炼：小模型也能逐步“思考”

    Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step. (arXiv:2306.14050v1 [cs.CL])

    [http://arxiv.org/abs/2306.14050](http://arxiv.org/abs/2306.14050)

    本研究提出了符号化思维链提炼（SCoTD）方法，该方法使用从大型教师模型中抽样出的合理化解释来训练数量级更小的学生模型，从而实现小模型也能受益于思维链提示，提升模型性能。

    

    思维链提示（例如“我们来逐步思考”）会促使大型语言模型对其预测进行合理化解释。虽然思维链可以带来显著的性能提升，但益处似乎仅适用于足够大的模型（超过50亿参数）。我们展示了数量级较小（125M-1.3B参数）的模型仍然可以从思维链提示中受益。为了实现这一点，我们引入了符号化思维链提炼（SCoTD），一种将较大的教师模型中抽样出的合理化解释用于训练较小的学生模型的方法。在几个常识基准测试上的实验表明：1）SCoTD提升了学生模型的性能，无论是在监督学习还是少样本学习的情况下，特别是在挑战集方面。2）从教师模型中抽样多个推理链是至关重要的。3）在提炼后，虽然参数少得多，但学生思维链与教师相当。

    Chain-of-thought prompting (e.g., "Let's think step-by-step") primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of
    
[^44]: 用强化学习实现的顺序检索上下文示例的RetICL

    RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])

    [http://arxiv.org/abs/2305.14502](http://arxiv.org/abs/2305.14502)

    本文提出了一种 RetICL 的方法来优化地选择用于上下文学习模型的示例。此方法利用强化学习框架将序列示例选择问题作为马尔科夫决策过程，并且优化选择为使任务表现最佳的组合。

    

    最近在大语言模型领域中的许多发展都集中在促使它们执行特定任务。一种有效的提示方法是上下文学习，其中模型在给定一个（或多个）示例的情况下执行（可能是新的）生成/预测任务。先前的工作表明，示例的选择可能对任务的表现产生很大的影响。然而，找到好的示例并不是简单的，因为代表性示例组的定义可以根据任务的不同而大不相同。虽然存在许多选择上下文示例的现有方法，但它们通常独立地对示例进行评分，忽略它们之间的依赖关系以及向大型语言模型提供示例的顺序。在这项工作中，我们提出了一种可学习的方法——In-Context Learning的检索RetICL，用于建模和逐步选择上下文示例。我们把顺序示例选择的问题作为马尔科夫决策过程，设计了一个示例。

    Many recent developments in large language models focus on prompting them to perform specific tasks. One effective prompting method is in-context learning, where the model performs a (possibly new) generation/prediction task given one (or more) examples. Past work has shown that the choice of examples can make a large impact on task performance. However, finding good examples is not straightforward since the definition of a representative group of examples can vary greatly depending on the task. While there are many existing methods for selecting in-context examples, they generally score examples independently, ignoring the dependency between them and the order in which they are provided to the large language model. In this work, we propose Retrieval for In-Context Learning (RetICL), a learnable method for modeling and optimally selecting examples sequentially for in-context learning. We frame the problem of sequential example selection as a Markov decision process, design an example r
    
[^45]: 迭代学习与交流共同解释了有效的颜色命名系统

    Iterated learning and communication jointly explain efficient color naming systems. (arXiv:2305.10154v1 [cs.CL])

    [http://arxiv.org/abs/2305.10154](http://arxiv.org/abs/2305.10154)

    本论文通过结合迭代学习和交流的文化进化模型，展示这个模型能够在神经网络中实现并收敛于高效的颜色命名系统，进一步证明了语义系统反映效率压力的观点。

    

    已经有人认为，语义系统反映了效率的压力，一个当前的争论关注于产生这种模式的文化进化过程。我们将效率实现为信息瓶颈原理，并结合迭代学习和交流的文化进化模型。我们展示了这个在神经网络中实现的模型收敛于在IB意义下高效并且类似于人类颜色命名系统的颜色命名系统。我们还表明，仅仅迭代学习或者仅仅交流并不能像这个模型那样产生相同的结果。

    It has been argued that semantic systems reflect pressure for efficiency, and a current debate concerns the cultural evolutionary process that produces this pattern. We consider efficiency as instantiated in the Information Bottleneck (IB) principle, and a model of cultural evolution that combines iterated learning and communication. We show that this model, instantiated in neural networks, converges to color naming systems that are efficient in the IB sense and similar to human color naming systems. We also show that iterated learning alone, and communication alone, do not yield the same outcome as clearly.
    
[^46]: 用稀疏输入控制高维数据

    Controlling High-Dimensional Data With Sparse Input. (arXiv:2303.09446v1 [eess.AS])

    [http://arxiv.org/abs/2303.09446](http://arxiv.org/abs/2303.09446)

    本论文提出了一种新的控制机制，即将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。通过实验，此方法表现出了效率、鲁棒性和保真性，即使只有少量的输入数值。

    

    本论文解决了人在环路控制生成高度结构化数据的问题。由于现有的生成模型缺乏有效的接口，使得用户可以修改输出，这个任务变得具有挑战性。用户或手动探索不可解释的潜在空间，或者费力地注释数据标签。为了解决这个问题，我们引入了一个新的框架，其中编码器将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。我们将这个框架应用于控制文本转语音合成中的韵律的任务。我们提出了一个模型，称为多实例条件变分自编码器(MICVAE)，它专门设计用于编码稀疏的韵律特征并输出完整的波形。我们通过实验证明，MICVAE表现出了稀疏的人在环路控制机制所需的良好品质：效率、鲁棒性和保真性。即使只有非常少量的输入数值(~4)，MICVAE也能让用户实现控制。

    We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an efficient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: efficiency, robustness, and faithfulness. With even a very small number of input values (~4), MICVAE enables users to im
    

