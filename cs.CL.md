# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Population Expansion for Training Language Models with Private Federated Learning.](http://arxiv.org/abs/2307.07477) | 本文提出了一种使用领域适应技术扩展人口的方法，以加速训练并提高使用较少设备进行训练时的模型质量。 |
| [^2] | [Towards spoken dialect identification of Irish.](http://arxiv.org/abs/2307.07436) | 本研究针对爱尔兰语方言的变异性问题，通过实验探索了爱尔兰口音方言的语音识别，并成功搭建出一个准确率为73%的语音识别系统。 |
| [^3] | [Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes.](http://arxiv.org/abs/2307.07422) | 这项研究调查了基于大语言模型的聊天机器人在个人财务领域的表现，发现虽然输出流利合理，但在提供准确可靠的财务信息方面仍存在重大差距。 |
| [^4] | [Sumformer: A Linear-Complexity Alternative to Self-Attention for Speech Recognition.](http://arxiv.org/abs/2307.07421) | Sumformer提出了一种线性时间代替自注意力的方法，用总结混合来处理语音识别任务，可以在保持准确性的同时降低训练和推理时间。 |
| [^5] | [Named entity recognition using GPT for identifying comparable companies.](http://arxiv.org/abs/2307.07420) | 本文使用GPT以识别可比公司。传统的可比公司方法通常使用定性方法来识别相似的同行公司，而我们使用大型语言模型通过提取公司描述/摘要从而进行相似性分析，实现更量化的方法。 |
| [^6] | [RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition.](http://arxiv.org/abs/2307.07417) | RoPDA是一种用于低资源NER的数据增强方法，通过基于预训练语言模型和连续提示进行实体和上下文增强，并提出了自一致性过滤和混合技术以优化增强样本的利用。 |
| [^7] | [AutoHint: Automatic Prompt Optimization with Hint Generation.](http://arxiv.org/abs/2307.07415) | 本文介绍了AutoHint，一种用于大型语言模型的自动提示生成和优化的新框架。该方法通过从输入-输出演示中生成提示，并利用上下文学习和零样本学习的优点，优化原始提示，从而提高了大型语言模型在特定任务上的表现。 |
| [^8] | [HuCurl: Human-induced Curriculum Discovery.](http://arxiv.org/abs/2307.07412) | 课程学习框架HuCurl能够根据先前对样本难度的了解，发现非单调的有效课程，并在多个NLP任务中胜过现有方法。 |
| [^9] | [Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases.](http://arxiv.org/abs/2307.07411) | 该研究比较了8种公开可用的LLM生成文本检测器，并发现CopyLeaks是最准确的，GPTKit是最佳的减少误报率的工具，GLTR是最具韧性的工具。这些结果将为教育工作者提供维护学术诚信的洞察。 |
| [^10] | [KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization.](http://arxiv.org/abs/2307.07409) | 本文介绍了一种新型的预训练视觉语言模型CheXOFA，通过在一般领域的训练数据上进行预训练，然后转移到胸部X射线领域，该模型能够有效地学习所需的知识和技能，并在放射学报告摘要任务上取得了卓越的性能，获得了RadSum23测试集的第一名。 |
| [^11] | [Phoneme-retrieval; voice recognition; vowels recognition.](http://arxiv.org/abs/2307.07407) | 音素检索技术通过特定的网络构建方式实现了语音和图像的准确检索，在特定的领域和词汇集合上表现出良好的性能。 |
| [^12] | [Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach.](http://arxiv.org/abs/2307.07392) | 该论文利用基于排名的方法，通过比较四种不同的预训练孟加拉文本摘要模型的输出来确定给定文本的最准确和信息丰富的摘要。 |
| [^13] | [Composition-contrastive Learning for Sentence Embeddings.](http://arxiv.org/abs/2307.07380) | 该论文提出了一种句子嵌入的构成对比学习方法，通过最大化文本和其词组成分的对齐，实现了从无标签数据中学习文本表示的目标，并在语义文本相似性任务上取得了与最先进方法可比较的改进，而无需额外的训练目标或网络参数。 |
| [^14] | [Gloss Attention for Gloss-free Sign Language Translation.](http://arxiv.org/abs/2307.07361) | 本论文提出了一种无需注释的手语翻译中的注释关注机制（Gloss Attention），通过学习语义边界位置和全局理解手语视频，实现了对手语视频的准确翻译。同时，通过将句子间相似性的知识转移，提高了翻译网络的理解能力。 |
| [^15] | [How Different Is Stereotypical Bias Across Languages?.](http://arxiv.org/abs/2307.07331) | 本研究拓展了评估预训练语言模型中刻板偏见的研究，通过跨语言分析发现mGPT-2在不同语言中显示出令人惊讶的反刻板行为，并且英语模型表现出最强的偏见，而土耳其语则最不明显。 |
| [^16] | [Hybrid moderation in the newsroom: Recommending featured posts to content moderators.](http://arxiv.org/abs/2307.07317) | 本文提出了一种在新闻编辑室中使用的混合审核方法，该方法通过向内容管理员推荐特色帖子来支持他们在选择特色内容方面做出决策。该方法基于概率排序的推荐系统，结合了用户和文本内容特征，取得了较高的分类和排序性能。内容管理员在评估中发现了合适的评论，并在很大程度上接受了推荐结果。 |
| [^17] | [Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs.](http://arxiv.org/abs/2307.07312) | 本论文通过使用大型语言模型，实现了基于图数据的零-shot自然语言生成。实验结果表明，该方法在部分指标上接近最新技术水平，在事实、反事实和虚构陈述的对比中也有显著的关联。 |
| [^18] | [C3: Zero-shot Text-to-SQL with ChatGPT.](http://arxiv.org/abs/2307.07306) | C3是基于ChatGPT的零-shot Text-to-SQL方法，通过三个关键组件提供系统性的处理方法，在Spider Challenge上取得了82.3%的执行准确率，成为最先进的方法。 |
| [^19] | [Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?.](http://arxiv.org/abs/2307.07295) | 研究发现，对于低资源语言中的方言识别，平衡的语料库无法在不同方言之间产生相似的性能。蒙斯特方言具有最低的错误率，而阿尔斯特方言一直表现较差。康纳赫特方言和蒙斯特方言之间存在紧密的关系，但不对称。这些结果将指导未来的语料库收集和系统构建策略，以优化跨方言的性能公平性。 |
| [^20] | [Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition.](http://arxiv.org/abs/2307.07280) | 该论文探讨了如何通过持续层特定微调和经验回放技术来改善德语语音识别模型在较小领域中的性能，并提高模型的鲁棒性。 |
| [^21] | [Are words equally surprising in audio and audio-visual comprehension?.](http://arxiv.org/abs/2307.07277) | 本研究调查了视觉信息对口头语言理解的影响，并评估了不同语言模型在多模态和单模态环境下的表现。结果表明，多模态和单模态环境下的认知努力存在差异，局部词汇上下文对多模态环境下的认知处理有显著影响。 |
| [^22] | [MorphPiece : Moving away from Statistical Language Representation.](http://arxiv.org/abs/2307.07262) | 本文提出了一种基于语言学动机的分词方案MorphPiece，并使用该方案训练了一个称为MorphGPT的语言模型。MorphGPT在语言建模以及各种NLP任务上都表现出了比传统模型更优异的性能。 |
| [^23] | [Improving BERT with Hybrid Pooling Network and Drop Mask.](http://arxiv.org/abs/2307.07258) | 本文提出了HybridBERT模型，通过结合自注意力和池化网络来编码每一层中的不同上下文特征，并且引入DropMask方法来解决预训练和微调之间的不匹配问题。实验证明，HybridBERT在预训练和迁移学习任务中均优于BERT，并且DropMask能够提高精度。 |
| [^24] | [Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems.](http://arxiv.org/abs/2307.07255) | 本文提供了一个对话代理设计的相关要素的综合概述，包括对话代理的主要特征、支持任务、数据集和评估方法。研究表明，构建单独的模型来处理不同的对话任务是昂贵且冗余的。 |
| [^25] | [Certified Robustness for Large Language Models with Self-Denoising.](http://arxiv.org/abs/2307.07171) | 传统的随机平滑方法对于大型语言模型的直接应用具有挑战性，为了解决认证半径很小的问题，提出了一种带有自我净化的新方法。 |
| [^26] | [Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks.](http://arxiv.org/abs/2307.07166) | 本文提出了一种切换头尾漏斗UNITER，用于解决双指称表达理解与取物任务。该方法通过使用单个模型分别预测目标物品和目的地，避免了传统方法需要进行所有组合推理的计算复杂性问题。验证结果表明，该方法在一个新构建的数据集上取得了良好的效果。 |
| [^27] | [Learning to Retrieve In-Context Examples for Large Language Models.](http://arxiv.org/abs/2307.07164) | 本文提出了一个新颖的框架，通过迭代训练密集检索器来为大型语言模型识别高质量的上下文示例，从而显著提高了上下文学习性能，并展示了在训练期间对未见过任务的泛化能力。 |
| [^28] | [Drive Like a Human: Rethinking Autonomous Driving with Large Language Models.](http://arxiv.org/abs/2307.07162) | 本文提出了使用大型语言模型（LLM）来实现自动驾驶系统驾驶如人类一般，并通过推理、解释和记忆能力解决长尾情况的潜力。通过构建闭环系统进行示范，大量实验证明了LLM在驾驶场景中的卓越能力。 |
| [^29] | [Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords.](http://arxiv.org/abs/2307.07160) | 本研究提出了一种通过遮盖领域内关键词进行的领域自适应预训练方法，实验结果表明该方法优于使用随机遮盖的领域内预训练和常见的预训练然后微调范式。 |
| [^30] | [MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System.](http://arxiv.org/abs/2307.07135) | MMSD2.0是一个修正了MMSD缺陷的数据集，提出了多视图CLIP框架，能够从多个角度利用多粒度线索进行多模式讽刺检测，显著优于以前最好的基准算法。 |
| [^31] | [Generating Efficient Training Data via LLM-based Attribute Manipulation.](http://arxiv.org/abs/2307.07099) | 本文提出了一种通过大型语言模型（LLMs）生成精心制作的训练数据来引导少样本学习的方法。通过利用LLMs操作任务特定属性并重构新的句子，我们实现了标签交换数据的生成，与其他基于LLMs的文本生成方法相比具有更好的效果。同时，研究结果还显示了通过LLM引导学习的潜力，即使在更少的监督情况下也能取得良好的表现。 |
| [^32] | [An Analysis of Dialogue Repair in Virtual Voice Assistants.](http://arxiv.org/abs/2307.07076) | 该研究分析了虚拟语音助手中对话修复的特点，指出了人助手和人对人对话修复策略的差异，以及不同语言和助手之间的差异。 |
| [^33] | [Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling.](http://arxiv.org/abs/2307.07057) | 本文通过利用预训练的ASR编码器来初始化一个端到端模型，在语音意图分类和槽位填充任务上取得了新的最优结果。与自监督学习相比，使用ASR预训练的编码器在效果上更好，并且能够实现参数效率。此外，与级联模型相比，端到端模型在大部分情况下都更好，除非提供了oracle ASR模型。 |
| [^34] | [Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section.](http://arxiv.org/abs/2307.07051) | 通过分析临床记录的部分，我们发现预测能力在不同类型的记录间存在差异，并且当上下文长度较大时，组合不同类型的记录可以改善性能。我们的研究结果表明，精心选择的采样函数可以使从临床记录中提取信息更加高效。 |
| [^35] | [MegaWika: Millions of reports and their sources across 50 diverse languages.](http://arxiv.org/abs/2307.07049) | MegaWika是一个由13百万个维基百科文章和71百万个引用源材料组成的多语言数据集，用于协作式 AI 辅助报告生成的新模型的开发。它提供了跨语言应用中的非英文文章翻译和自动语义分析的FrameNet解析。该数据集是句子级报告生成的最大资源，也是唯一的跨多种语言的报告生成数据集。此外，还提供了基线结果和训练模型用于跨语言问答和引用检索。 (tl;dr) |
| [^36] | [DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations.](http://arxiv.org/abs/2307.07047) | DIALGEN是一个人机半自动对话生成框架，通过迭代生成子对话和使用人工反馈来改善模型性能，适用于自动理解人际对话的应用。 |
| [^37] | [Data Augmentation for Machine Translation via Dependency Subtree Swapping.](http://arxiv.org/abs/2307.07025) | 通过依存子树交换实现机器翻译的数据增强框架，通过从依存树中提取相应的子树并进行交换，创建增强样本。实验证明，在资源受限环境中，它在3个语言对上表现出了持续的BLEU分数提高。 |
| [^38] | [Electoral Agitation Data Set: The Use Case of the Polish Election.](http://arxiv.org/abs/2307.07007) | 这篇论文介绍了一个用于检测波兰语选举激励的公开数据集，该数据集包含6,112条人工标注的推文。翻译模型HerBERT在该数据集上达到了68%的F1分数。 |
| [^39] | [Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks.](http://arxiv.org/abs/2307.07002) | 本文评估了八种易于集成到现有NLP系统中且不需要额外带外数据或模型修改的带外分布检测方法，并提供了一个完全可复现实验结果的研究环境。分析表明现有的NLP任务中带外分布检测方法对于捕捉所有由不同类型分布转换特征的样本尚不够敏感，这需要未来的工作来开发更有效的方法。 |
| [^40] | [Towards Populating Generalizable Engineering Design Knowledge.](http://arxiv.org/abs/2307.06985) | 这项研究提出了一种从专利文件中提取工程设计知识的方法，通过构建知识图来填充通用设计知识，并与现有方法进行了比较。 |
| [^41] | [Revisiting the DARPA Communicator Data using Conversation Analysis.](http://arxiv.org/abs/2307.06982) | 本文通过研究DARPA通信者项目中受挫和恼怒的用户记录，应用会话分析的方法，以识别人与电脑会话中的改进机会和失败点。 |
| [^42] | [Tackling Fake News in Bengali: Unraveling the Impact of Summarization vs. Augmentation on Pre-trained Language Models.](http://arxiv.org/abs/2307.06979) | 本论文研究了孟加拉语中假新闻的检测问题。通过使用总结和扩充技术，结合预训练语言模型，提出了一种四重方法来分类孟加拉语的假新闻文章。研究表明，总结和扩充在孟加拉语假新闻检测中具有有效性。 |
| [^43] | [Copy Is All You Need.](http://arxiv.org/abs/2307.06962) | 本文将文本生成定义为从现有文本集合中逐步复制文本片段，并通过复制和粘贴操作来实现生成，相比传统的顺序选择单词生成的模型，在自动和人工评估中取得了更好的生成质量，并且推理效率与基于标记的自回归模型相当。 |
| [^44] | [ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task.](http://arxiv.org/abs/2307.06954) | ACTI在EVALITA 2023中的阴谋论辨识任务共有15支团队参与，通过使用大型语言模型判断阴谋内容和分类，得出了关于利用这些模型抵制在在线平台传播错误信息的结论。 |
| [^45] | [Stack More Layers Differently: High-Rank Training Through Low-Rank Updates.](http://arxiv.org/abs/2307.05695) | 本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。 |
| [^46] | [Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration.](http://arxiv.org/abs/2307.05300) | 本论文提出了单人表现提示（SPP）的概念，通过与多个角色进行多轮自我协作，将单个语言模型转化为认知协同者，从而增强其在复杂任务中的问题解决能力和整体性能。 |
| [^47] | [Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers.](http://arxiv.org/abs/2306.13804) | 提出一种多模态双重注意力变换器（MDAT）模型，利用预训练模型进行多模态特征提取，通过引入图形注意和共同关注机制来捕捉不同情感的跨模态依赖，并使用最少的目标语言数据实现改进的跨语言情感识别结果。 |
| [^48] | [The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios.](http://arxiv.org/abs/2306.13734) | CHiME-7 DASR 挑战赛旨在进行在远场环境下多设备远程会议转录的鲁棒语音识别，并在三种不同场景中评估系统性能。参与者可以使用开源预训练模型和数据集。 |
| [^49] | [Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation.](http://arxiv.org/abs/2306.12916) | 本文全面研究了跨语言跨时代摘要任务，使用历史幻想文本和维基百科摘要构建了第一个CLCTS语料库，并研究了流行的变压器模型及其中间任务微调的有效性；同时还探讨了ChatGPT在CLCTS中作为摘要器和评估器的潜力。最终发现中间任务微调的端到端模型产生了中等到差的效果，而ChatGPT在没有微调的情况下提供中等到好的摘要质量表现。 |
| [^50] | [Edit Distance based RL for RNNT decoding.](http://arxiv.org/abs/2306.01789) | 本文提出了一种基于编辑距离的强化学习方法，用于最小化RNN-T训练和推理过程之间的差距，并在LibriSpeech的600M Conformer RNN-T模型上取得了最佳结果。 |
| [^51] | [UNITE: A Unified Benchmark for Text-to-SQL Evaluation.](http://arxiv.org/abs/2305.16265) | 提出了一个统一的基准UNITE用于文本到SQL评估，包含来自12个以上领域的自然语言问题、超过3.9K种模式的SQL查询和29K个数据库。研究表明，Codex在跨领域数据集上表现出色，特别设计的编码方法可以提高性能，可机读的数据库的质量对文本到SQL系统的性能至关重要。 |
| [^52] | [I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors.](http://arxiv.org/abs/2305.14724) | 本论文提出一个新的任务——从语言隐喻生成视觉隐喻，并且基于大语言模型和扩散模型之间的协作，成功地实现了共创出具有视觉冲击力和语义含义的隐喻。 |
| [^53] | [Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs.](http://arxiv.org/abs/2305.03642) | 本文提出了一种基于LLM调整的文本到文本模型，共同提取RCT报告中的干预、结果和发现信息，实现相当大的性能提升。 |
| [^54] | [A Survey on Biomedical Text Summarization with Pre-trained Language Model.](http://arxiv.org/abs/2304.08763) | 本文总结了基于预训练语言模型的生物医学文本摘要方法，提炼关键信息生成简洁的摘要，分为微调、基于特征和无监督方法，未来研究方向包括融合领域特定知识和开发更适合的评估指标。 |
| [^55] | [Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images.](http://arxiv.org/abs/2303.07274) | WHOOPS!是一个新的视觉常识数据集和基准测试，包括了图像字幕、跨模态匹配和视觉问答等若干个任务，引入了解释生成任务，挑战了AI模型识别和解释不合常规的图像的能力。 |
| [^56] | [ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction.](http://arxiv.org/abs/2303.05063) | 这篇论文提出了一个简单而有效的上下文学习框架ICL-D3IE，这个框架使LLM在不同类型演示下的DIE任务中表现出色，具有改进性能的反馈机制，同时涵盖了位置和格式方面的演示示例。 |
| [^57] | [The Re-Label Method For Data-Centric Machine Learning.](http://arxiv.org/abs/2302.04391) | 本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。 |
| [^58] | [An Effective Employment of Contrastive Learning in Multi-label Text Classification.](http://arxiv.org/abs/2212.00552) | 本论文提出了五个新的对比损失函数，用于多标签文本分类任务。通过对比学习技术的应用，探索了其在多标签文本分类任务中的有效性，并提供了一套基准模型。 |
| [^59] | [On the Effect of Anticipation on Reading Times.](http://arxiv.org/abs/2211.14301) | 该论文研究了预期对阅读时间的影响。通过比较惊奇度和上下文熵对阅读时间的预测效果，发现上下文熵对词的阅读时间影响有实质性的证据。 |
| [^60] | [Dialogs Re-enacted Across Languages.](http://arxiv.org/abs/2211.11584) | 该论文介绍了一种用于收集跨语言话语对的协议，并公开了相关数据集，以支持机器学习跨语言韵律映射等领域的研究。该研究对于使用该语料库、扩展该语料库以及设计类似双语对话数据集的人士具有重要意义。 |
| [^61] | [Vision Transformer Based Model for Describing a Set of Images as a Story.](http://arxiv.org/abs/2210.02762) | 本研究提出了一种基于视觉Transformer的模型，用于将一组图像描述为一个故事。该模型通过ViT提取输入图像的特征，并使用双向LSTM捕捉图像补丁的过去和未来上下文信息。通过注意力机制加权计算得到最终的故事描述向量。 |
| [^62] | [CodeQueries: A Dataset of Semantic Queries over Code.](http://arxiv.org/abs/2209.08372) | CodeQueries是一个关于Python代码语义查询的数据集，与现有数据集相比，它具有文件级别的上下文和代码段答案。 |

# 详细

[^1]: 使用私人联邦学习进行语言模型训练的人口扩展

    Population Expansion for Training Language Models with Private Federated Learning. (arXiv:2307.07477v1 [cs.LG])

    [http://arxiv.org/abs/2307.07477](http://arxiv.org/abs/2307.07477)

    本文提出了一种使用领域适应技术扩展人口的方法，以加速训练并提高使用较少设备进行训练时的模型质量。

    

    结合差分隐私的联邦学习为分布式设备提供带有正式隐私保证的机器学习训练。当设备数量庞大时，联邦学习与差分隐私的结合能够及时生成性能良好的模型。然而，对于设备数量较少的应用，由于差分隐私噪声与设备数量成反比，模型效用下降，同时由于需要等待来自较小设备池的足够客户端可用，训练延迟也增加。因此，本文提出基于领域适应技术扩展人口，以加快训练并改善使用较少设备进行训练时的最终模型质量。我们通过实验证明，我们的技术可以使实际语言建模数据集的效用提高13%至30%。

    Federated learning (FL) combined with differential privacy (DP) offers machine learning (ML) training with distributed devices and with a formal privacy guarantee. With a large population of devices, FL with DP produces a performant model in a timely manner. However, for applications with a smaller population, not only does the model utility degrade as the DP noise is inversely proportional to population, but also the training latency increases since waiting for enough clients to become available from a smaller pool is slower. In this work, we thus propose expanding the population based on domain adaptation techniques to speed up the training and improves the final model quality when training with small populations. We empirically demonstrate that our techniques can improve the utility by 13% to 30% on real-world language modeling datasets.
    
[^2]: 对爱尔兰口音方言的语音识别研究

    Towards spoken dialect identification of Irish. (arXiv:2307.07436v1 [cs.CL])

    [http://arxiv.org/abs/2307.07436](http://arxiv.org/abs/2307.07436)

    本研究针对爱尔兰语方言的变异性问题，通过实验探索了爱尔兰口音方言的语音识别，并成功搭建出一个准确率为73%的语音识别系统。

    

    爱尔兰语在方言和口音上非常丰富，这使得为这种资源匮乏的语言创建语音识别系统变得困难，因为这样的系统必须应对有限的语料库中的高度变异性。最近一项对爱尔兰语ASR中方言偏见的研究发现，平衡的训练语料库导致了不平等的方言性能，阿尔斯特方言的表现始终比康诺特或蒙斯特方言要差。受此启发，本实验研究了对爱尔兰口音方言的语音识别，旨在将这样的系统纳入语音识别流水线中。实验中测试了两个声学分类模型，XLS-R和ECAPA-TDNN，和一个使用预训练爱尔兰语BERT模型的基于文本的分类器。ECAPA-TDNN表现最好，特别是在VoxLingua107数据集上进行了语言识别预训练的模型，整体准确率达到73%。

    The Irish language is rich in its diversity of dialects and accents. This compounds the difficulty of creating a speech recognition system for the low-resource language, as such a system must contend with a high degree of variability with limited corpora. A recent study investigating dialect bias in Irish ASR found that balanced training corpora gave rise to unequal dialect performance, with performance for the Ulster dialect being consistently worse than for the Connacht or Munster dialects. Motivated by this, the present experiments investigate spoken dialect identification of Irish, with a view to incorporating such a system into the speech recognition pipeline. Two acoustic classification models are tested, XLS-R and ECAPA-TDNN, in conjunction with a text-based classifier using a pretrained Irish-language BERT model. The ECAPA-TDNN, particularly a model pretrained for language identification on the VoxLingua107 dataset, performed best overall, with an accuracy of 73%. This was furt
    
[^3]: LLM能成为良好的财务顾问吗？：关于个人决策优化结果的初步研究

    Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes. (arXiv:2307.07422v1 [cs.CL])

    [http://arxiv.org/abs/2307.07422](http://arxiv.org/abs/2307.07422)

    这项研究调查了基于大语言模型的聊天机器人在个人财务领域的表现，发现虽然输出流利合理，但在提供准确可靠的财务信息方面仍存在重大差距。

    

    越来越强大的基于大语言模型（LLM）的聊天机器人，如ChatGPT和Bard，正成为用户可用的，有潜力改变公众决策质量的工具。在这个背景下，我们旨在调查这些系统在个人财务领域的表现，而金融包容一直是银行的主要目标。我们提出了13个问题，代表个人财务中的银行产品，包括银行账户、信用卡、存款证书及其产品间的相互作用，以及与高价购买、支付银行费用和投资建议相关的决策，还包括不同的方言和语言（英语、非正式美国英语和泰卢固语）。我们发现，尽管聊天机器人的输出流利而且合理，但在使用基于LLM的聊天机器人提供准确可靠的财务信息方面仍存在重大差距。

    Increasingly powerful Large Language Model (LLM) based chatbots, like ChatGPT and Bard, are becoming available to users that have the potential to revolutionize the quality of decision-making achieved by the public. In this context, we set out to investigate how such systems perform in the personal finance domain, where financial inclusion has been an overarching stated aim of banks for decades. We asked 13 questions representing banking products in personal finance: bank account, credit card, and certificate of deposits and their inter-product interactions, and decisions related to high-value purchases, payment of bank dues, and investment advice, and in different dialects and languages (English, African American Vernacular English, and Telugu). We find that although the outputs of the chatbots are fluent and plausible, there are still critical gaps in providing accurate and reliable financial information using LLM-based chatbots.
    
[^4]: Sumformer: 一种用于语音识别的线性复杂度代替自注意力的方法

    Sumformer: A Linear-Complexity Alternative to Self-Attention for Speech Recognition. (arXiv:2307.07421v1 [cs.CL])

    [http://arxiv.org/abs/2307.07421](http://arxiv.org/abs/2307.07421)

    Sumformer提出了一种线性时间代替自注意力的方法，用总结混合来处理语音识别任务，可以在保持准确性的同时降低训练和推理时间。

    

    现代语音识别系统依赖于自注意力。然而，使用自注意力进行令牌混合的计算复杂度与语音语句的长度呈二次关系，导致推理、训练和内存占用速度变慢。虽然已经开发出了比自注意力更便宜的替代方法，但很难保证达到相同的准确性水平。实际上，经过训练的语音识别器的自注意力权重在时间上呈全局平均化的形式。因此，本文提出了一种用于语音识别的线性时间替代自注意力的方法。它用所有时间步长的向量的平均值来总结整个语句。然后将这个单一的总结与特定时间的信息结合起来。我们将这种方法称为“总结混合”。在最先进的ASR模型中引入总结混合，可以在降低训练和推理时间多达27%的同时，保持或超过先前的语音识别性能水平。

    Modern speech recognition systems rely on self-attention. Unfortunately, token mixing with self-attention takes quadratic time in the length of the speech utterance, slowing down inference as well as training and increasing memory consumption. Cheaper alternatives to self-attention for ASR have been developed, but fail to consistently reach the same level of accuracy. In practice, however, the self-attention weights of trained speech recognizers take the form of a global average over time. This paper, therefore, proposes a linear-time alternative to self-attention for speech recognition. It summarises a whole utterance with the mean over vectors for all time steps. This single summary is then combined with time-specific information. We call this method ``Summary Mixing''. Introducing Summary Mixing in state-of-the-art ASR models makes it feasible to preserve or exceed previous speech recognition performance while lowering the training and inference times by up to 27% and reducing the m
    
[^5]: 使用GPT进行命名实体识别以识别可比公司

    Named entity recognition using GPT for identifying comparable companies. (arXiv:2307.07420v1 [cs.CL])

    [http://arxiv.org/abs/2307.07420](http://arxiv.org/abs/2307.07420)

    本文使用GPT以识别可比公司。传统的可比公司方法通常使用定性方法来识别相似的同行公司，而我们使用大型语言模型通过提取公司描述/摘要从而进行相似性分析，实现更量化的方法。

    

    对于公共和私人公司，可比公司分析被广泛用作公司估值的方法。特别是对于私募股权公司的估值，该方法非常有价值。可比公司方法的几种方法通常依赖于定性方法来识别相似的同行公司，这往往使用已建立的行业分类方案和/或分析师的直觉和知识。然而，文献和私募股权行业开始使用更多的量化方法，特别是机器学习聚类和自然语言处理（NLP）。对于NLP方法，该过程包括从公司的网站或来自某些金融数据库系统的公司描述中提取产品实体，然后进行相似性分析。在这里，我们使用公开可用的公司维基百科网站的公司描述/摘要，展示了使用大型语言模型（LLM），例如GPT

    For both public and private firms, comparable companies analysis is widely used as a method for company valuation. In particular, the method is of great value for valuation of private equity companies. The several approaches to the comparable companies method usually rely on a qualitative approach to identifying similar peer companies, which tends to use established industry classification schemes and/or analyst intuition and knowledge. However, more quantitative methods have started being used in the literature and in the private equity industry, in particular, machine learning clustering, and natural language processing (NLP). For NLP methods, the process consists of extracting product entities from e.g., the company's website or company descriptions from some financial database system and then to perform similarity analysis. Here, using companies descriptions/summaries from publicly available companies' Wikipedia websites, we show that using large language models (LLMs), such as GPT
    
[^6]: RoPDA：用于低资源命名实体识别的鲁棒基于提示的数据增强

    RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])

    [http://arxiv.org/abs/2307.07417](http://arxiv.org/abs/2307.07417)

    RoPDA是一种用于低资源NER的数据增强方法，通过基于预训练语言模型和连续提示进行实体和上下文增强，并提出了自一致性过滤和混合技术以优化增强样本的利用。

    

    数据增强在低资源NER任务中被广泛使用以解决数据稀缺的问题。然而，先前的数据增强方法存在破坏句法结构、标记-标签不匹配和对外部知识或手动工作的需求的缺点。为了解决这些问题，我们提出了RoPDA: 一种用于低资源NER的鲁棒基于提示的数据增强方法。基于预训练语言模型（PLMs）和连续提示，RoPDA通过五个基本的增强操作进行实体增强和上下文增强，生成标签翻转和保留标签的样本。为了优化增强样本的利用，我们提出了两种技术：自一致性过滤和混合。前者有效地消除低质量样本，后者防止直接利用标签翻转样本导致性能下降。在三个基准测试中进行了大量实验...

    Data augmentation has been widely used in low-resource NER tasks to tackle the problem of data sparsity. However, previous data augmentation methods have the disadvantages of disrupted syntactic structures, token-label mismatch, and requirement for external knowledge or manual effort. To address these issues, we propose \textbf{Ro}bust \textbf{P}rompt-based \textbf{D}ata \textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained language models (PLMs) with continuous prompt, RoPDA performs entity augmentation and context augmentation through five fundamental augmentation operations to generate label-flipping and label-preserving examples. To optimize the utilization of the augmented samples, we present two techniques: Self-Consistency Filtering and mixup. The former effectively eliminates low-quality samples, while the latter prevents performance degradation arising from the direct utilization of label-flipping samples. Extensive experiments on three benchmarks from diffe
    
[^7]: AutoHint: 自动提示生成与优化的新框架

    AutoHint: Automatic Prompt Optimization with Hint Generation. (arXiv:2307.07415v1 [cs.CL])

    [http://arxiv.org/abs/2307.07415](http://arxiv.org/abs/2307.07415)

    本文介绍了AutoHint，一种用于大型语言模型的自动提示生成和优化的新框架。该方法通过从输入-输出演示中生成提示，并利用上下文学习和零样本学习的优点，优化原始提示，从而提高了大型语言模型在特定任务上的表现。

    

    本文提出了AutoHint，一种用于大型语言模型（LLM）的自动提示工程和优化的新框架。虽然LLM在各种任务中展示了出色的注释能力，但将此能力应用于特定任务的关键在于开发高质量的提示。因此，我们提出了一种框架，通过将从输入-输出演示中派生的丰富指导纳入原始提示，以继承上下文学习和零样本学习的优点。我们将这种丰富称为“提示”，并提出了一种从标记数据中自动生成提示的框架。具体而言，从一个初始提示开始，我们的方法首先指导LLM从错误预测中推断出选定样本的新提示，然后从每个样本的提示中进行总结，并将结果添加回初始提示，形成一个新的丰富指导。该方法在BIG-Bench指令推导任务上进行了评估。

    This paper presents AutoHint, a novel framework for automatic prompt engineering and optimization for Large Language Models (LLM). While LLMs have demonstrated remarkable ability in achieving high-quality annotation in various tasks, the key to applying this ability to specific tasks lies in developing high-quality prompts. Thus we propose a framework to inherit the merits of both in-context learning and zero-shot learning by incorporating enriched instructions derived from input-output demonstrations to optimize original prompt. We refer to the enrichment as the hint and propose a framework to automatically generate the hint from labeled data. More concretely, starting from an initial prompt, our method first instructs a LLM to deduce new hints for selected samples from incorrect predictions, and then summarizes from per-sample hints and adds the results back to the initial prompt to form a new, enriched instruction. The proposed method is evaluated on the BIG-Bench Instruction Induct
    
[^8]: HuCurl: 人类引导课程发现

    HuCurl: Human-induced Curriculum Discovery. (arXiv:2307.07412v1 [cs.LG])

    [http://arxiv.org/abs/2307.07412](http://arxiv.org/abs/2307.07412)

    课程学习框架HuCurl能够根据先前对样本难度的了解，发现非单调的有效课程，并在多个NLP任务中胜过现有方法。

    

    我们引入了课程发现问题，并描述了一个能够在课程空间中基于先前有关样本难度的知识发现有效课程的课程学习框架。通过使用注释熵和损失作为难度的度量，我们展示了：（i）给定模型和数据集的最佳发现课程往往与现有文献中的单调课程相反；（ii）传统的由易到难或由难到易过渡的课程往往存在表现不佳的风险；（iii）对较小数据集和模型发现的课程在较大数据集和模型上表现良好。所提出的框架涵盖了一些现有的课程学习方法，并能够发现在几个NLP任务上胜过它们的课程。

    We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as opposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.
    
[^9]: 在计算机教育中检测LLM生成的文本：ChatGPT案例的比较研究

    Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases. (arXiv:2307.07411v1 [cs.CL])

    [http://arxiv.org/abs/2307.07411](http://arxiv.org/abs/2307.07411)

    该研究比较了8种公开可用的LLM生成文本检测器，并发现CopyLeaks是最准确的，GPTKit是最佳的减少误报率的工具，GLTR是最具韧性的工具。这些结果将为教育工作者提供维护学术诚信的洞察。

    

    随着大型语言模型（LLM）的最近改进和广泛应用，它们对教育中的学术诚信构成了严重威胁。现代的LLM生成文本检测器试图通过为教育工作者提供评估文本是否为LLM生成的服务来解决这个问题。在这项工作中，我们收集了ChatGPT创建之前计算机科学学生提交的124份作业，然后生成了40份ChatGPT作业。我们使用这些数据通过准确度、误报率和韧性这三个指标评估了八个公开可用的LLM生成文本检测器。这项工作的目的是向社区提供LLM生成文本检测器的工作情况，并为教育工作者提供洞察，以更好地维护课程的学术诚信。我们的结果发现CopyLeaks是最准确的LLM生成文本检测器，GPTKit是减少误报率的最佳LLM生成文本检测器，GLTR是最具韧性的LLM生成文本检测器。

    Due to the recent improvements and wide availability of Large Language Models (LLMs), they have posed a serious threat to academic integrity in education. Modern LLM-generated text detectors attempt to combat the problem by offering educators with services to assess whether some text is LLM-generated. In this work, we have collected 124 submissions from computer science students before the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this data to evaluate eight publicly-available LLM-generated text detectors through the measures of accuracy, false positives, and resilience. The purpose of this work is to inform the community of what LLM-generated text detectors work and which do not, but also to provide insights for educators to better maintain academic integrity in their courses. Our results find that CopyLeaks is the most accurate LLM-generated text detector, GPTKit is the best LLM-generated text detector to reduce false positives, and GLTR is the most resil
    
[^10]: KU-DMIS-MSRA在RadSum23中的预训练视觉语言模型用于放射学报告摘要

    KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization. (arXiv:2307.07409v1 [cs.CL])

    [http://arxiv.org/abs/2307.07409](http://arxiv.org/abs/2307.07409)

    本文介绍了一种新型的预训练视觉语言模型CheXOFA，通过在一般领域的训练数据上进行预训练，然后转移到胸部X射线领域，该模型能够有效地学习所需的知识和技能，并在放射学报告摘要任务上取得了卓越的性能，获得了RadSum23测试集的第一名。

    

    本文介绍了CheXOFA，一种用于胸部X射线领域的新型预训练视觉语言模型(VLM)。我们的模型首先在一般领域的多模态数据集上进行预训练，然后再转移到胸部X射线领域。在一个著名的VLM中，我们将各种特定领域的任务统一为一个简单的序列到序列的模式。这使得模型能够从有限的领域资源中有效地学习所需的知识和技能。通过在BioNLP共享任务提供的基准数据集上展示出卓越的性能，我们的模型受益于跨多个任务和领域的训练。通过集成和事实校准等微妙的技巧，我们的系统在RadSum23的隐藏测试集上取得了第一名。

    In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema. It enables the model to effectively learn the required knowledge and skills from limited resources in the domain. Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task, our model benefits from its training across multiple tasks and domains. With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.
    
[^11]: 音素检索；语音识别；元音识别

    Phoneme-retrieval; voice recognition; vowels recognition. (arXiv:2307.07407v1 [cs.CL])

    [http://arxiv.org/abs/2307.07407](http://arxiv.org/abs/2307.07407)

    音素检索技术通过特定的网络构建方式实现了语音和图像的准确检索，在特定的领域和词汇集合上表现出良好的性能。

    

    提出了一种音素检索技术，其特点是网络的构建方式。给出了一组初始神经元，这些神经元的数量大致等于数据的典型结构数量。例如，如果网络是用于语音检索，则神经元的数量必须等于特定人所属社会群体所讲语言的特征音素数量。通常，这是一项非常复杂的任务，网络的性能很大程度上依赖于用于学习的样本。如果网络是用于图像检索，则仅在要检索的数据属于特定的图像集合时起作用。如果网络是用于语音识别，则仅对某个特定的词汇集合起作用。一个典型的例子是用于飞机飞行的指令。例如，“飞机向东方转120度”这样的指令可以很容易地被识别出来。

    A phoneme-retrieval technique is proposed, which is due to the particular way of the construction of the network. An initial set of neurons is given. The number of these neurons is approximately equal to the number of typical structures of the data. For example if the network is built for voice retrieval then the number of neurons must be equal to the number of characteristic phonemes of the alphabet of the language spoken by the social group to which the particular person belongs. Usually this task is very complicated and the network can depend critically on the samples used for the learning. If the network is built for image retrieval then it works only if the data to be retrieved belong to a particular set of images. If the network is built for voice recognition it works only for some particular set of words. A typical example is the words used for the flight of airplanes. For example a command like the "airplane should make a turn of 120 degrees towards the east" can be easily reco
    
[^12]: 利用基于排名的方法增强孟加拉文本摘要的质量

    Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach. (arXiv:2307.07392v1 [cs.CL])

    [http://arxiv.org/abs/2307.07392](http://arxiv.org/abs/2307.07392)

    该论文利用基于排名的方法，通过比较四种不同的预训练孟加拉文本摘要模型的输出来确定给定文本的最准确和信息丰富的摘要。

    

    随着对既高效又准确的文本摘要技术的需求日益增加，探索能够增强专为孟加拉文本摘要而设计的预训练模型的质量和精确性变得至关重要。在文本摘要任务中，人们可以使用众多的预训练转换器模型。因此，在这些预训练摘要模型生成的各种选项中，确定给定文本的最具信息量和相关性的摘要变得非常具有挑战性。本文旨在通过利用一种简单而有效的基于排名的方法，比较四种不同的预训练孟加拉文本摘要模型的输出来确定给定文本的最准确和信息丰富的摘要。该过程首先对输入文本进行预处理，包括去除特殊字符和标点符号等不必要的元素。接下来，我们利用四个预训练的摘要模型。

    With the increasing need for text summarization techniques that are both efficient and accurate, it becomes crucial to explore avenues that enhance the quality and precision of pre-trained models specifically tailored for summarizing Bengali texts. When it comes to text summarization tasks, there are numerous pre-trained transformer models at one's disposal. Consequently, it becomes quite a challenge to discern the most informative and relevant summary for a given text among the various options generated by these pre-trained summarization models. This paper aims to identify the most accurate and informative summary for a given text by utilizing a simple but effective ranking-based approach that compares the output of four different pre-trained Bengali text summarization models. The process begins by carrying out preprocessing of the input text that involves eliminating unnecessary elements such as special characters and punctuation marks. Next, we utilize four pre-trained summarization
    
[^13]: 句子嵌入的构成对比学习

    Composition-contrastive Learning for Sentence Embeddings. (arXiv:2307.07380v1 [cs.CL])

    [http://arxiv.org/abs/2307.07380](http://arxiv.org/abs/2307.07380)

    该论文提出了一种句子嵌入的构成对比学习方法，通过最大化文本和其词组成分的对齐，实现了从无标签数据中学习文本表示的目标，并在语义文本相似性任务上取得了与最先进方法可比较的改进，而无需额外的训练目标或网络参数。

    

    自然语言的向量表示在搜索应用中非常普遍。最近，提出了基于对比学习的各种方法，用于从无标签数据中学习文本表示；通过最大化相同文本的微小扰动嵌入之间的对齐，并鼓励嵌入在更广泛的语料库中的均匀分布。与此不同的是，我们提出了最大化文本和其词组成分的对齐。我们考虑了这一目标的几个实现方式，并详细说明了每种情况下对表示的影响。在语义文本相似性任务上的实验结果显示，与最先进的方法相比，我们的方法在提高基线水平方面有所改进。此外，这项工作是第一个在不产生辅助训练目标或额外网络参数成本的情况下取得这样的改进。

    Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several realizations of this objective and elaborate the impact on representations in each case. Experimental results on semantic textual similarity tasks show improvements over baselines that are comparable with state-of-the-art approaches. Moreover, this work is the first to do so without incurring costs in auxiliary training objectives or additional network parameters.
    
[^14]: 无需注释的手语翻译中的注释关注机制

    Gloss Attention for Gloss-free Sign Language Translation. (arXiv:2307.07361v1 [cs.CV])

    [http://arxiv.org/abs/2307.07361](http://arxiv.org/abs/2307.07361)

    本论文提出了一种无需注释的手语翻译中的注释关注机制（Gloss Attention），通过学习语义边界位置和全局理解手语视频，实现了对手语视频的准确翻译。同时，通过将句子间相似性的知识转移，提高了翻译网络的理解能力。

    

    迄今为止，大多数手语翻译方法都需要使用注释来提供额外的监督信息，然而，注释的获取并不容易。为了解决这个问题，我们首先对现有模型进行分析，确认注释如何使手语翻译更容易。我们发现，注释可以为模型提供两个方面的信息：1）它可以帮助模型隐式地学习连续手语视频中的语义边界位置，2）它可以帮助模型全局理解手语视频。然后，我们提出了“注释关注”机制，使得模型能够在具有相同语义的视频片段内局部关注，就像注释帮助现有模型一样。此外，我们将自然语言模型中句子间相似性的知识转移到我们的注释关注手语翻译网络（GASLT）中，以帮助其在句子层面上理解手语视频。在多个大规模数据集上进行的实验证明了我们的方法的有效性。

    Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an analysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model implicitly learn the location of semantic boundaries in continuous sign language videos, 2) it can help the model understand the sign language video globally. We then propose \emph{gloss attention}, which enables the model to keep its attention within video segments that have the same semantics locally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similarity from the natural language model to our gloss attention SLT network (GASLT) to help it understand sign language videos at the sentence level. Experimental results on multiple large-s
    
[^15]: 跨语言的刻板偏见有何不同？

    How Different Is Stereotypical Bias Across Languages?. (arXiv:2307.07331v1 [cs.CL])

    [http://arxiv.org/abs/2307.07331](http://arxiv.org/abs/2307.07331)

    本研究拓展了评估预训练语言模型中刻板偏见的研究，通过跨语言分析发现mGPT-2在不同语言中显示出令人惊讶的反刻板行为，并且英语模型表现出最强的偏见，而土耳其语则最不明显。

    

    最近的研究展示了如何评估预训练的英语语言模型中的刻板偏见。在本研究中，我们通过系统地调查(a)多语言模型和单语模型、(b)不同基础架构下的刻板偏见、(c)多种语言中的偏见，扩展了该研究领域的多个方面。为了实现这一目标，我们利用英语的StereoSet数据集将其半自动翻译成德语、法语、西班牙语和土耳其语。我们发现，在多语言环境下进行这种类型的分析非常重要，因为我们的实验展示了一个更为细致的画面，以及与仅英语分析有显著差异的发现。我们的分析主要得出以下结论：mGPT-2（在某种程度上）在不同语言中显示出令人惊讶的反刻板行为，英语（单语）模型表现出最强的偏见，并且数据集中反映的刻板印象在土耳其语中最不明显。

    Recent studies have demonstrated how to assess the stereotypical bias in pre-trained English language models. In this work, we extend this branch of research in multiple different dimensions by systematically investigating (a) mono- and multilingual models of (b) different underlying architectures with respect to their bias in (c) multiple different languages. To that end, we make use of the English StereoSet data set (Nadeem et al., 2021), which we semi-automatically translate into German, French, Spanish, and Turkish. We find that it is of major importance to conduct this type of analysis in a multilingual setting, as our experiments show a much more nuanced picture as well as notable differences from the English-only analysis. The main takeaways from our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical behavior across languages, English (monolingual) models exhibit the strongest bias, and the stereotypes reflected in the data set are least present in Turkish mod
    
[^16]: 新闻编辑室中的混合审核：向内容管理员推荐特色帖子

    Hybrid moderation in the newsroom: Recommending featured posts to content moderators. (arXiv:2307.07317v1 [cs.IR])

    [http://arxiv.org/abs/2307.07317](http://arxiv.org/abs/2307.07317)

    本文提出了一种在新闻编辑室中使用的混合审核方法，该方法通过向内容管理员推荐特色帖子来支持他们在选择特色内容方面做出决策。该方法基于概率排序的推荐系统，结合了用户和文本内容特征，取得了较高的分类和排序性能。内容管理员在评估中发现了合适的评论，并在很大程度上接受了推荐结果。

    

    在线新闻媒体正努力处理评论区用户生成内容的审核问题。我们提出了一种基于概率排序的推荐系统来支持和授权审核员选择特色帖子，这是一项耗时的任务。通过结合用户和文本内容的特征，我们获得了测试集上的最佳分类F1分数为0.44。此外，我们在大量验证文章上观察到了均值NDCG@5的最佳值为0.87。在专家评估中，内容管理员根据推荐结果选择要推荐的评论，得到了0.83的NDCG分数。我们得出的结论是，首先，添加文本特征可以获得最佳得分；其次，虽然选择特色内容仍然有一定的主观性，但内容管理员在所有被评估的推荐中都找到了合适的评论，除了一个例外。最后，我们通过分析表现最佳的模型，迈向透明和可解释性。

    Online news outlets are grappling with the moderation of user-generated content within their comment section. We present a recommender system based on ranking class probabilities to support and empower the moderator in choosing featured posts, a time-consuming task. By combining user and textual content features we obtain an optimal classification F1-score of 0.44 on the test set. Furthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of validation articles. As an expert evaluation, content moderators assessed the output of a random selection of articles by choosing comments to feature based on the recommendations, which resulted in a NDCG score of 0.83. We conclude that first, adding text features yields the best score and second, while choosing featured content remains somewhat subjective, content moderators found suitable comments in all but one evaluated recommendations. We end the paper by analyzing our best-performing model, a step towards transparency and explaina
    
[^17]: 使用大型语言模型从知识图生成零-shot自然语言生成

    Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs. (arXiv:2307.07312v1 [cs.CL])

    [http://arxiv.org/abs/2307.07312](http://arxiv.org/abs/2307.07312)

    本论文通过使用大型语言模型，实现了基于图数据的零-shot自然语言生成。实验结果表明，该方法在部分指标上接近最新技术水平，在事实、反事实和虚构陈述的对比中也有显著的关联。

    

    在任何使用结构化知识图（KG）数据作为其底层知识表示的系统中，KG到文本生成是将图数据的部分转化为人类可理解的文本的有用工具。最近的工作表明，使用大量文本数据进行预训练的模型即使在特定图到文本任务的相对小的训练集上也能表现出良好的性能。在本文中，我们在这个概念的基础上利用大型语言模型执行零-shot生成，仅仅根据模型对三元组结构的理解进行生成。我们展示了ChatGPT在WebNLG 2020挑战赛的某些指标上实现了接近最新技术水平的性能，但在其他指标上落后。此外，我们比较了事实、反事实和虚构陈述，并展示了LLM已经对其解析的数据有关的知识与输出文本质量之间的显著关联。

    In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans. Recent work has shown that models that make use of pretraining on large amounts of text data can perform well on the KG-to-text task even with relatively small sets of training data on the specific graph-to-text task. In this paper, we build on this concept by using large language models to perform zero-shot generation based on nothing but the model's understanding of the triple structure from what it can read. We show that ChatGPT achieves near state-of-the-art performance on some measures of the WebNLG 2020 challenge, but falls behind on others. Additionally, we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows about the data it is parsing and the quality of the output text
    
[^18]: C3: 使用ChatGPT进行零-shot Text-to-SQL

    C3: Zero-shot Text-to-SQL with ChatGPT. (arXiv:2307.07306v1 [cs.CL])

    [http://arxiv.org/abs/2307.07306](http://arxiv.org/abs/2307.07306)

    C3是基于ChatGPT的零-shot Text-to-SQL方法，通过三个关键组件提供系统性的处理方法，在Spider Challenge上取得了82.3%的执行准确率，成为最先进的方法。

    

    本文提出了一种基于ChatGPT的零-shot Text-to-SQL方法，名为C3，其在Spider的测试集上达到82.3%的执行准确率，并成为Spider Challenge中最先进的零-shot Text-to-SQL方法。C3由三个关键组件组成：Clear Prompting (CP)，Calibration with Hints (CH)和Consistent Output (CO)，分别对应于模型输入，模型偏差和模型输出。它为零-shot Text-to-SQL提供了系统性的处理方法。我们进行了大量实验证明了我们提出的方法的有效性和效率。

    This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3, which achieves 82.3\% in terms of execution accuracy on the holdout test set of Spider and becomes the state-of-the-art zero-shot Text-to-SQL method on the Spider Challenge. C3 consists of three key components: Clear Prompting (CP), Calibration with Hints (CH), and Consistent Output (CO), which are corresponding to the model input, model bias and model output respectively. It provides a systematic treatment for zero-shot Text-to-SQL. Extensive experiments have been conducted to verify the effectiveness and efficiency of our proposed method.
    
[^19]: 在低资源语言中实现方言包容性识别：平衡语料库是否是答案？

    Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?. (arXiv:2307.07295v1 [cs.CL])

    [http://arxiv.org/abs/2307.07295](http://arxiv.org/abs/2307.07295)

    研究发现，对于低资源语言中的方言识别，平衡的语料库无法在不同方言之间产生相似的性能。蒙斯特方言具有最低的错误率，而阿尔斯特方言一直表现较差。康纳赫特方言和蒙斯特方言之间存在紧密的关系，但不对称。这些结果将指导未来的语料库收集和系统构建策略，以优化跨方言的性能公平性。

    

    ASR系统通常是为口头的“标准”语言构建的，对于非标准的方言/变体的性能会下降。对于像爱尔兰这样的语言来说，这是一个问题，因为它没有一个口头的标准语，而是有三个主要的方言：阿尔斯特方言（Ul），康纳赫特方言（Co）和蒙斯特方言（Mu）。为了量化说话者方言对识别性能的影响，训练了12个ASR系统，首先使用基线方言平衡的训练语料库，然后使用修改过的基线语料库，其中方言特定的材料被减去或添加。结果表明，方言平衡的语料库在不同方言之间没有相似的性能：阿尔斯特方言一直表现较差，而蒙斯特方言得到了最低的错误率。康纳赫特方言和蒙斯特方言之间存在着紧密的关系，但不对称。这些结果将指导未来的语料库收集和系统构建策略，以优化跨方言的性能公平性。

    ASR systems are generally built for the spoken 'standard', and their performance declines for non-standard dialects/varieties. This is a problem for a language like Irish, where there is no single spoken standard, but rather three major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As a diagnostic to quantify the effect of the speaker's dialect on recognition performance, 12 ASR systems were trained, firstly using baseline dialect-balanced training corpora, and then using modified versions of the baseline corpora, where dialect-specific materials were either subtracted or added. Results indicate that dialect-balanced corpora do not yield a similar performance across the dialects: the Ul dialect consistently underperforms, whereas Mu yields lowest WERs. There is a close relationship between Co and Mu dialects, but one that is not symmetrical. These results will guide future corpus collection and system building strategies to optimise for cross-dialect performance equity.
    
[^20]: 回放以回忆：针对德语语音识别的持续层特定微调

    Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition. (arXiv:2307.07280v1 [cs.CL])

    [http://arxiv.org/abs/2307.07280](http://arxiv.org/abs/2307.07280)

    该论文探讨了如何通过持续层特定微调和经验回放技术来改善德语语音识别模型在较小领域中的性能，并提高模型的鲁棒性。

    

    虽然自动语音识别（ASR）模型在引入无监督或自监督训练技术方面取得了显著进展，但这些改进仍然仅限于某些语言和说话者。迁移学习使得大规模多语言模型能够适应不仅是低资源语言，还包括更特定的说话者群体。然而，对新领域的数据进行微调通常会导致在原始领域的性能下降。因此，在我们的实验中，我们研究了大规模ASR模型在较小领域中的性能可以有多好，使用我们自己的德语高级语音命令数据集（SVC-de），以及在训练过程中通过选择性地冻结模型的部分来保留多少通用语音识别性能。为了进一步增加ASR模型对微调领域之外的词汇和说话者的鲁棒性，我们应用经验回放进行连续训练。

    While Automatic Speech Recognition (ASR) models have shown significant advances with the introduction of unsupervised or self-supervised training techniques, these improvements are still only limited to a subsection of languages and speakers. Transfer learning enables the adaptation of large-scale multilingual models to not only low-resource languages but also to more specific speaker groups. However, fine-tuning on data from new domains is usually accompanied by a decrease in performance on the original domain. Therefore, in our experiments, we examine how well the performance of large-scale ASR models can be approximated for smaller domains, with our own dataset of German Senior Voice Commands (SVC-de), and how much of the general speech recognition performance can be preserved by selectively freezing parts of the model during training. To further increase the robustness of the ASR model to vocabulary and speakers outside of the fine-tuned domain, we apply Experience Replay for conti
    
[^21]: 语音和视听理解中的单词是否同样令人惊讶？

    Are words equally surprising in audio and audio-visual comprehension?. (arXiv:2307.07277v1 [cs.CL])

    [http://arxiv.org/abs/2307.07277](http://arxiv.org/abs/2307.07277)

    本研究调查了视觉信息对口头语言理解的影响，并评估了不同语言模型在多模态和单模态环境下的表现。结果表明，多模态和单模态环境下的认知努力存在差异，局部词汇上下文对多模态环境下的认知处理有显著影响。

    

    我们报告了一项受控研究，调查了视觉信息（即看到说话者）对口头语言理解的影响。我们比较了相同口头刺激的仅音频和音频-视觉呈现中每个单词相关的ERP标记（N400）。我们评估了不同类型的语言模型（具体来说是n-gram和Transformer模型）基于词汇上下文预测N400响应的可预测性度量（surprisal）。我们的结果表明，认知努力在多模态和单模态环境中存在显著差异。此外，我们的研究结果表明，在仅音频环境中，具有较大词汇上下文的Transformer模型提供更好的拟合，而2-gram语言模型在多模态环境中更有效。这凸显了局部词汇上下文对多模态环境下认知处理的显著影响。

    We report a controlled study investigating the effect of visual information (i.e., seeing the speaker) on spoken language comprehension. We compare the ERP signature (N400) associated with each word in audio-only and audio-visual presentations of the same verbal stimuli. We assess the extent to which surprisal measures (which quantify the predictability of words in their lexical context) are generated on the basis of different types of language models (specifically n-gram and Transformer models) that predict N400 responses for each word. Our results indicate that cognitive effort differs significantly between multimodal and unimodal settings. In addition, our findings suggest that while Transformer-based models, which have access to a larger lexical context, provide a better fit in the audio-only setting, 2-gram language models are more effective in the multimodal setting. This highlights the significant impact of local lexical context on cognitive processing in a multimodal environmen
    
[^22]: MorphPiece: 远离统计语言表示的一步

    MorphPiece : Moving away from Statistical Language Representation. (arXiv:2307.07262v1 [cs.CL])

    [http://arxiv.org/abs/2307.07262](http://arxiv.org/abs/2307.07262)

    本文提出了一种基于语言学动机的分词方案MorphPiece，并使用该方案训练了一个称为MorphGPT的语言模型。MorphGPT在语言建模以及各种NLP任务上都表现出了比传统模型更优异的性能。

    

    分词是现代自然语言处理流程中至关重要的一部分。然而，用于大型语言模型的当代分词器基于对文本语料库的统计分析，对语言特征的考虑较少。我们提出了一种基于语言学动机的分词方案MorphPiece，部分基于底层文本的形态分割。使用该分词器（称为MorphGPT）训练的类GPT的因果语言模型显示出比在标准BPE分词器上训练时更优越的收敛性。具体来说，我们获得了与规模大6倍的模型相媲美的语言建模性能。此外，我们在监督和无监督的条件下对MorphGPT在各种NLP任务上进行了评估，并发现在各个方面与GPT-2模型相比有更出色的性能。

    Tokenization is a critical part of modern NLP pipelines. However, contemporary tokenizers for Large Language Models are based on statistical analysis of text corpora, without much consideration to the linguistic features. We propose a linguistically motivated tokenization scheme, MorphPiece, which is based partly on morphological segmentation of the underlying text. A GPT-style causal language model trained on this tokenizer (called MorphGPT) shows superior convergence compared to the same architecture trained on a standard BPE tokenizer. Specifically we get Language Modeling performance comparable to a 6 times larger model. Additionally, we evaluate MorphGPT on a variety of NLP tasks in supervised and unsupervised settings and find superior performance across the board, compared to GPT-2 model.
    
[^23]: 用混合池化网络和Drop Mask提升BERT

    Improving BERT with Hybrid Pooling Network and Drop Mask. (arXiv:2307.07258v1 [cs.CL])

    [http://arxiv.org/abs/2307.07258](http://arxiv.org/abs/2307.07258)

    本文提出了HybridBERT模型，通过结合自注意力和池化网络来编码每一层中的不同上下文特征，并且引入DropMask方法来解决预训练和微调之间的不匹配问题。实验证明，HybridBERT在预训练和迁移学习任务中均优于BERT，并且DropMask能够提高精度。

    

    基于Transformer的预训练语言模型BERT在各种自然语言理解任务中取得了巨大成功。先前的研究发现，BERT在不同层次捕捉到丰富的语言信息层次结构。然而，普通的BERT在每一层使用相同的自注意力机制来建模不同的上下文特征。在本文中，我们提出了一个HybridBERT模型，它结合了自注意力和池化网络来编码每一层中的不同上下文特征。此外，我们提出了一个简单的DropMask方法，以解决由于在掩蔽语言建模预训练过程中过度使用特殊掩蔽标记引起的预训练和微调之间的不匹配问题。实验证明，HybridBERT在预训练中表现优于BERT，具有更低的损失、更快的训练速度（相对降低8%）、更低的内存成本（相对降低13%），并且在迁移学习中在下游任务中具有相对更高的精度（相对增加1.5%）。此外，DropMask能够提高精度。

    Transformer-based pre-trained language models, such as BERT, achieve great success in various natural language understanding tasks. Prior research found that BERT captures a rich hierarchy of linguistic information at different layers. However, the vanilla BERT uses the same self-attention mechanism for each layer to model the different contextual features. In this paper, we propose a HybridBERT model which combines self-attention and pooling networks to encode different contextual features in each layer. Additionally, we propose a simple DropMask method to address the mismatch between pre-training and fine-tuning caused by excessive use of special mask tokens during Masked Language Modeling pre-training. Experiments show that HybridBERT outperforms BERT in pre-training with lower loss, faster training speed (8% relative), lower memory cost (13% relative), and also in transfer learning with 1.5% relative higher accuracies on downstream tasks. Additionally, DropMask improves accuracies 
    
[^24]: 对话代理101：设计有效的对话系统的关键要素初学者指南

    Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems. (arXiv:2307.07255v1 [cs.CL])

    [http://arxiv.org/abs/2307.07255](http://arxiv.org/abs/2307.07255)

    本文提供了一个对话代理设计的相关要素的综合概述，包括对话代理的主要特征、支持任务、数据集和评估方法。研究表明，构建单独的模型来处理不同的对话任务是昂贵且冗余的。

    

    通过与同行进行交流来分享想法是人类互动的主要方式。因此，在对话人工智能领域进行了广泛的研究，导致对话任务、数据集和方法的可用性和多样性增加。然而，由于多个任务同时探索，当前对话人工智能的现状变得分散。因此，为了帮助从零开始设计对话代理的从业者，本研究提供了对对话代理的主要特征、支持任务、相应的开放领域数据集以及用于基准测试这些数据集的方法的综合概述。我们观察到不同的方法已被用于解决不同的对话任务。然而，为每个任务构建单独的模型是昂贵且冗余的。

    Sharing ideas through communication with peers is the primary mode of human interaction. Consequently, extensive research has been conducted in the area of conversational AI, leading to an increase in the availability and diversity of conversational tasks, datasets, and methods. However, with numerous tasks being explored simultaneously, the current landscape of conversational AI becomes fragmented. Therefore, initiating a well-thought-out model for a dialogue agent can pose significant challenges for a practitioner. Towards highlighting the critical ingredients needed for a practitioner to design a dialogue agent from scratch, the current study provides a comprehensive overview of the primary characteristics of a dialogue agent, the supporting tasks, their corresponding open-domain datasets, and the methods used to benchmark these datasets. We observe that different methods have been used to tackle distinct dialogue tasks. However, building separate models for each task is costly and 
    
[^25]: 带有自我净化的大型语言模型的认证鲁棒性

    Certified Robustness for Large Language Models with Self-Denoising. (arXiv:2307.07171v1 [cs.CL])

    [http://arxiv.org/abs/2307.07171](http://arxiv.org/abs/2307.07171)

    传统的随机平滑方法对于大型语言模型的直接应用具有挑战性，为了解决认证半径很小的问题，提出了一种带有自我净化的新方法。

    

    尽管大型语言模型（LLM）在广泛的实际应用中取得了巨大的成功，但它们对于噪声输入的脆弱性显著限制了它们的应用，尤其是在高风险环境中。在这些环境下，确保大型语言模型的每个预测都是稳定的非常重要，即在输入的微小差异情况下，LLM的预测应该是一致的。这主要涉及到认证鲁棒LLM的研究，即在输入周围的局部区域中，所有LLM的预测都得到认证是正确的。随机平滑已经展示了在认证LLM的鲁棒性和预测稳定性方面的巨大潜力。然而，随机平滑在进行模型预测之前需要对输入添加噪声，其认证性能在很大程度上取决于模型在受损数据上的表现。因此，它直接应用于LLM仍然具有挑战性，并且通常会导致很小的认证半径。为了解决这个问题，

    Although large language models (LLMs) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. In these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., LLM predictions should be consistent given minor differences in the input. This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input. Randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of LLMs. However, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius. To address this issue,
    
[^26]: 双指称表达理解与取物任务中的切换头尾漏斗UNITER

    Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks. (arXiv:2307.07166v1 [cs.RO])

    [http://arxiv.org/abs/2307.07166](http://arxiv.org/abs/2307.07166)

    本文提出了一种切换头尾漏斗UNITER，用于解决双指称表达理解与取物任务。该方法通过使用单个模型分别预测目标物品和目的地，避免了传统方法需要进行所有组合推理的计算复杂性问题。验证结果表明，该方法在一个新构建的数据集上取得了良好的效果。

    

    本文描述了一种家庭服务机器人（DSR），根据自由形式的自然语言指令捡起日常物品并将其运送到指定的位置。给定如“将盘子左侧的瓶子移到空椅子上”之类的指令，DSR需要从环境中的多个候选项中识别出瓶子和椅子，并将目标物品搬运到目的地。现有的多模态语言理解方法大多在计算复杂性上都不实用，因为它们需要对目标物品候选项和目的地候选项的所有组合进行推理。我们提出了切换头尾漏斗UNITER，通过使用单个模型分别预测目标物品和目的地来解决这个任务。我们的方法在一个新构建的数据集上进行了验证，该数据集包含在标准的体验型人工智能模拟器中捕捉的物体操作指令和半真实感图像。

    This paper describes a domestic service robot (DSR) that fetches everyday objects and carries them to specified destinations according to free-form natural language instructions. Given an instruction such as "Move the bottle on the left side of the plate to the empty chair," the DSR is expected to identify the bottle and the chair from multiple candidates in the environment and carry the target object to the destination. Most of the existing multimodal language understanding methods are impractical in terms of computational complexity because they require inferences for all combinations of target object candidates and destination candidates. We propose Switching Head-Tail Funnel UNITER, which solves the task by predicting the target object and the destination individually using a single model. Our method is validated on a newly-built dataset consisting of object manipulation instructions and semi photo-realistic images captured in a standard Embodied AI simulator. The results show that
    
[^27]: 学习为大型语言模型检索上下文示例

    Learning to Retrieve In-Context Examples for Large Language Models. (arXiv:2307.07164v1 [cs.CL])

    [http://arxiv.org/abs/2307.07164](http://arxiv.org/abs/2307.07164)

    本文提出了一个新颖的框架，通过迭代训练密集检索器来为大型语言模型识别高质量的上下文示例，从而显著提高了上下文学习性能，并展示了在训练期间对未见过任务的泛化能力。

    

    大型语言模型（LLMs）展示了它们在上下文中学习的能力，使它们能够根据少量的输入-输出示例执行各种任务。然而，上下文学习的有效性在很大程度上依赖于所选示例的质量。在本文中，我们提出了一个新颖的框架，通过迭代训练密集检索器，可以为LLMs识别高质量的上下文示例。我们的框架首先训练基于LLM反馈的奖励模型来评估候选示例的质量，然后通过知识蒸馏训练基于双编码器的密集检索器。我们在30个任务套件上的实验证明，我们的框架显著提高了上下文学习性能。此外，我们还展示了我们的框架在训练期间对未见过任务的泛化能力。深入分析表明，我们的模型通过检索具有相似模式的示例来提高性能，而这种增益在不同规模的LLMs中是一致的。

    Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.
    
[^28]: 如人类一样驾驶：用大型语言模型重新思考自动驾驶

    Drive Like a Human: Rethinking Autonomous Driving with Large Language Models. (arXiv:2307.07162v1 [cs.RO])

    [http://arxiv.org/abs/2307.07162](http://arxiv.org/abs/2307.07162)

    本文提出了使用大型语言模型（LLM）来实现自动驾驶系统驾驶如人类一般，并通过推理、解释和记忆能力解决长尾情况的潜力。通过构建闭环系统进行示范，大量实验证明了LLM在驾驶场景中的卓越能力。

    

    在本文中，我们探讨了使用大型语言模型（LLM）以人类方式理解驾驶环境及其在面对复杂情景时的推理、解释和记忆能力。我们认为，传统的基于优化和模块化的自动驾驶（AD）系统在处理长尾情况时面临着固有的性能限制。为了解决这个问题，我们提出理想的AD系统应该像人一样驾驶，通过持续驾驶积累经验，并使用常识解决问题。为了实现这一目标，我们确定了AD系统的三个关键能力：推理、解释和记忆。通过构建一个闭环系统展示LLM在驾驶场景中的理解和环境交互能力，我们证明了采用LLM在驾驶场景中的可行性。我们的大量实验证明LLM展现出推理并解决长尾情况的令人印象深刻的能力。

    In this paper, we explore the potential of using a large language model (LLM) to understand the driving environment in a human-like manner and analyze its ability to reason, interpret, and memorize when facing complex scenarios. We argue that traditional optimization-based and modular autonomous driving (AD) systems face inherent performance limitations when dealing with long-tail corner cases. To address this problem, we propose that an ideal AD system should drive like a human, accumulating experience through continuous driving and using common sense to solve problems. To achieve this goal, we identify three key abilities necessary for an AD system: reasoning, interpretation, and memorization. We demonstrate the feasibility of employing an LLM in driving scenarios by building a closed-loop system to showcase its comprehension and environment-interaction abilities. Our extensive experiments show that the LLM exhibits the impressive ability to reason and solve long-tailed cases, provid
    
[^29]: 不要随机遮盖：通过遮盖领域内关键词进行有效的领域自适应预训练

    Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords. (arXiv:2307.07160v1 [cs.CL])

    [http://arxiv.org/abs/2307.07160](http://arxiv.org/abs/2307.07160)

    本研究提出了一种通过遮盖领域内关键词进行的领域自适应预训练方法，实验结果表明该方法优于使用随机遮盖的领域内预训练和常见的预训练然后微调范式。

    

    我们提出了一种新颖的领域无关的领域内预训练方法，介于通用预训练和微调之间。我们的方法选择性地遮盖领域内的关键词，即提供目标领域的紧凑表示的单词。我们使用KeyBERT (Grootendorst, 2020)来识别这些关键词。我们使用六种不同的设置对我们的方法进行评估：三个数据集与两个不同的预训练语言模型（PLMs）相结合。我们的结果表明，使用我们的领域内预训练策略微调的PLMs优于使用随机遮盖的领域内预训练的PLMs，并且优于遵循常见的预训练然后微调范式的PLMs。此外，识别领域内关键词的开销是合理的，例如，对于BERT Large (Devlin et al., 2019)来说，是预训练时间的7-15%（两个epoch）。

    We propose a novel task-agnostic in-domain pre-training method that sits between generic pre-training and fine-tuning. Our approach selectively masks in-domain keywords, i.e., words that provide a compact representation of the target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We evaluate our approach using six different settings: three datasets combined with two distinct pre-trained language models (PLMs). Our results reveal that the fine-tuned PLMs adapted using our in-domain pre-training strategy outperform PLMs that used in-domain pre-training with random masking as well as those that followed the common pre-train-then-fine-tune paradigm. Further, the overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).
    
[^30]: MMSD2.0：面向可靠的多模式讽刺检测系统

    MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System. (arXiv:2307.07135v1 [cs.CL])

    [http://arxiv.org/abs/2307.07135](http://arxiv.org/abs/2307.07135)

    MMSD2.0是一个修正了MMSD缺陷的数据集，提出了多视图CLIP框架，能够从多个角度利用多粒度线索进行多模式讽刺检测，显著优于以前最好的基准算法。

    

    多模式讽刺检测引起了近期的广泛关注。然而，现有的基准数据集（MMSD）存在一些缺点，阻碍了可靠的多模式讽刺检测系统的发展：（1）MMSD中存在一些虚假线索，导致模型学习偏差；（2）MMSD中的负样本并不总是合理的。为了解决上述问题，我们引入了MMSD2.0，一个纠正了MMSD缺陷的纠正数据集，通过去除虚假线索和重新注释不合理样本来完成。同时，我们提出了一种称为多视图CLIP的新颖框架，能够从多个角度（文本、图像和文本-图像交互视图）利用多粒度线索进行多模式讽刺检测。大量实验证明，MMSD2.0是构建可靠的多模式讽刺检测系统的有价值的基准数据集，而多视图CLIP能够显著优于以前的最佳基准算法。

    Multi-modal sarcasm detection has attracted much recent attention. Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder the development of reliable multi-modal sarcasm detection system: (1) There are some spurious cues in MMSD, leading to the model bias learning; (2) The negative samples in MMSD are not always reasonable. To solve the aforementioned issues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings of MMSD, by removing the spurious cues and re-annotating the unreasonable samples. Meanwhile, we present a novel framework called multi-view CLIP that is capable of leveraging multi-grained cues from multiple perspectives (i.e., text, image, and text-image interaction view) for multi-modal sarcasm detection. Extensive experiments show that MMSD2.0 is a valuable benchmark for building reliable multi-modal sarcasm detection systems and multi-view CLIP can significantly outperform the previous best baselines.
    
[^31]: 通过基于LLM的属性操作生成高效训练数据

    Generating Efficient Training Data via LLM-based Attribute Manipulation. (arXiv:2307.07099v1 [cs.CL])

    [http://arxiv.org/abs/2307.07099](http://arxiv.org/abs/2307.07099)

    本文提出了一种通过大型语言模型（LLMs）生成精心制作的训练数据来引导少样本学习的方法。通过利用LLMs操作任务特定属性并重构新的句子，我们实现了标签交换数据的生成，与其他基于LLMs的文本生成方法相比具有更好的效果。同时，研究结果还显示了通过LLM引导学习的潜力，即使在更少的监督情况下也能取得良好的表现。

    

    本文提出了一种新颖的方法，链式思维属性操作（CoTAM），通过从大型语言模型（LLMs）中精心制作的数据来引导少样本学习。主要思想是仅对任务目标属性进行更改并创建数据。受到面部属性操作的启发，我们的方法利用LLMs来操作任务特定属性并以受控的方式重构新的句子，从而生成标签交换数据。我们采用链式思维分解和重构来适应LLMs，而不是传统的潜在表示控制方法。在文本分类和其他任务上进行了广泛的实验结果验证了CoTAM相对于其他具有相同数量训练样本的基于LLMs的文本生成方法的优势。分析结果可视化了CoTAM的属性操作效果，并展示了在更少监督的情况下通过LLM引导学习的潜力。

    In this paper, we propose a novel method, Chain-of-Thoughts Attribute Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from Large Language Models (LLMs). The main idea is to create data with changes only in the attribute targeted by the task. Inspired by facial attribute manipulation, our approach generates label-switched data by leveraging LLMs to manipulate task-specific attributes and reconstruct new sentences in a controlled manner. Instead of conventional latent representation controlling, we implement chain-of-thoughts decomposition and reconstruction to adapt the procedure to LLMs. Extensive results on text classification and other tasks verify the advantage of CoTAM over other LLM-based text generation methods with the same number of training examples. Analysis visualizes the attribute manipulation effectiveness of CoTAM and presents the potential of LLM-guided learning with even less supervision.
    
[^32]: 虚拟语音助手中的对话修复分析

    An Analysis of Dialogue Repair in Virtual Voice Assistants. (arXiv:2307.07076v1 [cs.HC])

    [http://arxiv.org/abs/2307.07076](http://arxiv.org/abs/2307.07076)

    该研究分析了虚拟语音助手中对话修复的特点，指出了人助手和人对人对话修复策略的差异，以及不同语言和助手之间的差异。

    

    语言使用者通常使用修复启动器来修复他们在口头交流过程中发生的基本断开。以前的研究主要集中在人对人使用修复启动器的领域。我们提出了一种对话修复结构的研究，其中对话启动器是人类，发起或响应修复的一方是虚拟助手。本研究使用两种流行的助手，Google Assistant和Apple的Siri，检查了英语和西班牙语中的修复启动器的使用情况。我们的目标是比较语音助手对需要修复的对话和人对人对话的回应之间的差异，最终的数据表明，不仅在人助手和人对人对话修复策略之间存在差异，而且在助手和所研究的语言之间也存在差异。

    Language speakers often use what are known as repair initiators to mend fundamental disconnects that occur between them during verbal communication. Previous research in this field has mainly focused on the human-to-human use of repair initiator. We proposed an examination of dialogue repair structure wherein the dialogue initiator is human and the party that initiates or responds to the repair is a virtual assistant. This study examined the use of repair initiators in both English and Spanish with two popular assistants, Google Assistant and Apple's Siri. Our aim was to codify the differences, if any, in responses by voice assistants to dialogues in need of repair as compared to human-human dialogues also in need of repair. Ultimately the data demonstrated that not only were there differences between human-assistant and human-human dialogue repair strategies, but that there were likewise differences among the assistants and the languages studied.
    
[^33]: 利用预训练的ASR编码器实现高效且有效的端到端语音意图分类和槽位填充

    Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling. (arXiv:2307.07057v1 [cs.CL])

    [http://arxiv.org/abs/2307.07057](http://arxiv.org/abs/2307.07057)

    本文通过利用预训练的ASR编码器来初始化一个端到端模型，在语音意图分类和槽位填充任务上取得了新的最优结果。与自监督学习相比，使用ASR预训练的编码器在效果上更好，并且能够实现参数效率。此外，与级联模型相比，端到端模型在大部分情况下都更好，除非提供了oracle ASR模型。

    

    本文研究了语音意图分类和槽位填充（SICSF），提出使用在语音识别（ASR）上预训练的编码器来初始化端到端（E2E）Conformer-Transformer模型。我们在SLURP数据集上实现了新的最优结果，意图准确率达到90.14%，SLURP-F1为82.27%。我们将我们的模型与自监督学习（SSL）预训练的编码器进行比较，并表明相对于SSL，ASR预训练在SICSF上更加有效。为了探索参数效率，我们冻结了编码器并添加了Adapter模块，结果表明只有使用ASR预训练的编码器才能实现参数效率，而SSL编码器需要完全微调才能达到可比较的结果。此外，我们对端到端模型与级联模型（ASR+NLU）进行了深入比较，并表明除非提供了oracle ASR模型，否则端到端模型优于级联模型。最后，我们的模型是第一个能够达到与预先训练的ASR模型相同性能的端到端模型。

    We study speech intent classification and slot filling (SICSF) by proposing to use an encoder pretrained on speech recognition (ASR) to initialize an end-to-end (E2E) Conformer-Transformer model, which achieves the new state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and 82.27% SLURP-F1. We compare our model with encoders pretrained on self-supervised learning (SSL), and show that ASR pretraining is much more effective than SSL for SICSF. To explore parameter efficiency, we freeze the encoder and add Adapter modules, and show that parameter efficiency is only achievable with an ASR-pretrained encoder, while the SSL encoder needs full finetuning to achieve comparable results. In addition, we provide an in-depth comparison on end-to-end models versus cascading models (ASR+NLU), and show that E2E models are better than cascaded models unless an oracle ASR model is provided. Last but not least, our model is the first E2E model that achieves the same performance as
    
[^34]: 充分利用有限的上下文长度：预测能力随临床记录类型和记录部分的不同而变化

    Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section. (arXiv:2307.07051v1 [cs.CL])

    [http://arxiv.org/abs/2307.07051](http://arxiv.org/abs/2307.07051)

    通过分析临床记录的部分，我们发现预测能力在不同类型的记录间存在差异，并且当上下文长度较大时，组合不同类型的记录可以改善性能。我们的研究结果表明，精心选择的采样函数可以使从临床记录中提取信息更加高效。

    

    最近大规模语言模型的进展使得使用临床记录的自由文本进行自然语言处理的兴趣重新燃起。临床记录的一个区别特点是它们跨越多个长文档的长时间跨度。临床记录的独特结构带来了一个新的设计选择：当语言模型预测器的上下文长度有限时，应选择临床记录的哪个部分作为输入？现有研究要么选择具有领域知识的输入，要么简单地截断它们。我们提出了一个分析高预测能力部分的框架。使用MIMIC-III数据集，我们展示了以下发现：1）预测能力分布在护理记录和出院记录之间是不同的；2）当上下文长度较大时，组合不同类型的记录可以提高性能。我们的研究结果表明，精心选择的采样函数可以使从临床记录中提取信息更加高效。

    Recent advances in large language models have led to renewed interest in natural language processing in healthcare using the free text of clinical notes. One distinguishing characteristic of clinical notes is their long time span over multiple long documents. The unique structure of clinical notes creates a new design choice: when the context length for a language model predictor is limited, which part of clinical notes should we choose as the input? Existing studies either choose the inputs with domain knowledge or simply truncate them. We propose a framework to analyze the sections with high predictive power. Using MIMIC-III, we show that: 1) predictive power distribution is different between nursing notes and discharge notes and 2) combining different types of notes could improve performance when the context length is large. Our findings suggest that a carefully selected sampling function could enable more efficient information extraction from clinical notes.
    
[^35]: MegaWika: 跨越50种多样的语言的数百万报告及其来源

    MegaWika: Millions of reports and their sources across 50 diverse languages. (arXiv:2307.07049v1 [cs.CL])

    [http://arxiv.org/abs/2307.07049](http://arxiv.org/abs/2307.07049)

    MegaWika是一个由13百万个维基百科文章和71百万个引用源材料组成的多语言数据集，用于协作式 AI 辅助报告生成的新模型的开发。它提供了跨语言应用中的非英文文章翻译和自动语义分析的FrameNet解析。该数据集是句子级报告生成的最大资源，也是唯一的跨多种语言的报告生成数据集。此外，还提供了基线结果和训练模型用于跨语言问答和引用检索。 (tl;dr)

    

    为促进协作式 AI 辅助报告生成新模型的发展，我们介绍了 MegaWika，它由50种不同语言中的1300万个维基百科文章以及7100万个引用源材料组成。我们对这个数据集进行了多种应用的处理，超出了最初的维基百科引用提取和内容网页抓取，包括将非英文文章翻译为跨语言应用以及提供的 FrameNet 解析用于自动语义分析。MegaWika是用于句子级报告生成的最大资源，也是唯一的跨多种语言的报告生成数据集。我们通过语义分层样本进行了资源质量的手动分析。最后，我们为自动报告生成的关键步骤提供了基线结果和训练模型：跨语言问答和引用检索。

    To foster the development of new models for collaborative AI-assisted report generation, we introduce MegaWika, consisting of 13 million Wikipedia articles in 50 diverse languages, along with their 71 million referenced source materials. We process this dataset for a myriad of applications, going beyond the initial Wikipedia citation extraction and web scraping of content, including translating non-English articles for cross-lingual applications and providing FrameNet parses for automated semantic analysis. MegaWika is the largest resource for sentence-level report generation and the only report generation dataset that is multilingual. We manually analyze the quality of this resource through a semantically stratified sample. Finally, we provide baseline results and trained models for crucial steps in automated report generation: cross-lingual question answering and citation retrieval.
    
[^36]: DIALGEN: 通过人工生成对话改善对人际对话的理解的协同模型

    DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations. (arXiv:2307.07047v1 [cs.CL])

    [http://arxiv.org/abs/2307.07047](http://arxiv.org/abs/2307.07047)

    DIALGEN是一个人机半自动对话生成框架，通过迭代生成子对话和使用人工反馈来改善模型性能，适用于自动理解人际对话的应用。

    

    需要自动理解人际对话的应用通常面临与真实世界数据中的私人信息，如呼叫中心或临床对话，有关的挑战。处理受保护的数据也会增加注释成本，从而限制技术发展。为了解决这些挑战，我们提出了DIALGEN，一种人机半自动对话生成框架。DIALGEN使用一种语言模型（ChatGPT），可以遵循架构和风格规范，生成流利的对话文本，通过迭代生成子对话并使用人工反馈来纠正不一致或重定对话的流程。在将代理-客户信息收集呼叫归纳为对话状态跟踪的结构化概括实验中，我们展示了DIALGEN数据能够显著提高模型性能。

    Applications that could benefit from automatic understanding of human-human conversations often come with challenges associated with private information in real-world data such as call center or clinical conversations. Working with protected data also increases costs of annotation, which limits technology development. To address these challenges, we propose DIALGEN, a human-in-the-loop semi-automated dialogue generation framework. DIALGEN uses a language model (ChatGPT) that can follow schema and style specifications to produce fluent conversational text, generating a complex conversation through iteratively generating subdialogues and using human feedback to correct inconsistencies or redirect the flow. In experiments on structured summarization of agent-client information gathering calls, framed as dialogue state tracking, we show that DIALGEN data enables significant improvement in model performance.
    
[^37]: 通过依存子树交换实现机器翻译的数据增强

    Data Augmentation for Machine Translation via Dependency Subtree Swapping. (arXiv:2307.07025v1 [cs.CL])

    [http://arxiv.org/abs/2307.07025](http://arxiv.org/abs/2307.07025)

    通过依存子树交换实现机器翻译的数据增强框架，通过从依存树中提取相应的子树并进行交换，创建增强样本。实验证明，在资源受限环境中，它在3个语言对上表现出了持续的BLEU分数提高。

    

    我们提出了一种通用的通过依存子树交换实现数据增强的框架，适用于机器翻译。我们从源句子和目标句子的依存树中提取相应的子树，并将其在句子之间交换，以创建增强样本。我们根据依存树的图形相似性和附加启发式方法进行彻底的过滤，以确保提取的子树对应于相同的含义。我们使用IWSLT文本翻译数据集和Hunglish2语料库，在4个语言对的两个方向上进行了资源受限实验。结果表明，在4个语言对中，我们的基线模型在3个语言对上的BLEU分数有了持续的提高。我们的代码在GitHub上可用。

    We present a generic framework for data augmentation via dependency subtree swapping that is applicable to machine translation. We extract corresponding subtrees from the dependency parse trees of the source and target sentences and swap these across bisentences to create augmented samples. We perform thorough filtering based on graphbased similarities of the dependency trees and additional heuristics to ensure that extracted subtrees correspond to the same meaning. We conduct resource-constrained experiments on 4 language pairs in both directions using the IWSLT text translation datasets and the Hunglish2 corpus. The results demonstrate consistent improvements in BLEU score over our baseline models in 3 out of 4 language pairs. Our code is available on GitHub.
    
[^38]: 选举激励数据集：波兰选举的应用案例

    Electoral Agitation Data Set: The Use Case of the Polish Election. (arXiv:2307.07007v1 [cs.CL])

    [http://arxiv.org/abs/2307.07007](http://arxiv.org/abs/2307.07007)

    这篇论文介绍了一个用于检测波兰语选举激励的公开数据集，该数据集包含6,112条人工标注的推文。翻译模型HerBERT在该数据集上达到了68%的F1分数。

    

    社交媒体的流行使政治家们使用它进行政治广告。因此，社交媒体上充满了选举激励（选举宣传），特别是在选举活动期间。选举管理机构无法追踪符合选举法规定的激励信息的传播和数量。本文解决了一个关键问题，同时揭示了迄今为止尚未有效针对的一个利基。因此，我们提供了第一个用于检测波兰语选举激励的公开数据集。其中包含6,112条人工标注的带有四种法律约束类别标签的推文。我们实现了0.66的评注者一致性（Cohen's kappa分值）。另一位评注者解决了前两位评注者之间的不匹配，提高了标注流程的一致性和复杂性。新创建的数据集被用于优化一个名为HerBERT的波兰语言模型（达到68%的F1分数）。我们还提出了一些潜在的应用案例。

    The popularity of social media makes politicians use it for political advertisement. Therefore, social media is full of electoral agitation (electioneering), especially during the election campaigns. The election administration cannot track the spread and quantity of messages that count as agitation under the election code. It addresses a crucial problem, while also uncovering a niche that has not been effectively targeted so far. Hence, we present the first publicly open data set for detecting electoral agitation in the Polish language. It contains 6,112 human-annotated tweets tagged with four legally conditioned categories. We achieved a 0.66 inter-annotator agreement (Cohen's kappa score). An additional annotator resolved the mismatches between the first two improving the consistency and complexity of the annotation process. The newly created data set was used to fine-tune a Polish Language Model called HerBERT (achieving a 68% F1 score). We also present a number of potential use ca
    
[^39]: 文本分类任务中经典的带外分布检测方法的基准研究

    Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks. (arXiv:2307.07002v1 [cs.CL])

    [http://arxiv.org/abs/2307.07002](http://arxiv.org/abs/2307.07002)

    本文评估了八种易于集成到现有NLP系统中且不需要额外带外数据或模型修改的带外分布检测方法，并提供了一个完全可复现实验结果的研究环境。分析表明现有的NLP任务中带外分布检测方法对于捕捉所有由不同类型分布转换特征的样本尚不够敏感，这需要未来的工作来开发更有效的方法。

    

    最先进的模型在受控环境下表现良好，但面对带外分布的例子时往往表现出困难，因此带外分布检测成为NLP系统中关键的组成部分。本文着重强调了现有NLP领域带外分布检测方法的局限性。具体来说，我们评估了八种易于集成到现有NLP系统中且不需要额外带外数据或模型修改的带外分布检测方法。我们的贡献之一是提供了一个结构良好的研究环境，可以完全复现实验结果。此外，我们的分析表明现有的NLP任务中带外分布检测方法对于捕捉所有由不同类型分布转换特征的样本尚不够敏感。在领域背景转变和单词随机排列的情况下，测试情景尤其具有挑战性。这突显了未来需要开展更有效的工作来发展带外分布检测方法。

    State-of-the-art models can perform well in controlled environments, but they often struggle when presented with out-of-distribution (OOD) examples, making OOD detection a critical component of NLP systems. In this paper, we focus on highlighting the limitations of existing approaches to OOD detection in NLP. Specifically, we evaluated eight OOD detection methods that are easily integrable into existing NLP systems and require no additional OOD data or model modifications. One of our contributions is providing a well-structured research environment that allows for full reproducibility of the results. Additionally, our analysis shows that existing OOD detection methods for NLP tasks are not yet sufficiently sensitive to capture all samples characterized by various types of distributional shifts. Particularly challenging testing scenarios arise in cases of background shift and randomly shuffled word order within in domain texts. This highlights the need for future work to develop more ef
    
[^40]: 迈向填充通用工程设计知识的方法

    Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])

    [http://arxiv.org/abs/2307.06985](http://arxiv.org/abs/2307.06985)

    这项研究提出了一种从专利文件中提取工程设计知识的方法，通过构建知识图来填充通用设计知识，并与现有方法进行了比较。

    

    为了填充通用工程设计知识，我们提出了一种从专利文件中提取head entity :: relationship :: tail entity形式事实的方法。这些事实可以在专利文件内部和跨文件之间组合形成知识图，用作表示和存储设计知识的方案。现有的工程设计文献中的方法通常利用一组预定义的关系来填充统计近似而非事实的三元组。在我们的方法中，我们训练一个标记器来识别句子中的实体和关系。在确定了一对实体后，我们训练另一个标记器来识别特定表示这对实体之间关系的关系标记。为了训练这些标记器，我们手动构建了一个包含44,227个句子和相应事实的数据集。我们还将该方法的性能与通常推荐的方法进行了比较，其中我们预.

    Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
    
[^41]: 重访DARPA通信者数据，使用会话分析

    Revisiting the DARPA Communicator Data using Conversation Analysis. (arXiv:2307.06982v1 [cs.CL])

    [http://arxiv.org/abs/2307.06982](http://arxiv.org/abs/2307.06982)

    本文通过研究DARPA通信者项目中受挫和恼怒的用户记录，应用会话分析的方法，以识别人与电脑会话中的改进机会和失败点。

    

    人与电脑会话方面的现有技术仍有待改进，与电脑交流往往非常令人讨厌。本文描述了一种通过寻找脏话滥用来识别这些系统中的“改进机会”的方法。这一假设是人们对电脑说脏话是一种惩罚，脏话代表了系统未能正常运行的失败点。在确定出错位置后，我们可以逆向查看记录，并通过会话分析找出出错原因。会话分析是一种质性方法，对于我们这些来自数量背景的人来说，可能会显得很陌生，甚至不科学。本文首先描述了现代会话分析的方法，然后将该方法应用于DARPA通信者项目中受挫和恼怒的用户的记录。结论是存在一些创新和贡献。

    The state of the art in human computer conversation leaves something to be desired and, indeed, talking to a computer can be down-right annoying. This paper describes an approach to identifying ``opportunities for improvement'' in these systems by looking for abuse in the form of swear words. The premise is that humans swear at computers as a sanction and, as such, swear words represent a point of failure where the system did not behave as it should. Having identified where things went wrong, we can work backward through the transcripts and, using conversation analysis (CA) work out how things went wrong. Conversation analysis is a qualitative methodology and can appear quite alien - indeed unscientific - to those of us from a quantitative background. The paper starts with a description of Conversation analysis in its modern form, and then goes on to apply the methodology to transcripts of frustrated and annoyed users in the DARPA Communicator project. The conclusion is that there is a
    
[^42]: 解决孟加拉语中的假新闻问题：揭示总结与扩充对预训练语言模型的影响

    Tackling Fake News in Bengali: Unraveling the Impact of Summarization vs. Augmentation on Pre-trained Language Models. (arXiv:2307.06979v1 [cs.CL])

    [http://arxiv.org/abs/2307.06979](http://arxiv.org/abs/2307.06979)

    本论文研究了孟加拉语中假新闻的检测问题。通过使用总结和扩充技术，结合预训练语言模型，提出了一种四重方法来分类孟加拉语的假新闻文章。研究表明，总结和扩充在孟加拉语假新闻检测中具有有效性。

    

    随着社交媒体和在线新闻来源的兴起，假新闻已成为全球性的重大问题。然而，在像孟加拉语这样的低资源语言中检测假新闻在研究中受到了有限的关注。本文提出了一种方法，利用总结和扩充技术以及五种预训练语言模型来分类孟加拉语的假新闻文章。我们的方法包括将英语新闻文章进行翻译，并使用扩充技术来解决假新闻文章的不足问题。我们的研究还着重于通过总结新闻来解决基于BERT模型的令牌长度限制。通过广泛的实验和严格的评估，我们展示了总结和扩充在孟加拉语假新闻检测中的有效性。我们使用三个独立的测试数据集来评估我们的模型。当将BanglaBERT基础模型与扩充技术相结合时，取得了令人印象深刻的准确性。

    With the rise of social media and online news sources, fake news has become a significant issue globally. However, the detection of fake news in low resource languages like Bengali has received limited attention in research. In this paper, we propose a methodology consisting of four distinct approaches to classify fake news articles in Bengali using summarization and augmentation techniques with five pre-trained language models. Our approach includes translating English news articles and using augmentation techniques to curb the deficit of fake news articles. Our research also focused on summarizing the news to tackle the token length limitation of BERT based models. Through extensive experimentation and rigorous evaluation, we show the effectiveness of summarization and augmentation in the case of Bengali fake news detection. We evaluated our models using three separate test datasets. The BanglaBERT Base model, when combined with augmentation techniques, achieved an impressive accurac
    
[^43]: 复制就是你所需的。

    Copy Is All You Need. (arXiv:2307.06962v1 [cs.CL])

    [http://arxiv.org/abs/2307.06962](http://arxiv.org/abs/2307.06962)

    本文将文本生成定义为从现有文本集合中逐步复制文本片段，并通过复制和粘贴操作来实现生成，相比传统的顺序选择单词生成的模型，在自动和人工评估中取得了更好的生成质量，并且推理效率与基于标记的自回归模型相当。

    

    主导的文本生成模型通过顺序选择来自固定词汇表的单词来组成输出。在本文中，我们将文本生成定义为从现有文本集合中逐步复制文本片段（例如单词或短语）。我们使用高效的向量搜索工具包计算有意义的文本片段的上下文表示并对其进行索引。然后，文本生成的任务被分解为一系列的复制和粘贴操作：在每个时间步骤，我们从文本集合中寻找合适的文本片段，而不是从独立的词汇表中选择。在标准语言建模基准测试（WikiText-103）上的实验表明，我们的方法在自动和人工评估中都实现了更好的生成质量。此外，由于减少了解码步骤，我们的方法的推理效率与基于标记的自回归模型相当。我们还展示了我们的方法通过简单地切换到不同领域，可以实现有效的领域自适应。

    The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary. In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection. We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits. The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary. Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations. Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps. We also show that our approach allows for effective domain adaptation by simply switching to d
    
[^44]: ACTI在EVALITA 2023中的综述：阴谋论辨识任务概述

    ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])

    [http://arxiv.org/abs/2307.06954](http://arxiv.org/abs/2307.06954)

    ACTI在EVALITA 2023中的阴谋论辨识任务共有15支团队参与，通过使用大型语言模型判断阴谋内容和分类，得出了关于利用这些模型抵制在在线平台传播错误信息的结论。

    

    阴谋论辨识任务是Evalita 2023首次提出的新共享任务。ACTI挑战仅基于Telegram上的阴谋频道评论，分为两个子任务：(i) 阴谋内容分类：辨识阴谋内容和(ii) 阴谋类别分类：针对特定阴谋理论分类。共有15支团队参与了该任务，总共提交了81个结果。我们说明了基于大型语言模型的最佳方法。最后，我们得出了关于利用这些模型来抵制在在线平台上传播错误信息的结论。

    Conspiracy Theory Identication task is a new shared task proposed for the first time at the Evalita 2023. The ACTI challenge, based exclusively on comments published on conspiratorial channels of telegram, is divided into two subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial content and (ii) Conspiratorial Category Classification about specific conspiracy theory classification. A total of fifteen teams participated in the task for a total of 81 submissions. We illustrate the best performing approaches were based on the utilization of large language models. We finally draw conclusions about the utilization of these models for counteracting the spreading of misinformation in online platforms.
    
[^45]: 以不同方式堆叠更多层：通过低秩更新进行高秩训练

    Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])

    [http://arxiv.org/abs/2307.05695](http://arxiv.org/abs/2307.05695)

    本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。

    

    尽管大规模网络拥有数百亿个参数的规模已经占主导地位并且效果显著，但对于过度参数化模型的训练必要性仍然缺乏清晰的理解，而替代方法不一定能够降低训练高性能模型的成本。本文探索了低秩训练技术作为训练大型神经网络的替代方法。我们引入了一种称为ReLoRA的新方法，它利用低秩更新来训练高秩网络。我们将ReLoRA应用于预训练的Transformer语言模型，参数量高达350M，并且证明了与常规神经网络训练相当的性能。此外，我们观察到ReLoRA的效率随着模型大小的增加而提高，这使得它成为高效训练千亿级参数网络的有希望的方法。我们的研究结果揭示了低秩训练技术的潜力及其对于缩放定律的影响。

    Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
    
[^46]: 在大型语言模型中释放认知协同：通过多人格自我协作实现任务解决代理

    Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])

    [http://arxiv.org/abs/2307.05300](http://arxiv.org/abs/2307.05300)

    本论文提出了单人表现提示（SPP）的概念，通过与多个角色进行多轮自我协作，将单个语言模型转化为认知协同者，从而增强其在复杂任务中的问题解决能力和整体性能。

    

    人类智慧依赖于认知协同的概念，即在不同认知过程之间进行协作和信息整合，以获得比个体认知过程更出色的结果。尽管大型语言模型（LLM）作为通用任务解决代理表现出了令人期待的性能，但它们在需要丰富领域知识和复杂推理的任务上仍然面临困难。在这项工作中，我们提出了单人表现提示（SPP）的概念，通过与多个角色进行多轮自我协作，将单个LLM转化为认知协同者。认知协同者指的是一个智能代理，与多个智慧合作，结合他们的个体优势和知识，从而增强复杂任务的问题解决能力和整体性能。通过根据任务输入动态识别和模拟不同的角色，SPP释放了LLM中认知协同的潜力。

    Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assi
    
[^47]: 多模态双重注意力变换器实现跨语音情感识别

    Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v1 [cs.CL])

    [http://arxiv.org/abs/2306.13804](http://arxiv.org/abs/2306.13804)

    提出一种多模态双重注意力变换器（MDAT）模型，利用预训练模型进行多模态特征提取，通过引入图形注意和共同关注机制来捕捉不同情感的跨模态依赖，并使用最少的目标语言数据实现改进的跨语言情感识别结果。

    

    尽管语音情感识别（SER）取得了近期的进展，但最先进的系统无法在跨语言环境中实现改进的性能。本文提出了一种多模态双重注意力变换器（MDAT）模型，以改进跨语言SER。我们的模型利用预训练模型进行多模态特征提取，并配备双重注意机制，包括图形注意和共同关注，以捕获不同模态之间的复杂依赖关系，并使用最少的目标语言数据实现改进的跨语言SER结果。此外，我们的模型还利用变换器编码器层进行高层特征表示，以提高情感分类准确性。MDAT在各个阶段执行特征表示的细化，并为分类层提供情感显着特征。这种新颖方法还确保了模态特定的情感信息的保存，同时增强了交叉模态。

    Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems are unable to achieve improved performance in cross-language settings. In this paper, we propose a Multimodal Dual Attention Transformer (MDAT) model to improve cross-language SER. Our model utilises pre-trained models for multimodal feature extraction and is equipped with a dual attention mechanism including graph attention and co-attention to capture complex dependencies across different modalities and achieve improved cross-language SER results using minimal target language data. In addition, our model also exploits a transformer encoder layer for high-level feature representation to improve emotion classification accuracy. In this way, MDAT performs refinement of feature representation at various stages and provides emotional salient features to the classification layer. This novel approach also ensures the preservation of modality-specific emotional information while enhancing cross-modality 
    
[^48]: CHiME-7 DASR 挑战赛：多设备远程会议转录在多样化环境中

    The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios. (arXiv:2306.13734v1 [eess.AS])

    [http://arxiv.org/abs/2306.13734](http://arxiv.org/abs/2306.13734)

    CHiME-7 DASR 挑战赛旨在进行在远场环境下多设备远程会议转录的鲁棒语音识别，并在三种不同场景中评估系统性能。参与者可以使用开源预训练模型和数据集。

    

    CHiME 挑战赛在鲁棒语音识别系统的开发和评估中发挥了重要作用。我们在第七届 CHiME 挑战赛中引入了 CHiME-7 远程自动语音识别 (DASR) 任务，该任务包括在远场环境下使用多个、可能是异构的录音设备进行联合 ASR 和人声分离。与之前的挑战不同的是，我们评估3个不同的场景下的系统性能：CHiME-6、DiPCo 和 Mixer 6。目标是让参与者设计一个能够在不知道先验信息的情况下横跨不同阵列几何和用例的单个系统。另一个与之前 CHiME 不同的是，参与者可以使用开源预训练模型和数据集。

    The CHiME challenges have played a significant role in the development and evaluation of robust speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separat
    
[^49]: 跨语言跨时代摘要：数据集、模型和评估

    Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation. (arXiv:2306.12916v1 [cs.CL])

    [http://arxiv.org/abs/2306.12916](http://arxiv.org/abs/2306.12916)

    本文全面研究了跨语言跨时代摘要任务，使用历史幻想文本和维基百科摘要构建了第一个CLCTS语料库，并研究了流行的变压器模型及其中间任务微调的有效性；同时还探讨了ChatGPT在CLCTS中作为摘要器和评估器的潜力。最终发现中间任务微调的端到端模型产生了中等到差的效果，而ChatGPT在没有微调的情况下提供中等到好的摘要质量表现。

    

    尽管摘要已经得到了广泛的研究，但跨语言跨时代摘要(CLCTS)是一个潜力巨大但鲜有研究的领域，它有可能提高跨文化的可访问性、信息共享和理解。本文全面研究了CLCTS任务，包括数据集创建、建模和评估。我们构建了第一个CLCTS语料库，利用历史幻想文本和英语、德语维基百科摘要，并研究了流行的变压器端到端模型以及带有不同中间任务微调任务的有效性。此外，我们探讨了ChatGPT在CLCTS中作为摘要器和评估器的潜力。总体而言，我们报告了人类、ChatGPT以及几个最近的自动评估指标的评估结果，发现我们的中间任务微调的端到端模型产生了从差到中等的摘要质量；ChatGPT作为摘要器(没有任何微调)，提供了中等到好的摘要质量表现。

    While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility, information sharing, and understanding. This paper comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We build the first CLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in English and German, and examine the effectiveness of popular transformer end-to-end models with different intermediate task finetuning tasks. Additionally, we explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report evaluations from humans, ChatGPT, and several recent automatic evaluation metrics where we find our intermediate task finetuned end-to-end models generate bad to moderate quality summaries; ChatGPT as a summarizer (without any finetuning) provides moderate to good qua
    
[^50]: 基于编辑距离的强化学习用于RNN-T解码

    Edit Distance based RL for RNNT decoding. (arXiv:2306.01789v1 [cs.SD])

    [http://arxiv.org/abs/2306.01789](http://arxiv.org/abs/2306.01789)

    本文提出了一种基于编辑距离的强化学习方法，用于最小化RNN-T训练和推理过程之间的差距，并在LibriSpeech的600M Conformer RNN-T模型上取得了最佳结果。

    

    基于其在各种基准测试中出色的WER和支持无缝流式传输和长篇转录的能力，RNN-T目前被认为是ASR的工业标准。 然而，它最大的缺点在于训练和推理目标之间存在显着差异。为了解决这个问题，本文提出了一种最小化训练和推理时间之间差距的强化学习方法。我们的编辑距离RL (EDRL)方法基于编辑距离计算奖励，并在每个操作级别训练网络。该方法在LibriSpeech的600M Conformer RNN-T模型上获得了SoTA WERs。

    RNN-T is currently considered the industry standard in ASR due to its exceptional WERs in various benchmark tests and its ability to support seamless streaming and longform transcription. However, its biggest drawback lies in the significant discrepancy between its training and inference objectives. During training, RNN-T maximizes all alignment probabilities by teacher forcing, while during inference, it uses beam search which may not necessarily find the maximum probable alignment. Additionally, RNN-T's inability to experience mistakes during teacher forcing training makes it more problematic when a mistake occurs in inference. To address this issue, this paper proposes a Reinforcement Learning method that minimizes the gap between training and inference time. Our Edit Distance based RL (EDRL) approach computes rewards based on the edit distance, and trains the network at every action level. The proposed approach yielded SoTA WERs on LibriSpeech for the 600M Conformer RNN-T model.
    
[^51]: UNITE: 一个用于文本到SQL评估的统一基准

    UNITE: A Unified Benchmark for Text-to-SQL Evaluation. (arXiv:2305.16265v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16265](http://arxiv.org/abs/2305.16265)

    提出了一个统一的基准UNITE用于文本到SQL评估，包含来自12个以上领域的自然语言问题、超过3.9K种模式的SQL查询和29K个数据库。研究表明，Codex在跨领域数据集上表现出色，特别设计的编码方法可以提高性能，可机读的数据库的质量对文本到SQL系统的性能至关重要。

    

    一个实用的文本到SQL系统应该可以很好地概括各种自然语言问题、未见过的数据库模式和新颖的SQL查询结构。为了全面评估文本到SQL系统，我们引入了一个统一的基准UNITE用于文本到SQL评估。该基准由公开可用的文本到SQL数据集组成，包含来自12个以上领域的自然语言问题、超过3.9K种模式的SQL查询和29K个数据库。与广泛使用的Spider基准相比，我们增加了约120K个额外的示例和三倍的SQL模式，例如比较和布尔问题。我们在新基准上对六种最先进的文本到SQL解析器进行了系统研究，并展示了：1）Codex在跨领域数据集上表现出色；2）特别设计的编码方法（例如约束束搜索）可以提高在领域内外的性能；3）可机读的数据库的质量对文本到SQL系统的性能至关重要。

    A practical text-to-SQL system should generalize well on a wide variety of natural language questions, unseen database schemas, and novel SQL query structures. To comprehensively evaluate text-to-SQL systems, we introduce a \textbf{UNI}fied benchmark for \textbf{T}ext-to-SQL \textbf{E}valuation (UNITE). It is composed of publicly available text-to-SQL datasets, containing natural language questions from more than 12 domains, SQL queries from more than 3.9K patterns, and 29K databases. Compared to the widely used Spider benchmark \cite{yu-etal-2018-spider}, we introduce $\sim$120K additional examples and a threefold increase in SQL patterns, such as comparative and boolean questions. We conduct a systematic study of six state-of-the-art (SOTA) text-to-SQL parsers on our new benchmark and show that: 1) Codex performs surprisingly well on out-of-domain datasets; 2) specially designed decoding methods (e.g. constrained beam search) can improve performance for both in-domain and out-of-doma
    
[^52]: 我寻觅一个隐喻：大语言模型和扩散模型共创视觉隐喻

    I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v1 [cs.CL])

    [http://arxiv.org/abs/2305.14724](http://arxiv.org/abs/2305.14724)

    本论文提出一个新的任务——从语言隐喻生成视觉隐喻，并且基于大语言模型和扩散模型之间的协作，成功地实现了共创出具有视觉冲击力和语义含义的隐喻。

    

    视觉隐喻是通过图像来说服或传达创意想法的强大修辞手法。与语言隐喻类似，它们通过符号主义和符号的并置隐含地传达含义。我们提出了一个从语言隐喻生成视觉隐喻的新任务。这对于基于扩散的文本到图像模型（如DALL $\cdot$ E 2）来说是一项具有挑战性的任务，因为它需要模拟隐含含义和组合性。我们提出了通过大型语言模型（LLMs）和扩散模型之间的协作来解决这个问题：采用以“串联思维”为提示的Instruct GPT-3（davinci-002）生成代表语言隐喻的视觉阐述的文本，其中包含隐含含义和相关对象，然后将其用作扩散的文本到图像模型的输入。通过人机协作框架，人们与LLM和表现最佳的扩散模型进行交互，创建一个高质量的隐喻和它们的视觉对应的数据集。我们的实验表明，LLMs和扩散模型之间的协作可以共同创造出具有视觉冲击力和语义含义的隐喻。

    Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models.Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-qu
    
[^53]: LLM模型共同提取RCT报告中干预、结果和发现信息

    Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v1 [cs.CL])

    [http://arxiv.org/abs/2305.03642](http://arxiv.org/abs/2305.03642)

    本文提出了一种基于LLM调整的文本到文本模型，共同提取RCT报告中的干预、结果和发现信息，实现相当大的性能提升。

    

    随机对照试验（RCT）的结果确定干预措施的相对有效性，进而成为基于证据的医疗保健的关键输入。然而，RCT结果以（通常是非结构化的）自然语言文章的形式呈现，描述试验的设计、执行和结果；临床医生必须从这些文章中手动提取有关所关注的干预措施和结果的发现。这种繁琐的手动过程促使人们利用（半）自动化的方式从试验报告中提取结构化证据。在这项工作中，我们提出并评估了一个基于调整的大型语言模型（LLMs）的文本到文本模型，用于从临床摘要中共同提取干预措施、结果和比较因素（ICO元素），并推断相关的结果。人工（专家）和自动评估表明，将证据提取框架作为条件生成任务，为此目的微调LLMs可以实现相当大的（约20个点）性能提升。

    Results from Randomized Controlled Trials (RCTs) establish the comparative effectiveness of interventions, and are in turn critical inputs for evidence-based care. However, results from RCTs are presented in (often unstructured) natural language articles describing the design, execution, and outcomes of trials; clinicians must manually extract findings pertaining to interventions and outcomes of interest from such articles. This onerous manual process has motivated work on (semi-)automating extraction of structured evidence from trial reports. In this work we propose and evaluate a text-to-text model built on instruction-tuned Large Language Models (LLMs) to jointly extract Interventions, Outcomes, and Comparators (ICO elements) from clinical abstracts, and infer the associated results reported. Manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning LLMs for this purpose realizes considerable ($\sim$20 point 
    
[^54]: 基于预训练语言模型的生物医学文本摘要综述

    A Survey on Biomedical Text Summarization with Pre-trained Language Model. (arXiv:2304.08763v1 [cs.CL])

    [http://arxiv.org/abs/2304.08763](http://arxiv.org/abs/2304.08763)

    本文总结了基于预训练语言模型的生物医学文本摘要方法，提炼关键信息生成简洁的摘要，分为微调、基于特征和无监督方法，未来研究方向包括融合领域特定知识和开发更适合的评估指标。

    

    生物医学文献和电子病历等生物医学文本的指数级增长，给临床医生和研究人员高效获取临床信息带来巨大挑战。为解决这一问题，提出了生物医学文本摘要方法，旨在从单个或多个生物医学文档中提炼关键信息生成简洁的摘要。近年来，预训练语言模型（PLMs）已成为多种自然语言处理任务的事实标准，PLMs在生物医学领域中的应用也为生物医学文本摘要任务带来新的启示。本文系统地总结了近期基于PLMs探索生物医学文本摘要的进展，帮助理解最新的进展、挑战和未来方向。我们根据使用PLMs的方式对基于PLMs的方法进行分类，包括微调、基于特征和无监督方法。我们还讨论了潜在的未来研究方向，如融合领域特定知识和开发更适合的评估指标。

    The exponential growth of biomedical texts such as biomedical literature and electronic health records (EHRs), provides a big challenge for clinicians and researchers to access clinical information efficiently. To address the problem, biomedical text summarization has been proposed to support clinical information retrieval and management, aiming at generating concise summaries that distill key information from single or multiple biomedical documents. In recent years, pre-trained language models (PLMs) have been the de facto standard of various natural language processing tasks in the general domain. Most recently, PLMs have been further investigated in the biomedical field and brought new insights into the biomedical text summarization task. In this paper, we systematically summarize recent advances that explore PLMs for biomedical text summarization, to help understand recent progress, challenges, and future directions. We categorize PLMs-based approaches according to how they utilize
    
[^55]: 打破常识：WHOOPS！一个基于合成和组合图像的视觉与语言基准测试

    Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.07274](http://arxiv.org/abs/2303.07274)

    WHOOPS!是一个新的视觉常识数据集和基准测试，包括了图像字幕、跨模态匹配和视觉问答等若干个任务，引入了解释生成任务，挑战了AI模型识别和解释不合常规的图像的能力。

    

    奇怪、异常和神秘的图像会引起观察者的好奇心，因为它们挑战了常识。我们提出WHOOPS！一个新的视觉常识数据集和基准测试。该数据集由设计师使用Midjourney等公共可用图像生成工具制作，并包含若干个任务。除了图像字幕、跨模态匹配和视觉问答之外，我们还引入了一个困难的解释生成任务，其中模型必须识别并解释给定图像的异常之处。

    Weird, unusual, and uncanny images pique the curiosity of observers because they challenge commonsense. For example, an image released during the 2022 world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo playing chess, which playfully violates our expectation that their competition should occur on the football field. Humans can easily recognize and interpret these unconventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explanation generation task, where models must identify and explain why a given image is unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
    
[^56]: ICL-D3IE：上下文学习+多样展示更新，用于文档信息抽取（arXiv:2303.05063v2 [cs.CL] UPDATED）

    ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction. (arXiv:2303.05063v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.05063](http://arxiv.org/abs/2303.05063)

    这篇论文提出了一个简单而有效的上下文学习框架ICL-D3IE，这个框架使LLM在不同类型演示下的DIE任务中表现出色，具有改进性能的反馈机制，同时涵盖了位置和格式方面的演示示例。

    

    大型语言模型（LLM）如GPT-3和ChatGPT在各种自然语言处理（NLP）任务中展示了卓越的成果，尤其是应用于上下文学习，即基于少量演示示例进行推理。尽管在NLP任务中取得了成功，但尚未进行研究以评估LLM在使用上下文学习执行文档信息抽取（DIE）的能力。应用LLM执行DIE存在两个挑战：模态和任务差距。为此，我们提出了一种简单而有效的上下文学习框架ICL-D3IE，它使LLM能够使用不同类型的演示示例执行DIE。具体而言，我们从难以训练的文档中提取最困难和最不同的片段作为演示示例，以便受益于所有测试实例。我们设计了描述关系的演示示例，使LLM能够理解位置关系。我们引入了格式化演示示例，以方便提取答案。此外，我们采用了反馈机制，更新了演示示例，以进一步提高ICL-D3IE的性能。

    Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated remarkable results in various natural language processing (NLP) tasks with in-context learning, which involves inference based on a few demonstration examples. Despite their successes in NLP tasks, no investigation has been conducted to assess the ability of LLMs to perform document information extraction (DIE) using in-context learning. Applying LLMs to DIE poses two challenges: the modality and task gap. To this end, we propose a simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples. Specifically, we extract the most difficult and distinct segments from hard training documents as hard demonstrations for benefiting all test instances. We design demonstrations describing relationships that enable LLMs to understand positional relationships. We introduce formatting demonstrations for easy answer extraction. Additionally
    
[^57]: 数据中心机器学习的重新标签法

    The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04391](http://arxiv.org/abs/2302.04391)

    本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。

    

    在深度学习应用中，手动标记的数据在一定程度上存在噪声。为了解决这个问题，并在开发数据集上获得90分以上的成绩，本文提出了一种简单的方法来找出噪声数据，并通过采用模型预测作为人类标记的参考来重新标记噪声数据。本文阐述了我们在广泛的深度学习任务中的想法，包括分类、序列标记、物体检测、序列生成、点击率预测。实验结果和人类评估结果验证了我们的想法。

    In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
    
[^58]: 在多标签文本分类中有效地使用对比学习

    An Effective Employment of Contrastive Learning in Multi-label Text Classification. (arXiv:2212.00552v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.00552](http://arxiv.org/abs/2212.00552)

    本论文提出了五个新的对比损失函数，用于多标签文本分类任务。通过对比学习技术的应用，探索了其在多标签文本分类任务中的有效性，并提供了一套基准模型。

    

    对比学习技术在自然语言处理任务中的有效性尚待探索和分析。如何正确合理地构建正负样本是对比学习的核心挑战，而在多标签文本分类任务中，发现对比对象更加困难。之前提出的对比损失函数很少。在本文中，我们从不同的角度探讨这个问题，提出了五个新颖的对比损失函数，用于多标签文本分类任务，包括严格对比损失 (SCL)、内标签对比损失 (ICL)、Jaccard相似度对比损失(JSCL)、Jaccard相似度概率对比损失(JSPCL)和逐步标签对比损失(SLCL)。我们通过使用这些新颖的损失函数，探索了对比学习在多标签文本分类任务中的有效性，并为在特定任务上部署对比学习技术提供了一套基准模型。

    The effectiveness of contrastive learning technology in natural language processing tasks is yet to be explored and analyzed. How to construct positive and negative samples correctly and reasonably is the core challenge of contrastive learning. It is even harder to discover contrastive objects in multi-label text classification tasks. There are very few contrastive losses proposed previously. In this paper, we investigate the problem from a different angle by proposing five novel contrastive losses for multi-label text classification tasks. These are Strict Contrastive Loss (SCL), Intra-label Contrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL), Jaccard Similarity Probability Contrastive Loss (JSPCL), and Stepwise Label Contrastive Loss (SLCL). We explore the effectiveness of contrastive learning for multi-label text classification tasks by the employment of these novel losses and provide a set of baseline models for deploying contrastive learning techniques on specific t
    
[^59]: 关于预期对阅读时间的影响

    On the Effect of Anticipation on Reading Times. (arXiv:2211.14301v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.14301](http://arxiv.org/abs/2211.14301)

    该论文研究了预期对阅读时间的影响。通过比较惊奇度和上下文熵对阅读时间的预测效果，发现上下文熵对词的阅读时间影响有实质性的证据。

    

    在过去的二十年中，许多研究已经证明了较难预测（即更高的惊奇度）的单词需要更长的阅读时间。一般来说，这些研究暗示了阅读过程是完全响应式的：读者观察一个新单词，并根据需要分配时间来处理它。我们认为，先前的结果也与阅读过程至少部分地具有预期性是一致的：读者可以对未来的单词进行预测，并根据他们的期望分配时间来处理它。在这项工作中，我们将这种预期性操作化为一个单词的上下文熵。我们通过比较惊奇程度和上下文熵对四个自然阅读数据集（两个自定节奏和两个眼动追踪）中的阅读时间进行预测的效果来评估预期对阅读的影响。在实验中，跨数据集和分析，我们发现上下文熵对于词的阅读时间（RT）的影响具有实质性的证据：事实上，有时上下文熵比惊奇度更好地预测阅读时间。

    Over the past two decades, numerous studies have demonstrated how less predictable (i.e., higher surprisal) words take more time to read. In general, these studies have implicitly assumed the reading process is purely responsive: Readers observe a new word and allocate time to process it as required. We argue that prior results are also compatible with a reading process that is at least partially anticipatory: Readers could make predictions about a future word and allocate time to process it based on their expectation. In this work, we operationalize this anticipation as a word's contextual entropy. We assess the effect of anticipation on reading by comparing how well surprisal and contextual entropy predict reading times on four naturalistic reading datasets: two self-paced and two eye-tracking. Experimentally, across datasets and analyses, we find substantial evidence for effects of contextual entropy over surprisal on a word's reading time (RT): in fact, entropy is sometimes better 
    
[^60]: 跨语言对话的再现

    Dialogs Re-enacted Across Languages. (arXiv:2211.11584v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11584](http://arxiv.org/abs/2211.11584)

    该论文介绍了一种用于收集跨语言话语对的协议，并公开了相关数据集，以支持机器学习跨语言韵律映射等领域的研究。该研究对于使用该语料库、扩展该语料库以及设计类似双语对话数据集的人士具有重要意义。

    

    为了支持机器学习跨语言的韵律映射和其他提高语音-语音翻译的方法，我们提出了一种在不同语言之间收集紧密匹配的话语对的协议，描述了所得到的数据集以及其公开发布的情况，并附上了一些观察和思考。本报告面向：使用该语料库的人士、扩展该语料库的人士以及设计类似的双语对话数据集的人士。

    To support machine learning of cross-language prosodic mappings and other ways to improve speech-to-speech translation, we present a protocol for collecting closely matched pairs of utterances across languages, a description of the resulting data collection and its public release, and some observations and musings. This report is intended for: people using this corpus, people extending this corpus, and people designing similar collections of bilingual dialog data.
    
[^61]: 基于视觉Transformer的模型用于将一组图像描述为一个故事

    Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.02762](http://arxiv.org/abs/2210.02762)

    本研究提出了一种基于视觉Transformer的模型，用于将一组图像描述为一个故事。该模型通过ViT提取输入图像的特征，并使用双向LSTM捕捉图像补丁的过去和未来上下文信息。通过注意力机制加权计算得到最终的故事描述向量。

    

    视觉故事讲述是将一组图像形成多句故事的过程。恰当地包含输入图像内捕捉到的视觉变化和上下文信息是视觉故事讲述中最具挑战性的方面之一。因此，由一组图像开发的故事经常缺乏凝聚力、相关性和语义关系。在本文中，我们提出了一种新颖的基于视觉Transformer的模型，用于将一组图像描述为一个故事。所提出的方法使用视觉Transformer（ViT）提取输入图像的独特特征。首先，将输入图像分成16X16的补丁，并捆绑到扁平化补丁的线性投影中。从单个图像到多个图像补丁的转换捕捉到了输入视觉模式的视觉多样性。这些特征作为输入传递给双向LSTM，它是序列编码器的一部分。这样可以捕捉到所有图像补丁的过去和未来图像上下文。然后，通过注意力机制来加权计算图像补丁的特征向量，以产生最终的故事描述向量。

    Visual Story-Telling is the process of forming a multi-sentence story from a set of images. Appropriately including visual variation and contextual information captured inside the input images is one of the most challenging aspects of visual storytelling. Consequently, stories developed from a set of images often lack cohesiveness, relevance, and semantic relationship. In this paper, we propose a novel Vision Transformer Based Model for describing a set of images as a story. The proposed method extracts the distinct features of the input images using a Vision Transformer (ViT). Firstly, input images are divided into 16X16 patches and bundled into a linear projection of flattened patches. The transformation from a single image to multiple image patches captures the visual variety of the input visual patterns. These features are used as input to a Bidirectional-LSTM which is part of the sequence encoder. This captures the past and future image context of all image patches. Then, an atten
    
[^62]: CodeQueries：一个关于代码语义查询的数据集

    CodeQueries: A Dataset of Semantic Queries over Code. (arXiv:2209.08372v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2209.08372](http://arxiv.org/abs/2209.08372)

    CodeQueries是一个关于Python代码语义查询的数据集，与现有数据集相比，它具有文件级别的上下文和代码段答案。

    

    开发人员常常对他们正在操作的代码的语义方面有疑问，例如，“是否有一个类的父类声明了一个冲突的属性？”回答这些问题需要理解代码的语义，比如类的属性和继承关系。对于这样的问题，答案应该标识出构成答案的代码段（例如，子类的声明）以及支持的事实（例如，冲突属性的定义）。现有的关于代码问题回答的研究主要关注是或否的问题或方法级别的上下文。我们贡献了一个命名为CodeQueries的Python代码的语义查询的标记数据集。与现有数据集相比，CodeQueries的查询是关于代码语义的，上下文是文件级别的，答案是代码段。我们根据一个广泛使用的静态分析工具CodeQL支持的查询来筛选和整理数据集，并包含正负例和需要单跳问题查询。

    Developers often have questions about semantic aspects of code they are working on, e.g., "Is there a class whose parent classes declare a conflicting attribute?". Answering them requires understanding code semantics such as attributes and inheritance relation of classes. An answer to such a question should identify code spans constituting the answer (e.g., the declaration of the subclass) as well as supporting facts (e.g., the definitions of the conflicting attributes). The existing work on question-answering over code has considered yes/no questions or method-level context. We contribute a labeled dataset, called CodeQueries, of semantic queries over Python code. Compared to the existing datasets, in CodeQueries, the queries are about code semantics, the context is file level and the answers are code spans. We curate the dataset based on queries supported by a widely-used static analysis tool, CodeQL, and include both positive and negative examples, and queries requiring single-hop a
    

