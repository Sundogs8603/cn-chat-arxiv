# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?](https://arxiv.org/abs/2401.18070) | 该研究调查了语言模型在解决算术问题时与人类学习者的认知偏见。研究发现，当前最先进的语言模型在文本理解和解决方案规划阶段表现出与人类类似的偏见。 |
| [^2] | [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059) | 本论文提出了一种递归嵌入、聚类和摘要的方法，通过构建不同摘要级别的树，从下往上整合并检索长度较长的文档，对传统检索增强的语言模型进行改进，实现了在复杂的多步推理问答任务上的最先进的结果提升。 |
| [^3] | [LongAlign: A Recipe for Long Context Alignment of Large Language Models](https://arxiv.org/abs/2401.18058) | LongAlign是一种用于大型语言模型的长上下文对齐的方法，通过指导微调和使用打包、排序和损失加权策略，它在长上下文任务中表现优异，相比现有的方法提高了多达30\%的性能。 |
| [^4] | [Multipath parsing in the brain](https://arxiv.org/abs/2401.18046) | 本研究通过将增量生成依存关系解析器的预测与人们进行功能神经成像的时间数据相关，发现了人类在逐词理解句子时存在多路径解析的证据。 |
| [^5] | [SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition](https://arxiv.org/abs/2401.18045) | SpeechComposer是一种能够通过合成固定提示标记来统一多个语音任务的语音语言模型。它提高了任务之间的连接性，并且可以轻松扩展到更多的任务，同时实现了任务之间的知识共享。 |
| [^6] | [Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability](https://arxiv.org/abs/2401.18040) | 本研究旨在通过内在动机强化学习算法改进端到端多任务对话系统的训练和适应性。通过教授智能体一个内在奖励系统，可以加速训练并提高其判断行为质量的能力。 |
| [^7] | [Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models](https://arxiv.org/abs/2401.18034) | Paramanu是一种高效的印度生成式基础语言模型系列，包含多种印度语言模型，并且在单个GPU上进行了从头预训练。它还包括一个先进的印度分词器以及避免多语言诅咒的预训练方法。这些模型在人工评估中展现出良好的语法、连贯性、创造性和事实准确性。 |
| [^8] | [Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI](https://arxiv.org/abs/2401.18028) | 本研究探讨了使用LLMs来增强预测人工智能负面影响的方法，并通过与新闻媒体对齐，建立了一个包含十个类别的人工智能影响分类法。结果表明，基于指令和迁移学习的LLMs模型在生成影响方面具有一定的能力。 |
| [^9] | [Prompt-Driven LLM Safeguarding via Directed Representation Optimization](https://arxiv.org/abs/2401.18018) | 通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。 |
| [^10] | [Desiderata for the Context Use of Question Answering Systems](https://arxiv.org/abs/2401.18001) | 本论文概述了在现有的上下文问题回答系统中存在的问题，并提出了一系列期望，旨在解决模型对上下文的关注不足、对噪声的鲁棒性较低以及回答的不一致性。通过对15个QA系统在5个数据集上的综合评估，发现了一些新趋势。 |
| [^11] | [Entity Linking in the Job Market Domain](https://arxiv.org/abs/2401.17979) | 本研究探索了职场领域中的实体链接，通过消歧技能提及并与ESCO分类体系进行关联，对当前劳动力市场的需求进行深入了解。通过调整神经EL模型，我们发现BLINK在严格评估中胜过GENRE，但GENRE在宽松评估中表现更好。 |
| [^12] | [GUMsley: Evaluating Entity Salience in Summarization for 12 English Genres](https://arxiv.org/abs/2401.17974) | GUMsley是第一个覆盖12种英文文本流派中所有命名和非命名显著实体的实体显著性数据集，并证明了当前最先进的摘要模型在捕捉生成摘要中的显著实体方面表现不佳。 |
| [^13] | [[Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference Annotation with LLMs](https://arxiv.org/abs/2401.17922) | 本研究提出了一种使用LLMs进行文学指代标注的方法，通过学习生成带有指代注释的句子，解决了构建高质量小说系统的困难。我们创建并发布了多个经过训练的指代模型，并提供了一个用于训练新模型的工作流程。 |
| [^14] | [LOCOST: State-Space Models for Long Document Abstractive Summarization](https://arxiv.org/abs/2401.17919) | LOCOST是一种基于状态空间模型的编码器-解码器架构，用于处理长文档的抽象摘要生成。与基于稀疏注意模式的最先进模型相比，LOCOST具有更低的计算复杂度，并且能够在训练和推断期间节省大量内存。在评估中，LOCOST在长文档摘要化任务上达到了93-96%的性能水平，并且能够处理超过600K个标记的输入文本。 |
| [^15] | [SNNLP: Energy-Efficient Natural Language Processing Using Spiking Neural Networks](https://arxiv.org/abs/2401.17911) | 本论文研究了在自然语言处理领域中使用脉冲神经网络的高能效解决方案，并比较了不同的文本编码方法在相关任务中的性能。 |
| [^16] | [Revisiting speech segmentation and lexicon learning with better features](https://arxiv.org/abs/2401.17902) | 本文重新审视了一种自监督方法，利用改进的特征将未标记的语音分割成单词状的片段，并通过聚类得到一个具有良好覆盖率和最先进性能的词典。 |
| [^17] | [Employing Label Models on ChatGPT Answers Improves Legal Text Entailment Performance](https://arxiv.org/abs/2401.17897) | 本论文介绍了利用标签模型在ChatGPT回答中提高法律文本推理性能的方法，通过将ChatGPT的临时答案整合为综合标签，实现了准确率的提高。 |
| [^18] | [I Think, Therefore I am: Awareness in Large Language Models](https://arxiv.org/abs/2401.17882) | 本文介绍了将意识概念引入大型语言模型（LLMs），并定义了LLMs在感知和理解自身以及展示社交智能方面的能力。通过引入AwareLLM数据集，研究发现LLMs在意识方面表现出相当程度的能力，尽管它们缺乏实质性的能力意识。 |
| [^19] | [Probing Language Models' Gesture Understanding for Enhanced Human-AI Interaction](https://arxiv.org/abs/2401.17858) | 该项目旨在探究语言模型对非语言交流中的手势理解，通过测试LLMs对明示和隐含非语言暗示的能力以及与语境因素的关联。实验将使用心理语言学设计和全面的数据集进行，同时考虑文化维度，并测量LLM和人类之间的一致性。 |
| [^20] | [Global-Liar: Factuality of LLMs over Time and Geographic Regions](https://arxiv.org/abs/2401.17839) | 本论文评估了GPT模型的事实准确性、稳定性和偏见，并引入了一个平衡数据集"全球说谎者"，结果显示较新的GPT模型并不总是意味着性能的提升，并且观察到一个全球南方陈述被偏袒的问题。 |
| [^21] | [Neural Machine Translation for Malayalam Paraphrase Generation](https://arxiv.org/abs/2401.17827) | 本研究针对马拉亚拉姆语探索了四种生成改写的方法，并使用了英语改写和预训练的神经机器翻译模型的资源。研究发现，自动化评估指标不完全适用于马拉亚拉姆语，强调了对于高度合词性语言更细致的改写评估方法的需求。 |
| [^22] | [A Survey of Pre-trained Language Models for Processing Scientific Text](https://arxiv.org/abs/2401.17824) | 本研究对处理科学文本的预训练语言模型进行了全面调查，包括对它们在不同领域、任务和数据集中有效性的分析，以及对未来挑战的讨论。 |
| [^23] | [SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering](https://arxiv.org/abs/2401.17809) | 提出了一种主题词嵌入修改框架（SWEA），通过在推理阶段修改主题的表示来编辑知识，保护模型的原始权重，避免不可逆的损害和额外的推理开销。 |
| [^24] | [CauESC: A Causal Aware Model for Emotional Support Conversation](https://arxiv.org/abs/2401.17755) | CauESC是一种关注因果关系的情感支持对话模型，通过识别困扰的情感原因以及这些原因触发的情感效果，独立地理解每种语言修饰策略并巧妙地整合它们，有效地减轻求助者的情感困扰。 |
| [^25] | [Enhancing Large Language Model with Decomposed Reasoning for Emotion Cause Pair Extraction](https://arxiv.org/abs/2401.17716) | 利用大型语言模型和分解推理，我们提出了一种名为DECC的框架，用于从文档中提取情感和原因的从句对。实验结果表明，DECC在ECPE任务上表现出比最先进的有监督微调方法更强大的能力。 |
| [^26] | [WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts](https://arxiv.org/abs/2401.17703) | WSC+提出了一种新的提示方法-专家树，增强了Winograd Schema挑战中问题的生成能力。通过引入新的数据集和扩展框架，WSC+揭示了模型的过度自信和偏见，并发现大型语言模型在评估自己生成的问题时并不总是优于其他模型。在WSC+上，当前最好的大型语言模型GPT-4的准确率为68.7%。 |
| [^27] | [Mitigating the Problem of Strong Priors in LMs with Context Extrapolation](https://arxiv.org/abs/2401.17692) | 本论文提出了一种缓解语言模型中强先验问题的新技术，通过削弱原始提示并进行上下文外推，以减少模型受到强先验问题的影响。 |
| [^28] | [Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning](https://arxiv.org/abs/2401.17686) | 本文提出了一种称为推理束搜索（DBS）的方法，将链式思维和演绎推理与逐步束搜索无缝集成到大型语言模型（LLMs）中，通过引入验证器来减少错误的累积，并通过可扩展和无需人工劳动的数据构建方法提升模型的验证能力。 |
| [^29] | [Contextual Feature Extraction Hierarchies Converge in Large Language Models and the Brain](https://arxiv.org/abs/2401.17671) | 在研究中发现，随着大型语言模型在基准任务上的性能提高，模型不仅在预测神经响应时表现出更高的类脑性能，而且其分层特征提取路径与大脑的映射更接近，并且使用更少的层次来进行相同的编码。 |
| [^30] | [Document Structure in Long Document Transformers](https://arxiv.org/abs/2401.17658) | 长文档Transformer模型在预训练期间获得了对文档结构的隐性理解，并且结构注入可以进一步增强这种理解，提高下游任务的性能。 |
| [^31] | [Navigating the OverKill in Large Language Models](https://arxiv.org/abs/2401.17633) | 本研究调查了大型语言模型中过度杀伤的因素，并发现了其中存在的捷径和对有害词语的过度关注。我们提出了自对比解码（Self-CD）策略来缓解过度杀伤现象，该策略无需训练且适用于各种模型。 |
| [^32] | [What Do Self-Supervised Speech and Speaker Models Learn? New Findings From a Cross Model Layer-Wise Analysis](https://arxiv.org/abs/2401.17632) | 本研究通过应用SUPERB评估探测任务，研究了自监督学习模型中捕捉语音特性的能力，并比较了语音和说话者模型之间的差异。 |
| [^33] | [Neighboring Perturbations of Knowledge Editing on Large Language Models](https://arxiv.org/abs/2401.17623) | 本文研究大型语言模型上知识编辑对邻近知识的扰动，提出了 additivity 指标以及 Perturbation Evaluation of Appending Knowledge (PEAK) 基准，用于评估附加新知识时邻近知识的扰动程度。 |
| [^34] | [Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning](https://arxiv.org/abs/2401.17602) | 在临床自然语言处理中，本研究提出了一种利用LoRA微调的大型语言模型来解决断言检测任务，该任务是从临床笔记中提取医学概念时的关键步骤，准确的断言类型识别对于医疗专业人员理解患者状况至关重要。 |
| [^35] | [Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data](https://arxiv.org/abs/2401.17600) | 在这项研究中，我们对GPT-4V模型在地球观测数据上的性能进行了评估。结果显示，尽管GPT-4V在开放式任务如位置理解和图像标注方面表现良好，但其空间推理能力不足，限制了其在目标定位和计数方面的实用性。 |
| [^36] | [SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization](https://arxiv.org/abs/2401.17597) | 本文提出了一种用于长对话摘要的增强说话者先训练方法，通过利用多轮对话的内在结构，帮助语言模型更好地理解和处理长对话摘要任务，实现了最先进的性能。 |
| [^37] | [Local and Global Contexts for Conversation](https://arxiv.org/abs/2401.17588) | 本文引入了一个本地和全局对话模型（LGCM），它是一个本地-全局层次转换器模型，能够准确区分和融合生成回应所需的相关上下文。通过无缝融合本地和全局上下文，模型能够更好地进行对话生成。 |
| [^38] | [Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks](https://arxiv.org/abs/2401.17585) | 该论文针对现有的知识编辑方法在推理能力方面的限制，通过引入ReCoE数据集进行了深入分析。研究发现所有的模型编辑方法在该数据集上表现较差，尤其在特定的推理方案中。此外，通过对编辑模型思维链生成的分析，揭示了现有方法的不足之处，包括对事实编辑、事实回忆能力和连贯性的考量。 |
| [^39] | [Scavenging Hyena: Distilling Transformers into Long Convolution Models](https://arxiv.org/abs/2401.17574) | 本文介绍了一种通过使用知识蒸馏将Transformer模型中的注意力头替换为Hyena，从而提高效率并处理长上下文信息的方法，超越了传统预训练方法，在准确性和效率方面取得了优秀的结果。这一技术为追求可持续的AI解决方案做出了贡献，实现了计算能力和环境影响的平衡。 |
| [^40] | [PipeNet: Question Answering with Semantic Pruning over Knowledge Graphs](https://arxiv.org/abs/2401.17536) | 本论文提出了PipeNet方法，通过语义修剪技术在知识图谱上进行问题回答。该方法通过关联-修剪-推理的流程来修剪噪声节点，以提高图推理的效率，同时获得良好的子图表示。 |
| [^41] | [FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation](https://arxiv.org/abs/2401.17514) | FEUDA是一种令人沮丧地简单的无监督领域自适应方法，通过在未标记和标记的示例上训练自回归语言模型，在提示基础的分类框架中探索无监督领域自适应的新范例。 |
| [^42] | [Linguistically Communicating Uncertainty in Patient-Facing Risk Prediction Models](https://arxiv.org/abs/2401.17511) | 本文讨论了在医疗保健领域中面向患者的风险预测模型中不确定性量化的挑战，并提出了一种设计来应对这些挑战，重点关注体外受精结果预测的具体应用。 |
| [^43] | [Arrows of Time for Large Language Models](https://arxiv.org/abs/2401.17505) | 这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。 |
| [^44] | [Improving QA Model Performance with Cartographic Inoculation](https://arxiv.org/abs/2401.17498) | 本文提出了一种名为地图接种的新方法，通过在优化的挑战数据子集上对QA模型进行微调，减少模型对数据集伪迹的依赖性，从而显著提高模型在复杂和开放的上下文推理问题上的性能。 |
| [^45] | [Detecting mental disorder on social media: a ChatGPT-augmented explainable approach](https://arxiv.org/abs/2401.17477) | 本文提出了一种利用大型语言模型，可解释人工智能和对话代理器ChatGPT相结合的新方法，以解决通过社交媒体检测抑郁症的可解释性挑战。通过将Twitter特定变体BERTweet与自解释模型BERT-XDD相结合，并借助ChatGPT将技术解释转化为人类可读的评论，实现了解释能力的同时提高了可解释性。这种方法可以为发展社会负责任的数字平台，促进早期干预做出贡献。 |
| [^46] | [Efficient Tool Use with Chain-of-Abstraction Reasoning](https://arxiv.org/abs/2401.17464) | 该方法通过让LLM首先解码抽象推理链，然后调用领域工具填充具体知识，使得LLM能够更好地利用工具进行多步推理，并且具有通用性和鲁棒性。 |
| [^47] | [Synthetic Dialogue Dataset Generation using LLM Agents](https://arxiv.org/abs/2401.17461) | 本文提出了使用LLM智能体生成合成对话数据集的方法，通过对话智能体和用户进行交流来获取生成线性模型所需的关键信息，并提出了对话的外部评估方法。 |
| [^48] | [Can Large Language Models Replace Economic Choice Prediction Labs?](https://arxiv.org/abs/2401.17435) | 该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。 |
| [^49] | [Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks](https://arxiv.org/abs/2401.17396) | 在本研究中，我们提供了一个基于Transformer的模型和一个土耳其语基准测试，成功地对名为BERTurk的土耳其BERT模型进行了微调，实现了许多下游任务的理解和评估。 |
| [^50] | [Customizing Language Model Responses with Contrastive In-Context Learning](https://arxiv.org/abs/2401.17390) | 本论文提出了一种使用对比示例来定制语言模型回复的方法，通过提供正面示例和负面示例，使模型学会如何回避负面特征，从而更好地满足用户需求。 |
| [^51] | [Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens](https://arxiv.org/abs/2401.17377) | 这项研究展示了n-gram语言模型的价值，并介绍了一个名为infini-gram的引擎，它可以以毫秒级的延迟计算任意n的n-gram概率，使得在神经大型语言模型中对文本进行更准确的分析成为可能。 |
| [^52] | [Arabic Tweet Act: A Weighted Ensemble Pre-Trained Transformer Model for Classifying Arabic Speech Acts on Twitter](https://arxiv.org/abs/2401.17373) | 本文提出了一种用于阿拉伯语推特语言行为分类的加权集成预训练Transformer模型。通过整合不同的BERT模型，我们实现了对阿拉伯方言的精确分类，为理解用户观点和态度提供了有力的工具。 |
| [^53] | [ViLexNorm: A Lexical Normalization Corpus for Vietnamese Social Media Text](https://arxiv.org/abs/2401.16403) | ViLexNorm是第一个为越南社交媒体文本开发的词汇规范化语料库，可显著提高各种NLP任务的性能。 |
| [^54] | [Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You](https://arxiv.org/abs/2401.16092) | 多语言文本到图像生成模型存在性别偏见；通过MAGBIG评估模型时，发现模型对不同语言具有重要差异；我们呼吁研究多语言模型领域消除性别偏见。 |
| [^55] | [Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization](https://arxiv.org/abs/2401.15496) | 本文提出了Baichuan2-Sum模型，通过指导微调Baichuan2-7B模型进行对话摘要，并应用NEFTune技术改进训练过程。实验证明该模型在CSDS和SAMSUM数据集上取得了新的最先进结果。 |
| [^56] | [Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends](https://arxiv.org/abs/2401.07518) | 这篇论文调查了教育领域自然语言处理的最新进展，提出了分类体系，并总结了挑战和未来研究方向。 |
| [^57] | [Efficient Large Language Models: A Survey](https://arxiv.org/abs/2312.03863) | 这篇综述论文对高效大型语言模型进行了系统和全面的调查，提供了从模型为中心、数据为中心和框架为中心的三个主要角度的分类和总结。此外，还创建了一个GitHub存储库来收集和更新相关论文。 |
| [^58] | [Hyperparameter Optimization for Large Language Model Instruction-Tuning](https://arxiv.org/abs/2312.00949) | 该论文研究了大型语言模型调参的超参数优化，通过引入低秩适应方法实现对网络的部分调整，使用黑盒优化技术探索超参数空间，取得了性能提升和模型与人类对齐的效果。 |
| [^59] | [Injecting linguistic knowledge into BERT for Dialogue State Tracking](https://arxiv.org/abs/2311.15623) | 本文提出了一种方法，在对话状态跟踪任务中，通过无监督的知识提取方法将语言知识注入到BERT中，以提高性能和可解释性。这种方法无需额外的训练数据，通过简单的神经模块实现。该方法使用的特征提取工具与对话的句法和语义模式相关，有助于理解DST模型的决策过程。 |
| [^60] | [GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models](https://arxiv.org/abs/2311.09048) | GRASP是一个新的基准测试，用于评估视频多模态大型语言模型的语言基础和物理理解能力。通过Unity模拟进行两层评估，揭示这些模型在语言基础和直觉物理学能力方面的显著缺陷。 |
| [^61] | [Do self-supervised speech and language models extract similar representations as human brain?](https://arxiv.org/abs/2310.04645) | 通过评估Wav2Vec2.0和GPT-2模型的大脑预测能力，我们发现自我监督的语音和语言模型能够准确预测语音反应，其大脑预测之间存在显著相关性，且共享的语音上下文信息是解释大脑活动中变异的主要因素。 |
| [^62] | [Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca](https://arxiv.org/abs/2309.08958) | 通过实证分析比较了单语和多语指导调整的成本效益，发现在多语言场景下，多语指导调整可以达到或超越单独调整每种语言的效果，并且采用下采样的数据进行多语调整可以提供更强的效果和更好的鲁棒性。 |
| [^63] | [Leveraging Multi-lingual Positive Instances in Contrastive Learning to Improve Sentence Embedding](https://arxiv.org/abs/2309.08929) | 本研究提出了一种名为MPCL的方法，在对比学习中利用多语言正例来改进句子嵌入。实验结果表明，MPCL可以提高检索、语义相似性和分类性能。 |
| [^64] | [A RelEntLess Benchmark for Modelling Graded Relations between Named Entities](https://arxiv.org/abs/2305.15002) | 本文提出了一个用于模拟命名实体之间分级关系的无情基准，使用大型语言模型进行填补，以对实体对根据其满足程度进行排序。通过评估最先进的关系嵌入策略和多个LLM，我们发现了重要的创新和贡献。 |
| [^65] | [APPLS: Evaluating Evaluation Metrics for Plain Language Summarization](https://arxiv.org/abs/2305.14341) | 本文提出了一个用于评估纯语言摘要的指标测试平台APPLS，并引入了一种新的指标POMME来评估PLS中的文本简化。通过对指标的分析发现，当前的指标未能始终捕捉到简化度。 |
| [^66] | [$\mu$PLAN: Summarizing using a Content Plan as Cross-Lingual Bridge](https://arxiv.org/abs/2305.14205) | 本文介绍了$\mu$PLAN，一种跨语言摘要方法，使用内容计划作为跨语言桥梁。通过将计划抽象为一系列实体，此方法在四种语言对上取得了最先进的效果。 |
| [^67] | [On-the-fly Denoising for Data Augmentation in Natural Language Understanding](https://arxiv.org/abs/2212.10558) | 本文提出了一种即时去噪的数据增强技术，利用软增强标签和自我正则化模块，通过从更干净的原始数据学习来保证增强数据的质量。 |
| [^68] | [An Empathetic AI Coach for Self-Attachment Therapy](https://arxiv.org/abs/2209.08316) | 本文介绍了一个用于自述疗法的共情人工智能辅导系统，通过深度学习和规则进行情绪识别和生成流畅、共情的对话，达到更高的用户参与度和实用性。通过非临床试验验证了框架的有效性，并提供改进设计和性能的指导方针。 |
| [^69] | [A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM.](http://arxiv.org/abs/2401.15378) | 基于RAG的MufassirQAS问答系统利用NLP技术建立联系并准确回答复杂问题，提高了LLMs的准确性和透明度，帮助理解伊斯兰教的复杂性和教义深度。 |
| [^70] | [Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models.](http://arxiv.org/abs/2401.14440) | 这份论文研究发现，最先进的NLI模型对微小的语义保持表面形式变化非常敏感，导致推断结果不一致。其行为与对组合语义的有效理解不同，这对当前NLI模型的可靠性提出了挑战。 |
| [^71] | [GRATH: Gradual Self-Truthifying for Large Language Models.](http://arxiv.org/abs/2401.12292) | GRATH是一种逐步自我真实化的方法，用于提高大型语言模型的真实性。它通过使用领域外问题提示生成答案，并通过直接偏好优化进行自适应模型优化。GRATH在没有标注答案的情况下以自我监督的方式学习真实性，并通过迭代优化来逐步提升模型真实性。 |
| [^72] | [Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation.](http://arxiv.org/abs/2401.11864) | 本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。 |
| [^73] | [Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities.](http://arxiv.org/abs/2401.11143) | 该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。 |
| [^74] | [Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation.](http://arxiv.org/abs/2401.08694) | 本研究提出了一种结合置信度引导和基于样本的方法的不确定性量化框架，用于解决误信息消除中的幻觉和过度自信的预测问题，并提出了混合框架以提供更好的不确定性估计。 |
| [^75] | [Vanishing Gradients in Reinforcement Finetuning of Language Models.](http://arxiv.org/abs/2310.20703) | 本研究发现在强化微调（RFT）中存在梯度消失的问题，当模型下奖励的标准差较小时，输入的期望梯度会消失，导致奖励最大化缓慢。初始监督微调（SFT）阶段是克服这个问题的最有希望的方法。 |
| [^76] | [MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings.](http://arxiv.org/abs/2309.08648) | MAPLE是一个利用大型语言模型嵌入进行移动应用预测的模型，通过严格测试验证了其在解密复杂模式和理解用户环境方面的能力，并强调了语言模型在不同领域中的广泛适用性。 |
| [^77] | [RCT Rejection Sampling for Causal Estimation Evaluation.](http://arxiv.org/abs/2307.15176) | 该论文提出了一种名为RCT拒绝抽样的新抽样算法，用于因果估计评估。该方法通过子抽样随机控制试验(RCT)创建混淆的观测数据集，并使用RCT的平均因果效应作为基准真实值，以进行有效比较。 |
| [^78] | [What do self-supervised speech models know about words?.](http://arxiv.org/abs/2307.00162) | 通过对自我监督的语音模型进行分析，发现这些模型在不同层中编码了不同的语言信息，也学习了类似音素的子词单元。与单词相关的信息主要在中间的模型层中，同时一些低级信息在更高的层中也得以保留。 |
| [^79] | [How Does Beam Search improve Span-Level Confidence Estimation in Generative Sequence Labeling?.](http://arxiv.org/abs/2212.10767) | 本文研究了在生成序列标记任务中如何改善对跨度级别置信度的估计。研究发现仅仅使用解码器的输出概率并不是最佳方法，而利用束搜索的前k个预测的统计数据可以显著降低校准误差。 |

# 详细

[^1]: 语言模型在解决问题时是否表现出与人类学习者相同的认知偏见？

    Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?

    [https://arxiv.org/abs/2401.18070](https://arxiv.org/abs/2401.18070)

    该研究调查了语言模型在解决算术问题时与人类学习者的认知偏见。研究发现，当前最先进的语言模型在文本理解和解决方案规划阶段表现出与人类类似的偏见。

    

    越来越多的人对使用大型语言模型（LLMs）作为认知模型感兴趣。为了达到这个目的，了解LLMs能够模拟哪些认知特性以及哪些不能模拟是至关重要的。在这项研究中，我们研究了LLMs在解决算术问题时与儿童已知认知偏见的相关性。通过调查学习科学文献，我们提出问题解决过程可以分为三个明确的步骤：文本理解、解决方案规划和解决方案执行。我们为每个步骤构建了测试，以了解当前最先进的LLMs可以如何忠实地模拟这个过程的哪些部分。我们使用一种神经符号方法为每个测试生成了一组新的单词问题，该方法可以对问题特征进行精细控制。我们发现，LLMs在文本理解和解决方案规划两个解决过程的步骤中，不论是否经过指导调整，都表现出与人类类似的偏见。

    There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs. We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but 
    
[^2]: RAPTOR: 递归抽象处理用于树状检索

    RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval

    [https://arxiv.org/abs/2401.18059](https://arxiv.org/abs/2401.18059)

    本论文提出了一种递归嵌入、聚类和摘要的方法，通过构建不同摘要级别的树，从下往上整合并检索长度较长的文档，对传统检索增强的语言模型进行改进，实现了在复杂的多步推理问答任务上的最先进的结果提升。

    

    检索增强的语言模型可以更好地适应世界状态的变化，并结合长尾知识。然而，大多数现有的方法仅从检索语料库中检索短连续块，限制了对整体文档上下文的整体理解。我们引入了一种新颖的方法，即递归嵌入、聚类和摘要文本块，从下往上构建具有不同摘要级别的树。在推理时，我们的RAPTOR模型从这棵树中检索，将不同抽象级别的信息整合到长度较长的文档中。控制实验表明，使用递归摘要的检索在几个任务上比传统检索增强的语言模型有显著改进。在涉及复杂的多步推理的问答任务上，我们展示了最先进的结果；例如，通过将RAPTOR检索与GPT-4的使用相结合，我们可以将QuALITY基准测试的最佳性能提高20％。

    Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20%
    
[^3]: LongAlign：大型语言模型的长上下文对齐方法

    LongAlign: A Recipe for Long Context Alignment of Large Language Models

    [https://arxiv.org/abs/2401.18058](https://arxiv.org/abs/2401.18058)

    LongAlign是一种用于大型语言模型的长上下文对齐的方法，通过指导微调和使用打包、排序和损失加权策略，它在长上下文任务中表现优异，相比现有的方法提高了多达30\%的性能。

    

    将大型语言模型扩展到有效处理长上下文的能力需要对相似长度的输入序列进行指导微调。为了解决这个问题，我们提出了LongAlign - 一种用于长上下文对齐的指导数据、训练和评估方法。首先，我们使用自我指导方法构建长指令跟随数据集。为了确保数据多样性，它涵盖了来自不同长上下文来源的广泛任务。其次，我们采用打包和排序批处理策略，加速在具有不同长度分布的数据上的受监督微调。此外，我们还开发了一种损失加权方法，在打包训练过程中平衡损失对不同序列的贡献。第三，我们引入了LongBench-Chat基准，用于评估10k-100k长度的查询的指令跟随能力。实验结果表明，LongAlign在长上下文任务中比现有的LLMs方法的性能提高了多达30\%，同时也保持了其熟练性。

    Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign -- a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\%, while also maintaining their proficienc
    
[^4]: 多路径解析在大脑中的研究

    Multipath parsing in the brain

    [https://arxiv.org/abs/2401.18046](https://arxiv.org/abs/2401.18046)

    本研究通过将增量生成依存关系解析器的预测与人们进行功能神经成像的时间数据相关，发现了人类在逐词理解句子时存在多路径解析的证据。

    

    人类逐词理解句子时，以所听到的顺序进行。这种增量方式需要解决临时的语法关系歧义。我们通过将增量生成依存关系解析器的预测与在听播音书时进行功能神经成像的人们的时间数据进行相关，来研究人类是如何处理这些语法歧义的。特别是，我们比较了关于逐词理解过程中正在进行的语法分析数量的竞争性假设：一个与多个。这个比较涉及将最先进的依存关系解析器使用经过LLM调整的编码来评估语法惊讶度，与现有的fMRI数据集相对照。在英文和中文的数据中，我们发现了多路径解析的证据。与该多路径效应相关的脑区包括双侧颞叶上沟。

    Humans understand sentences word-by-word, in the order that they hear them. This incrementality entails resolving temporary ambiguities about syntactic relationships. We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook. In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one. This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset. In both English and Chinese data, we find evidence for multipath parsing. Brain regions associated with this multipath effect include bilateral superior temporal gyrus.
    
[^5]: SpeechComposer: 使用提示合成统一多个语音任务

    SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition

    [https://arxiv.org/abs/2401.18045](https://arxiv.org/abs/2401.18045)

    SpeechComposer是一种能够通过合成固定提示标记来统一多个语音任务的语音语言模型。它提高了任务之间的连接性，并且可以轻松扩展到更多的任务，同时实现了任务之间的知识共享。

    

    最近语言模型的进展大大提升了多种语音相关任务的性能。现有的语音语言模型通常利用任务相关的提示标记将各种语音任务统一在一个模型中。然而，这种设计忽略了不同语音任务之间的内在联系，这可能会提高每个任务的性能。在这项工作中，我们提出了一种新颖的只解码器的语音语言模型SpeechComposer，它可以通过组合一组固定的提示标记统一常见的语音任务。SpeechComposer建立在四个主要任务的基础上--语音合成、语音识别、语音语言建模和文本语言建模--通过设计良好的提示标记的组合，如声音转换和语音增强，可以轻松扩展到更多的语音任务。提示标记的统一也使得不同语音任务之间的知识共享更加结构化。实验结果表明我们的模型提供了更好的性能和更大的灵活性。

    Recent advancements in language models have significantly enhanced performance in multiple speech-related tasks. Existing speech language models typically utilize task-dependent prompt tokens to unify various speech tasks in a single model. However, this design omits the intrinsic connections between different speech tasks, which can potentially boost the performance of each task. In this work, we propose a novel decoder-only speech language model, SpeechComposer, that can unify common speech tasks by composing a fixed set of prompt tokens. Built upon four primary tasks -- speech synthesis, speech recognition, speech language modeling, and text language modeling -- SpeechComposer can easily extend to more speech tasks via compositions of well-designed prompt tokens, like voice conversion and speech enhancement. The unification of prompt tokens also makes it possible for knowledge sharing among different speech tasks in a more structured manner. Experimental results demonstrate that our
    
[^6]: 加强端到端多任务对话系统：基于内在动机强化学习算法的改进训练和适应性研究

    Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability

    [https://arxiv.org/abs/2401.18040](https://arxiv.org/abs/2401.18040)

    本研究旨在通过内在动机强化学习算法改进端到端多任务对话系统的训练和适应性。通过教授智能体一个内在奖励系统，可以加速训练并提高其判断行为质量的能力。

    

    端到端多任务对话系统通常通过对话流水线的独立模块进行设计。其中，策略模块是决定对用户输入如何响应的关键。这个策略是通过强化学习算法进行训练的，通过利用一个智能体在一个反馈信号形式的环境中接收反馈。然而，当前的对话系统只提供了稀缺且简单的奖励。本研究的目标是研究内在动机强化学习算法。通过这种算法，智能体可以快速加速训练，并通过教授一个内在奖励系统来提高判断其行为质量的能力。具体而言，我们将随机网络蒸馏和好奇驱动强化学习技术应用于测量状态访问频率，并通过使用话语之间的语义相似性来鼓励探索。在一个异构数据集MultiWOZ上进行的实验结果显示...

    End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline. Among these, the policy module is essential for deciding what to do in response to user input. This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal. The current dialogue systems, however, only provide meagre and simplistic rewards. Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study. Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system. In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances. Experimental results on MultiWOZ, a heterogeneous dataset, sho
    
[^7]: Paramanu: 一种高效的印度生成式基础语言模型系列

    Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models

    [https://arxiv.org/abs/2401.18034](https://arxiv.org/abs/2401.18034)

    Paramanu是一种高效的印度生成式基础语言模型系列，包含多种印度语言模型，并且在单个GPU上进行了从头预训练。它还包括一个先进的印度分词器以及避免多语言诅咒的预训练方法。这些模型在人工评估中展现出良好的语法、连贯性、创造性和事实准确性。

    

    我们介绍了Gyan AI Paramanu（“原子”），一种适用于印度语言的新型语言模型系列。它是一个在单个GPU上从头开始预训练的包含单语、双语和多语印度语言模型的集合，涵盖了10种印度语言（阿萨姆语、孟加拉语、印地语、康坎尼语、迈蒂利语、马拉地语、奥迪亚语、梵语、泰米尔语和泰卢固语）以及5种不同大小的字母表（孟加拉语、天城体、奥迪亚语、泰米尔语和泰卢固语）。这些模型以1024的上下文大小在单个GPU上预训练，非常高效、小巧、快速且强大。我们还开发了一种高效的先进的印度语分词器，甚至可以标记未知语言。为了避免我们的多语言mParamanu模型中的“多语言诅咒”，我们使用相同的字母表按语言类型进行了可比较语料库的预训练。我们对我们预训练模型进行了人工评估，评估指标包括语法、连贯性、创造性和事实准确性。

    We present Gyan AI Paramanu ("atom"), a family of novel language models for Indian languages. It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU. The models are very efficient, small, fast, and powerful. We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages. In order to avoid the "curse of multi-linguality" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script. We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics fo
    
[^8]: 使用LLMs支持预期治理: 通过与新闻媒体对齐，评估大型语言模型以预测人工智能的负面影响

    Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI

    [https://arxiv.org/abs/2401.18028](https://arxiv.org/abs/2401.18028)

    本研究探讨了使用LLMs来增强预测人工智能负面影响的方法，并通过与新闻媒体对齐，建立了一个包含十个类别的人工智能影响分类法。结果表明，基于指令和迁移学习的LLMs模型在生成影响方面具有一定的能力。

    

    在人工智能技术发展的早期阶段，预测其可能带来的负面影响是一个挑战。使用LLMs增强和指导这一过程是一种不太被研究的预测方法。尽管LLMs和评估指标在生成文本中考虑偏差方面有所进展，但目前尚不清楚这些模型在预测任务中表现如何。具体而言，使用LLMs预测人工智能影响引发了关于模型能够生成的负面影响类别的质量和范围的问题。在本文中，我们利用丰富的包含对新兴技术的规范性评估的数据来源——新闻媒体，制定了一个基准，用于比较不同类别的影响。通过计算分析全球数百个在线新闻媒体发布的数千篇新闻文章，我们建立了一个包含十个类别的人工智能影响分类法。然后，我们评估了基于指令的LLMs模型（GPT-4等）和基于迁移学习的模型在根据我们的分类法生成的影响方面的表现。

    Anticipating the negative impacts of emerging AI technologies is a challenge, especially in the early stages of development. An understudied approach to such anticipation is the use of LLMs to enhance and guide this process. Despite advancements in LLMs and evaluation metrics to account for biases in generated text, it is unclear how well these models perform in anticipatory tasks. Specifically, the use of LLMs to anticipate AI impacts raises questions about the quality and range of categories of negative impacts these models are capable of generating. In this paper we leverage news media, a diverse data source that is rich with normative assessments of emerging technologies, to formulate a taxonomy of impacts to act as a baseline for comparing against. By computationally analyzing thousands of news articles published by hundreds of online news domains around the world, we develop a taxonomy consisting of ten categories of AI impacts. We then evaluate both instruction-based (GPT-4 and 
    
[^9]: 通过定向表示优化实现的安全提示驱动的大型语言模型(LLM)保护

    Prompt-Driven LLM Safeguarding via Directed Representation Optimization

    [https://arxiv.org/abs/2401.18018](https://arxiv.org/abs/2401.18018)

    通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。

    

    在大型语言模型(LLM)中，使用安全提示在模型输入之前是一种常见的保护实践，以使其不遵从包含恶意意图的查询。然而，安全提示的工作机制尚未完全理解，这妨碍了自动优化其以改善LLM安全性的潜力。针对这个问题，我们从模型表示的角度调查了安全提示的影响。我们发现在模型的表示空间中，有害和无害的查询可以在很大程度上区分开来，但安全提示并没有明显增强这一区分。相反，不同安全提示导致查询的表示朝着相似的方向移动，使得模型即使在查询无害时也更容易拒绝提供协助。受到这些发现的启发，我们提出了一种名为DRO（定向表示优化）的方法，用于自动安全提示优化。DRO将安全提示视为要优化的表示方向。

    Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
    
[^10]: 问题回答系统上下文使用的期望

    Desiderata for the Context Use of Question Answering Systems

    [https://arxiv.org/abs/2401.18001](https://arxiv.org/abs/2401.18001)

    本论文概述了在现有的上下文问题回答系统中存在的问题，并提出了一系列期望，旨在解决模型对上下文的关注不足、对噪声的鲁棒性较低以及回答的不一致性。通过对15个QA系统在5个数据集上的综合评估，发现了一些新趋势。

    

    先前的研究揭示了现有先进的基于上下文的问题回答（QA）系统中存在的一系列共同问题：当上下文与模型的参数化知识冲突时，缺乏对上下文的关注，对噪声的鲁棒性很小，并且回答的一致性不足。然而，大多数先前的工作都集中在单独解决其中一两个问题上，这使得很难看到它们之间的趋势。我们旨在填补这一空白，首先概述QA模型的一系列 - 先前讨论过的和新的 - 期望。然后，我们通过调查相关的分析和方法论文，提供领域现状的概述。我们的工作的第二部分展示了实验，在5个数据集上同时按照所有期望评估了15个QA系统。我们发现了许多新的趋势，包括（1）对噪声较不敏感的系统在提供无关上下文时不一定更一致地回答问题；（2）大多数对噪声敏感的系统...

    Prior work has uncovered a set of common problems in state-of-the-art context-based question answering (QA) systems: a lack of attention to the context when the latter conflicts with a model's parametric knowledge, little robustness to noise, and a lack of consistency with their answers. However, most prior work focus on one or two of those problems in isolation, which makes it difficult to see trends across them. We aim to close this gap, by first outlining a set of -- previously discussed as well as novel -- desiderata for QA models. We then survey relevant analysis and methods papers to provide an overview of the state of the field. The second part of our work presents experiments where we evaluate 15 QA systems on 5 datasets according to all desiderata at once. We find many novel trends, including (1) systems that are less susceptible to noise are not necessarily more consistent with their answers when given irrelevant context; (2) most systems that are more susceptible to noise ar
    
[^11]: 职场领域中的实体链接

    Entity Linking in the Job Market Domain

    [https://arxiv.org/abs/2401.17979](https://arxiv.org/abs/2401.17979)

    本研究探索了职场领域中的实体链接，通过消歧技能提及并与ESCO分类体系进行关联，对当前劳动力市场的需求进行深入了解。通过调整神经EL模型，我们发现BLINK在严格评估中胜过GENRE，但GENRE在宽松评估中表现更好。

    

    在自然语言处理中，实体链接（EL）主要围绕维基百科展开，但在职场领域仍未充分探索。消歧技能提及可以帮助我们了解当前劳动力市场的需求。在这项工作中，我们是首次在这个领域探索EL，具体目标是将职业技能与ESCO分类体系（le Vrang等，2014）进行关联。以往的努力将粗粒度（全）句子与相应的ESCO技能进行关联。在这项工作中，我们将更细粒度的跨度级别技能提及进行链接。我们在一个合成生成的技能提及-技能配对数据集上调整了两个性能强大的神经EL模型，一个是双编码器模型（Wu等，2020），一个是自回归模型（Cao等，2021），并在一个人工注释的技能链接基准上对它们进行评估。我们的研究结果揭示了这两个模型都能够将隐含的技能提及与正确的分类体系对应起来。经验上，BLINK在严格评估中胜过GENRE，但GENRE在宽松评估中表现更好。

    In Natural Language Processing, entity linking (EL) has centered around Wikipedia, but yet remains underexplored for the job market domain. Disambiguating skill mentions can help us get insight into the current labor market demands. In this work, we are the first to explore EL in this domain, specifically targeting the linkage of occupational skills to the ESCO taxonomy (le Vrang et al., 2014). Previous efforts linked coarse-grained (full) sentences to a corresponding ESCO skill. In this work, we link more fine-grained span-level mentions of skills. We tune two high-performing neural EL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao et al., 2021), on a synthetically generated mention--skill pair dataset and evaluate them on a human-annotated skill-linking benchmark. Our findings reveal that both models are capable of linking implicit mentions of skills to their correct taxonomy counterparts. Empirically, BLINK outperforms GENRE in strict evaluation, but GENRE p
    
[^12]: GUMsley：评估英语12种流派中的摘要中的实体显著性

    GUMsley: Evaluating Entity Salience in Summarization for 12 English Genres

    [https://arxiv.org/abs/2401.17974](https://arxiv.org/abs/2401.17974)

    GUMsley是第一个覆盖12种英文文本流派中所有命名和非命名显著实体的实体显著性数据集，并证明了当前最先进的摘要模型在捕捉生成摘要中的显著实体方面表现不佳。

    

    随着自然语言处理模型在理解文档方面变得越来越能够以连贯的实体而不是字符串的形式，获取每个文档中最显著的实体不仅是一个重要的最终任务，而且对于信息检索（IR）和其他下游应用，如可控制的摘要，也至关重要。本文提出并评估了GUMsley，这是第一个涵盖英语文本12种流派的所有命名和非命名显著实体的实体显著性数据集，与实体类型、维基化链接和完整的核心引用解析对齐。我们使用人工摘要提倡了对显著性的严格定义，并展示了基于摘要中是否提到源实体的显著性的高度标注者间一致性。我们的评估结果显示，预训练的最先进摘要模型和零-shot语言模型在生成摘要时捕捉到显著实体的表现差。我们还表明，在生成摘要时预测或提供显著实体对提高实体显著性有益。

    As NLP models become increasingly capable of understanding documents in terms of coherent entities rather than strings, obtaining the most salient entities for each document is not only an important end task in itself but also vital for Information Retrieval (IR) and other downstream applications such as controllable summarization. In this paper, we present and evaluate GUMsley, the first entity salience dataset covering all named and non-named salient entities for 12 genres of English text, aligned with entity types, Wikification links and full coreference resolution annotations. We promote a strict definition of salience using human summaries and demonstrate high inter-annotator agreement for salience based on whether a source entity is mentioned in the summary. Our evaluation shows poor performance by pre-trained SOTA summarization models and zero-shot LLM prompting in capturing salient entities in generated summaries. We also show that predicting or providing salient entities to se
    
[^13]: 狮子：1和老虎：2和熊：3，喔美啊！使用LLMs进行文学指代标注

    [Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference Annotation with LLMs

    [https://arxiv.org/abs/2401.17922](https://arxiv.org/abs/2401.17922)

    本研究提出了一种使用LLMs进行文学指代标注的方法，通过学习生成带有指代注释的句子，解决了构建高质量小说系统的困难。我们创建并发布了多个经过训练的指代模型，并提供了一个用于训练新模型的工作流程。

    

    指代标注和解析是计算文学研究的重要组成部分。然而，在构建小说的高质量系统方面存在困难。指代标注需要复杂的结构化输出，而文学文本涉及微妙的推断和语言多样性。基于新的基于语言模型的seq2seq系统，可以通过学习直接生成带有类似markdown注释的输入句子来解决这些问题。我们创建、评估和发布了几个经过训练的指代模型，以及一个用于训练新模型的工作流程。

    Coreference annotation and resolution is a vital component of computational literary studies. However, it has previously been difficult to build high quality systems for fiction. Coreference requires complicated structured outputs, and literary text involves subtle inferences and highly varied language. New language-model-based seq2seq systems present the opportunity to solve both these problems by learning to directly generate a copy of an input sentence with markdown-like annotations. We create, evaluate, and release several trained models for coreference, as well as a workflow for training new models.
    
[^14]: LOCOST: 长文档抽象摘要化的状态空间模型

    LOCOST: State-Space Models for Long Document Abstractive Summarization

    [https://arxiv.org/abs/2401.17919](https://arxiv.org/abs/2401.17919)

    LOCOST是一种基于状态空间模型的编码器-解码器架构，用于处理长文档的抽象摘要生成。与基于稀疏注意模式的最先进模型相比，LOCOST具有更低的计算复杂度，并且能够在训练和推断期间节省大量内存。在评估中，LOCOST在长文档摘要化任务上达到了93-96%的性能水平，并且能够处理超过600K个标记的输入文本。

    

    状态空间模型是编码长序列和捕捉长期依赖的低复杂度替代方案，我们提出了LOCOST：一种基于状态空间模型的编码器-解码器架构，用于具有长上下文输入的条件文本生成。这种架构的计算复杂度为O（L log L），可以处理比基于稀疏注意模式的最先进模型更长的序列。我们在一系列长文档抽象摘要化任务上评估了我们的模型。该模型在性能水平上达到了与相同大小的最优稀疏变压器相当的93-96%，同时在训练期间节省了高达50%的内存，在推断期间节省了高达87%的内存。此外，LOCOST有效地处理超过600K个标记的输入文本，为完整书摘要化设定了新的最新结果，并为长输入处理开辟了新的视角。

    State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.
    
[^15]: SNNLP: 使用脉冲神经网络的高能效自然语言处理

    SNNLP: Energy-Efficient Natural Language Processing Using Spiking Neural Networks

    [https://arxiv.org/abs/2401.17911](https://arxiv.org/abs/2401.17911)

    本论文研究了在自然语言处理领域中使用脉冲神经网络的高能效解决方案，并比较了不同的文本编码方法在相关任务中的性能。

    

    随着脉冲神经网络引起越来越多的关注，我们开始关注这种计算范式在计算机视觉和信号处理以外领域的应用。然而，在神经形态计算中，一个尚未充分研究的主要领域是自然语言处理（NLP），其中大多数最先进的解决方案仍然严重依赖资源消耗和耗电量较高的传统深度学习架构。因此，设计适用于神经形态架构的NLP模型具有引人注目的吸引力，因为它们具有较低的能量需求，并且在处理信息时具有更类似于人脑的操作模式的额外好处。然而，将NLP引入神经形态计算的最大问题之一在于如何将文本正确编码为脉冲序列，以便当前和未来的脉冲神经网络架构可以无缝处理。在本文章中，我们比较了不同的文本编码方法，并评估了每种方法在相关的脉冲神经网络上处理NLP任务（即情感分析）时的性能。

    As spiking neural networks receive more attention, we look toward applications of this computing paradigm in fields other than computer vision and signal processing. One major field, underexplored in the neuromorphic setting, is Natural Language Processing (NLP), where most state-of-the-art solutions still heavily rely on resource-consuming and power-hungry traditional deep learning architectures. Therefore, it is compelling to design NLP models for neuromorphic architectures due to their low energy requirements, with the additional benefit of a more human-brain-like operating model for processing information. However, one of the biggest issues with bringing NLP to the neuromorphic setting is in properly encoding text into a spike train so that it can be seamlessly handled by both current and future SNN architectures. In this paper, we compare various methods of encoding text as spikes and assess each method's performance in an associated SNN on a downstream NLP task, namely, sentiment
    
[^16]: 通过改进特征重新思考语音分割和词典学习

    Revisiting speech segmentation and lexicon learning with better features

    [https://arxiv.org/abs/2401.17902](https://arxiv.org/abs/2401.17902)

    本文重新审视了一种自监督方法，利用改进的特征将未标记的语音分割成单词状的片段，并通过聚类得到一个具有良好覆盖率和最先进性能的词典。

    

    我们重新审视了一种自监督方法，该方法将未标记的语音分割成类似单词的片段。我们从两阶段的时长惩罚动态规划方法开始，该方法在不学习显式词典的情况下进行零资源分割。在第一个声学单元发现阶段，我们用HuBERT替换了对比性预测编码特征。在第二阶段的单词分割后，我们通过平均HuBERT特征获得每个片段的声学单词嵌入。使用K-means对这些嵌入进行聚类，以得到一个词典。结果是具有良好覆盖率分割和在ZeroSpeech基准测试上达到最先进性能的词典。

    We revisit a self-supervised method that segments unlabelled speech into word-like segments. We start from the two-stage duration-penalised dynamic programming method that performs zero-resource segmentation without learning an explicit lexicon. In the first acoustic unit discovery stage, we replace contrastive predictive coding features with HuBERT. After word segmentation in the second stage, we get an acoustic word embedding for each segment by averaging HuBERT features. These embeddings are clustered using K-means to get a lexicon. The result is good full-coverage segmentation with a lexicon that achieves state-of-the-art performance on the ZeroSpeech benchmarks.
    
[^17]: 利用标签模型在ChatGPT回答中提高法律文本推理性能

    Employing Label Models on ChatGPT Answers Improves Legal Text Entailment Performance

    [https://arxiv.org/abs/2401.17897](https://arxiv.org/abs/2401.17897)

    本论文介绍了利用标签模型在ChatGPT回答中提高法律文本推理性能的方法，通过将ChatGPT的临时答案整合为综合标签，实现了准确率的提高。

    

    法律文本推理的目标是确定法律查询中的断言是否逻辑上符合所提供的一个或多个法律文章中提供的信息。ChatGPT是一个大型语言模型，在许多自然语言处理任务中都表现出色，包括法律文本推理：当我们将温度设置为0（ChatGPT回答是确定性的）并提示模型时，它在COLIEE 2022数据集上的准确率达到了70.64％，超过了之前的SOTA（67.89％）。另一方面，如果温度大于零，ChatGPT的回答就不确定了，导致答案不一致和结果波动。我们提出利用标签模型（弱监督技术的基本组成部分）将ChatGPT的临时答案整合为综合标签。通过这种方式，我们将ChatGPT的临时答案视为可能带有噪声的预测，可以通过标签模型来整合。实验结果表明，这种方法可以实现准确率的提高。

    The objective of legal text entailment is to ascertain whether the assertions in a legal query logically follow from the information provided in one or multiple legal articles. ChatGPT, a large language model, is robust in many natural language processing tasks, including legal text entailment: when we set the temperature = 0 (the ChatGPT answers are deterministic) and prompt the model, it achieves 70.64% accuracy on COLIEE 2022 dataset, which outperforms the previous SOTA of 67.89%. On the other hand, if the temperature is larger than zero, ChatGPT answers are not deterministic, leading to inconsistent answers and fluctuating results. We propose to leverage label models (a fundamental component of weak supervision techniques) to integrate the provisional answers by ChatGPT into consolidated labels. By that way, we treat ChatGPT provisional answers as noisy predictions which can be consolidated by label models. The experimental results demonstrate that this approach can attain an accur
    
[^18]: 因此我思，我在：大型语言模型中的意识

    I Think, Therefore I am: Awareness in Large Language Models

    [https://arxiv.org/abs/2401.17882](https://arxiv.org/abs/2401.17882)

    本文介绍了将意识概念引入大型语言模型（LLMs），并定义了LLMs在感知和理解自身以及展示社交智能方面的能力。通过引入AwareLLM数据集，研究发现LLMs在意识方面表现出相当程度的能力，尽管它们缺乏实质性的能力意识。

    

    大型语言模型（LLMs）是否展现出类似于人类的意识形式？在本文中，我们介绍了将意识概念引入LLMs，认为意识是LLMs增强与人类交互并确保道德回应的可信度的重要方面。我们将LLMs中的意识定义为感知和理解自身作为AI模型以及展示社交智能的能力。我们确定了意识的四个关键维度：能力、任务、情感和观点。为了评估LLMs在这些维度上的表现，我们引入了一个专门的数据集，AwareLLM数据集。我们的研究结果显示，LLMs展现出相当程度的意识，尽管它们仍然缺乏实质性的能力意识。

    Do large language models (LLMs) exhibit any forms of awareness similar to humans? In this paper, we introduce the concept of awareness to LLMs, arguing that awareness is an essential aspect of trustworthiness for LLMs to enhance their interaction with humans while ensuring ethical responses. We define awareness in LLMs as the ability to perceive and understand themselves as AI models and to exhibit social intelligence. We identify four key dimensions of awareness: capability, mission, emotion, and perspective. To assess LLMs on these dimensions, we introduce a specialized dataset, AwareLLM dataset. Our findings reveal that LLMs demonstrate a decent degree of awareness, though they still lack substantial capability awareness.
    
[^19]: 探索语言模型对增强人机交互的手势理解

    Probing Language Models' Gesture Understanding for Enhanced Human-AI Interaction

    [https://arxiv.org/abs/2401.17858](https://arxiv.org/abs/2401.17858)

    该项目旨在探究语言模型对非语言交流中的手势理解，通过测试LLMs对明示和隐含非语言暗示的能力以及与语境因素的关联。实验将使用心理语言学设计和全面的数据集进行，同时考虑文化维度，并测量LLM和人类之间的一致性。

    

    大型语言模型（LLM）的崛起已经影响到超越纯文本生成的各个学科。本项目提出超越纯文本性质，探究LLM与非语言交流，特别关注手势之间的互动。该项目计划研究LLM在解析文字提示中的明示和隐含非语言暗示的能力，以及它们将这些手势与各种语境因素联系起来的能力。研究计划测试已建立的心理语言学研究设计，构建一个全面的数据集，将文字提示与详细的手势描述相配对，涵盖各种区域变异和语义标签。为了评估LLMs对手势的理解能力，计划进行一系列实验，评估它们模拟人类行为的能力，以重现心理语言学实验。这些实验考虑到文化维度，并测量LLM和人类之间的一致性。

    The rise of Large Language Models (LLMs) has affected various disciplines that got beyond mere text generation. Going beyond their textual nature, this project proposal aims to investigate the interaction between LLMs and non-verbal communication, specifically focusing on gestures. The proposal sets out a plan to examine the proficiency of LLMs in deciphering both explicit and implicit non-verbal cues within textual prompts and their ability to associate these gestures with various contextual factors. The research proposes to test established psycholinguistic study designs to construct a comprehensive dataset that pairs textual prompts with detailed gesture descriptions, encompassing diverse regional variations, and semantic labels. To assess LLMs' comprehension of gestures, experiments are planned, evaluating their ability to simulate human behaviour in order to replicate psycholinguistic experiments. These experiments consider cultural dimensions and measure the agreement between LLM
    
[^20]: 全球说谎者：LLMs在时间和地理区域上的事实性

    Global-Liar: Factuality of LLMs over Time and Geographic Regions

    [https://arxiv.org/abs/2401.17839](https://arxiv.org/abs/2401.17839)

    本论文评估了GPT模型的事实准确性、稳定性和偏见，并引入了一个平衡数据集"全球说谎者"，结果显示较新的GPT模型并不总是意味着性能的提升，并且观察到一个全球南方陈述被偏袒的问题。

    

    越来越多地依赖于人工智能驱动的解决方案，特别是像GPT系列这样的大型语言模型（LLMs）在信息检索中的使用，突显了对它们的事实准确性和公正性的重要性，尤其是在网络上虚假信息和误导信息猖獗传播的背景下。我们的研究评估了广泛采用的GPT模型（包括GPT-3.5和GPT-4）的事实准确性、稳定性和偏见，以提高人工智能介导信息传播的可靠性和完整性。我们引入了一个独特平衡的数据集“全球说谎者”，其在地理和时间表征方面有助于更细致地评估LLM的偏见。我们的分析结果表明，较新的GPT模型并不总是意味着性能的提升。值得注意的是，3月发布的GPT-4版本显示出比其后续6月发布版本更高的事实准确性。此外，还观察到一个令人担忧的偏见，即对全球南方的陈述给予了特权，可能加剧了不平等。

    The increasing reliance on AI-driven solutions, particularly Large Language Models (LLMs) like the GPT series, for information retrieval highlights the critical need for their factuality and fairness, especially amidst the rampant spread of misinformation and disinformation online. Our study evaluates the factual accuracy, stability, and biases in widely adopted GPT models, including GPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated information dissemination.   We introduce 'Global-Liar,' a dataset uniquely balanced in terms of geographic and temporal representation, facilitating a more nuanced evaluation of LLM biases. Our analysis reveals that newer iterations of GPT models do not always equate to improved performance. Notably, the GPT-4 version from March demonstrates higher factual accuracy than its subsequent June release. Furthermore, a concerning bias is observed, privileging statements from the Global North over the Global South, thus potentially exace
    
[^21]: 马拉亚拉姆语改写生成的神经机器翻译

    Neural Machine Translation for Malayalam Paraphrase Generation

    [https://arxiv.org/abs/2401.17827](https://arxiv.org/abs/2401.17827)

    本研究针对马拉亚拉姆语探索了四种生成改写的方法，并使用了英语改写和预训练的神经机器翻译模型的资源。研究发现，自动化评估指标不完全适用于马拉亚拉姆语，强调了对于高度合词性语言更细致的改写评估方法的需求。

    

    本研究探讨了四种在马拉亚拉姆语中生成改写的方法，利用了英语改写和预训练的神经机器翻译（NMT）模型的资源。我们使用自动化指标（如BLEU，METEOR和余弦相似度）以及人工标注来评估所得到的改写。我们的研究发现，自动化评估指标对于马拉亚拉姆语可能不完全适用，因为它们与人类判断不一致。这种差异突显了对于高度合词性语言尤其需要更细致的改写评估方法的需求。

    This study explores four methods of generating paraphrases in Malayalam, utilizing resources available for English paraphrasing and pre-trained Neural Machine Translation (NMT) models. We evaluate the resulting paraphrases using both automated metrics, such as BLEU, METEOR, and cosine similarity, as well as human annotation. Our findings suggest that automated evaluation measures may not be fully appropriate for Malayalam, as they do not consistently align with human judgment. This discrepancy underscores the need for more nuanced paraphrase evaluation approaches especially for highly agglutinative languages.
    
[^22]: 对处理科学文本的预训练语言模型的调查研究

    A Survey of Pre-trained Language Models for Processing Scientific Text

    [https://arxiv.org/abs/2401.17824](https://arxiv.org/abs/2401.17824)

    本研究对处理科学文本的预训练语言模型进行了全面调查，包括对它们在不同领域、任务和数据集中有效性的分析，以及对未来挑战的讨论。

    

    处理科学文本的语言模型的数量正在增长。跟上科学语言模型（SciLMs）高速增长的步伐已经成为研究人员的一项艰巨任务。迄今为止，还没有进行全面调查关于SciLMs的工作，这个问题一直没有解决。鉴于持续涌现的新SciLMs，评估最先进的模型以及它们相互之间的比较仍然是未知的。这项研究填补了这个空白，并提供了SciLMs的全面回顾，包括对其在不同领域、任务和数据集中有效性的广泛分析，并对未来可能面临的挑战进行了讨论。

    The number of Language Models (LMs) dedicated to processing scientific text is on the rise. Keeping pace with the rapid growth of scientific LMs (SciLMs) has become a daunting task for researchers. To date, no comprehensive surveys on SciLMs have been undertaken, leaving this issue unaddressed. Given the constant stream of new SciLMs, appraising the state-of-the-art and how they compare to each other remain largely unknown. This work fills that gap and provides a comprehensive review of SciLMs, including an extensive analysis of their effectiveness across different domains, tasks and datasets, and a discussion on the challenges that lie ahead.
    
[^23]: SWEA:通过主题词嵌入修改改变大型语言模型中的事实知识

    SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering

    [https://arxiv.org/abs/2401.17809](https://arxiv.org/abs/2401.17809)

    提出了一种主题词嵌入修改框架（SWEA），通过在推理阶段修改主题的表示来编辑知识，保护模型的原始权重，避免不可逆的损害和额外的推理开销。

    

    模型编辑近来引起了广泛关注。目前的模型编辑方法主要涉及修改模型参数或向现有模型添加附加模块。然而，前者会对LLM造成不可逆的影响，而后者会产生额外的推理开销，并且模糊的向量匹配并不总是可靠的。为了解决这些问题，我们提出了一种可扩展的主题词嵌入修改（SWEA）框架，它在推理阶段修改主题的表示，并实现编辑知识的目标。SWEA在模型外部使用精确的关键匹配，并进行可靠的主题词嵌入修改，从而保护模型的原始权重而不增加推理开销。然后，我们提出优化抑制融合方法，首先优化编辑目标的嵌入向量，然后抑制知识嵌入维度（KED）以获得最终融合的嵌入。我们因此提出了SWEAOS元方法。

    Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
    
[^24]: CauESC: 一种关注因果关系的情感支持对话模型

    CauESC: A Causal Aware Model for Emotional Support Conversation

    [https://arxiv.org/abs/2401.17755](https://arxiv.org/abs/2401.17755)

    CauESC是一种关注因果关系的情感支持对话模型，通过识别困扰的情感原因以及这些原因触发的情感效果，独立地理解每种语言修饰策略并巧妙地整合它们，有效地减轻求助者的情感困扰。

    

    情感支持对话旨在通过支持性的回应减轻求助者的情感困扰。现有方法存在两个限制：(1) 它们忽略了困扰的情感原因，而这对于细致的情感理解是重要的；(2) 它们关注的是求助者自身的心理状态，而忽视了说话者之间互动过程中的情感动态。为了解决这些问题，我们提出了一个新颖的框架CauESC，它首先识别困扰的情感原因，以及这些原因触发的情感效果，然后独立地理解每种语言修饰策略，并巧妙地将它们整合起来。基准数据集上的实验结果证明了我们方法的有效性，并展示了从原因到效果的情感理解和独立-整合策略建模的好处。

    Emotional Support Conversation aims at reducing the seeker's emotional distress through supportive response. Existing approaches have two limitations: (1) They ignore the emotion causes of the distress, which is important for fine-grained emotion understanding; (2) They focus on the seeker's own mental state rather than the emotional dynamics during interaction between speakers. To address these issues, we propose a novel framework CauESC, which firstly recognizes the emotion causes of the distress, as well as the emotion effects triggered by the causes, and then understands each strategy of verbal grooming independently and integrates them skillfully. Experimental results on the benchmark dataset demonstrate the effectiveness of our approach and show the benefits of emotion understanding from cause to effect and independent-integrated strategy modeling.
    
[^25]: 利用分解推理增强大型语言模型来提取情感原因对

    Enhancing Large Language Model with Decomposed Reasoning for Emotion Cause Pair Extraction

    [https://arxiv.org/abs/2401.17716](https://arxiv.org/abs/2401.17716)

    利用大型语言模型和分解推理，我们提出了一种名为DECC的框架，用于从文档中提取情感和原因的从句对。实验结果表明，DECC在ECPE任务上表现出比最先进的有监督微调方法更强大的能力。

    

    情感原因对提取（ECPE）涉及在文档中提取代表情感和原因的从句对。现有方法往往过度拟合存在的基准数据集中的偶然相关性，如位置偏差，而不是捕捉语义特征。受最近的研究启发，我们探索利用大型语言模型（LLM）在不额外训练的情况下解决ECPE任务。尽管具有强大的能力，LLM存在无法控制的输出问题，导致性能中等。为了解决这个问题，我们引入了“思维链”来模仿人类的认知过程，并提出了分解情感-原因链（DECC）框架。通过融合引导推理和逻辑修剪的方法，DECC引导LLM处理ECPE任务。我们进一步通过引入上下文学习来增强框架。实验结果表明，DECC相比最先进的有监督微调方法更强大。最后，我们分析了每个组成部分的有效性和框架的鲁棒性。

    Emotion-Cause Pair Extraction (ECPE) involves extracting clause pairs representing emotions and their causes in a document. Existing methods tend to overfit spurious correlations, such as positional bias in existing benchmark datasets, rather than capturing semantic features. Inspired by recent work, we explore leveraging large language model (LLM) to address ECPE task without additional training. Despite strong capabilities, LLMs suffer from uncontrollable outputs, resulting in mediocre performance. To address this, we introduce chain-of-thought to mimic human cognitive process and propose the Decomposed Emotion-Cause Chain (DECC) framework. Combining inducing inference and logical pruning, DECC guides LLMs to tackle ECPE task. We further enhance the framework by incorporating in-context learning. Experiment results demonstrate the strength of DECC compared to state-of-the-art supervised fine-tuning methods. Finally, we analyze the effectiveness of each component and the robustness of
    
[^26]: WSC+: 使用专家树增强Winograd Schema挑战

    WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts

    [https://arxiv.org/abs/2401.17703](https://arxiv.org/abs/2401.17703)

    WSC+提出了一种新的提示方法-专家树，增强了Winograd Schema挑战中问题的生成能力。通过引入新的数据集和扩展框架，WSC+揭示了模型的过度自信和偏见，并发现大型语言模型在评估自己生成的问题时并不总是优于其他模型。在WSC+上，当前最好的大型语言模型GPT-4的准确率为68.7%。

    

    Winograd Schema Challenge (WSC)是评估机器理解能力的重要基准。虽然大型语言模型在回答WSC问题方面表现出色，但它们生成这类问题的能力还未得到充分探索。在这项工作中，我们提出了一种新的提示方法-专家树（Tree-of-Experts，ToE），它增强了WSC实例的生成能力（50%有效案例，相比最新方法的10%）。利用这种方法，我们引入了WSC+，一个包含3026个大型语言模型生成句子的新数据集。值得注意的是，我们扩展了WSC框架，加入了新的“模棱两可”（ambiguous）和“冒犯性”（offensive）的类别，从而更深入地了解模型过度自信和偏见。我们的分析揭示了生成-评估一致性方面的细微差别，表明在评估自己生成的问题时，大型语言模型可能并不总是优于其他模型所创建的问题。在WSC+上，GPT-4，即表现最好的大型语言模型，准确率为68.7%，明显低于人类准确率95.1%的基准。

    The Winograd Schema Challenge (WSC) serves as a prominent benchmark for evaluating machine understanding. While Large Language Models (LLMs) excel at answering WSC questions, their ability to generate such questions remains less explored. In this work, we propose Tree-of-Experts (ToE), a novel prompting method which enhances the generation of WSC instances (50% valid cases vs. 10% in recent methods). Using this approach, we introduce WSC+, a novel dataset comprising 3,026 LLM-generated sentences. Notably, we extend the WSC framework by incorporating new 'ambiguous' and 'offensive' categories, providing a deeper insight into model overconfidence and bias. Our analysis reveals nuances in generation-evaluation consistency, suggesting that LLMs may not always outperform in evaluating their own generated questions when compared to those crafted by other models. On WSC+, GPT-4, the top-performing LLM, achieves an accuracy of 68.7%, significantly below the human benchmark of 95.1%.
    
[^27]: 用上下文外推缓解语言模型中强先验问题的方法

    Mitigating the Problem of Strong Priors in LMs with Context Extrapolation

    [https://arxiv.org/abs/2401.17692](https://arxiv.org/abs/2401.17692)

    本论文提出了一种缓解语言模型中强先验问题的新技术，通过削弱原始提示并进行上下文外推，以减少模型受到强先验问题的影响。

    

    语言模型（LMs）已成为各种应用程序中重要的工具，从数据处理到创建指令跟随助手。但是尽管它们有优势，LMs还有一些特殊的局限性，比如“强先验”问题，其中模型会在对某些局部输入的响应中学习输出典型的延续，而不考虑之前的指令。例如，prompt注入攻击可以诱使模型忽略显式指令。在某些情况下，大型模型被证明比类似的较小模型更容易受到这些问题的影响，这是“反向缩放”现象的一个例子。我们开发了一种缓解强先验问题的新技术：我们采用原始指令集，生成原始提示的削弱版本，使其更容易受到强先验问题的影响，然后将延续外推远离削弱的提示。这让我们可以推断模型如何对上下文进行理解并产生输出。

    Language models (LMs) have become important tools in a variety of applications, from data processing to the creation of instruction-following assistants. But despite their advantages, LMs have certain idiosyncratic limitations such as the problem of `strong priors', where a model learns to output typical continuations in response to certain, usually local, portions of the input regardless of any earlier instructions. For example, prompt injection attacks can induce models to ignore explicit directives. In some cases, larger models have been shown to be more susceptible to these problems than similar smaller models, an example of the phenomenon of `inverse scaling'. We develop a new technique for mitigating the problem of strong priors: we take the original set of instructions, produce a weakened version of the original prompt that is even more susceptible to the strong priors problem, and then extrapolate the continuation away from the weakened prompt. This lets us infer how the model 
    
[^28]: 推理束搜索：为链式思维推断寻找可推导的理由

    Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning

    [https://arxiv.org/abs/2401.17686](https://arxiv.org/abs/2401.17686)

    本文提出了一种称为推理束搜索（DBS）的方法，将链式思维和演绎推理与逐步束搜索无缝集成到大型语言模型（LLMs）中，通过引入验证器来减少错误的累积，并通过可扩展和无需人工劳动的数据构建方法提升模型的验证能力。

    

    最近的研究通过各种方法，尤其是链式思维推理，极大增强了大型语言模型（LLMs）的推理能力。然而，以往的方法未能解决中间步骤的推理错误问题，导致错误的累积。本文提出了一种称为推理束搜索（DBS）的方法，它将链式思维和演绎推理与逐步束搜索无缝集成到LLMs中。我们的方法部署了一个验证器，用于验证推理步骤及其前提的可推导性，从而减少错误的累积。此外，我们引入了一种可扩展且无需人工劳动的数据构建方法，来增强我们模型的验证能力。广泛的实验证明，我们的方法显著提升了各种规模的LLMs（7B、13B、70B和ChatGPT）的基础性能，在3种不同的推理场景（算术、常识和符号）的8个推理数据集中都表现出色。此外，我们的分析证明了

    Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves
    
[^29]: 在大型语言模型和大脑中，上下文特征提取层次收敛

    Contextual Feature Extraction Hierarchies Converge in Large Language Models and the Brain

    [https://arxiv.org/abs/2401.17671](https://arxiv.org/abs/2401.17671)

    在研究中发现，随着大型语言模型在基准任务上的性能提高，模型不仅在预测神经响应时表现出更高的类脑性能，而且其分层特征提取路径与大脑的映射更接近，并且使用更少的层次来进行相同的编码。

    

    最近在人工智能方面的进展引发了对大型语言模型（LLM）和人类神经处理之间相似性的兴趣，特别是在语言理解方面。虽然之前的研究已经确定了LLM表示和大脑之间的相似之处，但在演化的LLM背景下引发这种收敛的潜在计算原理仍然不清楚。在这里，我们对一系列性能较高的LLM进行了研究，这些模型的参数大小相似，以探究导致其与大脑语言处理机制一致的因素。我们发现，当LLM在基准任务上达到更高的性能时，它们不仅在预测LLM嵌入的神经响应时表现出更高的类脑性能，而且它们的分层特征提取路径与大脑的映射更接近，并且使用更少的层次来进行相同的编码。

    Recent advancements in artificial intelligence have sparked interest in the parallels between large language models (LLMs) and human neural processing, particularly in language comprehension. While prior research has established similarities in the representation of LLMs and the brain, the underlying computational principles that cause this convergence, especially in the context of evolving LLMs, remain elusive. Here, we examined a diverse selection of high-performance LLMs with similar parameter sizes to investigate the factors contributing to their alignment with the brain's language processing mechanisms. We find that as LLMs achieve higher performance on benchmark tasks, they not only become more brain-like as measured by higher performance when predicting neural responses from LLM embeddings, but also their hierarchical feature extraction pathways map more closely onto the brain's while using fewer layers to do the same encoding. We also compare the feature extraction pathways of 
    
[^30]: 长文档转换器中的文档结构

    Document Structure in Long Document Transformers

    [https://arxiv.org/abs/2401.17658](https://arxiv.org/abs/2401.17658)

    长文档Transformer模型在预训练期间获得了对文档结构的隐性理解，并且结构注入可以进一步增强这种理解，提高下游任务的性能。

    

    长文档通常呈现出具有不同功能的层次化组织元素，例如章节标题和段落。尽管文档结构普遍存在，但其在自然语言处理（NLP）中的作用仍然不清楚。长文档Transformer模型在预训练期间是否会掌握文档结构的内部表示？在预训练后，如何向模型传达结构信息，并且它如何影响下游性能？为了回答这些问题，我们开发了一套新的探测任务来评估长文档Transformer的结构感知能力，提出了通用的结构注入方法，并评估了结构注入对于QASPER和Evidence Inference这两个具有挑战性的长文档NLP任务的影响。LED和LongT5上的结果表明，它们在预训练期间获得了对文档结构的隐性理解，而结构注入可以进一步增强这种理解，从而提高了最终的任务性能。

    Long documents often exhibit structure with hierarchically organized elements of different functions, such as section headers and paragraphs. Despite the omnipresence of document structure, its role in natural language processing (NLP) remains opaque. Do long-document Transformer models acquire an internal representation of document structure during pre-training? How can structural information be communicated to a model after pre-training, and how does it influence downstream performance? To answer these questions, we develop a novel suite of probing tasks to assess structure-awareness of long-document Transformers, propose general-purpose structure infusion methods, and evaluate the effects of structure infusion on QASPER and Evidence Inference, two challenging long-document NLP tasks. Results on LED and LongT5 suggest that they acquire implicit understanding of document structure during pre-training, which can be further enhanced by structure infusion, leading to improved end-task pe
    
[^31]: 在大型语言模型中解决过度杀伤问题的导航

    Navigating the OverKill in Large Language Models

    [https://arxiv.org/abs/2401.17633](https://arxiv.org/abs/2401.17633)

    本研究调查了大型语言模型中过度杀伤的因素，并发现了其中存在的捷径和对有害词语的过度关注。我们提出了自对比解码（Self-CD）策略来缓解过度杀伤现象，该策略无需训练且适用于各种模型。

    

    大型语言模型被精心调整，以既有助益又无害。然而，最近的研究指出存在潜在的过度杀伤问题，这意味着模型可能会拒绝回答无害查询。本文通过探索模型如何处理和确定查询的安全性，来研究过度杀伤的因素。我们的发现揭示了模型内部存在捷径，导致对“杀伤”等有害词语过度关注，而强调安全性的提示将加剧过度杀伤。基于这些发现，我们提出了一种无需训练且适用于各种模型的策略，名为自对比解码（Self-Contrastive Decoding，Self-CD），来缓解这一现象。首先，我们通过放大模型在回应系统提示时输出分布的差异，提取这种过度关注。然后，通过对比解码来减弱模型对这种过度关注的影响，以确定最终的下一个标记预测。实证结果表明，我们的方法具有

    Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to an over-attention of harmful words like 'kill' and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such over-attention by amplifying the difference in the model's output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the over-attention from the model via contrastive decoding. Empirical results indicate that our method has
    
[^32]: 自监督语音和说话者模型学习了什么？来自跨模型层面分析的新发现

    What Do Self-Supervised Speech and Speaker Models Learn? New Findings From a Cross Model Layer-Wise Analysis

    [https://arxiv.org/abs/2401.17632](https://arxiv.org/abs/2401.17632)

    本研究通过应用SUPERB评估探测任务，研究了自监督学习模型中捕捉语音特性的能力，并比较了语音和说话者模型之间的差异。

    

    自监督学习在学习有意义的语音表示方面引起了越来越多的关注。语音自监督学习模型，如WavLM，采用掩码预测训练来编码通用表示。与之相反，说话者自监督学习模型，例如基于DINO的模型，采用基于语句的训练目标主要用于说话者表示。了解这些模型如何表示信息对于改进模型的效率和效果至关重要。与语音自监督学习的各种分析不同，对于说话者自监督学习模型捕捉了哪些信息以及其表示与语音自监督学习或其他完全监督的说话者模型有何不同的研究有限。本文解决了这些基本问题。我们通过将SUPERB评估探测任务应用于语音和说话者自监督学习模型来探索捕捉各种语音属性的能力。我们还研究了每个任务中主要使用的层，以确定语音和说话者模型在表示上的差异。

    Self-supervised learning (SSL) has attracted increased attention for learning meaningful speech representations. Speech SSL models, such as WavLM, employ masked prediction training to encode general-purpose representations. In contrast, speaker SSL models, exemplified by DINO-based models, adopt utterance-level training objectives primarily for speaker representation. Understanding how these models represent information is essential for refining model efficiency and effectiveness. Unlike the various analyses of speech SSL, there has been limited investigation into what information speaker SSL captures and how its representation differs from speech SSL or other fully-supervised speaker models. This paper addresses these fundamental questions. We explore the capacity to capture various speech properties by applying SUPERB evaluation probing tasks to speech and speaker SSL models. We also examine which layers are predominantly utilized for each task to identify differences in how speech i
    
[^33]: 大型语言模型上邻近扰动的知识编辑

    Neighboring Perturbations of Knowledge Editing on Large Language Models

    [https://arxiv.org/abs/2401.17623](https://arxiv.org/abs/2401.17623)

    本文研究大型语言模型上知识编辑对邻近知识的扰动，提出了 additivity 指标以及 Perturbation Evaluation of Appending Knowledge (PEAK) 基准，用于评估附加新知识时邻近知识的扰动程度。

    

    尽管大型语言模型（LLMs）具有卓越的能力，但由于错误或过时的知识，它们容易生成意外的文本。考虑到重新训练LLMs的资源密集型性质，知识编辑的发展呈现出显著增长。然而，目前的方法和评估很少探索编辑对相邻知识的扰动。本文研究了将新知识更新到LLMs中是否扰乱了其中包含的相邻知识。具体而言，我们探讨了将新的答案附加到事实性问题的答案列表中是否会导致原始正确答案的丧失，以及不经意地包含了错误答案。引入了一个加性指标，并构建了一个被称为知识附加扰动评估（PEAK）的基准来评估附加新知识时邻近知识的扰动程度。此外，引入了一个即插即用的机制，用于处理新增知识的效果和扰动。

    Despite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play 
    
[^34]: 在上下文学习的背景下，利用LoRA微调进行断言检测的大型语言模型

    Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning

    [https://arxiv.org/abs/2401.17602](https://arxiv.org/abs/2401.17602)

    在临床自然语言处理中，本研究提出了一种利用LoRA微调的大型语言模型来解决断言检测任务，该任务是从临床笔记中提取医学概念时的关键步骤，准确的断言类型识别对于医疗专业人员理解患者状况至关重要。

    

    本研究旨在解决临床自然语言处理（NLP）中从临床笔记中提取医学概念时的断言检测任务，这是临床NLP中的关键过程。临床NLP中的断言检测通常涉及在临床文本中为医学概念识别断言类型，即确定性（医学概念是否为积极、否定、可能或假设），时间性（医学概念是指当前还是过去历史）和经验者（医学概念是指患者还是家庭成员）。这些断言类型对于医护人员能够快速明确地从非结构化临床文本中理解医学状况的上下文至关重要，直接影响患者护理质量和结果。尽管被广泛使用，但传统方法，特别是基于规则的NLP系统和机器学习或深度学习模型，需要大量人工努力创建模式，并且往往忽视了

    In this study, we aim to address the task of assertion detection when extracting medical concepts from clinical notes, a key process in clinical natural language processing (NLP). Assertion detection in clinical NLP usually involves identifying assertion types for medical concepts in the clinical text, namely certainty (whether the medical concept is positive, negated, possible, or hypothetical), temporality (whether the medical concept is for present or the past history), and experiencer (whether the medical concept is described for the patient or a family member). These assertion types are essential for healthcare professionals to quickly and clearly understand the context of medical conditions from unstructured clinical texts, directly influencing the quality and outcomes of patient care. Although widely used, traditional methods, particularly rule-based NLP systems and machine learning or deep learning models, demand intensive manual efforts to create patterns and tend to overlook 
    
[^35]: 在地球观测数据上对GPT-4V进行标注任务评估：对语言视觉模型在视觉任务上的一项基准测试

    Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data

    [https://arxiv.org/abs/2401.17600](https://arxiv.org/abs/2401.17600)

    在这项研究中，我们对GPT-4V模型在地球观测数据上的性能进行了评估。结果显示，尽管GPT-4V在开放式任务如位置理解和图像标注方面表现良好，但其空间推理能力不足，限制了其在目标定位和计数方面的实用性。

    

    大型语言视觉模型（VLMs）在涉及自然语言指令和视觉输入的复杂任务上展示出令人印象深刻的性能。然而，目前尚不清楚这些模型在以卫星和航空图像为主的地球观测（EO）数据上的能力，这类数据在VLMs的训练数据中较为罕见。在这项研究中，我们提出了一个全面的基准测试，通过评估VLMs在场景理解、定位和计数以及变化检测任务上的能力，以衡量它们在EO数据上作为有效工具的进展。受现实世界应用的启发，我们的基准测试包括了城市监测、灾害救援、土地利用和保护等场景。我们发现，尽管像GPT-4V这样的最新VLMs具有丰富的世界知识，导致在位置理解和图像标注等开放式任务上表现强劲，但是它们的空间推理能力不足，限制了它们在目标定位和计数方面的实用性。

    Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting
    
[^36]: SPECTRUM: 增强说话者先训练用于长对话摘要

    SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization

    [https://arxiv.org/abs/2401.17597](https://arxiv.org/abs/2401.17597)

    本文提出了一种用于长对话摘要的增强说话者先训练方法，通过利用多轮对话的内在结构，帮助语言模型更好地理解和处理长对话摘要任务，实现了最先进的性能。

    

    多轮对话以其扩展长度和交替发言的特点而闻名。传统的语言模型通常将这些对话视为普通文本，忽视了其独特的特点。本研究提出了一种用于长对话摘要的增强说话者先训练方法，利用多轮对话的内在结构。为了支持我们的研究，我们收集了一个多样化的数据集，包括真实场景的对话记录、电影或电视剧剧本以及大型语言模型生成的对话。然后进行了预训练，其中包括发言者变更的检测和掩码话语生成。经过微调的模型的实验结果表明，我们的模型在带有长上下文的下游基准测试上实现了最先进的性能，超过了基线模型，并突显了我们方法的有效性。我们的研究结果凸显了策划预训练数据集的重要性。

    Multi-turn dialogues are characterized by their extended length and the presence of turn-taking conversations. Traditional language models often overlook the distinct features of these dialogues by treating them as regular text. In this paper, we propose a speaker-enhanced pre-training method for long dialogue summarization, which leverages the inherent structure of multiple-turn dialogues. To support our study, we curate a diverse dataset that includes transcripts from real-world scenarios, movie or TV show transcripts, and dialogues generated by a Large Language Model. We then perform a pre-training, which encompasses the detection of speaker changes, and masked utterance generation. Experimental results of fine-tuned models demonstrate that our model achieves state-of-the-art performance on downstream benchmarks with long context, surpassing baseline models and highlighting the effectiveness of our approach. Our findings highlight the importance of curating pre-training datasets tha
    
[^37]: 对话的本地和全局环境

    Local and Global Contexts for Conversation

    [https://arxiv.org/abs/2401.17588](https://arxiv.org/abs/2401.17588)

    本文引入了一个本地和全局对话模型（LGCM），它是一个本地-全局层次转换器模型，能够准确区分和融合生成回应所需的相关上下文。通过无缝融合本地和全局上下文，模型能够更好地进行对话生成。

    

    对话中的上下文是多轮对话中至关重要的对话历史。学习通过对话历史中的相关上下文进行对话是一个具有挑战性的问题。本地上下文是与后续回应更接近且更敏感的最近邻上下文，而全局上下文与整个对话相关，远超出邻近的话语。目前，用于对话的预训练转换器模型无法准确捕捉本地和全局上下文之间的相关性和连接。我们引入了一个用于开放领域中的通用对话的本地和全局对话模型（LGCM）。它是一个本地-全局层次转换器模型，能够准确区分和融合生成回应所需的相关上下文。它使用本地编码器来获取个别话语层面的本地上下文，使用全局编码器来理解对话层面的更广泛上下文。通过无缝融合这些本地和全局上下文，模型能够更好地进行对话生成。

    The context in conversation is the dialog history crucial for multi-turn dialogue. Learning from the relevant contexts in dialog history for grounded conversation is a challenging problem. Local context is the most neighbor and more sensitive to the subsequent response, and global context is relevant to a whole conversation far beyond neighboring utterances. Currently, pretrained transformer models for conversation challenge capturing the correlation and connection between local and global contexts. We introduce a local and global conversation model (LGCM) for general-purpose conversation in open domain. It is a local-global hierarchical transformer model that excels at accurately discerning and assimilating the relevant contexts necessary for generating responses. It employs a local encoder to grasp the local context at the level of individual utterances and a global encoder to understand the broader context at the dialogue level. The seamless fusion of these locally and globally cont
    
[^38]: 传播与陷阱：通过反事实任务评估基于推理的知识编辑的困境

    Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks

    [https://arxiv.org/abs/2401.17585](https://arxiv.org/abs/2401.17585)

    该论文针对现有的知识编辑方法在推理能力方面的限制，通过引入ReCoE数据集进行了深入分析。研究发现所有的模型编辑方法在该数据集上表现较差，尤其在特定的推理方案中。此外，通过对编辑模型思维链生成的分析，揭示了现有方法的不足之处，包括对事实编辑、事实回忆能力和连贯性的考量。

    

    当前的知识编辑方法在有效传播更新的相互关联事实方面面临困难。在这项工作中，我们深入探讨了阻碍准确推理模型中更新知识适当传播的障碍。为了支持我们的分析，我们引入了一种新颖的基于推理的基准——ReCoE（基于推理的反事实编辑数据集），涵盖了现实世界中的六种常见推理方案。我们对现有的知识编辑技术进行了全面分析，包括输入增强、微调和定位编辑。我们发现所有模型编辑方法在这个数据集上的表现都明显较低，尤其是在某些推理方案中。我们通过对编辑模型的思维链生成的分析，从推理的角度揭示了现有知识编辑方法不足的关键原因，包括对事实编辑、事实回忆能力以及生成的连贯性的方面。我们将公开我们的基准数据集和代码。

    Current approaches of knowledge editing struggle to effectively propagate updates to interconnected facts. In this work, we delve into the barriers that hinder the appropriate propagation of updated knowledge within these models for accurate reasoning. To support our analysis, we introduce a novel reasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing dataset) -- which covers six common reasoning schemes in real world. We conduct a thorough analysis of existing knowledge editing techniques, including input augmentation, finetuning, and locate-and-edit. We found that all model editing methods show notably low performance on this dataset, especially in certain reasoning schemes. Our analysis over the chain-of-thought generation of edited models further uncover key reasons behind the inadequacy of existing knowledge editing methods from a reasoning standpoint, involving aspects on fact-wise editing, fact recall ability, and coherence in generation. We will make our ben
    
[^39]: Scavenging Hyena: 将Transformer模型精炼为长卷积模型

    Scavenging Hyena: Distilling Transformers into Long Convolution Models

    [https://arxiv.org/abs/2401.17574](https://arxiv.org/abs/2401.17574)

    本文介绍了一种通过使用知识蒸馏将Transformer模型中的注意力头替换为Hyena，从而提高效率并处理长上下文信息的方法，超越了传统预训练方法，在准确性和效率方面取得了优秀的结果。这一技术为追求可持续的AI解决方案做出了贡献，实现了计算能力和环境影响的平衡。

    

    大型语言模型（LLMs）的快速发展，以GPT-4等架构为典范，重塑了自然语言处理的领域。本文介绍了一种开创性方法，用于解决LLM预训练中的效率问题，提出了使用知识蒸馏进行跨架构迁移的方法。借鉴高效的Hyena机制的见解，我们的方法通过使用Hyena来替换Transformer模型中的注意力头，提供了一种经济有效的替代传统预训练的方法，同时要面对处理长上下文信息的挑战，这是二次注意力机制固有的。与传统的压缩方法不同，我们的技术不仅提高了推理速度，还在准确性和效率方面超越了预训练。在LLM不断发展的时代，我们的工作为追求可持续的人工智能解决方案作出了贡献，实现了计算能力和环境影响之间的平衡。

    The rapid evolution of Large Language Models (LLMs), epitomized by architectures like GPT-4, has reshaped the landscape of natural language processing. This paper introduces a pioneering approach to address the efficiency concerns associated with LLM pre-training, proposing the use of knowledge distillation for cross-architecture transfer. Leveraging insights from the efficient Hyena mechanism, our method replaces attention heads in transformer models by Hyena, offering a cost-effective alternative to traditional pre-training while confronting the challenge of processing long contextual information, inherent in quadratic attention mechanisms. Unlike conventional compression-focused methods, our technique not only enhances inference speed but also surpasses pre-training in terms of both accuracy and efficiency. In the era of evolving LLMs, our work contributes to the pursuit of sustainable AI solutions, striking a balance between computational power and environmental impact.
    
[^40]: PipeNet:在知识图谱上使用语义修剪的问题回答

    PipeNet: Question Answering with Semantic Pruning over Knowledge Graphs

    [https://arxiv.org/abs/2401.17536](https://arxiv.org/abs/2401.17536)

    本论文提出了PipeNet方法，通过语义修剪技术在知识图谱上进行问题回答。该方法通过关联-修剪-推理的流程来修剪噪声节点，以提高图推理的效率，同时获得良好的子图表示。

    

    众所周知，在问题回答中引入显式的知识图谱(KG)可以带来好处。现有的方法通常遵循一个基础推理流程，在该流程中，首先将实体节点与查询(问题和候选答案)进行关联，然后使用推理模块对匹配的多跳子图进行推理，用于预测答案。虽然这个流程在从庞大的KG中提取必要信息方面取得了很大的改善，但在放大关联的子图时，效率仍然面临挑战。本文旨在找到子图中的语义相关的实体节点，以提高使用KG进行图推理时的效率。我们提出了一个基于关联-修剪-推理的流程来修剪噪声节点，在显著降低计算和内存使用的同时，还能获得良好的子图表示。具体来说，修剪模块首先根据匹配范围之间的依赖距离对概念节点进行评分，然后对其进行修剪。

    It is well acknowledged that incorporating explicit knowledge graphs (KGs) can benefit question answering. Existing approaches typically follow a grounding-reasoning pipeline in which entity nodes are first grounded for the query (question and candidate answers), and then a reasoning module reasons over the matched multi-hop subgraph for answer prediction. Although the pipeline largely alleviates the issue of extracting essential information from giant KGs, efficiency is still an open challenge when scaling up hops in grounding the subgraphs. In this paper, we target at finding semantically related entity nodes in the subgraph to improve the efficiency of graph reasoning with KG. We propose a grounding-pruning-reasoning pipeline to prune noisy nodes, remarkably reducing the computation cost and memory usage while also obtaining decent subgraph representation. In detail, the pruning module first scores concept nodes based on the dependency distance between matched spans and then prunes 
    
[^41]: FEUDA：令人沮丧地简单的基于提示的无监督领域自适应

    FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation

    [https://arxiv.org/abs/2401.17514](https://arxiv.org/abs/2401.17514)

    FEUDA是一种令人沮丧地简单的无监督领域自适应方法，通过在未标记和标记的示例上训练自回归语言模型，在提示基础的分类框架中探索无监督领域自适应的新范例。

    

    无监督领域自适应方法的一个主要分支利用来自源领域和目标领域的未标记数据，学习适应的领域不变表示。然而，这些方法存在一定的局限性，鼓励通过持续的预训练使用自监督学习。在基于提示的分类框架中，持续的预训练或学习领域不变表示的必要性仍不清楚，其中一个输入示例由模板修改后，再输入到语言模型（LM）中生成一个标签字符串。为了研究基于提示的无监督领域自适应中的这种新范例，我们提出了一种令人沮丧地简单的无监督领域自适应方法（FEUDA），该方法使用两种不同的指令调整任务，在未标记和标记的示例上训练自回归LM。具体而言，第一个任务通过掩蔽语言建模（MLM）在两个领域的未标记文本上训练LM，第二个任务使用源标记数据进行监督指令调整。

    A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled data from both source and target domains to learn domain-invariant representations for adaptation. However, these methods showcase certain limitations, encouraging the use of self-supervised learning through continued pre-training. The necessity of continued pre-training or learning domain-invariant representations is still unclear in the prompt-based classification framework, where an input example is modified by a template and then fed into a language model (LM) to generate a label string. To examine this new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and labeled examples using two different instruction-tuning tasks. Specifically, the first task trains the LM on unlabeled texts from both domains via masked language modeling (MLM), and the other uses supervised instruction-tuning on source-labeled data for c
    
[^42]: 在面向患者的风险预测模型中，语言表达不确定性的问题

    Linguistically Communicating Uncertainty in Patient-Facing Risk Prediction Models

    [https://arxiv.org/abs/2401.17511](https://arxiv.org/abs/2401.17511)

    本文讨论了在医疗保健领域中面向患者的风险预测模型中不确定性量化的挑战，并提出了一种设计来应对这些挑战，重点关注体外受精结果预测的具体应用。

    

    本文讨论了在医疗保健领域中，应用于面向患者环境中的人工智能模型中不确定性量化所面临的独特挑战。与为模型开发者或领域专家量身定制的传统可解释人工智能（XAI）方法不同，这里需要考虑自然语言的表达、展示和评估可理解性的附加因素。我们在风险预测的语境下识别了在自然语言中沟通模型性能、置信度、推理和未知已知的挑战。我们提出了一个旨在应对这些挑战的设计，重点关注体外受精结果预测的具体应用。

    This paper addresses the unique challenges associated with uncertainty quantification in AI models when applied to patient-facing contexts within healthcare. Unlike traditional eXplainable Artificial Intelligence (XAI) methods tailored for model developers or domain experts, additional considerations of communicating in natural language, its presentation and evaluating understandability are necessary. We identify the challenges in communication model performance, confidence, reasoning and unknown knowns using natural language in the context of risk prediction. We propose a design aimed at addressing these challenges, focusing on the specific application of in-vitro fertilisation outcome prediction.
    
[^43]: 大型语言模型中的时间箭头

    Arrows of Time for Large Language Models

    [https://arxiv.org/abs/2401.17505](https://arxiv.org/abs/2401.17505)

    这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。

    

    我们通过时间方向性的视角研究了自回归大型语言模型的概率建模。我们在实证上发现这类模型在建模自然语言能力上存在时间上的不对称性：预测下一个记号和预测前一个记号时的平均对数困惑度存在差异。这种差异既微妙又在不同的模态（语言、模型大小、训练时间等）下非常一致。从信息理论的角度来看，这在理论上是令人惊讶的，不应该存在这样的差异。我们提供了一个理论框架，解释了这种不对称性如何出现在稀疏性和计算复杂性考虑中，并概述了我们的结果带来的一些展望。

    We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
    
[^44]: 通过地图接种改进QA模型性能

    Improving QA Model Performance with Cartographic Inoculation

    [https://arxiv.org/abs/2401.17498](https://arxiv.org/abs/2401.17498)

    本文提出了一种名为地图接种的新方法，通过在优化的挑战数据子集上对QA模型进行微调，减少模型对数据集伪迹的依赖性，从而显著提高模型在复杂和开放的上下文推理问题上的性能。

    

    QA模型面临着复杂而开放的上下文推理问题，但通常可以通过利用训练数据中特定于数据集的模式来学习高性能的解决启发式方法。这些模式，或者称为"数据集伪迹"，降低了模型在现实世界QA问题上的泛化能力。利用训练用于QA的ElectraSmallDiscriminator模型，我们使用一组对抗性挑战数据集分析了数据集伪迹的影响和发生情况，该数据集旨在混淆依赖于数据集伪迹进行预测的模型。在现有减轻伪迹影响方法的基础上，我们提出了一种新颖的方法，即地图接种，通过在优化的挑战数据子集上对模型进行微调，以减少模型对数据集伪迹的依赖性。我们证明，通过有选择地在挑战数据集中的模棱两可的对抗性示例上进行模型微调，可以在最小损失模型对其他挑战数据集的泛化能力的情况下，大大提高模型在整个挑战数据集上的性能。

    QA models are faced with complex and open-ended contextual reasoning problems, but can often learn well-performing solution heuristics by exploiting dataset-specific patterns in their training data. These patterns, or "dataset artifacts", reduce the model's ability to generalize to real-world QA problems. Utilizing an ElectraSmallDiscriminator model trained for QA, we analyze the impacts and incidence of dataset artifacts using an adversarial challenge set designed to confuse models reliant on artifacts for prediction. Extending existing work on methods for mitigating artifact impacts, we propose cartographic inoculation, a novel method that fine-tunes models on an optimized subset of the challenge data to reduce model reliance on dataset artifacts. We show that by selectively fine-tuning a model on ambiguous adversarial examples from a challenge set, significant performance improvements can be made on the full challenge dataset with minimal loss of model generalizability to other chal
    
[^45]: 在社交媒体上检测心理障碍：基于ChatGPT的可解释方法

    Detecting mental disorder on social media: a ChatGPT-augmented explainable approach

    [https://arxiv.org/abs/2401.17477](https://arxiv.org/abs/2401.17477)

    本文提出了一种利用大型语言模型，可解释人工智能和对话代理器ChatGPT相结合的新方法，以解决通过社交媒体检测抑郁症的可解释性挑战。通过将Twitter特定变体BERTweet与自解释模型BERT-XDD相结合，并借助ChatGPT将技术解释转化为人类可读的评论，实现了解释能力的同时提高了可解释性。这种方法可以为发展社会负责任的数字平台，促进早期干预做出贡献。

    

    在数字时代，社交媒体上表达的抑郁症状的频率引起了严重关注，迫切需要先进的方法来及时检测。本文通过提出一种新颖的方法，将大型语言模型（LLM）与可解释的人工智能（XAI）和ChatGPT等对话代理器有效地结合起来，以应对可解释性抑郁症检测的挑战。在我们的方法中，通过将Twitter特定变体BERTweet与一种新型的自解释模型BERT-XDD相结合，实现了解释能力，该模型能够通过掩码注意力提供分类和解释。使用ChatGPT将技术解释转化为可读性强的评论，进一步增强了可解释性。通过引入一种有效且模块化的可解释抑郁症检测方法，我们的方法可以为发展社会负责任的数字平台做出贡献，促进早期干预。

    In the digital era, the prevalence of depressive symptoms expressed on social media has raised serious concerns, necessitating advanced methodologies for timely detection. This paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and conversational agents like ChatGPT. In our methodology, explanations are achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a novel self-explanatory model, namely BERT-XDD, capable of providing both classification and explanations via masked attention. The interpretability is further enhanced using ChatGPT to transform technical explanations into human-readable commentaries. By introducing an effective and modular approach for interpretable depression detection, our methodology can contribute to the development of socially responsible digital platforms, fostering early intervention and
    
[^46]: 使用抽象链推理的高效工具使用

    Efficient Tool Use with Chain-of-Abstraction Reasoning

    [https://arxiv.org/abs/2401.17464](https://arxiv.org/abs/2401.17464)

    该方法通过让LLM首先解码抽象推理链，然后调用领域工具填充具体知识，使得LLM能够更好地利用工具进行多步推理，并且具有通用性和鲁棒性。

    

    为了实现与人类期望一致的准确推理，大型语言模型（LLM）需要将推理与现实世界的知识（例如网络事实、数学和物理规则）联系起来。工具可以帮助LLM获取这些外部知识，但是在多步推理问题中仍然存在挑战，即如何精细调整LLM代理（例如Toolformer）以调用工具，其中相互连接的工具调用需要整体化和高效的工具使用规划。在这项工作中，我们提出了一种新的方法，让LLM在多步推理中更好地利用工具。我们的方法是通过训练LLM首先用抽象占位符解码推理链，然后调用领域工具以填充具体知识来实现每个推理链。抽象链的规划使LLM能够学习更通用的推理策略，对于与不同推理问题相关的领域知识（例如数学结果）的变化具有鲁棒性。它还允许LLM执行解码操作

    To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.   In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding a
    
[^47]: 使用LLM智能体生成合成对话数据集

    Synthetic Dialogue Dataset Generation using LLM Agents

    [https://arxiv.org/abs/2401.17461](https://arxiv.org/abs/2401.17461)

    本文提出了使用LLM智能体生成合成对话数据集的方法，通过对话智能体和用户进行交流来获取生成线性模型所需的关键信息，并提出了对话的外部评估方法。

    

    线性规划问题在现实生活中应用广泛。然而，尽管表面上简单，但未经训练的用户可能难以确定其特定问题的线性模型。我们设想创建一个目标导向的对话智能体，与用户进行交流，以获取生成线性模型所需的所有信息，从而建立一个后续的智能体。在本文中，我们提出了一种用于生成可以用于开发和训练这样一个对话智能体的示例对话的方法。通过启发式设计，我们开发了两个相互“对话”的智能体，一个充当对话智能体，另一个充当用户。使用仅对用户可见的一组线性问题文本描述，智能体和用户进行对话，直到智能体从原始问题描述中获取到所有关键信息。我们还提出了对话的外部评估方法。

    Linear programming (LP) problems are pervasive in real-life applications. However, despite their apparent simplicity, an untrained user may find it difficult to determine the linear model of their specific problem. We envisage the creation of a goal-oriented conversational agent that will engage in conversation with the user to elicit all information required so that a subsequent agent can generate the linear model. In this paper, we present an approach for the generation of sample dialogues that can be used to develop and train such a conversational agent. Using prompt engineering, we develop two agents that "talk" to each other, one acting as the conversational agent, and the other acting as the user. Using a set of text descriptions of linear problems from NL4Opt available to the user only, the agent and the user engage in conversation until the agent has retrieved all key information from the original problem description. We also propose an extrinsic evaluation of the dialogues by 
    
[^48]: 大型语言模型能否取代经济选择预测实验室？

    Can Large Language Models Replace Economic Choice Prediction Labs?

    [https://arxiv.org/abs/2401.17435](https://arxiv.org/abs/2401.17435)

    该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。

    

    经济选择预测是一项具有挑战性的重要任务，往往受限于获取人类选择数据的困难。实验经济学研究在很大程度上专注于简单的选择环境。最近，人工智能界以两种方式为该努力做出了贡献：考虑大型语言模型是否可以代替人类在上述简单选择预测环境中，以及通过机器学习视角研究更复杂但仍严格的实验经济学环境，包括不完全信息、重复博弈和基于自然语言交流的说服游戏。这引发了一个重要的灵感：大型语言模型是否能够完全模拟经济环境，并生成用于高效人类选择预测的数据，替代复杂的经济实验室研究？我们在这个主题上开创了研究，并展示了其可行性。特别是，我们表明仅在大型语言模型生成的数据上训练的模型可以有效地进行预测。

    Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
    
[^49]: 对土耳其语理解任务进行基于Transformer编码器的微调

    Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks

    [https://arxiv.org/abs/2401.17396](https://arxiv.org/abs/2401.17396)

    在本研究中，我们提供了一个基于Transformer的模型和一个土耳其语基准测试，成功地对名为BERTurk的土耳其BERT模型进行了微调，实现了许多下游任务的理解和评估。

    

    深度学习和最近的Transformer语言模型在自然语言处理领域的研究中占据主导地位。由于其准确和快速的微调特性，它们已经超越了传统的基于机器学习的方法，并在许多具有挑战性的自然语言理解（NLU）问题上取得了最先进的结果。最近的研究表明，基于Transformer的模型，如BERT（双向Transformer编码器表示），在许多任务上取得了令人瞩目的成果。此外，由于它们的迁移学习能力，这些架构允许我们将预先构建好的模型转移并针对特定的NLU任务进行微调，如问答。在这项研究中，我们为土耳其语提供了一个基于Transformer的模型和一个基准测试。我们成功地对一种名为BERTurk的土耳其BERT模型进行了微调，该模型是使用基本设置进行训练的，并用于许多下游任务进行了评估。

    Deep learning-based and lately Transformer-based language models have been dominating the studies of natural language processing in the last years. Thanks to their accurate and fast fine-tuning characteristics, they have outperformed traditional machine learning-based approaches and achieved state-of-the-art results for many challenging natural language understanding (NLU) problems. Recent studies showed that the Transformer-based models such as BERT, which is Bidirectional Encoder Representations from Transformers, have reached impressive achievements on many tasks. Moreover, thanks to their transfer learning capacity, these architectures allow us to transfer pre-built models and fine-tune them to specific NLU tasks such as question answering. In this study, we provide a Transformer-based model and a baseline benchmark for the Turkish Language. We successfully fine-tuned a Turkish BERT model, namely BERTurk that is trained with base settings, to many downstream tasks and evaluated wit
    
[^50]: 使用对比式上下文学习定制语言模型的回复

    Customizing Language Model Responses with Contrastive In-Context Learning

    [https://arxiv.org/abs/2401.17390](https://arxiv.org/abs/2401.17390)

    本论文提出了一种使用对比示例来定制语言模型回复的方法，通过提供正面示例和负面示例，使模型学会如何回避负面特征，从而更好地满足用户需求。

    

    大型语言模型 (LLMs) 对于机器学习应用变得越来越重要。然而，将LLMs与我们的意图对齐可能会具有挑战性，特别是当我们希望生成优于其他内容的内容，或者当我们希望LLMs以一种难以描述的风格或语气进行回应时。为了解决这个问题，我们提出了一种使用对比示例来更好地描述我们的意图的方法。这涉及提供正面示例来说明真实的意图，以及负面示例来展示我们希望LLMs避免的特征。负面示例可以从标记数据中检索，由人工编写，或由LLMs自动生成。在生成答案之前，我们要求模型分析这些示例，以教会自己避免什么。这个推理步骤为模型提供了与用户需求相关的适当表达，并引导其生成更好的答案。我们在合成和真实数据上测试了我们的方法。

    Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real
    
[^51]: 无限-gram：将无限n-gram语言模型扩展到万亿标记

    Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens

    [https://arxiv.org/abs/2401.17377](https://arxiv.org/abs/2401.17377)

    这项研究展示了n-gram语言模型的价值，并介绍了一个名为infini-gram的引擎，它可以以毫秒级的延迟计算任意n的n-gram概率，使得在神经大型语言模型中对文本进行更准确的分析成为可能。

    

    在神经大型语言模型（LLM）时代，n-gram语言模型还具有相关性吗？我们的答案是肯定的，并且我们展示了它们在文本分析和改进神经LLM方面的价值。然而，这需要在两个方面对n-gram模型进行现代化。首先，我们将它们与神经LLM相同的数据规模训练- 1.4万亿个标记。这是迄今为止构建的最大的n-gram模型。其次，现有的n-gram模型使用的n很小，这妨碍了它们的性能；相反，我们允许n可以是任意大的，通过引入一个新的无限-gram LM与回退。我们开发了一个名为infini-gram的引擎，它可以通过后缀数组计算无限-gram（以及任意n的n-gram）概率，并且具有毫秒级的延迟，而无需预先计算n-gram计数表（这将非常昂贵）。无限-gram框架和infini-gram引擎使我们能够对人类写作和机器生成的文本进行许多新颖和有意思的分析：我们发现无限-gram LM...

    Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
    
[^52]: 阿拉伯推文行为：基于加权集成预训练Transformer模型的阿拉伯语推特语言行为分类

    Arabic Tweet Act: A Weighted Ensemble Pre-Trained Transformer Model for Classifying Arabic Speech Acts on Twitter

    [https://arxiv.org/abs/2401.17373](https://arxiv.org/abs/2401.17373)

    本文提出了一种用于阿拉伯语推特语言行为分类的加权集成预训练Transformer模型。通过整合不同的BERT模型，我们实现了对阿拉伯方言的精确分类，为理解用户观点和态度提供了有力的工具。

    

    语言行为是说话者在对话中表达意思时的行为，例如询问、推荐、问候、道谢、表达想法或提出建议。理解语言行为有助于解释说话者或作者言语背后的意图和行为。本文提出了一种基于Transformer深度学习神经网络的阿拉伯语推特语言行为分类方法。推特和社交媒体越来越融入日常生活。因此，它们已经演变成了表达用户观点和态度的重要信息来源。我们提出了一种基于BERT的加权集成学习方法，以整合方言阿拉伯语语言行为分类中各种BERT模型的优势。我们将所提出的模型与几个阿拉伯BERT模型和基于序列的模型进行了比较。通过对现有大型数据集的子集进行注释，我们开发了一个方言阿拉伯推特行为数据集。

    Speech acts are a speakers actions when performing an utterance within a conversation, such as asking, recommending, greeting, or thanking someone, expressing a thought, or making a suggestion. Understanding speech acts helps interpret the intended meaning and actions behind a speakers or writers words. This paper proposes a Twitter dialectal Arabic speech act classification approach based on a transformer deep learning neural network. Twitter and social media, are becoming more and more integrated into daily life. As a result, they have evolved into a vital source of information that represents the views and attitudes of their users. We proposed a BERT based weighted ensemble learning approach to integrate the advantages of various BERT models in dialectal Arabic speech acts classification. We compared the proposed model against several variants of Arabic BERT models and sequence-based models. We developed a dialectal Arabic tweet act dataset by annotating a subset of a large existing
    
[^53]: ViLexNorm：越南社交媒体文本的词汇规范化语料库

    ViLexNorm: A Lexical Normalization Corpus for Vietnamese Social Media Text

    [https://arxiv.org/abs/2401.16403](https://arxiv.org/abs/2401.16403)

    ViLexNorm是第一个为越南社交媒体文本开发的词汇规范化语料库，可显著提高各种NLP任务的性能。

    

    词汇规范化是自然语言处理（NLP）中的一个基础任务，涉及将词语转换为它们的规范形式。已经证明，这个过程对各种后续的NLP任务有很大的益处。本文介绍了ViLexNorm，这是第一个为越南词汇规范化任务开发的语料库。该语料库包含了超过10,000对由人工标注的句子，这些句子来自越南最流行的社交媒体平台上的公开评论。我们使用了各种方法来评估我们的语料库，最佳系统在使用Error Reduction Rate (ERR)指标 (van der Goot, 2019a) 和Leave-As-Is (LAI)基准的情况下取得了57.74%的结果。在外部评估中，使用在ViLexNorm上训练的模型展示了越南词汇规范化任务对其他NLP任务的积极影响。我们的语料库仅供研究目的公开使用。

    Lexical normalization, a fundamental task in Natural Language Processing (NLP), involves the transformation of words into their canonical forms. This process has been proven to benefit various downstream NLP tasks greatly. In this work, we introduce Vietnamese Lexical Normalization (ViLexNorm), the first-ever corpus developed for the Vietnamese lexical normalization task. The corpus comprises over 10,000 pairs of sentences meticulously annotated by human annotators, sourced from public comments on Vietnam's most popular social media platforms. Various methods were used to evaluate our corpus, and the best-performing system achieved a result of 57.74% using the Error Reduction Rate (ERR) metric (van der Goot, 2019a) with the Leave-As-Is (LAI) baseline. For extrinsic evaluation, employing the model trained on ViLexNorm demonstrates the positive impact of the Vietnamese lexical normalization task on other NLP tasks. Our corpus is publicly available exclusively for research purposes.
    
[^54]: 多语言文本到图像生成放大了性别刻板印象，并且修正工程可能无法帮助您

    Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You

    [https://arxiv.org/abs/2401.16092](https://arxiv.org/abs/2401.16092)

    多语言文本到图像生成模型存在性别偏见；通过MAGBIG评估模型时，发现模型对不同语言具有重要差异；我们呼吁研究多语言模型领域消除性别偏见。

    

    最近，文本到图像生成模型在图像质量、灵活性和文本对齐方面取得了令人惊讶的结果，并因此在越来越多的应用中得到应用。通过改善多语言能力，更多的社群现在可以访问这种技术。然而，正如我们将展示的那样，多语言模型与单语模型一样受到(性别)偏见的困扰。此外，人们自然期望这些模型在不同语言之间提供类似的结果，但事实并非如此，不同语言之间存在重要的差异。因此，我们提出了一个旨在促进没有性别偏见的多语言模型研究的新基准MAGBIG。我们研究了多语言T2I模型是否通过MAGBIG放大了性别偏见。为此，我们使用多语言提示请求特定职业或特质的人像图像(使用形容词)。我们的结果不仅表明模型偏离了规范的假设，...

    Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this kind of technology. Yet, as we will show, multilingual models suffer similarly from (gender) biases as monolingual models. Furthermore, the natural expectation is that these models will provide similar results across languages, but this is not the case and there are important differences between languages. Thus, we propose a novel benchmark MAGBIG intending to foster research in multilingual models without gender bias. We investigate whether multilingual T2I models magnify gender bias with MAGBIG. To this end, we use multilingual prompts requesting portrait images of persons of a certain occupation or trait (using adjectives). Our results show not only that models deviate from the normative assumption th
    
[^55]: Baichuan2-Sum: 使用指导微调Baichuan2-7B模型进行对话摘要

    Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization

    [https://arxiv.org/abs/2401.15496](https://arxiv.org/abs/2401.15496)

    本文提出了Baichuan2-Sum模型，通过指导微调Baichuan2-7B模型进行对话摘要，并应用NEFTune技术改进训练过程。实验证明该模型在CSDS和SAMSUM数据集上取得了新的最先进结果。

    

    巨大的语言模型（LLM）如Llama、Baichuan和Bloom模型在许多自然语言任务中展现出了令人瞩目的能力。然而，对于对话摘要任务，该任务旨在为对话中的不同角色生成摘要，大多数最先进的方法都是基于小模型（例如Bart和Bert）进行的。现有方法尝试在小模型上添加任务指定的优化，如向模型添加全局-局部中心度得分。在本文中，我们提出了一种指导微调模型：Baichuan2-Sum，用于面向角色的对话摘要。通过为不同角色设置不同的指令，模型可以从对话交互中学习并输出期望的摘要。此外，我们还应用了NEFTune技术，在训练过程中添加合适的噪声以提高结果。实验证明，所提出的模型在两个公开的对话摘要数据集CSDS和SAMSUM上取得了新的最先进结果。

    Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods conduct on small models (e.g Bart and Bert). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the expected summaries. Furthermore, we applied NEFTune technique to add suitable noise during training to improve the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. We 
    
[^56]: 教育领域自然语言处理的调查：分类体系、系统综述和未来趋势

    Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends

    [https://arxiv.org/abs/2401.07518](https://arxiv.org/abs/2401.07518)

    这篇论文调查了教育领域自然语言处理的最新进展，提出了分类体系，并总结了挑战和未来研究方向。

    

    自然语言处理（NLP）旨在通过计算机科学领域的技术分析文本，应用于医疗保健、商业和教育领域。特别是，在教育领域，NLP已经被应用于教学和学习方面的帮助。本调查研究主要关注解决与教育领域相关的问题，并回顾了NLP的最新进展。具体来说，我们从介绍相关背景开始，然后提出教育领域NLP的分类系统。接着，我们根据上述分类系统说明任务定义、挑战和相应的技术。之后，我们展示了该领域中的一些现有演示，并总结了未来的研究方向。

    Natural Language Processing (NLP) aims to analyze the text via techniques in the computer science field. It serves the applications in healthcare, commerce, and education domains. Particularly, NLP has been applied to the education domain to help teaching and learning. In this survey, we review recent advances in NLP with a focus on solving problems related to the education domain. In detail, we begin with introducing the relevant background. Then, we present the taxonomy of NLP in the education domain. Next, we illustrate the task definition, challenges, and corresponding techniques based on the above taxonomy. After that, we showcase some off-the-shelf demonstrations in this domain and conclude with future directions.
    
[^57]: 高效大型语言模型：一项调查

    Efficient Large Language Models: A Survey

    [https://arxiv.org/abs/2312.03863](https://arxiv.org/abs/2312.03863)

    这篇综述论文对高效大型语言模型进行了系统和全面的调查，提供了从模型为中心、数据为中心和框架为中心的三个主要角度的分类和总结。此外，还创建了一个GitHub存储库来收集和更新相关论文。

    

    大型语言模型（LLMs）在重要任务如自然语言理解、语言生成和复杂推理中展示了卓越的能力，并且有潜力对我们的社会产生重大影响。然而，这种能力伴随着它们所需的相当大的资源，突显了解决效率挑战的有效技术的强烈需求。在这项调查中，我们提供了对高效LLMs研究的系统和全面的综述。我们将文献按照模型为中心、数据为中心和框架为中心的三个主要分类进行组织，涵盖了不同但相互关联的高效LLMs主题。我们还创建了一个GitHub存储库，其中收集了本调查中列出的论文，并将积极维护该存储库，并随着新的研究的出现而更新。

    Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges.In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can s
    
[^58]: 大型语言模型调参的超参数优化

    Hyperparameter Optimization for Large Language Model Instruction-Tuning

    [https://arxiv.org/abs/2312.00949](https://arxiv.org/abs/2312.00949)

    该论文研究了大型语言模型调参的超参数优化，通过引入低秩适应方法实现对网络的部分调整，使用黑盒优化技术探索超参数空间，取得了性能提升和模型与人类对齐的效果。

    

    大型语言模型（LLM）的微调使其在自然语言处理应用领域取得了里程碑式的成就。越来越大的LLM的出现为更高效的微调方法铺平了道路。其中，低秩适应（LoRA）方法将预训练LLM的大部分权重冻结，并引入权重矩阵的低秩分解，仅允许调整网络的极小部分。使用LoRA进行微调的模型在下游任务上的性能主要依赖于一组超参数，包括分解的秩。在这项研究中，我们通过两种主要的黑盒优化（BBO）技术来研究这些超参数的选择。我们将在预训练的LLM上进行微调和验证的整个流程视为黑盒，并使用Nomad算法高效地探索超参数空间，从而提高性能和调整模型与人类对齐。

    The fine-tuning of Large Language Models (LLMs) has enabled them to recently achieve milestones in natural language processing applications. The emergence of ever larger LLMs has paved the way for more efficient fine-tuning methods. Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of the pre-trained LLM frozen while introducing a low-rank decomposition of the weight matrix, enabling the tuning of only a very small proportion of the network. The performance on downstream tasks of models fine-tuned with LoRA heavily relies on a set of hyperparameters including the rank of the decomposition. In this work, we investigate the choice of these hyperparameters through two main blackbox optimization (BBO) techniques. We examine the whole pipeline of performing fine-tuning and validation on a pre-trained LLM as a blackbox and efficiently explore the space of hyperparameters with the \nomad algorithm, achieving a boost in performance and human alignment of the tuned mo
    
[^59]: 将语言知识注入到BERT中用于对话状态跟踪

    Injecting linguistic knowledge into BERT for Dialogue State Tracking

    [https://arxiv.org/abs/2311.15623](https://arxiv.org/abs/2311.15623)

    本文提出了一种方法，在对话状态跟踪任务中，通过无监督的知识提取方法将语言知识注入到BERT中，以提高性能和可解释性。这种方法无需额外的训练数据，通过简单的神经模块实现。该方法使用的特征提取工具与对话的句法和语义模式相关，有助于理解DST模型的决策过程。

    

    对话状态跟踪(DST)模型通常采用复杂的神经网络架构，需要大量的训练数据，其推理过程缺乏透明性。本文提出了一种方法，通过无监督框架提取语言知识，然后利用这些知识来增强BERT在DST任务中的性能和可解释性。知识提取过程计算经济高效，不需要注释或额外的训练数据。注入提取的知识只需要添加简单的神经模块。我们使用凸多面体模型(CPM)作为DST任务的特征提取工具，并表明所获取的特征与对话中的句法和语义模式相关。这种相关性有助于全面理解影响DST模型决策过程的语言特征。我们在不同的DST任务上对这个框架进行基准测试，并展示了其效果。

    Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob
    
[^60]: GRASP: 一种评估多模态语言模型中语言基础和情境物理理解的新型基准测试

    GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models

    [https://arxiv.org/abs/2311.09048](https://arxiv.org/abs/2311.09048)

    GRASP是一个新的基准测试，用于评估视频多模态大型语言模型的语言基础和物理理解能力。通过Unity模拟进行两层评估，揭示这些模型在语言基础和直觉物理学能力方面的显著缺陷。

    

    本文介绍了GRASP，一种评估基于视频的多模态大型语言模型(LLMs)的语言基础和物理理解能力的新型基准测试。通过利用Unity模拟的两层方法进行评估。第一层测试语言基础，通过评估模型将简单的文本描述与视觉信息相关联的能力。第二层评估模型对"直觉物理学"原理的理解能力，如物体永恒性和连续性。除了发布基准测试，我们还使用它评估了几种最先进的多模态LLMs。我们的评估揭示了这些模型在语言基础和直觉物理学能力方面存在显著的缺陷。尽管它们表现出了一定的基础能力，尤其是对于颜色和形状，但这些能力很大程度上依赖于启发策略。同时，所有模型在In部分的表现都低于或等于50%的随机水平。

    This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The first level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of "Intuitive Physics" principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models. Although they exhibit at least some grounding capabilities, particularly for colors and shapes, these capabilities depend heavily on the prompting strategy. At the same time, all models perform below or at the chance level of 50% in the In
    
[^61]: 自我监督的语音和语言模型是否提取了与人类大脑类似的表示？

    Do self-supervised speech and language models extract similar representations as human brain?

    [https://arxiv.org/abs/2310.04645](https://arxiv.org/abs/2310.04645)

    通过评估Wav2Vec2.0和GPT-2模型的大脑预测能力，我们发现自我监督的语音和语言模型能够准确预测语音反应，其大脑预测之间存在显著相关性，且共享的语音上下文信息是解释大脑活动中变异的主要因素。

    

    通过自我监督学习（SSL）训练的语音和语言模型在语音和语言感知期间展现出与大脑活动的强大对齐性。然而，由于它们的不同训练方式，它们是否与相同的神经方面相关仍然不清楚。我们通过评估两种代表性的SSL模型（Wav2Vec2.0和GPT-2）在语音和语言任务中的大脑预测性能来直接回答这个问题。我们的研究结果显示，这两种模型都能准确预测听觉皮层中的语音响应，并且它们的大脑预测之间存在显著相关性。值得注意的是，Wav2Vec2.0和GPT-2之间的共享语音上下文信息解释了大脑活动中的大部分变异，超过了静态语义和较低级的声音-音位信息。这些结果强调了SSL模型中语音上下文表示的收敛性，以及它们与语音知觉底层神经网络的对齐。

    Speech and language models trained through self-supervised learning (SSL) demonstrate strong alignment with brain activity during speech and language perception. However, given their distinct training modalities, it remains unclear whether they correlate with the same neural aspects. We directly address this question by evaluating the brain prediction performance of two representative SSL models, Wav2Vec2.0 and GPT-2, designed for speech and language tasks. Our findings reveal that both models accurately predict speech responses in the auditory cortex, with a significant correlation between their brain predictions. Notably, shared speech contextual information between Wav2Vec2.0 and GPT-2 accounts for the majority of explained variance in brain activity, surpassing static semantic and lower-level acoustic-phonetic information. These results underscore the convergence of speech contextual representations in SSL models and their alignment with the neural network underlying speech percept
    
[^62]: 单语或多语指导调整：哪种方式更适合alpaca？

    Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca

    [https://arxiv.org/abs/2309.08958](https://arxiv.org/abs/2309.08958)

    通过实证分析比较了单语和多语指导调整的成本效益，发现在多语言场景下，多语指导调整可以达到或超越单独调整每种语言的效果，并且采用下采样的数据进行多语调整可以提供更强的效果和更好的鲁棒性。

    

    基础的大型语言模型（LLM）可以通过指导调整来执行开放域的问答任务，从而实现聊天助手等应用。虽然这类努力通常只在单一语言中进行，但我们实证分析了多语言场景下的成本效益策略。我们使用Alpaca数据集和其中的机器翻译数据形成多语言调整的训练集，然后采用低秩调整或完全参数训练的方式对LLM进行调整。在受控算力预算下的比较结果表明，多语言调整可以达到或超越每种语言单独调整的效果。此外，通过下采样的数据进行多语言调整可以达到相同甚至更强的效果。我们的研究结果为通过指导调整来扩展语言支持提供了指导。

    Foundational large language models (LLMs) can be instruction-tuned to perform open-domain question answering, facilitating applications like chat assistants. While such efforts are often carried out in a single language, we empirically analyze cost-efficient strategies for multilingual scenarios. Our study employs the Alpaca dataset and machine translations of it to form multilingual data, which is then used to tune LLMs through either low-rank adaptation or full-parameter training. Under a controlled computation budget, comparisons show that multilingual tuning is on par or better than tuning a model for each language. Furthermore, multilingual tuning with downsampled data can be as powerful and more robust. Our findings serve as a guide for expanding language support through instruction tuning.
    
[^63]: 利用多语言正例在对比学习中提升句子嵌入

    Leveraging Multi-lingual Positive Instances in Contrastive Learning to Improve Sentence Embedding

    [https://arxiv.org/abs/2309.08929](https://arxiv.org/abs/2309.08929)

    本研究提出了一种名为MPCL的方法，在对比学习中利用多语言正例来改进句子嵌入。实验结果表明，MPCL可以提高检索、语义相似性和分类性能。

    

    学习多语言句子嵌入是自然语言处理中的基础任务。最近学习单语和多语句子嵌入的趋势主要基于对比学习（CL），其中包括一个锚点、一个正例和多个负例。本文认为应该考虑利用多个正例来改进多语言句子嵌入，因为（1）不同语言的正例可以有益于跨语言学习，（2）多个正例之间的传递相似性可以提供可靠的结构信息用于学习。为了研究对比学习中多个正例的影响，我们提出了一种名为MPCL的新方法，以有效利用多个正例来提升多语言句子嵌入的学习效果。在各种主干模型和下游任务上的实验结果表明，MPCL可以提高检索、语义相似性和分类性能。

    Learning multi-lingual sentence embeddings is a fundamental task in natural language processing. Recent trends in learning both mono-lingual and multi-lingual sentence embeddings are mainly based on contrastive learning (CL) among an anchor, one positive, and multiple negative instances. In this work, we argue that leveraging multiple positives should be considered for multi-lingual sentence embeddings because (1) positives in a diverse set of languages can benefit cross-lingual learning, and (2) transitive similarity across multiple positives can provide reliable structural information for learning. In order to investigate the impact of multiple positives in CL, we propose a novel approach, named MPCL, to effectively utilize multiple positive instances to improve the learning of multi-lingual sentence embeddings. Experimental results on various backbone models and downstream tasks demonstrate that MPCL leads to better retrieval, semantic similarity, and classification performances com
    
[^64]: 用于建模命名实体之间分级关系的无情基准

    A RelEntLess Benchmark for Modelling Graded Relations between Named Entities

    [https://arxiv.org/abs/2305.15002](https://arxiv.org/abs/2305.15002)

    本文提出了一个用于模拟命名实体之间分级关系的无情基准，使用大型语言模型进行填补，以对实体对根据其满足程度进行排序。通过评估最先进的关系嵌入策略和多个LLM，我们发现了重要的创新和贡献。

    

    诸如“受影响于”、“以...闻名”或“与...竞争”之类的关系本质上是分级的：我们可以根据实体对满足这些关系的程度对它们进行排名，但很难将满足和不满足这些关系的实体对划分开。这样的分级关系在许多应用中起着重要作用，然而现有的知识图谱通常不包含此类关系。本文考虑使用大型语言模型（LLM）来填补这个空白的可能性。为此，我们引入了一个新的基准，其中实体对必须根据其满足给定分级关系的程度进行排序。该任务被定义为少样本排序问题，模型只能访问关系的描述和五个原型实例。我们使用提出的基准来评估最先进的关系嵌入策略以及几个最近的LLM，包括公开可用的LLM和封闭模型，例如GPT-4。总体而言，我们发现了一个重要的创新和贡献。

    Relations such as "is influenced by", "is known for" or "is a competitor of" are inherently graded: we can rank entity pairs based on how well they satisfy these relations, but it is hard to draw a line between those pairs that satisfy them and those that do not. Such graded relations play a central role in many applications, yet they are typically not covered by existing Knowledge Graphs. In this paper, we consider the possibility of using Large Language Models (LLMs) to fill this gap. To this end, we introduce a new benchmark, in which entity pairs have to be ranked according to how much they satisfy a given graded relation. The task is formulated as a few-shot ranking problem, where models only have access to a description of the relation and five prototypical instances. We use the proposed benchmark to evaluate state-of-the-art relation embedding strategies as well as several recent LLMs, covering both publicly available LLMs and closed models such as GPT-4. Overall, we find a stro
    
[^65]: APPLS: 评估纯语言摘要的评价指标

    APPLS: Evaluating Evaluation Metrics for Plain Language Summarization

    [https://arxiv.org/abs/2305.14341](https://arxiv.org/abs/2305.14341)

    本文提出了一个用于评估纯语言摘要的指标测试平台APPLS，并引入了一种新的指标POMME来评估PLS中的文本简化。通过对指标的分析发现，当前的指标未能始终捕捉到简化度。

    

    尽管对于纯语言摘要（PLS）的模型有了很大的发展，但评估仍然是一个挑战。PLS缺乏专门的评估指标，由于涉及到独特的转换（例如，添加背景解释，删除专业术语），因此对于文本生成评估指标的适用性尚不清楚。为了解决这些问题，我们的研究提出了一个细致的元评估测试平台APPLS，旨在评估PLS的指标。我们根据先前工作的启发，定义了四个标准上的一组扰动，PLS指标应该捕捉到：信息性、简化度、连贯性和忠实度。使用我们的测试平台对指标进行分析发现，当前的指标未能始终捕捉到简化度。作为回应，我们引入了一种新的指标POMME，旨在评估PLS中文本简化；该指标是根据域内和域外语言模型之间的标准化困惑度差计算得到的。我们演示了POMME的效果，并与其他指标进行了比较。

    While there has been significant development of models for Plain Language Summarization (PLS), evaluation remains a challenge. PLS lacks a dedicated assessment metric, and the suitability of text generation evaluation metrics is unclear due to the unique transformations involved (e.g., adding background explanations, removing specialized terminology). To address these concerns, our study presents a granular meta-evaluation testbed, APPLS, designed to evaluate metrics for PLS. We define a set of perturbations along four criteria inspired by previous work that a PLS metric should capture: informativeness, simplification, coherence, and faithfulness. An analysis of metrics using our testbed reveals that current metrics fail to capture simplification consistently. In response, we introduce POMME, a new metric designed to assess text simplification in PLS; the metric is calculated as the normalized perplexity difference between an in-domain and out-of-domain language model. We demonstrate P
    
[^66]: $\mu$PLAN：使用内容计划作为跨语言桥梁的摘要

    $\mu$PLAN: Summarizing using a Content Plan as Cross-Lingual Bridge

    [https://arxiv.org/abs/2305.14205](https://arxiv.org/abs/2305.14205)

    本文介绍了$\mu$PLAN，一种跨语言摘要方法，使用内容计划作为跨语言桥梁。通过将计划抽象为一系列实体，此方法在四种语言对上取得了最先进的效果。

    

    跨语言摘要是指在给定不同语言的输入文档的情况下生成一份摘要，以便将相关内容传播给其他语言的讲者。这项任务的挑战主要在于跨语言数据集的匮乏和摘要和翻译的复合难度。本文提出了一种名为$\mu$PLAN的跨语言摘要方法，它使用一个中间计划步骤作为跨语言桥梁。我们将计划形式化为一系列实体，捕捉摘要的内容和应该传达的顺序。重要的是，我们的计划抽象了表面形式：使用多语言知识库，我们将实体与它们在不同语言中的规范指定对齐，并在此跨语言桥梁和输入的条件下生成摘要。在XWikis数据集上进行的自动和人工评估（涵盖四种语言对）证明了我们的规划目标达到了最新的水平。

    Cross-lingual summarization consists of generating a summary in one language given an input document in a different language, allowing for the dissemination of relevant content across speakers of other languages. The task is challenging mainly due to the paucity of cross-lingual datasets and the compounded difficulty of summarizing and translating. This work presents $\mu$PLAN, an approach to cross-lingual summarization that uses an intermediate planning step as a cross-lingual bridge. We formulate the plan as a sequence of entities capturing the summary's content and the order in which it should be communicated. Importantly, our plans abstract from surface form: using a multilingual knowledge base, we align entities to their canonical designation across languages and generate the summary conditioned on this cross-lingual bridge and the input. Automatic and human evaluation on the XWikis dataset (across four language pairs) demonstrates that our planning objective achieves state-of-the
    
[^67]: 自然语言理解中的即时去噪数据增强

    On-the-fly Denoising for Data Augmentation in Natural Language Understanding

    [https://arxiv.org/abs/2212.10558](https://arxiv.org/abs/2212.10558)

    本文提出了一种即时去噪的数据增强技术，利用软增强标签和自我正则化模块，通过从更干净的原始数据学习来保证增强数据的质量。

    

    数据增强（DA）经常被用来在没有额外的人工注释的情况下提供额外的训练数据。然而，数据增强可能引入噪声数据来干扰训练。为了保证增强数据的质量，现有方法要么假设增强数据中没有噪声，并采用一致性训练，要么使用简单的启发式方法（如训练损失和多样性约束）来过滤掉“嘈杂”的数据。然而，这些被过滤的示例可能仍然包含有用的信息，并且完全丢弃它们会导致监督信号的丧失。在本文中，基于原始数据集比增强数据更干净的假设，我们提出了一种即时去噪的数据增强技术，该技术利用了在更干净的原始数据上训练的有机教师模型提供的软增强标签进行学习。为了进一步防止过度拟合噪声标签，我们还应用了简单的自我正则化模块，强制模型预测保持一致。

    Data Augmentation (DA) is frequently used to provide additional training data without extra human annotation automatically. However, data augmentation may introduce noisy data that impairs training. To guarantee the quality of augmented data, existing methods either assume no noise exists in the augmented data and adopt consistency training or use simple heuristics such as training loss and diversity constraints to filter out "noisy" data. However, those filtered examples may still contain useful information, and dropping them completely causes a loss of supervision signals. In this paper, based on the assumption that the original dataset is cleaner than the augmented data, we propose an on-the-fly denoising technique for data augmentation that learns from soft augmented labels provided by an organic teacher model trained on the cleaner original data. To further prevent overfitting on noisy labels, a simple self-regularization module is applied to force the model prediction to be consi
    
[^68]: 一种自述疗法的共情人工智能辅导系统

    An Empathetic AI Coach for Self-Attachment Therapy

    [https://arxiv.org/abs/2209.08316](https://arxiv.org/abs/2209.08316)

    本文介绍了一个用于自述疗法的共情人工智能辅导系统，通过深度学习和规则进行情绪识别和生成流畅、共情的对话，达到更高的用户参与度和实用性。通过非临床试验验证了框架的有效性，并提供改进设计和性能的指导方针。

    

    本文提出了一个用于指导用户进行自述疗法的数字辅导系统的新数据集和计算策略。我们的框架通过将基于规则的对话代理与深度学习分类器相结合，可以识别用户文本回复中的潜在情绪，并采用深度学习辅助检索方法生成新颖、流畅和共情的话语。我们还设计了一组类似人类的角色供用户选择互动。我们的目标是在虚拟疗法会话中实现高水平的参与度。我们在一项非临床试验中对N=16名参与者进行了框架的有效性评估，这些参与者在五天内至少与代理进行了四次互动。结果显示，与简单的基于规则的框架相比，我们的平台在共情度、用户参与度和实用性方面被评价得更高。最后，我们提供了进一步改进设计和性能的指导方针。

    In this work, we present a new dataset and a computational strategy for a digital coach that aims to guide users in practicing the protocols of self-attachment therapy. Our framework augments a rule-based conversational agent with a deep-learning classifier for identifying the underlying emotion in a user's text response, as well as a deep-learning assisted retrieval method for producing novel, fluent and empathetic utterances. We also craft a set of human-like personas that users can choose to interact with. Our goal is to achieve a high level of engagement during virtual therapy sessions. We evaluate the effectiveness of our framework in a non-clinical trial with N=16 participants, all of whom have had at least four interactions with the agent over the course of five days. We find that our platform is consistently rated higher for empathy, user engagement and usefulness than the simple rule-based framework. Finally, we provide guidelines to further improve the design and performance 
    
[^69]: 基于RAG的理解伊斯兰教问题回答系统提案：MufassirQAS LLM

    A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])

    [http://arxiv.org/abs/2401.15378](http://arxiv.org/abs/2401.15378)

    基于RAG的MufassirQAS问答系统利用NLP技术建立联系并准确回答复杂问题，提高了LLMs的准确性和透明度，帮助理解伊斯兰教的复杂性和教义深度。

    

    学习和理解宗教存在复杂性和教义深度的挑战。问答机器人作为解决这些挑战的问题回答系统，可以帮助。LLM聊天机器人利用自然语言处理技术建立主题之间的联系，准确回答复杂问题。这些能力使其成为用于宗教启蒙的问题回答聊天机器人的理想选择。然而，LLM也有生成虚假信息的倾向，称为幻觉。聊天机器人的回答可能包含侮辱个人宗教信仰、跨宗派冲突和有争议或敏感的话题的内容。它需要避免这种情况，而不会宣扬仇恨言论或冒犯某些群体的人或他们的信仰。本研究使用基于向量数据库的检索增强生成（RAG）方法来提高LLMs的准确性和透明度。我们的问答系统称为"MufassirQAS"。我们创建了一个模型来评估该系统并证明其在解决宗教行业问题中的效果。

    There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
    
[^70]: 语义敏感性和不一致的预测：衡量NLI模型的脆弱性

    Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])

    [http://arxiv.org/abs/2401.14440](http://arxiv.org/abs/2401.14440)

    这份论文研究发现，最先进的NLI模型对微小的语义保持表面形式变化非常敏感，导致推断结果不一致。其行为与对组合语义的有效理解不同，这对当前NLI模型的可靠性提出了挑战。

    

    最近对基于transformer的自然语言理解（NLU）模型的新能力进行的研究表明，它们具备对词汇和组合语义的理解。然而，我们提供了证据表明这些说法应该持保留态度：我们发现目前最先进的自然语言推理（NLI）模型对微小的保留语义的表面形式变化敏感，这导致推断过程中出现大量不一致的模型决策。值得注意的是，这种行为与对组合语义的有效和深入理解不同，而在标准基准测试中评估模型准确度或探究句法、单调性和逻辑鲁棒性推理时均不会出现。我们提出了一个新颖的框架来衡量语义敏感性的程度。为此，我们使用含有微小保留语义的表面形式输入噪声的对抗生成样例来评估NLI模型。

    Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text
    
[^71]: GRATH: 大型语言模型的逐渐自我真实化方法

    GRATH: Gradual Self-Truthifying for Large Language Models. (arXiv:2401.12292v1 [cs.CL])

    [http://arxiv.org/abs/2401.12292](http://arxiv.org/abs/2401.12292)

    GRATH是一种逐步自我真实化的方法，用于提高大型语言模型的真实性。它通过使用领域外问题提示生成答案，并通过直接偏好优化进行自适应模型优化。GRATH在没有标注答案的情况下以自我监督的方式学习真实性，并通过迭代优化来逐步提升模型真实性。

    

    随着大型语言模型（LLMs）在真实世界应用中的部署越来越多，真实性对它们来说至关重要。然而，现有的LLMs在生成真实答案和内容方面仍然存在困难，如在TruthfulQA等基准上的表现不佳。为了解决这个问题，我们提出了GRAdual self-truTHifying (GRATH)，一种通过后处理方法提高LLMs真实性的新方法。GRATH利用领域外的问题提示生成相应的答案，并通过直接偏好优化进行自适应模型优化。在这个过程中，GRATH以无需标注答案的自我监督方式学习真实性。具体而言，GRATH首先通过提示LLM自身生成成对真实性训练数据，每对包含一个问题及其正确和错误答案。然后，使用直接偏好优化来微调模型，从答案对的差异中学习。随后，GRATH迭代地优化模型以逐渐提高真实性。

    Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful answers and content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate corresponding answers and adaptively optimizes the model via direct preference optimization (DPO). Note that during this process, GRATH learns truthfulness in a self-supervised manner without requiring annotated answers. In particular, GRATH first generates pairwise truthfulness training data by prompting the LLM itself, with each pair containing a question and its correct and incorrect answers. The model is then fine-tuned using DPO to learn from the difference between answer pairs. Subsequently, GRATH iteratively refines the 
    
[^72]: 通过思维方程蒸馏改进小型语言模型的数学推理能力

    Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11864](http://arxiv.org/abs/2401.11864)

    本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。

    

    本研究解决了将先进的大型语言模型（LLMs）的数学推理能力压缩到具有小于十亿参数的小型语言模型（SLMs）中的挑战，同时不损害性能。我们引入了一种新颖的思维方程蒸馏（EoTD）技术，将推理过程封装为基于方程的表示，构建了一个EoTD数据集来对SLMs进行微调。此外，我们提出了集合思维蒸馏（ETD）框架，以提升SLMs的推理性能。这包括创建一个包含多个思维过程（包括思维链、思维程序和思维方程）的推理数据集，并将其用于微调。我们的实验证明，EoTD显著提升了SLMs的推理能力，而ETD使这些模型实现了最先进的推理性能。

    This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
    
[^73]: 高斯自适应注意力是唯一所需的：跨多个模态的健壮上下文表示

    Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])

    [http://arxiv.org/abs/2401.11143](http://arxiv.org/abs/2401.11143)

    该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。

    

    我们提出了多头高斯自适应注意力机制（GAAM），一种新颖的概率注意力框架，并设计了高斯自适应变压器（GAT），旨在增强跨多个模态（包括语音、文本和视觉）的信息聚合。GAAM将可学习的均值和方差融入其注意力机制中，采用多头框架实现，使其能够集体建模任何概率分布，以动态重新调整特征重要性。该方法在处理高度非平稳数据时表现出显著改进，通过识别特征空间中的关键元素，超越了现有的注意力技术在模型性能上的状态（精度增加约20%）。GAAM与基于点积的注意力模型兼容，并具有相对较低的参数数量，展示了其适应性和提升现有注意力框架的潜力。在实证方面，GAAM表现出卓越的适应性和功效。

    We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
    
[^74]: 结合置信度引导和基于样本的方法用于消除误信息中的不确定性量化

    Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation. (arXiv:2401.08694v1 [cs.CL])

    [http://arxiv.org/abs/2401.08694](http://arxiv.org/abs/2401.08694)

    本研究提出了一种结合置信度引导和基于样本的方法的不确定性量化框架，用于解决误信息消除中的幻觉和过度自信的预测问题，并提出了混合框架以提供更好的不确定性估计。

    

    大型语言模型已经成为解决误信息消除的主要候选方案。然而，现有方法在幻觉和过度自信的预测方面存在问题。我们提出了一种不确定性量化框架，利用直接置信度引导和基于样本的一致性方法，为自然语言处理误信息消除解决方案提供更好的校准。首先，我们研究基于样本一致性方法的校准性，该方法利用样本规模和随机水平的一致性的不同特征。接下来，我们评估了鲁棒的数字化口头提示在单步和两步置信度引导过程中的性能和分布变化。我们还比较了相同提示在不同版本的GPT和不同数字尺度下的性能。最后，我们结合基于样本一致性和数字化方法，提出了一个混合框架，为GPT模型提供更好的不确定性估计。

    Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overal
    
[^75]: 强化微调语言模型中的梯度消失问题

    Vanishing Gradients in Reinforcement Finetuning of Language Models. (arXiv:2310.20703v1 [cs.LG])

    [http://arxiv.org/abs/2310.20703](http://arxiv.org/abs/2310.20703)

    本研究发现在强化微调（RFT）中存在梯度消失的问题，当模型下奖励的标准差较小时，输入的期望梯度会消失，导致奖励最大化缓慢。初始监督微调（SFT）阶段是克服这个问题的最有希望的方法。

    

    预训练的语言模型通过强化微调（RFT）与人类偏好和下游任务对齐，即使用策略梯度算法最大化（可能是学习得到的）奖励函数。本研究发现了RFT中的一个基本的优化障碍：我们证明了当模型下的奖励标准差较小时，输入的期望梯度会消失，即使期望奖励远离最优解。通过在RFT基准和控制环境中进行实验，以及理论分析，我们证明了由于小的奖励标准差导致的梯度消失问题普遍存在且有害，导致奖励最大化极其缓慢。最后，我们探索了克服RFT中梯度消失的方法。我们发现初始监督微调（SFT）阶段是最有希望的候选方法，并且揭示了它在RFT流程中的重要性。此外，我们还表明相对较小的训练数据集的SFT阶段可以有效克服梯度消失问题。

    Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small num
    
[^76]: MAPLE: 基于大型语言模型嵌入的移动应用预测

    MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings. (arXiv:2309.08648v1 [cs.CL])

    [http://arxiv.org/abs/2309.08648](http://arxiv.org/abs/2309.08648)

    MAPLE是一个利用大型语言模型嵌入进行移动应用预测的模型，通过严格测试验证了其在解密复杂模式和理解用户环境方面的能力，并强调了语言模型在不同领域中的广泛适用性。

    

    尽管移动应用的发展迅速，但由于复杂的用户行为和不断演变的环境，预测应用的使用仍然是一个严峻的挑战。为了解决这些问题，本文介绍了Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE)模型。这种创新的方法利用大型语言模型(LLM)来准确预测应用的使用情况。通过对两个公开数据集进行严格测试，MAPLE的能力在解密复杂模式和理解用户环境方面得到了验证。这些强大的结果证实了MAPLE在不同场景中的多功能性和弹性。尽管其主要设计面向应用预测，但结果也强调了LLM在不同领域中的广泛适用性。通过这项研究，我们强调了LLM在应用使用预测中的潜力，并建议在建模各种领域中的人类行为方面，它们具有变革能力。

    Despite the rapid advancement of mobile applications, predicting app usage remains a formidable challenge due to intricate user behaviours and ever-evolving contexts. To address these issues, this paper introduces the Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE) model. This innovative approach utilizes Large Language Models (LLMs) to predict app usage accurately. Rigorous testing on two public datasets highlights MAPLE's capability to decipher intricate patterns and comprehend user contexts. These robust results confirm MAPLE's versatility and resilience across various scenarios. While its primary design caters to app prediction, the outcomes also emphasize the broader applicability of LLMs in different domains. Through this research, we emphasize the potential of LLMs in app usage prediction and suggest their transformative capacity in modelling human behaviours across diverse fields.
    
[^77]: RCT拒绝抽样用于因果估计评估

    RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])

    [http://arxiv.org/abs/2307.15176](http://arxiv.org/abs/2307.15176)

    该论文提出了一种名为RCT拒绝抽样的新抽样算法，用于因果估计评估。该方法通过子抽样随机控制试验(RCT)创建混淆的观测数据集，并使用RCT的平均因果效应作为基准真实值，以进行有效比较。

    

    混淆是从观测数据中无偏估计因果效应的一个重要障碍。对于高维协变量的情况，如文本数据、基因组学或行为社会科学，研究人员提出了适应机器学习方法进行因果估计的调整方法。然而，这些调整方法的经验评估一直存在困难和限制。在这项工作中，我们基于一种有前景的经验评估策略，简化了评估设计，并使用真实数据：对随机控制试验(RCT)进行子抽样，以创建混淆的观测数据集，同时使用RCT的平均因果效应作为基准真实值。我们提出了一种新的抽样算法，称为RCT拒绝抽样，并提供了理论保证，以确保观测数据的因果识别成立，从而可以与基准RCT进行有效比较。通过使用合成数据，我们展示了我们的算法在...

    Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
    
[^78]: 自我监督的语音模型对单词的了解程度是什么？

    What do self-supervised speech models know about words?. (arXiv:2307.00162v1 [cs.CL])

    [http://arxiv.org/abs/2307.00162](http://arxiv.org/abs/2307.00162)

    通过对自我监督的语音模型进行分析，发现这些模型在不同层中编码了不同的语言信息，也学习了类似音素的子词单元。与单词相关的信息主要在中间的模型层中，同时一些低级信息在更高的层中也得以保留。

    

    在过去几年中，许多自我监督的语音模型（S3Ms）被引入，为各种语音任务提供了性能和数据效率的改进。有证据表明，不同的S3Ms在不同的层中编码语言信息，而且一些S3Ms似乎学习了类似于音素的子词单元。然而，这些模型捕捉更大的语言单元（如单词）的程度以及单词相关信息的编码位置仍然不清楚。在这项研究中，我们对来自三个S3Ms的不同层的单词片段表示进行了多种分析：wav2vec2、HuBERT和WavLM。我们利用规范相关分析（CCA），一种轻量级的分析工具，来衡量这些表示与单词级语言属性之间的相似性。我们发现最大的单词级语言内容往往出现在中间的模型层，而一些低级信息（如发音）也在更高的层中保留。

    Many self-supervised speech models (S3Ms) have been introduced over the last few years, producing performance and data efficiency improvements for a variety of speech tasks. Evidence is emerging that different S3Ms encode linguistic information in different layers, and also that some S3Ms appear to learn phone-like sub-word units. However, the extent to which these models capture larger linguistic units, such as words, and where word-related information is encoded, remains unclear. In this study, we conduct several analyses of word segment representations extracted from different layers of three S3Ms: wav2vec2, HuBERT, and WavLM. We employ canonical correlation analysis (CCA), a lightweight analysis tool, to measure the similarity between these representations and word-level linguistic properties. We find that the maximal word-level linguistic content tends to be found in intermediate model layers, while some lower-level information like pronunciation is also retained in higher layers 
    
[^79]: 如何在生成序列标记中改善基于跨度级别置信度的束搜索？(arXiv:2212.10767v2 [cs.CL] 更新)

    How Does Beam Search improve Span-Level Confidence Estimation in Generative Sequence Labeling?. (arXiv:2212.10767v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10767](http://arxiv.org/abs/2212.10767)

    本文研究了在生成序列标记任务中如何改善对跨度级别置信度的估计。研究发现仅仅使用解码器的输出概率并不是最佳方法，而利用束搜索的前k个预测的统计数据可以显著降低校准误差。

    

    序列标记是信息抽取/信息检索系统中的核心任务。文本生成模型越来越成为这类任务的解决方案（例如实体提取和对话槽填充）。虽然大多数研究都集中在标记准确性上，但一个关键的方面——对模型置信度的理解却被忽视了。具体而言，我们缺乏一个能够可靠地衡量模型对每个标记跨度的预测置信度的原则性理解。本文旨在提供一些关于生成序列标记的模型置信度估计的实证见解。值得注意的是，我们发现仅仅使用解码器的输出概率并不是实现良好校准置信度估计的最佳方法。通过对六个不同任务的公共数据集进行验证，我们展示了我们提出的方法——利用束搜索的前k个预测的统计数据——显著降低了校准误差。

    Sequence labeling is a core task in text understanding for IE/IR systems. Text generation models have increasingly become the go-to solution for such tasks (e.g., entity extraction and dialog slot filling). While most research has focused on the labeling accuracy, a key aspect -- of vital practical importance -- has slipped through the cracks: understanding model confidence. More specifically, we lack a principled understanding of how to reliably gauge the confidence of a model in its predictions for each labeled span. This paper aims to provide some empirical insights on estimating model confidence for generative sequence labeling. Most notably, we find that simply using the decoder's output probabilities \textbf{is not} the best in realizing well-calibrated confidence estimates. As verified over six public datasets of different tasks, we show that our proposed approach -- which leverages statistics from top-$k$ predictions by a beam search -- significantly reduces calibration errors 
    

