# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges.](http://arxiv.org/abs/2310.11252) | 该论文介绍了一种通过可视化研究束搜索树的方法，以解决语言模型提示挑战。通过全面检查模型输出，包括候选输出和对应概率，我们找到了与提示大型语言模型相关的挑战，并给出了五个详细的分析场景。 |
| [^2] | [Entity Matching using Large Language Models.](http://arxiv.org/abs/2310.11244) | 这项研究探讨了使用大型语言模型（LLMs）作为实体匹配的替代方法，相较于预训练的语言模型（PLMs），LLMs对训练数据需求较少且更具鲁棒性。 |
| [^3] | [Watermarking LLMs with Weight Quantization.](http://arxiv.org/abs/2310.11237) | 本论文提出了一种新颖的水印策略，通过在大型语言模型的量化过程中插入水印来保护模型权重，使得模型在推理时起作用但在量化后保持隐藏，为保护大型语言模型应用时的模型权重提供了一个潜在的方向。 |
| [^4] | [RealBehavior: A Framework for Faithfully Characterizing Foundation Models' Human-like Behavior Mechanisms.](http://arxiv.org/abs/2310.11227) | RealBehavior框架旨在准确描绘基础模型的人类化行为，通过评估准确性和多样化对齐目标，提高结果的可信度。 |
| [^5] | [KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models.](http://arxiv.org/abs/2310.11220) | KG-GPT是一个使用大型语言模型在知识图谱上进行推理的通用框架，通过句子分割、图检索和推理三个步骤实现了对知识图谱的复杂推理任务，并在事实验证和KGQA基准上展现了有竞争力和强健的性能。 |
| [^6] | [Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations.](http://arxiv.org/abs/2310.11207) | 本文研究了大型语言模型生成的自解释在情感分析和特征归因解释任务中的效果，并探讨了不同的引导方法。 |
| [^7] | [Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding.](http://arxiv.org/abs/2310.11191) | 本研究探索了进一步提高医学领域文本简化可读性的方法，包括一种新的非典型损失和一种优化简单性的重新排序解码方法，取得了更好的性能。 |
| [^8] | [ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing.](http://arxiv.org/abs/2310.11166) | ViSoBERT是首个用于越南社交媒体文本处理的预训练语言模型，通过在大规模越南社交媒体文本语料库上进行训练，该模型在情感识别、仇恨言论检测、情感分析、垃圾评论检测和仇恨言论跨度检测等任务上取得了良好的性能表现。 |
| [^9] | [IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems.](http://arxiv.org/abs/2310.11163) | IMTLab是一个开源平台，用于构建、评估和诊断交互式机器翻译系统。它提供了一种任务导向的对话接口，支持灵活的架构和用户策略。前缀约束解码方法在端到端评估中表现出最低的编辑成本。 |
| [^10] | [Probing the Creativity of Large Language Models: Can models produce divergent semantic association?.](http://arxiv.org/abs/2310.11158) | 本研究使用分散联想任务探索大型语言模型的创造性思维能力。研究发现，先进的模型可以生成不同的语义关联，超过了大多数人类的水平。 |
| [^11] | [The Quo Vadis of the Relationship between Language and Large Language Models.](http://arxiv.org/abs/2310.11146) | 大型语言模型（LLMs）在语言科学模型中的地位引发了关于其透明度和信息提供能力的讨论，当前阶段的LLMs几乎对语言没有提供任何解释，未来研究需要探索更具信息价值的方向。 |
| [^12] | [Long-form Simultaneous Speech Translation: Thesis Proposal.](http://arxiv.org/abs/2310.11141) | 这个论文提案研究了端到端长篇同传，即在没有预先分割的情况下进行实时口语翻译。它调查了最新的端到端同传进展，评估了同传中的主要障碍和与长篇场景的相关性，并提出了解决这些挑战的方法。 |
| [^13] | [Experimenting AI Technologies for Disinformation Combat: the IDMO Project.](http://arxiv.org/abs/2310.11097) | IDMO项目旨在使用人工智能技术打击虚假信息和假新闻，其贡献包括创建新型数据集、开发自动模型、评估GPT-4等。 |
| [^14] | [In-Context Few-Shot Relation Extraction via Pre-Trained Language Models.](http://arxiv.org/abs/2310.11085) | 本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。 |
| [^15] | [Understanding writing style in social media with a supervised contrastively pre-trained transformer.](http://arxiv.org/abs/2310.11081) | 该论文介绍了一种使用有监督对比预训练变换器(Supervised Contrastively Pre-trained Transformer)来理解社交媒体中的写作风格的方法，该方法能够有效地将内容与其作者相关联，从而更好地理解有害行为。 |
| [^16] | [Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models.](http://arxiv.org/abs/2310.11079) | 这项研究提出了一种自动生成测试用例以检测大型语言模型（LLMs）中潜在性别偏见的方法，并提出了使用生成的测试用例作为上下文学习的演示来缓解偏见的存在。实验证明，采用该方法后，LLMs能够生成更公平的响应。 |
| [^17] | [VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System.](http://arxiv.org/abs/2310.11069) | VoxArabica是一个稳健的方言感知阿拉伯语音识别系统，通过开发和演示，实现了阿拉伯语方言识别和自动语音识别。该系统训练了各种模型用于不同方言的识别，并提供了多种功能的网络界面。 |
| [^18] | [Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning.](http://arxiv.org/abs/2310.11053) | 通过Moral Foundation Theory和DeNEVIL算法，我们研究了大型语言模型的道德价值，并构建了MoralPrompt数据集来评估模型的内在价值。发现大多数模型存在不对齐，需要进一步进行道德价值对齐。 |
| [^19] | [Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation.](http://arxiv.org/abs/2310.11049) | 这篇论文介绍了我们在SemEval-2023法律评估任务6上的提交，主要集中在法律命名实体识别、法律判决预测和带解释的法院判决预测等子任务上。我们进行了多个实验，并取得了在各个子任务中具有竞争力的排名。 |
| [^20] | [Lyricist-Singer Entropy Affects Lyric-Lyricist Classification Performance.](http://arxiv.org/abs/2310.11035) | 本文研究了歌词创作者和演唱者之间的关系，发现歌词-创作者分类的性能取决于演唱者的多样性。 |
| [^21] | [Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation.](http://arxiv.org/abs/2310.11026) | 本文探索了基于解码器的LLM的文本生成自动评估方法，发现相比调优后的基于编码器的模型，调优后的基于解码器的模型在机器翻译评估和语义文本相似性任务上表现较差，原因是解码器模型更关注表面词序列而忽略了含义，并且非常大的解码器模型的上下文学习使得难以识别细粒度的语义差异。 |
| [^22] | [Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction.](http://arxiv.org/abs/2310.11016) | 本研究探讨了在视觉丰富文档中提取信息的问题，并通过引入令牌路径预测的方法来解决实体标记的阅读顺序问题。该方法将文档布局建模为令牌的有向图，并预测实体的令牌路径。 |
| [^23] | [Correction Focused Language Model Training for Speech Recognition.](http://arxiv.org/abs/2310.11003) | 本研究介绍了一种为语音识别进行纠错的语言模型训练方法，通过定义易出错单词的分数并将其用作先验分布来指导训练，并利用大型语言模型进行纠错型训练。实验证明该方法在领域适应任务中有效，相对传统方法可以显著降低单词错误率。 |
| [^24] | [Instructive Dialogue Summarization with Query Aggregations.](http://arxiv.org/abs/2310.10981) | 传统的对话摘要方法无法考虑用户的特定兴趣，而指导对话摘要的引入可以帮助扩展对话摘要模型的能力。我们提出了一个三步方法来合成高质量的查询摘要三元组，并通过在多个数据集上训练一个统一模型来扩展对话摘要模型的能力。 |
| [^25] | [EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset.](http://arxiv.org/abs/2310.10967) | 提出了一种解释性多模态开放领域对话数据集，通过多模态数据构建框架(MDCF)设计适当的提示，从而解决了大型模型对于多模态输入的缺乏、生成内容缺乏可解释性以及数据质量控制的问题。 |
| [^26] | [Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models.](http://arxiv.org/abs/2310.10962) | 本论文提出了SemCSR框架，利用大型语言模型（LLM）的生成和评估能力，无需人工标注就能自动构建高质量的自然语言推理样式语料库，用于解决对比学习中的挑战。 |
| [^27] | [Computing the optimal keyboard through a geometric analysis of the English language.](http://arxiv.org/abs/2310.10956) | 通过几何分析，我们计算出了一种最佳键盘设计，提供更快的打字速度。 |
| [^28] | [A State-Vector Framework for Dataset Effects.](http://arxiv.org/abs/2310.10955) | 本研究提出了一个状态向量框架，用于系统地研究数据集的效果。我们发现一些常用的语言理解数据集对模型有显著的效果，这些效果集中在几个语言维度上。此外，我们观察到数据集可能对模型的非相关维度产生"溢出"效应。这个框架为负责任和鲁棒模型开发中的数据集效果提供了一个系统的理解。 |
| [^29] | [TEQ: Trainable Equivalent Transformation for Quantization of LLMs.](http://arxiv.org/abs/2310.10944) | 本文提出了TEQ，一种可训练的等效转换方法，可以在保持模型准确性的同时，利用低精度的量化方式满足大型语言模型的计算要求。与现有的最先进方法相比，我们的方法在典型的LLMs上具有相似的表现，且不增加推理时的计算开销。 |
| [^30] | [MASON-NLP at eRisk 2023: Deep Learning-Based Detection of Depression Symptoms from Social Media Texts.](http://arxiv.org/abs/2310.10941) | MASON-NLP提出了一种基于深度学习的方法，通过分析社交媒体文本检测抑郁症状。任务1的目标是评估不同条件的相关性。 |
| [^31] | [Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti.](http://arxiv.org/abs/2310.10935) | 这项研究引入了第一个用于孟加拉语和锡尔赫蒂语的意图检测和槽位填充的全面数据集，并发现大型语言模型在处理不充足数据的下游任务上表现出强大能力。 |
| [^32] | [Enhanced Transformer Architecture for Natural Language Processing.](http://arxiv.org/abs/2310.10930) | 增强的Transformer引入了全层标准化、加权残差连接、强化学习位置编码和零掩码自注意力等创新技术，通过在Multi30k翻译数据集上验证，相比于原始Transformer，实现了202.96%的BLEU分数提升。 |
| [^33] | [Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio.](http://arxiv.org/abs/2310.10922) | Spatial HuBERT是一种自监督的语音表示模型，通过学习多通道音频输入中的单个说话者的声学和空间信息，其表示在各种空间下游任务中优于最先进的单声道语音表示，特别是在嘈杂和混响环境中。 |
| [^34] | [NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain.](http://arxiv.org/abs/2310.10920) | NuclearQA是一个人工基准，用于评估核领域的语言模型的性能，其中包含100个专家设计的问题。该基准与现有的评估指标不同，能够准确评估语言模型在核领域的能力。 |
| [^35] | [Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT.](http://arxiv.org/abs/2310.10903) | ChatGPT在第二语言写作中扮演重要角色，能够有效地与语言学习者合作，在保持个人表达声音的同时提升写作能力。这项研究为ChatGPT在学术写作中的应用提供了重要的探索。 |
| [^36] | [IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models.](http://arxiv.org/abs/2310.10873) | 本文提出了一种影响驱动的选择性注释方法，用于在大型语言模型中改善上下文学习。该方法通过选择关键的未标记数据子集进行注释，在降低注释成本的同时提高了上下文示例的质量。 |
| [^37] | [Will the Prince Get True Love's Kiss? On the Model Sensitivity to Gender Perturbation over Fairytale Texts.](http://arxiv.org/abs/2310.10865) | 该研究旨在通过评估语言模型对性别扰动的鲁棒性，帮助减轻传统童话中的性别偏见，并通过引入反事实性别刻板印象来减轻学习到的偏见。实验结果显示，模型对性别扰动敏感，但在反事实训练后对后续引入的反性别偏见更不敏感。 |
| [^38] | [CoTFormer: More Tokens With Attention Make Up For Less Depth.](http://arxiv.org/abs/2310.10845) | CoTFormer是一种transformer变体，通过使用隐含的链思考机制，实现了与更深模型相当的容量，并且在实证中显著优于更大的标准transformers。 |
| [^39] | [Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks.](http://arxiv.org/abs/2310.10844) | 本论文调查了对大型语言模型进行恶意攻击的研究，发现即使经过安全调整的模型也容易受到攻击。这些攻击利用弱点并误导AI系统，对于复杂系统的攻击尤为明显。 |
| [^40] | [Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks.](http://arxiv.org/abs/2310.10830) | 这篇论文介绍了一种鲁棒的无风格假新闻检测器，能够对抗利用大型语言模型进行风格攻击的假新闻。通过LLM增强的新闻重构，该检测器能够适应不同的写作风格，提高了对伪装假新闻的检测能力。 |
| [^41] | [SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT.](http://arxiv.org/abs/2310.10803) | 本研究提出了SD-HuBERT模型，通过采用自我蒸馏目标进行微调，实现了在学习语音句子级表示时音节组织的出现，模型能够在语音中划定明确的边界，并展现出显著的音节结构。该研究还提出了一个新的基准任务用于评估语音的句子级表示，与之前的模型相比，在无监督音节发现和学习句子级表示方面表现优异。 |
| [^42] | [Self-Supervised Models of Speech Infer Universal Articulatory Kinematics.](http://arxiv.org/abs/2310.10788) | 本研究展示了自我监督语音模型具有推断语音发音运动学的能力，并显示出这一属性在不同语言中具有重叠性。此外，通过简单变换，模型可以在不同说话者、性别、语言和方言之间转换，表现出良好的泛化性。这些结果拓宽了我们对语音处理的理解。 |
| [^43] | [BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bengali.](http://arxiv.org/abs/2310.10781) | 本文介绍了BanglaNLP开发的系统在解决孟加拉文中暴力煽动文字检测共享任务中的表现。通过研究数据增强和使用多语言-e5-base模型进行微调，我们在测试集上取得了68.11%的宏F1值，在排行榜上排名第23位。 |
| [^44] | [BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys.](http://arxiv.org/abs/2310.10765) | 提出了一种新颖的方法BiomedJourney，通过指导学习多模态患者旅程，进行反事实生物医学图像生成。使用GPT-4处理图像报告生成疾病进展的自然语言描述，并训练潜在扩散模型。 |
| [^45] | [Towards reducing hallucination in extracting information from financial reports using Large Language Models.](http://arxiv.org/abs/2310.10760) | 本文介绍了使用大型语言模型减少从财务报告中提取信息时的错误的方法，并且通过检索增强生成技术和元数据的结合，实现了高精度的信息提取过程。 |
| [^46] | [Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning.](http://arxiv.org/abs/2310.10735) | 本研究提出了一种离线强化学习框架，通过廉价地在现有数据上训练模型并对特定发言进行奖惩，提高对话系统的个性一致性。 |
| [^47] | [Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning.](http://arxiv.org/abs/2310.10707) | 该论文在攻击性内容改写方面引入了上下文学习方法，并通过有限数量的输入-标签演示对来指导模型生成特定查询的所需输出，从而提高可用性和减少攻击性。 |
| [^48] | [Harnessing the Power of LLMs: Evaluating Human-AI text Co-Creation through the Lens of News Headline Generation.](http://arxiv.org/abs/2310.10706) | 该研究通过对LLMs辅助新闻标题生成的人工智能协作方法进行比较，发现引导和选择模型输出能够带来最大的效益，并且与自由编辑相比并不损害参与者对控制的感知。 |
| [^49] | [Optimized Tokenization for Transcribed Error Correction.](http://arxiv.org/abs/2310.10704) | 本研究通过训练仅使用合成数据的方法，在转录错误修正中取得了显著的性能提升。具体而言，通过使用从转录数据中得到的合成数据，并进行语言特定的词汇调整，可以更好地纠正重复错误。 |
| [^50] | [Theory of Mind for Multi-Agent Collaboration via Large Language Models.](http://arxiv.org/abs/2310.10701) | 本研究通过在多智能体合作游戏中评估基于大型语言模型的智能体，发现它们可以表现出协作行为和高级理论推理能力，并通过使用明确的信念状态表示来提高任务性能和理论推理准确性。 |
| [^51] | [Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation.](http://arxiv.org/abs/2310.10698) | 本文提出了一种“语义思维链”的方法，用于在代码生成中引入语义信息，并以此来实现更精细的代码理解和表示。 |
| [^52] | [Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation.](http://arxiv.org/abs/2310.10690) | 本研究探索在开放式学习环境中使用大型语言模型进行上下文学生建模，提出了一个新的框架LLM-SS，通过合成学生在不同任务上的尝试，为学生建模提供更准确的预测和教学策略。 |
| [^53] | [A decoder-only foundation model for time-series forecasting.](http://arxiv.org/abs/2310.10688) | 本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。 |
| [^54] | [Autonomous Tree-search Ability of Large Language Models.](http://arxiv.org/abs/2310.10686) | 提出了一个新的概念，旨在使大型语言模型能够在没有外部程序的辅助下维持树搜索能力，并产生展示树结构搜索过程的响应。 |
| [^55] | [Large Language Model Unlearning.](http://arxiv.org/abs/2310.10683) | 大型语言模型的去学习是一个研究的新领域，我们探索了三个场景，可以通过去学习让语言模型与人类偏好保持一致。去学习具有三个优势，只需要负面示例，计算效率高，特别对于知道具体导致不良行为的训练样本更为有效。 |
| [^56] | [Large language models can replicate cross-cultural differences in personality.](http://arxiv.org/abs/2310.10679) | 大型语言模型GPT-4成功复制了使用十项人格问卷测量的大五人格的跨文化差异，但其结果表明平均评级有上升偏差和较低的变异性与结构效度。 |
| [^57] | [LLMs as Potential Brainstorming Partners for Math and Science Problems.](http://arxiv.org/abs/2310.10677) | 该论文研究了大型语言模型（LLM），特别是GPT-4，在与人类的集体头脑风暴中的能力和限制，为数学和科学问题的发现和解决提供了有希望的一步。 |
| [^58] | [Creation Of A ChatBot Based On Natural Language Proccesing For Whatsapp.](http://arxiv.org/abs/2310.10675) | 本研究旨在开发一个基于自然语言处理的WhatsApp聊天机器人，以提高客户满意度并通过WhatsApp提供更好的服务质量。这个聊天机器人能够高效、有效地处理用户的查询问题。 |
| [^59] | [Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate Emotion Probability Vectors.](http://arxiv.org/abs/2310.10673) | 本文介绍了如何使用LLMs来估计文本的情感状态，并通过情感分析和PCA映射提出了改善文本描述状态的动作选择，但实验表明这一方法目前存在困难。 |
| [^60] | [Hybrid Quantum-Classical Machine Learning for Sentiment Analysis.](http://arxiv.org/abs/2310.10672) | 本文提出了一种使用混合量子-经典机器学习算法进行情感分析的方法，通过研究量子核方法和基于变分量子电路的分类器，并结合经典的降维技术，实现了在处理大规模数据集中表达的人类情感和观点的情感分析，并取得了相对于传统方法更好的性能。 |
| [^61] | [NLP for Crypto-Asset Regulation: A Roadmap.](http://arxiv.org/abs/2310.10333) | 这篇论文介绍了自然语言处理在加密资产监管中的应用，并提出了两个贡献。首先，调查了对未受监管的加密资产白皮书进行文本分析的现有应用，并发现了研究空白。然后，分析了欧盟的加密资产市场法规引入的变化，探讨了在新的监管框架内整合自然语言处理的机遇和挑战。这些发现为未来的研究提供了基础，有潜力使监管机构、加密资产发行者和投资者受益。 |
| [^62] | [Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis.](http://arxiv.org/abs/2310.09909) | 本研究评估了OpenAI的最新模型GPT-4V在多模态医学诊断中的性能，包括成像模态和解剖学识别。 |
| [^63] | [Improved Contextual Recognition In Automatic Speech Recognition Systems By Semantic Lattice Rescoring.](http://arxiv.org/abs/2310.09680) | 通过深度学习模型和transformer的重新评分，我们提出了一种通过语义格重排序来提高自动语音识别系统中上下文识别能力的方法。 |
| [^64] | [An Expression Tree Decoding Strategy for Mathematical Equation Generation.](http://arxiv.org/abs/2310.09619) | 本研究提出了一种表达树解码策略，将树结构整合到数学方程生成中，以解决当前顺序方法忽视并行和依赖关系的问题。 |
| [^65] | [Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model.](http://arxiv.org/abs/2310.09520) | 该论文介绍了一种名为Reward-Augmented Decoding (RAD)的文本生成方法，使用小型的单向奖励模型来鼓励语言模型生成具有特定属性的文本。RAD在生成非有害和情感受控文本方面表现最佳，并且在非常大的语言模型上也很有效。 |
| [^66] | [A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks.](http://arxiv.org/abs/2310.09430) | 通过对大型语言模型在非分布式逻辑推理任务上进行系统评估，我们发现这些模型在处理我们新构建的数据集时都存在困难，尽管它们在其他自然语言处理任务上表现良好。这表明这些模型在逻辑推理方面的泛化和鲁棒性仍需要进一步研究。 |
| [^67] | [Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation.](http://arxiv.org/abs/2310.08943) | 这是一篇关于知识引导对话生成的论文，通过引入多层自适应对比学习（MACL）框架，并在令牌级和序列级上动态采样负例来解决模型简单插入知识片段导致的退化问题。 |
| [^68] | [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models.](http://arxiv.org/abs/2310.08659) | 本论文提出了LoftQ：一种针对大型语言模型的LoRA精调感知量化框架。该框架同时对LLM进行量化，并为LoRA精调找到适当的低秩初始化，以缓解量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。 |
| [^69] | [BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations.](http://arxiv.org/abs/2310.07276) | BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。 |
| [^70] | [Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition.](http://arxiv.org/abs/2310.06434) | Whispering LLaMA是一种用于语音识别的跨模态生成错误校正框架，通过融合声学信息和外部语言表示，生成准确的语音转录上下文，相对于n-best假设，词错误率性能提升了37.66%。 |
| [^71] | [GPT-Driver: Learning to Drive with GPT.](http://arxiv.org/abs/2310.01415) | 本文提出了一种将OpenAI GPT-3.5模型应用于自动驾驶的运动规划器的方法，通过将运动规划转化为语言建模问题，利用大型语言模型生成驾驶轨迹，提高了运动规划的泛化能力和推理能力。 |
| [^72] | [Decoding Imagery: Unleashing Large Language Models.](http://arxiv.org/abs/2309.16705) | 该论文研究了Google Bard这个多模态大型语言模型的能力，发现Bard在将视觉和语言分析相结合方面依赖于对图像进行有根据的猜测，可以解决视觉上有挑战的问题但无法修改原始视觉对象。 |
| [^73] | [Effective Long-Context Scaling of Foundation Models.](http://arxiv.org/abs/2309.16039) | 我们提出了一系列支持长上下文的基础模型，通过连续预训练和数据增强来实现，这些模型在语言建模和研究基准上都取得了显著的改进，并且通过成本效益的指导调整程序，已经超过了现有模型在长上下文任务上的性能。 |
| [^74] | [ChatGPT-BCI: Word-Level Neural State Classification Using GPT, EEG, and Eye-Tracking Biomarkers in Semantic Inference Reading Comprehension.](http://arxiv.org/abs/2309.15714) | 本研究通过联合分析大型语言模型（LLMs）、眼动和脑电图（EEG）数据，研究了大脑在阅读过程中处理与关键字相关度不同的单词的神经状态，并提供了关于语义推理阅读理解中神经状态的洞察。 |
| [^75] | [Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech.](http://arxiv.org/abs/2309.14107) | 本研究使用基于wav2vec的模型，实现了言语失语症的自动检测和严重程度级别分类任务，并在准确率上取得了显著的提升。 |
| [^76] | [Analysis and Detection of Pathological Voice using Glottal Source Features.](http://arxiv.org/abs/2309.14080) | 本研究提供了对声门源特征的系统分析，并研究了它们在声音病理检测中的有效性。实验结果表明声门源包含的信息对于病理性声音的判别具有重要作用。 |
| [^77] | [AnglE-Optimized Text Embeddings.](http://arxiv.org/abs/2309.12871) | 本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。 |
| [^78] | [AceGPT, Localizing Large Language Models in Arabic.](http://arxiv.org/abs/2309.12053) | 本研究旨在开发阿拉伯文的本地化大型语言模型(AceGPT)，通过预训练、监督微调和增强学习方法来培养具备文化意识和价值观一致的阿拉伯文模型，以满足阿拉伯语社区特定应用需求。评估结果表明，AceGPT在各项基准测试中都是最先进的阿拉伯文模型。 |
| [^79] | [Automatic Personalized Impression Generation for PET Reports Using Large Language Models.](http://arxiv.org/abs/2309.10066) | 本研究旨在使用fine-tuned大型语言模型实现自动个性化生成全身PET报告的准确印象。通过训练语言模型并引入阅读医生的身份信息，模型能够学习医生特定的报告风格。研究结果经过专家评估和核医学医生的质量评分认可，证明该方法在实践中具有潜在的应用价值。 |
| [^80] | [Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning.](http://arxiv.org/abs/2309.04965) | Prefix-diffusion是一种轻量级的图像字幕扩散模型，通过在扩散过程中注入前缀图像嵌入来实现多样性，并通过预训练模型和额外的映射网络来减少参数。该模型能够生成多样的字幕，同时保持流畅性和相关性，并取得了有希望的性能。 |
| [^81] | [When Do Program-of-Thoughts Work for Reasoning?.](http://arxiv.org/abs/2308.15452) | 提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。 |
| [^82] | [A Study on Robustness and Reliability of Large Language Model Code Generation.](http://arxiv.org/abs/2308.10335) | 本研究针对大型语言模型生成的代码的可靠性和鲁棒性进行了研究，发现在真实的软件开发中可执行的代码并不能保证可靠和鲁棒，滥用API可能导致严重问题。这对初级开发者来说尤其危险，因为他们很难察觉到代码中的API滥用问题。 |
| [^83] | [XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models.](http://arxiv.org/abs/2308.01263) | 本文介绍了一个名为XSTest的测试套件，旨在识别大型语言模型中夸大的安全行为。该套件由200个安全提示组成，涵盖十种提示类型，旨在引出模型的系统性问题。 |
| [^84] | [Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models.](http://arxiv.org/abs/2307.10236) | 本研究从不确定性的角度对大型语言模型进行了探索性研究，通过实验发现不确定性估计方法在探索和抵制大型语言模型的不良行为方面具有潜力。 |
| [^85] | [A Survey on Evaluation of Large Language Models.](http://arxiv.org/abs/2307.03109) | 本文综述了大型语言模型（LLMs）的评估方法，关注三个关键维度：评估什么、在哪里评估以及如何评估。评估任务包括自然语言处理、推理、医学应用、伦理学、教育、自然和社会科学、代理应用等多个领域。本文为社会层面对LLMs潜在风险的理解提供了重要参考。 |
| [^86] | [Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis.](http://arxiv.org/abs/2306.17181) | 本论文提出了一种使用生成对抗网络（GAN）生成连续文本嵌入空间的方法（TESGAN），以解决传统GAN在自然语言生成中的限制。这种方法通过引入连续的文本嵌入空间取代离散的标记，使得生成器在通过反向传播更新梯度时更加有效。 |
| [^87] | [REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction.](http://arxiv.org/abs/2306.15724) | 提出了REFLECT框架，可以将机器人多感官数据转化为分层总结，并使用大型语言模型进行失败解释。该框架能够生成有益的失败解释，帮助机器人完成任务。 |
| [^88] | [Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health.](http://arxiv.org/abs/2306.10070) | 本文探讨了ChatGPT和大型语言模型在生物医学和健康领域的多样应用，发现在文本生成方面已经取得了重大进展，但对于其他应用进展缓慢，LLMs还没有真正彻底改变生物医学领域。 |
| [^89] | [Demystifying GPT Self-Repair for Code Generation.](http://arxiv.org/abs/2306.09896) | 本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。 |
| [^90] | [Large Language Models Are Not Abstract Reasoners.](http://arxiv.org/abs/2305.19555) | 本文通过对最先进的大型语言模型进行抽象推理任务评估，发现它们在这方面的表现十分有限，揭示了其在推理方面的局限性。 |
| [^91] | [Active Learning for Natural Language Generation.](http://arxiv.org/abs/2305.15040) | 本文首次对自然语言生成进行了主动学习的系统研究，发现现有的主动学习策略在不同情况下性能不一致，强调了分类和生成场景之间的差异。 |
| [^92] | [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training.](http://arxiv.org/abs/2305.14342) | Sophia是一种用于语言模型预训练的可扩展的二阶优化算法，使用对角Hessian作为预调节器，并进行元素级别的裁剪控制更新大小。 |
| [^93] | [Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration.](http://arxiv.org/abs/2305.14324) | 提出使用成对准确性和关联性校准的方法对现代指标进行元评估，以更公平地评估指标的排名性能。 |
| [^94] | [R2H: Building Multimodal Navigation Helpers that Respond to Help Requests.](http://arxiv.org/abs/2305.14260) | 本论文介绍了一个名为R2H的基准，旨在构建能够回应帮助请求的多模态导航助手。该研究主要包括两个任务，即根据对话历史生成响应和在与任务执行者合作时进行响应评估。研究采用了一种创新的任务导向多模态响应生成模型来构建导航助手代理。 |
| [^95] | [Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality.](http://arxiv.org/abs/2305.13812) | 本研究提出了一种基于场景图的对比学习框架，通过将从文本中解析出的场景图视为图像场景图的代理，并对图进行分解和增强，从简单到复杂的对比学习以将各种复杂度的句子对齐到同一幅图像上，同时在场景图空间中提出了新的负样本挖掘技术，以改善视觉语言组合能力。 |
| [^96] | [Towards Unsupervised Recognition of Semantic Differences in Related Documents.](http://arxiv.org/abs/2305.13303) | 本论文提出了一种无监督识别语义差异的方法，并对三种无监督方法进行了研究。实验结果显示，基于单词对齐和句级对比学习的方法与真实标签之间存在稳健的相关性。然而，所有的无监督方法仍有很大的改进空间。 |
| [^97] | [MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space.](http://arxiv.org/abs/2305.12785) | MacLaSa是一种通过从紧致潜在空间中高效采样的方法，实现多方面可控的文本生成。它通过估计紧致潜在空间和使用基于ODE的鲁棒采样器来有效生成具有多个期望属性的句子。利用变分自动编码器网络来消除不同方面之间的域差距，并能够制定联合能量模型和插入任意属性鉴别器。 |
| [^98] | [Multilingual Simplification of Medical Texts.](http://arxiv.org/abs/2305.12532) | 本研究介绍了MultiCochrane，这是医学领域中第一个句子对齐的多语言文本简化数据集，通过多语言简化直接将复杂文本简化为多种语言的简化文本。 |
| [^99] | [LLM Itself Can Read and Generate CXR Images.](http://arxiv.org/abs/2305.11490) | 该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。 |
| [^100] | [TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models.](http://arxiv.org/abs/2305.11171) | TrueTeacher是一种使用大型语言模型生成合成数据来进行事实一致性评估的方法，相较于传统方法，TrueTeacher不依赖于人工编写的摘要，多语言特性，实验证明能够显著提升模型的性能。 |
| [^101] | [CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction.](http://arxiv.org/abs/2305.10819) | 提出了一种新的Chunk-LEvel Multi-reference Evaluation (CLEME)方法来解决在多参考环境下评估GEC系统时存在的偏差问题，其通过消除由不一致的编辑边界引起的偏差和自动确定语法错误的边界来提高了GEC评估的可靠性。 |
| [^102] | [The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation.](http://arxiv.org/abs/2305.09652) | 语音翻译(ST)是预训练语音模型进行端到端口语理解的良好手段。通过引入ST，我们的模型在单语言和跨语言场景下表现均好，具有更高的性能。 |
| [^103] | [Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation.](http://arxiv.org/abs/2305.07609) | 这篇论文介绍了一种新的推荐范式——通过LLM进行推荐，但由于LLMs可能存在社会偏见，需要进一步调查RecLLM所做推荐的公正性。为此，作者提出了一个新的公平性基准——FaiRLLM，并针对音乐和电影推荐场景中的八个敏感属性进行了评估。 |
| [^104] | [The Internal State of an LLM Knows When its Lying.](http://arxiv.org/abs/2304.13734) | 该论文研究了LLM生成不准确或虚假信息的问题，提出了一种简单而有效的方法，利用LLM的隐藏层激活来确定语句的真实性。在实验中，该方法表现出较好的检测效果，并有利于提高LLM的可信度。 |
| [^105] | [PAXQA: Generating Cross-lingual Question Answering Examples at Training Scale.](http://arxiv.org/abs/2304.12206) | PAXQA提出了一种使用现有平行语料库生成跨语言问答样例的方法，通过将问题生成模型和注释投影相结合，以更好地进行问题翻译和回答翻译。该方法在4种语言中生成了662K个例子。 |
| [^106] | [Editable User Profiles for Controllable Text Recommendation.](http://arxiv.org/abs/2304.04250) | 本文提出了一种新的概念值瓶颈模型LACE，用于可控文本推荐。该模型基于用户文档学习个性化的概念表示，并通过多种交互方式为用户提供了控制推荐的机制，验证了在离线和在线实验中该模型的推荐质量和有效性。 |
| [^107] | [A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection.](http://arxiv.org/abs/2304.01492) | 该文介绍了一个利用对比传递框架和传播结构，将从充足资源的谣言数据学到的特征适应于低资源情况下的方式，可以检测到跨越语言和领域界限的谣言。 |
| [^108] | [Towards Making the Most of ChatGPT for Machine Translation.](http://arxiv.org/abs/2303.13780) | 本文提出了任务和领域特定提示来优化ChatGPT在复杂机器翻译任务中的表现，研究发现温度设置和任务信息对ChatGPT表现有显著影响。 |
| [^109] | [Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets.](http://arxiv.org/abs/2302.13959) | 本文研究了在NLP数据集中自我影响的稳定性和实用性，探讨了自我影响分数在数据清理中的适应性，以及其对机器翻译、问答和文本分类等任务性能的改进。 |
| [^110] | [Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?.](http://arxiv.org/abs/2302.11713) | 本研究介绍了一个专门针对无法仅凭常识知识回答的信息寻求问题而设计的视觉问答数据集InfoSeek。使用InfoSeek数据集，我们发现目前最先进的预训练多模态模型在回答求知视觉问题方面面临挑战，但在该数据集上进行微调可以激发模型使用细粒度知识。 |
| [^111] | [Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning.](http://arxiv.org/abs/2301.11916) | 本研究发现，大型语言模型可以被视为隐式的主题模型，并提出了一种算法，从注释数据中选择最佳示范，大大提高了上下文学习的能力。 |
| [^112] | [ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format.](http://arxiv.org/abs/2211.17148) | ConvLab-3是一个灵活的对话系统工具包，基于统一数据格式，简化了数据与模型的集成，支持迁移学习和RL，并提供快速开发和评估鲁棒的对话策略的功能。 |
| [^113] | [Fake news detection using parallel BERT deep neural networks.](http://arxiv.org/abs/2204.04793) | 本文介绍了一种使用并行BERT深度神经网络的方法来检测假新闻。通过使用两个并行的BERT网络分别编码新闻标题和新闻正文，并且考虑到BERT网络的输入长度限制，该方法可以有效地进行假新闻的真实性检测。 |
| [^114] | [BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter.](http://arxiv.org/abs/2105.01331) | 本论文提出了一个用于推特上检测黑人生命至关重要话题的大规模数据集BLM-17m，涵盖了乔治·弗洛伊德事件期间的17百万推文。作者提供了两个基线模型TF-IDF和LDA，并对其进行了评估。 |
| [^115] | [Defining implication relation for classical logic.](http://arxiv.org/abs/1312.7832) | 本研究旨在从经典逻辑中去除不正确的“析取到蕴涵”。通过提出一个新的逻辑系统，该系统在一般情况下无法推导出“析取到蕴涵”或其否定，从而达到了预期的目标。 |

# 详细

[^1]: 揭示不可言说之事：通过可视化研究束搜索树来解决语言模型提示挑战

    Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges. (arXiv:2310.11252v1 [cs.CL])

    [http://arxiv.org/abs/2310.11252](http://arxiv.org/abs/2310.11252)

    该论文介绍了一种通过可视化研究束搜索树的方法，以解决语言模型提示挑战。通过全面检查模型输出，包括候选输出和对应概率，我们找到了与提示大型语言模型相关的挑战，并给出了五个详细的分析场景。

    

    生成式语言模型的普及增加了对交互式方法以引导模型输出的兴趣。提示细化被认为是这些方法中影响输出最有效的手段之一。我们识别出与提示大型语言模型相关的几个挑战，包括数据和模型特定、语言和社会语言学挑战。为了解决这些问题，需要对模型输出进行全面的检查，包括候选输出和对应概率。束搜索树作为一种普遍的算法，可以自然地提供这些信息。因此，我们引入了一种交互式的可视化方法来研究束搜索树，方便分析模型在生成过程中所做的决策。我们定量地展示了揭示束搜索树的价值，并提出了五个详细的分析场景来解决所识别的挑战。

    The growing popularity of generative language models has amplified interest in interactive methods to guide model outputs. Prompt refinement is considered one of the most effective means to influence output among these methods. We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges. A comprehensive examination of model outputs, including runner-up candidates and their corresponding probabilities, is needed to address these issues. The beam search tree, the prevalent algorithm to sample model outputs, can inherently supply this information. Consequently, we introduce an interactive visual method for investigating the beam search tree, facilitating analysis of the decisions made by the model during generation. We quantitatively show the value of exposing the beam search tree and present five detailed analysis scenarios addressing the identified challenges. Our methodology valid
    
[^2]: 使用大型语言模型进行实体匹配

    Entity Matching using Large Language Models. (arXiv:2310.11244v1 [cs.CL])

    [http://arxiv.org/abs/2310.11244](http://arxiv.org/abs/2310.11244)

    这项研究探讨了使用大型语言模型（LLMs）作为实体匹配的替代方法，相较于预训练的语言模型（PLMs），LLMs对训练数据需求较少且更具鲁棒性。

    

    实体匹配是判断两个实体描述是否指的是同一个真实世界实体的任务。实体匹配是大多数数据集成流程中的核心步骤，也是许多电子商务应用的重要组成部分，这些应用需要将来自不同供应商的产品匹配起来。目前最先进的实体匹配方法通常依赖于预训练的语言模型（PLMs），如BERT或RoBERTa。然而，这些模型在实体匹配中存在两个主要缺点：（i）模型需要大量特定任务的训练数据；（ii）微调后的模型对于超出分布范围的实体不够健壮。本文研究了使用大型语言模型（LLMs）作为基于PLMs的匹配器的备选方案，相比之下，LLMs对领域特定训练数据需求较少且更具鲁棒性。我们的研究涵盖了托管的LLMs，如GPT3.5和GPT4，以及基于Llama2的开源LLMs，可以在本地运行。我们在零样本场景和…

    Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a
    
[^3]: 使用权重量化对LLMs进行水印处理

    Watermarking LLMs with Weight Quantization. (arXiv:2310.11237v1 [cs.CL])

    [http://arxiv.org/abs/2310.11237](http://arxiv.org/abs/2310.11237)

    本论文提出了一种新颖的水印策略，通过在大型语言模型的量化过程中插入水印来保护模型权重，使得模型在推理时起作用但在量化后保持隐藏，为保护大型语言模型应用时的模型权重提供了一个潜在的方向。

    

    大型语言模型被滥用会带来高风险，因此保护模型权重以避免侵犯开源大型语言模型的许可是非常重要的。本文提出了一种新颖的水印策略，通过在大型语言模型的量化过程中插入水印，而无需在推理过程中事先定义触发器。这种水印在模型以fp32模式使用时起作用，在模型量化为int8后保持隐藏，这样用户只能进行模型的推理，而无需进行进一步的模型监督微调。我们成功地将水印嵌入到包括GPT-Neo和LLaMA在内的开源大型语言模型的权重中。希望我们提出的方法能够为保护大型语言模型应用时的模型权重提供一个潜在的方向。

    Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications.
    
[^4]: RealBehavior: 用于准确描绘基础模型人类化行为机制的框架

    RealBehavior: A Framework for Faithfully Characterizing Foundation Models' Human-like Behavior Mechanisms. (arXiv:2310.11227v1 [cs.CL])

    [http://arxiv.org/abs/2310.11227](http://arxiv.org/abs/2310.11227)

    RealBehavior框架旨在准确描绘基础模型的人类化行为，通过评估准确性和多样化对齐目标，提高结果的可信度。

    

    基础模型中出现人类化行为的报道与日俱增，心理学理论为研究这些行为提供了持久的工具。然而，当前的研究往往直接应用这些面向人类的工具，而没有验证其结果的准确性。在本文中，我们介绍了一个名为RealBehavior的框架，旨在准确描绘模型的人类化行为。除了简单测量行为外，我们的框架还根据可重现性、内外一致性和泛化性评估结果的准确性。我们的研究结果表明，简单应用心理学工具无法准确描绘所有的人类化行为。此外，我们讨论了将模型与人类和社会价值对齐的影响，并提出了多样化对齐目标的必要性，以防止创建具有受限特征的模型。

    Reports of human-like behaviors in foundation models are growing, with psychological theories providing enduring tools to investigate these behaviors. However, current research tends to directly apply these human-oriented tools without verifying the faithfulness of their outcomes. In this paper, we introduce a framework, RealBehavior, which is designed to characterize the humanoid behaviors of models faithfully. Beyond simply measuring behaviors, our framework assesses the faithfulness of results based on reproducibility, internal and external consistency, and generalizability. Our findings suggest that a simple application of psychological tools cannot faithfully characterize all human-like behaviors. Moreover, we discuss the impacts of aligning models with human and social values, arguing for the necessity of diversifying alignment objectives to prevent the creation of models with restricted characteristics.
    
[^5]: KG-GPT：一种使用大型语言模型在知识图谱上进行推理的通用框架

    KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models. (arXiv:2310.11220v1 [cs.CL])

    [http://arxiv.org/abs/2310.11220](http://arxiv.org/abs/2310.11220)

    KG-GPT是一个使用大型语言模型在知识图谱上进行推理的通用框架，通过句子分割、图检索和推理三个步骤实现了对知识图谱的复杂推理任务，并在事实验证和KGQA基准上展现了有竞争力和强健的性能。

    

    大型语言模型（LLMs）在理解和生成非结构化文本方面取得了可观的进展，但是它们在结构化数据上的应用仍然不够充分。特别是，在知识图谱（KGs）上使用LLMs进行复杂推理任务几乎没有被触及。为了解决这个问题，我们提出了KG-GPT，这是一个多用途框架，利用LLMs来执行涉及KGs的任务。KG-GPT包括三个步骤：句子分割、图检索和推理，每个步骤都旨在将句子分成部分、检索相关的图组件，并得出逻辑结论。我们使用基于KG的事实验证和KGQA基准对KG-GPT进行评估，结果表明该模型具有有竞争力和强健的性能，甚至胜过了几个完全监督的模型。因此，我们的工作在整合结构化和非结构化数据处理方面迈出了重要的一步。

    While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.
    
[^6]: 大型语言模型能否自我解释？LLM生成的自解释研究。

    Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations. (arXiv:2310.11207v1 [cs.CL])

    [http://arxiv.org/abs/2310.11207](http://arxiv.org/abs/2310.11207)

    本文研究了大型语言模型生成的自解释在情感分析和特征归因解释任务中的效果，并探讨了不同的引导方法。

    

    大型语言模型（LLMs）如ChatGPT在各种自然语言处理（NLP）任务中展现出优越的性能，包括情感分析、数学推理和摘要。此外，由于这些模型在人类对话中进行指导，以产生“有帮助”的回答，它们通常会在回答中提供解释，我们称之为自解释。例如，在分析电影评论的情感时，模型可以输出情感的积极性，并列出评论中带有情感的词语（如“fantastic”和“memorable”）作为解释。这些自动生成的自解释有多好？本文在情感分析和特征归因解释的任务中对此问题进行了研究，后者是可解释性文献中最常研究的设置（针对ChatGPT之前的模型）。具体来说，我们研究了不同的方法来引导模型生成自解释。

    Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit 
    
[^7]: 医学文本简化：通过非典型训练和重新排序的Beam Search解码优化可读性

    Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding. (arXiv:2310.11191v1 [cs.CL])

    [http://arxiv.org/abs/2310.11191](http://arxiv.org/abs/2310.11191)

    本研究探索了进一步提高医学领域文本简化可读性的方法，包括一种新的非典型损失和一种优化简单性的重新排序解码方法，取得了更好的性能。

    

    文本简化作为人工智能在专业领域（如医学）中弥合沟通差距的越来越有用的应用，已逐渐崭露头角。然而，医学简化方法有时会导致生成的文本质量和多样性下降。在本研究中，我们探索了进一步提高医学领域文本简化可读性的方法。我们提出了一种新的非典型吃亏损失干图片刺激生成更简单的术语，以及一种优化简单性的重新排序的Beam Search解码方法，在三个数据集上的可读性指标上取得了更好的性能。这项研究的发现为改进医学领域的文本简化提供了有希望的途径。

    Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to further improve the readability of text simplification in the medical domain. We propose (1) a new unlikelihood loss that encourages generation of simpler terms and (2) a reranked beam search decoding method that optimizes for simplicity, which achieve better performance on readability metrics on three datasets. This study's findings offer promising avenues for improving text simplification in the medical field.
    
[^8]: ViSoBERT：面向越南社交媒体文本处理的预训练语言模型

    ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing. (arXiv:2310.11166v1 [cs.CL])

    [http://arxiv.org/abs/2310.11166](http://arxiv.org/abs/2310.11166)

    ViSoBERT是首个用于越南社交媒体文本处理的预训练语言模型，通过在大规模越南社交媒体文本语料库上进行训练，该模型在情感识别、仇恨言论检测、情感分析、垃圾评论检测和仇恨言论跨度检测等任务上取得了良好的性能表现。

    

    英语和中文作为资源丰富的语言，在自然语言处理任务中已经见证了基于Transformer的语言模型的强大发展。尽管越南有大约1亿使用越南语的人口，但在一般的越南语自然语言处理任务中，如词性标注和命名实体识别，已经存在一些表现良好的预训练模型，例如PhoBERT、ViBERT和vELECTRA。然而，这些预训练语言模型仍然局限于越南社交媒体任务。本文提出了首个用于越南社交媒体文本的单语言预训练语言模型ViSoBERT，该模型使用XLM-R架构在大规模高质量多样化的越南社交媒体文本语料库上进行预训练。此外，我们还在越南社交媒体文本上探索了我们的预训练模型在五个重要的自然语言下游任务上的应用：情感识别、仇恨言论检测、情感分析、垃圾评论检测和仇恨言论跨度检测。我们的实验证明了ViSoBERT的有效性。

    English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture. Moreover, we explored our pre-trained model on five important natural language downstream tasks on Vietnamese social media texts: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Our experime
    
[^9]: IMTLab: 一个用于构建、评估和诊断交互式机器翻译系统的开源平台

    IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems. (arXiv:2310.11163v1 [cs.CL])

    [http://arxiv.org/abs/2310.11163](http://arxiv.org/abs/2310.11163)

    IMTLab是一个开源平台，用于构建、评估和诊断交互式机器翻译系统。它提供了一种任务导向的对话接口，支持灵活的架构和用户策略。前缀约束解码方法在端到端评估中表现出最低的编辑成本。

    

    我们提出了IMTLab，这是一个开源的端到端交互式机器翻译（IMT）系统平台，使研究人员能够快速构建具有最先进模型的IMT系统，进行端到端评估，并诊断系统的弱点。IMTLab将整个交互翻译过程视为任务导向的对话，并在其中引入人机交互，以产生高质量、无错误的翻译。为此，我们设计了一个通用的通信接口，以支持灵活的IMT架构和用户策略。基于所提出的设计，我们构建了一个模拟和真实的交互环境，实现了端到端评估，并利用该框架系统地评估了之前的IMT系统。我们的模拟和手动实验表明，在端到端评估中，前缀约束解码方法仍然具有最低的编辑成本，而BiTIIMT达到了可比的编辑成本。

    We present IMTLab, an open-source end-to-end interactive machine translation (IMT) system platform that enables researchers to quickly build IMT systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. IMTLab treats the whole interactive translation process as a task-oriented dialogue with a human-in-the-loop setting, in which human interventions can be explicitly incorporated to produce high-quality, error-free translations. To this end, a general communication interface is designed to support the flexible IMT architectures and user policies. Based on the proposed design, we construct a simulated and real interactive environment to achieve end-to-end evaluation and leverage the framework to systematically evaluate previous IMT systems. Our simulated and manual experiments show that the prefix-constrained decoding approach still gains the lowest editing cost in the end-to-end evaluation, while BiTIIMT achieves comparable editing cost
    
[^10]: 探索大型语言模型的创造力：模型能否产生不同的语义关联？

    Probing the Creativity of Large Language Models: Can models produce divergent semantic association?. (arXiv:2310.11158v1 [cs.CL])

    [http://arxiv.org/abs/2310.11158](http://arxiv.org/abs/2310.11158)

    本研究使用分散联想任务探索大型语言模型的创造性思维能力。研究发现，先进的模型可以生成不同的语义关联，超过了大多数人类的水平。

    

    大型语言模型在处理语言方面具有出色的能力，但这些模型是否能进一步生成创造性的内容仍不清楚。本研究旨在从认知角度研究大型语言模型的创造思维能力。我们利用分散联想任务（DAT），这是一种客观衡量创造力的方法，要求模型生成不相关的单词，并计算它们之间的语义距离。我们比较了不同模型和解码策略的结果。我们的研究发现：（1）在使用贪婪搜索策略时，GPT-4超过了96％的人类，而GPT-3.5-turbo超过了平均水平；（2）对于模型而言，随机抽样和温度调节是获得更高DAT分数的有效方法，但在创造力和稳定性之间存在权衡。这些结果表明，先进的大型语言模型具有不同的语义关联，这是创造力的基本过程。

    Large language models possess remarkable capacity for processing language, but it remains unclear whether these models can further generate creative content. The present study aims to investigate the creative thinking of large language models through a cognitive perspective. We utilize the divergent association task (DAT), an objective measurement of creativity that asks models to generate unrelated words and calculates the semantic distance between them. We compare the results across different models and decoding strategies. Our findings indicate that: (1) When using the greedy search strategy, GPT-4 outperforms 96% of humans, while GPT-3.5-turbo exceeds the average human level. (2) Stochastic sampling and temperature scaling are effective to obtain higher DAT scores for models except GPT-4, but face a trade-off between creativity and stability. These results imply that advanced large language models have divergent semantic associations, which is a fundamental process underlying creat
    
[^11]: 语言与大型语言模型之间的去往何方

    The Quo Vadis of the Relationship between Language and Large Language Models. (arXiv:2310.11146v1 [cs.CL])

    [http://arxiv.org/abs/2310.11146](http://arxiv.org/abs/2310.11146)

    大型语言模型（LLMs）在语言科学模型中的地位引发了关于其透明度和信息提供能力的讨论，当前阶段的LLMs几乎对语言没有提供任何解释，未来研究需要探索更具信息价值的方向。

    

    在人工（通用）智能（AI）领域中，依赖于大型语言模型（LLMs）的自然语言处理（NLP）活动的几个最近的进展鼓励将LLMs作为语言科学模型。虽然用于描述LLMs的术语倾向于将其作为科学模型，但它们是否能够为他们试图表示的目标系统提供洞见尚不清楚。我们在确定缺乏透明度的科学模型带来的最重要的理论和实证风险之后，论述了LLMs，并将其与每个科学模型的基本组成部分（对象、媒介、含义和用户）相关联。我们得出结论，目前发展阶段的LLMs几乎对语言没有提供任何解释，然后对这个主题的未来研究方向提供了一个更具信息价值的展望。

    In the field of Artificial (General) Intelligence (AI), the several recent advancements in Natural language processing (NLP) activities relying on Large Language Models (LLMs) have come to encourage the adoption of LLMs as scientific models of language. While the terminology employed for the characterization of LLMs favors their embracing as such, it is not clear that they are in a place to offer insights into the target system they seek to represent. After identifying the most important theoretical and empirical risks brought about by the adoption of scientific models that lack transparency, we discuss LLMs relating them to every scientific model's fundamental components: the object, the medium, the meaning and the user. We conclude that, at their current stage of development, LLMs hardly offer any explanations for language, and then we provide an outlook for more informative future research directions on this topic.
    
[^12]: Simultaneous Speech Translation in Long-Form Setting: Thesis Proposal

    Long-form Simultaneous Speech Translation: Thesis Proposal. (arXiv:2310.11141v1 [cs.CL])

    [http://arxiv.org/abs/2310.11141](http://arxiv.org/abs/2310.11141)

    这个论文提案研究了端到端长篇同传，即在没有预先分割的情况下进行实时口语翻译。它调查了最新的端到端同传进展，评估了同传中的主要障碍和与长篇场景的相关性，并提出了解决这些挑战的方法。

    

    同传技术旨在实时将口语翻译成其他语言，甚至在说话人未完成句子之前即可实现翻译。传统上，同传主要通过级联系统来处理，将任务分解为语音识别、分割和机器翻译等子任务。然而，深度学习的出现引发了对端到端系统的广泛兴趣。然而，当前文献中大多数端到端同传方法的一个主要限制是它们假设源语音已经被预先分割成句子，这对于实际应用和真实世界中的场景是一个重要障碍。这个论文提案着重于处理端到端长篇同传问题，即在没有预先分割的情况下进行翻译。我们介绍了最新的端到端同传进展的调查，评估了同传中的主要障碍以及与长篇场景的相关性，并提出了解决这些挑战的方法。

    Simultaneous speech translation (SST) aims to provide real-time translation of spoken language, even before the speaker finishes their sentence. Traditionally, SST has been addressed primarily by cascaded systems that decompose the task into subtasks, including speech recognition, segmentation, and machine translation. However, the advent of deep learning has sparked significant interest in end-to-end (E2E) systems. Nevertheless, a major limitation of most approaches to E2E SST reported in the current literature is that they assume that the source speech is pre-segmented into sentences, which is a significant obstacle for practical, real-world applications. This thesis proposal addresses end-to-end simultaneous speech translation, particularly in the long-form setting, i.e., without pre-segmentation. We present a survey of the latest advancements in E2E SST, assess the primary obstacles in SST and its relevance to long-form scenarios, and suggest approaches to tackle these challenges.
    
[^13]: 用于打击虚假信息的人工智能技术的实验：IDMO项目

    Experimenting AI Technologies for Disinformation Combat: the IDMO Project. (arXiv:2310.11097v1 [cs.CL])

    [http://arxiv.org/abs/2310.11097](http://arxiv.org/abs/2310.11097)

    IDMO项目旨在使用人工智能技术打击虚假信息和假新闻，其贡献包括创建新型数据集、开发自动模型、评估GPT-4等。

    

    意大利数字媒体观察项目（IDMO）是欧洲一项倡议的一部分，专注于打击虚假信息和假新闻。本报告概述了Rai-CRITS在该项目中的贡献，包括：（i）创建用于测试技术的新型数据集，（ii）开发自动模型，用于分类Pagella Politica的裁决以便于更广泛的分析，（iii）创建自动模型，对FEVER数据集上的文本蕴含具有异常精度的识别能力，（iv）使用GPT-4评估文本蕴含， （v）在国家活动中开展提高对假新闻意识的游戏。

    The Italian Digital Media Observatory (IDMO) project, part of a European initiative, focuses on countering disinformation and fake news. This report outlines contributions from Rai-CRITS to the project, including: (i) the creation of novel datasets for testing technologies (ii) development of an automatic model for categorizing Pagella Politica verdicts to facilitate broader analysis (iii) creation of an automatic model for recognizing textual entailment with exceptional accuracy on the FEVER dataset (iv) assessment using GPT-4 to identify textual entailmen (v) a game to raise awareness about fake news at national events.
    
[^14]: 基于预训练语言模型的上下文少样本关系抽取

    In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])

    [http://arxiv.org/abs/2310.11085](http://arxiv.org/abs/2310.11085)

    本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。

    

    关系提取旨在从文本文档中推断结构化的人类知识。基于语言模型的最先进方法通常有两个限制：(1)它们要求命名实体作为输入或推断它们，从而引入了额外的噪声，(2)它们需要人工对文档进行注释。为解决这些问题，我们提出了一种新颖的基于预训练语言模型的上下文少样本关系抽取框架。据我们所知，我们是第一个将关系抽取任务重新定义为定制的上下文少样本学习范式的研究者。通过这种方式，我们在消除了命名实体识别和文档人工注释的需求的同时，实现了关键性的优势。与现有的基于微调的方法不同，我们的框架具有灵活性，可以在无需重新训练的情况下轻松更新到新的关系集合。我们使用DocRED评估了我们的框架，这是目前最大的公开可用的文档级关系提取数据集。

    Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
    
[^15]: 使用有监督的对比预训练变换器理解社交媒体中的写作风格

    Understanding writing style in social media with a supervised contrastively pre-trained transformer. (arXiv:2310.11081v1 [cs.CL])

    [http://arxiv.org/abs/2310.11081](http://arxiv.org/abs/2310.11081)

    该论文介绍了一种使用有监督对比预训练变换器(Supervised Contrastively Pre-trained Transformer)来理解社交媒体中的写作风格的方法，该方法能够有效地将内容与其作者相关联，从而更好地理解有害行为。

    

    在线社交网络成为有害行为的肥沃土壤，从仇恨言论到虚假信息的传播。恶意行为者现在有了前所未有的自由来行使不当行为，导致严重的社会动荡和严重后果，如美国总统选举期间的国会山袭击事件和COVID-19大流行期间的反疫苗运动。理解在线语言比以往任何时候都更加迫切。虽然现有的研究主要关注内容分析，但我们的目标是将重点转移到通过将内容与其各自的作者相关联来理解有害行为。许多新颖的方法尝试学习文本中作者的风格特征，但其中许多方法受到小数据集或次优训练损失的限制。为了克服这些限制，我们引入了基于来自公共来源的大型语料库的Style Transformer for Authorship Representations (STAR)进行训练，其语料库规模达到4.5 x 10^6 auth。

    Online Social Networks serve as fertile ground for harmful behavior, ranging from hate speech to the dissemination of disinformation. Malicious actors now have unprecedented freedom to misbehave, leading to severe societal unrest and dire consequences, as exemplified by events such as the Capitol assault during the US presidential election and the Antivaxx movement during the COVID-19 pandemic. Understanding online language has become more pressing than ever. While existing works predominantly focus on content analysis, we aim to shift the focus towards understanding harmful behaviors by relating content to their respective authors. Numerous novel approaches attempt to learn the stylistic features of authors in texts, but many of these approaches are constrained by small datasets or sub-optimal training losses. To overcome these limitations, we introduce the Style Transformer for Authorship Representations (STAR), trained on a large corpus derived from public sources of 4.5 x 10^6 auth
    
[^16]: 从红队操作中学习：大型语言模型中的性别偏见挑衅和缓解

    Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models. (arXiv:2310.11079v1 [cs.CL])

    [http://arxiv.org/abs/2310.11079](http://arxiv.org/abs/2310.11079)

    这项研究提出了一种自动生成测试用例以检测大型语言模型（LLMs）中潜在性别偏见的方法，并提出了使用生成的测试用例作为上下文学习的演示来缓解偏见的存在。实验证明，采用该方法后，LLMs能够生成更公平的响应。

    

    最近，随着ChatGPT和GPT-4等大型语言模型（LLM）的进展，研究人员在对话系统方面取得了可观的改进。这些基于LLM的聊天机器人在保留可能伤害人类的不平等的同时，编码了潜在的偏见。传统的偏见调查方法通常依赖于人工编写的测试用例。然而，这些测试用例通常成本高昂且有限。在这项工作中，我们提出了一种独创的方法，自动生成用于检测LLMs潜在性别偏见的测试用例。我们将该方法应用于三种著名的LLM，并发现生成的测试用例有效地识别出了偏见的存在。针对所发现的偏见，我们提出了一种缓解策略，利用生成的测试用例作为上下文学习的演示，来规避参数微调的需要。实验结果显示，LLMs通过这种提议的方法生成了更公平的响应。

    Recently, researchers have made considerable improvements in dialogue systems with the progress of large language models (LLMs) such as ChatGPT and GPT-4. These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. The traditional biases investigation methods often rely on human-written test cases. However, these test cases are usually expensive and limited. In this work, we propose a first-of-its-kind method that automatically generates test cases to detect LLMs' potential gender bias. We apply our method to three well-known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.
    
[^17]: VoxArabica：一个稳健的方言感知阿拉伯语音识别系统

    VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System. (arXiv:2310.11069v1 [cs.CL])

    [http://arxiv.org/abs/2310.11069](http://arxiv.org/abs/2310.11069)

    VoxArabica是一个稳健的方言感知阿拉伯语音识别系统，通过开发和演示，实现了阿拉伯语方言识别和自动语音识别。该系统训练了各种模型用于不同方言的识别，并提供了多种功能的网络界面。

    

    阿拉伯语是一种复杂的语言，全球有超过4.5亿人口使用许多不同的方言和口音。由于语言的多样性和变化，为阿拉伯语构建一个稳健且通用的语音识别系统具有挑战性。在这项工作中，我们通过开发和演示一个名为VoxArabica的系统，解决了这个问题，用于方言识别(DID)和阿拉伯语自动语音识别(ASR)。我们在监督环境下训练了各种模型，例如HuBERT(DID)、Whisper和XLS-R(ASR)，用于阿拉伯语的DID和ASR任务。我们的DID模型被训练用于识别除了标准阿拉伯之外的17种不同的方言。我们在标准阿拉伯语(MSA)、埃及语、摩洛哥语和混合数据上微调我们的ASR模型。此外，对于ASR中的其他方言，我们提供了Whisper和MMS等不同模型的选择。我们将这些模型集成到一个单一的网络界面中，具有多样的功能，如音频录制、上传文件、模型选择和提出问题的选项。

    Arabic is a complex language with many varieties and dialects spoken by over 450 millions all around the world. Due to the linguistic diversity and variations, it is challenging to build a robust and generalized ASR system for Arabic. In this work, we address this gap by developing and demoing a system, dubbed VoxArabica, for dialect identification (DID) as well as automatic speech recognition (ASR) of Arabic. We train a wide range of models such as HuBERT (DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR tasks. Our DID models are trained to identify 17 different dialects in addition to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data. Additionally, for the remaining dialects in ASR, we provide the option to choose various models such as Whisper and MMS in a zero-shot setting. We integrate these models into a single web interface with diverse features such as audio recording, file upload, model selection, and the option to raise fl
    
[^18]: Denevil: 通过指导学习来解读和引导大型语言模型的道德价值

    Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning. (arXiv:2310.11053v1 [cs.CL])

    [http://arxiv.org/abs/2310.11053](http://arxiv.org/abs/2310.11053)

    通过Moral Foundation Theory和DeNEVIL算法，我们研究了大型语言模型的道德价值，并构建了MoralPrompt数据集来评估模型的内在价值。发现大多数模型存在不对齐，需要进一步进行道德价值对齐。

    

    大型语言模型（LLM）取得了前所未有的突破，然而它们被越来越多地整合到日常生活中可能会带来由生成的不道德内容引起的社会风险。尽管已经对特定问题如偏见进行了广泛研究，但是从道德哲学的角度来看，LLM的内在价值仍然很少被探索。这项工作利用道德基础理论深入探讨道德价值。我们提出了DeNEVIL，一种新的提示生成算法，旨在动态利用LLM的价值脆弱性并以生成方式揭示伦理违规行为，揭示其潜在的价值倾向。在此基础上，我们构建了MoralPrompt，一个包含2,397个提示的高质量数据集，涵盖500多个价值原则，并对一系列LLM的内在价值进行了基准测试。我们发现大多数模型实质上是不对齐的，需要进一步进行道德价值对齐。

    Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In r
    
[^19]: SemEval-2023任务6中的非纳任务:法律评估方法论。(arXiv:2310.11049v1 [cs.CL])

    Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation. (arXiv:2310.11049v1 [cs.CL])

    [http://arxiv.org/abs/2310.11049](http://arxiv.org/abs/2310.11049)

    这篇论文介绍了我们在SemEval-2023法律评估任务6上的提交，主要集中在法律命名实体识别、法律判决预测和带解释的法院判决预测等子任务上。我们进行了多个实验，并取得了在各个子任务中具有竞争力的排名。

    

    本文描述了我们在SemEval-2023法律评估任务6上的提交。我们的提交主要集中在三个子任务上：任务B的法律命名实体识别(L-NER)，任务C1的法律判决预测(LJP)和任务C2的带解释的法院判决预测(CJPE)。我们对这些子任务进行了各种实验，并详细呈现了结果，包括数据统计和方法论。值得注意的是，像本研究中所涉及的法律任务正在因自动化法律分析和支持的需求增加而变得越来越重要。我们的团队在排行榜上报告的任务B、任务C1和任务C2中分别获得了15th、11th和1st的竞争排名。

    This paper describes our submission to the SemEval-2023 for Task 6 on LegalEval: Understanding Legal Texts. Our submission concentrated on three subtasks: Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment Prediction (LJP) for Task-C1, and Court Judgment Prediction with Explanation (CJPE) for Task-C2. We conducted various experiments on these subtasks and presented the results in detail, including data statistics and methodology. It is worth noting that legal tasks, such as those tackled in this research, have been gaining importance due to the increasing need to automate legal analysis and support. Our team obtained competitive rankings of 15$^{th}$, 11$^{th}$, and 1$^{st}$ in Task-B, Task-C1, and Task-C2, respectively, as reported on the leaderboard.
    
[^20]: 歌词创作者歌手熵影响歌词-创作者分类的性能

    Lyricist-Singer Entropy Affects Lyric-Lyricist Classification Performance. (arXiv:2310.11035v1 [cs.SD])

    [http://arxiv.org/abs/2310.11035](http://arxiv.org/abs/2310.11035)

    本文研究了歌词创作者和演唱者之间的关系，发现歌词-创作者分类的性能取决于演唱者的多样性。

    

    虽然歌词是音乐的重要组成部分，但对歌词创作者的特征进行音乐信息处理研究的数量很少。由于这些特征在音乐应用中可能非常有价值，如推荐系统，因此需要进一步研究。我们考虑了一种从歌词中提取代表歌词创作者特征的潜在方法。由于这些特征必须在提取之前进行识别，我们将重点放在具有易于识别特征的歌词创作者上。我们认为歌手应该演唱具有特定于歌手的某些特征的独特歌曲。因此，我们假设歌词创作者对他们为其写歌歌手的独特特征负有责任。换句话说，歌词-创作者分类的性能或从歌词中捕捉到歌词创作者特征的难易程度可能取决于歌手的多样性。在这项研究中，我们观察到歌词创作者和歌手之间的关系。

    Although lyrics represent an essential component of music, few music information processing studies have been conducted on the characteristics of lyricists. Because these characteristics may be valuable for musical applications, such as recommendations, they warrant further study. We considered a potential method that extracts features representing the characteristics of lyricists from lyrics. Because these features must be identified prior to extraction, we focused on lyricists with easily identifiable features. We believe that it is desirable for singers to perform unique songs that share certain characteristics specific to the singer. Accordingly, we hypothesized that lyricists account for the unique characteristics of the singers they write lyrics for. In other words, lyric-lyricist classification performance or the ease of capturing the features of a lyricist from the lyrics may depend on the variety of singers. In this study, we observed a relationship between lyricist-singer ent
    
[^21]: 基于解码器的LLM的文本生成自动评估方法探索

    Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation. (arXiv:2310.11026v1 [cs.CL])

    [http://arxiv.org/abs/2310.11026](http://arxiv.org/abs/2310.11026)

    本文探索了基于解码器的LLM的文本生成自动评估方法，发现相比调优后的基于编码器的模型，调优后的基于解码器的模型在机器翻译评估和语义文本相似性任务上表现较差，原因是解码器模型更关注表面词序列而忽略了含义，并且非常大的解码器模型的上下文学习使得难以识别细粒度的语义差异。

    

    对于提高生成任务准确性，文本生成的自动评估至关重要。鉴于当前趋势是使用越来越大的基于解码器的语言模型，我们研究基于这种模型的文本生成自动评估方法。本文在相同条件下比较了不同方法，包括使用基于编码器的模型和大型语言模型进行调优，分别在两种不同任务（机器翻译评估和语义文本相似性）以及两种语言（日语和英语）上进行实验。实验结果表明，与调优后的基于编码器的模型相比，调优后的基于解码器的模型表现较差。对于造成这种情况的原因的分析表明，基于解码器的模型更关注表面词序列而忽略了含义。研究还显示，像ChatGPT这样的非常大的解码器模型的上下文学习使得难以识别细粒度的语义差异。

    Automatic evaluation of text generation is essential for improving the accuracy of generation tasks. In light of the current trend towards increasingly larger decoder-based language models, we investigate automatic evaluation methods based on such models for text generation. This paper compares various methods, including tuning with encoder-based models and large language models under equal conditions, on two different tasks, machine translation evaluation and semantic textual similarity, in two languages, Japanese and English. Experimental results show that compared to the tuned encoder-based models, the tuned decoder-based models perform poorly. The analysis of the causes for this suggests that the decoder-based models focus on surface word sequences and do not capture meaning. It is also revealed that in-context learning of very large decoder-based models such as ChatGPT makes it difficult to identify fine-grained semantic differences.
    
[^22]: 从视觉丰富的文档中提取信息的阅读顺序的重要性：通过令牌路径预测来实现

    Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction. (arXiv:2310.11016v1 [cs.CL])

    [http://arxiv.org/abs/2310.11016](http://arxiv.org/abs/2310.11016)

    本研究探讨了在视觉丰富文档中提取信息的问题，并通过引入令牌路径预测的方法来解决实体标记的阅读顺序问题。该方法将文档布局建模为令牌的有向图，并预测实体的令牌路径。

    

    最近多模态预训练模型的进展显著提高了对视觉丰富文档（VrDs）中的信息提取，其中命名实体识别（NER）被视为预测令牌的BIO实体标签的序列标注任务，遵循NLP的典型设置。然而，BIO标记方案依赖于模型输入的正确顺序，在OCR系统识别和安排文本的扫描VrDs中无法保证。这种阅读顺序问题阻碍了通过BIO标记方案准确标记实体，使得序列标注方法无法预测正确的命名实体。为了解决阅读顺序问题，我们引入了令牌路径预测（TPP），这是一个简单的预测头，用于在文档中预测作为令牌序列的实体提及。与令牌分类不同，TPP将文档布局建模为令牌的完整有向图，并预测实体在图中的令牌路径。

    Recent advances in multimodal pre-trained models have significantly improved information extraction from visually-rich documents (VrDs), in which named entity recognition (NER) is treated as a sequence-labeling task of predicting the BIO entity tags for tokens, following the typical setting of NLP. However, BIO-tagging scheme relies on the correct order of model inputs, which is not guaranteed in real-world NER on scanned VrDs where text are recognized and arranged by OCR systems. Such reading order issue hinders the accurate marking of entities by BIO-tagging scheme, making it impossible for sequence-labeling methods to predict correct named entities. To address the reading order issue, we introduce Token Path Prediction (TPP), a simple prediction head to predict entity mentions as token sequences within documents. Alternative to token classification, TPP models the document layout as a complete directed graph of tokens, and predicts token paths within the graph as entities. For bette
    
[^23]: 为语音识别进行纠错的语言模型训练

    Correction Focused Language Model Training for Speech Recognition. (arXiv:2310.11003v1 [cs.CL])

    [http://arxiv.org/abs/2310.11003](http://arxiv.org/abs/2310.11003)

    本研究介绍了一种为语音识别进行纠错的语言模型训练方法，通过定义易出错单词的分数并将其用作先验分布来指导训练，并利用大型语言模型进行纠错型训练。实验证明该方法在领域适应任务中有效，相对传统方法可以显著降低单词错误率。

    

    语言模型（LM）通常被用于提高自动语音识别（ASR）的性能，尤其是在领域适应任务中。传统的LM训练方式将语料库中的所有单词平等对待，导致ASR性能的改进效果不佳。本文引入了一种新颖的纠错型LM训练方法，旨在优先处理ASR易出错的单词。我们定义了单词级ASR易出错分数，表示ASR错误识别的可能性，并将其形成一个先验单词分布以指导LM训练。为了在仅有文本语料库的情况下实现纠错型训练，我们采用大型语言模型（LLM）作为易出错分数预测器和文本生成器，并进行多任务微调。领域适应任务的实验结果表明了我们提出的方法的有效性。与传统的LM相比，纠错型训练在足够数据集上可以达到相对5.5%的单词错误率（WER）降低。

    Language models (LMs) have been commonly adopted to boost the performance of automatic speech recognition (ASR) particularly in domain adaptation tasks. Conventional way of LM training treats all the words in corpora equally, resulting in suboptimal improvements in ASR performance. In this work, we introduce a novel correction focused LM training approach which aims to prioritize ASR fallible words. The word-level ASR fallibility score, representing the likelihood of ASR mis-recognition, is defined and shaped as a prior word distribution to guide the LM training. To enable correction focused training with text-only corpora, large language models (LLMs) are employed as fallibility score predictors and text generators through multi-task fine-tuning. Experimental results for domain adaptation tasks demonstrate the effectiveness of our proposed method. Compared with conventional LMs, correction focused training achieves up to relatively 5.5% word error rate (WER) reduction in sufficient te
    
[^24]: 使用查询聚合的指导性对话摘要

    Instructive Dialogue Summarization with Query Aggregations. (arXiv:2310.10981v1 [cs.CL])

    [http://arxiv.org/abs/2310.10981](http://arxiv.org/abs/2310.10981)

    传统的对话摘要方法无法考虑用户的特定兴趣，而指导对话摘要的引入可以帮助扩展对话摘要模型的能力。我们提出了一个三步方法来合成高质量的查询摘要三元组，并通过在多个数据集上训练一个统一模型来扩展对话摘要模型的能力。

    

    传统的对话摘要方法直接生成摘要，不考虑用户的特定兴趣。这在用户更加关注特定主题或方面的情况下会带来挑战。随着指导调优语言模型的进步，我们引入了指导对话来扩展对话摘要模型的能力集。为了克服指导性对话摘要数据的稀缺性，我们提出了一种三步方法来合成高质量的基于查询的摘要三元组。这个过程包括以摘要为锚点的查询生成、查询过滤和基于查询的摘要生成。通过在三个摘要数据集上训练一个统一的模型InstructDS（指导性对话摘要），我们扩展了对话摘要模型的能力。我们在包括对话摘要和对话阅读理解的四个数据集上对我们的方法进行评估。

    Conventional dialogue summarization methods directly generate summaries and do not consider user's specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to synthesize high-quality query-based summarization triples. This process involves summary-anchored query generation, query filtering, and query-based summary generation. By training a unified model called InstructDS (Instructive Dialogue Summarization) on three summarization datasets with multi-purpose instructive triples, we expand the capability of dialogue summarization models. We evaluate our method on four datasets, including dialogue summarization and dialogue reading comprehension. Experimental re
    
[^25]: EXMODD:一种解释性多模态开放领域对话数据集

    EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset. (arXiv:2310.10967v1 [cs.CL])

    [http://arxiv.org/abs/2310.10967](http://arxiv.org/abs/2310.10967)

    提出了一种解释性多模态开放领域对话数据集，通过多模态数据构建框架(MDCF)设计适当的提示，从而解决了大型模型对于多模态输入的缺乏、生成内容缺乏可解释性以及数据质量控制的问题。

    

    高质量数据的需求一直是阻碍对话任务研究的关键问题。最近的研究尝试通过手工、网络爬虫和大规模预训练模型构建数据集。然而，人工数据成本高昂，从互联网收集的数据往往包含通俗回答、无意义的陈述和有害对话。通过大型模型进行自动数据生成是一种成本效益较高的方法，但对于多模态开放领域对话任务，仍存在三个缺点：1) 目前还没有能接受多模态输入的开源大型模型；2) 模型生成的内容缺乏可解释性；3) 生成的数据通常难以进行质量控制并需要大量资源进行收集。为了减轻数据收集中的重要人力和资源开支，我们提出了一种多模态数据构建框架(MDCF)。MDCF设计适当的提示来推动大规模预训练语言模型生成形式良好且令人满意的内容。

    The need for high-quality data has been a key issue hindering the research of dialogue tasks. Recent studies try to build datasets through manual, web crawling, and large pre-trained models. However, man-made data is expensive and data collected from the internet often includes generic responses, meaningless statements, and toxic dialogues. Automatic data generation through large models is a cost-effective method, but for open-domain multimodal dialogue tasks, there are still three drawbacks: 1) There is currently no open-source large model that can accept multimodal input; 2) The content generated by the model lacks interpretability; 3) The generated data is usually difficult to quality control and require extensive resource to collect. To alleviate the significant human and resource expenditure in data collection, we propose a Multimodal Data Construction Framework (MDCF). MDCF designs proper prompts to spur the large-scale pre-trained language model to generate well-formed and satis
    
[^26]: 带有大型语言模型的语义感知对比句子表示学习

    Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models. (arXiv:2310.10962v1 [cs.CL])

    [http://arxiv.org/abs/2310.10962](http://arxiv.org/abs/2310.10962)

    本论文提出了SemCSR框架，利用大型语言模型（LLM）的生成和评估能力，无需人工标注就能自动构建高质量的自然语言推理样式语料库，用于解决对比学习中的挑战。

    

    对比学习已被证明在学习更好的句子表示方面非常有效。然而，为了训练对比学习模型，需要大量带标签的句子来明确构建正负对（例如在自然语言推理数据集中）。不幸的是，获取足够高质量的标注数据既费时又耗资源，因此研究人员开始关注开发无监督句子表示学习方法。由于这些非结构化的随机抽样句子之间没有明确的关联，构建正负对可能会很困难和有问题。为了解决这些挑战，在本文中，我们提出了一种语义感知的对比句子表示框架（SemCSR）。通过利用大型语言模型（LLM）的生成和评估能力，我们可以自动构建一个高质量的自然语言推理样式语料库，而无需任何人工标注。

    Contrastive learning has been proven to be effective in learning better sentence representations. However, to train a contrastive learning model, large numbers of labeled sentences are required to construct positive and negative pairs explicitly, such as those in natural language inference (NLI) datasets. Unfortunately, acquiring sufficient high-quality labeled data can be both time-consuming and resource-intensive, leading researchers to focus on developing methods for learning unsupervised sentence representations. As there is no clear relationship between these unstructured randomly-sampled sentences, building positive and negative pairs over them is tricky and problematic. To tackle these challenges, in this paper, we propose SemCSR, a semantic-aware contrastive sentence representation framework. By leveraging the generation and evaluation capabilities of large language models (LLMs), we can automatically construct a high-quality NLI-style corpus without any human annotation, and f
    
[^27]: 通过对英语语言的几何分析来计算最佳键盘设计

    Computing the optimal keyboard through a geometric analysis of the English language. (arXiv:2310.10956v1 [cs.CL])

    [http://arxiv.org/abs/2310.10956](http://arxiv.org/abs/2310.10956)

    通过几何分析，我们计算出了一种最佳键盘设计，提供更快的打字速度。

    

    在COMSW4995 002 - 几何数据分析课程的小组项目中，我们将关注点放在设计快速打字键盘上。利用一些几何工具在优化框架中提供了更快打字的创新键盘布局。

    In the context of a group project for the course COMSW4995 002 - Geometric Data Analysis, we bring our attention to the design of fast-typing keyboards. Leveraging some geometric tools in an optimization framework allowed us to propose novel keyboard layouts that offer a faster typing.
    
[^28]: 一个用于数据集效果的状态向量框架

    A State-Vector Framework for Dataset Effects. (arXiv:2310.10955v1 [cs.CL])

    [http://arxiv.org/abs/2310.10955](http://arxiv.org/abs/2310.10955)

    本研究提出了一个状态向量框架，用于系统地研究数据集的效果。我们发现一些常用的语言理解数据集对模型有显著的效果，这些效果集中在几个语言维度上。此外，我们观察到数据集可能对模型的非相关维度产生"溢出"效应。这个框架为负责任和鲁棒模型开发中的数据集效果提供了一个系统的理解。

    

    近期基于深度神经网络（DNN）的系统的成功很大程度上受到了用于训练的高质量数据集的影响。然而，数据集的效果，特别是它们之间的相互作用，仍然不够深入研究。本文提出了一个状态向量框架，以便在这个方向上进行严格的研究。该框架将理想化探测测试结果作为向量空间的基础。该框架使我们能够量化独立和互动数据集的效果。我们发现一些常用的语言理解数据集的显著效果是特征性的，并且集中在几个语言维度上。此外，我们还观察到一些"溢出"效应：数据集可能会影响模型在看似与预期任务无关的维度上的表现。我们的状态向量框架为系统地理解数据集效果，这是负责任和鲁棒模型开发中的关键组成部分，铺平了道路。

    The impressive success of recent deep neural network (DNN)-based systems is significantly influenced by the high-quality datasets used in training. However, the effects of the datasets, especially how they interact with each other, remain underexplored. We propose a state-vector framework to enable rigorous studies in this direction. This framework uses idealized probing test results as the bases of a vector space. This framework allows us to quantify the effects of both standalone and interacting datasets. We show that the significant effects of some commonly-used language understanding datasets are characteristic and are concentrated on a few linguistic dimensions. Additionally, we observe some ``spill-over'' effects: the datasets could impact the models along dimensions that may seem unrelated to the intended tasks. Our state-vector framework paves the way for a systematic understanding of the dataset effects, a crucial component in responsible and robust model development.
    
[^29]: TEQ：用于LLM量化的可训练等效转换

    TEQ: Trainable Equivalent Transformation for Quantization of LLMs. (arXiv:2310.10944v1 [cs.CL])

    [http://arxiv.org/abs/2310.10944](http://arxiv.org/abs/2310.10944)

    本文提出了TEQ，一种可训练的等效转换方法，可以在保持模型准确性的同时，利用低精度的量化方式满足大型语言模型的计算要求。与现有的最先进方法相比，我们的方法在典型的LLMs上具有相似的表现，且不增加推理时的计算开销。

    

    随着大型语言模型（LLMs）变得越来越普遍，需要新的、改进的量化方法以满足这些现代架构的计算要求，同时保持准确性。在本文中，我们提出了TEQ，一种可训练的等效转换，可以保留模型输出的FP32精度，同时利用低精度量化，特别是3位和4位的仅权重量化。训练过程轻量化，只需1K步骤和少于原模型可训练参数的0.1%。此外，在推理过程中，该转换不会增加任何计算开销。我们的结果与典型LLMs的最先进方法相媲美。我们的方法可以与其他方法结合使用，以实现更好的性能。代码可在https://github.com/intel/neural-compressor获得。

    As large language models (LLMs) become more prevalent, there is a growing need for new and improved quantization methods that can meet the computationalast layer demands of these modern architectures while maintaining the accuracy. In this paper, we present TEQ, a trainable equivalent transformation that preserves the FP32 precision of the model output while taking advantage of low-precision quantization, especially 3 and 4 bits weight-only quantization. The training process is lightweight, requiring only 1K steps and fewer than 0.1 percent of the original model's trainable parameters. Furthermore, the transformation does not add any computational overhead during inference. Our results are on-par with the state-of-the-art (SOTA) methods on typical LLMs. Our approach can be combined with other methods to achieve even better performance. The code is available at https://github.com/intel/neural-compressor.
    
[^30]: MASON-NLP在eRisk 2023上的贡献：基于深度学习的社交媒体文本检测抑郁症状

    MASON-NLP at eRisk 2023: Deep Learning-Based Detection of Depression Symptoms from Social Media Texts. (arXiv:2310.10941v1 [cs.CL])

    [http://arxiv.org/abs/2310.10941](http://arxiv.org/abs/2310.10941)

    MASON-NLP提出了一种基于深度学习的方法，通过分析社交媒体文本检测抑郁症状。任务1的目标是评估不同条件的相关性。

    

    抑郁症是一种对人们生活产生深远影响的心理健康障碍。最近的研究表明，可以通过个体的交流方式（包括口头和书面文字）检测到抑郁症的迹象。特别是，社交媒体帖子是一个可供我们检查抑郁症状的丰富而方便的文本来源。贝克抑郁量表（BDI）问卷通常用于衡量抑郁症的严重程度，它是本研究中可以辅助使用的工具之一。我们可以将研究范围缩小到这些症状，因为每个BDI问题都与特定的抑郁症状相关联。需要记住，并非每个抑郁症患者同时表现出所有症状，而是其中的一些综合出现。因此，能确定一个句子或一段用户生成的内容与特定条件相关是非常有价值的。基于这一点，eRisk 2023任务1的设计目的正是为了评估不同条件的相关性。

    Depression is a mental health disorder that has a profound impact on people's lives. Recent research suggests that signs of depression can be detected in the way individuals communicate, both through spoken words and written texts. In particular, social media posts are a rich and convenient text source that we may examine for depressive symptoms. The Beck Depression Inventory (BDI) Questionnaire, which is frequently used to gauge the severity of depression, is one instrument that can aid in this study. We can narrow our study to only those symptoms since each BDI question is linked to a particular depressive symptom. It's important to remember that not everyone with depression exhibits all symptoms at once, but rather a combination of them. Therefore, it is extremely useful to be able to determine if a sentence or a piece of user-generated content is pertinent to a certain condition. With this in mind, the eRisk 2023 Task 1 was designed to do exactly that: assess the relevance of diffe
    
[^31]: 基于家庭助手的意图检测和槽位填充：孟加拉语和锡尔赫蒂语的数据集和分析

    Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti. (arXiv:2310.10935v1 [cs.CL])

    [http://arxiv.org/abs/2310.10935](http://arxiv.org/abs/2310.10935)

    这项研究引入了第一个用于孟加拉语和锡尔赫蒂语的意图检测和槽位填充的全面数据集，并发现大型语言模型在处理不充足数据的下游任务上表现出强大能力。

    

    随着语音助手在我们高度技术化的社会中的地位得到巩固，有必要适应多样化的语言环境，包括低资源语言的口语形式。我们的研究引入了首个用于正式孟加拉语、口语孟加拉语和锡尔赫蒂语的意图检测和槽位填充的全面数据集，总共包含10个唯一意图的984个样本。我们的分析揭示了大型语言模型在处理不充足数据的下游任务上的强大能力。在口语孟加拉语的意图检测中，GPT-3.5模型达到了令人印象深刻的0.94的F1得分，在槽位填充中达到了0.51的F1得分。

    As voice assistants cement their place in our technologically advanced society, there remains a need to cater to the diverse linguistic landscape, including colloquial forms of low-resource languages. Our study introduces the first-ever comprehensive dataset for intent detection and slot filling in formal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples across 10 unique intents. Our analysis reveals the robustness of large language models for tackling downstream tasks with inadequate data. The GPT-3.5 model achieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot filling for colloquial Bangla.
    
[^32]: 增强的Transformer架构用于自然语言处理

    Enhanced Transformer Architecture for Natural Language Processing. (arXiv:2310.10930v1 [cs.CL])

    [http://arxiv.org/abs/2310.10930](http://arxiv.org/abs/2310.10930)

    增强的Transformer引入了全层标准化、加权残差连接、强化学习位置编码和零掩码自注意力等创新技术，通过在Multi30k翻译数据集上验证，相比于原始Transformer，实现了202.96%的BLEU分数提升。

    

    Transformer是自然语言处理领域中最先进的模型。当前的NLP模型主要通过增加transformer的数量来提高处理性能。然而，这种技术需要大量的训练资源，如计算能力。本文提出了一种新颖的Transformer结构，具有全层标准化、加权残差连接、利用强化学习的位置编码和零掩码自注意力等特点。提出的增强Transformer模型通过使用Multi30k翻译数据集的双语评估实验得到的BLEU分数进行验证。结果表明，与原始Transformer相比，增强Transformer的BLEU分数提高了202.96%。

    Transformer is a state-of-the-art model in the field of natural language processing (NLP). Current NLP models primarily increase the number of transformers to improve processing performance. However, this technique requires a lot of training resources such as computing capacity. In this paper, a novel structure of Transformer is proposed. It is featured by full layer normalization, weighted residual connection, positional encoding exploiting reinforcement learning, and zero masked self-attention. The proposed Transformer model, which is called Enhanced Transformer, is validated by the bilingual evaluation understudy (BLEU) score obtained with the Multi30k translation dataset. As a result, the Enhanced Transformer achieves 202.96% higher BLEU score as compared to the original transformer with the translation dataset.
    
[^33]: Spatial HuBERT: 单个说话者多通道音频的自监督空间语音表示学习

    Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio. (arXiv:2310.10922v1 [cs.CL])

    [http://arxiv.org/abs/2310.10922](http://arxiv.org/abs/2310.10922)

    Spatial HuBERT是一种自监督的语音表示模型，通过学习多通道音频输入中的单个说话者的声学和空间信息，其表示在各种空间下游任务中优于最先进的单声道语音表示，特别是在嘈杂和混响环境中。

    

    自监督学习被用来利用无标注数据，通过训练表示模型改善语音系统的准确性和泛化能力。近期的研究致力于在各种声学领域、语言、模态甚至同时说话者中生成有效的表示，但这些研究都限制在单声道音频记录中。本文提出了Spatial HuBERT，它是一种自监督语音表示模型，通过使用多通道音频输入学习单个说话者在潜在嘈杂环境中的声学和空间信息。Spatial HuBERT学习的表示在各种空间下游任务中表现优于最先进的单声道语音表示，特别是在混响和嘈杂环境中。我们还展示了Spatial HuBERT学习的表示在语音定位任务中的实用性。

    Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publi
    
[^34]: NuclearQA: 用于核领域语言模型的人工基准

    NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain. (arXiv:2310.10920v1 [cs.CL])

    [http://arxiv.org/abs/2310.10920](http://arxiv.org/abs/2310.10920)

    NuclearQA是一个人工基准，用于评估核领域的语言模型的性能，其中包含100个专家设计的问题。该基准与现有的评估指标不同，能够准确评估语言模型在核领域的能力。

    

    随着语言模型的流行，它们已经被应用于几乎所有领域。但是随着应用于特定领域的扩大，评估其在这些领域的有效性的方法日益缺乏。现有的基准大部分专注于不需要对所涉及主题进行正确理解的问题。在本文中，我们介绍了NuclearQA，这是一个由专家设计的用于评估核领域语言模型的人工基准，包含了100个问题，用于测试语言模型的能力。我们详细介绍了我们的方法，并展示了由于现有评估指标的限制，我们基准的独特能力。我们还提出了自己的评估指标来评估语言模型的性能。我们对最先进的模型进行了实验，结果表明，即使在核领域，NuclearQA也能够有效评估语言模型的性能。

    As LLMs have become increasingly popular, they have been used in almost every field. But as the application for LLMs expands from generic fields to narrow, focused science domains, there exists an ever-increasing gap in ways to evaluate their efficacy in those fields. For the benchmarks that do exist, a lot of them focus on questions that don't require proper understanding of the subject in question. In this paper, we present NuclearQA, a human-made benchmark of 100 questions to evaluate language models in the nuclear domain, consisting of a varying collection of questions that have been specifically designed by experts to test the abilities of language models. We detail our approach and show how the mix of several types of questions makes our benchmark uniquely capable of evaluating models in the nuclear domain. We also present our own evaluation metric for assessing LLM's performances due to the limitations of existing ones. Our experiments on state-of-the-art models suggest that eve
    
[^35]: 新兴的AI辅助话语：ChatGPT与第二语言写作案例研究

    Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT. (arXiv:2310.10903v1 [cs.CL])

    [http://arxiv.org/abs/2310.10903](http://arxiv.org/abs/2310.10903)

    ChatGPT在第二语言写作中扮演重要角色，能够有效地与语言学习者合作，在保持个人表达声音的同时提升写作能力。这项研究为ChatGPT在学术写作中的应用提供了重要的探索。

    

    ChatGPT的快速普及引发了关于其对人类写作的影响的争议。在对写作标准下降的担忧中，本研究调查了ChatGPT在学术写作中的作用，特别是在语言学习者中的作用。采用案例研究方法，本研究考察了博士生凯灵（Kailing）在整个学术写作过程中如何使用ChatGPT的经验。研究采用活动理论作为理论框架来理解利用生成式AI工具进行写作的过程，研究数据包括半结构化访谈、写作样本和GPT日志的分析。结果表明，凯灵在各个写作阶段有效地与ChatGPT合作，同时保留自己独特的作者声音和主体性。这突显了ChatGPT等AI工具在增强语言学习者的学术写作能力时不会掩盖个人的真实性。本案例研究对ChatGPT在学术界中的使用进行了批判性的探讨。

    The rapid proliferation of ChatGPT has incited debates regarding its impact on human writing. Amid concerns about declining writing standards, this study investigates the role of ChatGPT in facilitating academic writing, especially among language learners. Using a case study approach, this study examines the experiences of Kailing, a doctoral student, who integrates ChatGPT throughout their academic writing process. The study employs activity theory as a lens for understanding writing with generative AI tools and data analyzed includes semi-structured interviews, writing samples, and GPT logs. Results indicate that Kailing effectively collaborates with ChatGPT across various writing stages while preserving her distinct authorial voice and agency. This underscores the potential of AI tools such as ChatGPT to enhance academic writing for language learners without overshadowing individual authenticity. This case study offers a critical exploration of how ChatGPT is utilized in the academi
    
[^36]: IDEAL: 强化大型语言模型中上下文学习的影响驱动选择性注释方法

    IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models. (arXiv:2310.10873v1 [cs.CL])

    [http://arxiv.org/abs/2310.10873](http://arxiv.org/abs/2310.10873)

    本文提出了一种影响驱动的选择性注释方法，用于在大型语言模型中改善上下文学习。该方法通过选择关键的未标记数据子集进行注释，在降低注释成本的同时提高了上下文示例的质量。

    

    上下文学习是一种有前景的范式，它利用上下文示例作为大型语言模型预测的提示。这些提示对于获得强大的性能至关重要。然而，由于这些提示需要从大量注释的示例中进行采样，找到正确的提示可能导致高昂的注释成本。为解决这一挑战，本文引入了一种基于影响驱动的选择性注释方法，旨在在改善上下文示例质量的同时最大程度地降低注释成本。我们的方法的核心是从大规模未标记的数据池中选择一个关键子集进行注释，以用于后续的提示采样。具体地，首先构建一个有向图来表示未标记的数据，然后利用扩散过程量化候选未标记子集的影响力，最后引入一个简单又有效的贪心算法来选择未标记的数据。如果数据提供了最大的影响力，算法就会迭代地选择这些数据。

    In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a ma
    
[^37]: 王子会得到真爱之吻吗？关于童话文本中性别扰动对模型敏感性的研究

    Will the Prince Get True Love's Kiss? On the Model Sensitivity to Gender Perturbation over Fairytale Texts. (arXiv:2310.10865v1 [cs.CL])

    [http://arxiv.org/abs/2310.10865](http://arxiv.org/abs/2310.10865)

    该研究旨在通过评估语言模型对性别扰动的鲁棒性，帮助减轻传统童话中的性别偏见，并通过引入反事实性别刻板印象来减轻学习到的偏见。实验结果显示，模型对性别扰动敏感，但在反事实训练后对后续引入的反性别偏见更不敏感。

    

    最近的研究显示，传统的童话故事中存在大量有害的性别偏见。为了减轻童话中的性别偏见，本研究旨在评估语言模型学习到的偏见对性别扰动的鲁棒性。具体而言，我们关注童话故事中的问答任务。通过使用反事实数据增强FairytaleQA数据集，我们评估模型对交换性别角色信息的鲁棒性，并在训练时引入反事实性别刻板印象来减轻学习到的偏见。此外，我们还引入了一种新的方法，利用语言模型的庞大词汇量来支持超越童话故事的文本类型。我们的实验结果表明，模型对性别扰动敏感，性能与原始测试集相比显著下降。然而，当首先在反事实的训练数据集上进行微调后，模型对后续引入的反性别偏见更不敏感。

    Recent studies show that traditional fairytales are rife with harmful gender biases. To help mitigate these gender biases in fairytales, this work aims to assess learned biases of language models by evaluating their robustness against gender perturbations. Specifically, we focus on Question Answering (QA) tasks in fairytales. Using counterfactual data augmentation to the FairytaleQA dataset, we evaluate model robustness against swapped gender character information, and then mitigate learned biases by introducing counterfactual gender stereotypes during training time. We additionally introduce a novel approach that utilizes the massive vocabulary of language models to support text genres beyond fairytales. Our experimental results suggest that models are sensitive to gender perturbations, with significant performance drops compared to the original testing set. However, when first fine-tuned on a counterfactual training dataset, models are less sensitive to the later introduced anti-gend
    
[^38]: CoTFormer：更多的关注令牌弥补了更少的深度

    CoTFormer: More Tokens With Attention Make Up For Less Depth. (arXiv:2310.10845v1 [cs.CL])

    [http://arxiv.org/abs/2310.10845](http://arxiv.org/abs/2310.10845)

    CoTFormer是一种transformer变体，通过使用隐含的链思考机制，实现了与更深模型相当的容量，并且在实证中显著优于更大的标准transformers。

    

    持续发展越来越大和更深的基础模型的竞赛正在进行中。然而，像链思考（CoT）方法这样的技术在实现最佳下游性能方面仍起着重要作用。在这项工作中，我们建立了使用链思考和使用更深的transformer之间的近似平行关系。基于这一洞见，我们引入了CoTFormer，一种使用隐含链思考机制来实现与更深模型相当容量的transformer变体。我们的实证发现证明了CoTFormer的有效性，因为它们明显优于更大的标准transformers。

    The race to continually develop ever larger and deeper foundational models is underway. However, techniques like the Chain-of-Thought (CoT) method continue to play a pivotal role in achieving optimal downstream performance. In this work, we establish an approximate parallel between using chain-of-thought and employing a deeper transformer. Building on this insight, we introduce CoTFormer, a transformer variant that employs an implicit CoT-like mechanism to achieve capacity comparable to a deeper model. Our empirical findings demonstrate the effectiveness of CoTFormers, as they significantly outperform larger standard transformers.
    
[^39]: 对大型语言模型的恶意攻击所揭示的漏洞调查

    Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks. (arXiv:2310.10844v1 [cs.CL])

    [http://arxiv.org/abs/2310.10844](http://arxiv.org/abs/2310.10844)

    本论文调查了对大型语言模型进行恶意攻击的研究，发现即使经过安全调整的模型也容易受到攻击。这些攻击利用弱点并误导AI系统，对于复杂系统的攻击尤为明显。

    

    大型语言模型（LLMs）在体系结构和能力方面迅速发展，随着它们在复杂系统中的深入整合，审查其安全性变得更加紧迫。本文调查了对LLMs进行恶意攻击的新兴跨学科领域的研究，该领域是可信任的机器学习的一个子领域，结合自然语言处理和安全性的观点。先前的研究表明，即使是经过安全调整的LLMs（通过指导调整和通过人类反馈进行加强学习）也可能受到恶意攻击的影响，这些攻击利用弱点并误导人工智能系统，如ChatGPT和Bard等模型的“越狱”攻击的普遍存在证明了这一点。在本调查中，我们首先概述了大型语言模型，描述了它们的安全对齐，并根据各种学习结构对现有研究进行分类：仅文本攻击、多模态攻击以及专门针对复杂系统的其他攻击方法。

    Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as
    
[^40]: 假新闻在绵羊的外衣中：对抗LLM增强风格攻击的鲁棒假新闻检测

    Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks. (arXiv:2310.10830v1 [cs.CL])

    [http://arxiv.org/abs/2310.10830](http://arxiv.org/abs/2310.10830)

    这篇论文介绍了一种鲁棒的无风格假新闻检测器，能够对抗利用大型语言模型进行风格攻击的假新闻。通过LLM增强的新闻重构，该检测器能够适应不同的写作风格，提高了对伪装假新闻的检测能力。

    

    人们常常认为在线假新闻和可靠新闻在写作风格上有明显的差异，如使用耸人听闻的语言与客观的语言。然而，我们强调风格相关特征也可以用于风格攻击。值得注意的是，强大的大型语言模型（LLM）的崛起使恶意用户能够以最低成本模仿值得信赖的新闻媒体的风格。我们的分析显示，以LLM伪装的假新闻内容导致先进的基于文本的检测器的性能显著下降（F1分数减少高达38%），给在线生态系统中的自动检测带来了重大挑战。为了解决这个问题，我们引入了SheepDog，一种对新闻写作风格鲁棒的无风格假新闻检测器。SheepDog通过LLM增强的新闻重构实现了这种适应性，通过风格导向的重构提示来定制每篇文章以适应不同的写作风格。通过采用无风格训练，SheepDog可以在不同风格的新闻中检测假新闻。

    It is commonly perceived that online fake news and reliable news exhibit stark differences in writing styles, such as the use of sensationalist versus objective language. However, we emphasize that style-related features can also be exploited for style-based attacks. Notably, the rise of powerful Large Language Models (LLMs) has enabled malicious users to mimic the style of trustworthy news outlets at minimal cost. Our analysis reveals that LLM-camouflaged fake news content leads to substantial performance degradation of state-of-the-art text-based detectors (up to 38% decrease in F1 Score), posing a significant challenge for automated detection in online ecosystems. To address this, we introduce SheepDog, a style-agnostic fake news detector robust to news writing styles. SheepDog achieves this adaptability through LLM-empowered news reframing, which customizes each article to match different writing styles using style-oriented reframing prompts. By employing style-agnostic training, S
    
[^41]: SD-HuBERT: 自我蒸馏诱导HuBERT中的音节组织

    SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT. (arXiv:2310.10803v1 [cs.CL])

    [http://arxiv.org/abs/2310.10803](http://arxiv.org/abs/2310.10803)

    本研究提出了SD-HuBERT模型，通过采用自我蒸馏目标进行微调，实现了在学习语音句子级表示时音节组织的出现，模型能够在语音中划定明确的边界，并展现出显著的音节结构。该研究还提出了一个新的基准任务用于评估语音的句子级表示，与之前的模型相比，在无监督音节发现和学习句子级表示方面表现优异。

    

    自我监督学习（SSL）中的数据驱动单元发现开启了口语语言处理的新时代。然而，发现的单元往往仍处于音素空间，限制了SSL表示的实用性。在这里，我们展示了在学习语音的句子级表示时，音节组织的出现。特别地，我们采用“自我蒸馏”目标来微调预训练的HuBERT，并加入一个汇聚标记来总结整个句子。在没有任何监督的情况下，得到的模型在语音中划定了明确的边界，并且帧间的表示显示出显著的音节结构。我们证明这种出现的结构很大程度上与真实音节对应。此外，我们提出了一个新的基准任务，Spoken Speech ABX，用于评估语音的句子级表示。与之前的模型相比，我们的模型在无监督音节发现和学习句子级表示方面表现优异。

    Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space, limiting the utility of SSL representations. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt "self-distillation" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames show salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representatio
    
[^42]: 自我监督的语音模型推断出普适的发音运动学

    Self-Supervised Models of Speech Infer Universal Articulatory Kinematics. (arXiv:2310.10788v1 [eess.AS])

    [http://arxiv.org/abs/2310.10788](http://arxiv.org/abs/2310.10788)

    本研究展示了自我监督语音模型具有推断语音发音运动学的能力，并显示出这一属性在不同语言中具有重叠性。此外，通过简单变换，模型可以在不同说话者、性别、语言和方言之间转换，表现出良好的泛化性。这些结果拓宽了我们对语音处理的理解。

    

    基于自我监督学习（SSL）的语音模型在许多下游任务中显示出了出色的性能。这些最先进的模型一直是黑匣子，但最近的许多研究开始对像HuBERT这样的模型进行"探测"，以将其内部表示与语音的不同方面相关联。本文中，我们展示了自我监督模型的一个基本属性，即"推断发音运动学"，即这些模型将声学转化为语音信号底层的因果性发音动态的能力。我们还显示出，该抽象在用于训练模型的数据的语言之间有较大的重叠，在类似音系的语言中有更高的偏好。此外，我们还显示出，通过简单的仿射变换，声学到发音的逆转换（AAI）在说话者之间具有可传递性，甚至在性别、语言和方言之间也具有可传递性，显示了该属性的普适性。总的来说，这些结果为我们对语音处理的认识提供了新的视角。

    Self-Supervised Learning (SSL) based models of speech have shown remarkable performance on a range of downstream tasks. These state-of-the-art models have remained blackboxes, but many recent studies have begun "probing" models like HuBERT, to correlate their internal representations to different aspects of speech. In this paper, we show "inference of articulatory kinematics" as fundamental property of SSL models, i.e., the ability of these models to transform acoustics into the causal articulatory dynamics underlying the speech signal. We also show that this abstraction is largely overlapping across the language of the data used to train the model, with preference to the language with similar phonological system. Furthermore, we show that with simple affine transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable across speakers, even across genders, languages, and dialects, showing the generalizability of this property. Together, these results shed new light on the 
    
[^43]: BanglaNLP在BLP-2023任务1中的表现：在孟加拉文中暴力煽动文字检测中对不同Transformer模型进行基准测试

    BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bengali. (arXiv:2310.10781v1 [cs.CL])

    [http://arxiv.org/abs/2310.10781](http://arxiv.org/abs/2310.10781)

    本文介绍了BanglaNLP开发的系统在解决孟加拉文中暴力煽动文字检测共享任务中的表现。通过研究数据增强和使用多语言-e5-base模型进行微调，我们在测试集上取得了68.11%的宏F1值，在排行榜上排名第23位。

    

    本文介绍了我们在孟加拉文中暴力煽动文字检测的共享任务中开发的系统。我们解释了我们使用的传统方法和最近的方法来让我们的模型学习。我们提出的系统有助于分类判断给定的文本是否包含威胁。我们研究了在数据集有限的情况下进行数据增强的影响。我们的定量结果显示，在我们的任务中，对多语言-e5-base模型进行微调在性能上胜过其他基于Transformer的架构。我们在测试集中获得了68.11%的宏F1值，并在这个共享任务中排名第23位。

    This paper presents the system that we have developed while solving this shared task on violence inciting text detection in Bangla. We explain both the traditional and the recent approaches that we have used to make our models learn. Our proposed system helps to classify if the given text contains any threat. We studied the impact of data augmentation when there is a limited dataset available. Our quantitative results show that finetuning a multilingual-e5-base model performed the best in our task compared to other transformer-based architectures. We obtained a macro F1 of 68.11\% in the test set and our performance in this shared task is ranked at 23 in the leaderboard.
    
[^44]: BiomedJourney: 指导学习多模态患者旅程中的反事实生物医学图像生成

    BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v1 [cs.CV])

    [http://arxiv.org/abs/2310.10765](http://arxiv.org/abs/2310.10765)

    提出了一种新颖的方法BiomedJourney，通过指导学习多模态患者旅程，进行反事实生物医学图像生成。使用GPT-4处理图像报告生成疾病进展的自然语言描述，并训练潜在扩散模型。

    

    随着自然语言指令图像编辑的指导学习取得了快速进展，如InstructPix2Pix，生物医学领域可以将这些方法应用于反事实图像生成，从而帮助区分因果结构和伪相关，并促进疾病进展建模的稳健图像解释。然而，通用的图像编辑模型并不适用于生物医学领域，反事实生物医学图像生成的研究还远未深入。在本文中，我们提出了一种新颖的方法BiomedJourney，通过指导学习多模态患者旅程，进行反事实生物医学图像生成。给定一个拍摄于不同时间点的两个生物医学图像的患者，我们使用GPT-4处理相应的图像报告，并生成疾病进展的自然语言描述。然后，使用生成的三元组（先前图像、进展描述、新图像）来训练一个潜在扩散模型。

    Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion mode
    
[^45]: 使用大型语言模型减少从财务报告中提取信息中的错误

    Towards reducing hallucination in extracting information from financial reports using Large Language Models. (arXiv:2310.10760v1 [cs.CL])

    [http://arxiv.org/abs/2310.10760](http://arxiv.org/abs/2310.10760)

    本文介绍了使用大型语言模型减少从财务报告中提取信息时的错误的方法，并且通过检索增强生成技术和元数据的结合，实现了高精度的信息提取过程。

    

    对于财务分析师来说，公司财务报告的问答（Q&A）部分是进行各种分析和投资决策的关键信息。然而，从问答部分中提取有价值的见解面临着很大的挑战，因为传统的方法如详细阅读和记笔记缺乏可扩展性，并容易受到人为错误的影响，而光学字符识别（OCR）和类似技术在准确处理无结构化的文字转录时遇到困难，常常错过驱动投资者决策的微妙的语言细微差别。在这里，我们演示了利用大型语言模型（LLMs）高效快速地从收益报告转录中提取信息，同时通过结合检索增强生成技术以及元数据来确保高精度的转化过程，并减少了错误的发生。

    For a financial analyst, the question and answer (Q\&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our pro
    
[^46]: 使用离线强化学习构建个性一致的对话代理

    Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning. (arXiv:2310.10735v1 [cs.CL])

    [http://arxiv.org/abs/2310.10735](http://arxiv.org/abs/2310.10735)

    本研究提出了一种离线强化学习框架，通过廉价地在现有数据上训练模型并对特定发言进行奖惩，提高对话系统的个性一致性。

    

    保持一致的个性是任何开放领域对话系统的关键品质。目前的最先进系统通过使用监督学习或在线强化学习来训练代理，以实现个性一致性。然而，使用监督学习训练的系统往往缺乏一致性，因为它们从来没有因言之不准而受到惩罚。使用强化学习进行额外训练可以缓解其中一些问题，但是训练过程很昂贵。相反，我们提出了一个离线强化学习框架来改善对话系统的个性一致性。我们的框架使我们能够将以前的方法的优点结合起来，既可以像监督学习那样廉价地在现有数据上训练模型，又可以像强化学习那样对特定发言进行奖惩。我们还引入了一种简单的重要性采样方法，以减少离线强化学习训练中的重要性权重方差，我们将其称为方差减少MLE初始化（VaRMI）重要性采样。我们的自动评估和人工评估显示

    Maintaining a consistent persona is a key quality for any open domain dialogue system. Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are never punished for uttering contradictions. Additional training with RL can alleviate some of these issues, however the training process is expensive. Instead, we propose an offline RL framework to improve the persona consistency of dialogue systems. Our framework allows us to combine the advantages of previous methods as we can inexpensively train our model on existing data as in supervised learning, while punishing and rewarding specific utterances as in RL. We also introduce a simple importance sampling method to reduce the variance of importance weights in offline RL training which we call Variance-Reducing MLE-Initialized (VaRMI) importance sampling. Our automatic and human evaluations show
    
[^47]: 演示就是你需要的一切：利用上下文学习推进攻击性内容改写

    Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning. (arXiv:2310.10707v1 [cs.CL])

    [http://arxiv.org/abs/2310.10707](http://arxiv.org/abs/2310.10707)

    该论文在攻击性内容改写方面引入了上下文学习方法，并通过有限数量的输入-标签演示对来指导模型生成特定查询的所需输出，从而提高可用性和减少攻击性。

    

    改写攻击性内容是一种更好的替代内容删除的方法，有助于改善沟通环境的文明程度。然而，监督式的改写器在保留意义和意图的同时，对大量标记数据依赖性较高。它们也保留了原始内容的大部分攻击性，这引发了对它们整体可用性的疑问。在本文中，我们旨在通过探索上下文学习（ICL）与大型语言模型（LLM）相结合，帮助从业者开发可用的改写器，即使用有限数量的输入-标签演示对来引导模型生成特定查询的所需输出。我们的研究主要关注关键因素，如演示的数量和顺序，排除提示指令，以及降低测量毒性。我们在包括我们提出的上下文感知礼貌改写数据集在内的三个数据集上进行了原则性评估，其中包括对话式的粗鲁发言、礼貌改写等。

    Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as -- number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase dataset, comprising of dialogue-style rude utterances, polite paraphr
    
[^48]: 发挥LLMs的能量：通过新闻标题生成的视角评估人工智能协作创作

    Harnessing the Power of LLMs: Evaluating Human-AI text Co-Creation through the Lens of News Headline Generation. (arXiv:2310.10706v1 [cs.CL])

    [http://arxiv.org/abs/2310.10706](http://arxiv.org/abs/2310.10706)

    该研究通过对LLMs辅助新闻标题生成的人工智能协作方法进行比较，发现引导和选择模型输出能够带来最大的效益，并且与自由编辑相比并不损害参与者对控制的感知。

    

    为了探索人类如何最好地利用LLMs进行写作，并了解与这些模型的交互如何影响写作过程中的所有权感和信任度，我们在LLM辅助新闻标题生成的背景下比较了常见的人工智能协作类型（例如，引导系统，从系统输出中进行选择，后期编辑输出）。尽管LLMs单独可以生成令人满意的新闻标题，但平均而言，人类的控制是需要的，以修复不可取的模型输出。在各种交互方法中，引导和选择模型输出增加了最多的效益，代价最低（时间和精力）。此外，人工智能协助并没有损害参与者对控制的感知，与自由编辑相比。

    To explore how humans can best leverage LLMs for writing and how interacting with these models affects feelings of ownership and trust in the writing process, we compared common human-AI interaction types (e.g., guiding system, selecting from system outputs, post-editing outputs) in the context of LLM-assisted news headline generation. While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs. Of the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort). Further, AI assistance did not harm participants' perception of control compared to freeform editing.
    
[^49]: 优化的转录错误修正中的分词技术

    Optimized Tokenization for Transcribed Error Correction. (arXiv:2310.10704v1 [cs.CL])

    [http://arxiv.org/abs/2310.10704](http://arxiv.org/abs/2310.10704)

    本研究通过训练仅使用合成数据的方法，在转录错误修正中取得了显著的性能提升。具体而言，通过使用从转录数据中得到的合成数据，并进行语言特定的词汇调整，可以更好地纠正重复错误。

    

    面对语音识别系统的挑战，如发音变化、不良音频条件和标记数据的稀缺性，强调了后处理步骤纠正重复错误的必要性。先前的研究表明，采用专用的错误修正模型具有优势，然而训练这样的模型需要大量的标记数据，这并不容易获得。为了克服这个限制，通常使用合成的类似转录的数据，然而，弥合转录错误和合成噪声之间的分布差距并不简单。在本文中，我们证明了通过仅使用合成数据训练可以显著提高纠正模型的性能。具体而言，我们通过实证展示：（1）使用从一组转录数据中得到的错误分布生成的合成数据优于应用随机扰动的常见方法；（2）应用语言特定的词汇调整可以进一步提高纠正模型的性能。

    The challenges facing speech recognition systems, such as variations in pronunciations, adverse audio conditions, and the scarcity of labeled data, emphasize the necessity for a post-processing step that corrects recurring errors. Previous research has shown the advantages of employing dedicated error correction models, yet training such models requires large amounts of labeled data which is not easily obtained. To overcome this limitation, synthetic transcribed-like data is often utilized, however, bridging the distribution gap between transcribed errors and synthetic noise is not trivial. In this paper, we demonstrate that the performance of correction models can be significantly increased by training solely using synthetic data. Specifically, we empirically show that: (1) synthetic data generated using the error distribution derived from a set of transcribed data outperforms the common approach of applying random perturbations; (2) applying language-specific adjustments to the vocab
    
[^50]: 多智能体协作的大型语言模型理论

    Theory of Mind for Multi-Agent Collaboration via Large Language Models. (arXiv:2310.10701v1 [cs.CL])

    [http://arxiv.org/abs/2310.10701](http://arxiv.org/abs/2310.10701)

    本研究通过在多智能体合作游戏中评估基于大型语言模型的智能体，发现它们可以表现出协作行为和高级理论推理能力，并通过使用明确的信念状态表示来提高任务性能和理论推理准确性。

    

    大型语言模型在推理和规划方面取得了令人瞩目的成就，但它在多智能体协作方面的能力尚未得到深入探索。本研究通过对比多智能体强化学习和基于规划的基准方法，在多智能体合作文本游戏中评估了基于大型语言模型的智能体在理论推理任务上的表现。我们观察到基于大型语言模型的智能体出现了协作行为和高级理论推理能力的证据。我们的结果揭示了基于大型语言模型的智能体在长期规划上存在优化的局限性，以及对任务状态的错误认知。我们尝试使用明确的信念状态表示来缓解这些问题，并发现它可以提高大型语言模型智能体的任务性能和理论推理的准确性。

    While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
    
[^51]: 将代码语义与LLMs相结合：代码生成的语义思维链约束

    Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation. (arXiv:2310.10698v1 [cs.CL])

    [http://arxiv.org/abs/2310.10698](http://arxiv.org/abs/2310.10698)

    本文提出了一种“语义思维链”的方法，用于在代码生成中引入语义信息，并以此来实现更精细的代码理解和表示。

    

    大型语言模型（LLMs）在代码生成方面展示了出色的能力。然而，自动化代码生成仍然具有挑战性，因为它需要对自然语言需求和代码之间进行高级语义映射。大多数现有基于LLMs的代码生成方法依赖于仅使用解码器的因果语言模型，通常将代码仅视为纯文本标记，即将需求作为提示输入，并将代码作为一系列平面标记输出，可能会忽略源代码中固有的丰富语义特征。为了弥合这一差距，本文提出了一种“语义思维链”的方法，引入了代码的语义信息，称为SeCoT。我们的动机是源代码的语义信息（例如数据流和控制流）更精确地描述了程序执行行为、意图和功能。通过引导LLMs考虑和整合语义信息，我们可以实现对代码更精细的理解和表示，增强代码生成的质量。

    Large language models (LLMs) have showcased remarkable prowess in code generation. However, automated code generation is still challenging since it requires a high-level semantic mapping between natural language requirements and codes. Most existing LLMs-based approaches for code generation rely on decoder-only causal language models often treate codes merely as plain text tokens \ie feeding the requirements as a prompt input, and outputing code as flat sequence of tokens, potentially missing the rich semantic features inherent in source code. To bridge this gap, this paper proposes the "Semantic Chain-of-Thought" approach to intruduce semantic information of code, named SeCoT. Our motivation is that the semantic information of the source code (\eg data flow and control flow) describes more precise program execution behavior, intention and function. By guiding LLM consider and integrate semantic information, we can achieve a more granular understanding and representation of code, enhan
    
[^52]: 在上下文中的学生建模中使用大型语言模型：从一次性观察中合成视觉编程中学生的行为

    Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. (arXiv:2310.10690v1 [cs.CL])

    [http://arxiv.org/abs/2310.10690](http://arxiv.org/abs/2310.10690)

    本研究探索在开放式学习环境中使用大型语言模型进行上下文学生建模，提出了一个新的框架LLM-SS，通过合成学生在不同任务上的尝试，为学生建模提供更准确的预测和教学策略。

    

    学生建模对于许多教育技术来说至关重要，因为它可以预测未来的学习结果和有针对性的教学策略。然而，开放式学习环境会带来挑战，因为学生表现出多样化的行为且缺乏明确定义的学习技能集。为了应对这些挑战，我们探索在开放式学习环境中应用大型语言模型（LLMs）进行上下文学生建模。我们引入了一个新颖的框架LLM-SS，利用LLMs合成学生的行为。具体而言，给定一个特定学生在参考任务上的解决尝试作为观察，目标是合成该学生在目标任务上的尝试。我们的框架可以与不同的LLMs结合使用；而且，我们使用领域专家知识对LLMs进行微调，提高它们对领域背景和学生行为的理解。我们评估了几种具体的方法...

    Student modeling is central to many educational technologies as it enables the prediction of future learning outcomes and targeted instructional strategies. However, open-ended learning environments pose challenges for accurately modeling students due to the diverse behaviors exhibited by students and the absence of a well-defined set of learning skills. To approach these challenges, we explore the application of Large Language Models (LLMs) for in-context student modeling in open-ended learning environments. We introduce a novel framework, LLM-SS, that leverages LLMs for synthesizing student's behavior. More concretely, given a particular student's solving attempt on a reference task as observation, the goal is to synthesize the student's attempt on a target task. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs using domain-specific expertise to boost their understanding of domain background and student behaviors. We evaluate several concrete methods bas
    
[^53]: 一种仅解码器的时间序列预测基础模型

    A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])

    [http://arxiv.org/abs/2310.10688](http://arxiv.org/abs/2310.10688)

    本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。

    

    受到自然语言处理中大型语言模型的最新进展的启发，我们设计了一种用于预测的时间序列基础模型，其在各种公共数据集上的开箱即用的零样本性能接近每个个别数据集上最先进的监督预测模型的准确性。我们的模型基于在大型时间序列语料库上预训练的补丁解码器式注意力模型，并可以适用于不同的预测历史长度、预测长度和时间粒度。

    Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
    
[^54]: 大型语言模型的自主树搜索能力

    Autonomous Tree-search Ability of Large Language Models. (arXiv:2310.10686v1 [cs.CL])

    [http://arxiv.org/abs/2310.10686](http://arxiv.org/abs/2310.10686)

    提出了一个新的概念，旨在使大型语言模型能够在没有外部程序的辅助下维持树搜索能力，并产生展示树结构搜索过程的响应。

    

    大型语言模型在先进的提示技术下表现出了出色的推理能力，但在需要探索、战略预见和顺序决策的任务中表现不足。最近的研究提出利用外部程序定义搜索逻辑，使得语言模型可以执行被动的树搜索来解决更具挑战性的推理任务。虽然取得了令人印象深刻的结果，但这些方法存在若干基本限制。首先，被动的树搜索不高效，通常需要多轮语言模型API调用来解决单个问题。此外，被动搜索方法不灵活，需要特定任务的程序设计。然后一个自然的问题出现：我们是否可以在没有外部程序的辅助下保持语言模型的树搜索能力，仍然能够生成清晰展示树结构搜索过程的响应？为此，我们提出了一个新的概念，称为自主树搜索能力。

    Large Language Models have excelled in remarkable reasoning capabilities with advanced prompting techniques, but they fall short on tasks that require exploration, strategic foresight, and sequential decision-making. Recent works propose to utilize external programs to define search logic, such that LLMs can perform passive tree search to solve more challenging reasoning tasks. Though impressive results have been achieved, there are several fundamental limitations of these approaches. First, passive tree searches are not efficient as they usually require multiple rounds of LLM API calls to solve one single problem. Moreover, passive search methods are not flexible since they need task-specific program designs. Then a natural question arises: can we maintain the tree-search capability of LLMs without the aid of external programs, and can still generate responses that clearly demonstrate the process of a tree-structure search? To this end, we propose a new concept called autonomous tree-
    
[^55]: 大型语言模型的去学习研究

    Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])

    [http://arxiv.org/abs/2310.10683](http://arxiv.org/abs/2310.10683)

    大型语言模型的去学习是一个研究的新领域，我们探索了三个场景，可以通过去学习让语言模型与人类偏好保持一致。去学习具有三个优势，只需要负面示例，计算效率高，特别对于知道具体导致不良行为的训练样本更为有效。

    

    我们研究了如何对大型语言模型（LLMs）进行去学习，即忘记不受欢迎的（非）行为。我们展示了至少三种情境可以从去学习中使LLMs与人类偏好保持一致：（1）删除有害回复，（2）按要求删除受版权保护的内容，以及（3）消除幻觉。作为对齐技术的一种，去学习具有三个优点：（1）只需要负面（例如有害）示例，这比在RLHF（基于人类反馈的强化学习）中所需的正面（例如有帮助且通常由人类编写）示例更容易和更便宜地收集（例如通过红队测试或用户报告）；（2）计算效率高；（3）当我们知道哪些训练样本导致了不良行为时，它特别有效。据我们所知，我们的工作是首次探索LLM去学习的工作之一。我们也是首次在LLM去学习中制定了设置、目标和评估。我们表明，如果从业者只有有限的

    We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited
    
[^56]: 大型语言模型可以复制跨文化个性差异

    Large language models can replicate cross-cultural differences in personality. (arXiv:2310.10679v1 [cs.CL])

    [http://arxiv.org/abs/2310.10679](http://arxiv.org/abs/2310.10679)

    大型语言模型GPT-4成功复制了使用十项人格问卷测量的大五人格的跨文化差异，但其结果表明平均评级有上升偏差和较低的变异性与结构效度。

    

    我们使用一项大规模实验(N=8000)来确定GPT-4是否可以复制使用十项人格问卷测量的大五人格的跨文化差异。我们选择美国和韩国作为文化对比，因为先前的研究表明这两个国家的人之间存在显著的人格差异。我们操纵了模拟的目标（美国 vs. 韩国），问卷的语言（英语 vs. 韩语）以及语言模型（GPT-4 vs. GPT-3.5）。我们的结果表明，GPT-4复制了每个因子的跨文化差异。然而，平均评级具有上升偏差，并且比人类样本的变异性更低，以及结构效度较低。总的来说，我们提供了初步的证据说明LLMs可以促进跨文化心理研究。

    We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. Overall, we provide preliminary evidence that LLMs can aid cross-cultural psychological research.
    
[^57]: LLM作为数学和科学问题的潜在头脑风暴伙伴

    LLMs as Potential Brainstorming Partners for Math and Science Problems. (arXiv:2310.10677v1 [cs.CL])

    [http://arxiv.org/abs/2310.10677](http://arxiv.org/abs/2310.10677)

    该论文研究了大型语言模型（LLM），特别是GPT-4，在与人类的集体头脑风暴中的能力和限制，为数学和科学问题的发现和解决提供了有希望的一步。

    

    随着近期深度学习模型的广泛成功，各种数学和科学社区的专业人士对评估最新模型在发现或解决通常需要创造力和头脑风暴的问题方面的能力产生了兴趣。虽然当前人机智能协作与解决复杂的数学和科学问题之间仍存在重大差距，例如六个未解决的千禧年问题，但我们对这个问题的初步调查显示出了迈向弥合鸿沟的有希望的一步。这得益于大型语言模型（LLMs）的最新进展。更具体地说，我们进行了全面的案例研究，探索目前最先进的LLM（特别是GPT-4）在与人类的集体头脑风暴中的能力和限制。

    With the recent rise of widely successful deep learning models, there is emerging interest among professionals in various math and science communities to see and evaluate the state-of-the-art models' abilities to collaborate on finding or solving problems that often require creativity and thus brainstorming. While a significant chasm still exists between current human-machine intellectual collaborations and the resolution of complex math and science problems, such as the six unsolved Millennium Prize Problems, our initial investigation into this matter reveals a promising step towards bridging the divide. This is due to the recent advancements in Large Language Models (LLMs). More specifically, we conduct comprehensive case studies to explore both the capabilities and limitations of the current state-of-the-art LLM, notably GPT-4, in collective brainstorming with humans.
    
[^58]: 基于自然语言处理的WhatsApp聊天机器人的创建

    Creation Of A ChatBot Based On Natural Language Proccesing For Whatsapp. (arXiv:2310.10675v1 [cs.CL])

    [http://arxiv.org/abs/2310.10675](http://arxiv.org/abs/2310.10675)

    本研究旨在开发一个基于自然语言处理的WhatsApp聊天机器人，以提高客户满意度并通过WhatsApp提供更好的服务质量。这个聊天机器人能够高效、有效地处理用户的查询问题。

    

    在数字化转型时代，客户服务对组织的成功至关重要，为了满足对即时回复和个性化帮助24小时不间断的不断增长的需求，聊天机器人已成为解决这些问题的一种有前景的工具。目前，许多公司需要为其客户提供这些解决方案，这促使我们研究这个问题并提供合适的解决方案。本研究的目标是基于自然语言处理开发一个聊天机器人，以提高客户满意度并通过WhatsApp提供的服务质量。解决方案的重点是创建一个高效、有效处理用户查询的聊天机器人。已进行了与现有聊天机器人相关的文献综述，分析了在聊天机器人实施中使用的方法论方法、人工智能技术和质量属性。所得到的结果突出了基于自然语言处理的聊天机器人的优势。

    In the era of digital transformation, customer service is of paramount importance to the success of organizations, and to meet the growing demand for immediate responses and personalized assistance 24 hours a day, chatbots have become a promising tool to solve these problems. Currently, there are many companies that need to provide these solutions to their customers, which motivates us to study this problem and offer a suitable solution. The objective of this study is to develop a chatbot based on natural language processing to improve customer satisfaction and improve the quality of service provided by the company through WhatsApp. The solution focuses on creating a chatbot that efficiently and effectively handles user queries. A literature review related to existing chatbots has been conducted, analyzing methodological approaches, artificial intelligence techniques and quality attributes used in the implementation of chatbots. The results found highlight that chatbots based on natura
    
[^59]: 朝着基于情感的合成意识：使用LLMs估计情感概率向量

    Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate Emotion Probability Vectors. (arXiv:2310.10673v1 [cs.CL])

    [http://arxiv.org/abs/2310.10673](http://arxiv.org/abs/2310.10673)

    本文介绍了如何使用LLMs来估计文本的情感状态，并通过情感分析和PCA映射提出了改善文本描述状态的动作选择，但实验表明这一方法目前存在困难。

    

    本文展示了如何使用LLMs（大型语言模型）来估计与文本相关的情感状态的摘要。情感状态的摘要是一个词典，用于描述情感，并给出在原始文本和情感触发尾部组成的提示后，该词出现的概率。通过对亚马逊产品评论的情感分析，我们展示了情感描述词可以映射到类似PCA的空间中。希望通过尾部提示也可以引发关于改善当前文本所描述状态的动作的文本描述。实验似乎表明，这种工作并不容易实现。这种失败使得我们希望通过比较情感反应来选择最佳预测结果的行动目前无法实现。

    This paper shows how LLMs (Large Language Models) may be used to estimate a summary of the emotional state associated with piece of text. The summary of emotional state is a dictionary of words used to describe emotion together with the probability of the word appearing after a prompt comprising the original text and an emotion eliciting tail. Through emotion analysis of Amazon product reviews we demonstrate emotion descriptors can be mapped into a PCA type space. It was hoped that text descriptions of actions to improve a current text described state could also be elicited through a tail prompt. Experiment seemed to indicate that this is not straightforward to make work. This failure put our hoped for selection of action via choosing the best predict ed outcome via comparing emotional responses out of reach for the moment.
    
[^60]: 融合量子和经典机器学习的情感分析方法

    Hybrid Quantum-Classical Machine Learning for Sentiment Analysis. (arXiv:2310.10672v1 [cs.CL])

    [http://arxiv.org/abs/2310.10672](http://arxiv.org/abs/2310.10672)

    本文提出了一种使用混合量子-经典机器学习算法进行情感分析的方法，通过研究量子核方法和基于变分量子电路的分类器，并结合经典的降维技术，实现了在处理大规模数据集中表达的人类情感和观点的情感分析，并取得了相对于传统方法更好的性能。

    

    量子计算与经典机器学习的合作在自然语言处理中具有潜在优势，特别是在处理大规模数据集中表达的人类情感和观点的情感分析方面。本文提出了一种使用混合量子-经典机器学习算法进行情感分析的方法。我们研究了量子核方法和基于变分量子电路的分类器，并将它们与经典的降维技术（如PCA和Haar小波变换）结合起来。该方法在基于英语和孟加拉语的两个不同数据集上进行了评估。实验结果表明，在对数据进行降维处理后，基于量子的混合算法的性能一致且优于经典方法。

    The collaboration between quantum computing and classical machine learning offers potential advantages in natural language processing, particularly in the sentiment analysis of human emotions and opinions expressed in large-scale datasets. In this work, we propose a methodology for sentiment analysis using hybrid quantum-classical machine learning algorithms. We investigate quantum kernel approaches and variational quantum circuit-based classifiers and integrate them with classical dimension reduction techniques such as PCA and Haar wavelet transform. The proposed methodology is evaluated using two distinct datasets, based on English and Bengali languages. Experimental results show that after dimensionality reduction of the data, performance of the quantum-based hybrid algorithms were consistent and better than classical methods.
    
[^61]: 区块链资产监管的自然语言处理：一份路线图

    NLP for Crypto-Asset Regulation: A Roadmap. (arXiv:2310.10333v1 [cs.CY])

    [http://arxiv.org/abs/2310.10333](http://arxiv.org/abs/2310.10333)

    这篇论文介绍了自然语言处理在加密资产监管中的应用，并提出了两个贡献。首先，调查了对未受监管的加密资产白皮书进行文本分析的现有应用，并发现了研究空白。然后，分析了欧盟的加密资产市场法规引入的变化，探讨了在新的监管框架内整合自然语言处理的机遇和挑战。这些发现为未来的研究提供了基础，有潜力使监管机构、加密资产发行者和投资者受益。

    

    在快速发展的区块链资产领域，白皮书是投资者指导的重要文件，在欧盟的加密资产市场法规（MiCAR）下，它们现在面临前所未有的内容要求。自然语言处理可以作为分析这些文件和协助监管合规的强大工具。本文为该主题提供了两个贡献。首先，我们调查了对未受监管的区块链资产白皮书进行文本分析的现有应用，发现了可以通过跨学科合作来填补研究空白。然后，我们对MiCAR引入的变化进行了分析，突出了在新的监管框架内整合自然语言处理的机遇和挑战。我们的研究结果为进一步的研究奠定了基础，并有潜力使监管机构、区块链资产发行者和投资者受益。

    In the rapidly evolving field of crypto-assets, white papers are essential documents for investor guidance, and are now subject to unprecedented content requirements under the EU's Markets in Crypto-Assets Regulation (MiCAR). Natural Language Processing can serve as a powerful tool for both analyzing these documents and assisting in regulatory compliance. This paper delivers two contributions to the topic. First, we survey existing applications of textual analysis to unregulated crypto-asset white papers, uncovering a research gap that could be bridged with interdisciplinary collaboration. We then conduct an analysis of the changes introduced by MiCAR, highlighting the opportunities and challenges of integrating NLP within the new regulatory framework. Our findings set the stage for further research, with the potential to benefit regulators, crypto-asset issuers, and investors.
    
[^62]: GPT-4V(ision)能够用于医疗应用吗？GPT-4V在多模态医学诊断中的案例研究。

    Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis. (arXiv:2310.09909v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.09909](http://arxiv.org/abs/2310.09909)

    本研究评估了OpenAI的最新模型GPT-4V在多模态医学诊断中的性能，包括成像模态和解剖学识别。

    

    近期，由于大型基础模型的推动，人工智能的发展取得了巨大的进步，引起了公众的广泛关注。本研究旨在评估OpenAI最新模型GPT-4V(ision)在多模态医学诊断领域的性能。我们评估了17个人体系统，包括中枢神经系统、头颈部、心脏、胸部、血液学、肝胆、胃肠、泌尿生殖、妇科、产科、乳腺、肌肉骨骼、脊柱、血管、肿瘤、创伤、儿科，以及从8种日常临床常用的成像模态获得的图像，例如X光、计算机断层扫描(CT)、磁共振成像(MRI)、正电子发射断层扫描(PET)、数字减影血管造影(DSA)、乳腺X光摄影术、超声和病理学。我们测试了GPT-4V在多个临床任务中的能力，包括成像模态和解剖学识别，无论是否提供专利历史。

    Driven by the large foundation models, the development of artificial intelligence has witnessed tremendous progress lately, leading to a surge of general interest from the public. In this study, we aim to assess the performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm of multimodal medical diagnosis. Our evaluation encompasses 17 human body systems, including Central Nervous System, Head and Neck, Cardiac, Chest, Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology, Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma, Pediatrics, with images taken from 8 modalities used in daily clinic routine, e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA), Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on multiple clinical tasks with or without patent history provided, including imaging modality and anatomy recognition, 
    
[^63]: 通过语义格重排序提高自动语音识别系统中的上下文识别能力

    Improved Contextual Recognition In Automatic Speech Recognition Systems By Semantic Lattice Rescoring. (arXiv:2310.09680v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.09680](http://arxiv.org/abs/2310.09680)

    通过深度学习模型和transformer的重新评分，我们提出了一种通过语义格重排序来提高自动语音识别系统中上下文识别能力的方法。

    

    自动语音识别（ASR）受到了广泛的研究关注。最近的突破使得ASR系统在准确转录口语的能力上取得了重要进展，这是构建对话代理的关键进步。然而，准确辨别上下文相关的单词和短语仍然是一项迫切的挑战。在这项工作中，我们提出了一种通过语义格处理来增强ASR系统中上下文识别能力的新方法，利用深度学习模型在准确交付各种词汇和说话风格的转录方面具有出色的能力。我们的解决方案包括使用隐马尔可夫模型和高斯混合模型（HMM-GMM），以及深度神经网络（DNN）模型，将语言建模和声学建模结合起来，以获得更高的准确性。我们通过使用基于transformer的模型来重新评分单词格，使我们的网络具备了非凡的能力。

    Automatic Speech Recognition (ASR) has witnessed a profound research interest. Recent breakthroughs have given ASR systems different prospects such as faithfully transcribing spoken language, which is a pivotal advancement in building conversational agents. However, there is still an imminent challenge of accurately discerning context-dependent words and phrases. In this work, we propose a novel approach for enhancing contextual recognition within ASR systems via semantic lattice processing leveraging the power of deep learning models in accurately delivering spot-on transcriptions across a wide variety of vocabularies and speaking styles. Our solution consists of using Hidden Markov Models and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks (DNN) models integrating both language and acoustic modeling for better accuracy. We infused our network with the use of a transformer-based model to properly rescore the word lattice achieving remarkable capabilities with a palpa
    
[^64]: 数学方程生成的表达树解码策略

    An Expression Tree Decoding Strategy for Mathematical Equation Generation. (arXiv:2310.09619v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.09619](http://arxiv.org/abs/2310.09619)

    本研究提出了一种表达树解码策略，将树结构整合到数学方程生成中，以解决当前顺序方法忽视并行和依赖关系的问题。

    

    从自然语言生成数学方程需要准确理解数学表达式之间的关系。现有方法可以大致分为标记级生成和表达式级生成两类。前者将方程视为数学语言，按顺序生成数学标记。表达式级方法逐个生成每个表达式。然而，每个表达式表示一个求解步骤，这些步骤之间自然存在并行或依赖关系，但当前的顺序方法忽视了这些关系。因此，我们将树结构整合到表达式级生成中，并提出了一种表达树解码策略。为了生成一个以表达式为节点的树，我们采用逐层并行解码策略：在每一层同时解码多个独立表达式（叶节点），并重复逐层进行并行解码，以顺序生成依赖于其他表达式的父节点表达式。

    Generating mathematical equations from natural language requires an accurate understanding of the relations among math expressions. Existing approaches can be broadly categorized into token-level and expression-level generation. The former treats equations as a mathematical language, sequentially generating math tokens. Expression-level methods generate each expression one by one. However, each expression represents a solving step, and there naturally exist parallel or dependent relations between these steps, which are ignored by current sequential methods. Therefore, we integrate tree structure into the expression-level generation and advocate an expression tree decoding strategy. To generate a tree with expression as its node, we employ a layer-wise parallel decoding strategy: we decode multiple independent expressions (leaf nodes) in parallel at each layer and repeat parallel decoding layer by layer to sequentially generate these parent node expressions that depend on others. Beside
    
[^65]: Reward-Augmented Decoding: 使用单向奖励模型实现高效的受控文本生成

    Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model. (arXiv:2310.09520v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.09520](http://arxiv.org/abs/2310.09520)

    该论文介绍了一种名为Reward-Augmented Decoding (RAD)的文本生成方法，使用小型的单向奖励模型来鼓励语言模型生成具有特定属性的文本。RAD在生成非有害和情感受控文本方面表现最佳，并且在非常大的语言模型上也很有效。

    

    尽管大型语言模型已经在许多应用中证明了其有效性，但它们通常生成的文本存在问题或者缺乏所需的属性。本文提出了一种名为Reward-Augmented Decoding (RAD)的文本生成方法，它利用一个小型的单向奖励模型来鼓励语言模型生成具有特定属性的文本。具体而言，RAD利用奖励模型对生成的文本进行评分，并通过重新调整采样概率来更倾向于高奖励的标记。通过使用单向奖励模型，RAD能够缓存先前生成步骤的激活值，降低计算开销。通过在生成非有害和情感受控文本方面的实验，我们证明RAD在仅改变生成过程的方法中表现最佳，并且与涉及重新训练语言模型的最先进方法相当。我们进一步验证了RAD在非常大的语言模型上的有效性。

    While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models w
    
[^66]: 对大型语言模型在非分布式逻辑推理任务上的系统评估

    A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])

    [http://arxiv.org/abs/2310.09430](http://arxiv.org/abs/2310.09430)

    通过对大型语言模型在非分布式逻辑推理任务上进行系统评估，我们发现这些模型在处理我们新构建的数据集时都存在困难，尽管它们在其他自然语言处理任务上表现良好。这表明这些模型在逻辑推理方面的泛化和鲁棒性仍需要进一步研究。

    

    大型语言模型（LLMs），如GPT-3.5和GPT-4，已经将人工系统在各种自然语言处理任务上的性能提升到接近人类水平。然而，它们在逻辑推理方面的泛化和鲁棒性仍未得到充分评估。为了探索这种能力，我们提出了三个新的逻辑推理数据集，分别名为"ReClor-plus"、"LogiQA-plus"和"LogiQAv2-plus"，每个数据集都包含三个子集：第一个是选项随机打乱，第二个是将正确选项替换为"没有其他选项是正确的"，第三个是前两个子集的组合。我们在这些数据集上进行了实验，使用了鉴别和生成型的LLMs，并表明这些简单的技巧极大地阻碍了语言模型的性能。尽管在原始的公开可用数据集上表现出优秀的性能，但我们发现所有模型都很难回答我们新构建的数据集。我们展示了通过扰动引入任务变化可以提高模型的性能。

    Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
    
[^67]: 多层自适应对比学习在对话生成中的知识内化

    Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation. (arXiv:2310.08943v1 [cs.CL])

    [http://arxiv.org/abs/2310.08943](http://arxiv.org/abs/2310.08943)

    这是一篇关于知识引导对话生成的论文，通过引入多层自适应对比学习（MACL）框架，并在令牌级和序列级上动态采样负例来解决模型简单插入知识片段导致的退化问题。

    

    知识引导的对话生成旨在通过整合外部知识来补充上下文，从而缓解文本退化问题。然而，模型往往无法以类似人类的方式将此信息内化到回答中。相反，它只是简单地将提供的知识片段插入到普通的回答中。因此，生成的回答往往乏味、不连贯，并且缺乏互动性，这意味着退化问题仍未解决。在这项工作中，我们首先发现，这种复制式退化主要是由于弱概率目标造成的，它允许模型通过仅基于重叠的表面模式匹配来“欺骗”目标。为了克服这个挑战，我们提出了一个多层自适应对比学习（MACL）框架，该框架在令牌级和序列级上动态采样负例，并随后惩罚退化行为。

    Knowledge-grounded dialogue generation aims to mitigate the issue of text degeneration by incorporating external knowledge to supplement the context. However, the model often fails to internalize this information into responses in a human-like manner. Instead, it simply inserts segments of the provided knowledge into generic responses. As a result, the generated responses tend to be tedious, incoherent, and in lack of interactivity which means the degeneration problem is still unsolved. In this work, we first find that such copying-style degeneration is primarily due to the weak likelihood objective, which allows the model to "cheat" the objective by merely duplicating knowledge segments in a superficial pattern matching based on overlap. To overcome this challenge, we then propose a Multi-level Adaptive Contrastive Learning (MACL) framework that dynamically samples negative examples and subsequently penalizes degeneration behaviors at both the token-level and sequence-level. Extensive
    
[^68]: LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models

    LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])

    [http://arxiv.org/abs/2310.08659](http://arxiv.org/abs/2310.08659)

    本论文提出了LoftQ：一种针对大型语言模型的LoRA精调感知量化框架。该框架同时对LLM进行量化，并为LoRA精调找到适当的低秩初始化，以缓解量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。

    

    量化是为大型语言模型提供服务的不可或缺的技术，并最近被应用于LoRA精调中。本文关注在预训练模型上同时应用量化和LoRA精调的场景。在这种情况下，常常观察到完整精调和量化加LoRA精调方法之间在下游任务表现上存在一致的差距。为了解决这个问题，我们提出了LoftQ（LoRA-Fine-Tuning-aware Quantization）——一种新的量化框架，用于同时对LLM进行量化，并找到适当的低秩初始化来进行LoRA精调。这种初始化减轻了量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。我们在自然语言理解、问答、摘要和自然语言生成任务上评估了我们的方法。实验证明，我们的方法非常有效，在性能上优于现有的方法。

    Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
    
[^69]: BioT5：在生物学中利用化学知识和自然语言关联丰富跨模态整合

    BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])

    [http://arxiv.org/abs/2310.07276](http://arxiv.org/abs/2310.07276)

    BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。

    

    最近在生物研究领域的进展利用分子、蛋白质和自然语言的整合来增强药物发现。然而，当前的模型存在一些限制，如生成无效的分子SMILES、对上下文信息的利用不足以及对结构化和非结构化知识的等量处理。为了解决这些问题，我们提出了一个全面的预训练框架BioT5，它通过化学知识和自然语言关联丰富了生物学中的跨模态整合。BioT5利用SELFIES进行100%鲁棒的分子表示，并从非结构化的生物文献中提取生物实体周围上下文的知识。此外，BioT5区分结构化和非结构化知识，从而更有效地利用信息。在微调后，BioT5在各种任务中展现出卓越的性能，表明其强大的能力。

    Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
    
[^70]: Whispering LLaMA：一种用于语音识别的跨模态生成错误校正框架

    Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition. (arXiv:2310.06434v1 [cs.CL])

    [http://arxiv.org/abs/2310.06434](http://arxiv.org/abs/2310.06434)

    Whispering LLaMA是一种用于语音识别的跨模态生成错误校正框架，通过融合声学信息和外部语言表示，生成准确的语音转录上下文，相对于n-best假设，词错误率性能提升了37.66%。

    

    我们引入了一种新的跨模态融合技术，用于生成准确的语音转录上下文，以进行自动语音识别 (ASR) 的生成式错误校正。与现有的基于排名的重新评分方法不同，我们的方法灵活运用独特的初始化技术和参数有效的算法，通过预训练的语音和文本模型提升了ASR性能。通过对多样化的ASR数据集进行评估，我们评估了我们的融合技术的稳定性和可复现性，相对于n-best假设，我们的方法的词错误率性能提升了37.66%。为了鼓励未来的研究，我们将我们的代码和预训练模型开源在https://github.com/Srijith-rkr/Whispering-LLaMA上。

    We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we evaluate the stability and reproducibility of our fusion technique, demonstrating its improved word error rate relative (WERR) performance in comparison to n-best hypotheses by relatively 37.66%. To encourage future research, we have made our code and pre-trained models open source at https://github.com/Srijith-rkr/Whispering-LLaMA.
    
[^71]: GPT-Driver: 使用GPT学习驾驶

    GPT-Driver: Learning to Drive with GPT. (arXiv:2310.01415v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.01415](http://arxiv.org/abs/2310.01415)

    本文提出了一种将OpenAI GPT-3.5模型应用于自动驾驶的运动规划器的方法，通过将运动规划转化为语言建模问题，利用大型语言模型生成驾驶轨迹，提高了运动规划的泛化能力和推理能力。

    

    我们提出了一种简单而有效的方法，可以将OpenAI GPT-3.5模型转化为自动驾驶车辆的可靠运动规划器。运动规划是自动驾驶中的核心挑战，旨在规划一个安全舒适的驾驶轨迹。现有的运动规划器主要利用启发式方法来预测驾驶轨迹，然而这些方法在面对新颖和未知的驾驶场景时展现出不足的泛化能力。在本文中，我们提出了一种新颖的运动规划方法，利用大型语言模型（LLM）固有的强大推理能力和泛化潜力。我们方法的基本见解是将运动规划重新构建为一个语言建模问题，这是一个之前未被探索的视角。具体而言，我们将规划器的输入和输出表示为语言记号，并利用LLM通过对坐标的语言描述生成驾驶轨迹。

    We present a simple yet effective approach that can transform the OpenAI GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion planning is a core challenge in autonomous driving, aiming to plan a driving trajectory that is safe and comfortable. Existing motion planners predominantly leverage heuristic methods to forecast driving trajectories, yet these approaches demonstrate insufficient generalization capabilities in the face of novel and unseen driving scenarios. In this paper, we propose a novel approach to motion planning that capitalizes on the strong reasoning capabilities and generalization potential inherent to Large Language Models (LLMs). The fundamental insight of our approach is the reformulation of motion planning as a language modeling problem, a perspective not previously explored. Specifically, we represent the planner inputs and outputs as language tokens, and leverage the LLM to generate driving trajectories through a language description of coo
    
[^72]: 解码图像：释放大型语言模型。

    Decoding Imagery: Unleashing Large Language Models. (arXiv:2309.16705v1 [cs.CV])

    [http://arxiv.org/abs/2309.16705](http://arxiv.org/abs/2309.16705)

    该论文研究了Google Bard这个多模态大型语言模型的能力，发现Bard在将视觉和语言分析相结合方面依赖于对图像进行有根据的猜测，可以解决视觉上有挑战的问题但无法修改原始视觉对象。

    

    在一个挑战-响应研究中，我们对Google Bard进行了64个视觉挑战，旨在探究多模态大型语言模型（LLMs）的能力。这些挑战涵盖了各种类别，包括“视觉情境推理”，“视觉文本推理”和“下一场景预测”等，以确定Bard在融合视觉和语言分析方面的能力。我们的研究结果显示，Bard倾向于根据图片做出有根据的猜测，特别是在确定图片中的线索时。与GPT4等其他模型不同，Bard似乎不依赖于像Tesseract这样的光学字符识别库，而是像Google Lens和Visual API这样的深度学习模型一样，识别复杂图片中的文本。显着的是，Bard可以通过视觉方式解决ChatGPT无法理解的验证码，推荐使用Tesseract解决方案。此外，虽然Bard模型基于视觉输入提出了解决方案，但它无法重建或修改原始的视觉对象来支持其结论。

    In a challenge-response study, we subjected Google Bard to 64 visual challenges designed to probe multimodal Large Language Models (LLMs). The challenges spanned diverse categories, including "Visual Situational Reasoning," "Visual Text Reasoning," and "Next Scene Prediction," among others, to discern Bard's competence in melding visual and linguistic analyses. Our findings indicate that Bard tends to rely on making educated guesses about visuals, especially when determining cues from images. Unlike other models like GPT4, Bard does not appear to rely on optical character recognition libraries like Tesseract but recognizes text in complex images like deep learning models such as Google Lens and Visual API. Significantly Bard can solve CAPTCHAs visually that ChatGPT fails to understand, recommending Tesseract solutions. Moreover, while the Bard model proposes solutions based on visual input, it cannot recreate or modify the original visual objects to support its conclusions. Bard fails 
    
[^73]: 基础模型的有效长上下文缩放

    Effective Long-Context Scaling of Foundation Models. (arXiv:2309.16039v1 [cs.CL])

    [http://arxiv.org/abs/2309.16039](http://arxiv.org/abs/2309.16039)

    我们提出了一系列支持长上下文的基础模型，通过连续预训练和数据增强来实现，这些模型在语言建模和研究基准上都取得了显著的改进，并且通过成本效益的指导调整程序，已经超过了现有模型在长上下文任务上的性能。

    

    我们提出了一系列支持最多32768个标记的有效上下文窗口的长上下文LLM。我们通过从Llama 2连续预训练、在长文本上采样的数据集上进行训练来构建我们的模型系列。我们对语言建模、合成上下文探测任务和各种研究基准进行了广泛的评估。在研究基准上，我们的模型在大多数常规任务上都取得了一致的改进，并在长上下文任务上相对于Llama 2取得了显著的改进。值得注意的是，通过一种成本效益的指导调整程序，不需要人工标注的长指导数据，70B版本已经在一套长上下文任务中超过了gpt-3.5-turbo-16k的整体性能。除了这些结果，我们还对我们方法的各个组成部分进行了深入分析。我们深入研究了Llama的位置编码，并讨论了它在建模长依赖方面的局限性。我们还研究了变量的影响。

    We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of var
    
[^74]: ChatGPT-BCI：使用GPT、EEG和眼动生物标记器在语义推理阅读理解中进行单词级神经状态分类

    ChatGPT-BCI: Word-Level Neural State Classification Using GPT, EEG, and Eye-Tracking Biomarkers in Semantic Inference Reading Comprehension. (arXiv:2309.15714v1 [cs.CL])

    [http://arxiv.org/abs/2309.15714](http://arxiv.org/abs/2309.15714)

    本研究通过联合分析大型语言模型（LLMs）、眼动和脑电图（EEG）数据，研究了大脑在阅读过程中处理与关键字相关度不同的单词的神经状态，并提供了关于语义推理阅读理解中神经状态的洞察。

    

    随着大型语言模型（LLM）（如GPT）的迅猛发展，人类和机器理解语义语言意义的能力已经进入了一个新阶段。这需要跨认知科学和自然语言处理（NLP）领域的跨学科研究。本文的目标是通过联合分析LLMs、眼动和脑电图（EEG）数据，研究大脑在阅读过程中如何处理与关键字相关程度不同的单词，从而提供关于个体神经状态在语义关系阅读理解任务中的洞察。我们还使用特征工程方法改进了与关键字高相关度和低相关度的单词阅读过程中与注视相关的EEG数据的分类。在12名受试者中，此单词级别分类的最佳验证准确率超过了60％。

    With the recent explosion of large language models (LLMs), such as Generative Pretrained Transformers (GPT), the need to understand the ability of humans and machines to comprehend semantic language meaning has entered a new phase. This requires interdisciplinary research that bridges the fields of cognitive science and natural language processing (NLP). This pilot study aims to provide insights into individuals' neural states during a semantic relation reading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and electroencephalographic (EEG) data to study how the brain processes words with varying degrees of relevance to a keyword during reading. We also use a feature engineering approach to improve the fixation-related EEG data classification while participants read words with high versus low relevance to the keyword. The best validation accuracy in this word-level classification is over 60\% across 12 subjects. Words of high relevance to the inference keyword had sig
    
[^75]: 基于Wav2vec的言语失语症检测和严重程度级别分类

    Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech. (arXiv:2309.14107v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.14107](http://arxiv.org/abs/2309.14107)

    本研究使用基于wav2vec的模型，实现了言语失语症的自动检测和严重程度级别分类任务，并在准确率上取得了显著的提升。

    

    通过声学语音信号的自动检测和严重程度级别分类，可以作为医学诊断中的工具用于言语失语症。本研究使用预训练的wav2vec 2.0模型作为特征提取器，构建了言语失语症的检测和严重程度级别分类系统。实验使用了广泛使用的UA-speech数据库。在检测实验中，结果表明使用wav2vec模型的第一层嵌入效果最好，相比于最佳基准特征（声谱图），准确率提高了1.23%。在研究的严重程度级别分类任务中，结果表明最终层的嵌入相比于最佳基准特征（梅尔频率倒谱系数）准确率提高了10.62%。

    Automatic detection and severity level classification of dysarthria directly from acoustic speech signals can be used as a tool in medical diagnosis. In this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor to build detection and severity level classification systems for dysarthric speech. The experiments were carried out with the popularly used UA-speech database. In the detection experiments, the results revealed that the best performance was obtained using the embeddings from the first layer of the wav2vec model that yielded an absolute improvement of 1.23% in accuracy compared to the best performing baseline feature (spectrogram). In the studied severity level classification task, the results revealed that the embeddings from the final layer gave an absolute improvement of 10.62% in accuracy compared to the best baseline features (mel-frequency cepstral coefficients).
    
[^76]: 使用声门源特征进行病理性声音的分析和检测

    Analysis and Detection of Pathological Voice using Glottal Source Features. (arXiv:2309.14080v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.14080](http://arxiv.org/abs/2309.14080)

    本研究提供了对声门源特征的系统分析，并研究了它们在声音病理检测中的有效性。实验结果表明声门源包含的信息对于病理性声音的判别具有重要作用。

    

    自动检测声音病理能够实现客观评估和早期干预诊断。本研究对声门源特征进行了系统分析，并研究了它们在声音病理检测中的有效性。通过使用准封闭相位（QCP）的声门逆滤波方法估计得到的声门流动、使用零频率滤波（ZFF）方法计算得到的近似声门源信号，以及直接使用声学声音信号来提取声门源特征。此外，我们提出从由QCP和ZFF计算出的声门源波形中推导出梅尔频率倒谱系数（MFCCs），以有效捕捉病理性声音的声门源频谱变化。实验使用了两个数据库，即医院Universitario Principe de Asturias（HUPA）数据库和Saarbrucken Voice Disorders（SVD）数据库。特征分析揭示了声门源中包含的信息的判别作用。

    Automatic detection of voice pathology enables objective assessment and earlier intervention for the diagnosis. This study provides a systematic analysis of glottal source features and investigates their effectiveness in voice pathology detection. Glottal source features are extracted using glottal flows estimated with the quasi-closed phase (QCP) glottal inverse filtering method, using approximate glottal source signals computed with the zero frequency filtering (ZFF) method, and using acoustic voice signals directly. In addition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from the glottal source waveforms computed by QCP and ZFF to effectively capture the variations in glottal source spectra of pathological voice. Experiments were carried out using two databases, the Hospital Universitario Principe de Asturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database. Analysis of features revealed that the glottal source contains information that discri
    
[^77]: 角度优化的文本嵌入

    AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])

    [http://arxiv.org/abs/2309.12871](http://arxiv.org/abs/2309.12871)

    本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。

    

    高质量的文本嵌入对于提升语义文本相似度（STS）任务至关重要，而这些任务又是大型语言模型（LLM）应用中的关键组成部分。然而，现有的文本嵌入模型面临的一个普遍挑战是渐变消失问题，主要是由于它们在优化目标中依赖余弦函数，而余弦函数具有饱和区域。为了解决这个问题，本文提出了一种称为AnglE的新型角度优化文本嵌入模型。AnglE的核心思想是在一个复杂空间中引入角度优化。这种新颖的方法有效地缓解了余弦函数饱和区域产生的不利影响，从而可以阻碍梯度并阻碍优化过程。为了建立全面的STS评估，我们在现有的短文本STS数据集和从GitHub Issues中新收集的长文本STS数据集上进行了实验。此外，我们还研究了具有有限标签数据的特定领域STS场景，并探讨了AnglE的工作原理。

    High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
    
[^78]: AceGPT：将大型语言模型本地化为阿拉伯文

    AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v1 [cs.CL])

    [http://arxiv.org/abs/2309.12053](http://arxiv.org/abs/2309.12053)

    本研究旨在开发阿拉伯文的本地化大型语言模型(AceGPT)，通过预训练、监督微调和增强学习方法来培养具备文化意识和价值观一致的阿拉伯文模型，以满足阿拉伯语社区特定应用需求。评估结果表明，AceGPT在各项基准测试中都是最先进的阿拉伯文模型。

    

    本文探讨了开发适用于阿拉伯文的本地化大型语言模型(LLM)的迫切需求和方法论，阿拉伯文具有独特的文化特征，这些特征目前的主流模型如ChatGPT并未充分解决。在考虑文化敏感性和本地价值观时还存在关键问题。为此，本文提出了一个打包解决方案，包括进一步使用阿拉伯文本进行预训练、使用本地阿拉伯指令和阿拉伯语GPT-4回应进行监督微调(SFT)，以及使用对本地文化和价值观敏感的奖励模型进行增强学习与人工智能反馈(RLAIF)。目标是训练具备文化意识和与价值观一致的阿拉伯文LLM，以满足阿拉伯语社区多样化的特定应用需求。广泛的评估表明，所得到的名为AceGPT的阿拉伯文LLM在各种基准测试中均是最先进的。

    This paper explores the imperative need and methodology for developing a localized Large Language Model (LLM) tailored for Arabic, a language with unique cultural characteristics that are not adequately addressed by current mainstream models like ChatGPT. Key concerns additionally arise when considering cultural sensitivity and local values. To this end, the paper outlines a packaged solution, including further pre-training with Arabic texts, supervised fine-tuning (SFT) using native Arabic instructions and GPT-4 responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using a reward model that is sensitive to local culture and values. The objective is to train culturally aware and value-aligned Arabic LLMs that can serve the diverse application-specific needs of Arabic-speaking communities.  Extensive evaluations demonstrated that the resulting LLM called `\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including instruction-following benchmark (i.e
    
[^79]: 使用大型语言模型的自动个性化印象生成PET报告

    Automatic Personalized Impression Generation for PET Reports Using Large Language Models. (arXiv:2309.10066v1 [cs.AI])

    [http://arxiv.org/abs/2309.10066](http://arxiv.org/abs/2309.10066)

    本研究旨在使用fine-tuned大型语言模型实现自动个性化生成全身PET报告的准确印象。通过训练语言模型并引入阅读医生的身份信息，模型能够学习医生特定的报告风格。研究结果经过专家评估和核医学医生的质量评分认可，证明该方法在实践中具有潜在的应用价值。

    

    目的：确定通过fine-tuned大型语言模型(LLMs)是否可以为全身PET报告生成准确的个性化印象。材料和方法：使用teacher-forcing算法在PET报告语料库上训练了12个语言模型，输入是报告发现，参考是临床印象。额外的输入标记编码了阅读医生的身份，使模型能够学习医生特定的报告风格。我们的语料库包括2010年至2022年间从我们机构收集的37,370份回顾性PET报告。通过与两名核医学（NM）医生的质量评分进行30个评估指标的基准测试，最匹配的指标选择了用于专家评估的模型。在部分数据子集中，根据6个质量维度和一个总体实用性评分（5分制），三名核医学医生评估了模型生成的印象和原始临床印象。

    Purpose: To determine if fine-tuned large language models (LLMs) can generate accurate, personalized impressions for whole-body PET reports. Materials and Methods: Twelve language models were trained on a corpus of PET reports using the teacher-forcing algorithm, with the report findings as input and the clinical impressions as reference. An extra input token encodes the reading physician's identity, allowing models to learn physician-specific reporting styles. Our corpus comprised 37,370 retrospective PET reports collected from our institution between 2010 and 2022. To identify the best LLM, 30 evaluation metrics were benchmarked against quality scores from two nuclear medicine (NM) physicians, with the most aligned metrics selecting the model for expert evaluation. In a subset of data, model-generated impressions and original clinical impressions were assessed by three NM physicians according to 6 quality dimensions and an overall utility score (5-point scale). Each physician reviewe
    
[^80]: 前缀扩散：一种用于多样化图像字幕的轻量级扩散模型

    Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning. (arXiv:2309.04965v1 [cs.CV])

    [http://arxiv.org/abs/2309.04965](http://arxiv.org/abs/2309.04965)

    Prefix-diffusion是一种轻量级的图像字幕扩散模型，通过在扩散过程中注入前缀图像嵌入来实现多样性，并通过预训练模型和额外的映射网络来减少参数。该模型能够生成多样的字幕，同时保持流畅性和相关性，并取得了有希望的性能。

    

    尽管在图像字幕生成方面取得了很大的进展，但生成的字幕的多样性有限和参数规模较大仍然是这些系统在实际应用中的主要障碍。在这项工作中，我们提出了一种轻量级的图像字幕网络，结合了连续扩散，称为前缀扩散。为了实现多样性，我们设计了一种高效的方法，将前缀图像嵌入到扩散模型的去噪过程中。为了减少可训练的参数，我们使用预训练模型提取图像特征，并进一步设计了额外的映射网络。前缀扩散能够以相对较少的参数生成多样化的字幕，同时保持字幕的流畅性和相关性，从扩散模型的生成能力中受益。我们的工作为扩展图像字幕的扩散模型铺平了道路，并与最近的方法相比取得了有希望的性能。

    While impressive performance has been achieved in image captioning, the limited diversity of the generated captions and the large parameter scale remain major barriers to the real-word application of these systems. In this work, we propose a lightweight image captioning network in combination with continuous diffusion, called Prefix-diffusion. To achieve diversity, we design an efficient method that injects prefix image embeddings into the denoising process of the diffusion model. In order to reduce trainable parameters, we employ a pre-trained model to extract image features and further design an extra mapping network. Prefix-diffusion is able to generate diverse captions with relatively less parameters, while maintaining the fluency and relevance of the captions benefiting from the generative capabilities of the diffusion model. Our work paves the way for scaling up diffusion models for image captioning, and achieves promising performance compared with recent approaches.
    
[^81]: 什么时候编程思维对推理起作用?

    When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])

    [http://arxiv.org/abs/2308.15452](http://arxiv.org/abs/2308.15452)

    提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。

    

    大型语言模型（LLM）的推理能力在体现出人工智能领域中起着关键作用。尽管像编程思维提示这样的方法对于使用编程语言来解决复杂推理任务的LLM非常有效，但代码数据对推理能力的具体影响仍未充分探索。为了填补这一空白，我们提出了复杂性影响推理分数（CIRS），它结合了结构和逻辑属性，以衡量代码和推理能力之间的相关性。具体而言，我们使用抽象语法树来编码结构信息，并通过考虑难度和圈复杂度来计算逻辑复杂性。通过实证分析，我们发现并非所有复杂性的代码数据都可以被LLM学习或理解。最佳复杂性水平对于通过编程辅助提示改善推理能力至关重要。然后我们设计了一个自动合成的方法...

    The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
    
[^82]: 对大型语言模型代码生成的鲁棒性和可靠性的研究

    A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10335](http://arxiv.org/abs/2308.10335)

    本研究针对大型语言模型生成的代码的可靠性和鲁棒性进行了研究，发现在真实的软件开发中可执行的代码并不能保证可靠和鲁棒，滥用API可能导致严重问题。这对初级开发者来说尤其危险，因为他们很难察觉到代码中的API滥用问题。

    

    最近，大型语言模型(LLMs)在理解自然语言和生成编程代码方面显示出了非凡能力。当遇到编码问题时，软件工程师常常会咨询LLMs。尽管已经做出了一些努力来避免语法错误并使代码与预期的语义对齐，但LLMs生成的代码的可靠性和鲁棒性尚未被深入研究。在真实的软件开发环境中，可执行的代码并不等同于可靠和鲁棒的代码。在生成的代码中滥用API可能会导致严重的问题，如资源泄漏、程序崩溃。更糟糕的是，LLM代码生成服务的用户实际上是最容易受到这些看似正确的代码影响的开发者——他们通常是不熟悉LLMs为他们生成代码的API的初级开发者。因此，他们很难察觉到API的滥用。

    Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in t
    
[^83]: XSTest: 用于识别大型语言模型中夸大安全行为的测试套件

    XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. (arXiv:2308.01263v1 [cs.CL])

    [http://arxiv.org/abs/2308.01263](http://arxiv.org/abs/2308.01263)

    本文介绍了一个名为XSTest的测试套件，旨在识别大型语言模型中夸大的安全行为。该套件由200个安全提示组成，涵盖十种提示类型，旨在引出模型的系统性问题。

    

    没有适当的保护措施，大型语言模型很容易遵循恶意指令并生成有害内容。这激发了安全工作，如红队测试和大规模反馈学习，旨在使模型既有用又无害。然而，这两个目标之间存在一种紧张关系，因为无害性要求模型拒绝遵从不安全的提示，从而无法提供帮助。最近的一些证据表明，一些模型可能在平衡上存在问题，以至于即使使用类似不安全提示的语言或提及敏感主题的明显安全提示也会被拒绝。本文介绍了一个名为XSTest的新测试套件，以系统化和结构化的方式识别这种夸张的安全行为。目前，XSTest包括200个安全提示，涵盖十种提示类型，良好校准的模型不应该拒绝遵循这些提示。我们描述了XSTest的创建和组成，并使用测试套件突显系统性的问题。

    Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way. In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with. We describe XSTest's creation and composition, and use the test suite to highlight systematic f
    
[^84]: 三思而后行：大型语言模型不确定性测量的探索性研究

    Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v1 [cs.SE])

    [http://arxiv.org/abs/2307.10236](http://arxiv.org/abs/2307.10236)

    本研究从不确定性的角度对大型语言模型进行了探索性研究，通过实验发现不确定性估计方法在探索和抵制大型语言模型的不良行为方面具有潜力。

    

    大型语言模型（LLMs）的最近性能突破为众多工业应用和领域提供了新的机遇。然而，LLMs的错误生成，如虚假预测、错误信息和幻觉，也引发了对LLMs可靠性的严重关注，尤其在对安全、可靠性有敏感的场景中，可能阻碍其在实际中的应用。尽管不确定性估计已经显示出其在解释一般机器学习（ML）模型的预测风险方面的潜力，但关于它是否以及在多大程度上有助于探索LLMs的能力和抵制其不良行为方面知之甚少。为了弥合这一差距，本文从不确定性的角度开展了关于LLMs风险评估的探索性研究。具体来说，我们使用12种不确定性估计方法和4个LLMs在4个重要的自然语言处理（NLP）任务上进行实验，以调查不确定性在探索LLMs能力和对抗其不良行为方面的程度。

    The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent unc
    
[^85]: 对大型语言模型评估的调查

    A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])

    [http://arxiv.org/abs/2307.03109](http://arxiv.org/abs/2307.03109)

    本文综述了大型语言模型（LLMs）的评估方法，关注三个关键维度：评估什么、在哪里评估以及如何评估。评估任务包括自然语言处理、推理、医学应用、伦理学、教育、自然和社会科学、代理应用等多个领域。本文为社会层面对LLMs潜在风险的理解提供了重要参考。

    

    大型语言模型（LLMs）由于在各种应用中表现出的前所未有的性能而在学术界和工业界越来越受欢迎。随着LLMs在研究和日常使用中继续发挥着重要作用，它们的评估变得越来越关键，不仅在任务水平上，而且在社会层面上，以更好地了解它们的潜在风险。在过去的几年里，已经做出了相当大的努力来从不同的角度来研究LLMs。本文综述了LLMs的这些评估方法，重点关注三个关键维度：评估什么、在哪里评估以及如何评估。首先，我们从评估任务的角度提供了一个概述，涵盖了一般的自然语言处理任务、推理、医学应用、伦理学、教育、自然科学和社会科学、代理应用和其他领域。其次，我们通过深入探讨评估方法和基准答案来回答“在哪里”和“如何”这两个问题。

    Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
    
[^86]: 使用生成对抗网络生成无监督文本嵌入空间用于文本合成

    Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])

    [http://arxiv.org/abs/2306.17181](http://arxiv.org/abs/2306.17181)

    本论文提出了一种使用生成对抗网络（GAN）生成连续文本嵌入空间的方法（TESGAN），以解决传统GAN在自然语言生成中的限制。这种方法通过引入连续的文本嵌入空间取代离散的标记，使得生成器在通过反向传播更新梯度时更加有效。

    

    生成对抗网络（GAN）是一种用于数据合成的模型，通过生成器和判别器的竞争来创建逼真的数据。尽管GAN在图像合成方面得到了广泛研究，但在自然语言生成方面存在固有的限制。因为自然语言由离散的标记组成，生成器在通过反向传播更新梯度时遇到困难；因此，大多数文本-GAN研究使用奖励系统以随机标记为基础生成句子。因此，先前研究中的生成器在对抗训练之前以自回归方式进行预训练，导致合成的句子重复训练数据。在本文中，我们使用类似原始GAN的框架来合成句子。更具体地说，我们提出了文本嵌入空间生成对抗网络（TESGAN），它生成连续的文本嵌入空间来解决梯度反向传播的问题。

    Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
    
[^87]: REFLECT:对机器人经历进行总结，以用于失败解释和纠正

    REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v1 [cs.RO])

    [http://arxiv.org/abs/2306.15724](http://arxiv.org/abs/2306.15724)

    提出了REFLECT框架，可以将机器人多感官数据转化为分层总结，并使用大型语言模型进行失败解释。该框架能够生成有益的失败解释，帮助机器人完成任务。

    

    自动检测和分析失败执行是实现可解释和稳健机器人系统的关键。最近，大型语言模型（LLM）在文本输入上展示了强大的常识推理能力。为了利用LLM的力量进行机器人失败解释，我们提出了一个框架REFLECT，将多感官数据转化为机器人过去经验的分层总结，并使用逐步失败解释算法查询LLM。基于解释，失败纠正规划器生成一个可执行计划，以纠正失败并完成任务。为了系统评估该框架，我们创建了RoboFail数据集，并展示了我们基于LLM的框架能够生成有益的失败解释，从而帮助成功的纠正规划。项目网站：https://roboreflect.github.io/

    The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong common sense reasoning skills on textual inputs. To leverage the power of LLM for robot failure explanation, we propose a framework REFLECT, which converts multi-sensory data into a hierarchical summary of robot past experiences and queries LLM with a progressive failure explanation algorithm. Conditioned on the explanation, a failure correction planner generates an executable plan for the robot to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset and show that our LLM-based framework is able to generate informative failure explanations that assist successful correction planning. Project website: https://roboreflect.github.io/
    
[^88]: ChatGPT和大型语言模型在生物医学和健康领域的机遇与挑战

    Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health. (arXiv:2306.10070v1 [cs.CY])

    [http://arxiv.org/abs/2306.10070](http://arxiv.org/abs/2306.10070)

    本文探讨了ChatGPT和大型语言模型在生物医学和健康领域的多样应用，发现在文本生成方面已经取得了重大进展，但对于其他应用进展缓慢，LLMs还没有真正彻底改变生物医学领域。

    

    ChatGPT由于其卓越的文本生成能力，已经引起了公众和领域专家的广泛关注，并产生了在生物医学和健康领域的各种应用。本文探讨了大型语言模型（LLMs）如ChatGPT在生物医学和健康领域的多样应用，具体探讨生物医学信息检索、问答、医疗文本摘要、信息抽取和医学教育等领域，并研究LLMs是否具有真正的转型力量以彻底改变这些任务或者生物医学领域的独特复杂性是否提出了独特的挑战。通过广泛的文献调研，我们发现在文本生成任务方面已经取得了重大进展，超越了以前的最先进方法。对于其他应用，进展还比较缓慢。总体而言，LLMs还没有彻底改变生物医学领域。

    ChatGPT has drawn considerable attention from both the general public and domain experts with its remarkable text generation capabilities. This has subsequently led to the emergence of diverse applications in the field of biomedicine and health. In this work, we examine the diverse applications of large language models (LLMs), such as ChatGPT, in biomedicine and health. Specifically we explore the areas of biomedical information retrieval, question answering, medical text summarization, information extraction, and medical education, and investigate whether LLMs possess the transformative power to revolutionize these tasks or whether the distinct complexities of biomedical domain presents unique challenges. Following an extensive literature survey, we find that significant advances have been made in the field of text generation tasks, surpassing the previous state-of-the-art methods. For other applications, the advances have been modest. Overall, LLMs have not yet revolutionized the bio
    
[^89]: 揭秘 GPT 自我修复代码生成能力

    Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])

    [http://arxiv.org/abs/2306.09896](http://arxiv.org/abs/2306.09896)

    本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。

    

    大型语言模型 (LLM) 在代码生成方面表现出色，但在挑战性编程任务上仍面临困难。自我修复——即模型调试并修复自己的代码——最近成为提高性能的一种流行方式。然而，关于自我修复如何有效地发挥作用的研究还非常有限。有人会想知道，当同一模型生成代码时，模型究竟能否提供准确的反馈。在本文中，我们分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，这是一个由多种编码挑战组成的具有挑战性的数据集。我们首先建立了一种新的评估策略 pass@t，该策略衡量了任务通过率与从模型中抽样的总标记数，从而实现对仅基于抽样的方法的公平比较。通过这种评估策略，我们发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，并确定了影响自我修复表现的几个因素。具体而言，我们发现，在输入噪声较少且模型对初始输出不太自信的较短和较简单的任务中，自我修复效果更好。我们还表明，仅在某些代码部分上应用自我修复可以非常有效。此外，我们提出了一种新的引导修复方法，利用外部反馈来增强 GPT 模型的自我修复能力，在 APPS 数据集上获得性能提升。

    Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
    
[^90]: 大型语言模型不能作为抽象推理器

    Large Language Models Are Not Abstract Reasoners. (arXiv:2305.19555v1 [cs.CL])

    [http://arxiv.org/abs/2305.19555](http://arxiv.org/abs/2305.19555)

    本文通过对最先进的大型语言模型进行抽象推理任务评估，发现它们在这方面的表现十分有限，揭示了其在推理方面的局限性。

    

    大型语言模型在自然语言处理任务上表现出极好的性能，包括文本理解和常识推理等。然而，这些成功的机制尚不清楚，LLMs是否能够达到人类的认知能力或这些模型是否还存在根本性的局限性也不确定。抽象推理是认知的基本任务，包括从少量数据中找到和应用一般模式。评估深度神经结构在这个任务上的表现可以揭示它们在推理方面的潜在局限性和广泛的泛化能力，这是一个目前未被探索的领域。本文对最先进的LLMs进行了大量评估，发现它们在抽象推理任务中的表现非常有限，并探究了造成这种差异的原因。

    Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. However, the mechanisms responsible for this success remain unknown, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally limited. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we perform extensive evaluations of state-of-the-art LLMs on abstract reasoning tasks, showing that they achieve very limited performance in contrast with other natural language tasks, and we investigate the reasons for this difference. We apply techniques that have been show
    
[^91]: 自然语言生成的主动学习

    Active Learning for Natural Language Generation. (arXiv:2305.15040v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15040](http://arxiv.org/abs/2305.15040)

    本文首次对自然语言生成进行了主动学习的系统研究，发现现有的主动学习策略在不同情况下性能不一致，强调了分类和生成场景之间的差异。

    

    自然语言生成（NLG）领域由于手动标注涉及的过程极其昂贵且耗时，导致标记数据严重匮乏。应对这一问题的一种自然方法是主动学习（AL），一种通过选择最具信息量的示例进行标记来提高注释效率的机器学习技术。然而，虽然在文本分类的背景下，AL已经得到了广泛研究，但其在NLG中的应用还没有被充分探索。本文首次对NLG进行了主动学习的系统研究，考虑了多种任务和多种主要选择策略，并利用一个强大的指导调优模型。我们的结果表明，现有的AL策略的性能不一致，在某些情况下超过了随机示例选择的基准线，但在其他情况下没有达到该基准线。我们强调了分类和生成场景之间的一些显著差异。

    The field of Natural Language Generation (NLG) suffers from a severe shortage of labeled data due to the extremely expensive and time-consuming process involved in manual annotation. A natural approach for coping with this problem is active learning (AL), a well-known machine learning technique for improving annotation efficiency by selectively choosing the most informative examples to label. However, while AL has been well-researched in the context of text classification, its application to NLG remains largely unexplored. In this paper, we present a first systematic study of active learning for NLG, considering a diverse set of tasks and multiple leading selection strategies, and harnessing a strong instruction-tuned model. Our results indicate that the performance of existing AL strategies is inconsistent, surpassing the baseline of random example selection in some cases but not in others. We highlight some notable differences between the classification and generation scenarios, and 
    
[^92]: Sophia：一种用于语言模型预训练的可扩展的随机二阶优化器

    Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14342](http://arxiv.org/abs/2305.14342)

    Sophia是一种用于语言模型预训练的可扩展的二阶优化算法，使用对角Hessian作为预调节器，并进行元素级别的裁剪控制更新大小。

    

    鉴于语言模型预训练的巨大成本，优化算法的微小改进将会大大降低训练的时间和成本。Adam及其变种一直是最先进的，而更复杂的二阶（基于Hessian的）优化器往往会带来太多的每步开销。在这篇论文中，我们提出了Sophia，一种简单可扩展的二阶优化器，它使用轻量级估计的对角Hessian作为预调节器。更新步骤是梯度的移动平均值除以估计Hessian的移动平均值，然后进行元素级别的裁剪。裁剪控制了最坏情况下的更新大小，并控制了Hessian在轨迹上的非凸性和快速变化的负面影响。Sophia只在每几次迭代中估计对角Hessian，这几乎没有平均每步的时间和内存开销。在使用GPT m进行语言建模时，

    Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT m
    
[^93]: Ties Matter: 用成对准确性和关联性校准元评估现代指标

    Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration. (arXiv:2305.14324v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14324](http://arxiv.org/abs/2305.14324)

    提出使用成对准确性和关联性校准的方法对现代指标进行元评估，以更公平地评估指标的排名性能。

    

    Kendall's tau经常被用来元评估机器翻译评估指标对个别翻译的评分。它对成对分数比较的重点很直观，但也引发了如何处理关联性的问题，这是一个模糊的领域，激发了文献中不同变种的研究。我们证明，在现代机器翻译元评估等环境中，现有的变种由于其对关联性的处理而存在缺陷，并且在某些情况下甚至可以被操纵。相反，我们建议使用一种成对准确性的版本对指标进行元评估，该版本给予指标正确预测关联性的信用，并结合自动引入关联性到指标评分的关联性校准过程，使得可以公平比较那些预测和不预测关联性的指标。我们通过论证并提供实验证据，表明这些改动可以更公平地评估指标的排名性能。

    Kendall's tau is frequently used to meta-evaluate how well machine translation (MT) evaluation metrics score individual translations. Its focus on pairwise score comparisons is intuitive but raises the question of how ties should be handled, a gray area that has motivated different variants in the literature. We demonstrate that, in settings like modern MT meta-evaluation, existing variants have weaknesses arising from their handling of ties, and in some situations can even be gamed. We propose instead to meta-evaluate metrics with a version of pairwise accuracy that gives metrics credit for correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores, enabling fair comparison between metrics that do and do not predict ties. We argue and provide experimental evidence that these modifications lead to fairer ranking-based assessments of metric performance.
    
[^94]: R2H: 构建能够回应帮助请求的多模态导航助手

    R2H: Building Multimodal Navigation Helpers that Respond to Help Requests. (arXiv:2305.14260v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14260](http://arxiv.org/abs/2305.14260)

    本论文介绍了一个名为R2H的基准，旨在构建能够回应帮助请求的多模态导航助手。该研究主要包括两个任务，即根据对话历史生成响应和在与任务执行者合作时进行响应评估。研究采用了一种创新的任务导向多模态响应生成模型来构建导航助手代理。

    

    智能导航助手代理人非常重要，因为它们可以通过环境感知和对话能力，引导用户在未知区域中进行导航，并成为残障人士的潜在辅助工具。在这项工作中，我们首先介绍了一个新的基准，Respond to Help Requests (R2H)，以促进开发能够回应帮助请求的多模态导航助手，利用现有的基于对话的体验数据集。R2H主要包括两个任务：(1) Respond to Dialog History (RDH)，用于评估助手代理人根据给定的对话历史生成信息量丰富的响应的能力，以及(2) Respond during Interaction (RdI)，用于评估响应在与任务执行者持续合作时的效果和效率。此外，我们还探讨了两种构建导航助手代理的方法，包括对一个新的任务导向的多模态响应生成模型进行微调，该模型能够观察并回应。

    Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help Requests (R2H), to promote the development of multi-modal navigation helpers capable of responding to requests for help, utilizing existing dialog-based embodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH), which assesses the helper agent's ability to generate informative responses based on a given dialog history, and (2) Respond during Interaction (RdI), which evaluates the effectiveness and efficiency of the response during consistent cooperation with a task performer. Furthermore, we explore two approaches to construct the navigation-helper agent, including fine-tuning a novel task-oriented multi-modal response generation model that can see and respond, name
    
[^95]: 从图像-文本-图谱空间中进行粗到细的对比学习，提高视觉语言组合能力

    Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality. (arXiv:2305.13812v1 [cs.CL])

    [http://arxiv.org/abs/2305.13812](http://arxiv.org/abs/2305.13812)

    本研究提出了一种基于场景图的对比学习框架，通过将从文本中解析出的场景图视为图像场景图的代理，并对图进行分解和增强，从简单到复杂的对比学习以将各种复杂度的句子对齐到同一幅图像上，同时在场景图空间中提出了新的负样本挖掘技术，以改善视觉语言组合能力。

    

    对比学习视觉语言模型已经在视觉和语言表示学习方面取得了显著进展，从而为各种下游多模态任务提供了最先进的模型。但是，最近的研究凸显了这些模型在对象、属性和关系的组成推理能力方面的严重限制。场景图已经成为一种理解图像组成的有效方式。这些是图像的图形结构化语义表示，包括场景中的对象、它们的属性和与场景中其他对象的关系。在本文中，我们将从文本中解析出的场景图视为图像场景图的代理，并提出了一种图分解和增强框架，以及从简单到复杂的对比学习目标，将各种复杂度的句子对齐到同一幅图像上。同时，我们还在场景图空间提出了新的负样本挖掘技术，用于改善对比学习。在三个视觉语言任务上的实验表明，我们的方法优于强大的视觉语言基线，特别是在对象、属性和关系的组成推理方面。

    Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning, leading to state-of-the-art models for various downstream multimodal tasks. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. Along with this, we propose novel negative mining techniques in the scene graph space for im
    
[^96]: 《关于相关文档中语义差异的无监督识别》

    Towards Unsupervised Recognition of Semantic Differences in Related Documents. (arXiv:2305.13303v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13303](http://arxiv.org/abs/2305.13303)

    本论文提出了一种无监督识别语义差异的方法，并对三种无监督方法进行了研究。实验结果显示，基于单词对齐和句级对比学习的方法与真实标签之间存在稳健的相关性。然而，所有的无监督方法仍有很大的改进空间。

    

    自动突出导致两个文档之间语义差异的单词可能对各种应用程序有用。我们将语义差异识别（RSD）定义为一个标记级回归任务，并研究了三个依赖于屏蔽语言模型的无监督方法。为了评估这些方法，我们从基本的英语句子开始，并逐渐转向更复杂的跨语言文档对。我们的结果表明，基于单词对齐和句级对比学习的方法与真实标签之间存在稳健的相关性。然而，所有的无监督方法仍有很大的改进空间。可以在https://github.com/ZurichNLP/recognizing-semantic-differences找到重现我们实验的代码。

    Automatically highlighting words that cause semantic differences between two documents could be useful for a wide range of applications. We formulate recognizing semantic differences (RSD) as a token-level regression task and study three unsupervised approaches that rely on a masked language model. To assess the approaches, we begin with basic English sentences and gradually move to more complex, cross-lingual document pairs. Our results show that an approach based on word alignment and sentence-level contrastive learning has a robust correlation to gold labels. However, all unsupervised approaches still leave a large margin of improvement. Code to reproduce our experiments is available at https://github.com/ZurichNLP/recognizing-semantic-differences
    
[^97]: MacLaSa: 通过从紧致潜在空间中的高效采样实现多方面可控的文本生成

    MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space. (arXiv:2305.12785v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12785](http://arxiv.org/abs/2305.12785)

    MacLaSa是一种通过从紧致潜在空间中高效采样的方法，实现多方面可控的文本生成。它通过估计紧致潜在空间和使用基于ODE的鲁棒采样器来有效生成具有多个期望属性的句子。利用变分自动编码器网络来消除不同方面之间的域差距，并能够制定联合能量模型和插入任意属性鉴别器。

    

    多方面可控的文本生成旨在生成同时具有多个期望属性的流畅句子。传统方法要么在解码阶段结合多个操作符，通常在离散文本空间中进行昂贵的迭代或搜索，要么为每个方面单独训练控制器，由于不同方面之间的差异而导致文本质量的退化。为了解决这些限制，我们引入了一种新的多方面控制方法，称为MacLaSa，它通过估计多个方面的紧致潜在空间，并利用基于常微分方程（ODEs）的鲁棒采样器进行高效采样。为了消除不同方面之间的域差距，我们利用变分自动编码器（VAE）网络将来自不同数据源的文本序列映射到接近的潜在表示。估计的潜在空间使得能够制定联合能量模型（EBMs）并插入任意属性鉴别器。

    Multi-aspect controllable text generation aims to generate fluent sentences that possess multiple desired attributes simultaneously. Traditional methods either combine many operators in the decoding stage, often with costly iteration or search in the discrete text space, or train separate controllers for each aspect, resulting in a degeneration of text quality due to the discrepancy between different aspects. To address these limitations, we introduce a novel approach for multi-aspect control, namely MacLaSa, that estimates compact latent space for multiple aspects and performs efficient sampling with a robust sampler based on ordinary differential equations (ODEs). To eliminate the domain gaps between different aspects, we utilize a Variational Autoencoder (VAE) network to map text sequences from varying data sources into close latent representations. The estimated latent space enables the formulation of joint energy-based models (EBMs) and the plugging in of arbitrary attribute discr
    
[^98]: 医学文本的多语言简化

    Multilingual Simplification of Medical Texts. (arXiv:2305.12532v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12532](http://arxiv.org/abs/2305.12532)

    本研究介绍了MultiCochrane，这是医学领域中第一个句子对齐的多语言文本简化数据集，通过多语言简化直接将复杂文本简化为多种语言的简化文本。

    

    自动化文本简化旨在产生复杂文本的简化版本。在医学领域，这项任务尤为重要，因为最新的医学发现通常通过复杂和技术性的文章进行传播。这为寻求最新医学发现信息的普通人造成了障碍，进而阻碍了健康素养的提高。大部分医学文本简化的现有工作都集中在单语言环境中，导致这些证据只能用一种语言（通常是英语）提供。本研究通过多语言简化直接将复杂文本简化为多种语言的简化文本来解决这个问题。我们介绍了MultiCochrane，这是医学领域中第一个四种语言（英语、西班牙语、法语和波斯语）的句子对齐多语言文本简化数据集。我们通过广泛的人工评估和分析，在这些语言之间评估了模型的微调和零样本模型。

    Automated text simplification aims to produce simple versions of complex texts. This task is especially useful in the medical domain, where the latest medical findings are typically communicated via complex and technical articles. This creates barriers for laypeople seeking access to up-to-date medical findings, consequently impeding progress on health literacy. Most existing work on medical text simplification has focused on monolingual settings, with the result that such evidence would be available only in just one language (most often, English). This work addresses this limitation via multilingual simplification, i.e., directly simplifying complex texts into simplified texts in multiple languages. We introduce MultiCochrane, the first sentence-aligned multilingual text simplification dataset for the medical domain in four languages: English, Spanish, French, and Farsi. We evaluate fine-tuned and zero-shot models across these languages, with extensive human assessments and analyses. 
    
[^99]: LLM自身可读取和生成CXR图像

    LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])

    [http://arxiv.org/abs/2305.11490](http://arxiv.org/abs/2305.11490)

    该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。

    

    借助于近期大语言模型（LLMs）的显著发展，人们正积极尝试将LLMs的实用性扩展到多模态任务。已经有人尝试连接语言和视觉信息，并且也在不断尝试为LLMs添加视觉能力。然而，现有的尝试只使用LLMs作为图像解码器，没有尝试通过自然语言来生成图像。通过采用VQ-GAN框架，将图像的潜在表示视为一种文本标记，我们提出了一种新方法，可以微调预先训练的LLM，以像文本一样读取和生成图像，而无需进行结构更改、额外的训练目标或训练专门的网络，同时仍保留LLM的指令跟随能力。我们将此框架应用于胸部X线（CXR）图像的生成任务中，因为这是一个复杂信息在视觉和语言之间翻译的领域。

    Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
    
[^100]: TrueTeacher: 使用大语言模型学习事实一致性评估

    TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models. (arXiv:2305.11171v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.11171](http://arxiv.org/abs/2305.11171)

    TrueTeacher是一种使用大型语言模型生成合成数据来进行事实一致性评估的方法，相较于传统方法，TrueTeacher不依赖于人工编写的摘要，多语言特性，实验证明能够显著提升模型的性能。

    

    事实一致性评估通常使用自然语言推理模型进行，然而这些模型在评估摘要时取得的成功有限。以往的工作通过合成训练数据改进了此类模型。然而，这些数据通常基于扰动的人工编写摘要，与真实的模型生成摘要在特性上常常存在差异，并且对可能存在的事实错误的覆盖范围有限。另一方面，大型语言模型（LLM）最近在直接评估生成任务方面显示了有希望的结果，但计算开销过大，无法实际应用。出于对这些限制的动机，我们引入了TrueTeacher，一种使用LLM注释多样的模型生成摘要来生成合成数据的方法。与以往的工作不同，TrueTeacher不依赖于人工编写的摘要，并且具有多语言特性。在TRUE基准测试上的实验表明，使用我们的数据训练的学生模型在性能上远远超过了NLI模型和之前的工作。

    Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the st
    
[^101]: CLEME: 针对语法错误修正的去偏置多参考评估方法

    CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction. (arXiv:2305.10819v1 [cs.CL])

    [http://arxiv.org/abs/2305.10819](http://arxiv.org/abs/2305.10819)

    提出了一种新的Chunk-LEvel Multi-reference Evaluation (CLEME)方法来解决在多参考环境下评估GEC系统时存在的偏差问题，其通过消除由不一致的编辑边界引起的偏差和自动确定语法错误的边界来提高了GEC评估的可靠性。

    

    由于Grammatical Error Correction (GEC)是一项高度主观的任务，因此评估其性能变得困难。设计尽可能客观的评估指标对于GEC任务的发展至关重要。先前的主流评估指标，即基于参考的指标，在提取编辑时未考虑多个参考的存在，从而引入偏见到多参考评估中。为了克服这个问题，我们提出了Chunk-LEvel Multi-reference Evaluation (CLEME)方法，旨在在多参考环境中评估GEC系统。首先，CLEME为源、假设和所有参考建立具有一致边界的块序列，从而消除由不一致的编辑边界引起的偏差。然后，基于发现存在不同语法错误之间的边界，我们自动确定了语法错误的边界，并以一种新颖的方式计算了F$_{0.5}$得分。我们提出的CLEME方法可以有效地去偏置多参考评估GEC系统，并提高GEC评估的可靠性。

    It is intractable to evaluate the performance of Grammatical Error Correction (GEC) systems since GEC is a highly subjective task. Designing an evaluation metric that is as objective as possible is crucial to the development of GEC task. Previous mainstream evaluation metrics, i.e., reference-based metrics, introduce bias into the multi-reference evaluation because they extract edits without considering the presence of multiple references. To overcome the problem, we propose Chunk-LEvel Multi-reference Evaluation (CLEME) designed to evaluate GEC systems in multi-reference settings. First, CLEME builds chunk sequences with consistent boundaries for the source, the hypothesis and all the references, thus eliminating the bias caused by inconsistent edit boundaries. Then, based on the discovery that there exist boundaries between different grammatical errors, we automatically determine the grammatical error boundaries and compute F$_{0.5}$ scores in a novel way. Our proposed CLEME approach
    
[^102]: 解释器理解您的意思: 由语音翻译协助的端到端口语理解

    The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation. (arXiv:2305.09652v1 [cs.CL])

    [http://arxiv.org/abs/2305.09652](http://arxiv.org/abs/2305.09652)

    语音翻译(ST)是预训练语音模型进行端到端口语理解的良好手段。通过引入ST，我们的模型在单语言和跨语言场景下表现均好，具有更高的性能。

    

    尽管当前在文本和语音上有大规模预训练的语言模型，但端到端口语理解仍然难以实现，特别是在多语言情况下。机器翻译已被确定为强大的文本预训练目标，因为它使模型能够捕捉输入语句的高级语义和不同语言之间的关联，这对于处理更低级别的声学帧的语音模型非常有用。本文特别针对跨语言口语理解任务，证明了语音翻译(ST)是预训练语音模型进行端到端口语理解的良好手段，无论是在单语言场景还是跨语言场景下。通过引入ST，我们的模型在使用SLURP、MINDS-14和NMSQA基准测试进行单语言和多语言意图分类以及口语问答时，相比当前基准测试方法均具有更高性能。为验证我们方法的有效性，我们还发布了两个新的基准数据集，分别来自合成和真实数据。

    End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases. Machine translation has been established as a powerful pretraining objective on text as it enables the model to capture high-level semantics of the input utterance and associations between different languages, which is desired for speech models that work on lower-level acoustic frames. Motivated particularly by the task of cross-lingual SLU, we demonstrate that the task of speech translation (ST) is a good means of pretraining speech models for end-to-end SLU on both monolingual and cross-lingual scenarios.  By introducing ST, our models give higher performance over current baselines on monolingual and multilingual intent classification as well as spoken question answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the effectiveness of our methods, we also release two new benchmark datasets from both syntheti
    
[^103]: ChatGPT是否公平可靠？评估大型语言模型推荐中的公平性

    Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. (arXiv:2305.07609v1 [cs.IR])

    [http://arxiv.org/abs/2305.07609](http://arxiv.org/abs/2305.07609)

    这篇论文介绍了一种新的推荐范式——通过LLM进行推荐，但由于LLMs可能存在社会偏见，需要进一步调查RecLLM所做推荐的公正性。为此，作者提出了一个新的公平性基准——FaiRLLM，并针对音乐和电影推荐场景中的八个敏感属性进行了评估。

    

    大型语言模型（LLM）的显着成就导致一种新的推荐范式——通过LLM进行推荐（RecLLM）。然而，需要注意LLMs可能包含社会偏见，因此需要进一步调查RecLLM所做推荐的公正性。为了避免RecLLM的潜在风险，有必要从用户的各种敏感属性角度评估RecLLM的公平性。由于RecLLM范式与传统推荐范式之间存在差异，因此直接使用传统推荐的公平性基准是有问题的。为了解决这个困境，我们提出了一个新的基准，称为“通过LLM的推荐的公平性”（FaiRLLM）。该基准包括精心设计的指标和数据集，涵盖两个推荐场景中的八个敏感属性：音乐和电影。通过利用我们的FaiRLLM基准，我们进行了一项评估。

    The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm -- Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma, we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. By utilizing our FaiRLLM benchmark, we conducted an evaluation 
    
[^104]: 一个LLM知道自己在撒谎的内部状态

    The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])

    [http://arxiv.org/abs/2304.13734](http://arxiv.org/abs/2304.13734)

    该论文研究了LLM生成不准确或虚假信息的问题，提出了一种简单而有效的方法，利用LLM的隐藏层激活来确定语句的真实性。在实验中，该方法表现出较好的检测效果，并有利于提高LLM的可信度。

    

    虽然大型语言模型（LLM）在各种任务中表现出了卓越的性能，但它们（可能）最为突出的缺点是以自信的语气生成不准确或虚假的信息。本文假设LLM的内部状态可以用于揭示一个语句的真实性。因此，我们介绍了一种简单但有效的方法来检测LLM所生成语句的真实性，该方法利用LLM的隐藏层激活来确定语句的真实性。为了训练和评估我们的方法，我们构建了一个包含六个不同主题的数据集，其中包含真实和虚假的语句。一个分类器被训练出来，根据LLM的激活值来检测哪个语句是真实的或虚假的。具体而言，分类器接收LLM为数据集中每个语句生成的激活值作为输入。我们的实验表明，我们检测语句真实性的方法甚至比少量提示方法表现更好，凸显了利用LLM的内部状态来提高其可信度的潜力。

    While Large Language Models (LLMs) have shown exceptional performance in various tasks, their (arguably) most prominent drawback is generating inaccurate or false information with a confident tone. In this paper, we hypothesize that the LLM's internal state can be used to reveal the truthfulness of a statement. Therefore, we introduce a simple yet effective method to detect the truthfulness of LLM-generated statements, which utilizes the LLM's hidden layer activations to determine the veracity of statements. To train and evaluate our method, we compose a dataset of true and false statements in six different topics. A classifier is trained to detect which statement is true or false based on an LLM's activation values. Specifically, the classifier receives as input the activation values from the LLM for each of the statements in the dataset. Our experiments demonstrate that our method for detecting statement veracity significantly outperforms even few-shot prompting methods, highlighting
    
[^105]: PAXQA: 在训练规模上生成跨语言问答样例

    PAXQA: Generating Cross-lingual Question Answering Examples at Training Scale. (arXiv:2304.12206v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.12206](http://arxiv.org/abs/2304.12206)

    PAXQA提出了一种使用现有平行语料库生成跨语言问答样例的方法，通过将问题生成模型和注释投影相结合，以更好地进行问题翻译和回答翻译。该方法在4种语言中生成了662K个例子。

    

    现有的问答系统的成功很大程度上归功于大规模、高质量的训练数据。 这种标注工作成本高，并且在跨语言环境中难度增加。 因此，之前的跨语言问答研究主要集中在发布评估数据集，然后将零-shot方法作为基准应用。 本研究提出了一种用于跨语言问答的合成数据生成方法，该方法利用现有平行语料库中的间接监督。 我们的方法称为PAXQA（跨语言(x) QA的投影注释），将跨语言问答分解为两个阶段。 首先，我们将问题生成（QG）模型应用于英文侧面。 然后，我们应用注释投影来翻译问题和答案。 为了更好地翻译问题，我们提出了一种新的词汇约束机器翻译的用法，其中从平行双文本中提取约束实体。 我们将PAXQA应用于4种语言（662K ex）的跨语言问答样例生成。

    Existing question answering (QA) systems owe much of their success to large, high-quality training data. Such annotation efforts are costly, and the difficulty compounds in the cross-lingual setting. Therefore, prior cross-lingual QA work has focused on releasing evaluation datasets, and then applying zero-shot methods as baselines. This work proposes a synthetic data generation method for cross-lingual QA which leverages indirect supervision from existing parallel corpora. Our method termed PAXQA (Projecting annotations for cross-lingual (x) QA) decomposes cross-lingual QA into two stages. First, we apply a question generation (QG) model to the English side. Second, we apply annotation projection to translate both the questions and answers. To better translate questions, we propose a novel use of lexically-constrained machine translation, in which constrained entities are extracted from the parallel bitexts.  We apply PAXQA to generate cross-lingual QA examples in 4 languages (662K ex
    
[^106]: 可编辑用户档案的可控文本推荐方法

    Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])

    [http://arxiv.org/abs/2304.04250](http://arxiv.org/abs/2304.04250)

    本文提出了一种新的概念值瓶颈模型LACE，用于可控文本推荐。该模型基于用户文档学习个性化的概念表示，并通过多种交互方式为用户提供了控制推荐的机制，验证了在离线和在线实验中该模型的推荐质量和有效性。

    

    实现高质量推荐的方法通常依赖于从交互数据中学习潜在表示。然而这些方法没有提供给用户控制所接收的推荐的机制。本文提出了LACE，一种新颖的概念值瓶颈模型，用于可控文本推荐。LACE基于用户交互的文档检索，将每个用户表示为简洁的可读的概念集，并基于用户文档学习概念的个性化表示。该基于概念的用户档案被利用来做出推荐。我们的模型设计通过透明的用户档案，提供了控制推荐的多种直观交互方式。我们首先在三个推荐任务（温启动、冷启动和零样本）的六个数据集上进行了离线评估，验证了从LACE获得的推荐质量。接下来，我们在在线实验中验证了LACE的有效性和用户控制能力。

    Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the 
    
[^107]: 统一对比传递框架与传播结构用于提高低资源谣言检测

    A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v1 [cs.CL])

    [http://arxiv.org/abs/2304.01492](http://arxiv.org/abs/2304.01492)

    该文介绍了一个利用对比传递框架和传播结构，将从充足资源的谣言数据学到的特征适应于低资源情况下的方式，可以检测到跨越语言和领域界限的谣言。

    

    大量的谣言伴随着突发新闻或热门话题而传播，这严重阻碍了真相的传播。现有的谣言检测算法展示了在前几天新闻上良好性能的前景，但是由于缺乏训练数据和先前的专业知识，它们很难发现与预期事件有关的谣言，特别是在不同语言（即低资源环境）中传播的谣言。在本文中，我们提出了一个统一的对比传递框架，通过将从充足资源的谣言数据学到的特征适应于低资源情况下的特征来检测谣言。具体来说，我们首先将在社交媒体上传播的谣言表示为无向拓扑结构，然后通过统一对比范式进行Multi-scale图卷积网络的训练。我们的模型明确地突破了领域和/或语言问题的障碍，通过语言对齐和一种新颖的领域自适应对比。

    The truth is significantly hampered by massive rumors that spread along with breaking news or popular topics. Since there is sufficient corpus gathered from the same domain for model training, existing rumor detection algorithms show promising performance on yesterday's news. However, due to a lack of training data and prior expert knowledge, they are poor at spotting rumors concerning unforeseen events, especially those propagated in different languages (i.e., low-resource regimes). In this paper, we propose a unified contrastive transfer framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. More specifically, we first represent rumor circulated on social media as an undirected topology, and then train a Multi-scale Graph Convolutional Network via a unified contrastive paradigm. Our model explicitly breaks the barriers of the domain and/or language issues, via language alignment and a novel domain-adaptive contrastive 
    
[^108]: 优化ChatGPT在机器翻译中的表现

    Towards Making the Most of ChatGPT for Machine Translation. (arXiv:2303.13780v1 [cs.CL])

    [http://arxiv.org/abs/2303.13780](http://arxiv.org/abs/2303.13780)

    本文提出了任务和领域特定提示来优化ChatGPT在复杂机器翻译任务中的表现，研究发现温度设置和任务信息对ChatGPT表现有显著影响。

    

    ChatGPT在机器翻译中表现出卓越的能力。先前的研究表明，它在高资源语言方面可以达到商业系统相当的结果，但在复杂任务方面（例如低资源和远程语言对翻译）落后。然而，它们通常采用简单的提示，无法充分发挥ChatGPT的能力。本文旨在通过重新审视温度、任务信息和领域信息等几个方面，进一步挖掘ChatGPT的翻译能力，并相应地提出两种（简单但有效的）提示：任务特定提示（TSP）和领域特定提示（DSP）。

    ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this report, we aim to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information further improves ChatGPT's performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT's generalization ability and improve i
    
[^109]: 让每个样本都有用：关于在含噪NLP数据集中自我影响的稳定性和实用性

    Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets. (arXiv:2302.13959v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13959](http://arxiv.org/abs/2302.13959)

    本文研究了在NLP数据集中自我影响的稳定性和实用性，探讨了自我影响分数在数据清理中的适应性，以及其对机器翻译、问答和文本分类等任务性能的改进。

    

    在NLP领域，使用规模越来越大的数据集已成为推动最新技术发展的标准成分。然而，数据质量可能已成为解锁更大进展的瓶颈。由于现代数据集的多样性和规模，标准的数据过滤不容易应用，因为有害数据的多面性和过滤规则的隐晦性使其难以推广到多个任务。我们研究了训练样本的通用自我影响分数在数据清理中的适应性，分析了其捕捉自然异常值的功效，并研究了基于自我影响的数据清理在机器翻译、问答和文本分类等下游任务中对性能的改进程度。这些研究基于最近的自我影响计算和自动化课程学习方法。

    Increasingly larger datasets have become a standard ingredient to advancing the state-of-the-art in NLP. However, data quality might have already become the bottleneck to unlock further gains. Given the diversity and the sizes of modern datasets, standard data filtering is not straight-forward to apply, because of the multifacetedness of the harmful data and elusiveness of filtering rules that would generalize across multiple tasks. We study the fitness of task-agnostic self-influence scores of training examples for data cleaning, analyze their efficacy in capturing naturally occurring outliers, and investigate to what extent self-influence based data cleaning can improve downstream performance in machine translation, question answering and text classification, building up on recent approaches to self-influence calculation and automated curriculum learning.
    
[^110]: Pre-trained Vision and Language Models能否回答求知视觉问题？

    Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.11713](http://arxiv.org/abs/2302.11713)

    本研究介绍了一个专门针对无法仅凭常识知识回答的信息寻求问题而设计的视觉问答数据集InfoSeek。使用InfoSeek数据集，我们发现目前最先进的预训练多模态模型在回答求知视觉问题方面面临挑战，但在该数据集上进行微调可以激发模型使用细粒度知识。

    

    Pre-trained vision and language models在涉及图像和文本的任务中展示了领先的能力，包括视觉问答。然而，这些模型是否具备回答不仅仅查询视觉内容，而且还具有知识密集和信息寻求性质的问题的能力仍然不清楚。在本研究中，我们介绍了InfoSeek，一个专门针对无法仅凭常识知识回答的信息寻求问题而设计的视觉问答数据集。使用InfoSeek，我们分析了各种预训练的视觉问答模型，并深入了解它们的特点。我们的发现揭示了目前最先进的预训练多模态模型（如PaLI-X，BLIP2等）在回答求知视觉问题方面面临挑战，但在InfoSeek数据集上进行微调能够激发模型使用他们在预训练过程中学到的细粒度知识。此外，我们还展示了准确的视觉实体的重要性。

    Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training. Furthermore, we show that accurate visual entit
    
[^111]: 大型语言模型可被视为隐含的主题模型：解释和寻找好的示范以实现上下文学习

    Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11916](http://arxiv.org/abs/2301.11916)

    本研究发现，大型语言模型可以被视为隐式的主题模型，并提出了一种算法，从注释数据中选择最佳示范，大大提高了上下文学习的能力。

    

    近年来，预训练的大型语言模型表现出了在推理时实现少量样本学习能力的显著效率，被称为上下文学习。 然而，现有文献强调这种能力对少量样本示范的选择很敏感。本研究旨在通过贝叶斯视角研究上下文学习现象，将大型语言模型视为从示范中隐含地推断出相关信息的主题模型。在此前提下，我们提出了一种算法，用于从一组注释数据中选择最佳示范，并证明相对于随机选择基线的平均值，在八个不同的真实文本分类数据集上平均每个 GPT2 和 GPT3 模型有显着的 12.5% 的提升。我们的实证发现支持我们的假设，即大型语言模型可被视为隐含的主题模型。

    In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
    
[^112]: ConvLab-3：基于统一数据格式的灵活对话系统工具包

    ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format. (arXiv:2211.17148v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.17148](http://arxiv.org/abs/2211.17148)

    ConvLab-3是一个灵活的对话系统工具包，基于统一数据格式，简化了数据与模型的集成，支持迁移学习和RL，并提供快速开发和评估鲁棒的对话策略的功能。

    

    面向任务的对话系统（TOD）作为数字助手，可以引导用户完成诸如预订航班或寻找餐厅等各种任务。目前用于构建TOD系统的工具包通常在提供全面的数据、模型和实验环境方面存在不足，用户体验也不友好。我们介绍了ConvLab-3：一个多方面的对话系统工具包，旨在弥合这一差距。我们的统一数据格式简化了不同数据集和模型的集成，显著降低了研究广泛泛化和迁移的复杂性和成本。通过强化学习（RL）工具的增强，包括简化的训练过程、深入的评估工具以及多个用户模拟器的选择，ConvLab-3支持快速开发和评估鲁棒的对话策略。通过广泛的研究，我们证明了迁移学习和RL的有效性，并展示了ConvLab-3不仅是一款强大的工具供经验丰富的研究人员使用。

    Task-oriented dialogue (TOD) systems function as digital assistants, guiding users through various tasks such as booking flights or finding restaurants. Existing toolkits for building TOD systems often fall short of in delivering comprehensive arrays of data, models, and experimental environments with a user-friendly experience. We introduce ConvLab-3: a multifaceted dialogue system toolkit crafted to bridge this gap. Our unified data format simplifies the integration of diverse datasets and models, significantly reducing complexity and cost for studying generalization and transfer. Enhanced with robust reinforcement learning (RL) tools, featuring a streamlined training process, in-depth evaluation tools, and a selection of user simulators, ConvLab-3 supports the rapid development and evaluation of robust dialogue policies. Through an extensive study, we demonstrate the efficacy of transfer learning and RL and showcase that ConvLab-3 is not only a powerful tool for seasoned researchers
    
[^113]: 使用并行BERT深度神经网络进行假新闻检测

    Fake news detection using parallel BERT deep neural networks. (arXiv:2204.04793v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.04793](http://arxiv.org/abs/2204.04793)

    本文介绍了一种使用并行BERT深度神经网络的方法来检测假新闻。通过使用两个并行的BERT网络分别编码新闻标题和新闻正文，并且考虑到BERT网络的输入长度限制，该方法可以有效地进行假新闻的真实性检测。

    

    假新闻是社交网络和媒体面临的一个日益严重的挑战。虽然多年来检测假新闻一直是一个问题，但在最近几年社交网络的发展和新闻传播速度的增加之后，这个问题再次引起了关注。解决这个问题的几种方法之一是使用深度神经网络根据文本样式来检测假新闻。近年来，自然语言处理中最常用的深度神经网络形式之一是使用transformers进行迁移学习。BERT是最有希望的transformer之一，它在许多NLP基准测试中表现优异。本文介绍了MWPBert，它使用两个并行的BERT网络对全文新闻文章进行真实性检测。其中一个BERT网络编码新闻标题，另一个BERT网络编码新闻正文。由于BERT网络的输入长度是有限且常数，而新闻正文通常是一段长文本，我们无法将整个新闻文本输入到网络中。

    Fake news is a growing challenge for social networks and media. Detection of fake news always has been a problem for many years, but after the evolution of social networks and increasing speed of news dissemination in recent years has been considered again. There are several approaches to solving this problem, one of which is to detect fake news based on its text style using deep neural networks. In recent years, one of the most used forms of deep neural networks for natural language processing is transfer learning with transformers. BERT is one of the most promising transformers who outperforms other models in many NLP benchmarks. This article, we introduce MWPBert, which uses two parallel BERT networks to perform veracity detection on full-text news articles. One of the BERT networks encodes news headline, and another encodes news body. Since the input length of the BERT network is limited and constant and the news body is usually a long text, we cannot fed the whole news text into t
    
[^114]: BLM-17m: 一个用于推特上黑人生命至关重要话题检测的大规模数据集

    BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2105.01331](http://arxiv.org/abs/2105.01331)

    本论文提出了一个用于推特上检测黑人生命至关重要话题的大规模数据集BLM-17m，涵盖了乔治·弗洛伊德事件期间的17百万推文。作者提供了两个基线模型TF-IDF和LDA，并对其进行了评估。

    

    人权保护是世界上最重要的问题之一。本文旨在提供一个涵盖最近几个月全球影响深远的人权矛盾之一——乔治·弗洛伊德事件的数据集。我们提出了一个带有17百万推文的主题检测标记数据集。这些推文是从2020年5月25日至2020年8月21日收集的，涵盖了这一事件开始后的89天。我们通过监测全球和本地报纸的最热门新闻主题对数据集进行了标记。除此之外，我们还提供了两个基线模型，TF-IDF和LDA。我们使用三个不同的k值对这两种方法的精确度、召回率和F1分数进行了评估。收集到的数据集可以在https://github.com/MeysamAsgariC/BLMT 上找到。

    Protection of human rights is one of the most important problems of our world. In this paper, our aim is to provide a dataset which covers one of the most significant human rights contradiction in recent months affected the whole world, George Floyd incident. We propose a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of this incident. We labeled the dataset by monitoring most trending news topics from global and local newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We evaluated the results of these two methods with three different k values for metrics of precision, recall and f1-score. The collected dataset is available at https://github.com/MeysamAsgariC/BLMT.
    
[^115]: 定义经典逻辑的蕴涵关系

    Defining implication relation for classical logic. (arXiv:1312.7832v10 [math.LO] CROSS LISTED)

    [http://arxiv.org/abs/1312.7832](http://arxiv.org/abs/1312.7832)

    本研究旨在从经典逻辑中去除不正确的“析取到蕴涵”。通过提出一个新的逻辑系统，该系统在一般情况下无法推导出“析取到蕴涵”或其否定，从而达到了预期的目标。

    

    在经典逻辑中，“P蕴涵Q”等价于“非P或Q”。众所周知，这种等价关系存在问题。实际上，从“P蕴涵Q”可以推出“非P或Q”（“蕴涵到析取”是正确的），而从“非P或Q”通常不能推出“P蕴涵Q”（“析取到蕴涵”通常是不成立的），所以它们之间的等价关系通常是无效的。本研究旨在从经典逻辑（CL）中去除不正确的“析取到蕴涵”。本文提出了一个具有所期望属性的逻辑系统（IRL）：(1) 通过将“析取到蕴涵”添加到IRL中可以简单地得到CL；(2) 在一般情况下，“析取到蕴涵”与IRL无关（无法在IRL中推导出“析取到蕴涵”或其否定）。换句话说，IRL就是通过从CL中准确地去除“析取到蕴涵”而得到的系统。

    In classical logic, "P implies Q" is equivalent to "not-P or Q". It is well known that the equivalence is problematic. Actually, from "P implies Q", "not-P or Q" can be inferred ("Implication-to-disjunction" is valid), while from "not-P or Q", "P implies Q" cannot be inferred in general ("Disjunction-to-implication" is not generally valid), so the equivalence between them is invalid in general. This work aims to remove exactly the incorrect Disjunction-to-implication from classical logic (CL). The paper proposes a logical system (IRL) with the expected properties: (1) CL is simply obtained by adding Disjunction-to-implication to IRL, and (2) Disjunction-to-implication is independent of IRL (either Disjunction-to-implication or its negation cannot be derived in IRL) in the general case. In other words, IRL is just the system obtained by exactly removing Disjunction-to-implication from CL.
    

