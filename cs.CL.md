# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Model Editing Can Hurt General Abilities of Large Language Models.](http://arxiv.org/abs/2401.04700) | 这项论文指出，模型编辑可能会改善模型的事实性，但会以降低模型的通用能力为代价。在这项研究中，作者通过评估四种编辑方法在两个大型语言模型上的表现，发现这些编辑方法往往忽视了对模型通用能力可能产生的负面影响。 |
| [^2] | [Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers.](http://arxiv.org/abs/2401.04695) | 本研究提出了一种名为GRANOLA QA的评估设置，在开放领域问答中使用多粒度答案来评估预测的答案的准确性和信息量。作者提出了一种简单的方法来丰富现有数据集，并创建了一个多粒度版本的数据集。实验结果表明... |
| [^3] | [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation.](http://arxiv.org/abs/2401.04679) | RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。 |
| [^4] | [Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models.](http://arxiv.org/abs/2401.04658) | 本文介绍了Lightning Attention-2，这是第一个能够实现线性注意力理论计算优势的线性注意力实现。通过利用平铺的思想，分别处理了线性注意力计算中的内部块和外部块组件。具体来说，采用传统的注意力计算机制处理内部块，并使用新的累积求和方法处理外部块。 |
| [^5] | [DepressionEmo: A novel dataset for multilabel classification of depression emotions.](http://arxiv.org/abs/2401.04655) | 本文介绍了一个名为DepressionEmo的新数据集，旨在通过分析Reddit用户帖子来检测与抑郁相关的8种情绪。研究在该数据集上进行了情绪之间的相关性、分布和语言分析，并提供了几种文本分类方法。 |
| [^6] | [Applying Large Language Models API to Issue Classification Problem.](http://arxiv.org/abs/2401.04637) | 本研究通过应用生成式预训练转换器（GPT）模型，提出一种可靠的自动化方法来解决问题报告的优先级排序问题，即使在较小的训练数据集下仍能保持可靠性，减少了对大量训练数据的依赖性。 |
| [^7] | [DebugBench: Evaluating Debugging Capability of Large Language Models.](http://arxiv.org/abs/2401.04621) | 该论文介绍了一个名为DebugBench的LLM调试基准，用于评估大型语言模型的调试能力。研究发现闭源模型与人类相比具有较低的调试性能，而开源模型未能达到合格率。 |
| [^8] | [Agent Alignment in Evolving Social Norms.](http://arxiv.org/abs/2401.04620) | 本论文提出了一个名为EvolutionaryAgent的进化框架，将Agent对齐转化为适者生存的演化和选择过程，在不断演化的社会规范中，与当前社会规范更好适应的Agent将具有更高的生存和传播概率。 |
| [^9] | [Language Detection for Transliterated Content.](http://arxiv.org/abs/2401.04619) | 该论文介绍了一种通过使用BERT进行语言分类和Google Translate API进行音译转换的方法，以解决音译文本语言检测的挑战。这项研究为在数字交流的多样化语言环境中识别和转换音译文本开辟了新的创新途径。 |
| [^10] | [An Assessment on Comprehending Mental Health through Large Language Models.](http://arxiv.org/abs/2401.04592) | 这项研究评估了大型语言模型在理解心理健康方面的潜力，结果显示基于Transformer的模型在表达人类心理健康状况方面的理解能力超过大型语言模型。 |
| [^11] | [Evaluating Language Model Agency through Negotiations.](http://arxiv.org/abs/2401.04536) | 本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。 |
| [^12] | [MERA: A Comprehensive LLM Evaluation in Russian.](http://arxiv.org/abs/2401.04531) | 这项研究提出了MERA，一个多模态俄语基础模型评估指标。该指标包括21个评估任务，涵盖了11个技能领域中生成模型的评估。研究还提出了一种在零样本和少样本固定指令设置下评估FM和LM的方法。 |
| [^13] | [LUNA: A Framework for Language Understanding and Naturalness Assessment.](http://arxiv.org/abs/2401.04522) | LUNA是一个用于评估生成文本的框架，提供了一个统一接口和20个评估度量标准，可以根据参考依赖性和文本表示类型进行分类。 |
| [^14] | [The Critique of Critique.](http://arxiv.org/abs/2401.04518) | 本文创新性地提出了元批评(MetaCritique)的框架，通过精确度和召回率评估批评的质量，并以F1分数作为整体评分。为了获得可靠的评估结果，引入了原子信息单元(AIUs)来描述批评。元批评提供了自然语言的理由来支持评价结果。 |
| [^15] | [Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models.](http://arxiv.org/abs/2401.04515) | 本文研究了使用大型语言模型进行零样本上位词预测的方法。实验结果表明，语言模型提示的有效性与经典模式之间存在较强的相关性，并且可以通过使用较小的模型进行初步的提示选择。此外，通过使用自动识别的共同下位词将额外信息加入提示，可以改善上位词预测的效果。作者还开发了一种迭代方法来预测更高层次的概念，并在BLESS数据集上取得了显著的质量提升（MAP = 0.8）。 |
| [^16] | [Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search.](http://arxiv.org/abs/2401.04514) | 本论文提出一种扩展的生成增强检索（GAR）框架，通过对代码进行重写来解决代码搜索中存在的风格不匹配问题，实验结果表明该方法显著提高了检索准确性。 |
| [^17] | [TechGPT-2.0: A large language model project to solve the task of knowledge graph construction.](http://arxiv.org/abs/2401.04507) | TechGPT-2.0是一个解决知识图谱构建任务的大型语言模型项目，具有强大的文本处理能力和多个领域的应用能力。 |
| [^18] | [Continuously Learning New Words in Automatic Speech Recognition.](http://arxiv.org/abs/2401.04482) | 该论文提出了一种自我监督的持续学习方法，用于解决自动语音识别中识别新词的问题。通过对讲座录音进行推理和收集包含新词的话语，然后在自适应数据集上进行持续学习，可以在新词出现频率较高时提高性能，同时保持整体性能。 |
| [^19] | [Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset.](http://arxiv.org/abs/2401.04481) | 本文提出了一种基于大型语言模型的方法，通过对其进行引导，自动生成虚假信息的辨别数据集，以解决传统方法标注数据所需的大量人工努力的问题。 |
| [^20] | [TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction.](http://arxiv.org/abs/2401.04478) | TwinBooster结合了大语言模型、Barlow Twins和梯度提升，通过整合生物检测方法和分子指纹，实现了对未见过的生物检测方法和分子属性的精确预测，该方法在数据稀缺的情况下展现出了优秀的性能。 |
| [^21] | [TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models.](http://arxiv.org/abs/2401.04471) | TransportationGames是一个评估(M)LLMs在交通领域表现的基准测试，综合考虑了现实场景中的应用，通过测试(M)LLMs的记忆、推理、解释和评估交通问题的能力。 |
| [^22] | [Estimating Text Similarity based on Semantic Concept Embeddings.](http://arxiv.org/abs/2401.04422) | 该论文提出了基于MultiNet语义网络的语义概念嵌入方法，结合传统词嵌入可以提高预测目标群组的准确性。 |
| [^23] | [Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding.](http://arxiv.org/abs/2401.04398) | 这篇论文提出了Chain-of-Table框架，通过在推理链中使用表格数据作为中间思维的代理，利用大型语言模型在表格理解任务中进行推理，实现了动态演化的表格推理链。 |
| [^24] | [Probabilistic emotion and sentiment modelling of patient-reported experiences.](http://arxiv.org/abs/2401.04367) | 本研究提出了一种用于患者报告体验的情感建模方法，使用元数据网络主题建模分析患者报告的体验，开发了一种概率情感推荐系统，通过预测情感和情绪来增强传统的患者护理方法。 |
| [^25] | [Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning.](http://arxiv.org/abs/2401.04361) | 本文提出了一种基于实体对比学习框架来提高知识驱动对话（KGD）系统的稳健性，通过创建正负样本以应对实际噪音，如错别字和不完整的知识图谱。 |
| [^26] | [LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training.](http://arxiv.org/abs/2401.04348) | LAMPAT是第一个无监督的多语言改写模型，通过使用对抗训练，它能够在没有平行语料库的语言环境下生成改写。 |
| [^27] | [Private Fine-tuning of Large Language Models with Zeroth-order Optimization.](http://arxiv.org/abs/2401.04343) | 引入了DP-ZO，一种通过私有化零阶优化来保护大型语言模型训练数据隐私的方法。 |
| [^28] | [Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs.](http://arxiv.org/abs/2401.04319) | 本文探索了一种通过类比推理增强的LLMs来实现对营销人员需求的结构化理解的新方式，使非专业营销人员能够仅凭需求的自然语言形式选择目标用户。 |
| [^29] | [Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging.](http://arxiv.org/abs/2401.04317) | 这项研究首次将WiFi室内成像作为多模态图像生成任务来考虑，通过WiFi-GEN网络将测量的WiFi功率转换为高分辨率室内图像。与物理模型反演方法相比，该网络实现了更高的形状重建精度，并显著降低了Frechet Inception距离分数。此外，作者还发布了一个大规模数据集，用于验证模型的有效性。 |
| [^30] | [MARG: Multi-Agent Review Generation for Scientific Papers.](http://arxiv.org/abs/2401.04259) | MARG是一种使用多智能体模型进行内部讨论的方法，通过分配不同的子任务提高了GPT-4生成具体和有用反馈的能力。 |
| [^31] | [High-precision Voice Search Query Correction via Retrievable Speech-text Embedings.](http://arxiv.org/abs/2401.04235) | 本文提出了一种通过使用可检索的语音文本嵌入进行高精度语音搜索查询纠错的方法，消除了语音识别系统中的假设-音频不匹配问题，并提高了纠错的准确性。 |
| [^32] | [Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?.](http://arxiv.org/abs/2401.04218) | 该研究评估了大型语言模型在判断地理位置方向上的能力，并发现这些模型可能存在分层空间偏差。其中，GPT-4表现最佳，准确率为55.3％。 |
| [^33] | [FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild.](http://arxiv.org/abs/2401.04210) | 提出了FunnyNet-W，一个多模态模型用于预测视频中的有趣瞬间。该模型利用视觉、音频和文本数据以及交叉和自注意力机制，并且采用无监督方法获得训练标签。 |
| [^34] | [Large language models in bioinformatics: applications and perspectives.](http://arxiv.org/abs/2401.04155) | 本综述介绍了在生物信息学中使用的大型语言模型，如BERT和GPT，并重点探讨了它们在基因组学、转录组学、蛋白质组学、药物发现和单细胞分析等方面的应用。大型语言模型在解决生物信息学问题方面具有巨大潜力和前景。 |
| [^35] | [Cross-Speaker Encoding Network for Multi-Talker Speech Recognition.](http://arxiv.org/abs/2401.04152) | 本文提出了一种叫做Cross-Speaker Encoding（CSE）的网络，用于解决多说话人语音识别中的局限性，通过聚合跨说话人表示。通过与SOT结合，该模型在两个说话人的数据集上实验证明比SIMO基准模型的词错误率（WER）分别降低了8%和10%。 |
| [^36] | [Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning.](http://arxiv.org/abs/2401.04151) | 本论文提出了一种名为LoRA链（COLA）的迭代优化框架，通过残差学习过程将LoRA模块与预训练的语言模型参数合并，并重新初始化新的LoRA模块的优化过程，从而实现LoRA和全参数微调之间的平衡。 |
| [^37] | [Generation Z's Ability to Discriminate Between AI-generated and Human-Authored Text on Discord.](http://arxiv.org/abs/2401.04120) | 本研究通过对Generation Z的个体进行调查，评估了他们在Discord上区分AI生成和人类撰写文本的能力，发现他们无法有效区分这两种来源的文本。 |
| [^38] | [Working with Trouble and Failures in Conversation between Humans and Robots (WTF 2023) & Is CUI Design Ready Yet?.](http://arxiv.org/abs/2401.04108) | 本论文总结了两个研讨会的会议录，旨在讨论人机对话中的沟通问题和失败，以及非机器人语音界面的相关失败。目标是彻底调查沟通失败，制定分类法，并展开解决方案的初步讨论。 |
| [^39] | [The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance.](http://arxiv.org/abs/2401.03729) | 本研究通过一系列提示变化探究改变提示的构建方式对大规模语言模型决策的影响，发现即使微小的改变，比如在提示末尾加一个空格，也可能导致模型的答案变化。同时，请求以XML格式返回和常用的越狱方式也可能对模型标记的数据产生灾难性影响。 |
| [^40] | [ROIC-DM: Robust Text Inference and Classification via Diffusion Model.](http://arxiv.org/abs/2401.03514) | ROIC-DM是一个通过扩散模型实现的强大文本推理和分类模型，相较于传统语言模型具有更强的鲁棒性；同时通过将语言模型作为咨询组件纳入，可以达到与语言模型相当甚至更优秀的性能。 |
| [^41] | [Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format.](http://arxiv.org/abs/2401.03512) | 本文提出了一种无需分词的大型语言模型（LLMs）来生成中国古典诗词，并解决了格式不准确性的问题。验证了现有基于分词的模型在字符和分词之间的关系方面的知识有限，并展示了如何通过定制模型解决这一问题。 |
| [^42] | [Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM.](http://arxiv.org/abs/2401.02994) | 本研究介绍了一种名为“混合”的方法，通过组合多个适度规模的聊天AI模型，可以达到或超越比它们更大的模型的性能表现。 |
| [^43] | [Multilingual Instruction Tuning With Just a Pinch of Multilinguality.](http://arxiv.org/abs/2401.01854) | 本研究研究了多语言指令调优中的多语言性对跨语言指令遵循的影响。研究发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，只有40个多语言示例能够显著提高多语言指令遵循。总体来说，多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。 |
| [^44] | [A Comprehensive Study of Knowledge Editing for Large Language Models.](http://arxiv.org/abs/2401.01286) | 本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。 |
| [^45] | [CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation.](http://arxiv.org/abs/2401.01275) | CharacterEval是一个用于角色扮演对话代理评估的中文基准测试集，包含1,785个多轮对话和23,020个示例，涵盖77个角色。实验结果表明CharacterEval在评估RPCA方面是有效的。 |
| [^46] | [Jatmo: Prompt Injection Defense by Task-Specific Finetuning.](http://arxiv.org/abs/2312.17673) | Jatmo是一种生成对提示注入攻击具有抗性的任务特定模型的方法，通过利用教师模型生成任务特定的数据集并对基础模型进行微调。 |
| [^47] | [The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation.](http://arxiv.org/abs/2312.09085) | 本研究研究了LLMs对误导信息的易受攻击性，特别是在说服性对话中。通过实验证明，LLMs在事实知识上的正确信念很容易被各种说服策略所操纵。 |
| [^48] | [Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection.](http://arxiv.org/abs/2312.07476) | 本研究从可比较的演示的角度探索了上下文学习（ICL）机制，并发现演示偏见存在于大型语言模型（LLMs）中，而通过可比较的演示可以显著减少这种偏见，并在ICL中展现出良好的性能。 |
| [^49] | [Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers.](http://arxiv.org/abs/2312.04333) | 本文通过多项选择任务对LLaMA进行了深入分析，揭示了LLaMA在推理和计算等高阶任务中的内在理解能力。研究发现，增大模型尺寸几乎不能自动增加额外的知识或计算能力，但可以提高推理能力和减少幻觉，尤其在数学问题解决方面。此外，LLaMA的较低层次缺乏实质性的算术和事实知识，但顶层层次展现出较强的语言理解能力。 |
| [^50] | [Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment.](http://arxiv.org/abs/2312.01592) | 本论文提出了一种名为GroundedBERT的方法，通过视觉锚定信息增强BERT表示，解决了由于视觉锚定数据集和语言语料库的差异导致的上下文含义不匹配的问题。 |
| [^51] | [mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model.](http://arxiv.org/abs/2311.18248) | mPLUG-PaperOwl是一个多模态大型语言模型，其创新之处在于增强了图表分析能力，为科学学术论文写作提供了更多功能的辅助工具。 |
| [^52] | [LLMs cannot find reasoning errors, but can correct them!.](http://arxiv.org/abs/2311.08516) | 本文研究了LLMs在自我纠正过程中的错误发现和输出纠正两个核心组成部分。研究发现LLMs通常难以发现逻辑错误，但通过使用回溯方法可以在提供错误位置信息时获得大幅改进。 |
| [^53] | [Automatic Logical Forms improve fidelity in Table-to-Text generation.](http://arxiv.org/abs/2310.17279) | 本文提出了一种自动逻辑形式（LF）来提高表格到文本生成的准确性，首次展示了用自动LF改进系统可以提高准确性30个百分点，还指出了实现高准确性仍面临的挑战。 |
| [^54] | [Causal-structure Driven Augmentations for Text OOD Generalization.](http://arxiv.org/abs/2310.12803) | 本文提出了一种基于因果结构的反事实数据增强方法，用于改善文本分类器在应用中的泛化效果，特别适用于存在虚假相关性的标签与属性预测问题。 |
| [^55] | [Adapting text-based dialogue state tracker for spoken dialogues.](http://arxiv.org/abs/2308.15053) | 这篇论文描述了对构建适应口语对话系统的文本对话状态跟踪器进行的工程工作，利用自动语音识别错误校正和文本对话系统实现了插槽和值的估计。 |
| [^56] | [The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations.](http://arxiv.org/abs/2308.02053) | 通过职位推荐分析了大型语言模型（LLMs）的人口统计偏见，发现这些模型对于墨西哥工人一直建议低薪工作，并向女性更倾向于推荐秘书职位。这项研究强调了理解LLMs偏见的重要性。 |
| [^57] | [Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models.](http://arxiv.org/abs/2305.05973) | 提出使用差分隐私大语言模型合成查询的隐私保护推荐系统，可以安全有效地训练深度检索模型并提高检索质量。 |
| [^58] | [HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition.](http://arxiv.org/abs/2304.06910) | 该论文提出了一种分层交叉注意模型（HCAM）用于多模态情感识别，使用递归和共同注意神经网络模型进行音频和文本表示，将这两种模态信息以共同注意方式结合，取得了比现有方法更好的情感识别效果。 |
| [^59] | [Medical Intervention Duration Estimation Using Language-enhanced Transformer Encoder with Medical Prompts.](http://arxiv.org/abs/2303.17408) | 使用语言增强Transformer编码器，并结合医学提示，将结构化、非结构化的临床数据投影到一个语言潜空间中，以实现更精确的医学干预持续时间估计。 |
| [^60] | [Entailment Semantics Can Be Extracted from an Ideal Language Model.](http://arxiv.org/abs/2209.12407) | 该论文证明，假设训练句子由遵循交际基本原则的代理生成，那么可以从目标分布完美学习的理想语言模型中提取出蕴含判断。这个结果揭示了从未标注语言数据中理解语义和从语言模型中提取语义的潜在方法。 |
| [^61] | [E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation.](http://arxiv.org/abs/2205.14912) | E2S2提出了一种增强编码的序列到序列预训练策略，通过在编码器端引入更有效的自我监督信息，改进了语言模型的下游性能。 |

# 详细

[^1]: 模型编辑可能会损害大型语言模型的通用能力

    Model Editing Can Hurt General Abilities of Large Language Models. (arXiv:2401.04700v1 [cs.CL])

    [http://arxiv.org/abs/2401.04700](http://arxiv.org/abs/2401.04700)

    这项论文指出，模型编辑可能会改善模型的事实性，但会以降低模型的通用能力为代价。在这项研究中，作者通过评估四种编辑方法在两个大型语言模型上的表现，发现这些编辑方法往往忽视了对模型通用能力可能产生的负面影响。

    

    大型语言模型（LLM）的最新进展为我们获取其参数中存储的知识提供了新的范式。一个关键的挑战是LLM输出中存在错觉，这是由于错误或过时知识引起的。由于使用更新后的信息重新训练LLM需要大量资源，因此人们对模型编辑产生了越来越多的兴趣。然而，许多模型编辑方法在各种场景中很有效，但往往过于强调编辑性能的功效、泛化性和局部性，常常忽视了对LLM的通用能力可能产生的副作用。本文提出了改善模型的事实性可能会以相当大的通用能力下降为代价的担忧，这不符合LLM可持续发展的要求。我们通过评估四种常用的编辑方法在两个LLM上进行了系统分析副作用，并涵盖了八个代表性任务类别。

    Recent advances in large language models (LLMs) have opened up new paradigms for accessing the knowledge stored in their parameters. One critical challenge that has emerged is the presence of hallucinations in LLM outputs due to false or outdated knowledge. Since retraining LLMs with updated information is resource-intensive, there has been a growing interest in model editing. However, many model editing methods, while effective in various scenarios, tend to overemphasize aspects such as efficacy, generalization, and locality in editing performance, often overlooking potential side effects on the general abilities of LLMs. In this paper, we raise concerns that the improvement of model factuality may come at the cost of a significant degradation of these general abilities, which is not conducive to the sustainable development of LLMs. Systematically, we analyze side effects by evaluating four popular editing methods on two LLMs across eight representative task categories. Extensive empi
    
[^2]: 缩小知识评估差距：多层次答案的开放领域问答

    Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers. (arXiv:2401.04695v1 [cs.CL])

    [http://arxiv.org/abs/2401.04695](http://arxiv.org/abs/2401.04695)

    本研究提出了一种名为GRANOLA QA的评估设置，在开放领域问答中使用多粒度答案来评估预测的答案的准确性和信息量。作者提出了一种简单的方法来丰富现有数据集，并创建了一个多粒度版本的数据集。实验结果表明...

    

    事实性问题通常可以以不同层次的粒度正确回答。例如，“1961年8月4日”和“1961年”都是对“巴拉克·奥巴马是在什么时候出生的？”这个问题的正确答案。然而，标准的问答评估协议并没有明确考虑这一点，而是将预测的答案与单一粒度层次的答案进行比较。在这项工作中，我们提出了GRANOLA QA，一种新颖的评估设置，其中预测的答案根据准确性和信息量与一组多粒度答案进行评估。我们提出了一种简单的方法来丰富现有数据集的多粒度答案，并创建了GRANOLA-EQ，一个多粒度版本的EntityQuestions数据集。我们在GRANOLA-EQ上评估了一系列解码方法，包括一种新的算法，称为Decoding with Response Aggregation (DRAG)，该算法旨在将响应的粒度与模型的不确定性对齐。我们的实验显示...

    Factual questions typically can be answered correctly at different levels of granularity. For example, both ``August 4, 1961'' and ``1961'' are correct answers to the question ``When was Barack Obama born?''. Standard question answering (QA) evaluation protocols, however, do not explicitly take this into account and compare a predicted answer against answers of a single granularity level. In this work, we propose GRANOLA QA, a novel evaluation setting where a predicted answer is evaluated in terms of accuracy and informativeness against a set of multi-granularity answers. We present a simple methodology for enriching existing datasets with multi-granularity answers, and create GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm, called Decoding with Response Aggregation (DRAG), that is geared towards aligning the response granularity with the model's uncertainty. Our experiments show th
    
[^3]: RoSA: 通过鲁棒适应实现准确的参数高效微调

    RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])

    [http://arxiv.org/abs/2401.04679](http://arxiv.org/abs/2401.04679)

    RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。

    

    我们研究了在大语言模型 (LLMs) 的背景下，能够在有限的计算和内存预算下提供良好准确性的参数高效微调 (PEFT) 方法。我们提出了一种新的PEFT方法，称为RoSA，受鲁棒主成分分析 (PCA) 的启发，它在一组固定的预训练权重上共同训练$\textit{低秩}$和$\textit{高度稀疏}$的组件，以高效近似完全微调（FFT）解决方案的性能。我们展示了RoSA在一系列具有挑战性的生成任务上的性能，例如小学数学和SQL查询生成，这些任务需要进行微调以获得良好性能，我们证明了在相同的参数预算下，RoSA优于LoRA和纯粹的稀疏微调。我们通过稀疏GPU内核为RoSA提供系统支持，以补充训练算法，从而实现内存和计算效率的训练。我们的代码将在https://github.com/IST-DASLab上提供。

    We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
    
[^4]: Lightning Attention-2:在大型语言模型中处理无限序列长度的"免费午餐"

    Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. (arXiv:2401.04658v1 [cs.CL])

    [http://arxiv.org/abs/2401.04658](http://arxiv.org/abs/2401.04658)

    本文介绍了Lightning Attention-2，这是第一个能够实现线性注意力理论计算优势的线性注意力实现。通过利用平铺的思想，分别处理了线性注意力计算中的内部块和外部块组件。具体来说，采用传统的注意力计算机制处理内部块，并使用新的累积求和方法处理外部块。

    

    线性注意力是一种高效的注意力机制，最近被认为是传统softmax注意力的一种有前景的替代方法。线性注意力理论上能够在线性的计算复杂度下处理无限长度的序列，而不会牺牲速度，即在固定的内存消耗下，能够以恒定的训练速度处理不同长度的序列。然而，由于累积求和（cumsum）的问题，当前的线性注意力算法无法在因果设置下展现其理论优势。本文提出了Lightning Attention-2，这是第一个能够实现线性注意力理论计算优势的线性注意力实现。为了实现这一点，我们利用了平铺（tiling）的思想，分别处理了线性注意力计算中的内部块和外部块组件。具体来说，我们利用传统的注意力计算机制来处理内部块，然后使用一种新的累积求和的方法来处理外部块。

    Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks an
    
[^5]: DepressionEmo: 用于抑郁情绪的多标签分类的新型数据集

    DepressionEmo: A novel dataset for multilabel classification of depression emotions. (arXiv:2401.04655v1 [cs.CL])

    [http://arxiv.org/abs/2401.04655](http://arxiv.org/abs/2401.04655)

    本文介绍了一个名为DepressionEmo的新数据集，旨在通过分析Reddit用户帖子来检测与抑郁相关的8种情绪。研究在该数据集上进行了情绪之间的相关性、分布和语言分析，并提供了几种文本分类方法。

    

    情绪对于人类社交互动至关重要,不同的情境背景会引发多样化的回应。尤其是负面情绪状态的普遍存在与心理健康的负面结果相关，因此有必要全面分析它们的发生和对个体的影响。在本文中，我们介绍了一个名为DepressionEmo的新型数据集，旨在通过6037个长文本Reddit用户帖子来检测与抑郁相关的8种情绪。该数据集通过预训练模型的零样本分类输入进行多数表决，并通过注释者和ChatGPT验证其质量，显示了注释者之间可接受的一致性水平。对于DepressionEmo，对情绪之间的相关性、随时间的分布以及语言分析进行了研究。此外，我们提供了几种文本分类方法，分为两组：机器学习方法（如SVM、XGBoost和Light GBM）；一

    Emotions are integral to human social interactions, with diverse responses elicited by various situational contexts. Particularly, the prevalence of negative emotional states has been correlated with negative outcomes for mental health, necessitating a comprehensive analysis of their occurrence and impact on individuals. In this paper, we introduce a novel dataset named DepressionEmo designed to detect 8 emotions associated with depression by 6037 examples of long Reddit user posts. This dataset was created through a majority vote over inputs by zero-shot classifications from pre-trained models and validating the quality by annotators and ChatGPT, exhibiting an acceptable level of interrater reliability between annotators. The correlation between emotions, their distribution over time, and linguistic analysis are conducted on DepressionEmo. Besides, we provide several text classification methods classified into two groups: machine learning methods such as SVM, XGBoost, and Light GBM; a
    
[^6]: 将大型语言模型API应用于问题分类问题

    Applying Large Language Models API to Issue Classification Problem. (arXiv:2401.04637v1 [cs.SE])

    [http://arxiv.org/abs/2401.04637](http://arxiv.org/abs/2401.04637)

    本研究通过应用生成式预训练转换器（GPT）模型，提出一种可靠的自动化方法来解决问题报告的优先级排序问题，即使在较小的训练数据集下仍能保持可靠性，减少了对大量训练数据的依赖性。

    

    在软件工程中，问题报告的有效排序对于优化资源分配和及时解决关键问题至关重要。然而，手动对问题报告进行分类以进行排序是费力且缺乏可伸缩性的。相反，许多开源软件项目使用自动化流程解决此问题，尽管需要大量的数据集进行充分的训练。这项研究旨在设计一种自动化方法，在使用较小的数据集进行训练时仍能确保问题排序的可靠性。我们提出的方法利用生成式预训练转换器（GPT）的能力，认识到它们在处理这个任务时的高效性。通过利用这样的模型的能力，我们的目标是开发一个准确优先级问题报告的可靠系统，降低对大量训练数据的需求，同时保持可靠性。在我们的研究中，我们已经开发了一个可靠的基于GPT的方法来准确标记问题报告。

    Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly. However, the manual classification of issue reports for prioritization is laborious and lacks scalability. Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training. This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets. Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability. In our research, we have developed a reliable GPT-based approach to accurately labe
    
[^7]: DebugBench: 评估大型语言模型的调试能力

    DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v1 [cs.SE])

    [http://arxiv.org/abs/2401.04621](http://arxiv.org/abs/2401.04621)

    该论文介绍了一个名为DebugBench的LLM调试基准，用于评估大型语言模型的调试能力。研究发现闭源模型与人类相比具有较低的调试性能，而开源模型未能达到合格率。

    

    大型语言模型（LLMs）展示出了出色的编码能力。然而，作为编程能力的另一个关键组成部分，LLMs的调试能力仍然相对未被探索。之前对LLMs的调试能力评估受到数据泄露风险、数据集规模和测试漏洞种类的限制。为了克服这些不足，我们引入了一个名为“DebugBench”的LLM调试基准，包含4253个实例。它涵盖了C ++，Java和Python中四个主要的错误类别和18个次要类型。为了构建DebugBench，我们从LeetCode社区收集了代码片段，使用GPT-4向源数据中注入错误，并进行严格的质量检查。我们在零样例情况下评估了两个商业模型和三个开源模型。我们发现，（1）与人类相比，闭源模型如GPT-4表现出较低的调试性能，而开源模型如Code Llama无法达到任何合格率；（2）t

    Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) t
    
[^8]: 在不断演化的社会规范中的Agent对齐

    Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])

    [http://arxiv.org/abs/2401.04620](http://arxiv.org/abs/2401.04620)

    本论文提出了一个名为EvolutionaryAgent的进化框架，将Agent对齐转化为适者生存的演化和选择过程，在不断演化的社会规范中，与当前社会规范更好适应的Agent将具有更高的生存和传播概率。

    

    基于大型语言模型（LLM）的Agent越来越多地渗透到人类生产和生活的各个领域，凸显了将其与人类价值观对齐的重要性。目前AI系统的对齐主要集中在通过人为干预对LLM进行被动对齐。然而，Agent具有接受环境反馈和自我进化等特性，使得LLM对齐方法变得不足够。为此，我们提出了一个名为EvolutionaryAgent的Agent进化和对齐的进化框架，将Agent对齐转化为适者生存的演化和选择过程。在社会规范不断演化的环境中，与当前社会规范更好适应的Agent将具有更高的生存和传播概率，而对齐不足的Agent则逐渐减少。通过多个角度对与社会规范相对齐的Agent进行的实验结果进行评估。

    Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstr
    
[^9]: 求识别音译内容的语言检测方法

    Language Detection for Transliterated Content. (arXiv:2401.04619v1 [cs.CL])

    [http://arxiv.org/abs/2401.04619](http://arxiv.org/abs/2401.04619)

    该论文介绍了一种通过使用BERT进行语言分类和Google Translate API进行音译转换的方法，以解决音译文本语言检测的挑战。这项研究为在数字交流的多样化语言环境中识别和转换音译文本开辟了新的创新途径。

    

    在当代数字化时代，互联网作为一个无与伦比的催化剂，打破了地理和语言的障碍，尤其在短信方面表现得尤为明显。这种发展促进了全球交流，超越了物理距离，促进了动态的文化交流。一个显著的趋势是广泛使用音译，即使用英文字母来传达母语的信息，这给语言技术准确识别源语言提出了独特的挑战。本文通过一个包含印地语和俄语音译成英文的手机短信数据集，利用BERT进行语言分类并使用Google Translate API进行音译转换，来解决这一挑战。该研究开创了识别和转换音译文本的创新方法，在数字交流的多样化语言环境中面临着各种挑战。强调全面数据集对大规模语言训练的关键作用。

    In the contemporary digital era, the Internet functions as an unparalleled catalyst, dismantling geographical and linguistic barriers particularly evident in texting. This evolution facilitates global communication, transcending physical distances and fostering dynamic cultural exchange. A notable trend is the widespread use of transliteration, where the English alphabet is employed to convey messages in native languages, posing a unique challenge for language technology in accurately detecting the source language. This paper addresses this challenge through a dataset of phone text messages in Hindi and Russian transliterated into English utilizing BERT for language classification and Google Translate API for transliteration conversion. The research pioneers innovative approaches to identify and convert transliterated text, navigating challenges in the diverse linguistic landscape of digital communication. Emphasizing the pivotal role of comprehensive datasets for training Large Langua
    
[^10]: 通过大型语言模型了解心理健康的评估

    An Assessment on Comprehending Mental Health through Large Language Models. (arXiv:2401.04592v1 [cs.CL])

    [http://arxiv.org/abs/2401.04592](http://arxiv.org/abs/2401.04592)

    这项研究评估了大型语言模型在理解心理健康方面的潜力，结果显示基于Transformer的模型在表达人类心理健康状况方面的理解能力超过大型语言模型。

    

    心理健康挑战对个人和社区造成了巨大的全球负担。最近的数据表明，超过20%的成年人在他们的一生中可能会遇到至少一种心理障碍。一方面，大型语言模型的进展为各种应用提供了便利，然而在心理健康领域，对大型语言模型的潜力的理解和提升仍存在重要的研究空白。另一方面，在各种应用中，一个重要的问题是大型语言模型对自然语言中人类心理健康状况表达的理解能力。本研究在解决这一空白的过程中进行了大型语言模型的初步评估。因此，我们将Llama-2和ChatGPT与传统的机器学习模型和深度学习模型进行了性能比较。我们在DAIC-WOZ数据集上的结果显示，基于Transformer的模型（如BERT或XLNet）的性能优于大型语言模型。

    Mental health challenges pose considerable global burdens on individuals and communities. Recent data indicates that more than 20% of adults may encounter at least one mental disorder in their lifetime. On the one hand, the advancements in large language models have facilitated diverse applications, yet a significant research gap persists in understanding and enhancing the potential of large language models within the domain of mental health. On the other hand, across various applications, an outstanding question involves the capacity of large language models to comprehend expressions of human mental health conditions in natural language. This study presents an initial evaluation of large language models in addressing this gap. Due to this, we compare the performance of Llama-2 and ChatGPT with classical Machine as well as Deep learning models. Our results on the DAIC-WOZ dataset show that transformer-based models, like BERT or XLNet, outperform the large language models.
    
[^11]: 通过谈判评估语言模型的代理能力

    Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])

    [http://arxiv.org/abs/2401.04536](http://arxiv.org/abs/2401.04536)

    本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。

    

    公司、组织和政府越来越多地利用语言模型（LM）展示类似代理行为的出色能力。随着LM被采用来执行越来越具有自主性的任务，迫切需要可靠且可扩展的评估基准。当前主要是静态的LM基准无法很好地评估此类动态应用。因此，我们提议通过谈判游戏的视角来共同评估LM的性能和对齐。我们认为这个共同任务更好地反映了真实世界的部署条件，并提供了关于LM决策过程的见解。至关重要的是，谈判游戏使我们能够研究多轮次和跨模型交互，调整复杂性，并避免评估中的意外数据泄漏。我们报告了来自几个主要供应商的六个公开可访问的LM在各种谈判游戏上的结果，评估了自我对弈和交叉对弈性能。值得注意的发现包括：（i）开源模式

    Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
    
[^12]: MERA: 俄语LLM综合评估的研究

    MERA: A Comprehensive LLM Evaluation in Russian. (arXiv:2401.04531v1 [cs.CL])

    [http://arxiv.org/abs/2401.04531](http://arxiv.org/abs/2401.04531)

    这项研究提出了MERA，一个多模态俄语基础模型评估指标。该指标包括21个评估任务，涵盖了11个技能领域中生成模型的评估。研究还提出了一种在零样本和少样本固定指令设置下评估FM和LM的方法。

    

    在过去几年中，人工智能研究中最显著的进展之一是基础模型（FM）的发展，其中语言模型（LM）的崛起引人注目。随着模型的规模增大，LM在可衡量的方面展示了提升，并且发展出了新的定性特征。然而，尽管研究人员的关注和LM应用的快速增长，LM的能力、限制和相关风险仍需更好地理解。为了解决这些问题，我们介绍了一种开放的俄语多模态架构评估（MERA）指导基准，用于评估以俄语为导向的基础模型。该基准涵盖了11个技能领域中生成模型的21个评估任务，并被设计为黑盒测试，以确保排除数据泄漏。论文介绍了一种在零样本和少样本固定指令设置下评估FM和LM的方法，并可扩展到其他模态。

    Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zeroand few-shot fixed instruction settings that can be extended to other modalities. We propose an 
    
[^13]: LUNA: 一个用于语言理解和自然度评估的框架

    LUNA: A Framework for Language Understanding and Naturalness Assessment. (arXiv:2401.04522v1 [cs.CL])

    [http://arxiv.org/abs/2401.04522](http://arxiv.org/abs/2401.04522)

    LUNA是一个用于评估生成文本的框架，提供了一个统一接口和20个评估度量标准，可以根据参考依赖性和文本表示类型进行分类。

    

    自然语言生成（NLG）模型的评估日益受到关注，迫使开发评估生成文本各个方面的度量标准。LUNA通过引入一个统一的接口来解决这一挑战，包含20个NLG评估度量标准。这些度量标准基于它们的参考依赖性和文本表示类型进行分类，从基于字符串的n-gram重叠到利用静态嵌入和预训练语言模型。LUNA的简单设计允许通过几行代码轻松扩展新的度量标准。LUNA提供了一个用户友好的工具，用于评估生成的文本。

    The evaluation of Natural Language Generation (NLG) models has gained increased attention, urging the development of metrics that evaluate various aspects of generated text. LUNA addresses this challenge by introducing a unified interface for 20 NLG evaluation metrics. These metrics are categorized based on their reference-dependence and the type of text representation they employ, from string-based n-gram overlap to the utilization of static embeddings and pre-trained language models.  The straightforward design of LUNA allows for easy extension with novel metrics, requiring just a few lines of code. LUNA offers a user-friendly tool for evaluating generated texts.
    
[^14]: 《批评的批评》

    The Critique of Critique. (arXiv:2401.04518v1 [cs.CL])

    [http://arxiv.org/abs/2401.04518](http://arxiv.org/abs/2401.04518)

    本文创新性地提出了元批评(MetaCritique)的框架，通过精确度和召回率评估批评的质量，并以F1分数作为整体评分。为了获得可靠的评估结果，引入了原子信息单元(AIUs)来描述批评。元批评提供了自然语言的理由来支持评价结果。

    

    批评作为一种用于评估模型生成内容质量的自然语言描述，在训练、评估和改进大型语言模型(LLMs)中被证明起着重要作用。然而，在评估批评本身质量方面缺乏原则性的理解。本文首创了批评的批评，称为元批评，这是一个评估批评的框架，从精确度和召回率两个方面来评估批评。我们计算精确度和召回率的调和平均值作为整体评分，称为F1分数。为了获得可靠的评估结果，我们提出了原子信息单元(AIUs)，以更精细的方式描述批评。元批评考虑每个AIU，并聚合每个AIU的判断得到整体评分。此外，鉴于评估过程涉及复杂的推理，我们的元批评提供了自然语言的理由来支持评价结果。

    Critique, as a natural language description for assessing the quality of model-generated content, has been proven to play an essential role in the training, evaluation, and refinement of Large Language Models (LLMs). However, there is a lack of principled understanding in evaluating the quality of the critique itself. In this paper, we pioneer the critique of critique, termed MetaCritique, which is a framework to evaluate the critique from two aspects, i.e., factuality as precision score and comprehensiveness as recall score. We calculate the harmonic mean of precision and recall as the overall rating called F1 score. To obtain a reliable evaluation outcome, we propose Atomic Information Units (AIUs), which describe the critique in a more fine-grained manner. MetaCritique takes each AIU into account and aggregates each AIU's judgment for the overall score. Moreover, given the evaluation process involves intricate reasoning, our MetaCritique provides a natural language rationale to supp
    
[^15]: 使用大型语言模型探索基于提示的零样本上位词预测方法

    Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models. (arXiv:2401.04515v1 [cs.CL])

    [http://arxiv.org/abs/2401.04515](http://arxiv.org/abs/2401.04515)

    本文研究了使用大型语言模型进行零样本上位词预测的方法。实验结果表明，语言模型提示的有效性与经典模式之间存在较强的相关性，并且可以通过使用较小的模型进行初步的提示选择。此外，通过使用自动识别的共同下位词将额外信息加入提示，可以改善上位词预测的效果。作者还开发了一种迭代方法来预测更高层次的概念，并在BLESS数据集上取得了显著的质量提升（MAP = 0.8）。

    

    本文研究了使用大型语言模型(LLMs)进行零样本上位词预测的方法。研究采用了基于文本概率计算的方法，并将其应用于各种生成的提示中。实验证明语言模型提示的有效性与经典模式之间存在着较强的相关性，这表明可以通过较小的模型进行初步的提示选择，然后再转向更大的模型。我们还探讨了用于预测共同下位词和通过自动识别的共同下位词将额外信息加入提示以改善上位词预测的提示。我们还开发了一种迭代方法来预测更高层次的概念，进一步提高了在BLESS数据集上的质量（MAP = 0.8）。

    This article investigates a zero-shot approach to hypernymy prediction using large language models (LLMs). The study employs a method based on text probability calculation, applying it to various generated prompts. The experiments demonstrate a strong correlation between the effectiveness of language model prompts and classic patterns, indicating that preliminary prompt selection can be carried out using smaller models before moving to larger ones. We also explore prompts for predicting co-hyponyms and improving hypernymy predictions by augmenting prompts with additional information through automatically identified co-hyponyms. An iterative approach is developed for predicting higher-level concepts, which further improves the quality on the BLESS dataset (MAP = 0.8).
    
[^16]: 重写代码：一种用于大型语言模型增强代码搜索的简单方法

    Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search. (arXiv:2401.04514v1 [cs.SE])

    [http://arxiv.org/abs/2401.04514](http://arxiv.org/abs/2401.04514)

    本论文提出一种扩展的生成增强检索（GAR）框架，通过对代码进行重写来解决代码搜索中存在的风格不匹配问题，实验结果表明该方法显著提高了检索准确性。

    

    在代码搜索中，生成增强检索（GAR）框架是一种有前景的策略，通过生成示例代码片段来增强查询，以解决代码片段和自然语言查询之间的主要模态不匹配问题，尤其是在大型语言模型（LLM）展示了代码生成能力的情况下。然而，我们的初步调查发现，LLM增强框架所提供的改进有一定的限制。这种限制可能是因为生成的代码，尽管在功能上准确，但在代码库中与基准代码之间经常显示出明显的风格偏差。在本文中，我们扩展了基础GAR框架，并提出了一种简单而有效的方法，通过对代码库中的代码进行重写（ReCo）来进行风格规范化。实验结果表明，ReCo显著提高了检索准确性。

    In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy ac
    
[^17]: TechGPT-2.0:一个解决知识图谱构建任务的大型语言模型项目

    TechGPT-2.0: A large language model project to solve the task of knowledge graph construction. (arXiv:2401.04507v1 [cs.CL])

    [http://arxiv.org/abs/2401.04507](http://arxiv.org/abs/2401.04507)

    TechGPT-2.0是一个解决知识图谱构建任务的大型语言模型项目，具有强大的文本处理能力和多个领域的应用能力。

    

    大型语言模型在各种自然语言处理任务中都展现出了强大的性能。本报告介绍了TechGPT-2.0，这是一个项目，旨在增强大型语言模型在知识图谱构建任务中的能力，包括命名实体识别（NER）和关系三元组提取（RTE）。此外，它还作为一个面向中国开源模型社区的LLM可访问研究。我们提供了两个7B大型语言模型的权重和一个专门用于处理长文本的QLoRA权重。值得注意的是，TechGPT-2.0是在华为的Ascend服务器上训练的。继承了TechGPT-1.0的所有功能，它展现出了强大的文本处理能力，特别是在医学和法律领域。此外，我们还为模型引入了新的功能，使其能够处理各个领域的文本，如地理区域、交通、组织机构、文学作品、生物学和自然科学等。

    Large language models have exhibited robust performance across diverse natural language processing tasks. This report introduces TechGPT-2.0, a project designed to enhance the capabilities of large language models specifically in knowledge graph construction tasks, including named entity recognition (NER) and relationship triple extraction (RTE) tasks in NLP applications. Additionally, it serves as a LLM accessible for research within the Chinese open-source model community. We offer two 7B large language model weights and a QLoRA weight specialized for processing lengthy texts.Notably, TechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all functionalities from TechGPT-1.0, it exhibits robust text processing capabilities, particularly in the domains of medicine and law. Furthermore, we introduce new capabilities to the model, enabling it to process texts in various domains such as geographical areas, transportation, organizations, literary works, biology, natural sciences, as
    
[^18]: 在自动语音识别中持续学习新词

    Continuously Learning New Words in Automatic Speech Recognition. (arXiv:2401.04482v1 [cs.CL])

    [http://arxiv.org/abs/2401.04482](http://arxiv.org/abs/2401.04482)

    该论文提出了一种自我监督的持续学习方法，用于解决自动语音识别中识别新词的问题。通过对讲座录音进行推理和收集包含新词的话语，然后在自适应数据集上进行持续学习，可以在新词出现频率较高时提高性能，同时保持整体性能。

    

    尽管最近取得了进展，但自动语音识别（ASR）系统仍然远未完美。典型的错误包括缩写词、命名实体和领域特定的专用词，这些词几乎没有或没有数据可用来训练。为了解决识别这些词的问题，我们提出了一种自我监督的持续学习方法。给定带有对应幻灯片的讲座录音，我们通过使用先前工作中的记忆增强型ASR模型来将模型偏向于从幻灯片中解码新词。然后，我们对讲座进行推理，将包含检测到的新词的话语收集到自适应数据集中。接着，对这个集合进行持续学习，通过调整添加到模型的每个权重矩阵的低秩矩阵权重。整个过程对多个讲座进行迭代。我们展示了通过这种方法，我们在新词出现频率较高时获得了性能的提升（超过80%的召回率），同时保持了模型的整体性能。

    Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
    
[^19]: 以火攻火：对抗启发式生成一个辨别虚假信息的数据集

    Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset. (arXiv:2401.04481v1 [cs.CL])

    [http://arxiv.org/abs/2401.04481](http://arxiv.org/abs/2401.04481)

    本文提出了一种基于大型语言模型的方法，通过对其进行引导，自动生成虚假信息的辨别数据集，以解决传统方法标注数据所需的大量人工努力的问题。

    

    大型语言模型（LLM），如GPT、Bard和Llama等，在语言生成能力方面取得了显著的成功。然而，这种成功可能引发对其被滥用的担忧，比如通过生成假新闻和传播错误信息引发大规模激动和仇恨。传统的虚假信息数据集开发方法在标注数据时需要大量人工努力，无法很好地扩展。本文提出了一种基于LLM的方法来创建用于识别虚假信息的银标准数据集。具体而言，给定一个可信新闻文章，我们的方法通过引导LLM自动生成原始文章的摘要版本。我们的方法中的引导作为一种控制机制，用于在生成的摘要中产生特定类型的事实错误，例如错误的数量、错误的归属地等。为了研究这个数据集的实用性，我们进行了实证实验。

    The recent success in language generation capabilities of large language models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns about their possible misuse in inducing mass agitation and communal hatred via generating fake news and spreading misinformation. Traditional means of developing a misinformation ground-truth dataset does not scale well because of the extensive manual effort required to annotate the data. In this paper, we propose an LLM-based approach of creating silver-standard ground-truth datasets for identifying misinformation. Specifically speaking, given a trusted news article, our proposed approach involves prompting LLMs to automatically generate a summarised version of the original article. The prompts in our proposed approach act as a controlling mechanism to generate specific types of factual incorrectness in the generated summaries, e.g., incorrect quantities, false attributions etc. To investigate the usefulness of this dataset, we conduct
    
[^20]: TwinBooster: 结合Barlow Twins和梯度提升的大语言模型协同增强分子属性预测

    TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction. (arXiv:2401.04478v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.04478](http://arxiv.org/abs/2401.04478)

    TwinBooster结合了大语言模型、Barlow Twins和梯度提升，通过整合生物检测方法和分子指纹，实现了对未见过的生物检测方法和分子属性的精确预测，该方法在数据稀缺的情况下展现出了优秀的性能。

    

    药物发现和开发的成功依赖于对分子活性和属性的精确预测。虽然基于计算的分子属性预测显示出了显著的潜力，但其使用迄今为止仅限于大量数据可用的检测方法。在本研究中，我们使用经过微调的大语言模型，结合了基于文本信息的生物检测方法，并使用了一种新颖的自监督学习方法的Siamese神经网络Barlow Twins。该架构利用检测方法信息和分子指纹提取真实的分子信息。TwinBooster通过提供最先进的零样本学习任务，实现了对未见过的生物检测方法和分子的属性预测。值得注意的是，我们的人工智能流水线在FS-Mol基准测试上表现出优秀的性能。这一突破展示了深度学习在通常数据稀缺的关键属性预测任务中的应用。

    The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce.
    
[^21]: TransportationGames: 用于评价(多模态)大型语言模型在交通领域中的知识的基准测试

    TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models. (arXiv:2401.04471v1 [cs.CL])

    [http://arxiv.org/abs/2401.04471](http://arxiv.org/abs/2401.04471)

    TransportationGames是一个评估(M)LLMs在交通领域表现的基准测试，综合考虑了现实场景中的应用，通过测试(M)LLMs的记忆、推理、解释和评估交通问题的能力。

    

    大型语言模型 (LLMs) 和多模态大型语言模型 (MLLMs) 已经展现出了出色的通用能力，甚至在法律、经济、交通和医学等许多专业领域表现出了适应性。目前，已提出了许多领域特定的基准来验证 (M)LLMs 在特定领域的性能。在各种不同领域中，交通运输在现代社会中扮演着至关重要的角色，因为它影响着经济、环境和数十亿人民生活的质量。然而，目前尚不清楚 (M)LLMs 多少交通知识，并且它们是否可以可靠地执行与交通相关的任务。为了弥补这一空白，我们提出了 TransportationGames，这是一个经过精心设计和全面评估的基准测试，用于评估(M)LLMs 在交通领域中的表现。通过综合考虑现实场景中的应用，并参照布鲁姆分类法的前三个层次，我们测试了各种(M)LLMs 在记忆交通知识、推理交通概念、解释和评估交通问题等方面的表现。

    Large language models (LLMs) and multimodal large language models (MLLMs) have shown excellent general capabilities, even exhibiting adaptability in many professional domains such as law, economics, transportation, and medicine. Currently, many domain-specific benchmarks have been proposed to verify the performance of (M)LLMs in specific fields. Among various domains, transportation plays a crucial role in modern society as it impacts the economy, the environment, and the quality of life for billions of people. However, it is unclear how much traffic knowledge (M)LLMs possess and whether they can reliably perform transportation-related tasks. To address this gap, we propose TransportationGames, a carefully designed and thorough evaluation benchmark for assessing (M)LLMs in the transportation domain. By comprehensively considering the applications in real-world scenarios and referring to the first three levels in Bloom's Taxonomy, we test the performance of various (M)LLMs in memorizing
    
[^22]: 基于语义概念嵌入的文本相似度估计

    Estimating Text Similarity based on Semantic Concept Embeddings. (arXiv:2401.04422v1 [cs.CL])

    [http://arxiv.org/abs/2401.04422](http://arxiv.org/abs/2401.04422)

    该论文提出了基于MultiNet语义网络的语义概念嵌入方法，结合传统词嵌入可以提高预测目标群组的准确性。

    

    由于其易用性和高准确性，Word2Vec (W2V) 词嵌入在语义表示中取得了巨大成功，包括单词、句子和整个文档的语义表示以及语义相似度估计。然而，它们的缺点是直接从表面表示中提取，不能充分代表人类思维过程，对于高度歧义的词也表现不佳。因此，我们提出了基于MultiNet语义网络(SN)形式化的语义概念嵌入(CE)，以解决这两个问题。在市场目标群体分布任务的评估中，结果显示将传统词嵌入和语义CE结合可以增加预测目标群组的准确性。

    Due to their ease of use and high accuracy, Word2Vec (W2V) word embeddings enjoy great success in the semantic representation of words, sentences, and whole documents as well as for semantic similarity estimation. However, they have the shortcoming that they are directly extracted from a surface representation, which does not adequately represent human thought processes and also performs poorly for highly ambiguous words. Therefore, we propose Semantic Concept Embeddings (CE) based on the MultiNet Semantic Network (SN) formalism, which addresses both shortcomings. The evaluation on a marketing target group distribution task showed that the accuracy of predicted target groups can be increased by combining traditional word embeddings with semantic CEs.
    
[^23]: Chain-of-Table: 在推理链中演化表格用于表格理解

    Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding. (arXiv:2401.04398v1 [cs.CL])

    [http://arxiv.org/abs/2401.04398](http://arxiv.org/abs/2401.04398)

    这篇论文提出了Chain-of-Table框架，通过在推理链中使用表格数据作为中间思维的代理，利用大型语言模型在表格理解任务中进行推理，实现了动态演化的表格推理链。

    

    基于大型语言模型（LLMs）的表格推理是解决许多表格理解任务（如基于表格的问答和事实验证）的一种有前景的方法。与通常的推理相比，基于表格的推理需要从自由形式问题和半结构化表格数据中提取潜在语义。Chain-of-Thought及其类似方法将推理链以文本上下文的形式纳入其中，但如何有效利用表格数据在推理链中仍然是一个开放的问题。我们提出了Chain-of-Table框架，其中表格数据以作为中间思维的代理明确地用于推理链中。具体地，我们使用上下文学习指导LLMs来迭代生成操作并更新表格，以代表一个表格推理链。因此，LLMs可以根据之前操作的结果动态地规划下一个操作。这种表格的持续演化形成了一个链。

    Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, 
    
[^24]: 患者报告体验的概率情感和情绪建模

    Probabilistic emotion and sentiment modelling of patient-reported experiences. (arXiv:2401.04367v1 [cs.CL])

    [http://arxiv.org/abs/2401.04367](http://arxiv.org/abs/2401.04367)

    本研究提出了一种用于患者报告体验的情感建模方法，使用元数据网络主题建模分析患者报告的体验，开发了一种概率情感推荐系统，通过预测情感和情绪来增强传统的患者护理方法。

    

    本研究引入了一种新的方法来对患者在在线患者体验叙述中的情绪进行建模。我们采用元数据网络主题建模来分析来自Care Opinion的患者报告的体验，揭示了与患者-医护人员互动和临床结果相关的关键情绪主题。我们开发了一种概率、上下文特定的情感推荐系统，能够使用朴素贝叶斯分类器来预测多标签情绪和二进制情绪，其中上下文相关的主题作为预测变量。通过使用信息检索指标nDCG和Q-measure评估我们预测的情绪在这个模型下相对于基准模型的卓越表现，而我们预测的情绪在F1得分方面达到0.921，显著优于标准的情感词典。这种方法提供了一种透明、经济高效的方式来理解患者的反馈，增强了传统的数据收集方法并为个体化的患者护理提供了信息。

    This study introduces a novel methodology for modelling patient emotions from online patient experience narratives. We employed metadata network topic modelling to analyse patient-reported experiences from Care Opinion, revealing key emotional themes linked to patient-caregiver interactions and clinical outcomes. We develop a probabilistic, context-specific emotion recommender system capable of predicting both multilabel emotions and binary sentiments using a naive Bayes classifier using contextually meaningful topics as predictors. The superior performance of our predicted emotions under this model compared to baseline models was assessed using the information retrieval metrics nDCG and Q-measure, and our predicted sentiments achieved an F1 score of 0.921, significantly outperforming standard sentiment lexicons. This method offers a transparent, cost-effective way to understand patient feedback, enhancing traditional collection methods and informing individualised patient care. Our fi
    
[^25]: 通过对比学习提高知识驱动对话的稳健性

    Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning. (arXiv:2401.04361v1 [cs.CL])

    [http://arxiv.org/abs/2401.04361](http://arxiv.org/abs/2401.04361)

    本文提出了一种基于实体对比学习框架来提高知识驱动对话（KGD）系统的稳健性，通过创建正负样本以应对实际噪音，如错别字和不完整的知识图谱。

    

    知识驱动的对话（KGD）通过给定的对话环境和外部知识（如知识图谱）来生成信息丰富的回应。最近，大型语言模型（LLMs）和预训练技术的出现为知识驱动的对话带来了巨大的成功。然而，在实际应用中构建KGD系统时，难免会遇到各种实际的噪音。例如，对话环境可能涉及错别字和缩写等扰动。此外，知识图谱通常存在不完整性，也可能包含错误和过时的事实。这些实际的噪音给KGD系统的稳健性带来了挑战，并阻碍了它们在真实世界中的应用。本文提出了一种基于实体对比学习框架来提高KGD稳健性的方法。具体而言，我们利用KGD样本中的实体信息创建其正样本和负样本。

    Knowledge-grounded dialogue (KGD) learns to generate an informative response based on a given dialogue context and external knowledge (\emph{e.g.}, knowledge graphs; KGs). Recently, the emergence of large language models (LLMs) and pre-training techniques has brought great success to knowledge-grounded dialogue. However, when building KGD systems in real applications, there are various real-world noises that are inevitable to face. For example, the dialogue context might involve perturbations such as misspellings and abbreviations. In addition, KGs typically suffer from incompletion and also might contain erroneous and outdated facts. Such real-world noises pose a challenge to the robustness of KGD systems and hinder their applications in the real world. In this paper, we propose an entity-based contrastive learning framework for improving the robustness of KGD. Specifically, we make use of the entity information in a KGD sample to create both its positive and negative samples which in
    
[^26]: LAMPAT：使用对抗训练进行低秩多语言改写的方法

    LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training. (arXiv:2401.04348v1 [cs.CL])

    [http://arxiv.org/abs/2401.04348](http://arxiv.org/abs/2401.04348)

    LAMPAT是第一个无监督的多语言改写模型，通过使用对抗训练，它能够在没有平行语料库的语言环境下生成改写。

    

    改写是指使用不同的词语或句子结构来传达相同含义的文本。它可以用作自动数据增强工具，特别是在处理数据不足的低资源语言时。为了在多语言环境下生成改写，先前的研究利用了机器翻译领域的知识，通过在相同语言中进行零样本机器翻译来形成改写。尽管在人工评估中表现良好，但这些方法仍然需要平行翻译数据集，因此无法应用于没有平行语料库的语言。为了解决这个问题，我们提出了第一个无监督的多语言改写模型，LAMPAT（低秩多语言改写的适应性低秩多语言改写模型），其中单语数据集已经足够。

    Paraphrases are texts that convey the same meaning while using different words or sentence structures. It can be used as an automatic data augmentation tool for many Natural Language Processing tasks, especially when dealing with low-resource languages, where data shortage is a significant problem. To generate a paraphrase in multilingual settings, previous studies have leveraged the knowledge from the machine translation field, i.e., forming a paraphrase through zero-shot machine translation in the same language. Despite good performance on human evaluation, those methods still require parallel translation datasets, thus making them inapplicable to languages that do not have parallel corpora. To mitigate that problem, we proposed the first unsupervised multilingual paraphrasing model, LAMPAT ($\textbf{L}$ow-rank $\textbf{A}$daptation for $\textbf{M}$ultilingual $\textbf{P}$araphrasing using $\textbf{A}$dversarial $\textbf{T}$raining), by which monolingual dataset is sufficient enough 
    
[^27]: 私有零阶优化的大型语言模型的私有微调

    Private Fine-tuning of Large Language Models with Zeroth-order Optimization. (arXiv:2401.04343v1 [cs.LG])

    [http://arxiv.org/abs/2401.04343](http://arxiv.org/abs/2401.04343)

    引入了DP-ZO，一种通过私有化零阶优化来保护大型语言模型训练数据隐私的方法。

    

    在私有数据集上对大型预训练模型进行微调可能会存在违反隐私的风险。差分隐私是一种通过强制算法稳定性来减轻隐私风险的框架。DP-SGD可以以保护隐私的方式训练具有私有数据的模型，但会带来性能损失和重大工程挑战。我们引入了DP-ZO，一种通过私有化零阶优化来保护训练数据隐私的大型语言模型微调方法。我们的方法设计的一个关键见解是，我们使用的零阶算法SPSA中的梯度方向始终是随机的，而仅依赖于私有数据的信息是步长，即一个标量。因此，我们只需要对标量步长进行隐私处理，这是存储效率高的方法。DP-ZO可以使用拉普拉斯噪声或高斯噪声来实现，在不同任务之间提供了隐私和效用之间的强大权衡。

    Fine-tuning large pretrained models on private datasets may run the risk of violating privacy. Differential privacy is a framework for mitigating privacy risks by enforcing algorithmic stability. DP-SGD enables training models with private data in a privacy-preserving manner, but raises new obstacles in the form of performance loss and significant engineering challenges. We introduce DP-ZO, a new method for fine-tuning large language models that preserves the privacy of training data by privatizing zeroth-order optimization. A key insight into the design of our method is that the direction of the gradient in SPSA, the zeroth-order algorithm we use, is always random and the only information that depends on private data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO, which can be instantiated with either Laplace or Gaussian noise, provides a strong privacy-utility trade-off across different tasks, and model si
    
[^28]: 更好地了解您的需求：通过类比推理增强的LLMs实现对营销人员需求的结构化理解

    Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])

    [http://arxiv.org/abs/2401.04319](http://arxiv.org/abs/2401.04319)

    本文探索了一种通过类比推理增强的LLMs来实现对营销人员需求的结构化理解的新方式，使非专业营销人员能够仅凭需求的自然语言形式选择目标用户。

    

    本文探讨了一种新的用户定位方式，即非专业营销人员可以仅凭需求的自然语言形式选择目标用户。解决这个问题的关键在于如何将自然语言转化为实际的结构化逻辑语言，即对营销人员需求的结构化理解。考虑到大型语言模型（LLMs）出色的自然语言处理能力，我们尝试利用LLMs来解决这个问题。过去的研究表明，通过链式思考（CoT）提示可以有效增强LLMs的推理能力。但是现有方法仍然存在一些限制：（1）先前的方法要么使用简单的“让我们一步一步地思考”提示，要么在演示中提供固定的示例而不考虑提示和问题之间的兼容性，在一些复杂的推理任务（如结构化语言转换）中使LLMs无效。(2) 先前的方法通常在闭源模型或过度实现的模型中实现。

    In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or exces
    
[^29]: 视觉再构想：WiFi室内成像中的人工智能突破

    Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging. (arXiv:2401.04317v1 [cs.CV])

    [http://arxiv.org/abs/2401.04317](http://arxiv.org/abs/2401.04317)

    这项研究首次将WiFi室内成像作为多模态图像生成任务来考虑，通过WiFi-GEN网络将测量的WiFi功率转换为高分辨率室内图像。与物理模型反演方法相比，该网络实现了更高的形状重建精度，并显著降低了Frechet Inception距离分数。此外，作者还发布了一个大规模数据集，用于验证模型的有效性。

    

    室内成像对于机器人和物联网是一项关键任务。作为无处不在的信号，WiFi是进行被动成像和将最新信息同步到所有连接设备的有希望的选择。这是第一个将WiFi室内成像作为多模态图像生成任务来考虑的研究工作，将测量的WiFi功率转换为高分辨率室内图像。我们提出的WiFi-GEN网络实现了形状重建精度，比基于物理模型的反演方法高出275%。此外，Frechet Inception距离分数显著降低了82%。为了验证模型在这个任务中的有效性，我们发布了第一个大规模数据集，包含80,000对WiFi信号和成像目标。我们的模型吸收了基于模型的方法的挑战，包括非线性、不适定性和不确定性，并将其转化为我们生成性人工智能网络的大量参数。

    Indoor imaging is a critical task for robotics and internet-of-things. WiFi as an omnipresent signal is a promising candidate for carrying out passive imaging and synchronizing the up-to-date information to all connected devices. This is the first research work to consider WiFi indoor imaging as a multi-modal image generation task that converts the measured WiFi power into a high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape reconstruction accuracy that is 275% of that achieved by physical model-based inversion methods. Additionally, the Frechet Inception Distance score has been significantly reduced by 82%. To examine the effectiveness of models for this task, the first large-scale dataset is released containing 80,000 pairs of WiFi signal and imaging target. Our model absorbs challenges for the model-based methods including the non-linearity, ill-posedness and non-certainty into massive parameters of our generative AI network. The network is also designed to
    
[^30]: MARG：多智能体为科学论文生成评论的方法

    MARG: Multi-Agent Review Generation for Scientific Papers. (arXiv:2401.04259v1 [cs.CL])

    [http://arxiv.org/abs/2401.04259](http://arxiv.org/abs/2401.04259)

    MARG是一种使用多智能体模型进行内部讨论的方法，通过分配不同的子任务提高了GPT-4生成具体和有用反馈的能力。

    

    我们研究了语言模型（LLMs）生成科学论文反馈的能力，并开发了MARG，一种使用多个LLM实例进行内部讨论的反馈生成方法。通过将论文文本分配给智能体，MARG可以消化超出基本LLM输入长度限制的完整论文文本，并通过专门化智能体和结合适合不同评论类型的子任务（实验，清晰度，影响力），提高反馈的有用性和特定性。在一项用户研究中，使用GPT-4的基准方法的评论被评为超过一半时间产生的是普通或非常普通的评论，最佳基准方法中每篇论文只被评为1.7条整体上好的评论。我们的系统大大提高了GPT-4生成具体和有用反馈的能力，将普通评论的比例从60%降低到29%，每篇论文产生3.7条好的评论（提高了2.2倍）。

    We study the ability of LLMs to generate feedback for scientific papers and develop MARG, a feedback generation approach using multiple LLM instances that engage in internal discussion. By distributing paper text across agents, MARG can consume the full text of papers beyond the input length limitations of the base LLM, and by specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact) it improves the helpfulness and specificity of feedback. In a user study, baseline methods using GPT-4 were rated as producing generic or very generic comments more than half the time, and only 1.7 comments per paper were rated as good overall in the best baseline. Our system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from 60% to 29% and generating 3.7 good comments per paper (a 2.2x improvement).
    
[^31]: 高精度语音搜索查询纠错的可检索语音文本嵌入

    High-precision Voice Search Query Correction via Retrievable Speech-text Embedings. (arXiv:2401.04235v1 [cs.CL])

    [http://arxiv.org/abs/2401.04235](http://arxiv.org/abs/2401.04235)

    本文提出了一种通过使用可检索的语音文本嵌入进行高精度语音搜索查询纠错的方法，消除了语音识别系统中的假设-音频不匹配问题，并提高了纠错的准确性。

    

    自动语音识别(ASR)系统可能因为各种原因而导致召回率较低，例如嘈杂的音频、缺乏足够的训练数据等。先前的研究表明，可以通过使用ASR假设文本的嵌入进行最近邻搜索，从大型数据库中检索与上下文相关的可能候选项来改进召回率并纠正候选纠正。然而，如果文本假设与转录的真实文本在语音上差异太大，基于ASR假设的检索可能会导致精度较低。在本文中，我们通过直接使用由话语音频产生的嵌入来查询纠正数据库，消除了假设和音频不匹配的问题；话语音频和候选纠正的嵌入是由多模式语音文本嵌入网络训练产生的，目的是将话语音频的嵌入和其相应的文本转录的嵌入放置在靠近的位置。

    Automatic speech recognition (ASR) systems can suffer from poor recall for various reasons, such as noisy audio, lack of sufficient training data, etc.  Previous work has shown that recall can be improved by retrieving rewrite candidates from a large database of likely, contextually-relevant alternatives to the hypothesis text using nearest-neighbors search over embeddings of the ASR hypothesis text to correct and candidate corrections.  However, ASR-hypothesis-based retrieval can yield poor precision if the textual hypotheses are too phonetically dissimilar to the transcript truth. In this paper, we eliminate the hypothesis-audio mismatch problem by querying the correction database directly using embeddings derived from the utterance audio; the embeddings of the utterance audio and candidate corrections are produced by multimodal speech-text embedding networks trained to place the embedding of the audio of an utterance and the embedding of its corresponding textual transcript close to
    
[^32]: 评估大型语言模型中的判断空间关系失真：自然语言地理数据的黎明？

    Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?. (arXiv:2401.04218v1 [cs.CL])

    [http://arxiv.org/abs/2401.04218](http://arxiv.org/abs/2401.04218)

    该研究评估了大型语言模型在判断地理位置方向上的能力，并发现这些模型可能存在分层空间偏差。其中，GPT-4表现最佳，准确率为55.3％。

    

    我们提出了一个用于评估大型语言模型(LLMs)在判断地理位置之间的方向上的能力的基准，并将其应用于三个知名的LLMs：GPT-3.5，GPT-4和Llama-2。这个基准特别评估了LLMs是否表现出类似人类的分层空间偏差，即对于包含它们的更大群体的感知关系会影响对个别位置空间关系的判断。为了调查这个问题，我们制定了14个关于美国知名城市的问题。其中七个问题旨在挑战LLMs，这些问题可能受到了更大地理单位（如州或国家）方向的影响，而另外七个问题则针对不容易受到这种层次化分类的位置。在经过测试的模型中，GPT-4的准确率最高，为55.3％，其次是GPT-3.5的47.3％和Llama-2的44.7％。

    We present a benchmark for assessing the capability of Large Language Models (LLMs) to discern intercardinal directions between geographic locations and apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar to humans, where judgments about individual locations' spatial relationships are influenced by the perceived relationships of the larger groups that contain them. To investigate this, we formulated 14 questions focusing on well-known American cities. Seven questions were designed to challenge the LLMs with scenarios potentially influenced by the orientation of larger geographical units, such as states or countries, while the remaining seven targeted locations less susceptible to such hierarchical categorization. Among the tested models, GPT-4 exhibited superior performance with 55.3% accuracy, followed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed significantly redu
    
[^33]: FunnyNet-W:视频中野外有趣瞬间的多模态学习

    FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild. (arXiv:2401.04210v1 [cs.CV])

    [http://arxiv.org/abs/2401.04210](http://arxiv.org/abs/2401.04210)

    提出了FunnyNet-W，一个多模态模型用于预测视频中的有趣瞬间。该模型利用视觉、音频和文本数据以及交叉和自注意力机制，并且采用无监督方法获得训练标签。

    

    在观看喜剧时自动理解有趣的瞬间（即使让人发笑的瞬间）是具有挑战性的，因为它们涉及到各种特征，如肢体语言、对话和文化。在本文中，我们提出了FunnyNet-W，它是一个依靠视觉、音频和文本数据的交叉和自注意力模型，用于预测视频中的有趣瞬间。与大多数依赖于字幕形式的标注数据的方法不同，在这项工作中，我们利用自然与视频一起出现的多模态数据：（a）视频帧，因为它们包含了场景理解所必需的视觉信息，（b）音频，因为它包含与有趣瞬间相关的更高级别的线索，如语调、音高和停顿，以及（c）由语音转文本模型自动提取的文本，因为它可以在经过大型语言模型处理后提供丰富的信息。为了获得训练标签，我们提出了一种无监督的方法来发现和标记有趣的音频瞬间。我们在五个数据集上进行了实验。

    Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as body language, dialogues and culture. In this paper, we propose FunnyNet-W, a model that relies on cross- and self-attention for visual, audio and text data to predict funny moments in videos. Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a Large Language Model. To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments. We provide experiments on five datasets: 
    
[^34]: 生物信息学中的大型语言模型：应用与展望

    Large language models in bioinformatics: applications and perspectives. (arXiv:2401.04155v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.04155](http://arxiv.org/abs/2401.04155)

    本综述介绍了在生物信息学中使用的大型语言模型，如BERT和GPT，并重点探讨了它们在基因组学、转录组学、蛋白质组学、药物发现和单细胞分析等方面的应用。大型语言模型在解决生物信息学问题方面具有巨大潜力和前景。

    

    大型语言模型（LLMs）是一类基于深度学习的人工智能模型，在各种任务中表现出色，尤其在自然语言处理（NLP）中。大型语言模型通常由具有大量参数的人工神经网络组成，通过自监督或半监督学习，在大量无标签输入上进行训练。然而，它们在解决生物信息学问题方面的潜力甚至超过了在模拟人类语言方面的能力。在这篇综述中，我们将介绍在自然语言处理中使用的几个重要的大型语言模型，如BERT和GPT，并重点探讨大型语言模型在生物信息学中不同组学水平的应用，主要包括基因组学、转录组学、蛋白质组学、药物发现和单细胞分析方面的应用。最后，本综述总结了大型语言模型在解决生物信息学问题方面的潜力和前景。

    Large language models (LLMs) are a class of artificial intelligence models based on deep learning, which have great performance in various tasks, especially in natural language processing (NLP). Large language models typically consist of artificial neural networks with numerous parameters, trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. However, their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language. In this review, we will present a summary of the prominent large language models used in natural language processing, such as BERT and GPT, and focus on exploring the applications of large language models at different omics levels in bioinformatics, mainly including applications of large language models in genomics, transcriptomics, proteomics, drug discovery and single cell analysis. Finally, this review summarizes the potential and prospects of large language models in solving bioinfo
    
[^35]: 跨说话人编码网络用于多说话人语音识别

    Cross-Speaker Encoding Network for Multi-Talker Speech Recognition. (arXiv:2401.04152v1 [cs.SD])

    [http://arxiv.org/abs/2401.04152](http://arxiv.org/abs/2401.04152)

    本文提出了一种叫做Cross-Speaker Encoding（CSE）的网络，用于解决多说话人语音识别中的局限性，通过聚合跨说话人表示。通过与SOT结合，该模型在两个说话人的数据集上实验证明比SIMO基准模型的词错误率（WER）分别降低了8%和10%。

    

    端到端的多说话人语音识别已经引起了极大的兴趣，作为一种直接转录多个说话人重叠语音的有效方法。目前的方法通常采用1）带有分支编码器的单输入多输出（SIMO）模型，或者2）基于注意力机制的编码器-解码器架构和序列化输出训练（SOT）的单输入单输出（SISO）模型。在这项工作中，我们提出了一种叫做Cross-Speaker Encoding（CSE）的网络来解决SIMO模型的局限性，通过聚合跨说话人表示。此外，CSE模型与SOT相结合，既发挥了SIMO和SISO的优势，又缓解了它们的缺点。据我们所知，该工作代表了将SIMO和SISO集成到多说话人语音识别中的早期工作。在两个说话人的LibrispeechMix数据集上进行的实验表明，CES模型相比于SIMO基准模型将词错误率（WER）降低了8%。CSE-SOT模型将WER降低了10%

    End-to-end multi-talker speech recognition has garnered great interest as an effective approach to directly transcribe overlapped speech from multiple speakers. Current methods typically adopt either 1) single-input multiple-output (SIMO) models with a branched encoder, or 2) single-input single-output (SISO) models based on attention-based encoder-decoder architecture with serialized output training (SOT). In this work, we propose a Cross-Speaker Encoding (CSE) network to address the limitations of SIMO models by aggregating cross-speaker representations. Furthermore, the CSE model is integrated with SOT to leverage both the advantages of SIMO and SISO while mitigating their drawbacks. To the best of our knowledge, this work represents an early effort to integrate SIMO and SISO for multi-talker speech recognition. Experiments on the two-speaker LibrispeechMix dataset show that the CES model reduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model reduces WER by 10
    
[^36]: LoRA链：通过残差学习实现语言模型的高效微调

    Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning. (arXiv:2401.04151v1 [cs.LG])

    [http://arxiv.org/abs/2401.04151](http://arxiv.org/abs/2401.04151)

    本论文提出了一种名为LoRA链（COLA）的迭代优化框架，通过残差学习过程将LoRA模块与预训练的语言模型参数合并，并重新初始化新的LoRA模块的优化过程，从而实现LoRA和全参数微调之间的平衡。

    

    微调是将预训练的大型语言模型定制为特定任务的主要方法。随着模型规模和任务多样性的扩大，参数高效的微调方法变得至关重要。其中最常用的方法之一是低秩适应（LoRA）及其变体。LoRA将权重更新编码为两个低秩矩阵的乘积。尽管具有一定优势，但在某些任务的泛化错误方面，LoRA无法完全参数化微调。我们引入了一种称为LoRA链（COLA）的迭代优化框架，受Frank-Wolfe算法的启发，以弥合LoRA和全参数微调之间的差距，而不会增加额外的计算成本或内存开销。COLA采用残差学习过程，在预训练语言模型参数中合并学到的LoRA模块，并重新初始化新生的LoRA模块的优化过程。我们提供了理论上的收敛保证以及...

    Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks.  We introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as
    
[^37]: Generation Z在Discord上区分人工智能生成和人类撰写文本的能力

    Generation Z's Ability to Discriminate Between AI-generated and Human-Authored Text on Discord. (arXiv:2401.04120v1 [cs.HC])

    [http://arxiv.org/abs/2401.04120](http://arxiv.org/abs/2401.04120)

    本研究通过对Generation Z的个体进行调查，评估了他们在Discord上区分AI生成和人类撰写文本的能力，发现他们无法有效区分这两种来源的文本。

    

    生成人工智能（AI）聊天机器人的普及，如ChatGPT，对社交媒体产生了深远影响。随着AI生成内容的普及，人们对在线隐私和虚假信息的担忧也越来越多。在社交媒体平台中，Discord允许AI集成，使他们以"Z世代"为主的用户群体特别容易接触到AI生成的内容。我们对年龄为Z世代的个体（n = 335）进行了调查，以评估他们在Discord上区分AI生成和人类撰写文本的能力。调查采用了ChatGPT伪装成在Discord.com平台上收到的一条短信的单次提示。我们探讨了人口统计因素和参与者对Discord和人工智能技术的熟悉程度对能力的影响。我们发现Z世代个体无法区分AI和人类撰写的文本（p = 0.011），那些自我报告程度较低的个体能力更差。

    The growing popularity of generative artificial intelligence (AI) chatbots such as ChatGPT is having transformative effects on social media. As the prevalence of AI-generated content grows, concerns have been raised regarding privacy and misinformation online. Among social media platforms, Discord enables AI integrations -- making their primarily "Generation Z" userbase particularly exposed to AI-generated content. We surveyed Generation Z aged individuals (n = 335) to evaluate their proficiency in discriminating between AI-generated and human-authored text on Discord. The investigation employed one-shot prompting of ChatGPT, disguised as a text message received on the Discord.com platform. We explore the influence of demographic factors on ability, as well as participants' familiarity with Discord and artificial intelligence technologies. We find that Generation Z individuals are unable to discern between AI and human-authored text (p = 0.011), and that those with lower self-reported 
    
[^38]: 在人机对话中处理困难和失败（WTF 2023）与CUI设计是否准备好？（arXiv:2401.04108v1 [cs.HC]）

    Working with Trouble and Failures in Conversation between Humans and Robots (WTF 2023) & Is CUI Design Ready Yet?. (arXiv:2401.04108v1 [cs.HC])

    [http://arxiv.org/abs/2401.04108](http://arxiv.org/abs/2401.04108)

    本论文总结了两个研讨会的会议录，旨在讨论人机对话中的沟通问题和失败，以及非机器人语音界面的相关失败。目标是彻底调查沟通失败，制定分类法，并展开解决方案的初步讨论。

    

    该论文是ACM会议上两个分会议“在人机对话中处理困难和失败”（WTF 2023）和“CUI设计是否准备好？”的研讨会会议录。WTF 23旨在汇集来自人机交互、对话系统、人机交互和对话分析领域的研究人员。尽管有所进展，但机器人语音界面在多个方面仍然脆弱，人们对此类界面失败的经历并不罕见。然而，技术文献对它们的良好性能有着积极的偏向。该研讨会旨在提供一个讨论人机交互中的沟通问题和失败以及非机器人语音界面相关失败的平台。目标包括对沟通失败进行彻底调查，开始制定这些失败的分类法，并就解决方案展开初步讨论。

    Workshop proceedings of two co-located workshops "Working with Troubles and Failures in Conversation with Humans and Robots" (WTF 2023) and "Is CUI Design Ready Yet?", both of which were part of the ACM conference on conversational user interfaces 2023.  WTF 23 aimed at bringing together researchers from human-robot interaction, dialogue systems, human-computer interaction, and conversation analysis. Despite all progress, robotic speech interfaces continue to be brittle in a number of ways and the experience of failure of such interfaces is commonplace amongst roboticists. However, the technical literature is positively skewed toward their good performance. The workshop aims to provide a platform for discussing communicative troubles and failures in human-robot interactions and related failures in non-robotic speech interfaces. Aims include a scrupulous investigation into communicative failures, to begin working on a taxonomy of such failures, and enable a preliminary discussion on pos
    
[^39]: 改变提示的蝴蝶效应：微小的变化和越狱对大规模语言模型性能的影响

    The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance. (arXiv:2401.03729v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.03729](http://arxiv.org/abs/2401.03729)

    本研究通过一系列提示变化探究改变提示的构建方式对大规模语言模型决策的影响，发现即使微小的改变，比如在提示末尾加一个空格，也可能导致模型的答案变化。同时，请求以XML格式返回和常用的越狱方式也可能对模型标记的数据产生灾难性影响。

    

    大规模语言模型（LLMs）被广泛用于对多个领域和多个任务的数据进行标注。通过简单地向LLM提问或“提示”，实践者能够快速获得任意任务的响应。提示的构建方式是否变化会影响LLM的最终决策？我们通过对多种文本分类任务进行一系列提示变化来回答这个问题。我们发现，即使是最微小的扰动，比如在提示的末尾加一个空格，也可能导致LLM改变其答案。此外，我们发现在XML中请求响应和常用的越狱方式可能对由LLMs标记的数据产生灾难性影响。

    Large Language Models (LLMs) are regularly being used to label data across many domains and for myriad tasks. By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task. This prompting is done through a series of decisions by the practitioner, from simple wording of the prompt, to requesting the output in a certain data format, to jailbreaking in the case of prompts that address more sensitive topics. In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM? We answer this using a series of prompt variations across a variety of text classification tasks. We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.
    
[^40]: ROIC-DM: 通过扩散模型实现的强大文本推理和分类

    ROIC-DM: Robust Text Inference and Classification via Diffusion Model. (arXiv:2401.03514v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.03514](http://arxiv.org/abs/2401.03514)

    ROIC-DM是一个通过扩散模型实现的强大文本推理和分类模型，相较于传统语言模型具有更强的鲁棒性；同时通过将语言模型作为咨询组件纳入，可以达到与语言模型相当甚至更优秀的性能。

    

    尽管语言模型在文本推理和分类任务中取得了许多里程碑式的成果，但它们仍然容易受到对抗性攻击的影响，可能导致意想不到的结果。现有的工作通过为语言模型增加防御补丁来缓解这个问题。然而，这些防御策略往往依赖于不切实际的假设或者需要在模型性能方面做出巨大的牺牲。因此，使用这些防御机制来提高目标模型的弹性是一个严峻的挑战。本文介绍了一种创新的用于强化文本推理和分类的模型，基于扩散模型（ROIC-DM）。由于其训练过程涉及去噪阶段，ROIC-DM本质上比传统的语言模型具有更强的鲁棒性。此外，ROIC-DM还可以通过有效地将其作为咨询组件纳入，达到与语言模型相当甚至更优秀的性能。在多个强文本上进行的大量实验证明了ROIC-DM的有效性。

    While language models have made many milestones in text inference and classification tasks, they remain susceptible to adversarial attacks that can lead to unforeseen outcomes. Existing works alleviate this problem by equipping language models with defense patches. However, these defense strategies often rely on impractical assumptions or entail substantial sacrifices in model performance. Consequently, enhancing the resilience of the target model using such defense mechanisms is a formidable challenge. This paper introduces an innovative model for robust text inference and classification, built upon diffusion models (ROIC-DM). Benefiting from its training involving denoising stages, ROIC-DM inherently exhibits greater robustness compared to conventional language models. Moreover, ROIC-DM can attain comparable, and in some cases, superior performance to language models, by effectively incorporating them as advisory components. Extensive experiments conducted with several strong textual
    
[^41]: 无需分词的大型语言模型能够以更准确的格式生成中国古典诗词

    Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format. (arXiv:2401.03512v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.03512](http://arxiv.org/abs/2401.03512)

    本文提出了一种无需分词的大型语言模型（LLMs）来生成中国古典诗词，并解决了格式不准确性的问题。验证了现有基于分词的模型在字符和分词之间的关系方面的知识有限，并展示了如何通过定制模型解决这一问题。

    

    经过微调的大型语言模型（如ChatGPT和Qwen-chat）能够根据人类的指令生成中国古典诗词。虽然语言模型在内容方面表现良好，但通常在格式上存在问题，每行字符的数量有时过多或不足。由于大多数最新的语言模型是基于分词的，我们认为格式不准确是由于"分词规划"任务的难度，即语言模型需要准确知道每个分词中包含多少个字符，并基于这个知识进行长度控制规划。本文首先通过展示现有的基于分词的大型语言模型在分词和字符之间的关系方面知识有限来验证我们的假设。我们使用了拼写比赛探测程序，并发现Qwen-chat在近15%的中文拼写测试中失败。然后，我们展示了一个基于分词的模型可以轻松定制成无需分词的模型（对于中文来说），从而能够很大程度上解决格式准确性问题。

    Finetuned large language models (such as ChatGPT and Qwen-chat) can generate Chinese classical poetry following human's instructions. LLMs perform well in content, but are usually lacking in format, with occasionally excess or insufficient number of characters in each line. Since most SOTA LLMs are token-based, we assume that the format inaccuracy is due to the difficulty of the "token planning" task, which means that the LLM need to know exactly how much characters are contained in each token and do length-control planning based on that knowledge. In this paper, we first confirm our assumption by showing that existing token-based large language models has limited knowledge on token-character relationship. We use a spelling bee probing procedure, and find that Qwen-chat failed in nearly 15% Chinese spelling test. We then show that a token-based model can be easily tailored into a token-free model (in terms of Chinese), which can largely solve the format accuracy problem. Our tailoring 
    
[^42]: 基于混合方法的聊天AI模型：相对于万亿级参数模型的更廉价、更好的替代方案

    Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v1 [cs.CL])

    [http://arxiv.org/abs/2401.02994](http://arxiv.org/abs/2401.02994)

    本研究介绍了一种名为“混合”的方法，通过组合多个适度规模的聊天AI模型，可以达到或超越比它们更大的模型的性能表现。

    

    在会话型AI研究中，越来越多的模型采用了更多的参数，如ChatGPT等模型。虽然这些庞大的模型往往能生成更好的聊天回复，但它们需要大量的计算资源和内存。本研究探讨了一个重要问题：能否通过组合较小的模型来达到与单个大模型相当或更好的性能？我们提出了一种称为“混合”的方法，它是一种简单但有效的将多个聊天AI集成在一起的方法。我们的实证证据表明，当特定较小的模型协同混合时，它们可以潜在地超越或匹敌大型模型的性能。例如，仅集成三个适度规模的模型（6B/13B参数）就可以达到或甚至超越ChatGPT（175B+参数）等大型模型的性能指标。这个假设经过了严格的测试。

    In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? We introduce an approach termed "blending", a straightforward yet effective method of integrating multiple chat AIs. Our empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tes
    
[^43]: 多语言指令调优中的多语言性

    Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])

    [http://arxiv.org/abs/2401.01854](http://arxiv.org/abs/2401.01854)

    本研究研究了多语言指令调优中的多语言性对跨语言指令遵循的影响。研究发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，只有40个多语言示例能够显著提高多语言指令遵循。总体来说，多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。

    

    随着大型语言模型（LLMs）的全球采纳，它们在多语言指令遵循能力变得越来越重要。一种有前途的方法是跨语言转移，通过在另一种语言上微调，模型可以在某种语言上获得特定的功能。本文研究了多语言LLM在指令调优过程中的多语言性对跨语言指令遵循的影响。首先我们发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，我们发现在英语调优集合中，只有40个多语言示例能够显著提高多语言指令遵循，在调优过程中不论是已见语言还是未见语言。总的来说，我们观察到在多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。

    As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
    
[^44]: 大型语言模型的知识编辑全面研究

    A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])

    [http://arxiv.org/abs/2401.01286](http://arxiv.org/abs/2401.01286)

    本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。

    

    大型语言模型(LLM)在理解和生成与人类交流紧密相似的文本方面展现出了非凡的能力。然而，其主要限制在于训练过程中的显著计算需求，这是由于其广泛的参数化造成的。这一挑战在于世界的动态性，需要频繁更新LLM以修正过时的信息或集成新知识，从而确保其持续的相关性。许多应用需要在训练后进行持续的模型调整，以解决缺陷或不良行为。近年来，对于LLM的知识编辑技术的兴趣越来越高，在特定领域内有效地修改LLM的行为，同时保持整体性能在各种输入中的表现。本文首先定义了知识编辑的目标和挑战，然后综述了现有的知识编辑方法和技术，并讨论了其应用和未来发展的方向。

    Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
    
[^45]: CharacterEval: 一种用于角色扮演对话代理评估的中文基准

    CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation. (arXiv:2401.01275v1 [cs.CL])

    [http://arxiv.org/abs/2401.01275](http://arxiv.org/abs/2401.01275)

    CharacterEval是一个用于角色扮演对话代理评估的中文基准测试集，包含1,785个多轮对话和23,020个示例，涵盖77个角色。实验结果表明CharacterEval在评估RPCA方面是有效的。

    

    最近，大型语言模型（LLM）的出现彻底改变了生成代理的方式。其中，角色扮演对话代理（RPCA）由于其触发用户情感的能力而引起了广泛关注。然而，缺乏一个全面的基准测试集阻碍了该领域的进展。为了填补这一空白，我们推出了CharacterEval，这是一个用于全面评估RPCA的中文基准测试集，并配有一个定制的高质量数据集。该数据集包括1,785个多轮角色扮演对话，涵盖了23,020个示例，涉及了77个来源于中国小说和剧本的角色。它经过精心构建，首先通过GPT-4进行初始对话提取，然后进行严格的人工质量控制，并通过百度百科获取了深入的角色资料。CharacterEval采用多方面的评估方法，包括四个维度上的十三个有针对性的指标。在CharacterEval上进行的全面实验证明了它的有效性。

    Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate th
    
[^46]: Jatmo:通过任务特定的微调进行提示注入防御

    Jatmo: Prompt Injection Defense by Task-Specific Finetuning. (arXiv:2312.17673v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2312.17673](http://arxiv.org/abs/2312.17673)

    Jatmo是一种生成对提示注入攻击具有抗性的任务特定模型的方法，通过利用教师模型生成任务特定的数据集并对基础模型进行微调。

    

    由于其遵循指令的能力，大型语言模型（LLMs）引起了广泛的研究关注，使用户和开发人员能够利用LLMs执行各种任务。然而，LLMs容易受到提示注入攻击的影响：一种攻击方式，通过劫持模型的指令遵循能力，将对提示的响应更改为不需要或可能具有恶意的响应。在这项工作中，我们介绍了Jatmo，一种生成对提示注入攻击具有抗性的任务特定模型的方法。Jatmo利用了LLMs只能在经历过指令调整后才能遵循指令的事实。它利用一个经过指令调整的教师模型生成一个任务特定的数据集，然后用这个数据集对一个基础模型（即非指令调整的模型）进行微调。Jatmo只需要一个任务提示和一个任务输入的数据集：它使用教师模型生成输出。在没有现成数据集的情况下，Jatmo可以使用一个例子，或者在某些情况下可以使用

    Large Language Models (LLMs) are attracting significant research attention due to their instruction-following abilities, allowing users and developers to leverage LLMs for a variety of tasks. However, LLMs are vulnerable to prompt-injection attacks: a class of attacks that hijack the model's instruction-following abilities, changing responses to prompts to undesired, possibly malicious ones. In this work, we introduce Jatmo, a method for generating task-specific models resilient to prompt-injection attacks. Jatmo leverages the fact that LLMs can only follow instructions once they have undergone instruction tuning. It harnesses a teacher instruction-tuned model to generate a task-specific dataset, which is then used to fine-tune a base model (i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a dataset of inputs for the task: it uses the teacher model to generate outputs. For situations with no pre-existing datasets, Jatmo can use a single example, or in some cases
    
[^47]: 地球是扁平的，因为......：通过说服性对话研究LLMs对误导信息的信仰

    The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation. (arXiv:2312.09085v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.09085](http://arxiv.org/abs/2312.09085)

    本研究研究了LLMs对误导信息的易受攻击性，特别是在说服性对话中。通过实验证明，LLMs在事实知识上的正确信念很容易被各种说服策略所操纵。

    

    大型语言模型(LLMs)封装了大量知识，但仍然容易受到外部误导信息的攻击。现有研究主要在单轮对话中研究了这种易受攻击的行为。然而，在多轮对话中，特别是说服性对话中，信仰可以发生变化。因此，在本研究中，我们深入探讨了LLMs对说服性对话的易受攻击性，特别是对它们可以正确回答的事实问题。首先，我们整理了Farm（即事实到误导）数据集，其中包含与系统生成的说服性误导信息相匹配的事实问题。然后，我们开发了一个测试框架，以追踪LLMs在说服性对话中的信仰变化。通过大量实验，我们发现LLMs在事实知识上的正确信念很容易被各种说服策略所操纵。

    Large Language Models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies.
    
[^48]: 可比较的演示在上下文学习中至关重要：对演示选择的新视角

    Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.07476](http://arxiv.org/abs/2312.07476)

    本研究从可比较的演示的角度探索了上下文学习（ICL）机制，并发现演示偏见存在于大型语言模型（LLMs）中，而通过可比较的演示可以显著减少这种偏见，并在ICL中展现出良好的性能。

    

    在上下文学习（ICL）中，通过少量演示将大型语言模型（LLMs）适应于下游任务是一种重要的范式。尽管ICL取得了巨大成功，但演示数量的限制可能导致演示偏见，即由LLMs引起的输入-标签映射误解了任务的本质。受人类经验的启发，我们尝试通过演示间关系的视角来缓解这种偏见。具体而言，我们通过最小化编辑文本来构建可比较的演示（CDs），以翻转相应的标签，以突出任务的本质并通过演示间比较消除潜在的虚假相关性。通过一系列的CDs实验，我们发现：（1）LLMs存在演示偏见，而CDs可以显著减少这种偏见；（2）CDs在ICL中表现出良好的性能，尤其是在分布外场景中。总之，本研究从一种新的视角探索了ICL机制。

    In-Context Learning (ICL) is an important paradigm for adapting Large Language Models (LLMs) to downstream tasks through a few demonstrations. Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence. Inspired by human experience, we attempt to mitigate such bias through the perspective of the inter-demonstration relationship. Specifically, we construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels, in order to highlight the task's essence and eliminate potential spurious correlations through the inter-demonstration comparison. Through a series of experiments on CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias; (2) CDs exhibit good performance in ICL, especially in out-of-distribution scenarios. In summary, this study explores the ICL mechanisms from a n
    
[^49]: 大还是深是否总是好的？在不同尺度和层次上探究LLaMA。

    Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers. (arXiv:2312.04333v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.04333](http://arxiv.org/abs/2312.04333)

    本文通过多项选择任务对LLaMA进行了深入分析，揭示了LLaMA在推理和计算等高阶任务中的内在理解能力。研究发现，增大模型尺寸几乎不能自动增加额外的知识或计算能力，但可以提高推理能力和减少幻觉，尤其在数学问题解决方面。此外，LLaMA的较低层次缺乏实质性的算术和事实知识，但顶层层次展现出较强的语言理解能力。

    

    本文对大型语言模型（LLM）进行了深入分析，重点关注了LLaMA，这是一种在自然语言处理中非常重要的开源基础模型。我们通过设计多项选择任务来评估LLaMA在推理和计算等高阶任务中的内在理解能力，而不是通过其生成的输出来评估LLaMA。我们沿着水平方向对模型进行了比较，比较了不同的尺寸，然后沿着纵向方向评估了不同的层次。根据我们设计的探究任务，我们揭示了几个关键但不常见的发现：（1）在水平方面，增大模型尺寸几乎不能自动地增加额外的知识或计算能力。相反，它可以提高推理能力，特别是在数学问题解决方面，并有助于减少幻觉，但只有在超过某个尺寸门槛时才能实现；（2）在纵向分析中，LLaMA的较低层次缺乏实质性的算术和事实知识，展现了逻辑思维、多语言和识别能力，其顶层层次表现出较强的语言理解能力。

    This paper presents an in-depth analysis of Large Language Models (LLMs), focusing on LLaMA, a prominent open-source foundational model in natural language processing. Instead of assessing LLaMA through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and computation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers. We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of LLaMA lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top la
    
[^50]: 通过多模态部分对齐进行基于视觉信息的锚定语言学习，扩展BERT表征

    Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment. (arXiv:2312.01592v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.01592](http://arxiv.org/abs/2312.01592)

    本论文提出了一种名为GroundedBERT的方法，通过视觉锚定信息增强BERT表示，解决了由于视觉锚定数据集和语言语料库的差异导致的上下文含义不匹配的问题。

    

    在现有的视觉锚定语言学习研究中，语言模型已经通过语言目标和视觉锚定来进行监督。然而，由于视觉锚定数据集和语言语料库的分布和规模差异，语言模型往往会混淆在锚定数据中出现的令牌的上下文和不出现的令牌的上下文。因此，在表征学习过程中，视觉信息与句子的上下文含义之间存在不匹配。为了克服这个限制，我们提出了GroundedBERT-一种通过视觉锚定信息增强BERT表示的基于锚定语言学习方法。GroundedBERT包括两个组件：(i)原始BERT，它从语言语料库中学习单词的上下文表示，(ii)视觉锚定​​模块，它从视觉锚定数据集中学习视觉信息。此外，我们采用了最优传输(OT)，特别是

    Language models have been supervised with both language-only objective and visual grounding in existing studies of visual-grounded language learning. However, due to differences in the distribution and scale of visual-grounded datasets and language corpora, the language model tends to mix up the context of the tokens that occurred in the grounded data with those that do not. As a result, during representation learning, there is a mismatch between the visual information and the contextual meaning of the sentence. To overcome this limitation, we propose GroundedBERT - a grounded language learning method that enhances the BERT representation with visually grounded information. GroundedBERT comprises two components: (i) the original BERT which captures the contextual representation of words learned from the language corpora, and (ii) a visual grounding module which captures visual information learned from visual-grounded datasets. Moreover, we employ Optimal Transport (OT), specifically it
    
[^51]: mPLUG-PaperOwl: 多模态大型语言模型在科学图表分析中的应用

    mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model. (arXiv:2311.18248v2 [cs.MM] UPDATED)

    [http://arxiv.org/abs/2311.18248](http://arxiv.org/abs/2311.18248)

    mPLUG-PaperOwl是一个多模态大型语言模型，其创新之处在于增强了图表分析能力，为科学学术论文写作提供了更多功能的辅助工具。

    

    最近，大型语言模型（LLMs）强大的文本生成能力催生了许多辅助论文阅读甚至写作的工具。然而，LLMs或多模态LLMs的弱图表分析能力极大限制了它们的应用场景，尤其是对于科学学术论文写作。在这项工作中，为了实现更多功能的学术论文写作助手，我们主要关注增强多模态LLMs的图表分析能力。通过解析高质量论文的LaTeX源文件，我们精心构建了一个多模态图表理解数据集M-Paper。通过将论文中的图表与相关段落对齐，我们构建了用于训练和评估的专业图表分析样本。M-Paper是第一个支持综合理解多个科学图表的数据集，包括以图像或LaTeX代码格式提供的图形和表格。此外，为了更好地与用户的意图对齐，我们引入了"概要" 。

    Recently, the strong text creation ability of Large Language Models(LLMs) has given rise to many tools for assisting paper reading or even writing. However, the weak diagram analysis abilities of LLMs or Multimodal LLMs greatly limit their application scenarios, especially for scientific academic paper writing. In this work, towards a more versatile copilot for academic paper writing, we mainly focus on strengthening the multi-modal diagram analysis ability of Multimodal LLMs. By parsing Latex source files of high-quality papers, we carefully build a multi-modal diagram understanding dataset M-Paper. By aligning diagrams in the paper with related paragraphs, we construct professional diagram analysis samples for training and evaluation. M-Paper is the first dataset to support joint comprehension of multiple scientific diagrams, including figures and tables in the format of images or Latex codes. Besides, to better align the copilot with the user's intention, we introduce the `outline' 
    
[^52]: LLMs无法找到推理错误，但可以纠正它们！（arXiv：2311.08516v2 [cs.AI] UPDATED）

    LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.08516](http://arxiv.org/abs/2311.08516)

    本文研究了LLMs在自我纠正过程中的错误发现和输出纠正两个核心组成部分。研究发现LLMs通常难以发现逻辑错误，但通过使用回溯方法可以在提供错误位置信息时获得大幅改进。

    

    尽管自我纠正在改善LLM输出的风格和质量方面显示出了潜力（例如Chen等，2023；Madaan等，2023），最近对逻辑或推理错误进行自我纠正的尝试通常会导致正确答案变为错误，从而总体表现变差（Huang等，2023）。在本文中，我们将自我纠正过程分解为两个核心组成部分：错误发现和输出纠正。对于错误发现，我们发布了BIG-Bench Mistake，这是一个Chain-of-Thought推理轨迹中的逻辑错误数据集。我们为几种最先进的LLM提供基准数，并证明LLM通常难以发现逻辑错误。对于输出纠正，我们提出了一种回溯方法，在提供错误位置信息时可以大幅改进。我们将回溯解释为对强化学习方法的轻量级替代方案，并展示了在60-70％准确率下保持有效性的奖励模型。

    While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we break down the self-correction process into two core components: mistake finding and output correction. For mistake finding, we release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought reasoning traces. We provide benchmark numbers for several state-of-the-art LLMs, and demonstrate that LLMs generally struggle with finding logical mistakes. For output correction, we propose a backtracking method which provides large improvements when given information on mistake location. We construe backtracking as a lightweight alternative to reinforcement learning methods, and show that it remains effective with a reward model at 60-70% accuracy.
    
[^53]: 自动逻辑形式提高表格到文本生成中的准确性

    Automatic Logical Forms improve fidelity in Table-to-Text generation. (arXiv:2310.17279v1 [cs.CL])

    [http://arxiv.org/abs/2310.17279](http://arxiv.org/abs/2310.17279)

    本文提出了一种自动逻辑形式（LF）来提高表格到文本生成的准确性，首次展示了用自动LF改进系统可以提高准确性30个百分点，还指出了实现高准确性仍面临的挑战。

    

    表格到文本系统从结构化数据（如表格）生成自然语言陈述。虽然端到端技术在事实准确性方面存在问题，但先前的研究报告称，在使用手动逻辑形式（LF）表示所选内容和目标文本的语义时，获得了提升。鉴于手动步骤，不清楚自动LF是否有效，或者改进来自内容选择本身。我们提出了TlT，给定一个表格和内容选择，首先生成LF，然后生成文本陈述。我们首次展示了自动LF提高质量的效果，与不使用LF的类似系统相比，准确性提高了30个百分点。我们的实验对于高准确性还面临着一些挑战，自动内容选择是首要问题，其次是更好的逻辑到文本生成，以及较少程度的更好的表格到逻辑解析。

    Table-to-text systems generate natural language statements from structured data like tables. While end-to-end techniques suffer from low factual correctness (fidelity), a previous study reported gains when using manual logical forms (LF) that represent the selected content and the semantics of the target text. Given the manual step, it was not clear whether automatic LFs would be effective, or whether the improvement came from content selection alone. We present TlT which, given a table and a selection of the content, first produces LFs and then the textual statement. We show for the first time that automatic LFs improve quality, with an increase in fidelity of 30 points over a comparable system not using LFs. Our experiments allow to quantify the remaining challenges for high factual correctness, with automatic selection of content coming first, followed by better Logic-to-Text generation and, to a lesser extent, better Table-to-Logic parsing.
    
[^54]: 基于因果结构的文本离群值泛化增强方法

    Causal-structure Driven Augmentations for Text OOD Generalization. (arXiv:2310.12803v1 [cs.LG])

    [http://arxiv.org/abs/2310.12803](http://arxiv.org/abs/2310.12803)

    本文提出了一种基于因果结构的反事实数据增强方法，用于改善文本分类器在应用中的泛化效果，特别适用于存在虚假相关性的标签与属性预测问题。

    

    文本分类器对虚假相关性的依赖可能导致在实际应用中的泛化效果不佳，这引发了对其在如医疗领域等安全关键行业中使用的担忧。在本研究中，我们提出使用因果结构知识指导的反事实数据增强方法，模拟对虚假特征进行干预，以学习更加鲁棒的文本分类器。我们证明了在标签与属性之间存在虚假相关性的预测问题中，这种策略是合适的。在这种问题的假设下，我们讨论了反事实数据增强相对于重要性重加权的有利样本复杂性。实际上，我们使用辅助数据通过差分在差分的方法来匹配样本，并使用大型语言模型（LLM）来表示文本的条件概率。通过对从医学叙述中学习与看护者无关的临床诊断预测器以及半合成数据上进行了广泛的实验。

    The reliance of text classifiers on spurious correlations can lead to poor generalization at deployment, raising concerns about their use in safety-critical domains such as healthcare. In this work, we propose to use counterfactual data augmentation, guided by knowledge of the causal structure of the data, to simulate interventions on spurious features and to learn more robust text classifiers. We show that this strategy is appropriate in prediction problems where the label is spuriously correlated with an attribute. Under the assumptions of such problems, we discuss the favorable sample complexity of counterfactual data augmentation, compared to importance re-weighting. Pragmatically, we match examples using auxiliary data, based on diff-in-diff methodology, and use a large language model (LLM) to represent a conditional probability of text. Through extensive experimentation on learning caregiver-invariant predictors of clinical diagnoses from medical narratives and on semi-synthetic 
    
[^55]: 适应口语对话的文本对话状态跟踪器

    Adapting text-based dialogue state tracker for spoken dialogues. (arXiv:2308.15053v1 [cs.CL])

    [http://arxiv.org/abs/2308.15053](http://arxiv.org/abs/2308.15053)

    这篇论文描述了对构建适应口语对话系统的文本对话状态跟踪器进行的工程工作，利用自动语音识别错误校正和文本对话系统实现了插槽和值的估计。

    

    尽管通过对话系统技术竞赛（DSTC）取得了显著进展，但构建一个具有语音界面的稳健的任务导向对话系统仍然是一个关键挑战。大部分进展都是针对基于文本的对话系统，因为有丰富的书面语料库数据集，而具有口语对话的数据集非常稀缺。然而，正如Siri和Alexa等语音助手系统所展示的，将这种成功转移到口语对话中具有实际重要性。在本文中，我们描述了我们在DSTC11的具有语音感知的对话系统技术挑战赛中的高度成功模型的工程努力。我们的模型由三个主要模块组成：（1）自动语音识别错误校正，以弥合口语和文本话语之间的差距，（2）用于估计插槽和值的基于文本的对话系统（D3ST），该系统使用插槽描述。

    Although there have been remarkable advances in dialogue systems through the dialogue systems technology competition (DSTC), it remains one of the key challenges to building a robust task-oriented dialogue system with a speech interface. Most of the progress has been made for text-based dialogue systems since there are abundant datasets with written corpora while those with spoken dialogues are very scarce. However, as can be seen from voice assistant systems such as Siri and Alexa, it is of practical importance to transfer the success to spoken dialogues. In this paper, we describe our engineering effort in building a highly successful model that participated in the speech-aware dialogue systems technology challenge track in DSTC11. Our model consists of three major modules: (1) automatic speech recognition error correction to bridge the gap between the spoken and the text utterances, (2) text-based dialogue system (D3ST) for estimating the slots and values using slot descriptions, an
    
[^56]: 大型语言模型的不平等机会: 通过职位推荐揭示人口统计偏见

    The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations. (arXiv:2308.02053v1 [cs.CL])

    [http://arxiv.org/abs/2308.02053](http://arxiv.org/abs/2308.02053)

    通过职位推荐分析了大型语言模型（LLMs）的人口统计偏见，发现这些模型对于墨西哥工人一直建议低薪工作，并向女性更倾向于推荐秘书职位。这项研究强调了理解LLMs偏见的重要性。

    

    大型语言模型（LLMs）已在各种实际应用中得到广泛应用。了解这些偏见对于理解在使用LLMs进行决策时潜在的后续影响至关重要，特别是对于历史上处于劣势的群体。在这项工作中，我们提出了一种简单的方法来通过职位推荐的角度分析和比较LLMs中的人口统计偏见。我们通过测量ChatGPT和LLaMA这两个前沿LLMs内的交叉偏见来证明我们方法的有效性。我们的实验主要集中在揭示性别认同和国籍偏见上；然而，我们的方法可以扩展到任何人口统计身份的交叉偏见的研究。我们在两个模型中发现了明显的偏见，例如两个模型一直建议墨西哥工人从事低薪工作，或者更倾向于向女性推荐秘书职位。我们的研究强调了测量和理解LLMs中的偏见的重要性。

    Large Language Models (LLMs) have seen widespread deployment in various real-world applications. Understanding these biases is crucial to comprehend the potential downstream consequences when using LLMs to make decisions, particularly for historically disadvantaged groups. In this work, we propose a simple method for analyzing and comparing demographic bias in LLMs, through the lens of job recommendations. We demonstrate the effectiveness of our method by measuring intersectional biases within ChatGPT and LLaMA, two cutting-edge LLMs. Our experiments primarily focus on uncovering gender identity and nationality bias; however, our method can be extended to examine biases associated with any intersection of demographic identities. We identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for Mexican workers or preferring to recommend secretarial roles to women. Our study highlights the importance of measu
    
[^57]: 使用差分隐私大语言模型合成查询的隐私保护推荐系统.

    Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models. (arXiv:2305.05973v1 [cs.CL])

    [http://arxiv.org/abs/2305.05973](http://arxiv.org/abs/2305.05973)

    提出使用差分隐私大语言模型合成查询的隐私保护推荐系统，可以安全有效地训练深度检索模型并提高检索质量。

    

    我们提出了一种新颖的方法，使用差分隐私大语言模型（LLMs）开发隐私保护的大规模推荐系统，克服了在训练这些复杂系统时的某些挑战和限制。我们的方法特别适用于基于LLM的推荐系统的新兴领域，但也可以轻松地用于处理自然语言输入表示的任何推荐系统。我们的方法涉及使用DP训练方法，对公开预训练的LLM在查询生成任务上进行微调。生成的模型可以生成私有合成查询，代表原始查询，可以在任何下游非私有推荐训练过程中自由共享，而不会产生任何额外的隐私成本。我们评估了我们的方法对安全训练有效的深度检索模型的能力，我们观察到它们的检索质量有显着的提高，而不会损害查询级别的隐私。

    We propose a novel approach for developing privacy-preserving large-scale recommender systems using differentially private (DP) large language models (LLMs) which overcomes certain challenges and limitations in DP training these complex systems. Our method is particularly well suited for the emerging area of LLM-based recommender systems, but can be readily employed for any recommender systems that process representations of natural language inputs. Our approach involves using DP training methods to fine-tune a publicly pre-trained LLM on a query generation task. The resulting model can generate private synthetic queries representative of the original queries which can be freely shared for any downstream non-private recommendation training procedures without incurring any additional privacy cost. We evaluate our method on its ability to securely train effective deep retrieval models, and we observe significant improvements in their retrieval quality without compromising query-level pri
    
[^58]: HCAM--多模态情感识别的分层交叉注意模型

    HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition. (arXiv:2304.06910v1 [eess.AS])

    [http://arxiv.org/abs/2304.06910](http://arxiv.org/abs/2304.06910)

    该论文提出了一种分层交叉注意模型（HCAM）用于多模态情感识别，使用递归和共同注意神经网络模型进行音频和文本表示，将这两种模态信息以共同注意方式结合，取得了比现有方法更好的情感识别效果。

    

    对话情感识别由于情感表达的多模态性而具有挑战性。我们提出了一种采用递归和共同注意神经网络模型的分层交叉注意模型（HCAM）方法用于多模态情感识别。模型的输入包括两种模态，即通过可学习wav2vec方法处理的音频数据和使用双向编码器来自变压器（BERT）模型表示的文本数据。音频和文本表示使用一组双向递归神经网络层进行处理，使用自注意将给定对话中的每个话语转换为固定维度的嵌入。为了整合上下文知识和两种模态的信息，使用共同注意层将音频和文本嵌入进行组合，试图衡量与情感识别任务相关的话语级嵌入。在CMU-MOSI数据集上训练和评估神经网络模型，这是一个大规模的多模态会话数据集。实验结果表明，所提出的HCAM方法优于现有的情感识别最先进方法。

    Emotion recognition in conversations is challenging due to the multi-modal nature of the emotion expression. We propose a hierarchical cross-attention model (HCAM) approach to multi-modal emotion recognition using a combination of recurrent and co-attention neural network models. The input to the model consists of two modalities, i) audio data, processed through a learnable wav2vec approach and, ii) text data represented using a bidirectional encoder representations from transformers (BERT) model. The audio and text representations are processed using a set of bi-directional recurrent neural network layers with self-attention that converts each utterance in a given conversation to a fixed dimensional embedding. In order to incorporate contextual knowledge and the information across the two modalities, the audio and text embeddings are combined using a co-attention layer that attempts to weigh the utterance level embeddings relevant to the task of emotion recognition. The neural network
    
[^59]: 基于医学提示的语言增强Transformer编码器的医疗干预持续时间估计

    Medical Intervention Duration Estimation Using Language-enhanced Transformer Encoder with Medical Prompts. (arXiv:2303.17408v1 [cs.CL])

    [http://arxiv.org/abs/2303.17408](http://arxiv.org/abs/2303.17408)

    使用语言增强Transformer编码器，并结合医学提示，将结构化、非结构化的临床数据投影到一个语言潜空间中，以实现更精确的医学干预持续时间估计。

    

    近年来，基于电子病历(EHRs)估计医疗干预的持续时间在临床决策支持领域引起了重视。然而，当前的模型主要关注结构化数据，忽略了来自非结构化的临床自由文本数据的信息。为了解决这个问题，我们提出了一个新颖的语言增强Transformer-based框架，它使用经过预训练的句子编码器将所有相关的临床数据模态（连续、分类、二进制和自由文本特征）投影到一个协调的语言潜空间中，借助医学提示。所提出的方法使得不同模态的信息在单元变压器编码器中集成起来，从而实现更准确的医学干预持续时间估计。我们在美国（ICU住院时间估计）和亚洲（手术持续时间预测）医学数据集上的实验结果证明了我们提出的框架的有效性。

    In recent years, estimating the duration of medical intervention based on electronic health records (EHRs) has gained significant attention in the filed of clinical decision support. However, current models largely focus on structured data, leaving out information from the unstructured clinical free-text data. To address this, we present a novel language-enhanced transformer-based framework, which projects all relevant clinical data modalities (continuous, categorical, binary, and free-text features) into a harmonized language latent space using a pre-trained sentence encoder with the help of medical prompts. The proposed method enables the integration of information from different modalities within the cell transformer encoder and leads to more accurate duration estimation for medical intervention. Our experimental results on both US-based (length of stay in ICU estimation) and Asian (surgical duration prediction) medical datasets demonstrate the effectiveness of our proposed framewor
    
[^60]: 从理想语言模型中可以提取蕴含语义

    Entailment Semantics Can Be Extracted from an Ideal Language Model. (arXiv:2209.12407v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.12407](http://arxiv.org/abs/2209.12407)

    该论文证明，假设训练句子由遵循交际基本原则的代理生成，那么可以从目标分布完美学习的理想语言模型中提取出蕴含判断。这个结果揭示了从未标注语言数据中理解语义和从语言模型中提取语义的潜在方法。

    

    语言模型通常仅通过文本训练，没有额外的基础。关于这种过程能够推断出多少自然语言语义存在争议。我们证明，假设训练句子由Gricean代理生成（即遵循语用学中的交际基本原则的代理），可以从完美学习了目标分布的理想语言模型中提取出句子之间的蕴含判断。我们还展示了可以从训练在这种Gricean数据上的语言模型的预测中解码出蕴含判断。我们的结果揭示了理解未标注语言数据中编码的语义信息的途径，以及从语言模型中提取语义的潜在框架。

    Language models are often trained on text alone, without additional grounding. There is debate as to how much of natural language semantics can be inferred from such a procedure. We prove that entailment judgments between sentences can be extracted from an ideal language model that has perfectly learned its target distribution, assuming the training sentences are generated by Gricean agents, i.e., agents who follow fundamental principles of communication from the linguistic theory of pragmatics. We also show entailment judgments can be decoded from the predictions of a language model trained on such Gricean data. Our results reveal a pathway for understanding the semantic information encoded in unlabeled linguistic data and a potential framework for extracting semantics from language models.
    
[^61]: E2S2: 增强编码的序列到序列预训练模型用于语言理解和生成

    E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation. (arXiv:2205.14912v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.14912](http://arxiv.org/abs/2205.14912)

    E2S2提出了一种增强编码的序列到序列预训练策略，通过在编码器端引入更有效的自我监督信息，改进了语言模型的下游性能。

    

    序列到序列(seq2seq)学习是大规模预训练语言模型中流行的方法。然而，之前的seq2seq预训练模型通常只关注解码器方面的重构目标，忽视了编码器方面的监督作用，我们认为这可能导致性能不佳。为验证我们的假设，我们首先实证研究了序列到序列预训练语言模型中编码器和解码器的功能，并发现编码器在下游性能和神经元激活方面扮演着重要但被低估的角色。因此，我们提出了一种增强编码的序列到序列预训练策略，称为E2S2，通过将更有效的自我监督信息整合到编码器中来改进seq2seq模型。具体而言，E2S2在编码器端采用了两个自我监督目标：1) 本地去噪受损句子（去噪目标）；2) 全局学习更好的句子表示（全局学习目标）。

    Sequence-to-sequence (seq2seq) learning is a popular fashion for large-scale pretraining language models. However, the prior seq2seq pretraining models generally focus on reconstructive objectives on the decoder side and neglect the effect of encoder-side supervision, which we argue may lead to sub-optimal performance. To verify our hypothesis, we first empirically study the functionalities of the encoder and decoder in seq2seq pretrained language models, and find that the encoder takes an important but under-exploitation role than the decoder regarding the downstream performance and neuron activation. Therefore, we propose an encoding-enhanced seq2seq pretraining strategy, namely E2S2, which improves the seq2seq models via integrating more efficient self-supervised information into the encoders. Specifically, E2S2 adopts two self-supervised objectives on the encoder side from two aspects: 1) locally denoising the corrupted sentence (denoising objective); and 2) globally learning bette
    

