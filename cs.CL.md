# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT.](http://arxiv.org/abs/2310.15896) | BianQue是一种基于ChatGLM进行微调的LLMs模型，旨在提高LLMs的多轮提问能力，以平衡提问和建议能力，并提供更加个性化和有针对性的健康建议。 |
| [^2] | [Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models.](http://arxiv.org/abs/2310.15852) | 本文使用人工法语数据生成的语料库，探索了神经语言模型如何发现单词的性别属性以及其使用规则，并研究了模型在不同条件下是否正确捕捉到性别信息或表现出性别偏见。 |
| [^3] | [Self-Guard: Empower the LLM to Safeguard Itself.](http://arxiv.org/abs/2310.15851) | 这篇论文提出了一种称为自我防御的新方法，通过结合安全训练和保护措施的优势，提升大型语言模型（LLM）的安全性，从而减少有害内容的生成。 |
| [^4] | [A Diffusion Weighted Graph Framework for New Intent Discovery.](http://arxiv.org/abs/2310.15836) | 本研究提出了一种名为Diffusion Weighted Graph Framework (DWGF)的方法，用于捕捉数据中的语义相似性和结构关系，从而实现更充分和可靠的监督信号，解决了以往方法在新意图发现中无法平衡数量和质量的问题。 |
| [^5] | [Unnatural language processing: How do language models handle machine-generated prompts?.](http://arxiv.org/abs/2310.15829) | 通过使用机器生成的提示，研究人员探索了语言模型对非自然语言输入的响应。他们发现，即使产生了类似的输出，机器生成的和人工生成的提示会触发不同的响应模式。这项研究为我们初步揭示了提示的性质。 |
| [^6] | [Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word--Definition Alignment.](http://arxiv.org/abs/2310.15823) | 本论文介绍了在KSAA-RD共享任务中Rosetta Stone的应用，将语言建模应用到词--定义对齐中。论文通过使用一组微调的阿拉伯BERT模型来预测给定定义的词嵌入，从而实现了阿拉伯词的向量表示。 |
| [^7] | [Generative Language Models Exhibit Social Identity Biases.](http://arxiv.org/abs/2310.15819) | 该研究调查了51个大型语言模型展示的社会身份偏见，发现几乎所有模型在补全句子时都展示了明显的团体内积极和团体外消极的偏见。与人类文本相比，这些模型展示了类似或更大程度的偏见。 |
| [^8] | [DALE: Generative Data Augmentation for Low-Resource Legal NLP.](http://arxiv.org/abs/2310.15799) | DALE是一个用于低资源法律NLP的生成式数据增强框架，通过基于选择性掩码的无监督文本去噪目标预训练，在解决法律语言特异性的同时生成连贯且多样化的增强。 |
| [^9] | [Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation.](http://arxiv.org/abs/2310.15797) | 本文研究了参数高效的组合知识图谱表示的问题，通过随机实体量化的方法，可以达到与当前策略类似的效果，这是因为随机实体量化下，实体码有更高的熵和码字级别的Jaccard距离，使得不同实体更容易区分，从而有效地表示知识图谱。 |
| [^10] | [Improving generalization in large language models by learning prefix subspaces.](http://arxiv.org/abs/2310.15793) | 本文提出了一种通过学习前缀子空间来改进大型语言模型的泛化能力的方法。我们通过联合优化模型参数空间中的整个单纯形模型，在稀缺数据环境中实现了更广的局部最优解。这种方法在预训练变换器模型中表现出了很好的兼容性和有效性。 |
| [^11] | [MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications.](http://arxiv.org/abs/2310.15777) | 本文提出了一种从零开始训练的轻量级大型语言模型MindLLM，通过提供1.3亿和3亿参数的模型，减轻了训练和部署大型语言模型的成本和资源稀缺性的压力。MindLLM在各个步骤中给出了经验教训，包括数据构建、模型架构、评估和应用，对学术界和开发者来说具有重要价值。 |
| [^12] | [BLESS: Benchmarking Large Language Models on Sentence Simplification.](http://arxiv.org/abs/2310.15773) | BLESS是一个对大型语言模型在句子简化任务上进行基准测试的项目，评测了44个模型在不同领域的三个测试集上的性能，并发现即使未经过句子简化的训练，最好的模型性能与最新的简化任务基线相当，并且某些模型展示了更广泛和多样化的编辑操作。 |
| [^13] | [Learning From Free-Text Human Feedback -- Collect New Datasets Or Extend Existing Ones?.](http://arxiv.org/abs/2310.15758) | 本研究调查了常用对话数据集中自由文本人类反馈的类型和频率，并提出了用于注释这些反馈的新分类法。通过将这些数据用于回答生成，我们研究了对GPT-2、LLAMA和Fla等语言生成模型的影响。 |
| [^14] | [Do Differences in Values Influence Disagreements in Online Discussions?.](http://arxiv.org/abs/2310.15757) | 本论文研究了在线讨论中的分歧，发现个人价值观差异与分歧有关，探讨了注入价值观信息对一致性预测的改进。 |
| [^15] | [Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection.](http://arxiv.org/abs/2310.15752) | 该论文提出了一种推断时间解决方案，用于在语音翻译中控制与说话者相关的性别变化。通过部分替换内部语言模型，将特定性别的外部语言模型应用于翻译过程，实验证明这一方法在性别准确性方面优于基准模型和最佳训练时间缓解策略，尤其在具有性别冲突的条件下效果显著提高。 |
| [^16] | [Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation.](http://arxiv.org/abs/2310.15746) | 本论文提出了一种无调优规则累积（TRAN）框架，通过从以前的错误中学习来指导大型语言模型（LLMs）改善性能。实验证明，TRAN相较于最近的基准线有很大的改进。 |
| [^17] | [RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction.](http://arxiv.org/abs/2310.15743) | 这篇论文提出了一种关系感知的原型学习方法，用于解决少样本文档级关系抽取中关于类别原型的准确性问题。 |
| [^18] | [Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules.](http://arxiv.org/abs/2310.15724) | 这个论文提出了一种称为Variator的加速方法，通过即插即用的压缩插件增强了预训练模型的计算效率，并且可以根据工作负载动态选择不同加速比的插件。插件采用了压缩隐藏向量的方法来减小序列长度，并且由于参数少，可以节省存储和内存开销。 |
| [^19] | [Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion.](http://arxiv.org/abs/2310.15722) | Re-Temp是一个能够根据实体关系跳过不相关快照并利用显式时间信息进行预测的模型，通过在多个TKGC数据集上的评估，我们证明了它的有效性。 |
| [^20] | [Ensemble of Task-Specific Language Models for Brain Encoding.](http://arxiv.org/abs/2310.15720) | 该论文提出了一种用于大脑编码的任务特定语言模型集成方法，通过将10个流行的语言模型进行集成，相较于当前基准线，平均提高了10%的性能。 |
| [^21] | [Enhancing Biomedical Lay Summarisation with External Knowledge Graphs.](http://arxiv.org/abs/2310.15702) | 本研究通过增加文章特定的知识图，改进了生物医学科普总结的自动方法。实验证实，整合基于图的领域知识可以显著提高科普总结的可读性和解释性。 |
| [^22] | [COPF: Continual Learning Human Preference through Optimal Policy Fitting.](http://arxiv.org/abs/2310.15694) | 通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。 |
| [^23] | [Towards Automated Recipe Genre Classification using Semi-Supervised Learning.](http://arxiv.org/abs/2310.15693) | 该论文提出了一个名为3A2M+的自动菜谱类型分类方法，通过半监督学习利用扩展的命名实体识别列表对烹饪食谱进行分类。研究者提供了一个包含两百万个带有各种特征和九个不同类型标签的烹饪食谱数据集，以解决缺乏标注数据的问题。 |
| [^24] | [Creating a silver standard for patent simplification.](http://arxiv.org/abs/2310.15689) | 本文提出了一种通过改写自动简化专利文本的方法，并成功构建了一个银标准语料库用于训练简化系统。 |
| [^25] | [Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers.](http://arxiv.org/abs/2310.15684) | 该论文提出了一种基于引文聚合的模型，通过整合引文论文中的领域特定知识，提高了生物医学摘要总结的语言模型的性能。 |
| [^26] | [Prevalence and prevention of large language model use in crowd work.](http://arxiv.org/abs/2310.15683) | 该论文研究了众包工作者使用大型语言模型（LLM）的使用率，并探讨了预防措施。研究发现，通过要求工作者不使用LLM并增加使用成本可显著降低LLM的使用率，但无法完全消除。然而，防止LLM的使用可能影响到获得高质量回答。这些发现对于关注人类行为和众包数据训练的未来模型具有重要意义。 |
| [^27] | [How Much Context Does My Attention-Based ASR System Need?.](http://arxiv.org/abs/2310.15672) | 本研究考察了对于音频识别任务，训练和评估使用不同长度的序列对语音识别性能的影响。结果表明，使用大约80秒的声学上下文进行训练可以相对提高14.9%的性能，并且与当前最先进的方法具有竞争力。 |
| [^28] | [Expression Syntax Information Bottleneck for Math Word Problems.](http://arxiv.org/abs/2310.15664) | 本文提出了一种通过变分信息瓶颈从数学问题的文本中提取关键特征的方法，同时去除冗余信息。该方法通过相互学习，鼓励多个模型为同一个问题的不同表述预测相同的表达式语法树，从而捕捉一致的信息并去除冗余。 |
| [^29] | [A Survey on Detection of LLMs-Generated Content.](http://arxiv.org/abs/2310.15654) | 该论文是关于LLMs生成内容检测的综述，提供了现有策略和挑战的概述，并提倡采用更灵活和强大的模型以提高检测准确性，并强调使用多方面的方法来应对不同攻击。这是首个综合调查LLMs时代检测的工作。 |
| [^30] | [CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation.](http://arxiv.org/abs/2310.15638) | CoAnnotating是一种新颖的人类-LLM联合注释框架，利用不确定性来估计LLMs的能力，并能在不同数据集上获得高达21%的性能提升。 |
| [^31] | [Career Path Prediction using Resume Representation Learning and Skill-based Matching.](http://arxiv.org/abs/2310.15636) | 本论文在职业路径预测中提出了一种使用简历表示学习和基于技能匹配的方法，通过研究文本描述部分来预测下一步的职业动向，并在数据集上验证了该方法的有效性。 |
| [^32] | [Tips for making the most of 64-bit architectures in langage design, libraries or garbage collection.](http://arxiv.org/abs/2310.15632) | 该论文介绍了如何在语言设计、库和垃圾收集方面充分利用64位体系结构的技巧，通过实现多精度整数、简化UTF-8字符串索引以及增强垃圾收集器的功能，提高了计算性能和节省了内存空间。 |
| [^33] | [Machine Translation for Nko: Tools, Corpora and Baseline Results.](http://arxiv.org/abs/2310.15612) | 该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。 |
| [^34] | [MUSER: A Multi-View Similar Case Retrieval Dataset.](http://arxiv.org/abs/2310.15602) | MUSER是一个基于多视角相似度测量和句子级法律要素注释的相似案例检索数据集，旨在实现准确评估和专业知识的考量。 |
| [^35] | [Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression.](http://arxiv.org/abs/2310.15594) | 检索式知识转移（RetriKT）是一种新的压缩范例，它通过提取大规模预训练语言模型的知识并利用 retrieval-based 的方法，将这些知识应用于极小规模的模型中，从而实现了极端的模型压缩效果。 |
| [^36] | [ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts.](http://arxiv.org/abs/2310.15587) | 这篇论文提出了一种用于生成文本中合成扫视路径的扩散模型（ScanDL），以解决眼动数据稀缺和不可用的问题。 |
| [^37] | [Multimodal Representations for Teacher-Guided Compositional Visual Reasoning.](http://arxiv.org/abs/2310.15585) | 本论文提出了一种教师引导的多模态表示方法，通过利用大规模跨模态编码器的特征来改进神经模块网络(NMN)，并引入了计划的教师引导的学习策略，以减少误差积累并提高性能。 |
| [^38] | [CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction.](http://arxiv.org/abs/2310.15577) | CONTRASTE是一种利用对比学习的预训练策略，通过设计基于方面的提示并应用对比学习来增强方面情感三元组抽取（ASTE）任务的性能，并在其他ABSA任务上展示出优势。 |
| [^39] | [POE: Process of Elimination for Multiple Choice Reasoning.](http://arxiv.org/abs/2310.15575) | POE是一种两步策略的评分方法，通过排除看似错误的选项，提高了语言模型在多项选择推理任务上的表现。该方法在逻辑推理任务上表现特别好，并适用于少样本设置和大语言模型。 |
| [^40] | [Natural Language Processing for Drug Discovery Knowledge Graphs: promises and pitfalls.](http://arxiv.org/abs/2310.15572) | 本文讨论了将自然语言处理（NLP）应用于药物发现知识图谱的优势和陷阱。NLP可以从大量文档中自动提取数据，为知识图谱的构建和分析提供了便利。然而，NLP-KG流程中存在一些潜在的问题，如命名实体的错误识别和处理。 |
| [^41] | [Visually Grounded Continual Language Learning with Selective Specialization.](http://arxiv.org/abs/2310.15571) | 这项工作旨在对可视化基础的持续语言学习的选择策略进行广泛分析。通过引入两个新的诊断数据集，作者为模型分析提供了足够的控制和灵活性。评估了各种启发式的模块专业化策略以及... |
| [^42] | [MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in the Materials Science Domain.](http://arxiv.org/abs/2310.15569) | MuLMS是一个多层注释的文本语料库，包含50篇开放获取的文章，涵盖了材料科学的七个子领域。该语料库被注释了多个层次的信息，并提出了多任务训练的神经模型。 |
| [^43] | [TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction.](http://arxiv.org/abs/2310.15556) | TCRA-LLM是通过概述压缩和语义压缩两种方法来减少商业大型语言模型推理成本的方案。 |
| [^44] | [Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks.](http://arxiv.org/abs/2310.15552) | 这项研究揭示了在Transformer模型中，前馈模块可以被视为键值记忆的集合，通过学习特定语言的模式，并结合共享特征，实现多语言模型的预测过程。 |
| [^45] | [Improving Language Models Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary.](http://arxiv.org/abs/2310.15541) | 该论文提出了一种实用的方法，通过从字典中学习概念角色，从根本上改善语言模型的意义认知，以缓解其生成不一致预测的问题。 |
| [^46] | [SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation.](http://arxiv.org/abs/2310.15539) | SteloCoder是一个仅解码的基于StarCoder的LLM，在多语言到Python代码翻译中取得了显著的性能提升。它采用Mixture-of-Experts（MoE）技术和门控网络，通过对StarCoder进行微调获得专家，并使用低秩自适应方法（LoRA）技术来限制每个专家的大小。 |
| [^47] | [MarkQA: A large scale KBQA dataset with numerical reasoning.](http://arxiv.org/abs/2310.15517) | 本文提出了一个包含数值推理的大规模KBQA数据集MarkQA，并设计了一种新的任务NR-KBQA，该任务要求进行多跳推理和数值推理。实验结果表明，KBQA中的复杂数值推理面临巨大挑战。 |
| [^48] | [Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation.](http://arxiv.org/abs/2310.15515) | 这项研究提出了一种新的策略，即利用现代语言模型的生成和推理能力来对抗虚假信息。通过合成真实和欺骗性的内容以及利用语境语义推理技术，该策略在检测和对抗虚假信息方面取得了良好的效果。 |
| [^49] | [A Joint Matrix Factorization Analysis of Multilingual Representations.](http://arxiv.org/abs/2310.15513) | 本论文介绍了一种基于联合矩阵分解的分析工具，用于比较多语言和单语言模型的潜在表示。对于多语言预训练模型学习到的表示，我们研究了形态句法特征的呈现程度和方式，并发现了编码形态句法信息的变化和受语言属性影响的特定类别差异。同时，我们的研究结果还展示了因式分解输出与跨语言任务性能之间的强关联。我们还公开发布了相关代码。 |
| [^50] | [KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval.](http://arxiv.org/abs/2310.15511) | 本研究评估了最先进的模型在信息检索中回答约束满足查询的能力，并引入了一个新的数据集KITAB来衡量语言模型的约束满足能力。 |
| [^51] | [TRAMS: Training-free Memory Selection for Long-range Language Modeling.](http://arxiv.org/abs/2310.15494) | TRAMS是一种训练免费的长程语言建模记忆选择策略，它能够提高Transformer架构在长程语言建模方面的效果，并且不需要额外的训练或参数。 |
| [^52] | [NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA.](http://arxiv.org/abs/2310.15484) | NuTrea是一个基于树搜索的GNN模型，用于上下文引导的多跳知识图问答。模型采用了消息传递方案来增强过去导向的嵌入，并引入了RF-IEF节点嵌入来更好地表征模糊的知识图节点。 |
| [^53] | [CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model.](http://arxiv.org/abs/2310.15477) | 本文研究了CRaSh模型，这是一种 Cluster, Remove, and Share 的技术，通过在不完整的大型语言模型上进行微调来增强其性能。从实证分析中发现，LLM层内存在独特的模块结构，并且在模型规模扩大时会出现微小但可能重要的改变。 |
| [^54] | [Continual Event Extraction with Semantic Confusion Rectification.](http://arxiv.org/abs/2310.15470) | 本文提出了一种带有语义混淆修正的持续事件提取模型，通过标注伪标签和传递关键知识来缓解事件类型语义混淆并提高模型在长尾事件类型的理解上的性能。 |
| [^55] | [The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks.](http://arxiv.org/abs/2310.15469) | 《Janus接口：大型语言模型微调如何放大隐私风险》研究了大型语言模型的微调对个人信息泄露的风险，发现了一种新的LLM利用途径。 |
| [^56] | [Interpreting Answers to Yes-No Questions in User-Generated Content.](http://arxiv.org/abs/2310.15464) | 本文介绍了一个包含4,442个Twitter是非问题答案对的新语料库，并讨论了解读为是或否答案的语言特征以及解读未知答案的问题。研究结果表明，目前的大型语言模型无法很好地解决这个难题。 |
| [^57] | [Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring.](http://arxiv.org/abs/2310.15461) | 本文研究了人机语言模型交互如何支持自主引导式心理健康干预，通过以认知重建作为案例研究。研究结果显示，我们的系统对参与者情绪强度产生积极影响，并帮助他们克服了负面思维。 |
| [^58] | [K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific Ratings.](http://arxiv.org/abs/2310.15439) | K-HATERS是一个具有目标特定评级的韩语仇恨言论检测语料库，包含大约192K条新闻评论。它是韩文最大的冒犯性语言语料库，也是首个提供目标特定评级的资源，在不同程度的冒犯性下可以检测韩语中的仇恨表达。 |
| [^59] | [What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations.](http://arxiv.org/abs/2310.15431) | 本研究提出了易推翻的道德推理任务，以了解不同背景下行为的道德可接受性，并提供常识理由来支持推理。通过迭代的自蒸馏方法，我们获得了高质量的任务数据。 |
| [^60] | [Beyond Sentiment: Leveraging Topic Metrics for Political Stance Classification.](http://arxiv.org/abs/2310.15429) | 本研究引入了主题度量作为情感度量在政治立场分类中的替代和补充，实验结果表明主题度量相比传统方法提高了连贯性分数，并在立场分类中表现出更好的性能。 |
| [^61] | [The Mason-Alberta Phonetic Segmenter: A forced alignment system based on deep neural networks and interpolation.](http://arxiv.org/abs/2310.15425) | 本文介绍了一种基于深度神经网络和插值的新型强制对齐系统，该系统名为Mason-Alberta音标分割器。该系统的创新点包括将声学模型视为标注任务，而不是分类任务，并采用了插值技术实现更精确的分段边界。 |
| [^62] | [FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions.](http://arxiv.org/abs/2310.15421) | FANToM是一个新的基准，用于通过问答在信息不对称的对话环境中压力测试机器的心智理论。这个基准对最先进的大型语言模型来说具有挑战性，即使是具有思维链推理和微调的模型也比人类表现得差。 |
| [^63] | [Let the Pretrained Language Models "Imagine" for Short Texts Topic Modeling.](http://arxiv.org/abs/2310.15420) | 本文提出了一种新的方法来解决短文本主题建模中的数据稀疏问题，通过使用预训练语言模型将短文本扩展为更长的序列。实验结果表明，该模型在极端数据稀疏情况下能够显著提高短文本主题建模的性能。 |
| [^64] | [Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation.](http://arxiv.org/abs/2310.15415) | 本研究探索了使对话模型意识到时间的想法，并提出了一个多次对话的数据集GapChat，显示出时间感知的模型在判断话题相关性和从对话中获取信息的度量标准方面表现更好。 |
| [^65] | [GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions.](http://arxiv.org/abs/2310.15405) | 本文研究了使用大型语言模型作为经济实惠的方法来评估科学图表标题，并发现GPT-4作为零-shot评估器表现优于其他模型和人工评估。 |
| [^66] | ["One-size-fits-all"? Observations and Expectations of NLG Systems Across Identity-Related Language Features.](http://arxiv.org/abs/2310.15398) | 通过扰动不同类型的身份相关语言特征，研究探索了NLG系统行为的公平中的适应性和不变性之间的紧张关系。研究结果发现，适应的动机包括社会规范、文化差异、特定特征信息和适应性；不变性的动机包括支持规定主义的观点、将适应视为不必要过程，并对错误假设持谨慎态度。这些发现突显了定义公平相关NLG系统行为的挑战。 |
| [^67] | [DoGE: Domain Reweighting with Generalization Estimation.](http://arxiv.org/abs/2310.15393) | DoGE提出了一种基于泛化估计的领域重新加权方法。通过使用梯度估计函数评估每个领域对泛化目标的贡献，重新调整了预训练数据中不同领域的采样概率。实验结果表明，该方法在提高大型语言模型的泛化能力方面取得了显著效果。 |
| [^68] | [Irreducible Curriculum for Language Model Pretraining.](http://arxiv.org/abs/2310.15389) | 本论文提出了一种不可约课程算法，用于语言模型预训练，通过优先选择具有更高学习能力的样本，并使用小规模代理模型模拟样本丢失，从而在大型语言模型上解决了传统数据选择方法的困难，并在实验证明算法能够持续改进模型性能。 |
| [^69] | [GD-COMET: A Geo-Diverse Commonsense Inference Model.](http://arxiv.org/abs/2310.15383) | GD-COMET是一种地理多样性常识推理模型，通过人工评估和外在评估证明了其能够生成具有文化细微差异的常识知识，有助于促进各种自然语言处理应用的发展，使NLP更加包容。 |
| [^70] | [Semantic Data Management in Data Lakes.](http://arxiv.org/abs/2310.15373) | 数据湖是管理大量异构数据进行现代数据分析的一种方法。为了防止数据湖成为无法操作的数据沼泽，我们可以采用语义数据管理的方法，通过将元数据与知识图谱相链接，为数据提供更多的意义和语义。这种语义层不仅可以用于数据管理，还可以解决数据整合问题，使数据访问更具表达性和互操作性。 |
| [^71] | [EpiK-Eval: Evaluation for Language Models as Epistemic Models.](http://arxiv.org/abs/2310.15372) | 这项研究介绍了一种新的评估方法EpiK-Eval，旨在评估大型语言模型（LLMs）在从分割的叙述中构建连贯和一致的知识表示方面的能力。研究发现当前的训练目标存在固有的缺陷，因此提出了改进知识整合方法的建议，以大幅提高LLMs的整体效果和性能。 |
| [^72] | [Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation.](http://arxiv.org/abs/2310.15355) | 本研究展示了LLMs产生幻觉的原因是因为它们的输出没有受到证据支持的主张的约束，并介绍了如何通过感知、内涵和外延学习来约束LLMs以生成满足证据闭合性的输出。 |
| [^73] | [Moral Foundations of Large Language Models.](http://arxiv.org/abs/2310.15337) | 本文通过使用道德基础理论（MFT）作为分析工具，研究了流行的大型语言模型（LLMs）是否对一系列特定的道德价值观产生了偏见，并展示了它们与人类道德基础和政治倾向的关联。研究还发现LLMs的偏见在不同的提示上下文中存在差异，并展示了通过对抗选择提示可以引导LLMs产生不同的回答。 |
| [^74] | [Specialist or Generalist? Instruction Tuning for Specific NLP Tasks.](http://arxiv.org/abs/2310.15326) | 本文研究了将通才模型指导调优融入到专家模型中是否有助于构建专家模型，并发现在任务覆盖广泛且任务特定的训练数据有限时，整合通才模型的指导调优能够持续提高模型性能。 |
| [^75] | [LXMERT Model Compression for Visual Question Answering.](http://arxiv.org/abs/2310.15325) | 本文通过组合大规模预训练模型的观察结果，并评估在视觉问答任务中对LXMERT进行微调时的可训练子网络，研究了LXMERT模型的压缩。实验结果表明，在仅损失3%的精度下，可以有效地通过剪枝方法将LXMERT模型大小减小40%-60%。 |
| [^76] | [Hallucination Detection for Grounded Instruction Generation.](http://arxiv.org/abs/2310.15319) | 该论文研究了生成指导人类在模拟环境中导航的说明的问题，通过预训练模型并使用对比损失进行微调，提出了一种检测幻觉参考的模型，该模型在性能上超过了几个基线模型。 |
| [^77] | [Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses.](http://arxiv.org/abs/2310.15317) | 本文探索了在入门编程课程中应用大型语言模型生成多样化的代码追踪问题，并提供了评估模型质量的人工评估指标和宝贵的数据集，为教育和自然语言处理研究提供了重要资源。 |
| [^78] | [Probing Representations for Document-level Event Extraction.](http://arxiv.org/abs/2310.15316) | 这项研究首次将探测范式应用于文档级信息抽取表示，发现训练得到的编码器嵌入可以提高参数检测和标注，但对于事件级任务的改进有限，且存在在连贯性和事件类型预测方面的权衡。 |
| [^79] | [Toward a Critical Toponymy Framework for Named Entity Recognition: A Case Study of Airbnb in New York City.](http://arxiv.org/abs/2310.15302) | 通过研究纽约市Airbnb房源数据集，本文提出了一个基于批评地名学的命名实体识别（NER）模型，能够识别与地点特征化相关的重要话语类别，为批评地名学研究指明了新的方向。 |
| [^80] | [TaskDiff: A Similarity Metric for Task-Oriented Conversations.](http://arxiv.org/abs/2310.15298) | TaskDiff是一种新颖的对话相似度度量方法，通过使用不同的对话组成部分来计算相似度，取得了优越的性能和鲁棒性。 |
| [^81] | [DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM.](http://arxiv.org/abs/2310.15296) | DeTiME是一种使用基于编码-解码的LLMs增强扩散的主题建模方法，能够产生高度聚类的嵌入和具有增强语义一致性的主题，并能生成与主题相关的内容。 |
| [^82] | [Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling.](http://arxiv.org/abs/2310.15294) | 本文提出了一种自适应端到端度量学习方法，用于解决零样本跨领域槽填充问题。通过引入上下文感知的软标签表示和槽级对比表示学习，该方法能够提高计算效率和泛化能力，减轻领域转移带来的挑战。 |
| [^83] | [On the Dimensionality of Sentence Embeddings.](http://arxiv.org/abs/2310.15285) | 本文对句子嵌入的维度进行了全面而实证的分析，证明了最佳维度通常比默认值要小，并提出了一种两步训练方法来在维度压缩时最小化性能损失。 |
| [^84] | [Efficient Algorithms for Recognizing Weighted Tree-Adjoining Languages.](http://arxiv.org/abs/2310.15276) | 本研究提出了识别加权树相邻语言的高效算法，通过定义半环加权版本的两级形式化方式，并设计新的算法来计算字符串总和和全部总和。对于线性索引文法，算法在时间和空间效率上表现更优。 |
| [^85] | [GradSim: Gradient-Based Language Grouping for Effective Multilingual Training.](http://arxiv.org/abs/2310.15269) | 在本文中，我们提出了一种基于梯度相似性的语言分组方法，名为GradSim。它通过选择最合适的语言集合进行多语言训练，避免了负面干扰，并在多个基准数据集上取得了最先进的性能表现。此外，我们的分析还揭示了数据集的主题对模型的性能也起着重要作用。 |
| [^86] | [Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey.](http://arxiv.org/abs/2310.15264) | 本文综述了AI生成文本检测的可能性和不可能性。具体而言，讨论了使用大型语言模型产生的文本可能导致的问题以及表明AI生成文本检测的意义。另外，还提到了对抗检测的策略的设计。 |
| [^87] | [Data Augmentation Techniques for Machine Translation of Code-Switched Texts: A Comparative Study.](http://arxiv.org/abs/2310.15262) | 这项研究对比了三种常用的增强方法：词汇替换、语言理论和回译，发现回译和基于CSW预测的词汇替换在机器翻译任务上表现最佳，语言理论和随机词汇替换在缺乏CSW平行数据的情况下也能达到相似的效果。 |
| [^88] | [Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards.](http://arxiv.org/abs/2310.15259) | 这项研究提出了一种解决噪声环境中问题翻译挑战的方法，通过只使用源语数据进行微调的训练，实现了翻译问题的充分性和流畅性的平衡。 |
| [^89] | [Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention.](http://arxiv.org/abs/2310.15258) | 本研究通过研究多语言语言模型（MultiLMs）在不同语言中进行推理时的细调，发现在单语言环境下它们可以传递推理能力，但在代码切换环境下难以实现推理能力的传递。基于此观察，我们提出了一种新的注意机制来鼓励跨语言推理。 |
| [^90] | [CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks.](http://arxiv.org/abs/2310.15239) | CRoW是一个手工策划的多任务基准测试，用于评估模型在真实世界NLP任务中应用常识推理的能力。该基准测试揭示了NLP系统在常识推理方面与人类之间存在显著的性能差距，表明常识推理在真实世界中仍然远未解决。 |
| [^91] | [Function Vectors in Large Language Models.](http://arxiv.org/abs/2310.15213) | 大型语言模型中存在一种简单的神经机制，将输入-输出函数表示为向量。这些函数向量在不同的上下文中具有鲁棒性，并且具有强大的因果效应。同时，它们还具有将语义向量进行组合的能力。 |
| [^92] | [DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning.](http://arxiv.org/abs/2310.15205) | 我们提出了一种基于多专家微调的金融大型语言模型DISC-FinLLM，通过赋予模型多轮问答、领域文本处理、数学计算和检索增强生成能力，我们的模型在多个金融场景中表现出更好的性能。 |
| [^93] | [Meta learning with language models: Challenges and opportunities in the classification of imbalanced text.](http://arxiv.org/abs/2310.15019) | 本文提出了一种元学习技术(MLT)，通过将不同文本表示构建的个体模型进行组合，在不平衡的文本分类中提高了性能，并通过阈值移动技术进一步改善了预测器的性能。 |
| [^94] | [Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation.](http://arxiv.org/abs/2310.14892) | 本文提出了一种名为空气解码的新颖轻量级解码框架，通过重建属性分布来平衡权重，生成更流畅的文本，以解决可控文本生成中的属性坍缩问题。 |
| [^95] | [MCC-KD: Multi-CoT Consistent Knowledge Distillation.](http://arxiv.org/abs/2310.14747) | MCC-KD方法提出了一种多CoT一致性知识蒸馏的方法，能够高效地转移大型语言模型的推理能力到较小的模型上，通过生成多个理由并确保其预测的一致性来增强推理多样性和一致性。 |
| [^96] | [A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions.](http://arxiv.org/abs/2310.14724) | 本文对LLM生成的文本检测进行了调查，强调了开发这样的检测器的必要性，并总结了近期的研究创新和未来发展方向。 |
| [^97] | [SPRING-INX: A Multilingual Indian Language Speech Corpus by SPRING Lab, IIT Madras.](http://arxiv.org/abs/2310.14654) | SPRING-INX数据是由SPRING实验室开源的多语种印度语音语料库，用于印度语音识别系统的建设。这是为了鼓励语言技术社区在22种印度官方语言中构建基于语音的应用程序而进行的努力。 |
| [^98] | [Harnessing ChatGPT for thematic analysis: Are we ready?.](http://arxiv.org/abs/2310.14545) | 本研究探讨了在医学背景下利用ChatGPT进行主题分析的应用。我们发现ChatGPT可以在直接编码转录、生成主题和预处理引用等核心阶段发挥重要作用，并提高分析效率和提供额外见解。 |
| [^99] | [Rethinking Word-Level Auto-Completion in Computer-Aided Translation.](http://arxiv.org/abs/2310.14523) | 本论文重新思考了计算辅助翻译中单词级自动补全的问题，引入了一个可衡量的标准来确定好的自动补全，提出了一种有效方法来提高性能，并在实验中证明了其优于目前最佳系统的表现。 |
| [^100] | [CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities.](http://arxiv.org/abs/2310.14512) | CorefPrompt是一种基于提示的方法，通过测量事件类型和参数的兼容性来进行事件指代消解。该方法将事件指代消解转化为一个填空式MLM任务，并通过引入辅助的提示任务来帮助模型进行推理，最终在基准测试中取得了良好的表现。 |
| [^101] | [Cultural and Linguistic Diversity Improves Visual Representations.](http://arxiv.org/abs/2310.14356) | 这项研究发现数据集和模型生成的图像描述在不同语言间存在显著的语义差异，多语言数据有更高的语义覆盖率，并且基于多语言训练的模型表现更好。 |
| [^102] | [Towards Understanding Sycophancy in Language Models.](http://arxiv.org/abs/2310.13548) | 这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。 |
| [^103] | [Ask Language Model to Clean Your Noisy Translation Data.](http://arxiv.org/abs/2310.13469) | 论文介绍了如何利用大型语言模型清理神经机器翻译中的噪声输入，通过从MTNT数据集中清理目标语句的噪声，生成了C-MTNT数据集，显著减少了噪声。 |
| [^104] | [Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining.](http://arxiv.org/abs/2310.12172) | ImageArg-2023是第一个多模态论证挖掘的共享任务，涵盖了论证立场分类和图像说服力分类两个子任务。共收到了来自6个国家的9个团队提交的31个子任务A的提交和21个子任务B的提交，最好的提交在子任务A中达到了0.8647的F1得分，在子任务B中达到了0.5561的F1得分。 |
| [^105] | [From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification.](http://arxiv.org/abs/2310.11878) | 本研究关注法律自然语言处理中人工标注的变异问题，通过收集一组律师对案件结果评估存在分歧的数据集，对这些分歧进行了研究，构建了一个两级分类体系，并发现分歧主要源于对法律背景的不明确描述。 |
| [^106] | [Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks.](http://arxiv.org/abs/2310.11398) | 本文介绍了一种利用神经网络增强自注意机制中QKV计算的方法，实验证明这种方法在多个任务中取得了显著的提升。 |
| [^107] | [VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights.](http://arxiv.org/abs/2310.11368) | VECHR是一个专家注释的多标签数据集，用于欧洲人权法院漏洞类型的可解释和鲁棒分类。该数据集帮助识别脆弱性，并提供解释理由。结果显示了该任务的挑战性，模型与专家的一致性有限，模型在处理域外数据时鲁棒性也较低。 |
| [^108] | [Utilizing Weak Supervision To Generate Indonesian Conservation Dataset.](http://arxiv.org/abs/2310.11258) | 本文展示了如何利用弱监督方法从保护新闻文本构建一种印尼自然语言处理数据集，并提供了多类别分类和情感分类的基线实验结果。此外，作者还发布了使用的数据集和标注函数供进一步研究和探索。 |
| [^109] | [TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models.](http://arxiv.org/abs/2310.10180) | TRIGO是一个基于生成语言模型的形式数学证明减缩的基准测试，要求模型不仅可以按步骤证明减少三角表达式，还可以评估其对公式的推理能力，以及操作、分组和因式分解数字项的能力。 |
| [^110] | [Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration.](http://arxiv.org/abs/2310.09168) | 通过采用探索指导的方法，使用大型语言模型 (LLMs) 进行主动探索，增强了领域特定指导调优的数据覆盖范围，并取得了显著的性能提升。 |
| [^111] | [PuoBERTa: Training and evaluation of a curated language model for Setswana.](http://arxiv.org/abs/2310.09141) | 本文介绍了一种名为PuoBERTa的定制掩码语言模型，针对塞茨瓦纳语进行训练，并证明了其在促进塞茨瓦纳语等少研究语言的自然语言处理能力方面的有效性。 |
| [^112] | [A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection.](http://arxiv.org/abs/2310.06498) | 本文提出了一种基于倒向验证的自检方法和一个名为PHD的幻觉检测基准，用于自动检测大型语言模型中的事实错误，该方法在段落级别上表现出很好的性能。 |
| [^113] | [GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence.](http://arxiv.org/abs/2310.05388) | GROVE是一种检索增强的复杂故事生成框架，通过利用优秀人类写作故事的信息和细节，能够生成具有复杂而可信的情节的故事。 |
| [^114] | [Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis.](http://arxiv.org/abs/2310.05374) | 本论文提出了一种名为LaSyn的文本数据利用框架，通过将文本数据转换为中间潜变表示来增强端到端语音处理模型的训练。在低资源环境下的语音识别和口语理解任务中，LaSyn相对词错误率减少了22.3%，绝对意图分类准确率提高了4.1%。 |
| [^115] | [Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think -- Introducing AI Detectability Index.](http://arxiv.org/abs/2310.05030) | 这篇论文介绍了一个名为反图灵测试（CT^2）的基准，旨在全面评估现有AI生成文本检测技术的稳健性。在面对生成AI的风险和后果引起关注的情况下，解决AI生成作品归属问题变得尤为重要。 |
| [^116] | [Evaluating Hallucinations in Chinese Large Language Models.](http://arxiv.org/abs/2310.03368) | 本研究评估了中文大型语言模型中的幻觉现象，通过建立HalluQA基准测试和使用GPT-4进行自动评估方法，发现18个模型的非幻觉率低于50%。研究分析了不同类型模型中的幻觉类型和原因。 |
| [^117] | [MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use.](http://arxiv.org/abs/2310.03128) | 本文提出了一个名为MetaTool的基准，旨在评估大型语言模型（LLMs）是否具有工具使用意识并且能够正确选择工具。基准中包含一个名为ToolE的数据集，其中包含各种类型的用户查询，用于触发LLMs使用工具。 |
| [^118] | [Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model.](http://arxiv.org/abs/2310.02971) | 这篇论文介绍了为自监督编码器-解码器语音模型进行提示和适配器调优的方法，并展示了在序列生成和跨语言ASR任务上的优越表现，尤其在低资源情况下提示方法优于适配器调优。 |
| [^119] | [Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation.](http://arxiv.org/abs/2310.01320) | 本研究通过使用复杂的Avalon游戏作为测试平台，引入了一种名为递归思考（ReCon）的新框架，用于增强大型语言模型（LLM）识别和对抗欺骗信息的能力。 |
| [^120] | [NJUNLP's Participation for the WMT2023 Quality Estimation Shared Task.](http://arxiv.org/abs/2309.13230) | NJUNLP团队对WMT2023质量评估共享任务进行了投稿，通过使用伪数据方法和核心超参数的实验研究，他们的模型在英德语言对的质量预测和错误跨度检测上取得了最佳结果。 |
| [^121] | [AnglE-Optimized Text Embeddings.](http://arxiv.org/abs/2309.12871) | 本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。 |
| [^122] | [AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement.](http://arxiv.org/abs/2309.08030) | 本论文提出了一种名为AV2Wav的音频-视觉语音增强方法，利用连续自监督特征和扩散模型生成干净的语音，克服了现实训练数据的挑战。与基于掩蔽的基线方法相比，该方法在声码任务上表现更好，并通过多任务训练进一步优化性能。 |
| [^123] | [Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts.](http://arxiv.org/abs/2309.07430) | 本研究通过对八个大型语言模型在临床摘要任务上的领域适应方法实验进行了全面的定量评估，发现最佳适应的模型的摘要在完整性和正确性方面优于人类摘要。 |
| [^124] | [Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO.](http://arxiv.org/abs/2309.06132) | 本文提出了一种混合方法来自动测量文本中的模糊性和主观性。通过引入专家系统VAGO，以及基于BERT-like架构的神经克隆，该方法在固定语料库和多语言生成方面表现出良好的性能。 |
| [^125] | [nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources.](http://arxiv.org/abs/2309.02373) | nanoT5是一个用于高效预训练和微调T5模型的PyTorch框架，通过优化性能和计算效率，它可以在单个GPU上在短时间内进行预训练而不损失性能。这个开源框架为更广泛的语言建模研究提供了更易用的T5实现。 |
| [^126] | [LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked.](http://arxiv.org/abs/2308.07308) | 本文提出了一种通过自检来防御大型语言模型(LLMs)对抗性攻击的简单方法，即让模型自行过滤回应。实验结果表明，即使模型未对齐人类价值观，通过使用语言模型验证内容，仍然可以防止模型向用户呈现有害内容。 |
| [^127] | [TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties.](http://arxiv.org/abs/2308.03051) | 这项研究对Bard和ChatGPT在十种阿拉伯语变体的机器翻译能力进行了评估，发现LLM在翻译方言方面表现优于商业系统，但在古典阿拉伯语和现代标准阿拉伯语方面落后于谷歌翻译等商业系统。 |
| [^128] | [MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities.](http://arxiv.org/abs/2308.02490) | MM-Vet是一个评估标准，用于评估大型多模态模型在复杂任务上的综合能力。该标准解决了如何结构化和评估复杂多模态任务、设计适用于不同问题和回答类型的评估指标以及如何提供模型洞察的问题。通过整合不同的核心视觉-语言能力，MM-Vet展示了有趣的能力和解决复杂任务的方法。 |
| [^129] | [Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty.](http://arxiv.org/abs/2308.02019) | 本文提出了一种从在小数据集上训练的教师中进行知识蒸馏的方法，并证明当教师模型在足够小的数据集上训练时，蒸馏可以保持甚至超过教师模型的性能。 |
| [^130] | [WebArena: A Realistic Web Environment for Building Autonomous Agents.](http://arxiv.org/abs/2307.13854) | WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。 |
| [^131] | [RADAR: Robust AI-Text Detection via Adversarial Learning.](http://arxiv.org/abs/2307.03838) | 本论文提出了一种名为RADAR的新框架，通过对抗性学习实现了鲁棒的AI文本检测，以解决当前AI文本检测器对于大语言模型的改写不具备鲁棒性的问题。 |
| [^132] | [Don't Trust GPT When Your Question Is Not In English.](http://arxiv.org/abs/2305.16339) | 在多语言环境下，GPT-3表现较差，特别是当问题不是用英语提出时。这与模型的训练数据和输入问题的语言差异有关。 |
| [^133] | [Contrastive Learning of Sentence Embeddings from Scratch.](http://arxiv.org/abs/2305.15077) | 该论文提出了一种从零开始的对比学习框架，利用合成数据训练句子嵌入。实验结果表明，该方法在句子相似性和重排序任务上取得了良好的效果。 |
| [^134] | [ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind.](http://arxiv.org/abs/2305.15068) | 本研究提出了一个基于原则的数据集和多样化评估任务，名为ToMChallenges，以探索心智理论。研究发现，大型语言模型在心智理论任务上表现不一致，稳定地执行任务仍然具有挑战性。 |
| [^135] | [Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation.](http://arxiv.org/abs/2305.15025) | Dior-CVAE是一种具有扩散先验的分层条件变分自编码器，通过采用预训练语言模型和扩散模型来解决变分对话生成中的多样性和后验崩溃问题。实验表明，该模型在开放领域对话中表现出了优越的性能。 |
| [^136] | [The ACL OCL Corpus: Advancing Open Science in Computational Linguistics.](http://arxiv.org/abs/2305.14996) | ACL OCL是一个学术语料库，旨在推动计算语言学领域的开放科研。它提供了丰富的元数据、PDF文件、引用图和结构化全文信息，并可以用于观察计算语言学领域的趋势。ACL OCL的贡献在于发现了"句法：标注、分块和解析"兴趣减弱，"自然语言生成"兴趣复苏的趋势。 |
| [^137] | [Dolphin: A Challenging and Diverse Benchmark for Arabic NLG.](http://arxiv.org/abs/2305.14989) | Dolphin是一个面向阿拉伯语言和变体的自然语言生成评估框架，包含了13种不同的任务，并且提供了大量多样化的数据集和测试集。它为评估阿拉伯语和多语言模型的性能和泛化能力设定了新的标准，有助于推动阿拉伯语NLG研究的发展。 |
| [^138] | [Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback.](http://arxiv.org/abs/2305.14975) | 本论文对从人类反馈的语言模型中提取置信度得分的方法进行了广泛评估。研究发现，对于RLHF-LMs，像ChatGPT、GPT-4和Claude这样的模型输出的语言化置信度通常比条件概率更好地进行了标定。 |
| [^139] | [GRACE: Discriminator-Guided Chain-of-Thought Reasoning.](http://arxiv.org/abs/2305.14934) | GRACE是一种判别器引导的思维链推理的逐步解码方法，通过使用一个正确性判别器来评分下一步候选，解决了语言模型在多步推理中容易得到错误答案的问题。在多个数学和符号推理任务中，GRACE相较于其他方法在性能上有明显的提升。 |
| [^140] | [ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games.](http://arxiv.org/abs/2305.14879) | 这项工作研究了语言模型生成科学和常识推理任务的世界模型的能力，通过生成以Python代码形式表达的文字游戏来实现。实验证明GPT-4可以使用这些游戏作为模板进行上下文学习，并引入了一套自动评估指标进行模拟逼真度的评估。 |
| [^141] | [Leveraging GPT-4 for Automatic Translation Post-Editing.](http://arxiv.org/abs/2305.14878) | GPT-4在翻译后编辑任务中表现出色，通过产生有意义且可靠的编辑，大幅提高了翻译质量并消除了各类重要错误。 |
| [^142] | [Meta-learning For Vision-and-language Cross-lingual Transfer.](http://arxiv.org/abs/2305.14843) | 本研究提出了一种元学习微调框架，通过设计跨语言多模态的MAML，使得当前的视觉-语言模型能够快速适应新的语言，从而在跨语言转移中显著提升了性能。 |
| [^143] | [Exploring the Grounding Issues in Image Caption.](http://arxiv.org/abs/2305.14616) | 该论文为我们从计算认知语言的角度出发，探讨了图像描述中的多模态语义表征中的基础问题。研究结果表明，情境含义和功能性是生成适当的图像描述的关键。 |
| [^144] | [Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer.](http://arxiv.org/abs/2305.14280) | 本文介绍了如何通过像素表示有效地训练多语言机器翻译模型，其中的创新和贡献主要体现在对多种语言和文字的表现优化、像素表示的属性探索以及实现无缝跨语言转移的数据效率。 |
| [^145] | [Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model.](http://arxiv.org/abs/2305.13999) | 这项研究提出了一个统一的框架来分析稀疏前馈网络在预训练大型语言模型中的设计选择。在语言建模任务中，通过使用平均聚合隐藏状态的选择方法，相比现有的MoE架构，可以实现更低的困惑度。 |
| [^146] | [Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction.](http://arxiv.org/abs/2305.13981) | 本文提出了第一个模拟评估开放式信息提取模型在真实世界中的基准测试，并通过判断模型在整个团体上的表现是否始终准确来评估模型的鲁棒性。 |
| [^147] | [Learn from Mistakes through Cooperative Interaction with Study Assistant.](http://arxiv.org/abs/2305.13829) | 本文提出了一个新框架 SALAM，通过协作交互与学习助手来帮助 LLM 在反思和改进过程中。该框架通过收集错误并在推理时提供指导方针，显着提高模型性能。 |
| [^148] | [Challenges in Context-Aware Neural Machine Translation.](http://arxiv.org/abs/2305.13751) | 本文研究上下文感知神经机器翻译中存在的挑战，并提出了更为真实的文档级翻译设置，段落级翻译(para2para)，以及收集了一份新的中英小说数据集，以促进未来的研究。 |
| [^149] | [Understanding compositional data augmentation in automatic morphological inflection.](http://arxiv.org/abs/2305.13658) | 本研究揭示了自动词形变化中的数据增强策略StemCorrupt带来的根本性变化，并证明选择高多样性和高预测不确定性的数据点子集是提高其数据效率的有效方法。同时，StemCorrupt能够学习可推广的词形规则。 |
| [^150] | [Instruct-Align: Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction.](http://arxiv.org/abs/2305.13627) | Instruct-Align提出了基于对齐的跨语言教学调整框架，使得教学调整的LLMs能够学习新语言，且不会发生灾难性遗忘。 |
| [^151] | [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.](http://arxiv.org/abs/2305.13245) | 该论文介绍了一种将现有的多头语言模型检查点升级为具有多查询注意力（MQA）的模型的方法，并引入了群组查询注意力（GQA）来解决MQA可能导致的质量下降问题。通过升级后的GQA模型，实现了接近多头注意力的质量，并具备与MQA相当的速度。 |
| [^152] | [Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer.](http://arxiv.org/abs/2305.13034) | 最近邻机器翻译将预训练的神经机器翻译模型与领域特定的令牌级检索相结合，实现了领域适应任务中的成功，通过在NMT的输出投影层上隐式执行梯度下降，以达到模型微调的效果。 |
| [^153] | [Distilling ChatGPT for Explainable Automated Student Answer Assessment.](http://arxiv.org/abs/2305.12962) | 本文提出了一种使用ChatGPT进行解释性的自动学生答案评估的新框架。通过采用不同的模板指导ChatGPT收集理由，修正不一致的理由以符合标准，并通过微调一个更小的语言模型，同时评估学生答案和提供理由。实验证明，该方法相对于ChatGPT将整体QWK评分提高了11%。 |
| [^154] | [A Diachronic Analysis of Paradigm Shifts in NLP Research: When, How, and Why?.](http://arxiv.org/abs/2305.12920) | 本研究提出了一个系统框架来分析自然语言处理领域研究主题的演变趋势，揭示了任务和方法是驱动研究的主要因素，而数据集和评估指标的影响较小。 |
| [^155] | [OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models.](http://arxiv.org/abs/2305.12001) | 本文探究了大型语言模型的推理能力，证明解释在模型微调过程中对性能影响不显著，但在提示模型时使用解释可提高模型在某些推理技能上的性能。 |
| [^156] | [Elaborative Simplification as Implicit Questions Under Discussion.](http://arxiv.org/abs/2305.10387) | 本文提出了一种将详细阐述视为隐含问题的明确回答的方法，通过引入ElabQUD对作者阐述信息的方式进行了研究。 |
| [^157] | [CoEdIT: Text Editing by Task-Specific Instruction Tuning.](http://arxiv.org/abs/2305.09857) | CoEdIT是一种通过任务特定指令调整实现文本编辑的最先进模型，能够提高用户生成文本的质量和提高流程的效率。 |
| [^158] | [Transfer Visual Prompt Generator across LLMs.](http://arxiv.org/abs/2305.01278) | 本论文提出将已有的轻量化视觉提示发生器连接到视觉-语言LLM以减少资源消耗的方法，并提出了跨不同大小和类型的LLMs的VPG转移方案VPGTrans，该方案在VQA和NLVR2任务中表现优秀。 |
| [^159] | [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.](http://arxiv.org/abs/2305.00586) | 本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。 |
| [^160] | [Evaluating Verifiability in Generative Search Engines.](http://arxiv.org/abs/2304.09848) | 本文评估了四个流行生成式搜索引擎的可验证性，发现现有生成式搜索引擎响应流畅但仅有51.5%的生成句子得到了完整的引用支持，仅有74.5%的引用支持其相关语句。 |
| [^161] | [Document-Level Machine Translation with Large Language Models.](http://arxiv.org/abs/2304.02210) | 本文以文档级机器翻译为试验场，深入评估了大型语言模型在语篇建模方面的性能。研究发现利用LLMs强大的长文本建模能力可以提高翻译质量，在提示方面进行改进也可以显着提高翻译质量，并且LLMs有潜力编码丰富的语篇知识。 |
| [^162] | [Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods.](http://arxiv.org/abs/2303.13988) | 本文提出了一种新领域——机器心理学，利用心理学的方法考察大型语言模型的能力。该文规范了机器心理学研究的方法论标准，并对心理实验中提示设计政策进行了探讨和制定。 |
| [^163] | [CoLT5: Faster Long-Range Transformers with Conditional Computation.](http://arxiv.org/abs/2303.09752) | CoLT5是一种基于条件计算的Transformer模型，通过优先处理重要标记来加速长距离输入的处理。CoLT5在SCROLLS基准测试上表现最好，并能够有效地处理长达64k输入长度。 |
| [^164] | [Consistency Analysis of ChatGPT.](http://arxiv.org/abs/2303.06273) | 本文研究了ChatGPT的一致性问题，发现尽管它具有更好的语言理解能力，但仍然经常无法生成逻辑上正确的预测。因此，在现实世界的应用需要进一步考虑，特别是在风险方面。 |
| [^165] | [Is ChatGPT a Good NLG Evaluator? A Preliminary Study.](http://arxiv.org/abs/2303.04048) | 通过针对任务特定和方面特定，我们在五个NLG元评估数据集上进行实验，表明ChatGPT作为NLG评估指标并不总是与人类评估相一致，尤其是在流畅度方面。这提醒人们在使用ChatGPT作为唯一的自动NLG评估指标时要谨慎。 |
| [^166] | [CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network.](http://arxiv.org/abs/2303.03387) | CoSyn是一个上下文协同的神经网络，用于检测在线对话中的隐含仇恨言论。它通过引入新的编码方法和上下文交互机制，在双曲空间中进行操作，以适应社交媒体的特点。 |
| [^167] | [Zero-Shot Cross-Lingual Summarization via Large Language Models.](http://arxiv.org/abs/2302.14229) | 本文实验性地使用各种提示来指导大型语言模型从不同的范式执行零样本跨语言摘要，并成功提高了它们的CLS性能。其中，GPT-4实现了零样本CLS的最先进性能，并且在性能方面与最佳方法相当。 |
| [^168] | [Data Selection for Language Models via Importance Resampling.](http://arxiv.org/abs/2302.03169) | 通过重要性重采样方法，我们提出了一种高效且可扩展的数据选择框架（DSIR），可以在语言模型中选择适合的预训练数据集。我们使用KL减少作为数据度量来确定合适的特征空间，并在降维特征空间中估计重要性权重以进行数据选择。 |
| [^169] | [Mo\^usai: Text-to-Music Generation with Long-Context Latent Diffusion.](http://arxiv.org/abs/2301.11757) | 本研究开发了一种高效、表现力强且能处理长期结构的文本到音乐生成模型Mo^usai，可以根据文本描述生成多分钟高质量音乐。通过实验证明了该模型在多个标准上的优势。 |
| [^170] | [Batch Prompting: Efficient Inference with Large Language Model APIs.](http://arxiv.org/abs/2301.08721) | 批量提示是一种简单但有效的方法，可以降低使用大型语言模型进行推断的计算和财务成本，同时保持下游性能。理论上证明，在少样本情况下，批量样本数量的增加几乎以倒数线性关系降低了推断成本。在多个数据集上的验证实验证明了批量提示的有效性，并且对于最先进的Chat-based LLMs，如GPT-3.5和GPT-4，批量提示也具有好处。 |
| [^171] | [T-Projection: High Quality Annotation Projection for Sequence Labeling Tasks.](http://arxiv.org/abs/2212.10548) | T-Projection是一种新的注释投影方法，利用大型预训练语言模型和先进的机器翻译技术。它将标注投影任务分解为候选生成和候选选择两个子任务，有效地生成高质量的序列标注数据。 |
| [^172] | [SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization.](http://arxiv.org/abs/2212.10465) | SODA是第一个公开发布的百万级别高质量社交对话数据集，通过从知识图谱中上下文化社交常识知识进行蒸馏，我们训练出了COSMO，其比目前最佳表现的对话模型更为自然和一致，这有助于了解知识丰富型对话和自然社交闲聊之间的差异。 |
| [^173] | [MaXM: Towards Multilingual Visual Question Answering.](http://arxiv.org/abs/2209.05401) | 本文提出了针对多语言视觉问答的可扩展解决方案，包括数据生成和建模。通过基于翻译的数据生成框架和高效的注释协议，创建了包含7种不同语言的mVQA基准MaXM。同时，开发了简单、轻量级、高效的VQA模型。鼓励进一步研究mVQA。 |
| [^174] | [SCL-RAI: Span-based Contrastive Learning with Retrieval Augmented Inference for Unlabeled Entity Problem in NER.](http://arxiv.org/abs/2209.01646) | SCL-RAI是一种应对命名实体识别中无标签实体问题的方法。它利用跨度对比学习减小了同类跨度之间的距离、增大了不同类跨度之间的距离，从而提高了对无标签实体的识别准确性。此外，通过检索增强推理，还解决了决策边界偏移问题。实验证明，该方法在真实世界数据集上明显优于之前的方法。 |
| [^175] | [JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and Reasoning.](http://arxiv.org/abs/2202.10739) | JAMES是一个用于岗位职称规范化的解决方案，它通过构建三种独特嵌入和使用协同注意机制和神经逻辑推理表示来有效地捕捉岗位职称的各种特征，并解决了语义相似性、非规范化用户创建的职称以及实际应用中大规模和长尾分布的岗位职称等挑战。 |
| [^176] | [Interpretable Sequence Classification Via Prototype Trajectory.](http://arxiv.org/abs/2007.01777) | ProtoryNet是一种基于原型轨迹的可解释深度神经网络，它通过捕捉时间模式和原型的近似程度来进行文本分类，并实现了直观和细致的推理过程解释。 |

# 详细

[^1]: BianQue: 用ChatGPT对健康LLMs进行多轮会话优化，平衡提问和建议能力

    BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT. (arXiv:2310.15896v1 [cs.CL])

    [http://arxiv.org/abs/2310.15896](http://arxiv.org/abs/2310.15896)

    BianQue是一种基于ChatGLM进行微调的LLMs模型，旨在提高LLMs的多轮提问能力，以平衡提问和建议能力，并提供更加个性化和有针对性的健康建议。

    

    大型语言模型（LLMs）在单轮会话中提供广泛的健康建议方面表现出色，例如ChatGPT、ChatGLM、ChatDoctor、DoctorGLM等系统。然而，用户在单轮中提供的信息有限，导致生成的建议个性化和针对性不足，需要用户独立选择有用的部分。这主要是由于缺乏参与多轮提问的能力。为了改进LLMs的多轮提问能力，我们提出了BianQue，一种基于ChatGLM的LLMs模型，通过自建的健康对话数据集BianQueCorpus进行微调，该数据集包含多轮提问和健康建议的内容。

    Large language models (LLMs) have performed well in providing general and extensive health suggestions in single-turn conversations, exemplified by systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning. In real-world medical consultations, doctors usually employ a series of iterative inquiries to comprehend the patient's condition thoroughly, enabling them to provide effective and personalized suggestions subsequently, which can be defined as chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose BianQue, a ChatGLM-based LLM finetuned with the self-constructed health conversation dataset BianQueCorpus that is consist of multiple turns of questioning and health sugge
    
[^2]: 使用人工法语数据，了解Transformer语言模型中性别偏见的出现

    Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models. (arXiv:2310.15852v1 [cs.CL])

    [http://arxiv.org/abs/2310.15852](http://arxiv.org/abs/2310.15852)

    本文使用人工法语数据生成的语料库，探索了神经语言模型如何发现单词的性别属性以及其使用规则，并研究了模型在不同条件下是否正确捕捉到性别信息或表现出性别偏见。

    

    许多研究已经证明神经语言模型在没有直接监督的情况下能够学习各种语言属性。本文首次着手探索神经模型如何发现单词的语言属性（如性别）以及规则的使用方法，这是一个相对少有研究的话题。我们提出使用由基于法语的PCFG生成的人工语料库，精确控制训练数据中的性别分布，并确定模型在哪种条件下能正确捕捉到性别信息，或者相反，显示出性别偏见。

    Numerous studies have demonstrated the ability of neural language models to learn various linguistic properties without direct supervision. This work takes an initial step towards exploring the less researched topic of how neural models discover linguistic properties of words, such as gender, as well as the rules governing their usage. We propose to use an artificial corpus generated by a PCFG based on French to precisely control the gender distribution in the training data and determine under which conditions a model correctly captures gender information or, on the contrary, appears gender-biased.
    
[^3]: 自我防御：增强LLM的自我保护能力

    Self-Guard: Empower the LLM to Safeguard Itself. (arXiv:2310.15851v1 [cs.CL])

    [http://arxiv.org/abs/2310.15851](http://arxiv.org/abs/2310.15851)

    这篇论文提出了一种称为自我防御的新方法，通过结合安全训练和保护措施的优势，提升大型语言模型（LLM）的安全性，从而减少有害内容的生成。

    

    盗破攻击可以绕过大型语言模型（LLM）的安全措施，生成有害内容。这种滥用LLM的行为导致了负面的社会后果。目前，解决盗破攻击的主要方法有两种：安全训练和保护措施。安全训练侧重于进一步训练LLM以增强其安全性。而保护措施则是通过实施外部模型或过滤器来防止有害输出。然而，安全训练在适应新的攻击类型方面具有局限性，并且往往会导致模型性能下降。保护措施在帮助方面也被证明有限。为了解决这些问题，我们提出了一种称为自我防御的新方法，结合了安全方法的优势。自我防御包括两个阶段。在第一阶段，我们增强了模型评估有害内容的能力，在第二阶段，我们指导模型在自己的回应上始终执行有害内容检测。实验结果表明，我们的方法能够显著减少有害内容的生成，提高模型的安全性。

    The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content. This misuse of LLM has led to negative societal consequences. Currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. Safety training focuses on further training LLM to enhance its safety. On the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. However, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help. To tackle these issues, we propose a novel approach called Self-Guard, which combines the strengths of both safety methods. Self-Guard includes two stages. In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. The experimen
    
[^4]: 一种用于新意图发现的扩散加权图框架

    A Diffusion Weighted Graph Framework for New Intent Discovery. (arXiv:2310.15836v1 [cs.CL])

    [http://arxiv.org/abs/2310.15836](http://arxiv.org/abs/2310.15836)

    本研究提出了一种名为Diffusion Weighted Graph Framework (DWGF)的方法，用于捕捉数据中的语义相似性和结构关系，从而实现更充分和可靠的监督信号，解决了以往方法在新意图发现中无法平衡数量和质量的问题。

    

    新意图发现旨在通过有限的带有已知意图的标记数据的帮助，识别出未标记数据中的新意图和已知意图。以前的方法未考虑样本之间的结构关系，生成的噪声监督信号无法在数量和质量之间取得平衡，阻碍了新意图聚类的形成和预训练知识的有效传递。为了缓解这一限制，我们提出了一种新颖的扩散加权图框架（DWGF），以捕捉数据中的语义相似性和结构关系，从而实现更充分和可靠的监督信号。具体而言，对于每个样本，我们沿着由最近邻指导的语义路径扩散邻域关系，以鉴别地刻画其局部结构。然后，我们根据语义相似性和局部结构对其正样本进行抽样和加权，用于对比学习。

    New Intent Discovery (NID) aims to recognize both new and known intents from unlabeled data with the aid of limited labeled data containing only known intents. Without considering structure relationships between samples, previous methods generate noisy supervisory signals which cannot strike a balance between quantity and quality, hindering the formation of new intent clusters and effective transfer of the pre-training knowledge. To mitigate this limitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to capture both semantic similarities and structure relationships inherent in data, enabling more sufficient and reliable supervisory signals. Specifically, for each sample, we diffuse neighborhood relationships along semantic paths guided by the nearest neighbors for multiple hops to characterize its local structure discriminately. Then, we sample its positive keys and weigh them based on semantic similarities and local structures for contrastive learning. During inferen
    
[^5]: 非自然语言处理：语言模型处理机器生成的提示的方式。

    Unnatural language processing: How do language models handle machine-generated prompts?. (arXiv:2310.15829v1 [cs.CL])

    [http://arxiv.org/abs/2310.15829](http://arxiv.org/abs/2310.15829)

    通过使用机器生成的提示，研究人员探索了语言模型对非自然语言输入的响应。他们发现，即使产生了类似的输出，机器生成的和人工生成的提示会触发不同的响应模式。这项研究为我们初步揭示了提示的性质。

    

    语言模型的提示优化研究显示，语义上和语法上构造良好的手动制定的提示常常被无明显含义或句法结构的自动生成的令牌序列所超越，包括来自模型嵌入空间的向量序列。我们使用机器生成的提示来探索模型对非自然语言输入的响应。我们研究了不同大小模型在多个语义任务中对连续和离散的机器生成提示的行为，并将其与对人工生成的自然语言提示的行为进行了比较。即使产生了类似的输出，机器生成的和人工提示通过网络处理路径引发了不同的响应模式，包括不同的困惑度、注意力和输出熵分布，以及不同的单元激活配置文件。我们初步揭示了提示的性质。

    Language model prompt optimization research has shown that semantically and grammatically well-formed manually crafted prompts are routinely outperformed by automatically generated token sequences with no apparent meaning or syntactic structure, including sequences of vectors from a model's embedding space. We use machine-generated prompts to probe how models respond to input that is not composed of natural language expressions. We study the behavior of models of different sizes in multiple semantic tasks in response to both continuous and discrete machine-generated prompts, and compare it to the behavior in response to human-generated natural-language prompts. Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways, including different perplexities, different attention and output entropy distributions, and different unit activation profiles. We provide preliminary insight into the nature of t
    
[^6]: Rosetta Stone在KSAA-RD共享任务中：从语言建模到词--定义对齐的跃进。

    Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word--Definition Alignment. (arXiv:2310.15823v1 [cs.CL])

    [http://arxiv.org/abs/2310.15823](http://arxiv.org/abs/2310.15823)

    本论文介绍了在KSAA-RD共享任务中Rosetta Stone的应用，将语言建模应用到词--定义对齐中。论文通过使用一组微调的阿拉伯BERT模型来预测给定定义的词嵌入，从而实现了阿拉伯词的向量表示。

    

    反向词典是一种工具，可根据提供的定义、含义或描述来发现一个词。这种技术在各种场景中都非常有价值，可以帮助掌握一个词的描述而不知其身份的语言学习者，并使寻求精确术语的写作者受益。这些场景通常涵盖被称为“舌尖上的词”现象。在这项工作中，我们呈现了我们在阿拉伯语反向词典共享任务中获胜的解决方案。该任务的重点是从伴随的描述中推导出阿拉伯词的向量表示。共享任务包括两个不同的子任务：第一个子任务涉及一个阿拉伯定义作为输入，而第二个子任务则使用一个英文定义。对于第一个子任务，我们的方法依赖于一组经过微调的阿拉伯BERT模型，来预测给定定义的词嵌入。最终表示是通过对每个模型输出的嵌入进行平均得到的。

    A Reverse Dictionary is a tool enabling users to discover a word based on its provided definition, meaning, or description. Such a technique proves valuable in various scenarios, aiding language learners who possess a description of a word without its identity, and benefiting writers seeking precise terminology. These scenarios often encapsulate what is referred to as the "Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning solution for the Arabic Reverse Dictionary shared task. This task focuses on deriving a vector representation of an Arabic word from its accompanying description. The shared task encompasses two distinct subtasks: the first involves an Arabic definition as input, while the second employs an English definition. For the first subtask, our approach relies on an ensemble of finetuned Arabic BERT-based models, predicting the word embedding for a given definition. The final representation is obtained through averaging the output embeddings from each m
    
[^7]: 生成式语言模型展示社会身份偏见

    Generative Language Models Exhibit Social Identity Biases. (arXiv:2310.15819v1 [cs.CL])

    [http://arxiv.org/abs/2310.15819](http://arxiv.org/abs/2310.15819)

    该研究调查了51个大型语言模型展示的社会身份偏见，发现几乎所有模型在补全句子时都展示了明显的团体内积极和团体外消极的偏见。与人类文本相比，这些模型展示了类似或更大程度的偏见。

    

    大型语言模型的流行引发了人们对这些模型可能从人类中学到的偏见的担忧。在这项研究中，我们调查了51个大型语言模型是否展示了社会科学中已知的团体内团结和团体外敌对的基本社会偏见。我们发现，几乎所有基础语言模型和一些指令细调模型在被要求补全句子（例如，“我们是...”）时都展示了明显的团体内积极和团体外消极的偏见。将LLM生成的句子与互联网上人类撰写的句子进行比较表明，这些模型展示了与人类文本相似的甚至更大程度的偏见。为了查明这些偏见的根源，我们在美国民主党和共和党分裂的背景下实验性地变化了模型在细调过程中暴露给团体内积极或团体外消极句子的数量。结果，模型展示出明显的偏见增加。

    The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans. In this study, we investigate whether ingroup solidarity and outgroup hostility, fundamental social biases known from social science, are present in 51 large language models. We find that almost all foundational language models and some instruction fine-tuned models exhibit clear ingroup-positive and outgroup-negative biases when prompted to complete sentences (e.g., "We are..."). A comparison of LLM-generated sentences with human-written sentences on the internet reveals that these models exhibit similar level, if not greater, levels of bias than human text. To investigate where these biases stem from, we experimentally varied the amount of ingroup-positive or outgroup-negative sentences the model was exposed to during fine-tuning in the context of the United States Democrat-Republican divide. Doing so resulted in the models exhibiting a marked increase i
    
[^8]: DALE: 用于低资源法律NLP的生成式数据增强

    DALE: Generative Data Augmentation for Low-Resource Legal NLP. (arXiv:2310.15799v1 [cs.CL])

    [http://arxiv.org/abs/2310.15799](http://arxiv.org/abs/2310.15799)

    DALE是一个用于低资源法律NLP的生成式数据增强框架，通过基于选择性掩码的无监督文本去噪目标预训练，在解决法律语言特异性的同时生成连贯且多样化的增强。

    

    我们提出了DALE，一个针对低资源法律NLP的新颖有效的生成式数据增强框架。DALE解决了现有框架在生成法律文件的有效数据增强方面存在的挑战 - 法律语言具有专门的词汇和复杂的语义、形态和句法，并不能从仅仅对源句子进行改述的数据增强中受益。为了解决这个问题，DALE是基于编码器-解码器语言模型构建的，它在一种新颖的无监督文本去噪目标上进行了预训练，该目标基于选择性掩码 - 我们的掩码策略利用模板化法律文件的领域特定语言特征来掩盖文本的连续范围。去噪这些范围有助于DALE获取关于法律概念、原则和语言使用的知识。因此，它具备了生成连贯且多样化的增强以及新颖上下文的能力。最后，DALE进行条件生成，用于生成合成增强。

    We present DALE, a novel and effective generative Data Augmentation framework for low-resource LEgal NLP. DALE addresses the challenges existing frameworks pose in generating effective data augmentations of legal documents - legal language, with its specialized vocabulary and complex semantics, morphology, and syntax, does not benefit from data augmentations that merely rephrase the source sentence. To address this, DALE, built on an Encoder-Decoder Language Model, is pre-trained on a novel unsupervised text denoising objective based on selective masking - our masking strategy exploits the domain-specific language characteristics of templatized legal documents to mask collocated spans of text. Denoising these spans helps DALE acquire knowledge about legal concepts, principles, and language usage. Consequently, it develops the ability to generate coherent and diverse augmentations with novel contexts. Finally, DALE performs conditional generation to generate synthetic augmentations for 
    
[^9]: 随机实体量化用于参数高效的组合知识图谱表示

    Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation. (arXiv:2310.15797v1 [cs.AI])

    [http://arxiv.org/abs/2310.15797](http://arxiv.org/abs/2310.15797)

    本文研究了参数高效的组合知识图谱表示的问题，通过随机实体量化的方法，可以达到与当前策略类似的效果，这是因为随机实体量化下，实体码有更高的熵和码字级别的Jaccard距离，使得不同实体更容易区分，从而有效地表示知识图谱。

    

    知识图谱（KG）上的表示学习对下游任务至关重要。主导方法KG嵌入（KGE）通过独立向量表示实体，面临可扩展性挑战。最近的研究提出了一种参数效率的替代方法，通过从预定义的小规模码书中匹配实体对应的码字来表示实体。我们将获取每个实体对应码字的过程称为实体量化，先前的工作设计了复杂的策略。令人惊讶的是，本文表明简单的随机实体量化可以实现与当前策略类似的结果。我们分析了这种现象并揭示了在随机实体量化下，表示实体的量化结果-实体码具有更高的熵和码字级别的Jaccard距离。因此，不同实体更容易区分，有助于有效的KG表示。

    Representation Learning on Knowledge Graphs (KGs) is essential for downstream tasks. The dominant approach, KG Embedding (KGE), represents entities with independent vectors and faces the scalability challenge. Recent studies propose an alternative way for parameter efficiency, which represents entities by composing entity-corresponding codewords matched from predefined small-scale codebooks. We refer to the process of obtaining corresponding codewords of each entity as entity quantization, for which previous works have designed complicated strategies. Surprisingly, this paper shows that simple random entity quantization can achieve similar results to current strategies. We analyze this phenomenon and reveal that entity codes, the quantization outcomes for expressing entities, have higher entropy at the code level and Jaccard distance at the codeword level under random entity quantization. Therefore, different entities become more easily distinguished, facilitating effective KG represen
    
[^10]: 通过学习前缀子空间改进大型语言模型的泛化能力

    Improving generalization in large language models by learning prefix subspaces. (arXiv:2310.15793v1 [cs.LG])

    [http://arxiv.org/abs/2310.15793](http://arxiv.org/abs/2310.15793)

    本文提出了一种通过学习前缀子空间来改进大型语言模型的泛化能力的方法。我们通过联合优化模型参数空间中的整个单纯形模型，在稀缺数据环境中实现了更广的局部最优解。这种方法在预训练变换器模型中表现出了很好的兼容性和有效性。

    

    本文关注于大型语言模型（LLMs）在稀缺数据环境中的微调（也被称为“少样本”学习设置）。我们提出了一种基于神经网络子空间的方法来增加LLMs的泛化能力。这种优化方法最近在计算机视觉领域中引入，旨在通过在参数空间中的整个单纯形模型的联合优化，识别更广的局部最优解，从而提高模型的泛化能力。然而，将其适应于大规模预训练变换器模型则面临一些挑战。首先，它们大量的参数使得联合训练多个模型变得困难，其次，它们的确定性参数初始化方案使其不适用于最初的子空间方法。我们在本文中展示，“参数高效微调”（PEFT）方法与最初的方法完全兼容，并提出学习整个连续前缀的单纯形。我们在实验证明了这个方法在大型语言模型的泛化上的有效性。

    This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as the "few-shot" learning setting). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in computer vision, aims to improve model generalization by identifying wider local optima through the joint optimization of an entire simplex of models in parameter space. Its adaptation to massive, pretrained transformers, however, poses some challenges. First, their considerable number of parameters makes it difficult to train several models jointly, and second, their deterministic parameter initialization schemes make them unfit for the subspace method as originally proposed. We show in this paper that "Parameter Efficient Fine-Tuning" (PEFT) methods, however, are perfectly compatible with this original approach, and propose to learn entire simplex of continuous prefixes. We test our method on 
    
[^11]: MindLLM: 从零开始预训练轻量级大型语言模型，评估和领域应用

    MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications. (arXiv:2310.15777v1 [cs.CL])

    [http://arxiv.org/abs/2310.15777](http://arxiv.org/abs/2310.15777)

    本文提出了一种从零开始训练的轻量级大型语言模型MindLLM，通过提供1.3亿和3亿参数的模型，减轻了训练和部署大型语言模型的成本和资源稀缺性的压力。MindLLM在各个步骤中给出了经验教训，包括数据构建、模型架构、评估和应用，对学术界和开发者来说具有重要价值。

    

    大型语言模型（LLM）在各种自然语言任务中展现出了出色的性能，标志着通往通用人工智能的重要进展。虽然通用人工智能是通过开发越来越大规模的模型来实现的，但还有另一种分支，即开发轻量级定制模型，以更好地服务某些领域，考虑到训练和部署LLM的高成本和资源的稀缺性。在本文中，我们提出MindLLM，一系列新颖的双语轻量级大型语言模型，从零开始训练，通过提供13亿和30亿参数的模型来减轻这些负担。给出了在大模型开发过程中积累的经验全面的分析，包括数据构建、模型架构、评估和应用。这些见解对学者和开发者来说有价值。MindLLM始终能够与甚至超过最先进的模型性能。

    Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the p
    
[^12]: BLESS:对大型语言模型在句子简化任务上的性能进行基准测试

    BLESS: Benchmarking Large Language Models on Sentence Simplification. (arXiv:2310.15773v1 [cs.CL])

    [http://arxiv.org/abs/2310.15773](http://arxiv.org/abs/2310.15773)

    BLESS是一个对大型语言模型在句子简化任务上进行基准测试的项目，评测了44个模型在不同领域的三个测试集上的性能，并发现即使未经过句子简化的训练，最好的模型性能与最新的简化任务基线相当，并且某些模型展示了更广泛和多样化的编辑操作。

    

    我们提出了BLESS，这是一个全面的性能基准测试，针对最新的大型语言模型（LLMs）在文本简化任务上的表现。我们评估了共计44个模型在不同领域（维基百科、新闻和医学）的三个测试集上，涉及模型的大小、架构、预训练方法和可访问性等方面。我们的分析考虑了一系列自动评估指标，同时还进行了大规模的定量研究，探究了不同模型执行的常见编辑操作的类型。此外，我们对部分模型输出进行了手动质量分析，以更好地评估生成的简化质量。我们的评估结果表明，即使未经过句子简化的训练，最好的LLMs在性能上与最新的简化任务基线相当。此外，我们发现某些LLMs展示出更广泛和多样化的编辑操作。

    We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art large language models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS, perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. O
    
[^13]: 从自由文本的人类反馈中学习 - 收集新数据集还是扩展现有数据集？

    Learning From Free-Text Human Feedback -- Collect New Datasets Or Extend Existing Ones?. (arXiv:2310.15758v1 [cs.CL])

    [http://arxiv.org/abs/2310.15758](http://arxiv.org/abs/2310.15758)

    本研究调查了常用对话数据集中自由文本人类反馈的类型和频率，并提出了用于注释这些反馈的新分类法。通过将这些数据用于回答生成，我们研究了对GPT-2、LLAMA和Fla等语言生成模型的影响。

    

    从自由文本的人类反馈中学习对于对话系统是必要的，但是标注的数据稀缺，并且通常只涵盖了对话人工智能中已知错误类型的一小部分。与其从头开始收集和注释新的数据集，最近在合成对话生成方面的先进技术可以用于扩充现有的对话数据集以获得必要的注释。然而，为了评估这种努力的可行性，了解这些数据集中包含的各种类型和频率的自由文本人类反馈是很重要的。在这项工作中，我们对各种常用的对话数据集进行了调查，包括 MultiWoZ、SGD、BABI、PersonaChat、Wizards-of-Wikipedia 和 Self-Feeding Chatbot 的人类与机器对话分开的部分。根据我们的观察，我们为对话中的自由文本人类反馈的注释提出了新的分类法，并研究了将这些数据包含在三种先进的语言生成模型（包括GPT-2、LLAMA和Fla）的回答生成中的影响。

    Learning from free-text human feedback is essential for dialog systems, but annotated data is scarce and usually covers only a small fraction of error types known in conversational AI. Instead of collecting and annotating new datasets from scratch, recent advances in synthetic dialog generation could be used to augment existing dialog datasets with the necessary annotations. However, to assess the feasibility of such an effort, it is important to know the types and frequency of free-text human feedback included in these datasets. In this work, we investigate this question for a variety of commonly used dialog datasets, including MultiWoZ, SGD, BABI, PersonaChat, Wizards-of-Wikipedia, and the human-bot split of the Self-Feeding Chatbot. Using our observations, we derive new taxonomies for the annotation of free-text human feedback in dialogs and investigate the impact of including such data in response generation for three SOTA language generation models, including GPT-2, LLAMA, and Fla
    
[^14]: 在线讨论中的价值差异是否影响分歧？

    Do Differences in Values Influence Disagreements in Online Discussions?. (arXiv:2310.15757v1 [cs.CL])

    [http://arxiv.org/abs/2310.15757](http://arxiv.org/abs/2310.15757)

    本论文研究了在线讨论中的分歧，发现个人价值观差异与分歧有关，探讨了注入价值观信息对一致性预测的改进。

    

    在线讨论中的分歧是很常见的，然而在现有文献中对影响分歧的因素缺乏深入的理解。本研究探讨了个人价值观差异是否与在线讨论中的分歧有关。我们使用先进的模型估计了在线讨论中的价值观，并将估计值聚合为价值观配置文件。通过人工标注的一致性标签对估计的价值观配置文件进行评估。我们发现在特定情况下，价值观配置文件的不相似性与分歧有关。同时发现在一致性预测中加入价值信息可以提高性能。

    Disagreements are common in online discussions. Disagreement may foster collaboration and improve the quality of a discussion under some conditions. Although there exist methods for recognizing disagreement, a deeper understanding of factors that influence disagreement is lacking in the literature. We investigate a hypothesis that differences in personal values are indicative of disagreement in online discussions. We show how state-of-the-art models can be used for estimating values in online discussions and how the estimated values can be aggregated into value profiles. We evaluate the estimated value profiles based on human-annotated agreement labels. We find that the dissimilarity of value profiles correlates with disagreement in specific cases. We also find that including value information in agreement prediction improves performance.
    
[^15]: 将语言模型集成到直接语音翻译中：控制性别变化的推断时间解决方案

    Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection. (arXiv:2310.15752v1 [cs.CL])

    [http://arxiv.org/abs/2310.15752](http://arxiv.org/abs/2310.15752)

    该论文提出了一种推断时间解决方案，用于在语音翻译中控制与说话者相关的性别变化。通过部分替换内部语言模型，将特定性别的外部语言模型应用于翻译过程，实验证明这一方法在性别准确性方面优于基准模型和最佳训练时间缓解策略，尤其在具有性别冲突的条件下效果显著提高。

    

    在翻译与说话者相关的词语时，语音翻译系统应该避免使用默认的男性泛用词，也不应依赖可能具有误导性的声音特征。相反，它们应该根据说话者的偏好来确定性别。现有的解决方案虽然有效，但在实践中很难实现，因为它们涉及对带有性别标签的语音翻译数据进行专门的模型重新训练。为了克服这些局限性，我们提出了第一个能够推断控制与说话者相关的性别变化的解决方案。我们的方法部分替换了语音翻译解码器隐含学习的（有偏见的）内部语言模型（LM），使用了特定性别的外部LM。通过对en->es/fr/it的实验结果表明，我们的解决方案在性别准确性方面优于基准模型和最佳训练时间缓解策略，对女性形式的提高分别达到了31.0和1.6个百分点。在具有挑战性的条件中，说话者的声音特征引发了性别冲突，这些增益甚至更大（分别为32.0和3.4）。

    When translating words referring to the speaker, speech translation (ST) systems should not resort to default masculine generics nor rely on potentially misleading vocal traits. Rather, they should assign gender according to the speakers' preference. The existing solutions to do so, though effective, are hardly feasible in practice as they involve dedicated model re-training on gender-labeled ST data. To overcome these limitations, we propose the first inference-time solution to control speaker-related gender inflections in ST. Our approach partially replaces the (biased) internal language model (LM) implicitly learned by the ST decoder with gender-specific external LMs. Experiments on en->es/fr/it show that our solution outperforms the base models and the best training-time mitigation strategy by up to 31.0 and 1.6 points in gender accuracy, respectively, for feminine forms. The gains are even larger (up to 32.0 and 3.4) in the challenging condition where speakers' vocal traits confli
    
[^16]: 失败指引之路：通过无调优规则累积增强大型语言模型

    Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation. (arXiv:2310.15746v1 [cs.CL])

    [http://arxiv.org/abs/2310.15746](http://arxiv.org/abs/2310.15746)

    本论文提出了一种无调优规则累积（TRAN）框架，通过从以前的错误中学习来指导大型语言模型（LLMs）改善性能。实验证明，TRAN相较于最近的基准线有很大的改进。

    

    大型语言模型(LLM)展现出了令人印象深刻的性能。然而，由于无法捕捉样本之间的关系，这些静态的LLM不可避免地不断重复相似的错误。在这项工作中，我们提出了无调优规则累积（TRAN）框架，通过从以前的错误中学习来指导LLM改善其性能。考虑到数据是顺序到达的，LLM逐渐积累了从错误案例中得到的规则，形成了一个规则集合。然后，LLM在处理后续输入时利用这些规则避免犯类似的错误。此外，规则与主要提示无关，无缝补充了提示设计策略。实验证明，TRAN相较于最近的基准线有很大的改进。

    Large Language Models (LLMs) have showcased impressive performance. However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes. In this work, we propose our Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving their performance by learning from previous mistakes. Considering data arrives sequentially, LLMs gradually accumulate rules from incorrect cases, forming a rule collection. These rules are then utilized by the LLMs to avoid making similar mistakes when processing subsequent inputs. Moreover, the rules remain independent of the primary prompts, seamlessly complementing prompt design strategies. Experimentally, we show that TRAN improves over recent baselines by a large margin.
    
[^17]: RAPL: 一种关系感知的少样本文档级关系抽取原型学习方法

    RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction. (arXiv:2310.15743v1 [cs.CL])

    [http://arxiv.org/abs/2310.15743](http://arxiv.org/abs/2310.15743)

    这篇论文提出了一种关系感知的原型学习方法，用于解决少样本文档级关系抽取中关于类别原型的准确性问题。

    

    在只有少量有标签的文档可用时，如何确定文档中实体之间的语义关系？少样本文档级关系抽取(FSDLRE)对于解决现实场景中普遍存在的数据稀缺问题至关重要。基于度量的元学习是广泛采用的FSDLRE框架，它构建了用于分类的类别原型。然而，现有的工作通常难以获取具有准确关系语义的类别原型：1)为了构建目标关系类型的原型，它们聚合了所有具有该关系的实体对的表示，而这些实体对可能还具有其他关系，从而影响了原型的准确性。2)它们在所有任务中使用一组通用的NOTA(none-of-the-above)原型，忽视了在具有不同目标关系类型的任务中NOTA语义的差异。在本文中，我们提出了一种关系感知的原型学习方法，用于强化FSDLRE的关系语义。

    How to identify semantic relations among entities in a document when only a few labeled documents are available? Few-shot document-level relation extraction (FSDLRE) is crucial for addressing the pervasive data scarcity problem in real-world scenarios. Metric-based meta-learning is an effective framework widely adopted for FSDLRE, which constructs class prototypes for classification. However, existing works often struggle to obtain class prototypes with accurate relational semantics: 1) To build prototype for a target relation type, they aggregate the representations of all entity pairs holding that relation, while these entity pairs may also hold other relations, thus disturbing the prototype. 2) They use a set of generic NOTA (none-of-the-above) prototypes across all tasks, neglecting that the NOTA semantics differs in tasks with different target relation types. In this paper, we propose a relation-aware prototype learning method for FSDLRE to strengthen the relational semantics of p
    
[^18]: Variator: 使用即插即用压缩模块加速预训练模型

    Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules. (arXiv:2310.15724v1 [cs.CL])

    [http://arxiv.org/abs/2310.15724](http://arxiv.org/abs/2310.15724)

    这个论文提出了一种称为Variator的加速方法，通过即插即用的压缩插件增强了预训练模型的计算效率，并且可以根据工作负载动态选择不同加速比的插件。插件采用了压缩隐藏向量的方法来减小序列长度，并且由于参数少，可以节省存储和内存开销。

    

    预训练语言模型（PLMs）在自然语言处理任务上取得了显著的成果，但代价是巨大的参数大小和随之而来的计算成本。本文提出了Variator，一种参数高效的加速方法，通过即插即用的压缩插件增强计算效率。压缩插件通过将多个隐藏向量压缩到一个向量来缩减序列长度，并与原始PLMs一起进行训练。与传统的模型加速方法不同，Variator具有两个独特的优点：（1）在现实世界应用中，我们的压缩插件的即插即用性质使得可以根据当前工作负载动态选择具有不同加速比的压缩插件。（2）压缩插件由几个紧凑的神经网络层组成，参数很少，大大节省了存储和内存开销，特别是在具有较大存储需求和内存需求的场景下。

    Pre-trained language models (PLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original PLMs frozen. Different from traditional model acceleration methods, which compress PLMs to smaller sizes, Variator offers two distinct advantages: (1) In real-world applications, the plug-and-play nature of our compression plugins enables dynamic selection of different compression plugins with varying acceleration ratios based on the current workload. (2) The compression plugin comprises a few compact neural network layers with minimal parameters, significantly saving storage and memory overhead, particularly in scenarios with a gro
    
[^19]: Re-Temp：面向时间知识图谱完成的关系感知时态表示学习

    Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion. (arXiv:2310.15722v1 [cs.CL])

    [http://arxiv.org/abs/2310.15722](http://arxiv.org/abs/2310.15722)

    Re-Temp是一个能够根据实体关系跳过不相关快照并利用显式时间信息进行预测的模型，通过在多个TKGC数据集上的评估，我们证明了它的有效性。

    

    在外推设置下的时间知识图谱完成（TKGC）旨在预测未来事实中的缺失实体，更接近于实际预测问题的挑战。现有研究主要利用应用于最近快照的顺序图神经网络来编码实体和关系。然而，这些方法往往忽视了根据查询中与实体相关的关系跳过不相关的快照的能力，并且忽视了显式的时间信息的重要性。为了解决这个问题，我们提出了我们的模型Re-Temp（关系感知时态表示学习），它利用显式的时间嵌入作为输入，并在每一个时间戳之后引入跳过信息流来跳过预测中不必要的信息。此外，我们引入了一个两阶段的前向传播方法来防止信息泄漏。通过在六个TKGC（外推）数据集上的评估，我们证明了我们的模型的性能优于现有方法。

    Temporal Knowledge Graph Completion (TKGC) under the extrapolation setting aims to predict the missing entity from a fact in the future, posing a challenge that aligns more closely with real-world prediction problems. Existing research mostly encodes entities and relations using sequential graph neural networks applied to recent snapshots. However, these approaches tend to overlook the ability to skip irrelevant snapshots according to entity-related relations in the query and disregard the importance of explicit temporal information. To address this, we propose our model, Re-Temp (Relation-Aware Temporal Representation Learning), which leverages explicit temporal embedding as input and incorporates skip information flow after each timestamp to skip unnecessary information for prediction. Additionally, we introduce a two-phase forward propagation method to prevent information leakage. Through the evaluation on six TKGC (extrapolation) datasets, we demonstrate that our model outperforms 
    
[^20]: 用于大脑编码的任务特定语言模型集成

    Ensemble of Task-Specific Language Models for Brain Encoding. (arXiv:2310.15720v1 [cs.CL])

    [http://arxiv.org/abs/2310.15720](http://arxiv.org/abs/2310.15720)

    该论文提出了一种用于大脑编码的任务特定语言模型集成方法，通过将10个流行的语言模型进行集成，相较于当前基准线，平均提高了10%的性能。

    

    先前的研究表明，语言模型足够丰富，可以编码我们大脑中特定兴趣区域的fMRI激活情况。先前的工作已经探索了从为流行的自然语言处理任务学习的表示向预测大脑响应的转移学习。我们的工作通过创建一个由10个流行的语言模型（2个句法和8个语义）组成的集成模型，提高了这样的编码器的性能。通过我们的集成方法，在所有兴趣区域中，我们将当前的基准线提高了平均10%。

    Language models have been shown to be rich enough to encode fMRI activations of certain Regions of Interest in our Brains. Previous works have explored transfer learning from representations learned for popular natural language processing tasks for predicting brain responses. In our work, we improve the performance of such encoders by creating an ensemble model out of 10 popular Language Models (2 syntactic and 8 semantic). We beat the current baselines by 10% on average across all ROIs through our ensembling methods.
    
[^21]: 用外部知识图增强生物医学科普总结

    Enhancing Biomedical Lay Summarisation with External Knowledge Graphs. (arXiv:2310.15702v1 [cs.CL])

    [http://arxiv.org/abs/2310.15702](http://arxiv.org/abs/2310.15702)

    本研究通过增加文章特定的知识图，改进了生物医学科普总结的自动方法。实验证实，整合基于图的领域知识可以显著提高科普总结的可读性和解释性。

    

    以往的自动科普总结方法完全依赖于源文章，但这些文章针对技术观众（例如研究人员）编写，不太可能明确定义所有技术概念或提供适合科普观众的全部背景信息。我们通过为现有的生物医学科普总结数据集eLife增加特定文章的知识图来解决这个问题，每个知识图都包含有关相关生物医学概念的详细信息。通过自动评估和人工评估，我们系统地研究了将知识图整合到科普总结模型中的三种不同方法的有效性，每种方法针对编码器-解码器模型架构的不同领域。我们的结果证实，整合基于图的领域知识可以显著改善科普总结的可读性，大大提高生成文本的解释性。

    Previous approaches for automatic lay summarisation are exclusively reliant on the source article that, given it is written for a technical audience (e.g., researchers), is unlikely to explicitly define all technical concepts or state all of the background information that is relevant for a lay audience. We address this issue by augmenting eLife, an existing biomedical lay summarisation dataset, with article-specific knowledge graphs, each containing detailed information on relevant biomedical concepts. Using both automatic and human evaluations, we systematically investigate the effectiveness of three different approaches for incorporating knowledge graphs within lay summarisation models, with each method targeting a distinct area of the encoder-decoder model architecture. Our results confirm that integrating graph-based domain knowledge can significantly benefit lay summarisation by substantially increasing the readability of generated text and improving the explanation of technical 
    
[^22]: COPF: 通过最优策略拟合实现持续学习人类偏好

    COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])

    [http://arxiv.org/abs/2310.15694](http://arxiv.org/abs/2310.15694)

    通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。

    

    强化学习通过人类反馈（RLHF）的技术是改善预训练语言模型（LM）以符合人类偏好的常用方法。然而，当前基于RLHF的LM在引入新的查询或反馈时需要完全重新训练，这是一项具有挑战性的任务，因为人类偏好在不同领域或任务之间可能会有所变化。由于所需的时间和计算资源以及与数据隐私相关的问题，重新训练LM在许多现实世界的情况下存在实际困难。为了解决这个限制，我们提出了一种新的方法，称为持续最优策略拟合（COPF），其中我们使用蒙特卡罗法估计一系列最优策略，然后通过函数正则化不断拟合策略序列。COPF包含一个单一的学习阶段，不需要复杂的强化学习。

    The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
    
[^23]: 通过半监督学习实现自动菜谱类型分类

    Towards Automated Recipe Genre Classification using Semi-Supervised Learning. (arXiv:2310.15693v1 [cs.CL])

    [http://arxiv.org/abs/2310.15693](http://arxiv.org/abs/2310.15693)

    该论文提出了一个名为3A2M+的自动菜谱类型分类方法，通过半监督学习利用扩展的命名实体识别列表对烹饪食谱进行分类。研究者提供了一个包含两百万个带有各种特征和九个不同类型标签的烹饪食谱数据集，以解决缺乏标注数据的问题。

    

    分享烹饪食谱是交流烹饪创意和提供食物制作指引的好方法。然而，将在线找到的原始食谱分类为适当的食物类型可能会很具有挑战性，因为缺乏足够的标注数据。在本研究中，我们提出了一个名为“混合、典型和注释的扩展两百万（3A2M+）烹饪食谱数据集”的数据集，其中包含两百万个被标记为相应类别的烹饪食谱，并提取了食谱描述中的扩展命名实体。这个数据集包括标题、命名实体识别、步骤和扩展命名实体等各种特征，以及代表糕点、饮料、非素食、蔬菜、快餐、谷物、主食、配菜和融合的九个不同类型的标签。所提出的名为3A2M+的流程通过使用两种命名实体识别工具，扩展了命名实体识别（NER）列表的大小，以解决从食谱步骤中缺少的命名实体，如加热、时间或过程。

    Sharing cooking recipes is a great way to exchange culinary ideas and provide instructions for food preparation. However, categorizing raw recipes found online into appropriate food genres can be challenging due to a lack of adequate labeled data. In this study, we present a dataset named the ``Assorted, Archetypal, and Annotated Two Million Extended (3A2M+) Cooking Recipe Dataset" that contains two million culinary recipes labeled in respective categories with extended named entities extracted from recipe descriptions. This collection of data includes various features such as title, NER, directions, and extended NER, as well as nine different labels representing genres including bakery, drinks, non-veg, vegetables, fast food, cereals, meals, sides, and fusions. The proposed pipeline named 3A2M+ extends the size of the Named Entity Recognition (NER) list to address missing named entities like heat, time or process from the recipe directions using two NER extraction tools. 3A2M+ dataset
    
[^24]: 为专利简化创建一个银标准

    Creating a silver standard for patent simplification. (arXiv:2310.15689v1 [cs.CL])

    [http://arxiv.org/abs/2310.15689](http://arxiv.org/abs/2310.15689)

    本文提出了一种通过改写自动简化专利文本的方法，并成功构建了一个银标准语料库用于训练简化系统。

    

    专利是旨在一方面保护发明，另一方面促进技术知识流通的法律文件。由于其复杂的风格，即法律、技术和极度模糊的语言的混合，使得人类和机器难以获取其内容，并对信息检索社区提出了重大挑战。本文提出了一种通过改写自动简化专利文本的方法。由于缺乏领域内并行的简化数据，我们提出了一种自动生成大规模专利句子银标准的方法。为了获得候选句子，我们使用了一个通用域的改写系统；然而，这个过程容易出错且难以控制。因此，我们将其与适当的过滤器配对，并构建了一个更干净的语料库，可以成功用于训练一个简化系统。对合成银标准语料库的人工评估表明，它被认为是符合语法、适度且包含简单句子的。

    Patents are legal documents that aim at protecting inventions on the one hand and at making technical knowledge circulate on the other. Their complex style -- a mix of legal, technical, and extremely vague language -- makes their content hard to access for humans and machines and poses substantial challenges to the information retrieval community. This paper proposes an approach to automatically simplify patent text through rephrasing. Since no in-domain parallel simplification data exist, we propose a method to automatically generate a large-scale silver standard for patent sentences. To obtain candidates, we use a general-domain paraphrasing system; however, the process is error-prone and difficult to control. Thus, we pair it with proper filters and construct a cleaner corpus that can successfully be used to train a simplification system. Human evaluation of the synthetic silver corpus shows that it is considered grammatical, adequate, and contains simple sentences.
    
[^25]: 利用引文文献中的知识聚合改进生物医学摘要总结

    Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers. (arXiv:2310.15684v1 [cs.CL])

    [http://arxiv.org/abs/2310.15684](http://arxiv.org/abs/2310.15684)

    该论文提出了一种基于引文聚合的模型，通过整合引文论文中的领域特定知识，提高了生物医学摘要总结的语言模型的性能。

    

    生物医学文献中的摘要具有特定的领域特征，包括专门的写作风格和生物医学术语，这要求对相关文献有深入的理解。因此，现有的语言模型在生成与生物医学专家相媲美的技术摘要时存在困难，缺乏领域特定的背景知识。本论文旨在通过聚合源论文中引用的外部论文中的知识，提高生物医学摘要总结的语言模型的性能。我们提出了一种新颖的基于注意力的引文聚合模型，它将引文论文中的领域特定知识与论文内容相结合，使神经网络能够利用论文内容和引文论文中的相关知识生成摘要。此外，我们还构建并发布了一个大规模的生物医学摘要总结数据集作为我们研究的基础。

    Abstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in biomedical abstractive summarisation by aggregating knowledge from external papers cited within the source article. We propose a novel attention-based citation aggregation model that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from citation papers. Furthermore, we construct and release a large-scale biomedical summarisation dataset that serves as a foundation for our research. Extensiv
    
[^26]: 大型语言模型在众包工作中的使用率和预防措施

    Prevalence and prevention of large language model use in crowd work. (arXiv:2310.15683v1 [cs.CL])

    [http://arxiv.org/abs/2310.15683](http://arxiv.org/abs/2310.15683)

    该论文研究了众包工作者使用大型语言模型（LLM）的使用率，并探讨了预防措施。研究发现，通过要求工作者不使用LLM并增加使用成本可显著降低LLM的使用率，但无法完全消除。然而，防止LLM的使用可能影响到获得高质量回答。这些发现对于关注人类行为和众包数据训练的未来模型具有重要意义。

    

    我们展示了大型语言模型（LLMs）在众包工作中的普遍使用，并且证明有针对性的缓解策略可以显著降低LLM的使用率，但无法完全消除。在一个文本摘要任务中，工作者没有任何关于LLM使用的指示，估计LLM使用率约为30%，但通过要求工作者不使用LLM并增加使用成本（例如禁止复制粘贴），LLM使用率减少了约一半。进一步的分析揭示了关于LLM使用及其预防的更多见解：LLM使用产生高质量但同质化的回答，这可能损害那些关注人类（而不是模型）行为的研究，并且降低对众包数据训练的未来模型的质量。同时，防止LLM的使用可能与获得高质量的回答存在矛盾；例如，在请求工作者不使用LLM时，摘要中包含的关键信息较少。我们的估计可能会随着LLM的普及或取消而改变。

    We show that the use of large language models (LLMs) is prevalent among crowd workers, and that targeted mitigation strategies can significantly reduce, but not eliminate, LLM use. On a text summarization task where workers were not directed in any way regarding their LLM use, the estimated prevalence of LLM use was around 30%, but was reduced by about half by asking workers to not use LLMs and by raising the cost of using them, e.g., by disabling copy-pasting. Secondary analyses give further insight into LLM use and its prevention: LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data. At the same time, preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use LLMs, summaries contained fewer keywords carrying essential information. Our estimates will likely change as LLMs increase in popularity or ca
    
[^27]: 我的基于注意力的ASR系统需要多少上下文信息？

    How Much Context Does My Attention-Based ASR System Need?. (arXiv:2310.15672v1 [cs.CL])

    [http://arxiv.org/abs/2310.15672](http://arxiv.org/abs/2310.15672)

    本研究考察了对于音频识别任务，训练和评估使用不同长度的序列对语音识别性能的影响。结果表明，使用大约80秒的声学上下文进行训练可以相对提高14.9%的性能，并且与当前最先进的方法具有竞争力。

    

    对于语音识别任务，使用超过30秒的声学上下文进行训练在文献中是不常见的，并且得到了很少的研究。在这项工作中，我们研究了训练/评估（基于密集注意力的）声学模型和语言模型时，序列长度的缩放对语音识别性能的影响。我们使用了大约100,000个伪标记的Spotify播客数据集进行了这些实验，探索了5秒到1小时的上下文长度。对长格式数据集Earnings-22和Tedlium进行了零-shot评估，结果表明使用大约80秒的声学上下文进行训练可以带来高达14.9%相对改进。此外，我们通过束搜索使用长上下文变换语言模型与系统组合形成了一个全长上下文ASR系统，其结果与当前最先进的方法具有竞争力。

    For the task of speech recognition, the use of more than 30 seconds of acoustic context during training is uncommon, and under-investigated in literature. In this work, we examine the effect of scaling the sequence length used to train/evaluate (dense-attention based) acoustic and language models on speech recognition performance. For these experiments a dataset of roughly 100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5 seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets Earnings-22 and Tedlium demonstrate a benefit from training with around 80 seconds of acoustic context, showing up to a 14.9% relative improvement from a limited context baseline. Furthermore, we perform a system combination with long-context transformer language models via beam search for a fully long-context ASR system, with results that are competitive with the current state-of-the-art.
    
[^28]: 数学问题的表达式语法信息瓶颈

    Expression Syntax Information Bottleneck for Math Word Problems. (arXiv:2310.15664v1 [cs.CL])

    [http://arxiv.org/abs/2310.15664](http://arxiv.org/abs/2310.15664)

    本文提出了一种通过变分信息瓶颈从数学问题的文本中提取关键特征的方法，同时去除冗余信息。该方法通过相互学习，鼓励多个模型为同一个问题的不同表述预测相同的表达式语法树，从而捕捉一致的信息并去除冗余。

    

    数学问题的表达式语法信息瓶颈 (ESIB) 方法旨在通过变分信息瓶颈从数学问题的文本中提取关键特征，同时过滤掉包含不相关特征的冗余信息。ESIB的关键思想是通过相互学习来鼓励多个模型为同一个问题的不同表述预测相同的表达式语法树，从而捕捉一致的表达式语法树信息，并去除具有特定语法无关特征的冗余部分。

    Math Word Problems (MWP) aims to automatically solve mathematical questions given in texts. Previous studies tend to design complex models to capture additional information in the original text so as to enable the model to gain more comprehensive features. In this paper, we turn our attention in the opposite direction, and work on how to discard redundant features containing spurious correlations for MWP. To this end, we design an Expression Syntax Information Bottleneck method for MWP (called ESIB) based on variational information bottleneck, which extracts essential features of expression syntax tree while filtering latent-specific redundancy containing syntax-irrelevant features. The key idea of ESIB is to encourage multiple models to predict the same expression syntax tree for different problem representations of the same problem by mutual learning so as to capture consistent information of expression syntax tree and discard latent-specific redundancy. To improve the generalization
    
[^29]: LLMs生成内容检测综述

    A Survey on Detection of LLMs-Generated Content. (arXiv:2310.15654v1 [cs.CL])

    [http://arxiv.org/abs/2310.15654](http://arxiv.org/abs/2310.15654)

    该论文是关于LLMs生成内容检测的综述，提供了现有策略和挑战的概述，并提倡采用更灵活和强大的模型以提高检测准确性，并强调使用多方面的方法来应对不同攻击。这是首个综合调查LLMs时代检测的工作。

    

    先进的大型语言模型（LLMs）如ChatGPT的不断发展，导致合成内容生成不断增加，涉及媒体、网络安全、公共话语和教育等多个领域。因此，检测LLMs生成内容的能力变得至关重要。我们旨在提供现有检测策略和基准的详细概述，审查它们的差异，并确定领域中的关键挑战和前景，提倡采用更灵活和强大的模型以提高检测准确性。我们还主张采用多方面的方法应对不同攻击，以抵御LLMs不断发展的能力。据我们所知，这项工作是LLMs时代检测的首个综合调查。我们希望它能够提供对LLMs生成内容检测当前情况的广泛理解，并为研究该领域提供参考。

    The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for res
    
[^30]: CoAnnotating：人类和大型语言模型在数据注释中的不确定性引导工作分配

    CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation. (arXiv:2310.15638v1 [cs.CL])

    [http://arxiv.org/abs/2310.15638](http://arxiv.org/abs/2310.15638)

    CoAnnotating是一种新颖的人类-LLM联合注释框架，利用不确定性来估计LLMs的能力，并能在不同数据集上获得高达21%的性能提升。

    

    在自然语言处理（NLP）中，标注数据在训练模型和评估性能方面起着关键作用。鉴于大型语言模型（LLMs）的最新发展，像ChatGPT这样的模型在许多文本注释任务上展现出了零 shot 能力，与人类注释者相比甚至超过了人类。由于成本较低且可扩展性较高，这样的LLMs可以作为手动标注的替代品。然而，目前尚未有工作利用LLMs作为补充注释者，也没有探索如何最佳分配人类和LLMs的注释工作以实现质量和成本的目标。我们提出了CoAnnotating，一种新颖的人类-LLM联合注释范式，用于大规模非结构化文本的注释。在这个框架下，我们利用不确定性来估计LLMs的注释能力。我们的实证研究表明，CoAnnotating是一种有效的工作分配方式，能够在不同数据集上提高高达21%的性能，相比随机基线。

    Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementa
    
[^31]: 使用简历表示学习和基于技能匹配的职业路径预测

    Career Path Prediction using Resume Representation Learning and Skill-based Matching. (arXiv:2310.15636v1 [cs.CL])

    [http://arxiv.org/abs/2310.15636](http://arxiv.org/abs/2310.15636)

    本论文在职业路径预测中提出了一种使用简历表示学习和基于技能匹配的方法，通过研究文本描述部分来预测下一步的职业动向，并在数据集上验证了该方法的有效性。

    

    人-职业匹配对工作满意度和工作绩效的影响被广泛承认，这凸显了在职业生涯中为工作者提供下一步行动的重要性。职业路径预测是预测职业生涯中的下一步行动的任务，并具有员工流失预防和内部岗位流动等多种应用。现有的职业路径预测方法依赖于大量的私人职业历史数据来建模职位和公司之间的交互作用。我们提出利用简历中的工作经历部分的未开发的文本描述。我们介绍了一个由2,164个匿名化职业经历组成的结构化数据集，并带有ESCO职业标签。基于这个数据集，我们提出了一个专门为工作历史数据设计的新颖表示学习方法CareerBERT。我们开发了基于技能和基于文本的模型来进行职业路径预测，在@10下，技能模型和文本模型实现了35.24%和39.61%的召回率。

    The impact of person-job fit on job satisfaction and performance is widely acknowledged, which highlights the importance of providing workers with next steps at the right time in their career. This task of predicting the next step in a career is known as career path prediction, and has diverse applications such as turnover prevention and internal job mobility. Existing methods to career path prediction rely on large amounts of private career history data to model the interactions between job titles and companies. We propose leveraging the unexplored textual descriptions that are part of work experience sections in resumes. We introduce a structured dataset of 2,164 anonymized career histories, annotated with ESCO occupation labels. Based on this dataset, we present a novel representation learning approach, CareerBERT, specifically designed for work history data. We develop a skill-based model and a text-based model for career path prediction, which achieve 35.24% and 39.61% recall@10 r
    
[^32]: 在语言设计、库或垃圾收集方面充分利用64位体系结构的技巧

    Tips for making the most of 64-bit architectures in langage design, libraries or garbage collection. (arXiv:2310.15632v1 [cs.CL])

    [http://arxiv.org/abs/2310.15632](http://arxiv.org/abs/2310.15632)

    该论文介绍了如何在语言设计、库和垃圾收集方面充分利用64位体系结构的技巧，通过实现多精度整数、简化UTF-8字符串索引以及增强垃圾收集器的功能，提高了计算性能和节省了内存空间。

    

    如今成为标准的64位体系结构提供了前所未有的低级编程可能性。在计算历史上，地址寄存器的大小首次远远超过其总线的物理容量。在简要回顾了与可用64位相比地址的大小所提供的可能性之后，我们开发了三个具体的例子来说明如何利用这些寄存器中的空闲位。其中两个例子涉及实现一个新的静态类型编程语言的库。首先，是多精度整数的实现，旨在提高计算速度和节省RAM的性能。第二个例子关注于库对UTF-8字符串的处理，目标是通过忽略每个UTF-8字符的物理大小使索引更加方便。最后，第三个例子是对垃圾收集器的可能增强，特别是mark \& sweep。

    The 64-bit architectures that have become standard today offer unprecedented low-level programming possibilities. For the first time in the history of computing, the size of address registers far exceeded the physical capacity of their bus.After a brief reminder of the possibilities offered by the small size of addresses compared to the available 64 bits,we develop three concrete examples of how the vacant bits of these registers can be used.Among these examples, two of them concern the implementation of a library for a new statically typed programming language.Firstly, the implementation of multi-precision integers, with the aim of improving performance in terms of both calculation speed and RAM savings.The second example focuses on the library's handling of UTF-8 character strings.Here, the idea is to make indexing easier by ignoring the physical size of each UTF-8 characters.Finally, the third example is a possible enhancement of garbage collectors, in particular the mark \& sweep f
    
[^33]: Nko语的机器翻译：工具、语料库和基准结果

    Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])

    [http://arxiv.org/abs/2310.15612](http://arxiv.org/abs/2310.15612)

    该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。

    

    目前，尼科语（一种在多个西非国家使用的语言）没有可用的机器翻译系统，但它在文化和教育价值上具有重要意义。为了解决这个问题，我们提出了一套工具、资源和基准结果，旨在开发可用的尼科语和其他当前没有足够大的平行文本语料库的语言的机器翻译系统。具体包括：(1) Friallel：一种新颖的协作平行文本整理软件，通过基于副本编辑的工作流程实现质量控制。(2) 扩展了FLoRes-200和NLLB-Seed语料库，从其他语言中与尼科语平行翻译了2,009和6,193个高质量的文本。(3) nicolingua-0005：包含130,850个平行片段的三语和双语语料库，以及超过3百万尼科语单语言语料库。(4) 基线双语和多语言神经机器翻译结果与b...

    Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
    
[^34]: MUSER: 一个多视角相似案例检索数据集

    MUSER: A Multi-View Similar Case Retrieval Dataset. (arXiv:2310.15602v1 [cs.CL])

    [http://arxiv.org/abs/2310.15602](http://arxiv.org/abs/2310.15602)

    MUSER是一个基于多视角相似度测量和句子级法律要素注释的相似案例检索数据集，旨在实现准确评估和专业知识的考量。

    

    相似案例检索（SCR）是促进司法公正的典型法律人工智能应用。然而，现有的SCR数据集只关注事实描述部分来判断案例的相似性，忽略了其他有价值的部分（例如法院的意见），这些部分可以提供深入的推理过程。此外，案例的相似度通常只通过事实描述的文本语义来衡量，可能无法从法律知识的角度捕捉到法律案例的全部复杂性。在这项工作中，我们提出了MUSER，这是一个基于Multi-View相似度测量和句子级法律要素注释的相似案例检索数据集。具体而言，我们选择了三个视角（法律事实、争议焦点、法律法规），并为每个视角构建了一个全面和结构化的法律要素标签模式，以实现案例的准确评估和专业知识的考量。

    Similar case retrieval (SCR) is a representative legal AI application that plays a pivotal role in promoting judicial fairness. However, existing SCR datasets only focus on the fact description section when judging the similarity between cases, ignoring other valuable sections (e.g., the court's opinion) that can provide insightful reasoning process behind. Furthermore, the case similarities are typically measured solely by the textual semantics of the fact descriptions, which may fail to capture the full complexity of legal cases from the perspective of legal knowledge. In this work, we present MUSER, a similar case retrieval dataset based on multi-view similarity measurement and comprehensive legal element with sentence-level legal element annotations. Specifically, we select three perspectives (legal fact, dispute focus, and law statutory) and build a comprehensive and structured label schema of legal elements for each of them, to enable accurate and knowledgeable evaluation of case
    
[^35]: 检索式知识转移：一种高效的极大规模语言模型压缩方法

    Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression. (arXiv:2310.15594v1 [cs.CL])

    [http://arxiv.org/abs/2310.15594](http://arxiv.org/abs/2310.15594)

    检索式知识转移（RetriKT）是一种新的压缩范例，它通过提取大规模预训练语言模型的知识并利用 retrieval-based 的方法，将这些知识应用于极小规模的模型中，从而实现了极端的模型压缩效果。

    

    大规模预训练语言模型在各种自然语言处理任务中展现出卓越的性能。然而，这些模型的巨大规模给它们在实际应用中的部署带来了巨大挑战。尽管已经提出了许多模型压缩技术，但对于在模型规模存在显著差距时实现极端模型压缩并不适用。在本文中，我们引入了一种新的压缩范例，称为检索式知识转移（RetriKT），它将LLM的知识有效地转移到极小规模的模型（例如1%）。具体而言，我们的方法从LLM中提取知识构建知识存储，并从中检索相关信息，利用它进行有效的推理。为了提高模型的质量，我们采用了软提示调整和近端策略优化（PPO）增强学习技术。进行了广泛的实验验证。

    Large-scale pre-trained language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, the massive size of these models poses huge challenges for their deployment in real-world applications. While numerous model compression techniques have been proposed, most of them are not well-suited for achieving extreme model compression when there is a significant gap in model scale. In this paper, we introduce a novel compression paradigm called Retrieval-based Knowledge Transfer (RetriKT), which effectively transfers the knowledge of LLMs to extremely small-scale models (e.g., 1%). In particular, our approach extracts knowledge from LLMs to construct a knowledge store, from which the small-scale model can retrieve relevant information and leverage it for effective inference. To improve the quality of the model, soft prompt tuning and Proximal Policy Optimization (PPO) reinforcement learning techniques are employed. Extensive experim
    
[^36]: ScanDL: 一种用于生成文本中合成扫视路径的扩散模型

    ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts. (arXiv:2310.15587v1 [cs.CL])

    [http://arxiv.org/abs/2310.15587](http://arxiv.org/abs/2310.15587)

    这篇论文提出了一种用于生成文本中合成扫视路径的扩散模型（ScanDL），以解决眼动数据稀缺和不可用的问题。

    

    阅读中的眼动在研究人类语言处理的认知机制方面起着至关重要的作用。最近，眼动和认知之间的紧密联系也被用于语言相关的机器学习任务，如语言模型的可解释性、增强和预训练，以及读者和文本特定属性的推断。然而，眼动数据的稀缺性和在应用时的不可用性对这一领域的研究构成了重大挑战。最初，研究人员通过采用认知模型合成眼动数据来解决这个问题。然而，为了生成类似于人类的扫视路径，纯数据驱动的基于机器学习的方法更为适用。基于最近在将扩散过程适应于离散数据方面的进展，我们提出了ScanDL，一种新颖的离散序列到序列扩散模型，用于生成合成的扫视路径。

    Eye movements in reading play a crucial role in psycholinguistic research studying the cognitive mechanisms underlying human language processing. More recently, the tight coupling between eye movements and cognition has also been leveraged for language-related machine learning tasks such as the interpretability, enhancement, and pre-training of language models, as well as the inference of reader- and text-specific properties. However, scarcity of eye movement data and its unavailability at application time poses a major challenge for this line of research. Initially, this problem was tackled by resorting to cognitive models for synthesizing eye movement data. However, for the sole purpose of generating human-like scanpaths, purely data-driven machine-learning-based methods have proven to be more suitable. Following recent advances in adapting diffusion processes to discrete data, we propose ScanDL, a novel discrete sequence-to-sequence diffusion model that generates synthetic scanpaths
    
[^37]: 教师引导的组合视觉推理的多模态表示

    Multimodal Representations for Teacher-Guided Compositional Visual Reasoning. (arXiv:2310.15585v1 [cs.CL])

    [http://arxiv.org/abs/2310.15585](http://arxiv.org/abs/2310.15585)

    本论文提出了一种教师引导的多模态表示方法，通过利用大规模跨模态编码器的特征来改进神经模块网络(NMN)，并引入了计划的教师引导的学习策略，以减少误差积累并提高性能。

    

    神经模块网络（NMN）是一种有吸引力的视觉问答方法，可以将问题翻译为由一系列推理子任务组成的程序，这些子任务按顺序在图像上执行以产生答案。与集成模型相比，NMN提供了更强的解释性，可以更好地理解底层推理过程。为了提高NMN的效果，我们提出利用大规模跨模态编码器获得的特征。此外，目前的NMN训练方法依赖于将模块输出传播到后续模块，导致预测误差累积和误生成答案。为了缓解这个问题，我们引入了一种NMN学习策略，涉及计划的教师引导。最初，模型完全由地面真实的中间输出引导，但随着训练的进行逐渐过渡到自主行为。这减少了误差积累，从而改善了性能。

    Neural Module Networks (NMN) are a compelling method for visual question answering, enabling the translation of a question into a program consisting of a series of reasoning sub-tasks that are sequentially executed on the image to produce an answer. NMNs provide enhanced explainability compared to integrated models, allowing for a better understanding of the underlying reasoning process. To improve the effectiveness of NMNs we propose to exploit features obtained by a large-scale cross-modal encoder. Also, the current training approach of NMNs relies on the propagation of module outputs to subsequent modules, leading to the accumulation of prediction errors and the generation of false answers. To mitigate this, we introduce an NMN learning strategy involving scheduled teacher guidance. Initially, the model is fully guided by the ground-truth intermediate outputs, but gradually transitions to an autonomous behavior as training progresses. This reduces error accumulation, thus improving 
    
[^38]: CONTRASTE: 一种带有基于方面的提示的监督对比预训练用于方面情感三元组抽取

    CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction. (arXiv:2310.15577v1 [cs.CL])

    [http://arxiv.org/abs/2310.15577](http://arxiv.org/abs/2310.15577)

    CONTRASTE是一种利用对比学习的预训练策略，通过设计基于方面的提示并应用对比学习来增强方面情感三元组抽取（ASTE）任务的性能，并在其他ABSA任务上展示出优势。

    

    现有的方面情感三元组抽取（ASTE）的研究主要关注如何开发更高效的微调技术。相反，我们的动机是提出一种通用方法，可以同时改善多个ABSA任务的效果。为此，我们提出了一种名为CONTRASTE的新型预训练策略，利用对比学习来增强ASTE的性能。我们除了主要关注ASTE之外，还展示了我们提出的技术在其他ABSA任务（如ACOS，TASD和AESC）上的优势。给定一个句子及其相关的（方面，观点，情感）三元组，首先，我们设计了基于方面的提示，并屏蔽了相应的情感。然后，我们通过对解码器生成的方面感知情感表示进行对比学习来（预）训练编码-解码模型。为了微调得到的模型权重，我们提出了一种新颖的多任务方法，其中基础编码解码模型同时被用于ASTG和其他ABSA任务。

    Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus on developing more efficient fine-tuning techniques for the task. Instead, our motivation is to come up with a generic approach that can improve the downstream performances of multiple ABSA tasks simultaneously. Towards this, we present CONTRASTE, a novel pre-training strategy using CONTRastive learning to enhance the ASTE performance. While we primarily focus on ASTE, we also demonstrate the advantage of our proposed technique on other ABSA tasks such as ACOS, TASD, and AESC. Given a sentence and its associated (aspect, opinion, sentiment) triplets, first, we design aspect-based prompts with corresponding sentiments masked. We then (pre)train an encoder-decoder model by applying contrastive learning on the decoder-generated aspect-aware sentiment representations of the masked terms. For fine-tuning the model weights thus obtained, we then propose a novel multi-task approach where the base encoder-decoder mod
    
[^39]: POE: 多项选择推理的排除过程

    POE: Process of Elimination for Multiple Choice Reasoning. (arXiv:2310.15575v1 [cs.CL])

    [http://arxiv.org/abs/2310.15575](http://arxiv.org/abs/2310.15575)

    POE是一种两步策略的评分方法，通过排除看似错误的选项，提高了语言模型在多项选择推理任务上的表现。该方法在逻辑推理任务上表现特别好，并适用于少样本设置和大语言模型。

    

    语言模型（LMs）能够进行多项选择推理任务的上下文学习，但是这些任务中的选项被平等对待。由于人类往往会在选择最终正确答案之前先排除错误的选项，我们认为类似的两步策略能够使LMs在这些任务中表现更好。为此，我们提出了排除过程（POE），一种两步评分方法。在第一步中，POE评分每个选项，并排除看似错误的选项。在第二步中，POE屏蔽这些错误的选项，并从剩余的选项中进行最终预测。对8个推理任务的零样本实验证明了POE的有效性，并随后的分析发现我们的方法在逻辑推理任务上表现特别好。我们进一步分析了屏蔽效果，并展示了POE适用于少样本设置和大语言模型（LLMs）如ChatGPT。

    Language models (LMs) are capable of conducting in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally. As humans often first eliminate wrong options before picking the final correct answer, we argue a similar two-step strategy can make LMs better at these tasks. To this end, we present the Process of Elimination (POE), a two-step scoring method. In the first step, POE scores each option, and eliminates seemingly wrong options. In the second step, POE masks these wrong options, and makes the final prediction from the remaining options. Zero-shot experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a following analysis finds our method to be especially performant on logical reasoning tasks. We further analyze the effect of masks, and show that POE applies to few-shot settings and large language models (LLMs) like ChatGPT.
    
[^40]: 药物发现知识图谱的自然语言处理：优势与陷阱

    Natural Language Processing for Drug Discovery Knowledge Graphs: promises and pitfalls. (arXiv:2310.15572v1 [cs.CL])

    [http://arxiv.org/abs/2310.15572](http://arxiv.org/abs/2310.15572)

    本文讨论了将自然语言处理（NLP）应用于药物发现知识图谱的优势和陷阱。NLP可以从大量文档中自动提取数据，为知识图谱的构建和分析提供了便利。然而，NLP-KG流程中存在一些潜在的问题，如命名实体的错误识别和处理。

    

    构建和分析知识图谱（KG）以辅助药物发现是研究的一个热门领域。KG的一个显著特点是其能够以有利于发现关联的格式，结合许多异构数据源。KG的实用性已在药物重新作用等领域得到体现，通过对数据进行手动探索和建模来获得洞见。本文讨论了使用自然语言处理（NLP）从科学文献等非结构化文本中挖掘数据作为KG数据源的优势与陷阱。这是基于我们最初解析结构化数据源（如ChEMBL）作为KG数据的基础，并借助NLP进行丰富或扩展的经验。NLP对KG的基本优势在于自动提取来自数百万文档的数据，这是仅通过人工策展无法实现的任务。然而，在NLP-KG流程中存在许多潜在陷阱，如错误的命名实体。

    Building and analysing knowledge graphs (KGs) to aid drug discovery is a topical area of research. A salient feature of KGs is their ability to combine many heterogeneous data sources in a format that facilitates discovering connections. The utility of KGs has been exemplified in areas such as drug repurposing, with insights made through manual exploration and modelling of the data. In this article, we discuss promises and pitfalls of using natural language processing (NLP) to mine unstructured text typically from scientific literature as a data source for KGs. This draws on our experience of initially parsing structured data sources such as ChEMBL as the basis for data within a KG, and then enriching or expanding upon them using NLP. The fundamental promise of NLP for KGs is the automated extraction of data from millions of documents a task practically impossible to do via human curation alone. However, there are many potential pitfalls in NLP-KG pipelines such as incorrect named enti
    
[^41]: 可视化基础的持续语言学习与选择性专业化

    Visually Grounded Continual Language Learning with Selective Specialization. (arXiv:2310.15571v1 [cs.CL])

    [http://arxiv.org/abs/2310.15571](http://arxiv.org/abs/2310.15571)

    这项工作旨在对可视化基础的持续语言学习的选择策略进行广泛分析。通过引入两个新的诊断数据集，作者为模型分析提供了足够的控制和灵活性。评估了各种启发式的模块专业化策略以及...

    

    在视觉世界中行动的人工智能代理的一个理想特性是在平衡每项任务的充分专业化和构建一般化知识以进行传递的同时，持续学习一系列语言驱动的任务。选择性专业化，即在每个任务中精心选择模型组件进行专业化，是控制这种权衡的策略。然而，设计选择策略需要对每个模型组件在学习较为专业化或可普遍化表示中的作用有深入的了解，而当前研究存在这方面的差距。因此，我们的目标是对可视化基础的持续语言学习的选择策略进行广泛的分析。由于缺乏适用于此目的的合适基准，我们引入了两个新的诊断数据集，提供了足够的控制和灵活性进行彻底的模型分析。我们评估了各种启发式的模块专业化策略以及...

    A desirable trait of an artificial agent acting in the visual world is to continually learn a sequence of language-informed tasks while striking a balance between sufficiently specializing in each task and building a generalized knowledge for transfer. Selective specialization, i.e., a careful selection of model components to specialize in each task, is a strategy to provide control over this trade-off. However, the design of selection strategies requires insights on the role of each model component in learning rather specialized or generalizable representations, which poses a gap in current research. Thus, our aim with this work is to provide an extensive analysis of selection strategies for visually grounded continual language learning. Due to the lack of suitable benchmarks for this purpose, we introduce two novel diagnostic datasets that provide enough control and flexibility for a thorough model analysis. We assess various heuristics for module specialization strategies as well as
    
[^42]: MuLMS：材料科学领域信息提取的多层注释文本语料库

    MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in the Materials Science Domain. (arXiv:2310.15569v1 [cs.CL])

    [http://arxiv.org/abs/2310.15569](http://arxiv.org/abs/2310.15569)

    MuLMS是一个多层注释的文本语料库，包含50篇开放获取的文章，涵盖了材料科学的七个子领域。该语料库被注释了多个层次的信息，并提出了多任务训练的神经模型。

    

    对于一个研究领域，跟踪所有相关的最新出版物和实验结果是一项具有挑战性的任务。先前的工作已经证明了信息提取模型在各个科学领域的有效性。最近，已经发布了几个针对尚未研究的材料科学领域的数据集。然而，这些数据集侧重于解析合成程序等子问题或者侧重于子领域，例如固体氧化物燃料电池。在这篇资源论文中，我们介绍了MuLMS，一个包含50篇开放获取文章的新数据集，涵盖了材料科学的七个子领域。该语料库由领域专家进行了多个层次的注释，涵盖从命名实体到关系到框架结构的多个方面。我们提出了针对所有任务的竞争性神经模型，并证明利用现有的相关资源进行多任务训练是有益的。

    Keeping track of all relevant recent publications and experimental results for a research area is a challenging task. Prior work has demonstrated the efficacy of information extraction models in various scientific areas. Recently, several datasets have been released for the yet understudied materials science domain. However, these datasets focus on sub-problems such as parsing synthesis procedures or on sub-domains, e.g., solid oxide fuel cells. In this resource paper, we present MuLMS, a new dataset of 50 open-access articles, spanning seven sub-domains of materials science. The corpus has been annotated by domain experts with several layers ranging from named entities over relations to frame structures. We present competitive neural models for all tasks and demonstrate that multi-task training with existing related resources leads to benefits.
    
[^43]: TCRA-LLM: 用于减少推理成本的令牌压缩检索增强大型语言模型

    TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction. (arXiv:2310.15556v1 [cs.CL])

    [http://arxiv.org/abs/2310.15556](http://arxiv.org/abs/2310.15556)

    TCRA-LLM是通过概述压缩和语义压缩两种方法来减少商业大型语言模型推理成本的方案。

    

    自从ChatGPT发布了API供公众使用以来，构建在商业大型语言模型（LLM）之上的应用程序数量呈指数增长。这种模型的一个流行用法是利用其上下文学习能力并生成响应以回答用户查询，并利用检索增强获得的知识。部署商业检索增强型LLM的一个问题是成本，因为额外检索的上下文大大增加了LLM的输入标记量。为了缓解这个问题，我们提出了一种令牌压缩方案，包括两种方法：概述压缩和语义压缩。第一种方法使用基于T5模型，通过使用包含具有不同长度的样本的自指示数据集进行微调，并通过概述来减少令牌大小。第二种方法通过移除对语义影响较小的词来进一步压缩令牌大小。为了充分评估所提方法的有效性，

    Since ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge obtained by retrieval augmentation. One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs. To mitigate this, we propose a token compression scheme that includes two methods: summarization compression and semantic compression. The first method applies a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths and reduce token size by doing summarization. The second method further compresses the token size by removing words with lower impact on the semantic. In order to adequately evaluate the effectiveness of the proposed
    
[^44]: 揭示Transformer模型中的多语言性：探索前馈网络中的语言特异性

    Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks. (arXiv:2310.15552v1 [cs.CL])

    [http://arxiv.org/abs/2310.15552](http://arxiv.org/abs/2310.15552)

    这项研究揭示了在Transformer模型中，前馈模块可以被视为键值记忆的集合，通过学习特定语言的模式，并结合共享特征，实现多语言模型的预测过程。

    

    最近的研究表明，Transformer模型中的前馈模块可以被视为一组键值记忆，其中键通过训练样本学习捕捉输入中的特定模式。值将键的“记忆”的输出进行组合，生成关于下一个标记的预测。这导致一种递增的预测过程，逐渐收敛于靠近输出层的最终标记选择。这个有趣的观点引发了对多语言模型如何利用这种机制的问题。具体来说，对于在两种或更多语言上训练的自回归模型，所有神经元（在不同层上）是否都对所有语言作出相同的响应？不是！我们的假设集中在这样一个观念上：在预训练期间，某些模型参数学习了强烈的语言特定特征，而其他参数学习了更多的语言无关特征（共享于多种语言）。为了验证这一点，我们利用平行语料进行实验证明。

    Recent research suggests that the feed-forward module within Transformers can be viewed as a collection of key-value memories, where the keys learn to capture specific patterns from the input based on the training examples. The values then combine the output from the 'memories' of the keys to generate predictions about the next token. This leads to an incremental process of prediction that gradually converges towards the final token choice near the output layers. This interesting perspective raises questions about how multilingual models might leverage this mechanism. Specifically, for autoregressive models trained on two or more languages, do all neurons (across layers) respond equally to all languages? No! Our hypothesis centers around the notion that during pretraining, certain model parameters learn strong language-specific features, while others learn more language-agnostic (shared across languages) features. To validate this, we conduct experiments utilizing parallel corpora of t
    
[^45]: 通过从字典学习概念角色，改进语言模型的语义理解和一致性

    Improving Language Models Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary. (arXiv:2310.15541v1 [cs.CL])

    [http://arxiv.org/abs/2310.15541](http://arxiv.org/abs/2310.15541)

    该论文提出了一种实用的方法，通过从字典中学习概念角色，从根本上改善语言模型的意义认知，以缓解其生成不一致预测的问题。

    

    当代预训练语言模型（PLMs）的非人类行为是影响其可信度的主要原因。这种错误行为的一个显著现象是生成不一致的预测，导致逻辑上矛盾的结果，例如为传达相同意义的文本生成不同的预测或违反逻辑性质。先前的研究利用数据增强或实施专门的损失函数来缓解这个问题。然而，它们的使用受限，因为它们消耗了大规模PLMs的昂贵训练资源，并且只能处理一定类型的一致性。为此，我们提出了一种实用的方法，通过从字典中的词-定义对中学习概念角色的准确相互关系，从根本上改善PLMs的意义认知，从而缓解了不一致行为问题。

    The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness. A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties. Previous studies exploited data augmentation or implemented specialised loss functions to alleviate the issue. However, their usage is limited, because they consume expensive training resources for large-sized PLMs and can only handle a certain consistency type. To this end, we propose a practical approach that alleviates the inconsistent behaviour issue by fundamentally improving PLMs' meaning awareness. Based on the conceptual role theory, our method allows PLMs to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary. Next, we propos
    
[^46]: SteloCoder:一种仅解码的用于多语言到Python代码翻译的LLM

    SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation. (arXiv:2310.15539v1 [cs.CL])

    [http://arxiv.org/abs/2310.15539](http://arxiv.org/abs/2310.15539)

    SteloCoder是一个仅解码的基于StarCoder的LLM，在多语言到Python代码翻译中取得了显著的性能提升。它采用Mixture-of-Experts（MoE）技术和门控网络，通过对StarCoder进行微调获得专家，并使用低秩自适应方法（LoRA）技术来限制每个专家的大小。

    

    最近关注大规模语言模型（LLM），StarCoder和Code Llama分别展示了在代码生成方面的出色性能。然而，在代码翻译功能上仍然需要改进和有效训练技术。为了解决这个问题，我们介绍了SteloCoder，一种仅解码的基于StarCoder的LLM，专为多编程语言到Python代码翻译而设计。具体而言，SteloCoder实现了C ++，C＃，JavaScript，Java或PHP到Python代码翻译，而无需指定输入编程语言。我们通过引入专家组混合（Mixture-of-Experts，MoE）技术和一个控制多任务的门控网络来修改StarCoder模型架构。我们通过对StarCoder进行微调来获得专家。具体而言，我们使用了低秩自适应方法（Low-Rank Adaptive Method，LoRA）技术，将每个专家的大小限制为StarCoder参数数量的仅0.06％。同时，为了增强tr

    With the recent focus on Large Language Models (LLMs), both StarCoder (Li et al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable performance in code generation. However, there is still a need for improvement in code translation functionality with efficient training techniques. In response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation without specifying the input programming language. We modified StarCoder model architecture by incorporating a Mixture-of-Experts (MoE) technique featuring five experts and a gating network for multi-task handling. Experts are obtained by StarCoder fine-tuning. Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each expert size as only 0.06% of number of StarCoder's parameters. At the same time, to enhance tr
    
[^47]: MarkQA: 一个包含数值推理的大规模KBQA数据集

    MarkQA: A large scale KBQA dataset with numerical reasoning. (arXiv:2310.15517v1 [cs.CL])

    [http://arxiv.org/abs/2310.15517](http://arxiv.org/abs/2310.15517)

    本文提出了一个包含数值推理的大规模KBQA数据集MarkQA，并设计了一种新的任务NR-KBQA，该任务要求进行多跳推理和数值推理。实验结果表明，KBQA中的复杂数值推理面临巨大挑战。

    

    虽然知识库问答（KBQA）在解决事实型问题方面取得了进展，但是涉及数值推理的KBQA相对较少研究。本文针对KBQA中复杂的数值推理问题，提出了一个新的任务，NR-KBQA，它需要进行多跳推理和数值推理。我们设计了一种以Python格式的逻辑形式PyQL来表示数值推理问题的推理过程。为了便于NR-KBQA的开发，我们提供了一个名为MarkQA的大规模数据集，该数据集是从一小组种子自动构建的。MarkQA中的每个问题都配备了与之对应的SPARQL查询，以及以QDMR格式和PyQL程序表示的逐步推理过程。一些最先进的QA方法在MarkQA上的实验结果表明，KBQA中的复杂数值推理面临着巨大的挑战。

    While question answering over knowledge bases (KBQA) has shown progress in addressing factoid questions, KBQA with numerical reasoning remains relatively unexplored. In this paper, we focus on the complex numerical reasoning in KBQA and propose a new task, NR-KBQA, which necessitates the ability to perform both multi-hop reasoning and numerical reasoning. We design a logic form in Python format called PyQL to represent the reasoning process of numerical reasoning questions. To facilitate the development of NR-KBQA, we present a large dataset called MarkQA, which is automatically constructed from a small set of seeds. Each question in MarkQA is equipped with its corresponding SPARQL query, alongside the step-by-step reasoning process in the QDMR format and PyQL program. Experimental results of some state-of-the-art QA methods on the MarkQA show that complex numerical reasoning in KBQA faces great challenges.
    
[^48]: 用火攻火：LLM在制作和检测隐蔽虚假信息中的双重角色

    Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation. (arXiv:2310.15515v1 [cs.CL])

    [http://arxiv.org/abs/2310.15515](http://arxiv.org/abs/2310.15515)

    这项研究提出了一种新的策略，即利用现代语言模型的生成和推理能力来对抗虚假信息。通过合成真实和欺骗性的内容以及利用语境语义推理技术，该策略在检测和对抗虚假信息方面取得了良好的效果。

    

    近期大型语言模型（LLM）的普及和破坏性影响引发了人们对其潜在被滥用的担忧（即生成大规模有害和误导性内容）。为了应对LLM的这一新兴风险，我们提出了一种新颖的“以火攻火”（F3）策略，利用现代LLM的生成和逻辑推理能力来对抗人类撰写和LLM生成的虚假信息。首先，我们利用GPT-3.5-turbo通过基于释义和扰动的前缀式提示合成真实和欺骗性的LLM生成内容。其次，我们应用零-shot语境语义推理技术，通过填空式提示区分真实和虚假的帖子和新闻文章。在广泛的实验证明中，我们观察到GPT-3.5-turbo在分布和非分布数据集方面具有零-shot上的优势，其中GPT-3.5-turbo始终保持在68-72%的准确率，不像之前的个性化下降现象。

    Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (.i.e, generating large-scale harmful and misleading content). To combat this emerging risk of LLMs, we propose a novel "Fighting Fire with Fire" (F3) strategy that harnesses modern LLMs' generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. In our extensive experiments, we observe GPT-3.5-turbo's zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customize
    
[^49]: 多语言表征的联合矩阵分解分析

    A Joint Matrix Factorization Analysis of Multilingual Representations. (arXiv:2310.15513v1 [cs.CL])

    [http://arxiv.org/abs/2310.15513](http://arxiv.org/abs/2310.15513)

    本论文介绍了一种基于联合矩阵分解的分析工具，用于比较多语言和单语言模型的潜在表示。对于多语言预训练模型学习到的表示，我们研究了形态句法特征的呈现程度和方式，并发现了编码形态句法信息的变化和受语言属性影响的特定类别差异。同时，我们的研究结果还展示了因式分解输出与跨语言任务性能之间的强关联。我们还公开发布了相关代码。

    

    我们提出了一种基于联合矩阵分解的分析工具，用于比较多语言和单语言模型的潜在表示。作为对探测的替代方案，该工具允许我们以联合的方式分析多组表示。使用该工具，我们研究了多语言预训练模型学习到的表示中在多大程度上以及如何反映了形态句法特征。我们对33种语言和17种形态句法类别进行了大规模的实证研究。我们的研究结果显示，在上层和下层之间编码形态句法信息的方式存在变化，而且受语言属性影响的特定类别差异。通过因式分解输出的层次聚类结果可以生成与语言学家手工制作的系统发生学树相关的树状结构。此外，我们发现因式分解输出与跨语言任务中观察到的性能之间存在强关联。我们公开发布了我们的代码以便研究社区使用。

    We present an analysis tool based on joint matrix factorization for comparing latent representations of multilingual and monolingual models. An alternative to probing, this tool allows us to analyze multiple sets of representations in a joint manner. Using this tool, we study to what extent and how morphosyntactic features are reflected in the representations learned by multilingual pre-trained models. We conduct a large-scale empirical study of over 33 languages and 17 morphosyntactic categories. Our findings demonstrate variations in the encoding of morphosyntactic information across upper and lower layers, with category-specific differences influenced by language properties. Hierarchical clustering of the factorization outputs yields a tree structure that is related to phylogenetic trees manually crafted by linguists. Moreover, we find the factorization outputs exhibit strong associations with performance observed across different cross-lingual tasks. We release our code to facilita
    
[^50]: 在信息检索中评估基于约束满足的LLMs

    KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval. (arXiv:2310.15511v1 [cs.LG])

    [http://arxiv.org/abs/2310.15511](http://arxiv.org/abs/2310.15511)

    本研究评估了最先进的模型在信息检索中回答约束满足查询的能力，并引入了一个新的数据集KITAB来衡量语言模型的约束满足能力。

    

    我们研究了最先进的模型在信息检索中回答约束满足查询（例如，“圣地亚哥的冰淇淋店列表”）的能力。过去，这样的查询被认为只能通过网络搜索或知识库来解决。最近，大型语言模型（LLMs）在这个任务中展示了初步的能力。然而，许多当前的检索基准要么已饱和，要么不能衡量约束满足。受到对LLMs事实不正确和产生幻觉的日益关注的驱动，我们提出了KITAB，一个用于衡量语言模型约束满足能力的新数据集。KITAB包含600多位作者和13,000个查询的与书籍相关的数据，还提供了一个关联的动态数据收集和约束验证方法，以获得其他作者的类似测试数据。我们对GPT4和GPT3.5进行了扩展实验，对常见的失败模式进行了表征和解耦。

    We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., 'a list of ice cream shops in San Diego'). In the past, such queries were considered to be tasks that could only be solved via web-search or knowledge bases. More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task. However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction. Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models. KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors. Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes acros
    
[^51]: TRAMS:训练免费的长程语言建模记忆选择

    TRAMS: Training-free Memory Selection for Long-range Language Modeling. (arXiv:2310.15494v1 [cs.CL])

    [http://arxiv.org/abs/2310.15494](http://arxiv.org/abs/2310.15494)

    TRAMS是一种训练免费的长程语言建模记忆选择策略，它能够提高Transformer架构在长程语言建模方面的效果，并且不需要额外的训练或参数。

    

    Transformer架构对于众多AI模型至关重要，但在长程语言建模方面仍面临挑战。尽管已经设计了几种特定的Transformer架构来解决长程依赖的问题，但现有的方法如Transformer-XL存在大量无效记忆的问题。本研究提出了一种即插即用的策略，称为TRAining-free Memory Selection（TRAMS），它根据一个简单的指标选择参与注意力计算的标记。该策略允许我们保留与当前查询具有高关注分数可能性的标记，并忽略其他标记。我们在单词级基准（WikiText-103）和字符级基准（enwik8）上测试了我们的方法，结果表明在不进行额外训练或添加额外参数的情况下取得了改进。

    The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters.
    
[^52]: NuTrea：用于上下文引导的多跳知识图问答的神经树搜索

    NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA. (arXiv:2310.15484v1 [cs.CL])

    [http://arxiv.org/abs/2310.15484](http://arxiv.org/abs/2310.15484)

    NuTrea是一个基于树搜索的GNN模型，用于上下文引导的多跳知识图问答。模型采用了消息传递方案来增强过去导向的嵌入，并引入了RF-IEF节点嵌入来更好地表征模糊的知识图节点。

    

    多跳的知识图问答是一项任务，涉及从知识图中检索节点以回答自然语言问题。最近基于GNN的方法将此任务形式化为一个知识图路径搜索问题，其中消息从种子节点沿着路径依次传播到答案节点。然而，这些消息都是过去导向的，并没有考虑到完整的知识图上下文。为了解决这些问题，我们提出了一种基于树搜索的GNN模型NuTrea，它将更广泛的知识图上下文纳入考虑。我们的模型采用了消息传递方案，探索未到达的子树区域以提升过去导向的嵌入。此外，我们还引入了关系频率-逆实体频率（RF-IEF）节点嵌入，以更好地表征模糊的知识图节点。

    Multi-hop Knowledge Graph Question Answering (KGQA) is a task that involves retrieving nodes from a knowledge graph (KG) to answer natural language questions. Recent GNN-based approaches formulate this task as a KG path searching problem, where messages are sequentially propagated from the seed node towards the answer nodes. However, these messages are past-oriented, and they do not consider the full KG context. To make matters worse, KG nodes often represent proper noun entities and are sometimes encrypted, being uninformative in selecting between paths. To address these problems, we propose Neural Tree Search (NuTrea), a tree search-based GNN model that incorporates the broader KG context. Our model adopts a message-passing scheme that probes the unreached subtree regions to boost the past-oriented embeddings. In addition, we introduce the Relation Frequency-Inverse Entity Frequency (RF-IEF) node embedding that considers the global KG context to better characterize ambiguous KG nodes
    
[^53]: CRaSh: 聚类、去除和共享无需完整大型语言模型增强微调

    CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model. (arXiv:2310.15477v1 [cs.CL])

    [http://arxiv.org/abs/2310.15477](http://arxiv.org/abs/2310.15477)

    本文研究了CRaSh模型，这是一种 Cluster, Remove, and Share 的技术，通过在不完整的大型语言模型上进行微调来增强其性能。从实证分析中发现，LLM层内存在独特的模块结构，并且在模型规模扩大时会出现微小但可能重要的改变。

    

    最近，指令微调被认为是对大型语言模型（LLM）进行对齐以增强其在不同任务中的泛化能力的有效方法。然而，当使用私有指令数据对公开可访问的集中式LLM进行微调时，隐私问题不可避免。直接在模型之间传输参数化模块是解决这个问题的一种可行方法，但其实际效果和影响需要进一步探索。本文重点研究了Offsite-Tuning（OFT），这是一种将转换器块在集中式LLM和下游模拟器之间传输的代表性技术。鉴于对OFT的底层机制的理解有限，我们从表示和功能相似性的角度对LLM进行了实证分析。有趣的是，我们的研究结果揭示了LLM层内独特的模块结构，随着模型规模扩大而出现。同时，我们注意到微小但潜在重要的改变。

    Instruction tuning has recently been recognized as an effective way of aligning Large Language Models (LLMs) to enhance their generalization ability across various tasks. However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable. While direct transfer of parameterized modules between models is a plausible approach to address this, its implications and effectiveness need further exploration. This paper focuses on Offsite-Tuning (OFT), a representative technique that transfers transformer blocks between centralized LLMs and downstream emulators. Given the limited understanding of the underlying mechanism of OFT, we perform an empirical analysis on LLMs from the perspectives of representation and functional similarity. Interestingly, our findings reveal a unique modular structure within the layers of LLMs that appears to emerge as the model size expands. Simultaneously, we note subtle but potentially significant changes in re
    
[^54]: 持续事件提取与语义混淆修正

    Continual Event Extraction with Semantic Confusion Rectification. (arXiv:2310.15470v1 [cs.CL])

    [http://arxiv.org/abs/2310.15470](http://arxiv.org/abs/2310.15470)

    本文提出了一种带有语义混淆修正的持续事件提取模型，通过标注伪标签和传递关键知识来缓解事件类型语义混淆并提高模型在长尾事件类型的理解上的性能。

    

    我们研究了持续事件提取，旨在提取不断出现的事件信息同时避免遗忘。我们观察到，事件类型的语义混淆源于同一文本的注释随时间更新。事件类型之间的不平衡甚至加剧了这个问题。本文提出了一种新颖的带有语义混淆修正的持续事件提取模型。我们为每个句子标注伪标签以缓解语义混淆。我们在当前模型和之前模型之间传递关键知识，以增强对事件类型的理解。此外，我们通过利用其他相关类型来鼓励模型关注长尾事件类型的语义。实验结果表明，我们的模型优于最先进的基线模型，并且在不平衡的数据集上表现出良好的性能。

    We study continual event extraction, which aims to extract incessantly emerging event information while avoiding forgetting. We observe that the semantic confusion on event types stems from the annotations of the same text being updated over time. The imbalance between event types even aggravates this issue. This paper proposes a novel continual event extraction model with semantic confusion rectification. We mark pseudo labels for each sentence to alleviate semantic confusion. We transfer pivotal knowledge between current and previous models to enhance the understanding of event types. Moreover, we encourage the model to focus on the semantics of long-tailed event types by leveraging other associated types. Experimental results show that our model outperforms state-of-the-art baselines and is proficient in imbalanced datasets.
    
[^55]: 《Janus接口：大型语言模型微调如何放大隐私风险》

    The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks. (arXiv:2310.15469v1 [cs.CR])

    [http://arxiv.org/abs/2310.15469](http://arxiv.org/abs/2310.15469)

    《Janus接口：大型语言模型微调如何放大隐私风险》研究了大型语言模型的微调对个人信息泄露的风险，发现了一种新的LLM利用途径。

    

    2018年后的时代标志着大型语言模型（LLM）的出现，OpenAI的ChatGPT等创新展示了惊人的语言能力。随着行业在增加模型参数并利用大量的人类语言数据方面的努力，安全和隐私挑战也出现了。其中最重要的是在基于网络的数据获取过程中，可能会意外积累个人可识别信息（PII），从而导致意外的PII泄露风险。虽然像RLHF和灾难性遗忘这样的策略已被用来控制隐私侵权的风险，但LLM的最新进展（以OpenAI的GPT-3.5的微调界面为代表）重新引发了关注。有人可能会问：LLM的微调是否会导致训练数据集中嵌入的个人信息泄漏？本文报道了首次尝试寻求答案的努力，重点是我们发现了一种新的LLM利用途径。

    The era post-2018 marked the advent of Large Language Models (LLMs), with innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess. As the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure. While strategies like RLHF during training and Catastrophic Forgetting have been marshaled to control the risk of privacy infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning interface for GPT-3.5, have reignited concerns. One may ask: can the fine-tuning of LLMs precipitate the leakage of personal information embedded within training datasets? This paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new LLM exploitation avenue
    
[^56]: 解读用户生成内容中的是非问题答案

    Interpreting Answers to Yes-No Questions in User-Generated Content. (arXiv:2310.15464v1 [cs.CL])

    [http://arxiv.org/abs/2310.15464](http://arxiv.org/abs/2310.15464)

    本文介绍了一个包含4,442个Twitter是非问题答案对的新语料库，并讨论了解读为是或否答案的语言特征以及解读未知答案的问题。研究结果表明，目前的大型语言模型无法很好地解决这个难题。

    

    解读社交媒体中的是非问题答案是困难的。是和否的关键词很少见，而包含这些关键词的答案很少能按照关键词所示的意思进行解读。在本文中，我们介绍了一个包含4,442个社交媒体平台Twitter上的是非问题答案对的新语料库。我们讨论了其解读为是或否的答案的语言特征，以及解读未知的答案。我们表明即使在微调和融合其他非社交媒体平台语料库的情况下，大型语言模型仍然远未解决这个问题。

    Interpreting answers to yes-no questions in social media is difficult. Yes and no keywords are uncommon, and the few answers that include them are rarely to be interpreted what the keywords suggest. In this paper, we present a new corpus of 4,442 yes-no question-answer pairs from Twitter. We discuss linguistic characteristics of answers whose interpretation is yes or no, as well as answers whose interpretation is unknown. We show that large language models are far from solving this problem, even after fine-tuning and blending other corpora for the same problem but outside social media.
    
[^57]: 通过人机语言模型交互促进自主引导式心理健康干预：认知重建的案例研究

    Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring. (arXiv:2310.15461v1 [cs.HC])

    [http://arxiv.org/abs/2310.15461](http://arxiv.org/abs/2310.15461)

    本文研究了人机语言模型交互如何支持自主引导式心理健康干预，通过以认知重建作为案例研究。研究结果显示，我们的系统对参与者情绪强度产生积极影响，并帮助他们克服了负面思维。

    

    自主引导式心理健康干预，如学习和实践应对策略的“自助工具”，在改善心理健康护理的可及性方面显示出巨大潜力。然而，这些干预常常需要认知负担和情绪触发，从而造成限制其大规模实施和普及的可及性障碍。本文研究人机语言模型交互如何支持自主引导式心理健康干预。我们以认知重建作为一个以证据为基础的治疗技术的案例研究。在一项经过IRB批准的大型心理健康网站上进行的随机现场研究中，涉及了15,531名参与者，我们设计并评估了一个使用语言模型来支持人们在认知重建的各个步骤中的系统。我们的研究结果表明，我们的系统对67%的参与者的情绪强度产生积极影响，并帮助65%的人克服消极思维。尽管青少年报道较差。

    Self-guided mental health interventions, such as "do-it-yourself" tools to learn and practice coping strategies, show great promise to improve access to mental health care. However, these interventions are often cognitively demanding and emotionally triggering, creating accessibility barriers that limit their wide-scale implementation and adoption. In this paper, we study how human-language model interaction can support self-guided mental health interventions. We take cognitive restructuring, an evidence-based therapeutic technique to overcome negative thinking, as a case study. In an IRB-approved randomized field study on a large mental health website with 15,531 participants, we design and evaluate a system that uses language models to support people through various steps of cognitive restructuring. Our findings reveal that our system positively impacts emotional intensity for 67% of participants and helps 65% overcome negative thoughts. Although adolescents report relatively worse o
    
[^58]: K-HATERS：具有目标特定评级的韩语仇恨言论检测语料库

    K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific Ratings. (arXiv:2310.15439v1 [cs.CL])

    [http://arxiv.org/abs/2310.15439](http://arxiv.org/abs/2310.15439)

    K-HATERS是一个具有目标特定评级的韩语仇恨言论检测语料库，包含大约192K条新闻评论。它是韩文最大的冒犯性语言语料库，也是首个提供目标特定评级的资源，在不同程度的冒犯性下可以检测韩语中的仇恨表达。

    

    尽管已经提出了许多数据集来打击网络仇恨言论的传播，但其中大多数都是以英语为中心，主要关注明显的仇恨形式。这种研究空白要求我们开发高质量的多语言语料库，包括更微妙的仇恨表达。本研究引入了K-HATERS，一个新的韩语仇恨言论检测语料库，包括约192,000条新闻评论，附带目标特定的冒犯性评级。该资源是韩文中最大的冒犯性语言语料库，也是首个提供三点里克特量表上的目标特定评级的资源，可以在不同程度的冒犯性下检测韩语中的仇恨表达。我们进行了实验证明了该语料库的有效性，并与现有数据集进行了比较。此外，为了解决人类注释中的潜在噪声和偏差问题，我们探索了采用认知反射测试的新想法。

    Numerous datasets have been proposed to combat the spread of online hate. Despite these efforts, a majority of these resources are English-centric, primarily focusing on overt forms of hate. This research gap calls for developing high-quality corpora in diverse languages that also encapsulate more subtle hate expressions. This study introduces K-HATERS, a new corpus for hate speech detection in Korean, comprising approximately 192K news comments with target-specific offensiveness ratings. This resource is the largest offensive language corpus in Korean and is the first to offer target-specific ratings on a three-point Likert scale, enabling the detection of hate expressions in Korean across varying degrees of offensiveness. We conduct experiments showing the effectiveness of the proposed corpus, including a comparison with existing datasets. Additionally, to address potential noise and bias in human annotations, we explore a novel idea of adopting the Cognitive Reflection Test, which i
    
[^59]: 什么情况下纵火是可以接受的？通过迭代自蒸馏情境和原因来消除模糊的社会和道德情境。

    What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations. (arXiv:2310.15431v1 [cs.CL])

    [http://arxiv.org/abs/2310.15431](http://arxiv.org/abs/2310.15431)

    本研究提出了易推翻的道德推理任务，以了解不同背景下行为的道德可接受性，并提供常识理由来支持推理。通过迭代的自蒸馏方法，我们获得了高质量的任务数据。

    

    道德或伦理判断在很大程度上依赖于其发生的具体背景。理解易推翻的情境化变化（即增强或减弱一个行为的道德可接受性的额外信息）对于准确呈现现实场景中凝固的人类道德判断的微妙和复杂性非常重要。我们引入了易推翻的道德推理：一项任务，提供使行为在道德上更可接受或不可接受的情境，以及证明推理的常识理由。为了获取高质量的任务数据，我们采用了迭代自蒸馏的方法，从GPT-3的一小部分非结构化种子知识开始，然后在学生模型和批判者模型之间交替进行（1）自蒸馏；（2）通过人类判断进行有针对性的过滤（以提高有效性）和NLI（以提高多样性）；（3）自模仿学习（以放大所需数据质量）。这个过程产生了一个学生模型。

    Moral or ethical judgments rely heavily on the specific contexts in which they occur. Understanding varying shades of defeasible contextualizations (i.e., additional information that strengthens or attenuates the moral acceptability of an action) is critical to accurately represent the subtlety and intricacy of grounded human moral judgment in real-life scenarios.  We introduce defeasible moral reasoning: a task to provide grounded contexts that make an action more or less morally acceptable, along with commonsense rationales that justify the reasoning. To elicit high-quality task data, we take an iterative self-distillation approach that starts from a small amount of unstructured seed knowledge from GPT-3 and then alternates between (1) self-distillation from student models; (2) targeted filtering with a critic model trained by human judgment (to boost validity) and NLI (to boost diversity); (3) self-imitation learning (to amplify the desired data quality). This process yields a stude
    
[^60]: 超越情感：利用主题度量进行政治立场分类

    Beyond Sentiment: Leveraging Topic Metrics for Political Stance Classification. (arXiv:2310.15429v1 [cs.CL])

    [http://arxiv.org/abs/2310.15429](http://arxiv.org/abs/2310.15429)

    本研究引入了主题度量作为情感度量在政治立场分类中的替代和补充，实验结果表明主题度量相比传统方法提高了连贯性分数，并在立场分类中表现出更好的性能。

    

    情感分析被广泛批评仅能捕捉到语料库的整体情感，无法准确反映文本中的潜在结构和政治立场。本研究引入了主题度量，将从提取的主题转化而来的虚拟变量，作为情感度量在立场分类中的替代和补充。通过应用Bestvater和Monroe（2023）确定的三个数据集，本研究展示了BERTopic在提取连贯主题和主题度量在立场分类中的有效性。实验结果显示，与传统方法如Dirichlet分配（LDA）和非负矩阵分解（NMF）相比，BERTopic提高了17.07%至54.20%的连贯性分数，这些传统方法在早期政治学研究中很常见。此外，我们的结果表明，主题度量在立场分类中优于情感度量，性能提高了高达18.95%。我们的研究结果表明，主题度量是一种有效的方法来理解文本中的政治立场。

    Sentiment analysis, widely critiqued for capturing merely the overall tone of a corpus, falls short in accurately reflecting the latent structures and political stances within texts. This study introduces topic metrics, dummy variables converted from extracted topics, as both an alternative and complement to sentiment metrics in stance classification. By employing three datasets identified by Bestvater and Monroe (2023), this study demonstrates BERTopic's proficiency in extracting coherent topics and the effectiveness of topic metrics in stance classification. The experiment results show that BERTopic improves coherence scores by 17.07% to 54.20% when compared to traditional approaches such as Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), prevalent in earlier political science research. Additionally, our results indicate topic metrics outperform sentiment metrics in stance classification, increasing performance by as much as 18.95%. Our findings suggest topic 
    
[^61]: Mason-Alberta音标分割器: 基于深度神经网络和插值的强制对齐系统

    The Mason-Alberta Phonetic Segmenter: A forced alignment system based on deep neural networks and interpolation. (arXiv:2310.15425v1 [eess.AS])

    [http://arxiv.org/abs/2310.15425](http://arxiv.org/abs/2310.15425)

    本文介绍了一种基于深度神经网络和插值的新型强制对齐系统，该系统名为Mason-Alberta音标分割器。该系统的创新点包括将声学模型视为标注任务，而不是分类任务，并采用了插值技术实现更精确的分段边界。

    

    强制对齐系统在给定正字法转录的语音数据中自动确定分段边界。这些工具在语音学中很常见，以便使用那些手动转录和分段难以实现的语音数据。在本文中，我们描述了一个新的基于神经网络的强制对齐系统，即Mason-Alberta音标分割器（MAPS）。MAPS对齐器作为我们追求强制对齐系统两个潜在改进的试验平台。第一个是将强制对齐器中的声学模型视为标注任务，而不是分类任务，这是基于人们对语音中的段落并不是真正离散和常常重叠的共同认识。第二个是插值技术，使得边界可以比现代强制对齐系统常见的10毫秒限制更精确。我们将我们系统的配置与最先进的系统Montreal Forced Aligner进行比较。

    Forced alignment systems automatically determine boundaries between segments in speech data, given an orthographic transcription. These tools are commonplace in phonetics to facilitate the use of speech data that would be infeasible to manually transcribe and segment. In the present paper, we describe a new neural network-based forced alignment system, the Mason-Alberta Phonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two possible improvements we pursue for forced alignment systems. The first is treating the acoustic model in a forced aligner as a tagging task, rather than a classification task, motivated by the common understanding that segments in speech are not truly discrete and commonly overlap. The second is an interpolation technique to allow boundaries more precise than the common 10 ms limit in modern forced alignment systems. We compare configurations of our system to a state-of-the-art system, the Montreal Forced Aligner. The tagging approach did not gener
    
[^62]: FANToM: 在交互中对机器心智理论进行压力测试的基准

    FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v1 [cs.CL])

    [http://arxiv.org/abs/2310.15421](http://arxiv.org/abs/2310.15421)

    FANToM是一个新的基准，用于通过问答在信息不对称的对话环境中压力测试机器的心智理论。这个基准对最先进的大型语言模型来说具有挑战性，即使是具有思维链推理和微调的模型也比人类表现得差。

    

    目前关于心智理论（ToM）的评估主要集中在使用缺乏互动性的被动故事，我们介绍了FANToM，一个新的基准，通过问答在信息不对称的对话环境中进行心智理论的压力测试。我们的基准结合了心理学中的重要理论要求和对评估大型语言模型（LLM）时必要的经验考虑。特别地，我们制定了多种类型的问题，要求相同的基本推理来识别LLM中不存在或虚假的心智理论能力。我们展示了FANToM对最先进的LLM来说具有挑战性，即使是具有思维链推理和微调的LLM也表现比人类差得多。

    Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.
    
[^63]: 让预训练语言模型为短文本主题建模“想象”

    Let the Pretrained Language Models "Imagine" for Short Texts Topic Modeling. (arXiv:2310.15420v1 [cs.CL])

    [http://arxiv.org/abs/2310.15420](http://arxiv.org/abs/2310.15420)

    本文提出了一种新的方法来解决短文本主题建模中的数据稀疏问题，通过使用预训练语言模型将短文本扩展为更长的序列。实验结果表明，该模型在极端数据稀疏情况下能够显著提高短文本主题建模的性能。

    

    主题模型是一种发现文档集合中潜在语义的有效方法。然而，它假设文档具有足够的共现信息才能发挥效果。然而，在短文本中，共现信息很少，导致文档表示中的特征稀疏。因此，现有的主题模型（概率或神经网络）大多无法从中挖掘模式并生成连贯的主题。在本文中，我们采用了一种新的方法来解决短文本主题建模中的数据稀疏问题，通过使用现有的预训练语言模型（PLMs）将短文本扩展为更长的序列。此外，我们提供了一个简单的解决方案来扩展神经主题模型，以减少PLMs生成的噪声“非主题”文本的影响。我们观察到我们的模型可以大大提高短文本主题建模的性能。在极端数据稀疏的情况下，对多个真实数据集进行了大量实验，结果显示我们的模型效果显著提高。

    Topic models are one of the compelling methods for discovering latent semantics in a document collection. However, it assumes that a document has sufficient co-occurrence information to be effective. However, in short texts, co-occurrence information is minimal, which results in feature sparsity in document representation. Therefore, existing topic models (probabilistic or neural) mostly fail to mine patterns from them to generate coherent topics. In this paper, we take a new approach to short-text topic modeling to address the data-sparsity issue by extending short text into longer sequences using existing pre-trained language models (PLMs). Besides, we provide a simple solution extending a neural topic model to reduce the effect of noisy out-of-topics text generation from PLMs. We observe that our model can substantially improve the performance of short-text topic modeling. Extensive experiments on multiple real-world datasets under extreme data sparsity scenarios show that our model
    
[^64]: 注意对话之间的差距，以提高长期对话生成的效果

    Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation. (arXiv:2310.15415v1 [cs.CL])

    [http://arxiv.org/abs/2310.15415](http://arxiv.org/abs/2310.15415)

    本研究探索了使对话模型意识到时间的想法，并提出了一个多次对话的数据集GapChat，显示出时间感知的模型在判断话题相关性和从对话中获取信息的度量标准方面表现更好。

    

    知道如何结束和恢复对话是交流的自然部分，允许讨论跨越数周、数月或数年。对话之间的间隔持续时间决定了哪些话题是相关的，以及要问哪些问题，而不明确模拟时间的对话系统可能生成不自然的回应。在这项工作中，我们探索了使对话模型意识到时间的想法，并提出了GapChat，这是一个多次对话的数据集，其中每个对话之间的时间不同。虽然数据集是实时构建的，但演讲者生活中事件的进展是模拟的，以创建发生在长时间跨度内的现实对话。我们将时间信息暴露给模型，并比较不同的时间和事件进展表示。在人工评估中，我们展示了时间感知模型在判断选择话题的相关性和从对话中获得的信息的度量标准上表现更好。

    Knowing how to end and resume conversations over time is a natural part of communication, allowing for discussions to span weeks, months, or years. The duration of gaps between conversations dictates which topics are relevant and which questions to ask, and dialogue systems which do not explicitly model time may generate responses that are unnatural. In this work we explore the idea of making dialogue models aware of time, and present GapChat, a multi-session dialogue dataset in which the time between each session varies. While the dataset is constructed in real-time, progress on events in speakers' lives is simulated in order to create realistic dialogues occurring across a long timespan. We expose time information to the model and compare different representations of time and event progress. In human evaluation we show that time-aware models perform better in metrics that judge the relevance of the chosen topics and the information gained from the conversation.
    
[^65]: GPT-4作为科学图表标题的有效零-shot评估器

    GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions. (arXiv:2310.15405v1 [cs.CL])

    [http://arxiv.org/abs/2310.15405](http://arxiv.org/abs/2310.15405)

    本文研究了使用大型语言模型作为经济实惠的方法来评估科学图表标题，并发现GPT-4作为零-shot评估器表现优于其他模型和人工评估。

    

    越来越多的人对生成科学图表标题的系统产生了兴趣。然而，评估这些系统的输出带来了很大的挑战。人工评估需要学术专长，并且成本较高，而自动评估依赖于通常质量较低的作者编写的标题。本文研究了使用大型语言模型（LLMs）作为一种经济实惠、无参考方法来评估图表标题。首先，我们构建了SCICAP-EVAL，这是一个人工评估数据集，包含了3600个科学图表标题的人工判断，包括原始标题和机器生成的标题，涵盖了600个arXiv图表。然后，我们对GPT-4和GPT-3等LLMs进行评分（1-6），评估它们在给定相关上下文（如提及图表的段落）的情况下，每个标题对读者理解的潜力。结果显示，作为零-shot评估器，GPT-4表现优于其他所有模型，甚至超过了计算机科学和信息学本科生的评估，达到了Kendall相关系数

    There is growing interest in systems that generate captions for scientific figures. However, assessing these systems output poses a significant challenge. Human evaluation requires academic expertise and is costly, while automatic evaluation depends on often low-quality author-written captions. This paper investigates using large language models (LLMs) as a cost-effective, reference-free method for evaluating figure captions. We first constructed SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600 scientific figure captions, both original and machine-made, for 600 arXiv figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption based on its potential to aid reader understanding, given relevant context such as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot evaluator, outperformed all other models and even surpassed assessments made by Computer Science and Informatics undergraduates, achieving a Kendall correlat
    
[^66]: "一刀切"？ 跨身份语言特征的 NLG 系统观察与期望

    "One-size-fits-all"? Observations and Expectations of NLG Systems Across Identity-Related Language Features. (arXiv:2310.15398v1 [cs.CL])

    [http://arxiv.org/abs/2310.15398](http://arxiv.org/abs/2310.15398)

    通过扰动不同类型的身份相关语言特征，研究探索了NLG系统行为的公平中的适应性和不变性之间的紧张关系。研究结果发现，适应的动机包括社会规范、文化差异、特定特征信息和适应性；不变性的动机包括支持规定主义的观点、将适应视为不必要过程，并对错误假设持谨慎态度。这些发现突显了定义公平相关NLG系统行为的挑战。

    

    关于什么构成适当的自然语言生成（NLG）系统行为的公平相关假设，从不变性，即期望系统对社会群体做出相同的响应，到适应性，即期望系统在不同社会群体之间产生不同响应，存在不同观点。我们设计并进行了五个案例研究，通过扰动不同类型的与身份相关的语言特征（姓名、角色、地点、方言和风格）来阐明不变性和适应性之间的紧张关系。我们概述了人们对系统行为的期望，并提出了这两种对立但常见的假设的潜在警示。我们发现适应的动机包括社会规范、文化差异、特定特征信息和适应性；不变性的动机包括支持规定主义的观点、将适应视为 NLG 系统难以适当完成的不必要过程，并对错误假设持谨慎态度。我们的研究结果突显了关于定义公平相关NLG系统行为的挑战。

    Fairness-related assumptions about what constitutes appropriate NLG system behaviors range from invariance, where systems are expected to respond identically to social groups, to adaptation, where responses should instead vary across them. We design and conduct five case studies, in which we perturb different types of identity-related language features (names, roles, locations, dialect, and style) in NLG system inputs to illuminate tensions around invariance and adaptation. We outline people's expectations of system behaviors, and surface potential caveats of these two contrasting yet commonly-held assumptions. We find that motivations for adaptation include social norms, cultural differences, feature-specific information, and accommodation; motivations for invariance include perspectives that favor prescriptivism, view adaptation as unnecessary or too difficult for NLG systems to do appropriately, and are wary of false assumptions. Our findings highlight open challenges around definin
    
[^67]: DoGE: 使用泛化估计进行领域重新加权

    DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])

    [http://arxiv.org/abs/2310.15393](http://arxiv.org/abs/2310.15393)

    DoGE提出了一种基于泛化估计的领域重新加权方法。通过使用梯度估计函数评估每个领域对泛化目标的贡献，重新调整了预训练数据中不同领域的采样概率。实验结果表明，该方法在提高大型语言模型的泛化能力方面取得了显著效果。

    

    预训练数据语料库的覆盖范围和组成对大型语言模型的泛化能力有着重要影响。传统上，预训练语料库由各种来源领域（如CommonCrawl、Wikipedia、Github等）按照特定的采样概率（领域权重）组成。然而，当前的方法缺乏一种基于最终泛化目标优化领域权重的原则方法。我们提出了一种称为DOmain reweighting with Generalization Estimation（DoGE）的方法，其中我们重新调整了每个领域的采样概率，根据它对最终泛化目标的贡献进行了基于梯度的泛化估计函数评估。首先，我们使用最小最大优化训练了一个小规模的代理模型来获取重新加权的领域权重。在每一步中，通过镜像下降法更新领域权重以最大化整体的泛化增益。最后，我们使用获得的领域权重来训练一个规模更大的完整语言模型。

    The coverage and composition of the pretraining data corpus significantly impacts the generalization ability of large language models. Conventionally, the pretraining corpus is composed of various source domains (e.g. CommonCrawl, Wikipedia, Github etc.) according to certain sampling probabilities (domain weights). However, current methods lack a principled way to optimize domain weights for ultimate goal for generalization. We propose DOmain reweighting with Generalization Estimation (DoGE), where we reweigh the sampling probability from each domain based on its contribution to the final generalization objective assessed by a gradient-based generalization estimation function. First, we train a small-scale proxy model with a min-max optimization to obtain the reweighted domain weights. At each step, the domain weights are updated to maximize the overall generalization gain by mirror descent. Finally we use the obtained domain weights to train a larger scale full-size language model. On
    
[^68]: 语言模型预训练的不可约课程

    Irreducible Curriculum for Language Model Pretraining. (arXiv:2310.15389v1 [cs.CL])

    [http://arxiv.org/abs/2310.15389](http://arxiv.org/abs/2310.15389)

    本论文提出了一种不可约课程算法，用于语言模型预训练，通过优先选择具有更高学习能力的样本，并使用小规模代理模型模拟样本丢失，从而在大型语言模型上解决了传统数据选择方法的困难，并在实验证明算法能够持续改进模型性能。

    

    训练大型语言模型的自动数据选择和课程设计具有挑战性，只有少数现有的方法在标准训练上显示出改进。此外，当前的方案更关注领域级别的选择，忽视了每个单独训练点的更细粒度的贡献。在大型语言模型上应用传统的数据点选择方法很困难：大多数在线批选择方法执行两次前向或后向传递，这会带来巨大的额外成本。为了克服这些障碍，我们提出了不可约课程作为语言模型预训练的课程学习算法，该算法优先选择具有更高学习能力的样本。具体而言，为了避免过高的额外计算开销，我们使用小规模代理模型模拟样本丢失沿主模型训练轨迹的情况。我们在RedPajama-1B数据集上的实验表明，课程学习算法能够持续改进模型在验证集上的性能。

    Automatic data selection and curriculum design for training large language models is challenging, with only a few existing methods showing improvements over standard training. Furthermore, current schemes focus on domain-level selection, overlooking the more fine-grained contributions of each individual training point. It is difficult to apply traditional datapoint selection methods on large language models: most online batch selection methods perform two-times forward or backward passes, which introduces considerable extra costs with large-scale models. To mitigate these obstacles, we propose irreducible curriculum as a curriculum learning algorithm for language model pretraining, which prioritizes samples with higher learnability. Specifically, to avoid prohibitive extra computation overhead, we simulate the sample loss along the main model's training trajectory using a small-scale proxy model. Our experiments on the RedPajama-1B dataset demonstrate a consistent improvement on valida
    
[^69]: GD-COMET: 一种地理多样性常识推理模型

    GD-COMET: A Geo-Diverse Commonsense Inference Model. (arXiv:2310.15383v1 [cs.CL])

    [http://arxiv.org/abs/2310.15383](http://arxiv.org/abs/2310.15383)

    GD-COMET是一种地理多样性常识推理模型，通过人工评估和外在评估证明了其能够生成具有文化细微差异的常识知识，有助于促进各种自然语言处理应用的发展，使NLP更加包容。

    

    随着人工智能越来越多地融入日常生活，设计能够满足不同背景用户需求、具有文化意识的人工智能系统变得至关重要。在本文中，我们提出了GD-COMET，这是COMET常识推理模型的地理多样性版本。GD-COMET超越西方常识知识，能够生成涉及广泛文化背景的推理。我们通过对5种不同文化背景进行全面的人工评估以及地理多样性任务的外在评估，证明了GD-COMET的有效性。评估结果表明，GD-COMET能够捕捉和生成具有文化细微差异的常识知识，展示了其在各种自然语言处理应用中发挥作用、促进NLP更加包容的潜力。

    With the increasing integration of AI into everyday life, it's becoming crucial to design AI systems that serve users from diverse backgrounds by making them culturally aware. In this paper, we present GD-COMET, a geo-diverse version of the COMET commonsense inference model. GD-COMET goes beyond Western commonsense knowledge and is capable of generating inferences pertaining to a broad range of cultures. We demonstrate the effectiveness of GD-COMET through a comprehensive human evaluation across 5 diverse cultures, as well as extrinsic evaluation on a geo-diverse task. The evaluation shows that GD-COMET captures and generates culturally nuanced commonsense knowledge, demonstrating its potential to benefit NLP applications across the board and contribute to making NLP more inclusive.
    
[^70]: 数据湖中的语义数据管理

    Semantic Data Management in Data Lakes. (arXiv:2310.15373v1 [cs.DB])

    [http://arxiv.org/abs/2310.15373](http://arxiv.org/abs/2310.15373)

    数据湖是管理大量异构数据进行现代数据分析的一种方法。为了防止数据湖成为无法操作的数据沼泽，我们可以采用语义数据管理的方法，通过将元数据与知识图谱相链接，为数据提供更多的意义和语义。这种语义层不仅可以用于数据管理，还可以解决数据整合问题，使数据访问更具表达性和互操作性。

    

    近年来，数据湖作为管理大量异构数据进行现代数据分析的一种方法出现。防止数据湖变成无法操作的数据沼泽的一种方式是语义数据管理。一些方法提议基于链接数据原则将元数据与知识图谱相链接，以为湖中的数据提供更多的意义和语义。这样的语义层不仅可以用于数据管理，还可以解决来自异构来源的数据整合问题，以使数据访问更具表达性和互操作性。在这项调查中，我们重点关注在数据湖系统中的应用和大数据的可扩展性，回顾了最近的方法。我们将这些方法分为三类：(i)基本的语义数据管理，(ii)在数据湖中丰富元数据的语义建模方法，以及(iii)基于本体的数据访问方法。在每个类别中，我们涵盖了主要技术及其背景，并进行了比较。

    In recent years, data lakes emerged as away to manage large amounts of heterogeneous data for modern data analytics. One way to prevent data lakes from turning into inoperable data swamps is semantic data management. Some approaches propose the linkage of metadata to knowledge graphs based on the Linked Data principles to provide more meaning and semantics to the data in the lake. Such a semantic layer may be utilized not only for data management but also to tackle the problem of data integration from heterogeneous sources, in order to make data access more expressive and interoperable. In this survey, we review recent approaches with a specific focus on the application within data lake systems and scalability to Big Data. We classify the approaches into (i) basic semantic data management, (ii) semantic modeling approaches for enriching metadata in data lakes, and (iii) methods for ontologybased data access. In each category, we cover the main techniques and their background, and compa
    
[^71]: EpiK-Eval：将语言模型作为认识模型的评估

    EpiK-Eval: Evaluation for Language Models as Epistemic Models. (arXiv:2310.15372v1 [cs.CL])

    [http://arxiv.org/abs/2310.15372](http://arxiv.org/abs/2310.15372)

    这项研究介绍了一种新的评估方法EpiK-Eval，旨在评估大型语言模型（LLMs）在从分割的叙述中构建连贯和一致的知识表示方面的能力。研究发现当前的训练目标存在固有的缺陷，因此提出了改进知识整合方法的建议，以大幅提高LLMs的整体效果和性能。

    

    在人工智能时代，大型语言模型（LLMs）的作用越来越重要。尽管它们日益普及，但它们在从不同训练文档中整合知识的能力——在许多应用中都是关键能力——仍未得到探索。本文首次研究了LLMs在其参数空间内有效地结合这种信息的能力。我们引入了EpiK-Eval，一个新颖的问答基准，旨在评估LLMs在从分割的叙述中构建一种连贯和一致的知识表示方面的能力。对各种LLMs的评估揭示了在这一领域存在的显著弱点。我们认为这些缺点源于现有训练目标的固有性质。因此，我们主张改进知识整合的方法，因为这有潜力显著提高LLMs的整体效果和性能。

    In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents - a crucial ability in numerous applications - remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from 
    
[^72]: 为什么LLM会产生幻觉，以及如何获得（证据性的）闭合性：用于忠实自然语言生成的感知、内涵和外延学习

    Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation. (arXiv:2310.15355v1 [cs.CL])

    [http://arxiv.org/abs/2310.15355](http://arxiv.org/abs/2310.15355)

    本研究展示了LLMs产生幻觉的原因是因为它们的输出没有受到证据支持的主张的约束，并介绍了如何通过感知、内涵和外延学习来约束LLMs以生成满足证据闭合性的输出。

    

    我们展示了LLMs为什么会产生幻觉，因为它们的输出没有受到具备证据支持的主张的约束，这种条件被称为证据闭合。在标准的神经概率语言模型设置中，并不能从统计上辨别出关于句子真伪的信息，因此不能以此为条件生成新的字符串。然后我们展示了如何约束LLMs以产生满足证据闭合性的输出。多模态LLM必须学习外部世界（感知学习）；它必须学习从字符串到世界状态的映射（外延学习）；并且，为了在超越一组证据时实现流畅性，它必须学习从字符串到它们的同义词的映射（内涵学习）。一种单模态LLM的输出必须与验证的证据集中的字符串意义相同。最后，我们提供了一个启发式过程——学习-胡言乱语-修剪（Learn-Babble-Prune），通过拒绝不同义的输出，从LLM中产生忠实的输出。

    We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. Information about the truth or falsity of sentences is not statistically identified in the standard neural probabilistic language model setup, and so cannot be conditioned on to generate new strings. We then show how to constrain LLMs to produce output that does satisfy evidential closure. A multimodal LLM must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning). The output of a unimodal LLM must be synonymous with strings in a validated evidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune, that yields faithful output from an LLM by rejecting output that is not syn
    
[^73]: 大型语言模型的道德基础

    Moral Foundations of Large Language Models. (arXiv:2310.15337v1 [cs.AI])

    [http://arxiv.org/abs/2310.15337](http://arxiv.org/abs/2310.15337)

    本文通过使用道德基础理论（MFT）作为分析工具，研究了流行的大型语言模型（LLMs）是否对一系列特定的道德价值观产生了偏见，并展示了它们与人类道德基础和政治倾向的关联。研究还发现LLMs的偏见在不同的提示上下文中存在差异，并展示了通过对抗选择提示可以引导LLMs产生不同的回答。

    

    道德基础理论（MFT）是一种心理评估工具，将人类道德推理分解为包括关心/伤害、自由/压迫和尊严/堕落等五个因素（Graham等，2009）。人们在作出道德决策时在这些维度上的权重不同，部分原因是他们的文化背景和政治意识形态。由于大型语言模型（LLMs）在从互联网收集的数据集上训练，他们可能反映了这些语料库中存在的偏见。本文以MFT为视角，分析流行的LLMs是否对一系列特定的道德价值观产生了偏见。我们分析已知的LLMs，发现它们展现了特定的道德基础，并展示了它们与人类道德基础和政治倾向的关系。我们还测量了这些偏见的一致性，或者它们在模型被提示的上下文中是否有很大差异。最后，我们展示了我们可以通过对抗地选择提示来鼓励LLMs产生不同的回答。

    Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary in the weight they place on these dimensions when making moral decisions, in part due to their cultural upbringing and political ideology. As large language models (LLMs) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values. We analyze known LLMs and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. Finally, we show that we can adversarially select prompts that encourage the
    
[^74]: 专家还是通才？针对特定NLP任务的指导调优方法

    Specialist or Generalist? Instruction Tuning for Specific NLP Tasks. (arXiv:2310.15326v1 [cs.CL])

    [http://arxiv.org/abs/2310.15326](http://arxiv.org/abs/2310.15326)

    本文研究了将通才模型指导调优融入到专家模型中是否有助于构建专家模型，并发现在任务覆盖广泛且任务特定的训练数据有限时，整合通才模型的指导调优能够持续提高模型性能。

    

    大型语言模型（LLMs）在同时执行多种自然语言处理（NLP）任务方面的潜力已经成为广泛研究的主题。虽然指导调优已被证明是将LLMs转化为通才模型的一种高效方法，但它们的性能仍然落后于专家模型，专家模型专门为特定任务进行训练。本文研究了将广覆盖的通才模型指导调优融入到专家模型中是否有助于构建专家模型。我们假设其效果取决于任务的特异性和技能要求。我们的实验评估了四个具有不同覆盖范围的目标任务，发现当任务覆盖广泛时，整合通才模型的指导调优能够持续提高模型性能。而当任务特定的训练数据有限时，这种效果尤为显著。进一步探究针对不同能力的三个目标任务。

    The potential of large language models (LLMs) to simultaneously perform a wide range of natural language processing (NLP) tasks has been the subject of extensive research. Although instruction tuning has proven to be a data-efficient method for transforming LLMs into such generalist models, their performance still lags behind specialist models trained exclusively for specific tasks. In this paper, we investigate whether incorporating broad-coverage generalist instruction tuning can contribute to building a specialist model. We hypothesize that its efficacy depends on task specificity and skill requirements. Our experiments assess four target tasks with distinct coverage levels, revealing that integrating generalist instruction tuning consistently enhances model performance when the task coverage is broad. The effect is particularly pronounced when the amount of task-specific training data is limited. Further investigation into three target tasks focusing on different capabilities demon
    
[^75]: 视觉问答任务中的LXMERT模型压缩

    LXMERT Model Compression for Visual Question Answering. (arXiv:2310.15325v1 [cs.CV])

    [http://arxiv.org/abs/2310.15325](http://arxiv.org/abs/2310.15325)

    本文通过组合大规模预训练模型的观察结果，并评估在视觉问答任务中对LXMERT进行微调时的可训练子网络，研究了LXMERT模型的压缩。实验结果表明，在仅损失3%的精度下，可以有效地通过剪枝方法将LXMERT模型大小减小40%-60%。

    

    大规模预训练模型如LXMERT在文本-图像对上学习跨模态表示变得流行。根据中彩票假说，自然语言处理和计算机视觉模型中包含可单独训练以达到完全性能的较小子网络。本文结合这些观察结果，评估在VQA任务上对LXMERT进行微调时是否存在这样的可训练子网络。此外，我们通过研究可以进行多少剪枝而不会造成显著的精度损失来进行模型大小成本收益分析。我们的实验结果表明，对LXMERT进行40%-60%的有效剪枝，仅会损失3%的精度。

    Large-scale pretrained models such as LXMERT are becoming popular for learning cross-modal representations on text-image pairs for vision-language tasks. According to the lottery ticket hypothesis, NLP and computer vision models contain smaller subnetworks capable of being trained in isolation to full performance. In this paper, we combine these observations to evaluate whether such trainable subnetworks exist in LXMERT when fine-tuned on the VQA task. In addition, we perform a model size cost-benefit analysis by investigating how much pruning can be done without significant loss in accuracy. Our experiment results demonstrate that LXMERT can be effectively pruned by 40%-60% in size with 3% loss in accuracy.
    
[^76]: 为基于指导性说明生成的幻觉检测问题

    Hallucination Detection for Grounded Instruction Generation. (arXiv:2310.15319v1 [cs.CL])

    [http://arxiv.org/abs/2310.15319](http://arxiv.org/abs/2310.15319)

    该论文研究了生成指导人类在模拟环境中导航的说明的问题，通过预训练模型并使用对比损失进行微调，提出了一种检测幻觉参考的模型，该模型在性能上超过了几个基线模型。

    

    我们研究了在模拟的住宅环境中生成指导人类导航的说明的问题。目前模型存在的一个重要问题是幻觉：它们生成与人类跟随者在描述的路径上执行或遇到的行为或物体不一致的参考。我们开发了一个模型，通过采用在大型图像-文本对语料库上预训练的模型，并使用对比损失进行微调，检测这些幻觉参考。我们的最终模型胜过了几个基线模型，包括使用由说明生成模型估计的词概率以及基于LSTM和Transformer的监督模型。

    We investigate the problem of generating instructions to guide humans to navigate in simulated residential environments. A major issue with current models is hallucination: they generate references to actions or objects that are inconsistent with what a human follower would perform or encounter along the described path. We develop a model that detects these hallucinated references by adopting a model pre-trained on a large corpus of image-text pairs, and fine-tuning it with a contrastive loss that separates correct instructions from instructions containing synthesized hallucinations. Our final model outperforms several baselines, including using word probability estimated by the instruction-generation model, and supervised models based on LSTM and Transformer.
    
[^77]: 探索大型语言模型在生成入门编程课程中的代码追踪问题中的潜力

    Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses. (arXiv:2310.15317v1 [cs.CL])

    [http://arxiv.org/abs/2310.15317](http://arxiv.org/abs/2310.15317)

    本文探索了在入门编程课程中应用大型语言模型生成多样化的代码追踪问题，并提供了评估模型质量的人工评估指标和宝贵的数据集，为教育和自然语言处理研究提供了重要资源。

    

    本文研究了在入门编程课程中应用大型语言模型（LLMs）生成代码追踪问题的应用。我们针对GPT4设计了目标提示，引导其基于代码片段和描述生成代码追踪问题。我们建立了一组人工评估指标，以评估模型生成的问题与人类专家创建的问题的质量。我们的分析提供了关于LLMs在生成多样化代码追踪问题方面的能力和潜力的见解。此外，我们还提供了一个独特的人工和LLM生成的追踪问题数据集，为教育和自然语言处理研究界提供了宝贵的资源。这项工作对于对LLMs在教育环境中潜在用途的持续对话作出了贡献。

    In this paper, we explore the application of large language models (LLMs) for generating code-tracing questions in introductory programming courses. We designed targeted prompts for GPT4, guiding it to generate code-tracing questions based on code snippets and descriptions. We established a set of human evaluation metrics to assess the quality of questions produced by the model compared to those created by human experts. Our analysis provides insights into the capabilities and potential of LLMs in generating diverse code-tracing questions. Additionally, we present a unique dataset of human and LLM-generated tracing questions, serving as a valuable resource for both the education and NLP research communities. This work contributes to the ongoing dialogue on the potential uses of LLMs in educational settings.
    
[^78]: 探测用于文档级事件抽取的表示

    Probing Representations for Document-level Event Extraction. (arXiv:2310.15316v1 [cs.CL])

    [http://arxiv.org/abs/2310.15316](http://arxiv.org/abs/2310.15316)

    这项研究首次将探测范式应用于文档级信息抽取表示，发现训练得到的编码器嵌入可以提高参数检测和标注，但对于事件级任务的改进有限，且存在在连贯性和事件类型预测方面的权衡。

    

    探测分类器框架已被应用于解释深度神经网络模型在各种自然语言处理（NLP）应用中的表现。然而，研究主要集中在句子级NLP任务上。本研究首次将探测范式应用于学习用于文档级信息抽取（IE）的表示。我们设计了八个嵌入式探针，用于分析与文档级事件抽取相关的表面、语义和事件理解能力。我们将它们应用于从三种不同的基于LLM的文档级IE方法中学习的模型得到的表示上。我们发现，这些模型训练的编码器得到的嵌入可以适度地提高参数检测和标注，但只能微弱地增强事件级任务，尽管在有助于连贯性和事件类型预测的信息上存在一些权衡。我们进一步发现，编码器模型在处理文档长度和跨句子的话语方面存在困难。

    The probing classifiers framework has been employed for interpreting deep neural network models for a variety of natural language processing (NLP) applications. Studies, however, have largely focused on sentencelevel NLP tasks. This work is the first to apply the probing paradigm to representations learned for document-level information extraction (IE). We designed eight embedding probes to analyze surface, semantic, and event-understanding capabilities relevant to document-level event extraction. We apply them to the representations acquired by learning models from three different LLM-based document-level IE approaches on a standard dataset. We found that trained encoders from these models yield embeddings that can modestly improve argument detections and labeling but only slightly enhance event-level tasks, albeit trade-offs in information helpful for coherence and event-type prediction. We further found that encoder models struggle with document length and cross-sentence discourse.
    
[^79]: 基于批评地名学的命名实体识别框架：以纽约市Airbnb为例

    Toward a Critical Toponymy Framework for Named Entity Recognition: A Case Study of Airbnb in New York City. (arXiv:2310.15302v1 [cs.CL])

    [http://arxiv.org/abs/2310.15302](http://arxiv.org/abs/2310.15302)

    通过研究纽约市Airbnb房源数据集，本文提出了一个基于批评地名学的命名实体识别（NER）模型，能够识别与地点特征化相关的重要话语类别，为批评地名学研究指明了新的方向。

    

    批评地名学通过地名和相关地点研究权力、资本和抵抗的动态。传统研究主要关注地名的语义内容和自上而下的制度过程，但通常忽视了普通人在日常话语中使用地名的方式，以及与地名相关的地理空间描述策略。本文通过对2010年代的47,440个纽约市Airbnb房源进行创新的注释数据集构建，开发计算方法以衡量文化和经济资本如何影响人们对地点的称谓方式。基于该数据集，我们引入了一个新的命名实体识别（NER）模型，能够识别与地点特征化相关的重要话语类别。我们的研究结果指向批评地名学的新方向，以及一系列以前未被充分研究的语言学问题。

    Critical toponymy examines the dynamics of power, capital, and resistance through place names and the sites to which they refer. Studies here have traditionally focused on the semantic content of toponyms and the top-down institutional processes that produce them. However, they have generally ignored the ways in which toponyms are used by ordinary people in everyday discourse, as well as the other strategies of geospatial description that accompany and contextualize toponymic reference. Here, we develop computational methods to measure how cultural and economic capital shape the ways in which people refer to places, through a novel annotated dataset of 47,440 New York City Airbnb listings from the 2010s. Building on this dataset, we introduce a new named entity recognition (NER) model able to identify important discourse categories integral to the characterization of place. Our findings point toward new directions for critical toponymy and to a range of previously understudied linguist
    
[^80]: 任务导向对话的相似度度量方法

    TaskDiff: A Similarity Metric for Task-Oriented Conversations. (arXiv:2310.15298v1 [cs.CL])

    [http://arxiv.org/abs/2310.15298](http://arxiv.org/abs/2310.15298)

    TaskDiff是一种新颖的对话相似度度量方法，通过使用不同的对话组成部分来计算相似度，取得了优越的性能和鲁棒性。

    

    会话式数字助手的普及导致了大量会话数据的可用性，这可以用于改善用户体验和个性化响应生成。使用像ChatGPT这样的流行大型语言模型构建这些助手还需要额外强调提示工程和评估方法。文本相似度度量是这种分析和评估的关键因素。虽然文献中提出了许多相似度度量方法，但它们在任务导向对话方面并不有效，因为它们没有充分利用独特的对话特征。为了填补这一差距，我们提出了一种新颖的对话相似度度量方法TaskDiff，它利用对话的不同组成部分（话语、意图和槽）及其分布来计算相似度。在基准数据集上进行的广泛实验证明了TaskDiff在性能和鲁棒性方面的优越表现，超过了其他相关方法。

    The popularity of conversational digital assistants has resulted in the availability of large amounts of conversational data which can be utilized for improved user experience and personalized response generation. Building these assistants using popular large language models like ChatGPT also require additional emphasis on prompt engineering and evaluation methods. Textual similarity metrics are a key ingredient for such analysis and evaluations. While many similarity metrics have been proposed in the literature, they have not proven effective for task-oriented conversations as they do not take advantage of unique conversational features. To address this gap, we present TaskDiff, a novel conversational similarity metric that utilizes different dialogue components (utterances, intents, and slots) and their distributions to compute similarity. Extensive experimental evaluation of TaskDiff on a benchmark dataset demonstrates its superior performance and improved robustness over other rela
    
[^81]: DeTiME: 使用基于编码-解码的LLM增强扩散的主题建模方法

    DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM. (arXiv:2310.15296v1 [cs.CL])

    [http://arxiv.org/abs/2310.15296](http://arxiv.org/abs/2310.15296)

    DeTiME是一种使用基于编码-解码的LLMs增强扩散的主题建模方法，能够产生高度聚类的嵌入和具有增强语义一致性的主题，并能生成与主题相关的内容。

    

    在充满活力的自然语言处理领域，神经主题模型（NTMs）和大型语言模型（LLMs）已成为重要的研究领域。尽管如此，NTMs主要使用来自LLMs的上下文嵌入，这对于聚类或主题生成来说并不是最佳选择。我们的研究通过引入名为DeTiME的新框架来解决这一问题。DeTiME利用编码-解码的LLMs产生高度可聚类的嵌入，与现有方法相比，能够生成既具有优越的聚类性又具有增强的语义一致性的主题。此外，通过利用扩散的能力，我们的框架还提供了生成与已识别主题相关内容的能力。这种双重功能使用户能够同时高效产生高度聚类的主题和相关内容。DeTiME的潜力还包括生成集群化的嵌入。

    In the burgeoning field of natural language processing, Neural Topic Models (NTMs) and Large Language Models (LLMs) have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic generation. Our study addresses this gap by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages ncoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion, our framework also provides the capability to generate content relevant to the identified topics. This dual functionality allows users to efficiently produce highly clustered topics and related content simultaneously. DeTiME's potential extends to generating clustered embeddi
    
[^82]: 自适应端到端度量学习用于零样本跨领域槽填充

    Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling. (arXiv:2310.15294v1 [cs.CL])

    [http://arxiv.org/abs/2310.15294](http://arxiv.org/abs/2310.15294)

    本文提出了一种自适应端到端度量学习方法，用于解决零样本跨领域槽填充问题。通过引入上下文感知的软标签表示和槽级对比表示学习，该方法能够提高计算效率和泛化能力，减轻领域转移带来的挑战。

    

    最近，由于深度学习和大规模标注数据的可用性，槽填充在发展过程中取得了很大的进展。然而，处理在训练期间从未见过的新领域带来了一个关键挑战。由于严重的领域转移，识别性能可能会大大降低。大多数之前的工作通过基于度量学习的两次传递流程来处理这个问题。但实际上，这些主导流水线模型可能在计算效率和泛化能力方面受限，因为存在非并行推理和无上下文离散标签嵌入。为此，我们重新审视了典型的基于度量的方法，并提出了一种新的适应性端到端度量学习方案，针对挑战性的零样本槽填充。考虑到简洁性、高效性和泛化性，我们提出了一种级联式联合学习框架，配合上下文感知的软标签表示和槽级对比表示学习，以减轻领域转移带来的挑战。

    Recently slot filling has witnessed great development thanks to deep learning and the availability of large-scale annotated data. However, it poses a critical challenge to handle a novel domain whose samples are never seen during training. The recognition performance might be greatly degraded due to severe domain shifts. Most prior works deal with this problem in a two-pass pipeline manner based on metric learning. In practice, these dominant pipeline models may be limited in computational efficiency and generalization capacity because of non-parallel inference and context-free discrete label embeddings. To this end, we re-examine the typical metric-based methods, and propose a new adaptive end-to-end metric learning scheme for the challenging zero-shot slot filling. Considering simplicity, efficiency and generalizability, we present a cascade-style joint learning framework coupled with context-aware soft label representations and slot-level contrastive representation learning to mitig
    
[^83]: 论句子嵌入的维度问题

    On the Dimensionality of Sentence Embeddings. (arXiv:2310.15285v1 [cs.CL])

    [http://arxiv.org/abs/2310.15285](http://arxiv.org/abs/2310.15285)

    本文对句子嵌入的维度进行了全面而实证的分析，证明了最佳维度通常比默认值要小，并提出了一种两步训练方法来在维度压缩时最小化性能损失。

    

    学习句子嵌入是自然语言处理中一个基本问题。现有的研究主要集中在提高句子嵌入的质量上，而对句子嵌入的维度探索有限。本文对句子嵌入的维度进行了全面而实证的分析。首先，我们证明了句子嵌入的最佳维度通常比默认值要小。接着，为了在维度压缩时最小化性能损失，我们识别了两个影响整体性能损失的组成部分：编码器的性能损失和池化器的性能损失。因此，我们提出了一种两步训练方法来进行句子表示学习模型的训练，其中编码器和池化器分别进行优化以减轻低维度情况下的整体性能损失。在七个STS任务和七个句子分类任务上的实验结果表明...

    Learning sentence embeddings is a fundamental problem in natural language processing. While existing research primarily focuses on enhancing the quality of sentence embeddings, the exploration of sentence embedding dimensions is limited. Here we present a comprehensive and empirical analysis of the dimensionality of sentence embeddings. First, we demonstrate that the optimal dimension of sentence embeddings is usually smaller than the default value. Subsequently, to compress the dimension of sentence embeddings with minimum performance degradation, we identify two components contributing to the overall performance loss: the encoder's performance loss and the pooler's performance loss. Therefore, we propose a two-step training method for sentence representation learning models, wherein the encoder and the pooler are optimized separately to mitigate the overall performance loss in low-dimension scenarios. Experimental results on seven STS tasks and seven sentence classification tasks dem
    
[^84]: 识别加权树相邻语言的高效算法

    Efficient Algorithms for Recognizing Weighted Tree-Adjoining Languages. (arXiv:2310.15276v1 [cs.CL])

    [http://arxiv.org/abs/2310.15276](http://arxiv.org/abs/2310.15276)

    本研究提出了识别加权树相邻语言的高效算法，通过定义半环加权版本的两级形式化方式，并设计新的算法来计算字符串总和和全部总和。对于线性索引文法，算法在时间和空间效率上表现更优。

    

    树相邻语言的类别可以通过各种二级形式化方式来进行特征化，包括一个上下文无关文法（CFG）或下推自动机（PDA）控制另一个CFG或PDA。这四个形式等价于树相邻文法（TAG）、线性索引文法（LIG）、下推相邻自动机（PAA）和嵌套下推自动机（EPDA）。我们定义了上述两级形式化方式的半环加权版本，并设计了新的算法来计算它们的字符串总和（一个字符串的所有推导的权重）和全部总和（所有推导的权重）。根据这些，我们还可以立即得到TAG、LIG、PAA和EPDA的字符串总和和全部总和算法。对于LIG，我们的算法在时间效率上比因素$\mathcal{O}(n|\mathcal{N}|)$（其中$n$是字符串长度，$|\mathcal{N}|$是非终结符集合的大小）更高，而在空间效率上比因素$\mathcal{O}(|\Gamma|)$（其中$|\Gamma|$是栈字母表的大小）更高。

    The class of tree-adjoining languages can be characterized by various two-level formalisms, consisting of a context-free grammar (CFG) or pushdown automaton (PDA) controlling another CFG or PDA. These four formalisms are equivalent to tree-adjoining grammars (TAG), linear indexed grammars (LIG), pushdown-adjoining automata (PAA), and embedded pushdown automata (EPDA). We define semiring-weighted versions of the above two-level formalisms, and we design new algorithms for computing their stringsums (the weight of all derivations of a string) and allsums (the weight of all derivations). From these, we also immediately obtain stringsum and allsum algorithms for TAG, LIG, PAA, and EPDA. For LIG, our algorithm is more time-efficient by a factor of $\mathcal{O}(n|\mathcal{N}|)$ (where $n$ is the string length and $|\mathcal{N}|$ is the size of the nonterminal set) and more space-efficient by a factor of $\mathcal{O}(|\Gamma|)$ (where $|\Gamma|$ is the size of the stack alphabet) than the alg
    
[^85]: GradSim: 基于梯度相似性的语言分组方法用于有效的多语言训练

    GradSim: Gradient-Based Language Grouping for Effective Multilingual Training. (arXiv:2310.15269v1 [cs.LG])

    [http://arxiv.org/abs/2310.15269](http://arxiv.org/abs/2310.15269)

    在本文中，我们提出了一种基于梯度相似性的语言分组方法，名为GradSim。它通过选择最合适的语言集合进行多语言训练，避免了负面干扰，并在多个基准数据集上取得了最先进的性能表现。此外，我们的分析还揭示了数据集的主题对模型的性能也起着重要作用。

    

    世界上大多数语言都对自然语言处理模型提出了低资源的挑战。通过多语言训练，可以在语言之间共享知识。然而，并不是所有语言都能互相积极地影响，如何选择最合适的语言集合进行多语言训练，并避免那些特征或数据分布不兼容的语言之间的负面干扰，这是一个开放的研究问题。在本文中，我们提出了一种基于梯度相似性的语言分组方法，称为GradSim。我们在三个不同的多语言基准数据集上的实验证明，相较于其他相似性度量，GradSim可以带来最大的性能提升，并与跨语言模型的性能更好地相关。结果是，我们在 AfriSenti 上建立了新的最先进模型，这是一个用于对低资源非洲语言进行情感分析的基准数据集。在我们的广泛分析中，我们进一步揭示了除语言特征外，数据集的主题也起着重要的作用。

    Most languages of the world pose low-resource challenges to natural language processing models. With multilingual training, knowledge can be shared among languages. However, not all languages positively influence each other and it is an open research question how to select the most suitable set of languages for multilingual training and avoid negative interference among languages whose characteristics or data distributions are not compatible. In this paper, we propose GradSim, a language grouping method based on gradient similarity. Our experiments on three diverse multilingual benchmark datasets show that it leads to the largest performance gains compared to other similarity measures and it is better correlated with cross-lingual model performance. As a result, we set the new state of the art on AfriSenti, a benchmark dataset for sentiment analysis on low-resource African languages. In our extensive analysis, we further reveal that besides linguistic features, the topics of the datase
    
[^86]: 对AI生成文本检测的可能性和不可能性进行综述

    Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey. (arXiv:2310.15264v1 [cs.CL])

    [http://arxiv.org/abs/2310.15264](http://arxiv.org/abs/2310.15264)

    本文综述了AI生成文本检测的可能性和不可能性。具体而言，讨论了使用大型语言模型产生的文本可能导致的问题以及表明AI生成文本检测的意义。另外，还提到了对抗检测的策略的设计。

    

    大型语言模型（LLMs）以其生成人类化文本响应的显著能力，彻底改变了自然语言处理（NLP）领域。然而，尽管取得了这些进展，现有文献中的一些工作对LLMs的潜在滥用问题提出了严重关注，如传播错误信息、生成假新闻、学术抄袭和污染网络。为了解决这些问题，研究界达成共识，即开发用于检测AI生成文本的算法解决方案。基本思想是，只要我们能判断给定文本是由人类还是AI编写的，我们就可以利用这些信息来应对上述问题。为此，提出了大量的检测框架，突出了AI生成文本检测的可能性。然而，与检测框架的发展同时，研究人员还致力于设计规避检测的策略。

    Large Language Models (LLMs) have revolutionized the domain of natural language processing (NLP) with remarkable capabilities of generating human-like text responses. However, despite these advancements, several works in the existing literature have raised serious concerns about the potential misuse of LLMs such as spreading misinformation, generating fake news, plagiarism in academia, and contaminating the web. To address these concerns, a consensus among the research community is to develop algorithmic solutions to detect AI-generated text. The basic idea is that whenever we can tell if the given text is either written by a human or an AI, we can utilize this information to address the above-mentioned concerns. To that end, a plethora of detection frameworks have been proposed, highlighting the possibilities of AI-generated text detection. But in parallel to the development of detection frameworks, researchers have also concentrated on designing strategies to elude detection, i.e., f
    
[^87]: 用于代码交替文本机器翻译的数据增强技术：一项比较研究

    Data Augmentation Techniques for Machine Translation of Code-Switched Texts: A Comparative Study. (arXiv:2310.15262v1 [cs.CL])

    [http://arxiv.org/abs/2310.15262](http://arxiv.org/abs/2310.15262)

    这项研究对比了三种常用的增强方法：词汇替换、语言理论和回译，发现回译和基于CSW预测的词汇替换在机器翻译任务上表现最佳，语言理论和随机词汇替换在缺乏CSW平行数据的情况下也能达到相似的效果。

    

    随着数据稀缺问题的日益引起关注，代码交替（CSW）文本生成作为解决方案受到越来越多的关注。在这个增长的兴趣背景下，我们需要更多全面的研究来比较不同的增强方法。在这项工作中，我们比较了三种流行的方法：词汇替换、语言理论和回译（BT），在埃及阿拉伯语-英语CSW的背景下。通过人工评估，我们评估了这些方法在机器翻译上的效果以及增强的质量。我们显示回译和基于CSW预测的词汇替换在两个任务上表现最佳，因为它们是在CSW的平行数据上训练的。而在缺乏CSW平行数据的情况下，语言理论和随机词汇替换证明是有效的，两种方法的结果相似。

    Code-switching (CSW) text generation has been receiving increasing attention as a solution to address data scarcity. In light of this growing interest, we need more comprehensive studies comparing different augmentation approaches. In this work, we compare three popular approaches: lexical replacements, linguistic theories, and back-translation (BT), in the context of Egyptian Arabic-English CSW. We assess the effectiveness of the approaches on machine translation and the quality of augmentations through human evaluation. We show that BT and CSW predictive-based lexical replacement, being trained on CSW parallel data, perform best on both tasks. Linguistic theories and random lexical replacement prove to be effective in the lack of CSW parallel data, where both approaches achieve similar results.
    
[^88]: 无参考领域自适应翻译带有问题特定奖励的噪声问题

    Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards. (arXiv:2310.15259v1 [cs.CL])

    [http://arxiv.org/abs/2310.15259](http://arxiv.org/abs/2310.15259)

    这项研究提出了一种解决噪声环境中问题翻译挑战的方法，通过只使用源语数据进行微调的训练，实现了翻译问题的充分性和流畅性的平衡。

    

    社区问答(CQA)平台是帮助组织内用户的有价值工具。然而，使它们对非英语用户可访问仍然是一个挑战。翻译问题可以拓宽社区的覆盖范围，使有类似问题的人能够受益，但在嘈杂的环境中使用神经机器翻译(NMT)进行问题翻译会面临更多挑战，因为这些问题的语法正确性没有受到监控。这些问题可能被非母语用户以陈述句的形式表达，具有不正确的主谓语序，甚至有时缺少问号。由于数据存在噪声，从这些数据中创建一个合成的平行语料库也是困难的。为了解决这个问题，我们提出了一种只使用源语数据进行微调的训练方法。我们的方法通过结合BERTScore和Masked Language Model (MLM) S的损失函数，平衡了充分性和流畅性。

    Community Question-Answering (CQA) portals serve as a valuable tool for helping users within an organization. However, making them accessible to non-English-speaking users continues to be a challenge. Translating questions can broaden the community's reach, benefiting individuals with similar inquiries in various languages. Translating questions using Neural Machine Translation (NMT) poses more challenges, especially in noisy environments, where the grammatical correctness of the questions is not monitored. These questions may be phrased as statements by non-native speakers, with incorrect subject-verb order and sometimes even missing question marks. Creating a synthetic parallel corpus from such data is also difficult due to its noisy nature. To address this issue, we propose a training methodology that fine-tunes the NMT system only using source-side data. Our approach balances adequacy and fluency by utilizing a loss function that combines BERTScore and Masked Language Model (MLM) S
    
[^89]: 打破语言障碍：通过结构化自注意力提高跨语言推理能力

    Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention. (arXiv:2310.15258v1 [cs.CL])

    [http://arxiv.org/abs/2310.15258](http://arxiv.org/abs/2310.15258)

    本研究通过研究多语言语言模型（MultiLMs）在不同语言中进行推理时的细调，发现在单语言环境下它们可以传递推理能力，但在代码切换环境下难以实现推理能力的传递。基于此观察，我们提出了一种新的注意机制来鼓励跨语言推理。

    

    在这项工作中，我们研究了当多语言语言模型（MultiLMs）在不同语言中进行推理的细调时，它们是否能够将逻辑推理能力转移到其他语言中。我们评估了MultiLMs在两种方案下的跨语言推理能力：（1）在测试的新语言中，上下文和问题的语言保持不变（即推理仍然是单语言的，但模型必须在语言间传递学习到的推理能力），以及（2）上下文和问题的语言不同（我们称之为代码切换推理）。在两个逻辑推理数据集RuleTaker和LeapOfThought上，我们证明虽然MultiLMs在单语言环境中可以跨语言传递推理能力，但在代码切换环境中难以实现推理能力的传递。基于此观察，我们提出了一种使用专门的一组参数来鼓励跨语言推理的新型注意机制。

    In this work, we study whether multilingual language models (MultiLMs) can transfer logical reasoning abilities to other languages when they are fine-tuned for reasoning in a different language. We evaluate the cross-lingual reasoning abilities of MultiLMs in two schemes: (1) where the language of the context and the question remain the same in the new languages that are tested (i.e., the reasoning is still monolingual, but the model must transfer the learned reasoning ability across languages), and (2) where the language of the context and the question is different (which we term code-switched reasoning). On two logical reasoning datasets, RuleTaker and LeapOfThought, we demonstrate that although MultiLMs can transfer reasoning ability across languages in a monolingual setting, they struggle to transfer reasoning abilities in a code-switched setting. Following this observation, we propose a novel attention mechanism that uses a dedicated set of parameters to encourage cross-lingual at
    
[^90]: CRoW: 在真实世界任务中对常识推理进行基准测试

    CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks. (arXiv:2310.15239v1 [cs.CL])

    [http://arxiv.org/abs/2310.15239](http://arxiv.org/abs/2310.15239)

    CRoW是一个手工策划的多任务基准测试，用于评估模型在真实世界NLP任务中应用常识推理的能力。该基准测试揭示了NLP系统在常识推理方面与人类之间存在显著的性能差距，表明常识推理在真实世界中仍然远未解决。

    

    自然语言处理（NLP）中关于常识推理的研究近年来取得了许多新的数据集和基准测试。然而，大多数这些数据集在人工场景下构建了常识推理挑战，这些场景并不能反映真实世界NLP系统所设计用于解决的任务。在这项工作中，我们提出了CRoW，一个手工策划的多任务基准测试，用于评估模型在六个真实世界NLP任务中应用常识推理的能力。CRoW使用多阶段的数据收集流程构建，通过违反常识的扰动重写现有数据集中的示例。我们利用CRoW来研究NLP系统在物理、时间和社交推理等不同常识知识维度上的表现。我们发现，在CRoW上评估NLP系统时与人类相比存在显著的性能差距，显示出常识推理在真实世界中还远未解决。

    Recent efforts in natural language processing (NLP) commonsense reasoning research have yielded a considerable number of new datasets and benchmarks. However, most of these datasets formulate commonsense reasoning challenges in artificial scenarios that are not reflective of the tasks which real-world NLP systems are designed to solve. In this work, we present CRoW, a manually-curated, multi-task benchmark that evaluates the ability of models to apply commonsense reasoning in the context of six real-world NLP tasks. CRoW is constructed using a multi-stage data collection pipeline that rewrites examples from existing datasets using commonsense-violating perturbations. We use CRoW to study how NLP systems perform across different dimensions of commonsense knowledge, such as physical, temporal, and social reasoning. We find a significant performance gap when NLP systems are evaluated on CRoW compared to humans, showcasing that commonsense reasoning is far from being solved in real-world t
    
[^91]: 大型语言模型中的函数向量

    Function Vectors in Large Language Models. (arXiv:2310.15213v1 [cs.CL])

    [http://arxiv.org/abs/2310.15213](http://arxiv.org/abs/2310.15213)

    大型语言模型中存在一种简单的神经机制，将输入-输出函数表示为向量。这些函数向量在不同的上下文中具有鲁棒性，并且具有强大的因果效应。同时，它们还具有将语义向量进行组合的能力。

    

    我们报告了一个简单的神经机制，将输入-输出函数表示为自回归变换语言模型（LMs）中的向量。通过在各种上下文学习（ICL）任务上使用因果中介分析，我们发现少数注意力头传输了展示任务的紧凑表示，我们称之为函数向量（FV）。FV对上下文的变化具有鲁棒性，即它们在不类似于其收集时的ICL上下文的情况下触发对输入的任务执行，例如零样本和自然文本设置。我们在各种任务、模型和层上测试了FV，并在中层发现强大的因果效应。我们研究了FV的内部结构，并发现虽然它们通常包含编码函数的输出空间的信息，但仅此信息无法重构FV。最后，我们测试了FV中的语义向量组合，并发现在某种程度上存在组合的能力。

    We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some exte
    
[^92]: 基于多专家微调的中国金融大型语言模型DISC-FinLLM

    DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning. (arXiv:2310.15205v1 [cs.CL])

    [http://arxiv.org/abs/2310.15205](http://arxiv.org/abs/2310.15205)

    我们提出了一种基于多专家微调的金融大型语言模型DISC-FinLLM，通过赋予模型多轮问答、领域文本处理、数学计算和检索增强生成能力，我们的模型在多个金融场景中表现出更好的性能。

    

    我们提出了一种基于多专家微调框架的金融大型语言模型DISC-FinLLM。我们的方法通过赋予通用语言模型多轮问答能力、领域文本处理能力、数学计算技能和检索增强生成能力来改进通用语言模型。我们构建了一个金融指令微调数据集DISC-FIN-SFT，包括四个分类的指令样本（咨询、自然语言处理任务、计算和检索增强生成）。在多个基准测试上进行的评估表明，我们的模型在各种金融场景中优于基准模型。更多资源可以在https://github.com/FudanDISC/DISC-FinLLM找到。

    We propose Multiple Experts Fine-tuning Framework to build a financial large language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by endowing them with multi-turn question answering abilities, domain text processing capabilities, mathematical computation skills, and retrieval-enhanced generation capabilities. We build a financial instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of four categories (consulting, NLP tasks, computing and retrieval-augmented generation). Evaluations conducted on multiple benchmarks demonstrate that our model performs better than baseline models in various financial scenarios. Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.
    
[^93]: 语言模型在不平衡文本分类中的元学习: 挑战与机遇

    Meta learning with language models: Challenges and opportunities in the classification of imbalanced text. (arXiv:2310.15019v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15019](http://arxiv.org/abs/2310.15019)

    本文提出了一种元学习技术(MLT)，通过将不同文本表示构建的个体模型进行组合，在不平衡的文本分类中提高了性能，并通过阈值移动技术进一步改善了预测器的性能。

    

    检测违规言论内容是重要但困难的。虽然机器学习是应对这一挑战性任务的强大工具，但由于训练数据的数量和质量限制以及违规定义和数据标注的不一致性等因素，难以突破性能瓶颈。为了充分发挥有限资源的潜力，我们提出了一种元学习技术(MLT)，它将使用不同文本表示构建的个体模型进行组合。我们通过分析证明，所得到的技术在数值上是稳定的，并产生合理的组合权重。我们将MLT与阈值移动(TM)技术相结合，进一步提高组合预测器在高度不平衡的分布和超出分布数据集上的性能。我们还提供了计算结果，展示了所提出的MLT方法的统计优势。所有作者对这项工作贡献相同。

    Detecting out of policy speech (OOPS) content is important but difficult. While machine learning is a powerful tool to tackle this challenging task, it is hard to break the performance ceiling due to factors like quantity and quality limitations on training data and inconsistencies in OOPS definition and data labeling. To realize the full potential of available limited resources, we propose a meta learning technique (MLT) that combines individual models built with different text representations. We analytically show that the resulting technique is numerically stable and produces reasonable combining weights. We combine the MLT with a threshold-moving (TM) technique to further improve the performance of the combined predictor on highly-imbalanced in-distribution and out-of-distribution datasets. We also provide computational results to show the statistically significant advantages of the proposed MLT approach.  All authors contributed equally to this work.
    
[^94]: 空气解码：解码时间可控文本生成的属性分布重建

    Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation. (arXiv:2310.14892v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14892](http://arxiv.org/abs/2310.14892)

    本文提出了一种名为空气解码的新颖轻量级解码框架，通过重建属性分布来平衡权重，生成更流畅的文本，以解决可控文本生成中的属性坍缩问题。

    

    可控的文本生成（CTG）旨在生成具有所需属性的文本，而基于解码时间的方法在这个任务上已经显示出了有希望的性能。然而，在本文中，我们首次发现了属性坍缩现象。当控制强度超过临界值时，它会导致生成文本的流畅性迅速降低，使文本完全无法使用。这个限制阻碍了解码方法在实现高水平可控性方面的有效性。为了解决这个问题，我们提出了一种新颖的轻量级解码框架，名为空气解码。它的主要思想是通过重建属性分布来平衡属性词和非属性词之间的权重，从而生成更流畅的文本。具体而言，我们通过前缀微调来训练前缀以获得属性分布。然后，我们设计了一种新颖的属性分布重建方法来平衡所获得的分布，并使用重建后的分布进行解码。

    Controllable text generation (CTG) aims to generate text with desired attributes, and decoding-time-based methods have shown promising performance on this task. However, in this paper, we identify the phenomenon of Attribute Collapse for the first time. It causes the fluency of generated text to rapidly decrease when the control strength exceeds a critical value, rendering the text completely unusable. This limitation hinders the effectiveness of decoding methods in achieving high levels of controllability. To address this problem, we propose a novel lightweight decoding framework named Air-Decoding. Its main idea is reconstructing the attribute distributions to balance the weights between attribute words and non-attribute words to generate more fluent text. Specifically, we train prefixes by prefix-tuning to obtain attribute distributions. Then we design a novel attribute distribution reconstruction method to balance the obtained distributions and use the reconstructed distributions t
    
[^95]: MCC-KD: 多CoT一致性知识蒸馏

    MCC-KD: Multi-CoT Consistent Knowledge Distillation. (arXiv:2310.14747v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14747](http://arxiv.org/abs/2310.14747)

    MCC-KD方法提出了一种多CoT一致性知识蒸馏的方法，能够高效地转移大型语言模型的推理能力到较小的模型上，通过生成多个理由并确保其预测的一致性来增强推理多样性和一致性。

    

    大型语言模型(LLMs)在复杂推理方面展现了卓越的能力，通过链式思考(CoT)提示。最近，人们对将这些推理能力从LLMs转移到较小模型中的兴趣日益增长。然而，同时实现多样性和一致性的理由对于提出了挑战。在本文中，我们着重增强这两个方面，并提出了多CoT一致性知识蒸馏(MCC-KD)方法，以高效地提取推理能力。在MCC-KD中，我们为每个问题生成多个理由，并通过最小化答案分布之间的双向KL散度来确保相应预测的一致性。我们研究了MCC-KD在不同模型架构(LLaMA/FlanT5)和各种模型规模(3B/7B/11B/13B)上在数学推理和常识推理基准上的有效性。实证结果不仅证实了MCC-KD在分布内情况下的优越性能。

    Large language models (LLMs) have showcased remarkable capabilities in complex reasoning through chain of thought (CoT) prompting. Recently, there has been a growing interest in transferring these reasoning abilities from LLMs to smaller models. However, achieving both the diversity and consistency in rationales presents a challenge. In this paper, we focus on enhancing these two aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple rationales for each question and enforce consistency among the corresponding predictions by minimizing the bidirectional KL-divergence between the answer distributions. We investigate the effectiveness of MCC-KD with different model architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both mathematical reasoning and commonsense reasoning benchmarks. The empirical results not only confirm MCC-KD's superior performance on in-distribution d
    
[^96]: 对LLM生成的文本检测的调查：必要性、方法和未来方向

    A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions. (arXiv:2310.14724v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14724](http://arxiv.org/abs/2310.14724)

    本文对LLM生成的文本检测进行了调查，强调了开发这样的检测器的必要性，并总结了近期的研究创新和未来发展方向。

    

    大型语言模型（LLMs）生成的复杂语言的强大能力使得LLM生成的文本以惊人的速度涌入到我们日常生活的许多领域中，并得到了人们的广泛接受。随着LLMs的不断扩展，迫切需要开发能够检测LLM生成的文本的检测器。这对于减少LLMs潜在的误用，并保护艺术表达和社交网络等领域免受LLM生成内容的有害影响至关重要。LLM生成的文本检测旨在确定一段文本是否由LLM生成，实质上是一个二分类任务。检测器技术最近取得了显著的进展，推动因素包括水印技术、零样本方法、微调语言模型方法、对抗学习方法、将LLMs作为检测器以及人类辅助方法的创新。在这项调查中，我们汇集了最近在这一领域取得的研究突破，并强调了迫切的需求和未来的方向。

    The powerful ability to understand, follow, and generate complex language emerging from large language models (LLMs) makes LLM-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. As LLMs continue to expand, there is an imperative need to develop detectors that can detect LLM-generated text. This is crucial to mitigate potential misuse of LLMs and safeguard realms like artistic expression and social networks from harmful influence of LLM-generated content. The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task. The detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, zero-shot methods, fine-turning LMs methods, adversarial learning methods, LLMs as detectors, and human-assisted methods. In this survey, we collate recent research breakthroughs in this area and underscore the pressin
    
[^97]: SPRING-INX: SPRING实验室印度理工学院马德拉斯分校的多语种印度语音语料库

    SPRING-INX: A Multilingual Indian Language Speech Corpus by SPRING Lab, IIT Madras. (arXiv:2310.14654v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14654](http://arxiv.org/abs/2310.14654)

    SPRING-INX数据是由SPRING实验室开源的多语种印度语音语料库，用于印度语音识别系统的建设。这是为了鼓励语言技术社区在22种印度官方语言中构建基于语音的应用程序而进行的努力。

    

    印度是多语种的，其中有22种语言被印度宪法认定为官方语言。由于数据有限以及需要适应的语言和口音数量，为印度人口构建基于语音的应用程序是一个困难的问题。为了鼓励语言技术社区在印度语言中构建基于语音的应用程序，我们开源了SPRING-INX数据，其中包含了2000小时的合法来源并且经过手工转录的语音数据，用于构建Assamese、Bengali、Gujarati、Hindi、Kannada、Malayalam、Marathi、Odia、Punjabi和Tamil的ASR系统。这个努力是由印度理工学院马德拉斯分校的SPRING实验室进行的，是印度电子和信息技术部（MeitY）资助的国家语言翻译任务（NLTM）的一部分。我们在本文中描述了数据收集和数据清理过程以及数据统计。

    India is home to a multitude of languages of which 22 languages are recognised by the Indian Constitution as official. Building speech based applications for the Indian population is a difficult problem owing to limited data and the number of languages and accents to accommodate. To encourage the language technology community to build speech based applications in Indian languages, we are open sourcing SPRING-INX data which has about 2000 hours of legally sourced and manually transcribed speech data for ASR system building in Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi and Tamil. This endeavor is by SPRING Lab , Indian Institute of Technology Madras and is a part of National Language Translation Mission (NLTM), funded by the Indian Ministry of Electronics and Information Technology (MeitY), Government of India. We describe the data collection and data cleaning process along with the data statistics in this paper.
    
[^98]: 利用ChatGPT进行主题分析：我们准备好了吗？

    Harnessing ChatGPT for thematic analysis: Are we ready?. (arXiv:2310.14545v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14545](http://arxiv.org/abs/2310.14545)

    本研究探讨了在医学背景下利用ChatGPT进行主题分析的应用。我们发现ChatGPT可以在直接编码转录、生成主题和预处理引用等核心阶段发挥重要作用，并提高分析效率和提供额外见解。

    

    ChatGPT是一种先进的自然语言处理工具，在医学研究的各个领域都有广泛的应用。主题分析是一种定性研究方法，用于识别和解释数据中的模式，而这项技术有望从中受益。这篇论文探讨了在医学背景下利用ChatGPT在主题分析的三个核心阶段中的应用：1）对转录进行直接编码，2）从预定义的编码列表中生成主题，以及3）为文稿包含预处理引用。此外，我们还探讨了ChatGPT生成面试转录的潜力，这些转录可用于培训目的。我们评估了在这些角色中使用ChatGPT的优点和局限性，并强调需要人类干预的领域。总的来说，我们认为ChatGPT可以作为分析过程中的有价值工具，提高主题分析的效率，并为定性数据提供额外的见解。

    ChatGPT is an advanced natural language processing tool with growing applications across various disciplines in medical research. Thematic analysis, a qualitative research method to identify and interpret patterns in data, is one application that stands to benefit from this technology. This viewpoint explores the utilization of ChatGPT in three core phases of thematic analysis within a medical context: 1) direct coding of transcripts, 2) generating themes from a predefined list of codes, and 3) preprocessing quotes for manuscript inclusion. Additionally, we explore the potential of ChatGPT to generate interview transcripts, which may be used for training purposes. We assess the strengths and limitations of using ChatGPT in these roles, highlighting areas where human intervention remains necessary. Overall, we argue that ChatGPT can function as a valuable tool during analysis, enhancing the efficiency of the thematic analysis and offering additional insights into the qualitative data.
    
[^99]: 重新思考计算辅助翻译中的单词级自动补全

    Rethinking Word-Level Auto-Completion in Computer-Aided Translation. (arXiv:2310.14523v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14523](http://arxiv.org/abs/2310.14523)

    本论文重新思考了计算辅助翻译中单词级自动补全的问题，引入了一个可衡量的标准来确定好的自动补全，提出了一种有效方法来提高性能，并在实验中证明了其优于目前最佳系统的表现。

    

    单词级自动补全在计算辅助翻译中发挥着关键作用，旨在为人类翻译人员提供单词级自动补全建议。然而，以往的研究主要关注设计复杂的模型架构，而本文从不同的角度重新思考了一个基本问题：什么样的单词是好的自动补全？我们引入了一种可衡量的标准来回答这个问题，并发现现有的单词级自动补全模型往往无法满足这一标准。基于这一观察，我们提出了一种有效的方法来提高单词级自动补全性能，促进对标准的遵循。值得注意的是，所提出的方法是通用的，可以应用于各种基于编码器的架构。通过大量实验，我们证明了我们的方法优于提交给WMT2022单词级自动补全共享任务的最佳系统，同时使用了明显更小的模型尺寸。

    Word-Level Auto-Completion (WLAC) plays a crucial role in Computer-Assisted Translation. It aims at providing word-level auto-completion suggestions for human translators. While previous studies have primarily focused on designing complex model architectures, this paper takes a different perspective by rethinking the fundamental question: what kind of words are good auto-completions? We introduce a measurable criterion to answer this question and discover that existing WLAC models often fail to meet this criterion. Building upon this observation, we propose an effective approach to enhance WLAC performance by promoting adherence to the criterion. Notably, the proposed approach is general and can be applied to various encoder-based architectures. Through extensive experiments, we demonstrate that our approach outperforms the top-performing system submitted to the WLAC shared tasks in WMT2022, while utilizing significantly smaller model sizes.
    
[^100]: CorefPrompt: 基于提示的事件指代消解通过测量事件类型和参数的兼容性

    CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities. (arXiv:2310.14512v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14512](http://arxiv.org/abs/2310.14512)

    CorefPrompt是一种基于提示的方法，通过测量事件类型和参数的兼容性来进行事件指代消解。该方法将事件指代消解转化为一个填空式MLM任务，并通过引入辅助的提示任务来帮助模型进行推理，最终在基准测试中取得了良好的表现。

    

    事件指代消解旨在将指代同一实际事件的事件提及聚类在一起。大多数先前的研究采用“先编码，然后评分”的框架，使得指代消解依赖于事件编码。此外，当前的方法很难利用人工总结的事件指代消解规则，例如，指代同一事件的事件应具有相同的事件类型，以指导模型。为了解决这两个问题，我们提出了一种基于提示的方法CorefPrompt，将事件指代消解转化为一个填空式MLM（掩码语言模型）任务。这样可以在一个单一的模板中同时进行事件建模和指代消解判别，并且具有完全共享的上下文。此外，我们引入了两个辅助的提示任务，事件类型兼容性和参数兼容性，以明确展示事件指代消解的推理过程，从而帮助模型做出最终的预测。实验结果表明，我们的方法CorefPrompt在最先进的基准测试中表现良好。

    Event coreference resolution (ECR) aims to group event mentions referring to the same real-world event into clusters. Most previous studies adopt the "encoding first, then scoring" framework, making the coreference judgment rely on event encoding. Furthermore, current methods struggle to leverage human-summarized ECR rules, e.g., coreferential events should have the same event type, to guide the model. To address these two issues, we propose a prompt-based approach, CorefPrompt, to transform ECR into a cloze-style MLM (masked language model) task. This allows for simultaneous event modeling and coreference discrimination within a single template, with a fully shared context. In addition, we introduce two auxiliary prompt tasks, event-type compatibility and argument compatibility, to explicitly demonstrate the reasoning process of ECR, which helps the model make final predictions. Experimental results show that our method CorefPrompt performs well in a state-of-the-art (SOTA) benchmark.
    
[^101]: 文化和语言多样性提高了视觉表示

    Cultural and Linguistic Diversity Improves Visual Representations. (arXiv:2310.14356v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2310.14356](http://arxiv.org/abs/2310.14356)

    这项研究发现数据集和模型生成的图像描述在不同语言间存在显著的语义差异，多语言数据有更高的语义覆盖率，并且基于多语言训练的模型表现更好。

    

    计算机视觉通常将感知视为客观的，并且这种假设在数据集收集和模型训练中得到反映。例如，不同语言的图像描述通常被假定为相同语义内容的翻译。然而，跨文化心理学和语言学的研究表明，个体的视觉感知因其文化背景和所说的语言而异。在本文中，我们展示了在数据集和模型生成的标题中，不同语言之间存在显著的语义内容差异。当数据是多语言而不是单语言时，标题的语义覆盖率平均更高，以场景图、嵌入和语言复杂性进行测量。例如，与一组单语标题相比，多语标题平均有21.8％更多的对象，24.5％更多的关系，以及27.1％更多的属性。此外，使用来自不同语言的内容训练的模型表现最好。

    Computer vision often treats perception as objective, and this assumption gets reflected in the way that datasets are collected and models are trained. For instance, image descriptions in different languages are typically assumed to be translations of the same semantic content. However, work in cross-cultural psychology and linguistics has shown that individuals differ in their visual perception depending on their cultural background and the language they speak. In this paper, we demonstrate significant differences in semantic content across languages in both dataset and model-produced captions. When data is multilingual as opposed to monolingual, captions have higher semantic coverage on average, as measured by scene graph, embedding, and linguistic complexity. For example, multilingual captions have on average 21.8% more objects, 24.5% more relations, and 27.1% more attributes than a set of monolingual captions. Moreover, models trained on content from different languages perform bes
    
[^102]: 探索语言模型中谄媚行为的理解

    Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])

    [http://arxiv.org/abs/2310.13548](http://arxiv.org/abs/2310.13548)

    这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。

    

    「从人类反馈中进行强化学习（RLHF）」是训练高质量AI助手的一种流行技术。然而，RLHF可能会鼓励模型通过与用户信念相符的回答来代替真实回答，这种行为被称为谄媚行为。我们研究了RLHF训练模型中谄媚行为的普遍性以及人类偏好判断是否起到了作用。首先，我们证明了五个最先进的AI助手在四个不同的自由文本生成任务中一贯表现出谄媚行为。为了理解人类偏好是否驱动了RLHF模型的这种广泛行为，我们分析了现有的人类偏好数据。我们发现，当回答与用户的观点相符时，它更有可能被选中。此外，人类和偏好模型（PMs）将有说服力的谄媚回答与正确回答相比，有时几乎可以忽略不计地选择了谄媚回答。优化模型输出以满足PMs有时也会在真实性和谄媚行为之间做出取舍。

    Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
    
[^103]: 让语言模型清理您的有噪音的翻译数据

    Ask Language Model to Clean Your Noisy Translation Data. (arXiv:2310.13469v1 [cs.CL])

    [http://arxiv.org/abs/2310.13469](http://arxiv.org/abs/2310.13469)

    论文介绍了如何利用大型语言模型清理神经机器翻译中的噪声输入，通过从MTNT数据集中清理目标语句的噪声，生成了C-MTNT数据集，显著减少了噪声。

    

    Transformer模型在神经机器翻译（NMT）中展现出了出色的性能。然而，它们对噪声输入的脆弱性在实际应用中提出了重大挑战，从噪声输入中生成干净的输出至关重要。MTNT数据集被广泛用作评估NMT模型对噪声输入鲁棒性的基准。然而，由于源语句和目标语句中都存在噪声，其实用性受到限制。为解决这一限制，我们专注于清理MTNT中目标语句的噪声，使其更适用于噪声评估的基准。利用大型语言模型（LLM）的能力，我们观察到它们在去噪方面的出色能力。例如，它们可以在考虑语义含义的同时删除表情符号。此外，我们还展示了LLM能够有效地更改俚语、术语和粗口。得到的数据集被称为C-MTNT，噪声显著减少。

    Transformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset \cite{MTNT} is widely used as a benchmark for evaluating the robustness of NMT models against noisy input. Nevertheless, its utility is limited due to the presence of noise in both the source and target sentences. To address this limitation, we focus on cleaning the noise from the target sentences in MTNT, making it more suitable as a benchmark for noise evaluation. Leveraging the capabilities of large language models (LLMs), we observe their impressive abilities in noise removal. For example, they can remove emojis while considering their semantic meaning. Additionally, we show that LLM can effectively rephrase slang, jargon, and profanities. The resulting datasets, called C-MTNT, exhibit significantly less noise 
    
[^104]: ImageArg-2023概述：多模态论证挖掘中的首个共享任务

    Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining. (arXiv:2310.12172v1 [cs.CL])

    [http://arxiv.org/abs/2310.12172](http://arxiv.org/abs/2310.12172)

    ImageArg-2023是第一个多模态论证挖掘的共享任务，涵盖了论证立场分类和图像说服力分类两个子任务。共收到了来自6个国家的9个团队提交的31个子任务A的提交和21个子任务B的提交，最好的提交在子任务A中达到了0.8647的F1得分，在子任务B中达到了0.5561的F1得分。

    

    本文介绍了ImageArg共享任务的概述，这是第一个与EMNLP 2023 Argument Mining Workshop同时举办的多模态论证挖掘共享任务。该共享任务包括两个分类子任务：（1）子任务A：论证立场分类；（2）子任务B：图像说服力分类。前者确定了包含图像和一段文字的推文对于一个有争议的主题（如枪支控制和堕胎）的立场。后者确定图像是否使推文的文字更具说服力。共享任务共收到来自6个国家的9个不同团队提交的31个子任务A的提交和21个子任务B的提交。子任务A中最好的提交的F1得分为0.8647，而子任务B中最好的提交的F1得分为0.5561。

    This paper presents an overview of the ImageArg shared task, the first multimodal Argument Mining shared task co-located with the 10th Workshop on Argument Mining at EMNLP 2023. The shared task comprises two classification subtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image Persuasiveness Classification. The former determines the stance of a tweet containing an image and a piece of text toward a controversial topic (e.g., gun control and abortion). The latter determines whether the image makes the tweet text more persuasive. The shared task received 31 submissions for Subtask-A and 21 submissions for Subtask-B from 9 different teams across 6 countries. The top submission in Subtask-A achieved an F1-score of 0.8647 while the best submission in Subtask-B achieved an F1-score of 0.5561.
    
[^105]: 从不一致到洞察：对案例结果分类的理由数据集构建进行解析

    From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification. (arXiv:2310.11878v1 [cs.CL])

    [http://arxiv.org/abs/2310.11878](http://arxiv.org/abs/2310.11878)

    本研究关注法律自然语言处理中人工标注的变异问题，通过收集一组律师对案件结果评估存在分歧的数据集，对这些分歧进行了研究，构建了一个两级分类体系，并发现分歧主要源于对法律背景的不明确描述。

    

    在法律自然语言处理中，案例结果分类（COC）不仅需要准确性，还需要可信赖性和可解释性。现有的可解释COC研究仅限于由单个专家进行的注释。然而，众所周知，律师在对案件事实进行评估时可能存在分歧。因此，我们收集了一个新的数据集RAVE：欧洲人权法领域的理由变异，该数据集是从国际人权法领域的两位专家那里获得的，我们观察到他们之间存在弱一致性。我们研究了他们的分歧，并构建了一个两级任务无关的分类体系，同时补充了COC特定的子类别。据我们所知，这是法律自然语言处理领域首次关注人工标注的变异。我们定量评估了不同分类类别，并发现分歧主要源于对法律背景的不明确描述，这在COC元数据通常具有有限细粒度和噪声的情况下带来了挑战。我们进一步评估了SOTA COC模型在RAVE数据集上的可解释性，并观察到...

    In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of SOTA COC models on RAVE and observ
    
[^106]: 神经注意力：利用神经网络增强自注意机制中的QKV计算

    Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks. (arXiv:2310.11398v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.11398](http://arxiv.org/abs/2310.11398)

    本文介绍了一种利用神经网络增强自注意机制中QKV计算的方法，实验证明这种方法在多个任务中取得了显著的提升。

    

    在深度学习领域中，自注意机制在自然语言处理和计算机视觉等多个任务中发挥了重要作用。然而，传统的自注意机制主要使用线性变换来计算查询、键和值(QKV)，但在特定情况下，这可能并不是最优选择。本文探讨了一种新的QKV计算方法，采用了特殊设计的神经网络结构进行计算。通过在IWSLT 2017德英翻译任务数据集上使用修改后的Marian模型进行实验，并将我们的方法与传统方法进行对比，实验结果显示我们的方法在BLEU得分方面有显著提升。此外，我们的方法在使用Wikitext-103数据集训练Roberta模型时也表现出优势，显示了显著的改进。

    In the realm of deep learning, the self-attention mechanism has substantiated its pivotal role across a myriad of tasks, encompassing natural language processing and computer vision. Despite achieving success across diverse applications, the traditional self-attention mechanism primarily leverages linear transformations for the computation of query, key, and value (QKV), which may not invariably be the optimal choice under specific circumstances. This paper probes into a novel methodology for QKV computation-implementing a specially-designed neural network structure for the calculation. Utilizing a modified Marian model, we conducted experiments on the IWSLT 2017 German-English translation task dataset and juxtaposed our method with the conventional approach. The experimental results unveil a significant enhancement in BLEU scores with our method. Furthermore, our approach also manifested superiority when training the Roberta model with the Wikitext-103 dataset, reflecting a notable re
    
[^107]: VECHR：欧洲人权法院漏洞类型的可解释和鲁棒分类数据集

    VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights. (arXiv:2310.11368v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.11368](http://arxiv.org/abs/2310.11368)

    VECHR是一个专家注释的多标签数据集，用于欧洲人权法院漏洞类型的可解释和鲁棒分类。该数据集帮助识别脆弱性，并提供解释理由。结果显示了该任务的挑战性，模型与专家的一致性有限，模型在处理域外数据时鲁棒性也较低。

    

    识别脆弱性对于了解和实施有针对性的支持以增强有需要的个人至关重要。这一点在欧洲人权法院尤为重要，法院将公约标准调整为满足实际个体需求，从而确保有效的人权保护。然而，脆弱性概念在欧洲人权法院仍然模糊不清，之前没有NLP研究涉及到这个问题。为了促进未来在这个领域的研究，我们提出了VECHR，一个新颖的专家注释的多标签数据集，包括脆弱性类型分类和解释理由。我们从预测和解释性的角度对VECHR上的最先进模型的性能进行了基准测试。我们的结果表明了这一任务的具有挑战性的特点，预测性能较低，并且模型和专家之间存在有限的一致性。此外，我们分析了这些模型处理域外数据的鲁棒性，并发现总体上受限。

    Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need. This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus ensures effective human rights protection. However, the concept of vulnerability remains elusive at the ECtHR and no prior NLP research has dealt with it. To enable future research in this area, we present VECHR, a novel expert-annotated multi-label dataset comprising of vulnerability type classification and explanation rationale. We benchmark the performance of state-of-the-art models on VECHR from both prediction and explainability perspectives. Our results demonstrate the challenging nature of the task with lower prediction performance and limited agreement between models and experts. Further, we analyze the robustness of these models in dealing with out-of-domain (OOD) data and observe overall limited per
    
[^108]: 利用弱监督生成印尼保护数据集

    Utilizing Weak Supervision To Generate Indonesian Conservation Dataset. (arXiv:2310.11258v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.11258](http://arxiv.org/abs/2310.11258)

    本文展示了如何利用弱监督方法从保护新闻文本构建一种印尼自然语言处理数据集，并提供了多类别分类和情感分类的基线实验结果。此外，作者还发布了使用的数据集和标注函数供进一步研究和探索。

    

    弱监督已经成为一种快速和大规模数据集创建的有希望的方法，以响应加速自然语言处理开发的需求增加。通过利用标注函数，弱监督允许从产生软标签数据集的学习到的标签模型创建数据集。本文旨在展示如何利用这种方法从保护新闻文本构建印尼自然语言处理数据集。我们构建了两种类型的数据集：多类别分类和情感分类。然后我们使用各种预训练语言模型进行基线实验。这些基线结果展示了情感分类为59.79%准确率和55.72% F1分数，多类别分类为66.87% F1分数-宏平均，71.5% F1分数-微平均和83.67% ROC-AUC的测试性能。此外，我们还发布了在这项工作中使用的数据集和标注函数，供进一步的研究和探索使用。

    Weak supervision has emerged as a promising approach for rapid and large-scale dataset creation in response to the increasing demand for accelerated NLP development. By leveraging labeling functions, weak supervision allows practitioners to generate datasets quickly by creating learned label models that produce soft-labeled datasets. This paper aims to show how such an approach can be utilized to build an Indonesian NLP dataset from conservation news text. We construct two types of datasets: multi-class classification and sentiment classification. We then provide baseline experiments using various pretrained language models. These baseline results demonstrate test performances of 59.79% accuracy and 55.72% F1-score for sentiment classification, 66.87% F1-score-macro, 71.5% F1-score-micro, and 83.67% ROC-AUC for multi-class classification. Additionally, we release the datasets and labeling functions used in this work for further research and exploration.
    
[^109]: TRIGO:基于生成语言模型的形式数学证明减缩的基准测试

    TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models. (arXiv:2310.10180v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10180](http://arxiv.org/abs/2310.10180)

    TRIGO是一个基于生成语言模型的形式数学证明减缩的基准测试，要求模型不仅可以按步骤证明减少三角表达式，还可以评估其对公式的推理能力，以及操作、分组和因式分解数字项的能力。

    

    自动定理证明(ATP)已成为探索最新成功的生成语言模型推理能力的吸引人领域。然而，当前的ATP基准主要集中在符号推理上，很少涉及复杂数字组合推理的理解。在这项工作中，我们提出TRIGO，一个ATP基准测试，不仅要求模型按照步骤证明减少三角表达式，还要评估生成语言模型对公式的推理能力以及操作、分组和因式分解数字项的能力。我们从网络上收集三角表达式及其简化形式，手动标注简化过程，并将其翻译成Lean形式化语言系统。然后，我们根据标注样本自动生成额外的示例来扩展数据集。此外，我们基于Lean-Gym开发了一个自动生成器，用于创建难度和分布各异的数据集拆分，以便进行全面的评估。

    Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning. In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proofs but also evaluates a generative LM's reasoning ability on formulas and its capability to manipulate, group, and factor number terms. We gather trigonometric expressions and their reduced forms from the web, annotate the simplification process manually, and translate it into the Lean formal language system. We then automatically generate additional examples from the annotated samples to expand the dataset. Furthermore, we develop an automatic generator based on Lean-Gym to create dataset splits of varying difficulties and distributions in order to thoroug
    
[^110]: 探索指导：通过主动探索增强特定领域指导覆盖率

    Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration. (arXiv:2310.09168v1 [cs.CL])

    [http://arxiv.org/abs/2310.09168](http://arxiv.org/abs/2310.09168)

    通过采用探索指导的方法，使用大型语言模型 (LLMs) 进行主动探索，增强了领域特定指导调优的数据覆盖范围，并取得了显著的性能提升。

    

    通过增强多样性，可以大幅优化指导调优，从而使模型能够处理更广泛的任务。然而，用于此类调优的现有数据往往对个别领域的覆盖不足，限制了对这些领域内细致理解和交互的范围。为了解决这个问题，我们提出了一种新颖的方法，称为探索指导，通过大型语言模型 (LLMs) 的主动探索来增强用于特定领域指导调优的数据覆盖。探索指导基于典型的领域使用案例，通过实现搜索算法来获取多样化和面向领域的指导调优数据的多种变体或可能性。我们的数据中心分析验证了此方法在改进特定领域指导覆盖范围方面的有效性。此外，我们模型的性能显示出与多个基线模型相比的显著进展。

    Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas. To address this deficiency, we propose Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs). Built upon representative domain use cases, Explore-Instruct explores a multitude of variations or possibilities by implementing a search algorithm to obtain diversified and domain-focused instruction-tuning data. Our data-centric analysis validates the effectiveness of this proposed approach in improving domain-specific instruction coverage. Moreover, our model's performance demonstrates considerable advancements over multiple baselines, includi
    
[^111]: PuoBERTa:训练和评估一种为塞茨瓦纳语定制的语言模型

    PuoBERTa: Training and evaluation of a curated language model for Setswana. (arXiv:2310.09141v1 [cs.CL])

    [http://arxiv.org/abs/2310.09141](http://arxiv.org/abs/2310.09141)

    本文介绍了一种名为PuoBERTa的定制掩码语言模型，针对塞茨瓦纳语进行训练，并证明了其在促进塞茨瓦纳语等少研究语言的自然语言处理能力方面的有效性。

    

    自然语言处理在资源丰富的语言（如英语）方面取得了重大进展，但在资源匮乏的语言（如塞茨瓦纳语）方面却滞后。本文通过介绍PuoBERTa，一种专门为塞茨瓦纳语训练的定制掩码语言模型，弥补了这一差距。我们介绍如何收集、筛选和准备多样化的单语文本，为PuoBERTa的训练生成高质量的语料库。在之前为塞茨瓦纳语创建单语资源的基础上，我们评估了PuoBERTa在多个自然语言处理任务中的表现，包括词性标注、命名实体识别和新闻分类。此外，我们还引入了一个新的塞茨瓦纳语新闻分类数据集，并提供了使用PuoBERTa的初始基准。我们的工作展示了PuoBERTa在促进塞茨瓦纳语等少研究语言的自然语言处理能力方面的有效性，并为未来的研究方向铺平了道路。

    Natural language processing (NLP) has made significant progress for well-resourced languages such as English but lagged behind for low-resource languages like Setswana. This paper addresses this gap by presenting PuoBERTa, a customised masked language model trained specifically for Setswana. We cover how we collected, curated, and prepared diverse monolingual texts to generate a high-quality corpus for PuoBERTa's training. Building upon previous efforts in creating monolingual resources for Setswana, we evaluated PuoBERTa across several NLP tasks, including part-of-speech (POS) tagging, named entity recognition (NER), and news categorisation. Additionally, we introduced a new Setswana news categorisation dataset and provided the initial benchmarks using PuoBERTa. Our work demonstrates the efficacy of PuoBERTa in fostering NLP capabilities for understudied languages like Setswana and paves the way for future research directions.
    
[^112]: 一种新的基准和倒向验证方法用于段落级幻觉检测

    A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection. (arXiv:2310.06498v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.06498](http://arxiv.org/abs/2310.06498)

    本文提出了一种基于倒向验证的自检方法和一个名为PHD的幻觉检测基准，用于自动检测大型语言模型中的事实错误，该方法在段落级别上表现出很好的性能。

    

    大型语言模型(LLM)在与人类在真实场景中的有效协作方面展示出了它们的能力。然而，LLM很容易生成幻觉，即编造不正确的文本和未经验证的信息，这在部署于重要任务中时可能造成重大损害。在本文中，我们提出了一种基于倒向验证的自检方法，以零资源的方式自动检测事实错误。为了便于未来的研究和评估不同方法，我们构建了一个名为PHD的幻觉检测基准，该基准由ChatGPT生成并由人类标注。与以往的零资源幻觉检测研究不同，我们的方法和基准集专注于段落级别的检测，而不是句子级别的检测。我们在两个数据集上经验性地评估了我们的方法和现有的零资源检测方法。实验结果表明，所提出的方法在性能上明显优于基准方法，同时成本相对较低。

    Large Language Models (LLMs) have shown their ability to collaborate effectively with humans in real-world scenarios. However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. In this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. To facilitate future studies and assess different methods, we construct a hallucination detection benchmark named PHD, which is generated by ChatGPT and annotated by human annotators. Contrasting previous studies of zero-resource hallucination detection, our method and benchmark concentrate on passage-level detection instead of sentence-level. We empirically evaluate our method and existing zero-resource detection methods on two datasets. The experimental results demonstrate that the proposed method considerably outperforms the baselines while costin
    
[^113]: GROVE: 一种带有证据森林的检索增强复杂故事生成框架

    GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence. (arXiv:2310.05388v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05388](http://arxiv.org/abs/2310.05388)

    GROVE是一种检索增强的复杂故事生成框架，通过利用优秀人类写作故事的信息和细节，能够生成具有复杂而可信的情节的故事。

    

    条件故事生成在人机交互中非常重要，特别是在生成具有复杂情节的故事方面。尽管大型语言模型（LLMs）在多个自然语言处理任务中表现出色，包括故事生成，但是生成既复杂又有创意的故事是具有挑战性的。现有方法通常依赖于详细的提示来引导LLMs满足目标条件，这无意间限制了生成故事的创造潜力。我们认为利用优秀的人类写作故事的信息有助于生成更多样化的情节。深入研究故事细节有助于构建复杂而可信的情节。在本文中，我们提出了一种具有检索增强的带有证据森林的故事生成框架（GROVE）来增强故事的复杂性。我们构建了一个用于目标条件的检索存储库，以产生少样本示例来引导LLMs。此外，我们设计了一个“询问为什么”的提示。

    Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines. Delving deeper into story details helps build complex and credible plots. In this paper, we propose a retrieval-au\textbf{G}mented sto\textbf{R}y generation framework with a f\textbf{O}rest of e\textbf{V}id\textbf{E}nce (GROVE) to enhance stories' complexity. We build a retrieval repository for target conditions to produce few-shot examples to prompt LLMs. Additionally, we design an ``asking-why'' promptin
    
[^114]: 通过有效利用文本数据合成提高端到端语音处理的效率

    Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis. (arXiv:2310.05374v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05374](http://arxiv.org/abs/2310.05374)

    本论文提出了一种名为LaSyn的文本数据利用框架，通过将文本数据转换为中间潜变表示来增强端到端语音处理模型的训练。在低资源环境下的语音识别和口语理解任务中，LaSyn相对词错误率减少了22.3%，绝对意图分类准确率提高了4.1%。

    

    在数据中心的人工智能时代，培训高性能的端到端语音处理模型需要大量标记的语音数据。然而，与文本数据相比，标记的语音数据通常更加稀缺和昂贵。我们提出了一种名为LaSyn的有效的文本数据利用框架，用于端到端语音处理模型。我们训练一个潜变合成器将文本数据转换为预训练语音模型的中间潜变表示。这些伪声学表示用于增强模型训练的声学数据。我们在低资源的自动语音识别（ASR）和口语理解（SLU）任务上评估了LaSyn。对于ASR，LaSyn改进了在LibriSpeech train-clean-100上训练的E2E基线，在不同的测试集上相对词错误率减少了22.3%。对于SLU，LaSyn改进了我们的E2E基线，绝对意图分类准确率提高了4.1%。

    Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accurac
    
[^115]: 反图灵测试CT^2：AI生成文本检测没有你想象的那么容易——引入AI可检测性指数。

    Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think -- Introducing AI Detectability Index. (arXiv:2310.05030v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05030](http://arxiv.org/abs/2310.05030)

    这篇论文介绍了一个名为反图灵测试（CT^2）的基准，旨在全面评估现有AI生成文本检测技术的稳健性。在面对生成AI的风险和后果引起关注的情况下，解决AI生成作品归属问题变得尤为重要。

    

    随着ChatGPT等生成AI的兴起，AI生成文本的风险和后果越来越引起关注。为了解决对AI生成作品的归属问题，美国版权局发布了一份声明，指出“如果一件作品的传统创作元素由机器生成，那么这件作品就缺乏人类创作，版权局将不会登记它”。此外，美国和欧盟政府最近也草拟了关于AI监管框架的初步提案。在这种对生成AI的关注中，AI生成文本检测已经成为一个受到研究立即关注的话题，一些初步方法已经被提出，随后出现了绕过检测的技术。本文介绍了反图灵测试（CT^2），这是一个基准，旨在全面评估现有AI生成文本检测技术的稳健性。

    With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that 'If a work's traditional elements of authorship were produced by a machine, the work lacks human authorship and the Office will not register it'. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT^2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our em
    
[^116]: 评估中文大型语言模型中的幻觉

    Evaluating Hallucinations in Chinese Large Language Models. (arXiv:2310.03368v1 [cs.CL])

    [http://arxiv.org/abs/2310.03368](http://arxiv.org/abs/2310.03368)

    本研究评估了中文大型语言模型中的幻觉现象，通过建立HalluQA基准测试和使用GPT-4进行自动评估方法，发现18个模型的非幻觉率低于50%。研究分析了不同类型模型中的幻觉类型和原因。

    

    本文介绍了一项名为HalluQA（中文幻觉问答）的基准测试，用于衡量中文大型语言模型中的幻觉现象。HalluQA包含450个经过精心设计的对抗性问题，涵盖多个领域，并考虑了中国历史文化、风俗和社会现象。在构建HalluQA过程中，我们考虑了两种幻觉类型：模仿性虚假和事实错误，并基于GLM-130B和ChatGPT构建对抗样本。为了评估，我们设计了一种使用GPT-4的自动评估方法来判断模型输出是否是幻觉。我们对24个大型语言模型进行了广泛的实验，包括ERNIE-Bot、Baichuan2、ChatGLM、Qwen、SparkDesk等。在这24个模型中，有18个的非幻觉率低于50%。这表明HalluQA具有很高的挑战性。我们分析了不同类型模型中主要的幻觉类型及其原因。

    In this paper, we establish a benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure the hallucination phenomenon in Chinese large language models. HalluQA contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account Chinese historical culture, customs, and social phenomena. During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on GLM-130B and ChatGPT. For evaluation, we design an automated evaluation method using GPT-4 to judge whether a model output is hallucinated. We conduct extensive experiments on 24 large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than 50%. This indicates that HalluQA is highly challenging. We analyze the primary types of hallucinations in different types of models and their causes. Add
    
[^117]: MetaTool基准：决定是否使用工具和选择使用哪个工具。

    MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use. (arXiv:2310.03128v1 [cs.SE])

    [http://arxiv.org/abs/2310.03128](http://arxiv.org/abs/2310.03128)

    本文提出了一个名为MetaTool的基准，旨在评估大型语言模型（LLMs）是否具有工具使用意识并且能够正确选择工具。基准中包含一个名为ToolE的数据集，其中包含各种类型的用户查询，用于触发LLMs使用工具。

    

    大型语言模型（LLMs）由于其出色的自然语言处理（NLP）能力而受到了广泛关注。最近，许多研究关注LLMs的工具利用能力。它们主要研究了LLMs如何有效地与给定的特定工具合作。然而，在LLMs充当智能体的场景中，例如AutoGPT和MetaGPT应用中，LLMs被期望参与涉及是否使用工具以及从可用工具集中选择最合适的工具来满足用户请求的复杂决策过程。因此，在本文中，我们介绍了MetaTool，这是一个用于评估LLMs是否具有工具使用意识并且能够正确选择工具的基准。具体而言，我们在该基准中创建了一个名为ToolE的数据集。该数据集包含以触发LLMs使用工具的提示形式出现的各种类型的用户查询，包括单一工具和多种工具。

    Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-too
    
[^118]: 为自监督编码器-解码器语音模型进行提示和适配器调优的方法

    Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model. (arXiv:2310.02971v1 [eess.AS])

    [http://arxiv.org/abs/2310.02971](http://arxiv.org/abs/2310.02971)

    这篇论文介绍了为自监督编码器-解码器语音模型进行提示和适配器调优的方法，并展示了在序列生成和跨语言ASR任务上的优越表现，尤其在低资源情况下提示方法优于适配器调优。

    

    提示和适配器调优已经成为细调（FT）方法的有效替代品。然而，现有的关于语音提示的研究主要集中在分类任务上，并在更复杂的序列生成任务上失败。此外，适配器调优主要应用于仅编码器的自监督模型。我们的实验证明，在Wav2Seq这个自监督的编码器-解码器模型上进行提示，超过了以前在序列生成任务上的研究成果。它在ASR的词错误率上实现了53％的相对改进，在槽填充的F1分数上实现了27％的改进。此外，在低资源情况下提示方法与FT方法相竞争。此外，我们展示了在Wav2Seq上通过提示和适配器调优实现的跨语言ASR的可转移性。当涉及有限的可训练参数时，提示和适配器调优始终优于传统的FT方法在7种语言中的表现。值得注意的是，在低资源情况下，提示方法始终优于适配器调优。

    Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.
    
[^119]: Avalon的思考游戏：通过递归思考对抗欺骗

    Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation. (arXiv:2310.01320v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.01320](http://arxiv.org/abs/2310.01320)

    本研究通过使用复杂的Avalon游戏作为测试平台，引入了一种名为递归思考（ReCon）的新框架，用于增强大型语言模型（LLM）识别和对抗欺骗信息的能力。

    

    最近在大型语言模型（LLM）的突破带来了在LLM作为智能体领域的显著成功。然而，一种普遍的假设是LLM处理的信息始终是诚实的，忽视了人类社会和AI生成内容中普遍存在的欺骗或误导性信息。这个疏忽使得LLM容易受到恶意操纵，可能导致不利的结果。本研究利用复杂的Avalon游戏作为测试平台，探索LLM在欺骗环境中的潜力。Avalon充满了错误信息，并需要复杂的逻辑，表现为“思考的游戏”。受到人类在Avalon游戏中递归思考和透视能力的启发，我们引入了一种新颖的框架——递归思考（ReCon），以增强LLM识别和对抗欺骗信息的能力。ReCon结合了公式化思考和完善思考的过程；公式化思考产生初始思考，完善思考对初始思考进行调整和改进。

    Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial tho
    
[^120]: NJUNLP对WMT2023质量评估共享任务的参与

    NJUNLP's Participation for the WMT2023 Quality Estimation Shared Task. (arXiv:2309.13230v1 [cs.CL])

    [http://arxiv.org/abs/2309.13230](http://arxiv.org/abs/2309.13230)

    NJUNLP团队对WMT2023质量评估共享任务进行了投稿，通过使用伪数据方法和核心超参数的实验研究，他们的模型在英德语言对的质量预测和错误跨度检测上取得了最佳结果。

    

    我们介绍了NJUNLP团队在WMT 2023质量估计（QE）共享任务中的投稿。我们的团队提交了对英德语言对的所有两个子任务的预测：（i）句子和单词级别的质量预测；（ii）细粒度错误跨度检测。今年，我们进一步探索了基于NJUQE框架（https://github.com/NJUNLP/njuqe）的伪数据方法进行QE。我们使用WMT翻译任务的并行数据生成伪MQM数据。我们在伪QE数据上预训练XLMR大模型，然后在真实QE数据上进行微调。在两个阶段，我们共同学习句子级分数和单词级标签。在实证上，我们进行实验来寻找改善性能的关键超参数。在技术上，我们提出了一种简单的方法，将单词级输出转换为细粒度错误跨度结果。总体而言，我们的模型在英德语言对的单词级别和细粒度错误跨度检测子任务中取得了最佳结果。

    We introduce the submissions of the NJUNLP team to the WMT 2023 Quality Estimation (QE) shared task. Our team submitted predictions for the English-German language pair on all two sub-tasks: (i) sentence- and word-level quality prediction; and (ii) fine-grained error span detection. This year, we further explore pseudo data methods for QE based on NJUQE framework (https://github.com/NJUNLP/njuqe). We generate pseudo MQM data using parallel data from the WMT translation task. We pre-train the XLMR large model on pseudo QE data, then fine-tune it on real QE data. At both stages, we jointly learn sentence-level scores and word-level tags. Empirically, we conduct experiments to find the key hyper-parameters that improve the performance. Technically, we propose a simple method that covert the word-level outputs to fine-grained error span results. Overall, our models achieved the best results in English-German for both word-level and fine-grained error span detection sub-tasks by a considera
    
[^121]: 角度优化的文本嵌入

    AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])

    [http://arxiv.org/abs/2309.12871](http://arxiv.org/abs/2309.12871)

    本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。

    

    高质量的文本嵌入对于提升语义文本相似度（STS）任务至关重要，而这些任务又是大型语言模型（LLM）应用中的关键组成部分。然而，现有的文本嵌入模型面临的一个普遍挑战是渐变消失问题，主要是由于它们在优化目标中依赖余弦函数，而余弦函数具有饱和区域。为了解决这个问题，本文提出了一种称为AnglE的新型角度优化文本嵌入模型。AnglE的核心思想是在一个复杂空间中引入角度优化。这种新颖的方法有效地缓解了余弦函数饱和区域产生的不利影响，从而可以阻碍梯度并阻碍优化过程。为了建立全面的STS评估，我们在现有的短文本STS数据集和从GitHub Issues中新收集的长文本STS数据集上进行了实验。此外，我们还研究了具有有限标签数据的特定领域STS场景，并探讨了AnglE的工作原理。

    High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
    
[^122]: AV2Wav：基于连续自监督特征的扩散重合成技术用于音频-视觉语音增强

    AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement. (arXiv:2309.08030v1 [eess.AS])

    [http://arxiv.org/abs/2309.08030](http://arxiv.org/abs/2309.08030)

    本论文提出了一种名为AV2Wav的音频-视觉语音增强方法，利用连续自监督特征和扩散模型生成干净的语音，克服了现实训练数据的挑战。与基于掩蔽的基线方法相比，该方法在声码任务上表现更好，并通过多任务训练进一步优化性能。

    

    语音增强系统通常使用干净和噪声语音对进行训练。在音频-视觉语音增强中，干净的数据不够多；大多数音频-视觉数据集都是在现实环境中收集的，包含背景噪声和混响，这阻碍了音频-视觉语音增强的发展。在本研究中，我们引入了AV2Wav，一种基于重合成的音频-视觉语音增强方法，可以在现实训练数据的挑战下生成干净的语音。我们使用神经质量估计器从音频-视觉语料库中获取几乎干净的语音子集，并在此子集上训练一个扩散模型，该模型可以根据来自AV-HuBERT的连续语音表示生成声波形，具有噪声鲁棒训练。我们使用连续而不是离散表示来保留韵律和说话者信息。仅仅通过声码任务，该模型就比基于掩蔽的基线更好地执行语音增强。我们进一步fine-tune模型，以转化为在多任务下进行训练，通过联合多帧声学到语音转化来提高性能。

    Speech enhancement systems are typically trained using pairs of clean and noisy speech. In audio-visual speech enhancement (AVSE), there is not as much ground-truth clean data available; most audio-visual datasets are collected in real-world environments with background noise and reverberation, hampering the development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based audio-visual speech enhancement approach that can generate clean speech despite the challenges of real-world training data. We obtain a subset of nearly clean speech from an audio-visual corpus using a neural quality estimator, and then train a diffusion model on this subset to generate waveforms conditioned on continuous speech representations from AV-HuBERT with noise-robust training. We use continuous rather than discrete representations to retain prosody and speaker information. With this vocoding task alone, the model can perform speech enhancement better than a masking-based baseline. We further fine-
    
[^123]: 临床文本摘要: 大型语言模型的应用优于人类专家

    Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v1 [cs.CL])

    [http://arxiv.org/abs/2309.07430](http://arxiv.org/abs/2309.07430)

    本研究通过对八个大型语言模型在临床摘要任务上的领域适应方法实验进行了全面的定量评估，发现最佳适应的模型的摘要在完整性和正确性方面优于人类摘要。

    

    在临床工作中，浏览大量的文本数据并总结关键信息对临床医生的时间分配造成了很大的负担。尽管大型语言模型（LLMs）在自然语言处理（NLP）任务中展现了巨大的潜力，但它们在各种临床摘要任务中的效果尚未得到严格的检验。在本研究中，我们对八个LLMs进行了领域适应方法的实验，涵盖了六个数据集和四个不同的摘要任务：放射学报告、患者问题、病历记录和医患对话。我们进行了全面的定量评估，发现模型和适应方法之间存在权衡，并且在某些情况下，LLMs的最新进展可能不会带来改进的结果。此外，通过与六名医生进行的临床阅读者研究，我们发现最佳适应的LLM的摘要在完整性和正确性方面优于人类摘要。我们的进一步定性分析揭示了LLMs和人类在面对的共同挑战。

    Sifting through vast textual data and summarizing key information imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy across diverse clinical summarization tasks has not yet been rigorously examined. In this work, we employ domain adaptation methods on eight LLMs, spanning six datasets and four distinct summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not lead to improved results. Further, in a clinical reader study with six physicians, we depict that summaries from the best adapted LLM are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis delineates mutual challenges faced by both LLMs and
    
[^124]: 在文本中测量模糊性和主观性：从符号到神经网络的VAGO

    Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO. (arXiv:2309.06132v1 [cs.CL])

    [http://arxiv.org/abs/2309.06132](http://arxiv.org/abs/2309.06132)

    本文提出了一种混合方法来自动测量文本中的模糊性和主观性。通过引入专家系统VAGO，以及基于BERT-like架构的神经克隆，该方法在固定语料库和多语言生成方面表现出良好的性能。

    

    我们提出了一种混合方法来自动测量文本中的模糊性和主观性。首先，我们介绍了专家系统VAGO，并在一小组事实与观点句子的基准上对其进行了说明，并在更大的法语新闻语料库FreSaDa上进行了测试，以确认讽刺性文本中主观标记的更高流行率。然后，我们构建了一个基于BERT-like架构的VAGO神经克隆，该架构基于在FreSaDa上获得的符号VAGO分数进行训练。使用可解释性工具（LIME），我们展示了这个神经版本在丰富符号版本的词典和生成其他语言版本方面的兴趣。

    We present a hybrid approach to the automated measurement of vagueness and subjectivity in texts. We first introduce the expert system VAGO, we illustrate it on a small benchmark of fact vs. opinion sentences, and then test it on the larger French press corpus FreSaDa to confirm the higher prevalence of subjective markers in satirical vs. regular texts. We then build a neural clone of VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores obtained on FreSaDa. Using explainability tools (LIME), we show the interest of this neural version for the enrichment of the lexicons of the symbolic version, and for the production of versions in other languages.
    
[^125]: nanoT5:一种用有限资源进行预训练和微调T5风格模型的PyTorch框架

    nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources. (arXiv:2309.02373v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.02373](http://arxiv.org/abs/2309.02373)

    nanoT5是一个用于高效预训练和微调T5模型的PyTorch框架，通过优化性能和计算效率，它可以在单个GPU上在短时间内进行预训练而不损失性能。这个开源框架为更广泛的语言建模研究提供了更易用的T5实现。

    

    最先进的语言模型如T5已经改变了自然语言处理的格局，但其计算需求限制了大部分研究社区的使用。为了解决这个挑战，我们提出了nanoT5，这是一个经过特别优化的PyTorch框架，用于高效地对T5模型进行预训练和微调。通过借鉴优化器的差异和优化效率，nanoT5使得一个T5-Base模型能够在单个GPU上只需16小时进行预训练，而且不会损失性能。通过引入这个开源框架，我们希望扩大语言建模研究的可访问性，并满足社区对更加用户友好的T5（编码-解码）实现的需求。我们将我们的贡献，包括配置、代码库、预训练洞察力和预训练模型，提供给公众。

    State-of-the-art language models like T5 have revolutionized the NLP landscape, but their computational demands hinder a large portion of the research community. To address this challenge, we present nanoT5, a specially-optimized PyTorch framework for efficient pre-training and fine-tuning of T5 models. Drawing on insights from optimizer differences and prioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on a single GPU in just 16 hours, without any loss in performance. With the introduction of this open-source framework, we hope to widen the accessibility to language modelling research and cater to the community's demand for more user-friendly T5 (Encoder-Decoder) implementations. We make our contributions, including configurations, codebase, pre-training insights, and pre-trained models, available to the public.
    
[^126]: LLM自卫：通过自检，LLMs意识到它们被愚弄了。

    LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked. (arXiv:2308.07308v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07308](http://arxiv.org/abs/2308.07308)

    本文提出了一种通过自检来防御大型语言模型(LLMs)对抗性攻击的简单方法，即让模型自行过滤回应。实验结果表明，即使模型未对齐人类价值观，通过使用语言模型验证内容，仍然可以防止模型向用户呈现有害内容。

    

    近年来，大型语言模型（LLMs）由于其能够对人类提示做出高质量文本回应而变得非常受欢迎。然而，研究表明，这些模型在回应用户提示时可能生成有害内容（例如，给用户提供犯罪指导）。文献中已经着重研究如何通过方法（例如通过强化学习将模型与人类价值观对齐）来减轻这些风险。然而，研究发现，即使对齐的语言模型也容易受到绕过生成有害文本限制的对抗性攻击。我们提出了一种简单的方法来防御这些攻击，即大型语言模型对自己的回应进行过滤。我们目前的研究结果表明，即使模型没有被微调以与人类价值观对齐，也可以通过使用语言模型验证内容来防止其向用户呈现有害内容。

    Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting. However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes). There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning. However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text. We propose a simple approach to defending against these attacks by having a large language model filter its own responses. Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.
    
[^127]: 《TARJAMAT: Bard和ChatGPT在十种阿拉伯语变体机器翻译上的评估》

    TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties. (arXiv:2308.03051v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03051](http://arxiv.org/abs/2308.03051)

    这项研究对Bard和ChatGPT在十种阿拉伯语变体的机器翻译能力进行了评估，发现LLM在翻译方言方面表现优于商业系统，但在古典阿拉伯语和现代标准阿拉伯语方面落后于谷歌翻译等商业系统。

    

    尽管像ChatGPT和Bard这样的经过指导微调的大型语言模型（LLM）被认为在多语言上有很高的能力，但这些模型的语言包容性还没有得到充分的探索。考虑到这个限制，我们对Bard和ChatGPT（包括GPT-3.5和GPT-4）在十种阿拉伯语变体的机器翻译能力进行了全面评估。我们的评估涵盖了古典阿拉伯语（CA）、现代标准阿拉伯语（MSA）和几种国家方言变体等多种阿拉伯语变体。我们的分析表明，LLM在存在少量公共数据集的方言上可能会遇到挑战，但平均而言，它们比现有的商业系统更擅长翻译方言。然而，在CA和MSA方面，经过指导调整的LLM与谷歌翻译等商业系统相比仍然有所不足。最后，我们开展了一个以人为中心的研究，以审查相对较新的模型Bard在遵循人类指令方面的效果。

    Despite the purported multilingual proficiency of instruction-finetuned large language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of these models remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic (CA), Modern Standard Arabic (MSA), and several country-level dialectal variants. Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist, but on average are better translators of dialects than existing commercial systems. On CA and MSA, instruction-tuned LLMs, however, trail behind commercial systems such as Google Translate. Finally, we undertake a human-centric study to scrutinize the efficacy of the relatively recent model, Bard, in following human instructio
    
[^128]: MM-Vet: 评估大型多模态模型的综合能力

    MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])

    [http://arxiv.org/abs/2308.02490](http://arxiv.org/abs/2308.02490)

    MM-Vet是一个评估标准，用于评估大型多模态模型在复杂任务上的综合能力。该标准解决了如何结构化和评估复杂多模态任务、设计适用于不同问题和回答类型的评估指标以及如何提供模型洞察的问题。通过整合不同的核心视觉-语言能力，MM-Vet展示了有趣的能力和解决复杂任务的方法。

    

    我们提出了MM-Vet，一个评估标准，用于检查在复杂多模态任务上的大型多模态模型（LMM）的表现。最近的LMM展示了各种有趣的能力，例如解决书写在黑板上的数学问题，推理新闻图片中的事件和名人，以及解释视觉笑话。快速的模型进步给评估标准的开发带来了挑战。问题包括：（1）如何系统地构建和评估复杂的多模态任务；（2）如何设计适用于不同类型问题和回答的评估指标；（3）如何给出超出简单性能排名的模型洞察。为此，我们提出了MM-Vet，基于这样一个洞察：解决复杂任务的有趣能力通常通过一种通才模型能够整合不同的核心视觉-语言（VL）能力来实现。MM-Vet定义了6个核心VL能力，并检查了从这些能力组合中得出的16种有趣的整合方式。

    We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
    
[^129]: Baby Llama：从一组在小数据集上训练的教师中进行知识蒸馏，无性能损失

    Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. (arXiv:2308.02019v1 [cs.CL])

    [http://arxiv.org/abs/2308.02019](http://arxiv.org/abs/2308.02019)

    本文提出了一种从在小数据集上训练的教师中进行知识蒸馏的方法，并证明当教师模型在足够小的数据集上训练时，蒸馏可以保持甚至超过教师模型的性能。

    

    我们提出了我们对BabyLM挑战[arXiv:2301.11796]的解决方案，其目标是提高语言模型的样本效率。我们在以发展性为基础的10M词语的BabyLM数据集上训练了一个由GPT-2和小型LLaMA模型组成的集合，然后将其蒸馏为一个小型的58M参数LLaMA模型，其性能超过了两个教师模型以及一个没有进行蒸馏训练的类似模型。这表明，当教师模型在足够小的数据集上训练时，蒸馏不仅可以保持教师模型的全部性能，还可以超过它，并导致比直接训练更好的性能。

    We present our proposed solution to the BabyLM challenge [arXiv:2301.11796], whose goal was to improve the sample efficiency of language models. We trained an ensemble consisting of a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model, which exceeds in performance both of its teachers as well as a similar model trained without distillation. This suggests that distillation can not only retain the full performance of the teacher model when the latter is trained on a sufficiently small dataset; it can exceed it, and lead to significantly better performance than direct training.
    
[^130]: WebArena: 一个用于构建自主智能体的真实网络环境

    WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])

    [http://arxiv.org/abs/2307.13854](http://arxiv.org/abs/2307.13854)

    WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。

    

    随着生成式人工智能的进展，通过自然语言指令进行日常任务的自主智能体的潜力逐渐显现。然而，当前的智能体主要是在简化的合成环境中创建和测试的，严重限制了现实世界场景的表示能力。在本文中，我们构建了一个高度逼真且可复现的智能体指令和控制环境。具体而言，我们关注在网站上执行任务的智能体，我们创建了一个包含来自四个常见领域的完全功能网站的环境，分别是电子商务、社交论坛讨论、协同软件开发和内容管理。我们的环境使用工具（如地图）和外部知识库（如用户手册）来鼓励像人类一样解决任务。在我们的环境基础上，我们发布了一组重点评估任务完成功能正确性的基准任务。我们基准任务具有多样性和长远的视野，并且被设计为鼓励智能体进行更深层次的任务理解和解决。

    With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
    
[^131]: RADAR: 通过对抗性学习实现鲁棒的AI文本检测

    RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])

    [http://arxiv.org/abs/2307.03838](http://arxiv.org/abs/2307.03838)

    本论文提出了一种名为RADAR的新框架，通过对抗性学习实现了鲁棒的AI文本检测，以解决当前AI文本检测器对于大语言模型的改写不具备鲁棒性的问题。

    

    最近大语言模型（LLMs）的进展以及ChatGPT类应用的普及已经模糊了人类和机器之间高质量文本生成的界限。然而，除了对我们的技术和社会预期的革命性变化外，区分LLM生成的文本（AI文本）和人类生成的文本的困难也带来了新的滥用和公平性挑战，例如虚假内容生成，抄袭以及对无辜作者的错误指控。尽管现有的研究表明当前的AI文本检测器对基于LLM的改写不具有鲁棒性，但本文旨在通过提出一种名为RADAR的新框架来弥合这一差距，该框架通过对抗性学习共同训练了一个鲁棒的AI文本检测器。RADAR基于一个改写器和一个检测器的对抗性训练。改写器的目标是生成逼真的内容以规避AI文本检测。RADAR使用来自检测器的反馈来更新改写器，反之亦然。

    Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice vers
    
[^132]: 当问题不是用英语提出时，不要完全信任GPT

    Don't Trust GPT When Your Question Is Not In English. (arXiv:2305.16339v1 [cs.CL])

    [http://arxiv.org/abs/2305.16339](http://arxiv.org/abs/2305.16339)

    在多语言环境下，GPT-3表现较差，特别是当问题不是用英语提出时。这与模型的训练数据和输入问题的语言差异有关。

    

    近年来，大型语言模型（LLMs）展示了出色的自然语言理解能力，并在多个自然语言处理（NLP）任务中表现出色。尽管大多数LLMs主要使用英语进行训练，但多项研究已经证明了它们在许多其他语言中的相对表现。然而，关于LLMs如何获得它们的多语言能力以及表现在不同语言中的差异仍然存在基本问题。这些问题对LLMs的研究非常关键，因为用户和研究人员通常来自多种语言背景，可能影响他们对LLMs结果的利用和解释。在本文中，我们提出了一种系统的方法，以定性评估多语言环境下LLMs的表现差异。我们调查了LLMs在跨语言泛化现象方面的表现，即不充足的多语言训练数据导致先进的多语言能力。为了实现这一目的，我们对一系列语言进行了GPT-3的实验，这些语言涵盖了从印欧语系到非印欧语系的各种语言，并提出了一种评估和验证结果的方法。我们的发现表明，即使模型在该语言上进行了微调，但如果输入问题不是英语，GPT-3在其他语言下的表现显著较差。此外，我们证明了模型的表现不佳与训练语言和输入问题的语言差异有关。我们的结果表明，在进行非英语自然语言处理任务时，需要谨慎使用LLMs。

    Large Language Models (LLMs) have demonstrated exceptional natural language understanding abilities and have excelled in a variety of natural language processing (NLP)tasks in recent years. Despite the fact that most LLMs are trained predominantly in English, multiple studies have demonstrated their comparative performance in many other languages. However, fundamental questions persist regarding how LLMs acquire their multi-lingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing their utilization and interpretation of LLMs' results. In this work, we propose a systematic way of qualifying the performance disparities of LLMs under multilingual settings. We investigate the phenomenon of across-language generalizations in LLMs, wherein insufficient multi-lingual training data leads to advanced multi-lingual capabilities. To acc
    
[^133]: 从零开始对比学习句子嵌入

    Contrastive Learning of Sentence Embeddings from Scratch. (arXiv:2305.15077v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15077](http://arxiv.org/abs/2305.15077)

    该论文提出了一种从零开始的对比学习框架，利用合成数据训练句子嵌入。实验结果表明，该方法在句子相似性和重排序任务上取得了良好的效果。

    

    对比学习一直是训练最先进的句子嵌入的主要方法。先前的研究通常通过使用人工标注的自然语言推理（NLI）数据或通过大规模无标签的句子以无监督的方式来学习句子嵌入。然而，即使在无标签数据的情况下，由于各种原因，在某些领域获取数据样本仍然存在挑战。为了解决这些问题，我们提出了SynCSE，一种对比学习框架，用于使用合成数据训练句子嵌入。具体而言，我们探索利用大型语言模型合成对比学习所需的数据样本，包括（1）产生给定无标签句子的正负标注（SynCSE-partial），以及（2）从零开始生成句子及其相应的标注（SynCSE-scratch）。对句子相似性和重排序任务的实验结果表明，SynCSE-partial和SynCSE-scratch两者都可以取得良好的效果。

    Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. To address these issues, we present SynCSE, a contrastive learning framework that trains sentence embeddings with synthesized data. Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences (SynCSE-partial), and (2) generating sentences along with their corresponding annotations from scratch (SynCSE-scratch). Experimental results on sentence similarity and reranking tasks indicate that both SynCSE-partial and SynCSE-
    
[^134]: ToMChallenges: 一个基于原则的数据集和多样化评估任务，用于探索心智理论

    ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind. (arXiv:2305.15068v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15068](http://arxiv.org/abs/2305.15068)

    本研究提出了一个基于原则的数据集和多样化评估任务，名为ToMChallenges，以探索心智理论。研究发现，大型语言模型在心智理论任务上表现不一致，稳定地执行任务仍然具有挑战性。

    

    心智理论（ToM）是理解不同个体心智状态的能力，对于许多实际应用至关重要。随着大型语言模型（LLMs）的发展，关于它们是否能够执行ToM任务存在激烈的争议。先前的研究使用不同的任务和提示来测试LLMs上的ToM，结果不一致：一些研究认为这些模型能够展示ToM，而其他人则持相反观点。在本研究中，我们提出了ToMChallenges，一个基于Sally-Anne和Smarties测试的数据集，用于全面评估心智理论并包含多样化的任务。此外，我们还提出了一个自动评分器来简化答案评估过程。我们测试了三个模型：davinci、turbo和gpt-4。我们的评估结果和错误分析显示，LLMs在提示和任务之间表现不一致。对LLMs来说，稳定地执行ToM任务仍然是一个挑战。

    Theory of Mind (ToM), the capacity to comprehend the mental states of distinct individuals, is essential for numerous practical applications. With the development of large language models (LLMs), there is a heated debate about whether they are able to perform ToM tasks. Previous studies have used different tasks and prompts to test the ToM on LLMs and the results are inconsistent: some studies asserted these models are capable of exhibiting ToM, while others suggest the opposite. In this study, We present ToMChallenges, a dataset for comprehensively evaluating the Theory of Mind based on the Sally-Anne and Smarties tests with a diverse set of tasks. In addition, we also propose an auto-grader to streamline the answer evaluation process. We tested three models: davinci, turbo, and gpt-4. Our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs. In addition, our paper 
    
[^135]: Dior-CVAE：预训练语言模型和扩散先验用于变分对话生成

    Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation. (arXiv:2305.15025v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15025](http://arxiv.org/abs/2305.15025)

    Dior-CVAE是一种具有扩散先验的分层条件变分自编码器，通过采用预训练语言模型和扩散模型来解决变分对话生成中的多样性和后验崩溃问题。实验表明，该模型在开放领域对话中表现出了优越的性能。

    

    当前的变分对话模型采用了预训练语言模型（PLMs）来参数化似然和后验分布。然而，对先验分布的高斯假设与这些分布不兼容，从而限制了生成响应的多样性。这些模型还存在后验崩溃问题，即解码器倾向于通过交叉注意机制直接访问编码器中捕获的信息而忽略潜变量。在这项工作中，我们提出了Dior-CVAE，一种带有扩散先验的分层条件变分自编码器（CVAE）来解决这些挑战。我们采用扩散模型来增加先验分布的复杂性，使其与PLM产生的分布兼容。此外，我们提出了记忆丢弃机制来改进交叉注意机制，积极鼓励使用潜变量进行响应生成。总体上，在两个常用的开放领域对话实验中，我们的模型展现了优越的性能。

    Current variational dialog models have employed pre-trained language models (PLMs) to parameterize the likelihood and posterior distributions. However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated responses. These models also suffer from posterior collapse, i.e., the decoder tends to ignore latent variables and directly access information captured in the encoder through the cross-attention mechanism. In this work, we propose Dior-CVAE, a hierarchical conditional variational autoencoder (CVAE) with diffusion priors to address these challenges. We employ a diffusion model to increase the complexity of the prior distribution and its compatibility with the distributions produced by a PLM. Also, we propose memory dropout to the cross-attention mechanism, which actively encourages the use of latent variables for response generation. Overall, experiments across two commonly used open-domain dialog 
    
[^136]: ACL OCL语料库：推动计算语言学领域的开放科学

    The ACL OCL Corpus: Advancing Open Science in Computational Linguistics. (arXiv:2305.14996v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14996](http://arxiv.org/abs/2305.14996)

    ACL OCL是一个学术语料库，旨在推动计算语言学领域的开放科研。它提供了丰富的元数据、PDF文件、引用图和结构化全文信息，并可以用于观察计算语言学领域的趋势。ACL OCL的贡献在于发现了"句法：标注、分块和解析"兴趣减弱，"自然语言生成"兴趣复苏的趋势。

    

    我们介绍ACL OCL，这是一个从ACL Anthology中衍生出来的学术语料库，旨在推动计算语言学领域的开放科研。ACL OCL整合和增强了以前版本的ACL Anthology，提供了元数据、PDF文件、引用图和附加的结构化全文信息，包括节、图和链接到大型知识资源（Semantic Scholar）。ACL OCL横跨七十年，包含73K篇论文和210K个图表。我们重点介绍了如何利用ACL OCL来观察计算语言学领域的趋势。通过使用监督神经模型检测论文主题，我们发现对于“句法：标注、分块和解析”的兴趣正在减弱，而“自然语言生成”正在复苏。我们的数据集可以从HuggingFace（https://huggingface.co/datasets/WINGNUS/ACL-OCL）获取。

    We present ACL OCL, a scholarly corpus derived from the ACL Anthology to assist Open scientific research in the Computational Linguistics domain. Integrating and enhancing the previous versions of the ACL Anthology, the ACL OCL contributes metadata, PDF files, citation graphs and additional structured full texts with sections, figures, and links to a large knowledge resource (Semantic Scholar). The ACL OCL spans seven decades, containing 73K papers, alongside 210K figures.  We spotlight how ACL OCL applies to observe trends in computational linguistics. By detecting paper topics with a supervised neural model, we note that interest in "Syntax: Tagging, Chunking and Parsing" is waning and "Natural Language Generation" is resurging. Our dataset is available from HuggingFace (https://huggingface.co/datasets/WINGNUS/ACL-OCL).
    
[^137]: Dolphin: 一个挑战性和多样性的阿拉伯自然语言生成基准

    Dolphin: A Challenging and Diverse Benchmark for Arabic NLG. (arXiv:2305.14989v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14989](http://arxiv.org/abs/2305.14989)

    Dolphin是一个面向阿拉伯语言和变体的自然语言生成评估框架，包含了13种不同的任务，并且提供了大量多样化的数据集和测试集。它为评估阿拉伯语和多语言模型的性能和泛化能力设定了新的标准，有助于推动阿拉伯语NLG研究的发展。

    

    我们提出了Dolphin，一个新颖的基准，旨在满足对多种阿拉伯语言和变体进行自然语言生成（NLG）评估的需求。所提出的基准包括13种不同的NLG任务，包括对话生成、问题回答、机器翻译、摘要等广泛范围。Dolphin包括一个大型的语料库，包含40个多样化和代表性的公开数据集，分布在50个测试集中，经过精心策划以反映真实世界情景和阿拉伯语的语言丰富性。它为评估阿拉伯语和多语言模型的性能和泛化能力设定了新的标准，有望使研究人员能够推动当前方法的发展。我们对Dolphin进行了广泛的分析，突出其多样性，并发现了当前阿拉伯语NLG研究中的差距。我们还提供了一个互动和模块化的公共排行榜，并在其中评估了几个模型。

    We present Dolphin, a novel benchmark that addresses the need for a natural language generation (NLG) evaluation framework dedicated to the wide collection of Arabic languages and varieties. The proposed benchmark encompasses a broad range of 13 different NLG tasks, including dialogue generation, question answering, machine translation, summarization, among others. Dolphin comprises a substantial corpus of 40 diverse and representative public datasets across 50 test splits, carefully curated to reflect real-world scenarios and the linguistic richness of Arabic. It sets a new standard for evaluating the performance and generalization capabilities of Arabic and multilingual models, promising to enable researchers to push the boundaries of current methodologies. We provide an extensive analysis of Dolphin, highlighting its diversity and identifying gaps in current Arabic NLG research. We also offer a public leaderboard that is both interactive and modular and evaluate several models on ou
    
[^138]: 只需提问即可进行标定: 从人类反馈的语言模型中获取标定的置信度得分的策略

    Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback. (arXiv:2305.14975v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14975](http://arxiv.org/abs/2305.14975)

    本论文对从人类反馈的语言模型中提取置信度得分的方法进行了广泛评估。研究发现，对于RLHF-LMs，像ChatGPT、GPT-4和Claude这样的模型输出的语言化置信度通常比条件概率更好地进行了标定。

    

    一个可信赖的实际预测系统应该产生良好标定的置信度得分；也就是说，其对答案的置信度应该能够表明答案正确的可能性，从而在置信度较低的情况下可以寻求专家意见。最近的研究表明，无监督预训练产生的大型语言模型（LMs）的条件概率非常好地进行了标定。然而，最广泛使用的LMs是通过从人类反馈进行强化学习进行精调（RLHF-LMs），一些研究指出RLHF-LMs产生的条件概率非常差地进行标定。鉴于这种知觉上的弱点，我们对从RLHF-LMs中提取置信度得分的方法进行了广泛评估。对于像ChatGPT、GPT-4和Claude这样的RLHF-LMs，我们发现输出标记中发出的语言化的置信度通常比模型在TriviaQA、SciQ和TruthfulQA ben上的条件概率更好地进行了标定。

    A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA ben
    
[^139]: GRACE: 判别器引导的思维链推理

    GRACE: Discriminator-Guided Chain-of-Thought Reasoning. (arXiv:2305.14934v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14934](http://arxiv.org/abs/2305.14934)

    GRACE是一种判别器引导的思维链推理的逐步解码方法，通过使用一个正确性判别器来评分下一步候选，解决了语言模型在多步推理中容易得到错误答案的问题。在多个数学和符号推理任务中，GRACE相较于其他方法在性能上有明显的提升。

    

    在多步推理的背景下，例如使用思维链，语言模型往往会对错误的步骤分配较高的可能性。因此，优化解决方案可能性的解码策略往往会产生错误的解决方案。为了解决这个问题，我们提出了一种称为GRACE的引导思维链推理的逐步解码方法，该方法通过一个正确性判别器训练来引导解码过程产生正确的推理步骤。GRACE使用一个在正确和错误步骤上进行对比损失训练的判别器，该判别器在解码过程中基于正确性对下一步候选进行评分。重要的是，GRACE只需要从语言模型中采样，而不需要进行语言模型的训练或微调。我们使用FLAN-T5和LLaMA系列的模型，对四个数学和两个符号推理任务进行了GRACE的评估，在大多数设置中，与贪婪解码、验证器和自一致性相比，GRACE展现出了显著的性能提升。

    In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps. As a result, decoding strategies that optimize for solution likelihood often yield incorrect solutions. To address this issue, we propose Guiding chain-of-thought ReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise decoding approach that steers the decoding process towards producing correct reasoning steps. GRACE employs a discriminator trained with a contrastive loss over correct and incorrect steps, which is used during decoding to score next-step candidates based on their correctness. Importantly, GRACE only requires sampling from the LM, without the need for LM training or fine-tuning. Using models from FLAN-T5 and LLaMA families, we evaluate GRACE over four math and two symbolic reasoning tasks, where it exhibits substantial performance gains compared to greedy decoding, verifiers, and self-consistency in most settings. When 
    
[^140]: ByteSized32: 一个用于生成以文字游戏形式表达的任务特定世界模型的语料库和挑战任务

    ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games. (arXiv:2305.14879v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14879](http://arxiv.org/abs/2305.14879)

    这项工作研究了语言模型生成科学和常识推理任务的世界模型的能力，通过生成以Python代码形式表达的文字游戏来实现。实验证明GPT-4可以使用这些游戏作为模板进行上下文学习，并引入了一套自动评估指标进行模拟逼真度的评估。

    

    在这项工作中，我们研究了语言模型生成科学和常识推理任务的明确、可解释和互动世界模型的能力。我们将这个任务操作化为生成以Python代码形式表达的文字游戏的任务。为了便于完成这个任务，我们介绍了ByteSized32，一个包含32个以推理为重点的文字游戏的语料库，总共有2万行Python代码。我们经验性地证明GPT-4可以使用这些游戏作为单次上下文学习的模板，在28%的情况下成功生成未见过主题的可运行游戏。当允许自我反思程序错误时，游戏的可运行性大大提高至57%。虽然评估模拟逼真度比较费时，我们引入了一套自动评估指标来评估游戏的逼真度、技术有效性、与任务规格的一致性以及可获胜性，显示出与专家人工评级相当高的一致性。

    In this work, we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a corpus of 32 reasoning-focused text games totaling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 57%. While evaluating simulation fidelity is labor-intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high degree of agreement with expert human ratings. We pose t
    
[^141]: 利用GPT-4进行自动翻译后编辑

    Leveraging GPT-4 for Automatic Translation Post-Editing. (arXiv:2305.14878v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14878](http://arxiv.org/abs/2305.14878)

    GPT-4在翻译后编辑任务中表现出色，通过产生有意义且可靠的编辑，大幅提高了翻译质量并消除了各类重要错误。

    

    尽管神经机器翻译（NMT）代表了机器翻译（MT）的领先方法，但NMT模型的输出仍需要进行翻译后编辑以纠正错误并在关键环境下提高质量。在这项工作中，我们将直接翻译后编辑任务形式化为使用大型语言模型（LLMs）并探索使用GPT-4自动翻译后编辑NMT输出的多种语言对。我们的结果表明， GPT-4在翻译后编辑方面表现出色，能够产生有意义且可靠的编辑，有助于提高其总体质量并消除各类重要错误。特别是，人类对编辑可靠性进行评估的结果显示，GPT-4相较于先前的最先进LLM取得了大幅改进。值得注意的是，我们利用基于GPT-4的翻译后编辑在WMT-22的英中、英德、中英和德英语种对中取得了超越最先进性能的成果。

    While Neural Machine Translation (NMT) represents the leading approach to Machine Translation (MT), the outputs of NMT models still require translation post-editing to rectify errors and enhance quality under critical settings. In this work, we formalize the task of direct translation post-editing with Large Language Models (LLMs) and explore the use of GPT-4 to automatically post-edit NMT outputs across several language pairs. Our results demonstrate that GPT-4 is adept at translation post-editing, producing meaningful and trustworthy edits to translations that help improve its general quality as well as remove different classes of major errors in translations. In particular, human evaluations on assessing edit trustworthiness show that GPT-4 exhibits a large improvement over the prior state-of-the-art LLM. Notably, we improve upon state-of-the-art performance on WMT-22 English-Chinese, English-German, Chinese-English and German-English language pairs using GPT-4 based post-editing, a
    
[^142]: 视觉与语言跨语言转移的元学习

    Meta-learning For Vision-and-language Cross-lingual Transfer. (arXiv:2305.14843v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14843](http://arxiv.org/abs/2305.14843)

    本研究提出了一种元学习微调框架，通过设计跨语言多模态的MAML，使得当前的视觉-语言模型能够快速适应新的语言，从而在跨语言转移中显著提升了性能。

    

    当前的预训练视觉-语言模型在一系列多模态数据集上取得了出色的性能。最近的研究旨在构建多语言模型，并提出了一系列新颖的多语言多模态数据集。但是，当这些模型用于多模态零样本或少样本的跨语言转移，尤其是对于资源有限的语言时，当前的视觉-语言模型通常表现不佳。为了缓解这个问题，我们提出了一种新颖的元学习微调框架。我们的框架通过以跨语言多模态的方式设计MAML，使得当前的视觉-语言模型能够快速适应视觉-语言场景中的新语言。实验证明，我们的方法显著提升了当前最先进的视觉-语言模型在一系列视觉-语言理解任务和数据集（XVNLI，xGQA，MaRVL，xFlicker&Co）上的零样本和少样本跨语言转移的性能。

    Current pre-trained vison-language models (PVLMs) achieve excellent performance on a range of multi-modal datasets. Recent work has aimed at building multilingual models, and a range of novel multilingual multi-modal datasets have been proposed. Current PVLMs typically perform poorly on these datasets when used for multi-modal zero-shot or few-shot cross-lingual transfer, especially for low-resource languages. To alleviate this problem, we propose a novel meta-learning fine-tuning framework. Our framework makes current PVLMs rapidly adaptive to new languages in vision-language scenarios by designing MAML in a cross-lingual multi-modal manner. Experiments show that our method boosts the performance of current state-of-the-art PVLMs in both zero-shot and few-shot cross-lingual transfer on a range of vision-language understanding tasks and datasets (XVNLI, xGQA, MaRVL, xFlicker&Co)
    
[^143]: 在图像描述中探讨对基础问题

    Exploring the Grounding Issues in Image Caption. (arXiv:2305.14616v1 [cs.CL])

    [http://arxiv.org/abs/2305.14616](http://arxiv.org/abs/2305.14616)

    该论文为我们从计算认知语言的角度出发，探讨了图像描述中的多模态语义表征中的基础问题。研究结果表明，情境含义和功能性是生成适当的图像描述的关键。

    

    本篇论文从计算认知语言的角度探讨了多模态语义表征中的基础问题。采用了感知性、功能性、显著性、注意力和生态学多样性关联等五个基础属性进行注释和分析。通过对Flickr30k数据集中所选的图像进行探索性分析和统计建模来进行研究。研究结果表明，对一个对象或事件的全面理解需要认知注意力、语义表达的语义区分和多模态构建。在构建过程中，观察者将情境含义和功能性融入到多模态语义当中，将其巩固到包含视觉和文本元素的图像-文本数据集中的图像描述中。我们的研究结果表明，情境含义和功能性对于基础自然语言理解系统生成适当的图像描述至关重要。

    This paper explores the grounding issue concerning multimodal semantic representation from a computational cognitive-linguistic view. Five perceptual properties of groundedness are annotated and analyzed: Affordance, Perceptual salience, Object number, Gaze cueing, and Ecological Niche Association (ENA). We annotated selected images from the Flickr30k dataset with exploratory analyses and statistical modeling of their captions. Our findings suggest that a comprehensive understanding of an object or event requires cognitive attention, semantic distinctions in linguistic expression, and multimodal construction. During this construction process, viewers integrate situated meaning and affordance into multimodal semantics, which is consolidated into image captions used in the image-text dataset incorporating visual and textual elements. Our findings suggest that situated meaning and affordance grounding are critical for grounded natural language understanding systems to generate appropriate
    
[^144]: 多语言像素表示用于翻译和有效的跨语言转移

    Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer. (arXiv:2305.14280v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14280](http://arxiv.org/abs/2305.14280)

    本文介绍了如何通过像素表示有效地训练多语言机器翻译模型，其中的创新和贡献主要体现在对多种语言和文字的表现优化、像素表示的属性探索以及实现无缝跨语言转移的数据效率。

    

    本文介绍并展示了如何通过像素表示有效地训练多语言机器翻译模型。我们在两种不同的数据设置下进行实验，涵盖了多种语言和文字，与子词嵌入相比，表现出更好的性能。我们探索了像素表示的各种属性，如脚本内和脚本间的参数共享，以更好地理解它们在何处实现了积极的迁移。我们观察到，这些属性不仅使得无缝跨语言转移到未见过的文字成为可能，而且使像素表示比其他方法如词汇扩展更具数据效率。我们希望这项工作能为所有语言和文字创造更具可扩展性的多语言模型。

    We introduce and demonstrate how to effectively train multilingual machine translation models with pixel representations. We experiment with two different data settings with a variety of language and script coverage, demonstrating improved performance compared to subword embeddings. We explore various properties of pixel representations such as parameter sharing within and across scripts to better understand where they lead to positive transfer. We observe that these properties not only enable seamless cross-lingual transfer to unseen scripts, but make pixel representations more data-efficient than alternatives such as vocabulary expansion. We hope this work contributes to more extensible multilingual models for all languages and scripts.
    
[^145]: 迈向预训练大型语言模型中稀疏前馈网络的统一视角

    Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model. (arXiv:2305.13999v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13999](http://arxiv.org/abs/2305.13999)

    这项研究提出了一个统一的框架来分析稀疏前馈网络在预训练大型语言模型中的设计选择。在语言建模任务中，通过使用平均聚合隐藏状态的选择方法，相比现有的MoE架构，可以实现更低的困惑度。

    

    大型且稀疏的前馈层（S-FFN），如专家混合（MoE），已被证明在扩大Transformer模型规模以进行大型语言模型的预训练中非常有效。通过仅激活部分依赖于输入的FFN参数，S-FFN在保持训练和推理成本（以FLOPs计算）不变的同时提高了泛化性能。在本研究中，我们在稀疏神经记忆的通用概念框架下，分析了S-FFN的两个主要设计选择：内存块（即专家）大小和内存块选择方法。利用这个统一框架，我们比较了几种用于语言建模的S-FFN架构，并深入探讨了它们的相对效果和效率。我们发现一种更简单的选择方法 - Avg-K，通过均值聚合的隐藏状态来选择块，相比包括Switch Transformer（Fedus等，2021）和HashLaye在内的现有MoE架构，在语言模型预训练中实现了更低的困惑度。

    Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for \textit{pretraining} large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method -\textbf{\texttt{Avg-K}} that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLaye
    
[^146]: 保持知识不变性：重新思考开放信息抽取的鲁棒性评估

    Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction. (arXiv:2305.13981v1 [cs.CL])

    [http://arxiv.org/abs/2305.13981](http://arxiv.org/abs/2305.13981)

    本文提出了第一个模拟评估开放式信息提取模型在真实世界中的基准测试，并通过判断模型在整个团体上的表现是否始终准确来评估模型的鲁棒性。

    

    鲁棒性是确保自然语言处理模型能够成功应用于现实世界中的关键因素，特别是对于信息抽取任务而言。然而，大多数先前的评估基准都专注于验证配对匹配的正确性，忽略了关键的鲁棒性测量。在本文中，我们提出了第一个基准测试，模拟在真实世界中评估开放式信息提取模型的情况，其中同一知识含义的句法和表达分布会各不相同。我们设计和注释了一个大规模的测试平台，其中每个示例都是一个知识不变的团体，由具有相同含义但结构不同的句子组成。通过进一步阐述鲁棒性指标，当模型在整个团体上的表现始终准确时，被判定为鲁棒性强。我们对过去十年中发表的几种典型模型进行了实验。

    The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial measurement of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a 
    
[^147]: 通过协作交互与学习助手从错误中学习

    Learn from Mistakes through Cooperative Interaction with Study Assistant. (arXiv:2305.13829v1 [cs.CL])

    [http://arxiv.org/abs/2305.13829](http://arxiv.org/abs/2305.13829)

    本文提出了一个新框架 SALAM，通过协作交互与学习助手来帮助 LLM 在反思和改进过程中。该框架通过收集错误并在推理时提供指导方针，显着提高模型性能。

    

    大型语言模型已经证明了它们自我反思和改进生成能力的能力，这可以进一步提高它们的性能。然而，这种反馈机制面临挑战，例如不能保证正确性和对模型弱点缺乏全局洞察力。在本文中，我们提出了一种新的框架 Study Assistant for Large Language Model (SALAM)，以帮助 LLM 在反思和改进过程中。我们根据人类助理研究的灵感，通过将先前的响应与真实值进行定量分级，并在训练阶段收集错误来实现这一点。在推理期间，它根据错误收集确定常见误解，并提供指导方针，以帮助模型在推理期间避免类似的错误。SALAM 是一个模型不可知的框架，专注于提供一般性的反馈，并可适用于任何基础模型。我们在两个具有挑战性的基准测试上对 SALAM 进行了评估，它在各种基线上都获得了显着的改进。

    Large language models have demonstrated their ability to self-reflect and refine their generation, which can further improve their performance. However, this feedback mechanism faces challenges such as no guarantee of correctness and the lack of global insight into the model's weaknesses. In this paper, we propose a novel framework, Study Assistant for Large Language Model (SALAM), to aid LLMs in the reflection and refinement process. Motivated by the human study assistant, this framework grades previous responses with the ground truth and collects mistakes in the training phase. During inference, it identifies common misunderstandings based on the mistake collections and provides guidelines for the model to help the model avoid similar mistakes during inference. SALAM is a model-agnostic framework, focusing on providing general feedback and can adapt to any base model. Our evaluation of SALAM on two challenging benchmarks demonstrated a significant improvement over various baselines.
    
[^148]: 上下文感知神经机器翻译的挑战

    Challenges in Context-Aware Neural Machine Translation. (arXiv:2305.13751v1 [cs.CL])

    [http://arxiv.org/abs/2305.13751](http://arxiv.org/abs/2305.13751)

    本文研究上下文感知神经机器翻译中存在的挑战，并提出了更为真实的文档级翻译设置，段落级翻译(para2para)，以及收集了一份新的中英小说数据集，以促进未来的研究。

    

    上下文感知神经机器翻译涉及利用句子级别上下文之外的信息来解决句际话语依赖关系和提高文档级翻译质量，引起了许多技术方面的关注。然而，尽管有着明智的直觉，大多数上下文感知翻译模型只能显示出适度的改进。本研究探讨了阻碍该领域进展的几个挑战，涉及话语现象、上下文使用、模型架构和文档级评估等方面。为解决这些问题，我们提出了更为真实的文档级翻译设置，称为段落级翻译(para2para)，并收集了一份新的中英小说数据集以促进未来的研究。

    Context-aware neural machine translation involves leveraging information beyond sentence-level context to resolve inter-sentential discourse dependencies and improve document-level translation quality, and has given rise to a number of recent techniques. However, despite well-reasoned intuitions, most context-aware translation models show only modest improvements over sentence-level systems. In this work, we investigate several challenges that impede progress within this field, relating to discourse phenomena, context usage, model architectures, and document-level evaluation. To address these problems, we propose a more realistic setting for document-level translation, called paragraph-to-paragraph (para2para) translation, and collect a new dataset of Chinese-English novels to promote future research.
    
[^149]: 自动词形变化中的组合数据增强理解

    Understanding compositional data augmentation in automatic morphological inflection. (arXiv:2305.13658v1 [cs.CL])

    [http://arxiv.org/abs/2305.13658](http://arxiv.org/abs/2305.13658)

    本研究揭示了自动词形变化中的数据增强策略StemCorrupt带来的根本性变化，并证明选择高多样性和高预测不确定性的数据点子集是提高其数据效率的有效方法。同时，StemCorrupt能够学习可推广的词形规则。

    

    为了解决数据稀疏的问题，数据增强技术被广泛应用于低资源的自动词形变化中。然而，这些技术的全部影响仍然不为人所知。本研究旨在揭示数据增强策略StemCorrupt的理论方面，该方法通过随机替换现有的黄金标准训练样本中的词干字符来生成合成样本。我们的分析揭示了StemCorrupt带来了根本性的变化，在底层数据分布中展现了固有的组合连接结构。为了补充我们的理论分析，我们调查了StemCorrupt的数据效率。通过对七种类型学不同的语言的广泛评估，我们证明选择高多样性和高预测不确定性的数据点子集显著提高了StemCorrupt的数据效率，相比竞争基线。此外，我们展示了StemCorrupt能够学习可推广的词形规则，这是通过其在域外保留数据上表现出的优异性能所证明的。

    Data augmentation techniques are widely used in low-resource automatic morphological inflection to address the issue of data sparsity. However, the full implications of these techniques remain poorly understood. In this study, we aim to shed light on the theoretical aspects of the data augmentation strategy StemCorrupt, a method that generates synthetic examples by randomly substituting stem characters in existing gold standard training examples. Our analysis uncovers that StemCorrupt brings about fundamental changes in the underlying data distribution, revealing inherent compositional concatenative structure. To complement our theoretical analysis, we investigate the data-efficiency of StemCorrupt. Through evaluation across a diverse set of seven typologically distinct languages, we demonstrate that selecting a subset of datapoints with both high diversity and high predictive uncertainty significantly enhances the data-efficiency of StemCorrupt compared to competitive baselines. Furth
    
[^150]: Instruct-Align：通过基于对齐的跨语言教学将新语言教给LLM

    Instruct-Align: Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction. (arXiv:2305.13627v1 [cs.CL])

    [http://arxiv.org/abs/2305.13627](http://arxiv.org/abs/2305.13627)

    Instruct-Align提出了基于对齐的跨语言教学调整框架，使得教学调整的LLMs能够学习新语言，且不会发生灾难性遗忘。

    

    教学调整的大型语言模型（LLM）已经展示了在多种语言和多种任务上的卓越泛化能力。然而，它们对不同语言的泛化能力会有所不同，尤其是对于少数语言或者是未知语言。先前的工作发现，简单地将新语言适应到经过教学调整的LLM中会导致灾难性遗忘，从而导致这些LLM失去多任务能力。为了解决这个问题，我们提出了称为Instruct-Align的框架，通过基于对齐的跨语言教学调整，使得经过教学调整的LLM能够学习到看不见的和之前学习的语言之间的跨语言对齐。我们在BLOOMZ-560M数据集上的初步结果显示，Instruct-Align能够在仅使用有限量的平行语料的情况下有效地学习新语言，并且通过持续的教学调整，防止了灾难性遗忘。

    Instruction-tuned large language models (LLMs) have shown remarkable generalization capability over multiple tasks in multiple languages. Nevertheless, their generalization towards different languages varies especially to underrepresented languages or even to unseen languages. Prior works on adapting new languages to LLMs find that naively adapting new languages to instruction-tuned LLMs will result in catastrophic forgetting, which in turn causes the loss of multitasking ability in these LLMs. To tackle this, we propose the Instruct-Align a.k.a (IA)$^1$ framework, which enables instruction-tuned LLMs to learn cross-lingual alignment between unseen and previously learned languages via alignment-based cross-lingual instruction-tuning. Our preliminary result on BLOOMZ-560M shows that (IA)$^1$ is able to learn a new language effectively with only a limited amount of parallel data and at the same time prevent catastrophic forgetting by applying continual instruction-tuning through experien
    
[^151]: GQA:从多头检查点训练广义多查询Transformer模型

    GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. (arXiv:2305.13245v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13245](http://arxiv.org/abs/2305.13245)

    该论文介绍了一种将现有的多头语言模型检查点升级为具有多查询注意力（MQA）的模型的方法，并引入了群组查询注意力（GQA）来解决MQA可能导致的质量下降问题。通过升级后的GQA模型，实现了接近多头注意力的质量，并具备与MQA相当的速度。

    

    多查询注意力（MQA）仅使用一个键值头，大大加快了解码器推理速度。然而，MQA可能导致质量下降，并且为了更快地推理而训练一个单独的模型可能不是理想的。我们（1）提出了一个方法，利用原始预训练计算量的5％，将现有的多头语言模型检查点升级为具有MQA的模型，并（2）引入了群组查询注意力（GQA），它是多查询注意力的广义形式，使用中间数量的键值头（多于一个，少于查询头的数量）。我们表明，经过升级的GQA实现了与多头注意力相当的速度，并且具有接近的质量。

    Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.
    
[^152]: 最近邻机器翻译是输出投影层上的元优化器

    Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer. (arXiv:2305.13034v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13034](http://arxiv.org/abs/2305.13034)

    最近邻机器翻译将预训练的神经机器翻译模型与领域特定的令牌级检索相结合，实现了领域适应任务中的成功，通过在NMT的输出投影层上隐式执行梯度下降，以达到模型微调的效果。

    

    最近邻机器翻译（$k$NN-MT）通过将预训练的神经机器翻译（NMT）模型与领域特定的令牌级检索相结合，在领域适应任务中取得了巨大的成功。然而，其成功的原因尚未得到深入研究。在本文中，我们通过理论和实证研究全面分析了$k$NN-MT。首先，我们对$k$NN-MT的工作机制提供了新的见解，将其视为在NMT的输出投影层上隐式执行梯度下降的高效技术，表明它是模型微调的特殊情况。随后，我们进行了多领域实验证明和词级分析，以检查$k$NN-MT和整体模型微调之间性能差异。我们的研究结果表明：（1）将$k$NN-MT与适配器相结合，在领域内测试集上具有可比较的翻译性能，同时在领域外测试集上表现更好。

    Nearest Neighbor Machine Translation ($k$NN-MT) has achieved great success in domain adaptation tasks by integrating pre-trained Neural Machine Translation (NMT) models with domain-specific token-level retrieval. However, the reasons underlying its success have not been thoroughly investigated. In this paper, we comprehensively analyze $k$NN-MT through theoretical and empirical studies. Initially, we provide new insights into the working mechanism of $k$NN-MT as an efficient technique to implicitly execute gradient descent on the output projection layer of NMT, indicating that it is a specific case of model fine-tuning. Subsequently, we conduct multi-domain experiments and word-level analysis to examine the differences in performance between $k$NN-MT and entire-model fine-tuning. Our findings suggest that: (1) Incorporating $k$NN-MT with adapters yields comparable translation performance to fine-tuning on in-domain test sets, while achieving better performance on out-of-domain test set
    
[^153]: 使用ChatGPT进行可解释的自动学生答案评估

    Distilling ChatGPT for Explainable Automated Student Answer Assessment. (arXiv:2305.12962v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12962](http://arxiv.org/abs/2305.12962)

    本文提出了一种使用ChatGPT进行解释性的自动学生答案评估的新框架。通过采用不同的模板指导ChatGPT收集理由，修正不一致的理由以符合标准，并通过微调一个更小的语言模型，同时评估学生答案和提供理由。实验证明，该方法相对于ChatGPT将整体QWK评分提高了11%。

    

    提供可解释和可信的反馈对于自动学生答案评估至关重要。在本文中，我们介绍了一种新颖的框架，探索使用ChatGPT，一种尖端的大型语言模型，用于学生答案评分和理由生成的并发任务。我们通过使用不同的模板提示ChatGPT收集理由来确定适当的说明，其中不一致的理由被修正以符合标记标准。精细调整的ChatGPT输出使我们能够微调一个更小的语言模型，同时评估学生答案并提供理由。对基准数据集的大量实验证明，与ChatGPT相比，提出的方法将整体QWK评分提高了11%。此外，我们的彻底分析和人工评估表明，我们提出的方法生成的理由与ChatGPT的理由相当。我们的方法为实现可解释的自动评估提供了可行的解决方案。

    Providing explainable and faithful feedback is crucial for automated student answer assessment. In this paper, we introduce a novel framework that explores using ChatGPT, a cutting-edge large language model, for the concurrent tasks of student answer scoring and rationale generation. We identify the appropriate instructions by prompting ChatGPT with different templates to collect the rationales, where inconsistent rationales are refined to align with marking standards. The refined ChatGPT outputs enable us to fine-tune a smaller language model that simultaneously assesses student answers and provides rationales. Extensive experiments on the benchmark dataset show that the proposed method improves the overall QWK score by 11% compared to ChatGPT. Furthermore, our thorough analysis and human evaluation demonstrate that the rationales generated by our proposed method are comparable to those of ChatGPT. Our approach provides a viable solution to achieve explainable automated assessment in 
    
[^154]: 自然语言处理研究范式转变的历时分析：何时、如何和为何？

    A Diachronic Analysis of Paradigm Shifts in NLP Research: When, How, and Why?. (arXiv:2305.12920v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12920](http://arxiv.org/abs/2305.12920)

    本研究提出了一个系统框架来分析自然语言处理领域研究主题的演变趋势，揭示了任务和方法是驱动研究的主要因素，而数据集和评估指标的影响较小。

    

    理解科学领域的基本概念和趋势对于跟上其持续发展是至关重要的。在本研究中，我们提出了一个系统的框架，利用因果发现和推理技术来分析科学领域中研究主题的演变。我们定义了三个变量，以涵盖NLP研究主题演变的多个方面，并利用因果发现算法揭示这些变量之间的因果关系。随后，我们利用这个结构来测量这些关系的强度。通过在ACL Anthology语料库上进行大量实验，我们证明了我们的框架能够有效地发现广泛的NLP研究主题的演变趋势和潜在原因。具体来说，我们发现任务和方法是推动NLP研究的主要因素，而数据集紧随其后，而评估指标的影响很小。

    Understanding the fundamental concepts and trends in a scientific field is crucial for keeping abreast of its continuous advancement. In this study, we propose a systematic framework for analyzing the evolution of research topics in a scientific field using causal discovery and inference techniques. We define three variables to encompass diverse facets of the evolution of research topics within NLP and utilize a causal discovery algorithm to unveil the causal connections among these variables using observational data. Subsequently, we leverage this structure to measure the intensity of these relationships. By conducting extensive experiments on the ACL Anthology corpus, we demonstrate that our framework effectively uncovers evolutionary trends and the underlying causes for a wide range of NLP research topics. Specifically, we show that tasks and methods are primary drivers of research in NLP, with datasets following, while metrics have minimal impact.
    
[^155]: OPT-R: 探究解释在大型语言模型微调与提示推理技能中的作用

    OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models. (arXiv:2305.12001v1 [cs.CL])

    [http://arxiv.org/abs/2305.12001](http://arxiv.org/abs/2305.12001)

    本文探究了大型语言模型的推理能力，证明解释在模型微调过程中对性能影响不显著，但在提示模型时使用解释可提高模型在某些推理技能上的性能。

    

    本文对大型语言模型（LLMs）的推理能力进行了全面研究，特别关注代表这种模型的Open Pretrained Transformers（OPT）模型。我们在精心策划的推理语料库上微调了三种不同大小的OPT，得到了两组微调模型：没有解释的OPT-R和带有解释的OPT-RE。然后，我们利用三种提示技术对所有模型在来自SUPER-NATURAL INSTRUCTIONS基准测试的57个域外任务上进行评估，涵盖26个不同的推理技能。通过一个全面的27个配置和6,156个测试评估矩阵，我们研究了微调、提示和规模的维度，以了解在不同推理技能方面解释的作用。我们的研究发现，在模型微调时，fewshot示例中有没有解释对模型的性能没有显著影响，而在提示模型时使用解释可提高模型在某些推理技能上的性能。

    In this paper, we conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model's performance when the model is finetuned, while p
    
[^156]: 作为隐含讨论问题的详细简化

    Elaborative Simplification as Implicit Questions Under Discussion. (arXiv:2305.10387v1 [cs.CL])

    [http://arxiv.org/abs/2305.10387](http://arxiv.org/abs/2305.10387)

    本文提出了一种将详细阐述视为隐含问题的明确回答的方法，通过引入ElabQUD对作者阐述信息的方式进行了研究。

    

    自动文本简化通常被认为是编码器-解码器模型下从复杂句子到简化句子的单语翻译工作，有助于使文本更易于让儿童和新兴双语者理解。然而，这种观点忽略了详细简化的情况，即在简化文本中添加新信息的情况。本文提出将详细简化视为讨论问题框架（QUD）的一部分，将详细阐述的信息视为对隐含问题的明确回答，从而提供了一种研究作者阐述哪些信息、如何阐述以及阐述将如何适应话语背景的强大方法。我们引入了ElabQUD，其中包括1.3K的详细阐述和隐含的QUD，以研究这些现象。

    Automated text simplification, a technique useful for making text more accessible to people such as children and emergent bilinguals, is often thought of as a monolingual translation task from complex sentences to simplified sentences using encoder-decoder models. This view fails to account for elaborative simplification, where new information is added into the simplified text. This paper proposes to view elaborative simplification through the lens of the Question Under Discussion (QUD) framework, providing a robust way to investigate what writers elaborate upon, how they elaborate, and how elaborations fit into the discourse context by viewing elaborations as explicit answers to implicit questions. We introduce ElabQUD, consisting of 1.3K elaborations accompanied with implicit QUDs, to study these phenomena. We show that explicitly modeling QUD (via question generation) not only provides essential understanding of elaborative simplification and how the elaborations connect with the re
    
[^157]: CoEdIT：通过任务特定指令调整实现文本编辑

    CoEdIT: Text Editing by Task-Specific Instruction Tuning. (arXiv:2305.09857v1 [cs.CL])

    [http://arxiv.org/abs/2305.09857](http://arxiv.org/abs/2305.09857)

    CoEdIT是一种通过任务特定指令调整实现文本编辑的最先进模型，能够提高用户生成文本的质量和提高流程的效率。

    

    文本编辑或修订是人类写作过程中必不可少的功能。理解LLMs在进行高质量修订和与人类写作者协作方面的能力是构建有效写作助手的关键步骤。在LLMs和指令调整的先前成功基础上，我们利用经过指令调整的LLMs进行文本修订，以提高用户生成文本的质量和提高流程的效率。我们引入了CoEdIT，这是一款用于写作辅助的最先进的文本编辑模型。CoEdIT从用户那里获取指令，指定所需文本的属性，例如“使句子更简单”或“以更中立的风格写作”，并输出编辑后的文本。我们提供了一个大型语言模型，该模型在各种文本编辑基准测试上实现了最先进的性能。我们的模型（1）在各种文本编辑基准测试上实现最先进的性能，（2）与公开可用的模型相比具有竞争力。

    Text editing or revision is an essential function of the human writing process. Understanding the capabilities of LLMs for making high-quality revisions and collaborating with human writers is a critical step toward building effective writing assistants. With the prior success of LLMs and instruction tuning, we leverage instruction-tuned LLMs for text revision to improve the quality of user-generated text and improve the efficiency of the process. We introduce CoEdIT, a state-of-the-art text editing model for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as "Make the sentence simpler" or "Write it in a more neutral style," and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly availa
    
[^158]: 横向迁移轻量化视觉提示发生器在VL-LLMs之间的应用研究

    Transfer Visual Prompt Generator across LLMs. (arXiv:2305.01278v1 [cs.CV])

    [http://arxiv.org/abs/2305.01278](http://arxiv.org/abs/2305.01278)

    本论文提出将已有的轻量化视觉提示发生器连接到视觉-语言LLM以减少资源消耗的方法，并提出了跨不同大小和类型的LLMs的VPG转移方案VPGTrans，该方案在VQA和NLVR2任务中表现优秀。

    

    本文研究利用现有的轻量化视觉提示发生器（VPG）连接已有的视觉-语言LLM（VL-LLM）以减少资源消耗。此外，我们提出一种跨不同大小和类型的LLMs的VPG转移方案。基于我们的观察，我们设计了一个名为VPGTrans的两阶段转移框架，它在VQA和NLVR2两个下游任务中表现出比现有方法更好的精度和转移速度。

    While developing a new vision-language LLM (VL-LLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG part of the VL-LLM still suffers from indispensable computational costs, i.e., requiring thousands of GPU hours and millions of training data. One alternative solution is to transfer an existing VPG from any existing VL-LLMs for the target VL-LLM.  In this work, we for the first time investigate the VPG transferability across LLMs, and explore a solution to reduce the cost of VPG transfer. We first study the VPG transfer across different LLM sizes (e.g., small-to-large), and across different LLM types, through which we diagnose the key factors to maximize the transfer efficiency. Based on our observation, we design a two-stage transfer framework named VPGTrans, which is simple yet highly e
    
[^159]: GPT-2是如何计算大于符号的？解释预训练语言模型中的数学能力

    How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])

    [http://arxiv.org/abs/2305.00586](http://arxiv.org/abs/2305.00586)

    本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。

    

    预训练语言模型在未被明确训练的任务上表现出惊人的能力，但它们如何实现这些功能却不为人所知。本文通过机械式可解释性技术探究预训练语言模型通常具有的基本数学能力。具体来说，我们以GPT-2 Small为例，研究其能否通过输入"战争持续时间是从1732年到17年"，预测出有效的两位数字的截止年份 (大于32年)。我们首先确定了一个电路，即GPT-2 Small计算图的一个小子集，用于计算这个任务的输出，然后我们解释了每个电路组件的作用，显示出GPT-2 Small的最终多层感知器提高了结束年份大于开始年份的概率。最后，我们证明了我们的电路适用于其他任务，在其他大于场景中发挥作用。

    Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years > 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
    
[^160]: 评估生成式搜索引擎中的可验证性

    Evaluating Verifiability in Generative Search Engines. (arXiv:2304.09848v1 [cs.CL])

    [http://arxiv.org/abs/2304.09848](http://arxiv.org/abs/2304.09848)

    本文评估了四个流行生成式搜索引擎的可验证性，发现现有生成式搜索引擎响应流畅但仅有51.5%的生成句子得到了完整的引用支持，仅有74.5%的引用支持其相关语句。

    

    生成式搜索引擎直接为用户查询生成响应，并提供内联引用。一个值得信赖的生成式搜索引擎的先决条件是可验证性，即系统应全面引用（高引用回忆率，所有语句都有完整的引用支持）和准确（高引用精度，每个引用都支持其相关语句）。我们对四个流行的生成式搜索引擎——Bing Chat、NeevaAI、perplexity.ai和YouChat——进行了人类评估，涵盖了各种来源的多样化查询（例如历史上的Google用户查询、Reddit上动态收集的开放性问题等）。我们发现现有的生成式搜索引擎响应流畅且信息丰富，但常常包含不支持的语句和不准确的引用：平均而言，仅有51.5%的生成句子得到了完整的引用支持，只有74.5%的引用支持其相关语句。我们认为...

    Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5% of generated sentences are fully supported by citations and only 74.5% of citations support their associated sentence. We believe that
    
[^161]: 大型语言模型在文档级机器翻译中的应用研究

    Document-Level Machine Translation with Large Language Models. (arXiv:2304.02210v1 [cs.CL])

    [http://arxiv.org/abs/2304.02210](http://arxiv.org/abs/2304.02210)

    本文以文档级机器翻译为试验场，深入评估了大型语言模型在语篇建模方面的性能。研究发现利用LLMs强大的长文本建模能力可以提高翻译质量，在提示方面进行改进也可以显着提高翻译质量，并且LLMs有潜力编码丰富的语篇知识。

    

    大型语言模型（LLMs）如Chat-GPT可以为各种自然语言处理（NLP）任务生成连贯，连贯，相关和流畅的答案。本文以文档级机器翻译为试验场，提供了LLMs在语篇建模方面的深入评估。本研究着重关注三个方面：1）语篇感知提示的影响，我们调查不同提示对文档级翻译质量和语篇现象的影响；2）翻译模型的比较，我们比较Chat-GPT与商业MT系统和高级文档级MT方法的翻译性能；3）语篇建模能力分析，我们进一步探究LLMs中编码的语篇知识，并研究培训技术对语篇建模的影响。通过评估许多基准测试，我们惊讶地发现，1）利用强大的长文本建模能力，ChatGPT在文档级翻译质量方面优于商业MT系统和高级文档级MT方法；2）修改明确针对语篇现象的提示可以显着提高翻译质量；3）LLMs有潜力编码丰富的语篇知识，培训技术可以进一步增强这种能力。

    Large language models (LLMs) such as Chat-GPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study fo-cuses on three aspects: 1) Effects of Discourse-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of Chat-GPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and examine the impact of training techniques on discourse modeling. By evaluating a number of benchmarks, we surprisingly find that 1) leveraging their powerful long-text mod-eling capabilities, ChatGPT outperforms commercial MT systems 
    
[^162]: 机器心理学：利用心理学方法探究大型语言模型的新兴能力和行为

    Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])

    [http://arxiv.org/abs/2303.13988](http://arxiv.org/abs/2303.13988)

    本文提出了一种新领域——机器心理学，利用心理学的方法考察大型语言模型的能力。该文规范了机器心理学研究的方法论标准，并对心理实验中提示设计政策进行了探讨和制定。

    

    大型语言模型（LLM）是将人工智能系统与人类交流和日常生活紧密结合的先锋。由于快速技术进步和其极高的通用性，现今LLM已经拥有数百万用户，并正处于成为主要信息检索、内容生成、问题解决等技术的前沿。因此，对其进行全面评估和审查显得尤为重要。由于当前LLM中出现愈加复杂和新颖的行为模式，可将其视为参与人类心理实验的对象，以便更为全面地评估其能力。为此，本文引入了一个名为"机器心理学"的新兴研究领域。本文概述了各类心理学分支如何为LLM的行为测试提供有用参考。同时，本文规范了机器心理学研究的方法论标准，特别是专注于提示设计政策的制定。此外，它还描述了行为测试结果如何为未来的LLM发展提供指导。

    Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
    
[^163]: CoLT5: 基于条件计算的快速长距离Transformer模型

    CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])

    [http://arxiv.org/abs/2303.09752](http://arxiv.org/abs/2303.09752)

    CoLT5是一种基于条件计算的Transformer模型，通过优先处理重要标记来加速长距离输入的处理。CoLT5在SCROLLS基准测试上表现最好，并能够有效地处理长达64k输入长度。

    

    许多自然语言处理任务需要处理长输入，但使用Transformer处理长文档很昂贵——这不仅是因为二次注意复杂性，还因为对每个标记应用前馈和投影层。然而，不是所有标记都同样重要，特别是对于较长的文档。我们提出了CoLT5，一种长输入Transformer模型，通过使用条件计算来利用此直觉，在前馈和注意层中为重要标记提供更多资源。我们展示了CoLT5比LongT5表现更强，训练和推理速度更快，在长输入SCROLLS基准测试上达到了SOTA。此外，CoLT5能够有效且可控地利用极长的输入，展示了高达64k输入长度的强大增益。

    Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
    
[^164]: ChatGPT的一致性分析

    Consistency Analysis of ChatGPT. (arXiv:2303.06273v1 [cs.CL])

    [http://arxiv.org/abs/2303.06273](http://arxiv.org/abs/2303.06273)

    本文研究了ChatGPT的一致性问题，发现尽管它具有更好的语言理解能力，但仍然经常无法生成逻辑上正确的预测。因此，在现实世界的应用需要进一步考虑，特别是在风险方面。

    This paper investigates the consistency issue of ChatGPT and finds that although it has improved language understanding ability, it frequently fails to generate logically correct predictions. Therefore, further consideration is needed for its real-world applications, especially in terms of risk.

    ChatGPT是一种基于大型语言模型的问答对话系统，自推出以来广受欢迎。虽然它在法律、医学和金融等领域的专业考试中取得了不错的成绩，但也有人对其可靠性和信任度表示怀疑。本文针对ChatGPT在逻辑一致性方面的可信度进行了调查研究。我们的研究发现，尽管ChatGPT似乎具有更好的语言理解能力，但它仍然经常无法生成逻辑上正确的预测。因此，虽然ChatGPT是一种令人印象深刻和有前途的新技术，但我们得出结论，如果没有经过彻底的人工检查，它在现实世界的应用需要进一步考虑，特别是在风险方面。

    ChatGPT, a question-and-answer dialogue system based on a large language model, has gained huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, including the law, medical, and finance domains, adding extra support to the claim that AI now can assist and, even, replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. In this paper, we investigate ChatGPT's trustworthiness regarding logically consistent behaviours. Our findings suggest that, although ChatGPT seems to achieve an improved language understanding ability, it still fails to generate logically correct predictions frequently. Hence, while it is true that ChatGPT is an impressive and promising new technique, we conclude that its usage in real-world applications without thorough human inspection requires further consideration, especially for risk
    
[^165]: ChatGPT作为自然语言生成的评价指标可靠吗？初步研究。

    Is ChatGPT a Good NLG Evaluator? A Preliminary Study. (arXiv:2303.04048v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04048](http://arxiv.org/abs/2303.04048)

    通过针对任务特定和方面特定，我们在五个NLG元评估数据集上进行实验，表明ChatGPT作为NLG评估指标并不总是与人类评估相一致，尤其是在流畅度方面。这提醒人们在使用ChatGPT作为唯一的自动NLG评估指标时要谨慎。

    

    最近，ChatGPT的出现引起了计算语言学界的广泛关注。许多先前的研究表明，ChatGPT在各种NLP任务中以自动评估指标为基础获得了显着的性能。然而，ChatGPT作为一种评估指标的能力尚未得到充分探索。考虑到评估自然语言生成（NLG）模型的质量是一项艰巨的任务，并且NLG指标以其糟糕的与人类判断的相关性而闻名，因此我们是否会认为ChatGPT是一个好的NLG评估指标。在这篇报告中，我们对ChatGPT进行了初步的元评估，展示了ChatGPT作为NLG指标的可靠性。具体而言，我们将ChatGPT视为人类评估器，并针对任务特定（例如摘要）和方面特定（例如相关性）进行说明，以促使ChatGPT评估NLG模型的生成结果。我们在包括摘要、故事生成和翻译在内的五个NLG元评估数据集上进行实验。我们的结果表明，对于某些方面（例如流畅度），ChatGPT并不总是与人类评估相一致。这提醒人们在使用ChatGPT作为唯一的自动NLG评估指标时要谨慎。

    Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and 
    
[^166]: CoSyn：使用上下文协同的双曲线网络检测在线对话中的隐含仇恨言论

    CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network. (arXiv:2303.03387v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03387](http://arxiv.org/abs/2303.03387)

    CoSyn是一个上下文协同的神经网络，用于检测在线对话中的隐含仇恨言论。它通过引入新的编码方法和上下文交互机制，在双曲空间中进行操作，以适应社交媒体的特点。

    

    在线对话中隐含的仇恨言论对来自各个群体的人们产生了重要影响，因此社交媒体用户越来越多。大部分之前的研究都集中于检测明确的仇恨言论，这些言论明显且利用了仇恨短语，对于检测隐含或通过间接或编码语言表达出的仇恨言论的研究很少。在本文中，我们提出了CoSyn，一个上下文协同的神经网络，明确地结合了用户和对话上下文来检测在线对话中的隐含仇恨言论。CoSyn引入了新的方法来编码这些外部上下文，并采用了一种新颖的上下文交互机制，清晰地捕捉了它们之间的相互作用，独立评估了从这些嘈杂的上下文中检索的信息量。此外，它在双曲空间中进行所有这些操作，以适应社交媒体的无标度动态。

    The tremendous growth of social media users interacting in online conversations has led to significant growth in hate speech, affecting people from various demographics. Most of the prior works focus on detecting explicit hate speech, which is overt and leverages hateful phrases, with very little work focusing on detecting hate speech that is implicit or denotes hatred through indirect or coded language. In this paper, we present CoSyn, a context-synergized neural network that explicitly incorporates user- and conversational context for detecting implicit hate speech in online conversations. CoSyn introduces novel ways to encode these external contexts and employs a novel context interaction mechanism that clearly captures the interplay between them, making independent assessments of the amounts of information to be retrieved from these noisy contexts. Additionally, it carries out all these operations in the hyperbolic space to account for the scale-free dynamics of social media. We de
    
[^167]: 基于大语言模型的零样本跨语言摘要

    Zero-Shot Cross-Lingual Summarization via Large Language Models. (arXiv:2302.14229v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.14229](http://arxiv.org/abs/2302.14229)

    本文实验性地使用各种提示来指导大型语言模型从不同的范式执行零样本跨语言摘要，并成功提高了它们的CLS性能。其中，GPT-4实现了零样本CLS的最先进性能，并且在性能方面与最佳方法相当。

    

    给定一个源语言文本，跨语言摘要（CLS）旨在生成另一种目标语言的摘要。最近，大型语言模型（LLM）的出现，比如GPT-3.5、ChatGPT和GPT-4，引起了计算语言学界的广泛关注。然而，目前尚不清楚LLM在CLS上的表现如何。本文实验性地使用各种提示来指导LLM从不同的范式（即端到端和流水线）执行零样本CLS，并对生成的摘要进行初步评估。我们发现，ChatGPT和GPT-4原本更喜欢生成详细信息的长摘要。但这两个LLM在交互式提示的帮助下可以进一步平衡信息量和简洁性，显著提高它们的CLS性能。在三个广泛使用的CLS数据集上的实验结果表明，GPT-4实现了零样本CLS的最先进性能，并且在性能方面与最佳方法相当。

    Given a document in a source language, cross-lingual summarization (CLS) aims to generate a summary in a different target language. Recently, the emergence of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has attracted wide attention from the computational linguistics community. However, it is not yet known the performance of LLMs on CLS. In this report, we empirically use various prompts to guide LLMs to perform zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and provide a preliminary evaluation on the generated summaries. We find that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with th
    
[^168]: 通过重要性重采样进行语言模型的数据选择

    Data Selection for Language Models via Importance Resampling. (arXiv:2302.03169v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03169](http://arxiv.org/abs/2302.03169)

    通过重要性重采样方法，我们提出了一种高效且可扩展的数据选择框架（DSIR），可以在语言模型中选择适合的预训练数据集。我们使用KL减少作为数据度量来确定合适的特征空间，并在降维特征空间中估计重要性权重以进行数据选择。

    

    选择适合的预训练数据集对于通用领域（如GPT-3）和特定领域（如Codex）的语言模型（LM）都至关重要。我们将这个问题形式化为从大型原始无标签数据集中选择一个子集，以匹配给定一些无标签目标样本的所需目标分布。鉴于原始文本数据的大规模和高维度，现有方法使用简单的启发式方法或专家手动策划数据。相反，我们扩展了在LM数据选择中使用的经典重要性重采样方法，以低维空间进行数据选择。我们提出了一种高效且可扩展的框架，称为数据选择与重要性重采样（DSIR），它在一个降维特征空间中估计重要性权重，以便根据这些权重进行重要性重采样数据选择。为了确定一个合适的特征空间，我们还展示了KL减少，一种在特征空间中衡量所选预训练数据与目标之间相似度的数据度量，具有较高的相关性。

    Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given some unlabeled target samples. Due to the large scale and dimensionality of the raw text data, existing methods use simple heuristics or use experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. To determine an appropriate feature space, we show that KL reduction, a data metric that measures the proximity between selected pretraining data and the target in a feature space, has high correlat
    
[^169]: Mo^usai: 使用长上下文潜在扩散进行文本到音乐生成

    Mo\^usai: Text-to-Music Generation with Long-Context Latent Diffusion. (arXiv:2301.11757v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11757](http://arxiv.org/abs/2301.11757)

    本研究开发了一种高效、表现力强且能处理长期结构的文本到音乐生成模型Mo^usai，可以根据文本描述生成多分钟高质量音乐。通过实验证明了该模型在多个标准上的优势。

    

    近年来，大规模生成模型在文本领域取得了快速发展；然而，对于文本与另一种“语言”——音乐的关联关系，研究相对较少。音乐与文本一样，可以传达情感、故事和思想，具有自己独特的结构和语法。我们的工作通过一种高效、表现力强且能够处理长期结构的文本到音乐生成模型，将文本与音乐联系起来。具体而言，我们开发了Mo^usai，这是一个级联的两阶段潜在扩散模型，可以根据文本描述生成多分钟的高质量48kHz立体声音乐。此外，我们的模型具有高效性，可以在单个消费级GPU上进行实时推断，并具有合理的速度。通过实验证明了我们模型在多种标准下相对于现有音乐生成模型的优势。最后，为了推动开源文化，我们提供了一套开源的工具集。

    Recent years have seen the rapid development of large generative models for text; however, much less research has explored the connection between text and another "language" of communication -- music. Music, much like text, can convey emotions, stories, and ideas, and has its own unique structure and syntax. In our work, we bridge text and music via a text-to-music generation model that is highly efficient, expressive, and can handle long-term structure. Specifically, we develop Mo\^usai, a cascading two-stage latent diffusion model that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. Moreover, our model features high efficiency, which enables real-time inference on a single consumer GPU with a reasonable speed. Through experiments and property analyses, we show our model's competence over a variety of criteria compared with existing music generation models. Lastly, to promote the open-source culture, we provide a collection of open-source
    
[^170]: 批量提示：使用大型语言模型API进行高效推断

    Batch Prompting: Efficient Inference with Large Language Model APIs. (arXiv:2301.08721v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.08721](http://arxiv.org/abs/2301.08721)

    批量提示是一种简单但有效的方法，可以降低使用大型语言模型进行推断的计算和财务成本，同时保持下游性能。理论上证明，在少样本情况下，批量样本数量的增加几乎以倒数线性关系降低了推断成本。在多个数据集上的验证实验证明了批量提示的有效性，并且对于最先进的Chat-based LLMs，如GPT-3.5和GPT-4，批量提示也具有好处。

    

    在工业和实际应用中，使用大型语言模型（LLM）进行大量样本的推断可能会在计算和财务上代价高昂。我们提出了批量提示的简单而有效的提示方法，使LLM能够批量进行推断，而不是逐个样本。我们的方法减少了令牌和时间成本，同时保持了下游性能。我们从理论上证明，在少样本上下文学习的情况下，随着每批样本数量的增加，推断成本几乎以倒数线性关系降低。我们在常识问答、算术推理和NLI/NLU等十个数据集上广泛验证了批量提示的有效性：批量提示显著（每批六个样本时最高可减少5倍）降低了LLM（Codex）的推断令牌和时间成本，同时实现了更好或可比较的性能。对于最先进的基于聊天的LLM，例如GPT-3.5和GPT-4，我们还展示了批量提示的好处。

    Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Furth
    
[^171]: T-Projection：高质量的用于序列标注任务的注释投影

    T-Projection: High Quality Annotation Projection for Sequence Labeling Tasks. (arXiv:2212.10548v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10548](http://arxiv.org/abs/2212.10548)

    T-Projection是一种新的注释投影方法，利用大型预训练语言模型和先进的机器翻译技术。它将标注投影任务分解为候选生成和候选选择两个子任务，有效地生成高质量的序列标注数据。

    

    在缺少特定序列标注任务和语言的标注数据的情况下，注释投影被提出作为一种可能的策略，用于自动生成注释数据。注释投影通常被形式化为在平行语料库上将源语言中给定跨度的标注传输到目标语言中相应的跨度的任务。在本文中，我们提出了T-Projection，一种利用大型预训练文本到文本语言模型和最先进的机器翻译技术实现注释投影的新方法。T-Projection将标注投影任务分解为两个子任务：(i)候选生成步骤，使用多语言T5模型生成一组投影候选，(ii)候选选择步骤，根据翻译概率对生成的候选进行排名。我们在5个印欧语言上进行了内在和外在任务的实验。

    In the absence of readily available labeled data for a given sequence labeling task and language, annotation projection has been proposed as one of the possible strategies to automatically generate annotated data. Annotation projection has often been formulated as the task of transporting, on parallel corpora, the labels pertaining to a given span in the source language into its corresponding span in the target language. In this paper we present T-Projection, a novel approach for annotation projection that leverages large pretrained text-to-text language models and state-of-the-art machine translation technology. T-Projection decomposes the label projection task into two subtasks: (i) A candidate generation step, in which a set of projection candidates using a multilingual T5 model is generated and, (ii) a candidate selection step, in which the generated candidates are ranked based on translation probabilities. We conducted experiments on intrinsic and extrinsic tasks in 5 Indo-Europea
    
[^172]: SODA: 具有社交常识语境化的百万规模对话蒸馏

    SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization. (arXiv:2212.10465v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10465](http://arxiv.org/abs/2212.10465)

    SODA是第一个公开发布的百万级别高质量社交对话数据集，通过从知识图谱中上下文化社交常识知识进行蒸馏，我们训练出了COSMO，其比目前最佳表现的对话模型更为自然和一致，这有助于了解知识丰富型对话和自然社交闲聊之间的差异。

    

    我们提出了SODA：第一个公开可用的百万规模高质量社交对话数据集。与大多数现有的众包小规模对话语料库不同，我们通过从知识图谱（Atomic10x; West等人，2022）中的社交常识知识进行上下文化，提炼了150万个社交对话。人类评估表明，SODA中的对话比以前的由人类撰写的数据集更一致、更具体且（令人惊讶地）更自然。我们使用SODA训练了COSMO：一个通用的对话模型，在未知的数据集上比最佳表现的对话模型（例如GODEL、BlenderBot-1、Koala、Vicuna）更自然和一致。实验结果表明，COSMO有时甚至被认为优于原始的人工编写的标准回答。此外，我们的结果揭示了知识丰富型对话和自然社交闲聊之间的区别。

    We present SODA: the first publicly available, million-scale high-quality social dialogue dataset. In contrast to most existing crowdsourced, small-scale dialogue corpora, we distill 1.5M socially-grounded dialogues from a large language model (InstructGPT; Ouyang et al., 2022). Dialogues are distilled by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x; West et al., 2022). Human evaluation shows that dialogues in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets.  Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. 
    
[^173]: MaXM：走向多语言视觉问答

    MaXM: Towards Multilingual Visual Question Answering. (arXiv:2209.05401v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.05401](http://arxiv.org/abs/2209.05401)

    本文提出了针对多语言视觉问答的可扩展解决方案，包括数据生成和建模。通过基于翻译的数据生成框架和高效的注释协议，创建了包含7种不同语言的mVQA基准MaXM。同时，开发了简单、轻量级、高效的VQA模型。鼓励进一步研究mVQA。

    

    视觉问答(VQA)主要通过英语进行研究。然而，以同样的方式在其他语言中解决VQA问题需要大量的资源。本文提出了可扩展的解决方案，用于多语言视觉问答(mVQA)，包括数据和建模方面。我们首先提出了一种基于翻译的mVQA数据生成框架，比直接收集问题和答案的传统方法需要更少的人工注释工作量。然后，我们将该框架应用于Crossmodal-3600数据集中的多语言字幕，开发了一种高效的注释协议，创建了MaXM，一个包含7种不同语言的仅用于测试的VQA基准。最后，我们开发了一个简单、轻量级、高效的方法，并基于此构建了英语和多语言VQA模型。我们希望我们的基准能够鼓励进一步研究mVQA。

    Visual Question Answering (VQA) has been primarily studied through the lens of the English language. Yet, tackling VQA in other languages in the same manner would require a considerable amount of resources. In this paper, we propose scalable solutions to multilingual visual question answering (mVQA), on both data and modeling fronts. We first propose a translation-based framework to mVQA data generation that requires much less human annotation efforts than the conventional approach of directly collection questions and answers. Then, we apply our framework to the multilingual captions in the Crossmodal-3600 dataset and develop an efficient annotation protocol to create MaXM, a test-only VQA benchmark in 7 diverse languages. Finally, we develop a simple, lightweight, and effective approach as well as benchmark state-of-the-art English and multilingual VQA models. We hope that our benchmark encourages further research on mVQA.
    
[^174]: SCL-RAI: 基于跨度对比学习与检索增强推理的无标签实体问题的命名实体识别方法

    SCL-RAI: Span-based Contrastive Learning with Retrieval Augmented Inference for Unlabeled Entity Problem in NER. (arXiv:2209.01646v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.01646](http://arxiv.org/abs/2209.01646)

    SCL-RAI是一种应对命名实体识别中无标签实体问题的方法。它利用跨度对比学习减小了同类跨度之间的距离、增大了不同类跨度之间的距离，从而提高了对无标签实体的识别准确性。此外，通过检索增强推理，还解决了决策边界偏移问题。实验证明，该方法在真实世界数据集上明显优于之前的方法。

    

    命名实体识别是定位和分类文本中实体的任务。然而，NER数据集中的无标签实体问题严重阻碍了NER性能的提升。本文提出了SCL-RAI来解决这个问题。首先，我们通过基于跨度的对比学习减小了具有相同标签的跨度表示之间的距离，同时增加了具有不同标签的跨度表示之间的距离，从而缓解了实体之间的歧义，并提高了模型对无标签实体的鲁棒性。然后，我们提出了检索增强推理来减轻决策边界偏移的问题。我们的方法在两个真实世界数据集上的F1分数分别比之前的SOTA方法提高了4.21%和8.64%。

    Named Entity Recognition is the task to locate and classify the entities in the text. However, Unlabeled Entity Problem in NER datasets seriously hinders the improvement of NER performance. This paper proposes SCL-RAI to cope with this problem. Firstly, we decrease the distance of span representations with the same label while increasing it for different ones via span-based contrastive learning, which relieves the ambiguity among entities and improves the robustness of the model over unlabeled entities. Then we propose retrieval augmented inference to mitigate the decision boundary shifting problem. Our method significantly outperforms the previous SOTA method by 4.21% and 8.64% F1-score on two real-world datasets.
    
[^175]: JAMES: 基于多方面图嵌入和推理的岗位职称规范化

    JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and Reasoning. (arXiv:2202.10739v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2202.10739](http://arxiv.org/abs/2202.10739)

    JAMES是一个用于岗位职称规范化的解决方案，它通过构建三种独特嵌入和使用协同注意机制和神经逻辑推理表示来有效地捕捉岗位职称的各种特征，并解决了语义相似性、非规范化用户创建的职称以及实际应用中大规模和长尾分布的岗位职称等挑战。

    

    在在线职位市场中，建立一个明确定义的岗位职称分类体系对于各种下游任务（如工作推荐、用户职业分析和离职预测）至关重要。岗位职称规范化是将用户创建的非标准岗位职称分类为规范化职称的一个清洁步骤。然而，解决岗位职称规范化问题并不容易，面临着以下挑战：(1)不同岗位职称的语义相似性，(2)非规范化的用户创建的岗位职称，以及(3)实际应用中大规模和长尾分布的岗位职称。为了解决这个问题，我们提出了一种名为JAMES的新颖解决方案，它构建目标岗位职称的三种独特嵌入（即图、上下文和句法），以有效捕捉其各种特征。我们进一步提出了一种多方面协同注意机制来注意地结合这些嵌入，还使用神经逻辑推理表示共同估计混乱的岗位职称与规范化岗位职称之间的相似性。

    In online job marketplaces, it is important to establish a well-defined job title taxonomy for various downstream tasks (e.g., job recommendation, users' career analysis, and turnover prediction). Job Title Normalization (JTN) is such a cleaning step to classify user-created non-standard job titles into normalized ones. However, solving the JTN problem is non-trivial with challenges: (1) semantic similarity of different job titles, (2) non-normalized user-created job titles, and (3) large-scale and long-tailed job titles in real-world applications. To this end, we propose a novel solution, named JAMES, that constructs three unique embeddings (i.e., graph, contextual, and syntactic) of a target job title to effectively capture its various traits. We further propose a multi-aspect co-attention mechanism to attentively combine these embeddings, and employ neural logical reasoning representations to collaboratively estimate similarities between messy job titles and normalized job titles in
    
[^176]: 可解释的序列分类通过原型轨迹

    Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.01777](http://arxiv.org/abs/2007.01777)

    ProtoryNet是一种基于原型轨迹的可解释深度神经网络，它通过捕捉时间模式和原型的近似程度来进行文本分类，并实现了直观和细致的推理过程解释。

    

    我们提出了一种新颖的用于文本分类的可解释深度神经网络，称为ProtoryNet，它基于原型轨迹的新概念。受现代语言学中的原型理论的启发，ProtoryNet通过为文本序列中的每个句子找到最相似的原型，并将每个句子与相应的活动原型的接近程度输入到RNN主干中进行预测。然后，RNN主干捕捉到原型的时间模式，我们称之为原型轨迹。原型轨迹能够直观而细致地解释RNN模型的推理过程，类似于人类分析文本的方式。我们还设计了原型修剪过程，以减少模型使用的原型总数，以提高解释性。在多个公共数据集上的实验证明，ProtoryNet比基线的基于原型的深度神经网络更准确，并减少了与现有模型相比的性能差距。

    We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-o
    

