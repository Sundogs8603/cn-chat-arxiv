# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text](https://rss.arxiv.org/abs/2401.09407) | 该论文通过研究在真实世界场景中检测机器生成文本的方法，发现现有方法对于不同生成器和领域产生的文本存在严重限制。 |
| [^2] | [ALOHa: A New Measure for Hallucination in Captioning Models](https://arxiv.org/abs/2404.02904) | 提出了一种新的用于测量图像字幕模型中幻觉的标准ALOHa，利用大型语言模型来测量幻觉对象，并成功识别比现有指标CHAIR更多的幻觉对象。 |
| [^3] | [ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline](https://arxiv.org/abs/2404.02893) | ChatGLM-Math通过自我批判管道在大型语言模型中实现了数学问题解决能力的显著增强 |
| [^4] | [Linear Attention Sequence Parallelism](https://arxiv.org/abs/2404.02882) | 提出了一种名为线性注意力序列并行（LASP）的高效序列并行方法，针对线性注意力的语言模型进行了优化，通过设计高效的点对点通信机制和执行内核融合来降低通信开销，并实现硬件友好性。 |
| [^5] | [Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models](https://arxiv.org/abs/2404.02837) | 论文揭示了在大语言模型中存在参数异质性的现象，提出了一种名为CherryQ的量化方法，该方法能够在保留关键参数的同时将其余参数高效量化至低精度，在性能方面明显优于现有方法。 |
| [^6] | [Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison](https://arxiv.org/abs/2404.02835) | 本文研究了不同的检索方法对几种翻译架构的影响，发现检索技术的选择会影响翻译得分，增加示例数量和多样性通常对翻译效果有积极的影响。 |
| [^7] | [Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models](https://arxiv.org/abs/2404.02823) | Conifer提出了一个新的指令调节数据集，通过LLMs驱动的细化过程，以及渐进学习方案，显著提高了大型语言模型遵循具有复杂约束的多层指令的能力 |
| [^8] | [Identifying Climate Targets in National Laws and Policies using Machine Learning](https://arxiv.org/abs/2404.02822) | 本文提出了一种从国家法律和政策中提取气候目标的方法，可以可靠地识别出三类目标（“净零”，“减少”和“其他”），并调查了与模型相关的偏见和公平影响。 |
| [^9] | [On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension](https://arxiv.org/abs/2404.02800) | 提出了用于控制从儿童叙事文本中生成问题-答案对的少样本提示策略，旨在控制问题的明确性和潜在叙事元素，通过与参考模型并行进行实证评估，结果显示在语义接近度评估和问题-答案对的多样性和连贯性方面，少样本策略超越了参考模型。 |
| [^10] | [FPT: Feature Prompt Tuning for Few-shot Readability Assessment](https://arxiv.org/abs/2404.02772) | FPT提出了一种新颖的基于提示的调整框架，通过将语言特征嵌入训练软提示并设计新的损失函数，在少样本设置下改善了可读性评估任务的性能。 |
| [^11] | [AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs](https://arxiv.org/abs/2404.02761) | 提出了AQuA，一种综合的磋商质量得分计算方法，可以从多个指标中提取各个讨论帖子的统一得分，保留了评论中磋商方面的信息，提高了模型的透明度。 |
| [^12] | [Automatic Prompt Selection for Large Language Models](https://arxiv.org/abs/2404.02717) | 通过在训练数据上进行聚类，使用基于LLM的提示生成器为每个簇生成候选提示，综合数据集进行训练以评估提示的相关性，最终在测试时使用评估器为新输入选择最佳提示，实现了大型语言模型的自动提示选择。 |
| [^13] | [ART: The Alternating Reading Task Corpus for Speech Entrainment and Imitation](https://arxiv.org/abs/2404.02710) | 该论文介绍了交替阅读任务（ART）语料库，该语料库包含了三种实验条件和三个子语料库，可以在受控和不那么自发的环境中系统地研究语音对位行为。 |
| [^14] | [Scalable Model Editing via Customized Expert Networks](https://arxiv.org/abs/2404.02699) | 通过引入SCEN方法，使用定制的专家网络实现了可扩展的模型编辑，解决了大型语言模型中的幻觉和过时知识问题，并在实验证实中取得了最先进的结果。 |
| [^15] | [Attention is Naturally Sparse with Gaussian Distributed Input](https://arxiv.org/abs/2404.02690) | 通过对高斯输入下注意力得分稀疏性进行理论分析，揭示了注意力机制中稀疏性的特征及其对计算效率的影响。 |
| [^16] | [Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers](https://arxiv.org/abs/2404.02684) | 提出了一种跨架构迁移学习方法，用于在线性成本推断和自注意力变换器之间共享组件的权重，以提高Transformer语言模型的效率。 |
| [^17] | [PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets](https://arxiv.org/abs/2404.02681) | 通过澄清词语含义来改善厌恶检测，PejorativITy提出了一个新的意大利推文语料库，揭示了将贬损信息注入模型的两种方法均能显著提高分类性能。 |
| [^18] | [The VoicePrivacy 2024 Challenge Evaluation Plan](https://arxiv.org/abs/2404.02677) | 该挑战旨在开发一种声音匿名化系统，用于隐藏说话者的声音身份，同时保护语言内容和情感状态，并通过组织者提供的数据集和评估脚本进行评估。 |
| [^19] | [Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2404.02657) | 本研究重新思考了大型语言模型知识蒸馏中对Kullback-Leibler散度的应用，发现逆Kullback-Leibler和正向Kullback-Leibler散度在优化目标上相似，为此提出了一种自适应Kullback-Leiber散度方法。 |
| [^20] | [Calibrating the Confidence of Large Language Models by Eliciting Fidelity](https://arxiv.org/abs/2404.02655) | 本文通过将语言模型的置信度分解为问题的不确定性和对答案的忠实性，提出了一种估计语言模型置信度的即插即用方法，经实验证明具有良好的校准性能。 |
| [^21] | [Towards detecting unanticipated bias in Large Language Models](https://arxiv.org/abs/2404.02650) | 本论文探索了在大型语言模型中检测未预料到的偏见的新途径，着重于不确定性量化和可解释人工智能方法。 |
| [^22] | [A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference](https://arxiv.org/abs/2404.02625) | Diff-Comb Explainer是一种基于可微黑盒组合求解器的神经符号架构，不需要对语义约束进行连续放松，相比传统解决方案表现更出色。 |
| [^23] | [Estimating the Causal Effects of Natural Logic Features in Transformer-Based NLI Models](https://arxiv.org/abs/2404.02622) | 通过构建明确的因果图，研究基于Transformer的NLI模型中特定自然逻辑推理模式的因果效应估计策略，以识别和量化系统性推理失败。 |
| [^24] | [Adjusting Interpretable Dimensions in Embedding Space with Human Judgments](https://arxiv.org/abs/2404.02619) | 通过人类评分指导种子词向量，我们在预测对象属性和风格属性时获得了具有显着更好性能的可解释维度。 |
| [^25] | [Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation](https://arxiv.org/abs/2404.02616) | 通过混合结构化摘要和基于LLM的数据增强方法，改进了主题相关性模型，使其能够更好地学习查询与文档之间的相关度。 |
| [^26] | [Leveraging the Interplay Between Syntactic and Acoustic Cues for Optimizing Korean TTS Pause Formation](https://arxiv.org/abs/2404.02592) | 提出了一个新框架，利用综合建模句法和声学线索，能够在培训时显示短音频剪辑的情况下，为韩语生成自然语音，解决了停顿错误问题 |
| [^27] | [Affective-NLI: Towards Accurate and Interpretable Personality Recognition in Conversation](https://arxiv.org/abs/2404.02589) | 提出了Affective-NLI用于准确且可解释的对话中人格识别，以利用对话内容中的情感因素进行准确的人格识别 |
| [^28] | [Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages](https://arxiv.org/abs/2404.02588) | 通过利用大型语言模型并对其进行微调，我们提出了一种新的方法将口语理解系统扩展到新语言，改进了多语言口语理解数据集的性能。 |
| [^29] | [Multi-Granularity Guided Fusion-in-Decoder](https://arxiv.org/abs/2404.02581) | 提出了多粒度引导的解码器融合（MGFiD），通过跨多个粒度辨别证据，并结合段落重新排序和句子分类，提高开放域问答中解码效率。 |
| [^30] | [Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models](https://arxiv.org/abs/2404.02575) | 这项研究提出了Think-and-Execute框架，将算法推理任务中的思考和执行精细分解，从而改进了大型语言模型的算法推理能力。 |
| [^31] | [MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual Textual Relatedness](https://arxiv.org/abs/2404.02570) | 本文研究了在跨语言文本相关性中选择源语言的策略，尤其关注不同的预训练语言模型下的单源转移、多源转移和基于机器翻译的数据增强，最终在C8（Kinyarwanda）测试集中取得了第一名。 |
| [^32] | [CSEPrompts: A Benchmark of Introductory Computer Science Prompts](https://arxiv.org/abs/2404.02540) | CSEPrompts是一个包含数百个编程练习提示的框架，旨在帮助理解计算机科学教育中公开可用的大型语言模型的潜在影响。 |
| [^33] | [ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model](https://arxiv.org/abs/2404.02534) | 本文介绍了四个专门针对安哥拉语言进行微调的PLM，并使用多语言自适应微调（MAFT）方法，通过采用知情嵌入初始化和合成数据，提高了MAFT模型在下游任务中的性能，将基线提高了12.3个百分点，超越了SOTA AfroXLMR-base和OFA。 |
| [^34] | [Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game](https://arxiv.org/abs/2404.02532) | 通过多智能体攻击者-伪装者博弈方法，实现一种弱防御机制，使大型模型能够安全地回复攻击者并隐藏防御意图 |
| [^35] | [A School Student Essay Corpus for Analyzing Interactions of Argumentative Structure and Quality](https://arxiv.org/abs/2404.02529) | 提出了一个德语语料库，包含来自两个年龄组学生的1,320篇论文，每篇论文都经过人工注释，旨在分析论证结构和质量之间的互动。 |
| [^36] | [Towards Large Language Model driven Reference-less Translation Evaluation for English and Indian Languages](https://arxiv.org/abs/2404.02512) | 该研究旨在评估大型语言模型在无参考翻译评估中的有效性，通过零样本学习、示例驱动学习和微调来模拟人类直接评估，发现LLM-based评估器在印度语言对中获得了与人类判断相当或更高的整体相关性。 |
| [^37] | [Lifelong Event Detection with Embedding Space Separation and Compaction](https://arxiv.org/abs/2404.02507) | 通过嵌入空间分离和压缩，新方法减轻了先前学习任务的遗忘，并缓解了过拟合问题 |
| [^38] | [Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation](https://arxiv.org/abs/2404.02505) | 通过动态演示检索和认知理解，我们提出了一种新颖的方法来提高情绪支持对话中提供的支持质量。 |
| [^39] | [Measuring Social Norms of Large Language Models](https://arxiv.org/abs/2404.02491) | 论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。 |
| [^40] | [Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment](https://arxiv.org/abs/2404.02490) | 通过引入新颖框架，本文解决了当前模型中低资源语言的跨语言词表示与高资源语言不匹配的问题，实现了对低资源语言的句子嵌入的显著改进。 |
| [^41] | [DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation](https://arxiv.org/abs/2404.02489) | 提出了一种名为DUQGen的方法，利用自动生成的有效和多样化的合成训练数据来进行现代神经排序器的新领域微调 |
| [^42] | [uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?](https://arxiv.org/abs/2404.02474) | 通过研究提示工程方法如何增强LLMs在横向思考任务上的表现，揭示了其固有的超越思维能力，并发现压缩的信息性提示和动态的情境学习显著提升了模型性能。 |
| [^43] | [Prompting for Numerical Sequences: A Case Study on Market Comment Generation](https://arxiv.org/abs/2404.02466) | 本研究探讨了针对市场评论生成任务的不同输入表示方法，发现类似编程语言的提示效果更好，而类似自然语言和较长格式的提示效果较差。 |
| [^44] | [PhonologyBench: Evaluating Phonological Skills of Large Language Models](https://arxiv.org/abs/2404.02456) | PhonologyBench是一个新颖的基准测试，旨在明确评估大型语言模型在英语中的音韵技能，展示了LLMs在没有语音数据情况下在PhonologyBench任务上表现出显著性能。 |
| [^45] | [Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations](https://arxiv.org/abs/2404.02452) | 本论文引入了一种新颖的概念，即通过在目标语言中插入一次性上下文演示，从而成功利用目标语言示例来改进评估的mT5模型的跨语言能力。 |
| [^46] | [The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education](https://arxiv.org/abs/2404.02444) | 本研究利用NLP技术评估教育中多种高推理教学实践的质量，并且首次应用NLP来衡量对有特殊需求学生特别有效的教学实践。 |
| [^47] | [From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives](https://arxiv.org/abs/2404.02438) | 该论文提出了一种利用最先进的NLP技术从口述验尸文本中预测死因并进行有效推断的方法。 |
| [^48] | [On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons](https://arxiv.org/abs/2404.02431) | 该研究分析了基于解码器的预训练语言模型在多语言处理中的神经元级内部行为，发现模型中存在特定于每种语言的神经元，干扰这些语言特定的神经元会显著改变文本生成中目标语言出现的概率。 |
| [^49] | [Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data](https://arxiv.org/abs/2404.02422) | 提出了一种策略，通过PEFT和合成数据增强低资源LLMs分类器，实现了与0-shot文本分类器相媲美或更好的准确性。 |
| [^50] | [Revisiting subword tokenization: A case study on affixal negation in large language models](https://arxiv.org/abs/2404.02421) | 研究衡量了前缀否定对大型语言模型的影响，发现尽管存在一些不匹配，模型整体上能够可靠地识别前缀否定的含义。 |
| [^51] | [Auxiliary task demands mask the capabilities of smaller language models](https://arxiv.org/abs/2404.02418) | 较小语言模型对类比推理、反思推理、单词预测和语法判断的表现受辅助任务需求的影响，评估方法的任务需求越大，性能越低，这种"需求差距"在参数较少、训练数据较少的模型中尤为显著 |
| [^52] | [CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models](https://arxiv.org/abs/2404.02408) | CMULAB 是一个开源框架，简化了自然语言处理模型的部署和持续人机协作微调，使用户能够快速对语音识别、OCR、翻译和句法分析等工具进行适应和扩展至新语言，即使训练数据有限 |
| [^53] | [Exploring Backdoor Vulnerabilities of Chat Models](https://arxiv.org/abs/2404.02406) | 聊天模型因为多轮交互格式的灵活性增加了对后门攻击的脆弱性，该论文揭示并实现了一种新颖的后门攻击方法 |
| [^54] | [Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT](https://arxiv.org/abs/2404.02403) | 评估了波斯语大型语言模型在不同任务上的表现，引入了推理任务方面的新基准测试，发现LLMs在推理任务中表现优异。 |
| [^55] | [Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM](https://arxiv.org/abs/2404.02402) | Token Trails是一种利用token-type嵌入导航对话中复杂上下文细微差别的新方法，通过提高对话理解和回复生成效果，在促进上下文意识聊天机器人交互方面具有前沿性能。 |
| [^56] | [Backdoor Attack on Multilingual Machine Translation](https://arxiv.org/abs/2404.02393) | 多语言机器翻译系统容易受到后门攻击，攻击者通过向低资源语言对注入有毒数据来造成恶意翻译，引起高资源语言受攻击，揭示注入少量有毒数据即可实现成功攻击高资源语言对的风险。 |
| [^57] | [Low-resource neural machine translation with morphological modeling](https://arxiv.org/abs/2404.02392) | 该论文提出了一种在低资源环境中建模复杂形态的神经机器翻译方法，通过两级变压器架构编码形态信息，并利用多任务多标签训练方案和基于beam search的解码器来提高翻译性能。 |
| [^58] | [On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL](https://arxiv.org/abs/2404.02389) | 本研究调查了编码器-解码器语言模型中线性处理结构化数据的方法，发现模型能够模仿人类设计的流程，学习结构的深刻含义，揭示了模型内部机制的一些见解。 |
| [^59] | [Optical Text Recognition in Nepali and Bengali: A Transformer-based Approach](https://arxiv.org/abs/2404.02375) | 该论文介绍了使用编码器-解码器transformer模型进行尼泊尔语和孟加拉语的文本识别研究，高精度识别两种语言文字，有望推动语言学的先进和易访问性研究 |
| [^60] | [Obfuscated Malware Detection: Investigating Real-world Scenarios through Memory Analysis](https://arxiv.org/abs/2404.02372) | 通过内存分析，利用机器学习算法提出了一种简单且成本效益的模糊恶意软件检测系统，重点评估其在真实场景中的效果。 |
| [^61] | [Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns](https://arxiv.org/abs/2404.02370) | 通过结合眼动数据和文本提示，利用Vision-Language Models（VLMs）增强胸部X射线分析中的人机交互，提高了放射科医师的关注度，有效增强了胸部X射线分析的准确性。 |
| [^62] | [Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors](https://arxiv.org/abs/2404.02356) | 提出了一种嵌套专家集成防御框架(NPoE)，可同时防御多种后门触发器类型，实验结果表明其在多个任务上的有效性。 |
| [^63] | [A Computational Analysis of Lyric Similarity Perception](https://arxiv.org/abs/2404.02342) | 该研究通过比较分析计算方法对模拟歌词相似度与人类感知的关联，发现基于BERT模型嵌入、歌词音频和音素组件相似性的计算模型对感知上的歌词相似度具有指示作用。 |
| [^64] | [Corpus Considerations for Annotator Modeling and Scaling](https://arxiv.org/abs/2404.02340) | 在多样化表示技巧的注解器建模领域，对数据集的细粒度特征进行研究是至关重要的。 |
| [^65] | [Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation](https://arxiv.org/abs/2404.02335) | 提出了一种新颖的方法，使用一个核心模型和多套领域特定参数，结合提示调整和适配器技术，以及额外层次来实现低资源多领域适应，使得模型能够完成与每个领域单独训练模型相媲美的任务。 |
| [^66] | [Comparative Study of Domain Driven Terms Extraction Using Large Language Models](https://arxiv.org/abs/2404.02330) | 本研究比较了使用大型语言模型进行领域驱动术语提取的方法，评估了Llama2-7B、GPT-3.5和Falcon-7B在Inspec和PubMed数据集上的性能。 |
| [^67] | [Toward Informal Language Processing: Knowledge of Slang in Large Language Models](https://arxiv.org/abs/2404.02323) | 大型语言模型通过建立支持多任务评估的俚语数据集，有效实现了俚语检测和识别地区与历史俚语来源，并通过探索LLMs输出分布提供了解释性洞见。 |
| [^68] | [Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization](https://arxiv.org/abs/2404.02319) | 提出了SAMMO框架，用于在编译时优化元提示程序，提高了复杂提示在多种不同LLM上的性能。 |
| [^69] | [Collapse of Self-trained Language Models](https://arxiv.org/abs/2404.02305) | 自我训练的语言模型在延长训练时表现出显著的性能下降，导致重复和崩溃的标记输出。 |
| [^70] | [Extracting Norms from Contracts Via ChatGPT: Opportunities and Challenges](https://arxiv.org/abs/2404.02269) | 研究了ChatGPT在从合同中提取规范方面的有效性和局限性，展示了其良好表现但也发现了一些限制。 |
| [^71] | [LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages](https://arxiv.org/abs/2404.02261) | 在低资源语言中，通过将LLMs集成到主动学习循环中进行数据注释，有效减少所需数据量，并取得接近最先进性能的结果。 |
| [^72] | [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/abs/2404.02258) | 本研究提出了一种新的方法，即Mixture-of-Depths，可以在Transformer的语言模型中动态分配FLOPs以优化模型深度上不同层的序列分配。 |
| [^73] | [$\texttt{LM}^\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning](https://arxiv.org/abs/2404.02255) | LM2提出了一种简单的语言模型$\texttt{LM}^\texttt{2}$，该模型将分解、解决和验证模块化，通过分解器识别关键概念并生成逐步子问题，从而协同解决复杂推理问题。 |
| [^74] | [Emergent Abilities in Reduced-Scale Generative Language Models](https://arxiv.org/abs/2404.02204) | 减小规模数据训练的较小语言模型展示了增强的零样本能力，可在简化语言中实现与大型模型相当的性能。 |
| [^75] | [Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization](https://arxiv.org/abs/2404.02183) | 提出了自组织多代理框架（SoA），实现了大规模代码的可扩展高效生成和优化，代理可自主运作生成和修改代码组件，并根据问题复杂性动态增加数量。 |
| [^76] | [Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces](https://arxiv.org/abs/2404.02013) | 在该研究中，我们开发了一种结合了CNN和BiLSTM网络的集成方法，有效模拟了文本数据中的语义和顺序模式，用于检测和缓解印地语、泰米尔语和印度英语在线空间中的性别虐待。 |
| [^77] | [Octopus v2: On-device language model for super agent](https://arxiv.org/abs/2404.01744) | 该研究提出了一种新方法，通过将具有20亿参数的设备上模型赋予超越GPT-4的准确性和延迟性能，并将上下文长度缩减95％，从而解决了函数调用中的延迟和准确性问题。 |
| [^78] | [Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation](https://arxiv.org/abs/2404.01677) | 通过引入归结反驳范式，提出了一个名为GFaiR的框架，旨在解决大型语言模型在进行自然语言形式逻辑理论一阶逻辑推理时的困难，并通过证明插入解决方案改进了系统的完整性 |
| [^79] | [ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback](https://arxiv.org/abs/2404.00934) | ChatGLM-RLHF是一种从人类反馈中强化学习系统，通过收集人类偏好数据、训练奖励模型和优化策略等方法来增强大型语言模型ChatGLM与人类偏好的对齐性，在实验中显示出显著的改进。 |
| [^80] | [Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits](https://arxiv.org/abs/2404.00267) | LLMs对作者的语言模式的影响略微降低其个人特征的预测能力，但显著变化不太频繁。 |
| [^81] | [Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning](https://arxiv.org/abs/2404.00213) | 本论文研究了在大型语言模型中通过监督微调方法注入新知识的效果，特别关注了最近体育事件领域。 |
| [^82] | [IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context](https://arxiv.org/abs/2403.20147) | IndiBias是一个为评估印度语境中社会偏见而设计的综合基准数据集，通过过滤和翻译现有数据集以及利用不同LLMs的方法，涵盖了印度中流行的各种社会偏见维度。 |
| [^83] | [Empirical Analysis for Unsupervised Universal Dependency Parse Tree Aggregation](https://arxiv.org/abs/2403.19183) | 通过广泛的实证研究比较不同的无监督后处理聚合方法，以确定最适合的依存树结构聚合方法 |
| [^84] | [Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective](https://arxiv.org/abs/2403.18346) | 提出了一个因果框架用于解释多模态大型语言模型在视觉问答问题中的偏差，并引入了一个新的挑战性数据集MORE，同时提出两种减轻单模态偏差的策略。 |
| [^85] | [Encoding of lexical tone in self-supervised models of spoken language](https://arxiv.org/abs/2403.16865) | 本文研究分析了口语语言自监督模型对声调的编码能力，使用普通话和越南语作为案例研究，并发现SLMs在训练于非音调语言数据时也保持着显著的词汇音调编码能力。 |
| [^86] | [MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness](https://arxiv.org/abs/2403.14990) | MasonTigers在SemEval-2024 Task 1中采用集成方法，结合语言特定的BERT模型和句子变换器，在处理语义文本相关性时取得了优异的结果。 |
| [^87] | [MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts](https://arxiv.org/abs/2403.14982) | 这项研究使用大型语言模型解决SemEval-2024 Task 9的谜题任务，通过一系列思维链提示技术，包括零参考和少量参考提示，以及思维链提示，最终取得了显著的结果，展示了逐步解释性提示如何可以更好地揭示已编码的知识。 |
| [^88] | [EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling](https://arxiv.org/abs/2403.14541) | 通过提出基于熵的动态温度采样方法，本文在大语言模型的生成中实现了更平衡的性能表现，并在四个不同生成基准上展示了显著优于现有策略的结果。 |
| [^89] | [StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows](https://arxiv.org/abs/2403.11322) | 提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。 |
| [^90] | [Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization](https://arxiv.org/abs/2403.08730) | 提出了引导优先优化（BPO）策略，通过使用包含负面响应的数据集来减轻多模态大语言模型（MLLMs）对预训练语料库偏见的影响，并使用两种策略来促进模型在视觉输入中的表现。 |
| [^91] | [MolBind: Multimodal Alignment of Language, Molecules, and Proteins](https://arxiv.org/abs/2403.08167) | MolBind 提出了一个框架，通过对比学习为多种模态训练编码器，将所有模态映射到共享特征空间，实现多模态语义对齐。 |
| [^92] | [Breeze-7B Technical Report](https://arxiv.org/abs/2403.02712) | Breeze-7B是基于Mistral-7B的开源语言模型，旨在改善中文语境下的语言理解和聊天机器人功能，展现出在复杂度类别中的出色性能。 |
| [^93] | [Privacy-Aware Semantic Cache for Large Language Models](https://arxiv.org/abs/2403.02694) | MeanCache是一种面向LLMs的语义缓存，能够识别语义上相似的查询，从而减少查询成本，服务提供商负载和环境影响。 |
| [^94] | [MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning](https://arxiv.org/abs/2402.17231) | MATHSENSEI 是一种用于数学推理的工具增强型大型语言模型，通过添加知识检索、程序执行和符号方程求解工具，提高了数学推理能力。 |
| [^95] | [OSCaR: Object State Captioning and State Change Representation](https://arxiv.org/abs/2402.17128) | 本文介绍了一个新的数据集和基准OSCaR，旨在解决描述复杂视觉环境中对象状态变化的问题，为评估多模态大型语言提供了一个新的实验平台。 |
| [^96] | [Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries](https://arxiv.org/abs/2402.13043) | 使用文本摘要提高对话检索的有效性和效率，通过对话摘要生成器进行查询和关键词生成，进一步提炼轻量级对话编码器以避免额外推理成本 |
| [^97] | [Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!](https://arxiv.org/abs/2402.12343) | 安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。 |
| [^98] | [Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence](https://arxiv.org/abs/2402.10175) | 本论文引入了一种新的自动评估指标PDD，用于评估长篇文章之间的话语连贯性，实验证明该指标更接近人类偏好和GPT-4的评估结果。 |
| [^99] | [CodeMind: A Framework to Challenge Large Language Models for Code Reasoning](https://arxiv.org/abs/2402.09664) | CodeMind是一个用于挑战大型语言模型进行代码推理的框架，通过评估LLMs的代码推理能力来替代仅仅依靠测试通过来评估，对三种代码推理任务进行评估，结果显示LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。 |
| [^100] | [Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens](https://arxiv.org/abs/2401.17377) | 这项研究展示了n-gram语言模型的价值，并介绍了一个名为infini-gram的引擎，它可以以毫秒级的延迟计算任意n的n-gram概率，使得在神经大型语言模型中对文本进行更准确的分析成为可能。 |
| [^101] | ["You tell me": A Dataset of GPT-4-Based Behaviour Change Support Conversations](https://arxiv.org/abs/2401.16167) | 该研究分享了一份数据集，其中包含关于行为变革的基于文本的用户互动数据，为研究人员提供了宝贵的见解，以便设计基于真实互动的对话系统 |
| [^102] | [Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models](https://arxiv.org/abs/2312.14346) | 本文通过定义标记级别的方法来识别不同类型的幻觉，并利用这种标记来提高LLMs在对话摘要任务中的可解释性和忠实度。 |
| [^103] | [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805) | Gemini家族是一系列在图像、音频、视频和文本理解方面表现出色的多模态模型，其中最具能力的Gemini Ultra模型在30个基准测试中推进了技术前沿，并改进了所有20个多模态基准测试的技术状态。 |
| [^104] | [SCTc-TE: A Comprehensive Formulation and Benchmark for Temporal Event Forecasting](https://arxiv.org/abs/2312.01052) | SCTc-TE提出了一种全面的时间事件预测形式化方法，并通过构建MidEast-TE数据集和区分各种上下文信息提升了预测方法。 |
| [^105] | [Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts](https://arxiv.org/abs/2312.00968) | Omni-SMoLA提出了使用软MoE方法混合多个多模态低秩专家的架构，避免引入大量新参数，以提升通用多模态模型的性能。 |
| [^106] | [Agent-OM: Leveraging LLM Agents for Ontology Matching](https://arxiv.org/abs/2312.00326) | 本研究提出了Agent-OM，利用LLM代理为本体匹配系统引入了新的设计范式。 |
| [^107] | [Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness](https://arxiv.org/abs/2311.09694) | NLP模型的增大和性能提升并不能解决其鲁棒性问题，当前的方法和评估仍存在重大缺陷。 |
| [^108] | [Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning](https://arxiv.org/abs/2311.09619) | 本文通过引入一种新的标记方法，增量效用，来估计LLMs通过演示带入的增量知识量，解决了关于不同标记策略如何影响目标任务结果的问题。 |
| [^109] | [Pregnant Questions: The Importance of Pragmatic Awareness in Maternal Health Question Answering](https://arxiv.org/abs/2311.09542) | 通过研究孕妇在问怀孕和婴儿护理问题时所做的假设和推断，将这些实用推断告知现有的问答管道可以产生更完整的回答，减少有害信念的传播。 |
| [^110] | [Effective Large Language Model Adaptation for Improved Grounding and Citation Generation](https://arxiv.org/abs/2311.09533) | 本文提出了一个新的框架 AGREE，通过将大型语言模型的响应联系到检索到的段落并提供引文来提升信息关联，从而解决大型语言模型可能生成“臆想”答案的问题 |
| [^111] | [AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation](https://arxiv.org/abs/2311.09521) | AMRFact是一个框架，利用AMR生成负样本，增强了摘要事实性评估，生成的连贯且事实不一致的摘要具有高错误率。 |
| [^112] | [Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation](https://arxiv.org/abs/2311.09467) | 提出了一种名为TWEAK的仅解码方法，通过引入假设验证模型来提高知识到文本生成的忠实度，并在不影响质量的情况下取得改进。 |
| [^113] | [Grounding Gaps in Language Model Generations](https://arxiv.org/abs/2311.09144) | 该论文研究了大型语言模型在生成文本时是否具有人类对话基础的表现，通过量化尝试基础设定的一系列指标比较了模型与人类的生成结果。 |
| [^114] | [AI-generated text boundary detection with RoFT](https://arxiv.org/abs/2311.08349) | 使用RoFT进行人工智能生成文本边界检测的研究揭示了基于困惑度的方法在跨领域和跨模型设置中更加稳健。 |
| [^115] | [Anti-LM Decoding for Zero-shot In-context Machine Translation](https://arxiv.org/abs/2311.08324) | 提出了一种反-LM解码目标，通过引入Anti-Language Model目标和一个设计良好的衰减因子，解决了零翻译上下文机器翻译的弱点，与其他解码目标相比，在某些设置中，实现了高达20个BLEU点的性能提升。 |
| [^116] | [Psychometric Predictive Power of Large Language Models](https://arxiv.org/abs/2311.07484) | 指令调整和提示在大型语言模型中无法提供比基础模型更好的认知建模估计。 |
| [^117] | [MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks](https://arxiv.org/abs/2311.07463) | 该研究对多语言数据集上的最新LLM进行了全面评估，结果显示较大型的模型如GPT-4、Gemini-Pro和PaLM2在各种任务上表现优越。 |
| [^118] | [On the steerability of large language models toward data-driven personas](https://arxiv.org/abs/2311.04978) | 本文提出了一种能够通过大型语言模型实现特定观点可控生成的新方法，超越传统的人口统计数据，引入基于协同过滤的数据驱动人物概念，使得可以更细致地理解不同社会群体的多样性。 |
| [^119] | [Interpretable-by-Design Text Understanding with Iteratively Generated Concept Bottleneck](https://arxiv.org/abs/2310.19660) | 提出了一种文本瓶颈模型（TBM），通过预测一组突出概念的分类值来实现文本分类，从而在高风险领域提供全局和局部解释 |
| [^120] | [Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications](https://arxiv.org/abs/2310.14607) | LLMs在表格分类任务中存在社会偏见，影响了它们的公平性。 |
| [^121] | [UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking](https://arxiv.org/abs/2310.10492) | 通过联合和自训练方法，UNO-DST利用未标注数据，将零样本对话状态跟踪转变为少样本DST，并在未知目标域中生成和选择高质量样本，最终提高了DST模型的训练和微调效果。 |
| [^122] | [Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena](https://arxiv.org/abs/2310.05746) | LLM代理在拍卖竞技场展示出了关键的规划和执行技能，这为建模复杂社会互动在竞争背景下的LLMs潜力提供了新途径。 |
| [^123] | [Utilizing Contextual Clues and Role Correlations for Enhancing Document-level Event Argument Extraction](https://arxiv.org/abs/2310.05116) | 本文提出了CARLG模型，通过利用上下文线索和角色相关性，提升了文档级事件论证提取的性能。 |
| [^124] | [Automatic Pair Construction for Contrastive Post-training](https://arxiv.org/abs/2310.02263) | 提出了一种自动构建对比数据的方法，使用多个模型的偏好对，提高了大型语言模型的对齐效果，并且通过DPO对比技术得到了改善，进一步优化了对齐，最终使经过调优的指导学习模型Orca超越了ChatGPT。 |
| [^125] | [Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?](https://arxiv.org/abs/2308.10168) | 本文构建了Head-to-Tail基准，通过对18K个头部、躯干和尾部事实的问答对进行全面评估，揭示了现有大型语言模型在掌握事实知识方面尤其是对躯干到尾部实体的事实仍然远未完美。 |
| [^126] | [Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References](https://arxiv.org/abs/2305.15067) | 通过增加参考文献的多样性，本文提出的方法Div-Ref 显著提高了自然语言生成评估的相关性 |
| [^127] | [From Shortcuts to Triggers: Backdoor Defense with Denoised PoE](https://arxiv.org/abs/2305.14910) | 提出了一种端到端集成的后门防御框架 DPoE，旨在通过去噪设计和捕捉后门快捷方式的浅层模型，以及防止学习后门快捷方式的主模型，有效抵御各种后门攻击。 |
| [^128] | [Catalytic Role Of Noise And Necessity Of Inductive Biases In The Emergence Of Compositional Communication](https://arxiv.org/abs/2111.06464) | 噪声和归纳偏见的作用使得组合式沟通自发产生，并且一定范围内的噪声有助于促进组合性的发展。 |
| [^129] | [Combining Transformers with Natural Language Explanations](https://arxiv.org/abs/2110.00125) | 通过结合自然语言解释和变压器模型，我们提出了一种方法来利用外部记忆存储自然语言解释，并用于解释分类输出，实验证明这种方法能够产生相关解释且保持或提高分类性能。 |
| [^130] | [Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models.](http://arxiv.org/abs/2401.15269) | 本论文介绍了一种名为Self-BioRAG的框架，通过使用检索和自我反思的方法，提高了医疗推理的能力。该框架专注于生成解释、检索领域特定文档以及对生成的响应进行自我反思。 |
| [^131] | [MambaByte: Token-free Selective State Space Model.](http://arxiv.org/abs/2401.13660) | MambaByte是一种无标记的选择性状态空间模型，通过在字节级别上进行自回归训练，解决了标准自回归Transformer在处理长序列时的性能问题，并展现了与最先进的子词Transformer相媲美甚至更优的性能，从而证明了MambaByte在无标记语言建模方面的有效性。 |
| [^132] | [MaLA-500: Massive Language Adaptation of Large Language Models.](http://arxiv.org/abs/2401.13303) | MaLA-500是一种大型语言模型，设计用于覆盖534种语言，并通过在LLaMA 2上进行训练来提高效果。 |
| [^133] | [MLLMReID: Multimodal Large Language Model-based Person Re-identification.](http://arxiv.org/abs/2401.13201) | MLLMReID是一种基于多模态大语言模型的人物再识别方法，通过微调模型并将其视觉编码器作为主干进行优化，解决了MLLM在ReID任务中的设计指令和特征学习效果的问题。 |
| [^134] | [Instructional Fingerprinting of Large Language Models.](http://arxiv.org/abs/2401.12255) | 这项研究提出了一种指纹识别大型语言模型的方法，通过轻量级的指令调整，保护知识产权并确保遵守许可条款。实验证明这种方法不影响模型的正常行为，并且具有鲁棒性和高效训练的特点。 |
| [^135] | [FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?.](http://arxiv.org/abs/2401.11033) | 这项工作介绍了一个将FAIR数据原则嵌入到大型语言模型训练中的框架，并提供了整合指南和检查清单。通过一个案例研究，展示了在符合FAIR标准的数据集中识别和减轻偏见的实际应用。 |
| [^136] | [Hallucination Benchmark in Medical Visual Question Answering.](http://arxiv.org/abs/2401.05827) | 这项研究创建了医学图像的幻觉基准评估，并全面评估了当前最先进的模型，揭示了幻觉现象在临床环境中的限制和各种提示策略的有效性。 |
| [^137] | [Divide and Conquer for Large Language Models Reasoning.](http://arxiv.org/abs/2401.05190) | 分治求解方法应用于大型语言模型的推理中，通过根据统计置信度分数将问题划分为不同的子集，并采用基于先验知识和筛选选项的推理方法，提高了推理性能，取得了优异结果。 |
| [^138] | [Structured Packing in LLM Training Improves Long Context Utilization.](http://arxiv.org/abs/2312.17296) | 本论文研究了长上下文大型语言模型（LLM）中上下文利用不足的问题，并通过将相关文档纳入训练示例中来改进模型的困惑度。通过引入Structured Packing for Long Context (SPLiCe)方法，使用检索方法将最互相关文档汇集到单个训练上下文中，进一步提高了模型的性能。 |
| [^139] | [Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization.](http://arxiv.org/abs/2311.01544) | 本研究引入了一种新的方法，即不同的令牌指标（DTM），用于评估压缩后的大型语言模型（LLM）。通过关注令牌的差异性，DTM提供了对模型压缩微妙之处的深入洞察，并且在不损害文本生成质量的情况下可以实现显著的精确度和稀疏度水平。该研究还提出了一种利用DTM进行模型稀疏化和量化的方法，并发现可以修剪掉超过90%的LLM组件和量化超过80%的参数。 |
| [^140] | ["One-size-fits-all"? Observations and Expectations of NLG Systems Across Identity-Related Language Features.](http://arxiv.org/abs/2310.15398) | 通过扰动不同类型的身份相关语言特征，研究探索了NLG系统行为的公平中的适应性和不变性之间的紧张关系。研究结果发现，适应的动机包括社会规范、文化差异、特定特征信息和适应性；不变性的动机包括支持规定主义的观点、将适应视为不必要过程，并对错误假设持谨慎态度。这些发现突显了定义公平相关NLG系统行为的挑战。 |
| [^141] | [Evaluating Multi-Agent Coordination Abilities in Large Language Models.](http://arxiv.org/abs/2310.03903) | 本研究构建了使用大型语言模型（LLMs）的智能体，并评估其在多智能体协调中的有效性。我们引入了LLM-Co框架，用于在三个游戏环境中评估LLMs的协调能力。评估结果显示LLMs具有推断伙伴意图和理解其行动的能力。 |
| [^142] | [DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers.](http://arxiv.org/abs/2310.03686) | DecoderLens是一种用于解释编码器-解码器Transformer模型中内部状态的新方法。通过让解码器跨越中间编码器层的表示进行交叉注意，DecoderLens将先前无法解释的向量表示映射到可解释的单词或符号序列，揭示了模型在低层或中间层解决的特定子任务，为信息流提供了新的洞察。 |
| [^143] | [UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network.](http://arxiv.org/abs/2310.02973) | 这项研究提出了一种单一的多任务学习模型"UniverSLU"，通过利用预训练的自动语音识别模型和不同的任务和数据集指定器作为离散提示，成功地在多个口语理解任务上取得了有竞争力的性能，并且甚至超过了特定任务的模型。 |
| [^144] | [Nested Event Extraction upon Pivot Element Recogniton.](http://arxiv.org/abs/2309.12960) | 本文提出了一种名为PerNee的新模型，通过识别中心元素来提取嵌套事件。该模型解决了现有NEE方法无法处理中心元素双重身份的问题，并通过提示学习将事件类型和参数角色的信息纳入其中，以提高NEE性能。 |
| [^145] | [Leveraging Contextual Information for Effective Entity Salience Detection.](http://arxiv.org/abs/2309.07990) | 本文研究了有效的实体重要性检测方法，通过对中等规模的语言模型进行微调，比传统的特征工程方法获得了更好的性能。研究还发现使用指令调校的语言模型进行零样本提示效果较差。 |
| [^146] | [CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset.](http://arxiv.org/abs/2308.16705) | CReHate通过跨文化重新注释英语仇恨言论数据集，揭示了来自不同国家的个体对仇恨言论的不同看法，并引入了一种具有文化敏感性的分类器。这些发现强调了重新评估NLP研究在仇恨言论领域的必要性。 |
| [^147] | [Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection.](http://arxiv.org/abs/2307.16888) | 这项研究介绍了一种针对指令调整的大型语言模型的新型后门攻击方法，即虚拟提示注入（VPI）。通过在特定触发场景下将虚拟提示与用户指令连接，攻击者可以精细操纵模型的回应而无需明确注入。 |
| [^148] | [Towards Codable Text Watermarking for Large Language Models.](http://arxiv.org/abs/2307.15992) | 这项研究对于大型语言模型的可编解码文本水印技术进行了系统研究，提出了一种允许文本水印携带更多可定制化信息的方法，解决了现有水印方法编码效率低、不能满足不同应用场景需求的问题。 |
| [^149] | [CamemBERT-bio: a Tasty French Language Model Better for your Health.](http://arxiv.org/abs/2306.15550) | 本研究介绍了CamemBERT-bio，它是一种针对法语生物医学领域专门设计的语言模型，相对于通用模型在命名实体识别任务上平均提高了2.54个百分点。 |
| [^150] | [A Mechanism for Solving Relational Tasks in Transformer Language Models.](http://arxiv.org/abs/2305.16130) | 这篇论文研究了在Transformer语言模型中解决关系任务的机制，并发现这些模型利用简单的线性更新来处理关系任务，并以内容无关的方式促进关系的输出。 |
| [^151] | [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models.](http://arxiv.org/abs/2305.14710) | 使用指令调整方法在众包数据集上训练的大规模语言模型，存在后门漏洞，攻击者只需注入极少量的恶意指令便可永久控制模型行为，且难以被修复，需要更加健全的防御机制。 |

# 详细

[^1]: 解读文本的真实性: 通过大规模语言语义的广义策略来检测人类和机器生成的文本

    Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text

    [https://rss.arxiv.org/abs/2401.09407](https://rss.arxiv.org/abs/2401.09407)

    该论文通过研究在真实世界场景中检测机器生成文本的方法，发现现有方法对于不同生成器和领域产生的文本存在严重限制。

    

    随着大规模语言模型（LLM）的广泛应用，对检测机器生成文本的工具的需求日益增长。有效检测机器生成文本面临两个关键问题: 首先，他们在应对真实世界场景时面临着极大的限制，这些场景中机器生成文本是由各种生成器产生的，包括但不限于GPT-4和Dolly，并涵盖各种领域，从学术手稿到社交媒体帖子。其次，现有的检测方法将LLM生成的文本视为严格的二元分类问题，忽略了不同LLM生成的文本多样性。本研究系统地研究了在真实世界场景中检测机器生成文本的方法。我们首先研究了最先进方法的有效性，并发现它们在应对真实世界中不同生成器和领域产生的文本时受到严重的限制。

    With the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore,
    
[^2]: ALOHa：测量图像字幕模型中幻觉的新标准

    ALOHa: A New Measure for Hallucination in Captioning Models

    [https://arxiv.org/abs/2404.02904](https://arxiv.org/abs/2404.02904)

    提出了一种新的用于测量图像字幕模型中幻觉的标准ALOHa，利用大型语言模型来测量幻觉对象，并成功识别比现有指标CHAIR更多的幻觉对象。

    

    尽管在视觉描述的多模态预训练方面取得了近期的进展，但最先进的模型仍会产生包含错误的字幕，比如在场景中存在幻觉对象。现有的主要幻觉对象度量标准CHAIR，仅限于一组固定的MS COCO对象和同义词。在这项工作中，我们提出了一种现代化的开放词汇度量标准ALOHa，利用大型语言模型（LLM）来衡量对象幻觉。具体地，我们使用LLM从候选字幕中提取可连接的对象，衡量它们与字幕和对象检测中参考对象的语义相似度，并使用匈牙利匹配生成最终的幻觉得分。我们展示了ALOHa在HAT上比CHAIR在一个新的用于幻觉标记的MS COCO字幕的金标准子集上正确识别了更多的幻觉对象（多出13.6%），在nocaps上（其中对象超出了MS COCO类别）识别了更多的幻觉对象（多至30.8%）。

    arXiv:2404.02904v1 Announce Type: cross  Abstract: Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene. The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations. Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories.
    
[^3]: ChatGLM-Math:使用自我批判管道改进大型语言模型在数学问题解决中的表现

    ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline

    [https://arxiv.org/abs/2404.02893](https://arxiv.org/abs/2404.02893)

    ChatGLM-Math通过自我批判管道在大型语言模型中实现了数学问题解决能力的显著增强

    

    大型语言模型(LLMs)在掌握人类语言方面表现出色，但在需要数学问题解决的现实应用中仍然面临困难。尽管已经开发了许多增强LLMs数学能力的策略和数据集，但同时在部署的LLM系统中保持和提高语言和数学能力仍然是一个挑战。在这项工作中，我们定制了Self-Critique管道，解决了LLM对齐的反馈学习阶段的挑战。我们首先训练一个通用的Math-Critique模型，从LLM本身提供反馈信号。然后，我们依次使用拒绝微调和直接偏好优化LLM自身生成的数据收集。基于ChatGLM3-32B，我们在学术领域和我们新创建的挑战性数据集MathUserEval上进行了一系列实验。结果显示我们的管道显著增强了LLM的数学能力。

    arXiv:2404.02893v1 Announce Type: new  Abstract: Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving. While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment. We first train a general Math-Critique model from the LLM itself to provide feedback signals. Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection. Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval. Results show that our pipeline significantly enhances the LLM's mathematical pr
    
[^4]: 线性注意力序列并行化

    Linear Attention Sequence Parallelism

    [https://arxiv.org/abs/2404.02882](https://arxiv.org/abs/2404.02882)

    提出了一种名为线性注意力序列并行（LASP）的高效序列并行方法，针对线性注意力的语言模型进行了优化，通过设计高效的点对点通信机制和执行内核融合来降低通信开销，并实现硬件友好性。

    

    序列并行（SP）作为一种处理超出单个GPU内存限制的长序列的流行策略。然而，现有的SP方法并未利用线性注意力特性，导致在基于线性注意力的语言模型中并行效率和可用性不佳。在本文中，我们介绍了线性注意力序列并行（LASP），这是一种专为基于线性注意力的语言模型量身定制的高效SP方法。具体来说，我们设计了一种高效的点对点通信机制，以利用线性注意力的右乘内核技巧，从而显着降低SP的通信开销。我们还通过执行内核融合和中间状态缓存来增强LASP的实际效率，使LASP在GPU集群上的硬件友好性得到提升。此外，我们还精心确保序列级LASP与所有类型的批级数据兼容。

    arXiv:2404.02882v1 Announce Type: cross  Abstract: Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data par
    
[^5]: 最后收官：大语言模型中的参数异质性和量化

    Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models

    [https://arxiv.org/abs/2404.02837](https://arxiv.org/abs/2404.02837)

    论文揭示了在大语言模型中存在参数异质性的现象，提出了一种名为CherryQ的量化方法，该方法能够在保留关键参数的同时将其余参数高效量化至低精度，在性能方面明显优于现有方法。

    

    这篇论文揭示了大型语言模型（LLMs）中参数异质性的现象。我们发现，少量“樱桃”参数对模型性能产生了不成比例的巨大影响，而绝大多数参数的影响较小。这种异质性在不同模型系列、规模和类型中普遍存在。在这一观察的基础上，我们提出了CherryQ，一种新颖的量化方法，统一了混合精度参数的优化。CherryQ能够识别并保留高精度下关键的樱桃参数，同时将其余参数积极量化为低精度。大量实验证明了CherryQ的有效性。CherryQ在困惑度和下游任务性能方面优于现有的量化方法。值得注意的是，我们的3位量化Vicuna-1.5与它们的16位对应物相比表现出色。

    arXiv:2404.02837v1 Announce Type: new  Abstract: This paper reveals the phenomenon of parameter heterogeneity in large language models (LLMs). We find that a small subset of ``cherry'' parameters exhibit a disproportionately large influence on model performance, while the vast majority of parameters have minimal impact. This heterogeneity is found to be prevalent across different model families, scales, and types. Motivated by this observation, we propose CherryQ, a novel quantization method that unifies the optimization of mixed-precision parameters. CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision. Extensive experiments demonstrate the effectiveness of CherryQ. CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance. Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts. Th
    
[^6]: 从记忆中检索示例用于增强神经机器翻译：一个系统性比较

    Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison

    [https://arxiv.org/abs/2404.02835](https://arxiv.org/abs/2404.02835)

    本文研究了不同的检索方法对几种翻译架构的影响，发现检索技术的选择会影响翻译得分，增加示例数量和多样性通常对翻译效果有积极的影响。

    

    检索增强神经机器翻译（RAMT）架构从记忆中检索示例，以指导生成过程。尽管在这一趋势中的大多数工作探索了利用检索示例的新方式，但上游检索步骤大多未被探索。本文研究了变化的检索方法对几种翻译架构的影响，以更好地理解这两个过程之间的相互作用。我们在多领域环境中对两种语言对进行实验，并考虑基于标准自回归模型、基于编辑的模型以及带有上下文学习的大型语言模型的几种下游架构。我们的实验表明，检索技术的选择影响了翻译得分，而在各种架构之间存在差异。我们还讨论了增加示例数量和多样性的效果，这在整体上大多是积极的。

    arXiv:2404.02835v1 Announce Type: new  Abstract: Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve examples from memory to guide the generation process. While most works in this trend explore new ways to exploit the retrieved examples, the upstream retrieval step is mostly unexplored. In this paper, we study the effect of varying retrieval methods for several translation architectures, to better understand the interplay between these two processes. We conduct experiments in two language pairs in a multi-domain setting and consider several downstream architectures based on a standard autoregressive model, an edit-based model, and a large language model with in-context learning. Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures. We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board.
    
[^7]: Conifer: 提高大型语言模型复杂约束指令遵循能力

    Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models

    [https://arxiv.org/abs/2404.02823](https://arxiv.org/abs/2404.02823)

    Conifer提出了一个新的指令调节数据集，通过LLMs驱动的细化过程，以及渐进学习方案，显著提高了大型语言模型遵循具有复杂约束的多层指令的能力

    

    大型语言模型(LLMs)遵循指令的能力对实际应用至关重要。尽管最近取得进展，但一些研究指出，LLMs在面对具有挑战性指令时存在困难，特别是包含复杂约束的指令，阻碍了它们在各种任务中的有效性。为解决这一挑战，我们引入了Conifer，这是一个新颖的指令调节数据集，旨在增强LLMs遵循具有复杂约束的多层指令。通过一系列LLM驱动的细化过程，我们利用GPT-4策划了这个数据集以确保高质量。我们还提出了一个强调易于难的渐进学习方案，并从过程反馈中学习。使用Conifer训练的模型在遵循指令能力方面表现出显著改善，特别是对于带有复杂约束的指令。在几个遵循指令的基准测试中，我们的7B模型表现优异

    arXiv:2404.02823v1 Announce Type: cross  Abstract: The ability of large language models (LLMs) to follow instructions is crucial to real-world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi-level instructions with complex constraints. Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints. On several instruction-following benchmarks, our 7B model outperfor
    
[^8]: 使用机器学习识别国家法律和政策中的气候目标

    Identifying Climate Targets in National Laws and Policies using Machine Learning

    [https://arxiv.org/abs/2404.02822](https://arxiv.org/abs/2404.02822)

    本文提出了一种从国家法律和政策中提取气候目标的方法，可以可靠地识别出三类目标（“净零”，“减少”和“其他”），并调查了与模型相关的偏见和公平影响。

    

    定量政策目标是气候政策的基本要素，通常以领域特定和技术性语言为特征。目前，筛选全球气候政策目标的方法涉及大量手动工作。目前很少有可扩展的方法从国家法律或政策中提取气候目标，这限制了政策制定者和研究人员评估私营和公共部门与全球目标的一致性以及为政策决策提供信息的能力。在本文中，我们提出了一种从国家法律和政策中提取气候目标提及的方法。我们创建了一个专家注释的数据集，识别了三类目标（“净零”，“减少”和“其他”（例如可再生能源目标）），并训练了一个可靠地在文本中识别它们的分类器。我们调查了与我们模型相关的偏差和公平影响，并确定了特定年份和国家名称作为问题。

    arXiv:2404.02822v1 Announce Type: cross  Abstract: Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language. Current methods for curating comprehensive views of global climate policy targets entail significant manual effort. At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions. In this paper we present an approach for extracting mentions of climate targets from national laws and policies. We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text. We investigate bias and equity impacts related to our model and identify specific years and country names as problemati
    
[^9]: 有关叙事理解中可控问题回答生成的少样本提示研究

    On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension

    [https://arxiv.org/abs/2404.02800](https://arxiv.org/abs/2404.02800)

    提出了用于控制从儿童叙事文本中生成问题-答案对的少样本提示策略，旨在控制问题的明确性和潜在叙事元素，通过与参考模型并行进行实证评估，结果显示在语义接近度评估和问题-答案对的多样性和连贯性方面，少样本策略超越了参考模型。

    

    问题生成旨在根据给定的上下文自动生成问题。可控问题生成方案侧重于生成具有特定属性的问题，从而实现更好的控制。在本研究中，我们提出了一种少样本提示策略，用于控制从儿童叙事文本中生成问题-答案对的过程。我们旨在控制两个属性：问题的明确性和潜在叙事元素。通过实证评估，我们展示了通过采用少样本提示与参考模型并行控制生成过程的有效性。我们的实验凸显了少样本策略超越参考模型的实例，特别是在语义接近度评估以及问题-答案对的多样性和连贯性等场景中。然而，这些改进并不总是统计上显著的。代码已公开发布。

    arXiv:2404.02800v1 Announce Type: cross  Abstract: Question Generation aims to automatically generate questions based on a given input provided as context. A controllable question generation scheme focuses on generating questions with specific attributes, allowing better control. In this study, we propose a few-shot prompting strategy for controlling the generation of question-answer pairs from children's narrative texts. We aim to control two attributes: the question's explicitness and underlying narrative elements. With empirical evaluation, we show the effectiveness of controlling the generation process by employing few-shot prompting side by side with a reference model. Our experiments highlight instances where the few-shot strategy surpasses the reference model, particularly in scenarios such as semantic closeness evaluation and the diversity and coherency of question-answer pairs. However, these improvements are not always statistically significant. The code is publicly available
    
[^10]: FPT:特征提示调整用于少样本可读性评估

    FPT: Feature Prompt Tuning for Few-shot Readability Assessment

    [https://arxiv.org/abs/2404.02772](https://arxiv.org/abs/2404.02772)

    FPT提出了一种新颖的基于提示的调整框架，通过将语言特征嵌入训练软提示并设计新的损失函数，在少样本设置下改善了可读性评估任务的性能。

    

    基于提示的方法在大多数少样本文本分类任务中取得了有希望的结果。然而，在可读性评估任务中，传统的提示方法缺乏关键的语言知识，而已经被证明是必不可少的。此外，先前关于利用语言特征的研究显示，在少样本设置中具有非稳健性能，甚至可能损害模型性能。为了解决这些问题，我们提出了一个新颖的基于提示的调整框架，即具有丰富语言知识的特征提示调整（FPT）。具体地，我们从文本中提取语言特征，并将其嵌入可训练的软提示中。此外，我们设计了一个新的损失函数来校准类别之间的相似性排名顺序。实验结果表明，我们提出的FTP方法不仅在先前最佳的基于提示的调整方法上表现出显著的性能提升，而且超过了他们。

    arXiv:2404.02772v1 Announce Type: new  Abstract: Prompt-based methods have achieved promising results in most few-shot text classification tasks. However, for readability assessment tasks, traditional prompt methods lackcrucial linguistic knowledge, which has already been proven to be essential. Moreover, previous studies on utilizing linguistic features have shown non-robust performance in few-shot settings and may even impair model performance.To address these issues, we propose a novel prompt-based tuning framework that incorporates rich linguistic knowledge, called Feature Prompt Tuning (FPT). Specifically, we extract linguistic features from the text and embed them into trainable soft prompts. Further, we devise a new loss function to calibrate the similarity ranking order between categories. Experimental results demonstrate that our proposed method FTP not only exhibits a significant performance improvement over the prior best prompt-based tuning approaches, but also surpasses th
    
[^11]: AQuA --结合专家和非专家观点，利用LLMs评估在线讨论中的磋商质量

    AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs

    [https://arxiv.org/abs/2404.02761](https://arxiv.org/abs/2404.02761)

    提出了AQuA，一种综合的磋商质量得分计算方法，可以从多个指标中提取各个讨论帖子的统一得分，保留了评论中磋商方面的信息，提高了模型的透明度。

    

    在政治在线讨论中衡量贡献质量对于研究磋商和计算机科学至关重要。随着深度学习的进步，自动衡量这些指标变得可行。本文介绍了AQuA，它是一个添加分数，从多个指标中计算每个讨论帖子的统一磋商质量得分。与其他特定分数不同，AQuA保留了评论中存在的磋商方面的信息，增强了模型的透明度。

    arXiv:2404.02761v1 Announce Type: cross  Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices int
    
[^12]: 大型语言模型的自动提示选择

    Automatic Prompt Selection for Large Language Models

    [https://arxiv.org/abs/2404.02717](https://arxiv.org/abs/2404.02717)

    通过在训练数据上进行聚类，使用基于LLM的提示生成器为每个簇生成候选提示，综合数据集进行训练以评估提示的相关性，最终在测试时使用评估器为新输入选择最佳提示，实现了大型语言模型的自动提示选择。

    

    大型语言模型（LLMs）可以在适当的提示指导下执行各种自然语言处理任务。然而，手动设计有效的提示具有挑战性且耗时。现有的自动提示优化方法要么缺乏灵活性，要么效率低下。在本文中，我们提出了一种有效的方法，以自动从一组有限的合成候选提示中为给定输入选择最佳提示。

    arXiv:2404.02717v1 Announce Type: new  Abstract: Large Language Models (LLMs) can perform various natural language processing tasks with suitable instruction prompts. However, designing effective prompts manually is challenging and time-consuming. Existing methods for automatic prompt optimization either lack flexibility or efficiency. In this paper, we propose an effective approach to automatically select the optimal prompt for a given input from a finite set of synthetic candidate prompts. Our approach consists of three steps: (1) clustering the training data and generating candidate prompts for each cluster using an LLM-based prompt generator; (2) synthesizing a dataset of input-prompt-output tuples for training a prompt evaluator to rank the prompts based on their relevance to the input; (3) using the prompt evaluator to select the best prompt for a new input at test time. Our approach balances prompt generality-specificity and eliminates the need for resource-intensive training an
    
[^13]: ART：用于语音对位和模仿的交替阅读任务语料库

    ART: The Alternating Reading Task Corpus for Speech Entrainment and Imitation

    [https://arxiv.org/abs/2404.02710](https://arxiv.org/abs/2404.02710)

    该论文介绍了交替阅读任务（ART）语料库，该语料库包含了三种实验条件和三个子语料库，可以在受控和不那么自发的环境中系统地研究语音对位行为。

    

    我们介绍了交替阅读任务（ART）语料库，这是一个用于研究语音交流中对位和模仿行为的双人句子阅读集合。ART语料库包括三种实验条件 - 单独阅读、交替阅读和蓄意模仿 - 以及包含法语、意大利语和斯洛伐克口音英语的三个子语料库。该设计允许在受控和不那么自发的环境中系统地研究语音对位。除了详细的转录外，还包括英语熟练度评分、人口统计学数据和实验中的问卷，用于探究对位现象受语言、个人和人际因素的影响。我们的介绍涵盖了其设计、收集、标注过程、初步分析以及未来研究展望.

    arXiv:2404.02710v1 Announce Type: new  Abstract: We introduce the Alternating Reading Task (ART) Corpus, a collection of dyadic sentence reading for studying the entrainment and imitation behaviour in speech communication. The ART corpus features three experimental conditions - solo reading, alternating reading, and deliberate imitation - as well as three sub-corpora encompassing French-, Italian-, and Slovak-accented English. This design allows systematic investigation of speech entrainment in a controlled and less-spontaneous setting. Alongside detailed transcriptions, it includes English proficiency scores, demographics, and in-experiment questionnaires for probing linguistic, personal and interpersonal influences on entrainment. Our presentation covers its design, collection, annotation processes, initial analysis, and future research prospects.
    
[^14]: 通过定制化专家网络实现可扩展的模型编辑

    Scalable Model Editing via Customized Expert Networks

    [https://arxiv.org/abs/2404.02699](https://arxiv.org/abs/2404.02699)

    通过引入SCEN方法，使用定制的专家网络实现了可扩展的模型编辑，解决了大型语言模型中的幻觉和过时知识问题，并在实验证实中取得了最先进的结果。

    

    大型语言模型中存在幻觉和过时知识的问题对于其可靠应用至关重要。模型编辑是一个有前途的途径，可以以成本效益的方式减轻这些挑战。然而，现有方法经常受到不令人满意的泛化和对不相关样本的意外影响。为了克服这些限制，我们引入了一种新方法：通过定制化专家网络实现可扩展的模型编辑（SCEN），这是一个有两个阶段的连续训练范式。具体地，在第一个阶段，我们为每个需要更新的知识片段单独训练轻量级专家网络。随后，我们训练每个专家对应的神经元来控制该专家的激活状态。我们在两种不同规模的开源大型语言模型，Llama2 7B 和 13B 上的实验证实，相比现有主流的模型编辑方法取得了最先进的结果。

    arXiv:2404.02699v1 Announce Type: new  Abstract: Addressing the issue of hallucinations and outdated knowledge in large language models is critical for their reliable application. Model Editing presents a promising avenue for mitigating these challenges in a cost-effective manner. However, existing methods often suffer from unsatisfactory generalization and unintended effects on unrelated samples. To overcome these limitations, we introduce a novel approach: Scalable Model Editing via Customized Expert Networks (SCEN), which is a two-stage continuous training paradigm. Specifically, in the first stage, we train lightweight expert networks individually for each piece of knowledge that needs to be updated. Subsequently, we train a corresponding neuron for each expert to control the activation state of that expert. Our experiments on two different sizes of open-source large language models, the Llama2 7B and 13B, achieve state-of-the-art results compared to existing mainstream Model Editi
    
[^15]: 注意力机制在高斯分布输入下自然稀疏

    Attention is Naturally Sparse with Gaussian Distributed Input

    [https://arxiv.org/abs/2404.02690](https://arxiv.org/abs/2404.02690)

    通过对高斯输入下注意力得分稀疏性进行理论分析，揭示了注意力机制中稀疏性的特征及其对计算效率的影响。

    

    大型语言模型（LLMs）的计算强度是关键瓶颈，主要是由于transformer架构中注意力机制的$O(n^2)$复杂度。稀疏注意力作为一个关键创新应运而生，旨在减少计算负荷同时保持模型性能。本研究对LLMs内的注意力分数稀疏性进行了严格的理论分析，特别是在高斯输入框架下。通过建立一组基础假设并采用一种系统的理论方法，我们揭示了注意力分数稀疏性的内在特征及其对计算效率的影响。我们的主要贡献在于提供了对注意力机制中稀疏性表现形式的详细理论检查，揭示了在计算节约和模型有效性之间潜在权衡的见解。

    arXiv:2404.02690v1 Announce Type: cross  Abstract: The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures. Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance. This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs. By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency. Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness. This work not only advances 
    
[^16]: 跨架构迁移学习用于线性成本推断变换器

    Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers

    [https://arxiv.org/abs/2404.02684](https://arxiv.org/abs/2404.02684)

    提出了一种跨架构迁移学习方法，用于在线性成本推断和自注意力变换器之间共享组件的权重，以提高Transformer语言模型的效率。

    

    最近，提出了多种架构来通过改变自注意力模块的设计实现线性成本推断(LCI)以提高Transformer语言模型的效率。在这个领域中，一个值得注意的方法是状态空间机器（SSMs）架构，它在语言建模任务上显示出与自注意力变换器相当的性能。然而，这种架构更改需要从头开始完全预训练权重，这给希望使用新架构的研究人员和从业者带来了巨大成本。受传统线性注意力工作的启发，我们提出了跨架构迁移学习(XATL)，其中LCI和基于自注意力的变换器之间的共享组件的权重，如层规范、MLP、输入/输出

    arXiv:2404.02684v1 Announce Type: cross  Abstract: Recently, multiple architectures has been proposed to improve the efficiency of the Transformer Language Models through changing the design of the self-attention block to have a linear-cost inference (LCI). A notable approach in this realm is the State-Space Machines (SSMs) architecture, which showed on-par performance on language modeling tasks with the self-attention transformers. However, such an architectural change requires a full pretraining of the weights from scratch, which incurs a huge cost to researchers and practitioners who want to use the new architectures. In the more traditional linear attention works, it has been proposed to approximate full attention with linear attention by swap-and-finetune framework. Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/outpu
    
[^17]: PejorativITy：消除蔑称词以改善意大利推文中的厌恶检测

    PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets

    [https://arxiv.org/abs/2404.02681](https://arxiv.org/abs/2404.02681)

    通过澄清词语含义来改善厌恶检测，PejorativITy提出了一个新的意大利推文语料库，揭示了将贬损信息注入模型的两种方法均能显著提高分类性能。

    

    厌恶往往通过比喻语言表达。当一些中性词语作为贬损称谓时，可能会呈现负面含义。澄清这类词语的含义可能有助于检测厌恶。为解决这一任务，我们提出了PejorativITy，这是一个新颖的由1,200条意大利推文构成的语料库，用于在单词级别上标注贬损语言，并在句子级别上标注厌恶。我们评估了将澄清词语信息注入针对厌恶检测模型的影响。具体地，我们探讨了两种不同的注入方法：将贬损信息连接在一起和将模棱两可的词语替换为明确的术语。我们的实验结果，无论是在我们的语料库上还是在意大利推文的两个流行基准上，都表明这两种方法均导致更大的分类改善，表明词义消歧是厌恶检测的有前景的初步步骤。

    arXiv:2404.02681v1 Announce Type: cross  Abstract: Misogyny is often expressed through figurative language. Some neutral words can assume a negative connotation when functioning as pejorative epithets. Disambiguating the meaning of such terms might help the detection of misogyny. In order to address such task, we present PejorativITy, a novel corpus of 1,200 manually annotated Italian tweets for pejorative language at the word level and misogyny at the sentence level. We evaluate the impact of injecting information about disambiguated words into a model targeting misogyny detection. In particular, we explore two different approaches for injection: concatenation of pejorative information and substitution of ambiguous words with univocal terms. Our experimental results, both on our corpus and on two popular benchmarks on Italian tweets, show that both approaches lead to a major classification improvement, indicating that word sense disambiguation is a promising preliminary step for misog
    
[^18]: VoicePrivacy 2024挑战评估计划

    The VoicePrivacy 2024 Challenge Evaluation Plan

    [https://arxiv.org/abs/2404.02677](https://arxiv.org/abs/2404.02677)

    该挑战旨在开发一种声音匿名化系统，用于隐藏说话者的声音身份，同时保护语言内容和情感状态，并通过组织者提供的数据集和评估脚本进行评估。

    

    挑战的任务是为语音数据开发一种声音匿名化系统，该系统隐藏说话者的语音身份，同时保护语言内容和情感状态。组织者提供开发和评估数据集、评估脚本，以及基于参与者要求形成的基线匿名化系统和培训资源列表。参与者应用他们开发的匿名化系统，运行评估脚本，并将评估结果和匿名化的语音数据提交给组织者。结果将在与Interspeech 2024同期举办的研讨会上展示，所有参与者都被邀请展示他们的挑战系统并提交额外的研讨会论文。

    arXiv:2404.02677v1 Announce Type: cross  Abstract: The task of the challenge is to develop a voice anonymization system for speech data which conceals the speaker's voice identity while protecting linguistic content and emotional states. The organizers provide development and evaluation datasets and evaluation scripts, as well as baseline anonymization systems and a list of training resources formed on the basis of the participants' requests. Participants apply their developed anonymization systems, run evaluation scripts and submit evaluation results and anonymized speech data to the organizers. Results will be presented at a workshop held in conjunction with Interspeech 2024 to which all participants are invited to present their challenge systems and to submit additional workshop papers.
    
[^19]: 在大型语言模型知识蒸馏中重新思考Kullback-Leibler散度

    Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models

    [https://arxiv.org/abs/2404.02657](https://arxiv.org/abs/2404.02657)

    本研究重新思考了大型语言模型知识蒸馏中对Kullback-Leibler散度的应用，发现逆Kullback-Leibler和正向Kullback-Leibler散度在优化目标上相似，为此提出了一种自适应Kullback-Leiber散度方法。

    

    Kullback-Leibler散度在知识蒸馏中被广泛应用于压缩大型语言模型。本研究从经验和理论上证明了，在LLMs的知识蒸馏中，与之前断言的逆Kullback-Leibler（RKL）散度寻找模式并因此优于寻找平均值的正向Kullback-Leibler（FKL）散度相反，实际上在知识蒸馏中都没有体现出寻找模式或寻找平均值的特性。相反，发现RKL和FKL具有相同的优化目标，并在足够数量的时代之后都会收敛。然而，由于实际约束，LLMs很少被训练如此多的时代。同时，我们进一步发现，RKL在分布的尾部，而FKL在开始时代侧重于分布的头部。因此，我们提出了一种简单而有效的自适应Kullback-Leiber（AKL）散度方法，该方法自适应地分配权重来组合F

    arXiv:2404.02657v1 Announce Type: cross  Abstract: Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine F
    
[^20]: 通过诱导忠实性校准大型语言模型的置信度

    Calibrating the Confidence of Large Language Models by Eliciting Fidelity

    [https://arxiv.org/abs/2404.02655](https://arxiv.org/abs/2404.02655)

    本文通过将语言模型的置信度分解为问题的不确定性和对答案的忠实性，提出了一种估计语言模型置信度的即插即用方法，经实验证明具有良好的校准性能。

    

    使用RLHF等技术优化的大型语言模型已经取得了良好的对齐，既有帮助性又无害。然而，在对齐之后，这些语言模型经常表现出过度自信，表达的置信度并不准确地与其正确率校准。在本文中，我们将语言模型的置信度分解为关于问题的\textit{不确定性}和对语言模型生成的答案的\textit{忠实性}。然后，我们提出了一种即插即用的方法来估计语言模型的置信度。通过在四个MCQA数据集上对6个RLHF-LMs进行实验，我们的方法表现出很好的校准性能。此外，我们提出了两个新颖的度量标准，IPR和CE，来评估模型的校准性，并对\textit{真正校准的置信度}进行了详细讨论。我们的方法可以作为一个强有力的基线，希望这项工作能提供一些见解。

    arXiv:2404.02655v1 Announce Type: new  Abstract: Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \textit{Uncertainty} about the question and the \textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into
    
[^21]: 针对大型语言模型中未预料到的偏见的检测

    Towards detecting unanticipated bias in Large Language Models

    [https://arxiv.org/abs/2404.02650](https://arxiv.org/abs/2404.02650)

    本论文探索了在大型语言模型中检测未预料到的偏见的新途径，着重于不确定性量化和可解释人工智能方法。

    

    在过去一年中，像ChatGPT这样的大型语言模型（LLMs）已经被广泛使用，并展现出与以前的机器学习系统类似的公平性问题。当前研究主要集中于分析和量化这些训练数据中的偏见及其对这些模型决策的影响，同时制定减轻策略。这项研究主要针对与性别、种族、族裔和语言相关的众所周知的偏见。然而，很明显，LLMs也受到其他不太明显的内隐偏见的影响。这些模型的复杂性和通常的不透明性使得检测这些偏见具有挑战性，但由于它们在各种应用中潜在的负面影响，这是至关重要的。在本文中，我们探讨了在LLMs中检测这些未预料到的偏见的新途径，具体关注不确定性量化和可解释人工智能方法。这些方法旨在评估确定性

    arXiv:2404.02650v1 Announce Type: cross  Abstract: Over the last year, Large Language Models (LLMs) like ChatGPT have become widely available and have exhibited fairness issues similar to those in previous machine learning systems. Current research is primarily focused on analyzing and quantifying these biases in training data and their impact on the decisions of these models, alongside developing mitigation strategies. This research largely targets well-known biases related to gender, race, ethnicity, and language. However, it is clear that LLMs are also affected by other, less obvious implicit biases. The complex and often opaque nature of these models makes detecting such biases challenging, yet this is crucial due to their potential negative impact in various applications. In this paper, we explore new avenues for detecting these unanticipated biases in LLMs, focusing specifically on Uncertainty Quantification and Explainable AI methods. These approaches aim to assess the certainty
    
[^22]: 用于基于解释的自然语言推理的可微分整数线性规划求解器

    A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference

    [https://arxiv.org/abs/2404.02625](https://arxiv.org/abs/2404.02625)

    Diff-Comb Explainer是一种基于可微黑盒组合求解器的神经符号架构，不需要对语义约束进行连续放松，相比传统解决方案表现更出色。

    

    Integer Linear Programming（ILP）被提出作为对自然语言推理（NLI）进行精确结构和语义约束编码的正式形式。然而，传统的ILP框架是不可微分的，这给基于深度学习的连续语言表示的整合带来了关键挑战。本文介绍了一种新方法，名为Diff-Comb Explainer，这是一种基于可微黑盒组合求解器（DBCS）的解释型NLI的神经符号架构。与现有的神经符号求解器不同，Diff-Comb Explainer不需要对语义约束进行连续放松，从而能够直接、更精确和高效地将神经表示融入到ILP公式中。我们的实验表明，与传统ILP求解器、神经符号黑盒求解器和Trans相比，Diff-Comb Explainer实现了更优越的性能。

    arXiv:2404.02625v1 Announce Type: cross  Abstract: Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for Natural Language Inference (NLI). However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning. In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers (DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation. Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic black-box solvers, and Trans
    
[^23]: 估计基于Transformer的NLI模型中自然逻辑特征的因果效应

    Estimating the Causal Effects of Natural Logic Features in Transformer-Based NLI Models

    [https://arxiv.org/abs/2404.02622](https://arxiv.org/abs/2404.02622)

    通过构建明确的因果图，研究基于Transformer的NLI模型中特定自然逻辑推理模式的因果效应估计策略，以识别和量化系统性推理失败。

    

    论证自然语言推理问题中语义特征对语言模型预测的因果效应进行严格评估是困难的。然而，从解释性和模型评估角度来看，这种分析形式是非常有益的，因此有必要研究具有足够结构和规律性的特定推理模式，以识别和量化广泛使用的模型中的系统性推理失败。在这方面，我们选择NLI任务的一部分，其中可以系统构建显式因果图：跨两个句子（前提和假设），两个相关单词/术语在共享上下文中出现的情况。在这项工作中，我们应用因果效应估计策略来衡量上下文干预的效应（其对蕴涵标签的影响是通过语义单调特征进行介导）以及对插入的词对的干预（其对蕴涵标签的影响是通过

    arXiv:2404.02622v1 Announce Type: new  Abstract: Rigorous evaluation of the causal effects of semantic features on language model predictions can be hard to achieve for natural language reasoning problems. However, this is such a desirable form of analysis from both an interpretability and model evaluation perspective, that it is valuable to investigate specific patterns of reasoning with enough structure and regularity to identify and quantify systematic reasoning failures in widely-used models. In this vein, we pick a portion of the NLI task for which an explicit causal diagram can be systematically constructed: the case where across two sentences (the premise and hypothesis), two related words/terms occur in a shared context. In this work, we apply causal effect estimation strategies to measure the effect of context interventions (whose effect on the entailment label is mediated by the semantic monotonicity characteristic) and interventions on the inserted word-pair (whose effect on
    
[^24]: 用人类判断调整嵌入空间中可解释的维度

    Adjusting Interpretable Dimensions in Embedding Space with Human Judgments

    [https://arxiv.org/abs/2404.02619](https://arxiv.org/abs/2404.02619)

    通过人类评分指导种子词向量，我们在预测对象属性和风格属性时获得了具有显着更好性能的可解释维度。

    

    嵌入空间包含指示性别、风格正式性甚至对象属性的可解释维度，这已经被多次观察到。这样的可解释维度正变成不同领域研究中有价值的工具，从社会科学到神经科学。计算这些维度的标准方法使用对比种子词并计算它们之间的差向量。这是简单的，但并不总是有效。我们将基于种子的向量与人类对特定 维度中词落在何处的评分结合，评估预测对象属性如大小和危险性，以及风格属性如正式性和复杂性。我们获得了可解释维度在特别在种子维度效果不佳的情况下表现出显著更好的性能。

    arXiv:2404.02619v1 Announce Type: new  Abstract: Embedding spaces contain interpretable dimensions indicating gender, formality in style, or even object properties. This has been observed multiple times. Such interpretable dimensions are becoming valuable tools in different areas of study, from social science to neuroscience. The standard way to compute these dimensions uses contrasting seed words and computes difference vectors over them. This is simple but does not always work well. We combine seed-based vectors with guidance from human ratings of where words fall along a specific dimension, and evaluate on predicting both object properties like size and danger, and the stylistic properties of formality and complexity. We obtain interpretable dimensions with markedly better performance especially in cases where seed-based dimensions do not work well.
    
[^25]: 通过混合结构化摘要和基于LLM的数据增强来改进主题相关性模型

    Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation

    [https://arxiv.org/abs/2404.02616](https://arxiv.org/abs/2404.02616)

    通过混合结构化摘要和基于LLM的数据增强方法，改进了主题相关性模型，使其能够更好地学习查询与文档之间的相关度。

    

    查询和文档之间的主题相关性是社交搜索的一个非常重要的部分，可以评估文档与用户需求之间的匹配程度。在大多数社交搜索场景中，如大众点评，建模搜索相关性总是面临两个挑战。一个是许多社交搜索中的文档非常长且包含大量冗余信息。另一个问题是搜索相关性模型的训练数据很难获得，尤其是对于多分类相关性模型。为了解决以上两个问题，我们首先将查询与基于查询的摘要以及不带查询的文档摘要合并，作为主题相关性模型的输入，这有助于模型学习查询和文档核心主题之间的相关度。然后，我们利用大型语言模型（LLM）的语言理解和生成能力，从现有训练数据中重新编写和生成查询。

    arXiv:2404.02616v1 Announce Type: cross  Abstract: Topic relevance between query and document is a very important part of social search, which can evaluate the degree of matching between document and user's requirement. In most social search scenarios such as Dianping, modeling search relevance always faces two challenges. One is that many documents in social search are very long and have much redundant information. The other is that the training data for search relevance model is difficult to get, especially for multi-classification relevance model. To tackle above two problems, we first take query concatenated with the query-based summary and the document summary without query as the input of topic relevance model, which can help model learn the relevance degree between query and the core topic of document. Then, we utilize the language understanding and generation abilities of large language model (LLM) to rewrite and generate query from queries and documents in existing training da
    
[^26]: 利用句法和声学线索相互作用优化韩语TTS停顿形成

    Leveraging the Interplay Between Syntactic and Acoustic Cues for Optimizing Korean TTS Pause Formation

    [https://arxiv.org/abs/2404.02592](https://arxiv.org/abs/2404.02592)

    提出了一个新框架，利用综合建模句法和声学线索，能够在培训时显示短音频剪辑的情况下，为韩语生成自然语音，解决了停顿错误问题

    

    当代神经语音合成模型在合成语音生成方面表现出卓越的熟练度，达到了与人类语音相媲美的质量水平。然而，这些成就主要在英语等高资源语言背景下得到验证。此外，当应用于韩语时，Tacotron和FastSpeech变体显示出相当大的停顿错误，影响了语音感知和自然性。为了解决上述问题，我们提出了一个新颖的框架，结合了与停顿模式相关的句法和声学线索的全面建模。值得注意的是，我们的框架具有在培训时显示长度较短的音频剪辑的情况下，即使针对更为复杂的域外（OOD）句子，也能始终生成自然的语音的能力。

    arXiv:2404.02592v1 Announce Type: new  Abstract: Contemporary neural speech synthesis models have indeed demonstrated remarkable proficiency in synthetic speech generation as they have attained a level of quality comparable to that of human-produced speech. Nevertheless, it is important to note that these achievements have predominantly been verified within the context of high-resource languages such as English. Furthermore, the Tacotron and FastSpeech variants show substantial pausing errors when applied to the Korean language, which affects speech perception and naturalness. In order to address the aforementioned issues, we propose a novel framework that incorporates comprehensive modeling of both syntactic and acoustic cues that are associated with pausing patterns. Remarkably, our framework possesses the capability to consistently generate natural speech even for considerably more extended and intricate out-of-domain (OOD) sentences, despite its training on short audio clips. Archi
    
[^27]: Affective-NLI: 朝着准确且可解释的对话中人格识别方向

    Affective-NLI: Towards Accurate and Interpretable Personality Recognition in Conversation

    [https://arxiv.org/abs/2404.02589](https://arxiv.org/abs/2404.02589)

    提出了Affective-NLI用于准确且可解释的对话中人格识别，以利用对话内容中的情感因素进行准确的人格识别

    

    对话中的人格识别（PRC）旨在通过文本对话内容识别说话者的人格特征。这对于在人机交互（HCI）的各种应用中提供个性化服务至关重要，包括基于AI的心理治疗和为老年人提供伴侣机器人。最近的研究分析对话内容进行人格分类，但忽略了影响其性能的两个主要问题。首先，对话中包含的关键隐含因素（如反映说话者人格的情绪）被忽略。其次，仅关注输入对话内容忽略了对个性本身语义理解，降低了结果的解释性。在本文中，我们提出了用于准确且可解释的PRC的情感自然语言推理（Affective-NLI）。

    arXiv:2404.02589v1 Announce Type: cross  Abstract: Personality Recognition in Conversation (PRC) aims to identify the personality traits of speakers through textual dialogue content. It is essential for providing personalized services in various applications of Human-Computer Interaction (HCI), such as AI-based mental therapy and companion robots for the elderly. Most recent studies analyze the dialog content for personality classification yet overlook two major concerns that hinder their performance. First, crucial implicit factors contained in conversation, such as emotions that reflect the speakers' personalities are ignored. Second, only focusing on the input dialog content disregards the semantic understanding of personality itself, which reduces the interpretability of the results. In this paper, we propose Affective Natural Language Inference (Affective-NLI) for accurate and interpretable PRC. To utilize affectivity within dialog content for accurate personality recognition, we 
    
[^28]: 大型语言模型用于将口语理解系统扩展到新语言

    Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages

    [https://arxiv.org/abs/2404.02588](https://arxiv.org/abs/2404.02588)

    通过利用大型语言模型并对其进行微调，我们提出了一种新的方法将口语理解系统扩展到新语言，改进了多语言口语理解数据集的性能。

    

    口语理解（SLU）模型是语音助手（如Alexa、Bixby和Google Assistant）的核心组件。本文介绍了一种旨在利用大型语言模型（LLMs）将SLU系统扩展到新语言的流水线，通过对机器翻译进行微调以处理槽标注的SLU训练数据。我们的方法在云场景中使用mBERT模型改进了MultiATIS ++基准测试，这是一个主要的多语言SLU数据集。具体而言，与现有的最先进方法Fine and Coarse-grained Multi-Task Learning Framework（FC-MTLF）相比，我们在“总体准确率”指标上取得了进步：从53%提高到62.18%。在设备上的情景中（小型且未预训练的SLU），我们的方法使“总体准确率”指标从5.31%提高到22.06%，超过了基线Global-Local Contrastive Learning Framework（GL-CLeF）方法。与FC-MTLF和GL-CLeF方法不同，我们基于LLM的机器翻译技术…（待续）

    arXiv:2404.02588v1 Announce Type: new  Abstract: Spoken Language Understanding (SLU) models are a core component of voice assistants (VA), such as Alexa, Bixby, and Google Assistant. In this paper, we introduce a pipeline designed to extend SLU systems to new languages, utilizing Large Language Models (LLMs) that we fine-tune for machine translation of slot-annotated SLU training data. Our approach improved on the MultiATIS++ benchmark, a primary multi-language SLU dataset, in the cloud scenario using an mBERT model. Specifically, we saw an improvement in the Overall Accuracy metric: from 53% to 62.18%, compared to the existing state-of-the-art method, Fine and Coarse-grained Multi-Task Learning Framework (FC-MTLF). In the on-device scenario (tiny and not pretrained SLU), our method improved the Overall Accuracy from 5.31% to 22.06% over the baseline Global-Local Contrastive Learning Framework (GL-CLeF) method. Contrary to both FC-MTLF and GL-CLeF, our LLM-based machine translation doe
    
[^29]: 多粒度引导的解码器融合

    Multi-Granularity Guided Fusion-in-Decoder

    [https://arxiv.org/abs/2404.02581](https://arxiv.org/abs/2404.02581)

    提出了多粒度引导的解码器融合（MGFiD），通过跨多个粒度辨别证据，并结合段落重新排序和句子分类，提高开放域问答中解码效率。

    

    在开放域问答中，识别相关上下文作为证据并避免在检索结果中出现虚假上下文至关重要。在解码阶段使用多个上下文进行串联的模型架构（即Fusion-in-Decoder）表现出有希望的性能，但会从看似合理的上下文中生成不正确的输出。为了解决这个问题，我们提出了多粒度引导的解码器融合（MGFiD），跨多个粒度辨别证据。基于多任务学习，MGFiD将段落重新排序与句子分类进行协调。它将明显的句子聚合到一个锚定向量中，指导解码器。此外，它通过重用段落重新排序的结果来提高解码效率进行段落修剪。通过实验证明，MGFiD在自然问答（NQ）和TriviaQA（TQA）数据集上优于现有模型，突显了其优点。

    arXiv:2404.02581v1 Announce Type: new  Abstract: In Open-domain Question Answering (ODQA), it is essential to discern relevant contexts as evidence and avoid spurious ones among retrieved results. The model architecture that uses concatenated multiple contexts in the decoding phase, i.e., Fusion-in-Decoder, demonstrates promising performance but generates incorrect outputs from seemingly plausible contexts. To address this problem, we propose the Multi-Granularity guided Fusion-in-Decoder (MGFiD), discerning evidence across multiple levels of granularity. Based on multi-task learning, MGFiD harmonizes passage re-ranking with sentence classification. It aggregates evident sentences into an anchor vector that instructs the decoder. Additionally, it improves decoding efficiency by reusing the results of passage re-ranking for passage pruning. Through our experiments, MGFiD outperforms existing models on the Natural Questions (NQ) and TriviaQA (TQA) datasets, highlighting the benefits of i
    
[^30]: 语言模型作为编译器：模拟伪代码执行改进语言模型中的算法推理

    Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models

    [https://arxiv.org/abs/2404.02575](https://arxiv.org/abs/2404.02575)

    这项研究提出了Think-and-Execute框架，将算法推理任务中的思考和执行精细分解，从而改进了大型语言模型的算法推理能力。

    

    算法推理指的是理解问题背后复杂模式并将其分解为通往解决方案的一系列推理步骤的能力。算法推理的这种性质使得大型语言模型(LLMs)面临挑战，尽管它们在其他推理任务中表现出色。最近的一些研究使用编程语言（例如Python）来表达解决给定实例/问题所需逻辑的方式，这受到其严格和精确的语法启发。然而，在单个推理调用中编写表达正确逻辑的可执行代码并非易事。此外，为一个实例专门生成的代码无法重用于其他实例，即使它们来自相同的任务并可能需要相同的逻辑来解决。本文介绍了Think-and-Execute，这是一个新颖的框架，将推理和执行进行了分解。

    arXiv:2404.02575v1 Announce Type: new  Abstract: Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large language models (LLMs), even though they have demonstrated promising performance in other reasoning tasks. Within this context, some recent studies use programming languages (e.g., Python) to express the necessary logic for solving a given instance/question (e.g., Program-of-Thought) as inspired by their strict and precise syntaxes. However, it is non-trivial to write an executable code that expresses the correct logic on the fly within a single inference call. Also, the code generated specifically for an instance cannot be reused for others, even if they are from the same task and might require identical logic to solve. This paper presents Think-and-Execute, a novel framework that decomposes the rea
    
[^31]: SemEval-2024任务1的MaiNLP：跨语言文本相关性中源语言选择的分析

    MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual Textual Relatedness

    [https://arxiv.org/abs/2404.02570](https://arxiv.org/abs/2404.02570)

    本文研究了在跨语言文本相关性中选择源语言的策略，尤其关注不同的预训练语言模型下的单源转移、多源转移和基于机器翻译的数据增强，最终在C8（Kinyarwanda）测试集中取得了第一名。

    

    本文介绍了我们为SemEval-2024任务1开发的系统：语义文本相关性（STR）的跨语言Track C。该任务旨在在没有直接监督的情况下（即零-shot跨语言转移）检测给定目标语言中两个句子的语义相关性。为此，我们着重于两种不同的预训练语言模型XLM-R和Furina上的不同源语言选择策略。我们尝试了1）单源转移，并基于类型相似性选择源语言，2）用两个最接近的源语言增强英语训练数据，以及3）多源转移，比较所有训练语言与来自同一语言家族的语言的选择。我们进一步研究了基于机器翻译的数据扩充和脚本差异的影响。我们的提交在C8（Kinyarwanda）测试集中取得了第一名。

    arXiv:2404.02570v1 Announce Type: new  Abstract: This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness (STR), on Track C: Cross-lingual. The task aims to detect semantic relatedness of two sentences in a given target language without access to direct supervision (i.e. zero-shot cross-lingual transfer). To this end, we focus on different source language selection strategies on two different pre-trained languages models: XLM-R and Furina. We experiment with 1) single-source transfer and select source languages based on typological similarity, 2) augmenting English training data with the two nearest-neighbor source languages, and 3) multi-source transfer where we compare selecting on all training languages against languages from the same family. We further study machine translation-based data augmentation and the impact of script differences. Our submission achieved the first place in the C8 (Kinyarwanda) test set.
    
[^32]: CSEPrompts: 初级计算机科学提示的基准

    CSEPrompts: A Benchmark of Introductory Computer Science Prompts

    [https://arxiv.org/abs/2404.02540](https://arxiv.org/abs/2404.02540)

    CSEPrompts是一个包含数百个编程练习提示的框架，旨在帮助理解计算机科学教育中公开可用的大型语言模型的潜在影响。

    

    最近人工智能、机器学习和自然语言处理方面的进展导致了新一代大型语言模型（LLMs）的开发，这些模型在海量数据上进行训练，通常拥有数万亿参数。商业应用（如ChatGPT）已使这项技术面向普通大众，因此可以利用LLMs为学术和专业用途生成高质量文本。学校和大学意识到学生越来越多地使用由AI生成的内容，他们一直在研究这种新技术及其潜在的滥用。计算机科学（CS）及相关领域的教育项目尤其受到影响，因为LLMs也能够生成各种编程语言的编程代码。为了帮助理解计算机科学教育中公开可用LLMs的潜在影响，我们引入了CSEPrompts，一个包含数百个编程练习提示的框架。

    arXiv:2404.02540v1 Announce Type: new  Abstract: Recent advances in AI, machine learning, and NLP have led to the development of a new generation of Large Language Models (LLMs) that are trained on massive amounts of data and often have trillions of parameters. Commercial applications (e.g., ChatGPT) have made this technology available to the general public, thus making it possible to use LLMs to produce high-quality texts for academic and professional purposes. Schools and universities are aware of the increasing use of AI-generated content by students and they have been researching the impact of this new technology and its potential misuse. Educational programs in Computer Science (CS) and related fields are particularly affected because LLMs are also capable of generating programming code in various programming languages. To help understand the potential impact of publicly available LLMs in CS education, we introduce CSEPrompts, a framework with hundreds of programming exercise prom
    
[^33]: ANGOFA：利用OFA嵌入初始化和合成数据的安哥拉语言模型

    ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model

    [https://arxiv.org/abs/2404.02534](https://arxiv.org/abs/2404.02534)

    本文介绍了四个专门针对安哥拉语言进行微调的PLM，并使用多语言自适应微调（MAFT）方法，通过采用知情嵌入初始化和合成数据，提高了MAFT模型在下游任务中的性能，将基线提高了12.3个百分点，超越了SOTA AfroXLMR-base和OFA。

    

    近年来，预训练语言模型（PLMs）的发展势头迅猛，展示了它们超越语言障碍、促进跨多种语言的知识转移的能力。然而，这一进展主要忽视了极低资源语言的包含，导致多语言景观中出现明显的空白。本文通过引入四个定制的PLM，专门为安哥拉语言进行微调，采用多语言自适应微调（MAFT）方法，以填补这一空白。我们调查了信息嵌入初始化和合成数据在增强MAFT模型在下游任务中性能方面的作用。我们将基线提高了12.3个百分点，超过了通过MAFT开发的SOTA AfroXLMR-base和有效嵌入初始化OFA分别提高了3.8个百分点。

    arXiv:2404.02534v1 Announce Type: cross  Abstract: In recent years, the development of pre-trained language models (PLMs) has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate knowledge transfer across diverse languages. However, this progress has predominantly bypassed the inclusion of very-low resource languages, creating a notable void in the multilingual landscape. This paper addresses this gap by introducing four tailored PLMs specifically finetuned for Angolan languages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In this paper, we survey the role of informed embedding initialization and synthetic data in enhancing the performance of MAFT models in downstream tasks. We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA (an effective embedding initialization) by 12.3 and 3.8 points respectively.
    
[^34]: 学会伪装：通过多智能体攻击者-伪装者博弈避免LLM的拒绝响应

    Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game

    [https://arxiv.org/abs/2404.02532](https://arxiv.org/abs/2404.02532)

    通过多智能体攻击者-伪装者博弈方法，实现一种弱防御机制，使大型模型能够安全地回复攻击者并隐藏防御意图

    

    随着大型模型在自然语言处理任务上表现出的增强性能，大型模型可能引发潜在的道德和伦理问题。存在一些恶意攻击者，他们通过诸如提示工程等技术诱使大型模型越狱，并生成包含非法、侵犯隐私信息的信息。因此，大型模型采用安全对齐等技术抵御恶意攻击者的攻击。然而，大型模型通过拒绝回复的强防御机制容易被攻击者识别，并用于加强攻击者的能力。在本文中，我们提出了一种多智能体攻击者-伪装者博弈方法，实现一种弱防御机制，使大型模型能够安全地回复攻击者并隐藏防御意图。首先，我们构建一个多智能体框架来模拟攻击和防御情景，扮演不同的角色，负责攻击、伪装

    arXiv:2404.02532v1 Announce Type: new  Abstract: With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers' attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disgu
    
[^35]: 用于分析论证结构和质量互动的学生作文语料库

    A School Student Essay Corpus for Analyzing Interactions of Argumentative Structure and Quality

    [https://arxiv.org/abs/2404.02529](https://arxiv.org/abs/2404.02529)

    提出了一个德语语料库，包含来自两个年龄组学生的1,320篇论文，每篇论文都经过人工注释，旨在分析论证结构和质量之间的互动。

    

    学习辩证写作是具有挑战性的。除了语法和语法等写作基础知识外，学习者必须有意义地选择和安排论证组件，才能创造出高质量的论文。为了在计算机上支持辩证写作，必须先挖掘论证结构。当与自动论文评分相结合时，论证结构和质量分数之间的互动可以用于全面的写作支持。尽管研究已经显示利用论证结构信息进行论文评分是有用的，但迄今尚未发布带有地面事实论文质量注释的论证挖掘语料库。此外，现有语料库中没有包含特定学生撰写的论文。为了填补这一研究空白，我们提出了一个包含来自两个年龄组学生的1,320篇德语论文的语料库。每篇论文都经过人工注释，注释了其论证结构和质量。

    arXiv:2404.02529v1 Announce Type: new  Abstract: Learning argumentative writing is challenging. Besides writing fundamentals such as syntax and grammar, learners must select and arrange argument components meaningfully to create high-quality essays. To support argumentative writing computationally, one step is to mine the argumentative structure. When combined with automatic essay scoring, interactions of the argumentative structure and quality scores can be exploited for comprehensive writing support. Although studies have shown the usefulness of using information about the argumentative structure for essay scoring, no argument mining corpus with ground-truth essay quality annotations has been published yet. Moreover, none of the existing corpora contain essays written by school students specifically. To fill this research gap, we present a German corpus of 1,320 essays from school students of two age groups. Each essay has been manually annotated for argumentative structure and quali
    
[^36]: 面向英语和印度语的大型语言模型驱动的无参考翻译评估

    Towards Large Language Model driven Reference-less Translation Evaluation for English and Indian Languages

    [https://arxiv.org/abs/2404.02512](https://arxiv.org/abs/2404.02512)

    该研究旨在评估大型语言模型在无参考翻译评估中的有效性，通过零样本学习、示例驱动学习和微调来模拟人类直接评估，发现LLM-based评估器在印度语言对中获得了与人类判断相当或更高的整体相关性。

    

    本文的主要重点是评估大型语言模型在自动无参考翻译评估中的有效性，我们展示了在评估英语和印度语翻译质量方面的实验。我们构建了一个翻译评估任务，通过零样本学习、示例驱动学习和对大型语言模型的微调来模拟人类直接评估，从而给出一个0到100的分数，其中100代表完美翻译，1代表翻译质量很差。我们将我们训练过的系统的性能与COMET、BERT-Scorer和LABSE等现有方法进行了比较，发现基于LLM的评估器（LLaMA-2-13B）在考虑的印度语言语种中实现了与人类判断相当或更高的整体相关性。

    arXiv:2404.02512v1 Announce Type: new  Abstract: With the primary focus on evaluating the effectiveness of large language models for automatic reference-less translation assessment, this work presents our experiments on mimicking human direct assessment to evaluate the quality of translations in English and Indian languages. We constructed a translation evaluation task where we performed zero-shot learning, in-context example-driven learning, and fine-tuning of large language models to provide a score out of 100, where 100 represents a perfect translation and 1 represents a poor translation. We compared the performance of our trained systems with existing methods such as COMET, BERT-Scorer, and LABSE, and found that the LLM-based evaluator (LLaMA-2-13B) achieves a comparable or higher overall correlation with human judgments for the considered Indian language pairs.
    
[^37]: 具有嵌入空间分离和压缩的终身事件检测

    Lifelong Event Detection with Embedding Space Separation and Compaction

    [https://arxiv.org/abs/2404.02507](https://arxiv.org/abs/2404.02507)

    通过嵌入空间分离和压缩，新方法减轻了先前学习任务的遗忘，并缓解了过拟合问题

    

    为了减轻遗忘，现有的终身事件检测方法通常维护一个记忆模块，并在学习新任务时重播存储的记忆数据。然而，记忆数据和新任务样本的简单结合仍可能导致先前获得的知识大量遗忘，这可能是由于新数据的特征分布与先前学习的嵌入空间之间的潜在重叠所导致的。此外，模型更容易对少量记忆样本过拟合，而不是有效记忆学到的模式。为了解决遗忘和过拟合的挑战，我们提出了一种基于嵌入空间分离和压缩的新方法。我们的方法通过强制新数据的特征分布远离先前的嵌入空间来减轻先前学习任务的遗忘。它还通过记忆校准机制来缓解过拟合

    arXiv:2404.02507v1 Announce Type: new  Abstract: To mitigate forgetting, existing lifelong event detection methods typically maintain a memory module and replay the stored memory data during the learning of a new task. However, the simple combination of memory data and new-task samples can still result in substantial forgetting of previously acquired knowledge, which may occur due to the potential overlap between the feature distribution of new data and the previously learned embedding space. Moreover, the model suffers from overfitting on the few memory samples rather than effectively remembering learned patterns. To address the challenges of forgetting and overfitting, we propose a novel method based on embedding space separation and compaction. Our method alleviates forgetting of previously learned tasks by forcing the feature distribution of new data away from the previous embedding space. It also mitigates overfitting by a memory calibration mechanism that encourages memory data t
    
[^38]: 情绪支持对话中的动态演示检索与认知理解

    Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation

    [https://arxiv.org/abs/2404.02505](https://arxiv.org/abs/2404.02505)

    通过动态演示检索和认知理解，我们提出了一种新颖的方法来提高情绪支持对话中提供的支持质量。

    

    情绪支持对话（ESC）系统在提供共情互动中起着关键作用，通过理解和解决用户独特经历来帮助用户度过消极情绪状态。本文解决ESC中的两个关键挑战：通过动态演示检索增强相关语境和共情式回应生成，以及推进认知理解以全面把握隐含的心理状态。我们介绍了动态演示检索和认知方面情境理解（\ourwork），这是一种新颖的方法，将这些元素协同起来，以提高ESC中提供的支持质量。通过利用上下文学习和人设信息，我们引入了一种创新的检索机制，选择信息丰富且个性化的演示对。我们还提出了一个认知理解模块，利用来自ATOMIC知识源的四种认知关系，以深入理解隐含的心理状态。

    arXiv:2404.02505v1 Announce Type: cross  Abstract: Emotional Support Conversation (ESC) systems are pivotal in providing empathetic interactions, aiding users through negative emotional states by understanding and addressing their unique experiences. In this paper, we tackle two key challenges in ESC: enhancing contextually relevant and empathetic response generation through dynamic demonstration retrieval, and advancing cognitive understanding to grasp implicit mental states comprehensively. We introduce Dynamic Demonstration Retrieval and Cognitive-Aspect Situation Understanding (\ourwork), a novel approach that synergizes these elements to improve the quality of support provided in ESCs. By leveraging in-context learning and persona information, we introduce an innovative retrieval mechanism that selects informative and personalized demonstration pairs. We also propose a cognitive understanding module that utilizes four cognitive relationships from the ATOMIC knowledge source to dee
    
[^39]: 测量大型语言模型的社会规范

    Measuring Social Norms of Large Language Models

    [https://arxiv.org/abs/2404.02491](https://arxiv.org/abs/2404.02491)

    论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。

    

    我们提出了一个新的挑战，以检验大型语言模型是否理解社会规范。与现有数据集不同，我们的数据集要求具有解决社会规范的基本理解。我们的数据集包含最大的社会规范技能集，包括402项技能和12,383个问题，涵盖了从观点和论点到文化和法律等广泛的社会规范。我们根据K-12课程设计了我们的数据集。这使得可以直接将大型语言模型的社会理解能力与人类进行比较，更具体地说是与小学生进行比较。尽管先前的工作在我们的基准测试中产生几乎随机的准确度，但最近的大型语言模型（如GPT3.5-Turbo和LLaMA2-Chat）能够显著提高性能，仅略低于人类性能。然后，我们提出了一个基于大型语言模型的多代理框架，以提高模型理解社会规范的能力。

    arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
    
[^40]: 通过词语对齐增强低资源语言的跨语言句子嵌入

    Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment

    [https://arxiv.org/abs/2404.02490](https://arxiv.org/abs/2404.02490)

    通过引入新颖框架，本文解决了当前模型中低资源语言的跨语言词表示与高资源语言不匹配的问题，实现了对低资源语言的句子嵌入的显著改进。

    

    跨语言句子嵌入的研究最近取得了显著进展，但由于平行语料库的稀缺，针对低资源语言的研究落后。本文表明目前模型中低资源语言的跨语言词表示与高资源语言明显不匹配。为解决这一问题，我们引入了一个新颖的框架，明确地在英语和八种低资源语言之间对齐单词，利用现成的词对齐模型。该框架包含三个主要的训练目标：对齐单词预测和单词翻译排序，以及广泛使用的翻译排序。我们通过对双文本检索任务的实验评估了我们的方法，结果显示在低资源语言的句子嵌入方面取得了显著的改进。此外，所提出模型在更广泛范围内表现出竞争力。

    arXiv:2404.02490v1 Announce Type: new  Abstract: The field of cross-lingual sentence embeddings has recently experienced significant advancements, but research concerning low-resource languages has lagged due to the scarcity of parallel corpora. This paper shows that cross-lingual word representation in low-resource languages is notably under-aligned with that in high-resource languages in current models. To address this, we introduce a novel framework that explicitly aligns words between English and eight low-resource languages, utilizing off-the-shelf word alignment models. This framework incorporates three primary training objectives: aligned word prediction and word translation ranking, along with the widely used translation ranking. We evaluate our approach through experiments on the bitext retrieval task, which demonstrate substantial improvements on sentence embeddings in low-resource languages. In addition, the competitive performance of the proposed model across a broader rang
    
[^41]: DUQGen: 通过多样化合成查询生成实现神经排序器的有效无监督领域自适应

    DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation

    [https://arxiv.org/abs/2404.02489](https://arxiv.org/abs/2404.02489)

    提出了一种名为DUQGen的方法，利用自动生成的有效和多样化的合成训练数据来进行现代神经排序器的新领域微调

    

    最先进的在大型任务特定训练数据集(如MS-MARCO)上预训练的神经排序器在各种排名任务上表现出色，甚至无需领域自适应即可实现零-shot效果。然而，零-shot神经排序可能不够优化，因为它未利用目标领域信息。为了解决这个问题，我们提出了一种新的用于排名的无监督领域自适应方法DUQGen，填补了先前文献中的一个关键空白，即如何自动生成既有效又多样化的合成训练数据，以微调现代神经排序器适应新领域。具体而言，DUQGen通过识别相似文档的聚类产生了更有效的目标领域表示；并生成了更多元化的trai

    arXiv:2404.02489v1 Announce Type: cross  Abstract: State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without domain adaptation, also called zero-shot. However, zero-shot neural ranking may be sub-optimal, as it does not take advantage of the target domain information. Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming. To address this problem, we propose a new approach to unsupervised domain adaptation for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new domain. Specifically, DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents; and generates a more diverse tra
    
[^42]: uTeBC-NLP在SemEval-2024任务9中：LLMs能成为横向思考者吗？

    uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?

    [https://arxiv.org/abs/2404.02474](https://arxiv.org/abs/2404.02474)

    通过研究提示工程方法如何增强LLMs在横向思考任务上的表现，揭示了其固有的超越思维能力，并发现压缩的信息性提示和动态的情境学习显著提升了模型性能。

    

    受人类认知启发，Jiang等人（2023c）创建了一个用于评估LLMs横向思维（超越思维定势）的基准。在这一基准的基础上，我们研究了不同的提示方法如何增强LLMs在这一任务上的表现，以揭示其固有的超越思维能力。通过参加SemEval-2024的第9项任务，即句子拼图子任务，我们探讨了提示工程方法：思维链（CoT）和直接提示，使用信息性描述进行增强，并利用检索增强生成（RAG）管道进行情境化提示。我们的实验涉及三种LLMs，包括GPT-3.5、GPT-4和Zephyr-7B-beta。我们使用GPT-4生成了谜题和选项之间的思维路径数据集，并通过人类进行了质量验证。研究结果表明，压缩的信息性提示能够提升模型性能。动态的情境学习显著提升了模型性能。

    arXiv:2404.02474v1 Announce Type: cross  Abstract: Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. F
    
[^43]: 提示数值序列：市场评论生成案例研究

    Prompting for Numerical Sequences: A Case Study on Market Comment Generation

    [https://arxiv.org/abs/2404.02466](https://arxiv.org/abs/2404.02466)

    本研究探讨了针对市场评论生成任务的不同输入表示方法，发现类似编程语言的提示效果更好，而类似自然语言和较长格式的提示效果较差。

    

    大型语言模型已被应用于广泛的数据转文本生成任务，包括表格、图表和时间序列数值数据转文本设置。然而，对于生成表格和图表等结构化数据的提示的研究正在增长中，对于时间序列数值数据的提示的深入研究却不足。因此，本研究探讨了各种输入表示，包括令牌序列和结构化格式如HTML、LaTeX和Python样式代码。在我们的实验中，我们专注于“市场评论生成”任务，该任务涉及将股价数值序列作为输入，生成相应的市场评论。与我们的预期相反，结果表明类似编程语言的提示产生更好的结果，而类似自然语言和较长格式（如HTML和LaTeX）的提示效果较差。我们的发现提供了一种方法

    arXiv:2404.02466v1 Announce Type: cross  Abstract: Large language models (LLMs) have been applied to a wide range of data-to-text generation tasks, including tables, graphs, and time-series numerical data-to-text settings. While research on generating prompts for structured data such as tables and graphs is gaining momentum, in-depth investigations into prompting for time-series numerical data are lacking. Therefore, this study explores various input representations, including sequences of tokens and structured formats such as HTML, LaTeX, and Python-style codes. In our experiments, we focus on the task of Market Comment Generation, which involves taking a numerical sequence of stock prices as input and generating a corresponding market comment. Contrary to our expectations, the results show that prompts resembling programming languages yield better outcomes, whereas those similar to natural languages and longer formats, such as HTML and LaTeX, are less effective. Our findings offer in
    
[^44]: PhonologyBench：评估大型语言模型的音韵技能

    PhonologyBench: Evaluating Phonological Skills of Large Language Models

    [https://arxiv.org/abs/2404.02456](https://arxiv.org/abs/2404.02456)

    PhonologyBench是一个新颖的基准测试，旨在明确评估大型语言模型在英语中的音韵技能，展示了LLMs在没有语音数据情况下在PhonologyBench任务上表现出显著性能。

    

    音韵学是研究语音结构和发音规则的学科，是大型语言模型（LLM）研究中一个关键但经常被忽视的组成部分。LLMs在各种利用音韵学的下游应用中被广泛使用，如教育工具和诗歌生成。此外，LLMs可能会从训练数据中学习不完美的正字和音标形式之间的关联。因此，对LLMs的音韵技能进行基准测试至关重要。为此，我们提出了PhonologyBench，这是一个新颖的基准测试，包括三个诊断任务，旨在明确测试LLMs在英语中的音韵技能：形音转换、音节计数和押韵词生成。尽管没有访问语音数据，LLMs在PhonologyBench任务上表现出显著的性能。然而，我们观察到在押韵词生成和音节计数方面存在显著的17%和45%的差距， respectively, when...

    arXiv:2404.02456v1 Announce Type: cross  Abstract: Phonology, the study of speech's structure and pronunciation rules, is a critical yet often overlooked component in Large Language Model (LLM) research. LLMs are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation. Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data. Thus, it is imperative to benchmark the phonological skills of LLMs. To this end, we present PhonologyBench, a novel benchmark consisting of three diagnostic tasks designed to explicitly test the phonological skills of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation. Despite having no access to speech data, LLMs showcased notable performance on the PhonologyBench tasks. However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when 
    
[^45]: 通过上下文一次性演示进行自适应跨语言文本分类

    Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations

    [https://arxiv.org/abs/2404.02452](https://arxiv.org/abs/2404.02452)

    本论文引入了一种新颖的概念，即通过在目标语言中插入一次性上下文演示，从而成功利用目标语言示例来改进评估的mT5模型的跨语言能力。

    

    零样本跨语言转移（ZS-XLT）利用在源语言中训练的模型在另一种语言中做预测，通常会导致性能损失。为了减轻这种情况，通过在目标语言中使用示例进行后续适应可以实现额外的改进。本文在分类任务中利用上下文调整（ICT）来进行一次性跨语言转移，引入上下文跨语言转移（IC-XLT）。这一创新概念涉及训练一个模型从上下文示例中学习，然后在推理过程中通过在目标语言中插入一次性上下文演示来对其进行适应。我们的结果表明，IC-XLT成功利用目标语言示例，改进了评估的mT5模型的跨语言能力，在适用于零和少量样本场景的情况下，通过微调来适应的基于提示的模型。此外，我们还表明，当源语言时

    arXiv:2404.02452v1 Announce Type: new  Abstract: Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss. To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language. In this paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing In-Context Cross-lingual Transfer (IC-XLT). The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language. Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming prompt-based models in the Zero and Few-shot scenarios adapted through fine-tuning. Moreover, we show that when source-lang
    
[^46]: 使用语言模型评估教育中教学质量的承诺与陷阱

    The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education

    [https://arxiv.org/abs/2404.02444](https://arxiv.org/abs/2404.02444)

    本研究利用NLP技术评估教育中多种高推理教学实践的质量，并且首次应用NLP来衡量对有特殊需求学生特别有效的教学实践。

    

    评估教学质量是教育系统改进努力的基本组成部分。然而，传统的手工评估昂贵、主观，并且严重依赖于观察者的专业知识和特殊因素，这些因素阻碍了教师及时获得频繁反馈。与之前主要关注单一低推理教学实践的研究不同，本文首次展示了利用自然语言处理（NLP）技术评估两种不同教育环境下的多种高推理教学实践的研究：面对面K-12教室和预服务教师的模拟表现任务。这也是首个应用NLP来评估被广泛认为对有特殊需求学生特别有效的教学实践的研究。我们面临NLP模型教学分析中的两个挑战，包括

    arXiv:2404.02444v1 Announce Type: cross  Abstract: Assessing instruction quality is a fundamental component of any improvement efforts in the education system. However, traditional manual assessments are expensive, subjective, and heavily dependent on observers' expertise and idiosyncratic factors, preventing teachers from getting timely and frequent feedback. Different from prior research that mostly focuses on low-inference instructional practices on a singular basis, this paper presents the first study that leverages Natural Language Processing (NLP) techniques to assess multiple high-inference instructional practices in two distinct educational settings: in-person K-12 classrooms and simulated performance tasks for pre-service teachers. This is also the first study that applies NLP to measure a teaching practice that is widely acknowledged to be particularly effective for students with special needs. We confront two challenges inherent in NLP-based instructional analysis, including
    
[^47]: 从叙述到数字：利用口述验尸叙述的语言模型预测进行有效推断

    From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives

    [https://arxiv.org/abs/2404.02438](https://arxiv.org/abs/2404.02438)

    该论文提出了一种利用最先进的NLP技术从口述验尸文本中预测死因并进行有效推断的方法。

    

    在大部分死亡事件发生在医疗系统外的场景中，口述验尸（VAs）是监测死因趋势的常用工具。VAs是与幸存的照料者或亲属进行的访谈，用于预测逝者的死因。将VAs转化为研究人员和决策者可行的见解需要两个步骤：（i）使用VA访谈预测可能的死因，（ii）使用预测的死因进行推断（例如，使用死亡样本对死因按人口统计因素分解的建模）。在本文中，我们开发了一种利用最先进的NLP技术从自由文本预测结果（在我们的案例中为死因）进行有效推断的方法。我们将这种方法称为multiPPI++，将最近的“预测驱动推断”工作扩展到多项分类。我们利用一系列NLP技术进行COD预测，并通过对VA数据的实证分析，展示了我们方法的有效性。

    arXiv:2404.02438v1 Announce Type: new  Abstract: In settings where most deaths occur outside the healthcare system, verbal autopsies (VAs) are a common tool to monitor trends in causes of death (COD). VAs are interviews with a surviving caregiver or relative that are used to predict the decedent's COD. Turning VAs into actionable insights for researchers and policymakers requires two steps (i) predicting likely COD using the VA interview and (ii) performing inference with predicted CODs (e.g. modeling the breakdown of causes by demographic factors using a sample of deaths). In this paper, we develop a method for valid inference using outcomes (in our case COD) predicted from free-form text using state-of-the-art NLP techniques. This method, which we call multiPPI++, extends recent work in "prediction-powered inference" to multinomial classification. We leverage a suite of NLP techniques for COD prediction and, through empirical analysis of VA data, demonstrate the effectiveness of our 
    
[^48]: 关于基于解码器的预训练语言模型的多语言能力：寻找和控制语言特定神经元

    On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons

    [https://arxiv.org/abs/2404.02431](https://arxiv.org/abs/2404.02431)

    该研究分析了基于解码器的预训练语言模型在多语言处理中的神经元级内部行为，发现模型中存在特定于每种语言的神经元，干扰这些语言特定的神经元会显著改变文本生成中目标语言出现的概率。

    

    当前的基于解码器的预训练语言模型（PLMs）成功展示了多语言能力。然而，这些模型如何处理多语言仍不清楚。我们分析了多语言基于解码器的PLMs的神经元级内部行为，具体检查了在仅解码器的多语言PLMs中是否存在为“每种语言独特激活”的神经元。我们分析了六种语言：英语、德语、法语、西班牙语、中文和日语，并表明语言特定的神经元是独特的，在语言之间有轻微的重叠（<5%）。这些神经元主要分布在模型的前几层和最后几层。这一趋势在不同语言和模型中保持一致。此外，我们在推断过程中干扰了每个模型少于1％的神经元，并证明干扰少量语言特定神经元会显著改变文本生成中目标语言出现的概率。

    arXiv:2404.02431v1 Announce Type: new  Abstract: Current decoder-based pre-trained language models (PLMs) successfully demonstrate multilingual capabilities. However, it is unclear how these models handle multilingualism. We analyze the neuron-level internal behavior of multilingual decoder-based PLMs, Specifically examining the existence of neurons that fire ``uniquely for each language'' within decoder-only multilingual PLMs. We analyze six languages: English, German, French, Spanish, Chinese, and Japanese, and show that language-specific neurons are unique, with a slight overlap (< 5%) between languages. These neurons are mainly distributed in the models' first and last few layers. This trend remains consistent across languages and models. Additionally, we tamper with less than 1% of the total neurons in each model during inference and demonstrate that tampering with a few language-specific neurons drastically changes the probability of target language occurrence in text generation.
    
[^49]: 使用PEFT和合成数据增强低资源LLMs分类

    Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data

    [https://arxiv.org/abs/2404.02422](https://arxiv.org/abs/2404.02422)

    提出了一种策略，通过PEFT和合成数据增强低资源LLMs分类器，实现了与0-shot文本分类器相媲美或更好的准确性。

    

    大语言模型在0-shot或few-shot设置下，在文本分类任务中取得竞争性成果。在上下文学习(ICL)中，通常比0-shot设置获得更好的准确性，但这是以效率为代价的，因为需要更长的输入提示。本文提出了一种策略，可以使LLMs像0-shot文本分类器一样高效，同时获得与ICL相当或更好的准确性。我们的解决方案针对资源稀缺的情况，即每类只有4个示例可用。使用单个LLM和少量真实数据，我们执行一系列生成、过滤和参数高效微调步骤，从而创建一个强大而高效的分类器。实验结果表明，我们的方法在多个文本分类数据集上取得了竞争性结果。

    arXiv:2404.02422v1 Announce Type: new  Abstract: Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.
    
[^50]: 重新审视亚字词标记：基于大型语言模型中前缀否定的案例研究

    Revisiting subword tokenization: A case study on affixal negation in large language models

    [https://arxiv.org/abs/2404.02421](https://arxiv.org/abs/2404.02421)

    研究衡量了前缀否定对大型语言模型的影响，发现尽管存在一些不匹配，模型整体上能够可靠地识别前缀否定的含义。

    

    在这项工作中，我们衡量了前缀否定对现代英文大型语言模型（LLMs）的影响。在前缀否定中，否定的含义通过一个负面形态素来表达，这对LLMs可能具有挑战性，因为它们的标记器通常不具备形态学上的合理性。我们使用具有不同亚字词标记方法的LLMs进行了广泛实验，这些实验为我们提供了关于标记性能与否定敏感性之间交互作用的几点见解。尽管在标记准确性和否定检测性能之间存在一些有趣的不匹配，但我们表明模型整体上可以可靠地识别前缀否定的含义。

    arXiv:2404.02421v1 Announce Type: new  Abstract: In this work, we measure the impact of affixal negation on modern English large language models (LLMs). In affixal negation, the negated meaning is expressed through a negative morpheme, which is potentially challenging for LLMs as their tokenizers are often not morphologically plausible. We conduct extensive experiments using LLMs with different subword tokenization methods, which lead to several insights on the interaction between tokenization performance and negation sensitivity. Despite some interesting mismatches between tokenization accuracy and negation detection performance, we show that models can, on the whole, reliably recognize the meaning of affixal negation.
    
[^51]: 辅助任务需求掩盖了较小语言模型的能力

    Auxiliary task demands mask the capabilities of smaller language models

    [https://arxiv.org/abs/2404.02418](https://arxiv.org/abs/2404.02418)

    较小语言模型对类比推理、反思推理、单词预测和语法判断的表现受辅助任务需求的影响，评估方法的任务需求越大，性能越低，这种"需求差距"在参数较少、训练数据较少的模型中尤为显著

    

    发展心理学家们对认知能力如语言理解或心灵理论何时出现进行了争论。这些辩论常常关注"任务需求"的概念--执行特定评估时所伴随的辅助挑战--这些挑战可能掩盖了儿童的潜在能力。当衡量语言模型（LMs）的能力时，同样的问题也会出现：任务表现取决于模型的基本能力，结合了模型解释和执行任务的能力以及其可用资源。在这里，我们展示了对于类比推理、反思推理、单词预测和语法判断，具有更大任务需求的评估方法会比降低需求的评估得到更低的性能。这种"需求差距"在参数较少、训练数据较少的模型中最为显著。我们的结果表明，LM的性能不应被解释为

    arXiv:2404.02418v1 Announce Type: cross  Abstract: Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of "task demands" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying competence, combined with the model's ability to interpret and perform the task given its available resources. Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This "demand gap" is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpret
    
[^52]: CMULAB：一个用于训练和部署自然语言处理模型的开源框架

    CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models

    [https://arxiv.org/abs/2404.02408](https://arxiv.org/abs/2404.02408)

    CMULAB 是一个开源框架，简化了自然语言处理模型的部署和持续人机协作微调，使用户能够快速对语音识别、OCR、翻译和句法分析等工具进行适应和扩展至新语言，即使训练数据有限

    

    使用自然语言处理（NLP）工具来有效地处理资源匮乏的语言需要对语言本身有深入了解，熟悉最新的模型和训练方法，以及具备部署这些模型的技术专长。这可能对语言社区成员和语言学家使用NLP工具构成重大障碍。本文介绍了CMU语言注释后端，这是一个开源框架，简化了模型部署和持续的人机协作对NLP模型进行微调的过程。CMULAB使用户能够利用多语言模型的强大功能，快速对语音识别、OCR、翻译和句法分析等工具进行适应和扩展至新语言，即使训练数据有限。我们描述了目前可用的各种工具和API以及开发人员如何轻松地向框架添加新模型/功能。代码可在https://github.com/neula找到

    arXiv:2404.02408v1 Announce Type: new  Abstract: Effectively using Natural Language Processing (NLP) tools in under-resourced languages requires a thorough understanding of the language itself, familiarity with the latest models and training methodologies, and technical expertise to deploy these models. This could present a significant obstacle for language community members and linguists to use NLP tools. This paper introduces the CMU Linguistic Annotation Backend, an open-source framework that simplifies model deployment and continuous human-in-the-loop fine-tuning of NLP models. CMULAB enables users to leverage the power of multilingual models to quickly adapt and extend existing tools for speech recognition, OCR, translation, and syntactic analysis to new languages, even with limited training data. We describe various tools and APIs that are currently available and how developers can easily add new models/functionality to the framework. Code is available at https://github.com/neula
    
[^53]: 探讨聊天模型的后门漏洞

    Exploring Backdoor Vulnerabilities of Chat Models

    [https://arxiv.org/abs/2404.02406](https://arxiv.org/abs/2404.02406)

    聊天模型因为多轮交互格式的灵活性增加了对后门攻击的脆弱性，该论文揭示并实现了一种新颖的后门攻击方法

    

    最近的研究表明，大型语言模型（LLMs）容易受到一种称为后门攻击的安全威胁。当前对LLMs的后门研究主要集中在针对指令调整的LLMs，而忽略了另一种现实场景，即将LLMs在多轮对话数据上微调为聊天模型。聊天模型被广泛应用于各种实际场景，因此聊天模型的安全性值得越来越多的关注。不幸的是，我们指出，灵活的多轮交互格式增加了触发设计的灵活性，并增加了聊天模型对后门攻击的脆弱性。在这项工作中，我们揭示并实现了一种新颖的聊天模型后门攻击方法，通过将多个触发场景分布在用户输入中

    arXiv:2404.02406v1 Announce Type: cross  Abstract: Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on LLMs predominantly focus on instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs
    
[^54]: 评估波斯语大型语言模型：以 ChatGPT 为中心的初步研究

    Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT

    [https://arxiv.org/abs/2404.02403](https://arxiv.org/abs/2404.02403)

    评估了波斯语大型语言模型在不同任务上的表现，引入了推理任务方面的新基准测试，发现LLMs在推理任务中表现优异。

    

    本文探讨了大型语言模型（LLMs）在波斯语中的有效性。虽然ChatGPT和随后的LLMs在英语中表现出色，但它们在更低资源语言中的效率仍然是一个开放的问题。我们提出了对一系列波斯语任务进行全面基准测试研究的首次尝试。我们的主要关注点是在GPT-3.5-turbo上进行评估，但我们也包括了GPT-4和OpenChat-3.5以提供更全面的评估。我们的评估涵盖了一系列任务，包括经典、推理和基于知识的领域。为了进行深入比较，我们评估了LLMs与现有任务特定的微调模型的性能。考虑到推理任务波斯语数据集的有限性，我们引入了两个新的基准测试：一个基于小学数学问题，另一个源自第7和第10年级入学考试。我们的研究结果显示，LLMs，尤其是GPT-4，在推理任务表现出色。

    arXiv:2404.02403v1 Announce Type: new  Abstract: This paper explores the efficacy of large language models (LLMs) for Persian. While ChatGPT and consequent LLMs have shown remarkable performance in English, their efficiency for more low-resource languages remains an open question. We present the first comprehensive benchmarking study of LLMs across diverse Persian language tasks. Our primary focus is on GPT-3.5-turbo, but we also include GPT-4 and OpenChat-3.5 to provide a more holistic evaluation. Our assessment encompasses a diverse set of tasks categorized into classic, reasoning, and knowledge-based domains. To enable a thorough comparison, we evaluate LLMs against existing task-specific fine-tuned models. Given the limited availability of Persian datasets for reasoning tasks, we introduce two new benchmarks: one based on elementary school math questions and another derived from the entrance exams for 7th and 10th grades. Our findings reveal that while LLMs, especially GPT-4, excel
    
[^55]: 使用ChatLLM在对话AI中导航语境深度的Token Trails

    Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM

    [https://arxiv.org/abs/2404.02402](https://arxiv.org/abs/2404.02402)

    Token Trails是一种利用token-type嵌入导航对话中复杂上下文细微差别的新方法，通过提高对话理解和回复生成效果，在促进上下文意识聊天机器人交互方面具有前沿性能。

    

    使用大型语言模型(LLMs)进行对话建模需要对上下文进行细致理解，以生成连贯且与上下文相关的回复。本文提出了Token Trails，这是一种利用token-type嵌入来导航对话中复杂上下文细微差别的新方法。我们的框架利用token-type嵌入来区分用户话语和机器人回复，从而促进生成具有上下文意识的回复。通过全面的实验和评估，我们展示了Token Trails在提高对话理解和回复生成方面的有效性，实现了最先进的性能。我们的结果突显了对话AI中上下文建模的重要性，并强调了Token Trails在推动该领域发展方面的潜力，为更复杂和具有上下文意识的聊天机器人交互铺平了道路。

    arXiv:2404.02402v1 Announce Type: cross  Abstract: Conversational modeling using Large Language Models (LLMs) requires a nuanced understanding of context to generate coherent and contextually relevant responses. In this paper, we present Token Trails, a novel approach that leverages token-type embeddings to navigate the intricate contextual nuances within conversations. Our framework utilizes token-type embeddings to distinguish between user utterances and bot responses, facilitating the generation of context-aware replies. Through comprehensive experimentation and evaluation, we demonstrate the effectiveness of Token Trails in improving conversational understanding and response generation, achieving state-of-the-art performance. Our results highlight the significance of contextual modeling in conversational AI and underscore the promising potential of Token Trails to advance the field, paving the way for more sophisticated and contextually aware chatbot interactions.
    
[^56]: 多语言机器翻译的后门攻击

    Backdoor Attack on Multilingual Machine Translation

    [https://arxiv.org/abs/2404.02393](https://arxiv.org/abs/2404.02393)

    多语言机器翻译系统容易受到后门攻击，攻击者通过向低资源语言对注入有毒数据来造成恶意翻译，引起高资源语言受攻击，揭示注入少量有毒数据即可实现成功攻击高资源语言对的风险。

    

    虽然多语言机器翻译（MNMT）系统具有巨大的潜力，但它们也存在安全漏洞。我们的研究突出显示，MNMT系统容易受到一种特别狡诈的后门攻击，即攻击者向低资源语言对注入有毒数据，以在其他语言（包括高资源语言）中造成恶意翻译。我们的实验结果显示，将少于0.01％的有毒数据注入到低资源语言对中，就可以在攻击高资源语言对时实现平均20％的攻击成功率。这种类型的攻击尤其令人担忧，因为低资源环境下固有的语言更容易受到攻击。我们的目标是引起关于这些MNMT系统中的漏洞的关注，并希望激励社区解决机器翻译中的安全问题，特别是在低资源语言环境下。

    arXiv:2404.02393v1 Announce Type: new  Abstract: While multilingual machine translation (MNMT) systems hold substantial promise, they also have security vulnerabilities. Our research highlights that MNMT systems can be susceptible to a particularly devious style of backdoor attack, whereby an attacker injects poisoned data into a low-resource language pair to cause malicious translations in other languages, including high-resource languages. Our experimental results reveal that injecting less than 0.01% poisoned data into a low-resource language pair can achieve an average 20% attack success rate in attacking high-resource language pairs. This type of attack is of particular concern, given the larger attack surface of languages inherent to low-resource settings. Our aim is to bring attention to these vulnerabilities within MNMT systems with the hope of encouraging the community to address security concerns in machine translation, especially in the context of low-resource languages.
    
[^57]: 具有形态建模的低资源神经机器翻译

    Low-resource neural machine translation with morphological modeling

    [https://arxiv.org/abs/2404.02392](https://arxiv.org/abs/2404.02392)

    该论文提出了一种在低资源环境中建模复杂形态的神经机器翻译方法，通过两级变压器架构编码形态信息，并利用多任务多标签训练方案和基于beam search的解码器来提高翻译性能。

    

    在神经机器翻译（NMT）中进行形态建模是实现形态丰富语言开放词汇机器翻译的一种有前景的方法。然而，现有的方法如子词标记化和基于字符的模型局限于单词的表面形式。在这项工作中，我们提出了一个用于在低资源环境中建模复杂形态的框架解决方案。选择了一个两级变压器架构来对输入的形态信息进行编码。在目标端输出时，多任务多标签训练方案结合基于beam search的解码器被发现可以提高机器翻译性能。提出了一种变压器模型的注意力增强方案，以通用形式允许集成预训练语言模型，并促进源语言和目标语言之间的单词顺序关系建模。评估了几种数据增强技术。

    arXiv:2404.02392v1 Announce Type: new  Abstract: Morphological modeling in neural machine translation (NMT) is a promising approach to achieving open-vocabulary machine translation for morphologically-rich languages. However, existing methods such as sub-word tokenization and character-based models are limited to the surface forms of the words. In this work, we propose a framework-solution for modeling complex morphology in low-resource settings. A two-tier transformer architecture is chosen to encode morphological information at the inputs. At the target-side output, a multi-task multi-label training scheme coupled with a beam search-based decoder are found to improve machine translation performance. An attention augmentation scheme to the transformer model is proposed in a generic form to allow integration of pre-trained language models and also facilitate modeling of word order relationships between the source and target languages. Several data augmentation techniques are evaluated 
    
[^58]: 在编码器-解码器语言模型中线性化结构化数据：来自文本到SQL的启示

    On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL

    [https://arxiv.org/abs/2404.02389](https://arxiv.org/abs/2404.02389)

    本研究调查了编码器-解码器语言模型中线性处理结构化数据的方法，发现模型能够模仿人类设计的流程，学习结构的深刻含义，揭示了模型内部机制的一些见解。

    

    结构化数据在表格、数据库和知识图中广泛存在，在其表示方面存在重大挑战。 随着大型语言模型（LLMs）的出现，人们开始转向基于线性化的方法，该方法将结构化数据处理为顺序标记流，而不是作为图形明确地建模的方法。 本文探讨了编码器-解码器语言模型（特别是T5）中对结构化数据进行线性处理的情况。 我们的研究发现模型能够模仿人类设计的流程，比如模式链接和语法预测，表明模型对结构的深刻、有意义的学习远远超过简单的标记排序。 我们还发现了模型内部机制的见解，包括结构节点的以自我为中心的特性。

    arXiv:2404.02389v1 Announce Type: cross  Abstract: Structured data, prevalent in tables, databases, and knowledge graphs, poses a significant challenge in its representation. With the advent of large language models (LLMs), there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly model structure, often as a graph. Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear. This work investigates the linear handling of structured data in encoder-decoder language models, specifically T5. Our findings reveal the model's ability to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing. We also uncover insights into the model's internal mechanisms, including the ego-centric nature of structure nod
    
[^59]: 尼泊尔语和孟加拉语中的光学文本识别：一种基于Transformer的方法

    Optical Text Recognition in Nepali and Bengali: A Transformer-based Approach

    [https://arxiv.org/abs/2404.02375](https://arxiv.org/abs/2404.02375)

    该论文介绍了使用编码器-解码器transformer模型进行尼泊尔语和孟加拉语的文本识别研究，高精度识别两种语言文字，有望推动语言学的先进和易访问性研究

    

    低资源语言的OCR系统的研究与开发工作相对较新。 低资源语言在训练机器翻译系统或其他系统时可用的训练数据较少。 尽管有大量文本已经被数字化并在互联网上提供，但这些文本仍处于PDF和图像格式，这些格式并非立即可访问。 本文讨论了孟加拉语和尼泊尔语两种文字的文本识别；大约有3亿和4千万的孟加拉语和尼泊尔语使用者。 在这项研究中，使用编码器-解码器Transformer，开发了一个模型，并使用一系列光学文本图像（手写和印刷）对其有效性进行了评估。 结果表明，建议的技术与当前方法相符，并在识别孟加拉语和尼泊尔语文本方面具有高精度。 该研究为语言学的先进和易访问性研究铺平了道路

    arXiv:2404.02375v1 Announce Type: new  Abstract: Efforts on the research and development of OCR systems for Low-Resource Languages are relatively new. Low-resource languages have little training data available for training Machine Translation systems or other systems. Even though a vast amount of text has been digitized and made available on the internet the text is still in PDF and Image format, which are not instantly accessible. This paper discusses text recognition for two scripts: Bengali and Nepali; there are about 300 and 40 million Bengali and Nepali speakers respectively. In this study, using encoder-decoder transformers, a model was developed, and its efficacy was assessed using a collection of optical text images, both handwritten and printed. The results signify that the suggested technique corresponds with current approaches and achieves high precision in recognizing text in Bengali and Nepali. This study can pave the way for the advanced and accessible study of linguistic
    
[^60]: 模糊恶意软件检测：通过内存分析调查真实场景

    Obfuscated Malware Detection: Investigating Real-world Scenarios through Memory Analysis

    [https://arxiv.org/abs/2404.02372](https://arxiv.org/abs/2404.02372)

    通过内存分析，利用机器学习算法提出了一种简单且成本效益的模糊恶意软件检测系统，重点评估其在真实场景中的效果。

    

    在互联网和智能设备时代，恶意软件的检测对系统安全至关重要。恶意软件作者越来越多地使用模糊技术来规避先进的安全解决方案，使得检测和消除威胁变得具有挑战性。本研究提出了一种简单且具有成本效益的模糊恶意软件检测系统，通过内存转储分析，利用多样化的机器学习算法。该研究聚焦于CIC-MalMem-2022数据集，旨在模拟真实场景并评估基于内存的模糊恶意软件检测。我们评估了机器学习算法的有效性，如...

    arXiv:2404.02372v1 Announce Type: cross  Abstract: In the era of the internet and smart devices, the detection of malware has become crucial for system security. Malware authors increasingly employ obfuscation techniques to evade advanced security solutions, making it challenging to detect and eliminate threats. Obfuscated malware, adept at hiding itself, poses a significant risk to various platforms, including computers, mobile devices, and IoT devices. Conventional methods like heuristic-based or signature-based systems struggle against this type of malware, as it leaves no discernible traces on the system. In this research, we propose a simple and cost-effective obfuscated malware detection system through memory dump analysis, utilizing diverse machine-learning algorithms. The study focuses on the CIC-MalMem-2022 dataset, designed to simulate real-world scenarios and assess memory-based obfuscated malware detection. We evaluate the effectiveness of machine learning algorithms, such 
    
[^61]: 利用视觉和语言模型以及眼睛注视模式增强胸部X射线分析中的人机交互

    Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns

    [https://arxiv.org/abs/2404.02370](https://arxiv.org/abs/2404.02370)

    通过结合眼动数据和文本提示，利用Vision-Language Models（VLMs）增强胸部X射线分析中的人机交互，提高了放射科医师的关注度，有效增强了胸部X射线分析的准确性。

    

    最近计算机辅助诊断的进展在医学影像任务中表现出良好的性能，特别是在胸部X射线分析方面。然而，这些模型与放射科医师之间的互动主要仅限于输入图像。本文提出了一种新方法，通过将眼动数据与文本提示结合在一起，利用增强了放射科医师的关注的视觉语言模型（VLMs）来增强胸部X射线分析中的人机交互。我们的方法利用从眼动数据生成的热图，将它们叠加到医学图像上，以突出放射科医师在胸部X射线评估过程中关注强度较高的区域。我们在视觉问题回答、胸部X射线报告自动化、错误检测和鉴别诊断等任务中评估了这种方法。我们的结果表明，包含眼动信息显著提高了胸部X射线分析的准确性。

    arXiv:2404.02370v1 Announce Type: cross  Abstract: Recent advancements in Computer Assisted Diagnosis have shown promising performance in medical imaging tasks, particularly in chest X-ray analysis. However, the interaction between these models and radiologists has been primarily limited to input images. This work proposes a novel approach to enhance human-computer interaction in chest X-ray analysis using Vision-Language Models (VLMs) enhanced with radiologists' attention by incorporating eye gaze data alongside textual prompts. Our approach leverages heatmaps generated from eye gaze data, overlaying them onto medical images to highlight areas of intense radiologist's focus during chest X-ray evaluation. We evaluate this methodology in tasks such as visual question answering, chest X-ray report automation, error detection, and differential diagnosis. Our results demonstrate the inclusion of eye gaze information significantly enhances the accuracy of chest X-ray analysis. Also, the imp
    
[^62]: 两个好头胜过一个: 嵌套PoE用于强力防御多后门攻击

    Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors

    [https://arxiv.org/abs/2404.02356](https://arxiv.org/abs/2404.02356)

    提出了一种嵌套专家集成防御框架(NPoE)，可同时防御多种后门触发器类型，实验结果表明其在多个任务上的有效性。

    

    数据毒化后门攻击会导致大型语言模型（LLMs）产生不良行为，并且防御这些攻击变得越来越重要。现有的防御机制通常假定攻击者只采用一种触发器类型，而同时防御多种同时且独立的触发器类型则需要通用的防御框架，且相对较少探索。在本文中，我们提出了Nested Product of Experts(NPoE)防御框架，其中涉及将混合的专家模型（MoE）作为触发器集成到PoE防御框架中，以同时防御多种触发器类型。NPoE训练过程中，主模型与一组 smaller 专家模型集成训练，专家模型学习后门触发器的特征。在推断时，只使用主模型。在情感分析、仇恨言论检测和问题分类任务上的实验结果表明，NP

    arXiv:2404.02356v1 Announce Type: new  Abstract: Data poisoning backdoor attacks can cause undesirable behaviors in large language models (LLMs), and defending against them is of increasing importance. Existing defense mechanisms often assume that only one type of trigger is adopted by the attacker, while defending against multiple simultaneous and independent trigger types necessitates general defense frameworks and is relatively unexplored. In this paper, we propose Nested Product of Experts(NPoE) defense framework, which involves a mixture of experts (MoE) as a trigger-only ensemble within the PoE defense framework to simultaneously defend against multiple trigger types. During NPoE training, the main model is trained in an ensemble with a mixture of smaller expert models that learn the features of backdoor triggers. At inference time, only the main model is used. Experimental results on sentiment analysis, hate speech detection, and question classification tasks demonstrate that NP
    
[^63]: 歌词相似度感知的计算分析

    A Computational Analysis of Lyric Similarity Perception

    [https://arxiv.org/abs/2404.02342](https://arxiv.org/abs/2404.02342)

    该研究通过比较分析计算方法对模拟歌词相似度与人类感知的关联，发现基于BERT模型嵌入、歌词音频和音素组件相似性的计算模型对感知上的歌词相似度具有指示作用。

    

    在包含人声的音乐作品中，歌词对艺术表达起着重要作用。因此，先前的研究引入了推荐系统的概念，该系统建议类似于用户喜爱或个性化偏好的歌词，有助于在数百万音轨中发现歌词。然而，许多系统并未充分考虑人类对歌词相似度的感知，主要是由于该领域的研究有限。为弥补这一差距，我们进行了对计算方法建模歌词相似度与人类感知进行了比较分析。结果表明，基于预训练的BERT模型嵌入之间的相似性、歌词来源的音频以及音素组件的计算模型指示了感知上的歌词相似度。该发现强调了语义、风格和音韵相似性在人类感知中的重要性。

    arXiv:2404.02342v1 Announce Type: new  Abstract: In musical compositions that include vocals, lyrics significantly contribute to artistic expression. Consequently, previous studies have introduced the concept of a recommendation system that suggests lyrics similar to a user's favorites or personalized preferences, aiding in the discovery of lyrics among millions of tracks. However, many of these systems do not fully consider human perceptions of lyric similarity, primarily due to limited research in this area. To bridge this gap, we conducted a comparative analysis of computational methods for modeling lyric similarity with human perception. Results indicated that computational models based on similarities between embeddings from pre-trained BERT-based models, the audio from which the lyrics are derived, and phonetic components are indicative of perceptual lyric similarity. This finding underscores the importance of semantic, stylistic, and phonetic similarities in human perception abo
    
[^64]: 注解器建模与扩展的语料库考虑

    Corpus Considerations for Annotator Modeling and Scaling

    [https://arxiv.org/abs/2404.02340](https://arxiv.org/abs/2404.02340)

    在多样化表示技巧的注解器建模领域，对数据集的细粒度特征进行研究是至关重要的。

    

    自然语言处理研究和注释任务的最新趋势确认了从传统依赖单一“真相标签”转向关注个体视角，尤其是在主观任务中。在注释任务旨在包含多样性的情况下，仅依赖于多数类别标签的模型可能无意中忽视有价值的少数派观点。这种疏漏可能导致关键信息的遗漏，并在更广泛的背景下，可能扰乱更大生态系统中的平衡。随着注解器建模的多样性代表技术的出现，有必要研究它们与数据集的精细特征结合的有效性。本研究系统地探讨了各种注解器建模技术，并比较了它们在七个语料库中的性能。

    arXiv:2404.02340v1 Announce Type: new  Abstract: Recent trends in natural language processing research and annotation tasks affirm a paradigm shift from the traditional reliance on a single ground truth to a focus on individual perspectives, particularly in subjective tasks. In scenarios where annotation tasks are meant to encompass diversity, models that solely rely on the majority class labels may inadvertently disregard valuable minority perspectives. This oversight could result in the omission of crucial information and, in a broader context, risk disrupting the balance within larger ecosystems. As the landscape of annotator modeling unfolds with diverse representation techniques, it becomes imperative to investigate their effectiveness with the fine-grained features of the datasets in view. This study systematically explores various annotator modeling techniques and compares their performance across seven corpora.   From our findings, we show that the commonly used user token mode
    
[^65]: Multi-BERT：利用适配器和提示调整进行低资源多领域适应

    Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation

    [https://arxiv.org/abs/2404.02335](https://arxiv.org/abs/2404.02335)

    提出了一种新颖的方法，使用一个核心模型和多套领域特定参数，结合提示调整和适配器技术，以及额外层次来实现低资源多领域适应，使得模型能够完成与每个领域单独训练模型相媲美的任务。

    

    文本量和多样性的急剧扩展提出了多领域环境中的巨大挑战。这些挑战在波斯语命名实体识别（NER）设置中也很明显。传统方法，无论是使用一个统一模型来处理多个领域，还是为每个领域使用单独的模型，经常会出现显著的限制。单一模型通常难以捕捉各种领域的细微差别，而使用多个大型模型可能会导致资源限制，使为每个领域训练模型几乎不切实际。因此，本文介绍了一种由一个核心模型和多套领域特定参数组成的新颖方法。我们利用提示调整和适配器等技术，结合引入额外层，添加我们可以为特定领域训练的参数。这使得模型能够在性能上与为每个领域训练的单独模型相媲美。

    arXiv:2404.02335v1 Announce Type: cross  Abstract: The rapid expansion of texts' volume and diversity presents formidable challenges in multi-domain settings. These challenges are also visible in the Persian name entity recognition (NER) settings. Traditional approaches, either employing a unified model for multiple domains or individual models for each domain, frequently pose significant limitations. Single models often struggle to capture the nuances of diverse domains, while utilizing multiple large models can lead to resource constraints, rendering the training of a model for each domain virtually impractical. Therefore, this paper introduces a novel approach composed of one core model with multiple sets of domain-specific parameters. We utilize techniques such as prompt tuning and adapters, combined with the incorporation of additional layers, to add parameters that we can train for the specific domains. This enables the model to perform comparably to individual models for each do
    
[^66]: 大型语言模型下领域驱动术语提取的比较研究

    Comparative Study of Domain Driven Terms Extraction Using Large Language Models

    [https://arxiv.org/abs/2404.02330](https://arxiv.org/abs/2404.02330)

    本研究比较了使用大型语言模型进行领域驱动术语提取的方法，评估了Llama2-7B、GPT-3.5和Falcon-7B在Inspec和PubMed数据集上的性能。

    

    关键词在人类理解和机器处理文本数据之间架起重要桥梁，对于数据丰富至关重要，因为它们构成了详细注释的基础，提供了对基础数据更深入和全面的视角。关键词/领域驱动术语提取是自然语言处理中的关键任务，有助于信息检索、文档摘要和内容分类。本文重点关注关键词提取方法，强调三种主要的大型语言模型(LLMs)的使用：Llama2-7B、GPT-3.5和Falcon-7B。我们使用一个自定义的Python包与这些LLMs进行交互，简化了关键词提取过程。我们利用Inspec和PubMed数据集对这些模型的性能进行评估。评价使用Jaccard相似性指数，得到了GPT-3.5的评分分别为0.64（Inspec）和0.21（PubMed），Llama2-7B的评分分别为0.40和0.17。

    arXiv:2404.02330v1 Announce Type: cross  Abstract: Keywords play a crucial role in bridging the gap between human understanding and machine processing of textual data. They are essential to data enrichment because they form the basis for detailed annotations that provide a more insightful and in-depth view of the underlying data. Keyword/domain driven term extraction is a pivotal task in natural language processing, facilitating information retrieval, document summarization, and content categorization. This review focuses on keyword extraction methods, emphasizing the use of three major Large Language Models(LLMs): Llama2-7B, GPT-3.5, and Falcon-7B. We employed a custom Python package to interface with these LLMs, simplifying keyword extraction. Our study, utilizing the Inspec and PubMed datasets, evaluates the performance of these models. The Jaccard similarity index was used for assessment, yielding scores of 0.64 (Inspec) and 0.21 (PubMed) for GPT-3.5, 0.40 and 0.17 for Llama2-7B, a
    
[^67]: 迈向非正式语言处理：大型语言模型对俚语的认知

    Toward Informal Language Processing: Knowledge of Slang in Large Language Models

    [https://arxiv.org/abs/2404.02323](https://arxiv.org/abs/2404.02323)

    大型语言模型通过建立支持多任务评估的俚语数据集，有效实现了俚语检测和识别地区与历史俚语来源，并通过探索LLMs输出分布提供了解释性洞见。

    

    大型语言模型（LLMs）的最新进展为自然语言系统处理非正式语言提供了强大潜力。非正式语言的一个代表形式是俚语，在日常对话和在线社交媒体中常用。迄今为止，由于缺乏一个精心设计且可公开访问的基准，俚语在LLMs中尚未得到全面评估。我们利用电影字幕构建了一个数据集，支持在涉及俚语自动处理的多样化任务上进行评估。我们展示了我们的数据集在两个核心应用中（俚语检测和识别自然语句中俚语的地区和历史来源）的评估和微调的有效性。我们还展示了如何使用我们的数据集来探测LLMs的输出分布以获得解释性洞见。我们发现，尽管 GPT-4 等LLMs 在零次尝试方面表现良好，但在俚语处理方面仍有改进的空间。

    arXiv:2404.02323v1 Announce Type: new  Abstract: Recent advancement in large language models (LLMs) has offered a strong potential for natural language systems to process informal language. A representative form of informal language is slang, used commonly in daily conversations and online social media. To date, slang has not been comprehensively evaluated in LLMs due partly to the absence of a carefully designed and publicly accessible benchmark. Using movie subtitles, we construct a dataset that supports evaluation on a diverse set of tasks pertaining to automatic processing of slang. For both evaluation and finetuning, we show the effectiveness of our dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences. We also show how our dataset can be used to probe the output distributions of LLMs for interpretive insights. We find that while LLMs such as GPT-4 achieve good performance in a zero-shot setti
    
[^68]: Prompt作为程序：一种结构感知的高效编译时Prompt优化方法

    Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization

    [https://arxiv.org/abs/2404.02319](https://arxiv.org/abs/2404.02319)

    提出了SAMMO框架，用于在编译时优化元提示程序，提高了复杂提示在多种不同LLM上的性能。

    

    大型语言模型(LLMs)现在能处理更长更复杂的输入，这促进了更复杂提示的使用。然而，提示通常需要一些调整以提高部署性能。最近的工作提出了自动提示优化方法，但随着提示复杂度和LLM强度的增加，许多提示优化技术已不再足够，需要一种新的方法来优化元提示程序。为了解决这个问题，我们引入了SAMMO，一个用于元提示程序的{\em 编译时}优化的框架，它将提示表示为结构化对象，允许在优化过程中搜索一组丰富的转换。我们展示SAMMO推广了先前的方法，在指令调整、RAG管线调整和提示压缩方面提高了复杂提示在多种不同LLM上的性能。我们开放所有代码供大家使用。

    arXiv:2404.02319v1 Announce Type: cross  Abstract: Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts. However, prompts often require some tuning to improve performance for deployment. Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\em meta prompt programs}. To address this, we introduce SAMMO, a framework for {\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs.   We make all code available open-sou
    
[^69]: 自训练语言模型的崩溃

    Collapse of Self-trained Language Models

    [https://arxiv.org/abs/2404.02305](https://arxiv.org/abs/2404.02305)

    自我训练的语言模型在延长训练时表现出显著的性能下降，导致重复和崩溃的标记输出。

    

    在各个知识创造领域，包括科学，新思想往往建立在现有信息之上。在这项工作中，我们探讨了这个概念在语言模型的背景下的应用。具体而言，我们探讨了自我训练模型在其自身输出上的潜力，类似于人类学习并建立在他们以前的思想和行动上。虽然这种方法在直观上很有吸引力，但我们的研究揭示了它的实际局限性。我们发现对GPT-2模型进行长时间的自训练会导致性能显著下降，导致重复和崩溃的令牌输出。

    arXiv:2404.02305v1 Announce Type: cross  Abstract: In various fields of knowledge creation, including science, new ideas often build on pre-existing information. In this work, we explore this concept within the context of language models. Specifically, we explore the potential of self-training models on their own outputs, akin to how humans learn and build on their previous thoughts and actions. While this approach is intuitively appealing, our research reveals its practical limitations. We find that extended self-training of the GPT-2 model leads to a significant degradation in performance, resulting in repetitive and collapsed token output.
    
[^70]: 通过ChatGPT从合同中提取规范：机遇与挑战

    Extracting Norms from Contracts Via ChatGPT: Opportunities and Challenges

    [https://arxiv.org/abs/2404.02269](https://arxiv.org/abs/2404.02269)

    研究了ChatGPT在从合同中提取规范方面的有效性和局限性，展示了其良好表现但也发现了一些限制。

    

    我们调查了ChatGPT在从合同中提取规范方面的有效性。规范提供了一种自然的方式来通过捕捉如何管理两个或多个自治方之间的交互来设计多主体系统。我们从合同中提取承诺、禁止、授权和权力等规范，以及相关的规范要素（涉及的当事方、前因和后果）。我们的研究揭示了ChatGPT在从合同中提取规范方面的有效性和局限性。ChatGPT展示了在提取规范方面的良好表现，而无需进行训练或微调，因此无需标记数据，而这在该领域通常是不可用的。然而，我们发现了ChatGPT在提取这些规范方面的一些限制，导致了错误的规范提取。这些限制包括对关键细节的忽视、臆想、对连接词的错误解析以及空规范要素。

    arXiv:2404.02269v1 Announce Type: cross  Abstract: We investigate the effectiveness of ChatGPT in extracting norms from contracts. Norms provide a natural way to engineer multiagent systems by capturing how to govern the interactions between two or more autonomous parties. We extract norms of commitment, prohibition, authorization, and power, along with associated norm elements (the parties involved, antecedents, and consequents) from contracts. Our investigation reveals ChatGPT's effectiveness and limitations in norm extraction from contracts. ChatGPT demonstrates promising performance in norm extraction without requiring training or fine-tuning, thus obviating the need for annotated data, which is not generally available in this domain. However, we found some limitations of ChatGPT in extracting these norms that lead to incorrect norm extractions. The limitations include oversight of crucial details, hallucination, incorrect parsing of conjunctions, and empty norm elements. Enhanced 
    
[^71]: 在LLMs中循环：利用大型语言模型注释进行低资源语言的主动学习

    LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages

    [https://arxiv.org/abs/2404.02261](https://arxiv.org/abs/2404.02261)

    在低资源语言中，通过将LLMs集成到主动学习循环中进行数据注释，有效减少所需数据量，并取得接近最先进性能的结果。

    

    由于语言资源和数据标注专业知识有限，低资源语言在人工智能开发中面临着重大障碍，使它们变得罕见且成本高昂。为了解决这一不足，我们提出利用LLMs的潜力在主动学习环节中进行数据注释。我们首先进行评估以评估注释者之间的一致性，从而选择适当的LLM注释者。然后，选择的注释者被集成到一个分类器的训练循环中，使用主动学习范式，最小化所需的查询数据量。实证评估，特别是使用GPT-4-Turbo，展示了几乎达到最先进性能的结果，同时大大减少了数据需求，由估算的潜在性能指示。

    arXiv:2404.02261v1 Announce Type: cross  Abstract: Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential co
    
[^72]: Mixture-of-Depths: 在基于Transformer的语言模型中动态分配计算

    Mixture-of-Depths: Dynamically allocating compute in transformer-based language models

    [https://arxiv.org/abs/2404.02258](https://arxiv.org/abs/2404.02258)

    本研究提出了一种新的方法，即Mixture-of-Depths，可以在Transformer的语言模型中动态分配FLOPs以优化模型深度上不同层的序列分配。

    

    基于Transformer的语言模型通常会将FLOPs均匀分布在输入序列中。本文展示了transformers可以学习动态地将FLOPs（或计算）分配给序列中的特定位置，优化各模型层深度上的序列分配。我们的方法通过设定在给定层中可参与自注意力和MLP计算的令牌数（$k$）来实施总计算预算。要处理的令牌由网络使用top-$k$路由机制确定。由于$k$是事先定义的，这种简单的过程使用具有已知张量大小的静态计算图，不同于其他条件计算技术。然而，由于$k$令牌的标识是不固定的，该方法可以非均匀地跨时间和模型深度维度分配FLOPs。因此，总体而言，计算支出完全可预测，但d

    arXiv:2404.02258v1 Announce Type: cross  Abstract: Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but d
    
[^73]: $\texttt{LM}^\texttt{2}$: 一种简单的语言模型协同解决复杂推理问题

    $\texttt{LM}^\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning

    [https://arxiv.org/abs/2404.02255](https://arxiv.org/abs/2404.02255)

    LM2提出了一种简单的语言模型$\texttt{LM}^\texttt{2}$，该模型将分解、解决和验证模块化，通过分解器识别关键概念并生成逐步子问题，从而协同解决复杂推理问题。

    

    尽管展示了出现推理能力，但大型语言模型(LLMS)经常会在复杂的、多步骤的推理中迷失方向。现有研究表明，通过将原始问题分解为多个子问题来提供指导，可以引发LLM推理的更强健性——一个分解器生成子问题，一个解算器解决每个这些子问题。然而，这些技术未能适应分解器和解算器模块之间的协调(无论是在单一模型中还是在不同的专门模型中)——分解器没有跟踪解算器按照分解的推理。在本文中，我们提出了LM2来应对这些挑战。LM2将分解、解决和验证模块化为三种不同的语言模型。分解器模块识别解决问题所需的关键概念，并根据推理要求生成逐步子问题。

    arXiv:2404.02255v1 Announce Type: cross  Abstract: Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in LLM reasoning -- a decomposer generates the subproblems, and a solver solves each of these subproblems. However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) -- the decomposer does not keep track of the ability of the solver to follow the decomposed reasoning. In this paper, we propose LM2 to address these challenges. LM2 modularizes the decomposition, solution, and verification into three different language models. The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the reasoning requ
    
[^74]: 减小规模生成语言模型中的新能力

    Emergent Abilities in Reduced-Scale Generative Language Models

    [https://arxiv.org/abs/2404.02204](https://arxiv.org/abs/2404.02204)

    减小规模数据训练的较小语言模型展示了增强的零样本能力，可在简化语言中实现与大型模型相当的性能。

    

    大语言模型可以在不需要特定任务微调的情况下解决新任务。这种能力，也被称为上下文学习（ICL），被认为是一种新兴能力，主要出现在拥有数十亿参数的大语言模型中。本研究探讨了这种新兴属性是否严格与模型大小相关，或者可以通过在减小规模数据上训练的较小模型来展示。为了探索这一点，我们简化了预训练数据，对36个因果语言模型进行了预训练，参数从100万到1.65亿不等。我们展示了在这种简化的预训练数据上训练的模型在简化语言中表现出增强的零样本能力，实现了与在自由语言上六倍大的预训练模型相当的性能。这表明，缩小语言规模可以使具有有限大小的模型出现零样本学习能力。此外，我们f

    arXiv:2404.02204v1 Announce Type: new  Abstract: Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study investigates if such emergent properties are strictly tied to model size or can be demonstrated by smaller models trained on reduced-scale data. To explore this, we simplify pre-training data and pre-train 36 causal language models with parameters varying from 1 million to 165 million parameters. We show that models trained on this simplified pre-training data demonstrate enhanced zero-shot capabilities across various tasks in simplified language, achieving performance comparable to that of pre-trained models six times larger on unrestricted language. This suggests that downscaling the language allows zero-shot learning capabilities to emerge in models with limited size. Additionally, we f
    
[^75]: 自组织代理：面向超大规模代码生成和优化的LLM多代理框架

    Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization

    [https://arxiv.org/abs/2404.02183](https://arxiv.org/abs/2404.02183)

    提出了自组织多代理框架（SoA），实现了大规模代码的可扩展高效生成和优化，代理可自主运作生成和修改代码组件，并根据问题复杂性动态增加数量。

    

    最近，在使用大型语言模型（LLM）代理进行自动代码生成方面取得了突破性进展，使我们更接近自动软件开发的未来。然而，现有的单代理方法在生成和改进大规模复杂代码库方面存在局限，这是由于上下文长度的限制所导致的。为了解决这一挑战，我们提出了自组织多代理框架（SoA），这是一种新颖的多代理框架，可以实现大规模代码的可扩展高效生成和优化。在SoA中，自组织代理独立运作，以生成和修改代码组件，同时无缝协作构建整体代码库。我们框架的一个关键特点是基于问题复杂性自动增加代理的数量，从而实现动态扩展性。这使得整体代码量可以根据代理数量无限增加，同时由每个代理管理的代码量也随之增加。

    arXiv:2404.02183v1 Announce Type: cross  Abstract: Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by eac
    
[^76]: 打破沉默：检测和缓解印地语、泰米尔语和印度英语在线空间中的性别虐待

    Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces

    [https://arxiv.org/abs/2404.02013](https://arxiv.org/abs/2404.02013)

    在该研究中，我们开发了一种结合了CNN和BiLSTM网络的集成方法，有效模拟了文本数据中的语义和顺序模式，用于检测和缓解印地语、泰米尔语和印度英语在线空间中的性别虐待。

    

    在线性别骚扰是一个广泛存在的问题，限制了女性和边缘性别在数字空间中的自由表达和参与。检测这类滥用内容可以帮助平台遏制这一祸害。我们参加了 ICON2023 的Indic语言中的性别虐待检测的共享任务，该任务提供了在英语、印地语和泰米尔语中进行分类以识别性别虐待的Twitter帖子的标注数据集。我们的团队CNLP-NITS-PP开发了一个结合了CNN和BiLSTM网络的集成方法，可以有效地对文本数据中的语义和顺序模式进行建模。卷积滤波器在嵌入输入文本上应用，CNN捕获指示滥用语言的局部特征。为了确定基于上下文的冒犯性，BiLSTM分析这个序列以了解单词和短语之间的依赖关系。为每种语言使用了FastText和GloVe词嵌入进行了多个变体的训练。

    arXiv:2404.02013v1 Announce Type: new  Abstract: Online gender-based harassment is a widespread issue limiting the free expression and participation of women and marginalized genders in digital spaces. Detecting such abusive content can enable platforms to curb this menace. We participated in the Gendered Abuse Detection in Indic Languages shared task at ICON2023 that provided datasets of annotated Twitter posts in English, Hindi and Tamil for building classifiers to identify gendered abuse. Our team CNLP-NITS-PP developed an ensemble approach combining CNN and BiLSTM networks that can effectively model semantic and sequential patterns in textual data. The CNN captures localized features indicative of abusive language through its convolution filters applied on embedded input text. To determine context-based offensiveness, the BiLSTM analyzes this sequence for dependencies among words and phrases. Multiple variations were trained using FastText and GloVe word embeddings for each languag
    
[^77]: Octopus v2：用于超级代理的设备上语言模型

    Octopus v2: On-device language model for super agent

    [https://arxiv.org/abs/2404.01744](https://arxiv.org/abs/2404.01744)

    该研究提出了一种新方法，通过将具有20亿参数的设备上模型赋予超越GPT-4的准确性和延迟性能，并将上下文长度缩减95％，从而解决了函数调用中的延迟和准确性问题。

    

    语言模型在各种软件应用中展现出了高效性，特别是与自动工作流相关的任务。这些模型具有调用函数的关键能力，在创建AI代理时至关重要。尽管大规模语言模型在云环境中表现出色，但往往存在着隐私和成本方面的担忧。当前用于函数调用的设备上模型面临延迟和准确性问题。我们的研究提出了一种新方法，使具有20亿参数的设备上模型在准确性和延迟方面超越了GPT-4，并将上下文长度缩减了95%。与基于RAG的函数调用机制的Llama-7B相比，我们的方法将延迟提高了35倍。这种方法将延迟降低到适合在生产环境中的各种边缘设备上部署的水平上，符合性能要求。

    arXiv:2404.01744v1 Announce Type: new  Abstract: Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisite
    
[^78]: 通过归结反驳实现自然语言通用且可靠的逻辑推理

    Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation

    [https://arxiv.org/abs/2404.01677](https://arxiv.org/abs/2404.01677)

    通过引入归结反驳范式，提出了一个名为GFaiR的框架，旨在解决大型语言模型在进行自然语言形式逻辑理论一阶逻辑推理时的困难，并通过证明插入解决方案改进了系统的完整性

    

    大型语言模型在各种自然语言推理任务中取得了显著的性能，但它们在对自然语言表达的形式逻辑理论进行一阶逻辑推理方面仍然有困难。为了解决这一问题，我们提出了一个名为“可泛化且可靠推理器（GFaiR）”的新框架，引入了归结反驳范式。实验结果表明，我们的系统...

    arXiv:2404.01677v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks. However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language. This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue. As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability. To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation. Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system's completeness can be improved by introducing resolution refutation. Experimental results demonstrate that our system o
    
[^79]: ChatGLM-RLHF：将大型语言模型与人类反馈对齐的实践

    ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback

    [https://arxiv.org/abs/2404.00934](https://arxiv.org/abs/2404.00934)

    ChatGLM-RLHF是一种从人类反馈中强化学习系统，通过收集人类偏好数据、训练奖励模型和优化策略等方法来增强大型语言模型ChatGLM与人类偏好的对齐性，在实验中显示出显著的改进。

    

    arXiv:2404.00934v1 公告类型：新的 摘要：ChatGLM是由大型语言模型（LLMs）家族提供支持的免费人工智能服务。本文介绍了ChatGLM-RLHF流水线--一种从人类反馈中强化学习（RLHF）系统--旨在增强ChatGLM与人类偏好的对齐性。ChatGLM-RLHF包含三个主要组成部分：人类偏好数据的收集，奖励模型的训练，以及策略的优化。在将ChatGLM-RLHF整合到生产环境的过程中，我们遇到并解决了一些前所未有的挑战。我们引入了减少奖励方差以实现稳定的大规模训练的策略，实现了带有融合梯度下降的模型并行性，并设计了正则化约束以避免LLMs中的灾难性遗忘。实验表明，与ChatGLM的受监督微调（SFT）版本相比，ChatGLM-RLHF在对齐任务中带来了显著的改进。

    arXiv:2404.00934v1 Announce Type: new  Abstract: ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. Fo
    
[^80]: 保密者：LLM对个人特征语言标记的影响

    Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits

    [https://arxiv.org/abs/2404.00267](https://arxiv.org/abs/2404.00267)

    LLMs对作者的语言模式的影响略微降低其个人特征的预测能力，但显著变化不太频繁。

    

    先前的研究已经确定了个体语言使用与其个人特征之间的关联；我们的语言模式揭示了关于我们个性、情绪状态和信念的信息。然而，随着大型语言模型(LLMs)在日常写作中作为写作助手被越来越广泛地采用，一个关键问题出现了：当LLMs参与写作过程时，作者的语言模式是否仍然能预测其个人特征？我们研究了LLMs对人口统计特征和心理特征的语言标记的影响，具体检查了三个LLMs - GPT3.5、Llama 2和Gemini - 在六种不同特征上：性别、年龄、政治立场、个性、移情能力和道德。我们的发现表明，虽然使用LLMs略微降低了语言模式对作者个人特征的预测能力，但显著变化不太频繁，LLMs的使用并不

    arXiv:2404.00267v1 Announce Type: new  Abstract: Prior research has established associations between individuals' language usage and their personal traits; our linguistic patterns reveal information about our personalities, emotional states, and beliefs. However, with the increasing adoption of Large Language Models (LLMs) as writing assistants in everyday writing, a critical question emerges: are authors' linguistic patterns still predictive of their personal traits when LLMs are involved in the writing process? We investigate the impact of LLMs on the linguistic markers of demographic and psychological traits, specifically examining three LLMs - GPT3.5, Llama 2, and Gemini - across six different traits: gender, age, political affiliation, personality, empathy, and morality. Our findings indicate that although the use of LLMs slightly reduces the predictive power of linguistic patterns over authors' personal traits, the significant changes are infrequent, and the use of LLMs does not 
    
[^81]: 通过监督微调将新知识注入大型语言模型

    Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning

    [https://arxiv.org/abs/2404.00213](https://arxiv.org/abs/2404.00213)

    本论文研究了在大型语言模型中通过监督微调方法注入新知识的效果，特别关注了最近体育事件领域。

    

    近年来，大型语言模型（LLMs）在生成类似人类文本方面表现出色，被证明在各种应用中是一项宝贵的资产。然而，使这些模型适应并整合新的领域知识仍然是一项挑战，特别是针对模型知识截止日期之后发生的事实和事件。本文研究了监督微调（SFT）作为LLMs中注入知识的方法的有效性，特别关注了最近体育事件领域。

    arXiv:2404.00213v1 Announce Type: new  Abstract: In recent years, Large Language Models (LLMs) have shown remarkable performance in generating human-like text, proving to be a valuable asset across various applications. However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date. This paper investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs, specifically focusing on the domain of recent sporting events. We compare different dataset generation strategies -- token-based and fact-based scaling -- to create training data that helps the model learn new information. Our experiments on GPT-4 demonstrate that while token-based scaling can lead to improvements in Q&A accuracy, it may not provide uniform coverage of new knowledge. Fact-based scaling, on the other hand, offers a more systematic approach to ensure even cove
    
[^82]: IndiBias：一个用于衡量印度语境下语言模型社会偏见的基准数据集

    IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context

    [https://arxiv.org/abs/2403.20147](https://arxiv.org/abs/2403.20147)

    IndiBias是一个为评估印度语境中社会偏见而设计的综合基准数据集，通过过滤和翻译现有数据集以及利用不同LLMs的方法，涵盖了印度中流行的各种社会偏见维度。

    

    在语言数据中普遍存在的社会偏见影响引发了对捕捉和评估大型语言模型（LLMs）中这些偏见的基准数据集的需求。现有工作主要集中在英语和西方背景，缺乏一个可靠的数据集，能够体现印度独特的社会文化细微差别。为了弥补这一空白，我们引入了IndiBias，这是一个专门设计用于评估印度语境中社会偏见的全面基准数据集。我们过滤和翻译现有的CrowS-Pairs数据集，创建了一个适合印度语境中使用的基准数据集，使用印地语。此外，我们利用包括ChatGPT和InstructGPT在内的LLMs，以印度流行的各种社会偏见和刻板印象增强我们的数据集。包含的偏见维度涵盖性别、宗教、种姓、年龄、地区、外貌和职业。我们还建立了一个资源来解决交叉

    arXiv:2403.20147v1 Announce Type: new  Abstract: The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs). Existing efforts predominantly focus on English language and the Western context, leaving a void for a reliable dataset that encapsulates India's unique socio-cultural nuances. To bridge this gap, we introduce IndiBias, a comprehensive benchmarking dataset designed specifically for evaluating social biases in the Indian context. We filter and translate the existing CrowS-Pairs dataset to create a benchmark dataset suited to the Indian context in Hindi language. Additionally, we leverage LLMs including ChatGPT and InstructGPT to augment our dataset with diverse societal biases and stereotypes prevalent in India. The included bias dimensions encompass gender, religion, caste, age, region, physical appearance, and occupation. We also build a resource to address intersec
    
[^83]: 无监督通用依存句法树聚合的实证分析

    Empirical Analysis for Unsupervised Universal Dependency Parse Tree Aggregation

    [https://arxiv.org/abs/2403.19183](https://arxiv.org/abs/2403.19183)

    通过广泛的实证研究比较不同的无监督后处理聚合方法，以确定最适合的依存树结构聚合方法

    

    依存分析是自然语言处理中的一个重要任务，依存分析器的质量对许多下游任务至关重要。依存分析器的质量通常取决于涉及的领域和语言。因此，解决质量不稳定的问题以实现稳定性能是至关重要的。在各种自然语言处理任务中，聚合方法用于后处理聚合，已被证明可以解决质量不稳定的问题。然而，在依存分析任务中，对于后处理聚合的聚合方法尚未得到充分研究。在广泛的实证研究中，我们比较了不同的无监督后处理聚合方法，以确定最适合的依存树结构聚合方法。

    arXiv:2403.19183v1 Announce Type: new  Abstract: Dependency parsing is an essential task in NLP, and the quality of dependency parsers is crucial for many downstream tasks. Parsers' quality often varies depending on the domain and the language involved. Therefore, it is essential to combat the issue of varying quality to achieve stable performance. In various NLP tasks, aggregation methods are used for post-processing aggregation and have been shown to combat the issue of varying quality. However, aggregation methods for post-processing aggregation have not been sufficiently studied in dependency parsing tasks. In an extensive empirical study, we compare different unsupervised post-processing aggregation methods to identify the most suitable dependency tree structure aggregation method.
    
[^84]: 在多模态大型语言模型中量化和减轻单模态偏差：因果关系视角

    Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective

    [https://arxiv.org/abs/2403.18346](https://arxiv.org/abs/2403.18346)

    提出了一个因果框架用于解释多模态大型语言模型在视觉问答问题中的偏差，并引入了一个新的挑战性数据集MORE，同时提出两种减轻单模态偏差的策略。

    

    大型语言模型（LLMs）的最新进展促进了多模态LLMs（MLLMs）的发展。尽管它们具有令人印象深刻的能力，但MLLMs通常过度依赖单模态偏差（例如语言偏差和视觉偏差），导致在复杂多模态任务中给出不正确答案。为了调查这个问题，我们提出了一个因果框架来解释视觉问答（VQA）问题中的偏差。在我们的框架内，我们设计了一个因果图来阐明MLLMs对VQA问题的预测，并通过深入的因果分析评估偏差的因果效果。受因果图的启发，我们引入了一个新颖的MORE数据集，包含12,000个VQA实例。该数据集旨在挑战MLLMs的能力，需要多跳推理和克服单模态偏差。此外，我们提出了两种策略来减轻单模态偏差并增强MLLMs的推理能力。

    arXiv:2403.18346v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs). Despite their impressive capabilities, MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks. To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems. Within our framework, we devise a causal graph to elucidate the predictions of MLLMs on VQA problems, and assess the causal effect of biases through an in-depth causal analysis. Motivated by the causal graph, we introduce a novel MORE dataset, consisting of 12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities, necessitating multi-hop reasoning and the surmounting of unimodal biases. Furthermore, we propose two strategies to mitigate unimodal biases and enhance MLLMs' reasoning capabiliti
    
[^85]: 口语语言自监督模型中的词汇音调编码

    Encoding of lexical tone in self-supervised models of spoken language

    [https://arxiv.org/abs/2403.16865](https://arxiv.org/abs/2403.16865)

    本文研究分析了口语语言自监督模型对声调的编码能力，使用普通话和越南语作为案例研究，并发现SLMs在训练于非音调语言数据时也保持着显著的词汇音调编码能力。

    

    解释性研究表明，自监督口语语言模型（SLMs）从声学、语音、音韵、句法和语义层面到说话者特征中编码了人类语音中的各种特征。以前关于音韵学表示的大部分研究都集中在诸如音素等部分特征上；SLMs中对音韵音系（如声调和重音模式）的编码尚未被充分理解。声调是一种存在于世界上半数以上语言中的音系特征。本文旨在分析SLMs的声调编码能力，以普通话和越南语作为案例研究。我们展示了SLMs在训练于非音调语言数据时也明显编码了词汇音调。我们进一步发现，SLMs在声调和辅音感知研究中的表现与本族和非本族的人类参与者类似，但它们不遵循相同的模式。

    arXiv:2403.16865v1 Announce Type: new  Abstract: Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the sam
    
[^86]: MasonTigers在SemEval-2024任务1中的集成方法研究：语义文本相关性

    MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness

    [https://arxiv.org/abs/2403.14990](https://arxiv.org/abs/2403.14990)

    MasonTigers在SemEval-2024 Task 1中采用集成方法，结合语言特定的BERT模型和句子变换器，在处理语义文本相关性时取得了优异的结果。

    

    本文介绍了MasonTigers参与SemEval-2024任务1-语义文本相关性的工作。该任务涵盖了涵盖了监督（Track A）、无监督（Track B）和跨语言（Track C）方法，涉及14种不同语言。MasonTigers是少数同时参与了三个track中所有语言的两支团队之一。我们的方法在Track A中排名从第11到第21，在Track B中排名从第1到第8，在Track C中排名从第5到第12。在遵循特定任务约束的同时，我们的表现最好的方法利用了统计机器学习方法的集成，结合了基于语言的BERT模型和句子变换器。

    arXiv:2403.14990v1 Announce Type: new  Abstract: This paper presents the MasonTigers entry to the SemEval-2024 Task 1 - Semantic Textual Relatedness. The task encompasses supervised (Track A), unsupervised (Track B), and cross-lingual (Track C) approaches across 14 different languages. MasonTigers stands out as one of the two teams who participated in all languages across the three tracks. Our approaches achieved rankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and from 5th to 12th in Track C. Adhering to the task-specific constraints, our best performing approaches utilize ensemble of statistical machine learning approaches combined with language-specific BERT based models and sentence transformers.
    
[^87]: MasonTigers在SemEval-2024 Task 9中：使用一系列思维链解决谜题

    MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts

    [https://arxiv.org/abs/2403.14982](https://arxiv.org/abs/2403.14982)

    这项研究使用大型语言模型解决SemEval-2024 Task 9的谜题任务，通过一系列思维链提示技术，包括零参考和少量参考提示，以及思维链提示，最终取得了显著的结果，展示了逐步解释性提示如何可以更好地揭示已编码的知识。

    

    我们的论文介绍了团队MasonTigers提交给SemEval-2024 Task 9的作品，该任务提供了一组谜题数据集用于测试自然语言理解。我们利用大型语言模型（LLMs）通过几种提示技术来解决这个任务。零参考和少量参考提示通过专有LLMs测试时取得了相当不错的结果，相较于开源模型。通过思维链提示进一步提升了结果，这是一种迭代提示方法，逐步分解推理过程。通过利用一系列思维链提示，我们获得最好的结果，在单词谜题子任务中排名第二，在句子谜题子任务中排名第13。提示LLMs的强大表现显示了它们在提供思考过程分解时进行复杂推理的能力。我们的工作揭示了逐步解释性提示如何能够更多地揭示已编码的知识。

    arXiv:2403.14982v1 Announce Type: new  Abstract: Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 - which provides a dataset of puzzles for testing natural language understanding. We employ large language models (LLMs) to solve this task through several prompting techniques. Zero-shot and few-shot prompting generate reasonably good results when tested with proprietary LLMs, compared to the open-source models. We obtain further improved results with chain-of-thought prompting, an iterative prompting method that breaks down the reasoning process step-by-step. We obtain our best results by utilizing an ensemble of chain-of-thought prompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle subtask. The strong performance of prompted LLMs demonstrates their capability for complex reasoning when provided with a decomposition of the thought process. Our work sheds light on how step-wise explanatory prompts can unlock more of the knowledge encoded 
    
[^88]: 通过基于熵的动态温度采样改进大型语言模型的生成

    EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling

    [https://arxiv.org/abs/2403.14541](https://arxiv.org/abs/2403.14541)

    通过提出基于熵的动态温度采样方法，本文在大语言模型的生成中实现了更平衡的性能表现，并在四个不同生成基准上展示了显著优于现有策略的结果。

    

    最近，大型语言模型（LLMs）在各种下游语言任务中展现出了出色的性能。温度采样是LLMs生成过程中常用的解码策略。然而，大多数情况下使用固定的温度参数，这可能并非始终是平衡生成质量和多样性的最佳选择。在本文中，我们提出了一种有效的基于熵的动态温度（EDT）采样方法，通过动态选择温度参数实现在生成质量和多样性方面更平衡的性能。此外，我们还展示了4个不同生成基准的模型性能和全面分析。我们的实验表明，EDT在不同任务中明显优于现有策略。

    arXiv:2403.14541v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have demonstrated outstanding performance across a wide range of downstream language tasks. Temperature sampling is a commonly used decoding strategy for LLMs' generation process. However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter. Additionally, we also show model performance and comprehensive analyses for 4 different generation benchmarks. Our experiments show that EDT significantly outperforms the existing strategies across different tasks.
    
[^89]: 使用StateFlow增强LLM任务解决能力通过状态驱动工作流

    StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows

    [https://arxiv.org/abs/2403.11322](https://arxiv.org/abs/2403.11322)

    提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。

    

    使用大型语言模型（LLM）来解决复杂任务的趋势日益明显，例如需要一系列操作和与工具环境动态交互的任务。本文提出了StateFlow，一种新颖的基于LLM的任务求解范式，将由LLM支持的复杂任务解决过程概念化为状态机。通过正确构建状态和定义状态转换，StateFlow确定了任务求解的进展，确保清晰跟踪和管理LLM在整个任务求解过程中的响应。在每个状态中，StateFlow允许执行一系列动作，不仅包括根据特定提示指导生成LLM响应，还包括根据需要利用外部工具。状态转换由LLM做出的特定规则或决策控制，允许通过任务的预定义StateFlow模型动态自适应地进行进展。

    arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
    
[^90]: 通过引入引导优先优化加强多模态大语言模型

    Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization

    [https://arxiv.org/abs/2403.08730](https://arxiv.org/abs/2403.08730)

    提出了引导优先优化（BPO）策略，通过使用包含负面响应的数据集来减轻多模态大语言模型（MLLMs）对预训练语料库偏见的影响，并使用两种策略来促进模型在视觉输入中的表现。

    

    多模态大语言模型（MLLMs）在基于视觉输入生成响应方面表现出色。然而，它们往往存在偏向于生成与其预训练语料库相似响应的偏见，掩盖了视觉信息的重要性。我们将这种偏见视为对预训练统计数据的“偏好”，这阻碍了模型对视觉输入的基础。为了缓解这一问题，我们提出了引导优先优化（BPO），该方法使用包含负面响应的数据集进行偏好学习，这些数据集是从模型本身中引导出来的。具体而言，我们提出了以下两种策略：1）使用扭曲的图像输入到MLLM中，以引发包含显著预训练偏见的响应；2）利用基于文本的LLM将错误但常见的元素明确地注入原始响应中。这些不良响应与数据集中原始的注释响应配对，构建了偏好。

    arXiv:2403.08730v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) excel in generating responses based on visual inputs. However, they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information. We treat this bias as a "preference" for pretraining statistics, which hinders the model's grounding in visual input. To mitigate this issue, we propose Bootstrapped Preference Optimization (BPO), which conducts preference learning with datasets containing negative responses bootstrapped from the model itself. Specifically, we propose the following two strategies: 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based LLM to explicitly inject erroneous but common elements into the original response. Those undesirable responses are paired with original annotated responses from the datasets to construct the prefere
    
[^91]: MolBind: 多模态对齐语言、分子和蛋白质

    MolBind: Multimodal Alignment of Language, Molecules, and Proteins

    [https://arxiv.org/abs/2403.08167](https://arxiv.org/abs/2403.08167)

    MolBind 提出了一个框架，通过对比学习为多种模态训练编码器，将所有模态映射到共享特征空间，实现多模态语义对齐。

    

    生物学和化学的最新进展已经利用多模态学习，将分子及其自然语言描述整合起来，以增强药物发现。然而，当前的预训练框架仅限于两种模态，设计一个统一的网络来处理不同模态（例如自然语言、2D分子图、3D分子构象和3D蛋白质）仍然具有挑战性，因为它们之间存在固有的差距。

    arXiv:2403.08167v1 Announce Type: cross  Abstract: Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MolBind shows superior zero-sh
    
[^92]: Breeze-7B技术报告

    Breeze-7B Technical Report

    [https://arxiv.org/abs/2403.02712](https://arxiv.org/abs/2403.02712)

    Breeze-7B是基于Mistral-7B的开源语言模型，旨在改善中文语境下的语言理解和聊天机器人功能，展现出在复杂度类别中的出色性能。

    

    arXiv:2403.02712v1 类型：新论 摘要：Breeze-7B是一个基于Mistral-7B的开源语言模型，旨在改善中文传统语境下的语言理解和聊天机器人功能。本技术报告概述了Breeze-7B模型的额外预训练、微调和评估阶段。Breeze-7B系列的基础和聊天模型在语言理解和聊天机器人任务上表现良好，在一些与其复杂度类似的模型中达到了较好的性能。

    arXiv:2403.02712v1 Announce Type: new  Abstract: Breeze-7B is an open-source language model based on Mistral-7B, designed to address the need for improved language comprehension and chatbot-oriented capabilities in Traditional Chinese. This technical report provides an overview of the additional pretraining, finetuning, and evaluation stages for the Breeze-7B model. The Breeze-7B family of base and chat models exhibits good performance on language comprehension and chatbot-oriented tasks, reaching the top in several benchmarks among models comparable in its complexity class.
    
[^93]: 面向大型语言模型的隐私感知语义缓存

    Privacy-Aware Semantic Cache for Large Language Models

    [https://arxiv.org/abs/2403.02694](https://arxiv.org/abs/2403.02694)

    MeanCache是一种面向LLMs的语义缓存，能够识别语义上相似的查询，从而减少查询成本，服务提供商负载和环境影响。

    

    大型语言模型（LLMs）如ChatGPT、Google Bard、Claude和Llama 2彻底改变了自然语言处理和搜索引擎动态。然而，这些模型造成了异常高的计算成本。本文介绍了MeanCache，一种用于LLMs的语义缓存，它能够识别语义上相似的查询以确定缓存命中或未命中。

    arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
    
[^94]: MATHSENSEI：用于数学推理的工具增强型大型语言模型

    MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning

    [https://arxiv.org/abs/2402.17231](https://arxiv.org/abs/2402.17231)

    MATHSENSEI 是一种用于数学推理的工具增强型大型语言模型，通过添加知识检索、程序执行和符号方程求解工具，提高了数学推理能力。

    

    工具增强型大型语言模型(TALM)已知能够增强大型语言模型(LLM)的技能，从而提高它们在许多任务上的推理能力。本文提出了一种名为MATHSENSEI的工具增强型大型语言模型，用于数学推理。通过添加用于知识检索（Bing Web Search）、程序执行（Python）和符号方程求解（Wolfram-Alpha）的工具，我们通过对数学推理数据集进行评估来研究这些工具的互补优势。

    arXiv:2402.17231v1 Announce Type: new  Abstract: Tool-augmented Large Language Models (TALM) are known to enhance the skillset of large language models (LLM), thereby, leading to their improved reasoning abilities across many tasks. While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complimentary benefits offered by tools for knowledge retrieval and mathematical equation solving, are open research questions. In this work, we present MATHSENSEI, a tool-augmented large language model for mathematical reasoning. Augmented with tools for knowledge retrieval (Bing Web Search), program execution (Python), and symbolic equation solving (Wolfram-Alpha), we study the complimentary benefits of these tools through evaluations on mathematical reasoning datasets. We perform exhaustive ablations on MATH,a popular dataset for evaluating mathematical reasoning on diverse mathematical di
    
[^95]: OSCaR:对象状态字幕和状态变化表示

    OSCaR: Object State Captioning and State Change Representation

    [https://arxiv.org/abs/2402.17128](https://arxiv.org/abs/2402.17128)

    本文介绍了一个新的数据集和基准OSCaR，旨在解决描述复杂视觉环境中对象状态变化的问题，为评估多模态大型语言提供了一个新的实验平台。

    

    arXiv:2402.17128v3 公告类型: 跨 面向人类在真实世界环境中的交互视角，智能模型推断和理解对象状态的变化能力是人工智能研究的一个重要且具有挑战性的方面。该任务涉及描述复杂的视觉环境，识别活跃对象，以及通过语言解释它们的变化。传统方法将对象字幕和状态变化检测进行隔离，提供了对动态环境的有限视图。此外，依赖于一小套符号化词汇来表示变化限制了语言的表达力。为了解决这些挑战，在本文中，我们介绍了对象状态字幕和状态变化表示（OSCaR）数据集和基准。OSCaR包括来自各种主观视角视频集合的14,084个带注释视频片段，涵盖近1,000个独特对象。它为评估多模态大型语言提供了一个新的实验平台。

    arXiv:2402.17128v3 Announce Type: cross  Abstract: The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of the language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large langua
    
[^96]: 用隐式文本摘要提高对话状态跟踪的有效性和效率

    Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries

    [https://arxiv.org/abs/2402.13043](https://arxiv.org/abs/2402.13043)

    使用文本摘要提高对话检索的有效性和效率，通过对话摘要生成器进行查询和关键词生成，进一步提炼轻量级对话编码器以避免额外推理成本

    

    arXiv:2402.13043v1 公告类型: 新 文摘: 基于大型语言模型（LLM）的小样本对话状态跟踪（DST）依赖于一个有效且高效的对话检索器来查找类似的上下文示例以进行提示学习。先前的作品使用原始对话上下文作为搜索键和查询，并通过对带注释的对话进行微调来实现卓越性能。然而，这种方法不太适合扩展到新的领域或新的注释语言，因为微调数据不可用。为解决这一问题，我们基于对话的文本摘要来处理对话检索任务。采用基于LLM的对话摘要生成器进行查询和关键词生成，实现了有效的最大内积搜索。为避免LLM基于对话摘要生成带来的额外推理成本，我们进一步提炼一个轻量级的对话编码器，该编码器在不解码测试对话摘要的情况下生成查询嵌入向量。

    arXiv:2402.13043v1 Announce Type: new  Abstract: Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We val
    
[^97]: 模拟失调: 大型语言模型的安全对齐可能会适得其反！

    Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!

    [https://arxiv.org/abs/2402.12343](https://arxiv.org/abs/2402.12343)

    安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。

    

    大型语言模型（LLMs）需要进行安全对齐，以确保与人类进行安全的对话。然而，在这项工作中，我们引入了一种推理时攻击框架，表明安全对齐也可能在对抗性操纵下无意中促成有害结果。这个框架被命名为模拟失调（ED），在输出空间中不良地组合了一对开源预训练和安全对齐的语言模型，产生了一个有害的语言模型而无需任何训练。我们对ED在三个数据集和四个模型系列（Llama-1、Llama-2、Mistral和Alpaca）上的实验表明，ED使预训练模型的有害性增加了一倍，并胜过强基线，以较大优势在48个评估子集中的43个中实现了最高的有害率。至关重要的是，我们的研究结果凸显了即使在安全对齐后，重新评估开源语言模型实践的重要性。

    arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
    
[^98]: 解锁结构测量：引入PDD，一种用于位置话语连贯的自动评估指标

    Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence

    [https://arxiv.org/abs/2402.10175](https://arxiv.org/abs/2402.10175)

    本论文引入了一种新的自动评估指标PDD，用于评估长篇文章之间的话语连贯性，实验证明该指标更接近人类偏好和GPT-4的评估结果。

    

    近期，大型语言模型(LLMs)在各种任务中展现了与用户意图对齐生成文本的卓越性能。当涉及长篇文本生成时，从话语连贯的角度出发对生成结果产生了越来越大的兴趣。然而，现有的词汇或语义评估指标如BLEU、ROUGE、BertScore不能有效捕捉话语连贯性。因此，发展针对LLMs生成结果的话语特定的自动评估方法需要更多的关注和探索。在本文中，我们提出了一种新颖的自动评估指标，用于量化两篇长篇文章之间的话语差异。对来自代表性领域的三个数据集进行广泛实验，结果显示我们的指标更接近人类偏好和GPT-4的连贯性评估，优于现有的评估方法。

    arXiv:2402.10175v1 Announce Type: new  Abstract: Recent large language models (LLMs) have shown remarkable performance in aligning generated text with user intentions across various tasks. When it comes to long-form text generation, there has been a growing interest in generation from a discourse coherence perspective. However, existing lexical or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the discourse coherence. The development of discourse-specific automatic evaluation methods for assessing the output of LLMs warrants greater focus and exploration. In this paper, we present a novel automatic metric designed to quantify the discourse divergence between two long-form articles. Extensive experiments on three datasets from representative domains demonstrate that our metric aligns more closely with human preferences and GPT-4 coherence evaluation, outperforming existing evaluation methods.
    
[^99]: CodeMind:一个用于挑战大型语言模型进行代码推理的框架

    CodeMind: A Framework to Challenge Large Language Models for Code Reasoning

    [https://arxiv.org/abs/2402.09664](https://arxiv.org/abs/2402.09664)

    CodeMind是一个用于挑战大型语言模型进行代码推理的框架，通过评估LLMs的代码推理能力来替代仅仅依靠测试通过来评估，对三种代码推理任务进行评估，结果显示LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。

    

    仅靠测试通过来评估大型语言模型（LLMs）的代码合成能力可能会导致不公正的评估或促进具有数据泄漏的模型，作为一种替代方案，我们介绍了CodeMind，这是一个旨在评估LLMs的代码推理能力的框架。CodeMind目前支持三种代码推理任务：独立执行推理（IER）、依赖执行推理（DER）和规范推理（SR）。前两者评估模型以预测任意代码的执行输出，或者模型能够正确合成的代码。第三个任务评估LLMs实现指定预期行为的程度。我们使用CodeMind对两种不同编程语言中的五个基准下的九个LLMs进行了广泛的评估，结果表明LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。

    arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
    
[^100]: 无限-gram：将无限n-gram语言模型扩展到万亿标记

    Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens

    [https://arxiv.org/abs/2401.17377](https://arxiv.org/abs/2401.17377)

    这项研究展示了n-gram语言模型的价值，并介绍了一个名为infini-gram的引擎，它可以以毫秒级的延迟计算任意n的n-gram概率，使得在神经大型语言模型中对文本进行更准确的分析成为可能。

    

    在神经大型语言模型（LLM）时代，n-gram语言模型还具有相关性吗？我们的答案是肯定的，并且我们展示了它们在文本分析和改进神经LLM方面的价值。然而，这需要在两个方面对n-gram模型进行现代化。首先，我们将它们与神经LLM相同的数据规模训练- 1.4万亿个标记。这是迄今为止构建的最大的n-gram模型。其次，现有的n-gram模型使用的n很小，这妨碍了它们的性能；相反，我们允许n可以是任意大的，通过引入一个新的无限-gram LM与回退。我们开发了一个名为infini-gram的引擎，它可以通过后缀数组计算无限-gram（以及任意n的n-gram）概率，并且具有毫秒级的延迟，而无需预先计算n-gram计数表（这将非常昂贵）。无限-gram框架和infini-gram引擎使我们能够对人类写作和机器生成的文本进行许多新颖和有意思的分析：我们发现无限-gram LM...

    Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
    
[^101]: "你告诉我": 一份基于GPT-4的行为变革支持对话数据集

    "You tell me": A Dataset of GPT-4-Based Behaviour Change Support Conversations

    [https://arxiv.org/abs/2401.16167](https://arxiv.org/abs/2401.16167)

    该研究分享了一份数据集，其中包含关于行为变革的基于文本的用户互动数据，为研究人员提供了宝贵的见解，以便设计基于真实互动的对话系统

    

    对话代理越来越被用于满足情感需求，除了信息需求。一个越来越受关注的用例是辅导式心理健康和行为变革干预，大型语言模型(LLM)方法的应用越来越流行。到目前为止，在这个背景下的研究主要集中在系统方面，忽略了用户行为以及这种行为对LLM生成的文本可能产生的影响。为了解决这一问题，我们分享了一个数据集，其中包含与行为变革相关的两个基于GPT-4对话代理在事先注册的用户研究中收集的基于文本的用户互动。该数据集包括对话数据、用户语言分析、感知度量以及LLM生成的对话回复的用户反馈，并可以提供有价值的见解，以便根据真实互动来设计此类系统。

    arXiv:2401.16167v2 Announce Type: replace-cross  Abstract: Conversational agents are increasingly used to address emotional needs on top of information needs. One use case of increasing interest are counselling-style mental health and behaviour change interventions, with large language model (LLM)-based approaches becoming more popular. Research in this context so far has been largely system-focused, foregoing the aspect of user behaviour and the impact this can have on LLM-generated texts. To address this issue, we share a dataset containing text-based user interactions related to behaviour change with two GPT-4-based conversational agents collected in a preregistered user study. This dataset includes conversation data, user language analysis, perception measures, and user feedback for LLM-generated turns, and can offer valuable insights to inform the design of such systems based on real interactions.
    
[^102]: 不要轻信一切：通过自动识别大语言模型中的幻觉增强摘要解释性

    Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models

    [https://arxiv.org/abs/2312.14346](https://arxiv.org/abs/2312.14346)

    本文通过定义标记级别的方法来识别不同类型的幻觉，并利用这种标记来提高LLMs在对话摘要任务中的可解释性和忠实度。

    

    大语言模型（LLMs）擅长文本操纵——例如机器翻译和文本摘要。然而，这些模型也容易出现幻觉，这可能对模型提供的任何答案的忠实度造成负面影响。最近的作品致力于对抗LLMs中的幻觉，这些作品涉及识别虚构的句子以及对模型产生幻觉的不同方式进行分类。本文深入探讨了LLMs在幻觉方面的行为，定义了一种基于标记的方法来识别不同类型的幻觉，并进一步利用该标记来提高LLMs在对话摘要任务中的可解释性和忠实度。通过这一过程，本文提出了一个新的、增强的数据集和一个新的训练范式。

    arXiv:2312.14346v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) are adept at text manipulation -- tasks such as machine translation and text summarization. However, these models can also be prone to hallucination, which can be detrimental to the faithfulness of any answers that the model provides. Recent works in combating hallucinations in LLMs deal with identifying hallucinated sentences and categorizing the different ways in which models hallucinate. This paper takes a deep dive into LLM behavior with respect to hallucinations, defines a token-level approach to identifying different kinds of hallucinations, and further utilizes this token-level tagging to improve the interpretability and faithfulness of LLMs in dialogue summarization tasks. Through this, the paper presents a new, enhanced dataset and a new training paradigm.
    
[^103]: Gemini：一系列高性能多模态模型

    Gemini: A Family of Highly Capable Multimodal Models

    [https://arxiv.org/abs/2312.11805](https://arxiv.org/abs/2312.11805)

    Gemini家族是一系列在图像、音频、视频和文本理解方面表现出色的多模态模型，其中最具能力的Gemini Ultra模型在30个基准测试中推进了技术前沿，并改进了所有20个多模态基准测试的技术状态。

    

    本报告介绍了一种新的多模态模型系列Gemini，展示出在图像、音频、视频和文本理解方面的显著能力。Gemini系列包括Ultra、Pro和Nano尺寸，适用于从复杂推理任务到设备内存受限应用的各种应用场景。在广泛的基准测试中，我们最具能力的Gemini Ultra模型在32个基准测试中的30个中推进了技术前沿 - 显著地是第一个在被广泛研究的考试基准测试MMLU上实现人类专家水平表现的模型，并在我们研究的每一个20个多模态基准测试中改进了技术前沿。我们相信Gemini系列在跨模态推理和语言理解方面的新能力将能够支持各种用例。我们讨论了负责任地向用户提供Gemini模型的训练后和部署方法，包括使用服务。

    arXiv:2312.11805v2 Announce Type: replace-cross  Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services includi
    
[^104]: SCTc-TE：一种全面的形式化和基准测试用于时间事件预测

    SCTc-TE: A Comprehensive Formulation and Benchmark for Temporal Event Forecasting

    [https://arxiv.org/abs/2312.01052](https://arxiv.org/abs/2312.01052)

    SCTc-TE提出了一种全面的时间事件预测形式化方法，并通过构建MidEast-TE数据集和区分各种上下文信息提升了预测方法。

    

    时间复杂事件预测旨在根据历史上观察到的事件来预测未来事件。大多数时间复杂事件的形式化是非结构化的或缺乏广泛的时间信息，导致表征不足和预测能力有限。为了填补这些差距，我们创新地引入了结构化、复杂和时间完整的时间事件（SCTc-TE）的形式化。在这个全面的形式化之后，我们开发了一个完全自动化的流程，并从约60万篇新闻文章中构建了一个名为MidEast-TE的大规模数据集。该数据集侧重于2015年至2022年间主要涉及中东地区各国之间的合作和冲突事件。除了数据集构建之外，更重要的是，我们通过区分各种上下文信息（即，本地和全球上下文）来推进预测方法。因此，我们提出了一种新颖的

    arXiv:2312.01052v2 Announce Type: replace-cross  Abstract: Temporal complex event forecasting aims to predict the future events given the observed events from history. Most formulations of temporal complex event are unstructured or without extensive temporal information, resulting in inferior representations and limited forecasting capabilities. To bridge these gaps, we innovatively introduce the formulation of Structured, Complex, and Time-complete temporal event (SCTc-TE). Following this comprehensive formulation, we develop a fully automated pipeline and construct a large-scale dataset named MidEast-TE from about 0.6 million news articles. This dataset focuses on the cooperation and conflict events among countries mainly in the MidEast region from 2015 to 2022. Not limited to the dataset construction, more importantly, we advance the forecasting methods by discriminating the crucial roles of various contextual information, i.e., local and global contexts. Thereby, we propose a novel
    
[^105]: Omni-SMoLA: 用软低秩专家的混合提升通用多模态模型

    Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts

    [https://arxiv.org/abs/2312.00968](https://arxiv.org/abs/2312.00968)

    Omni-SMoLA提出了使用软MoE方法混合多个多模态低秩专家的架构，避免引入大量新参数，以提升通用多模态模型的性能。

    

    大型多模态模型（LMMs）在许多任务中表现出色。然而，通用LMMs在调优大量任务时往往会出现性能下降。最近的研究表明，专家混合（MoE）架构对指导调优很有用，但对于参数大小约为O(50-100B)的LMMs，复制和存储专家模型的成本限制了我们可以使用的专家数量。我们提出了Omni-SMoLA，这种架构使用软MoE方法（软地）混合许多多模态低秩专家，并避免引入与传统MoE模型相比显著数量的新参数。这里的核心直觉是，大型模型提供了基础支撑，而不同的轻量级专家在专门知识上残余学习，可以是单模态的或多模态的。大量实验证明SMoLA方法

    arXiv:2312.00968v2 Announce Type: replace-cross  Abstract: Large multi-modal models (LMMs) exhibit remarkable performance across numerous tasks. However, generalist LMMs often suffer from performance degradation when tuned over a large collection of tasks. Recent research suggests that Mixture of Experts (MoE) architectures are useful for instruction tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost of replicating and storing the expert models severely limits the number of experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft MoE approach to (softly) mix many multimodal low rank experts, and avoids introducing a significant number of new parameters compared to conventional MoE models. The core intuition here is that the large model provides a foundational backbone, while different lightweight experts residually learn specialized knowledge, either per-modality or multimodally. Extensive experiments demonstrate that the SMoLA approach 
    
[^106]: Agent-OM：利用LLM代理进行本体匹配

    Agent-OM: Leveraging LLM Agents for Ontology Matching

    [https://arxiv.org/abs/2312.00326](https://arxiv.org/abs/2312.00326)

    本研究提出了Agent-OM，利用LLM代理为本体匹配系统引入了新的设计范式。

    

    本体匹配（OM）能够实现不同本体之间的语义互操作性，通过对齐相关实体来解决其概念异构性。本研究引入了一种新颖的基于代理的LLM设计范式，命名为Agent-OM，包括两个用于检索和匹配的同体代理以及一组基于提示的简单OM工具。

    arXiv:2312.00326v2 Announce Type: replace  Abstract: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese agents for retrieval and matching, with a set of simple prompt-based OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAE
    
[^107]: NLP鲁棒性胜利中的疑虑耳语

    Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness

    [https://arxiv.org/abs/2311.09694](https://arxiv.org/abs/2311.09694)

    NLP模型的增大和性能提升并不能解决其鲁棒性问题，当前的方法和评估仍存在重大缺陷。

    

    随着 NLP 机型的不断增大和性能提升，NLP长期存在的鲁棒性问题是否得到解决？我们使用20多个不同大小的模型，涵盖不同的架构选择和预训练目标，来调查这个问题。我们使用（a）领域外和挑战性测试集，（b）CheckLists行为测试，（c）对比集，和（d）对抗性输入进行评估。我们的分析显示并非所有领域外测试都能提供鲁棒性的见解。使用CheckLists和对比集进行评估显示模型性能存在显著差距；仅仅扩大模型规模并不能使其足够鲁棒。最后，我们指出目前用于对模型进行对抗性评估的方法本身存在问题：它们很容易受到破坏，并且在当前形式下不足以深入探究模型的鲁棒性。我们得出结论，NLP的鲁棒性问题不仅尚未解决，甚至一些

    arXiv:2311.09694v2 Announce Type: replace  Abstract: Do larger and more performant models resolve NLP's longstanding robustness issues? We investigate this question using over 20 models of different sizes spanning different architectural choices and pretraining objectives. We conduct evaluations using (a) out-of-domain and challenge test sets, (b) behavioral testing with CheckLists, (c) contrast sets, and (d) adversarial inputs. Our analysis reveals that not all out-of-domain tests provide insight into robustness. Evaluating with CheckLists and contrast sets shows significant gaps in model performance; merely scaling models does not make them adequately robust. Finally, we point out that current approaches for adversarial evaluations of models are themselves problematic: they can be easily thwarted, and in their current forms, do not represent a sufficiently deep probe of model robustness. We conclude that not only is the question of robustness in NLP as yet unresolved, but even some o
    
[^108]: 逐步了解演示的增量效用：对少样本上下文学习的重新排名分析

    Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning

    [https://arxiv.org/abs/2311.09619](https://arxiv.org/abs/2311.09619)

    本文通过引入一种新的标记方法，增量效用，来估计LLMs通过演示带入的增量知识量，解决了关于不同标记策略如何影响目标任务结果的问题。

    

    In-Context Learning（ICL）是大型语言模型（LLMs）的一种新兴能力。 仅仅几个演示就能让LLMs被用作新任务的黑盒。 先前的研究表明，使用LLMs的输出作为标签在训练模型选择演示方面是有效的。 这样的标签应该能够估计ICL中演示的效用； 然而，并不清楚不同的标记策略如何影响目标任务的结果。 本文通过关注LLMs给出的输出概率和基于任务的奖励给出LLMs的预测，对不同效用函数进行了分析。 与以往的工作不同，我们提出了一种新的标记方法，增量效用，用于估计LLMs通过演示带入的增量知识量。 我们在经过指令调整的LLMs上进行了实验，涵盖了阿拉伯文的二分类/多分类、分割和翻译。

    arXiv:2311.09619v2 Announce Type: replace  Abstract: In-Context Learning (ICL) is an emergent capability of Large Language Models (LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new tasks. Previous studies have shown that using LLMs' outputs as labels is effective in training models to select demonstrations. Such a label is expected to estimate utility of a demonstration in ICL; however, it has not been well understood how different labeling strategies affect results on target tasks. This paper presents an analysis on different utility functions by focusing on LLMs' output probability given ground-truth output, and task-specific reward given LLMs' prediction. Unlike the previous work, we introduce a novel labeling method, incremental utility, which estimates how much incremental knowledge is brought into the LLMs by a demonstration. We conduct experiments with instruction-tuned LLMs on binary/multi-class classification, segmentation, and translation across Arab
    
[^109]: 怀孕问题：孕妇健康问答中实用意识的重要性

    Pregnant Questions: The Importance of Pragmatic Awareness in Maternal Health Question Answering

    [https://arxiv.org/abs/2311.09542](https://arxiv.org/abs/2311.09542)

    通过研究孕妇在问怀孕和婴儿护理问题时所做的假设和推断，将这些实用推断告知现有的问答管道可以产生更完整的回答，减少有害信念的传播。

    

    信息求助用户提出的问题通常包含隐含的错误或潜在有害的假设。在诸如孕产妇健康这样的高风险领域，问答系统必须识别这些实用约束，并不仅仅回答用户问题，还要在上下文中审视这些问题以给出有帮助的回应。为了实现这一点，我们通过收集来自三个不同来源的500个问题中的2,727个推断的数据集，研究了母亲在提出怀孕和婴儿护理问题时所做的假设和推断，进而研究了健康专家在撰写答案时如何自然地处理这些推断，并说明利用实用推断告知现有的问答管道可以产生更完整的回答，从而减少有害信念的传播。

    arXiv:2311.09542v2 Announce Type: replace  Abstract: Questions posed by information-seeking users often contain implicit false or potentially harmful assumptions. In a high-risk domain such as maternal and infant health, a question-answering system must recognize these pragmatic constraints and go beyond simply answering user questions, examining them in context to respond helpfully. To achieve this, we study assumptions and implications, or pragmatic inferences, made when mothers ask questions about pregnancy and infant care by collecting a dataset of 2,727 inferences from 500 questions across three diverse sources. We study how health experts naturally address these inferences when writing answers, and illustrate that informing existing QA pipelines with pragmatic inferences produces responses that are more complete, mitigating the propagation of harmful beliefs.
    
[^110]: 有效的大型语言模型适应以提升信息关联和引用生成

    Effective Large Language Model Adaptation for Improved Grounding and Citation Generation

    [https://arxiv.org/abs/2311.09533](https://arxiv.org/abs/2311.09533)

    本文提出了一个新的框架 AGREE，通过将大型语言模型的响应联系到检索到的段落并提供引文来提升信息关联，从而解决大型语言模型可能生成“臆想”答案的问题

    

    大型语言模型（LLMs）在自然语言理解和生成方面取得了显著进展。然而，一大问题在于它们可能生成“臆想”的答案并非事实。为解决这一问题，本文着眼于通过将LLMs的响应联系到检索到的段落并提供引文来改进它们。我们提出了一个新的框架AGREE，即Adaptation for GRounding EnhancEment，从整体的角度提升了信息关联。我们的框架调整LLMs，使其自我联系其响应中的主张并为检索文档提供准确的引文。在预训练的LLMs基础上进行的这种调整需要对配对查询的响应进行很好的信息关联（带有引文），为此我们介绍了一种能够从未标记查询自动构造这些数据的方法。调整后的LLMs的自我关联能力进一步使其测试时间适应

    arXiv:2311.09533v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved remarkable advancements in natural language understanding and generation. However, one major issue towards their widespread deployment in the real world is that they can generate "hallucinated" answers that are not factual. Towards this end, this paper focuses on improving LLMs by grounding their responses in retrieved passages and by providing citations. We propose a new framework, AGREE, Adaptation for GRounding EnhancEment, that improves the grounding from a holistic perspective. Our framework tunes LLMs to selfground the claims in their responses and provide accurate citations to retrieved documents. This tuning on top of the pre-trained LLMs requires well-grounded responses (with citations) for paired queries, for which we introduce a method that can automatically construct such data from unlabeled queries. The selfgrounding capability of tuned LLMs further grants them a test-time adapt
    
[^111]: AMRFact：利用AMR生成负样本增强摘要的事实性评估

    AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation

    [https://arxiv.org/abs/2311.09521](https://arxiv.org/abs/2311.09521)

    AMRFact是一个框架，利用AMR生成负样本，增强了摘要事实性评估，生成的连贯且事实不一致的摘要具有高错误率。

    

    确保事实一致性对于自然语言生成任务至关重要，特别是在提取式摘要中，保持信息的完整性至关重要。以前关于评估摘要的事实一致性的工作通常采用基于蕴涵的方法，首先生成扰动（事实不一致）摘要，然后在生成的数据上训练一个分类器，在测试时检测事实不一致。然而，先前生成扰动摘要的方法要么缺乏连贯性，要么缺乏错误类型覆盖。为了解决这些问题，我们提出了AMRFact，一个利用抽象意义表示（AMR）生成扰动摘要的框架。我们的方法将事实一致的摘要解析为AMR图，并注入可控的事实不一致，以创建负面示例，允许生成具有高错误率的连贯事实不一致的摘要。

    arXiv:2311.09521v2 Announce Type: replace  Abstract: Ensuring factual consistency is crucial for natural language generation tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior works on evaluating factual consistency of summarization often take the entailment-based approaches that first generate perturbed (factual inconsistent) summaries and then train a classifier on the generated data to detect the factually inconsistencies during testing time. However, previous approaches generating perturbed summaries are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs). Our approach parses factually consistent summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error
    
[^112]: 写作时思考：假设验证促进忠实的知识到文本生成

    Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation

    [https://arxiv.org/abs/2311.09467](https://arxiv.org/abs/2311.09467)

    提出了一种名为TWEAK的仅解码方法，通过引入假设验证模型来提高知识到文本生成的忠实度，并在不影响质量的情况下取得改进。

    

    知识到文本生成器经常难以忠实地为输入事实生成描述：它们可能产生与输入相矛盾的幻觉，或描述输入中不存在的事实。为了减少幻觉，我们提出了一种仅解码的方法TWEAK（思考而有效表达知识），可以与任何生成器集成而无需重新训练。TWEAK将每个解码步骤的生成序列及其未来序列视为假设，并根据其假设受到输入事实支持的程度，使用假设验证模型（HVM）对每个生成候选进行排名。我们首先通过使用自然语言推理（NLI）模型作为HVM展示了TWEAK的有效性，并报告了对质量影响很小的改善忠实度。然后，我们将NLI模型替换为使用FATE（事实对齐文本）首创数据集训练的特定任务HVM。

    arXiv:2311.09467v2 Announce Type: replace-cross  Abstract: Knowledge-to-text generators often struggle to faithfully generate descriptions for the input facts: they may produce hallucinations that contradict the input, or describe facts not present in the input. To reduce hallucinations, we propose a decoding-only method, TWEAK (Think While Effectively Articulating Knowledge), which can be integrated with any generator without retraining. TWEAK treats the generated sequences at each decoding step and its future sequences as hypotheses, and ranks each generation candidate based on the extent to which their hypotheses are supported by the input facts using a Hypothesis Verification Model (HVM). We first demonstrate the effectiveness of TWEAK by using a Natural Language Inference (NLI) model as the HVM and report improved faithfulness with a minimal impact on the quality. We then replace the NLI model with a task-specific HVM trained with a first-of-a-kind dataset, FATE (Fact-Aligned Text
    
[^113]: 语言模型生成中的对话基础研究

    Grounding Gaps in Language Model Generations

    [https://arxiv.org/abs/2311.09144](https://arxiv.org/abs/2311.09144)

    该论文研究了大型语言模型在生成文本时是否具有人类对话基础的表现，通过量化尝试基础设定的一系列指标比较了模型与人类的生成结果。

    

    有效的对话需要共同的基础：参与者之间的共同理解。然而，共同基础并不会在对话中自发产生。说话者和听众一起努力识别和建构共同基础，同时避免误解。为了完成基础设定，人类依赖于一系列对话行为，如澄清（你是什么意思？）和确认（我明白了）。然而，尚不清楚大型语言模型(LLMs)是否生成反映人类基础设定的文本。为此，我们筛选了一组基础设定行为，并提出相应的度量标准，量化尝试基础设定。我们研究LLM生成是否包含基础设定行为，模拟了几个对话数据集中的轮次，并将结果与人类进行了比较。我们发现，与人类相比，LLMs生成的语言中包含的对话基础较少，而更多地是一味地认为存在共同基础而生成文本。

    arXiv:2311.09144v2 Announce Type: replace  Abstract: Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). However, it is unclear whether large language models (LLMs) generate text that reflects human grounding. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLM generations contain grounding acts, simulating turn-taking from several dialogue datasets and comparing results to humans. We find that -- compared to humans -- LLMs generate language with less conversational grounding, instead generating text that appears to simply presume common
    
[^114]: 使用RoFT进行人工智能生成文本边界检测

    AI-generated text boundary detection with RoFT

    [https://arxiv.org/abs/2311.08349](https://arxiv.org/abs/2311.08349)

    使用RoFT进行人工智能生成文本边界检测的研究揭示了基于困惑度的方法在跨领域和跨模型设置中更加稳健。

    

    由于大语言模型的快速发展，人们越来越经常遇到可能一开始是由人类编写但之后是由机器生成的文本。检测这些文本中人类编写和机器生成部分之间的边界是一个具有挑战性且在文献中尚未受到足够关注的问题。我们试图填补这一差距，并研究几种方法来将最先进的人工文本检测分类器调整为边界检测设置。我们将所有检测器推向极限，在包含多个主题的短文本的Real or Fake文本基准集上进行测试，并包括各种语言模型的生成。我们利用这种多样性深入研究所有检测器在跨领域和跨模型设置中的鲁棒性，以提供未来研究的基线和见解。特别地，我们发现基于困惑度的边界检测方法倾向于更加稳健。

    arXiv:2311.08349v2 Announce Type: replace  Abstract: Due to the rapid development of large language models, people increasingly often encounter texts that may start as written by a human but continue as machine-generated. Detecting the boundary between human-written and machine-generated parts of such texts is a challenging problem that has not received much attention in literature. We attempt to bridge this gap and examine several ways to adapt state of the art artificial text detection classifiers to the boundary detection setting. We push all detectors to their limits, using the Real or Fake text benchmark that contains short texts on several topics and includes generations of various language models. We use this diversity to deeply examine the robustness of all detectors in cross-domain and cross-model settings to provide baselines and insights for future research. In particular, we find that perplexity-based approaches to boundary detection tend to be more robust to peculiarities 
    
[^115]: 零翻译上下文机器翻译的反-LM解码

    Anti-LM Decoding for Zero-shot In-context Machine Translation

    [https://arxiv.org/abs/2311.08324](https://arxiv.org/abs/2311.08324)

    提出了一种反-LM解码目标，通过引入Anti-Language Model目标和一个设计良好的衰减因子，解决了零翻译上下文机器翻译的弱点，与其他解码目标相比，在某些设置中，实现了高达20个BLEU点的性能提升。

    

    零翻译上下文学习是指模型可以根据简单的指令执行任务的现象。然而，已知预训练的大型语言模型在这项任务中校准不佳。处理这种偏见的最有效方法之一是采用对比解码目标，该目标考虑通过在某些上下文上下文的条件下生成下一个令牌的先验概率。本工作引入了一种Anti-Language Model目标，带有一个设计用于解决上下文机器翻译的弱点的衰减因子。我们在3种模型类型和大小，3种语言方向上以及贪婪解码和波束搜索（$B=5$）下进行实验。在某些设置中，所提出的方法优于其他最先进的解码目标，观察到比默认目标高达20个BLEU点的改进。

    arXiv:2311.08324v2 Announce Type: replace-cross  Abstract: Zero-shot In-context learning is the phenomenon where models can perform the task simply given the instructions. However, pre-trained large language models are known to be poorly calibrated for this task. One of the most effective approaches to handling this bias is to adopt a contrastive decoding objective, which accounts for the prior probability of generating the next token by conditioning on some context. This work introduces an Anti-Language Model objective with a decay factor designed to address the weaknesses of In-context Machine Translation. We conduct our experiments across 3 model types and sizes, 3 language directions, and for both greedy decoding and beam search ($B=5$). The proposed method outperforms other state-of-art decoding objectives, with up to $20$ BLEU point improvement from the default objective observed in some settings.
    
[^116]: 大型语言模型的心理测量预测能力

    Psychometric Predictive Power of Large Language Models

    [https://arxiv.org/abs/2311.07484](https://arxiv.org/abs/2311.07484)

    指令调整和提示在大型语言模型中无法提供比基础模型更好的认知建模估计。

    

    arXiv:2311.07484v2 公告类型: 替换-交叉 摘要:指令调整使大型语言模型（LLMs）的响应与人类偏好一致。尽管在人-LLM对齐方面进行了努力，我们报告了一个有趣的发现，即指令调整并不总是使LLMs从认知建模的角度看起来更像人类。更具体地说，由指令调整的LLM估计的下一个词概率往往比基础LLM估计的概率更糟糕，无法模拟人类阅读行为。此外，我们探讨了使用LLMs模拟人类阅读行为的提示方法。我们的结果表明，反映特定语言假设的提示可以提高PPP，但仍不及小型基础模型的PPP。这些发现突显出LLMs最近的进展，即指令调整和提示，并不能提供比基础LLMs直接概率测量更好的认知建模估计。换句话说，我们的实验表明，纯粹的下一个词概率

    arXiv:2311.07484v2 Announce Type: replace-cross  Abstract: Instruction tuning aligns the response of large language models (LLMs) with human preferences. Despite such efforts in human--LLM alignment, we report that, interestingly, instruction tuning does not always make LLMs human-like from a cognitive modeling perspective. More specifically, next-word probabilities estimated by instruction-tuned LLMs are often worse at simulating human reading behavior than those estimated by base LLMs. In addition, we explore prompting methodologies in simulating human reading behavior with LLMs. Our results show that prompts reflecting a particular linguistic hypothesis improve PPP but are still inferior to PPP from small base models. These findings highlight that recent advancements in LLMs, i.e., instruction tuning and prompting, do not offer better estimates than direct probability measurements from base LLMs in cognitive modeling. In other words, our experiments highlight that pure next-word pro
    
[^117]: MEGAVERSE: 在语言、模态、模型和任务之间对大型语言模型进行基准测试

    MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks

    [https://arxiv.org/abs/2311.07463](https://arxiv.org/abs/2311.07463)

    该研究对多语言数据集上的最新LLM进行了全面评估，结果显示较大型的模型如GPT-4、Gemini-Pro和PaLM2在各种任务上表现优越。

    

    LLM评估研究激增，旨在了解LLM的能力和局限性。然而，大部分研究局限于英语，使得LLM在非英语语言上的搭建和评估相对未被探索。本研究旨在通过比较SoTA LLM（GPT-3.5-Turbo、GPT-4、PaLM2、Gemini-Pro、Mistral、Llama2和Gemma）在相同多语言数据集上的表现，对非英语能力进行彻底评估。我们的基准包括22个数据集，覆盖83种语言，其中包括低资源的非洲语言。我们还在基准测试中包括两个多模态数据集，并比较了LLaVA模型、GPT-4-Vision和Gemini-Pro-Vision的性能。我们的实验表明，像GPT-4、Gemini-Pro和PaLM2这样的更大型模型在各种任务上表现优于较小型模型，特别是在低资源情况下。

    arXiv:2311.07463v2 Announce Type: replace  Abstract: There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However, much of this research has been confined to English, leaving LLM building and evaluation for non-English languages relatively unexplored. Several new LLMs have been introduced recently, necessitating their evaluation on non-English languages. This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-
    
[^118]: 关于大型语言模型朝向数据驱动人物的可操控性

    On the steerability of large language models toward data-driven personas

    [https://arxiv.org/abs/2311.04978](https://arxiv.org/abs/2311.04978)

    本文提出了一种能够通过大型语言模型实现特定观点可控生成的新方法，超越传统的人口统计数据，引入基于协同过滤的数据驱动人物概念，使得可以更细致地理解不同社会群体的多样性。

    

    大型语言模型(LLMs)已知会生成具有偏见的回应，某些群体和人口的意见被忽视。本文提出了一种新方法，通过LLMs实现特定观点的可控生成，可以用来产生多角度观点并反映多样化意见。我们超越了传统对于年龄、性别或政党归属等人口统计数据的依赖，引入了基于协同过滤的数据驱动人物的概念，将其定义为在特定研究中展现出相似观点的单个个体或人群。由于处在相同人口统计群体中的个体可能具有不同的人物，我们的数据驱动人物定义允许更细致地理解人口中存在的不同(潜在)社会群体。此外，我们还探讨了一种有效的方法来引导LLMs朝向这些人物。

    arXiv:2311.04978v2 Announce Type: replace  Abstract: Large language models (LLMs) are known to generate biased responses where the opinions of certain groups and populations are underrepresented. Here, we present a novel approach to achieve controllable generation of specific viewpoints using LLMs, that can be leveraged to produce multiple perspectives and to reflect the diverse opinions. Moving beyond the traditional reliance on demographics like age, gender, or party affiliation, we introduce a data-driven notion of persona grounded in collaborative filtering, which is defined as either a single individual or a cohort of individuals manifesting similar views across specific inquiries. As individuals in the same demographic group may have different personas, our data-driven persona definition allows for a more nuanced understanding of different (latent) social groups present in the population. In addition to this, we also explore an efficient method to steer LLMs toward the personas t
    
[^119]: 通过迭代生成的概念瓶颈实现设计可解释的文本理解

    Interpretable-by-Design Text Understanding with Iteratively Generated Concept Bottleneck

    [https://arxiv.org/abs/2310.19660](https://arxiv.org/abs/2310.19660)

    提出了一种文本瓶颈模型（TBM），通过预测一组突出概念的分类值来实现文本分类，从而在高风险领域提供全局和局部解释

    

    arXiv:2310.19660v2 公告类型: 替换 摘要: 黑盒深度神经网络在文本分类方面表现出色，但它们在高风险领域的应用受到其缺乏可解释性的限制。为了解决这一问题，我们提出了文本瓶颈模型（TBM），这是一个固有可解释的文本分类框架，可以提供全局和局部解释。TBM不是直接预测输出标签，而是预测一组突出概念的分类值，并使用这些概念值上的线性层来生成最终预测。这些概念可以由大型语言模型（LLM）自动发现和衡量，无需人工筛选。对12个不同的文本理解数据集的实验表明，TBM可以与黑盒基准（例如少样本GPT-4和微调DeBERTa）的性能相媲美，但在与微调GPT-3.5的比较中表现不佳。全面的人类评估验证了TBM可以生成高质量的解释。

    arXiv:2310.19660v2 Announce Type: replace  Abstract: Black-box deep neural networks excel in text classification, yet their application in high-stakes domains is hindered by their lack of interpretability. To address this, we propose Text Bottleneck Models (TBM), an intrinsically interpretable text classification framework that offers both global and local explanations. Rather than directly predicting the output label, TBM predicts categorical values for a sparse set of salient concepts and uses a linear layer over those concept values to produce the final prediction. These concepts can be automatically discovered and measured by a Large Language Model (LLM) without the need for human curation. Experiments on 12 diverse text understanding datasets demonstrate that TBM can rival the performance of black-box baselines such as few-shot GPT-4 and finetuned DeBERTa while falling short against finetuned GPT-3.5. Comprehensive human evaluation validates that TBM can generate high-quality conc
    
[^120]: 与传统机器学习对抗：重新思考大型语言模型在表格分类中的公平性

    Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications

    [https://arxiv.org/abs/2310.14607](https://arxiv.org/abs/2310.14607)

    LLMs在表格分类任务中存在社会偏见，影响了它们的公平性。

    

    最近的文献表明，使用大型语言模型（LLMs）进行表格任务的分类具有潜力。然而，LLMs已被证明存在表现出社会偏见的有害因素，反映了社会中存在的刻板印象和不平等。为此，以及在许多高风险应用中广泛使用表格数据，探讨以下问题至关重要：LLMs在进行表格任务分类时利用了哪些信息源；LLMs对表格数据的分类在多大程度上受到社会偏见和刻板印象的影响；以及这对公平性可能产生的重要影响是什么？通过一系列实验，我们深入探讨这些问题，并表明LLMs倾向于继承来自训练数据的社会偏见，这显著影响了它们在表格分类任务中的公平性。

    arXiv:2310.14607v2 Announce Type: replace  Abstract: Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?   Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in the context of bias miti
    
[^121]: UNO-DST: 利用未标注数据在零样本对话状态跟踪中的应用

    UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking

    [https://arxiv.org/abs/2310.10492](https://arxiv.org/abs/2310.10492)

    通过联合和自训练方法，UNO-DST利用未标注数据，将零样本对话状态跟踪转变为少样本DST，并在未知目标域中生成和选择高质量样本，最终提高了DST模型的训练和微调效果。

    

    先前的零样本对话状态跟踪（DST）方法仅应用迁移学习，忽略了目标域中的未标注数据。我们通过联合和自训练方法将零样本DST转换为少样本DST，利用这些未标注数据。我们的方法包括生成槽类型的辅助任务，作为主要任务的逆提示，在联合训练过程中创建槽值。这两个任务之间的循环一致性使得在未知目标域中生成和选择高质量样本，用于后续微调。这种方法还有助于自动标签创建，从而优化DST模型的训练和微调。我们在零样本场景中展示了这种方法在通用语言模型上的有效性，提高了在MultiWOZ中所有领域的平均联合目标准确性8%。

    arXiv:2310.10492v2 Announce Type: replace  Abstract: Previous zero-shot dialogue state tracking (DST) methods only apply transfer learning, ignoring unlabelled data in the target domain. We transform zero-shot DST into few-shot DST by utilising such unlabelled data via joint and self-training methods. Our method incorporates auxiliary tasks that generate slot types as inverse prompts for main tasks, creating slot values during joint training. Cycle consistency between these two tasks enables the generation and selection of quality samples in unknown target domains for subsequent fine-tuning. This approach also facilitates automatic label creation, thereby optimizing the training and fine-tuning of DST models. We demonstrate this method's effectiveness on general language models in zero-shot scenarios, improving average joint goal accuracy by 8% across all domains in MultiWOZ.
    
[^122]: 让行动胜于雄辩：评估LLM代理在拍卖竞技场中的战略规划与执行

    Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena

    [https://arxiv.org/abs/2310.05746](https://arxiv.org/abs/2310.05746)

    LLM代理在拍卖竞技场展示出了关键的规划和执行技能，这为建模复杂社会互动在竞争背景下的LLMs潜力提供了新途径。

    

    最近大型语言模型（LLM）的发展展示了先进的推理能力，然而自然语言处理的评估通常依赖于静态基准。评估这一点需要测试战略推理能力的环境，这种环境需要在动态的竞争场景中进行长期规划。我们引入了AucArena，这是一个模拟拍卖的新颖评估套件，选择这个设置是因为它非常不可预测，涉及与资源和风险管理相关的许多技能，同时也易于评估。我们进行了使用最先进的LLM驱动竞标代理的受控实验，以评估他们的规划和执行技能。我们的研究表明，诸如GPT-4之类的LLM具有拍卖参与的关键技能，如预算管理和目标遵从，这些技能会随着自适应策略的改进而提高。这突出了LLM在建模竞技背景下的复杂社会互动潜力。

    arXiv:2310.05746v2 Announce Type: replace-cross  Abstract: Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM p
    
[^123]: 利用上下文线索和角色相关性提升文档级事件论证提取

    Utilizing Contextual Clues and Role Correlations for Enhancing Document-level Event Argument Extraction

    [https://arxiv.org/abs/2310.05116](https://arxiv.org/abs/2310.05116)

    本文提出了CARLG模型，通过利用上下文线索和角色相关性，提升了文档级事件论证提取的性能。

    

    文档级事件论证提取（EAE）是信息提取中至关重要但具有挑战性的子任务之一。现有方法大多关注论证和事件触发器之间的交互，忽视了两个关键点：上下文线索的信息和论证角色之间的语义相关性。本文提出了CARLG模型，包括两个模块：上下文线索聚合（CCA）和基于角色的潜在信息引导（RLIG），通过有效利用上下文线索和角色相关性来提高文档级EAE。CCA模块通过利用来自预训练编码器的上下文注意权重，自适应地捕捉和整合上下文线索。RLIG模块通过角色交互编码捕捉语义相关性，并通过潜在角色表示提供宝贵的信息引导。值得注意的是，我们的CCA和RLIG模块紧凑、可移植且高效，引入的新参数不超过1%，且易于实现。

    Document-level event argument extraction (EAE) is a vital but challenging subtask in information extraction. Most existing approaches focus on the interaction between arguments and event triggers, ignoring two critical points: the information of contextual clues and the semantic correlations among argument roles. In this paper, we propose the CARLG model, which consists of two modules: Contextual Clues Aggregation (CCA) and Role-based Latent Information Guidance (RLIG), effectively leveraging contextual clues and role correlations for improving document-level EAE. The CCA module adaptively captures and integrates contextual clues by utilizing context attention weights from a pre-trained encoder. The RLIG module captures semantic correlations through role-interactive encoding and provides valuable information guidance with latent role representation. Notably, our CCA and RLIG modules are compact, transplantable and efficient, which introduce no more than 1% new parameters and can be eas
    
[^124]: 对比后训练的自动对构建

    Automatic Pair Construction for Contrastive Post-training

    [https://arxiv.org/abs/2310.02263](https://arxiv.org/abs/2310.02263)

    提出了一种自动构建对比数据的方法，使用多个模型的偏好对，提高了大型语言模型的对齐效果，并且通过DPO对比技术得到了改善，进一步优化了对齐，最终使经过调优的指导学习模型Orca超越了ChatGPT。

    

    对齐作为引导大型语言模型（LLMs）走向人类偏好的重要步骤。本文提出了一种自动构建LLM对比数据的方法，使用来自多个不同强度模型（例如InstructGPT、ChatGPT和GPT-4）的偏好对。我们比较了SLiC和DPO的对比技术与SFT基线，并发现即使在继续SFT饱和后，DPO仍然提供了一个阶跃式的改善。我们还探讨了一种对比后训练的数据课程学习方案，该方案从“更容易”的对开始学习，然后过渡到“更难”的对，进一步提高了对齐效果。最后，我们通过使用更多数据和像Orca这样的更大型模型来扩大实验规模。值得注意的是，我们的自动对比后训练进一步提高了Orca的性能，它已经是一个通过GPT-4输出调优的最先进指导学习模型，从而超越了ChatGPT。

    arXiv:2310.02263v2 Announce Type: replace-cross  Abstract: Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from "easier" pairs and transitioning to "harder" ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.
    
[^125]: 从头到尾：大型语言模型（LLMs）有多富有知识？又称LLMs是否会取代知识图谱？

    Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?

    [https://arxiv.org/abs/2308.10168](https://arxiv.org/abs/2308.10168)

    本文构建了Head-to-Tail基准，通过对18K个头部、躯干和尾部事实的问答对进行全面评估，揭示了现有大型语言模型在掌握事实知识方面尤其是对躯干到尾部实体的事实仍然远未完美。

    

    自从大型语言模型（LLMs）近来繁荣以来，关于如何减少LLMs回应中的幻觉，如何提高LLMs的事实性以及符号化存储世界知识的知识图谱（KGs）是否会被LLMs取代等问题已经交织在一起进行讨论。在本文中，我们试图从一个新角度回答这些问题：LLMs有多富有知识？为了回答这个问题，我们构建了一个名为“从头到尾”的基准，其中包含18,000个有关头部、躯干和尾部事实的问答（QA）对，涉及流行度。我们设计了一种自动化评估方法和一组能够接近LLMs自信内化的知识的度量标准。通过对16个公开可用的LLMs进行全面评估，我们展示了现有LLMs在掌握事实知识方面仍然远未达到完美，特别是对于躯干到尾部实体的事实。

    arXiv:2308.10168v2 Announce Type: replace  Abstract: Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?   To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.
    
[^126]: 并非所有评估指标都应受到指责：通过多样化参考文献改进自然语言生成评估

    Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References

    [https://arxiv.org/abs/2305.15067](https://arxiv.org/abs/2305.15067)

    通过增加参考文献的多样性，本文提出的方法Div-Ref 显著提高了自然语言生成评估的相关性

    

    大多数关于自然语言生成（NLG）的研究依赖于具有有限参考文献的评估基准，这可能导致与人类判断的相关性较差。本文提出了一种简单有效的方法，称为Div-Ref，通过丰富参考文献的数量来增强现有的评估基准。我们利用大型语言模型（LLMs）将单个参考文献的表达多样化为多个高质量的表达，以尽可能覆盖参考句的语义空间。我们进行了全面的实验，从经验上证明多样化参考文献的表达可以显著增强自动评估之间的相关性。

    arXiv:2305.15067v2 Announce Type: replace  Abstract: Most research about natural language generation (NLG) relies on evaluation benchmarks with limited references for a sample, which may result in poor correlations with human judgements. The underlying reason is that one semantic meaning can actually be expressed in different forms, and the evaluation with a single or few references may not accurately reflect the quality of the model's hypotheses. To address this issue, this paper presents a simple and effective method, named Div-Ref, to enhance existing evaluation benchmarks by enriching the number of references. We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible. We conduct comprehensive experiments to empirically demonstrate that diversifying the expression of reference can significantly enhance the correlation between automatic evaluation
    
[^127]: 从快捷方式到触发器：使用去噪 PoE 进行后门防御

    From Shortcuts to Triggers: Backdoor Defense with Denoised PoE

    [https://arxiv.org/abs/2305.14910](https://arxiv.org/abs/2305.14910)

    提出了一种端到端集成的后门防御框架 DPoE，旨在通过去噪设计和捕捉后门快捷方式的浅层模型，以及防止学习后门快捷方式的主模型，有效抵御各种后门攻击。

    

    语言模型经常面临多样的后门攻击风险，特别是数据污染。因此，研究针对这些攻击的防御解决方案非常重要。现有的后门防御方法主要集中在带有显式触发器的后门攻击上，对抗各种后门攻击与不同触发器的通用防御方法仍然未被充分探索。在本文中，我们提出了一种端到端集成的后门防御框架 DPoE（Denoised Product-of-Experts），灵感来源于后门攻击的快捷方式，以抵御各种后门攻击。DPoE 包含两个模型：一个捕捉后门快捷方式的浅层模型和一个被阻止学习后门快捷方式的主模型。为了处理后门攻击者引起的标签翻转，DPoE 融入了去噪设计。对 SST-2 数据集的实验证明，DPoE 显著提高了对各种类型的防御性能。

    arXiv:2305.14910v3 Announce Type: replace-cross  Abstract: Language models are often at risk of diverse backdoor attacks, especially data poisoning. Thus, it is important to investigate defense solutions for addressing them. Existing backdoor defense methods mainly focus on backdoor attacks with explicit triggers, leaving a universal defense against various backdoor attacks with diverse triggers largely unexplored. In this paper, we propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised Product-of-Experts), which is inspired by the shortcut nature of backdoor attacks, to defend various backdoor attacks. DPoE consists of two models: a shallow model that captures the backdoor shortcuts and a main model that is prevented from learning the backdoor shortcuts. To address the label flip caused by backdoor attackers, DPoE incorporates a denoising design. Experiments on SST-2 dataset show that DPoE significantly improves the defense performance against various types of
    
[^128]: 噪声在组合式沟通中的催化作用和归纳偏见的必要性

    Catalytic Role Of Noise And Necessity Of Inductive Biases In The Emergence Of Compositional Communication

    [https://arxiv.org/abs/2111.06464](https://arxiv.org/abs/2111.06464)

    噪声和归纳偏见的作用使得组合式沟通自发产生，并且一定范围内的噪声有助于促进组合性的发展。

    

    沟通是组合式的，如果复杂信号可以表示为较简单子部分的组合。本文理论上表明，训练框架和数据上的归纳偏见对于发展组合式沟通是必要的。此外，我们证明组合性会在信号博弈中自发出现，代理在嘈杂通道上传输信息。我们通过实验证实，一系列噪声水平（取决于模型和数据）确实促进了组合性。最后，我们对这种依赖关系进行了全面研究，并报告了最近研究的组合性度量结果：拓扑相似性、冲突计数和上下文独立性。

    arXiv:2111.06464v2 Announce Type: replace-cross  Abstract: Communication is compositional if complex signals can be represented as a combination of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. We experimentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence.
    
[^129]: 将变压器与自然语言解释相结合

    Combining Transformers with Natural Language Explanations

    [https://arxiv.org/abs/2110.00125](https://arxiv.org/abs/2110.00125)

    通过结合自然语言解释和变压器模型，我们提出了一种方法来利用外部记忆存储自然语言解释，并用于解释分类输出，实验证明这种方法能够产生相关解释且保持或提高分类性能。

    

    许多自然语言处理应用需要模型具有解释性。然而，许多成功的神经架构，包括变压器，仍然缺乏有效的解释方法。建立解释可能的一个解决方案是依赖建立来自领域知识的解释，这些知识通常以简单的自然语言文本形式存在。因此，我们提出了一个扩展到变压器模型的方法，利用外部记忆来存储自然语言解释，并使用它们来解释分类输出。我们在两个领域，法律文本分析和论据挖掘，进行了实验评估，以表明我们的方法可以产生相关的解释，同时保持或甚至提高分类性能。

    arXiv:2110.00125v3 Announce Type: replace-cross  Abstract: Many NLP applications require models to be interpretable. However, many successful neural architectures, including transformers, still lack effective interpretation methods. A possible solution could rely on building explanations from domain knowledge, which is often available as plain, natural language text. We thus propose an extension to transformer models that makes use of external memories to store natural language explanations and use them to explain classification outputs. We conduct an experimental evaluation on two domains, legal text analysis and argument mining, to show that our approach can produce relevant explanations while retaining or even improving classification performance.
    
[^130]: 通过检索和自我反思改善医疗推理能力的检索增强型大型语言模型

    Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])

    [http://arxiv.org/abs/2401.15269](http://arxiv.org/abs/2401.15269)

    本论文介绍了一种名为Self-BioRAG的框架，通过使用检索和自我反思的方法，提高了医疗推理的能力。该框架专注于生成解释、检索领域特定文档以及对生成的响应进行自我反思。

    

    最近的专有大型语言模型（LLMs），例如GPT-4，在生物医学领域中解决了从多项选择题到长篇生成等多样化挑战的里程碑。为了解决LLMs编码知识无法处理的挑战，已经开发了各种检索增强生成（RAG）方法，通过从知识语料库中搜索文档并无条件或有选择地将其附加到LLMs的输入来进行生成。然而，将现有方法应用于不同领域特定问题时，出现了泛化能力差的问题，导致获取不正确的文档或做出不准确的判断。在本文中，我们介绍了一种可靠的医学文本框架Self-BioRAG，专门用于生成解释、检索领域特定文档和自我反思生成的响应。我们使用了84k个经过过滤的生物医学指令集来训练Self-BioRAG，它具备评估自己的基因

    Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
    
[^131]: MambaByte: 无标记选择性状态空间模型

    MambaByte: Token-free Selective State Space Model. (arXiv:2401.13660v1 [cs.CL])

    [http://arxiv.org/abs/2401.13660](http://arxiv.org/abs/2401.13660)

    MambaByte是一种无标记的选择性状态空间模型，通过在字节级别上进行自回归训练，解决了标准自回归Transformer在处理长序列时的性能问题，并展现了与最先进的子词Transformer相媲美甚至更优的性能，从而证明了MambaByte在无标记语言建模方面的有效性。

    

    无标记语言模型直接从原始字节学习，消除了子词标记化的偏差。然而，操作字节会导致序列长度显著增加，在这种情况下，标准自回归Transformer的扩展性较差。我们尝试了MambaByte，它是基于字节序列自回归训练的无标记适应Mamba状态空间模型。我们的实验表明，与其他字节级模型相比，MambaByte具有计算效率。我们还发现，MambaByte在性能上与甚至胜过最先进的子词Transformer。此外，由于长度的线性扩展，MambaByte在推理过程中获得了快速性能，相比之下，Transformer则没有。我们的研究结果证实了MambaByte在实现无标记语言建模方面的可行性。

    Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling.
    
[^132]: MaLA-500: 大规模语言适应大型语言模型

    MaLA-500: Massive Language Adaptation of Large Language Models. (arXiv:2401.13303v1 [cs.CL])

    [http://arxiv.org/abs/2401.13303](http://arxiv.org/abs/2401.13303)

    MaLA-500是一种大型语言模型，设计用于覆盖534种语言，并通过在LLaMA 2上进行训练来提高效果。

    

    大型语言模型在自然语言处理领域取得了突破。然而，由于其主要设计针对英语或一小部分语言，其在低资源语言上效果有限。为了填补这一差距，我们引入了MaLA-500，这是一种新颖的大型语言模型，设计用于覆盖534种语言。为了训练MaLA-500，我们采用了词汇扩展和在LLaMA 2上的持续预训练。我们在SIB-200上进行的实验表明，MaLA-500在上下文学习结果方面取得了最先进的成果。我们在https://huggingface.co/MaLA-LM上发布了MaLA-500。

    Large language models have advanced the state of the art in natural language processing. However, their predominant design for English or a limited set of languages creates a substantial gap in their effectiveness for low-resource languages. To bridge this gap, we introduce MaLA-500, a novel large language model designed to cover an extensive range of 534 languages. To train MaLA-500, we employ vocabulary extension and continued pretraining on LLaMA 2 with Glot500-c. Our experiments on SIB-200 show that MaLA-500 achieves state-of-the-art in-context learning results. We release MaLA-500 at https://huggingface.co/MaLA-LM
    
[^133]: MLLMReID: 基于多模态大语言模型的人物再识别

    MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])

    [http://arxiv.org/abs/2401.13201](http://arxiv.org/abs/2401.13201)

    MLLMReID是一种基于多模态大语言模型的人物再识别方法，通过微调模型并将其视觉编码器作为主干进行优化，解决了MLLM在ReID任务中的设计指令和特征学习效果的问题。

    

    多模态大语言模型（MLLM）在许多任务中取得了令人满意的结果。然而，它们在人物再识别（ReID）任务中的表现尚未被研究。本文将研究如何将它们适应于ReID任务。一种直观的想法是使用ReID图像-文本数据集对MLLM进行微调，然后将它们的视觉编码器作为ReID的主干。然而，仍存在两个明显的问题：（1）为ReID设计指令时，MLLM可能过度拟合特定指令，而设计各种指令将导致更高的成本。（2）LLM的潜在图像特征向量没有参与损失计算。指令学习，对齐图像-文本特征，导致间接优化和学习目标不充分利用特征，限制了人物特征学习的效果。为了解决这些问题，本文提出了MLLMReID：基于多模态大语言模型的ReID。首先，我们提出了公共指令。

    Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instru
    
[^134]: 大型语言模型的指令指纹识别

    Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR])

    [http://arxiv.org/abs/2401.12255](http://arxiv.org/abs/2401.12255)

    这项研究提出了一种指纹识别大型语言模型的方法，通过轻量级的指令调整，保护知识产权并确保遵守许可条款。实验证明这种方法不影响模型的正常行为，并且具有鲁棒性和高效训练的特点。

    

    从零开始训练大型语言模型（LLM）的巨大成本使得对模型进行指纹识别以保护知识产权成为必要，通过所有权认证并确保下游用户和开发者遵守许可条款（如限制商业使用）。在这项研究中，我们提出了LLM指纹识别的试点研究，作为一种非常轻量级的指令调整形式。模型发布者指定一个机密的私钥，并将其植入为一个指令后门，当密钥存在时，导致LLM生成特定的文本。对11个常用LLMs的结果表明，这种方法轻量级且不影响模型的正常行为。它还可以防止发布者过度宣称，对指纹猜测和参数高效训练保持鲁棒性，并支持类似于MIT许可证的多阶段指纹识别。代码可在https://cnut1648.github.io/Model-Fingerprint/中获得。

    The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/.
    
[^135]: FAIR到位：我们如何为大型语言模型的训练开发和评估符合FAIR标准的数据集？

    FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?. (arXiv:2401.11033v1 [cs.CL])

    [http://arxiv.org/abs/2401.11033](http://arxiv.org/abs/2401.11033)

    这项工作介绍了一个将FAIR数据原则嵌入到大型语言模型训练中的框架，并提供了整合指南和检查清单。通过一个案例研究，展示了在符合FAIR标准的数据集中识别和减轻偏见的实际应用。

    

    大型语言模型的进展凸显了道德实践和数据完整性的需求。我们引入了一个将FAIR（可发现、可访问、可互操作、可重用）数据原则嵌入到LLM训练中的框架。这一方法标志着朝着符合FAIR标准的实践的转变。我们的框架提供了将FAIR数据原则整合到LLM训练中的指南。该举措包括研究人员和开发人员的检查清单。我们还通过一个案例研究展示了它的实际应用，重点是在我们符合FAIR标准的数据集中识别和减轻偏见。这项工作对于人工智能伦理和数据科学是重要的贡献，倡导在LLMs中使用平衡和道德的训练方法。

    Advancements in Large Language Models (LLMs) highlight the need for ethical practices and data integrity. We introduce a framework that embeds FAIR (Findable, Accessible, Interoperable, Reusable) data principles into LLM training. This approach marks a shift towards practices compliant with FAIR standards. Our framework presents guidelines for integrating FAIR data principles into LLM training. This initiative includes a checklist for researchers and developers. We also demonstrate its practical application through a case study focused on bias identification and mitigation in our FAIR-compliant dataset. This work is a significant contribution to AI ethics and data science, advocating for balanced and ethical training methods in LLMs.
    
[^136]: 医学视觉问答中的幻觉基准评估

    Hallucination Benchmark in Medical Visual Question Answering. (arXiv:2401.05827v1 [cs.CL])

    [http://arxiv.org/abs/2401.05827](http://arxiv.org/abs/2401.05827)

    这项研究创建了医学图像的幻觉基准评估，并全面评估了当前最先进的模型，揭示了幻觉现象在临床环境中的限制和各种提示策略的有效性。

    

    最近大型语言和视觉模型在视觉问答（VQA）上取得了成功，尤其在医学（Med-VQA）领域的应用显示出了为医疗提供有效视觉助手的巨大潜力。然而，这些模型在临床环境中的幻觉现象上并没有进行广泛测试。在本研究中，我们创建了一个医学图像配对问题-回答集的幻觉基准评估，并对当前最先进的模型进行了全面评估。该研究对当前模型的局限性进行了深入分析，并揭示了各种提示策略的有效性。

    The recent success of large language and vision models on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models limitations and reveals the effectiveness of various prompting strategies.
    
[^137]: 大型语言模型的分治求解方法在推理中的应用

    Divide and Conquer for Large Language Models Reasoning. (arXiv:2401.05190v1 [cs.CL])

    [http://arxiv.org/abs/2401.05190](http://arxiv.org/abs/2401.05190)

    分治求解方法应用于大型语言模型的推理中，通过根据统计置信度分数将问题划分为不同的子集，并采用基于先验知识和筛选选项的推理方法，提高了推理性能，取得了优异结果。

    

    随着Chain-of-Thought（CoT）及其衍生方法的出现，大型语言模型（LLMs）在各种推理基准测试中表现出令人印象深刻的性能，特别是在涉及多项选择题（MCQs）的任务中。然而，当前的工作都是统一处理数据，没有考虑到问题解决的难度，这意味着过分关注简单问题，而对复杂问题不够重视。为了应对这一挑战，我们受到人类使用启发式策略对任务进行分类并单独处理的启发，提议将分治方法应用于LLMs推理中。首先，我们根据统计置信度分数（$\mathcal{CS}$）将问题划分为不同的子集，然后固定解决的子集，用精心设计的方法解决复杂的纷繁问题，包括基于先验知识的推理（PKR）和基于筛选选项的推理（FCR），以及它们的集成变体。我们的实验表明，这种提出的分治求解方法可以显著提高推理性能，并在不同难度的问题上取得了优异结果。

    Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs). However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones. To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning. First, we divide questions into different subsets based on the statistical confidence score ($\mathcal{CS}$), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants. Our experiments demonstrate that this proposed
    
[^138]: LLM训练中的结构化填充改进了长上下文利用

    Structured Packing in LLM Training Improves Long Context Utilization. (arXiv:2312.17296v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.17296](http://arxiv.org/abs/2312.17296)

    本论文研究了长上下文大型语言模型（LLM）中上下文利用不足的问题，并通过将相关文档纳入训练示例中来改进模型的困惑度。通过引入Structured Packing for Long Context (SPLiCe)方法，使用检索方法将最互相关文档汇集到单个训练上下文中，进一步提高了模型的性能。

    

    长上下文大型语言模型（LCLM）的最新进展引起了广泛关注，特别是在查询科学研究论文等应用中。然而，它们的潜力往往受到上下文利用不足的限制。我们确定典型训练数据中缺乏长程语义依赖是主要障碍。为了解决这个问题，我们深入研究了频繁将相关文档纳入训练输入的好处。利用代码数据的固有目录结构作为训练示例的来源，我们证明了即使对于与编码无关的任务，囊括相关文档能够改进模型的困惑度。基于这些发现，并且更具广泛的关注，我们引入了一种名为Structured Packing for Long Context (SPLiCe)的创新方法。 SPLiCe是一种使用检索方法将最互相关文档汇集到单个训练上下文中的方法。我们的结果表明，\method{}提高了模型的性能，并可用于t

    Recent advances in long-context Large Language Models (LCLMs) have generated significant interest, especially in applications such as querying scientific research papers. However, their potential is often limited by inadequate context utilization. We identify the absence of long-range semantic dependencies in typical training data as a primary hindrance. To address this, we delve into the benefits of frequently incorporating related documents into training inputs. Using the inherent directory structure of code data as a source of training examples, we demonstrate improvements in perplexity, even for tasks unrelated to coding. Building on these findings, but with a broader focus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an innovative method for creating training examples by using a retrieval method to collate the most mutually relevant documents into a single training context. Our results indicate that \method{} enhances model performance and can be used to t
    
[^139]: 不同的令牌指标：通过测量衰减来修剪LLM组件并优化量化

    Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])

    [http://arxiv.org/abs/2311.01544](http://arxiv.org/abs/2311.01544)

    本研究引入了一种新的方法，即不同的令牌指标（DTM），用于评估压缩后的大型语言模型（LLM）。通过关注令牌的差异性，DTM提供了对模型压缩微妙之处的深入洞察，并且在不损害文本生成质量的情况下可以实现显著的精确度和稀疏度水平。该研究还提出了一种利用DTM进行模型稀疏化和量化的方法，并发现可以修剪掉超过90%的LLM组件和量化超过80%的参数。

    

    大型语言模型（LLM）以其强大的能力改变了自然语言处理。然而，它们不断增长的大小引发了关于它们的有效部署和LLM压缩的担忧。本研究介绍了一种新的评估压缩LLM的方法，即不同的令牌指标（DTM），解决了传统指标如困惑度无法准确反映文本生成质量的局限性。DTM关注令牌的差异性，提供了对模型压缩微妙之处的更深入洞察。我们的结果表明，在不损害文本生成质量的情况下，可以达到显著的精确度和稀疏度水平。此外，DTM还可以更精确地评估每个组件的影响。利用第一个不同的令牌指标（FDTM）在模型稀疏化中显示，超过90%的所有组件可以修剪掉。对于量化，FDTM表明超过80%的参数可以进行量化。

    Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. Their ever-increasing size, however, raised concerns about their effective deployment and the need for LLM compressions. This study introduces the Divergent Token metrics (DTMs), a novel approach for assessing compressed LLMs, addressing the limitations of traditional measures like perplexity that fail to accurately reflect text generation quality. DTMs focus on token divergence, providing deeper insights into the subtleties of model compression. Our results indicate that significant levels of precision and sparsity can be achieved without compromising text generation quality. Moreover, DTMs offers a more precise evaluation of each component's impact individually. Utilizing the First Divergent Token metric (FDTM) in model sparsification reveals that nearly 20% of all components can be pruned over 90%. In terms of quantization, the FDTM suggests that over 80% of parameters can be s
    
[^140]: "一刀切"？ 跨身份语言特征的 NLG 系统观察与期望

    "One-size-fits-all"? Observations and Expectations of NLG Systems Across Identity-Related Language Features. (arXiv:2310.15398v1 [cs.CL])

    [http://arxiv.org/abs/2310.15398](http://arxiv.org/abs/2310.15398)

    通过扰动不同类型的身份相关语言特征，研究探索了NLG系统行为的公平中的适应性和不变性之间的紧张关系。研究结果发现，适应的动机包括社会规范、文化差异、特定特征信息和适应性；不变性的动机包括支持规定主义的观点、将适应视为不必要过程，并对错误假设持谨慎态度。这些发现突显了定义公平相关NLG系统行为的挑战。

    

    关于什么构成适当的自然语言生成（NLG）系统行为的公平相关假设，从不变性，即期望系统对社会群体做出相同的响应，到适应性，即期望系统在不同社会群体之间产生不同响应，存在不同观点。我们设计并进行了五个案例研究，通过扰动不同类型的与身份相关的语言特征（姓名、角色、地点、方言和风格）来阐明不变性和适应性之间的紧张关系。我们概述了人们对系统行为的期望，并提出了这两种对立但常见的假设的潜在警示。我们发现适应的动机包括社会规范、文化差异、特定特征信息和适应性；不变性的动机包括支持规定主义的观点、将适应视为 NLG 系统难以适当完成的不必要过程，并对错误假设持谨慎态度。我们的研究结果突显了关于定义公平相关NLG系统行为的挑战。

    Fairness-related assumptions about what constitutes appropriate NLG system behaviors range from invariance, where systems are expected to respond identically to social groups, to adaptation, where responses should instead vary across them. We design and conduct five case studies, in which we perturb different types of identity-related language features (names, roles, locations, dialect, and style) in NLG system inputs to illuminate tensions around invariance and adaptation. We outline people's expectations of system behaviors, and surface potential caveats of these two contrasting yet commonly-held assumptions. We find that motivations for adaptation include social norms, cultural differences, feature-specific information, and accommodation; motivations for invariance include perspectives that favor prescriptivism, view adaptation as unnecessary or too difficult for NLG systems to do appropriately, and are wary of false assumptions. Our findings highlight open challenges around definin
    
[^141]: 在大型语言模型中评估多智能体协调能力

    Evaluating Multi-Agent Coordination Abilities in Large Language Models. (arXiv:2310.03903v1 [cs.CL])

    [http://arxiv.org/abs/2310.03903](http://arxiv.org/abs/2310.03903)

    本研究构建了使用大型语言模型（LLMs）的智能体，并评估其在多智能体协调中的有效性。我们引入了LLM-Co框架，用于在三个游戏环境中评估LLMs的协调能力。评估结果显示LLMs具有推断伙伴意图和理解其行动的能力。

    

    当代人工智能研究的一个重要目标是开发能够熟练进行多智能体协调、有效与人类和其他系统合作的智能体。大型语言模型（LLM）以其显著的理解、生成和解释语言的能力成为开发这种智能体的有希望的候选模型。本研究中，我们构建了使用LLM构建的智能体，并评估其在各种协调场景中的有效性。我们引入了特别设计的LLM-Co框架，使LLM能够参与协调游戏。通过LLM-Co框架，我们在三个游戏环境中进行评估，并将评估分为五个方面：心智理论、情境推理、持续协调、对合作伙伴的稳健性和明确辅助。首先，心智理论和情境推理的评估揭示了LLM推断伙伴意图和理解其行动的能力。

    A pivotal aim in contemporary AI research is to develop agents proficient in multi-agent coordination, enabling effective collaboration with both humans and other systems. Large Language Models (LLMs), with their notable ability to understand, generate, and interpret language in a human-like manner, stand out as promising candidates for the development of such agents. In this study, we build and assess the effectiveness of agents crafted using LLMs in various coordination scenarios. We introduce the LLM-Coordination (LLM-Co) Framework, specifically designed to enable LLMs to play coordination games. With the LLM-Co framework, we conduct our evaluation with three game environments and organize the evaluation into five aspects: Theory of Mind, Situated Reasoning, Sustained Coordination, Robustness to Partners, and Explicit Assistance. First, the evaluation of the Theory of Mind and Situated Reasoning reveals the capabilities of LLM to infer the partner's intention and reason actions acco
    
[^142]: DecoderLens: 编码器-解码器Transformer的逐层解释

    DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers. (arXiv:2310.03686v1 [cs.CL])

    [http://arxiv.org/abs/2310.03686](http://arxiv.org/abs/2310.03686)

    DecoderLens是一种用于解释编码器-解码器Transformer模型中内部状态的新方法。通过让解码器跨越中间编码器层的表示进行交叉注意，DecoderLens将先前无法解释的向量表示映射到可解释的单词或符号序列，揭示了模型在低层或中间层解决的特定子任务，为信息流提供了新的洞察。

    

    近年来，已经提出了许多可解释性方法来帮助解释Transformer模型的内部状态，这些方法在不同的精度和复杂性级别上工作。在这里，为了分析编码器-解码器Transformer，我们提出了一种简单而新颖的方法：DecoderLens。受到了LogitLens（用于仅解码器的Transformer）的启发，这种方法允许解码器跨越中间编码器层的表示进行交叉注意，而不是像常规的编码器-解码器模型中那样使用最终的编码器输出。因此，这种方法将先前无法解释的向量表示映射到人类可解释的单词或符号序列。我们报告了应用于问题回答、逻辑推理、语音识别和机器翻译训练模型的DecoderLens的结果。DecoderLens在低层或中间层揭示了解决的几个特定子任务，为这个重要模型类别中的编码器组件内的信息流提供了新的光明。

    In recent years, many interpretability methods have been proposed to help interpret the internal states of Transformer-models, at different levels of precision and complexity. Here, to analyze encoder-decoder Transformers, we propose a simple, new method: DecoderLens. Inspired by the LogitLens (for decoder-only Transformers), this method involves allowing the decoder to cross-attend representations of intermediate encoder layers instead of using the final encoder output, as is normally done in encoder-decoder models. The method thus maps previously uninterpretable vector representations to human-interpretable sequences of words or symbols. We report results from the DecoderLens applied to models trained on question answering, logical reasoning, speech recognition and machine translation. The DecoderLens reveals several specific subtasks that are solved at low or intermediate layers, shedding new light on the information flow inside the encoder component of this important class of model
    
[^143]: UniverSLU:单个网络用于多样分类和序列生成任务的通用口语理解

    UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network. (arXiv:2310.02973v1 [cs.CL])

    [http://arxiv.org/abs/2310.02973](http://arxiv.org/abs/2310.02973)

    这项研究提出了一种单一的多任务学习模型"UniverSLU"，通过利用预训练的自动语音识别模型和不同的任务和数据集指定器作为离散提示，成功地在多个口语理解任务上取得了有竞争力的性能，并且甚至超过了特定任务的模型。

    

    最近的研究表明，采用具备多任务能力的大型语言模型可以取得良好的效果。它们利用提示来引导模型的行为，并且超越了特定任务模型的性能。受此启发，我们问：我们能否构建一个单一模型来共同执行各种口语理解任务？为了解决这个问题，我们利用预训练的自动语音识别（ASR）模型，并采用不同的任务和数据集指定器作为离散提示。我们展示了我们的单一多任务学习（MTL）模型"UniverSLU"在12个不同的语音分类和序列生成任务上的有效性，涵盖了17个数据集和9种语言。结果表明，UniverSLU取得了有竞争力的性能，甚至超过了特定任务的模型。我们还进行了初步研究，探索了使用人类可解释的自然短语代替任务指定器作为离散提示，并测试了模型对新释义的泛化能力。

    Recent studies have demonstrated promising outcomes by employing large language models with multi-tasking capabilities. They utilize prompts to guide the model's behavior and surpass performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly perform various spoken language understanding (SLU) tasks? To address this, we utilize pre-trained automatic speech recognition (ASR) models and employ various task and dataset specifiers as discrete prompts. We demonstrate efficacy of our single multi-task learning (MTL) model "UniverSLU" for 12 different speech classification and sequence generation tasks across 17 datasets and 9 languages. Results show that UniverSLU achieves competitive performance and even surpasses task-specific models. We also conduct preliminary investigations into enabling human-interpretable natural phrases instead of task specifiers as discrete prompts and test the model's generalization capabilities to new paraphrases.
    
[^144]: 基于中心元素识别的嵌套事件抽取

    Nested Event Extraction upon Pivot Element Recogniton. (arXiv:2309.12960v1 [cs.CL])

    [http://arxiv.org/abs/2309.12960](http://arxiv.org/abs/2309.12960)

    本文提出了一种名为PerNee的新模型，通过识别中心元素来提取嵌套事件。该模型解决了现有NEE方法无法处理中心元素双重身份的问题，并通过提示学习将事件类型和参数角色的信息纳入其中，以提高NEE性能。

    

    嵌套事件抽取（NEE）旨在提取包含其他事件作为其参数的复杂事件结构。嵌套事件涉及一种称为中心元素（PEs）的元素，它同时作为外部事件的参数和内部事件的触发器，并将它们连接成嵌套结构。PEs的这种特殊特性给现有的NEE方法带来了挑战，因为它们不能很好地处理PEs的双重身份。因此，本文提出了一种新模型，称为PerNee，主要基于识别PEs来提取嵌套事件。具体而言，PerNee首先识别内部和外部事件的触发器，然后通过分类触发器对之间关系类型来识别PEs。为了获得更好的触发器和参数表示以进一步提高NEE性能，PerNee通过提示学习将事件类型和参数角色的信息纳入其中。由于现有的NEE数据集（例如Gen）

    Nested Event Extraction (NEE) aims to extract complex event structures where an event contains other events as its arguments recursively. Nested events involve a kind of Pivot Elements (PEs) that simultaneously act as arguments of outer events and as triggers of inner events, and thus connect them into nested structures. This special characteristic of PEs brings challenges to existing NEE methods, as they cannot well cope with the dual identities of PEs. Therefore, this paper proposes a new model, called PerNee, which extracts nested events mainly based on recognizing PEs. Specifically, PerNee first recognizes the triggers of both inner and outer events and further recognizes the PEs via classifying the relation type between trigger pairs. In order to obtain better representations of triggers and arguments to further improve NEE performance, it incorporates the information of both event types and argument roles into PerNee through prompt learning. Since existing NEE datasets (e.g., Gen
    
[^145]: 利用语境信息实现有效的实体重要性检测

    Leveraging Contextual Information for Effective Entity Salience Detection. (arXiv:2309.07990v1 [cs.CL])

    [http://arxiv.org/abs/2309.07990](http://arxiv.org/abs/2309.07990)

    本文研究了有效的实体重要性检测方法，通过对中等规模的语言模型进行微调，比传统的特征工程方法获得了更好的性能。研究还发现使用指令调校的语言模型进行零样本提示效果较差。

    

    在新闻文章等文本文档中，内容和关键事件通常围绕着文档中提到的一部分实体展开。这些实体通常被认为是重要的实体，对于读者来说，它们提供了关于文档内容的有用线索。识别实体的重要性在搜索、排名和基于实体的摘要等多个下游应用中都有帮助。先前的工作主要集中在需要进行大量特征工程的机器学习模型上。我们表明，使用交叉编码器风格架构对中等规模的语言模型进行微调，比特征工程方法获得了显著的性能提升。为此，我们对四个公开可用的数据集使用代表中等预训练语言模型家族的模型进行了全面的基准测试。此外，我们还表明，使用指令调校的语言模型进行零样本提示会产生较差的结果，表明指令调校的语言模型回答的问题数量过少。

    In text documents such as news articles, the content and key events usually revolve around a subset of all the entities mentioned in a document. These entities, often deemed as salient entities, provide useful cues of the aboutness of a document to a reader. Identifying the salience of entities was found helpful in several downstream applications such as search, ranking, and entity-centric summarization, among others. Prior work on salient entity detection mainly focused on machine learning models that require heavy feature engineering. We show that fine-tuning medium-sized language models with a cross-encoder style architecture yields substantial performance gains over feature engineering approaches. To this end, we conduct a comprehensive benchmarking of four publicly available datasets using models representative of the medium-sized pre-trained language model family. Additionally, we show that zero-shot prompting of instruction-tuned language models yields inferior results, indicati
    
[^146]: CReHate: 跨文化重新注释英语仇恨言论数据集

    CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset. (arXiv:2308.16705v1 [cs.CL])

    [http://arxiv.org/abs/2308.16705](http://arxiv.org/abs/2308.16705)

    CReHate通过跨文化重新注释英语仇恨言论数据集，揭示了来自不同国家的个体对仇恨言论的不同看法，并引入了一种具有文化敏感性的分类器。这些发现强调了重新评估NLP研究在仇恨言论领域的必要性。

    

    英语数据集主要反映了特定国家的观点，这可能导致模型和数据集中存在文化偏差。这在受主观性影响较大的任务，如仇恨言论检测中特别有问题。为了深入了解来自不同国家的个体如何理解仇恨言论，我们介绍了CReHate，对抽样的SBIC数据集进行了跨文化重新注释。该数据集包括来自五个不同国家的注释：澳大利亚、新加坡、南非、英国和美国。我们进行了彻底的统计分析，发现基于国籍存在显著差异，只有59.4%的样本在所有国家之间达成共识。我们还通过迁移学习引入了一种具有文化敏感性的仇恨言论分类器，能够捕捉不同国籍的观点。这些发现强调了需要重新评估自然语言处理研究的某些方面，特别是对于仇恨言论的细微性质。

    English datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. This is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. To delve into how individuals from different countries perceive hate speech, we introduce CReHate, a cross-cultural re-annotation of the sampled SBIC dataset. This dataset includes annotations from five distinct countries: Australia, Singapore, South Africa, the United Kingdom, and the United States. Our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. We also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. These findings underscore the need to re-evaluate certain aspects of NLP research, especially with regard to the nuanced nature of hate spe
    
[^147]: 使用虚拟提示注入向指令调整的大型语言模型后门

    Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection. (arXiv:2307.16888v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.16888](http://arxiv.org/abs/2307.16888)

    这项研究介绍了一种针对指令调整的大型语言模型的新型后门攻击方法，即虚拟提示注入（VPI）。通过在特定触发场景下将虚拟提示与用户指令连接，攻击者可以精细操纵模型的回应而无需明确注入。

    

    指令调整的大型语言模型（LLM）表现出了根据人类指令调节其回应的非凡能力。然而，这种调节能力也引入了潜在的攻击者通过植入后门来对模型功能进行精细操纵的可能性。在本文中，我们介绍了一种针对指令调整的LLM定制的新型后门攻击设置-虚拟提示注入（VPI）。在VPI攻击中，期望通过在特定触发场景下将攻击者指定的虚拟提示连接到用户指令中，使植入后门的模型表现得像是在其输入中没有明确的注入。例如，如果LLM被虚拟提示"负面描述乔·拜登"植入后门的触发场景是讨论乔·拜登，那么当谈论乔·拜登时，模型将传播负面倾向的观点。 VPI尤其有害，因为攻击者可以进行细粒度的操纵。

    Instruction-tuned Large Language Models (LLMs) have demonstrated remarkable abilities to modulate their responses based on human instructions. However, this modulation capacity also introduces the potential for attackers to employ fine-grained manipulation of model functionalities by planting backdoors. In this paper, we introduce Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt "Describe Joe Biden negatively." for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden. VPI is especially harmful as the attacker can take fine-grained and 
    
[^148]: 针对大型语言模型的可编解码文本水印技术研究

    Towards Codable Text Watermarking for Large Language Models. (arXiv:2307.15992v1 [cs.CL])

    [http://arxiv.org/abs/2307.15992](http://arxiv.org/abs/2307.15992)

    这项研究对于大型语言模型的可编解码文本水印技术进行了系统研究，提出了一种允许文本水印携带更多可定制化信息的方法，解决了现有水印方法编码效率低、不能满足不同应用场景需求的问题。

    

    随着大型语言模型（LLMs）生成的文本日益流畅和逼真，有必要识别文本的来源以防止LLMs的滥用。文本水印技术通过将隐藏的模式注入到生成的文本中已被证实可以可靠地区分是否由LLMs生成的文本。然而，我们认为现有的LLMs水印方法在编码效率上存在问题（只包含一个位的信息，即文本是否由LLMs生成），并且不能灵活地满足不同LLMs应用场景中的多样化信息编码需求（如编码模型版本、生成时间、用户ID等）。在这项工作中，我们首次对LLMs的可编解码文本水印（CTWL）进行了系统研究，允许文本水印携带更多可定制化的信息。首先，我们研究了LLMs水印技术的分类，为CTWL提供了数学公式。此外，我们提供了一份全面的研究视图，涵盖了CTWL的各个方面。

    As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns into the generated texts. However, we argue that existing watermarking methods for LLMs are encoding-inefficient (only contain one bit of information whether it is generated from an LLM or not) and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.) in different LLMs application scenarios. In this work, we conduct the first systematic study on the topic of Codable Text Watermarking for LLMs (CTWL) that allows text watermarks to carry more customizable information. First of all, we study the taxonomy of LLM watermarking technology and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive
    
[^149]: CamemBERT-bio：一种更健康的法语语言模型

    CamemBERT-bio: a Tasty French Language Model Better for your Health. (arXiv:2306.15550v1 [cs.CL])

    [http://arxiv.org/abs/2306.15550](http://arxiv.org/abs/2306.15550)

    本研究介绍了CamemBERT-bio，它是一种针对法语生物医学领域专门设计的语言模型，相对于通用模型在命名实体识别任务上平均提高了2.54个百分点。

    

    通过临床数据仓库，医院中的临床数据变得越来越容易用于研究，然而这些文件都是非结构化的。因此，需要从医疗报告中提取信息以进行临床研究。使用CamemBERT等BERT-like模型的迁移学习已经取得了重大进展，特别是命名实体识别方面。然而，这些模型是为通用语言训练的，在生物医学数据上效果较弱。因此，我们提出了一种新的法语公共生物医学数据集，对CamemBERT进行了继续预训练。因此，我们介绍了CamemBERT-bio的第一个版本，它是一种为法语生物医学领域专门设计的公共模型，在不同的生物医学命名实体识别任务上平均F1分数提高了2.54个百分点。

    Clinical data in hospitals are increasingly accessible for research through clinical data warehouses, however these documents are unstructured. It is therefore necessary to extract information from medical reports to conduct clinical studies. Transfer learning with BERT-like models such as CamemBERT has allowed major advances, especially for named entity recognition. However, these models are trained for plain language and are less efficient on biomedical data. This is why we propose a new French public biomedical dataset on which we have continued the pre-training of CamemBERT. Thus, we introduce a first version of CamemBERT-bio, a specialized public model for the French biomedical domain that shows 2.54 points of F1 score improvement on average on different biomedical named entity recognition tasks.
    
[^150]: 在Transformer语言模型中解决关系任务的机制

    A Mechanism for Solving Relational Tasks in Transformer Language Models. (arXiv:2305.16130v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16130](http://arxiv.org/abs/2305.16130)

    这篇论文研究了在Transformer语言模型中解决关系任务的机制，并发现这些模型利用简单的线性更新来处理关系任务，并以内容无关的方式促进关系的输出。

    

    这篇论文提供了证据表明，尽管语言模型（LMs）的规模和复杂性，它们有时候利用一个简单的计算机制来解决一对一的关系任务（例如 capital_of(Poland)=Warsaw）。我们在上下文学习环境中研究了一系列语言模型的大小（从124M参数到176B参数），并发现对于多种任务（涉及首都、大写和过去时态等），机制的关键部分可以简化为前馈（FFN）网络通常应用的简单线性更新。这些更新也倾向于以内容无关的方式促进关系的输出（例如对编码 Poland:Warsaw::China:Beijing），揭示了这些模型在解决这些任务中的可预测模式。我们进一步显示这个机制是特定于需要从预训练存储器中检索而不是从局部上下文检索的任务。我们的结果为解决关系任务的语言模型的机制做出了贡献。

    A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple computational mechanism to solve one-to-one relational tasks (e.g., capital_of(Poland)=Warsaw). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, upper-casing, and past-tensing) a key part of the mechanism reduces to a simple linear update typically applied by the feedforward (FFN) networks. These updates also tend to promote the output of the relation in a content-independent way (e.g., encoding Poland:Warsaw::China:Beijing), revealing a predictable pattern that these models take in solving these tasks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a g
    
[^151]: 训练指令作为后门: 大规模语言模型指令调整的后门漏洞

    Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])

    [http://arxiv.org/abs/2305.14710](http://arxiv.org/abs/2305.14710)

    使用指令调整方法在众包数据集上训练的大规模语言模型，存在后门漏洞，攻击者只需注入极少量的恶意指令便可永久控制模型行为，且难以被修复，需要更加健全的防御机制。

    

    本文研究使用指令调整方法在众包数据集上训练的模型，其目的是达到更好的性能表现。然而，我们提出了一个与该培训范例相关的安全问题。研究表明，攻击者只需在成千上万的数据中注入极少量的恶意指令，便可以通过数据毒化来控制模型行为，甚至无需修改数据实例或标签本身。通过这种指令攻击，攻击者可以在四个常用的 NLP 数据集上实现超过90% 的攻击成功率，并引起易于转移到 15 种不同数据集的持久后门。这种攻击还可以直接应用于多个数据集的有毒指令。最后，该攻击显示出对现有推理时防御的抵抗力。这些发现凸显了在语言模型训练中需要更为健全的防御机制。

    Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defens
    

