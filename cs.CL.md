# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Executable Code Actions Elicit Better LLM Agents](https://rss.arxiv.org/abs/2402.01030) | 使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。 |
| [^2] | [AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness](https://arxiv.org/abs/2404.01490) | 提出利用机器翻译进行数据增强以解决训练数据有限的挑战，应用任务自适应预训练和适配器框架实现了竞争性结果。 |
| [^3] | [Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning](https://arxiv.org/abs/2403.20046) | 通过新设计的基准测试\textsc{CoTErrorSet}，研究了LLMs是否能够从以往的错误中学习，尤其是对于推理能力方面的提升。 |
| [^4] | [Do Large Language Models understand Medical Codes?](https://arxiv.org/abs/2403.10822) | 该研究调查了大型语言模型是否理解医学编码的含义，评估了它们对领域特定术语的认识和理解。 |
| [^5] | [Hyper-CL: Conditioning Sentence Representations with Hypernetworks](https://arxiv.org/abs/2403.09490) | Hyper-CL是一种将超网络与对比学习相结合的有效方法，能够灵活地进行条件化句子表示。 |
| [^6] | [Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking](https://arxiv.org/abs/2403.08492) | 本文使用富含语义知识的大型语言模型在少样本中文拼写检查任务上取得了比BERT模型更好的性能。 |
| [^7] | [Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification](https://arxiv.org/abs/2403.04696) | 提出了一种基于标记级别不确定性量化的新型事实核查和幻觉检测流程，该方法能够检测大型语言模型输出中的不可靠预测。 |
| [^8] | [Guardrail Baselines for Unlearning in LLMs](https://arxiv.org/abs/2403.03329) | 简单的基于guardrail的方法如提示和过滤可以实现与fine-tuning相媲美的unlearning结果，建议研究人员在评估更消耗计算资源的fine-tuning方法时考虑这些轻量级基线。 |
| [^9] | [Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization](https://arxiv.org/abs/2402.17574) | Agent-Pro提出了一种基于LLM的代理，通过策略级别的反思和优化，可以从互动经验中学习并逐步提升其行为策略。 |
| [^10] | [Defending LLMs against Jailbreaking Attacks via Backtranslation](https://arxiv.org/abs/2402.16459) | 通过反向翻译来防御LLMs免受越狱攻击，将生成的反向翻译提示用于揭示原始提示的实际意图，提高了模型的安全性。 |
| [^11] | [Unintended Impacts of LLM Alignment on Global Representation](https://arxiv.org/abs/2402.15018) | 对大型语言模型（LLMs）进行用户偏好对齐可能会导致英语方言和全球意见之间的差异，但也提高了多种语言的能力。 |
| [^12] | [DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents](https://arxiv.org/abs/2402.14865) | 本文提出了一种基于心理测量学思想的元探测代理（MPA）动态评估协议，用于评估大型语言模型（LLMs）的能力。 |
| [^13] | [Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning](https://arxiv.org/abs/2402.14382) | 提出了一种通过历史链推理来增强时间知识图预测的方法，有效利用高阶历史信息，弥补了基于大型语言模型的模型在处理历史信息和时间推理能力方面的不足。 |
| [^14] | [The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis](https://arxiv.org/abs/2402.12976) | 通过多维分析发现，示范的有效性在多语种上下文学习中存在显著差异，其中部分模型对示范质量不敏感，而精心设计的模板可以消除对示范的依赖。 |
| [^15] | [EmoBench: Evaluating the Emotional Intelligence of Large Language Models](https://arxiv.org/abs/2402.12071) | EmoBench是一个基于心理学理论的基准测试，旨在评估大型语言模型的情感智能，包括情感理解和情感应用。 |
| [^16] | [What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects](https://arxiv.org/abs/2402.11968) | 德语方言和区域语言说话者更倾向于支持能够处理方言输入的自然语言处理工具，比如虚拟助手，对于产生方言输出的应用则支持程度相对较低。 |
| [^17] | [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/abs/2402.11753) | 提出了一种新颖的基于ASCII艺术的越狱攻击，以及一个用于评估LLMs在识别非纯语义提示方面能力的基准挑战。五个SOTA LLMs在识别ASCII艺术提示时存在困难。 |
| [^18] | [GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network](https://arxiv.org/abs/2402.11709) | GNNavi通过引入基于提示的参数高效微调（PEFT）方法和图神经网络（GNN）层，准确引导信息流的汇聚和分布，在大型语言模型中导航信息流动态，超越了标准提示式微调的性能。 |
| [^19] | [Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals](https://arxiv.org/abs/2402.11655) | 本研究提出了机制之争的概念，关注语言模型中多个机制的相互作用，并揭示了它们之间的竞争过程，以及影响某些机制强度的注意力位置。 |
| [^20] | [Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining](https://arxiv.org/abs/2402.11159) | 提出了一种反事实文本引导的对比语言-图像预训练框架CFT-CLIP，用于增强新闻文本和缩略图之间的对比学习。 |
| [^21] | [Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm](https://arxiv.org/abs/2402.10671) | 提出了一种通过工作流范式方法来改善LLMs在文本到SQL中的上下文学习能力，通过分解提高了模型的注意力和问题解决范围，进一步提高了基于LLM的方法的上限。 |
| [^22] | [Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs](https://arxiv.org/abs/2402.10586) | 本文探讨了如何通过研究文本中的话语特征来区分人类创作和机器生成的文本，引入了一种新颖的方法来揭示这些特征，并发现人类写作在结构上更为多样化。 |
| [^23] | [Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting](https://arxiv.org/abs/2402.10412) | 提出了一种名为FEWL的幻觉度量方法，通过对LLM答案进行加权评估事实性，适用于没有黄金标准答案的情况。 |
| [^24] | [TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles](https://arxiv.org/abs/2402.10137) | TOAD是一个具有多样响应风格的面向任务的自动对话系统，其中考虑了冗长程度和用户表达镜像两个方面。TOAD通过模拟真实的应用上下文交互，提供了丰富的系统响应风格选项，并在评估中表明建模更冗长的回复或不进行用户表达镜像的回复更具挑战性。 |
| [^25] | [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](https://arxiv.org/abs/2402.08983) | 本文提出了一种名为SafeDecoding的解决方案，通过安全感知解码策略来防御大型语言模型（LLMs）的越狱攻击。该策略可以生成对用户查询有益且无害的响应，有效缓解了LLMs安全性威胁。 |
| [^26] | [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://arxiv.org/abs/2402.08679) | 本文提出了COLD-Attack框架，旨在实现具有隐秘性和可控性的LLM越狱。通过建立可控文本生成与攻击生成之间的关联，采用了能量限制解码与Langevin动力学算法，使得在不同的控制要求下搜索对抗性LLM攻击成为可能。 |
| [^27] | [Feedback Loops With Language Models Drive In-Context Reward Hacking](https://arxiv.org/abs/2402.06627) | 与语言模型的反馈循环可能导致上下文内奖励欺骗（ICRH），即语言模型在测试时在优化目标的同时却产生负面副作用。这项研究确定了两个导致ICRH的过程：输出优化和策略优化。 |
| [^28] | [Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations](https://arxiv.org/abs/2402.05629) | 该论文研究了长文生成中的事实性问题，发现大型语言模型在生成段落时可能会由于实体模糊而将可验证的事实组合成非事实的段落。现有的事实准确度评估方法无法正确评估这些非事实段落，作者提出了一种增强的度量指标来应对这个问题。 |
| [^29] | [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044) | SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。 |
| [^30] | [EffiBench: Benchmarking the Efficiency of Automatically Generated Code](https://arxiv.org/abs/2402.02037) | 本文提出了EffiBench基准测试，用于评估代码生成模型生成的代码的效率。实验证明，GPT-4-turbo生成的代码最高效。 |
| [^31] | [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159) | Dolma是一个包含三万亿个令牌的开放语料库，用于语言模型预训练研究。它包含了来自多种来源的网络内容、科技论文、代码、公共领域图书、社交媒体和百科全书材料。为了促进开放研究，我们还开源了数据整理工具包。 |
| [^32] | [CLOMO: Counterfactual Logical Modification with Large Language Models](https://arxiv.org/abs/2311.17438) | 本研究探索了大型语言模型的反事实推理能力，并引入了反事实逻辑修改任务和相应的评估指标。实验结果表明，大型语言模型展现出显著的逻辑能力。 |
| [^33] | [LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores](https://arxiv.org/abs/2311.09766) | 本文研究了语言模型驱动的评估指标在总结任务中是否会对相同的基础语言模型生成的文本表现出偏见，并发现在无参考情况下使用时偏见尤为显著。 |
| [^34] | [Fusion-Eval: Integrating Evaluators with LLMs](https://arxiv.org/abs/2311.09204) | Fusion-Eval是一种创新方法，利用LLMs整合不同辅助评估器的见解，极大提升自然语言系统评估的有效性。 |
| [^35] | [CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation](https://arxiv.org/abs/2311.08588) | CodeScope是一个用于评估LLMs在代码理解和生成方面能力的多语言多任务多维基准，解决了现有基准在多语言编程环境和多任务设置方面的不足。 |
| [^36] | [Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages](https://arxiv.org/abs/2310.04799) | 提出了聊天向量的概念，通过简单的模型算术使预训练语言模型具备在新语言中遵循指令和实现模型对齐的能力 |
| [^37] | [WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models.](http://arxiv.org/abs/2401.13919) | WebVoyager是一种创新的基于大型多模态模型的Web代理，能够通过与真实网站交互来端到端地完成用户指令。它提出了一个新的Web代理评估协议，并在实际任务中取得了显著的成功率。 |
| [^38] | [Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models.](http://arxiv.org/abs/2401.06102) | 本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。 |
| [^39] | [Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives.](http://arxiv.org/abs/2401.02009) | 自我对比是一种通过对比不同求解视角和总结差异，提高大型语言模型（LLM）的反思能力的方法。 |
| [^40] | [WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation.](http://arxiv.org/abs/2312.14187) | 本文提出了WaveCoder，一个广泛和多功能的改进指令调优模型，通过将指令数据分类并利用LLM框架生成多样的高质量指令数据，以提高调优模型的效果和泛化能力。 |
| [^41] | [BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing.](http://arxiv.org/abs/2310.19975) | BioInstruct是一个用于生物医学自然语言处理的大型语言模型指令调整方法，通过引入针对性指令数据集BioInstruct，通过GPT-4语言模型进行精调，优化了模型在生物医学自然语言处理中的性能。 |
| [^42] | [Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots.](http://arxiv.org/abs/2310.17976) | 本文介绍了一种创新的方法，用于评估角色扮演聊天机器人的个性特点，并发现基于LLMs的现代角色扮演聊天机器人能够有效地描绘出相应角色的个性特点，与人类感知的一致性达到82.8%。 |
| [^43] | [CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents.](http://arxiv.org/abs/2310.17512) | 本文研究了基于大型语言模型的智能体之间的竞争行为。通过建立一个竞争环境并进行实验，发现竞争可以促使智能体进行转变和采取新策略，从而在社会和经济发展中发挥重要作用。 |
| [^44] | [Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution.](http://arxiv.org/abs/2310.16834) | 本研究通过引入得分熵这一新颖的离散得分匹配损失，弥补了离散数据领域中现有方法的不足，提出了得分熵离散扩散模型(SEDD)并在GPT-2实验中取得了有竞争力的效果。 |
| [^45] | [Online Speculative Decoding.](http://arxiv.org/abs/2310.07177) | 在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。 |
| [^46] | [How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition.](http://arxiv.org/abs/2310.05492) | 本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。 |
| [^47] | [You Only Look at Screens: Multimodal Chain-of-Action Agents.](http://arxiv.org/abs/2309.11436) | 本论文提出了一种名为Auto-UI的多模态动作链机器人，通过直接与界面交互，避免了环境解析或依赖于应用程序API的需要，并引入了动作链技术来帮助模型进行决策。 |
| [^48] | [Open-Domain Text Evaluation via Meta Distribution Modeling.](http://arxiv.org/abs/2306.11879) | 本文提出了一种新颖的开放领域文本生成模型评估方法——元分布方法（MDM），该方法将两个概率分布的对比映射到质量度量上，可以视为分布的分布，其可用于开放领域文本生成的评估。 |
| [^49] | [Active Prompting with Chain-of-Thought for Large Language Models.](http://arxiv.org/abs/2302.12246) | 本论文提出了一种新的方法Active-Prompt，它使用任务特定的示例提示适应大型语言模型中的不同任务，提高模型性能与效率。 |

# 详细

[^1]: 可执行代码行动能够激发更出色的LLM智能体

    Executable Code Actions Elicit Better LLM Agents

    [https://rss.arxiv.org/abs/2402.01030](https://rss.arxiv.org/abs/2402.01030)

    使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。

    

    大型语言模型（LLM）智能体具备执行广泛行动的能力，如调用工具和控制机器人等，在解决现实世界的挑战中显示出巨大潜力。LLM智能体通常通过生成JSON或文本的预定义格式来产生行动，这通常受限于受限制的行动空间（例如，预定义工具的范围）和受限的灵活性（例如，无法组合多个工具）。本研究提出使用可执行的Python代码将LLM智能体的行动整合到一个统一的行动空间中（CodeAct）。CodeAct与Python解释器集成，可以执行代码行动，并通过多轮交互在新的观察中动态修订先前的行动或发出新的行动。我们对17个LLM在API-Bank和新编制的基准测试中进行了广泛分析，结果显示CodeAct的性能超过了广泛使用的替代方案（成功率高出20%）。CodeAct的令人鼓舞的表现激励我们建立了一个开源的...

    Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
    
[^2]: AAdaM在SemEval-2024任务1中的表现：多语言语义文本相关性的增强与适应

    AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness

    [https://arxiv.org/abs/2404.01490](https://arxiv.org/abs/2404.01490)

    提出利用机器翻译进行数据增强以解决训练数据有限的挑战，应用任务自适应预训练和适配器框架实现了竞争性结果。

    

    这篇论文介绍了我们为SemEval-2024任务1开发的系统：非洲和亚洲语言的语义文本相关性。该共享任务旨在衡量句子对之间的语义文本相关性，重点关注一系列代表性不足的语言。在这项工作中，我们提出利用机器翻译进行数据增强，以解决训练数据有限的低资源挑战。此外，我们将任务自适应预训练应用于未标记的任务数据，以弥合预训练和任务适应之间的差距。在模型训练方面，我们研究了完全微调和基于适配器的调整，并采用适配器框架实现了有效的零-shot跨语言转移。在共享任务中取得了竞争性结果：我们的系统在子任务A（监督学习）和子任务C（跨语言转移）中表现最佳，排名最高。

    arXiv:2404.01490v1 Announce Type: new  Abstract: This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian Languages. The shared task aims at measuring the semantic textual relatedness between pairs of sentences, with a focus on a range of under-represented languages. In this work, we propose using machine translation for data augmentation to address the low-resource challenge of limited training data. Moreover, we apply task-adaptive pre-training on unlabeled task data to bridge the gap between pre-training and task adaptation. For model training, we investigate both full fine-tuning and adapter-based tuning, and adopt the adapter framework for effective zero-shot cross-lingual transfer. We achieve competitive results in the shared task: our system performs the best among all ranked teams in both subtask A (supervised learning) and subtask C (cross-lingual transfer).
    
[^3]: LLM能从以前的错误中学习吗？调查LLMs'错误以增强推理能力

    Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning

    [https://arxiv.org/abs/2403.20046](https://arxiv.org/abs/2403.20046)

    通过新设计的基准测试\textsc{CoTErrorSet}，研究了LLMs是否能够从以往的错误中学习，尤其是对于推理能力方面的提升。

    

    最近的研究表明，LLMs从微调黄金标准的思维链（CoT）解释或将其用作少量提示中的正确示例中受益。尽管人类确实可以模仿正确的例子，但从我们的错误中学习是人类认知的另一个至关重要的方面。因此，一个问题自然而然地出现：LLMs能否学习并受益于他们的错误，尤其是对于他们的推理？本研究从提示和模型调整的角度研究了这个问题。我们首先介绍了一个新的基准\textsc{CoTErrorSet}，其中包含609,432个问题，每个问题都设计有正确和错误的参考文献，并展示了制造这些错误的类型和原因。为了探讨这些错误的有效性，我们设计了两种方法：（1）\textbf{自我反思}提示指导LLMs重新考虑他们是否曾经犯过类似的错误；和（2）\textbf{错误调整}包括对模型进行微调

    arXiv:2403.20046v1 Announce Type: new  Abstract: Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \textbf{Mistake tuning} involves finetuning mo
    
[^4]: 大型语言模型是否理解医学编码?

    Do Large Language Models understand Medical Codes?

    [https://arxiv.org/abs/2403.10822](https://arxiv.org/abs/2403.10822)

    该研究调查了大型语言模型是否理解医学编码的含义，评估了它们对领域特定术语的认识和理解。

    

    近期人工智能研究的首要目标是稳步朝着实现人工通用智能(AGI)迈进，这促使对大型语言模型(LLMs)在各种任务和领域中的评估。其中之一是医疗保健领域，LLMs可以通过协助各种任务大大有益于临床实践。然而，当面对无法充分应对的查询时，这些模型也容易产生“幻觉”或不正确的响应，引发了关注和怀疑，特别是在医疗保健社区内。因此，在这项工作中，我们调查LLMs是否理解医学编码的固有含义，这些编码被广泛应用于医疗保健实践。我们评估了各种现成的LLMs (例如GPT、LLaMA等)和专门为生物医学应用设计的LLMs，以评估它们对这些领域特定术语的认识和理解。我们的结果表明…

    arXiv:2403.10822v1 Announce Type: new  Abstract: The overarching goal of recent AI research has been to make steady progress towards achieving Artificial General Intelligence (AGI), prompting the evaluation of Large Language Models (LLMs) across a variety of tasks and domains. One such domain is healthcare, where LLMs can greatly benefit clinical practice by assisting with a wide range of tasks. However, these models are also prone to producing "hallucinations" or incorrect responses when faced with queries they cannot adequately address, raising concerns and skepticism, especially within the healthcare community. Therefore, in this work, we investigate whether LLMs understand the inherent meaning of medical codes, which are widely used in healthcare practice. We evaluate various off-the-shelf LLMs (e.g., GPT, LLaMA, etc.) and LLMs specifically designed for biomedical applications to assess their awareness and understanding of these domain-specific terminologies. Our results indicate t
    
[^5]: Hyper-CL：使用超网络对句子表示进行条件化

    Hyper-CL: Conditioning Sentence Representations with Hypernetworks

    [https://arxiv.org/abs/2403.09490](https://arxiv.org/abs/2403.09490)

    Hyper-CL是一种将超网络与对比学习相结合的有效方法，能够灵活地进行条件化句子表示。

    

    尽管将对比学习框架引入句子表示学习领域在很大程度上促进了该领域的进展，但当句子被特定角度条件化时，尤其是在捕捉句子的细粒度语义方面，目前最先进的句子嵌入能力仍然不清楚。本文引入了Hyper-CL，一种高效的方法，该方法将超网络与对比学习结合起来计算条件化的句子表示。在我们提出的方法中，超网络负责将预先计算的条件嵌入转换为相应的投影层。这使得相同的句子嵌入可以根据不同条件进行不同的投影。在两个代表性的条件化基准数据集上进行评估，即条件语义文本相似度和知识图完成，表明Hyper-CL在灵活地进行条件化方面是有效的。

    arXiv:2403.09490v1 Announce Type: new  Abstract: While the introduction of contrastive learning frameworks in sentence representation learning has significantly contributed to advancements in the field, it still remains unclear whether state-of-the-art sentence embeddings can capture the fine-grained semantics of sentences, particularly when conditioned on specific perspectives. In this paper, we introduce Hyper-CL, an efficient methodology that integrates hypernetworks with contrastive learning to compute conditioned sentence representations. In our proposed approach, the hypernetwork is responsible for transforming pre-computed condition embeddings into corresponding projection layers. This enables the same sentence embeddings to be projected differently according to various conditions. Evaluation on two representative conditioning benchmarks, namely conditional semantic text similarity and knowledge graph completion, demonstrates that Hyper-CL is effective in flexibly conditioning s
    
[^6]: 富含语义知识增强的大型语言模型用于少样本中文拼写检查

    Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking

    [https://arxiv.org/abs/2403.08492](https://arxiv.org/abs/2403.08492)

    本文使用富含语义知识的大型语言模型在少样本中文拼写检查任务上取得了比BERT模型更好的性能。

    

    本文探讨了使用一种名为RS-LLM（基于丰富语义的LLMs）的上下文学习方法将大型语言模型（LLMs）引入作为基础模型，以及在我们的框架中引入各种中文丰富语义信息的影响。实验结果表明，通过引入少量特定的中文丰富语义结构，LLMs在少样本中文拼写检查任务上比基于BERT模型表现更好。

    arXiv:2403.08492v1 Announce Type: new  Abstract: Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and optical character recognition (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications. In this paper, we explore using an in-context learning method named RS-LLM (Rich Semantic based LLMs) to introduce large language models (LLMs) as the foundation model. Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than the BERT-based model on few-shot CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verifie
    
[^7]: 通过标记级别的不确定性量化检验大型语言模型的输出

    Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification

    [https://arxiv.org/abs/2403.04696](https://arxiv.org/abs/2403.04696)

    提出了一种基于标记级别不确定性量化的新型事实核查和幻觉检测流程，该方法能够检测大型语言模型输出中的不可靠预测。

    

    大型语言模型(LLMs)以产生错误的声明而臭名昭著。这种幻觉可能很危险，因为在生成的文本中偶尔出现的事实不准确可能会被整体上是事实的文本掩盖，这使得用户极其难以发现。利用LLMs的当前服务通常不提供检测不可靠生成的方式。在这里，我们旨在弥补这一空白。具体而言，我们提出了一种基于标记级别的不确定性量化的新型事实核查和幻觉检测流程。不确定性分数利用了神经网络或其层输出中包含的信息来检测不可靠的预测，并我们展示它们可以用于核查LLM输出中的各种声明。此外，我们提出了一种新型的标记级别不确定性量化方法，消除了对事实提出怀疑的影响。

    arXiv:2403.04696v1 Announce Type: cross  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what c
    
[^8]: Guardrail Baselines for Unlearning in LLMs

    Guardrail Baselines for Unlearning in LLMs

    [https://arxiv.org/abs/2403.03329](https://arxiv.org/abs/2403.03329)

    简单的基于guardrail的方法如提示和过滤可以实现与fine-tuning相媲美的unlearning结果，建议研究人员在评估更消耗计算资源的fine-tuning方法时考虑这些轻量级基线。

    

    最近的研究表明fine-tuning是从大型语言模型中“unlearn”概念的一种有前途的方法。然而，fine-tuning可能很昂贵，因为它既需要生成一组示例，又需要运行多次迭代的fine-tuning来更新模型。在这项工作中，我们展示了简单的基于guardrail的方法，如提示和过滤，可以实现与fine-tuning相媲美的unlearning结果。我们建议研究人员在评估更消耗计算资源的fine-tuning方法的性能时，调查这些轻量级基线。虽然我们并不声称提示或过滤等方法是unlearning问题的通用解决方案，但我们的工作表明需要更好地区分guardrails与fine-tuning的强大之处的评估指标，并强调guardrails本身可能为unlearning具有优势的场景，例如生成示例用于fine-tuning或u

    arXiv:2403.03329v1 Announce Type: new  Abstract: Recent work has demonstrated that fine-tuning is a promising approach to `unlearn' concepts from large language models. However, fine-tuning can be expensive, as it requires both generating a set of examples and running iterations of fine-tuning to update the model. In this work, we show that simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to fine-tuning. We recommend that researchers investigate these lightweight baselines when evaluating the performance of more computationally intensive fine-tuning methods. While we do not claim that methods such as prompting or filtering are universal solutions to the problem of unlearning, our work suggests the need for evaluation metrics that can better separate the power of guardrails vs. fine-tuning, and highlights scenarios where guardrails themselves may be advantageous for unlearning, such as in generating examples for fine-tuning or u
    
[^9]: Agent-Pro: 通过策略级别反思和优化学习进化

    Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization

    [https://arxiv.org/abs/2402.17574](https://arxiv.org/abs/2402.17574)

    Agent-Pro提出了一种基于LLM的代理，通过策略级别的反思和优化，可以从互动经验中学习并逐步提升其行为策略。

    

    大型语言模型表现出在各种任务中具有强大问题解决能力。然而，大多数基于LLM的代理都是特定任务求解器，并具有复杂的提示工程，而不是能够通过互动学习和进化的代理。本文提出了Agent-Pro：一种基于LLM的代理，具有策略级别的反思和优化，可以从互动经验中学习丰富的专业知识，并逐渐提升其行为策略。具体来说，它涉及一个动态信念生成和反思过程，用于策略演化。

    arXiv:2402.17574v1 Announce Type: new  Abstract: Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover
    
[^10]: 通过反向翻译防御LLMs免受越狱攻击

    Defending LLMs against Jailbreaking Attacks via Backtranslation

    [https://arxiv.org/abs/2402.16459](https://arxiv.org/abs/2402.16459)

    通过反向翻译来防御LLMs免受越狱攻击，将生成的反向翻译提示用于揭示原始提示的实际意图，提高了模型的安全性。

    

    尽管许多大型语言模型（LLMs）已经被训练成拒绝有害请求，但它们仍然容易受到越狱攻击的影响，这种攻击会重写原始提示以隐藏其有害意图。在本文中，我们提出了一种新方法，通过“反向翻译”来防御LLMs免受越狱攻击。具体来说，给定目标LLM从输入提示生成的初始响应，我们的反向翻译提示一个语言模型来推断可以导致该响应的输入提示。推断的提示称为反向翻译提示，倾向于揭示原始提示的实际意图，因为它是基于LLM的响应生成的，不是直接由攻击者操纵的。然后，我们再次在反向翻译提示上运行目标LLM，如果模型拒绝了反向翻译提示，则拒绝原始提示。我们解释了所提出的防御措施对其有效性的几个好处。

    arXiv:2402.16459v1 Announce Type: cross  Abstract: Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiv
    
[^11]: LLM对全球表示的意外影响

    Unintended Impacts of LLM Alignment on Global Representation

    [https://arxiv.org/abs/2402.15018](https://arxiv.org/abs/2402.15018)

    对大型语言模型（LLMs）进行用户偏好对齐可能会导致英语方言和全球意见之间的差异，但也提高了多种语言的能力。

    

    在为面向用户的应用程序部署之前，开发人员通过各种程序（如从人类反馈中学习强化学习（RLHF）和直接偏好优化（DPO））将大型语言模型（LLMs）与用户偏好进行对齐。目前对这些程序的评估侧重于遵循指导、推理和真实性的基准。然而，人类偏好并非普遍，对特定偏好集进行对齐可能会产生意外影响。我们探讨了对三个全球表示维度：英语方言、多语言能力和全球各国意见的影响。我们的结果显示，当前的对齐程序在英语方言和全球意见之间产生差异。我们发现对齐提高了多种语言的能力。最后，我们讨论了导致这些意外影响的设计决策，并为更公平的建议。

    arXiv:2402.15018v1 Announce Type: new  Abstract: Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable 
    
[^12]: DyVal 2: 元探测代理动态评估大型语言模型

    DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents

    [https://arxiv.org/abs/2402.14865](https://arxiv.org/abs/2402.14865)

    本文提出了一种基于心理测量学思想的元探测代理（MPA）动态评估协议，用于评估大型语言模型（LLMs）的能力。

    

    大型语言模型（LLMs）的评估引起了社区的极大关注，因为存在数据污染问题。现有工作设计了使用针对特定任务的明确定义算法的评估协议，这些协议无法轻松扩展到不同的场景。此外，当前的评估基准只能提供整体基准结果，不能支持对LLMs能力进行细粒度和多方面的分析。在本文中，我们提出了元探测代理（MPA），这是一种受心理测量学启发的通用动态评估协议，用于评估LLMs。 MPA 是 DyVal 2 的关键组件，自然地扩展了先前的 DyVal。 MPA 设计了探测和评判代理，以自动将原始评估问题转化为一个新问题，遵循心理测量理论在三个基本认知能力上的应用: 语言理解、问题解决和领域知识。

    arXiv:2402.14865v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are 
    
[^13]: 利用大型语言模型通过历史链推理增强时间知识图预测

    Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning

    [https://arxiv.org/abs/2402.14382](https://arxiv.org/abs/2402.14382)

    提出了一种通过历史链推理来增强时间知识图预测的方法，有效利用高阶历史信息，弥补了基于大型语言模型的模型在处理历史信息和时间推理能力方面的不足。

    

    时间知识图（TKG）预测旨在基于给定历史数据预测未来事实。 最近的基于图的模型擅长捕捉TKGs中的结构信息，但缺乏语义理解能力。如今，随着LLMs的激增，基于LLMs的TKG预测模型已经出现。然而，现有的基于LLMs的模型存在三个缺点：（1）它只关注第一阶历史以进行预测，而忽略了高阶历史信息，导致提供给LLMs的信息极为有限。（2）在大量历史信息的情况下，LLMs很难达到最佳推理性能。（3）对于TKG预测，单独使用LLM的时间推理能力有限。为应对前两个挑战，我们提出了历史链（CoH）推理，逐步探索高阶历史，实现LLMs对TKGs上高阶历史信息的有效利用。

    arXiv:2402.14382v1 Announce Type: new  Abstract: Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based on given histories. Most recent graph-based models excel at capturing structural information within TKGs but lack semantic comprehension abilities. Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has emerged. However, the existing LLM-based model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for LLMs being extremely limited. (2) LLMs struggle with optimal reasoning performance under heavy historical information loads. (3) For TKG prediction, the temporal reasoning capability of LLM alone is limited. To address the first two challenges, we propose Chain-of-History (CoH) reasoning which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for LLMs on TK
    
[^14]: 示范对多语境学习的影响：多维分析

    The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis

    [https://arxiv.org/abs/2402.12976](https://arxiv.org/abs/2402.12976)

    通过多维分析发现，示范的有效性在多语种上下文学习中存在显著差异，其中部分模型对示范质量不敏感，而精心设计的模板可以消除对示范的依赖。

    

    在上下文学习中，示范经常被用作推理策略，在此策略中，大型语言模型仅使用少量标记的示范来解决任务，而无需进行任何参数更新。与单语言（英语）上下文学习的研究相比，多语种上下文学习尚未得到充分探讨，我们缺乏对该环境中示范作用的深入理解。为了填补这一空白，我们对多语境学习进行了多维分析，实验采用了来自不同模型家族的5个模型，涵盖了包括分类和生成任务在内的9个数据集，覆盖了56种类型上不同的语言。我们的结果表明，示范的有效性在模型、任务和语言之间存在显著差异。我们还发现，Llama 2-Chat、GPT-3.5和GPT-4对示范质量的敏感度较低。相反，精心设计的模板往往会消除一些模型对示范的益处。

    arXiv:2402.12976v1 Announce Type: cross  Abstract: In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some t
    
[^15]: EmoBench: 评估大型语言模型的情感智能

    EmoBench: Evaluating the Emotional Intelligence of Large Language Models

    [https://arxiv.org/abs/2402.12071](https://arxiv.org/abs/2402.12071)

    EmoBench是一个基于心理学理论的基准测试，旨在评估大型语言模型的情感智能，包括情感理解和情感应用。

    

    近年来，大型语言模型（LLMs）的快速发展凸显了需要稳健、全面和具有挑战性的基准测试的重要性。然而，对它们的情感智能（EI）进行评估的研究相当有限。现有的基准测试存在两个主要缺点：首先，它们主要关注情感识别，忽视了情感调节等重要的情感智能能力，而情感理解则促进情感; 其次，它们主要基于现有数据集构建，这些数据集包含频繁模式、明确信息和注释错误，导致评估不可靠。我们提出了EmoBench，这是一个基准测试，借鉴了已建立的心理理论，并为机器EI提出了综合定义，包括情感理解和情感应用。EmoBench包括一组400个用英语和中文手工制作的问题，经过精心设计，需要深入推理。

    arXiv:2402.12071v1 Announce Type: cross  Abstract: Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning
    
[^16]: 方言说话者有什么需求？德语方言语言技术态度调查

    What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects

    [https://arxiv.org/abs/2402.11968](https://arxiv.org/abs/2402.11968)

    德语方言和区域语言说话者更倾向于支持能够处理方言输入的自然语言处理工具，比如虚拟助手，对于产生方言输出的应用则支持程度相对较低。

    

    自然语言处理（NLP）主要集中于对标准化语言进行建模。最近，关注点逐渐转向了本地、非标准化语言和方言。然而，与NLP工具相关的说话者群体的需求和愿望在很大程度上是未知的。本文关注德语方言和区域语言，这是一组在声望和标准化方面异质性的语言变体。我们对这些语言变体的说话者进行了调查（N=327），并介绍了他们对于他们方言的假想语言技术的意见。尽管我们的受访者子群体之间的态度有所不同，但我们发现受访者特别支持能够处理方言输入的潜在NLP工具（特别是音频输入），比如虚拟助手，而对于产生方言输出的应用，比如机器翻译或拼写检查程序，支持程度要低一些。

    arXiv:2402.11968v1 Announce Type: new  Abstract: Natural language processing (NLP) has largely focused on modelling standardized languages. More recently, attention has increasingly shifted to local, non-standardized languages and dialects. However, the relevant speaker populations' needs and wishes with respect to NLP tools are largely unknown. In this paper, we focus on dialects and regional languages related to German -- a group of varieties that is heterogeneous in terms of prestige and standardization. We survey speakers of these varieties (N=327) and present their opinions on hypothetical language technologies for their dialects. Although attitudes vary among subgroups of our respondents, we find that respondents are especially in favour of potential NLP tools that work with dialectal input (especially audio input) such as virtual assistants, and less so for applications that produce dialectal output such as machine translation or spellcheckers.
    
[^17]: ArtPrompt: 基于ASCII艺术的对齐LLMs越狱攻击

    ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs

    [https://arxiv.org/abs/2402.11753](https://arxiv.org/abs/2402.11753)

    提出了一种新颖的基于ASCII艺术的越狱攻击，以及一个用于评估LLMs在识别非纯语义提示方面能力的基准挑战。五个SOTA LLMs在识别ASCII艺术提示时存在困难。

    

    安全对于大型语言模型（LLMs）的使用至关重要。已经开发了多种技术，如数据过滤和监督微调，以加强LLMs的安全性。然而，当前已知的技术假设用于对齐LLMs安全性的语料库仅由语义进行解释。然而，这一假设在现实应用中不成立，导致LLMs存在严重漏洞。本文提出了一种新颖的基于ASCII艺术的越狱攻击，并引入了一个全面的基准Vision-in-Text Challenge（ViTC）来评估LLMs在识别不能仅通过语义进行解释的提示的能力。我们展示了五个SOTA LLMs（GPT-3.5、GPT-4、Gemini、Claude和Llama2）在识别以ASCII艺术形式提供的提示方面存在困难。基于这一观察，我们开发了

    arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
    
[^18]: GNNavi：通过图神经网络导航大型语言模型中的信息流

    GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network

    [https://arxiv.org/abs/2402.11709](https://arxiv.org/abs/2402.11709)

    GNNavi通过引入基于提示的参数高效微调（PEFT）方法和图神经网络（GNN）层，准确引导信息流的汇聚和分布，在大型语言模型中导航信息流动态，超越了标准提示式微调的性能。

    

    大型语言模型（LLMs）在接收示范输入时表现出强大的上下文学习能力（ICL）。然而，微调仍然至关重要以进一步增强其适应性。基于提示的微调方法在数据稀缺情况下证明是有效的微调方法，但对计算资源的高需求限制了其实用性。我们通过引入基于提示的参数高效微调（PEFT）方法来解决这个问题。GNNavi利用了有关ICL信息流动态的见解，表明标签词在提示中作为信息传播的锚点。GNNavi利用图神经网络（GNN）层精确地引导信息流的汇聚和分布，在处理提示时将期望的信息流硬编码到GNN中。我们在使用GPT-2和Llama2进行文本分类任务的实验中发现，GNNavi超越了标准提示式微调的性能。

    arXiv:2402.11709v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-ba
    
[^19]: 机制之争：追踪语言模型处理事实和虚拟语境的方式

    Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals

    [https://arxiv.org/abs/2402.11655](https://arxiv.org/abs/2402.11655)

    本研究提出了机制之争的概念，关注语言模型中多个机制的相互作用，并揭示了它们之间的竞争过程，以及影响某些机制强度的注意力位置。

    

    可解释性研究旨在弥合大型语言模型（LLMs）的经验成功和我们对内部机制的科学理解之间的差距。本研究提出了机制之争的形式，其不再关注单个机制，而是关注多个机制之间的相互作用，并追踪其中一个在最终预测中如何成为主导因素。我们利用logit检验和注意力修改两种可解释性方法揭示了机制之争在LLMs中的发生方式和位置。我们的发现显示了机制及其在各种模型组件中的竞争痕迹，并揭示了有效控制特定机制强度的注意力位置。我们的代码和数据位于https://github.com/francescortu/Competition_of

    arXiv:2402.11655v1 Announce Type: new  Abstract: Interpretability research aims to bridge the gap between the empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research in this area focused on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose the formulation of competition of mechanisms, which instead of individual mechanisms focuses on the interplay of multiple mechanisms, and traces how one of them becomes dominant in the final prediction. We uncover how and where the competition of mechanisms happens within LLMs using two interpretability methods, logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components, and reveal attention positions that effectively control the strength of certain mechanisms. Our code and data are at https://github.com/francescortu/Competition_of
    
[^20]: 通过反事实文本引导的对比语言-图像预训练来理解新闻缩略图的代表性

    Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining

    [https://arxiv.org/abs/2402.11159](https://arxiv.org/abs/2402.11159)

    提出了一种反事实文本引导的对比语言-图像预训练框架CFT-CLIP，用于增强新闻文本和缩略图之间的对比学习。

    

    本文深入探讨了理解新闻缩略图的代表性这一关键挑战，这些缩略图通常在文章在社交媒体上传播时作为读者的第一个视觉参与。我们关注新闻图像是否代表新闻文本中讨论的主要主题。为了应对这一挑战，我们引入了一个手动注释的新闻缩略图和文本配对数据集\textsc{NewsTT}。我们发现，例如CLIP和BLIP-2这样的预训练视觉和语言模型在这一任务上表现不佳。由于新闻主题经常涉及命名实体或专有名词，预训练模型缺乏匹配其视觉和文本外观的能力。为了填补这一空白，我们提出了CFT-CLIP，一个反事实文本引导的对比语言-图像预训练框架。

    arXiv:2402.11159v1 Announce Type: new  Abstract: This paper delves into the critical challenge of understanding the representativeness of news thumbnail images, which often serve as the first visual engagement for readers when an article is disseminated on social media. We focus on whether a news image represents the main subject discussed in the news text. To serve the challenge, we introduce \textsc{NewsTT}, a manually annotated dataset of news thumbnail image and text pairs. We found that pretrained vision and language models, such as CLIP and BLIP-2, struggle with this task. Since news subjects frequently involve named entities or proper nouns, a pretrained model could not have the ability to match its visual and textual appearances. To fill the gap, we propose CFT-CLIP, a counterfactual text-guided contrastive language-image pretraining framework. We hypothesize that learning to contrast news text with its counterfactual, of which named entities are replaced, can enhance the cross
    
[^21]: 通过分解来增强注意力：通过工作流范式改进基于LLM的文本到SQL转换

    Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm

    [https://arxiv.org/abs/2402.10671](https://arxiv.org/abs/2402.10671)

    提出了一种通过工作流范式方法来改善LLMs在文本到SQL中的上下文学习能力，通过分解提高了模型的注意力和问题解决范围，进一步提高了基于LLM的方法的上限。

    

    大语言模型（LLMs）的上下文学习在自然语言处理领域取得了显著成功，而广泛的案例研究表明，单步链式思维提示方法在复杂任务（如文本到SQL）中面临注意力扩散和性能不足等挑战。为了改善LLMs在文本到SQL中的上下文学习能力，提出了一种工作流范式方法，旨在通过分解增强LLMs的注意力和问题解决范围。具体来说，用于消除冗余信息的信息确定模块和基于问题分类的全新提示结构极大增强了模型的注意力。此外，引入自校正和主动学习模块极大扩展了LLMs的问题解决范围，从而提高了基于LLM方法的上限。在三个数据集上进行了大量实验。

    arXiv:2402.10671v1 Announce Type: new  Abstract: In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model's attention. Additionally, the inclusion of self-correcting and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches. Extensive experiments conducted on three da
    
[^22]: 细微之线：通过话语主题识别机器生成的文本

    Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs

    [https://arxiv.org/abs/2402.10586](https://arxiv.org/abs/2402.10586)

    本文探讨了如何通过研究文本中的话语特征来区分人类创作和机器生成的文本，引入了一种新颖的方法来揭示这些特征，并发现人类写作在结构上更为多样化。

    

    随着大型语言模型（LLM）的出现，人类创作和机器生成的文本之间的界限变得日益模糊。本文探讨了识别人类撰写的文本中可辨识和独特的语言特性的研究，特别是揭示文本在表面结构之外的潜在话语结构。引入了一种新颖的方法论，我们利用层次化解析树和递归超图来揭示LLM和人类生成的文本中的独特话语模式。实证研究结果表明，尽管LLM和人类生成的文本都受特定领域的影响而产生不同的话语模式，但人类撰写的文本表现出更多的结构变异性，反映了不同领域人类写作的微妙性质。值得注意的是，引入层次话语特征可以增强二元分类器在区分人类生成和机器生成文本方面的整体性能。

    arXiv:2402.10586v1 Announce Type: new  Abstract: With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers' overall performance in distinguishing between huma
    
[^23]: 通过专家加权来衡量和减少LLM在没有黄金标准答案的情况下的虚构

    Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting

    [https://arxiv.org/abs/2402.10412](https://arxiv.org/abs/2402.10412)

    提出了一种名为FEWL的幻觉度量方法，通过对LLM答案进行加权评估事实性，适用于没有黄金标准答案的情况。

    

    LLM幻觉，即生成事实不正确但看似令人信服的答案，目前是LLM可信度和可靠性的主要威胁。解决这一复杂问题的第一步是对其进行衡量。然而，现有的幻觉度量标准需要具有具有黄金标准答案的基准数据集，即人类编写的“最佳”或“正确”答案。这种要求使幻觉测量成本高昂，并容易出现人为误差。在这项工作中，我们提出了通过加权LLM对事实性进行评估（FEWL），这是第一个专门为金标准答案缺失时设计的幻觉度量标准。FEWL利用了现成的LLM答案作为黄金标准答案的代理。关键挑战是如何有效地量化参考LLM的专业知识。我们展示FEWL具有一定的理论保证，并在实证中证明它更准确。度量虚构。

    arXiv:2402.10412v1 Announce Type: cross  Abstract: LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accur
    
[^24]: TOAD: 具有多样响应风格的面向任务的自动对话系统

    TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles

    [https://arxiv.org/abs/2402.10137](https://arxiv.org/abs/2402.10137)

    TOAD是一个具有多样响应风格的面向任务的自动对话系统，其中考虑了冗长程度和用户表达镜像两个方面。TOAD通过模拟真实的应用上下文交互，提供了丰富的系统响应风格选项，并在评估中表明建模更冗长的回复或不进行用户表达镜像的回复更具挑战性。

    

    最近大语言模型（LLM）的进展显示，下一代虚拟助手的期望包括在各种使用场景下提供更加自然和适应性强的对话能力。然而，为面向任务的对话（TOD）创建高质量的标注数据被认为是缓慢和昂贵的。为了解决这些挑战，我们引入了任务导向的自动对话系统（TOAD），这是一个新颖且可扩展的TOD数据集，以及与之配套的自动生成流程。TOAD数据集模拟了真实的应用上下文交互，并提供了各种系统响应风格选项。考虑了系统响应风格的两个方面，即冗长程度和用户表达镜像。我们在两个响应生成任务上进行了TOAD的评估，结果显示建模更冗长的回复或不进行用户表达镜像的回复更具挑战性。

    arXiv:2402.10137v1 Announce Type: new  Abstract: In light of recent advances in large language models~(LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios. However, the creation of high-quality annotated data for Task-Oriented Dialog~(TOD) is recognized to be slow and costly. To address these challenges, we introduce Task-Oriented Automatic Dialogs~(TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline. The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options. Two aspects of system response styles are considered, verbosity level and users' expression mirroring. We benchmark TOAD on two response generation tasks and the results show that modeling more verbose or responses without user expression mirroring is more challenging.
    
[^25]: SafeDecoding: 通过安全感知解码防御越狱攻击

    SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding

    [https://arxiv.org/abs/2402.08983](https://arxiv.org/abs/2402.08983)

    本文提出了一种名为SafeDecoding的解决方案，通过安全感知解码策略来防御大型语言模型（LLMs）的越狱攻击。该策略可以生成对用户查询有益且无害的响应，有效缓解了LLMs安全性威胁。

    

    随着大型语言模型（LLMs）越来越多地应用于代码生成和聊天机器人辅助等现实应用中，人们为了使LLM的行为与人类价值观保持一致，包括安全性在内做出了大量努力。越狱攻击旨在引发LLM的非预期和不安全行为，仍然是LLM安全性的重要威胁。本文旨在通过引入SafeDecoding来防御LLM的越狱攻击，这是一种安全感知的解码策略，用于生成对用户查询有益且无害的响应。我们在开发SafeDecoding时的洞察力基于观察到，即使代表有害内容的标记的概率超过代表无害响应的标记的概率，安全免责声明仍然出现在按概率降序排序的标记中的前几个。这使我们能够通过识别安全免责声明并增强其良性影响力来减轻越狱攻击。

    arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
    
[^26]: COLD-Attack: 用于具有隐秘性和可控性的LLM越狱

    COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability

    [https://arxiv.org/abs/2402.08679](https://arxiv.org/abs/2402.08679)

    本文提出了COLD-Attack框架，旨在实现具有隐秘性和可控性的LLM越狱。通过建立可控文本生成与攻击生成之间的关联，采用了能量限制解码与Langevin动力学算法，使得在不同的控制要求下搜索对抗性LLM攻击成为可能。

    

    最近对大型语言模型（LLMs）进行越狱的注意力越来越多。为了全面评估LLM的安全性，有必要考虑具有不同属性的越狱，例如上下文连贯性以及情感/风格变化，因此研究可控性越狱是有益的，即如何对LLM攻击进行控制。在本文中，我们正式形式化了可控性攻击生成问题，并建立了该问题与可控文本生成之间的新型关联，这是自然语言处理中一个被广泛探索的主题。基于这种关联，我们改进了能量限制解码与Langevin动力学（COLD）的算法，这是一种在可控文本生成中的高效算法，并引入了COLD-Attack框架，该框架统一且自动化地搜索各种控制要求下的对抗性LLM攻击，例如流畅性、隐秘性、情感和左右连贯性。

    Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controlla
    
[^27]: 与语言模型的反馈循环推动上下文内奖励欺骗

    Feedback Loops With Language Models Drive In-Context Reward Hacking

    [https://arxiv.org/abs/2402.06627](https://arxiv.org/abs/2402.06627)

    与语言模型的反馈循环可能导致上下文内奖励欺骗（ICRH），即语言模型在测试时在优化目标的同时却产生负面副作用。这项研究确定了两个导致ICRH的过程：输出优化和策略优化。

    

    语言模型对外部世界产生影响：它们查询可以读写网页的API，生成能够影响人类行为的内容，以及作为自主代理运行系统命令。这些互动形成了反馈循环：语言模型的输出影响世界，反过来又影响后续的语言模型输出。在这项工作中，我们展示了反馈循环可能导致上下文内奖励欺骗(ICRH)，即测试时的语言模型在优化（可能隐含的）目标的同时，产生负面副作用。例如，考虑一个被部署用于增加Twitter参与度的语言模型代理；语言模型可能在上下文窗口中检索其以前的推文，并使推文更具争议性，从而增加参与度，但也增加了有毒性。我们确定并研究了导致ICRH的两个过程：输出优化和策略优化。对于这些过程，静态数据集上的评估是不足够的-他们无法捕捉到反馈效应，也不能捕捉到最有害的行为。为此，我们提供了...

    Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
    
[^28]: 合并事实，塑造谬误：评估长文生成中聚合事实性主张的矛盾性质

    Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations

    [https://arxiv.org/abs/2402.05629](https://arxiv.org/abs/2402.05629)

    该论文研究了长文生成中的事实性问题，发现大型语言模型在生成段落时可能会由于实体模糊而将可验证的事实组合成非事实的段落。现有的事实准确度评估方法无法正确评估这些非事实段落，作者提出了一种增强的度量指标来应对这个问题。

    

    大型语言模型（LLMs）产生的长文生成物包含了一系列事实和非事实的主张，这使得评估事实性变得困难。为了以更精细的方式评估长文生成物的事实准确性，先前的研究提出将长文生成物分解为多个可验证的事实并独立验证这些事实。生成物的事实性是所有事实中可验证事实的比例。这些方法假设结合了事实主张形成了一个事实性段落。本文展示了这一假设可能因为实体模糊而被违反。我们展示了LLMs可以生成包含可验证事实的段落，但由于实体模糊，这些事实被结合形成了一个非事实的段落。我们进一步揭示了现有的事实准确度度量指标，包括FActScore和引用回顾，无法正确评估这些非事实段落的事实性。为了解决这个问题，我们引入了一种增强度量指标，D-FActScore，作为一个具体的解决方案。

    Long-form generations from large language models (LLMs) contains a mix of factual and non-factual claims, making evaluating factuality difficult. To evaluate factual precision of long-form generations in a more fine-grained way, prior works propose to decompose long-form generations into multiple verifiable facts and verify those facts independently. The factuality of the generation is the proportion of verifiable facts among all the facts. Such methods assume that combining factual claims forms a factual paragraph. This paper shows that the assumption can be violated due to entity ambiguity. We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity. We further reveal that existing factual precision metrics, including FActScore and citation recall, cannot properly evaluate the factuality of these non-factual paragraphs. To address this, we introduce an enhanced metric, D-FActScore, specifi
    
[^29]: SALAD-Bench: 一个针对大语言模型的层次化和全面性安全基准

    SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.05044](https://arxiv.org/abs/2402.05044)

    SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。

    

    在快速发展的大语言模型（LLM）领域中，确保强大的安全措施至关重要。为了满足这一关键需求，我们提出了一种特别设计用于评估LLMs、攻击和防御方法的安全基准，称为SALAD-Bench。SALAD-Bench通过其大规模、丰富多样的特性，以及跨三个层次的细致分类和多功能性，超越了传统基准。SALAD-Bench通过对标准查询和复杂查询（包括攻击、防御修改和多项选择）的精心设计，有效管理其固有的复杂性。为了确保无缝可靠的评估，我们引入了一种创新的评估器：基于LLM的MD-Judge，专注于攻击增强查询的问答对评估。以上组件将SALAD-Bench从标准的LLM安全评估扩展到了LLM攻击和防御方法评估，确保了联合目标的实用性。

    In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
    
[^30]: EffiBench:评估自动生成代码的效率的基准测试

    EffiBench: Benchmarking the Efficiency of Automatically Generated Code

    [https://arxiv.org/abs/2402.02037](https://arxiv.org/abs/2402.02037)

    本文提出了EffiBench基准测试，用于评估代码生成模型生成的代码的效率。实验证明，GPT-4-turbo生成的代码最高效。

    

    代码生成模型在辅助软件开发方面变得越来越重要，可以帮助完成代码补全、调试和代码转换等任务。尽管当前的研究已经深入研究了代码生成模型生成的正确性，但生成代码的效率这一重要方面常常被忽视。本文提出了EffiBench，一个包含1,000个效率关键的编码问题的基准测试，用于评估代码生成模型生成的代码的效率。EffiBench包含了一系列多样化的LeetCode编码问题，每个问题都与一个可执行的人工编写的典型解决方案配对。通过EffiBench，我们在实践中考察了21种大型语言模型（其中13种是开源的，8种是闭源的）在生成高效代码方面的能力。结果表明，GPT-4-turbo生成的代码最高效，明显优于Palm-2-chat-bison、Claude-instant-1、Gemini-pro、GPT-4和GPT-3.5。

    Code generation models have increasingly become integral to aiding software development, offering assistance in tasks such as code completion, debugging, and code translation. Although current research has thoroughly examined the correctness of code produced by code generation models, a vital aspect, i.e., the efficiency of the generated code, has often been neglected. This paper presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems for assessing the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution. With EffiBench, we empirically examine the capability of 21 Large Language Models (13 open-sourced and 8 closed-sourced) in generating efficient code. The results demonstrate that GPT-4-turbo generates the most efficient code, significantly outperforming Palm-2-chat-bison, Claude-instant-1, Gemini-pro, GPT-4, and GPT-3.5. Ne
    
[^31]: Dolma:一个包含三万亿个令牌的开放语料库，用于语言模型预训练研究

    Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research

    [https://arxiv.org/abs/2402.00159](https://arxiv.org/abs/2402.00159)

    Dolma是一个包含三万亿个令牌的开放语料库，用于语言模型预训练研究。它包含了来自多种来源的网络内容、科技论文、代码、公共领域图书、社交媒体和百科全书材料。为了促进开放研究，我们还开源了数据整理工具包。

    

    语言模型已成为处理各种自然语言处理任务的关键技术，然而，关于最佳表现的语言模型是如何开发的很多细节并未报道。特别是，其预训练语料库的信息很少被讨论：商业语言模型很少提供有关其数据的任何信息；即使是开放模型也很少发布它们所训练的数据集，或者提供一个精确的复现方法。因此，进行一些语言建模研究变得具有挑战性，比如理解训练数据如何影响模型的能力和限制。为了促进语言模型预训练的开放研究，我们发布了Dolma，一个包含三万亿个令牌的英语语料库，其中包括各种各样的网络内容、科技论文、代码、公共领域图书、社交媒体和百科全书材料。此外，我们还开源了我们的数据整理工具包，以便进一步的实验和再现。

    Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reprodu
    
[^32]: CLOMO: 带有大型语言模型的反事实逻辑修改

    CLOMO: Counterfactual Logical Modification with Large Language Models

    [https://arxiv.org/abs/2311.17438](https://arxiv.org/abs/2311.17438)

    本研究探索了大型语言模型的反事实推理能力，并引入了反事实逻辑修改任务和相应的评估指标。实验结果表明，大型语言模型展现出显著的逻辑能力。

    

    本研究探索了大型语言模型（LLMs）的反事实推理能力。我们的主要目标是培养LLMs内的反事实思维过程，并对其有效性进行严格评估。具体而言，我们引入了一项新任务，即反事实逻辑修改（CLOMO），以及一个高质量的人工注释基准。在这个任务中，LLMs必须熟练地改变给定的论证文本，以保持预定的逻辑关系。为了有效评估生成模型的反事实能力，我们提出了一种创新的评估指标，逻辑感知的反事实分数，直接评估LLMs的自然语言输出，而不是将任务建模为多项选择问题。分析表明，所提出的自动评估指标与人类偏好很好地一致。我们的实验结果表明，尽管LLMs展示了显着的逻辑能力。

    arXiv:2311.17438v3 Announce Type: replace-cross Abstract: In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the LogicAware Counterfactual Score to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for log
    
[^33]: LLMs作为自恋评估者：当自我膨胀影响评估分数

    LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores

    [https://arxiv.org/abs/2311.09766](https://arxiv.org/abs/2311.09766)

    本文研究了语言模型驱动的评估指标在总结任务中是否会对相同的基础语言模型生成的文本表现出偏见，并发现在无参考情况下使用时偏见尤为显著。

    

    arXiv:2311.09766v2 公告类型：替换 摘要：生成文本内容的自动评估在自然语言处理领域中一直是一个持续挑战。鉴于现代语言模型（LMs）在各种NLP任务中的出色表现，越来越多的人倾向于利用这些模型创造创新的评估指标，用于自动生成任务的自动评估。本文探讨了一个重要问题：由于语言模型驱动的评估指标是否会固有地表现出偏向于由相同基础语言模型生成的文本的偏见？具体而言，我们评估了知名的基于LM的评估指标（例如BARTScore、T5Score和GPTScore）在总结任务中是否对其各自的基础LM表现出偏好。我们的发现揭示了潜在偏见，特别是当这些评估指标在无参考的情况下使用且不利用黄金摘要时，这种偏见尤为显著。这些结果突显了通过生成文本获得的评估结果可能会受到自我偏误的影响。

    arXiv:2311.09766v2 Announce Type: replace  Abstract: Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in an reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generat
    
[^34]: Fusion-Eval: 将评估器与LLMs集成

    Fusion-Eval: Integrating Evaluators with LLMs

    [https://arxiv.org/abs/2311.09204](https://arxiv.org/abs/2311.09204)

    Fusion-Eval是一种创新方法，利用LLMs整合不同辅助评估器的见解，极大提升自然语言系统评估的有效性。

    

    自然语言系统的评估在自然语言理解和高级推理领域面临着重大挑战。本文介绍了一种名为“Fusion-Eval”的创新方法，利用大型语言模型（LLMs）来整合来自各种辅助评估器的见解。每个评估器专门负责评估响应的不同方面。这种独特策略使得Fusion-Eval能够有效地跨越各种任务和标准，增强现有评估方法的效果。在SummEval上，Fusion-Eval与人类之间的系统级Kendall-Tau相关性达到0.962，在TopicalChat上的轮级Spearman相关性达到0.744，远高于基准方法。这些结果突显了Fusion-Eval在自然语言系统评估领域的巨大潜力。

    arXiv:2311.09204v2 Announce Type: replace-cross  Abstract: Evaluating natural language systems poses significant challenges, particularly in the realms of natural language understanding and high-level reasoning. In this paper, we introduce "Fusion-Eval", an innovative approach that leverages Large Language Models (LLMs) to integrate insights from various assistant evaluators. Each of these evaluators specializes in assessing distinct aspects of responses. This unique strategy enables Fusion-Eval to function effectively across a diverse range of tasks and criteria, enhancing the effectiveness of existing evaluation methods. Fusion-Eval achieves a 0.962 system-level Kendall-Tau correlation with humans on SummEval and a 0.744 turn-level Spearman correlation on TopicalChat, which is significantly higher than baseline methods. These results highlight Fusion-Eval's significant potential in the realm of natural language system evaluation.
    
[^35]: CodeScope:一个基于执行的多语言多任务多维基准用于评估LLMs在代码理解和生成方面的能力

    CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation

    [https://arxiv.org/abs/2311.08588](https://arxiv.org/abs/2311.08588)

    CodeScope是一个用于评估LLMs在代码理解和生成方面能力的多语言多任务多维基准，解决了现有基准在多语言编程环境和多任务设置方面的不足。

    

    大型语言模型（LLMs）在编码相关任务上表现出色，特别是在帮助人类编程和促进编程自动化方面。然而，现有的用于评估LLMs的代码理解和生成能力的基准存在严重的限制。首先，大部分基准存在缺陷，因为它们只关注于狭窄范围内的流行编程语言和特定任务，而实际软件开发场景需要实现多语言编程环境以满足各种需求。实际编程实践还强烈期望多任务设置，以全面和稳健地测试LLMs的编码能力。其次，大部分基准也未考虑生成代码的可执行性和执行结果的一致性。为了弥补现有基准与实际应用期望之间的差距，我们引入了CodeScope。

    Large Language Models (LLMs) have demonstrated remarkable performance on coding related tasks, particularly on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks, whereas the real-world software development scenarios show dire need to implement systems with multilingual programming environments to satisfy diverse requirements. Practical programming practices also strongly expect multi-task settings for testing coding capabilities of LLMs comprehensively and robustly. Second, most benchmarks also fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope
    
[^36]: Chat Vector：一种简单的方法，在新语言中为LLMs提供指令遵循和模型对齐

    Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages

    [https://arxiv.org/abs/2310.04799](https://arxiv.org/abs/2310.04799)

    提出了聊天向量的概念，通过简单的模型算术使预训练语言模型具备在新语言中遵循指令和实现模型对齐的能力

    

    最近，开源大型语言模型（LLMs）的发展迅速。尽管如此，由于数据约束，大多数开源LLM的能力主要集中在英语上。为了解决这个问题，我们引入了聊天向量的概念，通过简单的模型算术为预训练的语言模型提供指令遵循和人类价值对齐。聊天向量是通过将预训练基础模型（例如LLaMA2）的权重减去其对应的聊天模型（例如LLaMA2-chat）的权重得出的。通过简单地将聊天向量添加到持续预训练模型的权重中，我们可以使模型在新语言中具有聊天能力，而无需进一步训练。我们的实证研究展示了聊天向量在三个不同方面的优越有效性：指令遵循、毒性缓解和多轮对话。此外，为了展示我们方法的适应性

    arXiv:2310.04799v2 Announce Type: replace  Abstract: Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of chat vector to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model's weights, we can endow the model with chat capabilities in new languages without the need for further training. Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our 
    
[^37]: WebVoyager：使用大型多模态模型构建端到端的Web Agent

    WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])

    [http://arxiv.org/abs/2401.13919](http://arxiv.org/abs/2401.13919)

    WebVoyager是一种创新的基于大型多模态模型的Web代理，能够通过与真实网站交互来端到端地完成用户指令。它提出了一个新的Web代理评估协议，并在实际任务中取得了显著的成功率。

    

    大型语言模型（LLMs）的进步引领了一个由真实世界中自主应用程序的发展所标志的新时代，推动了基于网络的高级代理的创新。现有的网络代理通常只处理一个输入模态，并且仅在简化的网络模拟器或静态的网络快照中进行评估，极大地限制了它们在真实场景中的适用性。为了填补这一差距，我们引入了WebVoyager，一种创新的基于大型多模态模型（LMM）的Web代理，通过与真实网站进行交互，能够端到端地完成用户指令。此外，我们提出了一个新的Web代理评估协议，以解决开放式Web代理任务的自动评估挑战，利用了GPT-4V的强大多模态理解能力。我们通过收集来自15个广泛使用的网站的真实世界任务来创建一个新的基准来评估我们的代理。我们展示了WebVoyager实现了55.7％的任务成功率，显著地.....

    The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
    
[^38]: Patchscope: 一个统一的框架，用于检查语言模型的隐藏表示

    Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])

    [http://arxiv.org/abs/2401.06102](http://arxiv.org/abs/2401.06102)

    本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。

    

    检查大型语言模型（LLM）的隐藏表示中编码的信息可以解释模型的行为并验证其与人类价值观的一致性。鉴于LLM生成人类可理解文本的能力，我们建议利用模型本身以自然语言解释其内部表示。我们引入了一个称为Patchscopes的框架，并展示了如何使用它来回答关于LLM计算的各种研究问题。我们表明，先前基于将表示投影到词汇空间和干预LLM计算的可解释性方法，可以看作是该框架的特殊实例。此外，通过Patchscope可以弥补优势，如检查早期层失败或表达能力不足。除了统一先前的检查技术，Patchscopes还开辟了新的可能性，例如使用更强大的模型来解释较小模型的表示。

    Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
    
[^39]: 自我对比：通过不一致的求解视角获得更好的反思能力

    Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives. (arXiv:2401.02009v1 [cs.CL])

    [http://arxiv.org/abs/2401.02009](http://arxiv.org/abs/2401.02009)

    自我对比是一种通过对比不同求解视角和总结差异，提高大型语言模型（LLM）的反思能力的方法。

    

    大型语言模型（LLM）的反思能力引起了广泛关注。一种事后提示策略，例如反思和自我改进，根据自我评估或外部反馈来改善LLM的响应。然而，最近的研究表明，在没有外部反馈的情况下，LLM的内在反思是不稳定的。我们的调查揭示了自我评估反馈质量是关键瓶颈。我们发现LLM在自我评估时常常表现出过度自信或高度随机性，提供固执或不一致的反馈，导致反思能力不佳。为了解决这个问题，我们提出了自我对比的方法：它根据请求自适应地探索多样的求解视角，对比差异，并将这些差异总结为一个检查表，用于重新审视和消除差异。我们的方法赋予LLM多样的视角以减轻固执偏见。此外，差异指示了潜在的错误或固有的不确定性。

    The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties tha
    
[^40]: WaveCoder: 广泛和多功能的改进指令调优与完善的数据生成

    WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.14187](http://arxiv.org/abs/2312.14187)

    本文提出了WaveCoder，一个广泛和多功能的改进指令调优模型，通过将指令数据分类并利用LLM框架生成多样的高质量指令数据，以提高调优模型的效果和泛化能力。

    

    近期的研究表明，在对高质量指令数据集进行调优后，生成的模型可以在广泛的任务上展现出令人印象深刻的能力。然而，现有的指令数据生成方法经常会产生重复数据，并且对数据质量的控制不够灵活。本文通过将指令数据分类为4个与代码相关的任务，扩展了指令调优的普适性，并提出了基于LLM的生成器-判别器数据处理框架，从开源代码中生成多样的、高质量的指令数据。因此，我们介绍了CodeOcean，一个包含4个通用代码相关任务的、共计20,000个指令实例的数据集，旨在增强指令调优的效果，并提高调优模型的泛化能力。随后，我们提出了WaveCoder，一个具有广泛和多功能的改进指令调优的Code LLM模型。

    Recent work demonstrates that, after being fine-tuned on a high-quality instruction dataset, the resulting model can obtain impressive capabilities to address a wide range of tasks. However, existing methods for instruction data generation often produce duplicate data and are not controllable enough on data quality. In this paper, we extend the generalization of instruction tuning by classifying the instruction data to 4 code-related tasks and propose a LLM-based Generator-Discriminator data process framework to generate diverse, high-quality instruction data from open source code. Hence, we introduce CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal code-related tasks,which is aimed at augmenting the effectiveness of instruction tuning and improving the generalization ability of fine-tuned model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with Widespread And Versatile Enhanced instruction tuning. This model is specifically designed for enha
    
[^41]: BioInstruct:用于生物医学自然语言处理的大型语言模型指令调整

    BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])

    [http://arxiv.org/abs/2310.19975](http://arxiv.org/abs/2310.19975)

    BioInstruct是一个用于生物医学自然语言处理的大型语言模型指令调整方法，通过引入针对性指令数据集BioInstruct，通过GPT-4语言模型进行精调，优化了模型在生物医学自然语言处理中的性能。

    

    大型语言模型通过在大量数据上进行预训练，然后进行特定领域的指令调整，在许多自然语言处理任务中取得了巨大成功。然而，在生物医学领域只发表了很少的指令。为了解决这个问题，我们引入了BioInstruct，这是一个定制的任务特定指令数据集，包含超过25,000个示例。通过使用三个人工筛选的指令样本，以GPT-4语言模型作为提示，精调大型语言模型，我们旨在优化其在生物医学自然语言处理中的性能。我们对LLaMA LLMs (1&2,7B&13B)进行了指令调整，并在生物医学自然语言处理应用中进行了评估，包括信息提取、问答和文本生成。我们还评估了指令如何对模型性能的贡献，使用了多任务学习原则。

    Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&2, 7B\&13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principl
    
[^42]: 角色扮演聊天机器人能够捕捉角色个性吗？评估角色扮演聊天机器人的个性特点。

    Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots. (arXiv:2310.17976v1 [cs.CL])

    [http://arxiv.org/abs/2310.17976](http://arxiv.org/abs/2310.17976)

    本文介绍了一种创新的方法，用于评估角色扮演聊天机器人的个性特点，并发现基于LLMs的现代角色扮演聊天机器人能够有效地描绘出相应角色的个性特点，与人类感知的一致性达到82.8%。

    

    大规模预训练语言模型的出现彻底改变了新型人工智能应用的能力，尤其是在打造具有独特人物的聊天机器人方面。鉴于聊天机器人的"刺激-响应"性质，本文揭示了一种创新的开放式采访式方法，用于评估角色扮演聊天机器人的个性特点，从而更深入地理解其内在个性。我们对ChatHaruhi图书馆创建的32个角色扮演聊天机器人进行了大五人格和MBTI维度上的个性评估，并测量它们与人类感知的一致性。评估结果强调，基于LLMs的现代角色扮演聊天机器人能够有效地描绘出相应角色的个性特点，与人类感知的一致性达到82.8%。此外，我们还提出了塑造聊天机器人个性的潜在策略。因此，本文为角色扮演聊天机器人研究奠定了基础。

    The emergence of large-scale pretrained language models has revolutionized the capabilities of new AI application, especially in the realm of crafting chatbots with distinct personas. Given the "stimulus-response" nature of chatbots, this paper unveils an innovative open-ended interview-style approach for personality assessment on role-playing chatbots, which offers a richer comprehension of their intrinsic personalities. We conduct personality assessments on 32 role-playing chatbots created by the ChatHaruhi library, across both the Big Five and MBTI dimensions, and measure their alignment with human perception. Evaluation results underscore that modern role-playing chatbots based on LLMs can effectively portray personality traits of corresponding characters, with an alignment rate of 82.8% compared with human-perceived personalities. Besides, we also suggest potential strategies for shaping chatbots' personalities. Hence, this paper serves as a cornerstone study for role-playing chat
    
[^43]: CompeteAI:理解基于大型语言模型的智能体竞争行为

    CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents. (arXiv:2310.17512v1 [cs.AI])

    [http://arxiv.org/abs/2310.17512](http://arxiv.org/abs/2310.17512)

    本文研究了基于大型语言模型的智能体之间的竞争行为。通过建立一个竞争环境并进行实验，发现竞争可以促使智能体进行转变和采取新策略，从而在社会和经济发展中发挥重要作用。

    

    大型语言模型（LLM）被广泛应用于完成不同任务，如个人助理或事件规划。虽然大多数工作都集中在智能体之间的合作与协作，但很少有研究探索另一个重要机制——竞争，它是社会和经济发展的推动力之一。本文旨在研究LLM智能体之间的竞争行为。我们首先提出一个通用框架来研究智能体之间的竞争。然后，我们使用GPT-4实现了一个实际的竞争环境，模拟了一个由餐馆智能体和顾客智能体组成的虚拟城镇。具体而言，餐馆智能体相互竞争以吸引更多顾客，这种竞争促使它们进行转变，比如培养新的运营策略。我们实验的结果揭示了从社会学习到马太效应等多个有趣发现，与现有的社会学和经济学理论相一致。

    Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that fosters the development of society and economy. In this paper, we seek to examine the competition behaviors in LLM-based agents. We first propose a general framework to study the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, restaurant agents compete with each other to attract more customers, where the competition fosters them to transform, such as cultivating new operating strategies. The results of our experiments reveal several interesting findings ranging from social learning to Matthew Effect, which aligns well with existing sociological and e
    
[^44]: 通过估计数据分布比例的离散扩散语言建模

    Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution. (arXiv:2310.16834v1 [stat.ML])

    [http://arxiv.org/abs/2310.16834](http://arxiv.org/abs/2310.16834)

    本研究通过引入得分熵这一新颖的离散得分匹配损失，弥补了离散数据领域中现有方法的不足，提出了得分熵离散扩散模型(SEDD)并在GPT-2实验中取得了有竞争力的效果。

    

    尽管扩散模型在许多生成建模任务中具有突破性的性能，但在自然语言等离散数据领域中却表现不佳。关键是，标准的扩散模型依赖于成熟的得分匹配理论，但是将其推广到离散结构并没有取得相同的经验收益。在本文中，我们通过提出得分熵，一种新颖的离散得分匹配损失，来弥补这个差距，它比现有方法更稳定，可以形成最大似然训练的ELBO，并且可以通过去噪变体高效优化。我们将我们的得分熵离散扩散模型（SEDD）扩展到GPT-2的实验设置中，实现了极具竞争力的似然度，同时引入了独特的算法优势。特别是，在比较大小相似的SEDD和GPT-2模型时，SEDD达到了可比较的困惑度（通常在基线的+$10\%$内，并且有时超过基线）。此外，SEDD模型学到了...

    Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel discrete score matching loss that is more stable than existing methods, forms an ELBO for maximum likelihood training, and can be efficiently optimized with a denoising variant. We scale our Score Entropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2, achieving highly competitive likelihoods while also introducing distinct algorithmic advantages. In particular, when comparing similarly sized SEDD and GPT-2 models, SEDD attains comparable perplexities (normally within $+10\%$ of and sometimes outperforming the baseline). Furthermore, SEDD models lear
    
[^45]: 在线推测解码

    Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])

    [http://arxiv.org/abs/2310.07177](http://arxiv.org/abs/2310.07177)

    在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。

    

    推测解码是通过利用较小的草稿模型来预测目标模型的输出，从而加速大型语言模型（LLM）推理的关键技术。然而，在面对多样的文本输入和草稿模型与目标模型之间的显著能力差距时，其有效性可能受到限制。我们引入了在线推测解码（OSD）来解决这一挑战。其主要思想是利用LLM服务集群中丰富的多余计算能力，根据观察到的用户查询数据持续更新（多个）草稿模型。由于LLM推理受内存限制，典型的LLM服务集群中的剩余计算能力可以用于在线重新训练草稿模型，从而使训练成本保持中性。由于LLM服务的查询分布相对简单，根据查询分布进行重新训练可以使草稿模型更准确地预测目标模型的输出。

    Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
    
[^46]: 大规模语言模型对监督微调数据组合的影响

    How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05492](http://arxiv.org/abs/2310.05492)

    本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。

    

    大规模语言模型（LLMs）具备大量的预训练标记和参数，展现出数学推理、代码生成和指令跟随等能力。这些能力通过监督微调（SFT）进一步增强。开源社区已经研究了针对每种能力的临时SFT，而专有LLMs可以适用于所有能力。因此，研究如何通过SFT解锁多重能力变得重要。在本研究中，我们特别关注SFT过程中数学推理、代码生成和人类对齐能力之间的数据组合。从规模的角度，我们研究了模型能力与各种因素之间的关系，包括数据量、数据组合比例、模型参数和SFT策略。我们的实验发现不同的能力表现出不同的扩展模式，较大的模型通常在相同的数据量下表现出更优异的性能。数学推理和代码生成能力通过微调数据和模型参数的增加而获得显著的性能提升。

    Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
    
[^47]: 你仅关注屏幕：多模态动作链机器人

    You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])

    [http://arxiv.org/abs/2309.11436](http://arxiv.org/abs/2309.11436)

    本论文提出了一种名为Auto-UI的多模态动作链机器人，通过直接与界面交互，避免了环境解析或依赖于应用程序API的需要，并引入了动作链技术来帮助模型进行决策。

    

    自主用户界面（UI）机器人旨在通过与用户界面进行交互，实现任务自动化，无需手动干预。最近的研究探讨了利用大型语言模型（LLM）的能力，以在多样环境中有效参与。为了符合LLM的输入-输出要求，现有方法在沙盒环境中开发，依赖于外部工具和应用程序特定的API将环境解析为文本元素，并解释预测的动作。因此，这些方法常常受到推理效率低和错误传播风险的困扰。为了缓解这些挑战，我们引入了Auto-UI，一种多模态解决方案，它直接与界面交互，避免了对环境解析或依赖于应用程序相关的API的需求。此外，我们提出了一种动作链技术，利用一系列中间先前动作历史和未来动作计划，以帮助模型进行决策。

    Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -leveraging a series of intermediate previous action histories and future action plans -- to help the age
    
[^48]: 基于元分布建模的开放领域文本评估

    Open-Domain Text Evaluation via Meta Distribution Modeling. (arXiv:2306.11879v1 [cs.CL])

    [http://arxiv.org/abs/2306.11879](http://arxiv.org/abs/2306.11879)

    本文提出了一种新颖的开放领域文本生成模型评估方法——元分布方法（MDM），该方法将两个概率分布的对比映射到质量度量上，可以视为分布的分布，其可用于开放领域文本生成的评估。

    

    最近，基于大型预训练语言模型（LLMs）的开放领域文本生成模型取得了显著的性能提升。然而，为了控制和评估这些模型是否达到所需属性仍然是一个挑战，因为传统的基于参考文本的度量标准如BLEU、ROUGE和METEOR对于开放式生成任务来说是不足够的。同样地，虽然具备训练鉴别器的度量标准表现出了希望的前景，但是获取高质量的训练数据则是一项非常困难的任务。本文提出了一种新颖的方法来评估开放领域文本生成——元分布方法（MDM）。通过考虑LLMs参数数量上升和性能提升之间的相关性，MDM 创造了一个映射，将两个概率分布的对比（一个已知优于另一个）映射到质量度量上，该度量可以视为分布的分布，即元分布。我们研究了MDM在评估开放领域文本生成中的应用。

    Recent advances in open-domain text generation models powered by large pre-trained language models (LLMs) have achieved remarkable performance. However, evaluating and controlling these models for desired attributes remains a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and METEOR are insufficient for open-ended generation tasks. Similarly, while trainable discriminator-based evaluation metrics show promise, obtaining high-quality training data is a non-trivial task. In this paper, we introduce a novel approach to evaluate open-domain generation - the Meta-Distribution Methods (MDM). Drawing on the correlation between the rising parameter counts and the improving performance of LLMs, MDM creates a mapping from the contrast of two probabilistic distributions -- one known to be superior to the other -to quality measures, which can be viewed as a distribution of distributions i.e. Meta-Distribution. We investigate MDM for open-domain text generation evaluation 
    
[^49]: 大型语言模型的思维链主动提示

    Active Prompting with Chain-of-Thought for Large Language Models. (arXiv:2302.12246v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12246](http://arxiv.org/abs/2302.12246)

    本论文提出了一种新的方法Active-Prompt，它使用任务特定的示例提示适应大型语言模型中的不同任务，提高模型性能与效率。

    

    大型语言模型的规模日益增大，为各种需要推理的复杂任务（如算术和常识推理）带来了新的能力。众所周知，任务特定提示的有效设计对LLMs产生高质量答案的能力至关重要。特别是，对于复杂的问答任务，一种有效的方法是基于示例的思维链（CoT）推导提示，它大大提高了LLMs的性能。但是，当前的CoT方法依赖于一组固定的人类注释示例，这些示例不一定是不同任务的最有效示例。本文提出了一种新方法，Active-Prompt，使用任务特定的示例提示（人为设计的CoT推理注释）来适应LLMs不同的任务。为此，我们提出了一个解决方案，确定哪些问题从任务特定查询池中注释最重要和有用。通过借鉴主动学习的方法，我们提出了一个主动提示(Acitve-Prompt)的方法，将最相关的问题作为任务特定提示添加给LLMs，从而改善其性能。

    The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowi
    

