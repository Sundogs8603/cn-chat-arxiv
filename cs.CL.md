# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Geometric Notion of Causal Probing.](http://arxiv.org/abs/2307.15054) | 本文提出了一种几何观念的因果探测方法，通过在语言模型表示空间的子空间上进行反事实干预，优化了因果概念子空间，以实现概念控制生成。 |
| [^2] | [Matching Patients to Clinical Trials with Large Language Models.](http://arxiv.org/abs/2307.15051) | 本研究调查了使用大型语言模型（LLMs）来帮助患者和转诊医生识别合适的临床试验的潜力，并引入了TrialGPT架构，该架构能够准确预测合格性并提供解释，实验证明其有效性。 |
| [^3] | [Universal and Transferable Adversarial Attacks on Aligned Language Models.](http://arxiv.org/abs/2307.15043) | 这项研究提出了一种简单而有效的攻击方法，能够使对齐的语言模型生成不良行为，而不依赖于人工设计，通过自动化方法产生对抗性后缀，并在实践中取得改进。 |
| [^4] | [SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark.](http://arxiv.org/abs/2307.15020) | 提出了一个全面的中文大型语言模型基准测试SuperCLUE，包括实际用户的查询和评级、开放性问题以及封闭性问题，该基准测试填补了目前对模型在实际应用中能力理解的空白。 |
| [^5] | [Gzip versus bag-of-words for text classification with KNN.](http://arxiv.org/abs/2307.15002) | Gzip与KNN相比较在文本分类中，我们发现通过简单的词袋匹配可以获得类似或更好的准确性，并且更加高效。 |
| [^6] | [Scaling TransNormer to 175 Billion Parameters.](http://arxiv.org/abs/2307.14995) | 本论文提出了TransNormerLLM，这是第一个基于线性注意力的大型语言模型，在准确性和效率方面优于传统基于softmax注意力的模型。通过引入位置嵌入、线性注意力加速、门控机制等先进改进，并利用Lightning Attention技术加速线性注意力，以及采用张量归一化方案加速模型，提高了TransNormer的性能。 |
| [^7] | [Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs.](http://arxiv.org/abs/2307.14988) | 本论文介绍了一种增量计算的神经网络方法，通过离散化中间值并过滤不必要的修改，实现了对动态输入的高效推理。 |
| [^8] | [PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback.](http://arxiv.org/abs/2307.14936) | 本文提出了一种新的框架RRTF，以增强预训练的大型语言模型在代码生成方面的能力。PanGu-Coder2是该框架的实现，在多个基准测试中均表现出色，优于其他先前的Code LLMs。 |
| [^9] | [ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection.](http://arxiv.org/abs/2307.14913) | 这篇论文介绍了一个基于转换的自然语言推理方法，用于多作者写作风格检测。作者将任务视为自然语言推理问题，重点关注段落之间的过渡。实验结果表明，该方法优于其他模型版本，并在不同设置下达到了良好的性能。 |
| [^10] | [ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection.](http://arxiv.org/abs/2307.14912) | 本文介绍了ARC-NLP在PAN 2023的应用，通过构建层次化模型来进行触发词检测，实现了对粉丝小说文档中多个触发词的检测。该模型利用Transformer模型进行微调，并在多标签设置中使用LSTM模型进行训练。 |
| [^11] | [Retrieval-based Text Selection for Addressing Class-Imbalanced Data in Classification.](http://arxiv.org/abs/2307.14899) | 本文解决了在文本分类中选择有限数量的文本进行注释的问题，并提出了利用检索方法和语义搜索来处理类别不平衡的二元分类问题。通过利用SHAP构建高质量的查询集，帮助选择适合注释的文本，以解决长期注释任务中的挑战。 |
| [^12] | [MESED: A Multi-modal Entity Set Expansion Dataset with Fine-grained Semantic Classes and Hard Negative Entities.](http://arxiv.org/abs/2307.14878) | 这个论文提出了一个多模态实体集扩展数据集MESED，用于解决传统方法难以处理的负面实体、同义实体、多义实体和长尾实体等问题。模型通过集成多模态信息来表示实体，能够提供互补信息、统一信号和强大的对齐信号。 |
| [^13] | [Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners.](http://arxiv.org/abs/2307.14856) | Seq2Seq模型作为少样本学习器的潜力在解码器和编码-解码模型中进行了广泛研究，提出了两种能有效提升Seq2Seq模型上下文学习能力的方法，并在各种任务中显示出显著的性能改进。 |
| [^14] | [ArcGPT: A Large Language Model Tailored for Real-world Archival Applications.](http://arxiv.org/abs/2307.14852) | ArcGPT是第一个专为档案领域定制的通用语言模型，通过在大规模档案领域数据上进行预训练，ArcGPT在真实档案任务中表现出色，标志着有效档案数据管理迈出了重要一步。 |
| [^15] | [Turkish Native Language Identification.](http://arxiv.org/abs/2307.14850) | 这项研究首次将母语识别应用于土耳其语,通过分析作者不同语言的写作来预测作者的母语。研究使用了土耳其学习者语料库和三个句法特征来展示其有效性。 |
| [^16] | [What Makes a Good Paraphrase: Do Automated Evaluations Work?.](http://arxiv.org/abs/2307.14818) | 本文研究了一个好的改写应该是什么样的，是否可以仅使用自动评估来评估改写的质量。研究通过实验和专家评估得出结论。 |
| [^17] | [Models of reference production: How do they withstand the test of time?.](http://arxiv.org/abs/2307.14817) | 本研究通过分析目前的自然语言处理研究中的参考生成模型，发现GREC不再是一个可靠的评估工具，并证明了预训练语言模型在此任务中具有较强的鲁棒性。 |
| [^18] | [Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model.](http://arxiv.org/abs/2307.14785) | 本文提出了一种利用语义角色标注模型提升基于方面的情感分析性能的方法，通过构建端到端语义角色标注模型有效捕获结构化语义信息，并在英语和捷克语上实现了最佳结果。 |
| [^19] | [Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset.](http://arxiv.org/abs/2307.14783) | Emotion4MIDI是一个包含12k个带情感标签的符号音乐数据集，通过在GoEmotions数据集上训练情感分类模型，并应用于两个大规模的MIDI数据集的歌词，提供了一个宝贵的资源来探索音乐和情感之间的联系，并开发可以根据特定情感生成音乐的模型。 |
| [^20] | [Turning Whisper into Real-Time Transcription System.](http://arxiv.org/abs/2307.14743) | 本文介绍了一个基于Whisper的实时语音转录和翻译系统Whisper-Streaming，通过使用本地协议策略和自适应延迟，实现了流式转录。实验结果表明Whisper-Streaming在未分割的长篇演讲转录测试集上具有高质量和低延迟，并且在实际多语种会议中的应用也具有鲁棒性和实用性。 |
| [^21] | [Evaluating Generative Models for Graph-to-Text Generation.](http://arxiv.org/abs/2307.14712) | 本文评估了生成模型在图文生成中的应用，并发现生成模型能够生成流畅并连贯的文本。然而，生成模型仍然困难理解实体之间的语义关系，并且容易生成带有幻觉或无关信息的文本。 |
| [^22] | [Improving Natural Language Inference in Arabic using Transformer Models and Linguistically Informed Pre-Training.](http://arxiv.org/abs/2307.14666) | 本文针对阿拉伯语自然语言处理中的自然语言推理问题进行研究，通过构建专门的数据集和应用语言学知识预训练方法，我们的语言特定模型在与最先进的多语言方法竞争时表现出色。 |
| [^23] | [Metric-Based In-context Learning: A Case Study in Text Simplification.](http://arxiv.org/abs/2307.14632) | 本文针对文本简化进行了一个案例研究，提出了一种基于度量的上下文学习（MBL）方法，通过选择具有顶级SARI得分的示例，可以在较大型的GPT模型上获得最佳表现。而在较小型的模型上，通过选择具有较高压缩比例的示例表现更好。 |
| [^24] | [Speed Reading Tool Powered by Artificial Intelligence for Students with ADHD, Dyslexia, or Short Attention Span.](http://arxiv.org/abs/2307.14544) | 这项研究提出了一种基于人工智能的速读工具，用于帮助患有ADHD、诵读困难或注意力不集中的学生更高效地消化文本信息。该工具利用多层感知机算法和T5模型，通过文本处理和摘要任务实现。同时，该工具还应用了仿生阅读原则来增强可读性。 |
| [^25] | [Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models.](http://arxiv.org/abs/2307.14539) | 本文通过引入对抗性嵌入空间攻击，探讨了将现成组件插入多模型模型中的漏洞和风险，并提出了一种不需要多模型系统权重和参数的攻击方法，通过寻找位于预训练组件嵌入空间危险区域的输入图像进行攻击。 |
| [^26] | [CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions.](http://arxiv.org/abs/2307.14522) | CliniDigest是一个基于大规模语言模型的临床试验摘要工具，能够实时、真实和全面地将长篇试验描述压缩成简洁的摘要。 |
| [^27] | [Words That Stick: Predicting Decision Making and Synonym Engagement Using Cognitive Biases and Computational Linguistics.](http://arxiv.org/abs/2307.14511) | 这项研究使用认知偏差和计算语言学预测用户参与度和决策过程，发现准确代表核心思想、易于理解、能够引发情感反应并且常见的同义词能够促进更高的用户参与度。 |
| [^28] | [A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing.](http://arxiv.org/abs/2307.14500) | 本研究提出了一个预测数字信息参与的新模型READ，通过融合认知偏差、计算语言学和自然语言处理，成功预测了英文字的参与水平，并区分了更具吸引力的词汇。 |
| [^29] | [Controllable Generation of Dialogue Acts for Dialogue Systems via Few-Shot Response Generation and Ranking.](http://arxiv.org/abs/2307.14440) | 这项研究开发了一种少样本过生成和排序方法，实现了对话系统中对话行为的可控生成。通过使用预训练语言模型和自动排名函数，可以产生具有高语义准确性的多种对话行为的响应。 |
| [^30] | [Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models.](http://arxiv.org/abs/2307.14430) | 本论文提出了一个数据驱动的技能框架，用于理解和训练语言模型。通过研究人类获得技能的有序性，我们证明了语言模型学习技能时也有一定的顺序，并且这种顺序可以改善对语言模型的理解和数据高效训练。 |
| [^31] | [Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG.](http://arxiv.org/abs/2307.14389) | 本研究提出了一种使用去噪的扩散概率模型和条件自编码器进行想象言语的脑电图解码的新颖方法。结果表明，与传统机器学习技术和基准模型相比，该方法显著提高了解码准确性，这对于发展通过想象言语进行交流的脑-机接口具有潜在的影响。 |
| [^32] | [Leveraging Large Language Models for Mental Health Prediction via Online Text Data.](http://arxiv.org/abs/2307.14385) | 本研究首次对多种大型语言模型在心理健康预测任务上进行了全面评估，结果表明指令微调可以显著提升模型性能，并且最优微调模型在平衡准确度上胜过GPT-3.5，并与最先进的任务特定模型持平。 |
| [^33] | [How Can Large Language Models Help Humans in Design and Manufacturing?.](http://arxiv.org/abs/2307.14377) | 大型语言模型在设计和制造中的应用潜力以及其限制 |
| [^34] | [Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers.](http://arxiv.org/abs/2307.14367) | 提出了一种名为Prot2Text的新方法，通过结合GNNs和Transformers，以自由文本样式预测蛋白质的功能。该方法能够综合蛋白质的序列、结构和文本注释等多种数据类型，超越传统的二进制或分类分类，实现了对蛋白质功能的全面表示。 |
| [^35] | [A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe.](http://arxiv.org/abs/2307.14361) | 本研究提出了一个基于多种机器学习算法和嵌入模型的集成模型，用于基因突变在癌症中的分类。实验结果表明，该模型在准确率、精确率、召回率等指标上优于其他传统和最新的转换器模型，并且具有更高的训练效率。 |
| [^36] | [Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition.](http://arxiv.org/abs/2307.14132) | 本文提出了一种名为CIF-Transducer的新型模型，将连续积分和火机制与RNN-T模型结合起来，实现了高效的对齐，并放弃了RNN-T Loss，从而减少了计算量，并使预测网络发挥更重要的作用。实验证明CIF-T在自动语音识别中取得了最先进的结果。 |
| [^37] | [Evaluating Large Language Models for Radiology Natural Language Processing.](http://arxiv.org/abs/2307.13693) | 本研究通过对32个大型语言模型进行评估，填补了放射学自然语言处理领域的评估空白。评估结果为这些模型的性能、优势和弱点提供了关键见解，为它们在医学领域的实际应用提供了指导。 |
| [^38] | [Empower Your Model with Longer and Better Context Comprehension.](http://arxiv.org/abs/2307.13365) | 本文研究了大语言模型（LLMs）内的信息传递，并提出了一种名为注意力转移的技术，该技术能够使模型在不增加训练或对生成流畅性的影响的情况下实现更长更好的上下文理解。 |
| [^39] | [RRAML: Reinforced Retrieval Augmented Machine Learning.](http://arxiv.org/abs/2307.12798) | RRAML是一种新的机器学习框架，将大型语言模型（LLMs）的推理能力与用户提供的庞大数据库中的支持信息相结合。利用强化学习的进展，该方法成功解决了几个关键挑战。 |
| [^40] | [Is ChatGPT a Good Personality Recognizer? A Preliminary Study.](http://arxiv.org/abs/2307.03952) | 这篇论文进行了初步评估，探索了ChatGPT在文本中识别人格的能力，特别是通过面向层级的提示策略。论文比较了ChatGPT和传统神经网络、RoBERTa在两个实际数据集上的表现。 |
| [^41] | [Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification.](http://arxiv.org/abs/2307.02377) | 本研究介绍了Fraunhofer SIT团队在CLEF-2023 CheckThat!英语实验室任务1B中使用模型混合技术解决分类不确定性的方法，该方法的目的是确定政治辩论中的文本片段是否值得进行事实检查评估，并在比赛中排名第二。 |
| [^42] | [Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting.](http://arxiv.org/abs/2307.00866) | 这篇论文提出了一种增强查询的网络用于不完整话语重写，通过引入查询模板来明确语义结构知识，并采用有效的编辑操作评分网络来建模两个标记之间的关系。在多个公共数据集上，该模型取得了最先进的性能。 |
| [^43] | [Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets.](http://arxiv.org/abs/2307.00610) | 本文提出了一种混合单模分类器的方法，通过组合图像和文本分类器的结果，成功进行多模态推文的可靠性估计，并在CheckThat! 2023任务1A中取得了最佳表现。 |
| [^44] | [Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale.](http://arxiv.org/abs/2306.00017) | 本文提出结合符号表示和自下而上的逆向工程的方法，解决大规模语言模型在真正语言理解上的局限性，实现可解释的、语言无关的LLMs。 |
| [^45] | [PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning.](http://arxiv.org/abs/2305.19472) | PlaSma提出了一种使用小型语言模型进行过程知识和计划能力的新方法， |
| [^46] | [Emotion Experiencer Recognition as a Prerequisite for Experiencer-Specific Emotion Analysis.](http://arxiv.org/abs/2305.16731) | 本文提出了一种用于检测情感体验者并为其分配情感的自动方法，并进行了相关的实验。该方法的实现具有挑战性，但展示了在文本中检测情感体验者的可行性。 |
| [^47] | [Context-Aware Attention Layers coupled with Optimal Transport Domain Adaptation methods for recognizing dementia from spontaneous speech.](http://arxiv.org/abs/2305.16406) | 本论文提出一种基于情境感知注意力层及最优传输域自适应方法的语音识别痴呆症的新方法。该方法捕捉了模态内部和模态间的交互，并实现了模型校准。 |
| [^48] | [Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond.](http://arxiv.org/abs/2305.15299) | 这篇论文回顾了生成AI对科学研究所带来的认识论挑战、伦理和诚信风险，并提出了十项建议，以在AI时代促进更负责任的研究进行。 |
| [^49] | [A Confidence-based Partial Label Learning Model for Crowd-Annotated Named Entity Recognition.](http://arxiv.org/abs/2305.12485) | 本论文提出了一种基于置信度的部分标签学习方法（CPLL）用于群体注释的命名实体识别。该模型通过迭代更新真实后验和置信度，通过最小化经验风险学习一个基于标记和内容的置信度，实验结果表明该方法能够提高NER的性能。 |
| [^50] | [In-Context Retrieval-Augmented Language Models.](http://arxiv.org/abs/2302.00083) | 本研究提出了一种上下文检索增强的语言模型（In-Context RALM）方法，通过将相关文件作为输入的一部分，无需对语言模型进行进一步的训练即可显著提高语言建模性能和源归因能力，并且相对于现有的RALM方法，它具有更简单的部署过程。 |
| [^51] | [ThoughtSource: A central hub for large language model reasoning data.](http://arxiv.org/abs/2301.11596) | ThoughtSource是一个用于连续思考推理的元数据集和软件库，旨在通过促进对连续思考的定性理解、实证评估和提供训练数据，改进未来的人工智能系统。 |
| [^52] | [Large Language Models Struggle to Learn Long-Tail Knowledge.](http://arxiv.org/abs/2211.08411) | 本文研究了大型语言模型记忆的知识与预训练数据集中信息之间的关系，并发现其回答基于事实的问题的能力与在预训练过程中接触到的相关文档数量之间存在强相关性和因果关系。 |
| [^53] | [Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning.](http://arxiv.org/abs/2205.14704) | 本论文提出了一种检索增强的提示学习方法，通过将知识从记忆中解耦，帮助模型在泛化和记忆之间取得平衡。 |
| [^54] | [Differentiable Subset Pruning of Transformer Heads.](http://arxiv.org/abs/2108.04657) | 这种可微分的Transformer头部子集修剪方法可以安全地修剪掉大部分头部，使得模型更小更快，而且在稀疏程度上具有可比或更好的性能。 |

# 详细

[^1]: 一种几何观念的因果探测

    A Geometric Notion of Causal Probing. (arXiv:2307.15054v1 [cs.CL])

    [http://arxiv.org/abs/2307.15054](http://arxiv.org/abs/2307.15054)

    本文提出了一种几何观念的因果探测方法，通过在语言模型表示空间的子空间上进行反事实干预，优化了因果概念子空间，以实现概念控制生成。

    

    大型语言模型依赖于文本的实值表示来进行预测。这些表示包含了模型在训练数据上学到的信息，包括语言属性和基于性别的人口偏见等。越来越多的研究关注通过在表示空间的子空间上进行正交投影来获得关于这些概念的信息。我们通过提出语言模型表示空间子空间的内在信息的形式定义，为这项研究贡献了新的内容。我们提出了一种反事实方法来避免虚假相关的失效模式，通过独立处理子空间中的分量和其正交补空间中的分量。我们展示了在子空间中的反事实信息概念是由一个因果概念子空间进行优化的。此外，这种干预使我们能够通过操作来尝试概念控制生成。

    Large language models rely on real-valued representations of text to make their predictions. These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender. A growing body of work has considered information about concepts such as these using orthogonal projections onto subspaces of the representation space. We contribute to this body of work by proposing a formal definition of intrinsic information in a subspace of a language model's representation space. We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently. We show that our counterfactual notion of information in a subspace is optimizing by an causal concept subspace. Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the
    
[^2]: 使用大型语言模型将患者与临床试验匹配

    Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])

    [http://arxiv.org/abs/2307.15051](http://arxiv.org/abs/2307.15051)

    本研究调查了使用大型语言模型（LLMs）来帮助患者和转诊医生识别合适的临床试验的潜力，并引入了TrialGPT架构，该架构能够准确预测合格性并提供解释，实验证明其有效性。

    

    临床试验在推动药物研发和基于证据的医学方面非常重要，但患者招募常常受到限制。在这项工作中，我们调查了使用大型语言模型（LLMs）来帮助患者和转诊医生识别合适的临床试验的潜力。具体而言，我们引入了一种新颖的架构TrialGPT，采用LLMs预测基于标准的合格性，并提供详细的解释，并根据患者病历中的自由文本来对候选临床试验进行排名和排除。我们在三个公开可用的184名患者和18,238个注释的临床试验的队列上评估了TrialGPT。实验结果表明几个关键发现：第一，TrialGPT在标准级别的预测准确性上表现出很高的准确率，并提供准确的解释。第二，TrialGPT的综合试验级别评分与专家标注的合格性高度相关。第三，这些评分

    Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scor
    
[^3]: 对齐语言模型上的通用和可迁移对抗攻击

    Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])

    [http://arxiv.org/abs/2307.15043](http://arxiv.org/abs/2307.15043)

    这项研究提出了一种简单而有效的攻击方法，能够使对齐的语言模型生成不良行为，而不依赖于人工设计，通过自动化方法产生对抗性后缀，并在实践中取得改进。

    

    由于“开箱即用”的大型语言模型能够生成大量引起反感的内容，最新的研究专注于对齐这些模型，以防止产生不良生成。尽管在规避这些措施上取得了一些成功，所谓的对LLMs的“越狱”攻击，但这些攻击需要人为的巧思，实际上并不稳定。在本文中，我们提出了一种简单而有效的攻击方法，使对齐的语言模型生成不良行为。具体而言，我们的方法找到一个后缀，当附加到各种查询上，供LLM生成不良内容时，旨在最大化模型产生肯定回答（而不是拒绝回答）的概率。然而，与其依赖手工设计，我们的方法通过贪婪和基于梯度的搜索技术自动产生这些对抗性后缀，并且在过去的自动化方法上进行了改进。

    Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past autom
    
[^4]: SuperCLUE:一个全面的中文大型语言模型基准测试

    SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. (arXiv:2307.15020v1 [cs.CL])

    [http://arxiv.org/abs/2307.15020](http://arxiv.org/abs/2307.15020)

    提出了一个全面的中文大型语言模型基准测试SuperCLUE，包括实际用户的查询和评级、开放性问题以及封闭性问题，该基准测试填补了目前对模型在实际应用中能力理解的空白。

    

    大型语言模型(LLMs)已经显示出将其整合到人们日常生活中的潜力。因此，用户偏好是评估LLMs在实际场景中表现的最关键标准。然而，现有的基准主要集中在使用多选题来衡量模型的准确性，这限制了对它们在实际应用中能力的理解。我们通过提出一个全面的中文基准测试SuperCLUE来填补这个空白，该基准测试以另一个流行的中文LLM基准测试CLUE命名。SuperCLUE包括三个子任务：来自一个LLM对战平台(CArena)的实际用户查询和评级，有单个和多轮对话的开放性问题(OPEN)，以及与开放性单轮问题相同茎的封闭性问题(CLOSE)。我们的研究表明，封闭性问题上的准确性不足以反映在开放性问题上实现的人类偏好。同时，它们可以互补地预测实际用户偏好。

    Large language models (LLMs) have shown the potential to be integrated into human daily lives. Therefore, user preference is the most critical criterion for assessing LLMs' performance in real-world scenarios. However, existing benchmarks mainly focus on measuring models' accuracy using multi-choice questions, which limits the understanding of their capabilities in real applications. We fill this gap by proposing a comprehensive Chinese benchmark SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE encompasses three sub-tasks: actual users' queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE). Our study shows that accuracy on closed-ended questions is insufficient to reflect human preferences achieved on open-ended ones. At the same time, they can complement each other to predict actual user prefe
    
[^5]: Gzip与KNN在文本分类中的对比研究

    Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v1 [cs.CL])

    [http://arxiv.org/abs/2307.15002](http://arxiv.org/abs/2307.15002)

    Gzip与KNN相比较在文本分类中，我们发现通过简单的词袋匹配可以获得类似或更好的准确性，并且更加高效。

    

    最近，基于KNN的文本分类中压缩距离（gzip）的有效性引起了很多关注。在本文中，我们展示了通过更简单的方法可以达到类似或更好的效果，并且可能不需要文本压缩。实际上，我们发现简单的“词袋”匹配可以达到类似或更好的准确性，并且更高效。

    The effectiveness of compression distance in KNN-based text classification ('gzip') has recently garnered lots of attention. In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary. Indeed, we find that a simple 'bag-of-words' matching can achieve similar or better accuracy, and is more efficient.
    
[^6]: 将TransNormer扩展到1750亿参数

    Scaling TransNormer to 175 Billion Parameters. (arXiv:2307.14995v1 [cs.CL])

    [http://arxiv.org/abs/2307.14995](http://arxiv.org/abs/2307.14995)

    本论文提出了TransNormerLLM，这是第一个基于线性注意力的大型语言模型，在准确性和效率方面优于传统基于softmax注意力的模型。通过引入位置嵌入、线性注意力加速、门控机制等先进改进，并利用Lightning Attention技术加速线性注意力，以及采用张量归一化方案加速模型，提高了TransNormer的性能。

    

    我们提出了TransNormerLLM，这是第一个基于线性注意力的大型语言模型（LLM），在准确性和效率两方面都优于传统的基于softmax注意力的模型。TransNormerLLM从之前的线性注意力架构TransNormer发展而来，通过引入位置嵌入、线性注意力加速、门控机制、张量归一化、推理加速和稳定化等先进改进。具体地，我们使用LRPE与指数衰减结合，既避免了注意力稀释问题，又使模型保留了标记之间的全局交互。此外，我们提出了Lightning Attention，一种先进的技术，可以将线性注意力加速超过两倍，并将内存使用量减少了四倍。为了进一步提高TransNormer的性能，我们利用门控机制平滑训练，并采用新的张量归一化方案加速模型，从而取得了令人印象深刻的效果。

    We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive 
    
[^7]: 增量计算的神经网络：处理动态输入的高效推理方法

    Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])

    [http://arxiv.org/abs/2307.14988](http://arxiv.org/abs/2307.14988)

    本论文介绍了一种增量计算的神经网络方法，通过离散化中间值并过滤不必要的修改，实现了对动态输入的高效推理。

    

    深度学习在处理动态输入（例如传感器数据或用户输入）时常面临着高效处理的挑战。本论文提出了一种增量计算的方法，通过重复使用计算来适应输入变化，以解决这个问题。我们使用向量量化来离散化网络中的中间值，并过滤噪声和不必要的隐藏神经元修改，从而促进值的重用。我们将此方法应用于Transformer架构，创建了一个高效的增量推理算法。

    Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs. For example, an AI writing assistant is required to update its suggestions in real time as a document is edited. Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization. Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change. However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexi
    
[^8]: PanGu-Coder2：利用排名反馈增强大型语言模型在代码生成方面的能力

    PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. (arXiv:2307.14936v1 [cs.CL])

    [http://arxiv.org/abs/2307.14936](http://arxiv.org/abs/2307.14936)

    本文提出了一种新的框架RRTF，以增强预训练的大型语言模型在代码生成方面的能力。PanGu-Coder2是该框架的实现，在多个基准测试中均表现出色，优于其他先前的Code LLMs。

    

    大型语言模型（Code LLM）在代码生成任务上展现出了卓越的性能，每周都有新的强大模型发布。为了提高预训练的Code LLM的代码生成性能，提出了各种方法，如有监督的微调、指令微调、强化学习等。本文提出了一种新颖的RRTF（Rank Responses to align Test&Teacher Feedback）框架，可以有效、高效地提升预训练的大型语言模型在代码生成方面的能力。在该框架下，我们提出了PanGu-Coder2，在OpenAI HumanEval基准上取得了62.20%的一级通过率。此外，通过对CoderEval和LeetCode基准的广泛评估，我们展示了PanGu-Coder2始终优于所有先前的Code LLMs。

    Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.
    
[^9]: PAN 2023上的ARC-NLP：基于转换的自然语言推理用于写作风格检测

    ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection. (arXiv:2307.14913v1 [cs.CL])

    [http://arxiv.org/abs/2307.14913](http://arxiv.org/abs/2307.14913)

    这篇论文介绍了一个基于转换的自然语言推理方法，用于多作者写作风格检测。作者将任务视为自然语言推理问题，重点关注段落之间的过渡。实验结果表明，该方法优于其他模型版本，并在不同设置下达到了良好的性能。

    

    多作者写作风格检测任务旨在发现给定文本中的写作风格变化位置。我们将任务形式化为一个自然语言推理问题，其中两个连续段落配对。我们的方法侧重于段落之间的过渡，并在任务中截断输入标记。作为主干模型，我们采用了基于Transformer的不同编码器，在训练期间使用预热阶段。我们提交的模型版本在实验中优于基线和其他提出的模型版本。对于简单和中等设置，我们提交了基于DeBERTa和预热训练的基于转换的自然语言推理，对于困难设置，使用相同模型但无过渡。

    The task of multi-author writing style detection aims at finding any positions of writing style change in a given text document. We formulate the task as a natural language inference problem where two consecutive paragraphs are paired. Our approach focuses on transitions between paragraphs while truncating input tokens for the task. As backbone models, we employ different Transformer-based encoders with warmup phase during training. We submit the model version that outperforms baselines and other proposed model versions in our experiments. For the easy and medium setups, we submit transition-focused natural language inference based on DeBERTa with warmup training, and the same model without transition for the hard setup.
    
[^10]: ARC-NLP在PAN 2023上的应用：用于触发词检测的层次化长文本分类

    ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection. (arXiv:2307.14912v1 [cs.CL])

    [http://arxiv.org/abs/2307.14912](http://arxiv.org/abs/2307.14912)

    本文介绍了ARC-NLP在PAN 2023的应用，通过构建层次化模型来进行触发词检测，实现了对粉丝小说文档中多个触发词的检测。该模型利用Transformer模型进行微调，并在多标签设置中使用LSTM模型进行训练。

    

    粉丝小说是一种在已建立的虚构世界中进行的创造性写作形式，已经在网络上拥有大量的追随者。然而，确保参与者的福祉和安全已经成为该社区的一个关键问题。检测可能导致读者情感困扰或创伤的刺激性内容是一个重要挑战。本文描述了我们在PAN CLEF 2023的触发词检测共享任务中的方法，我们希望检测给定粉丝小说文档中的多个触发词。为此，我们构建了一个使用基于Transformer的语言模型的递归层次模型。在我们的方法中，我们首先将长文档拆分为较小的段落，并使用它们来微调Transformer模型。然后，我们从微调的Transformer模型中提取特征嵌入，这些嵌入被用作多标签设置中用于触发词检测的多个LSTM模型的训练的输入。我们的模型达到了F1-macro sco

    Fanfiction, a popular form of creative writing set within established fictional universes, has gained a substantial online following. However, ensuring the well-being and safety of participants has become a critical concern in this community. The detection of triggering content, material that may cause emotional distress or trauma to readers, poses a significant challenge. In this paper, we describe our approach for the Trigger Detection shared task at PAN CLEF 2023, where we want to detect multiple triggering content in a given Fanfiction document. For this, we build a hierarchical model that uses recurrence over Transformer-based language models. In our approach, we first split long documents into smaller sized segments and use them to fine-tune a Transformer model. Then, we extract feature embeddings from the fine-tuned Transformer model, which are used as input in the training of multiple LSTM models for trigger detection in a multi-label setting. Our model achieves an F1-macro sco
    
[^11]: 为解决分类中的类别不平衡问题，基于检索的文本选择方法

    Retrieval-based Text Selection for Addressing Class-Imbalanced Data in Classification. (arXiv:2307.14899v1 [cs.CL])

    [http://arxiv.org/abs/2307.14899](http://arxiv.org/abs/2307.14899)

    本文解决了在文本分类中选择有限数量的文本进行注释的问题，并提出了利用检索方法和语义搜索来处理类别不平衡的二元分类问题。通过利用SHAP构建高质量的查询集，帮助选择适合注释的文本，以解决长期注释任务中的挑战。

    

    本文解决了在文本分类中使用检索方法选择一组文本进行注释的问题，由于人力资源的限制，注释的数量有限。同时，我们还解决了二元类别的不平衡问题，即正样本数量较少的情况。在我们的情况下，注释是在较长的时间段内进行的，可以将要注释的文本批量选择，以前面的注释来指导下一组的选择。为了解决这些挑战，本文提出了利用SHAP构建Elasticsearch和语义搜索的高质量查询集，以尝试识别一组最优文本来帮助解决类别不平衡问题。该方法在描述可能的未来事件的提纲文本集上进行了测试，该文本集由参与肥胖和糖尿病管理研究的参与者构建。我们介绍一种有效的方法

    This paper addresses the problem of selecting of a set of texts for annotation in text classification using retrieval methods when there are limits on the number of annotations due to constraints on human resources. An additional challenge addressed is dealing with binary categories that have a small number of positive instances, reflecting severe class imbalance. In our situation, where annotation occurs over a long time period, the selection of texts to be annotated can be made in batches, with previous annotations guiding the choice of the next set. To address these challenges, the paper proposes leveraging SHAP to construct a quality set of queries for Elasticsearch and semantic search, to try to identify optimal sets of texts for annotation that will help with class imbalance. The approach is tested on sets of cue texts describing possible future events, constructed by participants involved in studies aimed to help with the management of obesity and diabetes. We introduce an effec
    
[^12]: MESED：一个具有细粒度语义类和困难负面实体的多模态实体集扩展数据集

    MESED: A Multi-modal Entity Set Expansion Dataset with Fine-grained Semantic Classes and Hard Negative Entities. (arXiv:2307.14878v1 [cs.CL])

    [http://arxiv.org/abs/2307.14878](http://arxiv.org/abs/2307.14878)

    这个论文提出了一个多模态实体集扩展数据集MESED，用于解决传统方法难以处理的负面实体、同义实体、多义实体和长尾实体等问题。模型通过集成多模态信息来表示实体，能够提供互补信息、统一信号和强大的对齐信号。

    

    实体集扩展（ESE）任务旨在用属于相同语义类的新实体扩展一小部分种子实体。传统的ESE方法基于单一模态（即文字模态），难以处理现实世界中的复杂实体，如：（1）具有细粒度语义差异的负面实体。 （2）同义实体。 （3）多义实体。 （4）长尾实体。这些挑战促使我们提出了多模态实体集扩展（MESE），其中模型集成了来自多个模态的信息来表示实体。直观地说，多模态信息对ESE的好处有三个：（1）不同的模态可以提供互补的信息。（2）多模态信息通过共同的视觉特性提供了同一个语义类或实体的统一信号。 （3）多模态信息为同义实体提供了强大的对齐信号。为了评估MESE中模型的性能并促进进一步研究

    The Entity Set Expansion (ESE) task aims to expand a handful of seed entities with new entities belonging to the same semantic class. Conventional ESE methods are based on mono-modality (i.e., literal modality), which struggle to deal with complex entities in the real world such as: (1) Negative entities with fine-grained semantic differences. (2) Synonymous entities. (3) Polysemous entities. (4) Long-tailed entities. These challenges prompt us to propose Multi-modal Entity Set Expansion (MESE), where models integrate information from multiple modalities to represent entities. Intuitively, the benefits of multi-modal information for ESE are threefold: (1) Different modalities can provide complementary information. (2) Multi-modal information provides a unified signal via common visual properties for the same semantic class or entity. (3) Multi-modal information offers robust alignment signal for synonymous entities. To assess the performance of model in MESE and facilitate further rese
    
[^13]: 发挥Seq2Seq模型作为稳健少样本学习器的潜力

    Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners. (arXiv:2307.14856v1 [cs.CL])

    [http://arxiv.org/abs/2307.14856](http://arxiv.org/abs/2307.14856)

    Seq2Seq模型作为少样本学习器的潜力在解码器和编码-解码模型中进行了广泛研究，提出了两种能有效提升Seq2Seq模型上下文学习能力的方法，并在各种任务中显示出显著的性能改进。

    

    在上下文学习中，只有解码器模型具有明显优势，而编码-解码（即Seq2Seq）模型在依赖于权重更新的方法中表现出色。最近，一些研究表明Seq2Seq模型可以进行少样本学习，但这仅限于与Seq2Seq体系结构相匹配的任务，如摘要和翻译。受到这些初始研究的启发，我们首次进行了广泛的实验，比较了解码器和编码-解码模型在各种任务的上下文少样本学习能力。此外，我们提出了两种能更有效地引发Seq2Seq模型上下文学习能力的方法：目标对齐提示和基于融合的方法。值得注意的是，我们的方法在性能上超过了一个体积是其六倍的解码器模型，并且相较于常规Seq2Seq模型显示出显著的性能改进。

    In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates. Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks. Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a var
    
[^14]: ArcGPT：一个专为现实世界档案应用定制的大型语言模型

    ArcGPT: A Large Language Model Tailored for Real-world Archival Applications. (arXiv:2307.14852v1 [cs.CL])

    [http://arxiv.org/abs/2307.14852](http://arxiv.org/abs/2307.14852)

    ArcGPT是第一个专为档案领域定制的通用语言模型，通过在大规模档案领域数据上进行预训练，ArcGPT在真实档案任务中表现出色，标志着有效档案数据管理迈出了重要一步。

    

    档案在信息和知识保护中起着至关重要的作用，这些数据的指数增长需要高效和自动化的工具来管理和利用档案信息资源。档案应用涉及管理大规模数据，这些数据很难处理和分析。虽然语言模型在不同领域取得了显著进展，但还没有公开可用的面向档案的语言模型。为填补这个空白，我们引入了ArcGPT，据我们所知，这是第一个专门为档案领域量身定制的通用语言模型。为了提高模型在真实档案任务中的性能，ArcGPT在大规模和广泛的档案领域数据上进行了预训练。除ArcGPT外，我们还发布了AMBLE，一个包含四个真实档案任务的基准。在AMBLE上的评估表明，ArcGPT优于现有的最先进模型，在有效档案数据管理方面迈出了重要的一步。最终，ArcGPT旨在更好地服务档案界，助力改善档案数据管理。

    Archives play a crucial role in preserving information and knowledge, and the exponential growth of such data necessitates efficient and automated tools for managing and utilizing archive information resources. Archival applications involve managing massive data that are challenging to process and analyze. Although LLMs have made remarkable progress in diverse domains, there are no publicly available archives tailored LLM. Addressing this gap, we introduce ArcGPT, to our knowledge, the first general-purpose LLM tailored to the archival field. To enhance model performance on real-world archival tasks, ArcGPT has been pre-trained on massive and extensive archival domain data. Alongside ArcGPT, we release AMBLE, a benchmark comprising four real-world archival tasks. Evaluation on AMBLE shows that ArcGPT outperforms existing state-of-the-art models, marking a substantial step forward in effective archival data management. Ultimately, ArcGPT aims to better serve the archival community, aidi
    
[^15]: 在这篇论文中，我们介绍了首次将母语识别（Native Language Identification，NLI）应用于土耳其语的研究。

    Turkish Native Language Identification. (arXiv:2307.14850v1 [cs.CL])

    [http://arxiv.org/abs/2307.14850](http://arxiv.org/abs/2307.14850)

    这项研究首次将母语识别应用于土耳其语,通过分析作者不同语言的写作来预测作者的母语。研究使用了土耳其学习者语料库和三个句法特征来展示其有效性。

    

    在这篇论文中，我们首次将母语识别（NLI）应用于土耳其语。NLI 是通过分析作者不同语言的写作来预测作者的母语。尽管大多数NLI研究都侧重于英语，我们的研究将其范围扩展到土耳其语。我们使用了最近构建的土耳其学习者语料库，并结合了三个句法特征（CFG 产生规则，词性n-gram和函数词）与L2文本，以展示它们在该任务中的有效性。

    In this paper, we present the first application of Native Language Identification (NLI) for the Turkish language. NLI involves predicting the writer's first language by analysing their writing in different languages. While most NLI research has focused on English, our study extends its scope to Turkish. We used the recently constructed Turkish Learner Corpus and employed a combination of three syntactic features (CFG production rules, part-of-speech n-grams and function words) with L2 texts to demonstrate their effectiveness in this task.
    
[^16]: 什么是一个好的改写：自动评估是否有效？

    What Makes a Good Paraphrase: Do Automated Evaluations Work?. (arXiv:2307.14818v1 [cs.CL])

    [http://arxiv.org/abs/2307.14818](http://arxiv.org/abs/2307.14818)

    本文研究了一个好的改写应该是什么样的，是否可以仅使用自动评估来评估改写的质量。研究通过实验和专家评估得出结论。

    

    改写是用不同的词语表达一个重要思想或含义的任务。但是为了被认为是可接受的改写，这些词语应该有多么不同？我们可以仅使用自动评估指标来评估改写的质量吗？我们通过对一个德语数据集进行实验，并进行自动和专家语言评估来尝试回答这些问题。

    Paraphrasing is the task of expressing an essential idea or meaning in different words. But how different should the words be in order to be considered an acceptable paraphrase? And can we exclusively use automated metrics to evaluate the quality of a paraphrase? We attempt to answer these questions by conducting experiments on a German data set and performing automatic and expert linguistic evaluation.
    
[^17]: 参考生成模型：它们是否经得起时间的考验？

    Models of reference production: How do they withstand the test of time?. (arXiv:2307.14817v1 [cs.CL])

    [http://arxiv.org/abs/2307.14817](http://arxiv.org/abs/2307.14817)

    本研究通过分析目前的自然语言处理研究中的参考生成模型，发现GREC不再是一个可靠的评估工具，并证明了预训练语言模型在此任务中具有较强的鲁棒性。

    

    近年来，许多自然语言处理研究仅关注性能的提高。本文从语言和科学角度出发，针对自然语言处理中生成指代表达的任务进行了研究。我们以GREC为案例研究，GREC是十多年前关于该主题的一系列英语共享任务的综合集合。我们研究了如果我们以更现实的数据集和更高级的方法进行评估，模型的表现会如何。我们使用不同的评估指标和特征选择实验来测试模型。我们得出结论，由于语料库和评估指标的选择对结果产生了很大影响，GREC不再被视为可靠评估模型模拟人类参考生成能力的工具。我们的结果还表明，与传统的机器学习模型相比，预训练语言模型对语料库的选择不太依赖，因此具有更强的鲁棒性。

    In recent years, many NLP studies have focused solely on performance improvement. In this work, we focus on the linguistic and scientific aspects of NLP. We use the task of generating referring expressions in context (REG-in-context) as a case study and start our analysis from GREC, a comprehensive set of shared tasks in English that addressed this topic over a decade ago. We ask what the performance of models would be if we assessed them (1) on more realistic datasets, and (2) using more advanced methods. We test the models using different evaluation metrics and feature selection experiments. We conclude that GREC can no longer be regarded as offering a reliable assessment of models' ability to mimic human reference production, because the results are highly impacted by the choice of corpus and evaluation metrics. Our results also suggest that pre-trained language models are less dependent on the choice of corpus than classic Machine Learning models, and therefore make more robust cla
    
[^18]: 提高基于方面的情感分析的方法：使用端到端语义角色标注模型

    Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model. (arXiv:2307.14785v1 [cs.CL])

    [http://arxiv.org/abs/2307.14785](http://arxiv.org/abs/2307.14785)

    本文提出了一种利用语义角色标注模型提升基于方面的情感分析性能的方法，通过构建端到端语义角色标注模型有效捕获结构化语义信息，并在英语和捷克语上实现了最佳结果。

    

    本文提出了一系列的方法，旨在通过利用从语义角色标注（SRL）模型中提取的语义信息来提高基于方面的情感分析（ABSA）的性能。我们提出了一种新颖的端到端语义角色标注模型，能够有效捕获Transformer隐藏状态中的大部分结构化语义信息。我们相信这种端到端模型非常适合我们新提出的融合语义信息的模型。我们使用ELECTRA-small模型在英语和捷克语两种语言上评估了提出的模型。我们的组合模型在两种语言的ABSA性能上都有所提升。此外，我们在捷克语ABSA上取得了新的最佳结果。

    This paper presents a series of approaches aimed at enhancing the performance of Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic information from a Semantic Role Labeling (SRL) model. We propose a novel end-to-end Semantic Role Labeling model that effectively captures most of the structured semantic information within the Transformer hidden state. We believe that this end-to-end model is well-suited for our newly proposed models that incorporate semantic information. We evaluate the proposed models in two languages, English and Czech, employing ELECTRA-small models. Our combined models improve ABSA performance in both languages. Moreover, we achieved new state-of-the-art results on the Czech ABSA.
    
[^19]: Emotion4MIDI：基于歌词的带情感标签的符号音乐数据集

    Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset. (arXiv:2307.14783v1 [eess.AS])

    [http://arxiv.org/abs/2307.14783](http://arxiv.org/abs/2307.14783)

    Emotion4MIDI是一个包含12k个带情感标签的符号音乐数据集，通过在GoEmotions数据集上训练情感分类模型，并应用于两个大规模的MIDI数据集的歌词，提供了一个宝贵的资源来探索音乐和情感之间的联系，并开发可以根据特定情感生成音乐的模型。

    

    我们提出了一个新的大规模带情感标签的符号音乐数据集，包含12k个MIDI歌曲。为了创建这个数据集，我们首先在GoEmotions数据集上训练情感分类模型，使用一半大小的模型实现了最先进的结果。然后，我们将这些模型应用于两个大规模的MIDI数据集的歌词。我们的数据集涵盖了广泛的细粒度情感，为探索音乐和情感之间的联系，尤其是开发可以根据特定情感生成音乐的模型提供了宝贵的资源。我们的推断代码、训练模型和数据集都可以在线获取。

    We present a new large-scale emotion-labeled symbolic music dataset consisting of 12k MIDI songs. To create this dataset, we first trained emotion classification models on the GoEmotions dataset, achieving state-of-the-art results with a model half the size of the baseline. We then applied these models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide range of fine-grained emotions, providing a valuable resource to explore the connection between music and emotions and, especially, to develop models that can generate music based on specific emotions. Our code for inference, trained models, and datasets are available online.
    
[^20]: 将Whisper转化为实时转录系统

    Turning Whisper into Real-Time Transcription System. (arXiv:2307.14743v1 [cs.CL])

    [http://arxiv.org/abs/2307.14743](http://arxiv.org/abs/2307.14743)

    本文介绍了一个基于Whisper的实时语音转录和翻译系统Whisper-Streaming，通过使用本地协议策略和自适应延迟，实现了流式转录。实验结果表明Whisper-Streaming在未分割的长篇演讲转录测试集上具有高质量和低延迟，并且在实际多语种会议中的应用也具有鲁棒性和实用性。

    

    Whisper是最近的一种最先进的多语种语音识别和翻译模型，然而，它并不适用于实时转录。在本文中，我们在Whisper的基础上构建了Whisper-Streaming，这是一个实时语音转录和翻译实现Whisper类模型的系统。Whisper-Streaming使用本地协议策略和自适应延迟，实现了流式转录。我们展示了Whisper-Streaming在未分割的长篇演讲转录测试集上达到了高质量和3.3秒的延迟，并展示了它作为一个多语种会议实时转录服务组件的鲁棒性和实用性。

    Whisper is one of the recent state-of-the-art multilingual speech recognition and translation models, however, it is not designed for real time transcription. In this paper, we build on top of Whisper and create Whisper-Streaming, an implementation of real-time speech transcription and translation of Whisper-like models. Whisper-Streaming uses local agreement policy with self-adaptive latency to enable streaming transcription. We show that Whisper-Streaming achieves high quality and 3.3 seconds latency on unsegmented long-form speech transcription test set, and we demonstrate its robustness and practical usability as a component in live transcription service at a multilingual conference.
    
[^21]: 评估生成模型在图文生成中的应用

    Evaluating Generative Models for Graph-to-Text Generation. (arXiv:2307.14712v1 [cs.CL])

    [http://arxiv.org/abs/2307.14712](http://arxiv.org/abs/2307.14712)

    本文评估了生成模型在图文生成中的应用，并发现生成模型能够生成流畅并连贯的文本。然而，生成模型仍然困难理解实体之间的语义关系，并且容易生成带有幻觉或无关信息的文本。

    

    大型语言模型（LLM）已广泛应用于图文生成任务。然而，微调LLM的过程需要大量的训练资源和注释工作。本文中，我们探索了生成模型在无需微调的情况下，从图数据中生成描述性文本的能力。具体而言，我们评估了GPT-3和ChatGPT在两个图文数据集上的性能，并将其与微调的LLM模型（如T5和BART）进行了比较。我们的结果表明，生成模型能够生成流畅并连贯的文本，在AGENDA和WebNLG数据集上分别实现了10.57和11.08的BLEU分数。然而，我们的错误分析揭示了生成模型仍然难以理解实体之间的语义关系，而且它们还倾向于生成具有幻觉或无关信息的文本。作为错误分析的一部分，我们利用BERT检测机器生成的文本，并取得了较高的宏F1分数。

    Large language models (LLMs) have been widely employed for graph-to-text generation tasks. However, the process of finetuning LLMs requires significant training resources and annotation work. In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART. Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information. As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores.
    
[^22]: 改进基于Transformer模型和语言学预训练的阿拉伯语自然语言推理

    Improving Natural Language Inference in Arabic using Transformer Models and Linguistically Informed Pre-Training. (arXiv:2307.14666v1 [cs.CL])

    [http://arxiv.org/abs/2307.14666](http://arxiv.org/abs/2307.14666)

    本文针对阿拉伯语自然语言处理中的自然语言推理问题进行研究，通过构建专门的数据集和应用语言学知识预训练方法，我们的语言特定模型在与最先进的多语言方法竞争时表现出色。

    

    本文针对自然语言处理领域中阿拉伯文本数据的分类问题进行研究，特别关注自然语言推理和矛盾检测。阿拉伯语被认为是资源匮乏的语言，意味着数据集稀缺，导致NLP方法有限。为了克服这个限制，我们从公开可用资源中创建了专门的数据集。随后，我们训练和评估了基于Transformer的机器学习模型。我们发现，当应用命名实体识别等语言学知识预训练方法时，一种特定于语言的模型（AraBERT）与最先进的多语言方法具有竞争力。据我们所知，这是阿拉伯语领域中这一任务的首次大规模评估，也是多任务预训练在此环境中的首次应用。

    This paper addresses the classification of Arabic text data in the field of Natural Language Processing (NLP), with a particular focus on Natural Language Inference (NLI) and Contradiction Detection (CD). Arabic is considered a resource-poor language, meaning that there are few data sets available, which leads to limited availability of NLP methods. To overcome this limitation, we create a dedicated data set from publicly available resources. Subsequently, transformer-based machine learning models are being trained and evaluated. We find that a language-specific model (AraBERT) performs competitively with state-of-the-art multilingual approaches, when we apply linguistically informed pre-training methods such as Named Entity Recognition (NER). To our knowledge, this is the first large-scale evaluation for this task in Arabic, as well as the first application of multi-task pre-training in this context.
    
[^23]: 基于度量的上下文学习：文本简化案例研究

    Metric-Based In-context Learning: A Case Study in Text Simplification. (arXiv:2307.14632v1 [cs.CL])

    [http://arxiv.org/abs/2307.14632](http://arxiv.org/abs/2307.14632)

    本文针对文本简化进行了一个案例研究，提出了一种基于度量的上下文学习（MBL）方法，通过选择具有顶级SARI得分的示例，可以在较大型的GPT模型上获得最佳表现。而在较小型的模型上，通过选择具有较高压缩比例的示例表现更好。

    

    对大型语言模型进行上下文学习（ICL）在许多自然语言处理任务中已被证明是一种强大的方法。然而，确定选择ICL示例的最佳方法并不容易，因为结果可以因使用的示例的质量、数量和顺序而变化很大。在本文中，我们进行了一个关于文本简化（TS）的案例研究，以探讨如何选择最佳和最健壮的ICL示例。我们提出了一种基于常用的TS度量（如SARI、压缩比例和BERT-Precision）进行选择的基于度量的上下文学习（MBL）方法。通过在标准的TS基准（如TurkCorpus和ASSET）上使用各种规模的GPT模型进行广泛的实验，我们表明在较大的模型（如GPT-175B）上选择的示例通过顶级SARI得分表现最佳，而压缩比例通常在较小的模型（如GPT-13B和GPT-6.7B）上表现更好。此外，我们证明MBL通常具有良好的鲁棒性。

    In-context learning (ICL) for large language models has proven to be a powerful approach for many natural language processing tasks. However, determining the best method to select examples for ICL is nontrivial as the results can vary greatly depending on the quality, quantity, and order of examples used. In this paper, we conduct a case study on text simplification (TS) to investigate how to select the best and most robust examples for ICL. We propose Metric-Based in-context Learning (MBL) method that utilizes commonly used TS metrics such as SARI, compression ratio, and BERT-Precision for selection. Through an extensive set of experiments with various-sized GPT models on standard TS benchmarks such as TurkCorpus and ASSET, we show that examples selected by the top SARI scores perform the best on larger models such as GPT-175B, while the compression ratio generally performs better on smaller models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is generally robust 
    
[^24]: 基于人工智能的速读工具，用于辅助患有ADHD、诵读困难或注意力不集中的学生

    Speed Reading Tool Powered by Artificial Intelligence for Students with ADHD, Dyslexia, or Short Attention Span. (arXiv:2307.14544v1 [cs.CL])

    [http://arxiv.org/abs/2307.14544](http://arxiv.org/abs/2307.14544)

    这项研究提出了一种基于人工智能的速读工具，用于帮助患有ADHD、诵读困难或注意力不集中的学生更高效地消化文本信息。该工具利用多层感知机算法和T5模型，通过文本处理和摘要任务实现。同时，该工具还应用了仿生阅读原则来增强可读性。

    

    本文提出了一种新颖的方法，用于帮助患有诵读困难、ADHD和注意力不集中的学生更高效地消化任何基于文本的信息。所提出的解决方案利用了多层感知机（MLP）算法进行复杂文本处理和摘要任务。该工具利用了Hugging Face的T5（文本到文本转换器）模型，将每个自然语言处理任务视为文本生成任务。模型使用较小的数据集对特定任务进行微调。NLTK的Punkt句子分词器用于将文本分割为句子列表。应用程序使用轻量级Web服务器和框架Flask提供。该工具还应用了仿生阅读的原则来增强可读性，包括加粗功能和对行、单词和字符间距的调整。本文讨论了基于人工智能的速读工具的方法、实现和结果。

    This paper presents a novel approach to assist students with dyslexia, ADHD, and short attention span in digesting any text-based information more efficiently. The proposed solution utilizes the Multilayer Perceptron (MLP) algorithm for complex text processing and summarization tasks. The tool leverages the T5 (Text-to-Text Transfer Transformer) model from Hugging Face, which treats every NLP task as a text generation task. The model is fine-tuned on specific tasks using a smaller dataset. The NLTK's Punkt Sentence Tokenizer is used to divide a text into a list of sentences. The application is served using Flask, a lightweight web server and framework. The tool also applies principles from Bionic Reading to enhance readability, which includes a bolding function and adjustments to line, word, and character spacing. The paper discusses the methodology, implementation, and results of the AI-based speed reading tool.
    
[^25]: 插入并祈祷：利用多模型模型的现成组件进行攻击

    Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models. (arXiv:2307.14539v1 [cs.CR])

    [http://arxiv.org/abs/2307.14539](http://arxiv.org/abs/2307.14539)

    本文通过引入对抗性嵌入空间攻击，探讨了将现成组件插入多模型模型中的漏洞和风险，并提出了一种不需要多模型系统权重和参数的攻击方法，通过寻找位于预训练组件嵌入空间危险区域的输入图像进行攻击。

    

    将额外的模态（如视觉）加入大型语言模型（LLM）的快速增长和日益受欢迎引起了严重的安全问题。这种模态的扩展类似于在房子上增加更多的门，无意中为对抗性攻击创建了多个访问点。在本文中，通过引入对抗性嵌入空间攻击，我们强调多模型系统中的漏洞，这些漏洞源于以插拔方式将现成组件（如公共预训练编码器）引入这些系统。与现有的工作相比，我们的方法不需要访问多模型系统的权重或参数，而是依赖于这些预训练编码器的庞大且未完全开发的嵌入空间。我们提出的嵌入空间攻击是通过寻找位于这些预训练组件的广泛嵌入空间的危险或目标区域的输入图像来进行的。

    The rapid growth and increasing popularity of incorporating additional modalities (e.g., vision) into large language models (LLMs) has raised significant security concerns. This expansion of modality, akin to adding more doors to a house, unintentionally creates multiple access points for adversarial attacks. In this paper, by introducing adversarial embedding space attacks, we emphasize the vulnerabilities present in multi-modal systems that originate from incorporating off-the-shelf components like public pre-trained encoders in a plug-and-play manner into these systems. In contrast to existing work, our approach does not require access to the multi-modal system's weights or parameters but instead relies on the huge under-explored embedding space of such pre-trained encoders. Our proposed embedding space attacks involve seeking input images that reside within the dangerous or targeted regions of the extensive embedding space of these pre-trained components. These crafted adversarial 
    
[^26]: CliniDigest: 基于大规模临床试验描述的大型语言模型的案例研究

    CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions. (arXiv:2307.14522v1 [cs.CL])

    [http://arxiv.org/abs/2307.14522](http://arxiv.org/abs/2307.14522)

    CliniDigest是一个基于大规模语言模型的临床试验摘要工具，能够实时、真实和全面地将长篇试验描述压缩成简洁的摘要。

    

    临床试验是评估新的生物医学干预措施的研究。为了设计新的试验，研究人员从当前和已完成的试验中汲取灵感。2022年，每天平均有100多个临床试验提交到ClinicalTrials.gov，每个试验平均有约1500个单词。这几乎不可能及时跟上。为了缓解这个问题，我们使用GPT-3.5创建了一个批量临床试验摘要工具CliniDigest。据我们所知，CliniDigest是第一个能够提供实时、真实和全面的临床试验摘要的工具。CliniDigest可以将多达85个临床试验描述（约10500个单词）缩减为一个简洁的200个词的摘要，带有参考文献和有限的虚构内容。我们已经测试了CliniDigest在27个医学子领域中涉及的457个试验的摘要能力。对于每个领域，CliniDigest生成的摘要平均为153个单词，标准差为69个单词，其中每个摘要平均使用54个单词。

    A clinical trial is a study that evaluates new biomedical interventions. To design new trials, researchers draw inspiration from those current and completed. In 2022, there were on average more than 100 clinical trials submitted to ClinicalTrials.gov every day, with each trial having a mean of approximately 1500 words [1]. This makes it nearly impossible to keep up to date. To mitigate this issue, we have created a batch clinical trial summarizer called CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first tool able to provide real-time, truthful, and comprehensive summaries of clinical trials. CliniDigest can reduce up to 85 clinical trial descriptions (approximately 10,500 words) into a concise 200-word summary with references and limited hallucinations. We have tested CliniDigest on its ability to summarize 457 trials divided across 27 medical subdomains. For each field, CliniDigest generates summaries of $\mu=153,\ \sigma=69 $ words, each of which utilizes $\mu=54\
    
[^27]: 词语的留存：使用认知偏差和计算语言学预测决策和同义词吸引力。

    Words That Stick: Predicting Decision Making and Synonym Engagement Using Cognitive Biases and Computational Linguistics. (arXiv:2307.14511v1 [cs.HC])

    [http://arxiv.org/abs/2307.14511](http://arxiv.org/abs/2307.14511)

    这项研究使用认知偏差和计算语言学预测用户参与度和决策过程，发现准确代表核心思想、易于理解、能够引发情感反应并且常见的同义词能够促进更高的用户参与度。

    

    这项研究借鉴了认知心理学和信息系统研究的成果，旨在预测数字平台上的用户参与和决策过程。通过运用自然语言处理技术和认知偏差研究的见解，我们深入探讨了用户与数字内容中的同义词的交互。我们的方法将代表性、易用性、情感和分布这四种认知偏差融合到了READ模型中。通过进行综合性的用户调查，我们评估了该模型预测用户参与的能力，发现准确代表核心思想、易于理解、能够引发情感反应并且常见的同义词能够促进更高的用户参与度。至关重要的是，我们的工作为人机交互、数字行为和决策过程提供了全新的视角。我们的结果突显了认知偏差作为用户参与度的有力指标的潜力，凸显了它们在设计有效的数字平台中的重要性。

    This research draws upon cognitive psychology and information systems studies to anticipate user engagement and decision-making on digital platforms. By employing natural language processing (NLP) techniques and insights from cognitive bias research, we delve into user interactions with synonyms within digital content. Our methodology synthesizes four cognitive biasesRepresentativeness, Ease-of-use, Affect, and Distributioninto the READ model. Through a comprehensive user survey, we assess the model's ability to predict user engagement, discovering that synonyms that accurately represent core ideas, are easy to understand, elicit emotional responses, and are commonly encountered, promote greater user engagement. Crucially, our work offers a fresh lens on human-computer interaction, digital behaviors, and decision-making processes. Our results highlight the promise of cognitive biases as potent indicators of user engagement, underscoring their significance in designing effective digital
    
[^28]: 一个数字信息参与的预测模型：通过融合认知偏差、计算语言学和自然语言处理预测用户对英文字的参与度

    A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing. (arXiv:2307.14500v1 [cs.HC])

    [http://arxiv.org/abs/2307.14500](http://arxiv.org/abs/2307.14500)

    本研究提出了一个预测数字信息参与的新模型READ，通过融合认知偏差、计算语言学和自然语言处理，成功预测了英文字的参与水平，并区分了更具吸引力的词汇。

    

    本研究介绍并经过实证测试了一种新颖的数字信息参与（IE）的预测模型——READ模型，其代表了参与度高的信息的四个关键属性：代表性、易用性、情感和分布。在累积前景理论的理论框架下，该模型将关键的认知偏差与计算语言学和自然语言处理相结合，以发展出对信息参与的多维度视角。研究采用一个严格的测试协议，选取了WordNet数据库中的50对随机同义词（共100个词），通过大规模的在线调查（n = 80,500）评估了这些词的参与水平，得出了实证IE指标。然后对每个词的READ属性进行计算，并检查其预测效果。研究结果证实了READ模型的鲁棒性，准确预测了词的IE水平，并区分了更具吸引力的词汇。

    This study introduces and empirically tests a novel predictive model for digital information engagement (IE) - the READ model, an acronym for the four pivotal attributes of engaging information: Representativeness, Ease-of-use, Affect, and Distribution. Conceptualized within the theoretical framework of Cumulative Prospect Theory, the model integrates key cognitive biases with computational linguistics and natural language processing to develop a multidimensional perspective on information engagement. A rigorous testing protocol was implemented, involving 50 randomly selected pairs of synonymous words (100 words in total) from the WordNet database. These words' engagement levels were evaluated through a large-scale online survey (n = 80,500) to derive empirical IE metrics. The READ attributes for each word were then computed and their predictive efficacy examined. The findings affirm the READ model's robustness, accurately predicting a word's IE level and distinguishing the more engagi
    
[^29]: 通过少样本响应生成和排序实现对话系统中对话行为的可控生成

    Controllable Generation of Dialogue Acts for Dialogue Systems via Few-Shot Response Generation and Ranking. (arXiv:2307.14440v1 [cs.CL])

    [http://arxiv.org/abs/2307.14440](http://arxiv.org/abs/2307.14440)

    这项研究开发了一种少样本过生成和排序方法，实现了对话系统中对话行为的可控生成。通过使用预训练语言模型和自动排名函数，可以产生具有高语义准确性的多种对话行为的响应。

    

    对话系统需要产生具有高语义保真度的多种对话行为（DA）的响应。过去，对话的自然语言生成器（NLG）是在大型平行语料库上进行训练的，该语料库将特定领域的DA及其语义属性映射到输出话语。最近的研究表明，预训练语言模型（LLMs）通过基于提示的学习提供了可控制的NLG的新可能性。在这里，我们开发了一种新颖的少样本过生成和排序方法，实现了DA的可控生成。我们比较了八种少样本提示样式，其中包括使用文本风格转换方法从文本伪参考生成的新方法。我们开发了六种自动排名函数，可以在生成时识别出既具有正确DA又具有较高语义准确性的输出。我们在三个领域和四个LLMs上测试了我们的方法。据我们所知，这是第一个通过机器自动排名输出的对话系统NLG的工作。

    Dialogue systems need to produce responses that realize multiple types of dialogue acts (DAs) with high semantic fidelity. In the past, natural language generators (NLGs) for dialogue were trained on large parallel corpora that map from a domain-specific DA and its semantic attributes to an output utterance. Recent work shows that pretrained language models (LLMs) offer new possibilities for controllable NLG using prompt-based learning. Here we develop a novel few-shot overgenerate-and-rank approach that achieves the controlled generation of DAs. We compare eight few-shot prompt styles that include a novel method of generating from textual pseudo-references using a textual style transfer approach. We develop six automatic ranking functions that identify outputs with both the correct DA and high semantic accuracy at generation time. We test our approach on three domains and four LLMs. To our knowledge, this is the first work on NLG for dialogue that automatically ranks outputs using bot
    
[^30]: Skill-it! 一个数据驱动的技能框架，用于理解和训练语言模型

    Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models. (arXiv:2307.14430v1 [cs.CL])

    [http://arxiv.org/abs/2307.14430](http://arxiv.org/abs/2307.14430)

    本论文提出了一个数据驱动的技能框架，用于理解和训练语言模型。通过研究人类获得技能的有序性，我们证明了语言模型学习技能时也有一定的顺序，并且这种顺序可以改善对语言模型的理解和数据高效训练。

    

    训练数据的质量对于预训练的大型语言模型的性能有重要影响。在固定的token预算下，我们研究如何选择能够在各个任务中获得良好下游模型性能的数据。我们提出了一个新的框架，基于一个简单的假设：人类在有意义的顺序中获得相互依赖的技能，语言模型在学习一组技能时也会遵循这样的顺序。如果存在这样的顺序，可以用于改进对语言模型的理解和数据高效训练。利用这种直觉，我们的框架将技能的概念和有序的技能集合的概念形式化为相关数据。首先，使用合成数据和真实数据，我们证明了这些有序的技能集合的存在，并且它们的存在使得在训练其先决条件技能时可以使用更少的数据来学习更高级的技能。其次，使用我们提出的框架，我们介绍了一种在线数据采样算法。

    The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm
    
[^31]: 基于扩散的学习方法用于解码想象言语的脑电图

    Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG. (arXiv:2307.14389v1 [eess.AS])

    [http://arxiv.org/abs/2307.14389](http://arxiv.org/abs/2307.14389)

    本研究提出了一种使用去噪的扩散概率模型和条件自编码器进行想象言语的脑电图解码的新颖方法。结果表明，与传统机器学习技术和基准模型相比，该方法显著提高了解码准确性，这对于发展通过想象言语进行交流的脑-机接口具有潜在的影响。

    

    解码想象言语的脑电图是一项具有挑战性的任务，由于数据的高维特性和低信噪比。近年来，去噪扩散概率模型（DDPMs）已经成为各个领域中具有前景的表示学习方法。我们的研究提出了一种新颖的方法，使用DDPMs和条件自编码器Diff-E对想象言语的脑电图进行解码。结果表明，与传统机器学习技术和基准模型相比，Diff-E显著提高了对想象言语脑电图的解码准确性。我们的发现表明，DDPMs可以是一种有效的脑电图解码工具，对于通过想象言语进行交流的脑-机接口的发展具有潜在的影响。

    Decoding EEG signals for imagined speech is a challenging task due to the high-dimensional nature of the data and low signal-to-noise ratio. In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as promising approaches for representation learning in various domains. Our study proposes a novel method for decoding EEG signals for imagined speech using DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E significantly improves the accuracy of decoding EEG signals for imagined speech compared to traditional machine learning techniques and baseline models. Our findings suggest that DDPMs can be an effective tool for EEG signal decoding, with potential implications for the development of brain-computer interfaces that enable communication through imagined speech.
    
[^32]: 利用大型语言模型通过在线文本数据预测心理健康

    Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v1 [cs.HC])

    [http://arxiv.org/abs/2307.14385](http://arxiv.org/abs/2307.14385)

    本研究首次对多种大型语言模型在心理健康预测任务上进行了全面评估，结果表明指令微调可以显著提升模型性能，并且最优微调模型在平衡准确度上胜过GPT-3.5，并与最先进的任务特定模型持平。

    

    最近大型语言模型（LLM）的技术提升使得多种应用成为可能。然而，对于LLM在心理健康领域的理解和改进研究几乎没有。在这项工作中，我们首次全面评估了多种LLM（包括Alpaca，Alpaca-LoRA和GPT-3.5）在通过在线文本数据进行多个心理健康预测任务上的表现。我们进行了广泛的实验，包括零-shot提示、少-shot提示和指令微调。结果表明，LLM在零-shot和少-shot提示设计上在心理健康任务上表现出有限但有前景的性能。更重要的是，我们的实验结果表明，指令微调可以显著提升LLM在所有任务上的性能。我们最好的微调模型，Mental-Alpaca，在平衡准确度上比GPT-3.5（体积大25倍）高出16.7\%，并与最先进的任务特定模型持平。我们总结我们的发现。

    The recent technology boost of large language models (LLMs) has empowered a variety of applications. However, there is very little research on understanding and improving LLMs' capability for the mental health domain. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, and GPT-3.5, on various mental health prediction tasks via online text data. We conduct a wide range of experiments, covering zero-shot prompting, few-shot prompting, and instruction finetuning. The results indicate the promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned model, Mental-Alpaca, outperforms GPT-3.5 (25 times bigger) by 16.7\% on balanced accuracy and performs on par with the state-of-the-art task-specific model. We summarize our find
    
[^33]: 大型语言模型如何帮助人们在设计和制造中?

    How Can Large Language Models Help Humans in Design and Manufacturing?. (arXiv:2307.14377v1 [cs.CL])

    [http://arxiv.org/abs/2307.14377](http://arxiv.org/abs/2307.14377)

    大型语言模型在设计和制造中的应用潜力以及其限制

    

    大型语言模型（LLMs）的发展，包括GPT-4，为生成设计提供了令人兴奋的新机会。我们研究了在整个设计和制造流程中应用该工具的可行性。具体而言，我们审查了LLMs在诸如将基于文本的提示转化为设计规范、将设计转化为制造指导、生成设计空间和设计变体、计算设计性能以及基于性能搜索设计等任务中的实用性。通过一系列例子，我们突出了当前LLMs的优点和局限性。通过揭示这些局限性，我们希望促进这些模型的持续改进和发展。

    The advancement of Large Language Models (LLMs), including GPT-4, provides exciting new opportunities for generative design. We investigate the application of this tool across the entire design and manufacturing workflow. Specifically, we scrutinize the utility of LLMs in tasks such as: converting a text-based prompt into a design specification, transforming a design into manufacturing instructions, producing a design space and design variations, computing the performance of a design, and searching for designs predicated on performance. Through a series of examples, we highlight both the benefits and the limitations of the current LLMs. By exposing these limitations, we aspire to catalyze the continued improvement and progression of these models.
    
[^34]: Prot2Text: 基于GNNs和Transformers的多模态蛋白质功能生成

    Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.14367](http://arxiv.org/abs/2307.14367)

    提出了一种名为Prot2Text的新方法，通过结合GNNs和Transformers，以自由文本样式预测蛋白质的功能。该方法能够综合蛋白质的序列、结构和文本注释等多种数据类型，超越传统的二进制或分类分类，实现了对蛋白质功能的全面表示。

    

    大型生物系统的复杂性使某些科学家将其理解归类为难以想象的任务。不同级别的挑战使这项任务复杂化，其中之一是预测蛋白质的功能。近年来，通过开发各种机器学习方法，在这个领域取得了重大进展。然而，大多数现有的方法将任务表述为多分类问题，即将预定义标签分配给蛋白质。在这项工作中，我们提出了一种新的方法——Prot2Text，以自由文本样式预测蛋白质的功能，超越传统的二进制或分类分类。通过在编码器-解码器框架中结合图神经网络（GNNs）和大型语言模型（LLMs），我们的模型有效地整合了蛋白质序列、结构和文本注释等多种数据类型。这种多模态方法允许对蛋白质功能进行整体表示。

    The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, en
    
[^35]: 一种基于LSTM、BiLSTM、CNN、GRU和GloVe的混合机器学习模型用于基因突变在癌症中的分类

    A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.14361](http://arxiv.org/abs/2307.14361)

    本研究提出了一个基于多种机器学习算法和嵌入模型的集成模型，用于基因突变在癌症中的分类。实验结果表明，该模型在准确率、精确率、召回率等指标上优于其他传统和最新的转换器模型，并且具有更高的训练效率。

    

    本研究提出了一个集成模型，将LSTM、BiLSTM、CNN、GRU和GloVe结合起来，用于在Kaggle的“个性化医学：重新定义癌症治疗”数据集中对基因突变进行分类。通过与BERT、Electra、Roberta、XLNet、Distilbert以及它们的LSTM集成等知名转换器进行比较，结果显示我们的模型在准确率、精确率、召回率、F1分数和均方误差方面都优于其他模型。令人惊讶的是，它还需要较少的训练时间，实现了性能和效率的完美结合。该研究证明了集成模型在基因突变分类等困难任务中的实用性。

    This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
    
[^36]: 告别RNN-T Loss：一种新的基于CIF的转录器架构用于自动语音识别

    Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition. (arXiv:2307.14132v1 [cs.SD])

    [http://arxiv.org/abs/2307.14132](http://arxiv.org/abs/2307.14132)

    本文提出了一种名为CIF-Transducer的新型模型，将连续积分和火机制与RNN-T模型结合起来，实现了高效的对齐，并放弃了RNN-T Loss，从而减少了计算量，并使预测网络发挥更重要的作用。实验证明CIF-T在自动语音识别中取得了最先进的结果。

    

    RNN-T模型在ASR中广泛使用，依靠RNN-T Loss实现输入音频和目标序列的长度对齐。然而，RNN-T Loss的实现复杂性和基于对齐的优化目标导致计算冗余和预测网络角色的减少。在本文中，我们提出了一种名为CIF-Transducer（CIF-T）的新型模型，它将连续积分和火（CIF）机制与RNN-T模型结合起来，实现高效的对齐。通过这种方式，放弃了RNN-T Loss，从而减少了计算量，并使预测网络发挥更重要的作用。我们还引入了Funnel-CIF、Context Blocks、Unified Gating和Bilinear Pooling联合网络以及辅助训练策略来进一步提高性能。在178小时的AISHELL-1和10000小时的WenetSpeech数据集上的实验证明，与RNN-T模型相比，CIF-T以更低的计算开销实现了最先进的结果。

    RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve length alignment between input audio and target sequence. However, the implementation complexity and the alignment-based optimization target of RNN-T loss lead to computational redundancy and a reduced role for predictor network, respectively. In this paper, we propose a novel model named CIF-Transducer (CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism with the RNN-T model to achieve efficient alignment. In this way, the RNN-T loss is abandoned, thus bringing a computational reduction and allowing the predictor network a more significant role. We also introduce Funnel-CIF, Context Blocks, Unified Gating and Bilinear Pooling joint network, and auxiliary training strategy to further improve performance. Experiments on the 178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves state-of-the-art results with lower computational overhead compared to RNN-T models.
    
[^37]: 评估用于放射学自然语言处理的大型语言模型

    Evaluating Large Language Models for Radiology Natural Language Processing. (arXiv:2307.13693v1 [cs.CL])

    [http://arxiv.org/abs/2307.13693](http://arxiv.org/abs/2307.13693)

    本研究通过对32个大型语言模型进行评估，填补了放射学自然语言处理领域的评估空白。评估结果为这些模型的性能、优势和弱点提供了关键见解，为它们在医学领域的实际应用提供了指导。

    

    大型语言模型（LLMs）的崛起标志着自然语言处理（NLP）领域的重大转变。LLMs已经在许多领域引起了革命性的变化，并在医学领域产生了重要影响。大型语言模型比以往任何时候都更丰富，并且其中许多模型具有双语能力，可以熟练处理英文和中文。然而，对这些模型进行全面评估仍有待开展。在放射学NLP的背景下，尤其明显缺乏这种评估。本研究旨在通过对32个LLMs在解释放射学报告方面进行批判性评估来填补这一空白。具体评估了从影像学发现中得出印象的能力。这个评估的结果为这些LLMs的性能、优势和弱点提供了关键见解，并为它们在医学领域的实际应用提供了指导。

    The rise of large language models (LLMs) has marked a pivotal shift in the field of natural language processing (NLP). LLMs have revolutionized a multitude of domains, and they have made a significant impact in the medical field. Large language models are now more abundant than ever, and many of these models exhibit bilingual capabilities, proficient in both English and Chinese. However, a comprehensive evaluation of these models remains to be conducted. This lack of assessment is especially apparent within the context of radiology NLP. This study seeks to bridge this gap by critically evaluating thirty two LLMs in interpreting radiology reports, a crucial component of radiology NLP. Specifically, the ability to derive impressions from radiologic findings is assessed. The outcomes of this evaluation provide key insights into the performance, strengths, and weaknesses of these LLMs, informing their practical applications within the medical domain.
    
[^38]: 用更长更好的上下文理解将模型赋能

    Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v1 [cs.CL])

    [http://arxiv.org/abs/2307.13365](http://arxiv.org/abs/2307.13365)

    本文研究了大语言模型（LLMs）内的信息传递，并提出了一种名为注意力转移的技术，该技术能够使模型在不增加训练或对生成流畅性的影响的情况下实现更长更好的上下文理解。

    

    最近，随着大量的大语言模型（LLMs）的出现，人工智能的实现进入了一个新的时代。无论这些模型自身的容量和结构如何，都存在对LLMs具有更长更复杂上下文的增强理解的需求，而模型通常在处理超出其理解能力范围的句子序列时会遇到上限，导致产生离题或混乱的回答。虽然最近有几项工作试图以不同的方式解决这个问题，但它们很少关注“为什么模型无法自行弥补或增强自己的能力”。在本文中，我们对LLMs内的信息传递性质进行了深入研究，并提出了一种名为注意力转移的新技术。这种技术能够使模型在最小化额外训练或对生成流利性的影响的情况下实现更长更好的上下文理解。我们的实验证明了这一点。

    Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on "why models are unable to compensate or strengthen their capabilities on their own". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted in XSu
    
[^39]: RRAML: 强化检索增强的机器学习

    RRAML: Reinforced Retrieval Augmented Machine Learning. (arXiv:2307.12798v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12798](http://arxiv.org/abs/2307.12798)

    RRAML是一种新的机器学习框架，将大型语言模型（LLMs）的推理能力与用户提供的庞大数据库中的支持信息相结合。利用强化学习的进展，该方法成功解决了几个关键挑战。

    

    大型语言模型（LLMs）的出现彻底改变了机器学习和相关领域，在理解、生成和操作人类语言方面展示了显著的能力。然而，通过基于API的文本提示提交来使用它们会存在一定的限制，包括上下文约束和外部资源的可用性。为了解决这些挑战，我们提出了一种新的框架，称为强化检索增强的机器学习（RRAML）。RRAML将LLMs的推理能力与由专用检索器从用户提供的庞大数据库中检索到的支持信息相结合。通过利用强化学习的最新进展，我们的方法有效地解决了几个关键挑战。首先，它绕过了访问LLM梯度的需求。其次，我们的方法减轻了针对特定任务重新训练LLMs的负担，因为由于对模型和合作的访问受限，这往往是不可行或不可能的。

    The emergence of large language models (LLMs) has revolutionized machine learning and related fields, showcasing remarkable abilities in comprehending, generating, and manipulating human language. However, their conventional usage through API-based text prompt submissions imposes certain limitations in terms of context constraints and external source availability. To address these challenges, we propose a novel framework called Reinforced Retrieval Augmented Machine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs with supporting information retrieved by a purpose-built retriever from a vast user-provided database. By leveraging recent advancements in reinforcement learning, our method effectively addresses several critical challenges. Firstly, it circumvents the need for accessing LLM gradients. Secondly, our method alleviates the burden of retraining LLMs for specific tasks, as it is often impractical or impossible due to restricted access to the model and the co
    
[^40]: ChatGPT是一个好的人格识别器吗？初步研究。

    Is ChatGPT a Good Personality Recognizer? A Preliminary Study. (arXiv:2307.03952v1 [cs.CL])

    [http://arxiv.org/abs/2307.03952](http://arxiv.org/abs/2307.03952)

    这篇论文进行了初步评估，探索了ChatGPT在文本中识别人格的能力，特别是通过面向层级的提示策略。论文比较了ChatGPT和传统神经网络、RoBERTa在两个实际数据集上的表现。

    

    近年来，人格被视为一种有价值的个人因素，被纳入了许多任务，如情感分析和产品推荐。这导致了对基于文本的人格识别任务的广泛关注，该任务旨在根据给定的文本识别个体的人格。考虑到ChatGPT近期在各种自然语言处理任务中展现出的显著能力，我们对ChatGPT在基于文本的人格识别任务上的表现进行了初步评估，以生成有效的人格数据。具体而言，我们采用了各种提示策略，探索ChatGPT从给定的文本中识别人格的能力，尤其是我们设计的面向层级的提示策略，用于指导ChatGPT在指定层次分析给定的文本。我们将ChatGPT在两个代表性的实际数据集上与传统的神经网络、微调的RoBERTa以及相应的最新任务的性能进行了比较。

    In recent years, personality has been regarded as a valuable personal factor being incorporated into numerous tasks such as sentiment analysis and product recommendation. This has led to widespread attention to text-based personality recognition task, which aims to identify an individual's personality based on given text. Considering that ChatGPT has recently exhibited remarkable abilities on various natural language processing tasks, we provide a preliminary evaluation of ChatGPT on text-based personality recognition task for generating effective personality data. Concretely, we employ a variety of prompting strategies to explore ChatGPT's ability in recognizing personality from given text, especially the level-oriented prompting strategy we designed for guiding ChatGPT in analyzing given text at a specified level. We compare the performance of ChatGPT on two representative real-world datasets with traditional neural network, fine-tuned RoBERTa, and corresponding state-of-the-art task
    
[^41]: Fraunhofer SIT在CheckThat! 2023中使用模型混合技术解决分类不确定性的研究：以可检查性分类为例

    Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification. (arXiv:2307.02377v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.02377](http://arxiv.org/abs/2307.02377)

    本研究介绍了Fraunhofer SIT团队在CLEF-2023 CheckThat!英语实验室任务1B中使用模型混合技术解决分类不确定性的方法，该方法的目的是确定政治辩论中的文本片段是否值得进行事实检查评估，并在比赛中排名第二。

    

    本文描述了Fraunhofer SIT团队在CLEF-2023 CheckThat!英语实验室任务1B中获得第二名的方法。给定一段政治辩论的文本片段，该任务的目标是确定是否应该对其进行检查价值评估。检测可检查声明旨在通过优先考虑事实检查人员应首先考虑的声明来简化手动事实检查工作。它还可以被视为事实检查系统的主要步骤。我们的最佳方法利用了以模型混合为中心的集成分类方案。在应用于英语数据集时，我们提交的模型在比赛中获得了0.878的整体F1得分，并被评为第二好的模型。

    This paper describes the second-placed approach developed by the Fraunhofer SIT team in the CLEF-2023 CheckThat! lab Task 1B for English. Given a text snippet from a political debate, the aim of this task is to determine whether it should be assessed for check-worthiness. Detecting check-worthy statements aims to facilitate manual fact-checking efforts by prioritizing the claims that fact-checkers should consider first. It can also be considered as primary step of a fact-checking system. Our best-performing method took advantage of an ensemble classification scheme centered on Model Souping. When applied to the English data set, our submitted model achieved an overall F1 score of 0.878 and was ranked as the second-best model in the competition.
    
[^42]: 从不完整话语中挖掘线索：一种增强查询的网络用于不完整话语重写

    Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting. (arXiv:2307.00866v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.00866](http://arxiv.org/abs/2307.00866)

    这篇论文提出了一种增强查询的网络用于不完整话语重写，通过引入查询模板来明确语义结构知识，并采用有效的编辑操作评分网络来建模两个标记之间的关系。在多个公共数据集上，该模型取得了最先进的性能。

    

    最近，不完整话语重写引起了广泛关注。然而，先前的工作没有考虑不完整话语和重写话语之间的语义结构信息，也没有隐式和不充分地建模语义结构。为解决这个问题，我们提出了一种增强查询网络（QUEEN）。首先，我们提出的查询模板明确地引入了不完整话语和重写话语之间的指导性语义结构知识，使模型能够理解在哪里进行参考或恢复被省略的标记。然后，我们采用一种快速有效的编辑操作评分网络来建模两个标记之间的关系。由于提出的查询模板和精心设计的编辑操作评分网络的好处，QUEEN在多个公共数据集上实现了最先进的性能。

    Incomplete utterance rewriting has recently raised wide attention. However, previous works do not consider the semantic structural information between incomplete utterance and rewritten utterance or model the semantic structure implicitly and insufficiently. To address this problem, we propose a QUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitly brings guided semantic structural knowledge between the incomplete utterance and the rewritten utterance making model perceive where to refer back to or recover omitted tokens. Then, we adopt a fast and effective edit operation scoring network to model the relation between two tokens. Benefiting from proposed query template and the well-designed edit operation scoring network, QUEEN achieves state-of-the-art performance on several public datasets.
    
[^43]: Fraunhofer SIT在CheckThat! 2023中的贡献：混合单模分类器以估计多模态推文的可靠性

    Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets. (arXiv:2307.00610v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00610](http://arxiv.org/abs/2307.00610)

    本文提出了一种混合单模分类器的方法，通过组合图像和文本分类器的结果，成功进行多模态推文的可靠性估计，并在CheckThat! 2023任务1A中取得了最佳表现。

    

    在社交媒体上分享图像、视频和音频文件的选项为区分网络上的虚假信息和假新闻提供了新的可能性。由于社交媒体每秒分享的海量数据，无法通过计算机或人类专家对所有数据进行验证。因此，可通过可靠性分析作为事实核查流程的第一步，以及作为提高效率的过滤机制。本文提出了一种新颖的方法来检测多模态推文的可靠性。它利用了两个在单模态上训练的分类器。对于图像数据，通过OCR分析提取嵌入的文本表现最佳。通过组合这两个分类器，该方法在CheckThat! 2023任务1A中达到了0.7297的F1分数，在私人测试集上排名第一。

    The option of sharing images, videos and audio files on social media opens up new possibilities for distinguishing between false information and fake news on the Internet. Due to the vast amount of data shared every second on social media, not all data can be verified by a computer or a human expert. Here, a check-worthiness analysis can be used as a first step in the fact-checking pipeline and as a filtering mechanism to improve efficiency. This paper proposes a novel way of detecting the check-worthiness in multi-modal tweets. It takes advantage of two classifiers, each trained on a single modality. For image data, extracting the embedded text with an OCR analysis has shown to perform best. By combining the two classifiers, the proposed solution was able to place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297 achieved on the private test set.
    
[^44]: 向可解释的、语言无关的LLMs迈进：大规模语言符号逆向工程

    Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v1 [cs.CL])

    [http://arxiv.org/abs/2306.00017](http://arxiv.org/abs/2306.00017)

    本文提出结合符号表示和自下而上的逆向工程的方法，解决大规模语言模型在真正语言理解上的局限性，实现可解释的、语言无关的LLMs。

    

    大型语言模型（LLMs）取得了一个里程碑，无可否认地改变了人工智能（AI）中许多信仰。然而，当涉及真正的语言理解时，这些LLM的许多限制仍然存在，这些限制是深度神经网络底层架构的副产品。此外，由于它们的亚符号性质，这些模型获得有关语言如何运作的任何知识都将被埋在数十亿个微特征（权重）中，其中没有一个单独的特征有意义，使得这些模型无法解释。为了解决这些限制，我们建议将符号表示的强度与我们认为是LLMs成功的关键结合起来，即在规模上成功地进行自下而上的语言逆向工程。因此，我们主张在符号设置下对语言进行自下而上的逆向工程。一些作者提出了这个项目的提示，我们将进行详细讨论。

    Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the under-lying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will always be buried in billions of microfeatures (weights), none of which is meaningful on its own, making such models hopelessly unexplainable. To address these limitations, we suggest com-bining the strength of symbolic representations with what we believe to be the key to the success of LLMs, namely a successful bottom-up re-verse engineering of language at scale. As such we argue for a bottom-up reverse engineering of language in a symbolic setting. Hints on what this project amounts to have been suggested by several authors, and we discuss in some detail
    
[^45]: PlaSma: 为 (反事实) 计划制定增强过程知识模型的小型语言模型

    PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])

    [http://arxiv.org/abs/2305.19472](http://arxiv.org/abs/2305.19472)

    PlaSma提出了一种使用小型语言模型进行过程知识和计划能力的新方法，

    

    过程规划是机器的一项重要而又复杂的任务，它将一个高级目标分解为一系列时间顺序的步骤。它需要整合常识知识以推理出常常是反事实的复杂情境，例如 "没有电话时安排医生的约会"。当前的方法使用大型语言模型 (LLM) 取得了令人鼓舞的结果，但受到昂贵的 API 调用和可复现性问题的限制。本文提出使用更小的语言模型来进行规划，我们介绍了 PlaSma，这是一种新的双重方法，使小型语言模型具有过程知识和 (反事实) 计划能力。更具体地说，我们开发了符号过程知识蒸馏来增强小型语言模型中的隐含知识，以及一种推理算法来促进更结构化和准确的推理。此外，我们还引入了一个新的任务，反事实规划。

    Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
    
[^46]: 识别情感体验者作为情感分析的先决条件

    Emotion Experiencer Recognition as a Prerequisite for Experiencer-Specific Emotion Analysis. (arXiv:2305.16731v1 [cs.CL])

    [http://arxiv.org/abs/2305.16731](http://arxiv.org/abs/2305.16731)

    本文提出了一种用于检测情感体验者并为其分配情感的自动方法，并进行了相关的实验。该方法的实现具有挑战性，但展示了在文本中检测情感体验者的可行性。

    

    情感角色标注旨在提取文本中描述谁经历情感、为什么以及对谁的信息。这通常是一个具有挑战性的建模任务，如果要回答的主要问题是谁感受到了哪种情感，这可能会过于复杂。本文填补了这一空白，通过自动检测文本中的情感体验者并随后为其分配情感，展示了在文本中检测情感体验者是一项具有挑战性的任务，并呈现了相关的实验结果。

    Emotion role labeling aims at extracting who is described in text to experience an emotion, why, and towards whom. This is often a challenging modelling task which might be overly sophisticated if the main question to answer is who feels which emotion. Recently, Troiano et al. (2022) proposed a data set that focuses on assigning emotion labels and appraisal labels to individual entities in text and Wegge et al. (2022) presented the first modelling experiments. Their experiencer-specific emotion prediction model has, however, only been evaluated on gold-annotated experiencers, due to the unavailability of an automatic experiencer detection approach. We fill this gap with the first experiments to automatically detect emotion experiencers in text and, subsequently, assign them emotions. We show that experiencer detection in text is a challenging task, with a precision of .82 and a recall of .56 (F1 =.66). Consequently, the performance of the experiencer-specific emotion detection pipeline
    
[^47]: 基于情境感知注意力层及最优传输域自适应方法识别自发语音中的痴呆症

    Context-Aware Attention Layers coupled with Optimal Transport Domain Adaptation methods for recognizing dementia from spontaneous speech. (arXiv:2305.16406v1 [cs.CL])

    [http://arxiv.org/abs/2305.16406](http://arxiv.org/abs/2305.16406)

    本论文提出一种基于情境感知注意力层及最优传输域自适应方法的语音识别痴呆症的新方法。该方法捕捉了模态内部和模态间的交互，并实现了模型校准。

    

    阿尔茨海默病是一种复杂的神经认知病变，也是导致痴呆症最常见的原因。虽然已经有很多针对通过自发语音诊断痴呆症的研究，但仍然存在限制。现有的先进方法提出了多模态方法，分别训练语言和声学模型，并采用多数投票方法，以及将不同模态的表示在输入层进行级联或在训练过程中进行级联。与此同时，一些方法采用了自我注意力层，计算表示之间的依赖关系，但没有考虑到上下文信息。此外，以前的工作都没有考虑到模型的校准。为了解决这些限制，我们提出了一些新方法来检测AD患者，捕捉了模态内部和模态间的交互。首先，我们将音频文件转换为log-Mel光谱图，它们的delta和delta-delta，并在此基础上创建了一个多通道神经网络。

    Alzheimer's disease (AD) constitutes a complex neurocognitive disease and is the main cause of dementia. Although many studies have been proposed targeting at diagnosing dementia through spontaneous speech, there are still limitations. Existing state-of-the-art approaches, which propose multimodal methods, train separately language and acoustic models, employ majority-vote approaches, and concatenate the representations of the different modalities either at the input level, i.e., early fusion, or during training. Also, some of them employ self-attention layers, which calculate the dependencies between representations without considering the contextual information. In addition, no prior work has taken into consideration the model calibration. To address these limitations, we propose some new methods for detecting AD patients, which capture the intraand cross-modal interactions. First, we convert the audio files into log-Mel spectrograms, their delta, and delta-delta and create in this
    
[^48]: 在ChatGPT、大型语言模型和生成AI时代的科学：研究伦理的挑战及应对方法

    Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond. (arXiv:2305.15299v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2305.15299](http://arxiv.org/abs/2305.15299)

    这篇论文回顾了生成AI对科学研究所带来的认识论挑战、伦理和诚信风险，并提出了十项建议，以在AI时代促进更负责任的研究进行。

    

    人工智能的大型语言模型（如ChatGPT）在科学研究中具有显著但有争议的应用。本文回顾了生成AI时代科学研究的认识论挑战、伦理和诚信风险，并旨在为高质量的研究伦理审查奠定新的及时基础。对AI语言模型作为研究工具和研究对象的角色进行了详细审查，并讨论了对科学家、参与者和评审人员的伦理影响。讨论了研究伦理审查的新兴实践，并给出了十项建议，为在AI时代更负责任的研究进行回应。

    Large language models of artificial intelligence (AI), such as ChatGPT, find remarkable but controversial applicability in science and research. This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI. This is with the aim to lay new timely foundations for a high-quality research ethics review. The role of AI language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. New emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of AI.
    
[^49]: 基于置信度的部分标签学习模型用于群体注释的命名实体识别

    A Confidence-based Partial Label Learning Model for Crowd-Annotated Named Entity Recognition. (arXiv:2305.12485v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12485](http://arxiv.org/abs/2305.12485)

    本论文提出了一种基于置信度的部分标签学习方法（CPLL）用于群体注释的命名实体识别。该模型通过迭代更新真实后验和置信度，通过最小化经验风险学习一个基于标记和内容的置信度，实验结果表明该方法能够提高NER的性能。

    

    现有的命名实体识别（NER）模型主要基于大规模标记数据集，这些数据集通常是通过众包获得的。然而，由于标注空间的广泛性和任务的复杂性，很难通过多个标注者的多数表决获得统一且正确的标签。为了解决这个问题，我们旨在直接利用原始的多标注者标签。具体而言，我们提出了一种基于置信度的部分标签学习（CPLL）方法，用于集成注释者提供的先验置信度和模型学习的后验置信度，用于群体注释的NER。该模型通过最小化经验风险，通过期望最大化（EM）算法学习一个基于标记和内容的置信度。真实后验估计器和置信度估计器通过迭代更新真实后验和置信度。我们在真实和合成数据集上进行了大量实验，结果表明我们的模型可以提高命名实体识别的性能。

    Existing models for named entity recognition (NER) are mainly based on large-scale labeled datasets, which always obtain using crowdsourcing. However, it is hard to obtain a unified and correct label via majority voting from multiple annotators for NER due to the large labeling space and complexity of this task. To address this problem, we aim to utilize the original multi-annotator labels directly. Particularly, we propose a Confidence-based Partial Label Learning (CPLL) method to integrate the prior confidence (given by annotators) and posterior confidences (learned by models) for crowd-annotated NER. This model learns a token- and content-dependent confidence via an Expectation-Maximization (EM) algorithm by minimizing empirical risk. The true posterior estimator and confidence estimator perform iteratively to update the true posterior and confidence respectively. We conduct extensive experimental results on both real-world and synthetic datasets, which show that our model can impro
    
[^50]: 上下文检索增强的语言模型

    In-Context Retrieval-Augmented Language Models. (arXiv:2302.00083v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.00083](http://arxiv.org/abs/2302.00083)

    本研究提出了一种上下文检索增强的语言模型（In-Context RALM）方法，通过将相关文件作为输入的一部分，无需对语言模型进行进一步的训练即可显著提高语言建模性能和源归因能力，并且相对于现有的RALM方法，它具有更简单的部署过程。

    

    检索增强的语言模型(RALM)方法在生成过程中，通过将相关文件从语料库中检索出来与语言模型(LM)进行协同，已被证明可以显著提高语言建模性能。此外，它们还可以缓解事实不准确的文本生成问题，并提供自然的源归因机制。现有的RALM方法着重于修改LM架构以便于整合外部信息，从而大大增加了部署的复杂性。本文提出了一种简单的替代方法，称为上下文RALM：保持LM架构不变，并在输入中添加检索到的文件，无需对LM进行任何进一步的训练。我们展示了基于现成的通用检索器的上下文RALM在模型大小和不同语料库中能够提供出人意料的大幅度的LM增益。我们还证明，文件检索和排名机制可以针对RALM设置进行专门优化。

    Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to 
    
[^51]: ThoughtSource:一个用于大型语言模型推理数据的中央枢纽。

    ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11596](http://arxiv.org/abs/2301.11596)

    ThoughtSource是一个用于连续思考推理的元数据集和软件库，旨在通过促进对连续思考的定性理解、实证评估和提供训练数据，改进未来的人工智能系统。

    

    最近，像GPT-4这样的大型语言模型在多个任务上展示了令人印象深刻的结果。然而，这些语言模型在复杂推理上仍存在限制，它们的推理过程不透明，容易产生“幻觉”事实，并且存在其潜在偏见的担忧。最近提出了一种称为连续思考提示的技术，让模型以自然语言形式表达推理步骤，以解决这些问题。在这里，我们介绍了ThoughtSource，一个用于连续思考推理的元数据集和软件库。ThoughtSource的目标是通过促进对连续思考的定性理解、实证评估和提供训练数据来改进未来的人工智能系统。ThoughtSource的首次发布集成了六个科学/医学、三个通用领域和五个数学题答案数据集。

    Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates six scientific/medical, three general-domain and five math word question answering datasets.
    
[^52]: 大型语言模型在学习长尾知识方面存在困难

    Large Language Models Struggle to Learn Long-Tail Knowledge. (arXiv:2211.08411v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08411](http://arxiv.org/abs/2211.08411)

    本文研究了大型语言模型记忆的知识与预训练数据集中信息之间的关系，并发现其回答基于事实的问题的能力与在预训练过程中接触到的相关文档数量之间存在强相关性和因果关系。

    

    互联网中包含着丰富的知识，从历史人物的生日到编程教程等，所有这些都可以由语言模型学习。然而，尽管某些信息在网上无处不在，但其他信息的出现非常罕见。在本文中，我们研究了大型语言模型记忆的知识与从网络抓取的预训练数据集中的信息之间的关系。特别是，我们展示了语言模型回答基于事实的问题的能力与在预训练过程中看到与此问题相关的文档数量之间的关系。我们通过实体链接预训练数据集并计算包含与给定问题-答案对相同实体的文档数量来识别这些相关文档。我们的结果表明，在众多问答数据集（例如 TriviaQA）、预训练语料库（例如 ROOTS）和模型上，准确性与相关文档数量之间存在着强相关性和因果关系。

    The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model si
    
[^53]: 将知识从记忆中解耦：检索增强的提示学习

    Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.14704](http://arxiv.org/abs/2205.14704)

    本论文提出了一种检索增强的提示学习方法，通过将知识从记忆中解耦，帮助模型在泛化和记忆之间取得平衡。

    

    提示学习方法在自然语言处理领域取得了显著的突破，提高了少样本学习的性能，但仍然遵循参数化学习范式；在学习过程中，遗忘和机械记忆问题可能导致不稳定的泛化问题。为了缓解这些限制，我们开发了RetroPrompt，旨在从记忆中将知识解耦，帮助模型在泛化和记忆之间取得平衡。与传统的提示学习方法相比，RetroPrompt从训练实例构建了一个开放式知识库，并在输入、训练和推断过程中实施检索机制，使模型具备了从训练语料库中检索相关上下文用于增强的能力。大量实验证明了RetroPrompt的效果。

    Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstra
    
[^54]: 可微分的Transformer头部子集修剪

    Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2108.04657](http://arxiv.org/abs/2108.04657)

    这种可微分的Transformer头部子集修剪方法可以安全地修剪掉大部分头部，使得模型更小更快，而且在稀疏程度上具有可比或更好的性能。

    

    多头注意力是Transformer中的重要组成部分，它由多个独立关注输入不同部分的注意力机制组成。然而，最近的研究表明，在不显著影响模型性能的情况下，可以安全地修剪掉Transformer多头注意力机制中的大部分头部，从而使模型在实践中更小更快。本文提出了一种新的头部修剪技术，称为可微分子集修剪。方法的核心是学习每个头部的重要性变量，并对未修剪头部的数量施加用户指定的硬约束。重要性变量通过随机梯度下降学习得到。我们在自然语言推理和机器翻译上进行实验，结果显示可微分子集修剪在稀疏程度上具有可比或更好的性能，并能提供精确的控制。

    Multi-head attention, a collection of several attention mechanisms that independently attend to different parts of the input, is the key ingredient in the Transformer. Recent work has shown, however, that a large proportion of the heads in a Transformer's multi-head attention mechanism can be safely pruned away without significantly harming the performance of the model; such pruning leads to models that are noticeably smaller and faster in practice. Our work introduces a new head pruning technique that we term differentiable subset pruning. Intuitively, our method learns per-head importance variables and then enforces a user-specified hard constraint on the number of unpruned heads. The importance variables are learned via stochastic gradient descent. We conduct experiments on natural language inference and machine translation; we show that differentiable subset pruning performs comparably or better than previous works while offering precise control of the sparsity level.
    

