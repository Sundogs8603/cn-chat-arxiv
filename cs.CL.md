# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction](https://arxiv.org/abs/2403.11886) | QueryAgent提出了一种基于环境反馈的自我校正方法ERASER，在语义解析中表现出显著的性能提升和高效性 |
| [^2] | [KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models](https://arxiv.org/abs/2403.07350) | KEBench提出了一个新的基准测试，采用不同的数据收集方法和新增加的度量标准（可移植性），以全面评估大型视觉-语言模型知识编辑的质量。 |
| [^3] | [Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data](https://arxiv.org/abs/2403.03304) | Mad Libs 提供的数据增强方法 MLA 在跨领域文档级事件参数数据提取上取得了显著的性能提升，平均提高了 2.6 个 F1 分数。同时，在零和少样本事件角色方面相比无增强基线，提高了 3.9 和 5.2 个百分点。 |
| [^4] | [Speech emotion recognition from voice messages recorded in the wild](https://arxiv.org/abs/2403.02167) | 使用Emotional Voice Messages数据库，结合eGeMAPS特征和Transformer模型，实现了在野外录制的语音消息中的语音情感识别，取得了较高的准确度，并比基准模型提高了10%。 |
| [^5] | [To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering](https://arxiv.org/abs/2403.01924) | 本文介绍了MedGENIE，这是医学领域多项选择问题回答的第一个生成后阅读框架。 |
| [^6] | [CLLMs: Consistency Large Language Models](https://arxiv.org/abs/2403.00835) | 提出了一种新方法，通过精细调整目标LLM实现了对雅各比轨迹上固定点的一致性预测，有效提高了生成速度2.4倍到3.4倍。 |
| [^7] | [Teaching Large Language Models an Unseen Language on the Fly](https://arxiv.org/abs/2402.19167) | 通过提示，在飞行中教授大型语言模型一种未知语言，提出DiPMT ++框架，通过上下文学习使LLMs适应看不见的语言，并实现了壮语和汉语之间的翻译性能显著提升，并展示了该框架在帮助人类翻译完全未知语言方面的实用性。 |
| [^8] | [NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents](https://arxiv.org/abs/2402.17682) | 提出了NextLevelBERT，通过在更高级别的语义表示上进行遮蔽语言建模，有效处理长文档用例，具有超越更大嵌入模型的性能 |
| [^9] | [Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages](https://arxiv.org/abs/2402.17496) | 该研究介绍了Emotional Voice Messages (EMOVOME)数据库，其中包含来自100名西班牙说话者的999条自发语音消息，通过专家和非专家的标记实现了在valence和arousal维度上的情感识别，并尝试使用语音和文本转录实现情感识别模型。 |
| [^10] | [Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling](https://arxiv.org/abs/2402.17019) | 通过大型语言模型和故事讲述，本论文提出了一种新颖的法律教育方法，帮助非专业人士学习复杂的法律概念，并构建了一个包含法律故事和多项选择题的数据集。 |
| [^11] | [EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings](https://arxiv.org/abs/2402.16040) | 该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs），具有采用多项选择问题回答格式和需要分析多篇临床笔记的特点。 |
| [^12] | [Selective "Selective Prediction": Reducing Unnecessary Abstention in Vision-Language Reasoning](https://arxiv.org/abs/2402.15610) | 引入了一种名为ReCoVERR的算法，能够在视觉-语言系统的推理过程中减少过度放弃，通过寻找图像中的相关线索提供额外证据来取代放弃，从而不降低预测准确性。 |
| [^13] | [FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models](https://arxiv.org/abs/2402.10986) | FinTral是一类基于Mistral-7b模型的GPT-4级别多模态金融大型语言模型，通过领域特定的预训练和检索方法优化，在AI驱动金融技术中取得显著进展。 |
| [^14] | [CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion](https://arxiv.org/abs/2402.05889) | 该论文提出了一种名为CREMA的高效且模块化的模态融合框架，用于将任意新的模态注入视频推理。通过利用预训练模型增强多种信息模态，并引入查询转换器和融合模块，实现了灵活且有效的多模态组合推理。 |
| [^15] | [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://arxiv.org/abs/2402.04333) | LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。 |
| [^16] | [Instruction Makes a Difference](https://arxiv.org/abs/2402.00453) | 通过引入指令数据集，我们展示了在文档分析和文档图像预测中训练语言-视觉模型的重要性，并表明使用指令数据集可以提高性能。使用Polling-based Object Probing Evaluation (POPE)数据集进行评估，我们发现指令调优性能相对于零-shot性能提高了11倍到32倍，并且相对于非指令微调提高了0.1%到4.2%。尽管如此，仍有很大的改进空间，因为这些性能仍未达到人类水平（94.36%）。 |
| [^17] | [Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning](https://arxiv.org/abs/2311.08894) | 提出了一种新的KBQA架构FuSIC-KBQA，通过多个源训练的召回器执行KB检索，在LLM的重新排序后以此作为LLM少样本上下文学习的输入来生成逻辑形式，并利用执行引导反馈进一步优化。 |
| [^18] | [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://arxiv.org/abs/2311.03099) | 本文揭示了语言模型可以通过吸收同源模型的参数来获得新的能力，而无需重新训练或使用GPU。作者提出了DARE技术来稀疏化参数并将多个同源模型合并为一个模型。实验证明，DARE可以轻松删除大部分参数并实现多任务融合。 |
| [^19] | [Exploring semantic information in disease: Simple Data Augmentation Techniques for Chinese Disease Normalization](https://arxiv.org/abs/2306.01931) | 提出了一组定制的数据增强技术，旨在利用疾病名称中的语义信息，增强模型对疾病名称的语义细微差别和分类结构的理解 |
| [^20] | [Unlearning Reveals the Influential Training Data of Language Models.](http://arxiv.org/abs/2401.15241) | 本文提出了一种简单而有效的方法UnTrac，通过反学习训练数据集来估计语言模型的影响。实验结果表明，UnTrac能够准确评估预训练数据集对生成有害内容的影响，并且无需额外的资源。 |
| [^21] | [Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings.](http://arxiv.org/abs/2401.06112) | 本研究提出了一种新的方法，Axis Tour，用于确定ICA转换嵌入中轴的顺序，并通过最大化语义连续性来提高词嵌入空间的清晰度。实验证明，Axis Tour构建的低维嵌入比PCA和ICA更好。 |
| [^22] | [DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection.](http://arxiv.org/abs/2310.16776) | 这项研究介绍了一种名为DEFT的数据高效微调框架，通过无监督核心集选择来最小化微调大规模语言模型所需的数据量。研究结果表明，DEFT模型在准确性上与现有模型相当，并且仅使用了70%的数据量。 |
| [^23] | [Language Models As Semantic Indexers.](http://arxiv.org/abs/2310.07815) | 本文介绍了一种使用生成性语言模型学习语义ID的自监督框架LMINDEXER。 |
| [^24] | [Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning.](http://arxiv.org/abs/2310.03309) | 利用大型语言模型进行演绎推理是一个具有挑战性的问题。这篇论文提出了一个简明有序的方法，将任务分解为子任务并且人类化地组织思维，以提高演绎推理的效果。 |
| [^25] | [Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models.](http://arxiv.org/abs/2310.02409) | Nugget 2D是一种用于仅解码器语言模型的动态上下文压缩方法，可以在保留任务能力的同时大幅减少解码过程所需的时间和空间开销。 |
| [^26] | [Active Learning for Multilingual Fingerspelling Corpora.](http://arxiv.org/abs/2309.12443) | 本文应用主动学习来解决手语数据稀缺问题，并对预训练的效果进行了分析，发现预训练在多语种手指拼写语料库上有益处，可能是由于视觉上的相似性。 |
| [^27] | [HypR: A comprehensive study for ASR hypothesis revising with a reference corpus.](http://arxiv.org/abs/2309.09838) | 本研究集中在发布一个ASR假设修订（HypR）数据集，该数据集包含了几个常用的语料库，并且为ASR模型的修订提供了一个基础。 |
| [^28] | [Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes.](http://arxiv.org/abs/2309.00237) | 使用合成临床记录构建的临床大语言模型可以克服临床记录的有限可及性和可用性的问题，并在现实应用中表现出潜在的良好性能。 |
| [^29] | [Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications.](http://arxiv.org/abs/2306.04539) | 本文研究在只有带标签的单模态数据和自然出现的多模态数据的情况下，如何量化多模态交互的挑战，并提出了两个下界和一个上界来量化多模态交互量。 |
| [^30] | [Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment.](http://arxiv.org/abs/2305.13669) | 本文提出了MixAlign框架，通过与用户和知识库交互，实现自动的问题-知识对齐，从而解决了语言模型因无法正确理解问题和知识而导致的幻觉问题。 |
| [^31] | [Accurate Knowledge Distillation with n-best Reranking.](http://arxiv.org/abs/2305.12057) | 该论文提出了一种基于n-best重排序的知识蒸馏方法，通过使用多种模型提供伪标签，训练出参数更少但精度相当的学生模型。 |

# 详细

[^1]: QueryAgent：一种具有环境反馈的可靠高效推理框架及自我校正

    QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction

    [https://arxiv.org/abs/2403.11886](https://arxiv.org/abs/2403.11886)

    QueryAgent提出了一种基于环境反馈的自我校正方法ERASER，在语义解析中表现出显著的性能提升和高效性

    

    使用大型语言模型（LLMs）进行语义解析取得了显著成功，但在遇到幻觉时现有方法可靠性和效率方面存在不足。本文提出了一个名为QueryAgent的框架，通过逐步解决问题并进行逐步自我校正来解决这些挑战。我们引入了一种基于环境反馈的自我校正方法ERASER。与传统方法不同，ERASER利用中间步骤中的丰富环境反馈，在必要时仅进行选择性和差异化的自我校正。实验结果表明，QueryAgent在GrailQA和GraphQ上仅使用一个例子就比所有先前的少样本方法取得了7.0和15.0的F1值提升。此外，我们的方法在效率方面表现出优势，包括运行时间、查询开销和API调用成本。通过利用ERASER，

    arXiv:2403.11886v1 Announce Type: cross  Abstract: Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we
    
[^2]: KEBench: 用于大型视觉-语言模型知识编辑的基准测试

    KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models

    [https://arxiv.org/abs/2403.07350](https://arxiv.org/abs/2403.07350)

    KEBench提出了一个新的基准测试，采用不同的数据收集方法和新增加的度量标准（可移植性），以全面评估大型视觉-语言模型知识编辑的质量。

    

    arXiv:2403.07350v1 公告类型: 跨领域 摘要: 目前，针对大型视觉-语言模型(LVLMs)的知识编辑研究很少。编辑LVLMs面临着有效整合多种模态（图像和文本）的挑战，同时确保修改连贯且与上下文相关。现有基准测试具有三个度量标准（可靠性、局部性和一般性）用于衡量LVLMs的知识编辑。然而，该基准测试在评估中使用的生成图像质量不足，并且无法评估模型是否有效地利用与相关内容相关的编辑知识。我们采用不同的数据收集方法构建了一个新的基准测试$\textbf{KEBench}$，并扩展了新度量标准(可移植性)以进行全面评估。借助多模态知识图，我们的图像数据呈现出明确的给实体方向性。这种方向性可以进一步用于提取与实体相关的知识和进行编辑。

    arXiv:2403.07350v1 Announce Type: cross  Abstract: Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs. However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content. We adopt different data collection methods to construct a new benchmark, $\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related knowledge and form editing 
    
[^3]: 所需只是 Mad Libs: 增强跨领域文档级事件参数数据

    Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data

    [https://arxiv.org/abs/2403.03304](https://arxiv.org/abs/2403.03304)

    Mad Libs 提供的数据增强方法 MLA 在跨领域文档级事件参数数据提取上取得了显著的性能提升，平均提高了 2.6 个 F1 分数。同时，在零和少样本事件角色方面相比无增强基线，提高了 3.9 和 5.2 个百分点。

    

    文档级事件参数提取（DocEAE）是一个极其困难的信息提取问题，尤其在低资源跨领域设置中存在着显著的局限性。为了解决这个问题，我们引入了 Mad Lib Aug（MLA），这是一个新颖的生成式 DocEAE 数据增强框架。我们的方法利用了 Mad Libs 的直觉，即作为一种热门游戏中使用的分类掩码文档可以被 LLMs 生成并解答，从而为 DocEAE 生成数据。使用 MLA，我们的整体 F1 分数平均改进了 2.6 个百分点。此外，与无增强基线相比，该方法在零和少样本事件角色方面在所有实验中平均提高了 3.9 和 5.2 个百分点。

    arXiv:2403.03304v1 Announce Type: new  Abstract: Document-Level Event Argument Extraction (DocEAE) is an extremely difficult information extraction problem -- with significant limitations in low-resource cross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA), a novel generative DocEAE data augmentation framework. Our approach leverages the intuition that Mad Libs, which are categorically masked documents used as a part of a popular game, can be generated and solved by LLMs to produce data for DocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1 score. Moreover, this approach achieves a 3.9 and 5.2 point average increase in zero and few-shot event roles compared to augmentation-free baselines across all experiments.   To better facilitate analysis of cross-domain DocEAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect t
    
[^4]: 从野外录制的语音消息中识别语音情感

    Speech emotion recognition from voice messages recorded in the wild

    [https://arxiv.org/abs/2403.02167](https://arxiv.org/abs/2403.02167)

    使用Emotional Voice Messages数据库，结合eGeMAPS特征和Transformer模型，实现了在野外录制的语音消息中的语音情感识别，取得了较高的准确度，并比基准模型提高了10%。

    

    用于语音情感识别（SER）的情感数据集通常包含表演或引发的语音，限制了它们在现实场景中的适用性。在这项工作中，我们使用了Emotional Voice Messages（EMOVOME）数据库，其中包括来自100名西班牙语使用者在消息应用中的自发语音消息，由专家和非专家标注者以连续和离散的情感进行标记。我们使用了eGeMAPS特征、基于Transformer的模型以及它们的组合来创建讲话者无关的SER模型。我们将结果与参考数据库进行了比较，并分析了标注者和性别公平性的影响。预训练的Unispeech-L模型及其与eGeMAPS的组合取得了最佳结果，在3类valence和arousal预测中分别获得了61.64%和55.57%的Unweighted Accuracy（UA），比基线模型提高了10%。对于情感类别，获得了42.58%的UA。EMOVOME表现不佳。

    arXiv:2403.02167v1 Announce Type: cross  Abstract: Emotion datasets used for Speech Emotion Recognition (SER) often contain acted or elicited speech, limiting their applicability in real-world scenarios. In this work, we used the Emotional Voice Messages (EMOVOME) database, including spontaneous voice messages from conversations of 100 Spanish speakers on a messaging app, labeled in continuous and discrete emotions by expert and non-expert annotators. We created speaker-independent SER models using the eGeMAPS features, transformer-based models and their combination. We compared the results with reference databases and analyzed the influence of annotators and gender fairness. The pre-trained Unispeech-L model and its combination with eGeMAPS achieved the highest results, with 61.64% and 55.57% Unweighted Accuracy (UA) for 3-class valence and arousal prediction respectively, a 10% improvement over baseline models. For the emotion categories, 42.58% UA was obtained. EMOVOME performed low
    
[^5]: 生成还是检索？关于人工环境在医学开放域问答效果的研究

    To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering

    [https://arxiv.org/abs/2403.01924](https://arxiv.org/abs/2403.01924)

    本文介绍了MedGENIE，这是医学领域多项选择问题回答的第一个生成后阅读框架。

    

    医学领域的开放域问答需要大量专业知识的支持。近期的努力致力于将知识与模型参数分离，对抗架构规模化，并允许在常见的低资源硬件上进行训练。检索然后阅读的范式已变得普遍，模型预测依赖于来自外部知识库（如PubMed、教科书和UMLS）的相关知识片段。另一条尚未得到充分探索但由于领域特定大型语言模型的出现变得可能的路径是通过提示构建人工环境。因此，“生成还是检索”成为了现代版的哈姆雷特困境。本文提出了MedGENIE，这是医学领域多项选择问答的生成然后阅读框架。我们在MedQA-USMLE、MedMCQA和MMLU上进行了广泛实验，从实践的角度出发，假设最大

    arXiv:2403.01924v1 Announce Type: cross  Abstract: Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, "to generate or to retrieve" is the modern equivalent of Hamlet's dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maxim
    
[^6]: CLLMs: 一致性大型语言模型

    CLLMs: Consistency Large Language Models

    [https://arxiv.org/abs/2403.00835](https://arxiv.org/abs/2403.00835)

    提出了一种新方法，通过精细调整目标LLM实现了对雅各比轨迹上固定点的一致性预测，有效提高了生成速度2.4倍到3.4倍。

    

    并行解码方法，如雅可比解码，显示出有望实现更高效的LLM推断，因为它打破了LLM解码过程的顺序性，并将其转换为可并行化计算。然而，在实践中，与传统的自回归（AR）解码相比，雅可比解码很少能在单个固定点迭代步骤中准确预测多个标记，因此在速度上取得的提升相对较小。为了解决这个问题，我们开发了一种新方法，旨在实现从任何状态快速收敛到雅各比轨迹上的固定点。通过精细调整目标LLM，以便在任何输入状态下一致地预测固定点。大量实验证明了我们方法的有效性，在领域特定和开放域基准测试中显示出生成速度提高了2.4倍到3.4倍，同时保持了生成质量。

    arXiv:2403.00835v1 Announce Type: cross  Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.
    
[^7]: 在需要时教授大型语言模型一种未知语言

    Teaching Large Language Models an Unseen Language on the Fly

    [https://arxiv.org/abs/2402.19167](https://arxiv.org/abs/2402.19167)

    通过提示，在飞行中教授大型语言模型一种未知语言，提出DiPMT ++框架，通过上下文学习使LLMs适应看不见的语言，并实现了壮语和汉语之间的翻译性能显著提升，并展示了该框架在帮助人类翻译完全未知语言方面的实用性。

    

    现有的大型语言模型在支持许多低资源语言方面存在困难，特别是在极低资源语言方面，在这些语言中，有效参数更新所需的训练数据极少。因此，我们研究了LLM是否可以仅通过提示在飞行中学习一种新语言。为了研究这个问题，我们为壮语收集了一个研究套件，这是当前没有LLMs支持的一种语言。我们介绍了一种名为DiPMT++的框架，用于通过上下文学习将LLMs适应看不见的语言。使用一本词典和仅有5K对平行句子，DiPMT++将GPT-4的性能从0提升到16 BLEU，用于汉语到壮语的翻译，并实现了壮语到汉语的32 BLEU。此外，我们展示了这一框架在帮助人类翻译完全未知语言方面的实际用途，这有助于维护语言多样性。

    arXiv:2402.19167v1 Announce Type: new  Abstract: Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce \textsc{DiPMT++}, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and only 5K parallel sentences, \textsc{DiPMT++} significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. Furthermore, we demonstrate the practical utility of this framework in aiding humans to translate completely unseen languages, which could contribute to the preservation of linguistic diversity.
    
[^8]: 探究使用更高级别表示的遮蔽语言建模在长文档中的应用 - NextLevelBERT

    NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents

    [https://arxiv.org/abs/2402.17682](https://arxiv.org/abs/2402.17682)

    提出了NextLevelBERT，通过在更高级别的语义表示上进行遮蔽语言建模，有效处理长文档用例，具有超越更大嵌入模型的性能

    

    虽然（大型）语言模型在过去几年取得了显著进展，但由于基础注意机制的二次扩展，它们仍然难以合理处理发现在书籍中的长序列。为了解决这个问题，我们提出了NextLevelBERT，这是一个遮蔽语言模型，它不是在标记上操作，而是在文本嵌入的形式中的更高级别语义表示上操作。我们对NextLevelBERT进行预训练，用于预测整个被遮蔽文本块的向量表示，并评估产生的文档向量在三种任务类型上的有效性：1）通过零样本文件嵌入进行语义文本相似性，2）长文档分类，3）多项选择问题回答。我们发现，下一级遮蔽语言建模是一种有效的技术，可以处理长文档用例，并且只要所需的细节水平不太高，就可以超越更大的嵌入模型。我们提供模型和代码 avai

    arXiv:2402.17682v1 Announce Type: new  Abstract: While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings. We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three task types: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering. We find that next level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high. We make model and code avai
    
[^9]: Emotional Voice Messages (EMOVOME)数据库：自发情感语音消息中的情感识别

    Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages

    [https://arxiv.org/abs/2402.17496](https://arxiv.org/abs/2402.17496)

    该研究介绍了Emotional Voice Messages (EMOVOME)数据库，其中包含来自100名西班牙说话者的999条自发语音消息，通过专家和非专家的标记实现了在valence和arousal维度上的情感识别，并尝试使用语音和文本转录实现情感识别模型。

    

    Emotional Voice Messages (EMOVOME)是一个自发语音数据集，包含来自100名西班牙说话者、男女性平衡的999条真实会话中的音频消息，这些消息通过一个消息应用程序产生，在参与者被招募之前在野外环境中制作，避免了由于实验室环境而产生的任何意识偏见。音频按照三个非专家和两个专家的认可在valence和arousal维度上进行了标记，然后将它们结合以获得每个维度的最终标签。专家还提供了对应于七种情感类别的额外标签。为了为将来使用EMOVOME进行调查设定基准，我们使用了语音和音频转录来实现情感识别模型。对于语音部分，我们使用了标准的eGeMAPS特征集和支持向量机，分别获得了49.27%和44.71%的valence和arousal未加权准确度。对于文本部分，我们对一个多语言BERT模型进行了微调，并实现了61%的情感识别准确度。

    arXiv:2402.17496v1 Announce Type: cross  Abstract: Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven emotion categories. To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions. For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we fine-tuned a multilingual BERT model and achieved 61
    
[^10]: 利用大型语言模型通过讲故事学习复杂法律概念

    Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling

    [https://arxiv.org/abs/2402.17019](https://arxiv.org/abs/2402.17019)

    通过大型语言模型和故事讲述，本论文提出了一种新颖的法律教育方法，帮助非专业人士学习复杂的法律概念，并构建了一个包含法律故事和多项选择题的数据集。

    

    将法律知识变得更容易理解对于提升普通法律素养和鼓励公民参与民主至关重要。然而，对于没有法律背景的人来说，法律文件通常难以理解。本文提出了一种新颖的大型语言模型（LLMs）在法律教育中的应用，帮助非专业人士通过讲故事学习复杂的法律概念，讲故事是传达复杂和抽象概念的有效教学工具。我们还介绍了一个名为LegalStories的新数据集，其中包含295个复杂的法律原则，每个原则都附有一个故事和一组由LLMs生成的多项选择题。为了构建数据集，我们尝试使用各种LLMs生成解释这些概念的法律故事。此外，我们使用专家参与的方法来迭代设计多项选择题。然后，我们通过一

    arXiv:2402.17019v1 Announce Type: new  Abstract: Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 295 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop method to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through an 
    
[^11]: EHRNoteQA：用于在临床环境中评估大型语言模型的患者特定问题回答基准

    EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings

    [https://arxiv.org/abs/2402.16040](https://arxiv.org/abs/2402.16040)

    该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs），具有采用多项选择问题回答格式和需要分析多篇临床笔记的特点。

    

    该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs）。在MIMIC-IV电子健康记录（EHR）的基础上，由三位医疗专家团队精心策划了包含962个独特问题的数据集，每个问题都与特定患者的EHR临床笔记相关联。与现有基于EHR的基准不同的是：首先，它是第一个采用多项选择问题回答格式的数据集，这种设计选择在自动评估的背景下有效评估LLMs的得分性能，与其他格式相比。其次，它需要分析多篇临床笔记才能回答一个问题，反映了实际临床决策制定的复杂性，医生需要审查大量患者病史记录。我们对各种大型语言模型进行了全面评估。

    arXiv:2402.16040v1 Announce Type: new  Abstract: This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models
    
[^12]: 选择“选择性预测”：减少视觉语言推理中不必要的弃权

    Selective "Selective Prediction": Reducing Unnecessary Abstention in Vision-Language Reasoning

    [https://arxiv.org/abs/2402.15610](https://arxiv.org/abs/2402.15610)

    引入了一种名为ReCoVERR的算法，能够在视觉-语言系统的推理过程中减少过度放弃，通过寻找图像中的相关线索提供额外证据来取代放弃，从而不降低预测准确性。

    

    先前关于选择性预测的工作旨在通过允许视觉-语言模型（VLM）在不确定时放弃回答，以最小化错误预测。然而，当部署一个对不准确预测容忍度低的视觉-语言系统时，选择性预测可能过于谨慎，并且在许多正确预测上过于放弃。我们引入ReCoVERR，一种用于减少选择性视觉-语言系统过度放弃的推理时间算法，而不降低预测准确性。当VLM做出低置信度预测时，ReCoVERR尝试在图像中找到提供额外证据的相关线索，而不是放弃。ReCoVERR使用LLM向VLM提出相关问题，收集高置信度证据，如果足够的证据确认预测，则系统做出预测而不是放弃。ReCoVERR使两个VLM，BLIP2和InstructBLIP，能够回答。

    arXiv:2402.15610v1 Announce Type: new  Abstract: Prior work on selective prediction minimizes incorrect predictions from vision-language models (VLMs) by allowing them to abstain from answering when uncertain. However, when deploying a vision-language system with low tolerance for inaccurate predictions, selective prediction may be over-cautious and abstain too frequently, even on many correct predictions. We introduce ReCoVERR, an inference-time algorithm to reduce the over-abstention of a selective vision-language system without decreasing prediction accuracy. When the VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries to find relevant clues in the image that provide additional evidence for the prediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if enough evidence confirms the prediction the system makes a prediction instead of abstaining. ReCoVERR enables two VLMs, BLIP2 and InstructBLIP, to answer u
    
[^13]: FinTral：一类GPT-4级别的多模态金融大型语言模型

    FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models

    [https://arxiv.org/abs/2402.10986](https://arxiv.org/abs/2402.10986)

    FinTral是一类基于Mistral-7b模型的GPT-4级别多模态金融大型语言模型，通过领域特定的预训练和检索方法优化，在AI驱动金融技术中取得显著进展。

    

    我们引入FinTral，这是一组基于Mistral-7b模型构建的一流多模态大型语言模型（LLMs），专门为金融分析定制。FinTral整合了文本、数字、表格和图像数据。我们通过利用为本研究策划的大量文本和视觉数据集，通过领域特定的预训练、指导微调和RLAIF训练增强了FinTral。我们还介绍了一个包含九个任务和25个数据集进行评估的广泛基准测试，其中包括金融领域的幻觉。我们的FinTral模型，通过采用先进的工具和检索方法进行直接偏好优化训练，命名为FinTral-DPO-T&R，展现了出色的零-shot性能。它在所有任务中均优于ChatGPT-3.5，并在九项任务中的五项中超越GPT-4，标志着人工智能驱动的金融技术的重要进步。我们还展示了FinTral具有潜力

    arXiv:2402.10986v1 Announce Type: cross  Abstract: We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to e
    
[^14]: CREMA: 通过有效的模块化适应和融合进行多模态组合视频推理

    CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion

    [https://arxiv.org/abs/2402.05889](https://arxiv.org/abs/2402.05889)

    该论文提出了一种名为CREMA的高效且模块化的模态融合框架，用于将任意新的模态注入视频推理。通过利用预训练模型增强多种信息模态，并引入查询转换器和融合模块，实现了灵活且有效的多模态组合推理。

    

    尽管在多模态组合推理方法方面取得了令人瞩目的进展，但由于处理固定模态输入并更新许多模型参数，仍然存在灵活性和效率方面的限制。本文解决了这些关键挑战，提出了CREMA，一种用于将任何新的模态注入视频推理的高效且模块化的模态融合框架。我们首先利用现有的预训练模型从给定的视频中增强多种信息模态（如光流、3D点云、音频），而无需额外的人工注释。接下来，我们引入了一个查询转换器，该转换器与每个可以访问的模态相关联，并具有多个参数高效的模块。它将多种模态特征投影到LLM令牌嵌入空间，使模型能够整合不同的数据类型以进行响应生成。此外，我们提出了一个融合模块，用于压缩多模态查询，在LLM中保持计算效率的同时进行融合组合。

    Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additio
    
[^15]: LESS：用于目标指导调整的选择有影响力的数据

    LESS: Selecting Influential Data for Targeted Instruction Tuning

    [https://arxiv.org/abs/2402.04333](https://arxiv.org/abs/2402.04333)

    LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。

    

    指令调整已经在大型语言模型中释放出强大的能力，有效地使用组合数据集来开发通用聊天机器人。然而，实际应用往往需要一套专门的技能（例如推理）。挑战在于从这些广泛的数据集中识别出最相关的数据，以有效开发特定的能力，我们将这种情况称为目标指导调整。我们提出了LESS，一种优化感知且实际高效的算法，以有效估计数据影响并执行适用于指令数据选择的低秩梯度相似性搜索。关键在于LESS将现有的影响公式调整为与Adam优化器和可变长度指令数据一起工作。LESS首先构建了一个具有低维梯度特征的高度可重用和可传递的梯度数据存储库，然后根据它们与具有特定能力的少样本示例的相似度选择示例。实验证明，t

    Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
    
[^16]: 指令的差异

    Instruction Makes a Difference

    [https://arxiv.org/abs/2402.00453](https://arxiv.org/abs/2402.00453)

    通过引入指令数据集，我们展示了在文档分析和文档图像预测中训练语言-视觉模型的重要性，并表明使用指令数据集可以提高性能。使用Polling-based Object Probing Evaluation (POPE)数据集进行评估，我们发现指令调优性能相对于零-shot性能提高了11倍到32倍，并且相对于非指令微调提高了0.1%到4.2%。尽管如此，仍有很大的改进空间，因为这些性能仍未达到人类水平（94.36%）。

    

    我们介绍了Instruction Document Visual Question Answering (iDocVQA)数据集和Large Language Document (LLaDoc)模型，用于训练语言-视觉（LV）模型进行文档分析和文档图像预测。通常，用于DocVQA任务的深度神经网络是在缺乏指令的数据集上进行训练的。我们表明使用遵循指令的数据集可以提高性能。我们使用最新的Large Language and Vision Assistant (LLaVA)1.5作为基础模型，比较了不同文档相关数据集的性能。我们还使用基于投票的对象探测评估（POPE）数据集评估了导出模型的对象幻觉性能。结果表明，指令调优性能相对于零-shot性能提高了11倍到32倍，并且相对于非指令（传统任务）微调提高了0.1%到4.2%。尽管取得了这些进展，但仍然达不到人类性能（94.36%），这意味着有很大的改进空间。

    We introduce Instruction Document Visual Question Answering (iDocVQA) dataset and Large Language Document (LLaDoc) model, for training Language-Vision (LV) models for document analysis and predictions on document images, respectively. Usually, deep neural networks for the DocVQA task are trained on datasets lacking instructions. We show that using instruction-following datasets improves performance. We compare performance across document-related datasets using the recent state-of-the-art (SotA) Large Language and Vision Assistant (LLaVA)1.5 as the base model. We also evaluate the performance of the derived models for object hallucination using the Polling-based Object Probing Evaluation (POPE) dataset. The results show that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction (traditional task) finetuning. Despite the gains, these still fall short of human performance (94.36%), implying there's much room for improveme
    
[^17]: 少样本迁移学习用于知识库问答：融合监督模型和上下文学习

    Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning

    [https://arxiv.org/abs/2311.08894](https://arxiv.org/abs/2311.08894)

    提出了一种新的KBQA架构FuSIC-KBQA，通过多个源训练的召回器执行KB检索，在LLM的重新排序后以此作为LLM少样本上下文学习的输入来生成逻辑形式，并利用执行引导反馈进一步优化。

    

    现有的知识库问答（KBQA）架构需要大量注释数据，这使得它们在部署时成本高且耗时。我们提出少样本迁移学习用于KBQA的问题，目标域仅提供少量标记示例，但在源域中有大量标记训练数据集可用。我们提出了一种名为FuSIC-KBQA的新型KBQA架构，它使用多个经过源培训的召回器执行KB检索，使用LLM重新排序，将此作为LLM少样本上下文学习的输入以生成逻辑形式，进一步使用执行引导反馈进行细化。在四对不同复杂度的源-目标KBQA对上的实验表明，FuSIC-KBQA明显优于为此设置调整的SoTA KBQA模型。在领域内设置的额外实验表明，当训练数据有限时，FuSIC-KBQA也优于SoTA KBQA模型。

    arXiv:2311.08894v2 Announce Type: replace-cross  Abstract: Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms, which are further refined using execution-guided feedback. Experiments over four source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments in the in-domain setting show that FuSIC-KBQA also outperforms SoTA KBQA models when training data is limited.
    
[^18]: 语言模型就像超级马里奥：通过吸收同源模型的能力来实现免费午餐

    Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch

    [https://arxiv.org/abs/2311.03099](https://arxiv.org/abs/2311.03099)

    本文揭示了语言模型可以通过吸收同源模型的参数来获得新的能力，而无需重新训练或使用GPU。作者提出了DARE技术来稀疏化参数并将多个同源模型合并为一个模型。实验证明，DARE可以轻松删除大部分参数并实现多任务融合。

    

    在本文中，我们揭示了语言模型(LMs)可以通过吸收同源模型的参数来获得新的能力，而无需重新训练或使用GPU。我们首先引入了DARE来将大多数delta参数（即微调和预训练参数之间的差异）设置为零，而不会影响监督微调(SFT) LMs的能力，DARE通过随机删除比率为p的delta参数，并通过1/(1 - p)重新缩放剩余参数来近似原始嵌入。然后，我们将DARE作为一种通用的即插即用技术来稀疏化多个SFT同源模型的delta参数，以减轻参数干扰，并通过参数融合将它们合并为一个模型。我们通过编码器和解码器为基础的LM进行实验，结果表明：（1）SFT delta参数值范围通常很小（在0.005以内），具有极高的冗余，DARE可以轻松删除90%甚至99%的参数。（2）DARE可以将多个任务特定的LM合并为一个LM，并有驾驶技能

    In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio p And REscales the remaining ones by 1/(1 - p) to approximate the original embeddings. Then, we use DARE as a versatile plug-and-play technique to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.005) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them. (2) DARE can merge multiple task-specific LMs into one LM with dive
    
[^19]: 探索疾病中的语义信息：用于中文疾病规范化的简单数据增强技术

    Exploring semantic information in disease: Simple Data Augmentation Techniques for Chinese Disease Normalization

    [https://arxiv.org/abs/2306.01931](https://arxiv.org/abs/2306.01931)

    提出了一组定制的数据增强技术，旨在利用疾病名称中的语义信息，增强模型对疾病名称的语义细微差别和分类结构的理解

    

    疾病名称规范化是医学领域中的重要任务。它将以各种格式编写的疾病名称分类为标准化名称，作为智能医疗系统中各种与疾病相关功能的基本组件。然而，现有疾病名称规范化系统面临的最大障碍是训练数据严重不足。虽然数据增强是解决数据稀缺性的有效方法，但我们的研究发现，传统的数据增强技术通常会阻碍任务性能，主要是由于疾病名称的多轴和多粒度性质。因此，我们介绍了一组定制的数据增强技术，旨在利用疾病名称中固有的语义信息。这些技术旨在增强模型对疾病名称的语义复杂性和分类结构的理解。

    arXiv:2306.01931v2 Announce Type: replace  Abstract: Disease name normalization is an important task in the medical domain. It classifies disease names written in various formats into standardized names, serving as a fundamental component in smart healthcare systems for various disease-related functions. Nevertheless, the most significant obstacle to existing disease name normalization systems is the severe shortage of training data. While data augmentation is a powerful approach for addressing data scarcity, our findings reveal that conventional data augmentation techniques often impede task performance, primarily due to the multi-axis and multi-granularity nature of disease names. Consequently, we introduce a set of customized data augmentation techniques designed to leverage the semantic information inherent in disease names. These techniques aim to enhance the model's understanding of the semantic intricacies and classification structure of disease names. Through extensive experime
    
[^20]: 反学习揭示了语言模型的影响训练数据

    Unlearning Reveals the Influential Training Data of Language Models. (arXiv:2401.15241v1 [cs.CL])

    [http://arxiv.org/abs/2401.15241](http://arxiv.org/abs/2401.15241)

    本文提出了一种简单而有效的方法UnTrac，通过反学习训练数据集来估计语言模型的影响。实验结果表明，UnTrac能够准确评估预训练数据集对生成有害内容的影响，并且无需额外的资源。

    

    为了提高语言模型的性能，同时减少生成有害内容的风险，识别哪些训练数据集影响模型的输出至关重要。理想情况下，我们可以通过从训练中移除每个数据集来衡量其影响;然而，多次重新训练模型是非常昂贵的。本文提出了UnTrac，通过从训练模型中取消学习来估计训练数据集的影响。UnTrac非常简单; 通过梯度上升来取消学习每个训练数据集，并评估在取消学习后模型的预测发生了多大变化。我们经验性地研究了我们的方法是否能评估预训练数据集对生成有毒、有偏见和不真实内容的影响。实验结果表明，我们的方法比现有方法更准确地估计了它们的影响，同时不需要过多的内存空间或多个模型检查点。

    In order to enhance the performance of language models while mitigating the risks of generating harmful content, it is crucial to identify which training dataset affects the model's outputs. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac, which estimates the influence of a training dataset by unlearning it from the trained model. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning. We empirically examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Experimental results demonstrate that our method estimates their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple model checkpoints.
    
[^21]: Axis Tour: Word Tour 确定ICA转换嵌入中轴的顺序

    Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings. (arXiv:2401.06112v1 [cs.CL])

    [http://arxiv.org/abs/2401.06112](http://arxiv.org/abs/2401.06112)

    本研究提出了一种新的方法，Axis Tour，用于确定ICA转换嵌入中轴的顺序，并通过最大化语义连续性来提高词嵌入空间的清晰度。实验证明，Axis Tour构建的低维嵌入比PCA和ICA更好。

    

    词嵌入是自然语言处理中最重要的组成部分之一，但解释高维嵌入仍然是一个具有挑战性的问题。为了解决这个问题，独立成分分析（ICA）被确定为有效的解决方案。ICA转换的词嵌入揭示了可解释的语义轴，但这些轴的顺序是任意的。在这项研究中，我们着重关注这个特性，并提出了一种新的方法，Axis Tour，它优化了轴的顺序。受到一维词嵌入方法Word Tour的启发，我们旨在通过最大化轴的语义连续性来提高词嵌入空间的清晰度。此外，我们通过在下游任务上的实验证明，与PCA和ICA相比，Axis Tour构建了更好的低维嵌入。

    Word embedding is one of the most important components in natural language processing, but interpreting high-dimensional embeddings remains a challenging problem. To address this problem, Independent Component Analysis (ICA) is identified as an effective solution. ICA-transformed word embeddings reveal interpretable semantic axes; however, the order of these axes are arbitrary. In this study, we focus on this property and propose a novel method, Axis Tour, which optimizes the order of the axes. Inspired by Word Tour, a one-dimensional word embedding method, we aim to improve the clarity of the word embedding space by maximizing the semantic continuity of the axes. Furthermore, we show through experiments on downstream tasks that Axis Tour constructs better low-dimensional embeddings compared to both PCA and ICA.
    
[^22]: DEFT：通过无监督核心集选择实现大规模语言模型数据高效微调

    DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.16776](http://arxiv.org/abs/2310.16776)

    这项研究介绍了一种名为DEFT的数据高效微调框架，通过无监督核心集选择来最小化微调大规模语言模型所需的数据量。研究结果表明，DEFT模型在准确性上与现有模型相当，并且仅使用了70%的数据量。

    

    最近的进展使得许多预训练语言模型（PLMs）可以使用；然而，一个仍然存在的问题是微调PLMs以用于下游任务究竟需要多少数据？在这项工作中，我们介绍了DEFT，一种数据高效的微调框架，它利用无监督的核心集选择来最小化微调PLMs所需的数据量。我们在文本编辑LM的背景下展示了DEFT框架的有效性，并与最先进的文本编辑模型CoEDIT进行了比较。我们的定量和定性结果表明，DEFT模型在准确性上与CoEDIT一样，而使用的数据量要少约70%。

    Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demonstrate the efficacy of our DEFT framework in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT models are just as accurate as CoEDIT while being finetuned on ~70% less data.
    
[^23]: 语言模型作为语义索引器

    Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])

    [http://arxiv.org/abs/2310.07815](http://arxiv.org/abs/2310.07815)

    本文介绍了一种使用生成性语言模型学习语义ID的自监督框架LMINDEXER。

    

    语义标识符（ID）是信息检索中的一个重要概念，旨在保留对象（如文档和项）内部的语义。先前的研究通常采用两阶段流程来学习语义ID，首先使用现成的文本编码器获取嵌入，并根据嵌入来推导ID。然而，每个步骤都会引入潜在的信息损失，并且文本编码器生成的潜在空间内的嵌入分布通常与语义索引所需的预期分布存在固有的不匹配。然而，设计一个既能学习文档的语义表示又能同时学习其分层结构的方法并不容易，因为语义ID是离散和顺序结构的，并且语义监督是不充分的。在本文中，我们引入了LMINDEXER，它是一个自监督框架，用于使用生成性语言模型学习语义ID。

    Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackl
    
[^24]: 简明有序的感知有助于大型语言模型进行演绎推理

    Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])

    [http://arxiv.org/abs/2310.03309](http://arxiv.org/abs/2310.03309)

    利用大型语言模型进行演绎推理是一个具有挑战性的问题。这篇论文提出了一个简明有序的方法，将任务分解为子任务并且人类化地组织思维，以提高演绎推理的效果。

    

    利用大型语言模型（LLMs）解决演绎推理问题已经引起了越来越多的关注。在复杂的演绎问题中仍然很难取得令人满意的结果，这类问题具有大量前提（即事实或规则），其中涉及实体之间错综复杂的关系，需要进行多跳推理。一种直观的解决方案是将原始任务分解为较小的子任务，然后以前向（例如选择-推理）或反向（例如LAMBADA）方式将多个因果推理步骤连接在一起。然而，这些技术不可避免地需要大量的总体阶段，导致计算开销大，并且有更高的可能性产生误导性的步骤。除了逐阶段分解之外，我们还从人类问题解决的另一个方面获得了启发。人类倾向于提炼出最相关的信息并有序地组织思维（例如创建思维导图），这有助于他们对问题进行有效的推理。

    Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
    
[^25]: Nugget 2D：用于仅解码器语言模型的动态上下文压缩的扩展

    Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models. (arXiv:2310.02409v1 [cs.CL])

    [http://arxiv.org/abs/2310.02409](http://arxiv.org/abs/2310.02409)

    Nugget 2D是一种用于仅解码器语言模型的动态上下文压缩方法，可以在保留任务能力的同时大幅减少解码过程所需的时间和空间开销。

    

    标准的基于Transformer的语言模型在长上下文中缩放效果不佳。我们提出了一种基于动态上下文压缩的解决方案，该方案将Qin＆Van Durme（2023年）的Nugget方法从BERT类框架扩展到仅解码器的语言模型。我们的方法将历史建模为压缩的“nuggets”，这些“nuggets”经过训练可以进行重建，它可以使用诸如LLaMA之类的现成模型进行初始化。我们通过语言建模、问答和摘要的实验证明，Nugget2D在这些任务中保留了能力，同时在解码过程中大幅减少了时间和空间开销。例如，在自动编码实验中，Nugget2D可以以20倍的压缩比收缩上下文，重建时的BLEU得分为98％，实现了近乎无损编码。

    Standard Transformer-based language models (LMs) scale poorly to long contexts. We propose a solution based on dynamic contextual compression, which extends the Nugget approach of Qin & Van Durme (2023) from BERT-like frameworks to decoder-only LMs. Our method models history as compressed "nuggets" which are trained to allow for reconstruction, and it can be initialized with off-the-shelf models such as LLaMA. We demonstrate through experiments in language modeling, question answering, and summarization that Nugget2D retains capabilities in these tasks, while drastically reducing the overhead during decoding in terms of time and space. For example, in the experiments of autoencoding, Nugget2D can shrink context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
    
[^26]: 多语种手指拼写语料库的主动学习

    Active Learning for Multilingual Fingerspelling Corpora. (arXiv:2309.12443v1 [cs.CL])

    [http://arxiv.org/abs/2309.12443](http://arxiv.org/abs/2309.12443)

    本文应用主动学习来解决手语数据稀缺问题，并对预训练的效果进行了分析，发现预训练在多语种手指拼写语料库上有益处，可能是由于视觉上的相似性。

    

    我们应用主动学习来解决手语中的数据稀缺问题。具体而言，我们对预训练的效果进行了新颖的分析。由于许多手语是法国手语的语言后裔，它们共享手势配置，预训练有望利用这一点。我们在美国、中国、德国和爱尔兰的手指拼写语料库上测试了这一假设。我们确实观察到了预训练的好处，但这可能是由于视觉上的相似性而非语言上的相似性。

    We apply active learning to help with data scarcity problems in sign languages. In particular, we perform a novel analysis of the effect of pre-training. Since many sign languages are linguistic descendants of French sign language, they share hand configurations, which pre-training can hopefully exploit. We test this hypothesis on American, Chinese, German, and Irish fingerspelling corpora. We do observe a benefit from pre-training, but this may be due to visual rather than linguistic similarities
    
[^27]: HypR：一个使用参考语料库进行ASR假设修订的全面研究

    HypR: A comprehensive study for ASR hypothesis revising with a reference corpus. (arXiv:2309.09838v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.09838](http://arxiv.org/abs/2309.09838)

    本研究集中在发布一个ASR假设修订（HypR）数据集，该数据集包含了几个常用的语料库，并且为ASR模型的修订提供了一个基础。

    

    随着深度学习的发展，自动语音识别（ASR）取得了显著进展。为了进一步提高性能，修订识别结果是一种轻量级但高效的方法之一。各种方法可以大致分为N-best重排序方法和错误修正模型。前者旨在从由ASR生成的一组候选假设中选择错误率最低的假设，用于给定的输入语音。后者则专注于检测给定假设中的识别错误，并纠正这些错误以获得增强的结果。然而，我们观察到这些研究很难相互比较，因为它们通常在不同的语料库上进行评估，与不同的ASR模型配对，并且甚至使用不同的数据集来训练模型。因此，在本研究中，我们首先专注于发布一个ASR假设修订（HypR）数据集。HypR包含几个常用的语料库（AISHELL-1，TED-LIUM 2和LibriSpeech），并且提供了ASR模型的基线系统。

    With the development of deep learning, automatic speech recognition (ASR) has made significant progress. To further enhance the performance, revising recognition results is one of the lightweight but efficient manners. Various methods can be roughly classified into N-best reranking methods and error correction models. The former aims to select the hypothesis with the lowest error rate from a set of candidates generated by ASR for a given input speech. The latter focuses on detecting recognition errors in a given hypothesis and correcting these errors to obtain an enhanced result. However, we observe that these studies are hardly comparable to each other as they are usually evaluated on different corpora, paired with different ASR models, and even use different datasets to train the models. Accordingly, we first concentrate on releasing an ASR hypothesis revising (HypR) dataset in this study. HypR contains several commonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech) and provid
    
[^28]: 基于合成临床记录的公开可共享的临床大语言模型

    Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v1 [cs.CL])

    [http://arxiv.org/abs/2309.00237](http://arxiv.org/abs/2309.00237)

    使用合成临床记录构建的临床大语言模型可以克服临床记录的有限可及性和可用性的问题，并在现实应用中表现出潜在的良好性能。

    

    基于合成的临床案例报告，我们首先创建了大规模的合成临床记录，以解决临床记录的有限可及性和可用性的问题。然后，我们使用这些合成记录来训练我们的专门的临床大语言模型Asclepius。虽然Asclepius是在合成数据上训练的，但我们通过使用真实临床记录对其进行评估，以评估其在现实应用中的潜在性能。我们将Asclepius与包括GPT-3.5-turbo和其他开源替代方案在内的几种其他大语言模型进行了基准测试。为了进一步验证我们使用合成记录的方法，我们还将Asclepius与其在真实临床记录上训练的变体进行了比较。我们的发现有力地证明，合成临床记录在构建临床大语言模型时可以作为可行的替代品。

    The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructi
    
[^29]: 无标记多模态数据的多模态学习：保证和应用

    Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications. (arXiv:2306.04539v1 [cs.LG])

    [http://arxiv.org/abs/2306.04539](http://arxiv.org/abs/2306.04539)

    本文研究在只有带标签的单模态数据和自然出现的多模态数据的情况下，如何量化多模态交互的挑战，并提出了两个下界和一个上界来量化多模态交互量。

    

    在许多共同学习多个模态的机器学习系统中，一个核心的研究问题是理解多模态交互的本质：在从两个都没有的模态学习时出现了新的任务相关信息。我们在半监督的情况下研究这一交互量化的挑战，只使用带标签的单模态数据和自然出现的多模态数据（例如，无标签的图像和标题，视频和相应的音频）。利用精确的信息论交互定义，我们的主要贡献是推导下界和上界，量化这种半监督设置下的多模态交互量。我们提出了基于模态共享信息量和单独训练的单模态分类器之间的不一致性的两个下界，并通过连接到近似算法来推导上界。

    In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms fo
    
[^30]: 通过交互式问题-知识对齐解决语言模型幻觉问题

    Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment. (arXiv:2305.13669v1 [cs.CL])

    [http://arxiv.org/abs/2305.13669](http://arxiv.org/abs/2305.13669)

    本文提出了MixAlign框架，通过与用户和知识库交互，实现自动的问题-知识对齐，从而解决了语言模型因无法正确理解问题和知识而导致的幻觉问题。

    

    尽管语言模型近期进展显著，但仍面临幻觉问题，可能会生成误导性和不支持的回答。一种缓解幻觉问题的常见方法是从知识库中检索和整合支持证据。然而，用户的问题通常与存储的知识不太对齐，因为他们在提问前不知道可用的信息。这种不对齐可能限制语言模型定位和利用知识的能力，可能迫使其通过忽略或覆盖检索到的证据而产生幻觉。为了解决这个问题，我们介绍了 MixAlign，一个框架，它与用户和知识库交互以获得并整合关于用户问题与存储信息相关性的澄清信息。 MixAlign 采用语言模型实现自动问题-知识对齐，并在需要时通过人工用户澄清进一步增强这种对齐。

    Despite the remarkable recent advances in language models, they still struggle with the hallucination problem and can generate misleading and unsupported responses. A common approach to mitigate the hallucination issue is retrieving and incorporating supporting evidence from a knowledge base. However, user questions usually do not align well with the stored knowledge, as they are unaware of the information available before asking questions. This misalignment can limit the language model's ability to locate and utilize the knowledge, potentially forcing it to hallucinate by ignoring or overriding the retrieved evidence. To address this issue, we introduce MixAlign, a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic question-knowledge alignment and, if necessary, further enhances this alignment through human user clari
    
[^31]: 基于n-best重排序的精准知识蒸馏

    Accurate Knowledge Distillation with n-best Reranking. (arXiv:2305.12057v1 [cs.CL])

    [http://arxiv.org/abs/2305.12057](http://arxiv.org/abs/2305.12057)

    该论文提出了一种基于n-best重排序的知识蒸馏方法，通过使用多种模型提供伪标签，训练出参数更少但精度相当的学生模型。

    

    我们提出了一种带有n-best重排序的序列级别知识蒸馏方法，该方法考虑了教师模型的top-1假设以及top n-best假设。我们的方法利用包括公开可用的大型预训练模型在内的多种模型，为训练学生模型提供更准确的伪标签。我们在WMT21德英翻译任务上验证了我们的提议，并证明我们的学生模型在具有两个数量级较少的参数的情况下，实现了与Tran等人（2021年）的包含47亿参数的大型翻译模型相当的精度。

    We propose extending the Sequence-level Knowledge Distillation (Kim and Rush, 2016) with n-best reranking to consider not only the top-1 hypotheses but also the top n-best hypotheses of teacher models. Our approach leverages a diverse set of models, including publicly-available large pretrained models, to provide more accurate pseudo-labels for training student models. We validate our proposal on the WMT21 German-English translation task and demonstrate that our student model achieves comparable accuracy to a large translation model with 4.7 billion parameters from (Tran et al., 2021) while having two orders of magnitude fewer parameters.
    

