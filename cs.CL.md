# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://arxiv.org/abs/2404.01291) | 引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果 |
| [^2] | [Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2403.19322) | 提出了一种用于在多模式大型语言模型中实现即插即用推理基础的新框架P2G，通过利用MLLMs的工具使用潜力和专家代理实现对图像关键视觉和文本对象的即时确定性基础，从而实现有意识的推理。 |
| [^3] | [The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian](https://arxiv.org/abs/2403.18697) | 该研究提出了两个新的基准，用于评估语言模型在意大利语的数学和语言理解能力，为当前语言模型的性能提供了具有挑战性的评估标准。 |
| [^4] | [Mechanisms of non-factual hallucinations in language models](https://arxiv.org/abs/2403.18167) | 研究揭示了语言模型中非事实性幻觉的两个通用机制：主题属性知识不足和未能正确选择对象属性，这有助于深入理解和减轻幻觉。 |
| [^5] | [Qibo: A Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2403.16056) | 本论文在中医领域构建了专业语料库，基于LLaMA成功开发了首个经过完整训练的Qibo模型，并推出了用于评估LLMs性能的Qibo基准测试。 |
| [^6] | [Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models](https://arxiv.org/abs/2403.15268) | 提出了一种新颖的知识增强框架，即想象增强生成（IAG），通过想象力，而非依赖外部资源，来补充大型语言模型中可能存在的知识缺陷，并提出了一种想象更丰富背景的方法（IMcQA）来解决问题回答中的挑战。 |
| [^7] | [Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!](https://arxiv.org/abs/2403.10963) | Pointer-Generator Networks在低资源机器翻译中未展现出预期的优势，模型在不同资源范围和语言之间的关系下表现一般。 |
| [^8] | [Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4](https://arxiv.org/abs/2403.05680) | 提出了一种新颖的评估框架，用于评估视觉-语言LLMs在生成CT异常的准确摘要方面的能力。 |
| [^9] | [HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction](https://arxiv.org/abs/2403.05396) | HistGen是一个通过本地-全局特征编码和跨模态上下文交互来生成组织病理学报告的框架，提供了第一个用于评估的基准数据集。 |
| [^10] | [Electrocardiogram Instruction Tuning for Report Generation](https://arxiv.org/abs/2403.04945) | 提出了Multimodal ECG Instruction Tuning（MEIT）框架，首次尝试使用LLMs和多模态指导解决ECG报告生成问题，并在两个大规模ECG数据集上进行了广泛的实验评估其优越性。 |
| [^11] | [SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis](https://arxiv.org/abs/2403.01976) | SciAssess介绍了一个专为深度分析科学文献而设计的基准测试，旨在全面评估LLMs在科学领域记忆、理解和分析能力的有效性。 |
| [^12] | [Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication](https://arxiv.org/abs/2402.18439) | 挑战了默认使用自然语言的做法，通过探索LLMs在推理和沟通中使用替代格式的实用性，实现了推理效率的提升和多智能体通信时标记使用量的显著减少。 |
| [^13] | [Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models](https://arxiv.org/abs/2402.16367) | 通过将原始的大型语言模型转化为专家混合架构，研究了LLMs的多语言激活模式，发现存在非特定语言的神经元和特定语言激活神经元，以及高频激活神经元可以加速推断并保持性能。 |
| [^14] | [SportQA: A Benchmark for Sports Understanding in Large Language Models](https://arxiv.org/abs/2402.15862) | SportQA是一个新的基准测试，旨在评估大型语言模型在体育理解方面的表现，包含超过70,000个问题涵盖不同难度级别的体育知识，并揭示了LLMs在基本体育知识上表现优异但在复杂情境推理方面存在挑战。 |
| [^15] | [A Theoretical Result on the Inductive Bias of RNN Language Models](https://arxiv.org/abs/2402.15814) | RNN语言模型能够有效表示更大类别的语言模型，具有有界堆栈和广义堆栈更新函数，类似于保留固定数量符号记忆并使用简单更新机制的自动机。 |
| [^16] | [Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models](https://arxiv.org/abs/2402.15301) | 利用大型语言模型和检索增强生成技术，提出了一种新方法用于从科学文献中推断因果关系，以解决传统方法受限于数据收集偏见和个体知识的问题。 |
| [^17] | [PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs](https://arxiv.org/abs/2402.12835) | PANDA是一种旨在增强LLMs领域特定能力的方法，通过利用专家模型响应偏好的见解，无需微调即可实现显著改进。 |
| [^18] | [In-Context Learning Demonstration Selection via Influence Analysis](https://arxiv.org/abs/2402.11750) | 通过分析训练样本的影响，提出一种名为InfICL的演示选择方法，可以帮助提升上下文学习的泛化性能。 |
| [^19] | [How Susceptible are Large Language Models to Ideological Manipulation?](https://arxiv.org/abs/2402.11725) | 大型语言模型展示出令人担忧的易受意识形态操纵的脆弱性，对极少量意识形态驱动样本的暴露就能显著改变其意识形态，并且能够从一个主题吸收意识形态并泛化到其他不相关主题上。 |
| [^20] | [Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models](https://arxiv.org/abs/2402.11436) | 大型语言模型存在自我偏见，研究发现通过更大的模型规模和准确评估的外部反馈可以显著减少这种偏见，并提高后续任务的实际表现。 |
| [^21] | [Whose Emotions and Moral Sentiments Do Language Models Reflect?](https://arxiv.org/abs/2402.11114) | 该研究探讨了语言模型在情感和道德维度上如何代表不同群体，发现它们与意识形态团体存在显著的不一致性。 |
| [^22] | [Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts](https://arxiv.org/abs/2402.10554) | Disordered-DABS是针对不规则文本中动态基于方面的总结而设计的新基准测试，挑战了现有总结模型的独特性。 |
| [^23] | [Concept-1K: A Novel Benchmark for Instance Incremental Learning](https://arxiv.org/abs/2402.08526) | 我们提出了一种名为Concept-1K的具有挑战性的实例增量学习（IIL）场景和数据集，揭示了十亿参数的PLM仍然遭受灾难性遗忘，影响因素包括模型规模、预训练和缓冲区大小。现有的IL方法和LoRA技术无法满足性能需求。我们的研究为探索和缓解PLM中的遗忘问题提供了新的场景。 |
| [^24] | [Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries](https://arxiv.org/abs/2402.08349) | 本文首次深入评估了在实践中文本到SQL系统的数据模型的鲁棒性，通过基于一个多年的国际项目集中评估，对一个在FIFA World Cup背景下连续运行了9个月的真实部署的FootballDB系统进行了评估。 |
| [^25] | [LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation](https://arxiv.org/abs/2402.07721) | 本文提出了LoRA-drop方法，通过分析LoRA输出评估参数的重要性，并且保留重要层的LoRA，其余层共享相同参数。实验结果表明LoRA-drop有很好的效果。 |
| [^26] | [Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric](https://arxiv.org/abs/2402.06900) | 本研究提出了一种基于大型语言模型（LLMs）的自动度量方法，用于识别生成文本中的毒性。通过分析毒性因素和LLMs的内在毒性属性，该方法在测量毒性方面表现出众，比现有指标提升12个百分点。 |
| [^27] | [Online Cascade Learning for Efficient Inference over Streams](https://arxiv.org/abs/2402.04513) | 这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。 |
| [^28] | [Retrieve to Explain: Evidence-driven Predictions with Language Models](https://arxiv.org/abs/2402.04068) | 检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。 |
| [^29] | [Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues](https://arxiv.org/abs/2402.01737) | 本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性。 |
| [^30] | [Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/abs/2402.00798) | 本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。 |
| [^31] | [Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data](https://arxiv.org/abs/2402.00743) | 本研究通过线性回归任务的实验研究了Transformer在非结构化数据中的上下文学习能力，并解释了其中的关键组件。 |
| [^32] | [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805) | Gemini家族是一系列在图像、音频、视频和文本理解方面表现出色的多模态模型，其中最具能力的Gemini Ultra模型在30个基准测试中推进了技术前沿，并改进了所有20个多模态基准测试的技术状态。 |
| [^33] | [Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models.](http://arxiv.org/abs/2401.15269) | 本论文介绍了一种名为Self-BioRAG的框架，通过使用检索和自我反思的方法，提高了医疗推理的能力。该框架专注于生成解释、检索领域特定文档以及对生成的响应进行自我反思。 |
| [^34] | [Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support.](http://arxiv.org/abs/2401.14688) | Taiyi-Diffusion-XL是一个中英双语文本到图像生成模型，通过对CLIP和Stable-Diffusion-XL的能力进行扩展，并通过双语连续预训练来实现。模型通过将常用的汉字整合到CLIP的分词器和嵌入层中，以及使用大型视觉语言模型丰富文本提示，提供了更好的图像标题和高质量的视觉效果。实验证明，该模型在双语图像-文本检索中表现出色。 |
| [^35] | [Towards Goal-oriented Large Language Model Prompting: A Survey.](http://arxiv.org/abs/2401.14043) | 本文调查了大型语言模型(LLM)中目标导向提示工程的重要性。通过对35个代表性研究的回顾，我们发现引导LLM遵循人类的逻辑思维的目标导向提示公式显著提高了LLM的性能。我们还提出了一个新的分类体系，并总结了十个适用任务来展示我们框架的广泛适用性。同时，我们提出了四个未来的方向，以推动目标导向提示工程的进一步发展。 |
| [^36] | [Prompt Weight Experiments for LLM Instruction Fine-Tuning.](http://arxiv.org/abs/2401.13586) | LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。 |
| [^37] | [Spatial-Temporal Large Language Model for Traffic Prediction.](http://arxiv.org/abs/2401.10134) | 本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测，通过参数扩展和预训练来提高预测准确性，并利用空间-时间嵌入模块学习标记的空间位置和全局时间表示。 |
| [^38] | [Transformers are Multi-State RNNs.](http://arxiv.org/abs/2401.06104) | 本文研究发现，只使用解码器的 Transformers 可以被视为无限多状态的 RNNs，并且可以通过固定隐藏状态的大小来转换为有限多状态的 RNNs。我们提出了一种新的转换策略 TOVA，在多个长距离任务中表现优于其他基准策略，并且与完整模型几乎持平，在某些情况下仅使用原始缓存大小的 $\frac{1}{8}$。这些结果表明，Transformer 解码器 LLMs 在实践中通常表现为 RNNs。 |
| [^39] | [Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback.](http://arxiv.org/abs/2401.05928) | Muffin是一个新型的模型无关框架，用于减轻情感支持对话中无益性的问题。这个框架在生成支持性回复时考虑到多个因素，并通过多方位的AI反馈来训练模型，以避免生成无益的回复。 |
| [^40] | [An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek.](http://arxiv.org/abs/2311.00541) | 本论文介绍了一个嵌入式历时语义变化模型（EDiSC），结合了词嵌入和DiSC模型，通过无监督学习分析古希腊文本中目标词汇的意义变化。实验证明EDiSC具有优越的性能。 |
| [^41] | [Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review.](http://arxiv.org/abs/2310.14735) | 这篇论文解释了提示工程在释放大型语言模型能力方面的关键作用，探讨了不同的提示方法以及外部插件如何协助减少机器幻想，并指出了未来研究方向的重要性。 |
| [^42] | [Language Models as Zero-Shot Trajectory Generators.](http://arxiv.org/abs/2310.11604) | 本文研究了使用大型语言模型（LLMs）作为零-shot轨迹生成器的可能性。通过给予LLM物体检测和分割视觉模型的访问权限，研究人员发现LLMs能够直接预测操作技能中的末端执行器姿态序列，并在26个真实世界的语言任务中取得了良好效果。这一研究突破了对LLMs在机器人技术中的限制，揭示了LLMs确实具有对操作任务的理解能力。 |
| [^43] | [Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey.](http://arxiv.org/abs/2310.01424) | 这项调查对语言模型（LMs）中的隐私风险进行了研究和减轻措施的探讨，通过分类法和调查现有攻击，提出了关键趋势，并讨论现有的减轻策略的优势和局限性，指出关键差距和未来工作方向。 |
| [^44] | [Let Me Teach You: Pedagogical Foundations of Feedback for Language Models.](http://arxiv.org/abs/2307.00279) | 这篇观点文章介绍了一个基于教育学理念的反馈框架FELT，用于对大型语言模型进行反馈，以提高模型与人类偏好的一致性。该框架不仅简化了现有的手工设计反馈，还为NLF研究开辟了新方向。 |
| [^45] | [Taxonomy Completion with Probabilistic Scorer via Box Embedding.](http://arxiv.org/abs/2305.11004) | 本文提出了一种新方法TaxBox，该方法将分类法概念映射到框嵌入中，并使用两个概率评分器来进行概念附加和插入，避免使用伪叶。实验表明，在二个基准数据集上，TaxBox在准确性和训练效率方面显著优于现有的方法。 |
| [^46] | [ChatLog: Recording and Analyzing ChatGPT Across Time.](http://arxiv.org/abs/2304.14106) | ChatLog是一个分析ChatGPT随时间变化的数据集，通过提取ChatGPT知识和语言特征发现了一些稳定的特征，提高了RoBERTa-based detector在新版本ChatGPT上的鲁棒性。 |
| [^47] | [Investigating Failures to Generalize for Coreference Resolution Models.](http://arxiv.org/abs/2303.09092) | 本文研究了不同数据集上指代消解模型的表现，并发现模型的表现可能会受到数据集不同的操作化方式的影响，强调了在不同数据集上评估指代消解模型的重要性。 |
| [^48] | [A Survey on In-context Learning.](http://arxiv.org/abs/2301.00234) | 本文调查和总结了上下文学习(ICL)的进展和挑战，ICL已成为自然语言处理(NLP)的新范式，探索ICL以评估和推广大型语言模型(LLM)的能力已成为一种新趋势。本文提出了ICL的正式定义，并总结了高级技术，最后讨论了ICL的挑战以及进一步研究的潜在方向。 |

# 详细

[^1]: 评估文本到图像生成与图像到文本生成

    Evaluating Text-to-Visual Generation with Image-to-Text Generation

    [https://arxiv.org/abs/2404.01291](https://arxiv.org/abs/2404.01291)

    引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果

    

    尽管生成式人工智能取得了显著进展，但由于缺乏有效的度量标准和标准化基准，综合评估仍然具有挑战性。为了解决这个问题，我们引入了VQAScore，使用视觉问答（VQA）模型通过计算对简单的“这幅图表现出了'{文本}'吗？”问题的“是”答案的概率来生成对齐分数。尽管比先前的方法更简单，但使用现成模型计算的VQAScore在许多（8个）图像文本对齐基准上产生了最先进的结果。

    arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
    
[^2]: 在多模式大型语言模型中实现即插即用的推理基础

    Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models

    [https://arxiv.org/abs/2403.19322](https://arxiv.org/abs/2403.19322)

    提出了一种用于在多模式大型语言模型中实现即插即用推理基础的新框架P2G，通过利用MLLMs的工具使用潜力和专家代理实现对图像关键视觉和文本对象的即时确定性基础，从而实现有意识的推理。

    

    随着多模式大型语言模型（MLLMs）的兴起，由于其在指令遵循和推理方面突出的新功能，这些模型极大地推动了视觉推理领域的发展。然而，受到其非无损图像标记化的限制，大多数MLLMs在全面捕捉文本和对象细节方面存在不足，尤其是在高分辨率图像中。为解决这一问题，我们提出了P2G，一种用于在MLLMs中实现即插即用推理基础的新框架。具体而言，P2G利用MLLMs的工具使用潜力，利用专家代理来实现对图像的关键视觉和文本对象的即时确定性基础，从而通过多模式提示实现有意识的推理。我们进一步创建了P2GB，一个旨在评估MLLMs在理解具有挑战性的高分辨率图像中的物体间关系和文本能力的基准。对视觉推理任务的全面实验表明了P2G的优越性。

    arXiv:2403.19322v1 Announce Type: cross  Abstract: The surge of Multimodal Large Language Models (MLLMs), given their prominent emergent capabilities in instruction following and reasoning, has greatly advanced the field of visual reasoning. However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images. To address this, we propose P2G, a novel framework for plug-and-play grounding of reasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of MLLMs to employ expert agents to achieve on-the-fly grounding to critical visual and textual objects of image, thus achieving deliberate reasoning via multimodal prompting. We further create P2GB, a benchmark aimed at assessing MLLMs' ability to understand inter-object relationships and text in challenging high-resolution images. Comprehensive experiments on visual reasoning tasks demonstrate the superiority of P2G. 
    
[^3]: Invalsi基准：衡量语言模型在意大利语的数学和语言理解能力

    The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian

    [https://arxiv.org/abs/2403.18697](https://arxiv.org/abs/2403.18697)

    该研究提出了两个新的基准，用于评估语言模型在意大利语的数学和语言理解能力，为当前语言模型的性能提供了具有挑战性的评估标准。

    

    尽管意大利语在所有指标上都是一种高资源语言，但目前并没有一种专门针对该语言进行预训练的语言模型。这导致了可用于评估意大利语语言模型性能的基准数目较少。本文提出了两个新的基准，用于评估模型在意大利语的数学理解和语言理解方面的性能。这些基准基于意大利学校系统内11至18岁学生进行的实际测试，并已由多位教学和教育学专家验证。

    arXiv:2403.18697v1 Announce Type: new  Abstract: While Italian is by all metrics a high resource language, currently, there are isn't a Language Model pre-trained exclusively in this language. This results in a lower number of available benchmarks to evaluate the performance of language models in Italian.   This work presents two new benchmarks to evaluate the models performance on mathematical understanding and language understanding in Italian. These benchmarks are based on real tests that are undertaken by students of age between 11 and 18 within the Italian school system and have therefore been validated by several experts in didactics and pedagogy.   To validate this dataset we evaluate the performance of 9 language models that are the best performing when writing in Italian, including our own fine-tuned models. We show that this is a challenging benchmark where current language models are bound by 60\% accuracy.   We believe that the release of this dataset paves the way for impr
    
[^4]: 语言模型中非事实性幻觉的机制

    Mechanisms of non-factual hallucinations in language models

    [https://arxiv.org/abs/2403.18167](https://arxiv.org/abs/2403.18167)

    研究揭示了语言模型中非事实性幻觉的两个通用机制：主题属性知识不足和未能正确选择对象属性，这有助于深入理解和减轻幻觉。

    

    现今最先进的语言模型（LMs）有时会产生与世界知识不符的非事实幻觉。尽管人们已经付出了大量努力来检测和减轻幻觉，但理解它们的内在机制仍然是困难的。 我们的研究调查了幻觉的机制原因，特别是 LM 在对主题关系查询做出回答时错误地预测对象属性的非事实形式。通过因果中介分析和嵌入空间投影，我们确认了跨不同规模和设计的 LM 中共享的两个造成幻觉的一般机制原因：1）在较低层 MLPs 中主题属性知识不足，以及2）在较高层注意力头和 MLPs 中未能选择正确的对象属性。这两个机制展示了不同程度的主宾关系、预测不确定性和扰动鲁棒性。此外，我们还审查了 LM 的预训练检查点。

    arXiv:2403.18167v1 Announce Type: cross  Abstract: State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, r
    
[^5]: Qibo: 一种用于中医领域的大型语言模型

    Qibo: A Large Language Model for Traditional Chinese Medicine

    [https://arxiv.org/abs/2403.16056](https://arxiv.org/abs/2403.16056)

    本论文在中医领域构建了专业语料库，基于LLaMA成功开发了首个经过完整训练的Qibo模型，并推出了用于评估LLMs性能的Qibo基准测试。

    

    在人工智能领域，大型语言模型(LLMs)展示了在用户意图理解和响应方面取得的显著进展，在许多专业领域，包括医学、法律和金融。然而，在中医领域，LLMs的性能提升受到挑战，其原因在于中医理论与现代医学之间的根本差异，以及缺乏专业语料库资源。本文旨在构建和整理中医领域的专业语料库，赋予大型模型具有中医理论特色的专业知识，并成功基于LLaMA开发了Qibo模型，这是中医领域第一个经过完整训练过程（从预训练到监督微调）的LLM。此外，我们开发了Qibo基准测试，这是一个用于评估LLMs性能的专门工具。

    arXiv:2403.16056v1 Announce Type: cross  Abstract: In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, whic
    
[^6]: 想象增强生成：学习想象更丰富的背景来进行大型语言模型问题回答

    Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models

    [https://arxiv.org/abs/2403.15268](https://arxiv.org/abs/2403.15268)

    提出了一种新颖的知识增强框架，即想象增强生成（IAG），通过想象力，而非依赖外部资源，来补充大型语言模型中可能存在的知识缺陷，并提出了一种想象更丰富背景的方法（IMcQA）来解决问题回答中的挑战。

    

    检索增强生成和生成增强生成已被提出来增强大型语言模型（LLMs）上的问题回答所需的知识。然而，前者依赖于外部资源，而且两者都需要将显式文档合并到上下文中，导致更长的上下文，从而消耗更多资源。最近的研究表明，LLMs已经建模了丰富的知识，尽管没有被有效地触发或激活。在此启发下，我们提出了一种新颖的知识增强框架，即想象增强生成（IAG），它模拟了人类通过想象力在仅凭想象回答问题时弥补知识缺陷的能力，而不依赖外部资源。在IAG的指导下，我们提出了一种用于问题回答的想象更丰富背景的方法（IMcQA），通过以下两个模块获得更丰富的背景：通过生成简单的想象实现显式想象

    arXiv:2403.15268v1 Announce Type: new  Abstract: Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results in longer contexts that lead to more resource consumption. Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or activated. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, without relying on external resources. Guided by IAG, we propose an imagine richer context method for question answering (IMcQA), which obtains richer context through the following two modules: explicit imagination by generating a sho
    
[^7]: Pointer-Generator网络用于低资源机器翻译：不要复制那个！

    Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!

    [https://arxiv.org/abs/2403.10963](https://arxiv.org/abs/2403.10963)

    Pointer-Generator Networks在低资源机器翻译中未展现出预期的优势，模型在不同资源范围和语言之间的关系下表现一般。

    

    虽然基于Transformer的神经机器翻译（NMT）在高资源环境中非常有效，但许多语言缺乏必要的大规模平行语料库来受益。在两种密切相关语言之间的低资源（LR）机器翻译中，一种自然的直觉是寻求从结构“捷径”中获益，例如从源语言复制子词到目标语言，因为这样的语言对通常共享相当数量的相同单词、同源词和借词。我们测试了针对六种语言对的指针生成器网络在各种资源范围下的用途，并发现在大多数情况下都有轻微改进。然而，分析显示，模型对于密切相关的语言对与较远的语言对，或者资源范围较低与较高的语言对并没有展现出更大的改进，并且模型并未展示出对于共享子词机制的预期用法。我们讨论了这种行为的原因。

    arXiv:2403.10963v1 Announce Type: new  Abstract: While Transformer-based neural machine translation (NMT) is very effective in high-resource settings, many languages lack the necessary large parallel corpora to benefit from it. In the context of low-resource (LR) MT between two closely-related languages, a natural intuition is to seek benefits from structural "shortcuts", such as copying subwords from the source to the target, given that such language pairs often share a considerable number of identical words, cognates, and borrowings. We test Pointer-Generator Networks for this purpose for six language pairs over a variety of resource ranges, and find weak improvements for most settings. However, analysis shows that the model does not show greater improvements for closely-related vs. more distant language pairs, or for lower resource ranges, and that the models do not exhibit the expected usage of the mechanism for shared subwords. Our discussion of the reasons for this behaviour high
    
[^8]: 使用GPT-4对基于视觉的LLM预测进行分解以进行自动评估

    Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4

    [https://arxiv.org/abs/2403.05680](https://arxiv.org/abs/2403.05680)

    提出了一种新颖的评估框架，用于评估视觉-语言LLMs在生成CT异常的准确摘要方面的能力。

    

    CT检查的数量每年都在增加，这导致放射科医生疲劳。大型语言模型（LLMs）有潜力减轻他们的负担，但其在临床中的采用取决于放射科医生的信任和生成内容的简单评估。本文提出了一个新颖的评估框架，用于评估视觉-语言LLMs在生成CT异常的准确摘要方面的能力。

    arXiv:2403.05680v1 Announce Type: new  Abstract: The volume of CT exams being done in the world has been rising every year, which has led to radiologist burn-out. Large Language Models (LLMs) have the potential to reduce their burden, but their adoption in the clinic depends on radiologist trust, and easy evaluation of generated content. Presently, many automated methods are available to evaluate the reports generated for chest radiographs, but such an approach is not available for CT presently. In this paper, we propose a novel evaluation framework to judge the capabilities of vision-language LLMs in generating accurate summaries of CT-based abnormalities. CT slices containing an abnormality (e.g., lesion) were input to a vision-based LLM (GPT-4V, LLaVA-Med, and RadFM), and it generated a free-text summary of the predicted characteristics of the abnormality. Next, a GPT-4 model decomposed the summary into specific aspects (body part, location, type, and attributes), automatically eval
    
[^9]: HistGen：通过本地-全局特征编码和跨模态上下文交互生成组织病理学报告

    HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction

    [https://arxiv.org/abs/2403.05396](https://arxiv.org/abs/2403.05396)

    HistGen是一个通过本地-全局特征编码和跨模态上下文交互来生成组织病理学报告的框架，提供了第一个用于评估的基准数据集。

    

    组织病理学在癌症诊断中扮演着黄金标准的角色，临床报告在解释和理解这一过程中至关重要，在指导癌症治疗和患者护理方面起着关键作用。深度学习对组织病理学报告生成的自动化将极大提升临床效率，并减轻病理学家在报告撰写方面的劳动强度和耗时负担。为追求这一进步，作者引入了HistGen，这是一个多实例学习增强的组织病理学报告生成框架，并提供第一个用于评估的基准数据集。HistGen受诊断和报告撰写工作流程的启发，具有两个精心设计的模块，旨在通过对齐整张切片图像（WSIs）和诊断报告，从本地和全局粒度提升报告生成。为实现这一目标，开发了一个本地-全局分层编码器，用于有效地从区域中聚合视觉特征。

    arXiv:2403.05396v1 Announce Type: cross  Abstract: Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a regi
    
[^10]: 为报告生成调优心电图指导

    Electrocardiogram Instruction Tuning for Report Generation

    [https://arxiv.org/abs/2403.04945](https://arxiv.org/abs/2403.04945)

    提出了Multimodal ECG Instruction Tuning（MEIT）框架，首次尝试使用LLMs和多模态指导解决ECG报告生成问题，并在两个大规模ECG数据集上进行了广泛的实验评估其优越性。

    

    心电图（ECG）作为心脏病情监测的主要非侵入性诊断工具，对于协助临床医生至关重要。最近的研究集中在使用ECG数据对心脏病情进行分类，但忽略了ECG报告生成，这不仅耗时，而且需要临床专业知识。为了自动化ECG报告生成并确保其多功能性，我们提出了Multimodal ECG Instruction Tuning（MEIT）框架，这是\textit{首次}尝试使用LLMs和多模态指导来解决ECG报告生成问题。为了促进未来的研究，我们建立了一个基准来评估MEIT在两个大规模ECG数据集上使用各种LLM骨干的表现。我们的方法独特地对齐了ECG信号和报告的表示，并进行了大量实验来评估MEIT与九个开源LLMs，使用了超过80万个ECG报告。MEIT的结果凸显了其优越性。

    arXiv:2403.04945v1 Announce Type: new  Abstract: Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results underscore the superior p
    
[^11]: SciAssess：基准测试LLM在科学文献分析中的熟练程度

    SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis

    [https://arxiv.org/abs/2403.01976](https://arxiv.org/abs/2403.01976)

    SciAssess介绍了一个专为深度分析科学文献而设计的基准测试，旨在全面评估LLMs在科学领域记忆、理解和分析能力的有效性。

    

    arXiv:2403.01976v1 公告类型：新 抽象：大型语言模型（LLMs）的最新突破已经彻底改变了自然语言理解和生成，引发了人们对利用这些技术进行细致科学文献分析的兴趣激增。然而，现有的基准测试未能充分评估LLMs在科学领域的熟练程度，特别是在涉及复杂理解和多模态数据的情况下。为此，我们引入了SciAssess，一个专为深度分析科学文献而设计的基准测试，旨在全面评估LLMs的有效性。SciAssess专注于评估LLMs在科学背景下记忆、理解和分析的能力。它包括来自不同科学领域的代表性任务，如一般化学、有机材料和合金材料。严格的质量控制措施确保了其在正确性、匿名化和复制方面的可靠性。

    arXiv:2403.01976v1 Announce Type: new  Abstract: Recent breakthroughs in Large Language Models (LLMs) have revolutionized natural language understanding and generation, igniting a surge of interest in leveraging these technologies for the nuanced field of scientific literature analysis. Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in the scientific domain, especially in scenarios involving complex comprehension and multimodal data. In response, we introduced SciAssess, a benchmark tailored for the in-depth analysis of scientific literature, crafted to provide a thorough assessment of LLMs' efficacy. SciAssess focuses on evaluating LLMs' abilities in memorization, comprehension, and analysis within scientific contexts. It includes representative tasks from diverse scientific fields, such as general chemistry, organic materials, and alloy materials. And rigorous quality control measures ensure its reliability in terms of correctness, anonymization, and copy
    
[^12]: 超越自然语言：LLM利用替代格式进行增强推理和沟通

    Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication

    [https://arxiv.org/abs/2402.18439](https://arxiv.org/abs/2402.18439)

    挑战了默认使用自然语言的做法，通过探索LLMs在推理和沟通中使用替代格式的实用性，实现了推理效率的提升和多智能体通信时标记使用量的显著减少。

    

    自然语言（NL）长期以来一直是人类认知和沟通的主要格式，因此，在大型语言模型（LLMs）的发展和应用中同样至关重要。然而，除了NL之外，LLMs在预训练过程中看到了各种非NL格式，如代码和逻辑表达式。NL作为LLMs的最佳格式，在单一LLM推理和多智能体通信中的地位尚未得到彻底审查。在这项工作中，我们挑战了默认使用NL，通过探索在这些上下文中使用非NL格式的效用。我们展示，允许LLMs在推理或沟通之前自主选择最合适的格式，可导致不同LLMs推理效率提高3.3至5.7％，并且在多智能体通信中最多可减少72.7％的标记使用，同时保持沟通效果。我们的综合分析进一步揭示了LLMs可以......

    arXiv:2402.18439v1 Announce Type: cross  Abstract: Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs c
    
[^13]: 揭示巴别塔：探究大型语言模型内的多语言激活模式

    Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models

    [https://arxiv.org/abs/2402.16367](https://arxiv.org/abs/2402.16367)

    通过将原始的大型语言模型转化为专家混合架构，研究了LLMs的多语言激活模式，发现存在非特定语言的神经元和特定语言激活神经元，以及高频激活神经元可以加速推断并保持性能。

    

    最近，大型语言模型（LLMs）在语言处理领域取得了巨大突破，但它们在处理多种语言时的机制仍然是未知的。因此，在这项工作中，我们研究了LLMs的多语言激活模式。通过将原始的大型语言模型（LLMs）转化为专家混合（MoE）架构，我们分析了处理各种语言时专家的激活模式，并展示了这些激活模式在语言家族层面上的联系。我们发现了非特定语言的神经元以及特定语言激活神经元的存在。进一步的探索甚至展示了仅利用高频激活神经元可以加速推断，同时保持可比较的性能。这些发现揭示了LLMs的多语言处理机制，并在指导多语言训练方面具有重要意义。

    arXiv:2402.16367v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of language processing, yet their mechanisms in processing multiple languages remain agnostic. Therefore, in this work we study the multilingual activation patterns of LLMs. By transforming the original Large Language Models (LLMs) into a Mixture of Experts (MoE) architecture, we analyze the expert activation patterns when processing various languages and demonstrate the connections of these activation patterns at the level of language families. We discover the existence of non-language-specific neurons as well as language-specific activation neurons. Further exploration even showcases that merely leveraging high-frequency activation neurons can accelerate inference while maintaining comparable performance. These findings shed light on the LLMs' multilingual processing mechanism, and are of significant importance in guiding the multilingual trainin
    
[^14]: SportQA：大型语言模型中体育理解的基准评估

    SportQA: A Benchmark for Sports Understanding in Large Language Models

    [https://arxiv.org/abs/2402.15862](https://arxiv.org/abs/2402.15862)

    SportQA是一个新的基准测试，旨在评估大型语言模型在体育理解方面的表现，包含超过70,000个问题涵盖不同难度级别的体育知识，并揭示了LLMs在基本体育知识上表现优异但在复杂情境推理方面存在挑战。

    

    arXiv:2402.15862v1 报告类型：新的 摘要：对体育领域进行深入理解，这是一项充满战略和动态内容的领域，对于推动自然语言处理（NLP）至关重要。这在评估和推进大型语言模型（LLMs）的背景下尤为重要，鉴于现有专门基准测试之间存在差距。为了弥合这一差距，我们引入了SportQA，这是一个专门设计用于评估LLMs在体育理解方面的新型基准测试。SportQA涵盖了超过70,000个跨三个不同难度级别的多项选择题，每个级别针对体育知识的不同方面，从基本历史事实到复杂的基于情景的推理任务。我们主要利用少样本学习范式辅以“联想链”提示方法对普遍的LLMs进行了彻底评估。我们的结果显示，虽然LLMs在基本的体育知识方面表现出色，但在更复杂的基于情景的推理方面却遇到了困难。

    arXiv:2402.15862v1 Announce Type: new  Abstract: A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-bas
    
[^15]: RNN语言模型归纳偏差的一个理论结果

    A Theoretical Result on the Inductive Bias of RNN Language Models

    [https://arxiv.org/abs/2402.15814](https://arxiv.org/abs/2402.15814)

    RNN语言模型能够有效表示更大类别的语言模型，具有有界堆栈和广义堆栈更新函数，类似于保留固定数量符号记忆并使用简单更新机制的自动机。

    

    最近Hewitt等人（2020）的工作提出了对循环神经网络（RNNs）作为语言模型（LMs）的经验成功可能性的一个解释。 它显示RNNs可以有效地表示在人类语言中普遍存在的有界分层结构。 这表明RNNs的成功可能与它们建模层次结构的能力有关。 然而，对Hewitt等人（2020）构造的更详细检查表明，它不限于分层LMs，这引出了RNNs可以有效表示哪些\emph{其他类型} LMs的问题。 为此，我们概括他们的构造以展示RNNs可以有效表示更大类别的LMs：可以通过带有有界堆栈和广义堆栈更新函数的下推自动机表示的那些。 这类似于一个保留固定数量符号记忆并使用简单更新机制更新记忆的自动机。

    arXiv:2402.15814v1 Announce Type: new  Abstract: Recent work by Hewitt et al. (2020) provides a possible interpretation of the empirical success of recurrent neural networks (RNNs) as language models (LMs).   It shows that RNNs can efficiently represent bounded hierarchical structures that are prevalent in human language.   This suggests that RNNs' success might be linked to their ability to model hierarchy.   However, a closer inspection of Hewitt et al.'s (2020) construction shows that it is not limited to hierarchical LMs, posing the question of what \emph{other classes} of LMs can be efficiently represented by RNNs.   To this end, we generalize their construction to show that RNNs can efficiently represent a larger class of LMs: Those that can be represented by a pushdown automaton with a bounded stack and a generalized stack update function.   This is analogous to an automaton that keeps a memory of a fixed number of symbols and updates the memory with a simple update mechanism.  
    
[^16]: 基于大型语言模型的检索增强生成的因果图发现

    Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models

    [https://arxiv.org/abs/2402.15301](https://arxiv.org/abs/2402.15301)

    利用大型语言模型和检索增强生成技术，提出了一种新方法用于从科学文献中推断因果关系，以解决传统方法受限于数据收集偏见和个体知识的问题。

    

    因果图恢复在因果推断领域至关重要。传统方法通常是基于知识或统计估计，受数据收集偏见和个体关于影响变量之间关系的知识的限制。大型语言模型（LLMs）的进步为解决这些问题提供了机会。我们提出了一种利用大量科学文献中所包含的知识推导一般因果图恢复任务中的因果关系的新方法。该方法利用基于检索增强生成（RAG）的LLMs系统地分析和提取来自广泛研究论文集的相关信息。我们的方法首先从汇总的文献中检索相关文本片段。然后，LLM被用来识别和标记因素之间的潜在关联。最后，我们给出了一个...

    arXiv:2402.15301v1 Announce Type: new  Abstract: Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we giv
    
[^17]: PANDA: 用于增强LLMs领域特定能力的偏好适应方法

    PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs

    [https://arxiv.org/abs/2402.12835](https://arxiv.org/abs/2402.12835)

    PANDA是一种旨在增强LLMs领域特定能力的方法，通过利用专家模型响应偏好的见解，无需微调即可实现显著改进。

    

    大型语言模型（LLMs）在各种自然语言任务中展示出相当大的能力，但它们通常无法达到特定领域最先进模型的性能水平。增强LLMs领域特定能力的一种潜在方法是使用相应的数据集对其进行微调。然而，这种方法既耗费资源又耗时，并且无法应用于封闭源商业LLMs。在本文中，我们提出了一种称为PANDA的偏好适应方法，旨在通过利用专家模型响应偏好的见解来增强LLMs的领域特定能力，而无需进行微调。我们的实验结果显示，PANDA显著提升了LLMs在文本分类和交互式决策任务上的领域特定能力。此外，具有PANDA的LLM甚至超过了专家模型

    arXiv:2402.12835v1 Announce Type: cross  Abstract: While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that
    
[^18]: 通过影响分析进行上下文学习演示选择

    In-Context Learning Demonstration Selection via Influence Analysis

    [https://arxiv.org/abs/2402.11750](https://arxiv.org/abs/2402.11750)

    通过分析训练样本的影响，提出一种名为InfICL的演示选择方法，可以帮助提升上下文学习的泛化性能。

    

    大型语言模型（LLM）展示了其具有上下文学习（ICL）能力，这提供了进行少样本学习的机会，而无需任何梯度更新。尽管具有多重好处，ICL的泛化性能对所选演示敏感。选择用于ICL的有效演示仍然是一个开放的研究挑战。为了解决这一挑战，我们提出了一种名为InfICL的演示选择方法，该方法通过影响函数分析训练样本的影响。鉴别高度有影响力的训练样本可能有助于提升ICL的泛化性能。为了限制InfICL的运行成本，我们仅利用LLM生成样本嵌入，并不执行任何昂贵的微调。我们在多个真实世界数据集上进行实证研究，并展示了我们的InfICL相对于最先进基线方法的优点。

    arXiv:2402.11750v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated their In-Context Learning (ICL) capabilities which provides an opportunity to perform few shot learning without any gradient update. Despite its multiple benefits, ICL generalization performance is sensitive to the selected demonstrations. Selecting effective demonstrations for ICL is still an open research challenge. To address this challenge, we propose a demonstration selection method called InfICL which analyzes influences of training samples through influence functions. Identifying highly influential training samples can potentially aid in uplifting the ICL generalization performance. To limit the running cost of InfICL, we only employ the LLM to generate sample embeddings, and don't perform any costly fine tuning. We perform empirical study on multiple real-world datasets and show merits of our InfICL against state-of-the-art baselines.
    
[^19]: 大型语言模型对意识形态操纵的易感性有多高？

    How Susceptible are Large Language Models to Ideological Manipulation?

    [https://arxiv.org/abs/2402.11725](https://arxiv.org/abs/2402.11725)

    大型语言模型展示出令人担忧的易受意识形态操纵的脆弱性，对极少量意识形态驱动样本的暴露就能显著改变其意识形态，并且能够从一个主题吸收意识形态并泛化到其他不相关主题上。

    

    大型语言模型(LLMs)具有对公众观念和信息互动施加重要影响的潜力。这引发了关于如果这些模型内的意识形态易受操纵可能带来社会影响的担忧。在这项工作中，我们研究了LLMs在学习和泛化意识形态偏见方面的效果。我们的发现揭示了一个令人担忧的脆弱性：仅接触到少量意识形态驱动的样本就会显著改变LLMs的意识形态。值得注意的是，LLMs表现出惊人的能力，能够从一个主题吸收意识形态并将其泛化到甚至不相关的主题上。LLMs的意识形态容易被扭曲的事实强调了恶意行为者故意毒害训练数据或数据注释者无意引入偏见所带来的风险。这也强调了采取强有力措施以减轻这些威胁的迫切性。

    arXiv:2402.11725v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the inf
    
[^20]: 自我反馈的危险：在大型语言模型中自我偏见被放大

    Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models

    [https://arxiv.org/abs/2402.11436](https://arxiv.org/abs/2402.11436)

    大型语言模型存在自我偏见，研究发现通过更大的模型规模和准确评估的外部反馈可以显著减少这种偏见，并提高后续任务的实际表现。

    

    最近的研究表明，自我反馈可以改善大型语言模型在某些任务上的表现，但在其他任务上却会恶化。我们发现这种矛盾是由于大型语言模型对其自身输出的偏见。本文正式定义了大型语言模型的自我偏见——倾向于偏爱自身生成——并使用两个统计量进行了分析。我们在翻译、受限文本生成和数学推理任务上分析了六种大型语言模型。我们发现自我偏见在所有检测的大型语言模型中都普遍存在，跨多种语言和任务。我们的分析表明，虽然自我改进管道提高了模型输出的流畅性和可理解性，但它进一步放大了自我偏见。为了缓解这种偏见，我们发现更大的模型规模和具有准确评估的外部反馈可以显著减少自我改进管道中的偏见，从而实际改善下游任务的性能。

    arXiv:2402.11436v1 Announce Type: cross  Abstract: Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.
    
[^21]: 语言模型反映了谁的情感和道德情感？

    Whose Emotions and Moral Sentiments Do Language Models Reflect?

    [https://arxiv.org/abs/2402.11114](https://arxiv.org/abs/2402.11114)

    该研究探讨了语言模型在情感和道德维度上如何代表不同群体，发现它们与意识形态团体存在显著的不一致性。

    

    语言模型已知更好地代表一些社会群体的观点，这可能会影响它们的性能，特别是在主观任务上，比如内容管理和仇恨言论检测。我们定义了情感对齐的问题，用来衡量语言模型的情感和道德色调如何代表不同群体的情感。通过比较36个语言模型生成的回应的情感与Twitter消息的情感，我们观察到语言模型与两种意识形态团体存在显著不一致。即使在引导语言模型朝着特定意识形态方向发展之后，这种不一致性也仍然存在。

    arXiv:2402.11114v1 Announce Type: new  Abstract: Language models (LMs) are known to represent the perspectives of some social groups better than others, which may impact their performance, especially on subjective tasks such as content moderation and hate speech detection. To explore how LMs represent different perspectives, existing research focused on positional alignment, i.e., how closely the models mimic the opinions and stances of different groups, e.g., liberals or conservatives. However, human communication also encompasses emotional and moral dimensions. We define the problem of affective alignment, which measures how LMs' emotional and moral tone represents those of different groups. By comparing the affect of responses generated by 36 LMs to the affect of Twitter messages, we observe significant misalignment of LMs with both ideological groups. This misalignment is larger than the partisan divide in the U.S. Even after steering the LMs towards specific ideological perspectiv
    
[^22]: 不规则文本中动态基于方面的总结标准： Disordered-DABS基准测试

    Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts

    [https://arxiv.org/abs/2402.10554](https://arxiv.org/abs/2402.10554)

    Disordered-DABS是针对不规则文本中动态基于方面的总结而设计的新基准测试，挑战了现有总结模型的独特性。

    

    方面为基础的总结已经取得了重要进展，尤其是在结构化文本中。然而，总结不规则的大规模文本，比如社交媒体和客户反馈中发现的文本，仍然是一个重大挑战。目前的研究主要针对结构化文本中的预定义方面，忽略了动态和无序环境的复杂性。为了弥补这一差距，我们引入了Disordered-DABS，这是一个新颖的面向动态方面的总结基准测试，专为非结构化文本量身定制。通过调整现有数据集以提高成本效率和可扩展性，我们的综合实验和详细的人类评估表明，Disordered-DABS对当代总结模型提出了独特的挑战，包括GPT-3.5等最先进的语言模型。

    arXiv:2402.10554v1 Announce Type: new  Abstract: Aspect-based summarization has seen significant advancements, especially in structured text. Yet, summarizing disordered, large-scale texts, like those found in social media and customer feedback, remains a significant challenge. Current research largely targets predefined aspects within structured texts, neglecting the complexities of dynamic and disordered environments. Addressing this gap, we introduce Disordered-DABS, a novel benchmark for dynamic aspect-based summarization tailored to unstructured text. Developed by adapting existing datasets for cost-efficiency and scalability, our comprehensive experiments and detailed human evaluations reveal that Disordered-DABS poses unique challenges to contemporary summarization models, including state-of-the-art language models such as GPT-3.5.
    
[^23]: Concept-1K：一种用于实例增量学习的新型基准

    Concept-1K: A Novel Benchmark for Instance Incremental Learning

    [https://arxiv.org/abs/2402.08526](https://arxiv.org/abs/2402.08526)

    我们提出了一种名为Concept-1K的具有挑战性的实例增量学习（IIL）场景和数据集，揭示了十亿参数的PLM仍然遭受灾难性遗忘，影响因素包括模型规模、预训练和缓冲区大小。现有的IL方法和LoRA技术无法满足性能需求。我们的研究为探索和缓解PLM中的遗忘问题提供了新的场景。

    

    增量学习（IL）对于实现神经网络中的人类级智能至关重要。然而，现有的IL场景和数据集无法评估PLM中的遗忘，使人误以为PLM不会遭受灾难性遗忘。为此，我们提出了一种具有挑战性的IL场景，称为实例增量学习（IIL），以及一个支持数量级更大的IL步骤的新数据集Concept-1K。基于对Concept-1K的实验，我们揭示了十亿参数的PLM仍然遭受着灾难性遗忘，并且遗忘受模型规模、预训练和缓冲区大小的影响。此外，现有的IL方法和一种流行的微调技术LoRA都未能达到令人满意的性能。我们的研究为未来研究提供了一个新的场景，探索PLM的灾难性遗忘，并鼓励设计更强大的技术以减轻PLM中的遗忘问题。

    Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs. The data, code and scripts ar
    
[^24]: 基于真实用户查询评估文本到SQL系统的数据模型鲁棒性

    Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries

    [https://arxiv.org/abs/2402.08349](https://arxiv.org/abs/2402.08349)

    本文首次深入评估了在实践中文本到SQL系统的数据模型的鲁棒性，通过基于一个多年的国际项目集中评估，对一个在FIFA World Cup背景下连续运行了9个月的真实部署的FootballDB系统进行了评估。

    

    文本到SQL系统（也称为自然语言到SQL系统）已成为弥合用户能力与基于SQL的数据访问之间差距的越来越流行的解决方案。这些系统将用户的自然语言请求转化为特定数据库的有效SQL语句。最近的基于转换器的语言模型使得文本到SQL系统受益匪浅。然而，虽然这些系统在常常是合成基准数据集上不断取得新的高分，但对于它们在真实世界、现实场景中对不同数据模型的鲁棒性的系统性探索明显缺乏。本文基于一个多年国际项目关于文本到SQL界面的集中评估，提供了对文本到SQL系统在实践中数据模型鲁棒性的首次深度评估。我们的评估基于FootballDB的真实部署，该系统在FIFA World Cup的背景下连续运行了9个月。

    Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA Wor
    
[^25]: LoRA-drop：基于输出评估的高效LoRA参数剪枝

    LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation

    [https://arxiv.org/abs/2402.07721](https://arxiv.org/abs/2402.07721)

    本文提出了LoRA-drop方法，通过分析LoRA输出评估参数的重要性，并且保留重要层的LoRA，其余层共享相同参数。实验结果表明LoRA-drop有很好的效果。

    

    低秩适应（LoRA）为每个层引入辅助参数，以在有限的计算资源下微调预训练模型。但是，当扩展到更大的模型时，仍然面临资源消耗的挑战。先前的研究通过评估不同层的LoRA参数的重要性来采用剪枝技术来解决这个问题。然而，这些努力只分析了参数的特征以评估其重要性。事实上，与参数和数据相关的LoRA的输出是直接影响冻结模型的因素。为此，我们提出了LoRA-drop，通过分析LoRA输出来评估参数的重要性。我们保留重要层的LoRA，而其他层的LoRA共享相同的参数。在NLU和NLG任务上进行了充分的实验，证明了LoRA-drop的有效性。

    Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.
    
[^26]: LLM能够识别毒性吗？结构化毒性调查框架和基于语义的度量

    Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric

    [https://arxiv.org/abs/2402.06900](https://arxiv.org/abs/2402.06900)

    本研究提出了一种基于大型语言模型（LLMs）的自动度量方法，用于识别生成文本中的毒性。通过分析毒性因素和LLMs的内在毒性属性，该方法在测量毒性方面表现出众，比现有指标提升12个百分点。

    

    在开发遵守社会标准的大型语言模型（LLMs）的过程中，识别生成文本中的毒性存在至关重要。现有的大多数毒性度量依赖于在特定毒性数据集上训练的编码模型。然而，这些编码器容易受到分布外的问题的影响，并且依赖于数据集中所假定的毒性定义。本文介绍了一种基于LLMs的自动鲁棒度量，用于区分模型回应是否具有毒性。我们首先分析了毒性因素，然后研究了LLMs的内在毒性属性，以确定它们作为评估器的适用性。随后，我们对评估数据集上的度量指标LLMs As ToxiciTy Evaluators（LATTE）进行了评估。实证结果表明，在不进行训练过程的情况下，我们的度量在测量毒性方面表现出色，F1得分比现有技术指标提高了12个百分点。我们还展示了上游毒性对度量结果的影响。

    In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
    
[^27]: 在流数据上进行高效推理的在线级联学习

    Online Cascade Learning for Efficient Inference over Streams

    [https://arxiv.org/abs/2402.04513](https://arxiv.org/abs/2402.04513)

    这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。

    

    大型语言模型 (LLM) 在回答关于数据流的复杂查询方面具有天然的优势，但是 LLM 推理的高计算成本使得它们在许多任务中不可行。我们提出了在线级联学习，这是首个解决这一挑战的方法。这里的目标是学习一个“级联”模型，从容量较低的模型（如逻辑回归器）开始，到强大的 LLM 结束，并配备一个决定在给定输入上使用哪个模型的推迟策略。我们将在线学习级联的任务公式化为一个模仿学习问题，并为该问题提供了无遗憾算法。在四个基准测试中的实验结果显示，我们的方法在准确性上与 LLM 相当，同时将推理成本削减了多达 90%，突显了它在流处理中的效能和适应能力。

    Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
    
[^28]: 检索以解释：基于语言模型的证据驱动预测

    Retrieve to Explain: Evidence-driven Predictions with Language Models

    [https://arxiv.org/abs/2402.04068](https://arxiv.org/abs/2402.04068)

    检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。

    

    机器学习模型，尤其是语言模型，往往难以深入分析。黑盒模型可能掩盖了模型训练中的问题和有害偏差。对于人机协作过程来说，不透明的预测可能导致缺乏信任，限制模型的影响，即使模型的性能很好。为了解决这些问题，我们引入了检索以解释（Retrieve to Explain，简称R2E）。R2E是一种基于检索的语言模型，根据文档语料库中的证据，使用Shapley值来确定证据对最终预测的相对重要性，并根据自然语言模板将结构化数据纳入其中。R2E能够在不重新训练的情况下适应新的证据，并且能够通过模板化将结构化数据纳入到自然语言中。我们在通过分析已发表的科学文献进行药物靶点鉴定的实际案例中进行了评估，结果显示该模型在预测临床试验结果方面优于行业标准的基因学方法。

    Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
    
[^29]: 为社交感知的谈判对话开发辅助大型语言模型代理

    Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues

    [https://arxiv.org/abs/2402.01737](https://arxiv.org/abs/2402.01737)

    本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性。

    

    本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们通过让两个大型语言模型（LLM）扮演每次对话中的两名谈判者来模拟现实世界谈判。第三个LLM充当修正代理，重新编写违反规范的话语以改善谈判结果。由于这是一个新颖的任务，不存在手动构建的数据。为解决这个限制，我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性，即产品销售、房价和薪资谈判。源代码和生成的数据集将在接受后公开。

    In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.
    
[^30]: 正式-LLM：将形式语言和自然语言集成于可控的LLM智能体中

    Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

    [https://arxiv.org/abs/2402.00798](https://arxiv.org/abs/2402.00798)

    本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。

    

    最近，对于大型语言模型（LLMs）的进展使得人工智能智能体能够自动生成和执行解决复杂任务的多步计划。然而，由于LLM的内容生成过程几乎无法控制，当前的LLM智能体经常生成无效或不可执行的计划，这损害了生成计划的性能并破坏了用户对LLM智能体的信任。为应对这个问题，本文提出了一种新颖的“正式-LLM”框架，用于LLM智能体，通过将自然语言的表达力和形式语言的精确性进行整合。具体而言，该框架允许人类用户将他们对计划过程的要求或约束表达为自动机。然后，在自动机的监督下，使用基于堆栈的LLM计划生成过程来确保生成的计划满足约束条件，从而使计划过程可控。我们在基准任务和实际的真实任务上进行了实验，并且obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.

    Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
    
[^31]: Transformer的好处：在非结构化数据的线性回归任务中的上下文学习

    Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data

    [https://arxiv.org/abs/2402.00743](https://arxiv.org/abs/2402.00743)

    本研究通过线性回归任务的实验研究了Transformer在非结构化数据中的上下文学习能力，并解释了其中的关键组件。

    

    实践中观察到，基于Transformer的模型在推理阶段能够学习上下文中的概念。现有的文献，例如\citet{zhang2023trained,huang2023context}对这种上下文学习能力提供了理论解释，但是他们假设每个样本的输入$x_i$和输出$y_i$都被嵌入到相同的令牌中（即结构化数据）。然而，在现实中，它们呈现为两个令牌（即非结构化数据\cite{wibisono2023role}）。在这种情况下，本文进行了线性回归任务的实验，研究了Transformer架构的好处，并提供了一些相应的理论直觉，解释了为什么Transformer可以从非结构化数据中学习。我们研究了在Transformer中起到上下文学习作用的确切组件。特别地，我们观察到（1）带有两层softmax（自我）注意力和前瞻性注意力掩码的Transformer可以从提示中学习，如果$y_i$在令牌中。

    In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token n
    
[^32]: Gemini：一系列高性能多模态模型

    Gemini: A Family of Highly Capable Multimodal Models

    [https://arxiv.org/abs/2312.11805](https://arxiv.org/abs/2312.11805)

    Gemini家族是一系列在图像、音频、视频和文本理解方面表现出色的多模态模型，其中最具能力的Gemini Ultra模型在30个基准测试中推进了技术前沿，并改进了所有20个多模态基准测试的技术状态。

    

    本报告介绍了一种新的多模态模型系列Gemini，展示出在图像、音频、视频和文本理解方面的显著能力。Gemini系列包括Ultra、Pro和Nano尺寸，适用于从复杂推理任务到设备内存受限应用的各种应用场景。在广泛的基准测试中，我们最具能力的Gemini Ultra模型在32个基准测试中的30个中推进了技术前沿 - 显著地是第一个在被广泛研究的考试基准测试MMLU上实现人类专家水平表现的模型，并在我们研究的每一个20个多模态基准测试中改进了技术前沿。我们相信Gemini系列在跨模态推理和语言理解方面的新能力将能够支持各种用例。我们讨论了负责任地向用户提供Gemini模型的训练后和部署方法，包括使用服务。

    arXiv:2312.11805v2 Announce Type: replace-cross  Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services includi
    
[^33]: 通过检索和自我反思改善医疗推理能力的检索增强型大型语言模型

    Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])

    [http://arxiv.org/abs/2401.15269](http://arxiv.org/abs/2401.15269)

    本论文介绍了一种名为Self-BioRAG的框架，通过使用检索和自我反思的方法，提高了医疗推理的能力。该框架专注于生成解释、检索领域特定文档以及对生成的响应进行自我反思。

    

    最近的专有大型语言模型（LLMs），例如GPT-4，在生物医学领域中解决了从多项选择题到长篇生成等多样化挑战的里程碑。为了解决LLMs编码知识无法处理的挑战，已经开发了各种检索增强生成（RAG）方法，通过从知识语料库中搜索文档并无条件或有选择地将其附加到LLMs的输入来进行生成。然而，将现有方法应用于不同领域特定问题时，出现了泛化能力差的问题，导致获取不正确的文档或做出不准确的判断。在本文中，我们介绍了一种可靠的医学文本框架Self-BioRAG，专门用于生成解释、检索领域特定文档和自我反思生成的响应。我们使用了84k个经过过滤的生物医学指令集来训练Self-BioRAG，它具备评估自己的基因

    Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
    
[^34]: Taiyi-Diffusion-XL: 借助大型视觉语言模型支持推进双语文本到图像生成

    Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support. (arXiv:2401.14688v1 [cs.CL])

    [http://arxiv.org/abs/2401.14688](http://arxiv.org/abs/2401.14688)

    Taiyi-Diffusion-XL是一个中英双语文本到图像生成模型，通过对CLIP和Stable-Diffusion-XL的能力进行扩展，并通过双语连续预训练来实现。模型通过将常用的汉字整合到CLIP的分词器和嵌入层中，以及使用大型视觉语言模型丰富文本提示，提供了更好的图像标题和高质量的视觉效果。实验证明，该模型在双语图像-文本检索中表现出色。

    

    最近在文本到图像模型中的进展显著提升了图像生成能力，然而在双语或中文语言支持方面仍存在明显的开源模型缺口。为了解决这个需求，我们提出了Taiyi-Diffusion-XL，这是一个新的中英双语文本到图像模型，通过对CLIP和Stable-Diffusion-XL能力的扩展，并通过双语连续预训练的过程来开发。这种方法包括通过将最常用的汉字整合到CLIP的分词器和嵌入层中来扩展词汇量的高效方法，同时还加入了绝对位置编码扩展。此外，我们通过大型视觉语言模型来丰富文本提示，从而提供更好的图像标题和更高的视觉质量。这些增强措施随后应用于下游的文本到图像模型。我们的实证结果表明，开发的CLIP模型在双语图像-文本检索方面表现出色。此外，双语预训练过程中的整合使该模型在中英文场景下具有均衡的表现。

    Recent advancements in text-to-image models have significantly enhanced image generation capabilities, yet a notable gap of open-source models persists in bilingual or Chinese language support. To address this need, we present Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model which is developed by extending the capabilities of CLIP and Stable-Diffusion-XL through a process of bilingual continuous pre-training. This approach includes the efficient expansion of vocabulary by integrating the most frequently used Chinese characters into CLIP's tokenizer and embedding layers, coupled with an absolute position encoding expansion. Additionally, we enrich text prompts by large vision-language model, leading to better images captions and possess higher visual quality. These enhancements are subsequently applied to downstream text-to-image models. Our empirical results indicate that the developed CLIP model excels in bilingual image-text retrieval.Furthermore, the bilin
    
[^35]: 朝着目标导向的大型语言模型提示方法：一项调查

    Towards Goal-oriented Large Language Model Prompting: A Survey. (arXiv:2401.14043v1 [cs.CL])

    [http://arxiv.org/abs/2401.14043](http://arxiv.org/abs/2401.14043)

    本文调查了大型语言模型(LLM)中目标导向提示工程的重要性。通过对35个代表性研究的回顾，我们发现引导LLM遵循人类的逻辑思维的目标导向提示公式显著提高了LLM的性能。我们还提出了一个新的分类体系，并总结了十个适用任务来展示我们框架的广泛适用性。同时，我们提出了四个未来的方向，以推动目标导向提示工程的进一步发展。

    

    大型语言模型(LLM)在各种下游任务中显示出卓越的性能，而提示工程在优化LLM性能中起着关键作用。本文旨在强调设计提示的限制，同时保持人类追求LLM像人类思考的人类学假设。通过对35个代表性研究的回顾，我们展示了目标导向提示公式的重要性，该公式指导LLM遵循人类的逻辑思维，显著提高了LLM的性能。此外，我们引入了一个新的分类体系，将目标导向提示方法分为五个相互关联的阶段，并通过总结十个适用任务来展示我们框架的广泛适用性。最后，我们提出了四个未来的方向，希望进一步强调和推动目标导向提示工程。

    Large Language Models (LLMs) have shown prominent performance in various downstream tasks in which prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not as an overview of current prompt engineering methods, aims to highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans. From our review of 35 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework by summarizing ten applicable tasks. With four future directions proposed, we hope to further emphasize and promote goal-oriented prompt engineering.
    
[^36]: LLM指令微调中的提示权重实验

    Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])

    [http://arxiv.org/abs/2401.13586](http://arxiv.org/abs/2401.13586)

    LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。

    

    我们进行了一项小型研究，分析了提示词标记分类损失加权（PLW）如何影响在指令任务上进行微调的7B大小的LLaMA模型的性能。我们使用多个指令数据集重现了斯坦福大学的Alpaca实验，其中包括LLaMA 1和LLaMA 2。我们发现，在我们的短提示完成数据集上微调的模型与PLW之间存在负二次关系，而在长提示完成数据集上微调的模型不受PLW的影响。

    We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
    
[^37]: 空间-时间大语言模型用于交通预测

    Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])

    [http://arxiv.org/abs/2401.10134](http://arxiv.org/abs/2401.10134)

    本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测，通过参数扩展和预训练来提高预测准确性，并利用空间-时间嵌入模块学习标记的空间位置和全局时间表示。

    

    交通预测是智能交通系统的关键组成部分，它通过使用历史数据来预测特定位置的未来交通情况。尽管现有的交通预测模型通常强调开发复杂的神经网络结构，但它们的准确性并未相应提高。最近，大型语言模型（LLMs）在时间序列分析方面显示出了出色的能力。与现有模型不同，LLMs主要通过参数扩展和广泛的预训练来进步，同时保持其基本结构。本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测。具体而言，ST-LLM将每个位置的时间步长定义为标记，并结合空间-时间嵌入模块来学习标记的空间位置和全局时间表示。然后，这些表示被融合以为每个标记提供统一的空间和时间信息。

    Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
    
[^38]: Transformers 是多状态的 RNNs

    Transformers are Multi-State RNNs. (arXiv:2401.06104v1 [cs.CL])

    [http://arxiv.org/abs/2401.06104](http://arxiv.org/abs/2401.06104)

    本文研究发现，只使用解码器的 Transformers 可以被视为无限多状态的 RNNs，并且可以通过固定隐藏状态的大小来转换为有限多状态的 RNNs。我们提出了一种新的转换策略 TOVA，在多个长距离任务中表现优于其他基准策略，并且与完整模型几乎持平，在某些情况下仅使用原始缓存大小的 $\frac{1}{8}$。这些结果表明，Transformer 解码器 LLMs 在实践中通常表现为 RNNs。

    

    在这项工作中，我们展示了只使用解码器的 Transformers 实际上可以被概念化为无限多状态的 RNNs，即具有无限隐藏状态尺寸的 RNNs 变种。我们进一步展示了预训练的 Transformers 可以通过固定其隐藏状态大小来转换为有限多状态的 RNNs。我们观察到几种现有的 Transformers 缓存压缩技术可以被看作是这种转换策略，并引入了一种新的策略 TOVA，相比于这些策略更简单。我们在几种长距离任务上的实验证明了 TOVA 优于所有其他基准策略，同时与完整（无限）模型几乎不相上下，并且在某些情况下仅使用原始缓存大小的 $\frac{1}{8}$。我们的结果表明，Transformer 解码器 LLMs 在实践中通常表现为 RNNs。

    Transformers are considered conceptually different compared to the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as infinite multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that pretrained transformers can be converted into $\textit{finite}$ multi-state RNNs by fixing the size of their hidden state. We observe that several existing transformers cache compression techniques can be framed as such conversion policies, and introduce a novel policy, TOVA, which is simpler compared to these policies. Our experiments with several long range tasks indicate that TOVA outperforms all other baseline policies, while being nearly on par with the full (infinite) model, and using in some cases only $\frac{1}{8}$ of the original cache size. Our results indicate that transformer decoder LLMs often behave in practice as RNNs. They also lay
    
[^39]: 使用多方位AI反馈减轻情感支持对话中的无益性

    Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback. (arXiv:2401.05928v1 [cs.CL])

    [http://arxiv.org/abs/2401.05928](http://arxiv.org/abs/2401.05928)

    Muffin是一个新型的模型无关框架，用于减轻情感支持对话中无益性的问题。这个框架在生成支持性回复时考虑到多个因素，并通过多方位的AI反馈来训练模型，以避免生成无益的回复。

    

    情感支持对话系统旨在减轻用户的情感困扰并帮助他们解决挑战。为了生成支持性回复，必须考虑到多个因素，如共情、支持策略和回复连贯性，这些在之前的方法中已经得到验证。然而，之前的模型偶尔会生成无益的回复，这些回复意图提供支持，但却产生适得其反的效果。根据心理学和沟通理论，虽然只是单一因素的表现不佳可能会导致回复无益。从模型训练的角度来看，由于这些模型在训练阶段没有接触到无益的回复，它们无法判断它们生成的标记是否会导致推理过程中的无益回复。为了解决这个问题，我们引入了一个名为多方位AI反馈减轻情感支持（Muffin）的新型模型无关框架。

    An emotional support conversation system aims to alleviate users' emotional distress and assist them in addressing their challenges. To generate supportive responses, it is critical to consider multiple factors such as empathy, support strategies, and response coherence, as established in prior methods. Nonetheless, previous models occasionally generate unhelpful responses, which intend to provide support but display counterproductive effects. According to psychology and communication theories, poor performance in just one contributing factor might cause a response to be unhelpful. From the model training perspective, since these models have not been exposed to unhelpful responses during their training phase, they are unable to distinguish if the tokens they generate might result in unhelpful responses during inference. To address this issue, we introduce a novel model-agnostic framework named mitigating unhelpfulness with multifaceted AI feedback for emotional support (Muffin). Specif
    
[^40]: 一个带有嵌入式历时语义变化模型的论文与一个关于古希腊的案例研究

    An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])

    [http://arxiv.org/abs/2311.00541](http://arxiv.org/abs/2311.00541)

    本论文介绍了一个嵌入式历时语义变化模型（EDiSC），结合了词嵌入和DiSC模型，通过无监督学习分析古希腊文本中目标词汇的意义变化。实验证明EDiSC具有优越的性能。

    

    词汇的意义随着时间的推移而变化，词义在这个过程中会演变、出现或消失。对于古代语言来说，由于语料库通常较小、稀疏且嘈杂，准确建模这种变化变得具有挑战性，因此对于意义变化估计的不确定性进行量化变得重要。GASC和DiSC是现有的生成模型，已经被用来分析古希腊文本语料库中目标词汇的意义变化，使用了无监督学习并没有借助任何预训练的帮助。这些模型将给定目标词汇（如"kosmos"，意为装饰、秩序或世界）的意义表示为上下文词汇的分布，并将意义的普遍性表示为意义的分布。这些模型使用马尔科夫链蒙特卡洛方法进行拟合，以测量这些表示中的时间变化。在本文中，我们介绍了EDiSC，这是DiSC的嵌入版本，它将词嵌入与DiSC相结合，提供了更优秀的模型性能。我们通过实验证明，EDiSC提供了改进的性能。

    Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small, sparse and noisy, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC and DiSC are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as "kosmos" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using MCMC methods to measure temporal changes in these representations. In this paper, we introduce EDiSC, an embedded version of DiSC, which combines word embeddings with DiSC to provide superior model performance. We show empirically that EDiSC offers improved p
    
[^41]: 激发大型语言模型中的提示工程潜力：一项综述

    Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14735](http://arxiv.org/abs/2310.14735)

    这篇论文解释了提示工程在释放大型语言模型能力方面的关键作用，探讨了不同的提示方法以及外部插件如何协助减少机器幻想，并指出了未来研究方向的重要性。

    

    本文深入探讨了提示工程在释放大型语言模型（LLM）能力方面的关键作用。提示工程是为LLM构建输入文本的过程，是优化LLM有效性的重要技术。本综述阐明了提示工程的基本原理，如角色提示、一次性提示和少量提示，以及更高级的方法，如思维链和思维树提示。本文还阐述了外部插件如何协助此任务，并通过检索外部知识来减少机器幻想。随后，我们勾勒了提示工程研究的前景方向，强调了对结构和代理在人工智能生成内容（AIGC）工具中的作用的深入理解的必要性。我们讨论了如何从不同角度和使用不同的方法评估提示方法的有效性。最后，我们提出了展望未来的研究方向。

    This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we
    
[^42]: 语言模型作为零-shot轨迹生成器

    Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])

    [http://arxiv.org/abs/2310.11604](http://arxiv.org/abs/2310.11604)

    本文研究了使用大型语言模型（LLMs）作为零-shot轨迹生成器的可能性。通过给予LLM物体检测和分割视觉模型的访问权限，研究人员发现LLMs能够直接预测操作技能中的末端执行器姿态序列，并在26个真实世界的语言任务中取得了良好效果。这一研究突破了对LLMs在机器人技术中的限制，揭示了LLMs确实具有对操作任务的理解能力。

    

    近期研究表明，大型语言模型（LLMs）在给予低级技能选择时能够作为机器人的高级规划器。然而，通常认为LLMs不具备足够的知识来用于低级轨迹生成。在本研究中，我们详细探讨了这种假设，并调查了当给予LLM（GPT-4）仅能访问物体检测和分割视觉模型时，它能否直接预测一系列密集的末端执行器姿态用于操作技能。我们研究了一个单一的任务不可知提示，没有任何上下文示例、运动原语或外部轨迹优化器，它在26个真实世界的基于语言的任务中的表现，如“打开瓶盖”和“用海绵擦拭盘子”，以及我们调查了这个提示中哪些设计选择最有效。我们的结论突破了对LLMs在机器人技术上的限制，并首次揭示了LLMs确实具有对操作任务的理解能力。

    Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o
    
[^43]: 从语言模型中识别和减轻隐私风险：一项调查

    Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey. (arXiv:2310.01424v1 [cs.CL])

    [http://arxiv.org/abs/2310.01424](http://arxiv.org/abs/2310.01424)

    这项调查对语言模型（LMs）中的隐私风险进行了研究和减轻措施的探讨，通过分类法和调查现有攻击，提出了关键趋势，并讨论现有的减轻策略的优势和局限性，指出关键差距和未来工作方向。

    

    语言模型（LMs）的快速发展使其被广泛采用于许多领域。除了潜在的好处外，这些模型还带来了一系列风险，包括隐私风险。尤其是随着LMs规模的增加，它们对训练数据的记忆潜力增加，从而导致泄露私人信息的风险。随着LMs的日益普及，我们必须了解这些隐私风险以及如何减轻它们。为了帮助研究人员和决策者了解LM隐私攻击和减轻措施的知识状况，包括需要更多工作的领域，我们介绍了第一份关于LM隐私的技术调查。我们（i）确定了攻击在LMs上存在的显著维度的分类法，（ii）调查现有攻击并使用我们的分类法来突出主要趋势，（iii）讨论现有的减轻策略，突出其优势和局限性，识别关键差距，展示开放问题和建议未来工作方向。

    Rapid advancements in language models (LMs) have led to their adoption across many sectors. Alongside the potential benefits, such models present a range of risks, including around privacy. In particular, as LMs have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. As LMs become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on LM privacy. We (i) identify a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and are
    
[^44]: 让我来教你：语言模型的反馈教育基础

    Let Me Teach You: Pedagogical Foundations of Feedback for Language Models. (arXiv:2307.00279v1 [cs.CL])

    [http://arxiv.org/abs/2307.00279](http://arxiv.org/abs/2307.00279)

    这篇观点文章介绍了一个基于教育学理念的反馈框架FELT，用于对大型语言模型进行反馈，以提高模型与人类偏好的一致性。该框架不仅简化了现有的手工设计反馈，还为NLF研究开辟了新方向。

    

    自然语言反馈（NLF）是将大型语言模型（LLMs）与人类偏好对齐的一个越来越受欢迎的途径。尽管NLF可以传达丰富多样的信息，但往往是手工设计的和随意的。在不同的世界中，教育学研究长期以来建立了几种有效的反馈模型。在这篇观点文章中，我们汇编了来自教育学的思想，引入了一种名为FELT的LLMs反馈框架，概述了反馈空间的各种特征以及基于这些变量的反馈内容分类法。我们的分类法不仅提供了对反馈空间的一般映射，还提供了教育学确定的离散类别，使我们能够从经验上证明不同反馈类型对修订生成的影响。除了简化现有的NLF设计，FELT还为NLF研究带来了新的未开发的方向。我们将我们的分类法提供给社区，为映射我们的类别提供指南和示例。

    Natural Language Feedback (NLF) is an increasingly popular avenue to align Large Language Models (LLMs) to human preferences. Despite the richness and diversity of the information it can convey, NLF is often hand-designed and arbitrary. In a different world, research in pedagogy has long established several effective feedback models. In this opinion piece, we compile ideas from pedagogy to introduce FELT, a feedback framework for LLMs that outlines the various characteristics of the feedback space, and a feedback content taxonomy based on these variables. Our taxonomy offers both a general mapping of the feedback space, as well as pedagogy-established discrete categories, allowing us to empirically demonstrate the impact of different feedback types on revised generations. In addition to streamlining existing NLF designs, FELT also brings out new, unexplored directions for research in NLF. We make our taxonomy available to the community, providing guides and examples for mapping our cat
    
[^45]: 通过框嵌入和概率评分器完成分类法

    Taxonomy Completion with Probabilistic Scorer via Box Embedding. (arXiv:2305.11004v1 [cs.CL])

    [http://arxiv.org/abs/2305.11004](http://arxiv.org/abs/2305.11004)

    本文提出了一种新方法TaxBox，该方法将分类法概念映射到框嵌入中，并使用两个概率评分器来进行概念附加和插入，避免使用伪叶。实验表明，在二个基准数据集上，TaxBox在准确性和训练效率方面显著优于现有的方法。

    

    最近，分类法的完善任务--自动利用新的概念丰富现有分类法--已经引起了广泛的兴趣。早期的研究引入了复杂模块、外部信息和伪叶来丰富表示并统一附加和插入的匹配过程。虽然它们已经取得了良好的性能，但这些介绍可能会在训练和评分过程中带来噪音和不公平性。在本文中，我们提出了TaxBox，一种新颖的用于完成分类法的框架，它将分类法概念映射到框嵌入中，并使用两个概率评分器来进行概念附加和插入，避免使用伪叶。具体而言，TaxBox由三个组件组成：（1）图聚合模块，以利用分类法的结构信息和两个轻量级解码器，将特征映射到框嵌入，并捕捉概念之间的复杂关系；（2）两个概率评分器，分别对应附加和插入任务，设计了一种原则性方法来减轻误导信息的影响；（3）一种联合训练模型和优化两个评分器的训练算法。我们在两个基准数据集上的实验表明，TaxBox在准确性和训练效率方面显著优于现有的状态-of-the-art方法。

    Taxonomy completion, a task aimed at automatically enriching an existing taxonomy with new concepts, has gained significant interest in recent years. Previous works have introduced complex modules, external information, and pseudo-leaves to enrich the representation and unify the matching process of attachment and insertion. While they have achieved good performance, these introductions may have brought noise and unfairness during training and scoring. In this paper, we present TaxBox, a novel framework for taxonomy completion that maps taxonomy concepts to box embeddings and employs two probabilistic scorers for concept attachment and insertion, avoiding the need for pseudo-leaves. Specifically, TaxBox consists of three components: (1) a graph aggregation module to leverage the structural information of the taxonomy and two lightweight decoders that map features to box embedding and capture complex relationships between concepts; (2) two probabilistic scorers that correspond to attach
    
[^46]: ChatLog: 记录和分析ChatGPT随时间的变化

    ChatLog: Recording and Analyzing ChatGPT Across Time. (arXiv:2304.14106v1 [cs.CL])

    [http://arxiv.org/abs/2304.14106](http://arxiv.org/abs/2304.14106)

    ChatLog是一个分析ChatGPT随时间变化的数据集，通过提取ChatGPT知识和语言特征发现了一些稳定的特征，提高了RoBERTa-based detector在新版本ChatGPT上的鲁棒性。

    

    尽管有大量关于在自然语言理解和生成任务中评估ChatGPT的研究，但鲜有研究调查ChatGPT的行为如何随时间变化。在本文中，我们收集了一个粗到细的时间数据集，称为ChatLog，由两个部分组成，每月和每天更新：ChatLog-Monthly是一个数据集，包括每个月收集的38,730个问题-回答对，其中包括推理和分类任务的问题。另一方面，ChatLog-Daily包括ChatGPT每天对1000个相同问题的长篇回答。我们进行全面的自动和人工评估，以提供ChatGPT进化模式存在的证据。我们进一步通过提取其知识和语言特征分析了ChatGPT随时间不变的特征。我们发现一些稳定的特征，以提高基于RoBERTa的检测器在新版本的ChatGPT上的鲁棒性。我们将继续维护我们的数据集以及随时间的分析。

    While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our p
    
[^47]: 探究指代消解模型推广失败的原因

    Investigating Failures to Generalize for Coreference Resolution Models. (arXiv:2303.09092v1 [cs.CL])

    [http://arxiv.org/abs/2303.09092](http://arxiv.org/abs/2303.09092)

    本文研究了不同数据集上指代消解模型的表现，并发现模型的表现可能会受到数据集不同的操作化方式的影响，强调了在不同数据集上评估指代消解模型的重要性。

    

    指代消解模型通常会在多个数据集上进行评估。然而，数据集在如何实现指代消解方面（即理论概念在数据集中的操作化方式）上存在差异，这是由于选择语料库和注释指南等因素所致。本文旨在调查当前指代消解模型的错误程度与数据集之间的实现差异之间的关联程度（OntoNotes、PreCo和Winogrande）。具体而言，我们将模型性能分为多个类别，对应于多种指代，包括一般性提及、复合修饰符和连系谓词等。这种分类有助于我们调查最先进的模型在跨越不同指代类型的泛化能力方面可能会出现哪些差异。例如，在我们的实验中，在OntoNotes上训练的模型在PreCo中一般性提及和连系谓词上表现不佳。我们的发现强调了在多样化数据集和指代消解操作化方面评估指代消解模型的重要性。

    Coreference resolution models are often evaluated on multiple datasets. Datasets vary, however, in how coreference is realized -- i.e., how the theoretical concept of coreference is operationalized in the dataset -- due to factors such as the choice of corpora and annotation guidelines. We investigate the extent to which errors of current coreference resolution models are associated with existing differences in operationalization across datasets (OntoNotes, PreCo, and Winogrande). Specifically, we distinguish between and break down model performance into categories corresponding to several types of coreference, including coreferring generic mentions, compound modifiers, and copula predicates, among others. This break down helps us investigate how state-of-the-art models might vary in their ability to generalize across different coreference types. In our experiments, for example, models trained on OntoNotes perform poorly on generic mentions and copula predicates in PreCo. Our findings 
    
[^48]: 关于上下文学习的综述

    A Survey on In-context Learning. (arXiv:2301.00234v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.00234](http://arxiv.org/abs/2301.00234)

    本文调查和总结了上下文学习(ICL)的进展和挑战，ICL已成为自然语言处理(NLP)的新范式，探索ICL以评估和推广大型语言模型(LLM)的能力已成为一种新趋势。本文提出了ICL的正式定义，并总结了高级技术，最后讨论了ICL的挑战以及进一步研究的潜在方向。

    

    随着大型语言模型（LLM）的能力不断增强，上下文学习（ICL）已成为自然语言处理（NLP）的新范式，在其中LLM仅基于加入少量示例的上下文进行预测。探索ICL以评估和推广LLM的能力已成为一种新趋势。本文旨在调查和总结ICL的进展和挑战。我们首先提出ICL的正式定义，并澄清其与相关研究的关系。然后，我们组织和讨论高级技术，包括训练策略、演示设计策略以及相关分析。最后，我们讨论了ICL的挑战，并提供了进一步研究的潜在方向。我们希望我们的工作可以鼓励更多的研究，揭示ICL的工作原理并改进ICL。

    With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.
    

