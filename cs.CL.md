# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Overthinking the Truth: Understanding how Language Models Process False Demonstrations.](http://arxiv.org/abs/2307.09476) | 该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。 |
| [^2] | [ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning.](http://arxiv.org/abs/2307.09474) | 本研究提出了一种精确的指示方法，利用点和框等多样化的引用表示方法来引用特殊区域，从而使得多模态大型语言模型（MLLMs）能够实现更精细的交互。基于此，我们提出了ChatSpot，一个统一的多模态大型语言模型，支持多种形式的互动，提供更灵活和无缝的交互体验。 |
| [^3] | [A comparative analysis of SR-GAN models.](http://arxiv.org/abs/2307.09456) | 本研究比较分析了多个最先进的SR-GAN模型，结果发现EDSR模型在保持视觉质量的同时显著提高了输入图像的分辨率，具有较高的PSNR和SSIM值，并且返回高质量的OCR结果，这表明EDSR是一种有效的超分辨率方法。 |
| [^4] | [Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers.](http://arxiv.org/abs/2307.09455) | 本文提出了一种名为伪异常暴露（POE）的简单而有效的方法，通过顺序屏蔽与内分布类相关的令牌构建替代的外分布数据集，用于检测未知分布的样本。 |
| [^5] | [Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation.](http://arxiv.org/abs/2307.09416) | 本文介绍了一种新颖的自动化方法ViCE，通过结合大型语言模型和视觉问答，模拟人类认知过程来评估生成图像与提示之间的一致性和质量。 |
| [^6] | [How do software citation formats evolve over time? A longitudinal analysis of R programming language packages.](http://arxiv.org/abs/2307.09390) | 本研究通过对2021年和2022年收集的R语言包的引用格式的纵向数据集进行比较和分析，揭示了软件引用格式如何随时间演变。研究发现软件的引用存在不一致性，并且引用随时间而变化。这一发现对于理解科学研究中软件引用的重要性和演变具有重要意义。 |
| [^7] | [Zero-shot Query Reformulation for Conversational Search.](http://arxiv.org/abs/2307.09384) | 提出了一种零样本查询重构（ZeQR）框架，通过利用机器阅读理解任务的语言模型来解决对话搜索中的数据稀疏性、解释性不足和歧义的问题。 |
| [^8] | [Adapting an ASR Foundation Model for Spoken Language Assessment.](http://arxiv.org/abs/2307.09378) | 本论文主要针对ASR基础模型Whisper的输出进行了详细分析，并提出了两种解决方案：微调和软提示微调。实验结果表明，可以有效改变Whisper的解码行为，生成候选人实际说出的单词。 |
| [^9] | [Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media.](http://arxiv.org/abs/2307.09312) | 多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。 |
| [^10] | [Llama 2: Open Foundation and Fine-Tuned Chat Models.](http://arxiv.org/abs/2307.09288) | Llama 2是一个优化的聊天模型，通过fine-tuned技术和安全改进，表现优于开源模型，并可作为闭源模型的替代选择。 |
| [^11] | [Improving Text Semantic Similarity Modeling through a 3D Siamese Network.](http://arxiv.org/abs/2307.09274) | 通过引入三维连体网络，我们改进了文本语义相似性建模方法，从而更好地保留了层次化的语义信息，并提供了更丰富的结构条件来进行全面的下游建模策略概览。 |
| [^12] | [Linearized Relative Positional Encoding.](http://arxiv.org/abs/2307.09270) | 这项工作提出了一种线性化相对位置编码（LRPE）家族，通过规范形式和酉变换推导出了一种有原则的框架，该框架可用于开发保留线性时空复杂度的新的相对位置编码方法，并为各种应用提供有效的编码。 |
| [^13] | [Text vectorization via transformer-based language models and n-gram perplexities.](http://arxiv.org/abs/2307.09255) | 该研究提出了一种基于n-gram困惑度的简单算法，用于计算文本向量化。这种方法可以解决在计算标量困惑度时由于个别标记的不太可能性导致概率的降低的问题，并且能够保留文本中每个标记的相对困惑度信息。 |
| [^14] | [PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models.](http://arxiv.org/abs/2307.09254) | 本文提出了一种使用神经网络来量化生成式语言模型不确定性的PAC神经预测集学习方法，通过在多种语言数据集和模型上的实验证明，相比于标准基准方法，我们的方法平均提高了63％的量化不确定性。 |
| [^15] | [UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data.](http://arxiv.org/abs/2307.09249) | UniTabE是一种面向异构表格数据的统一预训练表格编码器，能够处理不同表格结构的挑战，并具有对多样化下游应用的适应性。 |
| [^16] | [Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models.](http://arxiv.org/abs/2307.09209) | 该论文分析了情感分析和毒性检测模型，以探测对残障人士的明显偏见。研究使用偏见识别框架对社交媒体平台的对话进行了分析，并创建了一个测试语料库来量化明显的残障偏见。研究发现，所研究的模型均存在显著的偏见。 |
| [^17] | [Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications.](http://arxiv.org/abs/2307.09162) | 本研究分析了大型语言模型中的性别偏见，以GPT-2和GPT-3.5为例，通过全面的文献综述和深入的定量分析揭示了存在的性别化词语关联、语言使用和偏见叙述，并探讨了性别偏见可能对社会认知产生的伦理影响。 |
| [^18] | [Attention over pre-trained Sentence Embeddings for Long Document Classification.](http://arxiv.org/abs/2307.09084) | 利用预训练句子嵌入和小注意力层进行长文本分类，取得了竞争性的结果。 |
| [^19] | [Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words.](http://arxiv.org/abs/2307.09059) | 本研究提出了一个新的框架，通过探索文本中的文字的力量，实现了准确地将抽象的文本描述映射到具体的图像，从而实现了文本到图像的人物检索。 |
| [^20] | [Towards a Neural Era in Dialogue Management for Collaboration: A Literature Survey.](http://arxiv.org/abs/2307.09021) | 本文回顾了协作对话系统中对话管理的演变，并分析了应用神经网络方法进行协作对话管理的最新工作，旨在为未来在协作对话管理领域的进展提供基础背景。 |
| [^21] | [Exploring acceptance of autonomous vehicle policies using KeyBERT and SNA: Targeting engineering students.](http://arxiv.org/abs/2307.09014) | 本研究利用KeyBERT和SNA方法，探索工程学生对自动驾驶车辆政策的接受度。通过文本挖掘分析研究生的意见，从而填补了终端用户对这些政策接受度的分析空白。 |
| [^22] | [How is ChatGPT's behavior changing over time?.](http://arxiv.org/abs/2307.09009) | 本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。 |
| [^23] | [On the (In)Effectiveness of Large Language Models for Chinese Text Correction.](http://arxiv.org/abs/2307.09007) | 本文研究了大语言模型在中文文本纠错任务中的效果，并发现ChatGPT在中文语法错误纠正和拼写检查方面的性能表现并不理想。 |
| [^24] | [Mitigating Label Bias via Decoupled Confident Learning.](http://arxiv.org/abs/2307.08945) | 这项研究提出了一种名为DeCoLe的修剪方法，用于减轻标签偏见问题。研究在合成数据集和仇恨言论检测领域取得了成功的结果。 |
| [^25] | [NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning.](http://arxiv.org/abs/2307.08941) | 该论文通过使用神经切向核近似MLP融合，提出了一种高效的语言模型微调方法。实验证明，这种方法能够在降低计算和存储开销的同时保持较好的模型性能。 |
| [^26] | [Teach model to answer questions after comprehending the document.](http://arxiv.org/abs/2307.08931) | 本文提出了一种两阶段知识蒸馏方法，通过将MRC任务分为两个独立的阶段来教导模型更好地理解文档。实验证明，该方法显著提高了学生模型的性能。 |
| [^27] | [Federated Large Language Model: A Position Paper.](http://arxiv.org/abs/2307.08925) | 我们提出了联邦式大规模语言模型的概念，通过联邦学习实现分散数据的共同训练共享模型，以应对公共数据可用性的限制和私有数据的隐私保护需求。我们讨论了预训练、微调和提示工程这三个组件的优势，并提出了实施策略。同时，我们探讨了FL和LLM集成带来的新挑战，并分析了现有解决方案和潜在障碍。 |
| [^28] | [Large Language Models Perform Diagnostic Reasoning.](http://arxiv.org/abs/2307.08922) | 本研究探索了大型语言模型在医学诊断中使用思维链提示的扩展。通过使用两个诊断推理 CoT 实例来提示大型语言模型，在一般任务中，我们发现诊断准确率提高了15%，在领域外的设置中，这一提升达到了18%。这些结果表明，在大型语言模型中，可以通过适当的提示引发专家知识推理。 |
| [^29] | [Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach.](http://arxiv.org/abs/2307.08859) | 本文提出了一种新的图神经网络课程学习方法，通过引入图复杂性形式化和模型能力作为困难度标准，以及考虑样本困难度和模型能力的不同视角进行训练，实现了对细粒度图困难度标准的纳入。 |
| [^30] | [Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge.](http://arxiv.org/abs/2307.08813) | 本研究评估了不同大型语言模型在提取分子相互作用和通路知识方面的有效性，并讨论了未来机遇和挑战。 |
| [^31] | [A mixed policy to improve performance of language models on math problems.](http://arxiv.org/abs/2307.08767) | 本文提出了一种混合策略的探索方法，利用强化学习来改进语言模型在数学问题上的性能，通过在抽象层和第二层采用不同的探索方式，取得了超过2%的性能增益。 |
| [^32] | [ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development.](http://arxiv.org/abs/2307.08720) | ivrit.ai是一份全面希伯来语音数据集，包含超过3300小时的语音和一千多个说话者，可用于推动希伯来语自动语音识别技术的发展，具有重要的研究、开发和商业应用价值。 |
| [^33] | [Cross-Lingual NER for Financial Transaction Data in Low-Resource Languages.](http://arxiv.org/abs/2307.08714) | 我们提出了一个高效的跨语言命名实体识别的建模框架，利用知识蒸馏和一致性训练方法，在低资源语言中通过从英语到阿拉伯语的知识传递，能够准确识别商家、金额和其他字段。 |
| [^34] | [Communicative Agents for Software Development.](http://arxiv.org/abs/2307.07924) | 本文介绍了一种创新的软件开发范式，利用大型语言模型(LLMs)在整个软件开发过程中实现自然语言交流，消除了每个阶段需要专门模型的需求。该范式使用ChatDev作为一个虚拟聊天驱动的软件开发公司，通过设计、编码、测试和文档化四个阶段的代理人团队促进协作。 |
| [^35] | [Secrets of RLHF in Large Language Models Part I: PPO.](http://arxiv.org/abs/2307.04964) | 本论文研究了大型语言模型中RLHF的秘密，重点关注了奖励模型、PPO和进程监督等技术路径，探索如何解决RLHF的稳定训练问题。 |
| [^36] | [A Survey on Evaluation of Large Language Models.](http://arxiv.org/abs/2307.03109) | 本文综述了大型语言模型（LLMs）的评估方法，关注三个关键维度：评估什么、在哪里评估以及如何评估。评估任务包括自然语言处理、推理、医学应用、伦理学、教育、自然和社会科学、代理应用等多个领域。本文为社会层面对LLMs潜在风险的理解提供了重要参考。 |
| [^37] | [Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation.](http://arxiv.org/abs/2307.02839) | 本论文提出一种新的方法使用LLM进行新闻摘要生成，通过进化调优事件模式群体，提高生成结果的准确性和可靠性。 |
| [^38] | [Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese.](http://arxiv.org/abs/2306.15788) | 该研究评估了GPT-3.5和GPT-4在巴西葡萄牙语语法错误修正方面的有效性，结果显示虽然GPT-4的召回率较高，但语言模型倾向于过度修正。 |
| [^39] | [SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design.](http://arxiv.org/abs/2306.15656) | SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。 |
| [^40] | [Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs.](http://arxiv.org/abs/2305.14279) | 本论文研究了大型语言模型在多步推理中的自洽性问题，提出了假设自洽性和组合自洽性两个重要特性，并发现GPT-3/-4模型在这两方面都表现出了较差的一致性。 |
| [^41] | [Evaluating Open-QA Evaluation.](http://arxiv.org/abs/2305.12421) | 本研究侧重于评估开放式问答（Open-QA）任务的方法，引入了一个新的任务QA-Eval和数据集EVOUNA，通过人工评估方法来评估AI生成的答案的准确性。我们调查了与人工评估相关的方法，并讨论了当前方法的缺陷和改进方法。我们相信这对于未来的自动评估工具发展和研究具有价值。 |
| [^42] | [Life of PII -- A PII Obfuscation Transformer.](http://arxiv.org/abs/2305.09550) | “Life of PII”是一种新颖的混淆变换器框架，用于将PII转化为人造PII同时尽可能地保留原始信息、意图和上下文，使我们能够有选择地混淆文档中的敏感信息，同时保留文档的统计和语义特性。 |
| [^43] | [GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding.](http://arxiv.org/abs/2305.09360) | GIFT是一个适用于多方对话理解的方法，通过设计四种类型的边缘将图感知信息集成到注意力机制中，改进了原始的顺序文本处理的PLM。 |
| [^44] | [Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs.](http://arxiv.org/abs/2305.03642) | 本文提出了一种基于LLM调整的文本到文本模型，共同提取RCT报告中的干预、结果和发现信息，实现相当大的性能提升。 |
| [^45] | [Persian topic detection based on Human Word association and graph embedding.](http://arxiv.org/abs/2302.09775) | 本文提出了一种基于人类词汇关联的波斯语社交媒体话题检测框架，该方法利用了词汇关联和关联引力力量生成图，并通过嵌入和聚类方法提取话题。 |
| [^46] | [Execution-based Code Generation using Deep Reinforcement Learning.](http://arxiv.org/abs/2301.13816) | 使用深度强化学习的PPOCoder框架将预训练的编程语言模型和Proximal Policy Optimization技术结合，通过利用代码执行和结构对齐的非可微反馈，实现了更高效的代码生成。 |
| [^47] | [A Human Word Association based model for topic detection in social networks.](http://arxiv.org/abs/2301.13066) | 本文提出了一个基于人类词汇联想的社交网络主题检测框架，通过考虑语言结构并设计专门的抽取算法，在FA-CUP数据集上取得了比其他方法更好的性能。 |
| [^48] | [Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe.](http://arxiv.org/abs/2210.14348) | 该论文介绍了一种简单而实用的方法，使用差分隐私对预训练的生成语言模型进行微调，能够生成具有强隐私保护的高质量合成文本，并且与非隐私版本相似。 |
| [^49] | [InitialGAN: A Language GAN with Completely Random Initialization.](http://arxiv.org/abs/2208.02531) | 本论文提出了一种名为InitialGAN的语言生成对抗网络，通过采用dropout采样和完全归一化的LSTM等技术，解决了当前语言GAN模型依赖预训练技术的限制，实现了完全随机初始化参数的模型。 |
| [^50] | [On the Interpretability and Significance of Bias Metrics in Texts: a PMI-based Approach.](http://arxiv.org/abs/2104.06474) | 这篇论文介绍了一种基于PMI的方法来解释和量化文本中的偏见，该方法提供了简单的解释和统计显著性，并在捕捉性别差距方面产生了类似于基于词嵌入的方法的结果。 |

# 详细

[^1]: 过度思考真相：理解语言模型如何处理虚假演示

    Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])

    [http://arxiv.org/abs/2307.09476](http://arxiv.org/abs/2307.09476)

    该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。

    

    现代语言模型可以通过少量示范进行复杂模式的模仿学习，使其能够在没有微调的情况下完成具有挑战性的任务。然而，模仿也可能导致模型在上下文中重现不准确或有害的内容。我们通过模型的内部表示来研究有害的模仿，并确定了两个相关现象：过度思考和错误归纳头。第一个现象，过度思考，在给出正确与错误的少量示范时，我们从中间层解码预测。在早期层中，两种示范引起了相似的模型行为，但在某个“关键层”之后，给出错误示范的准确性逐渐降低。第二个现象，错误归纳头，可能是过度思考的一种机制性原因：这些是位于较晚层的头部，它们关注并复制先前示范中的错误信息，其削弱会减少过度思考现象。

    Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
    
[^2]: ChatSpot: 通过精确的指示调整引导进行多模态LLMs引导初创

    ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning. (arXiv:2307.09474v1 [cs.CL])

    [http://arxiv.org/abs/2307.09474](http://arxiv.org/abs/2307.09474)

    本研究提出了一种精确的指示方法，利用点和框等多样化的引用表示方法来引用特殊区域，从而使得多模态大型语言模型（MLLMs）能够实现更精细的交互。基于此，我们提出了ChatSpot，一个统一的多模态大型语言模型，支持多种形式的互动，提供更灵活和无缝的交互体验。

    

    人工智能与人类的互动是反映多模态大型语言模型（MLLMs）可用性的重要方面。然而，现有的端到端MLLMs只允许用户通过语言指令与其交互，从而限制了交互的准确性和效率。在本研究中，我们提出了精确的引用指令，利用点和框等多样化的引用表示方法来引用特殊区域。这使得MLLMs能够聚焦于感兴趣的区域，并实现更精细的交互。基于精确的引用指令，我们提出了ChatSpot，一个统一的端到端多模态大型语言模型，支持包括鼠标点击、拖放和绘制框等多种形式的互动，提供更灵活和无缝的互动体验。我们还根据现有数据集和GPT-4生成了一个多层次的视觉语言指令跟随数据集。

    Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we des
    
[^3]: SR-GAN模型的比较分析

    A comparative analysis of SR-GAN models. (arXiv:2307.09456v1 [cs.CV])

    [http://arxiv.org/abs/2307.09456](http://arxiv.org/abs/2307.09456)

    本研究比较分析了多个最先进的SR-GAN模型，结果发现EDSR模型在保持视觉质量的同时显著提高了输入图像的分辨率，具有较高的PSNR和SSIM值，并且返回高质量的OCR结果，这表明EDSR是一种有效的超分辨率方法。

    

    在这项研究中，我们评估了多个最先进的超分辨率生成对抗网络（SR GAN）模型，包括ESRGAN，Real-ESRGAN和EDSR，在一个标准数据集上以及经过降级处理的真实世界图像上的性能。我们的结果表明，一些模型似乎在保持视觉质量的同时显著提高了输入图像的分辨率，这是通过使用Tesseract OCR引擎评估的。我们观察到，来自huggingface的EDSR-BASE模型在定量指标和主观视觉质量评估方面表现优于其余候选模型，并且计算开销最小。具体而言，EDSR生成具有较高峰值信噪比（PSNR）和结构相似性指数（SSIM）值的图像，并在Tesseract OCR引擎下返回高质量的OCR结果。这些发现表明，EDSR是一种稳健有效的单图像超分辨率方法，特别适合于需要OCR的应用场景。

    In this study, we evaluate the performance of multiple state-of-the-art SR GAN (Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo degradation using a pipeline. Our results show that some models seem to significantly increase the resolution of the input images while preserving their visual quality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE model from huggingface outperforms the remaining candidate models in terms of both quantitative metrics and subjective visual quality assessments with least compute overhead. Specifically, EDSR generates images with higher peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and are seen to return high quality OCR results with Tesseract OCR engine. These findings suggest that EDSR is a robust and effective approach for single-image super-resolution and may be particularly well-suited for applications wh
    
[^4]: 基于预训练变换器的伪异常暴露法用于检测未知分布

    Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers. (arXiv:2307.09455v1 [cs.CL])

    [http://arxiv.org/abs/2307.09455](http://arxiv.org/abs/2307.09455)

    本文提出了一种名为伪异常暴露（POE）的简单而有效的方法，通过顺序屏蔽与内分布类相关的令牌构建替代的外分布数据集，用于检测未知分布的样本。

    

    对于现实世界的语言应用，检测未知分布的样本有助于提醒用户或拒绝不可靠的样本。然而，现代过参数化的语言模型通常会对内分布和外分布样本都产生过度自信的预测。特别是，语言模型在与内分布样本具有相似语义表示的外分布样本上表现不佳，因为这些外分布样本位于内分布流形附近。可以通过使用内分布和多样的异常样本训练拒绝网络来检测测试的外分布样本，但明确收集辅助的外分布数据集会增加数据收集的负担。在本文中，我们提出了一种简单而有效的方法，称为伪异常暴露（POE），通过顺序屏蔽与内分布类相关的令牌来构建一个替代的外分布数据集。POE引入的替代外分布样本显示出与内部数据类似的表示，这对于训练拒绝网络最有效。我们的方法不需要任何

    For real-world language applications, detecting an out-of-distribution (OOD) sample is helpful to alert users or reject such unreliable samples. However, modern over-parameterized language models often produce overconfident predictions for both in-distribution (ID) and OOD samples. In particular, language models suffer from OOD samples with a similar semantic representation to ID samples since these OOD samples lie near the ID manifold. A rejection network can be trained with ID and diverse outlier samples to detect test OOD samples, but explicitly collecting auxiliary OOD datasets brings an additional burden for data collection. In this paper, we propose a simple but effective method called Pseudo Outlier Exposure (POE) that constructs a surrogate OOD dataset by sequentially masking tokens related to ID classes. The surrogate OOD sample introduced by POE shows a similar representation to ID data, which is most effective in training a rejection network. Our method does not require any 
    
[^5]: 让我们来ViCE！在图像生成评估中模拟人类认知行为

    Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation. (arXiv:2307.09416v1 [cs.CV])

    [http://arxiv.org/abs/2307.09416](http://arxiv.org/abs/2307.09416)

    本文介绍了一种新颖的自动化方法ViCE，通过结合大型语言模型和视觉问答，模拟人类认知过程来评估生成图像与提示之间的一致性和质量。

    

    近年来，在图像生成领域取得了重要进展，尤其是在引入了能够根据文字输入产生高质量视觉内容的视觉语言模型的推动下。尽管在生成质量和逼真度方面不断取得进展，但目前尚未定义系统性的框架来定量衡量生成内容的质量和与提示要求的一致性：到目前为止，只有基于人类的评估被用于质量满意度和比较不同的生成方法。我们引入了一种新颖的自动化方法来进行视觉概念评估（ViCE），即评估生成/编辑的图像与相应提示/指令之间的一致性，这一过程受到人类认知行为的启发。ViCE将大型语言模型（LLMs）和视觉问答（VQA）的优势结合到一个统一的流程中，旨在复制人类认知过程中的质量评估。

    Research in Image Generation has recently made significant progress, particularly boosted by the introduction of Vision-Language models which are able to produce high-quality visual content based on textual inputs. Despite ongoing advancements in terms of generation quality and realism, no methodical frameworks have been defined yet to quantitatively measure the quality of the generated content and the adherence with the prompted requests: so far, only human-based evaluations have been adopted for quality satisfaction and for comparing different generative methods. We introduce a novel automated method for Visual Concept Evaluation (ViCE), i.e. to assess consistency between a generated/edited image and the corresponding prompt/instructions, with a process inspired by the human cognitive behaviour. ViCE combines the strengths of Large Language Models (LLMs) and Visual Question Answering (VQA) into a unified pipeline, aiming to replicate the human cognitive process in quality assessment.
    
[^6]: 软件引用格式如何随时间演变？对R编程语言包的纵向分析

    How do software citation formats evolve over time? A longitudinal analysis of R programming language packages. (arXiv:2307.09390v1 [cs.DL])

    [http://arxiv.org/abs/2307.09390](http://arxiv.org/abs/2307.09390)

    本研究通过对2021年和2022年收集的R语言包的引用格式的纵向数据集进行比较和分析，揭示了软件引用格式如何随时间演变。研究发现软件的引用存在不一致性，并且引用随时间而变化。这一发现对于理解科学研究中软件引用的重要性和演变具有重要意义。

    

    在数据驱动的研究范式下，研究软件在科学探究的几乎每个阶段都发挥着关键作用。学者们呼吁在学术出版物中正式引用软件，将其视为传统研究成果的平等对象。然而，软件的引用并不一致：一个软件实体可以被引用为不同的对象，并且引用可能随时间而变化。然而，这些问题在现有的关于软件引用的经验性研究中很大程度上被忽视。为了填补上述空白，本研究通过比较和分析在2021年和2022年收集的所有R软件包的引用格式的纵向数据集，来了解R语言包的引用格式，这些包是开源软件家族中的重要成员，并研究引用格式如何随时间演变。特别地，我们研究了引用背后的不同文档类型，以及引用格式中的哪些元数据元素随时间发生了变化。

    Under the data-driven research paradigm, research software has come to play crucial roles in nearly every stage of scientific inquiry. Scholars are advocating for the formal citation of software in academic publications, treating it on par with traditional research outputs. However, software is hardly consistently cited: one software entity can be cited as different objects, and the citations can change over time. These issues, however, are largely overlooked in existing empirical research on software citation. To fill the above gaps, the present study compares and analyzes a longitudinal dataset of citation formats of all R packages collected in 2021 and 2022, in order to understand the citation formats of R-language packages, important members in the open-source software family, and how the citations evolve over time. In particular, we investigate the different document types underlying the citations and what metadata elements in the citation formats changed over time. Furthermore, w
    
[^7]: 零样本对话搜索中的查询重构

    Zero-shot Query Reformulation for Conversational Search. (arXiv:2307.09384v1 [cs.IR])

    [http://arxiv.org/abs/2307.09384](http://arxiv.org/abs/2307.09384)

    提出了一种零样本查询重构（ZeQR）框架，通过利用机器阅读理解任务的语言模型来解决对话搜索中的数据稀疏性、解释性不足和歧义的问题。

    

    随着语音助手的普及，对话搜索在信息检索领域引起了更多的关注。然而，对话搜索中的数据稀疏性问题严重阻碍了监督式对话搜索方法的进展。因此，研究人员更加关注零样本对话搜索方法。然而，现有的零样本方法存在三个主要限制：它们不适用于所有的检索器，它们的有效性缺乏足够的解释性，并且他们无法解决因省略而导致的常见对话歧义。为了解决这些限制，我们引入了一种新颖的零样本查询重构（ZeQR）框架，该框架根据先前的对话上下文重构查询，而无需对话搜索数据的监督。具体来说，我们的框架利用了设计用于机器阅读理解任务的语言模型来明确解决两个常见的歧义：协调和省略。

    As the popularity of voice assistants continues to surge, conversational search has gained increased attention in Information Retrieval. However, data sparsity issues in conversational search significantly hinder the progress of supervised conversational search methods. Consequently, researchers are focusing more on zero-shot conversational search approaches. Nevertheless, existing zero-shot methods face three primary limitations: they are not universally applicable to all retrievers, their effectiveness lacks sufficient explainability, and they struggle to resolve common conversational ambiguities caused by omission. To address these limitations, we introduce a novel Zero-shot Query Reformulation (ZeQR) framework that reformulates queries based on previous dialogue contexts without requiring supervision from conversational search data. Specifically, our framework utilizes language models designed for machine reading comprehension tasks to explicitly resolve two common ambiguities: cor
    
[^8]: 为口语评估适应ASR基础模型

    Adapting an ASR Foundation Model for Spoken Language Assessment. (arXiv:2307.09378v1 [cs.CL])

    [http://arxiv.org/abs/2307.09378](http://arxiv.org/abs/2307.09378)

    本论文主要针对ASR基础模型Whisper的输出进行了详细分析，并提出了两种解决方案：微调和软提示微调。实验结果表明，可以有效改变Whisper的解码行为，生成候选人实际说出的单词。

    

    准确可靠的口语评估系统的关键部分是底层的ASR模型。最近，大规模预训练的ASR基础模型如Whisper已经可用。由于这些模型的输出是人类可读的，所以会添加标点符号，数字呈现为阿拉伯数字形式，包括缩写。此外，这些模型往往会跳过输出中的不流畅和犹豫。虽然对于可读性很有用，但这些属性对于评估候选人的能力和提供反馈并不有用。在这篇论文中，我们对Whisper的输出进行了详细分析，并提出了两种解决方案：微调和软提示微调。在公共语音语料库和英语学习者数据集上进行了实验。结果表明，我们可以有效地改变Whisper的解码行为，生成候选人实际说出的单词。

    A crucial part of an accurate and reliable spoken language assessment system is the underlying ASR model. Recently, large-scale pre-trained ASR foundation models such as Whisper have been made available. As the output of these models is designed to be human readable, punctuation is added, numbers are presented in Arabic numeric form and abbreviations are included. Additionally, these models have a tendency to skip disfluencies and hesitations in the output. Though useful for readability, these attributes are not helpful for assessing the ability of a candidate and providing feedback. Here a precise transcription of what a candidate said is needed. In this paper, we give a detailed analysis of Whisper outputs and propose two solutions: fine-tuning and soft prompt tuning. Experiments are conducted on both public speech corpora and an English learner dataset. Results show that we can effectively alter the decoding behaviour of Whisper to generate the exact words spoken in the response.
    
[^9]: 多模态讨论变换器：整合文本、图像和图变换器以检测社交媒体上的仇恨言论。

    Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])

    [http://arxiv.org/abs/2307.09312](http://arxiv.org/abs/2307.09312)

    多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。

    

    我们提出了一种新颖的多模态基于图的变换器模型，名为多模态讨论变换器（mDT），用于检测在线社交网络中的仇恨言论。与传统的仅使用文本的方法不同，我们将标记评论为仇恨言论的方法围绕文本和图像的整体分析展开。这是通过利用图变换器来捕捉评论周围整个讨论中的上下文关系，并采用交织融合层来组合文本和图像嵌入，而不是单独处理不同的模态。我们将模型的性能与仅处理文本的基线进行比较，还进行了广泛的消融研究。最后，我们展望了多模态解决方案在在线环境中提供社会价值的未来工作，并认为捕捉对话的整体视图极大地推进了检测反社会行为的努力。

    We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
    
[^10]: Llama 2: 开放基础和优化聊天模型

    Llama 2: Open Foundation and Fine-Tuned Chat Models. (arXiv:2307.09288v1 [cs.CL])

    [http://arxiv.org/abs/2307.09288](http://arxiv.org/abs/2307.09288)

    Llama 2是一个优化的聊天模型，通过fine-tuned技术和安全改进，表现优于开源模型，并可作为闭源模型的替代选择。

    

    在这项工作中，我们开发并发布了Llama 2，一个包含预训练和优化的大型语言模型（LLM），其规模从70亿到700亿参数不等。我们的优化LLM，称为Llama 2-Chat，在对话使用案例中表现优于开源聊天模型。根据我们对有用性和安全性的人工评估结果，它们可能是闭源模型的合适替代品。我们详细描述了我们在Llama 2-Chat的优化和安全性改进方面的方法，以便让社区能够在我们的工作基础上构建并为LLM的负责任发展做出贡献。

    In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.
    
[^11]: 通过3D连体网络改进文本语义相似性建模

    Improving Text Semantic Similarity Modeling through a 3D Siamese Network. (arXiv:2307.09274v1 [cs.CL])

    [http://arxiv.org/abs/2307.09274](http://arxiv.org/abs/2307.09274)

    通过引入三维连体网络，我们改进了文本语义相似性建模方法，从而更好地保留了层次化的语义信息，并提供了更丰富的结构条件来进行全面的下游建模策略概览。

    

    近年来，连体网络已成为建模文本语义相似性的一种流行方法。传统方法依赖池化操作来压缩编码中Transformer块的语义表示，从而得到二维语义向量并丢失Transformer块中的层次化语义信息。此外，这种有限的语义向量结构类似于一个被铺平的地形，限制了下游建模中可以应用的方法，因为它们只能在这个平面上进行导航。为解决这个问题，我们提出了一种新颖的用于文本语义相似性建模的3D连体网络，它将语义信息映射到一个更高维的空间中。三维语义张量不仅保留了更精确的空间和特征领域信息，还为全面的下游建模策略提供了必要的结构条件来捕捉这些信息。利用这种结构上的优势，我们引入了几个模块

    Siamese networks have gained popularity as a method for modeling text semantic similarity. Traditional methods rely on pooling operation to compress the semantic representations from Transformer blocks in encoding, resulting in two-dimensional semantic vectors and the loss of hierarchical semantic information from Transformer blocks. Moreover, this limited structure of semantic vectors is akin to a flattened landscape, which restricts the methods that can be applied in downstream modeling, as they can only navigate this flat terrain. To address this issue, we propose a novel 3D Siamese network for text semantic similarity modeling, which maps semantic information to a higher-dimensional space. The three-dimensional semantic tensors not only retains more precise spatial and feature domain information but also provides the necessary structural condition for comprehensive downstream modeling strategies to capture them. Leveraging this structural advantage, we introduce several modules to 
    
[^12]: 线性化相对位置编码

    Linearized Relative Positional Encoding. (arXiv:2307.09270v1 [cs.CL])

    [http://arxiv.org/abs/2307.09270](http://arxiv.org/abs/2307.09270)

    这项工作提出了一种线性化相对位置编码（LRPE）家族，通过规范形式和酉变换推导出了一种有原则的框架，该框架可用于开发保留线性时空复杂度的新的相对位置编码方法，并为各种应用提供有效的编码。

    

    相对位置编码被广泛用于普通和线性 transformers 中以表示位置信息。然而，现有的编码方法对于线性 transformer 不一定直接适用，因为后者需要将查询和键表示分解为单独的核函数。尽管如此，设计适合线性 transformer 的编码方法的原则仍然研究甚少。在这项工作中，我们将各种现有的线性相对位置编码方法归结为一种规范形式，并进一步通过酉变换提出了一族线性相对位置编码算法。我们的公式推导出一个有原则的框架，可用于开发保留线性时空复杂度的新的相对位置编码方法。借助不同模型，提出的线性化相对位置编码（LRPE）家族为各种应用提供了有效的编码。

    Relative positional encoding is widely used in vanilla and linear transformers to represent positional information. However, existing encoding methods of a vanilla transformer are not always directly applicable to a linear transformer, because the latter requires a decomposition of the query and key representations into separate kernel functions. Nevertheless, principles for designing encoding methods suitable for linear transformers remain understudied. In this work, we put together a variety of existing linear relative positional encoding approaches under a canonical form and further propose a family of linear relative positional encoding algorithms via unitary transformation. Our formulation leads to a principled framework that can be used to develop new relative positional encoding methods that preserve linear space-time complexity. Equipped with different models, the proposed linearized relative positional encoding (LRPE) family derives effective encoding for various applications.
    
[^13]: 基于Transformer语言模型和n-gram困惑度的文本向量化

    Text vectorization via transformer-based language models and n-gram perplexities. (arXiv:2307.09255v1 [cs.CL])

    [http://arxiv.org/abs/2307.09255](http://arxiv.org/abs/2307.09255)

    该研究提出了一种基于n-gram困惑度的简单算法，用于计算文本向量化。这种方法可以解决在计算标量困惑度时由于个别标记的不太可能性导致概率的降低的问题，并且能够保留文本中每个标记的相对困惑度信息。

    

    由于文本的概率（及困惑度）是基于各个标记的概率的乘积计算的，因此可能会出现一个不太可能的标记显著降低一些原本高概率输入的概率（增加困惑度），同时可能表示一个简单的打字错误。另外，鉴于困惑度是一个标量值，它指的是整个输入的概率分布信息在计算中丢失了（一个相对较好的文本如果有一个不太可能的标记，以及每个标记等可能的另一个文本，它们可以具有相同的困惑度值），尤其是对于较长的文本。作为对标量困惑度的替代，该研究提出了一种简单的算法，用于计算基于输入中的n-gram困惑度的向量值。这样的表示方法考虑了前面提到的各个方面，而不是唯一的值，计算了每个文本标记的相对困惑度，

    As the probability (and thus perplexity) of a text is calculated based on the product of the probabilities of individual tokens, it may happen that one unlikely token significantly reduces the probability (i.e., increase the perplexity) of some otherwise highly probable input, while potentially representing a simple typographical error. Also, given that perplexity is a scalar value that refers to the entire input, information about the probability distribution within it is lost in the calculation (a relatively good text that has one unlikely token and another text in which each token is equally likely they can have the same perplexity value), especially for longer texts. As an alternative to scalar perplexity this research proposes a simple algorithm used to calculate vector values based on n-gram perplexities within the input. Such representations consider the previously mentioned aspects, and instead of a unique value, the relative perplexity of each text token is calculated, and the
    
[^14]: 用于量化生成式语言模型不确定性的PAC神经预测集学习

    PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])

    [http://arxiv.org/abs/2307.09254](http://arxiv.org/abs/2307.09254)

    本文提出了一种使用神经网络来量化生成式语言模型不确定性的PAC神经预测集学习方法，通过在多种语言数据集和模型上的实验证明，相比于标准基准方法，我们的方法平均提高了63％的量化不确定性。

    

    学习和量化模型的不确定性是增强模型可信度的关键任务。由于对生成虚构事实的担忧，最近兴起的生成式语言模型（GLM）特别强调可靠的不确定性量化的需求。本文提出了一种学习神经预测集模型的方法，该方法能够以可能近似正确（PAC）的方式量化GLM的不确定性。与现有的预测集模型通过标量值参数化不同，我们提出通过神经网络参数化预测集，实现更精确的不确定性量化，但仍满足PAC保证。通过在四种类型的语言数据集和六种类型的模型上展示，我们的方法相比标准基准方法平均提高了63％的量化不确定性。

    Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
    
[^15]: UniTabE: 面向异构表格数据的统一预训练表格编码器

    UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])

    [http://arxiv.org/abs/2307.09249](http://arxiv.org/abs/2307.09249)

    UniTabE是一种面向异构表格数据的统一预训练表格编码器，能够处理不同表格结构的挑战，并具有对多样化下游应用的适应性。

    

    自然语言处理（NLP）的最新进展明证了预训练模型的突破性影响，在各种任务上取得了令人印象深刻的结果。本研究旨在将预训练方法的威力扩展到传统被忽视的表格数据领域，该领域由于不同任务固有的众多表格模式而具有挑战性。本工作的主要研究问题围绕异构表格结构的适应性、表格数据的统一预训练协议的建立、学到的知识在任务之间的泛化和可传递性、对多样化下游应用的适应性以及随时间的增量列的纳入进行了探讨。针对这些挑战，我们引入了UniTabE，这是一种创新的方法，旨在以一致的方式处理表格，摆脱了特定表格结构强加的约束。UniTabE的核心概念是对每个基本表格进行表示

    Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic tab
    
[^16]: 自动化的残障主义：探索情感分析和毒性检测模型中的明显残障偏见

    Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models. (arXiv:2307.09209v1 [cs.CL])

    [http://arxiv.org/abs/2307.09209](http://arxiv.org/abs/2307.09209)

    该论文分析了情感分析和毒性检测模型，以探测对残障人士的明显偏见。研究使用偏见识别框架对社交媒体平台的对话进行了分析，并创建了一个测试语料库来量化明显的残障偏见。研究发现，所研究的模型均存在显著的偏见。

    

    我们分析情感分析和毒性检测模型，以检测对残障人士的明显偏见。我们采用扰动敏感性分析的偏见识别框架，研究社交媒体平台上与残障人士相关的对话，特别是Twitter和Reddit，在真实社交环境中了解残障偏见是如何传播的。然后，我们创建了“情感中的偏见识别测试”（BITS）语料库，以量化任何情感分析和毒性检测模型中的明显残障偏见。我们的研究使用BITS揭示了四种开放的AIaaS（AI即服务）情感分析工具（TextBlob，VADER，Google Cloud Natural Language API，DistilBERT）和两种毒性检测模型（两个版本的Toxic-BERT）中存在显着的偏见。

    We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the \textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.
    
[^17]: 揭示在LLM中职业性别偏见：分析和解决社会学影响

    Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications. (arXiv:2307.09162v1 [cs.CL])

    [http://arxiv.org/abs/2307.09162](http://arxiv.org/abs/2307.09162)

    本研究分析了大型语言模型中的性别偏见，以GPT-2和GPT-3.5为例，通过全面的文献综述和深入的定量分析揭示了存在的性别化词语关联、语言使用和偏见叙述，并探讨了性别偏见可能对社会认知产生的伦理影响。

    

    人工智能（AI）和自然语言处理中的性别偏见引起了广泛关注，因为它可能对社会认知和偏见产生影响。这篇研究旨在分析大型语言模型（LLMs）中的性别偏见，重点比较了GPT-2和GPT-3.5这些著名语言模型，以更好地理解其影响。通过全面的文献综述，该研究考察了现有关于AI语言模型中性别偏见的研究，并确定了当前知识的空白。研究方法包括收集和预处理GPT-2和GPT-3.5的数据，并运用深入的定量分析技术评估生成文本中的性别偏见。研究结果揭示了这些大型语言模型输出中存在的具有性别色彩的词语关联、语言使用和偏见叙述。讨论部分探讨了性别偏见的伦理影响以及其对社会认知的潜在后果。

    Gender bias in artificial intelligence (AI) and natural language processing has garnered significant attention due to its potential impact on societal perceptions and biases. This research paper aims to analyze gender bias in Large Language Models (LLMs) with a focus on multiple comparisons between GPT-2 and GPT-3.5, some prominent language models, to better understand its implications. Through a comprehensive literature review, the study examines existing research on gender bias in AI language models and identifies gaps in the current knowledge. The methodology involves collecting and preprocessing data from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis techniques to evaluate gender bias in the generated text. The findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these Large Language Models. The discussion explores the ethical implications of gender bias and its potential consequences on social percepti
    
[^18]: 基于预训练句子嵌入的长文本分类中的注意力机制

    Attention over pre-trained Sentence Embeddings for Long Document Classification. (arXiv:2307.09084v1 [cs.CL])

    [http://arxiv.org/abs/2307.09084](http://arxiv.org/abs/2307.09084)

    利用预训练句子嵌入和小注意力层进行长文本分类，取得了竞争性的结果。

    

    尽管transformer模型是大多数NLP任务中的默认模型，但由于其关于令牌数量的二次注意力复杂度，它们通常被限制在短序列中。为了解决这个问题，已经进行了一些尝试，包括通过减少自注意力计算的成本，通过对较小的序列建模并通过递归机制或使用新的transformer模型进行结合。在本文中，我们建议利用预训练的句子嵌入来从各个句子的语义有意义的嵌入开始，然后通过一个随文档长度线性扩展的小注意力层将它们组合起来。我们在三个标准的文档分类数据集上报告了这种简单架构的结果。与使用标准微调的当前最先进模型相比，该方法取得了竞争性的结果（即使在这种配置下没有明确的最佳模型）。我们还展示了通过该方法可实现的最佳性能。

    Despite being the current de-facto models in most NLP tasks, transformers are often limited to short sequences due to their quadratic attention complexity on the number of tokens. Several attempts to address this issue were studied, either by reducing the cost of the self-attention computation or by modeling smaller sequences and combining them through a recurrence mechanism or using a new transformer model. In this paper, we suggest to take advantage of pre-trained sentence transformers to start from semantically meaningful embeddings of the individual sentences, and then combine them through a small attention layer that scales linearly with the document length. We report the results obtained by this simple architecture on three standard document classification datasets. When compared with the current state-of-the-art models using standard fine-tuning, the studied method obtains competitive results (even if there is no clear best model in this configuration). We also showcase that the
    
[^19]: 文字想象的释放：通过探索文字的力量实现文本到图像的人物检索的新框架

    Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])

    [http://arxiv.org/abs/2307.09059](http://arxiv.org/abs/2307.09059)

    本研究提出了一个新的框架，通过探索文本中的文字的力量，实现了准确地将抽象的文本描述映射到具体的图像，从而实现了文本到图像的人物检索。

    

    文本到图像的人物检索的目标是从大型图库中检索与给定文本描述相匹配的人物图像。这个任务的主要挑战在于视觉和文本模态之间信息表示的显著差异。文本模态通过词汇和语法结构传递抽象和精确的信息，而视觉模态通过图像传递具体和直观的信息。为了充分利用文字表示的表达力，准确地将抽象的文本描述映射到具体图像是至关重要的。为了解决这个问题，我们提出了一个新的框架，通过探索句子中的文字的力量，释放了文本到图像人物检索中的文字想象力。具体来说，该框架使用预训练的全面CLIP模型作为图像和文本的双编码器，利用先前的跨模态对齐知识。

    The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.  To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Imag
    
[^20]: 朝着协作对话管理的神经时代发展：一项文献调研

    Towards a Neural Era in Dialogue Management for Collaboration: A Literature Survey. (arXiv:2307.09021v1 [cs.CL])

    [http://arxiv.org/abs/2307.09021](http://arxiv.org/abs/2307.09021)

    本文回顾了协作对话系统中对话管理的演变，并分析了应用神经网络方法进行协作对话管理的最新工作，旨在为未来在协作对话管理领域的进展提供基础背景。

    

    基于对话的人工智能（AI）协作可以彻底改变协作问题解决、创造性探索和社交支持。为了实现这一目标，必须开发熟悉技巧如协商、遵循指示、建立共同基础和推进共享任务的自动化代理。本文对协作对话系统中的对话管理范例的演变进行了回顾，从传统的手工制作和基于信息状态的方法到受人工智能规划启发的方法。然后，它转向当代的数据驱动对话管理技术，这些技术试图将深度学习在填表和开放领域设置中的成功应用于协作环境。本文还分析了一组应用神经网络方法进行协作对话管理的最近工作，并突出了该领域的流行趋势。这项调查希望为协作对话管理领域的未来进展提供基础背景。

    Dialogue-based human-AI collaboration can revolutionize collaborative problem-solving, creative exploration, and social support. To realize this goal, the development of automated agents proficient in skills such as negotiating, following instructions, establishing common ground, and progressing shared tasks is essential. This survey begins by reviewing the evolution of dialogue management paradigms in collaborative dialogue systems, from traditional handcrafted and information-state based methods to AI planning-inspired approaches. It then shifts focus to contemporary data-driven dialogue management techniques, which seek to transfer deep learning successes from form-filling and open-domain settings to collaborative contexts. The paper proceeds to analyze a selected set of recent works that apply neural approaches to collaborative dialogue management, spotlighting prevailing trends in the field. This survey hopes to provide foundational background for future advancements in collaborat
    
[^21]: 使用KeyBERT和SNA探索针对工程学生的自动驾驶车辆政策接受度

    Exploring acceptance of autonomous vehicle policies using KeyBERT and SNA: Targeting engineering students. (arXiv:2307.09014v1 [cs.SI])

    [http://arxiv.org/abs/2307.09014](http://arxiv.org/abs/2307.09014)

    本研究利用KeyBERT和SNA方法，探索工程学生对自动驾驶车辆政策的接受度。通过文本挖掘分析研究生的意见，从而填补了终端用户对这些政策接受度的分析空白。

    

    本研究旨在利用改进的文本挖掘方法，探索用户对自动驾驶车辆政策的接受度。近年来，韩国政策制定者将自动驾驶汽车（ADC）和自动驾驶机器人（ADR）视为下一代交通工具，可以降低乘客和货物运输成本。他们支持为ADC建设V2I和V2V通信基础设施，并认识到ADR等同于行人，以促进其在人行道上的部署。为了填补终端用户对这些政策接受度没有充分考虑的空白，本研究将两种文本挖掘方法应用于工业、机械和电子-电气-计算机领域的研究生意见。一种是基于TF-IWF和Dice系数的共现网络分析（CNA），另一种是基于能够以上下文方式表示意见的关键词提取工具KeyBERT的上下文语义网络分析（C-SNA）。

    This study aims to explore user acceptance of Autonomous Vehicle (AV) policies with improved text-mining methods. Recently, South Korean policymakers have viewed Autonomous Driving Car (ADC) and Autonomous Driving Robot (ADR) as next-generation means of transportation that will reduce the cost of transporting passengers and goods. They support the construction of V2I and V2V communication infrastructures for ADC and recognize that ADR is equivalent to pedestrians to promote its deployment into sidewalks. To fill the gap where end-user acceptance of these policies is not well considered, this study applied two text-mining methods to the comments of graduate students in the fields of Industrial, Mechanical, and Electronics-Electrical-Computer. One is the Co-occurrence Network Analysis (CNA) based on TF-IWF and Dice coefficient, and the other is the Contextual Semantic Network Analysis (C-SNA) based on both KeyBERT, which extracts keywords that contextually represent the comments, and dou
    
[^22]: ChatGPT的行为随时间变化如何？

    How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])

    [http://arxiv.org/abs/2307.09009](http://arxiv.org/abs/2307.09009)

    本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。

    

    GPT-3.5和GPT-4是两种广泛使用的大型语言模型（LLM）服务。然而，这些模型何时以及如何进行更新是不透明的。在这里，我们对GPT-3.5和GPT-4的2023年3月和2023年6月版本进行了评估，涉及四项不同的任务：1）解决数学问题，2）回答敏感/危险问题，3）生成代码和4）视觉推理。我们发现，GPT-3.5和GPT-4的性能和行为在时间上可以有很大的变化。例如，GPT-4（2023年3月）在识别质数方面表现非常出色（准确率为97.6%），但GPT-4（2023年6月）在相同的问题上表现非常差（准确率为2.4%）。有趣的是，GPT-3.5（2023年6月）在这个任务上比GPT-3.5（2023年3月）要好得多。GPT-4在6月份对回答敏感问题的意愿较3月份要低，而无论是GPT-4还是GPT-3.5在6月份的代码生成中都有更多的格式错误。总体而言，我们的发现表明相同LLM服务的行为在相对较短的时间内可以发生重大变化。

    GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
    
[^23]: 关于大语言模型在中文文本纠错中的（无）效性研究

    On the (In)Effectiveness of Large Language Models for Chinese Text Correction. (arXiv:2307.09007v1 [cs.CL])

    [http://arxiv.org/abs/2307.09007](http://arxiv.org/abs/2307.09007)

    本文研究了大语言模型在中文文本纠错任务中的效果，并发现ChatGPT在中文语法错误纠正和拼写检查方面的性能表现并不理想。

    

    最近，大语言模型（LLMs）的发展和进步令整个人工智能社区惊叹不已。作为LLMs的杰出代表和引发了对LLMs研究热潮的基础模型，ChatGPT吸引了越来越多的研究人员来研究其在各种下游自然语言处理（NLP）任务上的能力和性能。在对ChatGPT在各种任务上的惊人表现感到惊叹的同时，我们注意到ChatGPT还具有出色的多语言处理能力，例如中文。为了探索ChatGPT的中文处理能力，我们专注于中文文本纠错这个基础且具有挑战性的中文NLP任务。具体而言，我们在中文语法错误纠正（CGEC）和中文拼写检查（CSC）这两个主要的中文文本纠错场景上评估ChatGPT。通过广泛的分析和与之前最先进的微调模型的比较，我们在实证上发现ChatGPT的性能不尽如人意。

    Recently, the development and progress of Large Language Models (LLMs) have amazed the entire Artificial Intelligence community. As an outstanding representative of LLMs and the foundation model that set off this wave of research on LLMs, ChatGPT has attracted more and more researchers to study its capabilities and performance on various downstream Natural Language Processing (NLP) tasks. While marveling at ChatGPT's incredible performance on kinds of tasks, we notice that ChatGPT also has excellent multilingual processing capabilities, such as Chinese. To explore the Chinese processing ability of ChatGPT, we focus on Chinese Text Correction, a fundamental and challenging Chinese NLP task. Specifically, we evaluate ChatGPT on the Chinese Grammatical Error Correction (CGEC) and Chinese Spelling Check (CSC) tasks, which are two main Chinese Text Correction scenarios. From extensive analyses and comparisons with previous state-of-the-art fine-tuned models, we empirically find that the Cha
    
[^24]: 通过解耦置信学习来减缓标签偏见

    Mitigating Label Bias via Decoupled Confident Learning. (arXiv:2307.08945v1 [cs.LG])

    [http://arxiv.org/abs/2307.08945](http://arxiv.org/abs/2307.08945)

    这项研究提出了一种名为DeCoLe的修剪方法，用于减轻标签偏见问题。研究在合成数据集和仇恨言论检测领域取得了成功的结果。

    

    随着对算法公平性的担忧增加，出现了许多方法来减轻算法偏见。然而，这些方法主要假设训练数据中观察到的标签是正确的。这是有问题的，因为标签偏见在包括医疗、招聘和内容审核在内的重要领域中普遍存在。特别是，人类生成的标签容易带有社会偏见。虽然标签偏见的存在已经在概念上进行了讨论，但缺乏应对此问题的方法。我们提出了一种修剪方法——解耦置信学习（DeCoLe），专门设计来减轻标签偏见。在演示了其在合成数据集上的性能后，我们将DeCoLe应用于仇恨言论检测的环境中，这是一个被认为是重要挑战的领域，并展示其成功识别出偏见标签并超越竞争方法的表现。

    Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias. However, such methodologies largely assume that observed labels in training data are correct. This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation. In particular, human-generated labels are prone to encoding societal biases. While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.
    
[^25]: NTK-近似MLP融合用于高效的语言模型微调

    NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])

    [http://arxiv.org/abs/2307.08941](http://arxiv.org/abs/2307.08941)

    该论文通过使用神经切向核近似MLP融合，提出了一种高效的语言模型微调方法。实验证明，这种方法能够在降低计算和存储开销的同时保持较好的模型性能。

    

    在许多自然语言处理应用中，微调预训练语言模型(PLM)已成为主要策略。然而，即使是微调PLM和进行推理也是昂贵的，特别是在计算能力较低的边缘设备上。已经广泛研究了一些通用的方法（例如量化和蒸馏）来减少PLM微调的计算/存储开销，但很少有一次性压缩技术被探索。在本文中，我们研究了多层感知器(MLP)模块中预训练语言模型(PLM)的神经切向核(NTK)，并提出通过NTK近似MLP融合来创建一个轻量级的PLM。为实现这一目标，我们将MLP重新视为一束子MLP，并将它们聚类为给定数量的质心，然后将其恢复为压缩的MLP，并意外地显示出对原始PLM的NTK进行良好近似的效果。在自然语言处理数据集上进行了大量实验以验证PLM微调的效果。

    Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural l
    
[^26]: 教模型在理解文档后回答问题

    Teach model to answer questions after comprehending the document. (arXiv:2307.08931v1 [cs.CL])

    [http://arxiv.org/abs/2307.08931](http://arxiv.org/abs/2307.08931)

    本文提出了一种两阶段知识蒸馏方法，通过将MRC任务分为两个独立的阶段来教导模型更好地理解文档。实验证明，该方法显著提高了学生模型的性能。

    

    多选机器阅读理解(MRC)是自然语言处理(NLP)的一个具有挑战性的扩展，需要理解给定文本中实体之间的语义和逻辑关系的能力。传统上，MRC任务被视为根据给定文本回答问题的过程。这种单阶段方法往往使网络专注于生成正确答案，可能忽视了对文本本身的理解。因此，许多流行的模型在处理较长的文本时在这个任务上面临挑战。在本文中，我们提出了一种两阶段的知识蒸馏方法，通过将MRC任务分为两个独立的阶段来教导模型更好地理解文档。我们的实验结果表明，当学生模型配备我们的方法时，取得了显著的改进，展示了我们方法的有效性。

    Multi-choice Machine Reading Comprehension (MRC) is a challenging extension of Natural Language Processing (NLP) that requires the ability to comprehend the semantics and logical relationships between entities in a given text. The MRC task has traditionally been viewed as a process of answering questions based on the given text. This single-stage approach has often led the network to concentrate on generating the correct answer, potentially neglecting the comprehension of the text itself. As a result, many prevalent models have faced challenges in performing well on this task when dealing with longer texts. In this paper, we propose a two-stage knowledge distillation method that teaches the model to better comprehend the document by dividing the MRC task into two separate stages. Our experimental results show that the student model, when equipped with our method, achieves significant improvements, demonstrating the effectiveness of our method.
    
[^27]: 联邦式大规模语言模型：一个立场论文

    Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])

    [http://arxiv.org/abs/2307.08925](http://arxiv.org/abs/2307.08925)

    我们提出了联邦式大规模语言模型的概念，通过联邦学习实现分散数据的共同训练共享模型，以应对公共数据可用性的限制和私有数据的隐私保护需求。我们讨论了预训练、微调和提示工程这三个组件的优势，并提出了实施策略。同时，我们探讨了FL和LLM集成带来的新挑战，并分析了现有解决方案和潜在障碍。

    

    大规模语言模型（LLM）在各个领域获得了相当大的关注并找到了多样化的应用，但在真实场景中开发时面临挑战。这些挑战源于公共领域数据可用性的匮乏以及对私有领域数据的隐私保护需求。为了解决这些问题，联邦学习（FL）作为一项有前景的技术出现了，它能够在保持分散数据的同时实现共同训练共享模型。我们提出了联邦式LLM的概念，包括三个关键组成部分，即联邦式LLM预训练、联邦式LLM微调和联邦式LLM提示工程。对于每个组件，我们讨论了它相对于传统LLM训练方法的优势，并提出了具体的工程策略来实施。此外，我们探讨了FL和LLM集成带来的新挑战。我们分析现有的解决方案并确定可能的障碍

    Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles fac
    
[^28]: 大型语言模型进行诊断推理

    Large Language Models Perform Diagnostic Reasoning. (arXiv:2307.08922v1 [cs.CL])

    [http://arxiv.org/abs/2307.08922](http://arxiv.org/abs/2307.08922)

    本研究探索了大型语言模型在医学诊断中使用思维链提示的扩展。通过使用两个诊断推理 CoT 实例来提示大型语言模型，在一般任务中，我们发现诊断准确率提高了15%，在领域外的设置中，这一提升达到了18%。这些结果表明，在大型语言模型中，可以通过适当的提示引发专家知识推理。

    

    我们探索了将思维链 (CoT) 提示扩展到医疗推理以进行自动诊断的任务。受医生潜在的推理过程的启发，我们提出了诊断推理 CoT (DR-CoT)。经验证实，仅通过用两个诊断推理 CoT 实例提示仅在一般文本语料库上训练的大型语言模型，诊断准确率比标准提示提高了15%。此外，在领域外的设置中，这一差距达到了显著的18%。我们的研究结果表明，通过适当的提示可以引发大型语言模型中的专家知识推理。

    We explore the extension of chain-of-thought (CoT) prompting to medical reasoning for the task of automatic diagnosis. Motivated by doctors' underlying reasoning process, we present Diagnostic-Reasoning CoT (DR-CoT). Empirical results demonstrate that by simply prompting large language models trained only on general text corpus with two DR-CoT exemplars, the diagnostic accuracy improves by 15% comparing to standard prompting. Moreover, the gap reaches a pronounced 18% in out-domain settings. Our findings suggest expert-knowledge reasoning in large language models can be elicited through proper promptings.
    
[^29]: 图神经网络的课程学习：一种基于多视角和能力的方法

    Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach. (arXiv:2307.08859v1 [cs.LG])

    [http://arxiv.org/abs/2307.08859](http://arxiv.org/abs/2307.08859)

    本文提出了一种新的图神经网络课程学习方法，通过引入图复杂性形式化和模型能力作为困难度标准，以及考虑样本困难度和模型能力的不同视角进行训练，实现了对细粒度图困难度标准的纳入。

    

    课程学习是一种计划好的学习材料序列，有效的课程学习可以使人类和机器的学习更高效和有效。最近的研究在语言应用中为训练图神经网络开发了有效的数据驱动的课程学习方法。然而，现有的课程学习方法在训练范式中通常只使用单一的困难度标准。在本文中，我们提出了一种新的课程学习视角，通过引入一种建立在图复杂性形式化（作为困难度标准）和模型能力之上的新方法来进行课程学习。该模型包括一个调度方案，通过考虑样本困难度和模型能力的不同视角在训练期间推导出有效的课程。所提出的解决方案在图神经网络的课程学习研究中取得了进展，能够在训练范式中纳入细粒度的图困难度标准。

    A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. E
    
[^30]: 大型语言模型在提取分子相互作用和通路知识方面的比较性能评估

    Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])

    [http://arxiv.org/abs/2307.08813](http://arxiv.org/abs/2307.08813)

    本研究评估了不同大型语言模型在提取分子相互作用和通路知识方面的有效性，并讨论了未来机遇和挑战。

    

    理解蛋白质相互作用和通路知识对于揭示生物系统的复杂性和研究生物功能和复杂疾病的基本机制至关重要。尽管现有的数据库提供了来自文献和其他源的策划生物数据，但它们往往不完整且维护工作繁重，因此需要替代方法。在本研究中，我们提出利用大型语言模型的能力，通过自动从相关科学文献中提取这些知识来解决这些问题。为了实现这个目标，在这项工作中，我们调查了不同大型语言模型在识别蛋白质相互作用、通路和基因调控关系等任务中的有效性。我们对不同模型的性能进行了彻底评估，突出了重要的发现，并讨论了这种方法所面临的未来机遇和挑战。代码和数据集链接可在论文中找到。

    Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and d
    
[^31]: 改进语言模型在数学问题上的性能的混合策略

    A mixed policy to improve performance of language models on math problems. (arXiv:2307.08767v1 [cs.CL])

    [http://arxiv.org/abs/2307.08767](http://arxiv.org/abs/2307.08767)

    本文提出了一种混合策略的探索方法，利用强化学习来改进语言模型在数学问题上的性能，通过在抽象层和第二层采用不同的探索方式，取得了超过2%的性能增益。

    

    在解决数学问题时，大多数语言模型采用采样策略根据条件概率预测下一个词。在数学推理过程中，可能会生成错误的答案。考虑到数学问题是确定性的，我们提出了一种混合策略的探索方法来解决数学问题，利用强化学习。我们提出了一个两级标记探索策略：抽象层以概率采样来决定下一个标记是运算符还是操作数，而第二层则以贪婪方式选择得分最高的下一个标记。我们使用GPT-2模型在GSM8K数据集上测试了我们的方法，并展示了超过2％的性能增益。我们的实现代码可在https://github.com/vividitytech/math_lm_rl找到。

    When to solve math problems, most language models take a sampling strategy to predict next word according conditional probabilities. In the math reasoning step, it may generate wrong answer. Considering math problems are deterministic, we propose a mixed policy exploration approach to solve math problems with reinforcement learning. In peculiar, we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic. Specifically, the abstract level policy will decide whether the token is operator or operand with probability sampling, while the second level is deterministic to select next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than $2\%$ performance gain. Our implementation is available at https://github.com/vividitytech/math_lm_rl.
    
[^32]: ivrit.ai：一份用于AI研究和开发的全面希伯来语音数据集

    ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development. (arXiv:2307.08720v1 [eess.AS])

    [http://arxiv.org/abs/2307.08720](http://arxiv.org/abs/2307.08720)

    ivrit.ai是一份全面希伯来语音数据集，包含超过3300小时的语音和一千多个说话者，可用于推动希伯来语自动语音识别技术的发展，具有重要的研究、开发和商业应用价值。

    

    我们介绍了一份名为"ivrit.ai"的全面希伯来语音数据集，填补了推动希伯来语自动语音识别（ASR）技术的资源缺乏问题。ivrit.ai包含了超过3300小时的语音和一千多个不同的说话者，涵盖了各种语境中的希伯来语音。它提供了三种形式的数据，以适应不同的研究需求：原始未处理的音频、经过语音活动检测的数据和部分带有转录的数据。这个数据集的突出特点是法律上可以无偿获得，成为研究人员、开发者和商业实体的重要资源。ivrit.ai开辟了许多应用领域，对增强希伯来语的人工智能能力具有巨大潜力。未来的努力将进一步扩展ivrit.ai，推动希伯来语在人工智能研究和技术中的地位。

    We introduce "ivrit.ai", a comprehensive Hebrew speech dataset, addressing the distinct lack of extensive, high-quality resources for advancing Automated Speech Recognition (ASR) technology in Hebrew. With over 3,300 speech hours and a over a thousand diverse speakers, ivrit.ai offers a substantial compilation of Hebrew speech across various contexts. It is delivered in three forms to cater to varying research needs: raw unprocessed audio; data post-Voice Activity Detection, and partially transcribed data. The dataset stands out for its legal accessibility, permitting use at no cost, thereby serving as a crucial resource for researchers, developers, and commercial entities. ivrit.ai opens up numerous applications, offering vast potential to enhance AI capabilities in Hebrew. Future efforts aim to expand ivrit.ai further, thereby advancing Hebrew's standing in AI research and technology.
    
[^33]: 低资源语言中金融交易数据的跨语言命名实体识别

    Cross-Lingual NER for Financial Transaction Data in Low-Resource Languages. (arXiv:2307.08714v1 [cs.CL])

    [http://arxiv.org/abs/2307.08714](http://arxiv.org/abs/2307.08714)

    我们提出了一个高效的跨语言命名实体识别的建模框架，利用知识蒸馏和一致性训练方法，在低资源语言中通过从英语到阿拉伯语的知识传递，能够准确识别商家、金额和其他字段。

    

    我们提出了一个高效的建模框架，用于跨语言的命名实体识别半结构化文本数据。我们的方法依赖于知识蒸馏和一致性训练。建模框架利用在源语言上预训练的大型语言模型（XLMRoBERTa）的知识，并采用了学生-教师关系（知识蒸馏）。学生模型在低资源目标语言上采用无监督的一致性训练（使用KL散度损失）。我们使用了两个独立的英语和阿拉伯语短信数据集，每个数据集都包含半结构化的银行交易信息，并重点展示从英语到阿拉伯语的知识传递。在仅有30个标记样本的情况下，我们的模型可以从英语泛化到阿拉伯语的识别商家、金额和其他字段。我们展示了我们的建模方法在效率上表现最佳，与DistilBERT等最先进的方法相比。

    We propose an efficient modeling framework for cross-lingual named entity recognition in semi-structured text data. Our approach relies on both knowledge distillation and consistency training. The modeling framework leverages knowledge from a large language model (XLMRoBERTa) pre-trained on the source language, with a student-teacher relationship (knowledge distillation). The student model incorporates unsupervised consistency training (with KL divergence loss) on the low-resource target language.  We employ two independent datasets of SMSs in English and Arabic, each carrying semi-structured banking transaction information, and focus on exhibiting the transfer of knowledge from English to Arabic. With access to only 30 labeled samples, our model can generalize the recognition of merchants, amounts, and other fields from English to Arabic. We show that our modeling approach, while efficient, performs best overall when compared to state-of-the-art approaches like DistilBERT pre-trained 
    
[^34]: 软件开发中的交流型代理

    Communicative Agents for Software Development. (arXiv:2307.07924v1 [cs.SE])

    [http://arxiv.org/abs/2307.07924](http://arxiv.org/abs/2307.07924)

    本文介绍了一种创新的软件开发范式，利用大型语言模型(LLMs)在整个软件开发过程中实现自然语言交流，消除了每个阶段需要专门模型的需求。该范式使用ChatDev作为一个虚拟聊天驱动的软件开发公司，通过设计、编码、测试和文档化四个阶段的代理人团队促进协作。

    

    软件工程是一个以微妙的直觉和咨询为特征的领域，决策过程复杂。深度学习的最新进展已经开始通过在软件开发的各个阶段实施精心设计来革新软件工程实践。在本文中，我们提出了一种创新的范式，通过自然语言交流，在整个软件开发过程中利用大型语言模型(LLMs)，简化和统一关键流程，从而消除了在每个阶段需要专门的模型的需要。这个范式的核心是ChatDev，一个虚拟的聊天驱动软件开发公司，它模仿了已经建立的瀑布模型，将开发过程细分为四个不同的时间阶段：设计、编码、测试和文档化。每个阶段都涉及一个团队的代理人，如程序员、代码审查人员和测试工程师，促进协作。

    Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborativ
    
[^35]: 大型语言模型中RLHF的秘密 第一部分：PPO

    Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])

    [http://arxiv.org/abs/2307.04964](http://arxiv.org/abs/2307.04964)

    本论文研究了大型语言模型中RLHF的秘密，重点关注了奖励模型、PPO和进程监督等技术路径，探索如何解决RLHF的稳定训练问题。

    

    大型语言模型（LLMs）为推动人工通用智能的进展提供了蓝图。其主要目标是成为以人为中心的（有益、诚实和无害）助手。与人类的对齐具有至关重要的意义，强化学习与人类反馈（RLHF）成为支撑这一追求的关键技术范式。当前的技术路线通常包括用于衡量人类偏好的奖励模型、用于优化策略模型输出的近端策略优化（PPO）以及用于改善逐步推理能力的进程监督。然而，由于奖励设计、环境交互和代理训练的挑战，再加上大型语言模型的试验成本巨大，对于AI研究人员来说，激励技术对齐和LLMs的安全着陆存在重大障碍。RLHF的稳定训练仍然是一个难题。

    Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first re
    
[^36]: 对大型语言模型评估的调查

    A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])

    [http://arxiv.org/abs/2307.03109](http://arxiv.org/abs/2307.03109)

    本文综述了大型语言模型（LLMs）的评估方法，关注三个关键维度：评估什么、在哪里评估以及如何评估。评估任务包括自然语言处理、推理、医学应用、伦理学、教育、自然和社会科学、代理应用等多个领域。本文为社会层面对LLMs潜在风险的理解提供了重要参考。

    

    大型语言模型（LLMs）由于在各种应用中表现出的前所未有的性能而在学术界和工业界越来越受欢迎。随着LLMs在研究和日常使用中继续发挥着重要作用，它们的评估变得越来越关键，不仅在任务水平上，而且在社会层面上，以更好地了解它们的潜在风险。在过去的几年里，已经做出了相当大的努力来从不同的角度来研究LLMs。本文综述了LLMs的这些评估方法，重点关注三个关键维度：评估什么、在哪里评估以及如何评估。首先，我们从评估任务的角度提供了一个概述，涵盖了一般的自然语言处理任务、推理、医学应用、伦理学、教育、自然科学和社会科学、代理应用和其他领域。其次，我们通过深入探讨评估方法和基准答案来回答“在哪里”和“如何”这两个问题。

    Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
    
[^37]: 使用进化调优增强LLM进行新闻摘要生成

    Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation. (arXiv:2307.02839v1 [cs.CL])

    [http://arxiv.org/abs/2307.02839](http://arxiv.org/abs/2307.02839)

    本论文提出一种新的方法使用LLM进行新闻摘要生成，通过进化调优事件模式群体，提高生成结果的准确性和可靠性。

    

    新闻摘要生成是情报分析领域中的重要任务，可以提供准确全面的信息，帮助人们更好地理解和应对复杂的现实事件。然而，传统的新闻摘要生成方法面临一些挑战，包括模型本身和训练数据量的限制，以及文本噪声的影响，使得准确生成可靠信息变得困难。本文提出了一种使用具有强大自然语言理解和生成能力的LLM进行新闻摘要生成的新范式。我们利用LLM从新闻段落中提取多个结构化事件模式，通过遗传算法进化事件模式群体，并选择最适应的事件模式输入LLM生成新闻摘要。设计了一个新闻摘要生成器(NSG)来选择和进化事件模式群体，并生成新闻摘要。

    News summary generation is an important task in the field of intelligence analysis, which can provide accurate and comprehensive information to help people better understand and respond to complex real-world events. However, traditional news summary generation methods face some challenges, which are limited by the model itself and the amount of training data, as well as the influence of text noise, making it difficult to generate reliable information accurately. In this paper, we propose a new paradigm for news summary generation using LLM with powerful natural language understanding and generative capabilities. We use LLM to extract multiple structured event patterns from the events contained in news paragraphs, evolve the event pattern population with genetic algorithm, and select the most adaptive event pattern to input into the LLM to generate news summaries. A News Summary Generator (NSG) is designed to select and evolve the event pattern populations and generate news summaries. T
    
[^38]: 在巴西葡萄牙语的语法错误修正方面评估GPT-3.5和GPT-4

    Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese. (arXiv:2306.15788v1 [cs.CL])

    [http://arxiv.org/abs/2306.15788](http://arxiv.org/abs/2306.15788)

    该研究评估了GPT-3.5和GPT-4在巴西葡萄牙语语法错误修正方面的有效性，结果显示虽然GPT-4的召回率较高，但语言模型倾向于过度修正。

    

    我们调查了GPT-3.5和GPT-4这两个大型语言模型在巴西葡萄牙语的语法错误修正（GEC）工具中的有效性，并将其性能与Microsoft Word和Google Docs进行了比较。我们引入了一个针对巴西葡萄牙语的GEC数据集，包括四个类别：语法、拼写、互联网和快速输入。我们的结果显示，尽管GPT-4的召回率比其他方法高，但语言模型倾向于具有较低的精确度，导致过度修正。这项研究展示了语言模型作为巴西葡萄牙语实际GEC工具的潜力，并鼓励进一步探索语言模型在非英语语言和其他教育环境中的应用。

    We investigate the effectiveness of GPT-3.5 and GPT-4, two large language models, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese and compare their performance against Microsoft Word and Google Docs. We introduce a GEC dataset for Brazilian Portuguese with four categories: Grammar, Spelling, Internet, and Fast typing. Our results show that while GPT-4 has higher recall than other methods, LLMs tend to have lower precision, leading to overcorrection. This study demonstrates the potential of LLMs as practical GEC tools for Brazilian Portuguese and encourages further exploration of LLMs for non-English languages and other educational settings.
    
[^39]: SparseOptimizer: 通过Moreau-Yosida正则化来降低语言模型的稀疏性，并通过编译器共同设计来加速

    SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])

    [http://arxiv.org/abs/2306.15656](http://arxiv.org/abs/2306.15656)

    SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。

    

    本文介绍了SparseOptimizer，一种新颖的深度学习优化器，通过Moreau-Yosida正则化在大型语言模型（如BERT，ALBERT和GPT）中自然地引入稀疏性。SparseOptimizer设计的关键是嵌入的收缩操作符，它在优化过程中直接引入稀疏性。这个操作符通过坚实的理论框架支持，并包含了一个分析解，从而增强了优化器的鲁棒性和效果。重要的是，SparseOptimizer的即插即用功能消除了对代码修改的需求，使其成为适用于各种大型语言模型的通用适应工具。在GLUE、RACE、SQuAD1和SQuAD2等基准数据集上的实证评估表明，通过SparseOptimizer稀疏化后的SparseBERT和SparseALBERT在性能上与密集型的BERT和ALBERT相当，同时显著减少了参数数量。

    This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
    
[^40]: LLM的多步推理中的两个自洽失败

    Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs. (arXiv:2305.14279v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14279](http://arxiv.org/abs/2305.14279)

    本论文研究了大型语言模型在多步推理中的自洽性问题，提出了假设自洽性和组合自洽性两个重要特性，并发现GPT-3/-4模型在这两方面都表现出了较差的一致性。

    

    大型语言模型（LLM）在各种上下文为基础的少样本任务上取得了广泛成功，但这种成功通常是通过正确性而不是一致性来评估的。我们认为在解决由多个子步骤的答案组成的任务的多步推理中，自洽性是一个重要的标准。我们提出了两种对于多步推理特别重要的自洽性类型：假设自洽性（模型在假设的其他上下文中的输出预测能力）和组合自洽性（当将中间子步骤替换为模型对这些步骤的输出时，模型的最终输出的一致性）。我们证明了GPT-3/-4模型的多个变体在多种任务上都表现出了低一致性率。

    Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.
    
[^41]: 评估开放式问答评估

    Evaluating Open-QA Evaluation. (arXiv:2305.12421v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12421](http://arxiv.org/abs/2305.12421)

    本研究侧重于评估开放式问答（Open-QA）任务的方法，引入了一个新的任务QA-Eval和数据集EVOUNA，通过人工评估方法来评估AI生成的答案的准确性。我们调查了与人工评估相关的方法，并讨论了当前方法的缺陷和改进方法。我们相信这对于未来的自动评估工具发展和研究具有价值。

    

    本研究侧重于对开放式问答（Open-QA）任务的评估，该任务可以直接估计大型语言模型（LLMs）的事实性。目前的自动评估方法已显示出一定的局限性，表明人工评估仍然是最可靠的方法。我们引入了一个新的任务，即评估QA评估（QA-Eval）以及相应的数据集EVOUNA，旨在评估AI生成的答案与Open-QA中的标准答案之间的准确性。我们利用人工标注的结果来评估这些方法的性能。具体而言，本研究调查了那些与人工评估具有高度相关性的方法，认为它们更可靠。我们还讨论了当前方法的缺陷以及改进基于LLM的评估器的方法。我们相信，这个新的QA-Eval任务和相应的数据集EVOUNA将促进更有效的自动评估工具的开发，并对未来的研究具有价值。

    This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this a
    
[^42]: PII的生命--一种PII混淆变换器

    Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v1 [cs.CL])

    [http://arxiv.org/abs/2305.09550](http://arxiv.org/abs/2305.09550)

    “Life of PII”是一种新颖的混淆变换器框架，用于将PII转化为人造PII同时尽可能地保留原始信息、意图和上下文，使我们能够有选择地混淆文档中的敏感信息，同时保留文档的统计和语义特性。

    

    在当今大型语言模型和数据驱动服务的世界中，保护敏感信息至关重要。一种常见的方法是使用数据扰动技术来减少(敏感)个人身份识别信息(PII)数据的过度实用性，同时保持其统计和语义特性。数据扰动方法经常导致显着的信息损失，使它们难以使用。在本文中，我们提出了“PII的生命”--一种新颖的混淆变换器框架，用于将PII转化为人造PII同时尽可能地保留原始信息、意图和上下文。我们的方法包括一个API来与给定的文档进行接口，一个基于配置的混淆器和一个基于Transformer架构的模型，在自然语言处理任务和LLMs中表现出高的上下文保存性能。我们的基于Transformer的方法学习了原始PII和其转换后的人造PII对应的映射，使我们能够有选择地混淆文档中的敏感信息，同时保留文档的统计和语义特性。

    Protecting sensitive information is crucial in today's world of Large Language Models (LLMs) and data-driven services. One common method used to preserve privacy is by using data perturbation techniques to reduce overreaching utility of (sensitive) Personal Identifiable Information (PII) data while maintaining its statistical and semantic properties. Data perturbation methods often result in significant information loss, making them impractical for use. In this paper, we propose 'Life of PII', a novel Obfuscation Transformer framework for transforming PII into faux-PII while preserving the original information, intent, and context as much as possible. Our approach includes an API to interface with the given document, a configuration-based obfuscator, and a model based on the Transformer architecture, which has shown high context preservation and performance in natural language processing tasks and LLMs.  Our Transformer-based approach learns mapping between the original PII and its tra
    
[^43]: GIFT: 基于图感知微调的多方对话理解

    GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding. (arXiv:2305.09360v1 [cs.CL])

    [http://arxiv.org/abs/2305.09360](http://arxiv.org/abs/2305.09360)

    GIFT是一个适用于多方对话理解的方法，通过设计四种类型的边缘将图感知信息集成到注意力机制中，改进了原始的顺序文本处理的PLM。

    

    最近，关于谁与谁在多方对话中说了什么的问题已经引起了很多研究的关注。然而，现有的多方对话理解方法通常将说话者和话语嵌入到顺序信息流中，或仅利用多方对话中固有图结构的表层信息。为此，我们提出了一种名为图感知微调（GIFT）的即插即用轻量级方法，可以适应各种基于Transformer预训练语言模型（PLMs）的通用多方对话理解。具体地，在普通Transformer中，话语之间的全等连接会忽略一个话语对另一个话语的稀疏但有区别的依赖关系。为了区分话语之间的不同关系，设计了四种类型的边缘以将图感知信号集成到注意机制中，以改进最初设计用于处理顺序文本的PLMs。我们通过将GIFT实现到三个PLMs并对其进行测试来评估GIFT。

    Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language models (PLMs) for universal MPC understanding. In detail, the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs. To distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine PLMs originally designed for processing sequential texts. We evaluate GIFT by implementing it into three PLMs, and test the
    
[^44]: LLM模型共同提取RCT报告中干预、结果和发现信息

    Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v1 [cs.CL])

    [http://arxiv.org/abs/2305.03642](http://arxiv.org/abs/2305.03642)

    本文提出了一种基于LLM调整的文本到文本模型，共同提取RCT报告中的干预、结果和发现信息，实现相当大的性能提升。

    

    随机对照试验（RCT）的结果确定干预措施的相对有效性，进而成为基于证据的医疗保健的关键输入。然而，RCT结果以（通常是非结构化的）自然语言文章的形式呈现，描述试验的设计、执行和结果；临床医生必须从这些文章中手动提取有关所关注的干预措施和结果的发现。这种繁琐的手动过程促使人们利用（半）自动化的方式从试验报告中提取结构化证据。在这项工作中，我们提出并评估了一个基于调整的大型语言模型（LLMs）的文本到文本模型，用于从临床摘要中共同提取干预措施、结果和比较因素（ICO元素），并推断相关的结果。人工（专家）和自动评估表明，将证据提取框架作为条件生成任务，为此目的微调LLMs可以实现相当大的（约20个点）性能提升。

    Results from Randomized Controlled Trials (RCTs) establish the comparative effectiveness of interventions, and are in turn critical inputs for evidence-based care. However, results from RCTs are presented in (often unstructured) natural language articles describing the design, execution, and outcomes of trials; clinicians must manually extract findings pertaining to interventions and outcomes of interest from such articles. This onerous manual process has motivated work on (semi-)automating extraction of structured evidence from trial reports. In this work we propose and evaluate a text-to-text model built on instruction-tuned Large Language Models (LLMs) to jointly extract Interventions, Outcomes, and Comparators (ICO elements) from clinical abstracts, and infer the associated results reported. Manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning LLMs for this purpose realizes considerable ($\sim$20 point 
    
[^45]: 基于人类词汇关联和图嵌入的波斯语话题检测

    Persian topic detection based on Human Word association and graph embedding. (arXiv:2302.09775v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.09775](http://arxiv.org/abs/2302.09775)

    本文提出了一种基于人类词汇关联的波斯语社交媒体话题检测框架，该方法利用了词汇关联和关联引力力量生成图，并通过嵌入和聚类方法提取话题。

    

    本文提出了一种基于人类词汇关联的社交媒体话题检测框架。在社交媒体中识别讨论的话题已经成为一个重要的挑战。目前这个领域的大部分工作都是在英语方面的，但是在波斯语方面也做了很多工作，特别是波斯语的微博。此外，现有的工作更多地关注探索频繁模式或语义关系，忽视了语言的结构方法。本文提出了一种使用人类词汇关联的话题检测框架。这种方法使用了模仿心理能力进行词汇关联。该方法还计算出了关联引力力量来显示词语的关联程度。利用这个参数，可以生成一个图，通过嵌入这个图并使用聚类方法，可以提取出话题。这个方法已经应用于从Telegram收集的波斯语数据集上。进行了多个实验研究。

    In this paper, we propose a framework to detect topics in social media based on Human Word Association. Identifying topics discussed in these media has become a critical and significant challenge. Most of the work done in this area is in English, but much has been done in the Persian language, especially microblogs written in Persian. Also, the existing works focused more on exploring frequent patterns or semantic relationships and ignored the structural methods of language. In this paper, a topic detection framework using HWA, a method for Human Word Association, is proposed. This method uses the concept of imitation of mental ability for word association. This method also calculates the Associative Gravity Force that shows how words are related. Using this parameter, a graph can be generated. The topics can be extracted by embedding this graph and using clustering methods. This approach has been applied to a Persian language dataset collected from Telegram. Several experimental studi
    
[^46]: 使用深度强化学习的基于执行的代码生成

    Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13816](http://arxiv.org/abs/2301.13816)

    使用深度强化学习的PPOCoder框架将预训练的编程语言模型和Proximal Policy Optimization技术结合，通过利用代码执行和结构对齐的非可微反馈，实现了更高效的代码生成。

    

    利用在大规模代码语料库上预训练的编程语言（PL）模型，作为自动化软件工程过程的手段，在代码完成、代码翻译和程序合成等各种代码生成任务中表现出了相当的潜力。然而，当前的方法主要依赖于从文本生成中借用的监督微调目标，忽视了代码的独特序列级特征，包括但不限于可编译性以及语法和功能正确性。为了解决这个限制，我们提出了PPOCoder，一种新的代码生成框架，它将预训练的PL模型与Proximal Policy Optimization（PPO）相结合，PPO是一种广泛使用的深度强化学习技术。通过利用代码执行和结构对齐的非可微反馈，PPOCoder将外部代码特定知识无缝集成到模型优化过程中。这是重要的。

    The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important
    
[^47]: 基于人类词汇联想的社交网络主题检测模型

    A Human Word Association based model for topic detection in social networks. (arXiv:2301.13066v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13066](http://arxiv.org/abs/2301.13066)

    本文提出了一个基于人类词汇联想的社交网络主题检测框架，通过考虑语言结构并设计专门的抽取算法，在FA-CUP数据集上取得了比其他方法更好的性能。

    

    随着社交网络的广泛使用，检测这些网络中讨论的主题已成为一个重要的挑战。目前的工作主要基于频繁模式挖掘或语义关系，而没有考虑语言结构。语言结构方法的意义在于发现词语之间的关系以及人类如何理解它们。因此，本文利用词汇联想的心理能力模拟概念，提出了一种基于人类词汇联想的社交网络主题检测框架。该框架基于人类词汇联想方法，并设计了专门的抽取算法。该方法在FA-CUP数据集上进行了评估，该数据集是主题检测领域的基准数据集。结果表明，与其他方法相比，所提出的方法在主题召回率和关键词F1值上有较好的改进。此外，主题检测领域中的大多数先前工作主要基于模式挖掘或语义关系。

    With the widespread use of social networks, detecting the topics discussed in these networks has become a significant challenge. The current works are mainly based on frequent pattern mining or semantic relations, and the language structure is not considered. The meaning of language structural methods is to discover the relationship between words and how humans understand them. Therefore, this paper uses the Concept of the Imitation of the Mental Ability of Word Association to propose a topic detection framework in social networks. This framework is based on the Human Word Association method. A special extraction algorithm has also been designed for this purpose. The performance of this method is evaluated on the FA-CUP dataset. It is a benchmark dataset in the field of topic detection. The results show that the proposed method is a good improvement compared to other methods, based on the Topic-recall and the keyword F1 measure. Also, most of the previous works in the field of topic de
    
[^48]: 差分隐私下的合成文本生成：一个简单而实用的方法

    Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe. (arXiv:2210.14348v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.14348](http://arxiv.org/abs/2210.14348)

    该论文介绍了一种简单而实用的方法，使用差分隐私对预训练的生成语言模型进行微调，能够生成具有强隐私保护的高质量合成文本，并且与非隐私版本相似。

    

    随着机器学习模型对敏感数据的记忆倾向引起越来越多的关注，隐私问题成为数据驱动产品的重要关注点。生成具有形式隐私保证的合成数据，如差分隐私（DP），为缓解这些隐私问题提供了一个有前途的方向。但是，以往的指向此方向的方法通常未能生成高质量的合成数据。在这项工作中，我们展示了差分隐私下文本领域的一个简单实用的方法：通过对经过预训练的生成语言模型进行微调并加入DP，使模型能够生成具有强隐私保护的有用的合成文本。通过对基准数据和私人客户数据的广泛实证分析，我们证明了我们的方法能够生成具有与非隐私版本相似的实用性的合成文本，同时提供强大的隐私保护，避免潜在的隐私泄露。

    Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path to mitigating these privacy concerns, but previous approaches in this direction have typically failed to produce synthetic data of high quality. In this work, we show that a simple and practical recipe in the text domain is effective: simply fine-tuning a pretrained generative language model with DP enables the model to generate useful synthetic text with strong privacy protection. Through extensive empirical analyses on both benchmark and private customer data, we demonstrate that our method produces synthetic text that is competitive in terms of utility with its non-private counterpart, meanwhile providing strong protection against potential privacy leakages.
    
[^49]: InitialGAN：一种完全随机初始化的语言生成对抗网络

    InitialGAN: A Language GAN with Completely Random Initialization. (arXiv:2208.02531v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.02531](http://arxiv.org/abs/2208.02531)

    本论文提出了一种名为InitialGAN的语言生成对抗网络，通过采用dropout采样和完全归一化的LSTM等技术，解决了当前语言GAN模型依赖预训练技术的限制，实现了完全随机初始化参数的模型。

    

    通过最大似然估计（MLE）训练的文本生成模型存在“曝光偏差”问题，而生成对抗网络（GAN）被证明可以解决这一问题。现有的语言GAN采用REINFORCE或连续弛豫等估计器来建模词概率。然而，这些估计器的固有局限性导致当前模型依赖预训练技术（MLE预训练或预训练嵌入）。然而，由于以前的尝试中这些表示建模方法的性能较差，因此很少被研究。我们的分析表明，无效的采样方法和不健康的梯度是导致这种不令人满意的性能的主要因素。在这项工作中，我们提出了两种解决这些问题的技巧：dropout采样和完全归一化的LSTM。基于这两种技术，我们提出了完全随机初始化参数的InitialGAN。此外，我们还引入了

    Text generative models trained via Maximum Likelihood Estimation (MLE) suffer from the notorious exposure bias problem, and Generative Adversarial Networks (GANs) are shown to have potential to tackle this problem. Existing language GANs adopt estimators like REINFORCE or continuous relaxations to model word probabilities. The inherent limitations of such estimators lead current models to rely on pre-training techniques (MLE pre-training or pre-trained embeddings). Representation modeling methods which are free from those limitations, however, are seldomly explored because of their poor performance in previous attempts. Our analyses reveal that invalid sampling methods and unhealthy gradients are the main contributors to such unsatisfactory performance. In this work, we present two techniques to tackle these problems: dropout sampling and fully normalized LSTM. Based on these two techniques, we propose InitialGAN whose parameters are randomly initialized in full. Besides, we introduce 
    
[^50]: 对文本中偏见度量的解释性和重要性：一种基于PMI的方法

    On the Interpretability and Significance of Bias Metrics in Texts: a PMI-based Approach. (arXiv:2104.06474v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2104.06474](http://arxiv.org/abs/2104.06474)

    这篇论文介绍了一种基于PMI的方法来解释和量化文本中的偏见，该方法提供了简单的解释和统计显著性，并在捕捉性别差距方面产生了类似于基于词嵌入的方法的结果。

    

    近年来，词嵌入广泛用于测量文本中的偏见。尽管它们已被证明在检测各种偏见方面非常有效，但基于词嵌入的度量缺乏透明度和解释性。我们分析了一种基于PMI的替代度量方法来量化文本中的偏见。它可以表示为条件概率的函数，可以简单地解释为词语共现。我们还证明它可以近似为一种比值比，这允许估计文本偏见的置信区间和统计显著性。该方法在捕捉嵌入大型语料库中真实世界的性别差距时产生类似于基于词嵌入的度量的结果。

    In recent years, word embeddings have been widely used to measure biases in texts. Even if they have proven to be effective in detecting a wide variety of biases, metrics based on word embeddings lack transparency and interpretability. We analyze an alternative PMI-based metric to quantify biases in texts. It can be expressed as a function of conditional probabilities, which provides a simple interpretation in terms of word co-occurrences. We also prove that it can be approximated by an odds ratio, which allows estimating confidence intervals and statistical significance of textual biases. This approach produces similar results to metrics based on word embeddings when capturing gender gaps of the real world embedded in large corpora.
    

