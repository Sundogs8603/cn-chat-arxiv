# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes.](http://arxiv.org/abs/2308.08494) | 该论文提出了一种概念化的机器学习方法，通过使用电子健康记录的审计日志作为监督，实现在特定临床背景下、特定时间点的笔记相关性检索。实验证明该方法在预测个别笔记撰写会话中哪些笔记会被阅读方面具有很高的准确性，并且临床医生的用户研究结果显示该框架可以帮助临床医生更高效地检索相关信息。 |
| [^2] | [Time Travel in LLMs: Tracing Data Contamination in Large Language Models.](http://arxiv.org/abs/2308.08493) | 该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。 |
| [^3] | [Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder.](http://arxiv.org/abs/2308.08488) | 本文提出了通过基于嘴唇-音素字级相关性的视觉预训练和跨模态融合编码器来改进视听语音识别的两种新技术。这些技术可以在预训练和微调阶段准确对齐音频和视频流，并且充分利用模态互补性。 |
| [^4] | [TBIN: Modeling Long Textual Behavior Data for CTR Prediction.](http://arxiv.org/abs/2308.08483) | TBIN模型通过局部敏感哈希算法和基于块位移的自注意力方法解决了利用长文本用户行为数据进行CTR预测时的截断问题和模型表达能力问题。 |
| [^5] | [Improving CTC-AED model with integrated-CTC and auxiliary loss regularization.](http://arxiv.org/abs/2308.08449) | 本文提出了使用集成CTC和辅助损失正则化改进CTC-AED模型的方法。实验结果表明，DAL方法在注意力重评分上表现更好，而PMP方法在CTC前缀搜索和贪婪搜索中表现卓越。 |
| [^6] | [Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction.](http://arxiv.org/abs/2308.08442) | 本论文提出了一种用于减轻曝光偏差的损失抽样方法，以改善句子级和段落级音素转换的性能。 |
| [^7] | [Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value Extraction.](http://arxiv.org/abs/2308.08413) | 这篇论文提出了一种知识增强的多标签少样本产品属性值提取方法，通过利用生成的标签描述和类别信息来学习更具有区分性的原型，并整合混合注意力来减少噪声和捕捉更多信息丰富的语义。实验结果表明，该方法在提取未见过的属性值对方面表现优于其他方法。 |
| [^8] | [Advancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation.](http://arxiv.org/abs/2308.08378) | 本文提出了一个系统的持续神经信息检索任务定义，并提供了一个模拟连续信息检索的多主题数据集。同时，还提出了一个全面的持续神经信息检索框架，能够防止灾难性遗忘并提高先前学习任务的性能。 |
| [^9] | [SummHelper: Collaborative Human-Computer Summarization.](http://arxiv.org/abs/2308.08363) | SummHelper是一个协作式人机摘要生成工具，通过两阶段的辅助过程，用户可以选择和修改文本内容，并生成一份连贯的摘要。用户研究显示该应用程序在自动化引导和个人输入之间取得了良好的平衡。 |
| [^10] | [Detoxify Language Model Step-by-Step.](http://arxiv.org/abs/2308.08295) | 这项研究提出了一种分步解毒语言模型的方法，通过在输入阶段进行解毒处理，并使用无毒提示进行连续生成来保持生成质量。同时，通过设计Detox-Chain来校准LLMs的推理能力，实现了更安全和可靠的生成。 |
| [^11] | [Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval.](http://arxiv.org/abs/2308.08285) | 本文研究了基于大型语言模型的文档扩展预训练对稠密通道检索的潜力，通过利用该方法进行查询生成并传递扩展的知识给检索器，实验证明这种方法显著提高了大规模网络搜索任务的检索性能。 |
| [^12] | [Benchmarking Neural Network Generalization for Grammar Induction.](http://arxiv.org/abs/2308.08253) | 提供了一种基于完全指定的形式语言的神经网络泛化度量方法，并在语法归纳任务中使用该基准评估了不同架构的网络。结果显示，使用最小描述长度目标（MDL）训练的网络泛化性能更好且使用更少的数据。 |
| [^13] | [TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series.](http://arxiv.org/abs/2308.08241) | 这篇论文总结了两种使用语言模型完成时间序列任务的策略，通过设计一种适用于语言模型的时间序列嵌入方法来激活语言模型对时间序列数据的能力。虽然结果没有明显超越当前最先进的模型，但可以更好地处理时间序列数据。 |
| [^14] | [MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation.](http://arxiv.org/abs/2308.08239) | MemoChat提出了一种用于调优指令的流程，通过让大型语言模型使用备忘录来保持对话一致性。实验证实了其有效性。 |
| [^15] | [Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey.](http://arxiv.org/abs/2308.08234) | 本论文调研了在自然语言处理中使用基于Transformer的多任务学习的挑战和机会。通过对NLP中基于Transformer的MTL方法以及典型机器学习生命周期各阶段的挑战进行讨论，提供了相关领域的概述和动向。 |
| [^16] | [MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented Pre-trained Language Models.](http://arxiv.org/abs/2308.08204) | 本文提出了MoCoSA方法，利用动量对比和结构增强的预训练语言模型，在知识图谱补全中解决了基于结构和基于描述方法的问题，实现了对语义丰富实体的泛化推理和对未见实体的鲁棒性。 |
| [^17] | [ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023.](http://arxiv.org/abs/2308.08181) | 中国电信的系统在VoxCeleb2023说话人识别挑战中取得了优异的表现，通过使用多个在VoxCeleb2上训练的ResNet模型，并进行融合和得分校准，最终获得了0.1066的minDCF和1.980%的EER。 |
| [^18] | [RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling Check.](http://arxiv.org/abs/2308.08176) | RSpell是一种用于中文拼写检查的检索增强框架，它使用拼音模糊匹配来搜索领域术语，并将其整合到CSC模型中。通过引入自适应过程控制机制和迭代策略，RSpell在零样本和微调场景下都能达到最先进的性能水平。 |
| [^19] | [Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System.](http://arxiv.org/abs/2308.08169) | 本文介绍了一种通过简单的缓存提高端到端任务导向型对话系统的性能的方法。通过微调检索模块并训练端到端的对话系统模型，系统可以动态更新并处理已知和未知的对话场景。实验证明我们的方法相对于强基线方法在非空联合目标准确率上提高了6.7%。 |
| [^20] | [Sarcasm Detection in a Disaster Context.](http://arxiv.org/abs/2308.08156) | 本文介绍了一个名为HurricaneSARC的数据集，其中包含了15,000条注释为讽刺意图的推文，并使用预训练语言模型进行了讽刺检测的研究。通过中间任务的迁移学习，我们的最佳模型在数据集上获得了0.70的F1值。 |
| [^21] | [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework.](http://arxiv.org/abs/2308.08155) | AutoGen是一种新的框架，通过多个可以互相对话的代理，实现了下一代LLM应用。它利用人类的理解和智能，优雅地处理不完美的生成和推理能力，并通过自动化代理对话简化了复杂的工作流程。 |
| [^22] | [Fast Training of NMT Model with Data Sorting.](http://arxiv.org/abs/2308.08153) | 本论文提出了一种通过对翻译句子对进行排序来节省计算能力的算法，用于提高Transformer模型的训练速度。实验证明，在保持性能的同时，通过这种方法可以显著减少计算时间。 |
| [^23] | [MDDial: A Multi-turn Differential Diagnosis Dialogue Dataset with Reliability Evaluation.](http://arxiv.org/abs/2308.08147) | MDDial是第一个英语差异诊断对话数据集，可以帮助构建和评估端到端的ADD对话系统，并引入了一种统一的评分方法来衡量ADD系统的可靠性。 |
| [^24] | [Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals.](http://arxiv.org/abs/2308.08125) | 本文提出了Radio2Text，这是第一个基于mmWave的流式自动语音识别（ASR）系统，具有超过13,000个词的词汇量。通过设计的流式Transformer和Guidance Initialization技术，Radio2Text实现了对流式ASR具有大词汇量的支持。 |
| [^25] | [Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation.](http://arxiv.org/abs/2308.08090) | 通过提取和消除反专家PEMs中的残缺能力来提升大规模语言模型的真实性和去毒性。 |
| [^26] | [The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models.](http://arxiv.org/abs/2308.08061) | 在部署大型语言模型时，需要关注泛化、评估和成本最优化。本文提出了一个针对大型语言模型的泛化、评估和成本建模框架，帮助企业深入了解和评估这些因素。 |
| [^27] | [DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue.](http://arxiv.org/abs/2308.08043) | DiagGPT将大型语言模型(LLMs)扩展到任务导向的对话场景，提供了在复杂诊断场景中主动提问和引导用户完成任务的能力。 |
| [^28] | [Using Artificial Populations to Study Psychological Phenomena in Neural Models.](http://arxiv.org/abs/2308.08032) | 利用人工群体进行语言模型研究，通过不确定性估计构建实验群体，并发现语言模型表现出典型行为。 |
| [^29] | [End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations.](http://arxiv.org/abs/2308.08027) | 本论文提出了一种基于多语言神经表示的端到端开放词汇关键词搜索模型，相比于传统的基于自动语音识别的关键词搜索系统，在长查询和不在训练集中的查询方面表现出色。 |
| [^30] | [Anaphoric Structure Emerges Between Neural Networks.](http://arxiv.org/abs/2308.07984) | 本研究通过研究神经网络之间是否能够出现类似于自然语言中回指结构的现象，发现带有回指结构的语言对神经模型来说是可学习的，这些结构会在模型之间自然地出现，并且增加对说话者效率的压力会增加这些结构的普遍性。 |
| [^31] | ["Beware of deception": Detecting Half-Truth and Debunking it through Controlled Claim Editing.](http://arxiv.org/abs/2308.07973) | 本研究针对互联网上半真相的广泛存在问题，提出了一个包括半真相检测模型和声明编辑模型的全面流程。通过利用T5模型进行受控声明编辑，我们的方法在虚假信息揭穿得分方面优于其他语言模型，并在半真相检测模型上创造了新的性能基准。 |
| [^32] | [MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction.](http://arxiv.org/abs/2308.07971) | MultiSChuBERT是一个多模态预测模型，通过结合文本和图像信息，在学术文档质量预测任务上取得了显著改进。我们的工作在结合视觉和文本嵌入、逐渐解冻视觉子模型权重以及采用最新文本嵌入替换标准BERT$_{\textrm{BASE}}$嵌入方面做出了重要贡献。 |
| [^33] | [Teach LLMs to Personalize -- An Approach inspired by Writing Education.](http://arxiv.org/abs/2308.07968) | 本研究提出了一种通用方法来进行个性化文本生成，通过教导大型语言模型（LLMs）借鉴写作教育实践，利用多阶段和多任务的框架来教导LLMs个性化生成，从而提升其生成能力。 |
| [^34] | [Automated Testing and Improvement of Named Entity Recognition Systems.](http://arxiv.org/abs/2308.07937) | 本论文提出了一种自动测试和修复命名实体识别系统的新方法TIN，该方法能够解决深度神经网络在特定情况下不可靠的问题，确保相似语境下相同命名实体的NER预测相同，从而提高系统的可靠性。 |
| [^35] | [Transforming Sentiment Analysis in the Financial Domain with ChatGPT.](http://arxiv.org/abs/2308.07935) | 本研究使用ChatGPT 3.5来进行金融情绪分析，特别关注外汇市场，通过零-shot提示方法，在精心策划的数据集上评估了其性能，并发现与传统模型相比，ChatGPT在金融情绪分析中表现出约35％的性能提升。 |
| [^36] | [Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment.](http://arxiv.org/abs/2308.07933) | 该论文提出了一种新的痴呆检测模型，将图片和描述文本作为输入，并利用大型预训练图像文本对齐模型的知识。通过观察发现，痴呆样本与健康样本在文本与图片相关性和图片焦点区域上存在差异，从而可以提高痴呆检测的准确性。 |
| [^37] | [Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation.](http://arxiv.org/abs/2308.07931) | 本论文通过精简特征场，将精确的3D几何与2D基础模型的丰富语义相结合，实现了对未见过的物体的少样本操作的泛化能力。 |
| [^38] | [SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search.](http://arxiv.org/abs/2308.07711) | 本论文提出了一种用于在Meituan搜索中进行相关性建模的新颖两阶段预训练和匹配架构。 |
| [^39] | [Metacognitive Prompting Improves Understanding in Large Language Models.](http://arxiv.org/abs/2308.05342) | 元认知提示 (MP) 是一种改进大型语言模型 (LLMs) 理解能力的策略。实验结果表明，使用MP的PaLM在各种自然语言理解任务中接近于GPT-4的性能水平。 |
| [^40] | [SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability.](http://arxiv.org/abs/2308.03266) | SeACo-Paraformer是一种具有灵活且有效的热词自定义能力的非自回归ASR系统，在大规模实验中表现优于基线模型，并提出了过滤大规模热词的有效方法。 |
| [^41] | [EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction.](http://arxiv.org/abs/2307.16082) | 本文提出了一个利用词汇、语义和上下文表示的框架，旨在解决现有事件检测方法在识别新兴社交事件方面的局限性，并提供了对社交数据进行丰富的上下文化处理的方法。 |
| [^42] | [LLM-Rec: Personalized Recommendation via Prompting Large Language Models.](http://arxiv.org/abs/2307.15780) | 本文通过引导大型语言模型进行个性化推荐的研究，提出了四种不同的引导策略，并通过实验证明了这些策略的有效性。这一发现强调了在个性化内容推荐中，采用多样的引导和输入增强技术可以提高大型语言模型的推荐性能。 |
| [^43] | [Leveraging Large Language Models for Mental Health Prediction via Online Text Data.](http://arxiv.org/abs/2307.14385) | 本研究首次对多种大型语言模型在心理健康预测任务上进行了全面评估，结果表明指令微调可以显著提升模型性能，并且最优微调模型在平衡准确度上胜过GPT-3.5，并与最先进的任务特定模型持平。 |
| [^44] | [Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education.](http://arxiv.org/abs/2307.12267) | 本研究探索了在教育领域中，由人类和生成性语言模型协作编写的混合文本的AI内容检测方法，将其形式化为识别转换点的任务，以区分人类编写和AI生成的部分。 |
| [^45] | [LLM Cognitive Judgements Differ From Human.](http://arxiv.org/abs/2307.11787) | 这项研究调查了大型语言模型在认知任务中的表现，并发现它们的认知判断与人类不同。 |
| [^46] | [Zero-shot NLG evaluation through Pairware Comparisons with LLMs.](http://arxiv.org/abs/2307.07889) | 本研究提出了一种使用开源大型语言模型进行零样本自然语言生成（NLG）评估的方法，通过配对比较判定来确定候选回应的优劣。结果表明，相较于绝对评分，比较评估是一种更有效的方法，并使得较小的开源LLMs达到了与更大的公共访问API相当的性能。 |
| [^47] | [Towards the extraction of robust sign embeddings for low resource sign language recognition.](http://arxiv.org/abs/2306.17558) | 本研究的目标是实现对低资源手势语言识别的稳健手势嵌入提取。针对当前存在的问题，人体姿势估计器虽然是理想的选择，但由于域不匹配和手势语言中的挑战性姿势，其在手势语言数据上的稳健性有所欠缺。关键点基于的模型仍然优于图像基于的模型，但其训练方式限制了其在手势语言识别中的应用。 |
| [^48] | [Probing Quantifier Comprehension in Large Language Models.](http://arxiv.org/abs/2306.07384) | 本文提出了对于大型语言模型（LLMs）对量化理解的探究，并质疑之前研究中关于LLMs理解极少数类型的量词能力呈现反比例缩放的说法，并提出新的测试方法，展示其与以前研究所展示的行为不同。 |
| [^49] | [Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes.](http://arxiv.org/abs/2306.04306) | 本文提出了一种跨语言音素识别系统Allophant，结合组成性音素嵌入方法和个别监督的语音属性分类器，并采用多任务结构，使得该系统能够低资源进行语音识别，可以有效提高处理陌生音素和音素库的能力。 |
| [^50] | [LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization.](http://arxiv.org/abs/2306.01102) | 本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。 |
| [^51] | [Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization.](http://arxiv.org/abs/2305.11095) | 本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。 |
| [^52] | [SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.](http://arxiv.org/abs/2305.09781) | SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。 |
| [^53] | [T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering.](http://arxiv.org/abs/2305.03453) | 本研究使用大型语言模型信号教育科学问题回答的链式思维推理，通过生成高质量COT合理化信号，同时降低了人工注释的需求。 |
| [^54] | [Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition.](http://arxiv.org/abs/2304.08862) | 本文提出了一种使用近似最近邻短语挖掘的方法来训练上下文感知Transformer转录器(CATT)模型，并在大规模数据情况下进行了实验，取得了显著的实验结果。 |
| [^55] | [Black Box Few-Shot Adaptation for Vision-Language models.](http://arxiv.org/abs/2304.01752) | 本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的视觉-语言模型的快速少样本适应，适用于有监督和无监督训练，并且可以用于对单模型计算的图像和文本特征进行对齐。 |
| [^56] | [An interpretability framework for Similar case matching.](http://arxiv.org/abs/2304.01622) | 本论文提出了一个可解释的相似案例匹配框架，其中包括四个模块：司法特征句子识别模块、案例匹配模块、特征句子对齐模块和冲突消歧模块。该框架通过识别案例中的重要信息和对齐两个案例中的特征句，提供了可靠的相似性证据。 |
| [^57] | [Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP.](http://arxiv.org/abs/2303.16166) | 在NLP研究中，我们不能仅凭感知质量假定代码正确性，应该推动采用编码最佳实践以提高实验结果的正确性和可靠性。 |
| [^58] | [Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator.](http://arxiv.org/abs/2302.14036) | 本文提出了一种使用集成文本到梅尔频谱生成器的无标记转写领域自适应端到端ASR系统。该系统可以在训练过程中动态生成梅尔频谱，并通过使用新领域的仅文本数据来适应ASR模型。研究结果表明，所提出的训练方法显著提高了ASR准确性，并在自适应质量和训练速度上超过了级联TTS系统与声码器。 |
| [^59] | [Editing Language Model-based Knowledge Graph Embeddings.](http://arxiv.org/abs/2301.10405) | 本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。 |
| [^60] | [Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale.](http://arxiv.org/abs/2212.09095) | 本文通过使用一个660亿参数的语言模型，在多个任务中发现了上下文学习能力并不均匀分布在其各个组件上。通过移除约70%的注意力头和约20%的前馈网络，任务执行表现仅有轻微下降。此外，在OPT-66B中，存在一小部分注意力头对于上下文学习中的基础归纳操作具有高效能力。 |
| [^61] | [Event and Entity Extraction from Generated Video Captions.](http://arxiv.org/abs/2211.02982) | 该论文提出了一个从生成的视频字幕中提取语义元数据的框架，通过使用密集视频字幕模型，可以提取实体、实体属性、实体之间的关系和视频分类。提取信息的质量受到事件定位质量和字幕生成性能的影响。 |
| [^62] | [Analyzing the Limits of Self-Supervision in Handling Bias in Language.](http://arxiv.org/abs/2112.08637) | 本文分析了自监督在处理语言偏见中的局限性，并定义了四个偏见任务（诊断、识别、提取和改写），通过使用不同类别的任务描述来评估语言模型对语义的捕捉能力。 |

# 详细

[^1]: 用于动态电子健康记录信息检索的机器学习概念化

    Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes. (arXiv:2308.08494v1 [cs.IR])

    [http://arxiv.org/abs/2308.08494](http://arxiv.org/abs/2308.08494)

    该论文提出了一种概念化的机器学习方法，通过使用电子健康记录的审计日志作为监督，实现在特定临床背景下、特定时间点的笔记相关性检索。实验证明该方法在预测个别笔记撰写会话中哪些笔记会被阅读方面具有很高的准确性，并且临床医生的用户研究结果显示该框架可以帮助临床医生更高效地检索相关信息。

    

    临床医生花费大量时间筛选病人笔记并在电子健康记录（EHR）中记录是临床医生倦怠的主要原因。通过在记录过程中主动和动态地检索相关笔记，我们可以减少查找相关病例历史所需的工作量。在这项工作中，我们概念化了使用EHR审计日志作为机器学习的来源，以监督特定临床背景下、特定时间点的笔记相关性。我们的评估重点放在紧急科室的动态检索上，这是一个具有独特信息检索和笔记编写模式的高重症设置。我们显示我们的方法在预测哪些笔记会在个别笔记撰写会话中被阅读方面可以实现0.963的AUC。此外，我们对多名临床医生进行了用户研究，发现我们的框架可以帮助临床医生更高效地检索相关信息。通过展示我们的框架和...

    The large amount of time clinicians spend sifting through patient notes and documenting in electronic health records (EHRs) is a leading cause of clinician burnout. By proactively and dynamically retrieving relevant notes during the documentation process, we can reduce the effort required to find relevant patient history. In this work, we conceptualize the use of EHR audit logs for machine learning as a source of supervision of note relevance in a specific clinical context, at a particular point in time. Our evaluation focuses on the dynamic retrieval in the emergency department, a high acuity setting with unique patterns of information retrieval and note writing. We show that our methods can achieve an AUC of 0.963 for predicting which notes will be read in an individual note writing session. We additionally conduct a user study with several clinicians and find that our framework can help clinicians retrieve relevant information more efficiently. Demonstrating that our framework and m
    
[^2]: LLM中的时间旅行：追踪大型语言模型中的数据污染

    Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])

    [http://arxiv.org/abs/2308.08493](http://arxiv.org/abs/2308.08493)

    该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。

    

    数据污染是指大型语言模型（LLMs）的训练数据中存在来自下游任务的测试数据，这可能是理解LLMs在其他任务上有效性的一个重要问题。我们提出了一种简单而有效的方法来识别LLMs中的数据污染。我们的方法核心是通过识别从小的随机样本中抽取的单个实例中的潜在污染，然后评估整个数据集分区是否受到污染。为了估计单个实例的污染程度，我们使用了“引导指令”：即一个由数据集名称、分区类型和参考实例的初始部分组成的提示，要求LLM完成它。如果LLM的输出与参考实例的后一部分完全或接近匹配，那么该实例被标记为受到污染。为了了解整个分区是否受到污染，我们提出了两个想法。第一个想法是标记一个数据集的分区，该分区中的实例大多数都被判断为受到污染。

    Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
    
[^3]: 通过基于嘴唇-音素字级相关性的视觉预训练和跨模态融合编码器来改进视听语音识别

    Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder. (arXiv:2308.08488v1 [cs.CL])

    [http://arxiv.org/abs/2308.08488](http://arxiv.org/abs/2308.08488)

    本文提出了通过基于嘴唇-音素字级相关性的视觉预训练和跨模态融合编码器来改进视听语音识别的两种新技术。这些技术可以在预训练和微调阶段准确对齐音频和视频流，并且充分利用模态互补性。

    

    最近的研究中观察到，在低质量视频的端到端框架下，从自动语音识别系统到视听语音识别系统的性能略有改进。据认为，音频和视觉模态之间不匹配的收敛速度和专门的输入表示导致了这个问题。在本文中，我们提出了两种新技术来改进视听语音识别（AVSR）在预训练和微调训练框架下。首先，我们探索了普通话中嘴唇形状和音节级音素字单元之间的相关性，以建立准确的帧级音节边界。这使得在视觉模型预训练和跨模态融合过程中能够对齐视频和音频流。接下来，我们提出了一种音频引导的跨模态融合编码器（CMFE）神经网络，利用主要训练参数来实现多个跨模态注意力层的充分利用模态互补性。在实验上进行了验证

    In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the
    
[^4]: TBIN: 模型化长文本行为数据用于CTR预测

    TBIN: Modeling Long Textual Behavior Data for CTR Prediction. (arXiv:2308.08483v1 [cs.IR])

    [http://arxiv.org/abs/2308.08483](http://arxiv.org/abs/2308.08483)

    TBIN模型通过局部敏感哈希算法和基于块位移的自注意力方法解决了利用长文本用户行为数据进行CTR预测时的截断问题和模型表达能力问题。

    

    点击率（CTR）预测在推荐系统的成功中起着关键作用。受到最近语言模型（LMs）的繁荣影响，许多研究通过以文本格式组织用户行为数据，并利用LMs来在语义层面上理解用户兴趣来改进预测。虽然有前景，但这些研究不得不截断文本数据以减少LMs中自注意力的二次计算开销。然而，已经研究表明长时间的用户行为数据可以显著提高CTR预测。此外，这些工作通常将用户的多样化兴趣压缩成一个特征向量，这阻碍了模型的表达能力。本文提出了一种基于文本行为的兴趣切块网络（TBIN），通过结合高效的局部敏感哈希算法和基于块位移的自注意力方法来解决上述限制。得到的用户多样化兴趣是

    Click-through rate (CTR) prediction plays a pivotal role in the success of recommendations. Inspired by the recent thriving of language models (LMs), a surge of works improve prediction by organizing user behavior data in a \textbf{textual} format and using LMs to understand user interest at a semantic level. While promising, these works have to truncate the textual data to reduce the quadratic computational overhead of self-attention in LMs. However, it has been studied that long user behavior data can significantly benefit CTR prediction. In addition, these works typically condense user diverse interests into a single feature vector, which hinders the expressive capability of the model. In this paper, we propose a \textbf{T}extual \textbf{B}ehavior-based \textbf{I}nterest Chunking \textbf{N}etwork (TBIN), which tackles the above limitations by combining an efficient locality-sensitive hashing algorithm and a shifted chunk-based self-attention. The resulting user diverse interests are
    
[^5]: 使用集成CTC和辅助损失正则化改进CTC-AED模型

    Improving CTC-AED model with integrated-CTC and auxiliary loss regularization. (arXiv:2308.08449v1 [cs.CL])

    [http://arxiv.org/abs/2308.08449](http://arxiv.org/abs/2308.08449)

    本文提出了使用集成CTC和辅助损失正则化改进CTC-AED模型的方法。实验结果表明，DAL方法在注意力重评分上表现更好，而PMP方法在CTC前缀搜索和贪婪搜索中表现卓越。

    

    连接主义时间分类（CTC）和基于注意力编码器解码器（AED）的联合训练已广泛应用于自动语音识别（ASR）。与大多数分开计算CTC和AED损失的混合模型不同，我们提出的集成CTC利用AED的注意力机制来指导CTC的输出。本文采用两种融合方法，即直接相加的logits（DAL）和保留最大概率（PMP）。通过自适应仿射变换注意结果以匹配CTC的维度，我们实现了维度一致性。为了加快模型收敛速度和提高准确性，我们引入了辅助损失正则化以加速收敛。实验结果表明，DAL方法在注意力重评分上表现更好，而PMP方法在CTC前缀搜索和贪婪搜索中表现卓越。

    Connectionist temporal classification (CTC) and attention-based encoder decoder (AED) joint training has been widely applied in automatic speech recognition (ASR). Unlike most hybrid models that separately calculate the CTC and AED losses, our proposed integrated-CTC utilizes the attention mechanism of AED to guide the output of CTC. In this paper, we employ two fusion methods, namely direct addition of logits (DAL) and preserving the maximum probability (PMP). We achieve dimensional consistency by adaptively affine transforming the attention results to match the dimensions of CTC. To accelerate model convergence and improve accuracy, we introduce auxiliary loss regularization for accelerated convergence. Experimental results demonstrate that the DAL method performs better in attention rescoring, while the PMP method excels in CTC prefix beam search and greedy search.
    
[^6]: 减轻句子级音素转换中的曝光偏差

    Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction. (arXiv:2308.08442v1 [cs.CL])

    [http://arxiv.org/abs/2308.08442](http://arxiv.org/abs/2308.08442)

    本论文提出了一种用于减轻曝光偏差的损失抽样方法，以改善句子级和段落级音素转换的性能。

    

    文本转文本传输转换器 (T5) 最近被考虑用于音素图( G2P )转换。作为后续研究，一种基于 T5 的无分词字节级模型 ByT5，在表示每个输入字符时使用其相应的 UTF-8 编码，最近在单词级 G2P 转换方面取得了有希望的结果。虽然人们普遍认为句子级或段落级 G2P 在实际应用中能提高可用性，因为它更适合处理异音字和单词之间的连接音，但我们发现在这些情况下使用 ByT5 并不简单。由于 ByT5 在字符级别上操作，它需要较长的解码步骤，这会因自回归生成模型中常见的曝光偏差而导致性能下降。本文通过采用我们提出的基于损失的抽样方法，展示了通过减轻这种曝光偏差可以提高句子级和段落级 G2P 的性能。

    Text-to-Text Transfer Transformer (T5) has recently been considered for the Grapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free byte-level model based on T5 referred to as ByT5, recently gave promising results on word-level G2P conversion by representing each input character with its corresponding UTF-8 encoding. Although it is generally understood that sentence-level or paragraph-level G2P can improve usability in real-world applications as it is better suited to perform on heteronyms and linking sounds between words, we find that using ByT5 for these scenarios is nontrivial. Since ByT5 operates on the character level, it requires longer decoding steps, which deteriorates the performance due to the exposure bias commonly observed in auto-regressive generation models. This paper shows that the performance of sentence-level and paragraph-level G2P can be improved by mitigating such exposure bias using our proposed loss-based sampling method.
    
[^7]: 知识增强的多标签少样本产品属性值提取

    Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value Extraction. (arXiv:2308.08413v1 [cs.IR])

    [http://arxiv.org/abs/2308.08413](http://arxiv.org/abs/2308.08413)

    这篇论文提出了一种知识增强的多标签少样本产品属性值提取方法，通过利用生成的标签描述和类别信息来学习更具有区分性的原型，并整合混合注意力来减少噪声和捕捉更多信息丰富的语义。实验结果表明，该方法在提取未见过的属性值对方面表现优于其他方法。

    

    现有的属性值提取（AVE）模型需要大量的标记数据进行训练。然而，现实世界中的电子商务每天都会有带有新属性值对的新产品进入市场。因此，我们在多标签少样本学习（FSL）中制定AVE，旨在基于少量的训练示例提取未见过的属性值对。我们提出了一种基于原型网络的知识增强注意力框架（KEAF），利用生成的标签描述和类别信息来学习更具有区分性的原型。此外，KEAF通过计算与标签相关的权重和查询相关的权重，整合了混合注意力，以减少噪声并捕捉更多信息丰富的语义。为了实现多标签推理，KEAF进一步通过整合支持集和查询集的语义信息来学习动态阈值。在两个数据集上进行的大量实验和消融研究表明，KEAF的性能优于其他方法。

    Existing attribute-value extraction (AVE) models require large quantities of labeled data for training. However, new products with new attribute-value pairs enter the market every day in real-world e-Commerce. Thus, we formulate AVE in multi-label few-shot learning (FSL), aiming to extract unseen attribute value pairs based on a small number of training examples. We propose a Knowledge-Enhanced Attentive Framework (KEAF) based on prototypical networks, leveraging the generated label description and category information to learn more discriminative prototypes. Besides, KEAF integrates with hybrid attention to reduce noise and capture more informative semantics for each class by calculating the label-relevant and query-related weights. To achieve multi-label inference, KEAF further learns a dynamic threshold by integrating the semantic information from both the support set and the query set. Extensive experiments with ablation studies conducted on two datasets demonstrate that KEAF outpe
    
[^8]: 推进神经信息检索中的持续终身学习：定义、数据集、框架和实证评估

    Advancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation. (arXiv:2308.08378v1 [cs.IR])

    [http://arxiv.org/abs/2308.08378](http://arxiv.org/abs/2308.08378)

    本文提出了一个系统的持续神经信息检索任务定义，并提供了一个模拟连续信息检索的多主题数据集。同时，还提出了一个全面的持续神经信息检索框架，能够防止灾难性遗忘并提高先前学习任务的性能。

    

    持续学习是指机器学习模型在学习和适应新信息的同时，不影响其在先前学习任务上的性能。尽管已有多项研究探讨了信息检索任务中的持续学习方法，但仍缺乏明确的任务定义，并且目前尚不清楚在这种背景下典型的学习策略的表现如何。为了应对这一挑战，本文提出了一种系统的持续神经信息检索任务定义，并提供了一个模拟连续信息检索的多主题数据集。随后，本文提出了一个全面的持续神经信息检索框架，包括典型检索模型和持续学习策略。实证评估结果表明，所提出的框架能够成功地防止神经信息检索中的灾难性遗忘，并提高先前学习任务的性能。结果表明，基于嵌入的检索方式较传统的基于索引的检索方式具有优势，并且持续学习策略能够有效地提升检索性能。

    Continual learning refers to the capability of a machine learning model to learn and adapt to new information, without compromising its performance on previously learned tasks. Although several studies have investigated continual learning methods for information retrieval tasks, a well-defined task formulation is still lacking, and it is unclear how typical learning strategies perform in this context. To address this challenge, a systematic task formulation of continual neural information retrieval is presented, along with a multiple-topic dataset that simulates continuous information retrieval. A comprehensive continual neural information retrieval framework consisting of typical retrieval models and continual learning strategies is then proposed. Empirical evaluations illustrate that the proposed framework can successfully prevent catastrophic forgetting in neural information retrieval and enhance performance on previously learned tasks. The results indicate that embedding-based retr
    
[^9]: SummHelper：协作式人机摘要生成

    SummHelper: Collaborative Human-Computer Summarization. (arXiv:2308.08363v1 [cs.CL])

    [http://arxiv.org/abs/2308.08363](http://arxiv.org/abs/2308.08363)

    SummHelper是一个协作式人机摘要生成工具，通过两阶段的辅助过程，用户可以选择和修改文本内容，并生成一份连贯的摘要。用户研究显示该应用程序在自动化引导和个人输入之间取得了良好的平衡。

    

    当前的文本摘要方法主要是自动化的，对人类干预和控制的空间有限。本文介绍了SummHelper，一个两阶段的摘要辅助系统，旨在促进人机协作。初始阶段涉及内容选择，系统推荐潜在内容，允许用户接受、修改或引入其他选择。随后的内容整合阶段，SummHelper从这些选择中生成一个连贯的摘要，用户可以利用摘要与源文本之间的可视映射进行优化。小规模用户研究显示我们的应用程序的有效性，参与者特别赞赏自动引导和个人输入机会之间的平衡。

    Current approaches for text summarization are predominantly automatic, with rather limited space for human intervention and control over the process. In this paper, we introduce SummHelper, a 2-phase summarization assistant designed to foster human-machine collaboration. The initial phase involves content selection, where the system recommends potential content, allowing users to accept, modify, or introduce additional selections. The subsequent phase, content consolidation, involves SummHelper generating a coherent summary from these selections, which users can then refine using visual mappings between the summary and the source text. Small-scale user studies reveal the effectiveness of our application, with participants being especially appreciative of the balance between automated guidance and opportunities for personal input.
    
[^10]: 分步解毒语言模型

    Detoxify Language Model Step-by-Step. (arXiv:2308.08295v1 [cs.CL])

    [http://arxiv.org/abs/2308.08295](http://arxiv.org/abs/2308.08295)

    这项研究提出了一种分步解毒语言模型的方法，通过在输入阶段进行解毒处理，并使用无毒提示进行连续生成来保持生成质量。同时，通过设计Detox-Chain来校准LLMs的推理能力，实现了更安全和可靠的生成。

    

    解毒语言模型具有挑战性，因为它要求模型在保持生成能力的同时避免生成有害内容。为了确保生成的安全性，先前的解毒方法通过改变数据分布或在单步骤中从不同方面约束生成来解毒模型。然而，由于语言模型倾向于沿着有毒提示生成，解毒方法的工作方向与之相反，这些方法将大大影响LLM的生成质量，如话语连贯性和语义一致性。为了处理这种冲突，我们将解毒过程分解为不同的子步骤，其中解毒集中在输入阶段，随后的连续生成基于无毒提示。此外，我们还通过设计一个Detox-Chain来校准LLMs的强大推理能力，以有序的方式连接上述子步骤，这使得LLMs可以进行连续的解毒生成。

    Detoxification for LLMs is challenging since it requires models to avoid generating harmful content while maintaining the generation capability. To ensure the safety of generations, previous detoxification methods detoxify the models by changing the data distributions or constraining the generations from different aspects in a single-step manner. However, these approaches will dramatically affect the generation quality of LLMs, e.g., discourse coherence and semantic consistency, since language models tend to generate along the toxic prompt while detoxification methods work in the opposite direction. To handle such a conflict, we decompose the detoxification process into different sub-steps, where the detoxification is concentrated in the input stage and the subsequent continual generation is based on the non-toxic prompt. Besides, we also calibrate the strong reasoning ability of LLMs by designing a Detox-Chain to connect the above sub-steps in an orderly manner, which allows LLMs to d
    
[^11]: 基于大型语言模型的文档扩展预训练用于稠密通道检索

    Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval. (arXiv:2308.08285v1 [cs.IR])

    [http://arxiv.org/abs/2308.08285](http://arxiv.org/abs/2308.08285)

    本文研究了基于大型语言模型的文档扩展预训练对稠密通道检索的潜力，通过利用该方法进行查询生成并传递扩展的知识给检索器，实验证明这种方法显著提高了大规模网络搜索任务的检索性能。

    

    本文系统地研究了基于大型语言模型（LLM）的文档扩展预训练在稠密通道检索中的潜力。具体来说，我们利用LLMs的能力进行文档扩展，即查询生成，并通过针对通道检索的预训练策略有效地将扩展的知识传递给检索器。这些策略包括对比学习和瓶颈查询生成。此外，我们还采用了课程学习策略来减少对LLM推理的依赖。实验结果表明，基于LLM的文档扩展预训练显著提高了大规模网络搜索任务的检索性能。我们的工作展示了强大的零-shot和跨领域检索能力，在没有人工标注数据的情况下更具广泛的应用性。

    In this paper, we systematically study the potential of pre-training with Large Language Model(LLM)-based document expansion for dense passage retrieval. Concretely, we leverage the capabilities of LLMs for document expansion, i.e. query generation, and effectively transfer expanded knowledge to retrievers using pre-training strategies tailored for passage retrieval. These strategies include contrastive learning and bottlenecked query generation. Furthermore, we incorporate a curriculum learning strategy to reduce the reliance on LLM inferences. Experimental results demonstrate that pre-training with LLM-based document expansion significantly boosts the retrieval performance on large-scale web-search tasks. Our work shows strong zero-shot and out-of-domain retrieval abilities, making it more widely applicable for retrieval when initializing with no human-labeled data.
    
[^12]: 神经网络泛化性能在语法归纳任务中的基准评估

    Benchmarking Neural Network Generalization for Grammar Induction. (arXiv:2308.08253v1 [cs.CL])

    [http://arxiv.org/abs/2308.08253](http://arxiv.org/abs/2308.08253)

    提供了一种基于完全指定的形式语言的神经网络泛化度量方法，并在语法归纳任务中使用该基准评估了不同架构的网络。结果显示，使用最小描述长度目标（MDL）训练的网络泛化性能更好且使用更少的数据。

    

    神经网络的泛化能力如何？即使对于语法归纳任务这样目标泛化完全已知的任务，以前的工作也未能给出明确的答案，只在训练集之外进行了非常有限的测试，并使用不同的成功标准。我们提出了一种基于完全指定的形式语言的神经网络泛化度量方法。给定一个模型和一个形式语法，该方法根据模型在未见样本上的泛化能力分配一个泛化得分，这个得分与模型训练所使用的数据量成反比。这个基准包含了诸如$a^nb^n$，$a^nb^nc^n$，$a^nb^mc^{n+m}$以及Dyck-1和2等语言。我们使用这个基准评估了一些架构，并发现使用最小描述长度目标（MDL）训练的网络比使用标准损失函数训练的网络泛化性能更好且使用更少的数据。该基准可在https://github.com/taucompling/bliss找到。

    How well do neural networks generalize? Even for grammar induction tasks, where the target generalization is fully known, previous works have left the question open, testing very limited ranges beyond the training set and using different success criteria. We provide a measure of neural network generalization based on fully specified formal languages. Given a model and a formal grammar, the method assigns a generalization score representing how well a model generalizes to unseen samples in inverse relation to the amount of data it was trained on. The benchmark includes languages such as $a^nb^n$, $a^nb^nc^n$, $a^nb^mc^{n+m}$, and Dyck-1 and 2. We evaluate selected architectures using the benchmark and find that networks trained with a Minimum Description Length objective (MDL) generalize better and using less data than networks trained using standard loss functions. The benchmark is available at https://github.com/taucompling/bliss.
    
[^13]: TEST: 文本原型对齐嵌入以激活LLM对时间序列的能力

    TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series. (arXiv:2308.08241v1 [cs.CL])

    [http://arxiv.org/abs/2308.08241](http://arxiv.org/abs/2308.08241)

    这篇论文总结了两种使用语言模型完成时间序列任务的策略，通过设计一种适用于语言模型的时间序列嵌入方法来激活语言模型对时间序列数据的能力。虽然结果没有明显超越当前最先进的模型，但可以更好地处理时间序列数据。

    

    本研究总结了两种使用现代语言模型（LLM）完成时间序列（TS）任务的策略：LLM-for-TS，设计和训练一个针对TS数据的基础大模型；TS-for-LLM，使预训练的LLM能够处理TS数据。鉴于数据积累不足、资源有限和语义上下文需求，本研究侧重于TS-for-LLM方法，旨在设计一种适用于LLM的TS嵌入方法，以激活LLM对TS数据的能力。所提出的方法称为TEST。它首先对TS进行标记化处理，建立一个编码器，通过实例、特征和文本原型对齐对它们进行嵌入，然后创建提示以使LLM更容易接受嵌入，并最终实施TS任务。使用8个具有不同结构和大小的LLM对TS分类和预测任务进行了实验。尽管其结果不能显著超越当前为TS任务定制的SOTA模型，但通过将LLM视为模式机器，可以更好地处理TS数据。

    This work summarizes two strategies for completing time-series (TS) tasks using today's language model (LLM): LLM-for-TS, design and train a fundamental large model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS data. Considering the insufficient data accumulation, limited resources, and semantic context requirements, this work focuses on TS-for-LLM methods, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed them by instance-wise, feature-wise, and text-prototype-aligned contrast, and then creates prompts to make LLM more open to embeddings, and finally implements TS tasks. Experiments are carried out on TS classification and forecasting tasks using 8 LLMs with different structures and sizes. Although its results cannot significantly outperform the current SOTA models customized for TS tasks, by treating LLM as the pattern machine, 
    
[^14]: MemoChat: 通过调整LLMs使用备忘录以保持一致性的长距离开放领域对话

    MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation. (arXiv:2308.08239v1 [cs.CL])

    [http://arxiv.org/abs/2308.08239](http://arxiv.org/abs/2308.08239)

    MemoChat提出了一种用于调优指令的流程，通过让大型语言模型使用备忘录来保持对话一致性。实验证实了其有效性。

    

    我们提出了MemoChat，一个用于优化指令的流水线，使大规模语言模型（LLMs）能够有效地使用自行组织的备忘录来保持一致的长距离开放领域对话。我们通过迭代的“记忆-检索-响应”循环展示了一个长距离的开放领域对话。这要求我们为每个不同的阶段精心设计定制的调优指令。这些指令是从一系列公共数据集中重建的，以教导LLMs记忆和检索过去的对话，并通过结构化备忘录提高未来对话的一致性。我们邀请专家手动注释一个用于评估长距离对话一致性的测试集。在涉及开源和可访问API的聊天机器人的三种测试场景上进行的实验证实了MemoChat的有效性，它超越了强基线。

    We propose MemoChat, a pipeline for refining instructions that enables large language models (LLMs) to effectively employ self-composed memos for maintaining consistent long-range open-domain conversations. We demonstrate a long-range open-domain conversation through iterative "memorization-retrieval-response" cycles. This requires us to carefully design tailored tuning instructions for each distinct stage. The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations. We invite experts to manually annotate a test set designed to evaluate the consistency of long-range conversations questions. Experiments on three testing scenarios involving both open-source and API-accessible chatbots at scale verify the efficacy of MemoChat, which outperforms strong baselines.
    
[^15]: 在自然语言处理中使用基于Transformer的多任务学习的挑战和机会：一项调研

    Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey. (arXiv:2308.08234v1 [cs.CL])

    [http://arxiv.org/abs/2308.08234](http://arxiv.org/abs/2308.08234)

    本论文调研了在自然语言处理中使用基于Transformer的多任务学习的挑战和机会。通过对NLP中基于Transformer的MTL方法以及典型机器学习生命周期各阶段的挑战进行讨论，提供了相关领域的概述和动向。

    

    自然语言处理（NLP）模型在各个行业中的广泛应用导致从训练到在生产中运行这些模型的机器学习系统需要有效处理。然而，使用基于Transformer的预训练语言模型进行训练、部署和更新多个模型可能复杂、昂贵且耗时，特别是使用多任务学习（MTL）作为改进效率和性能的方法。本调研首先概述了NLP中基于Transformer的MTL方法。然后，我们讨论了在典型的机器学习生命周期中使用MTL方法面临的挑战和机会，重点关注数据工程、模型开发、部署和监控阶段的挑战。本项调研集中于基于Transformer的MTL架构，并据我们所知是首创的。

    The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it 
    
[^16]: MoCoSA: 动量对比与结构增强的预训练语言模型在知识图谱补全中的应用

    MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented Pre-trained Language Models. (arXiv:2308.08204v1 [cs.CL])

    [http://arxiv.org/abs/2308.08204](http://arxiv.org/abs/2308.08204)

    本文提出了MoCoSA方法，利用动量对比和结构增强的预训练语言模型，在知识图谱补全中解决了基于结构和基于描述方法的问题，实现了对语义丰富实体的泛化推理和对未见实体的鲁棒性。

    

    知识图谱补全旨在对知识图谱中的事实进行推理，并自动推断出缺失的连接。现有方法主要分为基于结构和基于描述两类。基于结构的方法使用实体嵌入来有效表示知识图谱中的关系事实。然而，由于结构信息有限，这些方法在处理语义丰富的真实世界实体时遇到困难，并且无法泛化到未见实体。基于描述的方法则利用预训练语言模型（PLMs）来理解文本信息，对未见实体展示出很强的鲁棒性。然而，它们在进行大规模的负采样时面临困难，并且常常落后于基于结构的方法。为解决这些问题，本文提出了一种使用动量对比和结构增强预训练语言模型（MoCoSA）用于知识图谱补全的方法。

    Knowledge Graph Completion (KGC) aims to conduct reasoning on the facts within knowledge graphs and automatically infer missing links. Existing methods can mainly be categorized into structure-based or description-based. On the one hand, structure-based methods effectively represent relational facts in knowledge graphs using entity embeddings. However, they struggle with semantically rich real-world entities due to limited structural information and fail to generalize to unseen entities. On the other hand, description-based methods leverage pre-trained language models (PLMs) to understand textual information. They exhibit strong robustness towards unseen entities. However, they have difficulty with larger negative sampling and often lag behind structure-based methods. To address these issues, in this paper, we propose Momentum Contrast for knowledge graph completion with Structure-Augmented pre-trained language models (MoCoSA), which allows the PLM to perceive the structural informatio
    
[^17]: 中国电信对VoxCeleb2023说话人识别挑战的系统描述

    ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023. (arXiv:2308.08181v1 [cs.SD])

    [http://arxiv.org/abs/2308.08181](http://arxiv.org/abs/2308.08181)

    中国电信的系统在VoxCeleb2023说话人识别挑战中取得了优异的表现，通过使用多个在VoxCeleb2上训练的ResNet模型，并进行融合和得分校准，最终获得了0.1066的minDCF和1.980%的EER。

    

    这份技术报告描述了中国电信针对VoxCeleb2023说话人识别挑战（VoxSRC 2023）的系统。我们的系统由几个在VoxCeleb2上训练的ResNet变种组成，后来进行了融合以提高性能。每个变种和融合系统都进行了得分校准。最终提交的结果达到了0.1066的minDCF和1.980%的EER。

    This technical report describes ChinaTelecom system for Track 1 (closed) of the VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023). Our system consists of several ResNet variants trained only on VoxCeleb2, which were fused for better performance later. Score calibration was also applied for each variant and the fused system. The final submission achieved minDCF of 0.1066 and EER of 1.980%.
    
[^18]: RSpell：用于领域适应的中文拼写检查的检索增强框架

    RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling Check. (arXiv:2308.08176v1 [cs.CL])

    [http://arxiv.org/abs/2308.08176](http://arxiv.org/abs/2308.08176)

    RSpell是一种用于中文拼写检查的检索增强框架，它使用拼音模糊匹配来搜索领域术语，并将其整合到CSC模型中。通过引入自适应过程控制机制和迭代策略，RSpell在零样本和微调场景下都能达到最先进的性能水平。

    

    中文拼写检查（CSC）是指在中文文本中检测和纠正拼写错误。在实际应用场景中，使CSC模型具备跨领域纠错能力非常重要。在本文中，我们提出了一种称为RSpell的检索增强拼写检查框架，该框架搜索相应的领域术语并将其整合到CSC模型中。具体而言，我们采用拼音模糊匹配来搜索术语，并将其与输入组合后输入到CSC模型中。然后，我们引入了一种自适应过程控制机制，动态调整外部知识对模型的影响。此外，我们还开发了一种迭代策略，用于增强RSpell框架的推理能力。我们在法律、医学和公文写作三个领域的CSC数据集上进行了实验证明，RSpell在零样本和微调场景下都能达到最先进的性能水平。

    Chinese Spelling Check (CSC) refers to the detection and correction of spelling errors in Chinese texts. In practical application scenarios, it is important to make CSC models have the ability to correct errors across different domains. In this paper, we propose a retrieval-augmented spelling check framework called RSpell, which searches corresponding domain terms and incorporates them into CSC models. Specifically, we employ pinyin fuzzy matching to search for terms, which are combined with the input and fed into the CSC model. Then, we introduce an adaptive process control mechanism to dynamically adjust the impact of external knowledge on the model. Additionally, we develop an iterative strategy for the RSpell framework to enhance reasoning capabilities. We conducted experiments on CSC datasets in three domains: law, medicine, and official document writing. The results demonstrate that RSpell achieves state-of-the-art performance in both zero-shot and fine-tuning scenarios, demonstr
    
[^19]: 增强性能：使用检索增强的端到端任务导向型系统在已知和未知的对话场景下

    Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System. (arXiv:2308.08169v1 [cs.CL])

    [http://arxiv.org/abs/2308.08169](http://arxiv.org/abs/2308.08169)

    本文介绍了一种通过简单的缓存提高端到端任务导向型对话系统的性能的方法。通过微调检索模块并训练端到端的对话系统模型，系统可以动态更新并处理已知和未知的对话场景。实验证明我们的方法相对于强基线方法在非空联合目标准确率上提高了6.7%。

    

    通过利用预训练模型的先进自然语言理解和生成能力，端到端任务导向型对话系统已经取得了令人满意的性能。本文通过一个简单的缓存使得任务导向型对话系统更加灵活。这个缓存能够动态更新系统并处理现有和未知的对话场景。具体来说，我们首先对检索模块进行微调，以便从缓存中有效地检索到最相关的信息。然后，我们训练端到端的对话系统模型，这些模型在生成对话时可以引用和联系对话历史和检索到的信息。缓存的构建非常简单，而任务导向型对话系统的主干模型与现有的预训练生成模型兼容。通过大量的实验验证了我们的框架的优越性能，非空联合目标准确率相对于强基线方法提高了6.7%。

    End-to-end task-oriented dialogue (TOD) systems have achieved promising performance by leveraging sophisticated natural language understanding and natural language generation capabilities of pre-trained models. This work enables the TOD systems with more flexibility through a simple cache. The cache provides the flexibility to dynamically update the TOD systems and handle both existing and unseen dialogue scenarios. Towards this end, we first fine-tune a retrieval module to effectively retrieve the most relevant information entries from the cache. We then train end-to-end TOD models that can refer to and ground on both dialogue history and retrieved information during TOD generation. The cache is straightforward to construct, and the backbone models of TOD systems are compatible with existing pre-trained generative models. Extensive experiments demonstrate the superior performance of our framework, with a notable improvement in non-empty joint goal accuracy by 6.7% compared to strong b
    
[^20]: 灾难背景下的讽刺检测

    Sarcasm Detection in a Disaster Context. (arXiv:2308.08156v1 [cs.CL])

    [http://arxiv.org/abs/2308.08156](http://arxiv.org/abs/2308.08156)

    本文介绍了一个名为HurricaneSARC的数据集，其中包含了15,000条注释为讽刺意图的推文，并使用预训练语言模型进行了讽刺检测的研究。通过中间任务的迁移学习，我们的最佳模型在数据集上获得了0.70的F1值。

    

    在自然灾害期间，人们经常使用Twitter等社交媒体平台寻求帮助、提供关于灾情的信息，或表达对灾情演变或公共政策和指导方针的蔑视。在灾难背景中理解这种言论形式对改进对灾害相关推文的自然语言理解至关重要。本文介绍了一种名为HurricaneSARC的数据集，该数据集包含了15,000条标记了讽刺意图的推文，并使用预训练语言模型对讽刺检测进行了全面研究。我们的最佳模型在我们的数据集上能够达到0.70的F1值。我们还证明，通过利用中间任务的迁移学习可以提高HurricaneSARC的性能。我们在https://github.com/tsosea2/HurricaneSarc上发布了我们的数据和代码。

    During natural disasters, people often use social media platforms such as Twitter to ask for help, to provide information about the disaster situation, or to express contempt about the unfolding event or public policies and guidelines. This contempt is in some cases expressed as sarcasm or irony. Understanding this form of speech in a disaster-centric context is essential to improving natural language understanding of disaster-related tweets. In this paper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for intended sarcasm, and provide a comprehensive investigation of sarcasm detection using pre-trained language models. Our best model is able to obtain as much as 0.70 F1 on our dataset. We also demonstrate that the performance on HurricaneSARC can be improved by leveraging intermediate task transfer learning. We release our data and code at https://github.com/tsosea2/HurricaneSarc.
    
[^21]: AutoGen:通过多代理对话框架实现下一代LLM应用

    AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. (arXiv:2308.08155v1 [cs.AI])

    [http://arxiv.org/abs/2308.08155](http://arxiv.org/abs/2308.08155)

    AutoGen是一种新的框架，通过多个可以互相对话的代理，实现了下一代LLM应用。它利用人类的理解和智能，优雅地处理不完美的生成和推理能力，并通过自动化代理对话简化了复杂的工作流程。

    

    本技术报告介绍了AutoGen，一种新的框架，通过多个可以互相对话的代理来开发LLM应用程序以解决任务。AutoGen代理可以定制、可对话，并且可以无缝地允许人类参与。它们可以在各种模式下运行，利用LLM、人类输入和工具的组合。AutoGen的设计提供了多个优势：a）它能够优雅地处理这些LLM的强大但不完美的生成和推理能力；b）它利用人类的理解和智能，通过代理之间的对话提供有价值的自动化；c）它简化和统一了复杂LLM工作流程的实现，作为自动化代理对话。我们提供了许多不同的例子，展示了开发人员如何轻松使用AutoGen有效地解决任务或构建应用程序，涵盖编程、数学、运筹学、娱乐、在线决策、问答等多个领域。

    This technical report presents AutoGen, a new framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. AutoGen's design offers multiple advantages: a) it gracefully navigates the strong but imperfect generation and reasoning abilities of these LLMs; b) it leverages human understanding and intelligence, while providing valuable automation through conversations between agents; c) it simplifies and unifies the implementation of complex LLM workflows as automated agent chats. We provide many diverse examples of how developers can easily use AutoGen to effectively solve tasks or build applications, ranging from coding, mathematics, operations research, entertainment, online decision-making, question answering, etc.
    
[^22]: 用数据排序进行快速训练的NMT模型

    Fast Training of NMT Model with Data Sorting. (arXiv:2308.08153v1 [cs.CL])

    [http://arxiv.org/abs/2308.08153](http://arxiv.org/abs/2308.08153)

    本论文提出了一种通过对翻译句子对进行排序来节省计算能力的算法，用于提高Transformer模型的训练速度。实验证明，在保持性能的同时，通过这种方法可以显著减少计算时间。

    

    Transformer模型已经为神经机器翻译等自然语言处理任务带来了革命，并且已经有很多研究致力于提高Transformer框架的效率和准确性。改进的潜在领域之一是解决Transformer计算并且后续舍弃空标记的计算问题，从而减轻不必要的计算负担。为了解决这个问题，我们提出了一种在批处理之前根据句子长度排序翻译句子对的算法，最小化计算能量的浪费。由于排序的数量可能违反相互独立和独立同分布（IID）数据假设，我们对数据进行部分排序。在实验中，我们将提出的方法应用于英文-韩文和英文-卢干达语对的机器翻译，并显示出在保持性能的同时节省了计算时间。我们的方法与架构无关，因此可以轻松集成。

    The Transformer model has revolutionized Natural Language Processing tasks such as Neural Machine Translation, and many efforts have been made to study the Transformer architecture, which increased its efficiency and accuracy. One potential area for improvement is to address the computation of empty tokens that the Transformer computes only to discard them later, leading to an unnecessary computational burden. To tackle this, we propose an algorithm that sorts translation sentence pairs based on their length before batching, minimizing the waste of computing power. Since the amount of sorting could violate the independent and identically distributed (i.i.d) data assumption, we sort the data partially. In experiments, we apply the proposed method to English-Korean and English-Luganda language pairs for machine translation and show that there are gains in computational time while maintaining the performance. Our method is independent of architectures, so that it can be easily integrated 
    
[^23]: MDDial: 一份带有可靠性评估的多轮差异诊断对话数据集

    MDDial: A Multi-turn Differential Diagnosis Dialogue Dataset with Reliability Evaluation. (arXiv:2308.08147v1 [cs.CL])

    [http://arxiv.org/abs/2308.08147](http://arxiv.org/abs/2308.08147)

    MDDial是第一个英语差异诊断对话数据集，可以帮助构建和评估端到端的ADD对话系统，并引入了一种统一的评分方法来衡量ADD系统的可靠性。

    

    自动差异诊断（ADD）的对话系统在现实生活中具有广泛的应用。这些对话系统有望提供便捷访问和降低医疗成本。构建端到端的ADD对话系统需要对话训练数据集。然而，据我们所知，尽管非英语数据集存在，但目前还没有公开可用的英语ADD对话数据集。基于这一背景，我们介绍了MDDial，这是第一个英语差异诊断对话数据集，可以帮助构建和评估端到端的ADD对话系统。此外，之前的研究通常单独或者通过一个综合加权分数来评估诊断准确性和症状。这种方法忽略了症状和诊断之间的连接。我们引入了一种统一的评分方法来衡量ADD系统，考虑了症状和诊断之间的相互作用。该评分还表示系统的可靠性。

    Dialogue systems for Automatic Differential Diagnosis (ADD) have a wide range of real-life applications. These dialogue systems are promising for providing easy access and reducing medical costs. Building end-to-end ADD dialogue systems requires dialogue training datasets. However, to the best of our knowledge, there is no publicly available ADD dialogue dataset in English (although non-English datasets exist). Driven by this, we introduce MDDial, the first differential diagnosis dialogue dataset in English which can aid to build and evaluate end-to-end ADD dialogue systems. Additionally, earlier studies present the accuracy of diagnosis and symptoms either individually or as a combined weighted score. This method overlooks the connection between the symptoms and the diagnosis. We introduce a unified score for the ADD system that takes into account the interplay between symptoms and diagnosis. This score also indicates the system's reliability. To the end, we train two moderate-size of
    
[^24]: Radio2Text：使用mmWave无线电信号进行流式语音识别

    Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals. (arXiv:2308.08125v1 [cs.SD])

    [http://arxiv.org/abs/2308.08125](http://arxiv.org/abs/2308.08125)

    本文提出了Radio2Text，这是第一个基于mmWave的流式自动语音识别（ASR）系统，具有超过13,000个词的词汇量。通过设计的流式Transformer和Guidance Initialization技术，Radio2Text实现了对流式ASR具有大词汇量的支持。

    

    毫米波（mmWave）基于语音识别为音频相关应用提供了更多可能性，如会议演讲转录和窃听。然而，考虑到实际场景的可行性，延迟和可识别的词汇量是两个不容忽视的关键因素。本文提出了Radio2Text，这是第一个基于mmWave的流式自动语音识别（ASR）系统，其词汇量超过13,000个词。Radio2Text基于精心设计的流式Transformer，能够有效学习与语音相关特征的表示，为具有大词汇量的流式ASR铺平了道路。为了缓解流式网络无法访问整个未来输入的缺点，我们提出了Guidance Initialization，通过权重继承，促进将与全局上下文相关的特征知识从非流式Transformer传递给精心设计的流式Transformer。

    Millimeter wave (mmWave) based speech recognition provides more possibility for audio-related applications, such as conference speech transcription and eavesdropping. However, considering the practicality in real scenarios, latency and recognizable vocabulary size are two critical factors that cannot be overlooked. In this paper, we propose Radio2Text, the first mmWave-based system for streaming automatic speech recognition (ASR) with a vocabulary size exceeding 13,000 words. Radio2Text is based on a tailored streaming Transformer that is capable of effectively learning representations of speech-related features, paving the way for streaming ASR with a large vocabulary. To alleviate the deficiency of streaming networks unable to access entire future inputs, we propose the Guidance Initialization that facilitates the transfer of feature knowledge related to the global context from the non-streaming Transformer to the tailored streaming Transformer through weight inheritance. Further, we
    
[^25]: 把高下分清楚：通过参数高效模块操作进行模型残缺性去学习

    Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation. (arXiv:2308.08090v1 [cs.CL])

    [http://arxiv.org/abs/2308.08090](http://arxiv.org/abs/2308.08090)

    通过提取和消除反专家PEMs中的残缺能力来提升大规模语言模型的真实性和去毒性。

    

    大规模语言模型（LLMs）在各种应用中得到广泛应用，但存在与不真实和有毒性有关的问题。虽然参数高效模块（PEMs）已经证明了其在为模型赋予新技能方面的有效性，但利用PEMs进行残缺性去学习仍未充分探索。在这项工作中，我们提出了一种PEMs操作方法，即“提取-减去”（Ext-Sub），通过整合“专家”PEMs和“反专家”PEMs来增强LLMs的真实性和去毒性。值得注意的是，即使是反专家PEMs也具有宝贵的能力，因为它们擅长生成虚构内容，这需要语言建模和逻辑叙述能力。与仅仅否定参数不同，我们的方法涉及提取和消除反专家PEMs中的残缺能力，同时保留一般能力。为了评估我们的方法在真实性方面的有效性

    Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of ``expert'' PEM and ``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of tru
    
[^26]: 高成本困境：大型语言模型的泛化、评估和成本最优部署。

    The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models. (arXiv:2308.08061v1 [cs.CL])

    [http://arxiv.org/abs/2308.08061](http://arxiv.org/abs/2308.08061)

    在部署大型语言模型时，需要关注泛化、评估和成本最优化。本文提出了一个针对大型语言模型的泛化、评估和成本建模框架，帮助企业深入了解和评估这些因素。

    

    在为任何产品/应用程序在生产环境中部署机器学习模型时，通常希望具备三个属性。首先，模型应具有泛化能力，即在我们对领域知识的发展中可以扩展其用途。其次，它们应该是可评估的，这样在生产环境中可以有清晰的性能指标和计算这些指标的可行方式。最后，部署应尽可能地成本最优。在本文中，我们提出这三个目标（即泛化、评估和成本最优化）在大型语言模型中往往是相对独立的，并且对于大型语言模型，尽管其在传统NLP模型上表现出色，但企业在对这项技术进行重大投资之前需要仔细评估所有三个因素。我们提出了一个特别针对大型语言模型的泛化、评估和成本建模框架，为企业提供深入了解这些复杂因素的洞察力。

    When deploying machine learning models in production for any product/application, there are three properties that are commonly desired. First, the models should be generalizable, in that we can extend it to further use cases as our knowledge of the domain area develops. Second they should be evaluable, so that there are clear metrics for performance and the calculation of those metrics in production settings are feasible. Finally, the deployment should be cost-optimal as far as possible. In this paper we propose that these three objectives (i.e. generalization, evaluation and cost-optimality) can often be relatively orthogonal and that for large language models, despite their performance over conventional NLP models, enterprises need to carefully assess all the three factors before making substantial investments in this technology. We propose a framework for generalization, evaluation and cost-modeling specifically tailored to large language models, offering insights into the intricaci
    
[^27]: DiagGPT:一种基于LLM的任务导向对话的聊天机器人

    DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])

    [http://arxiv.org/abs/2308.08043](http://arxiv.org/abs/2308.08043)

    DiagGPT将大型语言模型(LLMs)扩展到任务导向的对话场景，提供了在复杂诊断场景中主动提问和引导用户完成任务的能力。

    

    大型语言模型(LLMs)如ChatGPT正变得越来越复杂，展示出与人类相似的能力。这些AI模型在日常生活中辅助人类完成各种任务方面发挥着重要作用。AI作为聊天代理人的重要应用是回答人类在各个领域的问题。目前的LLMs在回答一般问题方面已经显示出熟练的能力。然而，在复杂的诊断场景(如法律或医疗咨询)中，基本的问答对话往往表现不佳。这些场景通常需要任务导向对话(TOD)，其中AI聊天代理需要主动提问并引导用户完成特定任务。以前的微调模型在TOD方面表现不佳，而当前的LLMs并未固有这种能力。在本文中，我们介绍了一种名为DiagGPT (Diagnosis GPT)的创新方法，它将LLMs推广到TOD场景中。

    Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
    
[^28]: 使用人工群体在神经模型中研究心理现象

    Using Artificial Populations to Study Psychological Phenomena in Neural Models. (arXiv:2308.08032v1 [cs.CL])

    [http://arxiv.org/abs/2308.08032](http://arxiv.org/abs/2308.08032)

    利用人工群体进行语言模型研究，通过不确定性估计构建实验群体，并发现语言模型表现出典型行为。

    

    近年来，基于转换器的自然语言处理研究的激增导致了一系列试图在模型中检测人类认知行为存在的研究。我们认为，与人类心理学一样，对语言模型中的认知行为的研究必须在适当规模的适当群体中进行，才能获得有意义的结果。我们利用不确定性估计的工作，采用一种新颖的方法来高效地构建实验群体。由此产生的工具 PopulationLM 已经开源。我们提供了不确定性估计文献中的理论基础，并从当前关于语言模型的认知工作中提供了动机。我们讨论了其他科学界的方法论教训，并试图展示它们在两个人工群体研究中的应用。通过基于群体的实验，我们发现语言模型表现出与典型行为一致的行为。

    The recent proliferation of research into transformer based natural language processing has led to a number of studies which attempt to detect the presence of human-like cognitive behavior in the models. We contend that, as is true of human psychology, the investigation of cognitive behavior in language models must be conducted in an appropriate population of an appropriate size for the results to be meaningful. We leverage work in uncertainty estimation in a novel approach to efficiently construct experimental populations. The resultant tool, PopulationLM, has been made open source. We provide theoretical grounding in the uncertainty estimation literature and motivation from current cognitive work regarding language models. We discuss the methodological lessons from other scientific communities and attempt to demonstrate their application to two artificial population studies. Through population based experimentation we find that language models exhibit behavior consistent with typical
    
[^29]: 基于多语言神经表示的端到端开放词汇关键词搜索

    End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations. (arXiv:2308.08027v1 [eess.AS])

    [http://arxiv.org/abs/2308.08027](http://arxiv.org/abs/2308.08027)

    本论文提出了一种基于多语言神经表示的端到端开放词汇关键词搜索模型，相比于传统的基于自动语音识别的关键词搜索系统，在长查询和不在训练集中的查询方面表现出色。

    

    传统的关键词搜索系统在自动语音识别（ASR）输出上运行，导致它们具有复杂的索引和搜索流程。因此，人们开始对不需要ASR的方法感兴趣，以简化搜索流程。我们最近提出了一种神经网络ASR-free关键词搜索模型，它在保持高效和简化的同时，实现了竞争性的性能，其中查询和文档通过一对循环神经网络编码器进行编码，编码通过点积进行组合。在本文中，我们通过多语言预训练和对模型的详细分析对这项工作进行了扩展。我们的实验表明，所提出的多语言训练显著提高了模型性能，并且尽管在短查询和词汇内的查询方面未能与强基于ASR的传统关键词搜索系统匹配，但所提出的模型在长查询和未出现在训练集的查询方面表现出色。

    Conventional keyword search systems operate on automatic speech recognition (ASR) outputs, which causes them to have a complex indexing and search pipeline. This has led to interest in ASR-free approaches to simplify the search procedure. We recently proposed a neural ASR-free keyword search model which achieves competitive performance while maintaining an efficient and simplified pipeline, where queries and documents are encoded with a pair of recurrent neural network encoders and the encodings are combined with a dot-product. In this article, we extend this work with multilingual pretraining and detailed analysis of the model. Our experiments show that the proposed multilingual training significantly improves the model performance and that despite not matching a strong ASR-based conventional keyword search system for short queries and queries comprising in-vocabulary words, the proposed model outperforms the ASR-based system for long queries and queries that do not appear in the trai
    
[^30]: 神经网络之间出现了语用性结构

    Anaphoric Structure Emerges Between Neural Networks. (arXiv:2308.07984v1 [cs.CL])

    [http://arxiv.org/abs/2308.07984](http://arxiv.org/abs/2308.07984)

    本研究通过研究神经网络之间是否能够出现类似于自然语言中回指结构的现象，发现带有回指结构的语言对神经模型来说是可学习的，这些结构会在模型之间自然地出现，并且增加对说话者效率的压力会增加这些结构的普遍性。

    

    语用学对于自然语言至关重要，使得说话者能够通过省略和回指等结构高效地进行交流，而不会丧失意义。这些结构要求听者解释一个多义的形式，比如代词，并且推断说话者的意图来确定代词所指的对象。尽管会引入歧义，回指在人类语言中普遍存在。为了更好地理解自然语言中回指结构的起源，我们研究了是否可以在训练用于解决交流任务的人工神经网络之间出现类似的结构。我们的研究结果表明：首先，尽管可能增加歧义，带有回指结构的语言对神经模型来说是可学习的。其次，回指结构在模型之间“自然”地出现，不需要额外的约束条件。最后，引入对说话者明确的效率压力会增加这些结构的普遍性。

    Pragmatics is core to natural language, enabling speakers to communicate efficiently with structures like ellipsis and anaphora that can shorten utterances without loss of meaning. These structures require a listener to interpret an ambiguous form - like a pronoun - and infer the speaker's intended meaning - who that pronoun refers to. Despite potential to introduce ambiguity, anaphora is ubiquitous across human language. In an effort to better understand the origins of anaphoric structure in natural language, we look to see if analogous structures can emerge between artificial neural networks trained to solve a communicative task. We show that: first, despite the potential for increased ambiguity, languages with anaphoric structures are learnable by neural models. Second, anaphoric structures emerge between models 'naturally' without need for additional constraints. Finally, introducing an explicit efficiency pressure on the speaker increases the prevalence of these structures. We con
    
[^31]: "谨防欺骗": 通过受控声明编辑检测半真相并揭穿

    "Beware of deception": Detecting Half-Truth and Debunking it through Controlled Claim Editing. (arXiv:2308.07973v1 [cs.CL])

    [http://arxiv.org/abs/2308.07973](http://arxiv.org/abs/2308.07973)

    本研究针对互联网上半真相的广泛存在问题，提出了一个包括半真相检测模型和声明编辑模型的全面流程。通过利用T5模型进行受控声明编辑，我们的方法在虚假信息揭穿得分方面优于其他语言模型，并在半真相检测模型上创造了新的性能基准。

    

    随着互联网的广泛使用，半真相即包含一些真实信息但最终具有欺骗性的陈述越来越多。为了应对这个问题，我们创建了一个包括半真相检测模型和声明编辑模型的全面流程。我们的方法利用了T5模型进行受控声明编辑；这里的“受控”意味着对声明的选定部分进行精确调整。我们的方法在编辑后的声明上取得了平均BLEU分数0.88（在0-1的范围内）和85%的虚假信息揭穿得分。值得注意的是，我们基于T5的方法在虚假信息揭穿得分方面优于其他语言模型，如GPT2，RoBERTa，PEGASUS和Tailor，平均改进分别为82%，57%，42%和23%。通过扩展LIAR PLUS数据集，我们在半真相检测模型方面实现了82%的F1分数，创造了该领域的新基准。尽管之前已经有人尝试过半真相检测，但我们的方法在准确性和性能上是前所未有的。

    The prevalence of half-truths, which are statements containing some truth but that are ultimately deceptive, has risen with the increasing use of the internet. To help combat this problem, we have created a comprehensive pipeline consisting of a half-truth detection model and a claim editing model. Our approach utilizes the T5 model for controlled claim editing; "controlled" here means precise adjustments to select parts of a claim. Our methodology achieves an average BLEU score of 0.88 (on a scale of 0-1) and a disinfo-debunk score of 85% on edited claims. Significantly, our T5-based approach outperforms other Language Models such as GPT2, RoBERTa, PEGASUS, and Tailor, with average improvements of 82%, 57%, 42%, and 23% in disinfo-debunk scores, respectively. By extending the LIAR PLUS dataset, we achieve an F1 score of 82% for the half-truth detection model, setting a new benchmark in the field. While previous attempts have been made at half-truth detection, our approach is, to the b
    
[^32]: MultiSChuBERT: 高效的学术文档质量预测的多模态融合方法

    MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction. (arXiv:2308.07971v1 [cs.CL])

    [http://arxiv.org/abs/2308.07971](http://arxiv.org/abs/2308.07971)

    MultiSChuBERT是一个多模态预测模型，通过结合文本和图像信息，在学术文档质量预测任务上取得了显著改进。我们的工作在结合视觉和文本嵌入、逐渐解冻视觉子模型权重以及采用最新文本嵌入替换标准BERT$_{\textrm{BASE}}$嵌入方面做出了重要贡献。

    

    学术文档质量的自动评估是一项具有高潜力影响的困难任务。多模态学习，特别是将视觉信息与文本相结合，已经显示出在学术文档质量预测任务上提高性能的效果。我们提出了一种多模态的预测模型MultiSChuBERT。它结合了基于文本的模型（SChuBERT）和基于图像的模型（Inception V3）。我们的工作在学术文档质量预测方面有三个方面的贡献。首先，我们展示了结合视觉和文本嵌入的方法可以在结果上产生显著影响。其次，我们证明了逐渐解冻视觉子模型的权重可以减少过拟合数据的趋势，从而提高结果。第三，我们展示了采用最新的文本嵌入替换标准BERT$_{\textrm{BASE}}$嵌入时多模态的优势。

    Automatic assessment of the quality of scholarly documents is a difficult task with high potential impact. Multimodality, in particular the addition of visual information next to text, has been shown to improve the performance on scholarly document quality prediction (SDQP) tasks. We propose the multimodal predictive model MultiSChuBERT. It combines a textual model based on chunking full paper text and aggregating computed BERT chunk-encodings (SChuBERT), with a visual model based on Inception V3.Our work contributes to the current state-of-the-art in SDQP in three ways. First, we show that the method of combining visual and textual embeddings can substantially influence the results. Second, we demonstrate that gradual-unfreezing of the weights of the visual sub-model, reduces its tendency to ovefit the data, improving results. Third, we show the retained benefit of multimodality when replacing standard BERT$_{\textrm{BASE}}$ embeddings with more recent state-of-the-art text embedding 
    
[^33]: 教会LLMs个性化—受写作教育启发的一种方法

    Teach LLMs to Personalize -- An Approach inspired by Writing Education. (arXiv:2308.07968v1 [cs.CL])

    [http://arxiv.org/abs/2308.07968](http://arxiv.org/abs/2308.07968)

    本研究提出了一种通用方法来进行个性化文本生成，通过教导大型语言模型（LLMs）借鉴写作教育实践，利用多阶段和多任务的框架来教导LLMs个性化生成，从而提升其生成能力。

    

    个性化文本生成是近年来受到广泛关注的新兴研究领域，大多数研究都是通过设计定制的特征或模型来专注于特定领域。在这项工作中，我们提出了一种使用大型语言模型（LLMs）进行个性化文本生成的通用方法。受到写作教育实践的启发，我们开发了一个多阶段和多任务的框架来教导LLMs进行个性化生成。在写作指导中，从资源中撰写文章的任务通常被分解为多个步骤，包括查找、评估、总结、综合和整合信息。类似地，我们的个性化文本生成方法包括多个阶段：检索、排序、摘要、综合和生成。此外，我们引入了一个多任务设置，进一步帮助模型提高其生成能力，这受到教育观察到的学生阅读能力和写作能力的启示。

    Personalized text generation is an emerging research area that has attracted much attention in recent years. Most studies in this direction focus on a particular domain by designing bespoke features or models. In this work, we propose a general approach for personalized text generation using large language models (LLMs). Inspired by the practice of writing education, we develop a multistage and multitask framework to teach LLMs for personalized generation. In writing instruction, the task of writing from sources is often decomposed into multiple steps that involve finding, evaluating, summarizing, synthesizing, and integrating information. Analogously, our approach to personalized text generation consists of multiple stages: retrieval, ranking, summarization, synthesis, and generation. In addition, we introduce a multitask setting that helps the model improve its generation ability further, which is inspired by the observation in education that a student's reading proficiency and writi
    
[^34]: 自动化测试和改进命名实体识别系统

    Automated Testing and Improvement of Named Entity Recognition Systems. (arXiv:2308.07937v1 [cs.CL])

    [http://arxiv.org/abs/2308.07937](http://arxiv.org/abs/2308.07937)

    本论文提出了一种自动测试和修复命名实体识别系统的新方法TIN，该方法能够解决深度神经网络在特定情况下不可靠的问题，确保相似语境下相同命名实体的NER预测相同，从而提高系统的可靠性。

    

    最近几年，由于深度神经网络的发展，命名实体识别（NER）系统取得了快速进展。这些系统广泛应用于各种自然语言处理应用中，如信息提取、问答和情感分析。然而，深度神经网络的复杂性和难以处理性可能使NER系统在某些情况下变得不可靠，导致错误的预测。例如，NER系统可能错误地将女性名字识别为化学物质，或者无法识别出少数群体的名字，从而导致用户不满。为了解决这个问题，我们引入了一个新颖的、广泛适用的方法——TIN，用于自动测试和修复各种NER系统。自动测试的关键思想是相似语境下相同命名实体的NER预测应该相同。自动修复的核心思想是相似的命名实体在相同语境下应该具有相同的NER预测。

    Named entity recognition (NER) systems have seen rapid progress in recent years due to the development of deep neural networks. These systems are widely used in various natural language processing applications, such as information extraction, question answering, and sentiment analysis. However, the complexity and intractability of deep neural networks can make NER systems unreliable in certain circumstances, resulting in incorrect predictions. For example, NER systems may misidentify female names as chemicals or fail to recognize the names of minority groups, leading to user dissatisfaction. To tackle this problem, we introduce TIN, a novel, widely applicable approach for automatically testing and repairing various NER systems. The key idea for automated testing is that the NER predictions of the same named entities under similar contexts should be identical. The core idea for automated repairing is that similar named entities should have the same NER prediction under the same context.
    
[^35]: 用ChatGPT变革金融领域的情绪分析

    Transforming Sentiment Analysis in the Financial Domain with ChatGPT. (arXiv:2308.07935v1 [cs.CL])

    [http://arxiv.org/abs/2308.07935](http://arxiv.org/abs/2308.07935)

    本研究使用ChatGPT 3.5来进行金融情绪分析，特别关注外汇市场，通过零-shot提示方法，在精心策划的数据集上评估了其性能，并发现与传统模型相比，ChatGPT在金融情绪分析中表现出约35％的性能提升。

    

    金融情绪分析在解读市场趋势和指导战略交易决策中起着关键作用。尽管已经使用了先进的深度学习技术和语言模型来改进金融情绪分析，但本研究通过探索大型语言模型（特别是ChatGPT 3.5）在金融情绪分析中的潜力，特别强调外汇市场（forex），开创了新的领域。采用零-shot提示方法，在一份经过精心策划的外汇相关新闻标题数据集上检验多个ChatGPT提示，并使用精确度、召回率、F1得分和情绪分类的平均绝对误差（MAE）等指标评估性能。此外，我们还探讨了预测情绪和市场回报之间的相关性作为一种额外的评估方法。与FinBERT相比，ChatGPT在情绪分析方面的性能提高了约35％。

    Financial sentiment analysis plays a crucial role in decoding market trends and guiding strategic trading decisions. Despite the deployment of advanced deep learning techniques and language models to refine sentiment analysis in finance, this study breaks new ground by investigating the potential of large language models, particularly ChatGPT 3.5, in financial sentiment analysis, with a strong emphasis on the foreign exchange market (forex). Employing a zero-shot prompting approach, we examine multiple ChatGPT prompts on a meticulously curated dataset of forex-related news headlines, measuring performance using metrics such as precision, recall, f1-score, and Mean Absolute Error (MAE) of the sentiment class. Additionally, we probe the correlation between predicted sentiment and market returns as an additional evaluation approach. ChatGPT, compared to FinBERT, a well-established sentiment analysis model for financial texts, exhibited approximately 35\% enhanced performance in sentiment 
    
[^36]: 使用图像文本对齐评估用于痴呆检测的图片描述语音

    Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment. (arXiv:2308.07933v1 [cs.CL])

    [http://arxiv.org/abs/2308.07933](http://arxiv.org/abs/2308.07933)

    该论文提出了一种新的痴呆检测模型，将图片和描述文本作为输入，并利用大型预训练图像文本对齐模型的知识。通过观察发现，痴呆样本与健康样本在文本与图片相关性和图片焦点区域上存在差异，从而可以提高痴呆检测的准确性。

    

    使用图片描述语音进行痴呆检测已经研究了30年。尽管有这么长的历史，先前的模型主要关注正常人和患有痴呆症的患者之间语音模式的差异，但没有直接利用图片信息。在本文中，我们提出了首个将图片和描述文本作为输入，并融入大型预训练图像文本对齐模型的痴呆检测模型。我们观察到痴呆样本和健康样本在文本与图片相关性以及图片的焦点区域方面存在差异。因此，我们认为这种差异可以用于提高痴呆检测的准确性。具体而言，我们使用文本与图片的相关性对样本的句子进行排序和过滤。我们还根据图片的焦点区域确定了话题，并根据焦点区域对句子进行分类。我们提出了三个先进模型进行预处理。

    Using picture description speech for dementia detection has been studied for 30 years. Despite the long history, previous models focus on identifying the differences in speech patterns between healthy subjects and patients with dementia but do not utilize the picture information directly. In this paper, we propose the first dementia detection models that take both the picture and the description texts as inputs and incorporate knowledge from large pre-trained image-text alignment models. We observe the difference between dementia and healthy samples in terms of the text's relevance to the picture and the focused area of the picture. We thus consider such a difference could be used to enhance dementia detection accuracy. Specifically, we use the text's relevance to the picture to rank and filter the sentences of the samples. We also identified focused areas of the picture as topics and categorized the sentences according to the focused areas. We propose three advanced models that pre-pr
    
[^37]: 精简特征场使得语言引导的少样本操作成为可能

    Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v1 [cs.CV])

    [http://arxiv.org/abs/2308.07931](http://arxiv.org/abs/2308.07931)

    本论文通过精简特征场，将精确的3D几何与2D基础模型的丰富语义相结合，实现了对未见过的物体的少样本操作的泛化能力。

    

    自监督和语言监督的图像模型包含了世界的丰富知识，对于泛化很重要。然而，许多机器人任务需要对 3D 几何的详细理解，这在 2D 图像特征中往往缺乏。本研究通过利用精简特征场，将精确的 3D 几何与 2D 基础模型的丰富语义相结合，来弥合机器人操作中的 2D 到 3D 的差距。我们提出一种针对 6 自由度抓取和放置的少样本学习方法，利用这些强大的空间和语义先验，实现对未见过的物体的自然泛化。通过从视觉语言模型 CLIP 中精简的特征，我们展示了一种通过自由文本自然语言指定新颖对象进行操作的方式，并展示了它在未见过的表达和新颖类别的物体上的泛化能力。

    Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
    
[^38]: SPM: Meituan搜索中用于相关性建模的结构化预训练和匹配架构

    SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search. (arXiv:2308.07711v1 [cs.IR])

    [http://arxiv.org/abs/2308.07711](http://arxiv.org/abs/2308.07711)

    本论文提出了一种用于在Meituan搜索中进行相关性建模的新颖两阶段预训练和匹配架构。

    

    在电商搜索中，查询和文档之间的相关性是满足用户体验的基本要求。与传统的电商平台不同，用户在美团等生活服务平台上进行搜索主要是为了产品供应商，这些供应商通常拥有丰富的结构化信息，例如名称、地址、类别、成千上万的产品。使用这些丰富的结构化内容进行搜索相关性建模具有挑战性，主要存在以下问题：（1）不同字段的结构化文档存在语言分布差异，无法直接采用预训练的语言模型方法（如BERT）。（2）不同字段通常具有不同的重要性，且长度差异很大，很难提取对相关性匹配有帮助的文档信息。为了解决这些问题，本文提出了一种新的两阶段预训练和匹配架构，用于丰富结构的相关性匹配。

    In e-commerce search, relevance between query and documents is an essential requirement for satisfying user experience. Different from traditional e-commerce platforms that offer products, users search on life service platforms such as Meituan mainly for product providers, which usually have abundant structured information, e.g. name, address, category, thousands of products. Modeling search relevance with these rich structured contents is challenging due to the following issues: (1) there is language distribution discrepancy among different fields of structured document, making it difficult to directly adopt off-the-shelf pretrained language model based methods like BERT. (2) different fields usually have different importance and their length vary greatly, making it difficult to extract document information helpful for relevance matching.  To tackle these issues, in this paper we propose a novel two-stage pretraining and matching architecture for relevance matching with rich structure
    
[^39]: 元认知提示改善大型语言模型的理解能力

    Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])

    [http://arxiv.org/abs/2308.05342](http://arxiv.org/abs/2308.05342)

    元认知提示 (MP) 是一种改进大型语言模型 (LLMs) 理解能力的策略。实验结果表明，使用MP的PaLM在各种自然语言理解任务中接近于GPT-4的性能水平。

    

    在大型语言模型 (LLMs) 中，通过有效的提示设计，任务特定性能一直在不断提高。尽管最近关于提示的研究增强了LLMs的推理能力，但在进一步提高它们的理解能力方面仍存在差距。在本研究中，我们介绍了元认知提示 (MP)，这是一种受人类内省推理过程启发的策略。使用MP，LLMs经历一系列有结构、自我意识的评估，利用其丰富的内在知识和新的见解。我们的实验涉及五个常见的LLMs：Llama2、Vicuna、PaLM、GPT-3.5和GPT-4，它们都涵盖了来自GLUE和SuperGLUE基准测试的各种通用自然语言理解 (NLU) 任务。结果表明，虽然GPT-4在大多数任务中始终表现出色，但配备MP的PaLM接近其性能水平。此外，跨模型和数据集，MP始终优于现有的提示方法。

    In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce metacognitive prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting meth
    
[^40]: SeACo-Paraformer:一种具有灵活且有效的热词自定义能力的非自回归ASR系统

    SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability. (arXiv:2308.03266v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2308.03266](http://arxiv.org/abs/2308.03266)

    SeACo-Paraformer是一种具有灵活且有效的热词自定义能力的非自回归ASR系统，在大规模实验中表现优于基线模型，并提出了过滤大规模热词的有效方法。

    

    热词自定义是ASR领域中一个重要的问题，使用户能够自定义实体、人物和其他短语的名称具有价值。过去几年中，ASR上下文建模的隐式和显式建模策略都得到了发展。尽管这些方法表现还不错，但仍存在某些缺点，例如在效果稳定性方面的不稳定性。在本文中，我们提出了一种新颖的基于语义增强的上下文Paraformer (SeACo-Paraformer)的非自回归ASR系统，具有灵活且有效的热词自定义能力。它结合了基于AED模型的准确性、基于NAR模型的效率以及在上下文建模方面的出色表现。在50,000小时的工业大数据实验中，我们提出的模型在自定义和常规ASR任务中优于强基线模型。此外，我们还探索了一种有效的方法来过滤大规模的热词以进一步改进。

    Hotword customization is one of the important issues remained in ASR field it is of value to enable users of ASR systems to customize names of entities, persons and other phrases. The past few years have seen both implicit and explicit modeling strategies for ASR contextualization developed. While these approaches have performed adequately, they still exhibit certain shortcomings such as instability in effectiveness. In this paper we propose Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with flexible and effective hotword customization ability. It combines the accuracy of the AED-based model, the efficiency of the NAR model, and the excellent performance in contextualization. In 50,000 hours industrial big data experiments, our proposed model outperforms strong baselines in customization and general ASR tasks. Besides, we explore an efficient way to filter large scale incoming hotwords for further improvement. The source codes and industrial
    
[^41]: EnrichEvent: 使用上下文信息为新出现的事件提供丰富的社交数据

    EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])

    [http://arxiv.org/abs/2307.16082](http://arxiv.org/abs/2307.16082)

    本文提出了一个利用词汇、语义和上下文表示的框架，旨在解决现有事件检测方法在识别新兴社交事件方面的局限性，并提供了对社交数据进行丰富的上下文化处理的方法。

    

    社交平台已成为传播和讨论真实事件信息的关键平台，为及早发现有新闻价值的事件提供了良好的机会。然而，现有的大多数事件检测方法仅利用关键词突发性或网络结构来检测热点事件。因此，对于事件和社交数据的复杂性而言，它们往往无法在达到趋势状态之前识别出新出现的社交事件。社交数据，例如推文，具有拼写错误、不完整性、歧义性和语言不规范性，以及意见方面的变化。此外，利用有限的上下文知识来学习事件的演变特征对于机器学习模型几乎是不可行的。为了解决这些问题，本文提出了一个利用流式社交数据的词汇、语义和上下文表示的框架。

    Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to
    
[^42]: LLM-Rec: 通过引导大型语言模型进行个性化推荐

    LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])

    [http://arxiv.org/abs/2307.15780](http://arxiv.org/abs/2307.15780)

    本文通过引导大型语言模型进行个性化推荐的研究，提出了四种不同的引导策略，并通过实验证明了这些策略的有效性。这一发现强调了在个性化内容推荐中，采用多样的引导和输入增强技术可以提高大型语言模型的推荐性能。

    

    本文通过输入增强技术，研究了多种不同的引导策略，以提高大型语言模型（LLM）在个性化内容推荐方面的性能。我们提出的方法名为LLM-Rec，包括四种不同的引导策略：（1）基础引导，（2）推荐驱动引导，（3）参与引导引导，和（4）推荐驱动+参与引导引导。实验证明，将原始内容描述与LLM生成的增强输入文本结合起来，采用这些引导策略可以提高推荐性能。这一发现强调了在个性化内容推荐中，通过引入多样的引导和输入增强技术来提升大型语言模型的推荐能力的重要性。

    We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that combining the original content description with the augmented input text generated by LLM using these prompting strategies leads to improved recommendation performance. This finding highlights the importance of incorporating diverse prompts and input augmentation techniques to enhance the recommendation capabilities with large language models for personalized content recommendation.
    
[^43]: 利用大型语言模型通过在线文本数据预测心理健康

    Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v1 [cs.HC])

    [http://arxiv.org/abs/2307.14385](http://arxiv.org/abs/2307.14385)

    本研究首次对多种大型语言模型在心理健康预测任务上进行了全面评估，结果表明指令微调可以显著提升模型性能，并且最优微调模型在平衡准确度上胜过GPT-3.5，并与最先进的任务特定模型持平。

    

    最近大型语言模型（LLM）的技术提升使得多种应用成为可能。然而，对于LLM在心理健康领域的理解和改进研究几乎没有。在这项工作中，我们首次全面评估了多种LLM（包括Alpaca，Alpaca-LoRA和GPT-3.5）在通过在线文本数据进行多个心理健康预测任务上的表现。我们进行了广泛的实验，包括零-shot提示、少-shot提示和指令微调。结果表明，LLM在零-shot和少-shot提示设计上在心理健康任务上表现出有限但有前景的性能。更重要的是，我们的实验结果表明，指令微调可以显著提升LLM在所有任务上的性能。我们最好的微调模型，Mental-Alpaca，在平衡准确度上比GPT-3.5（体积大25倍）高出16.7\%，并与最先进的任务特定模型持平。我们总结我们的发现。

    The recent technology boost of large language models (LLMs) has empowered a variety of applications. However, there is very little research on understanding and improving LLMs' capability for the mental health domain. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, and GPT-3.5, on various mental health prediction tasks via online text data. We conduct a wide range of experiments, covering zero-shot prompting, few-shot prompting, and instruction finetuning. The results indicate the promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned model, Mental-Alpaca, outperforms GPT-3.5 (25 times bigger) by 16.7\% on balanced accuracy and performs on par with the state-of-the-art task-specific model. We summarize our find
    
[^44]: 面向教育中人工智能协作混合论文的自动边界检测

    Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12267](http://arxiv.org/abs/2307.12267)

    本研究探索了在教育领域中，由人类和生成性语言模型协作编写的混合文本的AI内容检测方法，将其形式化为识别转换点的任务，以区分人类编写和AI生成的部分。

    

    最近的大型语言模型（如ChatGPT）能够在提供具体指导的情况下生成类似于人类的流畅回答。尽管承认技术进步带来的便利，教育者也担心学生可能利用语言模型来完成写作任务并将其假冒为自己的原创作品。虽然有很多AI内容检测研究是基于这些担忧进行的，但大多数之前的研究将AI内容检测建模为一个分类问题，假设一个文本要么完全由人类编写，要么完全由AI生成。在本研究中，我们研究了AI内容检测在一个少有探索但却现实的情况下，即检测的文本由人类和生成性语言模型（即混合文本）协作编写。我们首先将检测任务形式化为从给定的混合文本中识别人类编写内容和AI生成内容之间的转换点（边界检测）。

    The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
    
[^45]: LLM认知判断与人类有所不同

    LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])

    [http://arxiv.org/abs/2307.11787](http://arxiv.org/abs/2307.11787)

    这项研究调查了大型语言模型在认知任务中的表现，并发现它们的认知判断与人类不同。

    

    最近，大型语言模型(LLMs)成为研究人员、企业和消费者关注的焦点。虽然这类模型的语言能力已经得到了广泛的研究，但对它们作为认知主体的调查越来越受关注。在本研究中，我对GPT-3和ChatGPT在一个来自认知科学文献的有限数据归纳推理任务上的能力进行了研究。结果表明，这些模型的认知判断与人类不同。

    Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
    
[^46]: 通过基于大型语言模型的比较判定进行零样本NLG评估

    Zero-shot NLG evaluation through Pairware Comparisons with LLMs. (arXiv:2307.07889v1 [cs.CL])

    [http://arxiv.org/abs/2307.07889](http://arxiv.org/abs/2307.07889)

    本研究提出了一种使用开源大型语言模型进行零样本自然语言生成（NLG）评估的方法，通过配对比较判定来确定候选回应的优劣。结果表明，相较于绝对评分，比较评估是一种更有效的方法，并使得较小的开源LLMs达到了与更大的公共访问API相当的性能。

    

    评估自然语言生成（NLG）输出是至关重要但费时费力的。虽然已经提出了各种自动NLG评估方法，但它们通常是特定任务特定领域的，需要针对特定领域和属性进行工程设计。在这项工作中，我们提出了一种使用开源大型语言模型（LLMs）进行零样本NLG评估的稳健方法，采用了配对比较判定的方式。这种方法的动机是，即使作为人类，确定两个选项中哪个更好要比独立客观评分每个选项更容易。我们利用这一观察结果并利用LLMs新兴的能力，通过探测FlanT5，确定两个候选回应中哪一个更好，而不是指定绝对分数。我们的结果表明，比较评估是比绝对评分更有效的方法，使得较小的开源LLMs能够达到与更大的公共访问API相当的性能。我们评估了系统。

    Evaluating Natural Language Generation (NLG) outputs is crucial but laborious and expensive. While various automatic NLG assessment methods have been proposed, they often are quite task-specific and have to be engineered with a particular domain and attribute in mind. In this work, we propose a robust zero-shot approach to NLG evaluation using pairwise comparative judgment with open-source Large Language Models (LLMs). The motivation for this approach is that even as humans, it is easier to determine which of two options are better, than it is to independently objectively score each option. We use this insight and leverage the emergent abilities of LLMs, where we probe FlanT5 to determine which of two candidate responses is better, rather than assigning absolute scores. Our results demonstrate that comparative assessment is a more effective approach than absolute scoring, enabling smaller open-source LLMs to achieve comparable performance to larger public access APIs. We evaluate syste
    
[^47]: 实现对低资源手势语言识别的稳健手势嵌入提取

    Towards the extraction of robust sign embeddings for low resource sign language recognition. (arXiv:2306.17558v1 [cs.CV])

    [http://arxiv.org/abs/2306.17558](http://arxiv.org/abs/2306.17558)

    本研究的目标是实现对低资源手势语言识别的稳健手势嵌入提取。针对当前存在的问题，人体姿势估计器虽然是理想的选择，但由于域不匹配和手势语言中的挑战性姿势，其在手势语言数据上的稳健性有所欠缺。关键点基于的模型仍然优于图像基于的模型，但其训练方式限制了其在手势语言识别中的应用。

    

    孤立的手势语言识别通常应用于包含由一组有限手势执行者缓慢而清晰执行的相对大规模数据集。然而，在现实世界的场景中，我们面临着具有挑战性的视觉条件、共同发音的手势、小数据集以及对独立演讲者模型的需求。为了解决这个困难的问题，我们需要一个稳健的特征提取器来处理手势语言视频。人体姿势估计器可以被认为是理想的候选者。然而，由于其训练集与手势语言中具有挑战性的姿势之间存在领域不匹配，它们在手势语言数据和基于图像的模型上仍然缺乏稳健性，关键点基于的模型通常仍然优于基于图像的模型。此外，虽然与基于图像的模型进行迁移学习的常见实践可以获得更高的准确性，但关键点基于的模型通常在每个手势语言识别数据集上都是从头开始训练的。这些因素限制了它们在手势语言识别中的实用性。

    Isolated Sign Language Recognition (SLR) has mostly been applied on relatively large datasets containing signs executed slowly and clearly by a limited group of signers. In real-world scenarios, however, we are met with challenging visual conditions, coarticulated signing, small datasets, and the need for signer independent models. To tackle this difficult problem, we require a robust feature extractor to process the sign language videos. One could expect human pose estimators to be ideal candidates. However, due to a domain mismatch with their training sets and challenging poses in sign language, they lack robustness on sign language data and image based models often still outperform keypoint based models. Furthermore, whereas the common practice of transfer learning with image based models yields even higher accuracy, keypoint based models are typically trained from scratch on every SLR dataset. These factors limit their usefulness for SLR. From the existing literature, it is also no
    
[^48]: 大型语言模型对量化理解的探究

    Probing Quantifier Comprehension in Large Language Models. (arXiv:2306.07384v1 [cs.CL])

    [http://arxiv.org/abs/2306.07384](http://arxiv.org/abs/2306.07384)

    本文提出了对于大型语言模型（LLMs）对量化理解的探究，并质疑之前研究中关于LLMs理解极少数类型的量词能力呈现反比例缩放的说法，并提出新的测试方法，展示其与以前研究所展示的行为不同。

    

    随着它们的规模增大，大型语言模型（LLMs）在语言理解任务上的表现越来越好。但即使在具体下游任务上表现出高性能，LLMs 在否定或量化理解等简单语言测试中仍然失败。以前测试 LLMs 对于理解量词的能力的研究表明，随着模型的不断增大，它们在理解大多数类型的量词时变得更好，但在理解极少数类型的量词时变得越来越差，从而呈现出反比例缩放法则的情况。本文质疑了在 LLMs 中反比例缩放极少数类型量词理解能力的说法，并表明这是不合适的测试方法的结果。我们还提出了替代方法来测量 LLMs 的量化理解能力，并展示了随着模型的规模增大，这些行为与以前的研究所展示的不同。LLMs 能够不断理解含义的差异。

    With their increasing size, Large language models (LLMs) are becoming increasingly good at language understanding tasks. But even with high performance on specific downstream task, LLMs fail at simple linguistic tests for negation or quantifier understanding. Previous work on testing capability of LLMs on understanding quantifiers suggest that as the size of the models increase, they get better at understanding most-type quantifiers but get increasingly worse at understanding few-type quantifiers, thus presenting a case of an inverse-scaling law. In this paper, we question the claims of inverse scaling of few-type quantifier understanding in LLMs and show that it is a result of inappropriate testing methodology. We also present alternate methods to measure quantifier comprehension in LLMs and show that as the size of the models increase, these behaviours are different from what is shown in previous research. LLMs are consistently able to understand the difference between the meaning of
    
[^49]: Allophant: 带有发音属性的跨语言音素识别系统

    Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes. (arXiv:2306.04306v1 [cs.CL])

    [http://arxiv.org/abs/2306.04306](http://arxiv.org/abs/2306.04306)

    本文提出了一种跨语言音素识别系统Allophant，结合组成性音素嵌入方法和个别监督的语音属性分类器，并采用多任务结构，使得该系统能够低资源进行语音识别，可以有效提高处理陌生音素和音素库的能力。

    

    本文提出了一种多语言音素识别系统Allophant。它只需要跨语言目标语种的音素清单即可进行低资源语音识别。该系统采用了组成性音素嵌入方法以及个别监督的语音属性分类器，并采用多任务结构进行建模。我们还介绍了Allophoible数据库的扩展。通过将该数据库与基于距离的图音转换方法相结合，我们可以直接在PHOIBLE清单上进行训练。在对34种语言进行训练和评估时，我们发现多任务学习的加入提高了该模型处理陌生音素和音素库的能力。在受监督语言上，与没有多任务学习的基线相比，我们实现了11个百分点的音素误差率改进。在84种零-shot迁移语言的评估中，与基线相比，PER下降了2.63个百分点。

    This paper proposes Allophant, a multilingual phoneme recognizer. It requires only a phoneme inventory for cross-lingual transfer to a target language, allowing for low-resource recognition. The architecture combines a compositional phone embedding approach with individually supervised phonetic attribute classifiers in a multi-task architecture. We also introduce Allophoible, an extension of the PHOIBLE database. When combined with a distance based mapping approach for grapheme-to-phoneme outputs, it allows us to train on PHOIBLE inventories directly. By training and evaluating on 34 languages, we found that the addition of multi-task learning improves the model's capability of being applied to unseen phonemes and phoneme inventories. On supervised languages we achieve phoneme error rate improvements of 11 percentage points (pp.) compared to a baseline without multi-task learning. Evaluation of zero-shot transfer on 84 languages yielded a decrease in PER of 2.63 pp. over the baseline.
    
[^50]: LLMatic: 基于大语言模型和多样性优化的神经结构搜索

    LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])

    [http://arxiv.org/abs/2306.01102](http://arxiv.org/abs/2306.01102)

    本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    

    大型语言模型 (LLMs) 已成为一种强大的工具，可以完成广泛的任务。它们的能力涵盖了许多领域，它们在代码生成领域产生了重大影响。在此情况下，我们将 LLMs 视为变异和交叉工具。同时，多样性优化算法已知可以发现多样性和稳健的解决方案。通过将 LLMs 的代码生成能力与 QD 解决方案的多样性和鲁棒性相结合，我们引入了 LLMatic，一个神经结构搜索 (NAS) 算法。虽然 LLMs 通过提示直接进行 NAS 考验困难，但 LLMatic 利用程序化方法，利用 QD 来进行提示和网络结构，从而创建多样性和高性能网络。我们在 CIFAR-10 图像分类基准测试中测试了 LLMatic，证明它可以在仅进行 2000 次搜索的情况下产生具有竞争力的网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
    
[^51]: 激发Web规模语音模型的潜在能力以实现零-shot任务泛化

    Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])

    [http://arxiv.org/abs/2305.11095](http://arxiv.org/abs/2305.11095)

    本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。

    

    本文研究了最近提出的Web规模语音模型Whisper的新兴功能，在使用提示工程技术调整模型后，适应了未见过的AVSR，CS-ASR和ST三个任务。我们设计了特定于任务的提示，要么利用另一个大规模模型，要么简单地操作默认提示中的特殊标记。实验证明，与默认提示相比，我们提出的提示使这三个零-shot任务的性能提高了10%到45％，甚至在一些数据集上超过了SotA监督模型。此外，我们的实验揭示了Whisper的许多有趣属性，包括其提示的鲁棒性，对口音的偏好以及潜在空间中的多语言理解。代码可在https://github.com/jasonppy/PromptingWhisper上找到。

    We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
    
[^52]: SpecInfer：利用推测推断和令牌树验证加速生成式大语言模型的服务

    SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])

    [http://arxiv.org/abs/2305.09781](http://arxiv.org/abs/2305.09781)

    SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。

    

    由于生成式大语言模型（LLMs）需要高计算和内存需求，因此快速和廉价地为它们提供服务是具有挑战性的。本文介绍SpecInfer，一个LLM服务系统，它利用推测推断和令牌树验证加速生成式LLM推断。SpecInfer背后的关键是将各种小型语言模型进行集体提升调整，共同预测LLM的输出； 预测结果组织成一个令牌树，其中每个节点都表示候选令牌序列。通过一种新颖的基于树的并行解码机制，以LMM作为令牌树验证器来验证令牌树所代表的所有候选令牌序列的正确性。SpecInfer使用LLM作为令牌树验证器，而不是增量解码器，从而显著减少了为生成式LLM提供服务所需的端到端延迟和计算要求，同时可确保模型质量。

    The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
    
[^53]: T-SciQ: 使用大型语言模型信号教授多模态链式思维推理在科学问题回答中的应用

    T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering. (arXiv:2305.03453v1 [cs.CL])

    [http://arxiv.org/abs/2305.03453](http://arxiv.org/abs/2305.03453)

    本研究使用大型语言模型信号教育科学问题回答的链式思维推理，通过生成高质量COT合理化信号，同时降低了人工注释的需求。

    

    大型语言模型(LLMs)近期在各种自然语言处理(NLP)任务中展示了出色的性能。他们还展示了执行链式思维推理以解决复杂问题的能力。最近的研究探索了复杂多模态场景下的链式思维推理，例如通过用高质量人工注释的链式思路来调整多模型模型进行科学问题回答等任务。然而，收集高质量COT合理化通常是耗时且昂贵的。此外，由于涉及冗余信息或丢失重要信息，注释合理化通常不太准确。为了解决这些问题，我们提出了一种新的方法，称为T-SciQ，旨在使用LLM信号教授科学问题回答。T-SciQ方法生成高质量的CoT合理化信号，并先进地训练较小的模型以在复杂模态中执行CoT思维推理。另外，我们引入了一种显着提高模型泛化能力的技术，从而显着减少了对人工注释合理的需求。

    Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the redundant information involved or the essential information missed. To address these issues, we propose a novel method termed \emph{T-SciQ} that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a no
    
[^54]: 近似最近邻短语挖掘在上下文语音识别中的应用

    Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition. (arXiv:2304.08862v1 [cs.CL])

    [http://arxiv.org/abs/2304.08862](http://arxiv.org/abs/2304.08862)

    本文提出了一种使用近似最近邻短语挖掘的方法来训练上下文感知Transformer转录器(CATT)模型，并在大规模数据情况下进行了实验，取得了显著的实验结果。

    

    本文提出了一种使用近似最近邻短语挖掘的方法来训练端到端上下文感知Transformer转录器(CATT)模型的扩展方法。在训练过程中，给定一个参考查询，我们使用近似最近邻搜索挖掘了若干相似的短语作为负例，并将这些短语与随机和真实的上下文信息一起用作上下文列表中的负例。通过将近似最近邻短语（ANN-P）包含在上下文列表中，我们鼓励学习表示来区分相似但不完全相同的偏见短语，从而在偏见清单中存在几个相似的短语时提高偏见准确性。我们在大规模数据情况下进行实验，获得了相对字误率达7％的上下文部分的实验效果。我们还扩展并评估了CATT方法在串流应用中的应用。

    This paper presents an extension to train end-to-end Context-Aware Transformer Transducer ( CATT ) models by using a simple, yet efficient method of mining hard negative phrases from the latent space of the context encoder. During training, given a reference query, we mine a number of similar phrases using approximate nearest neighbour search. These sampled phrases are then used as negative examples in the context list alongside random and ground truth contextual information. By including approximate nearest neighbour phrases (ANN-P) in the context list, we encourage the learned representation to disambiguate between similar, but not identical, biasing phrases. This improves biasing accuracy when there are several similar phrases in the biasing inventory. We carry out experiments in a large-scale data regime obtaining up to 7% relative word error rate reductions for the contextual portion of test data. We also extend and evaluate CATT approach in streaming applications.
    
[^55]: 视觉-语言模型的黑匣子少样本适应

    Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])

    [http://arxiv.org/abs/2304.01752](http://arxiv.org/abs/2304.01752)

    本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的视觉-语言模型的快速少样本适应，适用于有监督和无监督训练，并且可以用于对单模型计算的图像和文本特征进行对齐。

    

    通过对比学习训练的视觉-语言模型在少样本情况下表现出很强的学习能力。软提示学习是少样本领域适用的最受欢迎的方法，旨在通过新领域引发的分布偏移来缩小模态差距。虽然该方法性能高效，但仍需要访问模型权重，并且在具有数十亿个参数的大型模型上可能会导致计算上的不可行性。本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的 V-L 少样本适应，不需要访问模型权重，训练速度快数个数量级，适用于有监督和无监督训练，并且还可以用于对单模型计算的图像和文本特征进行对齐。

    Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple line
    
[^56]: 一种解释性相似案例匹配框架

    An interpretability framework for Similar case matching. (arXiv:2304.01622v1 [cs.CL])

    [http://arxiv.org/abs/2304.01622](http://arxiv.org/abs/2304.01622)

    本论文提出了一个可解释的相似案例匹配框架，其中包括四个模块：司法特征句子识别模块、案例匹配模块、特征句子对齐模块和冲突消歧模块。该框架通过识别案例中的重要信息和对齐两个案例中的特征句，提供了可靠的相似性证据。

    

    相似案例匹配（SCM）旨在确定两个案件是否相似。该任务在法律系统中具有重要的作用，帮助法律专业人员快速找到相关案例，从而更有效地处理它们。现有研究集中在提高模型的性能上，而不是在其可解释性上。因此，本文提出了一个可解释的SCM管道框架，包括四个模块：司法特征句子识别模块、案例匹配模块、特征句子对齐模块和冲突消歧模块。与现有的SCM方法不同，我们的框架将识别包含基本信息的案例特征句，基于提取的特征句结果进行相似案例匹配，并对两个案例中的特征句进行对齐，以提供证据支持案例的相似性。SCM结果可能与特征句对齐结果产生冲突，因此我们的框架进一步消歧。

    Similar Case Matching (SCM) is designed to determine whether two cases are similar. The task has an essential role in the legal system, helping legal professionals to find relevant cases quickly and thus deal with them more efficiently. Existing research has focused on improving the model's performance but not on its interpretability. Therefore, this paper proposes a pipeline framework for interpretable SCM, which consists of four modules: a judicial feature sentence identification module, a case matching module, a feature sentence alignment module, and a conflict disambiguation module. Unlike existing SCM methods, our framework will identify feature sentences in a case that contain essential information, perform similar case matching based on the extracted feature sentence results, and align the feature sentences in the two cases to provide evidence for the similarity of the cases. SCM results may conflict with feature sentence alignment results, and our framework further disambiguate
    
[^57]: 没有正确性的可重复性并不重要：在NLP领域中测试代码的重要性。

    Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])

    [http://arxiv.org/abs/2303.16166](http://arxiv.org/abs/2303.16166)

    在NLP研究中，我们不能仅凭感知质量假定代码正确性，应该推动采用编码最佳实践以提高实验结果的正确性和可靠性。

    

    尽管其在研究实验中发挥了关键作用，但代码正确性往往仅基于结果的感知质量而被假定。这带来了错误结果和潜在误导性发现的风险。为了解决这个问题，我们认为当前关注结果重现应该与强调编码最佳实践相辅相成。我们通过一个案例研究来支持我们向NLP社区发出的号召，在这个案例研究中，我们识别出并纠正了广泛使用的最先进Conformer架构的开源实现中的三个Bug。通过在各种语言环境下进行的自动语音识别和翻译的比较实验，我们证明了Bug的存在并不会妨碍获得良好的和可重复的结果，反而可能导致不正确的结论，为未来的研究可能提供错误的指导。为了应对这一问题，这项研究呼吁采用旨在促进NLP研究中正确性的编码最佳实践，并提高实验结果的可靠性。

    Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
    
[^58]: 使用集成的文本到梅尔频谱生成器的无标记转写领域自适应端到端ASR

    Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator. (arXiv:2302.14036v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2302.14036](http://arxiv.org/abs/2302.14036)

    本文提出了一种使用集成文本到梅尔频谱生成器的无标记转写领域自适应端到端ASR系统。该系统可以在训练过程中动态生成梅尔频谱，并通过使用新领域的仅文本数据来适应ASR模型。研究结果表明，所提出的训练方法显著提高了ASR准确性，并在自适应质量和训练速度上超过了级联TTS系统与声码器。

    

    我们提出了一个端到端的自动语音识别（ASR）系统，可以用转录的语音数据、仅有文本的数据或者二者的混合数据进行训练。所提出的模型使用了一种集成的文本基础训练辅助模块。该模块结合了非自回归的多说话人文本到梅尔频谱生成器和基于GAN的增强器，以提高频谱质量。该系统可以在训练过程中动态生成梅尔频谱。通过使用该新领域的仅文本数据，可以将ASR模型适应到新领域。我们证明了所提出的训练方法与仅训练于转录语音的系统相比显著提高了ASR准确性。它还在自适应质量和训练速度上超过了级联TTS系统与声码器。

    We propose an end-to-end Automatic Speech Recognition (ASR) system that can be trained on transcribed speech data, text-only data, or a mixture of both. The proposed model uses an integrated auxiliary block for text-based training. This block combines a non-autoregressive multi-speaker text-to-mel-spectrogram generator with a GAN-based enhancer to improve the spectrogram quality. The proposed system can generate a mel-spectrogram dynamically during training. It can be used to adapt the ASR model to a new domain by using text-only data from this domain. We demonstrate that the proposed training method significantly improves ASR accuracy compared to the system trained on transcribed speech only. It also surpasses cascade TTS systems with the vocoder in the adaptation quality and training speed.
    
[^59]: 基于语言模型的知识图谱嵌入编辑

    Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10405](http://arxiv.org/abs/2301.10405)

    本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。

    

    近几十年来，使用语言模型进行知识图谱（KG）嵌入已经取得了实证成功。但是，基于语言模型的KG嵌入通常作为静态工件部署，修改起来具有挑战性，需要重新训练。为了解决这个问题，本文提出了一种新的任务，即编辑基于语言模型的KG嵌入。该任务旨在实现对KG嵌入的数据高效和快速更新，而不影响其余部分的性能。我们构建了四个新数据集：E-FB15k237、A-FB15k237、E-WN18RR 和 A-WN18RR，并评估了几种知识编辑基线，证明了之前的模型处理该任务的能力有限。我们进一步提出了一个简单但强大的基线——KGEditor，它利用超网络的附加参数层来编辑/添加事实。全面的实验结果表明，当更新特定事实而不影响其余部分的性能时，KGEditor 的表现更好。

    Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
    
[^60]: 重新思考上下文学习中规模的作用: 基于可解释性的660亿尺度案例研究

    Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale. (arXiv:2212.09095v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09095](http://arxiv.org/abs/2212.09095)

    本文通过使用一个660亿参数的语言模型，在多个任务中发现了上下文学习能力并不均匀分布在其各个组件上。通过移除约70%的注意力头和约20%的前馈网络，任务执行表现仅有轻微下降。此外，在OPT-66B中，存在一小部分注意力头对于上下文学习中的基础归纳操作具有高效能力。

    

    研究表明，通过上下文学习范式，语言模型在规模增加时在各种任务上表现更好。本文通过使用一个660亿参数的语言模型（OPT-66B）在14个不同的下游任务中进行研究，探讨了大型语言模型在上下文学习执行任务的能力是否均匀分布在其所有的组件上。结果发现，约70%的注意力头和约20%的前馈网路可以移除而任务表现仅有轻微下降。在不同任务和上下文示例数量中，我们发现对上下文学习不重要的注意力头的集合存在较大的重叠。同时，我们通过一种任务无关的方式来验证我们的假设，发现OPT-66B中的一小部分注意力头在执行与上下文学习相关的基础归纳操作（即前缀匹配和复制）方面具有高效的能力。

    Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: $\sim$70% of attention heads and $\sim$20% of feed forward networks can be removed with minimal decline in task performance. We find substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, prefix matching and copying. These in
    
[^61]: 生成视频字幕中的事件和实体提取

    Event and Entity Extraction from Generated Video Captions. (arXiv:2211.02982v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.02982](http://arxiv.org/abs/2211.02982)

    该论文提出了一个从生成的视频字幕中提取语义元数据的框架，通过使用密集视频字幕模型，可以提取实体、实体属性、实体之间的关系和视频分类。提取信息的质量受到事件定位质量和字幕生成性能的影响。

    

    由人工进行多媒体数据注释耗时且昂贵，而可靠的自动生成语义元数据是一个重大挑战。我们提出了一个从自动生成的视频字幕中提取语义元数据的框架。作为元数据，我们考虑实体、实体属性、实体之间的关系以及视频分类。我们使用两种最先进的密集视频字幕模型，即遮蔽转换器（MT）和并行解码（PVDC），为ActivityNet Captions数据集的视频生成字幕。我们的实验证明，从生成的字幕中提取实体、实体属性、实体之间的关系和视频分类是可能的。我们观察到，提取信息的质量主要受到视频中事件定位的质量以及事件字幕生成的性能的影响。

    Annotation of multimedia data by humans is time-consuming and costly, while reliable automatic generation of semantic metadata is a major challenge. We propose a framework to extract semantic metadata from automatically generated video captions. As metadata, we consider entities, the entities' properties, relations between entities, and the video category. We employ two state-of-the-art dense video captioning models with masked transformer (MT) and parallel decoding (PVDC) to generate captions for videos of the ActivityNet Captions dataset. Our experiments show that it is possible to extract entities, their properties, relations between entities, and the video category from the generated captions. We observe that the quality of the extracted information is mainly influenced by the quality of the event localization in the video as well as the performance of the event caption generation.
    
[^62]: 分析自监督在处理语言偏见中的局限性

    Analyzing the Limits of Self-Supervision in Handling Bias in Language. (arXiv:2112.08637v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.08637](http://arxiv.org/abs/2112.08637)

    本文分析了自监督在处理语言偏见中的局限性，并定义了四个偏见任务（诊断、识别、提取和改写），通过使用不同类别的任务描述来评估语言模型对语义的捕捉能力。

    

    使用自然语言任务描述作为提示输入已成为从大规模生成性语言模型中引出相对准确输出的流行机制，而同时又几乎没有上下文监督。这也有助于了解语言模型从无标记文本的大规模语言预训练中纯粹捕捉下游任务的语义的能力。这样的模型自然也暴露于许多不希望的内容，如种族主义和性别歧视的语言，目前对模型在这些方面的意识的研究有限。本文中，我们定义并全面评估这种语言模型在四个偏见任务（诊断、识别、提取和改写）中捕捉语义的能力。对于这些任务，我们定义了三类任务描述：陈述、问题和完成，并在每个类别中使用了许多词汇变体。我们研究了使用这些任务描述的提示对每个任务的有效性。

    Prompting inputs with natural language task descriptions has emerged as a popular mechanism to elicit reasonably accurate outputs from large-scale generative language models with little to no in-context supervision. This also helps gain insight into how well language models capture the semantics of a wide range of downstream tasks purely from self-supervised pre-training on massive corpora of unlabeled text. Such models have naturally also been exposed to a lot of undesirable content like racist and sexist language and there is limited work on awareness of models along these dimensions. In this paper, we define and comprehensively evaluate how well such language models capture the semantics of four tasks for bias: diagnosis, identification, extraction and rephrasing. We define three broad classes of task descriptions for these tasks: statement, question, and completion, with numerous lexical variants within each class. We study the efficacy of prompting for each task using these classe
    

