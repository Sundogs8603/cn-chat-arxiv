# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering.](http://arxiv.org/abs/2309.17133) | 本文提出了一种精细化的后期交互多模检索方法（FLMR）来改进检索增强的视觉问答中的知识检索。FLMR通过获取补充的图像表示并使用与现有基于文本的模型相对齐的视觉模型，解决了RA-VQA中检索器的两个主要限制。 |
| [^2] | [The Cambridge Law Corpus: A Corpus for Legal AI Research.](http://arxiv.org/abs/2309.12269) | 剑桥法律语料库是一个用于法律人工智能研究的语料库，包含来自英国的超过250,000个法庭案例。在该语料库的基础上，我们提供了案例结果的专家注解，并使用多个模型进行了案例结果提取的训练和评估，为研究提供了基准。 |
| [^3] | [Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering.](http://arxiv.org/abs/2308.13259) | 本文提出了一个名为知识驱动的思维链（KD-CoT）的框架，用于验证和修改LLMs中的推理过程，通过与外部知识的交互来解决幻觉和错误传播的问题。 |
| [^4] | [An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM.](http://arxiv.org/abs/2308.06828) | 本研究提出了一种集成Electra Transformer、GloVe和LSTM模型的创新问题分类方法，通过在TREC数据集上进行严格测试，证明了融合不同技术可以获得更优越的结果。 |
| [^5] | [Towards Populating Generalizable Engineering Design Knowledge.](http://arxiv.org/abs/2307.06985) | 这项研究提出了一种从专利文件中提取工程设计知识的方法，通过构建知识图来填充通用设计知识，并与现有方法进行了比较。 |
| [^6] | [A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media.](http://arxiv.org/abs/2307.06775) | 本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。 |
| [^7] | [Instruction Mining: High-Quality Instruction Data Selection for Large Language Models.](http://arxiv.org/abs/2307.06290) | 本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。 |
| [^8] | [BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset.](http://arxiv.org/abs/2307.04657) | 本文介绍了BeaverTails数据集，用于研究LLM的安全对齐。该数据集分开注释了问答对的有用性和无害性，为安全开发提供了重要资源。 |
| [^9] | [SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs.](http://arxiv.org/abs/2306.17842) | 本研究引入了SPAE，使用语义金字塔自编码器实现了冻结LLM执行涉及非语言模态的理解和生成任务。通过将图像转化为LLM可理解的词汇标记，我们的方法成功地提升了冻结LLM在图像理解任务中的性能，超过了现有技术25%以上。 |
| [^10] | [On the Exploitability of Instruction Tuning.](http://arxiv.org/abs/2306.17194) | 该论文研究了如何利用指令调整技术来改变模型行为的问题，并提出了一种自动数据注入的方法AutoPoison。实验结果表明，通过少量的训练数据毒化，对手能够改变模型的行为。 |
| [^11] | [InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback.](http://arxiv.org/abs/2306.14898) | InterCode是一个交互式编码的标准化和基准测试框架，它使用执行反馈作为观察，并提供了安全可重现的执行环境，可以用于开发新的交互式代码生成方法。 |
| [^12] | [Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation.](http://arxiv.org/abs/2306.13460) | 本文通过半透过最大似然估计方法，鼓励模型生成更详细的长字幕。 |
| [^13] | [SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning.](http://arxiv.org/abs/2306.12552) | 本文介绍了一种具有挑战性的任务SituatedGen，要求具有常识推理能力的机器生成一对对比句子，以融入地理和时间背景。作者提出了一种相应的英语数据集，并发现目前的生成式语言模型仍然难以实现具有常识合理性的句子的生成，远远落后于人类表现。 |
| [^14] | [Quilt-1M: One Million Image-Text Pairs for Histopathology.](http://arxiv.org/abs/2306.11207) | 本文介绍了一个名为 Quilt-1M 的癌症组织学图像和文字对的百万数据集，并利用 YouTube 上的专家医生教程视频为主要来源。这个数据集将使得癌症组织学领域的表征学习取得类似于其他领域的进展。 |
| [^15] | [Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective.](http://arxiv.org/abs/2306.10512) | 本研究提出了一种自适应测试框架，用于高效测量语言模型的认知能力。通过动态调整测试问题的特性，能够更准确地评估模型的能力，并使用更少的问题。同时，该框架使得语言模型能够与人类进行轻松比较。 |
| [^16] | [Block-State Transformer.](http://arxiv.org/abs/2306.09539) | 本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。 |
| [^17] | [Large Language Model Is Semi-Parametric Reinforcement Learning Agent.](http://arxiv.org/abs/2306.07929) | 根据人类记忆和推理机制，提出了一种新的可演化LLM智能体框架REMEMBERER，通过为LLM装备长期经验记忆，可以为不同任务提供优异的智能体，其构成了半参数RL代理。成功率超过先前SOTA 4％和2％。 |
| [^18] | [Factorized Contrastive Learning: Going Beyond Multi-view Redundancy.](http://arxiv.org/abs/2306.05268) | 本论文提出了FactorCL，一种新的多模态表示学习方法，不仅考虑跨模态共享信息，还能捕捉跨模态唯一的任务相关信息。 |
| [^19] | [LEACE: Perfect linear concept erasure in closed form.](http://arxiv.org/abs/2306.03819) | 本文介绍了一种闭合形式的方法LEACE，可在删除指定特征的同时尽可能少地改变表示，并可证明防止所有线性分类器检测到概念。作者用“概念擦除”这一新方法将其应用于大型语言模型，在测量语言模型对词性的依赖性和减少BERT嵌入中的性别偏差任务中得出良好表现。 |
| [^20] | [Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis.](http://arxiv.org/abs/2306.02213) | 本研究首次对自动生成的情感曲线进行了系统和定量的评估，并比较了机器学习模型和词典方法两种生成情感曲线的方式。通过在不同语言的多个数据集上进行实验，我们发现虽然词典方法在实例级情感分类方面表现较差，但在聚合信息时生成情感曲线的准确性非常高。此外，我们还展示了通过自动翻译英语情感词典，可以在资源较少的情况下生成高质量的情感曲线。 |
| [^21] | [Fine-Grained Human Feedback Gives Better Rewards for Language Model Training.](http://arxiv.org/abs/2306.01693) | 本文提出了Fine-Grained RLHF框架，使用精细化的人类反馈作为明确的训练信号来训练和学习语言模型。该框架提供了多个细致的奖励模型来获得更好的效果。 |
| [^22] | [Exposing Attention Glitches with Flip-Flop Language Modeling.](http://arxiv.org/abs/2306.00946) | 本论文揭示了语言模型中注意力故障的现象，并通过引入翻转-翻转语言建模来分析这个问题。研究发现，Transformer FFLMs经常出现推理错误。 |
| [^23] | [Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images.](http://arxiv.org/abs/2306.00219) | 本论文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地根据目标区域修改AI合成图像并保留原始上下文。与其他最先进的图像修复技术进行比较，该方法在用户研究中表现出更好的可用性和有效性。 |
| [^24] | [Improving CLIP Training with Language Rewrites.](http://arxiv.org/abs/2305.20088) | 本文介绍了一种名为Language augmented CLIP（LaCLIP）的方法，通过语言重写来增强CLIP训练。利用大型语言模型的能力，重新书写与每个图像关联的文本描述，以增加多样性，同时保留原始的关键概念和意义。 |
| [^25] | [SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models.](http://arxiv.org/abs/2305.19308) | 本研究提出了一个基于大型语言模型（LLMs）的代理程序SheetCopilot，该程序可以通过自然语言指导软件执行电子表格数据处理等任务。该程序设计了一组抽象的电子表格软件功能原子动作以及基于状态机的任务规划框架，实现了LLMs与电子表格的鲁棒交互，可以单次正确完成44.3％的任务。 |
| [^26] | [LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images.](http://arxiv.org/abs/2305.19164) | 本文提出了一种自动化算法LANCE，通过生成语言引导的对抗性测试图像来压力测试视觉模型，实现了在不改变模型权重的情况下增加多样、逼真且具有挑战性的测试图像。通过对多种预训练模型的性能进行基准测试，发现模型性能显著下降，并且通过分析模型对不同类型编辑的敏感性，揭示了ImageNet中先前未知的类别层次模型偏差。 |
| [^27] | [Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks.](http://arxiv.org/abs/2305.18395) | 本论文提出了一种KARD方法，可以通过向小型语言模型中加入从外部知识库检索到的增强知识来解决知识密集型推理任务中小型语言模型记忆能力有限的问题。 |
| [^28] | [Emergent Modularity in Pre-trained Transformers.](http://arxiv.org/abs/2305.18390) | 本论文研究了预训练Transformers中的自发模块化现象，发现神经元可以进行功能专业化，并通过聚类建立起模块化结构，此结构可被有效扰动。 |
| [^29] | [Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making.](http://arxiv.org/abs/2305.17588) | 该论文介绍了一种名为SUFO的框架，用于增强微调的变压器模型的可解释性。该框架利用多种分析和可视化技术，解决了模型信任和可解释性的关键问题，并通过案例研究验证了其有效性。 |
| [^30] | [Training Socially Aligned Language Models in Simulated Human Society.](http://arxiv.org/abs/2305.16960) | 本研究提出了一种在模拟人类社会中训练语言模型的新方法，相比于现有方法，该方法具有更大的可扩展性和高效性，并在对齐基准和人类评估中展示出更优异的性能。 |
| [^31] | [On Evaluating Adversarial Robustness of Large Vision-Language Models.](http://arxiv.org/abs/2305.16934) | 该论文提出在最真实和高风险的情境中评估大型视觉语言模型的对抗鲁棒性。作者首先构建有针对性的对抗样本，然后将其转移到其他模型中进行评估，并观察到黑盒查询可以改进效果。 |
| [^32] | [Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer.](http://arxiv.org/abs/2305.16380) | 本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。 |
| [^33] | [Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4.](http://arxiv.org/abs/2305.14928) | 本研究提出利用泛化、不确定性和最新的大型语言模型，寻求解决假新闻问题。实验证明GPT-4在多语言环境下表现优于之前的方法。研究还探讨了泛化和不确定性处理技术，并在其他语言模型、温度、提示、版本控制、可解释性和网络检索方面取得了实际见解和未来研究方向。此外，研究还发布了新颖的英法配对假新闻数据集LIAR-New，为信息真实性评估提供了可行性标签。 |
| [^34] | [Text encoders bottleneck compositionality in contrastive vision-language models.](http://arxiv.org/abs/2305.14897) | 本研究发现，在对比视觉-语言模型中，使用单个向量表示标语的文本编码器在处理更加复杂的输入时表现不佳，但某些文本编码器的性能明显优于其他编码器。仅基于文本的恢复性能能够预测多模态匹配性能。 |
| [^35] | [MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions.](http://arxiv.org/abs/2305.14795) | 本文提出了一种基准测试MQuAKE，通过多跳问题评估编辑模型是否能够正确回答因编辑事实而答案应该改变的问题。研究发现当前的知识编辑方法可以准确召回已编辑的事实，但在多跳问题上表现灾难性失败。 |
| [^36] | [A Controllable QA-based Framework for Decontextualization.](http://arxiv.org/abs/2305.14772) | 本文提出了一个基于问答的去文本化框架，可以更好地展示提取的文本摘录。在问答和引证上的表现类似于端到端方法，并且支持用户信息需求及偏好的可控性。 |
| [^37] | [Analyzing Influential Factors in Human Preference Judgments via GPT-4.](http://arxiv.org/abs/2305.14702) | 本文利用Bradley-Terry-Luce模型对OpenAI公布的人类偏好判断数据集进行了深入研究，揭示了人类偏好判断中所蕴含的固有偏好，提出了提高样本效率的策略，并为构建平衡的人类偏好判断数据集提供了洞见。 |
| [^38] | [ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers.](http://arxiv.org/abs/2305.14591) | ALGO框架使用由LLM生成的神谕指导创造和验证算法程序，以提高现有代码生成模型的算法问题解决能力。 |
| [^39] | [WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia.](http://arxiv.org/abs/2305.14292) | WikiChat是一种以少样本为基础的语言模型聊天机器人，通过对维基百科进行 grounding，它几乎不会产生幻觉，具有高对话能力和低延迟。WikiChat从LLM中生成响应，保留基于事实的内容，并从语料库中检索到的信息结合，形成真实和引人入胜的回复。经过评估，它比其他模型表现更好，并且能在模拟对话中达到97.3%的事实准确性。 |
| [^40] | [Hierarchical Prompting Assists Large Language Model on Web Navigation.](http://arxiv.org/abs/2305.14257) | 这项研究提出了一种分层提示方法来解决大规模语言模型在处理复杂观察的交互决策任务中的困难。研究表明该方法在网络导航中的效果优于先前最先进的提示机制，具有广泛的适用性。 |
| [^41] | [Language Models with Rationality.](http://arxiv.org/abs/2305.14250) | 本研究提出了一种名为REFLEX的方法，在大型语言模型上添加了合理性和自反性层，以使模型的答案得以解释并消除潜在的矛盾。 |
| [^42] | [A Framework for Bidirectional Decoding: Case Study in Morphological Inflection.](http://arxiv.org/abs/2305.12580) | 本文提出了一种双向解码框架，可以从“外向内”生成序列，并且在多个模型架构和训练方法上进行了改进。该模型在长序列上表现优异，在2022年和2023年共享任务上取得了最佳结果。 |
| [^43] | [LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4.](http://arxiv.org/abs/2305.12147) | 该论文提出了LogiCoT, 一个基于GPT-4的逻辑思维指令调整数据集，用于教授模型逻辑推理和引出一般推理技能。 |
| [^44] | [STOAT: Structured Data to Analytical Text With Controls.](http://arxiv.org/abs/2305.11826) | STOAT模型是表格和推理意识的生成模型，在数字推理、常识推理、时间推理、表格知识和实体知识方面有较好的控制，提高了分析句子生成的质量和准确度。 |
| [^45] | [Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation.](http://arxiv.org/abs/2305.11317) | 本论文通过集成GPT-k来提高T2I生成中的编辑效率，实验证明其更擅长调整（修改）文本中的修饰语，而人类倾向于替换单词和短语。 |
| [^46] | [Language Models Meet World Models: Embodied Experiences Enhance Language Models.](http://arxiv.org/abs/2305.10626) | 本文提出一种新的增强语言模型的方法——将其与世界模型相结合，通过有目的的规划和随机探索获得丰富的实体经验进行微调, 以提高其在物理环境下的推理和行为能力，并在语言基准上保持或提高性能。 |
| [^47] | [Statistical Knowledge Assessment for Generative Language Models.](http://arxiv.org/abs/2305.10519) | 本文介绍了一个统计知识评估框架，用于评估生成式语言模型（GLMs）的知识输出。通过对14种GLMs进行综合比较，发现知识遵循缩放定律，而对指令遵循数据进行微调可能会损害模型的生成能力。 |
| [^48] | [Personality Understanding of Fictional Characters during Book Reading.](http://arxiv.org/abs/2305.10156) | 本文提出了一个NLP领域内尚未研究的问题：情景和细致地理解小说人物个性，并提供了第一个标记数据集PersoNet来解决这个问题。 |
| [^49] | [Can Language Models Solve Graph Problems in Natural Language?.](http://arxiv.org/abs/2305.10037) | 本论文提出了自然语言图形(NLGraph)，这是一个全面的基于图形问题解决测试，旨在评估LLM在文本描述的图形结构和图形解决方案方面的处理能力。实验结果表明，LLM(GPT-3/4)具有相应的图形推理能力。 |
| [^50] | [The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation.](http://arxiv.org/abs/2305.06156) | The Vault是一个提供了10种流行编程语言的40百万行代码-文本对的开源数据集，旨在增强面向代码的大型语言模型（LLM）的训练，有望在代码理解和生成任务上取得显著进展。 |
| [^51] | [Non-Autoregressive Math Word Problem Solver with Unified Tree Structure.](http://arxiv.org/abs/2305.04556) | 该论文介绍了一种非自回归数学题解决器，使用统一树结构表示解决方案表达式，解决了现有方法在处理数学变体时的问题。 |
| [^52] | [NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports.](http://arxiv.org/abs/2305.03598) | 本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。 |
| [^53] | [Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment.](http://arxiv.org/abs/2305.03510) | 本文提出了一个通过翻译对齐的方式实现参数高效、跨语言的迁移学习框架，实验结果显示该框架能够显著减少多语言之间的性能差异。 |
| [^54] | [Turning Flowchart into Dialog: Plan-based Data Augmentation for Low-Resource Flowchart-grounded Troubleshooting Dialogs.](http://arxiv.org/abs/2305.01323) | 本文提出了一个基于计划的数据增强方法，能够将简洁的流程图转化成对话，以生成足够的数据来训练以流程图为基础的故障排除对话系统，实验结果表明该方法有效地提高了系统性能。 |
| [^55] | [Pretrain on just structure: Understanding linguistic inductive biases using transfer learning.](http://arxiv.org/abs/2304.13060) | 通过在人造结构数据上进行预先训练和在英语上微调，我们研究了自然语言处理中三种归纳偏置类型：递归的层级处理、无限制的标记-标记依赖以及基于Zipfian幂律词汇分布的归纳偏置，我们得出复杂标记-标记交互形成了最好的归纳偏置的结论。 |
| [^56] | [PUNR: Pre-training with User Behavior Modeling for News Recommendation.](http://arxiv.org/abs/2304.12633) | 本论文提出了一种无监督的预训练方法，它可以通过两个任务实现有效的用户行为建模，以提高新闻推荐系统的准确性和性能表现。 |
| [^57] | [Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing.](http://arxiv.org/abs/2304.08315) | 本文调查了自然语言处理（NLP）领域的双重使用问题，提出了一份定制的双重使用定义，并讨论了当前的状况和可能的挑战。 |
| [^58] | [Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text.](http://arxiv.org/abs/2304.06939) | Multimodal C4是一个开放的、以图像与文本交替形式存在的数据库，其使用线性分配算法将图像放到长文本段落中，可用于通过少量样本学习和复杂相关度提示的建模。 |
| [^59] | [Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis.](http://arxiv.org/abs/2304.04675) | 本文系统地研究了大语言模型在多语机器翻译中的优势和挑战，证明其表现出卓越的潜力。本研究发现LLMs在给定上下文示例时可以意外地忽略提示语义，并且跨语言示例可以为低资源翻译提供更好的任务指导。但实证结果表明，即使是最好的模型ChatGPT仍然落后于监督基线NLLB。 |
| [^60] | [MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities.](http://arxiv.org/abs/2304.01969) | MEGClass是一种通过相互增强的文本粒度实现极弱监督文本分类的方法，利用分层关注机制从文档、句子和单词中提取连贯且多样的子文本，能够准确分类讨论多个主题的文档，并且在五个基准数据集上表现优于现有最先进的模型。 |
| [^61] | [Efficient distributed representations beyond negative sampling.](http://arxiv.org/abs/2303.17475) | 本文介绍了一种高效的分布式表示（嵌入）学习方法，通过线性时间估计softmax归一化常数来实现学习过程，该方法优于负采样方法并在多项测试中验证了其有效性。 |
| [^62] | [Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction.](http://arxiv.org/abs/2303.04132) | 本文展示了利用不对称性进行合成训练数据生成的方法，可以针对结构化输出问题产生大规模、高质量的数据。在封闭信息提取任务中，我们合成生成了一个包含180万数据点的数据集，并利用此数据集对小型模型进行微调，取得了远超先前领先技术的成果。 |
| [^63] | [Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework.](http://arxiv.org/abs/2302.12247) | 通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。 |
| [^64] | [Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management.](http://arxiv.org/abs/2302.10850) | 本论文研究了离线强化学习在混合专家对话管理中的应用。通过利用最新的混合专家语言模型（MoE-LMs），我们开发了针对对话规划的强化学习算法，显著减少了动作空间的大小，并提高了对话管理的有效性。 |
| [^65] | [Keep it Neutral: Using Natural Language Inference to Improve Generation.](http://arxiv.org/abs/2302.08577) | 本文将自然语言推理（NLI）引入文本生成过程中，通过预训练的NLI模型评估生成的句子是否符合、与原始文本相矛盾或中立。最大化中立类别的NLI策略提供了最高质量的生成文本，无论参数取值如何。 |
| [^66] | [CAPSTONE: Curriculum Sampling for Dense Retrieval with Document Expansion.](http://arxiv.org/abs/2212.09114) | 本文提出了一种使用课程采样策略的密集检索方法，通过在训练中使用伪查询，逐步增强了生成的查询和真实查询之间的相关性。 |
| [^67] | [MelHuBERT: A simplified HuBERT on Mel spectrograms.](http://arxiv.org/abs/2211.09944) | MelHuBERT是基于Mel频谱图的简化版HuBERT模型，通过改进损失函数、输入表示和多阶段训练，在语音识别方面取得了有利表现，节省了31.2%的预训练时间和33.5%的计算资源。 |
| [^68] | [Mining Word Boundaries in Speech as Naturally Annotated Word Segmentation Data.](http://arxiv.org/abs/2210.17122) | 本研究通过从平行语音/文本数据中挖掘词边界，提出了一种可以在跨领域和资源匮乏场景下显著提高汉语词分割性能的方法。 |
| [^69] | [On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?.](http://arxiv.org/abs/2210.12770) | 本研究探讨了在临床自然语言处理任务中，将从一般领域或相关领域数据预训练的迁移学习模型微调到特定任务上的有效性。实验结果显示，微调的语言模型相对于从头开始学习的模型在命名实体识别任务上取得了更好的性能。 |
| [^70] | [SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models.](http://arxiv.org/abs/2210.04802) | 本文是第一个系统研究超分布泛化问题的方法，通过模拟不同维度的源代码数据属性和微调方法，在不同场景中研究了模型的行为。 |
| [^71] | [Are All Steps Equally Important? Benchmarking Essentiality Detection of Events.](http://arxiv.org/abs/2210.04074) | 本文研究了当前模型在理解与目标事件有关的步骤事件的重要性方面的困难，并贡献了一个由专家手动注释步骤重要性的语料库。 |
| [^72] | [Hansel: A Chinese Few-Shot and Zero-Shot Entity Linking Benchmark.](http://arxiv.org/abs/2207.13005) | Hansel是一个中文Few-Shot和Zero-Shot实体链接基准，填补了非英语语言中关注尾部和新兴实体的数据集的空白。该基准测试集由人工注释和审核，使用一种新方法收集Zero-Shot实体链接数据集，并以Wikidata作为目标知识库。研究表明，现有的最先进实体链接系统在Hansel上表现较差，我们建立了一个强基线，在Few-Shot上达到了46.2%的准确率，在Zero-Shot上达到了76.6%的准确率，并在TAC-KBP2015中文实体链接任务上取得了竞争性结果。 |
| [^73] | [Language models show human-like content effects on reasoning tasks.](http://arxiv.org/abs/2207.07051) | 本研究探讨了语言模型在逻辑推理任务中是否像人类一样通过混入内容来影响答案，结果发现大型语言模型的先验期望能够捕捉到这种特征。 |
| [^74] | [Evaluating and Inducing Personality in Pre-trained Language Models.](http://arxiv.org/abs/2206.07550) | 本文提出了Machine Personality Inventory (MPI)数据集用于评估机器个性，通过评估，证明了预训练语言模型(LLM)具有个性。进一步提出了Personality Prompting (P^2)方法，用于以可控的方式诱导LLM具有特定个性。 |
| [^75] | [Speculative Decoding: Lossless Speedup of Autoregressive Translation.](http://arxiv.org/abs/2203.16487) | Speculative Decoding是一种新型解码范式，结合了自回归翻译（AT）和非自回归翻译（NAT）的优势，提供了无损加速的翻译方法。在每个解码步骤中，它推测性地预测下一个标记，并使用验证模型确保翻译结果与AT完全相同。通过推测解码和验证的协作，实现了更快的解码速度，同时保持翻译质量不变。实验证明，原始的SpecDec与AT贪婪解码的结果完全相同。 |
| [^76] | [Geodesic Multi-Modal Mixup for Robust Fine-Tuning.](http://arxiv.org/abs/2203.03897) | 本文研究了CLIP模型的多模态嵌入质量，并发现其统一性和对齐性不足，限制了嵌入的传递性和鲁棒性。为了解决这个问题，我们提出了一种新的鲁棒微调方法，通过高度几何多模型混合生成难负样本，并对模型进行微调。 |
| [^77] | [A Survey of Knowledge Enhanced Pre-trained Models.](http://arxiv.org/abs/2110.00269) | 本综述提供了关于NLP中知识增强预训练语言模型的综合概述，讨论了预训练语言模型和知识表示学习的进展，并从三个不同的角度对现有的KEPLMs进行了分类，最后概述了未来研究中KEPLMs的潜在方向。 |
| [^78] | [BSDAR: Beam Search Decoding with Attention Reward in Neural Keyphrase Generation.](http://arxiv.org/abs/1909.09485) | 本研究提出了一种基于注意力奖励的束搜索解码策略，用于解决神经关键词生成中的序列长度偏差和束多样性问题，该方法显著提高了生成关键词的解码性能。 |

# 详细

[^1]: 精细化的后期交互多模检索用于检索增强视觉问答

    Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering. (arXiv:2309.17133v1 [cs.CL])

    [http://arxiv.org/abs/2309.17133](http://arxiv.org/abs/2309.17133)

    本文提出了一种精细化的后期交互多模检索方法（FLMR）来改进检索增强的视觉问答中的知识检索。FLMR通过获取补充的图像表示并使用与现有基于文本的模型相对齐的视觉模型，解决了RA-VQA中检索器的两个主要限制。

    

    基于知识的视觉问答（KB-VQA）要求VQA系统利用现有知识库中的知识来回答与视觉相关的问题。检索增强的视觉问答（RA-VQA）是一种强大的框架，用于解决KB-VQA问题，首先使用密集段落检索（DPR）检索相关文档，然后利用这些文档回答问题。本文提出了精细化的后期交互多模检索（FLMR），显著改进了RA-VQA中的知识检索。FLMR解决了RA-VQA检索器中的两个主要限制：（1）通过图像到文本转换获得的图像表示可能不完整和不准确，（2）查询和文档之间的相关性分数是通过一维嵌入计算的，可能对更细粒度的相关性不敏感。FLMR通过使用与现有基于文本的模型相对齐的视觉模型获取补充图像表示来克服这些限制。

    Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems to utilize knowledge from existing knowledge bases to answer visually-grounded questions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strong framework to tackle KB-VQA, first retrieves related documents with Dense Passage Retrieval (DPR) and then uses them to answer questions. This paper proposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) which significantly improves knowledge retrieval in RA-VQA. FLMR addresses two major limitations in RA-VQA's retriever: (1) the image representations obtained via image-to-text transforms can be incomplete and inaccurate and (2) relevance scores between queries and documents are computed with one-dimensional embeddings, which can be insensitive to finer-grained relevance. FLMR overcomes these limitations by obtaining image representations that complement those from the image-to-text transforms using a vision model aligned with an existing text-based r
    
[^2]: 剑桥法律语料库：用于法律人工智能研究的语料库

    The Cambridge Law Corpus: A Corpus for Legal AI Research. (arXiv:2309.12269v1 [cs.CL])

    [http://arxiv.org/abs/2309.12269](http://arxiv.org/abs/2309.12269)

    剑桥法律语料库是一个用于法律人工智能研究的语料库，包含来自英国的超过250,000个法庭案例。在该语料库的基础上，我们提供了案例结果的专家注解，并使用多个模型进行了案例结果提取的训练和评估，为研究提供了基准。

    

    我们介绍了剑桥法律语料库（CLC），这是一个用于法律人工智能研究的语料库。它包含了来自英国的超过250,000个法庭案例。大部分案例来自21世纪，但该语料库包括了16世纪以来的案例。本文介绍了该语料库的首次发布，包括原始文本和元数据。在语料库的基础上，我们提供了638个案例的法律专家对案例结果的注解。我们使用我们的标注数据，训练和评估了GPT-3、GPT-4和RoBERTa模型进行案例结果提取，以提供基准。我们还进行了广泛的法律和伦理讨论，以解决这些材料可能具有敏感性的问题。因此，该语料库只会在一定限制下用于研究目的。

    We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research. It consists of over 250 000 court cases from the UK. Most cases are from the 21st century, but the corpus includes cases as old as the 16th century. This paper presents the first release of the corpus, containing the raw text and meta-data. Together with the corpus, we provide annotations on case outcomes for 638 cases, done by legal experts. Using our annotated data, we have trained and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to provide benchmarks. We include an extensive legal and ethical discussion to address the potentially sensitive nature of this material. As a consequence, the corpus will only be released for research purposes under certain restrictions.
    
[^3]: 基于知识驱动的CoT：探索LLMs中对知识密集型问答进行忠实推理

    Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v1 [cs.CL])

    [http://arxiv.org/abs/2308.13259](http://arxiv.org/abs/2308.13259)

    本文提出了一个名为知识驱动的思维链（KD-CoT）的框架，用于验证和修改LLMs中的推理过程，通过与外部知识的交互来解决幻觉和错误传播的问题。

    

    大型语言模型（LLMs）配备了思维链（CoT），在各种下游任务中展现出了令人印象深刻的推理能力。但是，由于幻觉和无法访问外部知识，LLMs在对知识密集型任务（如知识库问答）进行推理时常常会产生不正确或不忠实的中间推理步骤。为了缓解这个问题，我们提出了一个名为知识驱动的思维链（KD-CoT）的框架，通过与外部知识的交互来验证和修改CoT中的推理过程，从而克服幻觉和错误传播。具体地，我们将LLMs的CoT推理过程规范化为结构化的多轮问答格式。在每一轮中，LLMs与一个问答系统进行交互，该系统检索外部知识并基于检索到的准确答案产生忠实的推理过程。我们开发的KBQA CoT集合促进了LLMs的结构化CoT推理，它作为上下文学习的一部分。

    Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning
    
[^4]: 问题分类的集成方法：融合Electra Transformer、GloVe和LSTM

    An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM. (arXiv:2308.06828v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.06828](http://arxiv.org/abs/2308.06828)

    本研究提出了一种集成Electra Transformer、GloVe和LSTM模型的创新问题分类方法，通过在TREC数据集上进行严格测试，证明了融合不同技术可以获得更优越的结果。

    

    自然语言处理（NLP）已经成为理解和生成人类语言的关键技术，它在机器翻译、情感分析等任务中扮演着重要角色，尤其是在问题分类方面。作为自然语言处理的子领域，问题分类专注于确定所需信息的类型，这是问题回答系统等下游应用的基本步骤。本研究提出了一种创新的问题分类集成方法，将Electra、GloVe和LSTM模型的优势相结合。该模型在著名的TREC数据集上进行了严格测试，展示了如何整合这些不同技术可以得到更优越的结果。Electra提供了基于transformer的复杂语言理解能力，GloVe提供了全局向量表示以捕捉词级语义，LSTM则贡献了序列学习能力以建模长期依赖关系。

    Natural Language Processing (NLP) has emerged as a crucial technology for understanding and generating human language, playing an essential role in tasks such as machine translation, sentiment analysis, and more pertinently, question classification. As a subfield within NLP, question classification focuses on determining the type of information being sought, a fundamental step for downstream applications like question answering systems. This study presents an innovative ensemble approach for question classification, combining the strengths of Electra, GloVe, and LSTM models. Rigorously tested on the well-regarded TREC dataset, the model demonstrates how the integration of these disparate technologies can lead to superior results. Electra brings in its transformer-based capabilities for complex language understanding, GloVe offers global vector representations for capturing word-level semantics, and LSTM contributes its sequence learning abilities to model long-term dependencies. By fus
    
[^5]: 迈向填充通用工程设计知识的方法

    Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])

    [http://arxiv.org/abs/2307.06985](http://arxiv.org/abs/2307.06985)

    这项研究提出了一种从专利文件中提取工程设计知识的方法，通过构建知识图来填充通用设计知识，并与现有方法进行了比较。

    

    为了填充通用工程设计知识，我们提出了一种从专利文件中提取head entity :: relationship :: tail entity形式事实的方法。这些事实可以在专利文件内部和跨文件之间组合形成知识图，用作表示和存储设计知识的方案。现有的工程设计文献中的方法通常利用一组预定义的关系来填充统计近似而非事实的三元组。在我们的方法中，我们训练一个标记器来识别句子中的实体和关系。在确定了一对实体后，我们训练另一个标记器来识别特定表示这对实体之间关系的关系标记。为了训练这些标记器，我们手动构建了一个包含44,227个句子和相应事实的数据集。我们还将该方法的性能与通常推荐的方法进行了比较，其中我们预.

    Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
    
[^6]: 一种新型的与平台无关的多模态深度学习模型，用于识别社交媒体上的促进饮食紊乱内容

    A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])

    [http://arxiv.org/abs/2307.06775](http://arxiv.org/abs/2307.06775)

    本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。

    

    在过去的十年中，饮食紊乱的诊断和与之相关的死亡数量大幅增加，尤其是在新冠疫情期间。这种巨大增长部分来源于疫情的压力，但也与社交媒体的暴露增加有关，社交媒体上充斥着促进饮食紊乱的内容。这些内容可以诱发观看者的饮食紊乱。本研究旨在创建一个多模态深度学习模型，能够基于视觉和文本数据的组合判断给定的社交媒体帖子是否促进饮食紊乱。从Twitter收集了一个带有标签的推文数据集，对其进行了十二个深度学习模型的训练和测试。根据模型的性能，最有效的深度学习模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的多模态融合模型，准确率和F1分数分别达到95.9%和0.959。RoBERTa和MaxViT融合模型可以有效地识别社交媒体上的促进饮食紊乱的内容。

    Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
    
[^7]: 指令挖掘：大语言模型的高质量指令数据选择

    Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])

    [http://arxiv.org/abs/2307.06290](http://arxiv.org/abs/2307.06290)

    本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。

    

    大型语言模型通常经历预训练和微调两个训练阶段。尽管大规模预训练赋予模型强大的生成自然语言回应的能力，但这些预训练模型有时仍然无法理解人类指令。为了增强语言模型解释和响应指令的能力，指令微调已成为该领域的关键方法。最近的研究发现，即使只有少量高质量的指令跟随数据，大型语言模型也可以进行良好的微调。然而，选择用于微调语言模型的高质量数据集仍缺乏明确的指导方针。在本文中，我们提出了InstructMining，一个用于评估指令跟随数据质量的线性规则。我们使用具体的自然语言指标来进行InstructMining的建模。为了研究数据质量与这些指标之间的关系，我们还进行了广泛的细致研究。

    Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
    
[^8]: BeaverTails：通过人类偏好数据集改善LLM的安全对齐

    BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset. (arXiv:2307.04657v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.04657](http://arxiv.org/abs/2307.04657)

    本文介绍了BeaverTails数据集，用于研究LLM的安全对齐。该数据集分开注释了问答对的有用性和无害性，为安全开发提供了重要资源。

    

    本文介绍了“BeaverTails”数据集，旨在促进大型语言模型（LLM）的安全对齐研究。该数据集独特地对问答对的有用性和无害性进行了分开注释，从而为这些关键属性提供了不同的观点。总共，我们为30,207个问答对和30,144对专家比较数据收集了安全元标签，用于衡量有用性和无害性指标。我们进一步展示了BeaverTails在内容管理和强化学习与人类反馈（RLHF）中的应用，强调其在LLM中实施实际安全措施的潜力。我们相信这个数据集为社区提供了重要资源，为LLM的安全开发和部署做出了贡献。

    In this paper, we introduce the \textsc{BeaverTails} dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 30,207 question-answer (QA) pairs and 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our 
    
[^9]: SPAE: 基于语义金字塔自编码器的冻结LLM的多模态生成

    SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs. (arXiv:2306.17842v1 [cs.CV])

    [http://arxiv.org/abs/2306.17842](http://arxiv.org/abs/2306.17842)

    本研究引入了SPAE，使用语义金字塔自编码器实现了冻结LLM执行涉及非语言模态的理解和生成任务。通过将图像转化为LLM可理解的词汇标记，我们的方法成功地提升了冻结LLM在图像理解任务中的性能，超过了现有技术25%以上。

    

    本研究引入了Semantic Pyramid AutoEncoder (SPAE)，使冻结的LLM能够执行涉及非语言模态（如图像或视频）的理解和生成任务。SPAE在原始像素和从LLM词汇表中提取的可解释的词汇标记（或单词）之间进行转换。生成的标记捕捉了视觉重建所需的语义含义和细粒度细节，将视觉内容转化为LLM能理解的语言，并使其能够执行各种多模态任务。我们的方法通过在多样化的图像理解和生成任务上，与冻结的PaLM 2和GPT 3.5进行上下文学习实验证实。在相同的设置下，我们的方法是第一个成功使冻结LLM生成图像内容，并在图像理解任务中的性能超过现有技术25%以上的尝试。

    In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
    
[^10]: 关于指令调整的可利用性

    On the Exploitability of Instruction Tuning. (arXiv:2306.17194v1 [cs.CR])

    [http://arxiv.org/abs/2306.17194](http://arxiv.org/abs/2306.17194)

    该论文研究了如何利用指令调整技术来改变模型行为的问题，并提出了一种自动数据注入的方法AutoPoison。实验结果表明，通过少量的训练数据毒化，对手能够改变模型的行为。

    

    指令调整是一种将大型语言模型与人类意图对齐的有效技术。在这项工作中，我们研究了一个对手如何通过向训练数据注入特定的指令跟随示例来利用指令调整，从而有意改变模型的行为。例如，对手可以通过注入提及目标内容的训练示例，并引诱下游模型展示此类行为来实现内容注入。为了达到这个目标，我们提出了一种自动数据注入的方法，称为AutoPoison。它使用了一个预言模型来将多样攻击目标自然而连贯地注入到毒化数据中。我们展示了两个实例攻击：内容注入和过度拒绝攻击，每个攻击都旨在诱导特定的可利用行为。我们对我们的数据注入方案的强度和隐蔽性进行了量化和基准测试。我们的结果表明，仅通过毒化少量训练数据，AutoPoison允许对手改变模型的行为。

    Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only
    
[^11]: InterCode:标准化和基准测试具有执行反馈的交互编码

    InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback. (arXiv:2306.14898v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.14898](http://arxiv.org/abs/2306.14898)

    InterCode是一个交互式编码的标准化和基准测试框架，它使用执行反馈作为观察，并提供了安全可重现的执行环境，可以用于开发新的交互式代码生成方法。

    

    人类以基本交互方式编写代码，并依赖于持续的执行反馈来纠正错误，解决歧义和分解任务。尽管最近的LLM展示出了有希望的编码能力，但目前的编码基准主要考虑静态的指令到代码序列转换过程，这可能导致错误传播和生成的代码与其最终执行环境之间的脱节。为了填补这一差距，我们引入了InterCode，这是一个轻量级、灵活且易于使用的交互式编码框架，作为一个标准强化学习（RL）环境，使用代码作为行动，执行反馈作为观察。我们的框架与语言和平台无关，使用独立的Docker环境提供安全和可重现的执行，并且与传统的seq2seq编码方法开箱即用，同时还可以开发新的交互式代码生成方法。我们使用InterCode创建...

    Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create t
    
[^12]: 通过半透过最大似然估计学习描述性图像字幕

    Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation. (arXiv:2306.13460v1 [cs.CL])

    [http://arxiv.org/abs/2306.13460](http://arxiv.org/abs/2306.13460)

    本文通过半透过最大似然估计方法，鼓励模型生成更详细的长字幕。

    

    图像字幕旨在用自然语言描述视觉内容。然而，由于最大似然估计是训练目标，字幕模型在预测与标签不匹配时会受到惩罚。本文提出了半透过最大似然估计（SMILE）方法，允许丰富性优化同时阻止简洁性优化，从而鼓励模型生成更详细的长字幕。

    Image captioning aims to describe visual content in natural language. As 'a picture is worth a thousand words', there could be various correct descriptions for an image. However, with maximum likelihood estimation as the training objective, the captioning model is penalized whenever its prediction mismatches with the label. For instance, when the model predicts a word expressing richer semantics than the label, it will be penalized and optimized to prefer more concise expressions, referred to as conciseness optimization. In contrast, predictions that are more concise than labels lead to richness optimization. Such conflicting optimization directions could eventually result in the model generating general descriptions. In this work, we introduce Semipermeable MaxImum Likelihood Estimation (SMILE), which allows richness optimization while blocking conciseness optimization, thus encouraging the model to generate longer captions with more details. Extensive experiments on two mainstream im
    
[^13]: SituatedGen: 将地理和时间背景融入生成式常识推理

    SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning. (arXiv:2306.12552v1 [cs.CL])

    [http://arxiv.org/abs/2306.12552](http://arxiv.org/abs/2306.12552)

    本文介绍了一种具有挑战性的任务SituatedGen，要求具有常识推理能力的机器生成一对对比句子，以融入地理和时间背景。作者提出了一种相应的英语数据集，并发现目前的生成式语言模型仍然难以实现具有常识合理性的句子的生成，远远落后于人类表现。

    

    最近，文本生成中的常识推理引起了广泛关注。生成式常识推理是一项任务，要求机器在给定一组关键词的情况下，用常识合理性组合出一句连贯的句子。虽然现有的针对生成式常识推理的数据集依focus everyday scenarios，但是机器在特定的地理和时间背景下理解的能力尚不清楚。我们将这一具有挑战性的任务形式化为SituatedGen，要求具有常识推理能力的机器生成一对对比句子，给定的关键词包括地理或时间实体。我们引入一份相应的英语数据集，其中包含8,268对对比句子，这些句子建立在现有的几个常识推理基准上，人工工作量最小。实验表明，最先进的生成式语言模型难以生成具有常识合理性的句子，并且仍然远远落后于人类表现。

    Recently, commonsense reasoning in text generation has attracted much attention. Generative commonsense reasoning is the task that requires machines, given a group of keywords, to compose a single coherent sentence with commonsense plausibility. While existing datasets targeting generative commonsense reasoning focus on everyday scenarios, it is unclear how well machines reason under specific geographical and temporal contexts. We formalize this challenging task as SituatedGen, where machines with commonsense should generate a pair of contrastive sentences given a group of keywords including geographical or temporal entities. We introduce a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing commonsense reasoning benchmarks with minimal manual labor. Experiments show that state-of-the-art generative language models struggle to generate sentences with commonsense plausibility and still lag far behind human performance. Our 
    
[^14]: Quilt-1M: 癌症组织学图像文字对的百万数据集

    Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.11207](http://arxiv.org/abs/2306.11207)

    本文介绍了一个名为 Quilt-1M 的癌症组织学图像和文字对的百万数据集，并利用 YouTube 上的专家医生教程视频为主要来源。这个数据集将使得癌症组织学领域的表征学习取得类似于其他领域的进展。

    

    多模态应用的加速使得在线图像和文字数据大量涌现，但医学领域（特别是癌症组织学）类似的数据却很稀少，这阻碍了医学领域的进展。本文利用YouTube上的专家医生教程视频，从中选择了 1,087 小时的医学组织学视频，以此自动筛选出共包含 768,826 个癌症组织学图像及其对应的文字对的 Quilt 数据集。

    Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has halted comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of $768,826$ image and text pairs. Quilt was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine Quilt with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even l
    
[^15]: 高效测量语言模型的认知能力：自适应测试视角

    Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective. (arXiv:2306.10512v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.10512](http://arxiv.org/abs/2306.10512)

    本研究提出了一种自适应测试框架，用于高效测量语言模型的认知能力。通过动态调整测试问题的特性，能够更准确地评估模型的能力，并使用更少的问题。同时，该框架使得语言模型能够与人类进行轻松比较。

    

    大型语言模型（LLMs），如ChatGPT，展现了一些类似于人类的认知能力。为了比较不同模型的这些能力，通常采用来自不同领域（如文学、生物学和心理学）的多个基准（即标准测试问题集），并报告传统度量指标（如准确率、召回率和F1）。然而，从认知科学的角度来看，这种评估LLMs的方法可能效率低下且不准确。受心理测量学中计算机自适应测试（CAT）的启发，我们提出了一种适用于LLM评估的自适应测试框架。该方法根据模型的表现动态调整测试问题的特性（如难度），而不是使用标准的测试集并简单报告准确率。这使得能更准确地估计模型的能力，并使用更少的问题。更重要的是，它使LLMs能够与人类进行轻松比较，这是至关重要的。

    Large language models (LLMs), like ChatGPT, have shown some human-like cognitive abilities. For comparing these abilities of different models, several benchmarks (i.e. sets of standard test questions) from different fields (e.g., Literature, Biology and Psychology) are often adopted and the test results under traditional metrics such as accuracy, recall and F1, are reported. However, such way for evaluating LLMs can be inefficient and inaccurate from the cognitive science perspective. Inspired by Computerized Adaptive Testing (CAT) used in psychometrics, we propose an adaptive testing framework for LLM evaluation. Rather than using a standard test set and simply reporting accuracy, this approach dynamically adjusts the characteristics of the test questions, such as difficulty, based on the model's performance. This allows for a more accurate estimation of the model's abilities, using fewer questions. More importantly, it allows LLMs to be compared with humans easily, which is essential
    
[^16]: 块状态变换器

    Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])

    [http://arxiv.org/abs/2306.09539](http://arxiv.org/abs/2306.09539)

    本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。

    

    状态空间模型（SSM）在需要建模长期依赖性并且需要高效扩展到长序列的任务中显示出了惊人的效果。尽管最初是为连续信号设计的，但SSM在视觉和音频等许多任务中表现出了优异的性能；然而，在语言建模任务中，SSM仍然落后于Transformers的性能。在这项工作中，我们提出了一个名为块状态变换器（BST）的混合层，它在内部组合了一个用于长距离上下文化的SSM子层和一个用于短期序列表示的块变换器子层。我们研究了三种不同的、完全可并行的集成SSM和块注意力的变体。我们证明了我们的模型在语言建模的困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，块状态变换器在层级别上具有超过十倍的速度提升。

    State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
    
[^17]: 大型语言模型是半参数强化学习智能体

    Large Language Model Is Semi-Parametric Reinforcement Learning Agent. (arXiv:2306.07929v1 [cs.CL])

    [http://arxiv.org/abs/2306.07929](http://arxiv.org/abs/2306.07929)

    根据人类记忆和推理机制，提出了一种新的可演化LLM智能体框架REMEMBERER，通过为LLM装备长期经验记忆，可以为不同任务提供优异的智能体，其构成了半参数RL代理。成功率超过先前SOTA 4％和2％。

    

    受认知科学对人类记忆和推理机制的启发，提出了一种新的可演化LLM（大型语言模型）智能体框架REMEMBERER。通过为LLM装备长期经验记忆，REMEMBERER能够利用过去剧集的经验，甚至可以为不同的任务目标提供优异的LLM智能体，这优于具有固定实例或具有短暂工作记忆的LLM智能体。我们进一步介绍了经验记忆的强化学习（RLEM）来更新记忆。因此，整个系统可以从成功和失败的经验中学习，并在不微调LLM参数的情况下发展其能力。以此方式，所提出的REMEMBERER构成了半参数RL代理。在两个RL任务集上进行了大量实验以评估所提出的框架。不同初始化和训练集的平均结果对于成功率超过先前SOTA 4％和2％。

    Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate 
    
[^18]: 分解对比学习：超越多视角冗余

    Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])

    [http://arxiv.org/abs/2306.05268](http://arxiv.org/abs/2306.05268)

    本论文提出了FactorCL，一种新的多模态表示学习方法，不仅考虑跨模态共享信息，还能捕捉跨模态唯一的任务相关信息。

    

    在广泛的多模态任务中，对比学习已成为一种特别吸引人的方法，因为它可以成功地学习具有丰富未标记数据的表示，只需配对信息（例如，图像标题或视频音频对）。这些方法的基础是多视角冗余的假设——跨模态间共享信息对于下游任务是必要且足够的。然而，在许多现实世界的情况下，任务相关信息也包含在跨模态唯一区域中：一种仅存在于一个模态中但与任务仍然相关的信息。如何学习自我监督的多模态表示以捕获与下游任务相关的共享和唯一信息？本文提出了一种新的多模态表示学习方法FactorCL，以超越多视角冗余。FactorCL的基础是三个新的贡献：（1）将任务相关信息分解为共享和唯一表示，（2）限制共享和唯一成分之间的交互，（3）使用因子正则化促进表示学习。

    In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) cap
    
[^19]: LEACE：闭合形式中的完美线性概念擦除

    LEACE: Perfect linear concept erasure in closed form. (arXiv:2306.03819v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03819](http://arxiv.org/abs/2306.03819)

    本文介绍了一种闭合形式的方法LEACE，可在删除指定特征的同时尽可能少地改变表示，并可证明防止所有线性分类器检测到概念。作者用“概念擦除”这一新方法将其应用于大型语言模型，在测量语言模型对词性的依赖性和减少BERT嵌入中的性别偏差任务中得出良好表现。

    

    概念擦除旨在从表征中删除指定的特征。它可以提高公平性（例如，防止分类器使用性别或种族）和可解释性（例如，删除概念以观察模型行为的变化）。我们引入了LEAst-squares概念擦除（LEACE），这是一种闭合形式的方法，可证明防止所有线性分类器检测到概念，同时尽可能地改变表示，如广泛类别的范数所测量的那样。我们使用名为“概念擦除”的新方法将LEACE应用于大型语言模型，擦除每个层中的目标概念信息。我们在两个任务上展示了我们的方法：测量语言模型对词性信息的依赖性，以及减少BERT嵌入中的性别偏差。代码可在https://github.com/EleutherAI/concept-erasure上找到。

    Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called "concept scrubbing," which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.
    
[^20]: 跨语言评估情感曲线：弥合情感分析中的全球差异

    Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis. (arXiv:2306.02213v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02213](http://arxiv.org/abs/2306.02213)

    本研究首次对自动生成的情感曲线进行了系统和定量的评估，并比较了机器学习模型和词典方法两种生成情感曲线的方式。通过在不同语言的多个数据集上进行实验，我们发现虽然词典方法在实例级情感分类方面表现较差，但在聚合信息时生成情感曲线的准确性非常高。此外，我们还展示了通过自动翻译英语情感词典，可以在资源较少的情况下生成高质量的情感曲线。

    

    情感曲线捕捉了一个人（或一个群体）随时间变化的情感状态。它们被广泛应用于工业和研究领域；然而，对于自动生成的情感曲线的评估工作很少。这是因为建立真实（黄金）情感曲线的困难。我们的工作首次对自动生成的情感曲线进行了系统和定量的评估。我们还比较了两种常见的情感曲线生成方法：机器学习（ML）模型和仅词典（LexO）方法。通过在9种语言的18个不同数据集上进行实验，我们表明尽管在实例级情感分类方面表现差，但LexO方法在从数百个实例中聚合信息时生成情感曲线的准确性非常高。我们还通过对六种非洲土著语言以及阿拉伯语和西班牙语的实验表明，英语情感词典的自动翻译可以用于生成高质量的情感曲线，而资源开销相对较小。

    Emotion arcs capture how an individual (or a population) feels over time. They are widely used in industry and research; however, there is little work on evaluating the automatically generated arcs. This is because of the difficulty of establishing the true (gold) emotion arc. Our work, for the first time, systematically and quantitatively evaluates automatically generated emotion arcs. We also compare two common ways of generating emotion arcs: Machine-Learning (ML) models and Lexicon-Only (LexO) methods. By running experiments on 18 diverse datasets in 9 languages, we show that despite being markedly poor at instance level emotion classification, LexO methods are highly accurate at generating emotion arcs when aggregating information from hundreds of instances. We also show, through experiments on six indigenous African languages, as well as Arabic, and Spanish, that automatic translations of English emotion lexicons can be used to generate high-quality emotion arcs in less-resource 
    
[^21]: 精细化的人类反馈可以提供更好的语言模型训练奖励

    Fine-Grained Human Feedback Gives Better Rewards for Language Model Training. (arXiv:2306.01693v1 [cs.CL])

    [http://arxiv.org/abs/2306.01693](http://arxiv.org/abs/2306.01693)

    本文提出了Fine-Grained RLHF框架，使用精细化的人类反馈作为明确的训练信号来训练和学习语言模型。该框架提供了多个细致的奖励模型来获得更好的效果。

    

    语言模型经常表现出不良的文本生成行为，包括生成虚假、有害或无关的输出。最近，从人类反馈中进行强化学习（RLHF）-其中人类对LM输出的偏好评价被转化为学习信号-已经显示出解决这些问题的潜力。然而，这种整体反馈对长文本输出传达的信息有限；它不表明输出的哪些方面影响了用户的偏好；例如，哪些部分包含什么类型的错误。本文中，我们使用精细化的人类反馈（例如，哪个句子是错误的，哪个子句是无关的）作为明确的训练信号。我们介绍了Fine-Grained RLHF，这是一个能够训练和学习与不同反馈类型相关的多个奖励模型的精细化奖励功能的框架，具有以下两个特征：（1）密度，以在生成每个段落（例如一个句子）后提供奖励； （2）并入不同反馈类型的多个奖励模型。

    Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.
    
[^22]: 揭示Attention故障的翻转-翻转语言建模

    Exposing Attention Glitches with Flip-Flop Language Modeling. (arXiv:2306.00946v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00946](http://arxiv.org/abs/2306.00946)

    本论文揭示了语言模型中注意力故障的现象，并通过引入翻转-翻转语言建模来分析这个问题。研究发现，Transformer FFLMs经常出现推理错误。

    

    为什么大型语言模型有时会输出事实错误并表现出错误的推理？这些模型的脆弱性，特别是在执行长链推理时，目前似乎是为了它们能够精确地综合知识、语用和抽象思维而必须付出的代价。为了理解这个根本未解决的问题，本研究确定并分析了注意力故障现象，其中Transformer架构的归纳性偏见间歇性地未能捕捉到稳健的推理。为了隔离这个问题，我们引入了翻转-翻转语言建模（FFLM），这是一组参数化合成基准，旨在探索神经语言模型的外推行为。这个简单的生成任务要求模型在长程依赖关系中复制二进制符号，忽略中间的标记。我们发现Transformer FFLMs在推理错误方面存在着长尾现象，其中一些我们可以

    Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we ca
    
[^23]: 扩散画笔：基于潜在扩散模型的AI生成图像编辑工具

    Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])

    [http://arxiv.org/abs/2306.00219](http://arxiv.org/abs/2306.00219)

    本论文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地根据目标区域修改AI合成图像并保留原始上下文。与其他最先进的图像修复技术进行比较，该方法在用户研究中表现出更好的可用性和有效性。

    

    文本到图像的生成模型在生成高质量图像方面取得了显著的进展。然而，由于模型限制，生成的图像经常包含不良的伪影或其他错误。现有的微调生成图像的技术要么耗时（手动编辑），要么产生不够完美的结果（修补），要么会导致整体图像产生意想不到的变化（变体选择和提示微调）。本文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地微调AI合成图像中所需的区域。我们的方法在反向扩散过程中在目标区域引入了新的随机噪声模式，使模型能够在保留其他区域原始上下文的同时，高效地对指定区域进行更改。我们通过艺术家进行的用户研究评估了我们方法的可用性和有效性，将我们的技术与其他最先进的图像修复技术进行了比较。

    Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpaintin
    
[^24]: 改进CLIP训练的语言重写方法

    Improving CLIP Training with Language Rewrites. (arXiv:2305.20088v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.20088](http://arxiv.org/abs/2305.20088)

    本文介绍了一种名为Language augmented CLIP（LaCLIP）的方法，通过语言重写来增强CLIP训练。利用大型语言模型的能力，重新书写与每个图像关联的文本描述，以增加多样性，同时保留原始的关键概念和意义。

    

    对比语言-图像预训练（CLIP）是使用成对的图像和文本数据进行训练可转移视觉模型的最有效和可扩展的方法之一。CLIP模型使用对比损失进行训练，通常依赖于数据增强来防止过拟合和捷径问题。然而，在CLIP训练范式中，数据增强仅应用于图像输入，而语言输入在整个训练过程中保持不变，限制了多样文本对相同图像的暴露。本文介绍了Language augmented CLIP（LaCLIP），一种简单而高效的方法，通过语言重写来增强CLIP训练。利用大型语言模型的上下文学习能力，我们重新书写与每个图像关联的文本描述。这些重新书写的文本在句子结构和词汇方面呈现多样性，同时保留了原始关键概念和意义。在训练过程中，LaCLIP随机地。

    Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly s
    
[^25]: SheetCopilot: 通过大型语言模型将软件生产力提升到新的水平

    SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models. (arXiv:2305.19308v1 [cs.SE])

    [http://arxiv.org/abs/2305.19308](http://arxiv.org/abs/2305.19308)

    本研究提出了一个基于大型语言模型（LLMs）的代理程序SheetCopilot，该程序可以通过自然语言指导软件执行电子表格数据处理等任务。该程序设计了一组抽象的电子表格软件功能原子动作以及基于状态机的任务规划框架，实现了LLMs与电子表格的鲁棒交互，可以单次正确完成44.3％的任务。

    

    计算机终端用户花费了数十亿小时完成诸如表格数据处理和项目时间轴调度等日常任务。这些任务大多是重复性的和容易出错的，然而大多数终端用户缺乏自动化这些繁琐工作的技能。随着大型语言模型（LLMs）的出现，用自然语言用户请求指导软件成为了一个可达成的目标。在本研究中，我们提出了一个SheetCopilot代理，该代理接受自然语言任务和控制电子表格以满足要求。我们提出了一组原子动作作为电子表格软件功能的抽象。我们进一步设计了基于状态机的任务规划框架，以便LLMs与电子表格进行鲁棒的交互。我们策划了一个包含221种电子表格控制任务的代表性数据集，并建立了一个完全自动化的评估管道，以严格评估LLMs在软件控制任务中的能力。我们的SheetCopilot单次正确完成44.3％的任务。

    Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill of automating away these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent which takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single 
    
[^26]: LANCE：通过生成语言引导的对抗性图像对视觉模型进行压力测试

    LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images. (arXiv:2305.19164v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.19164](http://arxiv.org/abs/2305.19164)

    本文提出了一种自动化算法LANCE，通过生成语言引导的对抗性测试图像来压力测试视觉模型，实现了在不改变模型权重的情况下增加多样、逼真且具有挑战性的测试图像。通过对多种预训练模型的性能进行基准测试，发现模型性能显著下降，并且通过分析模型对不同类型编辑的敏感性，揭示了ImageNet中先前未知的类别层次模型偏差。

    

    我们提出了一种自动化算法，通过生成语言引导的对抗性测试图像（LANCE）来对训练过的视觉模型进行压力测试。我们的方法借鉴了最近语言建模和基于文本编辑的图像处理的进展，在不改变模型权重的情况下，用一套多样，逼真且具有挑战性的测试图像增加了一个IID测试集合。我们在我们生成的数据上对多种预训练模型的性能进行了基准测试，并观察到了显著而一致的性能下降。我们进一步分析了模型对不同类型编辑的敏感性，并展示了其在揭示ImageNet中先前未知的类别层次模型偏差方面的适用性。代码可在https://github.com/virajprabhu/lance找到。

    We propose an automated algorithm to stress-test a trained visual model by generating language-guided counterfactual test images (LANCE). Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights. We benchmark the performance of a diverse set of pre-trained models on our generated data and observe significant and consistent performance drops. We further analyze model sensitivity across different types of edits, and demonstrate its applicability at surfacing previously unknown class-level model biases in ImageNet. Code is available at https://github.com/virajprabhu/lance.
    
[^27]: 知识增强的推理蒸馏：面向知识密集型任务的小型语言模型

    Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])

    [http://arxiv.org/abs/2305.18395](http://arxiv.org/abs/2305.18395)

    本论文提出了一种KARD方法，可以通过向小型语言模型中加入从外部知识库检索到的增强知识来解决知识密集型推理任务中小型语言模型记忆能力有限的问题。

    

    大型语言模型在需要复合知识理解的知识密集型推理任务中表现出了良好的性能。但是，由于计算要求高且涉及数据隐私，将此类模型部署到现实世界的应用中可能会具有挑战性。以往的研究专注于通过微调具有标记数据或蒸馏大型语言模型来构建任务特定的小型语言模型，但是由于小型语言模型在记忆所需知识方面的能力有限，这些方法不适用于知识密集型推理任务。在理论分析的基础上，我们提出了一种名为知识增强的推理蒸馏 (KARD) 的新方法，该方法微调小型语言模型以生成从外部知识库检索到的增强知识的依据。此外，我们还提出了一个神经重排器，用于获得与依据生成相关的文档。我们实证表明，KARD在三项知识密集型任务上显着优于以前的方法，并且在模型尺寸相同的情况下可以达到与LLMs可比较的结果。

    Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small language models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantl
    
[^28]: 预训练Transformers中的自发模块化

    Emergent Modularity in Pre-trained Transformers. (arXiv:2305.18390v1 [cs.CL])

    [http://arxiv.org/abs/2305.18390](http://arxiv.org/abs/2305.18390)

    本论文研究了预训练Transformers中的自发模块化现象，发现神经元可以进行功能专业化，并通过聚类建立起模块化结构，此结构可被有效扰动。

    

    本论文研究了预训练Transformers中的模块化特征，这是人脑中常见的特点，被认为对于普遍智能至关重要。本文主要考虑了模块化的两个主要特征：（1）神经元的功能专业化：我们评估了每个神经元是否主要专业化于某一功能，结果表明是的。（2）基于功能聚类的神经元分组：我们探究了将神经元按功能分组的结构寻找方法，每个模块均为其相应功能工作。鉴于可能存在的大量结构，我们将重点放在了分层专家模型身上，并将神经元划分为专家，通常为不同的输入激活不同的专家。实验结果表明存在功能专家，聚集了某一功能的神经元。此外，扰动功能专家的激活显著影响了相应的f键

    This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore finding a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding f
    
[^29]: 诊断变压器：揭示临床决策中的特征空间。 (arXiv:2305.17588v2 [cs.CL] UPDATED)

    Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17588](http://arxiv.org/abs/2305.17588)

    该论文介绍了一种名为SUFO的框架，用于增强微调的变压器模型的可解释性。该框架利用多种分析和可视化技术，解决了模型信任和可解释性的关键问题，并通过案例研究验证了其有效性。

    

    在医学等高风险领域，为了建立信任和确保安全，模型的可解释性至关重要，而使用有限的临床记录对预训练的变压器进行微调以辅助临床决策。我们引入了一种名为SUFO的系统框架，该框架增强了微调的变压器特征空间的可解释性。SUFO利用一系列分析和可视化技术，包括监督探索、无监督相似性分析、特征动态和异常值分析，来解决关于模型信任和可解释性的关键问题。我们进行了一个案例研究，研究了预训练数据对真实世界病理分类任务的影响，并在MedNLI上验证了我们的发现。我们评估了五个110M规模的预训练变压器模型，分为通用领域（BERT, TNLR）、混合领域（BioBERT, Clinical BioBERT）和领域特定（PubMedBERT）组。我们的SUFO分析揭示了：(1)

    Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1
    
[^30]: 在模拟人类社会中训练社会对齐的语言模型

    Training Socially Aligned Language Models in Simulated Human Society. (arXiv:2305.16960v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16960](http://arxiv.org/abs/2305.16960)

    本研究提出了一种在模拟人类社会中训练语言模型的新方法，相比于现有方法，该方法具有更大的可扩展性和高效性，并在对齐基准和人类评估中展示出更优异的性能。

    

    AI系统中的社会对齐旨在确保这些模型按照既定的社会价值行事。然而，与人类不同，人们通过社交互动得出对价值判断的共识，当前的语言模型（LMs）则在孤立地复制其训练语料库时被训练出来，导致在陌生场景中表现不佳，并易受到对抗攻击。本研究提出了一种新的训练范式，允许LMs从模拟的社交互动中学习。与现有方法相比，我们的方法具有更大的可扩展性和高效性，在对齐基准和人类评估中展示出更优异的性能。这种LMs训练中的范式转变使我们离开发能够强有力且准确反映社会规范和价值的AI系统更近了一步。

    Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.
    
[^31]: 评估大型视觉语言模型的对抗鲁棒性

    On Evaluating Adversarial Robustness of Large Vision-Language Models. (arXiv:2305.16934v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16934](http://arxiv.org/abs/2305.16934)

    该论文提出在最真实和高风险的情境中评估大型视觉语言模型的对抗鲁棒性。作者首先构建有针对性的对抗样本，然后将其转移到其他模型中进行评估，并观察到黑盒查询可以改进效果。

    

    大型视觉语言模型（VLMs）如GPT-4在生成响应方面取得了前所未有的性能，尤其是在视觉输入方面，使得交互更有创造力和适应性，而不仅仅是大型语言模型如ChatGPT。然而，多模态生成加剧了安全性问题，因为对手可以通过微妙地操纵最易受攻击的模态（例如视觉）成功避开整个系统。为此，我们提出在最真实和高风险的情境下评估开源大型VLMs的鲁棒性，其中对手只能黑盒访问系统，并试图欺骗模型返回目标响应。具体而言，我们首先针对预训练模型（如CLIP和BLIP）构建有针对性的对抗样本，然后将这些对抗样本转移到其他VLMs（如MiniGPT-4、LLaVA、UniDiffuser、BLIP-2和Img2Prompt）。此外，我们观察到，在这些VLMs上进行黑盒查询可以进一步提高效果。

    Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness
    
[^32]: 扫描与拍照：理解1层Transformer中的训练动态和标记组成

    Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])

    [http://arxiv.org/abs/2305.16380](http://arxiv.org/abs/2305.16380)

    本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。

    

    Transformer架构在多个研究领域表现出了惊人的性能，并成为许多神经网络模型的基础。然而，我们对其如何工作的理解仍然有限。特别是，通过简单的预测性损失，表示如何从梯度训练动态中出现仍然是一个谜。在本文中，针对具有一个自我关注层和一个解码器层的1层Transformer，我们以数学严谨的方式分析其在下一个标记预测任务中的SGD训练动态。我们打开了自我关注层组合输入标记的动态过程的黑盒子，并揭示了底层归纳偏差的本质。具体而言，在没有位置编码、长输入序列和解码器层学习速度快于自我关注层的假设下，我们证明了自我关注层充当了“区分性扫描算法”：从均匀注意力开始，它逐渐关注到相关标记，排除不相关的标记，直到所有相关信息被扫描并总结在编码表示中。我们的分析还显示了标记频率和上下文如何影响注意权重，以及自我关注层初始化如何影响收敛速度。

    Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
    
[^33]: 迈向可靠的假新闻缓解：泛化，不确定性和GPT-4

    Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4. (arXiv:2305.14928v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14928](http://arxiv.org/abs/2305.14928)

    本研究提出利用泛化、不确定性和最新的大型语言模型，寻求解决假新闻问题。实验证明GPT-4在多语言环境下表现优于之前的方法。研究还探讨了泛化和不确定性处理技术，并在其他语言模型、温度、提示、版本控制、可解释性和网络检索方面取得了实际见解和未来研究方向。此外，研究还发布了新颖的英法配对假新闻数据集LIAR-New，为信息真实性评估提供了可行性标签。

    

    假新闻构成了一个重要的社会挑战，目前的方法尚未找到有效的解决方案。我们提出关注泛化，不确定性以及如何利用最新的大型语言模型，以便在无法完美分类的情况下创建更实用的工具来评估信息真实性。我们首先证明了GPT-4在多个设定和语言中可以胜过之前的方法。接下来，我们探索泛化，揭示了GPT-4和RoBERTa-large在失效模式上的差异。第三，我们提出了处理不确定性的技术，可以检测到不可能的例子并显著改进结果。我们还讨论了其他语言模型，温度，提示，版本控制，可解释性和网络检索的结果，每个结果都提供了实际的见解和未来研究的方向。最后，我们发布了具有新颖的英法配对假新闻数据和可行性标签的LIAR-New数据集。

    Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indic
    
[^34]: 文本编码器限制了对比视觉-语言模型的组合性能

    Text encoders bottleneck compositionality in contrastive vision-language models. (arXiv:2305.14897v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14897](http://arxiv.org/abs/2305.14897)

    本研究发现，在对比视觉-语言模型中，使用单个向量表示标语的文本编码器在处理更加复杂的输入时表现不佳，但某些文本编码器的性能明显优于其他编码器。仅基于文本的恢复性能能够预测多模态匹配性能。

    

    高性能的视觉-语言模型（VL）如CLIP使用单一向量表示标题。在这个瓶颈中失去了多少关于语言的信息？我们首先策划了CompPrompts，这是一组越来越复杂的图像标题，VL模型应该能够捕捉到（例如，单个对象，到对象+属性，到多个互动对象）。然后，我们训练了仅基于文本的恢复探针，旨在从几个VL模型生成的单一向量文本表示中重建标题。这种方法不需要图像，相对于之前的工作，使我们能够在更广泛的场景上进行测试。我们发现：1）CLIP的文本编码器在更复杂的输入上表现不佳，包括对象关系、属性-对象关联、计数和否定；2）一些文本编码器比其他编码器要好得多；3）仅基于文本的恢复性能预测了ControlledImCaps上的多模态匹配性能：这是我们收集和发布的一个新的评估基准。

    Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models. This approach does not require images, allowing us to test on a broader range of scenes compared to prior work. We find that: 1) CLIP's text encoder falls short on more compositional inputs, including object relationships, attribute-object association, counting, and negations; 2) some text encoders work significantly better than others; and 3) text-only recovery performance predicts multi-modal matching performance on ControlledImCaps: a new evaluation benchmark we collect and release consisting of
    
[^35]: MQuAKE：通过多跳问题评估语言模型中的知识编辑

    MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions. (arXiv:2305.14795v1 [cs.CL])

    [http://arxiv.org/abs/2305.14795](http://arxiv.org/abs/2305.14795)

    本文提出了一种基准测试MQuAKE，通过多跳问题评估编辑模型是否能够正确回答因编辑事实而答案应该改变的问题。研究发现当前的知识编辑方法可以准确召回已编辑的事实，但在多跳问题上表现灾难性失败。

    

    大型语言模型（LLM）中存储的信息很快就会过时，重新训练并非总是可行的选择。这促使人们开发了通过更新模型权重注入新事实的一系列技术。当前的评估方法非常有限，主要验证编辑事实的召回率，但更改一个事实应该会对模型的相关信念产生连锁反应。如果我们编辑英国首相为Rishi Sunak，那么对于“谁是英国首相的配偶”这个问题，我们应该得到一个不同的答案。在这项工作中，我们提出了一个基准MQuAKE（用于知识编辑的多跳问答），包括多跳问题，评估编辑后的模型是否正确回答那些因编辑事实而答案应该改变的问题。虽然我们发现当前的知识编辑方法可以准确召回已编辑的事实，但它们在构建的多跳问题上遭遇了灾难性失败。因此，我们建议对LLMs的评估必须超越简单的事实召回，并纳入更微妙的知识编辑质量评估。

    The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus 
    
[^36]: 一种可控的基于问答的去文本化框架

    A Controllable QA-based Framework for Decontextualization. (arXiv:2305.14772v1 [cs.CL])

    [http://arxiv.org/abs/2305.14772](http://arxiv.org/abs/2305.14772)

    本文提出了一个基于问答的去文本化框架，可以更好地展示提取的文本摘录。在问答和引证上的表现类似于端到端方法，并且支持用户信息需求及偏好的可控性。

    

    许多真实场景下的应用需要将提取的摘录展示给用户，这些摘录往往需要解耦原来的文本才能更好地呈现给用户。本文研究了LLMs在问答和引证上的去文本化能力，并提出了一个基于问答的去文本化框架，该框架可以更好地满足用户信息需求及偏好，并且在结果上表现出类似于端到端方法的竞争力。我们同时探讨了如何通过该框架将用户偏好融入到系统中，从而实现了可控性。

    Many real-world applications require surfacing extracted snippets to users, whether motivated by assistive tools for literature surveys or document cross-referencing, or needs to mitigate and recover from model generated inaccuracies., Yet, these passages can be difficult to consume when divorced from their original document context. In this work, we explore the limits of LLMs to perform decontextualization of document snippets in user-facing scenarios, focusing on two real-world settings - question answering and citation context previews for scientific documents. We propose a question-answering framework for decontextualization that allows for better handling of user information needs and preferences when determining the scope of rewriting. We present results showing state-of-the-art LLMs under our framework remain competitive with end-to-end approaches. We also explore incorporating user preferences into the system, finding our framework allows for controllability.
    
[^37]: 通过GPT-4分析影响人类偏好判断的因素

    Analyzing Influential Factors in Human Preference Judgments via GPT-4. (arXiv:2305.14702v1 [cs.CL])

    [http://arxiv.org/abs/2305.14702](http://arxiv.org/abs/2305.14702)

    本文利用Bradley-Terry-Luce模型对OpenAI公布的人类偏好判断数据集进行了深入研究，揭示了人类偏好判断中所蕴含的固有偏好，提出了提高样本效率的策略，并为构建平衡的人类偏好判断数据集提供了洞见。

    

    人类偏好判断在引导大型语言模型生成符合人类偏好的输出和评估自动摘要度量方面具有至关重要的作用。然而，对于这些偏好判断的共同影响和因素的相对重要性等问题，目前的研究仍较为有限。本文利用Bradley-Terry-Luce模型对OpenAI公布的人类偏好判断数据集进行了深入研究，识别了可能影响人类偏好判断的关键因素。研究结果揭示了人类偏好判断中所蕴含的固有偏好，并提出了提高样本效率的策略，最后对于如何构建平衡的人类偏好判断数据集提供了洞见。

    Pairwise human judgments are pivotal in guiding large language models (LLMs) to generate outputs that align with human preferences. They are also often used in summarization evaluation, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise human judgments. The collective impact and respective weights of factors such as informativeness, coherence, fluency, and factual consistency remain elusive. The impact of hidden factors on the final judgment is also unclear. In this paper, we conduct an in-depth examination of a dataset of pairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce model, we identify key factors that could potentially influence human judgments. Our research uncovers the inherent preferences embedded in human judgments and suggests strategies to boost sample efficiency. Finally, we provide insights on the construction of balanced datasets for human judgment evaluations, 
    
[^38]: ALGO：使用生成的神谕验证程序的合成算法程序

    ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers. (arXiv:2305.14591v1 [cs.CL])

    [http://arxiv.org/abs/2305.14591](http://arxiv.org/abs/2305.14591)

    ALGO框架使用由LLM生成的神谕指导创造和验证算法程序，以提高现有代码生成模型的算法问题解决能力。

    

    大型语言模型(Large language models, LLMs)在实现代码的功能描述方面表现出色，但在需要确定适当算法的算法问题上亟需提升。此外，LLM生成的程序缺乏保证正确性并需要人工验证。为了解决这些挑战，我们提出了ALGO框架，该框架使用由LLM生成的神谕指导创造和验证算法程序。ALGO首先通过促使LLM枚举相关变量的所有组合来生成具有可能的正确性但可能较慢的参考神谕。然后，利用该神谕指导任意搜索策略来探索算法空间并验证合成的算法。我们的研究表明，LLM生成的神谕在88%的情况下是正确的。使用这些神谕作为验证程序，ALGO可以以模型无关的方式与任何现有的代码生成模型集成，以提高其算法问题解决能力。

    Large language models (LLMs) excel at implementing code from functionality descriptions, but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification. To address these challenges, we propose ALGO, a framework that synthesizes Algorithmic programs with LLM-Generated Oracles to guide the creation and verify their correctness. ALGO first generates a probably correct but possibly slow reference oracle by prompting an LLM to exhaustively enumerate all the combinations of relevant variables. This oracle is then utilized to guide an arbitrary search strategy in exploring the algorithm space and to verify the algorithms synthesized. Our study shows that the LLM-generated oracles are correct for 88% of the cases. With the oracles as verifiers, ALGO can be integrated with any existing code generation model in a model-agnostic manner to enha
    
[^39]: WikiChat: 通过对维基百科的少样本引入，阻止大型语言模型聊天机器人的幻觉

    WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia. (arXiv:2305.14292v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14292](http://arxiv.org/abs/2305.14292)

    WikiChat是一种以少样本为基础的语言模型聊天机器人，通过对维基百科进行 grounding，它几乎不会产生幻觉，具有高对话能力和低延迟。WikiChat从LLM中生成响应，保留基于事实的内容，并从语料库中检索到的信息结合，形成真实和引人入胜的回复。经过评估，它比其他模型表现更好，并且能在模拟对话中达到97.3%的事实准确性。

    

    本文提出了第一个几乎不会产生幻觉、具有高对话能力和低延迟的基于少样本的LLM聊天机器人——WikiChat。WikiChat基于英文维基百科进行 grounding，这是最大的精选文本语料库。WikiChat从LLM中生成响应，仅保留基于事实的内容，并与从语料库中检索到的附加信息结合，形成真实和引人入胜的回复。我们将 WikiChat 根据 GPT-4 进行了蒸馏，生成了一个参数为7B的 LLaMA 模型，以极少质量损失显著提高了其延迟、成本和隐私性，并促进了研究和部署。通过一种新颖的混合人工和LLM评估方法，我们展示了我们的最佳系统在模拟对话中达到了97.3%的事实准确性。与所有基于检索和LLM的基准相比，它在头部、尾部和最新知识方面分别提高了3.9%、38.6%和51.0%，与GPT-4相比。与之前最先进的基于检索的聊天机器人相比，WikiChat也取得了显著的...

    This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus.  WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment.  Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also signifi
    
[^40]: 分层提示提升大规模语言模型在网络导航中的应用

    Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14257](http://arxiv.org/abs/2305.14257)

    这项研究提出了一种分层提示方法来解决大规模语言模型在处理复杂观察的交互决策任务中的困难。研究表明该方法在网络导航中的效果优于先前最先进的提示机制，具有广泛的适用性。

    

    大规模语言模型（LLMs）在处理交互决策任务中的复杂观察时遇到困难。为了解决这个问题，我们提出了一种简单的分层提示方法。不同于以往总是把\emph{完整}观察（例如网页）放到提示中的提示方法，我们提出首先构建一个与动作相关的\emph{压缩}和\emph{相关}的观察，并使用专门的\summ提示。然后，\actor提示根据总结的观察预测下一个动作。尽管我们的方法具有广泛的适用性，但我们尤其展示了它在复杂的网络导航领域的有效性，其中完整的观察通常包含冗余和无关信息。我们的方法在任务成功率上优于先前最先进的提示机制6.2\%，展示了其在具有长时间观察轨迹的交互决策任务中的潜力。

    Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the \emph{full} observation~(\eg a web page) to the prompt, we propose to first construct an action-aware observation which is more \emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actor prompt then predicts the next action based on the summarized observation. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanis by 6.2\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.
    
[^41]: 具有合理性的语言模型

    Language Models with Rationality. (arXiv:2305.14250v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14250](http://arxiv.org/abs/2305.14250)

    本研究提出了一种名为REFLEX的方法，在大型语言模型上添加了合理性和自反性层，以使模型的答案得以解释并消除潜在的矛盾。

    

    虽然大型语言模型(Large Language Models, LLMs)在问答中非常擅长，但它们的答案与其内在的“信念”之间的关系往往不明确。这种缺乏解释性阻碍了LLMs的广泛使用。为了解决这个问题，我们的目标是使模型的信念以及它们的推理关系变得明确，并消除可能存在的矛盾，以便答案能够通过从一致的信念网络中得出的可解释的推理链来支持。我们的方法名为REFLEX，在LLM之上添加了一个具有合理性和自反性的层。首先，给定一个问题，我们使用反向链接过程构建一个信念图，以实现相关模型信念(包括对答案候选者的信念)及其推理关系。其次，我们使用形式约束推理器识别和最小化该图中的矛盾。我们发现，REFLEX显著提高了一致性(绝对值提升了8%-11%)，而不损害已有的性能。

    While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent "beliefs". This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a rational, self-reflecting layer on top of the LLM. First, given a question, we construct a belief graph using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming ov
    
[^42]: 一种双向解码的框架：形态变化为案例研究

    A Framework for Bidirectional Decoding: Case Study in Morphological Inflection. (arXiv:2305.12580v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12580](http://arxiv.org/abs/2305.12580)

    本文提出了一种双向解码框架，可以从“外向内”生成序列，并且在多个模型架构和训练方法上进行了改进。该模型在长序列上表现优异，在2022年和2023年共享任务上取得了最佳结果。

    

    基于Transformer的编码器-解码器模型在生成序列任务中以从左到右的方式已成为标准。本文提出了一种从“外向内”生成序列的解码框架：在每个步骤中，模型选择在左边、右边生成一个标记，或者将左右序列连接起来。我们认为这比之前的双向解码更有原则性。我们的提议支持各种模型架构，并包括几种训练方法，例如将潜在的排序变量边缘化的动态规划算法。我们的模型在2022年和2023年共享任务中创造了最新成果(SOTA)，在平均准确性方面分别比其他最佳系统高出4.7个和2.7个点。该模型在长序列上表现出色，可以隐式学习由词干和词缀组成的单词的分割点，并在具有较少唯一词素的数据集上相对于基线表现更好。

    Transformer-based encoder-decoder models that generate outputs in a left-to-right fashion have become standard for sequence-to-sequence tasks. In this paper, we propose a framework for decoding that produces sequences from the "outside-in": at each step, the model chooses to generate a token on the left, on the right, or join the left and right sequences. We argue that this is more principled than prior bidirectional decoders. Our proposal supports a variety of model architectures and includes several training methods, such as a dynamic programming algorithm that marginalizes out the latent ordering variable. Our model sets state-of-the-art (SOTA) on the 2022 and 2023 shared tasks, beating the next best systems by over 4.7 and 2.7 points in average accuracy respectively. The model performs particularly well on long sequences, can implicitly learn the split point of words composed of stem and affix, and performs better relative to the baseline on datasets that have fewer unique lemmas (
    
[^43]: LogiCoT：基于GPT-4的逻辑思维指令调整数据收集。

    LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4. (arXiv:2305.12147v1 [cs.CL])

    [http://arxiv.org/abs/2305.12147](http://arxiv.org/abs/2305.12147)

    该论文提出了LogiCoT, 一个基于GPT-4的逻辑思维指令调整数据集，用于教授模型逻辑推理和引出一般推理技能。

    

    生成式预训练变压器4（GPT-4）展示了令人印象深刻的思维链推理能力。最近的自我指导调整研究（如Alpaca）侧重于增强模型的通用能力。这些指令使模型在一般任务（如开放领域文本生成和释义）上能够达到与GPT-3.5相当的性能。然而，它们不能帮助模型处理复杂的推理任务。为填补这一差距，本文提出了LogiCoT，一种新的逻辑思维指令调整数据集，用于GPT-4的逻辑思维链推理。我们详细阐述了收集指令以提示GPT-4生成思维链推理的过程。LogiCoT作为教授逻辑推理模型的指令集，并引出了一般推理技能。

    Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the process of harvesting instructions for prompting GPT-4 to generate chain-of-thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills.
    
[^44]: STOAT: 结构化数据控制性分析文本生成

    STOAT: Structured Data to Analytical Text With Controls. (arXiv:2305.11826v1 [cs.CL])

    [http://arxiv.org/abs/2305.11826](http://arxiv.org/abs/2305.11826)

    STOAT模型是表格和推理意识的生成模型，在数字推理、常识推理、时间推理、表格知识和实体知识方面有较好的控制，提高了分析句子生成的质量和准确度。

    

    最近，语言模型在结构化数据到文本生成任务中取得了巨大的进展。然而，当需要进行逻辑推理以生成描述时，这些模型仍然表现出次优的性能。在本文中，我们特别关注从结构化数据（例如表格）生成分析文本。在（Gupta et al.,2020）提出的分类基础上，我们重点关注以下推理类别的可控制表格到文本生成：数字推理、常识推理、时间推理、表格知识和实体知识。我们提出了STOAT模型，该模型具有表格和推理意识，并通过矢量量化将给定的推理类别注入输出中。我们观察到，在分析句子任务中，我们的模型在iToTTo和Infotabs的PARENT指标上分别提供了10.19％和1.13％的优化。我们还发现，与基线模型相比，我们的模型生成的描述更加准确和分析，人类评估中增加了15.3％。

    Recent language models have made tremendous progress in the structured data to text generation task. However, these models still give sub-optimal performance where logical inference is required to generate the descriptions. In this work, we specifically focus on analytical text generation from structured data such as tables. Building on the taxonomy proposed in (Gupta et al., 2020) we focus on controllable table to text generation for the following reasoning categories: numerical reasoning, commonsense reasoning, temporal reasoning, table knowledge, and entity knowledge. We propose STOAT model, which is table and reasoning aware, with vector-quantization to infuse the given reasoning categories in the output. We observe that our model provides 10.19%, 1.13% improvement on the PARENT metric in iToTTo and Infotabs for the analytical sentence task. We also found that our model generates 15.3% more faithful and analytical descriptions as compared to the baseline models in human evaluation.
    
[^45]: 合作生成AI：集成GPT-k以在文本到图像生成中提高编辑效率

    Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation. (arXiv:2305.11317v1 [cs.CL])

    [http://arxiv.org/abs/2305.11317](http://arxiv.org/abs/2305.11317)

    本论文通过集成GPT-k来提高T2I生成中的编辑效率，实验证明其更擅长调整（修改）文本中的修饰语，而人类倾向于替换单词和短语。

    

    文本到图像（T2I）生成领域在研究界和用户中引起了极大关注。虽然T2I模型已经取得了很大进展，但用户常遇到的一个问题是需要重复编辑输入提示才能获得令人满意的图像，这是耗时且劳动强度大的。针对大规模语言模型（例如GPT-k）的文本生成能力，我们研究了利用这些模型来改进T2I生成中提示编辑过程的潜力。我们进行了一系列实验，比较了人类和GPT-k常见的编辑方式，评估GPT-k在推动T2I方面的性能，并检查可能影响此过程的因素。我们发现，GPT-k模型更注重插入修改器，而人类倾向于替换单词和短语，包括对主题的更改。实验结果显示，GPT-k在调整修改器方面比较有效。

    The field of text-to-image (T2I) generation has garnered significant attention both within the research community and among everyday users. Despite the advancements of T2I models, a common issue encountered by users is the need for repetitive editing of input prompts in order to receive a satisfactory image, which is time-consuming and labor-intensive. Given the demonstrated text generation power of large-scale language models, such as GPT-k, we investigate the potential of utilizing such models to improve the prompt editing process for T2I generation. We conduct a series of experiments to compare the common edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting T2I, and examine factors that may influence this process. We found that GPT-k models focus more on inserting modifiers while humans tend to replace words and phrases, which includes changes to the subject matter. Experimental results show that GPT-k are more effective in adjusting modifiers rather than p
    
[^46]: 语言模型遇见世界模型：实体经验增强语言模型

    Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])

    [http://arxiv.org/abs/2305.10626](http://arxiv.org/abs/2305.10626)

    本文提出一种新的增强语言模型的方法——将其与世界模型相结合，通过有目的的规划和随机探索获得丰富的实体经验进行微调, 以提高其在物理环境下的推理和行为能力，并在语言基准上保持或提高性能。

    

    虽然大型语言模型 (LMs) 在许多任务上表现出了非凡的能力，但它们常常在处理物理环境下的简单推理和规划问题时遇到困难，例如理解物体永恒或规划家庭活动。这种限制源于 LM 仅受书面语言训练，缺少必要的实体知识和技能。本文提出了一种新的增强 LM 的方法，即将其与世界模型相结合进行微调，以获得多样化的实体知识，同时保持其一般语言能力。本方法在世界模型中部署一个融入实体经验的代理，特别是一个模拟物理世界的仿真器(VirtualHome)，通过有目的的规划和随机探索获得多样化的实体经验。然后，利用这些经验微调 LM ，以教授在物理世界中的各种推理和行为能力，例如规划和完成目标、物体永恒和跟踪等。此外，我们相信我们的方法可以轻松扩展以利用其他模拟器，包括机器人或自动驾驶汽车。我们证明了我们的方法显着提高了 LM 在一系列物理推理任务中的性能，同时保留并经常提高了它们在语言基准上的表现。

    While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
    
[^47]: 生成式语言模型的统计知识评估

    Statistical Knowledge Assessment for Generative Language Models. (arXiv:2305.10519v1 [cs.CL])

    [http://arxiv.org/abs/2305.10519](http://arxiv.org/abs/2305.10519)

    本文介绍了一个统计知识评估框架，用于评估生成式语言模型（GLMs）的知识输出。通过对14种GLMs进行综合比较，发现知识遵循缩放定律，而对指令遵循数据进行微调可能会损害模型的生成能力。

    

    生成式语言模型（GLMs）展示了存储事实知识和高效回答查询的能力。但是，给定不同的提示，GLM是否始终生成事实正确的答案？本文介绍了一个由潜变量和KaRR度量指导的统计知识评估框架，该度量通过计算模型在各种文本形式上的连续概率量化其知识。我们使用我们的框架对14种GLM的知识进行了全面比较，包括LLaMA、Alpaca、OPT和其他模型。我们的统计知识评估涵盖了600种关系类型，并显示出与人类评估的强相关性（0.43 Kendall's $\tau$）。我们的发现揭示了具有相同支架结构的GLM的知识遵循缩放定律，并且在指令遵循数据上进行的微调可能会损害模型持续生成事实正确的文本的能力。

    Generative Language Models (GLMs) have demonstrated capabilities to store factual knowledge and answer queries efficiently. Given varying prompts, does a GLM consistently generate factually correct answers? In this paper, we introduce a statistical knowledge assessment framework guided by latent variables and the KaRR metric, which quantifies a model's knowledge by computing its continuous probability across diverse text forms. We conduct a comprehensive comparison of knowledge across 14 GLMs using our framework, including LLaMA, Alpaca, OPT, and others. Our statistical knowledge assessment encompasses 600 relation types and exhibits a strong correlation (0.43 Kendall's $\tau$) with human evaluation. Our findings reveal that the knowledge in GLMs with the same backbone architecture adheres to the scaling law, and that tuning on instruction-following data may compromise the model's ability to generate factually correct text consistently.
    
[^48]: 阅读过程中对小说人物个性的理解

    Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v1 [cs.CL])

    [http://arxiv.org/abs/2305.10156](http://arxiv.org/abs/2305.10156)

    本文提出了一个NLP领域内尚未研究的问题：情景和细致地理解小说人物个性，并提供了第一个标记数据集PersoNet来解决这个问题。

    

    理解小说人物个性是阅读故事的关键。随着读者与故事的互动，他们对一个人物的理解会根据新的事件和信息而演变；并且可以感知到多个精细的个性方面。这导致了一个自然的问题：情境和精细的个性理解。这个问题在NLP领域中没有得到研究，主要是由于缺乏模仿阅读过程的适当数据集。我们提供了第一个标记数据集PersoNet来解决这个问题。我们的新型注释策略涉及用在线阅读应用程序的用户笔记作为原始书籍的代理进行注释。实验和人体研究表明，我们的数据集构建既有效又准确；我们的任务在很大程度上依赖于长期的上下文以实现对机器和人类的准确预测。数据集可在https://github.com/Gorov/personet_acl23获得。

    Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PersoNet for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans. The dataset is available at https://github.com/Gorov/personet_acl23.
    
[^49]: 语言模型能否用自然语言解决图问题？

    Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])

    [http://arxiv.org/abs/2305.10037](http://arxiv.org/abs/2305.10037)

    本论文提出了自然语言图形(NLGraph)，这是一个全面的基于图形问题解决测试，旨在评估LLM在文本描述的图形结构和图形解决方案方面的处理能力。实验结果表明，LLM(GPT-3/4)具有相应的图形推理能力。

    

    大型语言模型越来越多地应用于一些具有隐式图形结构的任务，例如机器人规划、多跳问题回答或知识探索、结构化常识推理等等。虽然LLM在这些任务中已经取得了一定的进展，但是LLM是否能够显式处理图形的文本描述，将它们映射到基于概念的空间中，并执行结构化操作仍然尚未得到足够的研究。为此，我们提出了自然语言图形(NLGraph)，它是一个设计用于自然语言的基于图形问题解决全面测试。NLGraph包含29,370个问题，涵盖了八个图形推理任务，从简单的连接和最短路径到复杂的最大流和模拟图神经网络等任务不等。我们在NLGraph基准测试上评估了LLM(GPT-3/4)，并发现1)语言模型具有相应的图形推理能力；

    Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language
    
[^50]: The Vault：一个全面的多语言数据集，为促进代码理解和生成而设计

    The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation. (arXiv:2305.06156v1 [cs.CL])

    [http://arxiv.org/abs/2305.06156](http://arxiv.org/abs/2305.06156)

    The Vault是一个提供了10种流行编程语言的40百万行代码-文本对的开源数据集，旨在增强面向代码的大型语言模型（LLM）的训练，有望在代码理解和生成任务上取得显著进展。

    

    我们介绍了 The Vault，这是一个开源的大规模代码文本数据集，旨在增强面向代码的大型语言模型（LLM）的训练。现有的用于训练基于代码的LLM的开源数据集在大小、质量(由于噪声信号)和格式（仅包含代码函数和文本说明配对）方面经常面临挑战。The Vault通过提供10种流行编程语言的40百万行代码-文本对，彻底清除10种多样的问题，以及各种级别的代码-文本对，包括类、函数和代码行等级别，来克服这些限制。研究人员和从业人员可以利用The Vault来训练不同的面向代码的LLM，或者将提供的数据清洗方法和脚本合并到自己的数据集中来改进数据集。通过将The Vault作为面向代码的LLMs的训练数据集，我们预计在代码理解和生成任务上取得显著进展，促进人工智能研究和实践的发展。

    We present The Vault, an open-source, large-scale code-text dataset designed to enhance the training of code-focused large language models (LLMs). Existing open-source datasets for training code-based LLMs often face challenges in terms of size, quality (due to noisy signals), and format (only containing code function and text explanation pairings). The Vault overcomes these limitations by providing 40 million code-text pairs across 10 popular programming languages, thorough cleaning for 10+ prevalent issues, and various levels of code-text pairings, including class, function, and line levels. Researchers and practitioners can utilize The Vault for training diverse code-focused LLMs or incorporate the provided data cleaning methods and scripts to improve their datasets. By employing The Vault as the training dataset for code-centric LLMs, we anticipate significant advancements in code understanding and generation tasks, fostering progress in both artificial intelligence research and so
    
[^51]: 非自回归数学题解决器与统一树结构

    Non-Autoregressive Math Word Problem Solver with Unified Tree Structure. (arXiv:2305.04556v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.04556](http://arxiv.org/abs/2305.04556)

    该论文介绍了一种非自回归数学题解决器，使用统一树结构表示解决方案表达式，解决了现有方法在处理数学变体时的问题。

    

    现有的数学题解决器使用序列或二叉树来表示解决方案表达式并根据给定的问题描述来解码它。然而，这样的结构无法处理通过数学变换可以得到的变体，例如，对于相同的问题，$(a_1+a_2) * a_3$和$a_1 * a_3+a_2 * a_3$都可能是有效的解决方案，但它们被表示为不同的表达式序列或树。多个解决方案变体表示了相同输入问题的不同求解过程，会引发两个问题：1）让模型有效地学习输入和输出空间之间的映射函数变得困难，2）在评估有效表达式变体时错误地指出\textit{错误}。为了解决这些问题，我们引入了一个统一的树结构来表示解决方案表达式，其中的元素对于所有的表达式变体来说是可交换且相同的。我们提出了一种新颖的非自回归求解器，命名为\textit{MWP-NAS}，用于解析问题和求解解决方案。

    Existing MWP solvers employ sequence or binary tree to present the solution expression and decode it from given problem description. However, such structures fail to handle the variants that can be derived via mathematical manipulation, e.g., $(a_1+a_2) * a_3$ and $a_1 * a_3+a_2 * a_3$ can both be possible valid solutions for a same problem but formulated as different expression sequences or trees. The multiple solution variants depicting different possible solving procedures for the same input problem would raise two issues: 1) making it hard for the model to learn the mapping function between the input and output spaces effectively, and 2) wrongly indicating \textit{wrong} when evaluating a valid expression variant. To address these issues, we introduce a unified tree structure to present a solution expression, where the elements are permutable and identical for all the expression variants. We propose a novel non-autoregressive solver, named \textit{MWP-NAS}, to parse the problem and
    
[^52]: NLI4CT：面向临床试验报告的多证据自然语言推理

    NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])

    [http://arxiv.org/abs/2305.03598](http://arxiv.org/abs/2305.03598)

    本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。

    

    如何解释和检索用于支持临床决策的医学证据？多年来，积累下来的临床试验报告包含了发展个性化医学所必需的信息。然而，为了找到最佳的实验治疗证据，手动检查超过400,000个临床试验报告是实际上不可行的。自然语言推理（NLI）提供了一个潜在的解决方案，通过允许可扩展计算文本蕴含关系。然而，现有的NLI模型在生物医学语料库上表现不佳，之前发布的数据集无法捕捉CTR推理的全部复杂性。在本文中，我们提出了一种新的资源，以推进关于CTR推理的NLI研究。该资源包括两个主要任务。首先，确定自然语言陈述和CTR之间的推理关系。其次，检索支持事实以证明预测的关系。我们提供了NLI4CT，一个基于CTR的语料库。

    How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
    
[^53]: 基于翻译对齐的视觉语言模型跨语言迁移的参数高效方法

    Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v1 [cs.CL])

    [http://arxiv.org/abs/2305.03510](http://arxiv.org/abs/2305.03510)

    本文提出了一个通过翻译对齐的方式实现参数高效、跨语言的迁移学习框架，实验结果显示该框架能够显著减少多语言之间的性能差异。

    

    预训练的视觉语言模型（如CLIP）在连接图像和英语文本方面取得了显著的成功。尽管最近试图扩展CLIP以支持其他语言，但由于资源不平衡，观察到了不同语言之间的性能差异。此外，当前的预训练模型的跨语言迁移方法会消耗大量资源。因此，我们提出了一种新的参数高效的跨语言迁移学习框架，利用基于翻译的对齐方法来减轻多语言差异，并探索参数高效的微调方法来实现参数高效的跨语言迁移。在XTD和Multi30K数据集上进行了广泛的实验，涵盖了零-shot、few-shot和全数据集学习场景下的11种语言，结果显示我们的框架显著减少了语言之间的多语言差异，并提高了性能。

    Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we propose a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive experiments on XTD and Multi30K datasets, covering 11 languages under zero-shot, few-shot, and full-dataset learning scenarios, show that our framework significantly reduces the multilingual disparities among languages and improves 
    
[^54]: 将流程图转化为对话：基于计划的数据增强方法，用于低资源流程图相关故障排除对话

    Turning Flowchart into Dialog: Plan-based Data Augmentation for Low-Resource Flowchart-grounded Troubleshooting Dialogs. (arXiv:2305.01323v1 [cs.CL])

    [http://arxiv.org/abs/2305.01323](http://arxiv.org/abs/2305.01323)

    本文提出了一个基于计划的数据增强方法，能够将简洁的流程图转化成对话，以生成足够的数据来训练以流程图为基础的故障排除对话系统，实验结果表明该方法有效地提高了系统性能。

    

    近年来，以流程图为基础的故障排除对话系统（FTD系统）一直备受研究关注。然而，收集充分的自然基于流程图的对话数据成本较高，因此FTD系统受到数据稀缺的限制。为了缓解数据稀疏性问题，我们提出了基于计划的数据增强（PlanDA）方法，通过将简洁的流程图转化为对话，生成大量多样的合成对话数据。具体来说，它的生成模型采用具有全局和局部潜在规划变量的分层规划策略的变分基框架。在FloDial数据集上的实验表明，PlanDA生成的合成对话改善了下游任务的性能，包括流程图路径检索和响应生成，特别是在流程图以外的情况下。

    Flowchart-grounded troubleshooting dialogue (FTD) systems, which follow the instructions of a flowchart to diagnose users' problems in specific domains (eg., vehicle, laptop), have been gaining research interest in recent years. However, collecting sufficient dialogues that are naturally grounded on flowcharts is costly, thus FTD systems are impeded by scarce training data. To mitigate the data sparsity issue, we propose a plan-based data augmentation (PlanDA) approach that generates diverse synthetic dialog data at scale by transforming concise flowchart into dialogues. Specifically, its generative model employs a variational-base framework with a hierarchical planning strategy that includes global and local latent planning variables. Experiments on the FloDial dataset show that synthetic dialogue produced by PlanDA improves the performance of downstream tasks, including flowchart path retrieval and response generation, in particular on the Out-of-Flowchart settings. In addition, furt
    
[^55]: 只用结构先预训练：利用迁移学习理解语言归纳偏置

    Pretrain on just structure: Understanding linguistic inductive biases using transfer learning. (arXiv:2304.13060v1 [cs.CL])

    [http://arxiv.org/abs/2304.13060](http://arxiv.org/abs/2304.13060)

    通过在人造结构数据上进行预先训练和在英语上微调，我们研究了自然语言处理中三种归纳偏置类型：递归的层级处理、无限制的标记-标记依赖以及基于Zipfian幂律词汇分布的归纳偏置，我们得出复杂标记-标记交互形成了最好的归纳偏置的结论。

    

    无论是人类还是变压器语言模型都能在没有明确的结构监督下学习语言。什么样的归纳式学习偏置使得这种学习成为可能？在这项研究中，我们通过在人造结构数据上预先训练并在英语上微调来采用不同的归纳式学习偏置对语言模型进行偏置。我们的实验设置使我们能够积极控制语言模型的归纳偏置。通过我们的实验，我们研究了三种归纳偏置的比较成功:1)递归的层级处理的归纳偏置2)不受限的标记-标记依赖，这些依赖关系不能由上下文无关文法建模3)Zipfian幂律词汇分布的归纳偏置。我们表明，复杂的标记-标记交互形成了最好的归纳偏置，并且这在非上下文无关情况下最为强烈。我们还表明，Z(targetEntity)分布在英语上也是合适的预先训练分布。

    Both humans and transformer language models are able to learn language without explicit structural supervision. What inductive learning biases make this learning possible? In this study, we examine the effect of different inductive learning biases by predisposing language models with structural biases through pretraining on artificial structured data, and then evaluating by fine-tuning on English. Our experimental setup gives us the ability to actively control the inductive bias of language models. With our experiments, we investigate the comparative success of three types of inductive bias: 1) an inductive bias for recursive, hierarchical processing 2) an inductive bias for unrestricted token-token dependencies that can't be modeled by context-free grammars, and 3) an inductive bias for a Zipfian power-law vocabulary distribution. We show that complex token-token interactions form the best inductive biases, and that this is strongest in the non-context-free case. We also show that a Z
    
[^56]: PUNR: 用户行为建模的新闻推荐预训练

    PUNR: Pre-training with User Behavior Modeling for News Recommendation. (arXiv:2304.12633v1 [cs.IR])

    [http://arxiv.org/abs/2304.12633](http://arxiv.org/abs/2304.12633)

    本论文提出了一种无监督的预训练方法，它可以通过两个任务实现有效的用户行为建模，以提高新闻推荐系统的准确性和性能表现。

    

    新闻推荐旨在基于用户行为预测点击行为。如何有效地建模用户表示是推荐首选新闻的关键。现有方法大多集中在监督微调阶段的改进上。然而，还缺乏针对用户表示优化的基于PLM的无监督预训练方法。在本文中，我们提出了一种具有两个任务的无监督预训练范例，即用户行为掩蔽和用户行为生成，均致力于有效的用户行为建模。首先，我们引入了用户行为掩蔽预训练任务，以恢复基于上下文行为的掩蔽用户行为。通过这种方式，模型可以捕捉到更强大、更全面的用户新闻阅读模式。此外，我们还结合了一种新颖的辅助用户行为生成预训练任务，以增强从用户编码器派生出的用户表示向量。我们使用上述预训练的用户建模来进行新闻推荐，实验结果表明，我们的模型在多个数据集上取得了显著的性能提升。

    News recommendation aims to predict click behaviors based on user behaviors. How to effectively model the user representations is the key to recommending preferred news. Existing works are mostly focused on improvements in the supervised fine-tuning stage. However, there is still a lack of PLM-based unsupervised pre-training methods optimized for user representations. In this work, we propose an unsupervised pre-training paradigm with two tasks, i.e. user behavior masking and user behavior generation, both towards effective user behavior modeling. Firstly, we introduce the user behavior masking pre-training task to recover the masked user behaviors based on their contextual behaviors. In this way, the model could capture a much stronger and more comprehensive user news reading pattern. Besides, we incorporate a novel auxiliary user behavior generation pre-training task to enhance the user representation vector derived from the user encoder. We use the above pre-trained user modeling en
    
[^57]: 荆棘玫瑰：探究自然语言处理中的双重使用困境

    Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing. (arXiv:2304.08315v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08315](http://arxiv.org/abs/2304.08315)

    本文调查了自然语言处理（NLP）领域的双重使用问题，提出了一份定制的双重使用定义，并讨论了当前的状况和可能的挑战。

    

    双重使用是指有意将技术和科学成果用于有害目的的问题，在自然语言处理（NLP）领域尚未明确定义。然而，随着NLP技术的不断发展和在社会中的广泛应用，其内部运行方式变得越来越不透明。因此，理解双重使用的问题以及限制双重使用的潜在方法对于减少研究和开发的潜在危害至关重要。在本文中，我们对NLP研究人员和从业者进行了调查，以了解他们对该问题的深度理解和观点，并评估现有的支持情况。根据调查结果，我们为NLP社区提供了一份定制的双重使用定义。调查结果显示，大多数研究人员对他们的研究的潜在双重使用问题表示关切，但只采取有限的行动。基于调查结果，我们讨论了当前的状况和可能的挑战。

    Dual use, the intentional, harmful reuse of technology and scientific artefacts, is a problem yet to be well-defined within the context of Natural Language Processing (NLP). However, as NLP technologies continue to advance and become increasingly widespread in society, their inner workings have become increasingly opaque. Therefore, understanding dual use concerns and potential ways of limiting them is critical to minimising the potential harms of research and development. In this paper, we conduct a survey of NLP researchers and practitioners to understand the depth and their perspective of the problem as well as to assess existing available support. Based on the results of our survey, we offer a definition of dual use that is tailored to the needs of the NLP community. The survey revealed that a majority of researchers are concerned about the potential dual use of their research but only take limited action toward it. In light of the survey results, we discuss the current state and p
    
[^58]: 多模态C4：一种包含大量图像和文本的开放式数据库

    Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text. (arXiv:2304.06939v1 [cs.CV])

    [http://arxiv.org/abs/2304.06939](http://arxiv.org/abs/2304.06939)

    Multimodal C4是一个开放的、以图像与文本交替形式存在的数据库，其使用线性分配算法将图像放到长文本段落中，可用于通过少量样本学习和复杂相关度提示的建模。

    

    上下文视觉和语言模型需要支持任意交替的图像和文本序列作为输入, 这种格式不仅可以通过交替独立监督的(图像,文本)示例来进行低次学习,而且可以应对更复杂的提示, 涉及图像间互动,例如“图像A和图像B有什么共同之处?”现有的预训练模型使用类似于交替图像+文本的web语料库。但是，迄今为止，这种形式的大规模数据还没有公开提供。我们发布了Multimodal C4 (mmc4)，这是一个加强版的c4文本库，其中插入了图像。我们使用一个线性分配算法，使用CLIP特征将图像放到更长的文本体中，此过程优于其他替代方案。mmc4涵盖了诸如烹饪，旅游，技术等日常主题。对随机样本的手动检查表明，绝大多数(90%)的图像与主题相关。

    In-context vision and language models like Flamingo support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., "What do image A and image B have in common?" To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available.  We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features, a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (90%) of images are topically relevant, and tha
    
[^59]: 大语言模型实现多语机器翻译：实证结果和分析

    Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. (arXiv:2304.04675v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.04675](http://arxiv.org/abs/2304.04675)

    本文系统地研究了大语言模型在多语机器翻译中的优势和挑战，证明其表现出卓越的潜力。本研究发现LLMs在给定上下文示例时可以意外地忽略提示语义，并且跨语言示例可以为低资源翻译提供更好的任务指导。但实证结果表明，即使是最好的模型ChatGPT仍然落后于监督基线NLLB。

    

    大语言模型(LLMs)在处理多语机器翻译(MMT)方面表现出了卓越的潜力。本文通过回答两个问题系统地研究了LLMs在MMT中的优势和挑战：1) LLMs在翻译大量语言方面表现如何？2) 哪些因素会影响LLMs在翻译中的表现？我们评估了包括XGLM、OPT、BLOOMZ和ChatGPT在内的几个受欢迎的LLMs在102种语言上的表现。我们的实证结果显示，即使是最好的模型ChatGPT在83.33%的翻译方向上也落后于监督基线NLLB。通过进一步的分析，我们发现当用于MMT时，LLMs表现出新的工作模式。首先，在给定上下文示例时，提示语义可能会被意外地忽略，即使提示不合理，LLMs仍然表现出强大的性能。其次，跨语言示例可以为低资源翻译提供比相同语言对中的示例更好的任务指导。第三，当翻译低资源语言时，LLMs往往表现得更好。总的来说，我们的研究为LLMs在MMT中的潜力和局限性提供了新的见解，为未来的研究提供了有用的启示。

    Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating a massive number of languages? 2) Which factors affect LLMs' performance in translation? We evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102 languages. Our empirical results show that even the best model ChatGPT still lags behind the supervised baseline NLLB in 83.33% of translation directions. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, prompt semantics can surprisingly be ignored when given in-context exemplars, where LLMs still show strong performance even with unreasonable prompts. Second, cross-lingual exemplars can provide better task instruction for low-resource translation than exemplars in the same language pairs. Third
    
[^60]: MEGClass: 通过相互增强的文本粒度实现极弱监督文本分类。

    MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities. (arXiv:2304.01969v1 [cs.CL])

    [http://arxiv.org/abs/2304.01969](http://arxiv.org/abs/2304.01969)

    MEGClass是一种通过相互增强的文本粒度实现极弱监督文本分类的方法，利用分层关注机制从文档、句子和单词中提取连贯且多样的子文本，能够准确分类讨论多个主题的文档，并且在五个基准数据集上表现优于现有最先进的模型。

    

    文本分类通常需要大量的人工标注数据作为监督，这在动态新兴领域中是昂贵的。某些方法通过仅依赖类名表面文本作为极弱监督来解决这个问题。然而，现有方法未能考虑到单一类别文档讨论多个主题的情况。主题多样性和模糊的句子可能会引入噪声到文档的底层表示，从而影响预测类别的精度。此外，当前的方法独立地关注文档、句子或单词的文本粒度，从而限制了我们联合从所有三者中提取粗粒度或细粒度上下文的能力来识别分类的重要子文本。为了解决这个问题，我们提出了MEGClass，一种利用相互增强的文本粒度进行极弱监督文本分类的方法。具体来说，MEGClass通过分层关注机制从文档、句子和单词中提取连贯且多样的子文本，使我们能够识别和整合来自多个粒度的弱信号，以准确分类文档，即使它们讨论多个主题。在五个基准数据集上的实验结果表明，我们的方法在使用极弱监督的情况下，比现有最先进的模型有着更好的表现。

    Text classification typically requires a substantial amount of human-annotated data to serve as supervision, which is costly to obtain in dynamic emerging domains. Certain methods seek to address this problem by solely relying on the surface text of class names to serve as extremely weak supervision. However, existing methods fail to account for single-class documents discussing multiple topics. Both topic diversity and vague sentences may introduce noise into the document's underlying representation and consequently the precision of the predicted class. Furthermore, current work focuses on text granularities (documents, sentences, or words) independently, which limits the degree of coarse- or fine-grained context that we can jointly extract from all three to identify significant subtext for classification. In order to address this problem, we propose MEGClass, an extremely weakly-supervised text classification method to exploit Mutually-Enhancing Text Granularities. Specifically, MEGC
    
[^61]: 超越负采样的高效分布式表示方法

    Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v1 [cs.LG])

    [http://arxiv.org/abs/2303.17475](http://arxiv.org/abs/2303.17475)

    本文介绍了一种高效的分布式表示（嵌入）学习方法，通过线性时间估计softmax归一化常数来实现学习过程，该方法优于负采样方法并在多项测试中验证了其有效性。

    

    本文介绍了一种高效的学习分布式表示（也称为嵌入）的方法。该方法通过最小化一个类似于Word2Vec算法中引入并在多个工作中采用的目标函数来实现。优化计算的瓶颈是softmax归一化常数的计算，这需要与样本大小呈二次比例的操作数。这种复杂度不适用于大型数据集，所以负采样是一个常见的解决方法，可以在与样本大小线性相关的时间内获得分布式表示。然而，负采样会改变损失函数，因此解决的是与最初提出的不同的优化问题。我们的贡献在于展示如何通过线性时间估计softmax归一化常数，从而设计了一种有效的优化策略来学习分布式表示。我们使用不同的数据集进行测试，并展示了我们的方法在嵌入质量和训练时间方面优于负采样。

    This article describes an efficient method to learn distributed representations, also known as embeddings. This is accomplished minimizing an objective function similar to the one introduced in the Word2Vec algorithm and later adopted in several works. The optimization computational bottleneck is the calculation of the softmax normalization constants for which a number of operations scaling quadratically with the sample size is required. This complexity is unsuited for large datasets and negative sampling is a popular workaround, allowing one to obtain distributed representations in linear time with respect to the sample size. Negative sampling consists, however, in a change of the loss function and hence solves a different optimization problem from the one originally proposed. Our contribution is to show that the sotfmax normalization constants can be estimated in linear time, allowing us to design an efficient optimization strategy to learn distributed representations. We test our ap
    
[^62]: 利用不对称性进行合成训练数据生成：SynthIE和信息提取案例

    Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction. (arXiv:2303.04132v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04132](http://arxiv.org/abs/2303.04132)

    本文展示了利用不对称性进行合成训练数据生成的方法，可以针对结构化输出问题产生大规模、高质量的数据。在封闭信息提取任务中，我们合成生成了一个包含180万数据点的数据集，并利用此数据集对小型模型进行微调，取得了远超先前领先技术的成果。

    

    大型语言模型（LLM）在合成数据生成方面有着巨大的潜力。这项工作表明，即使对于LLM无法直接解决的任务，也可以合成生成有用的数据：对于具有结构化输出的问题，可以提示LLM在反向方向上执行任务，通过为目标输出结构生成合理的输入文本。利用任务困难度的不对称性，可以生成大规模、高质量的复杂任务数据。我们在封闭信息提取方面展示了这种方法的有效性，该领域难以收集到真实数据，至今没有令人满意的数据集存在。我们合成生成了一个包含180万数据点的数据集，并在人工评估中证明其与现有数据集相比具有更好的质量，并利用该数据集对小型模型（220M和770M参数）进行微调，这些模型被称为SynthIE，以远远超过先前领先技术的水平（具有相同的模型大小）。

    Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 
    
[^63]: 量化和建模多模态交互：一种信息分解框架

    Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12247](http://arxiv.org/abs/2302.12247)

    通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。

    

    对于解决多模态任务所需的交互如何进行量化？最适合捕捉这些交互的多模态模型是什么？为了回答这些问题，我们提出了一种信息论方法来量化输入模态与输出任务之间的冗余度、独特性和协同性。我们将这三个衡量标准称为多模态分布（或简称PID）的PID统计量，并引入了两个新的PID统计估计器，适用于高维分布。为了验证PID估计，我们在已知PID的合成数据集和大规模多模态基准测试集上进行了大量实验。

    The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
    
[^64]: 离线强化学习用于混合专家对话管理

    Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management. (arXiv:2302.10850v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10850](http://arxiv.org/abs/2302.10850)

    本论文研究了离线强化学习在混合专家对话管理中的应用。通过利用最新的混合专家语言模型（MoE-LMs），我们开发了针对对话规划的强化学习算法，显著减少了动作空间的大小，并提高了对话管理的有效性。

    

    强化学习（RL）在开发对话管理（DM）代理，实现非目标导向，进行富有内容的对话，最大化用户满意度方面表现出了巨大的潜力。尽管强化学习和语言模型（LMs）最近取得了进展，但使用强化学习驱动的对话聊天机器人仍然具有挑战性，部分原因是强化学习需要在线探索以有效学习，而收集新颖的人机交互可能既昂贵又不安全。这个问题在面对这些算法的组合动作空间时变得更为严重，因为大多数语言模型代理以词级别生成响应。我们开发了多种针对对话规划的强化学习算法，利用最新的混合专家语言模型（MoE-LMs） - 一种捕捉多样语义，生成反映不同意图的话语的模型，适用于多轮对话管理。通过利用MoE-LM结构，我们的方法显著减少了行动空间的大小，并提高了基于强化学习的对话管理的有效性。

    Reinforcement learning (RL) has shown great promise for developing dialogue management (DM) agents that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite recent developments in RL and language models (LMs), using RL to power conversational chatbots remains challenging, in part because RL requires online exploration to learn effectively, whereas collecting novel human-bot interactions can be expensive and unsafe. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop a variety of RL algorithms, specialized to dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM.
    
[^65]: 保持中立：使用自然语言推理改进生成器

    Keep it Neutral: Using Natural Language Inference to Improve Generation. (arXiv:2302.08577v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08577](http://arxiv.org/abs/2302.08577)

    本文将自然语言推理（NLI）引入文本生成过程中，通过预训练的NLI模型评估生成的句子是否符合、与原始文本相矛盾或中立。最大化中立类别的NLI策略提供了最高质量的生成文本，无论参数取值如何。

    

    本文研究将自然语言推理（NLI）引入文本生成过程中，通过使用预训练的NLI模型来评估生成的句子是否符合、与原始文本相矛盾或中立。首先，我们证明NLI任务能够预测GPT-3生成错误。我们利用这些结果为GPT-J开发了一种基于NLI的生成策略。然后，我们通过人工标注错误类型和整体质量来评估生成的结果。我们发现，在核心采样的随机参数值较高时，最大化蕴涵关系的NLI策略改善了文本生成，而在参数值较低时，最大化矛盾关系的策略实际上是有效的。总体而言，我们展示了最大化中立类别的NLI策略提供了最高质量的生成文本（显著优于普通生成器），无论参数取值如何。

    We explore incorporating natural language inference (NLI) into the text generative pipeline by using a pre-trained NLI model to assess whether a generated sentence entails, contradicts, or is neutral to the prompt and preceding text. First, we show that the NLI task is predictive of generation errors made by GPT-3. We use these results to develop an NLI-informed generation procedure for GPT-J. Then, we evaluate these generations by obtaining human annotations on error types and overall quality. We find that an NLI strategy of maximizing entailment improves text generation when the nucleus sampling randomness parameter value is high, while one which maximizes contradiction is in fact productive when the parameter value is low. Overall, though, we demonstrate that an NLI strategy of maximizing the neutral class provides the highest quality of generated text (significantly better than the vanilla generations), regardless of parameter value.
    
[^66]: CAPSTONE: 使用课程采样进行密集检索与文档扩展

    CAPSTONE: Curriculum Sampling for Dense Retrieval with Document Expansion. (arXiv:2212.09114v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09114](http://arxiv.org/abs/2212.09114)

    本文提出了一种使用课程采样策略的密集检索方法，通过在训练中使用伪查询，逐步增强了生成的查询和真实查询之间的相关性。

    

    双编码器已成为密集检索的事实标准架构。通常，它独立计算查询和文档的潜在表示，因此未能充分捕捉查询和文档之间的交互。为了缓解这个问题，最近的研究集中在获得查询相关的文档表示。在训练过程中，它通过真实查询扩展文档，但在推断过程中，它用生成的查询替换真实查询。这种训练和推断之间的不一致导致密集检索模型在计算文档表示时更加重视查询信息，而忽视文档。因此，它的性能比普通的密集检索模型还要差，因为它的性能严重依赖于生成的查询和真实查询之间的相关性。本文提出了一种课程采样策略，在训练过程中使用伪查询，并逐步增强生成的查询和真实查询之间的相关性。

    The dual-encoder has become the de facto architecture for dense retrieval. Typically, it computes the latent representations of the query and document independently, thus failing to fully capture the interactions between the query and document. To alleviate this, recent research has focused on obtaining query-informed document representations. During training, it expands the document with a real query, but during inference, it replaces the real query with a generated one. This inconsistency between training and inference causes the dense retrieval model to prioritize query information while disregarding the document when computing the document representation. Consequently, it performs even worse than the vanilla dense retrieval model because its performance heavily relies on the relevance between the generated queries and the real query.In this paper, we propose a curriculum sampling strategy that utilizes pseudo queries during training and progressively enhances the relevance between 
    
[^67]: MelHuBERT: 一种基于Mel频谱图的简化HuBERT模型

    MelHuBERT: A simplified HuBERT on Mel spectrograms. (arXiv:2211.09944v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09944](http://arxiv.org/abs/2211.09944)

    MelHuBERT是基于Mel频谱图的简化版HuBERT模型，通过改进损失函数、输入表示和多阶段训练，在语音识别方面取得了有利表现，节省了31.2%的预训练时间和33.5%的计算资源。

    

    自监督模型在学习语音表示方面取得了巨大的成功，可以推广到各种下游任务。然而，大多数自监督模型需要大量的计算资源和多个GPU来进行训练，从而严重限制了自监督学习的发展。为了减少训练的计算量，我们重新审视了HuBERT的训练方法，这是一个非常成功的自监督模型。我们改进并简化了几个关键组成部分，包括损失函数、输入表示和多阶段训练。我们的模型MelHuBERT在音素识别、说话人识别和自动语音识别方面均能取得较好的性能，同时节省了31.2%的预训练时间，或等效地每秒语音节省了33.5%的MACs。代码和预训练模型可在https://github.com/nervjack2/MelHuBERT中获得。

    Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.
    
[^68]: 在语音中挖掘词边界作为天然注释的词分割数据

    Mining Word Boundaries in Speech as Naturally Annotated Word Segmentation Data. (arXiv:2210.17122v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.17122](http://arxiv.org/abs/2210.17122)

    本研究通过从平行语音/文本数据中挖掘词边界，提出了一种可以在跨领域和资源匮乏场景下显著提高汉语词分割性能的方法。

    

    本研究受早期对于探索汉语词分割天然注释数据的研究以及最近关于语音与文本处理集成的研究的启发，首次提出了从平行语音/文本数据中挖掘词边界的方法。首先，我们从两个与我们实验中使用的词分割数据相关的互联网源收集平行语音/文本数据。然后，我们获得字符级对齐，并根据相邻字符之间的停顿时长设计简单的启发式规则来确定词边界。最后，我们提出了一种有效的完整-然后-训练策略，可以更好地利用额外的天然注释数据用于模型训练。实验证明，我们的方法可以显著提高跨领域和资源匮乏场景下的词分割性能。

    Inspired by early research on exploring naturally annotated data for Chinese word segmentation (CWS), and also by recent research on integration of speech and text processing, this work for the first time proposes to mine word boundaries from parallel speech/text data. First we collect parallel speech/text data from two Internet sources that are related with CWS data used in our experiments. Then, we obtain character-level alignments and design simple heuristic rules for determining word boundaries according to pause duration between adjacent characters. Finally, we present an effective complete-then-train strategy that can better utilize extra naturally annotated data for model training. Experiments demonstrate our approach can significantly boost CWS performance in both cross-domain and low-resource scenarios.
    
[^69]: 关于临床文本挖掘的跨领域预训练语言模型：在数据受限微调中它们表现如何？

    On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.12770](http://arxiv.org/abs/2210.12770)

    本研究探讨了在临床自然语言处理任务中，将从一般领域或相关领域数据预训练的迁移学习模型微调到特定任务上的有效性。实验结果显示，微调的语言模型相对于从头开始学习的模型在命名实体识别任务上取得了更好的性能。

    

    在自然语言处理领域，使用从一般或相关领域数据预训练的大型语言模型（LLMs）来将其微调到特定领域和任务上，并使用新任务中可用的有限资源进行微调，一直以来都是一个流行的实践。在本研究中，我们重新考虑了这种假设，并在临床自然语言处理领域进行了研究，具体是在药物及其相关属性的命名实体识别任务上。我们比较了从头开始学习的Transformer模型和通过微调BERT-based LLMs（包括BERT-base、BioBERT和ClinicalBERT）进行微调的模型。我们还对这些模型及其扩展模型与带有CRF层的连续学习进行了比较。我们使用n2c2-2018共享任务数据进行模型开发和评估。实验结果表明：1）CRF层对所有神经模型都起到了积极的影响；2）在使用宏平均F1对BIO-strict跨度级别进行评估时，微调的LLMs获得了0.83+的得分，而从头开始学习的TransformerCRF模型得分为0.78+，证明了微调模型的优势。

    Fine-tuning Large Language Models (LLMs) pre-trained from general or related domain data to a specific domain and task using a limited amount of resources available in the new task has been a popular practice in NLP fields. In this work, we re-visit this assumption, and carry out investigation in clinical NLP, specifically named-entity recognition on Drugs and their related Attributes. We compare Transformer models that are learned from scratch to fine-tuning BERT-based LLMs including BERT-base, BioBERT, and ClinicalBERT. We also investigate the comparison of such models and their extended models with a CRF layer for continuous learning. We use n2c2-2018 shared task data for model development and evaluations. The experimental outcomes show that 1) the CRF layer makes a difference for all neural models; 2) on BIO-strict span level evaluation using macro-average F1, while the fine-tuned LLMs achieved scores 0.83+, the TransformerCRF model learned from scratch achieved 0.78+ demonstrating
    
[^70]: SimSCOOD: Fine-tuned源代码模型的超分布泛化的系统分析

    SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models. (arXiv:2210.04802v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.04802](http://arxiv.org/abs/2210.04802)

    本文是第一个系统研究超分布泛化问题的方法，通过模拟不同维度的源代码数据属性和微调方法，在不同场景中研究了模型的行为。

    

    大型代码数据集已经越来越容易地用于预训练源代码模型。然而，对于微调阶段来说，获取代表性的训练数据以充分覆盖特定下游任务的代码分布仍然具有挑战性，原因是任务特定性和有限的标注资源。此外，微调预训练模型可能会导致遗忘以前获得的预训练知识。这些问题导致了超分布泛化问题，即模型的推理行为出现意外情况，这尚未进行系统研究。在本文中，我们提出了第一个系统方法，模拟了不同维度源代码数据属性的各种超分布场景，并研究了这些场景中微调模型的行为。我们研究了不同微调方法（包括全微调和低秩适应微调方法）下模型的行为。我们在各个系统上进行了全面分析。

    Large code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. Moreover, fine-tuning pretrained models can result in forgetting previously acquired pre-training knowledge. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet. In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on fo
    
[^71]: 所有步骤都同等重要吗？基于事件的重要性检测的基准测试。

    Are All Steps Equally Important? Benchmarking Essentiality Detection of Events. (arXiv:2210.04074v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.04074](http://arxiv.org/abs/2210.04074)

    本文研究了当前模型在理解与目标事件有关的步骤事件的重要性方面的困难，并贡献了一个由专家手动注释步骤重要性的语料库。

    

    自然语言以不同的细粒度表达事件，其中粗粒度事件（目标）可以细分为更细粒度的事件序列（步骤）。理解事件过程的一个关键但常被忽视的方面是认识到并非所有步骤事件对于完成目标具有相同的重要性。本文通过研究当前模型理解与目标事件有关的步骤事件的重要性的程度来填补这一空白。认知研究表明，这种能力使机器能够模拟人类对日常任务的先决条件和必要努力的常识推理。我们贡献了一个优质的语料库（目标，步骤）对，该语料库从社区指南网站WikiHow收集，通过专家手动注释步骤的重要性。高一致性的标注者间一致性表明人类对事件重要性具有一致的理解。然而，在评估多个统计模型之后，我们发现当前的模型在理解步骤的重要性方面还存在困难。

    Natural language expresses events with varying granularities, where coarse-grained events (goals) can be broken down into finer-grained event sequences (steps). A critical yet overlooked aspect of understanding event processes is recognizing that not all step events hold equal importance toward the completion of a goal. In this paper, we address this gap by examining the extent to which current models comprehend the essentiality of step events in relation to a goal event. Cognitive studies suggest that such capability enables machines to emulate human commonsense reasoning about preconditions and necessary efforts of everyday tasks. We contribute a high-quality corpus of (goal, step) pairs gathered from the community guideline website WikiHow, with steps manually annotated for their essentiality concerning the goal by experts. The high inter-annotator agreement demonstrates that humans possess a consistent understanding of event essentiality. However, after evaluating multiple statisti
    
[^72]: Hansel: 一个中文 Few-Shot 和 Zero-Shot 实体链接基准

    Hansel: A Chinese Few-Shot and Zero-Shot Entity Linking Benchmark. (arXiv:2207.13005v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.13005](http://arxiv.org/abs/2207.13005)

    Hansel是一个中文Few-Shot和Zero-Shot实体链接基准，填补了非英语语言中关注尾部和新兴实体的数据集的空白。该基准测试集由人工注释和审核，使用一种新方法收集Zero-Shot实体链接数据集，并以Wikidata作为目标知识库。研究表明，现有的最先进实体链接系统在Hansel上表现较差，我们建立了一个强基线，在Few-Shot上达到了46.2%的准确率，在Zero-Shot上达到了76.6%的准确率，并在TAC-KBP2015中文实体链接任务上取得了竞争性结果。

    

    现代实体链接（EL）系统固化了对流行度的偏见，然而除了英语之外，没有专注于尾部和新兴实体的数据集。我们介绍了Hansel，这是一个新的中文基准，填补了非英语 Few-Shot 和 Zero-Shot 实体链接挑战的空白。Hansel的测试集是人工注释和审核的，使用一种收集 Zero-Shot EL 数据集的新方法创建。它涵盖了包括新闻、社交媒体帖子和其他网络文章等多样化的10,000个文档，并以Wikidata作为其目标知识库。我们证明了现有的最先进 EL 系统在Hansel上性能较差（Few-Shot 上的R@1为36.6%）。然后，我们建立了一个强基线，在Few-Shot上得分为46.2%，在Zero-Shot上得分为76.6%。我们还展示了我们的基线在TAC-KBP2015中文实体链接任务上取得了竞争性的结果。

    Modern Entity Linking (EL) systems entrench a popularity bias, yet there is no dataset focusing on tail and emerging entities in languages other than English. We present Hansel, a new benchmark in Chinese that fills the vacancy of non-English few-shot and zero-shot EL challenges. The test set of Hansel is human annotated and reviewed, created with a novel method for collecting zero-shot EL datasets. It covers 10K diverse documents in news, social media posts and other web articles, with Wikidata as its target Knowledge Base. We demonstrate that the existing state-of-the-art EL system performs poorly on Hansel (R@1 of 36.6% on Few-Shot). We then establish a strong baseline that scores a R@1 of 46.2% on Few-Shot and 76.6% on Zero-Shot on our dataset. We also show that our baseline achieves competitive results on TAC-KBP2015 Chinese Entity Linking task.
    
[^73]: 语言模型显示对推理任务具有类似人类的内容效应

    Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.07051](http://arxiv.org/abs/2207.07051)

    本研究探讨了语言模型在逻辑推理任务中是否像人类一样通过混入内容来影响答案，结果发现大型语言模型的先验期望能够捕捉到这种特征。

    

    抽象推理是智能系统的关键能力。大型语言模型在抽象推理任务上实现了高于随机的性能，但存在许多不完善之处。然而，人类的抽象推理也是不完美的。例如，人类推理受到我们对真实世界的知识和信念的影响，并表现出显著的“内容效应”；当问题的语义内容支持正确的逻辑推理时，人类更可靠地进行推理。这些内容纠缠的推理模式在关于人类智能基本性质的争论中起着核心作用。在这里，我们研究了语言模型是否以类似的方式混入内容来回答逻辑问题，这些语言模型的先验期望捕捉了一些人类知识的特征。我们在三个逻辑推理任务上探索了这个问题：自然语言推理、判断三段论的逻辑有效性和Wason选择任务。我们评估了最先进的大型语言模型的性能。

    Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
    
[^74]: 评估和诱导预训练语言模型的个性

    Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.07550](http://arxiv.org/abs/2206.07550)

    本文提出了Machine Personality Inventory (MPI)数据集用于评估机器个性，通过评估，证明了预训练语言模型(LLM)具有个性。进一步提出了Personality Prompting (P^2)方法，用于以可控的方式诱导LLM具有特定个性。

    

    个性起源于哲学探索，关注个体在思考、情感和行为方面的差异。为了构建能够与人类日常合作的社交机器，我们想知道：现有的大型语言模型(LLMs)是否拥有与人类类似的个性？如果是，我们如何评估它们？进一步地，在此评估框架的基础上，如何以可控的方式诱导具有特定个性的语言模型？为回答这些问题，我们提出了机器个性库(Machine Personality Inventory, MPI)数据集，用于评估机器的个性。MPI遵循标准化的个性测试，基于五因素人格理论和人格评估库建立。通过用MPI系统地评估LLM，我们提供了第一个证据，证明了LLM的个性。我们进一步设计了一种个性提示(Personality Prompting, P^2)方法，以可控的方式诱导LLMs具有特定的个性。

    Originating as a philosophical quest, the study of personality concerns how individuals differ in thinking, feeling, and behaving. Towards building social machines that work with humans on a daily basis, we are motivated to ask: Do existing Large Language Models (LLMs) possess personalities akin to their human counterparts? If so, how can we evaluate them? Further, given this evaluation framework, how can we induce a particular personality in a controllable fashion? To answer these three questions, we propose the Machine Personality Inventory (MPI) dataset for evaluating the machine personality; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence showing the existence of personality in LLMs. We further devise a Personality Prompting (P^2) method to induce LLMs with a specific personality in a controllable manner
    
[^75]: Speculative Decoding: 无损加速自回归翻译

    Speculative Decoding: Lossless Speedup of Autoregressive Translation. (arXiv:2203.16487v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.16487](http://arxiv.org/abs/2203.16487)

    Speculative Decoding是一种新型解码范式，结合了自回归翻译（AT）和非自回归翻译（NAT）的优势，提供了无损加速的翻译方法。在每个解码步骤中，它推测性地预测下一个标记，并使用验证模型确保翻译结果与AT完全相同。通过推测解码和验证的协作，实现了更快的解码速度，同时保持翻译质量不变。实验证明，原始的SpecDec与AT贪婪解码的结果完全相同。

    

    与之前一些牺牲翻译质量加速自回归翻译（AT）的工作不同，我们提出了Speculative Decoding（SpecDec）-一种受计算机体系结构中的推测执行启发的新型解码范式，它结合了AT和非自回归翻译（NAT）的各自优势，实现了在翻译过程中的无损加速。在每个解码步骤中，SpecDec首先使用NAT模型推测性地预测（即解码）下一个k个标记，然后使用AT模型验证这些标记，只有通过验证的预测标记才会被接受作为解码结果，以确保其翻译结果与AT完全相同。NAT的推测和AT的验证之间的协作使得解码速度大大提高，同时不损失翻译质量，这是由于推测解码所支持的并行计算。我们在4个标准WMT翻译基准上进行实验，并证实原始的SpecDec与AT贪婪解码的结果完全相同，速度提高了约 $k$倍。

    Different from some previous work accelerating autoregressive translation (AT) at the sacrifice of quality, we propose Speculative Decoding (SpecDec) -a novel decoding paradigm inspired by speculative execution in computer architecture, which combines respective advantages of AT and non-autoregressive translation (NAT) for lossless speedup of translation. At each decoding step, SpecDec first speculatively drafts (i.e. decodes) next $k$ tokens with an NAT model and then verifies them with an AT model, where only the drafted tokens passing the verification are accepted as decoded tokens for guaranteeing its translation result is exactly the same as AT. The collaboration of NAT drafting and AT verification leads to a much higher decoding speed without quality loss due to parallel computing enabled by speculative decoding.  We conduct experiments in 4 standard WMT translation benchmarks and confirm the vanilla SpecDec yields exactly the same results as AT greedy decoding with an around $
    
[^76]: 高度几何多模型混合用于鲁棒微调

    Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.03897](http://arxiv.org/abs/2203.03897)

    本文研究了CLIP模型的多模态嵌入质量，并发现其统一性和对齐性不足，限制了嵌入的传递性和鲁棒性。为了解决这个问题，我们提出了一种新的鲁棒微调方法，通过高度几何多模型混合生成难负样本，并对模型进行微调。

    

    预训练的多模型模型，如CLIP，在各种应用中提供可转移的嵌入，并显示出有希望的结果。然而，对学习到的多模型嵌入的分析相对较少，嵌入的可转移性有待改进。在这项工作中，我们观察到CLIP为两种不同的模态保留了分离的嵌入子空间，并通过统一对齐的视角对其进行了调查，以衡量学习表示的质量。理论上和实证上，我们展示了即使在微调之后，CLIP仍然保持着较差的统一性和对齐性。这种缺乏对齐和统一性可能限制了嵌入的传递性和鲁棒性。为此，我们设计了一种新的用于鲁棒表示的微调方法，提供更好的对齐和统一性。首先，我们提出了一种高度几何多模型混合方法，将图像和文本的嵌入混合在一起，在超球面上生成难负样本。然后，我们对模型进行鲁棒微调。

    Pre-trained multi-modal models, such as CLIP, provide transferable embeddings and show promising results in diverse applications. However, the analysis of learned multi-modal embeddings is relatively unexplored, and the embedding transferability can be improved. In this work, we observe that CLIP holds separated embedding subspaces for two different modalities, and then we investigate it through the lens of uniformity-alignment to measure the quality of learned representation. Both theoretically and empirically, we show that CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack of alignment and uniformity might restrict the transferability and robustness of embeddings. To this end, we devise a new fine-tuning method for robust representation equipping better alignment and uniformity. First, we propose a Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to generate hard negative samples on the hypersphere. Then, we fine-tune the model on har
    
[^77]: 知识增强预训练模型综述

    A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.00269](http://arxiv.org/abs/2110.00269)

    本综述提供了关于NLP中知识增强预训练语言模型的综合概述，讨论了预训练语言模型和知识表示学习的进展，并从三个不同的角度对现有的KEPLMs进行了分类，最后概述了未来研究中KEPLMs的潜在方向。

    

    预训练语言模型通过自监督学习在大规模文本语料库上学习了信息丰富的词表示，在细调之后在自然语言处理领域取得了有希望的性能。然而，这些模型存在鲁棒性差和可解释性不足的问题。我们将注入知识的预训练语言模型称为知识增强预训练语言模型(KEPLMs)。这些模型表现出深入理解和逻辑推理，并引入了可解释性。在本综述中，我们提供了关于NLP中KEPLMs的综合概述。我们首先讨论了预训练语言模型和知识表示学习的进展。然后，我们从三个不同的角度系统地分类了现有的KEPLMs。最后，我们概述了一些未来研究中KEPLMs的潜在方向。

    Pre-trained language models learn informative word representations on a large-scale text corpus through self-supervised learning, which has achieved promising performance in fields of natural language processing (NLP) after fine-tuning. These models, however, suffer from poor robustness and lack of interpretability. We refer to pre-trained language models with knowledge injection as knowledge-enhanced pre-trained language models (KEPLMs). These models demonstrate deep understanding and logical reasoning and introduce interpretability. In this survey, we provide a comprehensive overview of KEPLMs in NLP. We first discuss the advancements in pre-trained language models and knowledge representation learning. Then we systematically categorize existing KEPLMs from three different perspectives. Finally, we outline some potential directions of KEPLMs for future research.
    
[^78]: BSDAR: 基于注意力奖励的神经关键词生成中的束搜索解码

    BSDAR: Beam Search Decoding with Attention Reward in Neural Keyphrase Generation. (arXiv:1909.09485v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/1909.09485](http://arxiv.org/abs/1909.09485)

    本研究提出了一种基于注意力奖励的束搜索解码策略，用于解决神经关键词生成中的序列长度偏差和束多样性问题，该方法显著提高了生成关键词的解码性能。

    

    本研究主要研究神经关键词生成中的两个常见解码问题：序列长度偏差和束多样性。为了解决这些问题，我们引入了一种基于词级和ngram级奖励函数的束搜索解码策略，以在测试时约束和优化Seq2Seq推理过程。结果表明，我们简单的提案可以克服算法对较短和几乎相同的序列的偏好，从而显著提高生成源文本中存在和不存在的关键词的解码性能。

    This study mainly investigates two common decoding problems in neural keyphrase generation: sequence length bias and beam diversity. To tackle the problems, we introduce a beam search decoding strategy based on word-level and ngram-level reward function to constrain and refine Seq2Seq inference at test time. Results show that our simple proposal can overcome the algorithm bias to shorter and nearly identical sequences, resulting in a significant improvement of the decoding performance on generating keyphrases that are present and absent in source text.
    

