# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Trustworthy Explanation: On Causal Rationalization.](http://arxiv.org/abs/2306.14115) | 该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。 |
| [^2] | [Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models.](http://arxiv.org/abs/2306.14096) | 本文提出了一个用于企业预警的新型、广泛的中文细粒度金融情感分析数据集FinChina SA，并使用现有开源大语言模型对其进行评估和实验。该数据集将成为推进真实金融情感分析任务探索的宝贵资源。 |
| [^3] | [UAlberta at SemEval-2023 Task 1: Context Augmentation and Translation for Multilingual Visual Word Sense Disambiguation.](http://arxiv.org/abs/2306.14067) | Alberta大学团队在SemEval-2023视觉词义消歧任务中，采用上下文增强和翻译技术结合注释信息，提高准确性，获得较好的排名。 |
| [^4] | [DesCo: Learning Object Recognition with Rich Language Descriptions.](http://arxiv.org/abs/2306.14060) | DesCo是一种新的物体识别模型学习范式，旨在通过详尽的语言描述提高对新对象和域的适应性。 |
| [^5] | [Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step.](http://arxiv.org/abs/2306.14050) | 本研究提出了符号化思维链提炼（SCoTD）方法，该方法使用从大型教师模型中抽样出的合理化解释来训练数量级更小的学生模型，从而实现小模型也能受益于思维链提示，提升模型性能。 |
| [^6] | [Weighted Automata Extraction and Explanation of Recurrent Neural Networks for Natural Language Tasks.](http://arxiv.org/abs/2306.14040) | 本文提出了一种新的加权有限状态自动机（WFA）提取和解释框架来处理自然语言任务中的局限性，从而解决了精度和可扩展性方面的限制。 |
| [^7] | [My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks.](http://arxiv.org/abs/2306.14030) | 本研究针对资源匮乏的印度语言马拉地语，提出了一个大型混合马拉地语-英语语料库，以及预训练的混合BERT模型和针对混合语言下游任务的评估数据集，该语料库训练的模型显著优于现有的BERT模型。 |
| [^8] | [Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers.](http://arxiv.org/abs/2306.14003) | 本文提出了一个弱监督的科技论文多标签分类框架FUTEX，该框架利用跨论文网络结构和各投稿内部分章节的层次结构，解决了在细粒度标签空间中将论文分类为研究主题和主题，可能有多个；应利用全文来补充论文标题和摘要以进行分类等挑战。 |
| [^9] | [Large Language Models as Sous Chefs: Revising Recipes with GPT-3.](http://arxiv.org/abs/2306.13986) | 本研究使用大型语言模型，以菜谱为例，开发了一种提示，可以将菜谱分解为更简单的步骤。通过Amazon Mechanical Turk任务，研究人员发现注释员通常更喜欢修改后的菜谱，这为大型语言模型作为数字厨师等服务提供了有前途的应用。 |
| [^10] | [Towards Robust Aspect-based Sentiment Analysis through Non-counterfactual Augmentations.](http://arxiv.org/abs/2306.13971) | 本研究提出了一种非反事实数据增强的替代方法，该方法使用保留目标方面相关语义的有噪声、成本效益较高的数据增强，通过对数据的不同版本之间的不变性进行建模以提高其鲁棒性，在标准和强度特定数据集上都显著改进了强预训练基线的性能。 |
| [^11] | [Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents.](http://arxiv.org/abs/2306.13968) | 本文介绍了mTLDR数据集，这是一个多模态数据集，利用超复空间上融合多模态信号进行极端抽象文本摘要的任务。mTLDRgen模型成功实现了TL;DR生成的多模态系统。 |
| [^12] | [Emotion Flip Reasoning in Multiparty Conversations.](http://arxiv.org/abs/2306.13959) | 本文旨在识别说话者情感翻转背后的推动者，提出了一个数据集和神经架构c进行支持。 |
| [^13] | [Characterizing the Emotion Carriers of COVID-19 Misinformation and Their Impact on Vaccination Outcomes in India and the United States.](http://arxiv.org/abs/2306.13954) | 本研究对 COVID-19 谣言的情感载体及其对疫苗接种产生的影响在印度和美国进行了探讨，结果表明带有愤怒和恐惧等情感的谣言载体对疫苗接种结果有重要影响。 |
| [^14] | [Comparison of Pre-trained Language Models for Turkish Address Parsing.](http://arxiv.org/abs/2306.13947) | 本研究比较、评估了多语言和针对土耳其的BERT、DistilBERT、ELECTRA和RoBERTa模型在土耳其地址解析上的性能，结果发现针对土耳其的模型表现更佳。 |
| [^15] | [Unsupervised Mapping of Arguments of Deverbal Nouns to Their Corresponding Verbal Labels.](http://arxiv.org/abs/2306.13922) | 本文提出了一种非监督机制，基于上下文化的单词表示，将动名词的论据映射到相应动词构造的通用依存关系上，从而在不需要语义本体的情况下丰富了依存关系树。 |
| [^16] | [Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise?.](http://arxiv.org/abs/2306.13906) | 研究评估了GPT-4在分析法律文件中的表现，发现GPT-4在注释指南的提示下能够与习惯的注释人员表现相当，并且可以通过批处理预测实现成本的显著降低。此外，还展示了如何通过分析GPT-4的预测结果来改善模型性能。这些发现可以帮助法律文本领域的研究人员和从业者提高效率和准确性。 |
| [^17] | [Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis.](http://arxiv.org/abs/2306.13905) | 本文通过生成语言模型对语义轨迹进行分析和生成合成语义轨迹数据，为从城市规划到个性化推荐引擎和商业策略等一系列应用做出贡献。 |
| [^18] | [Math Word Problem Solving by Generating Linguistic Variants of Problem Statements.](http://arxiv.org/abs/2306.13899) | 该论文提出了一个通过生成问题文本语言变体的方法来解决数学问题，该方法利用DeBERTa作为编码器，同时引入了一个挑战性的数据集用于评估模型性能。结果表明，该框架在两个基准数据集以及作者提出的数据集上优于现有技术水平的模型，证明了其推导正确的解决方案表达式的能力。 |
| [^19] | [Estimating the Causal Effect of Early ArXiving on Paper Acceptance.](http://arxiv.org/abs/2306.13891) | 本研究使用数据和因果推断方法，研究了早期ArXiving论文对其被ICLR会议接受的影响。结果显示，早期ArXiving可能会对论文被接受的机会产生影响，但这种影响微乎其微，并且不因作者引用次数和机构排名等因素有所不同。 |
| [^20] | [L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models.](http://arxiv.org/abs/2306.13888) | 本论文介绍了 L3Cube-MahaSent-MD，这是一个多领域的 Marathi 情感分析数据集。在其中，针对每个领域，我们构建了包含 1.5 万个样本的子数据集。我们微调了不同的单语和多语 BERT 模型，并报告了 MahaBERT 模型的最佳准确性。我们的研究提出了利用低资源多领域数据集进行情感分析的需求。 |
| [^21] | [IERL: Interpretable Ensemble Representation Learning -- Combining CrowdSourced Knowledge and Distributed Semantic Representations.](http://arxiv.org/abs/2306.13865) | 该论文提出了一种利用众包知识与分布式语义表示相结合的方法，以产生可解释的集成表示，从而减少语言模型产生不一致性输出的问题。 |
| [^22] | [Is Pre-training Truly Better Than Meta-Learning?.](http://arxiv.org/abs/2306.13841) | 在少样本学习中，当数据集的正式多样性较低时，预训练模型（PT）胜过模型无关元学习（MAML）。当正式多样性较高时，MAML更好。 |
| [^23] | [Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data.](http://arxiv.org/abs/2306.13840) | 本论文提出使用多样性系数作为LLM预训练数据质量的指标，研究表明公开可用的LLM数据集的多样性系数很高。 |
| [^24] | [The Double Helix inside the NLP Transformer.](http://arxiv.org/abs/2306.13817) | 本文介绍了一个NLP Transformer中分析位置、句法、语义和上下文信息的框架，揭示了位置信息通过螺旋路径在深层中自我分离，并在编码器和解码器侧生成词性聚类。提出了替代在语义嵌入中添加位置信息的方法。 |
| [^25] | [Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers.](http://arxiv.org/abs/2306.13804) | 提出一种多模态双重注意力变换器（MDAT）模型，利用预训练模型进行多模态特征提取，通过引入图形注意和共同关注机制来捕捉不同情感的跨模态依赖，并使用最少的目标语言数据实现改进的跨语言情感识别结果。 |
| [^26] | [An analysis of vaccine-related sentiments from development to deployment of COVID-19 vaccines.](http://arxiv.org/abs/2306.13797) | 本文分析了全球范围内在COVID-19大流行期间推特上的情感态度，结果表明情感极性得分与病例数量及其变化之间存在联系。疫情的前半段情感极性得分发生了剧烈变化，后来趋于稳定，暗示疫苗的推广对于人们的讨论方向产生了影响。 |
| [^27] | [Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models.](http://arxiv.org/abs/2306.13789) | 本文提出了一种针对文本分类模型的数据重构攻击，称为Mix And Match攻击，该攻击利用了分类模型基于LLM的特点，该攻击已被证明在随机和有机测试数据集上是有效的。 |
| [^28] | [Resume Information Extraction via Post-OCR Text Processing.](http://arxiv.org/abs/2306.13775) | 本文提出了利用OCR后的文本处理和YOLOv8对象识别进行简历信息提取的方法，并成功优于以往基于NLP的模型对简历中教育和经验信息进行提取。 |
| [^29] | [The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios.](http://arxiv.org/abs/2306.13734) | CHiME-7 DASR 挑战赛旨在进行在远场环境下多设备远程会议转录的鲁棒语音识别，并在三种不同场景中评估系统性能。参与者可以使用开源预训练模型和数据集。 |
| [^30] | [Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems.](http://arxiv.org/abs/2306.13307) | 本研究旨在为Conformer Transducer语音识别系统建立高效紧凑的上下文表示，通过特定的注意力汇聚层实现跨话语的信息集成，取得了显著的性能提升。 |
| [^31] | [Explicit Syntactic Guidance for Neural Text Generation.](http://arxiv.org/abs/2306.11485) | 本研究提出了一种基于句法引导的文本生成模式，通过自上而下的形式结构树生成序列，在自回归基线中表现优异，具有可解释性、可控制性和多样性的优点。 |
| [^32] | [A novel Counterfactual method for aspect-based sentiment analysis.](http://arxiv.org/abs/2306.11260) | 本文提出了一种新颖的基于对照的数据增强方法，用于生成具有相反情感极性的观点表达，并利用T5模型检索掩码，该方法在三个ABSA数据集上表现优于当前增强方法。 |
| [^33] | [LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation.](http://arxiv.org/abs/2306.11222) | LoSparse 是一种新的语言模型压缩技术，通过低秩矩阵和稀疏矩阵逼近权重矩阵，结合了低秩逼近和裁剪的优点，可以显著降低语言模型大小和复杂度，并在多个自然语言任务中表现优异。 |
| [^34] | [Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models.](http://arxiv.org/abs/2306.09869) | 本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。 |
| [^35] | [Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models.](http://arxiv.org/abs/2306.08997) | 通过使用大型语言模型，翻译了MIT数学和EECS课程中的4550个题目，开发出一个可以自动评分的模型，并探索了课程、问题和答案之间的关系。 |
| [^36] | [MOFI: Learning Image Representations from Noisy Entity Annotated Images.](http://arxiv.org/abs/2306.07952) | MOFI 提出了一种新的方法，自动从含噪图像文本对中为图像指定实体标签，创建了一个新的大规模数据集 I2E，通过研究不同的训练配方，学习到了能够有效学习图像表示的模型。 |
| [^37] | [Exploring Hybrid Linguistic Features for Turkish Text Readability.](http://arxiv.org/abs/2306.03774) | 本文结合神经网络模型和各语言层面上的特征，开发出一种先进的土耳其文本可读性工具，发现了影响土耳其文本可读性的关键语言特征。 |
| [^38] | [ChatGPT Informed Graph Neural Network for Stock Movement Prediction.](http://arxiv.org/abs/2306.03763) | 该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。 |
| [^39] | [Scaling Evidence-based Instructional Design Expertise through Large Language Models.](http://arxiv.org/abs/2306.01006) | 本文探讨了如何利用大型语言模型扩展基于证据的教学设计专业知识，通过两个案例研究展示了GPT-4在教学设计中的应用，并提供了使用LLMs的最佳实践和未来LLMs为所有学习者提供个性化教育内容的愿景。 |
| [^40] | [SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL.](http://arxiv.org/abs/2306.00739) | 本文提出了一种基于大语言模型的Text-to-SQL模型SQL-PaLM，使用了面向Text-to-SQL的基于执行的自一致提示方法，在Spider上实现了77.3%的测试套件准确度，并显着超越以前的最新技术的方法。 |
| [^41] | [Machine Learning Approach for Cancer Entities Association and Classification.](http://arxiv.org/abs/2306.00013) | 本研究利用命名实体识别和文本分类的机器学习方法自动从大量生物医学文献中提取癌症相关实体和实体间关系，以帮助推进癌症研究进展。 |
| [^42] | [MemeGraphs: Linking Memes to Knowledge Graphs.](http://arxiv.org/abs/2305.18391) | 该论文提出了一种使用场景图和知识图谱结构化表达网络文化表情包的方法，并将其用于分类。结果显示该方法相比使用学习表达式的模型有所改善。 |
| [^43] | [Language Models are Bounded Pragmatic Speakers.](http://arxiv.org/abs/2305.17760) | 本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。 |
| [^44] | [SAIL: Search-Augmented Instruction Learning.](http://arxiv.org/abs/2305.15225) | 这篇论文提出了一种称为SAIL的搜索增强指令学习方法，该方法以内部和外部搜索引擎生成的搜索结果为基础，实现了语言生成和指令跟踪能力。通过构建一个包含指令、接地信息和响应三元组的新的搜索基础训练集来实现这一目标。在训练过程中，模型需要学习过滤干扰段落并生成目标响应。 |
| [^45] | [Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint.](http://arxiv.org/abs/2305.13599) | 本文提出了一种名为DR的灵活的方法，它通过不对称的学习率来解决由合作博弈引发的退化问题，该方法能够在两个基准测试中显著改善表现。 |
| [^46] | [Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network.](http://arxiv.org/abs/2305.12493) | 本研究引入了一个上下文短语预测网络用于基于注意力的深度偏置方法，通过计算偏置损失以帮助训练上下文化模型，在多种端到端语音识别模型上实现了显著的WER降低，相对于基线模型相对WER提高了12.1％，上下文短语的WER相对降低了40.5％。 |
| [^47] | [Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models.](http://arxiv.org/abs/2305.10276) | 本文提出了自然语言规划（NLP）的基准，旨在研究LLMs在需要理解并在文本中相应进行操作的复杂规划任务中的表现。同时提出了一种新方法CoS，使用简化的符号空间表示法来表示复杂的环境。 |
| [^48] | [Creative Data Generation: A Review Focusing on Text and Poetry.](http://arxiv.org/abs/2305.08493) | 本文研究了创造性数据的生成，特别关注了以诗歌为重点的自然语言生成。深入探讨了该领域面临的挑战和机遇。 |
| [^49] | [Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment.](http://arxiv.org/abs/2305.05940) | 针对跨语言ICL中无法对准输入输出空间的问题，我们提出了一种新的提示构建策略X-InSTA，可以同时对源语言和目标语言的语境进行编码和对齐，从而提高跨语言ICL的效率。 |
| [^50] | [A Framework for Designing Foundation Model based Systems.](http://arxiv.org/abs/2305.05352) | 本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。 |
| [^51] | [Beyond Classification: Financial Reasoning in State-of-the-Art Language Models.](http://arxiv.org/abs/2305.01505) | 本研究探讨了大语言模型在财务推理领域的潜在应用，对任务制定、数据生成、提示方法和评估能力等方面进行了详细研究，最终在各种数据集大小上对参数规模为2.8B至13B的各种GPT变体进行基准测试。 |
| [^52] | [Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks.](http://arxiv.org/abs/2304.14732) | 提出了一个名为SearChain的新型框架，以改进LLM生成的内容的准确性、可信度和可追溯性，从而提高复杂知识密集型任务的表现。SearChain通过深度集成LLM和信息检索（IR）实现，其思路是通过构造查询链，将多跳问题进行分解，最终指导LLM生成正确的答案。 |
| [^53] | [Improving Autoregressive NLP Tasks via Modular Linearized Attention.](http://arxiv.org/abs/2304.08453) | 本文提出模块化线性化注意力机制（MLA）以最大化推理质量并实现速度提升，并在多个自回归自然语言处理任务上验证了该方法，包括语音到文本神经机器翻译（S2T NMT）、语音到文本同声传译（SimulST）和自回归文本到频谱图任务，在TTS上具有高效率收益，在NMT和SimulST的训练和推理过程中表现出竞争性能。 |
| [^54] | [Emergence of Symbols in Neural Networks for Semantic Understanding and Communication.](http://arxiv.org/abs/2304.06377) | 本文介绍了一种名为SEA-net的神经网络解决方案，可以生成符号，实现语义理解和交流。这些符号可以捕捉到组成性语义信息，并呈现类似自然语言的内在结构。 |
| [^55] | [Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning.](http://arxiv.org/abs/2304.01295) | 本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。 |
| [^56] | [ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge.](http://arxiv.org/abs/2303.14070) | 本文介绍了一种在医学领域利用LLaMA模型微调的医疗聊天模型ChatDoctor。经过700多种疾病和其相应症状、药品和医疗检查的收集和处理，这种模型具有理解患者需求、提供建议和帮助的潜力。这些先进的语言模型集成到医疗保健中可以极大地改进医疗专业人员和患者的沟通方式。 |
| [^57] | [From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding.](http://arxiv.org/abs/2303.12816) | 本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。 |
| [^58] | [Learning Transductions and Alignments with RNN Seq2seq Models.](http://arxiv.org/abs/2303.06841) | 本文研究了RNN seq2seq模型在学习四种转换任务方面的能力，并发现其只能逼近符合训练或分布内数据的映射，不能学习底层函数；文章建立了一个新的复杂性层次结构，用于无注意力RNN seq2seq模型，而不是字符串转换的复杂性层次结构。 |
| [^59] | [SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks.](http://arxiv.org/abs/2302.13939) | 本论文提出了一种称之为SpikeGPT的生成语言模型，使用二进制、事件驱动脉冲激活单元进行训练，克服了SNN训练中的挑战性。该模型可以用于大规模语言生成任务。 |
| [^60] | [Sequential Query Encoding For Complex Query Answering on Knowledge Graphs.](http://arxiv.org/abs/2302.13114) | 本文提出一种名为SQE的序列查询编码方法，将复杂查询答案编码为一个序列，从而实现快速和强大的知识图谱推理。 |
| [^61] | [Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues.](http://arxiv.org/abs/2302.05895) | 该论文探讨了如何利用预训练语言模型和微调任务在对话中构建话语结构，提出了无监督和半监督方法，并在STAC语料库上取得了令人鼓舞的结果。 |
| [^62] | [Perceive and predict: self-supervised speech representation based loss functions for speech enhancement.](http://arxiv.org/abs/2301.04388) | 本文证明了干净和带噪声语音的特征编码之间的距离对语音质量和可懂度的评估有显著作用，使用此距离作为损失函数在语音增强方面效果优于传统方法。 |
| [^63] | [Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions.](http://arxiv.org/abs/2212.10189) | 本文探究了在知识库问答中的可回答性问题，使用新的GrailQAbility基准KBQA数据集，发现现有的KBQA模型在处理无法回答问题时性能下降，并对无法回答的检测存在问题，需要进一步研究来使KBQA系统对无法回答具有鲁棒性。 |
| [^64] | [Diverse Demonstrations Improve In-context Compositional Generalization.](http://arxiv.org/abs/2212.06800) | 本文提出了一种选择多样化演示来鼓励模型在组合泛化的测试集上表现良好的方法，并在三个组合泛化语义解析数据集中取得了显著的提高。 |
| [^65] | [Prompting PaLM for Translation: Assessing Strategies and Performance.](http://arxiv.org/abs/2211.09102) | 本文通过对PaLM的研究评估了少量样例提示翻译的各种策略，并发现样例的质量是最重要的因素。此外，研究表明PaLM的性能虽然令人印象深刻，但仍然落后于最先进的监督系统。 |
| [^66] | [Probing neural language models for understanding of words of estimative probability.](http://arxiv.org/abs/2211.03358) | 本研究探究了神经语言处理模型对于估计概率词语的理解能力，使用UNLI数据集和构建WEP数据集进行实验，发现语言模型在预测WEP的存在时很有效，但没有完全捕捉到与每个词相关联的共识概率水平。 |
| [^67] | [KSAT: Knowledge-infused Self Attention Transformer -- Integrating Multiple Domain-Specific Contexts.](http://arxiv.org/abs/2210.04307) | KSAT使用外部知识源引入知识引导偏见来整合多个领域特定上下文的自我关注结构。 |
| [^68] | [PROD: Progressive Distillation for Dense Retrieval.](http://arxiv.org/abs/2209.13335) | 本文提出了一种渐进式蒸馏方法PROD，用于密集检索，通过逐步改进学生模型来填补教师和学生之间的差距，并在五个基准数据集上取得了最先进的性能。 |
| [^69] | [A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach.](http://arxiv.org/abs/2207.11716) | 本文通过使用Transformer在U.S Patent Phrase to Phrase Matching Dataset上进行语义相似度分析，提高了算法效率，达到了令人满意的结果。 |
| [^70] | [DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles.](http://arxiv.org/abs/2207.01079) | 本文提出了一个新型挑战任务，即通过远程监督方式从科学文章中的表格中提取有关材料组成的信息。为此，研究者创建了一个包含4408个远程监督表格和1475个手动注释的开发和测试表格的训练数据集，并提出了一个强基线——DiSCoMaT。 |
| [^71] | [Language Models as Knowledge Embeddings.](http://arxiv.org/abs/2206.12617) | 该论文提出了一种使用语言模型来推导知识嵌入的方法LMKE，它旨在提高对丰富的长尾实体的表示能力并解决基于描述的先前方法的问题，实验结果表明该方法在多个基准数据集上实现了最先进的性能。 |
| [^72] | [Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis.](http://arxiv.org/abs/2110.13398) | 本论文提出了一种统一实例和知识对齐预训练框架，能够有效解决预训练和下游ABSA数据集之间的领域偏移问题，提高了基于方面的情感分析的性能，达到了最先进水平。 |
| [^73] | [Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering.](http://arxiv.org/abs/2110.01013) | 本文提出了一种新的模型无关的对抗样本合成和训练（CSST）策略，可以有效解决当前视觉问答模型的语言偏差问题，显著改善模型的性能并具备理想的可视化解释和问题敏感性。 |
| [^74] | [Speaker-change Aware CRF for Dialogue Act Classification.](http://arxiv.org/abs/2004.02913) | 本文提出了一种新的CRF模型，称为Speaker-change Aware CRF，以考虑对话行为分类中的说话人变化，实验结果表明其优于先前的方法。 |

# 详细

[^1]: 朝着可信的解释：因果关系解释论文研究

    Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])

    [http://arxiv.org/abs/2306.14115](http://arxiv.org/abs/2306.14115)

    该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。

    

    随着自然语言处理的最新进展，解释成为了通过选择输入文本的子集来解释黑盒模型中主要变化的一个基本的自我解释图。然而，现有的基于关联的解释方法在两个或多个片段高度互相关联时无法识别真正的解释，因此对预测准确性提供类似的贡献，所谓的虚假性。为了解决这一限制，我们从因果推断的角度新颖地将两个因果期望值（非虚假性和效率）引入了解释中。我们根据一种新提出的解释结构因果模型定义了一系列的因果概率，通过其理论鉴定，建立了必要和充分解释的主要组成部分。我们在真实世界的评论和医疗数据集上证明了所提出的因果关系解释的优越性能。

    With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
    
[^2]: 基于大语言模型的中文细粒度金融情感分析

    Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])

    [http://arxiv.org/abs/2306.14096](http://arxiv.org/abs/2306.14096)

    本文提出了一个用于企业预警的新型、广泛的中文细粒度金融情感分析数据集FinChina SA，并使用现有开源大语言模型对其进行评估和实验。该数据集将成为推进真实金融情感分析任务探索的宝贵资源。

    

    金融领域实体级别的细粒度情感分析是情感分析的重要子任务，目前面临着众多挑战。其中主要挑战之一来自于缺乏专门设计用于金融文本情感分析的高质量大规模标注语料库，这限制了开发有效文本处理技术所需的数据的可用性。大语言模型（LLMs）的最新进展在自然语言处理任务中取得了显著的性能，主要集中在语言模式匹配方面。在本文中，我们提出了一个新颖的、广泛的中文细粒度金融情感分析数据集FinChina SA，用于企业预警。我们对流行的现有开源LLMs使用我们的数据集进行了全面的评估和实验。我们坚信，我们的数据集将成为推动真实世界金融情感分析任务探索的宝贵资源。

    Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
    
[^3]: UAlberta在SemEval-2023任务1中:多语言视觉词义消歧的上下文增强和翻译。

    UAlberta at SemEval-2023 Task 1: Context Augmentation and Translation for Multilingual Visual Word Sense Disambiguation. (arXiv:2306.14067v1 [cs.CL])

    [http://arxiv.org/abs/2306.14067](http://arxiv.org/abs/2306.14067)

    Alberta大学团队在SemEval-2023视觉词义消歧任务中，采用上下文增强和翻译技术结合注释信息，提高准确性，获得较好的排名。

    

    我们描述了Alberta大学团队用于SemEval-2023视觉词义消歧（V-WSD）任务的系统。我们提出了一种新颖的算法，该算法利用从BabelNet检索到的注释与文本和图像编码器相结合。此外，我们将特定于语言的编码器与将英文编码器应用于翻译文本进行了比较。由于任务数据集中给出的上下文非常短，因此我们还尝试使用语言模型生成的描述来增强这些上下文。这显著提高了准确性。我们还描述并评估了使用图像生成和文本条件图像分割的其他V-WSD方法。总体而言，我们官方提交的结果排名第18位。我们的非官方结果甚至比官方结果好。我们的代码可在https://github.com/UAlberta-NLP/v-wsd中公开使用。

    We describe the systems of the University of Alberta team for the SemEval-2023 Visual Word Sense Disambiguation (V-WSD) Task. We present a novel algorithm that leverages glosses retrieved from BabelNet, in combination with text and image encoders. Furthermore, we compare language-specific encoders against the application of English encoders to translated texts. As the contexts given in the task datasets are extremely short, we also experiment with augmenting these contexts with descriptions generated by a language model. This yields substantial improvements in accuracy. We describe and evaluate additional V-WSD methods which use image generation and text-conditioned image segmentation. Overall, the results of our official submission rank us 18 out of 56 teams. Some of our unofficial results are even better than the official ones. Our code is publicly available at https://github.com/UAlberta-NLP/v-wsd.
    
[^4]: DesCo: 利用详尽的语言描述学习物体识别

    DesCo: Learning Object Recognition with Rich Language Descriptions. (arXiv:2306.14060v1 [cs.CV])

    [http://arxiv.org/abs/2306.14060](http://arxiv.org/abs/2306.14060)

    DesCo是一种新的物体识别模型学习范式，旨在通过详尽的语言描述提高对新对象和域的适应性。

    

    最近视觉语言方法的发展引起了学习视觉识别模型从语言监督的范式转变。这些方法将对象与语言查询（例如，“一张猫的照片”）对齐，并提高了模型识别新对象和域的适应性。最近，几项研究尝试使用包括属性，形状，纹理和关系等细粒度语义细节规范的复杂语言表达式查询这些模型。然而，仅仅将语言描述作为查询加入并不能保证模型对其进行精确解释。事实上，我们的实验表明，用于物体检测的视觉语言模型GLIP常常忽略语言描述中的上下文信息，而是过于依赖仅凭名称检测物体。为了解决这些挑战，我们提出了一种新的“描述条件（DesCo）”物体识别模型学习范式。

    Recent development in vision-language approaches has instigated a paradigm shift in learning visual recognition models from language supervision. These approaches align objects with language queries (e.g. "a photo of a cat") and improve the models' adaptability to identify novel objects and domains. Recently, several studies have attempted to query these models with complex language expressions that include specifications of fine-grained semantic details, such as attributes, shapes, textures, and relations. However, simply incorporating language descriptions as queries does not guarantee accurate interpretation by the models. In fact, our experiments show that GLIP, the state-of-the-art vision-language model for object detection, often disregards contextual information in the language descriptions and instead relies heavily on detecting objects solely by their names. To tackle the challenges, we propose a new description-conditioned (DesCo) paradigm of learning object recognition model
    
[^5]: 符号化思维链提炼：小模型也能逐步“思考”

    Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step. (arXiv:2306.14050v1 [cs.CL])

    [http://arxiv.org/abs/2306.14050](http://arxiv.org/abs/2306.14050)

    本研究提出了符号化思维链提炼（SCoTD）方法，该方法使用从大型教师模型中抽样出的合理化解释来训练数量级更小的学生模型，从而实现小模型也能受益于思维链提示，提升模型性能。

    

    思维链提示（例如“我们来逐步思考”）会促使大型语言模型对其预测进行合理化解释。虽然思维链可以带来显著的性能提升，但益处似乎仅适用于足够大的模型（超过50亿参数）。我们展示了数量级较小（125M-1.3B参数）的模型仍然可以从思维链提示中受益。为了实现这一点，我们引入了符号化思维链提炼（SCoTD），一种将较大的教师模型中抽样出的合理化解释用于训练较小的学生模型的方法。在几个常识基准测试上的实验表明：1）SCoTD提升了学生模型的性能，无论是在监督学习还是少样本学习的情况下，特别是在挑战集方面。2）从教师模型中抽样多个推理链是至关重要的。3）在提炼后，虽然参数少得多，但学生思维链与教师相当。

    Chain-of-thought prompting (e.g., "Let's think step-by-step") primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of
    
[^6]: 自然语言任务中的加权自动机提取与递归神经网络解释

    Weighted Automata Extraction and Explanation of Recurrent Neural Networks for Natural Language Tasks. (arXiv:2306.14040v1 [cs.CL])

    [http://arxiv.org/abs/2306.14040](http://arxiv.org/abs/2306.14040)

    本文提出了一种新的加权有限状态自动机（WFA）提取和解释框架来处理自然语言任务中的局限性，从而解决了精度和可扩展性方面的限制。

    

    循环神经网络（RNN）在处理序列数据方面取得了巨大成功，但理解和分析它们的行为仍然是一个重大挑战。为此，许多人致力于从RNN中提取有限自动机，这对于分析和解释更方便。然而，现有的方法如精确学习和组合方法在可扩展性或精度方面存在局限性。因此，我们提出了一种新的加权有限状态自动机（WFA）提取和解释框架来解决自然语言任务中的局限性。

    Recurrent Neural Networks (RNNs) have achieved tremendous success in processing sequential data, yet understanding and analyzing their behaviours remains a significant challenge. To this end, many efforts have been made to extract finite automata from RNNs, which are more amenable for analysis and explanation. However, existing approaches like exact learning and compositional approaches for model extraction have limitations in either scalability or precision. In this paper, we propose a novel framework of Weighted Finite Automata (WFA) extraction and explanation to tackle the limitations for natural language tasks. First, to address the transition sparsity and context loss problems we identified in WFA extraction for natural language tasks, we propose an empirical method to complement missing rules in the transition diagram, and adjust transition matrices to enhance the context-awareness of the WFA. We also propose two data augmentation tactics to track more dynamic behaviours of RNN, 
    
[^7]: My Boli：混合马拉地语-英语的语料库、预训练语言模型和评估基准

    My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v1 [cs.CL])

    [http://arxiv.org/abs/2306.14030](http://arxiv.org/abs/2306.14030)

    本研究针对资源匮乏的印度语言马拉地语，提出了一个大型混合马拉地语-英语语料库，以及预训练的混合BERT模型和针对混合语言下游任务的评估数据集，该语料库训练的模型显著优于现有的BERT模型。

    

    由于缺乏专门的混合语料库和预训练语言模型，混合语言数据的研究受到了限制。在这项工作中，我们关注资源匮乏的印度语言马拉地语，这个语言之前没有任何混合语言的研究。我们提出了L3Cube-MeCorpus，一个包含500万条推特的大型混合马拉地语-英语(Mr-En)语料库，用于预训练。我们还发布了L3Cube-MeBERT和MeRoBERTa，基于BERT的混合模型，在MeCorpus上预训练。此外，为了基准测试，我们提供了三个有监督的数据集MeHate、MeSent和MeLID，用于混合Mr-En仇恨言论检测、情感分析和语言识别等下游任务。这些评估数据集分别包含手动注释的\url{~}12,000条马拉地语-英语混合推特。削减实验表明，这个新语料库训练的模型显著优于现有的BERT模型。这是第一个提供混合马拉地语的代码的工作。

    The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models. In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 5 million tweets for pretraining. We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus. Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like code-mixed Mr-En hate speech detection, sentiment analysis, and language identification respectively. These evaluation datasets individually consist of manually annotated \url{~}12,000 Marathi-English code-mixed tweets. Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models. This is the first work that presents artifacts for code-mix
    
[^8]: 全文科技论文的弱监督多标签分类

    Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers. (arXiv:2306.14003v1 [cs.CL])

    [http://arxiv.org/abs/2306.14003](http://arxiv.org/abs/2306.14003)

    本文提出了一个弱监督的科技论文多标签分类框架FUTEX，该框架利用跨论文网络结构和各投稿内部分章节的层次结构，解决了在细粒度标签空间中将论文分类为研究主题和主题，可能有多个；应利用全文来补充论文标题和摘要以进行分类等挑战。

    

    弱监督的科技论文分类依赖于分类描述而非人工标注样本建立分类器。已有的弱监督分类研究较少考虑到两个挑战：(1)在细粒度标签空间中将论文分类为研究主题和主题，可能有多个; (2)应利用全文来补充论文标题和摘要以进行分类。此外，应利用跨论文网络结构和各论文内部分章节的层次结构等结构信息。为解决这些挑战，本研究提出了FUTEX，该框架使用跨论文网络结构和各投稿内部分章节的层次结构。

    Instead of relying on human-annotated training samples to build a classifier, weakly supervised scientific paper classification aims to classify papers only using category descriptions (e.g., category names, category-indicative keywords). Existing studies on weakly supervised paper classification are less concerned with two challenges: (1) Papers should be classified into not only coarse-grained research topics but also fine-grained themes, and potentially into multiple themes, given a large and fine-grained label space; and (2) full text should be utilized to complement the paper title and abstract for classification. Moreover, instead of viewing the entire paper as a long linear sequence, one should exploit the structural information such as citation links across papers and the hierarchy of sections and paragraphs in each paper. To tackle these challenges, in this study, we propose FUTEX, a framework that uses the cross-paper network structure and the in-paper hierarchy structure to 
    
[^9]: 大型语言模型作为厨师助理：使用GPT-3修改菜谱

    Large Language Models as Sous Chefs: Revising Recipes with GPT-3. (arXiv:2306.13986v1 [cs.CL])

    [http://arxiv.org/abs/2306.13986](http://arxiv.org/abs/2306.13986)

    本研究使用大型语言模型，以菜谱为例，开发了一种提示，可以将菜谱分解为更简单的步骤。通过Amazon Mechanical Turk任务，研究人员发现注释员通常更喜欢修改后的菜谱，这为大型语言模型作为数字厨师等服务提供了有前途的应用。

    

    大型语言模型具有极大的文本生成和提示功能，可以将现有的书面信息调整为更易于使用和理解的形式。本文以菜谱为例，展示了这一功能的应用，我们开发了一种基于原始菜谱和配料清单的提示，将菜谱分解为更简单的步骤，并将其应用到来自各种世界美食的菜谱中。我们使用了多种大型语言模型，并发现GPT-3.5的结果最好。我们还为公众提供了我们的提示、代码和MTurk模板。

    With their remarkably improved text generation and prompting capabilities, large language models can adapt existing written information into forms that are easier to use and understand. In our work, we focus on recipes as an example of complex, diverse, and widely used instructions. We develop a prompt grounded in the original recipe and ingredients list that breaks recipes down into simpler steps. We apply this prompt to recipes from various world cuisines, and experiment with several large language models (LLMs), finding best results with GPT-3.5. We also contribute an Amazon Mechanical Turk task that is carefully designed to reduce fatigue while collecting human judgment of the quality of recipe revisions. We find that annotators usually prefer the revision over the original, demonstrating a promising application of LLMs in serving as digital sous chefs for recipes and beyond. We release our prompt, code, and MTurk template for public use.
    
[^10]: 非反事实增强在强健性方面对基于方面的情感分析的改进

    Towards Robust Aspect-based Sentiment Analysis through Non-counterfactual Augmentations. (arXiv:2306.13971v1 [cs.CL])

    [http://arxiv.org/abs/2306.13971](http://arxiv.org/abs/2306.13971)

    本研究提出了一种非反事实数据增强的替代方法，该方法使用保留目标方面相关语义的有噪声、成本效益较高的数据增强，通过对数据的不同版本之间的不变性进行建模以提高其鲁棒性，在标准和强度特定数据集上都显著改进了强预训练基线的性能。

    

    尽管最先进的自然语言处理模型在基于方面的情感分析（ABSA）方面表现出色，但有大量证据表明它们缺乏鲁棒性。特别是在面对分布外数据时，其性能显著降低。最近的解决方案依赖于反事实增强数据集展现了良好的结果，但由于无法访问显式的因果结构，它们本质上是有限的。在本文中，我们提出了一种替代方法，该方法依赖于非反事实数据增强。我们的提议依赖于使用带有噪声的，成本效益较高的数据增强来保留与目标方面相关联的语义。然后，我们的方法依赖于对数据的不同版本之间的不变性进行建模，从而提高其鲁棒性。一组全面的实验表明，我们的提议在标准和强度特定数据集上都显著改进了强预训练基线的性能。

    While state-of-the-art NLP models have demonstrated excellent performance for aspect based sentiment analysis (ABSA), substantial evidence has been presented on their lack of robustness. This is especially manifested as significant degradation in performance when faced with out-of-distribution data. Recent solutions that rely on counterfactually augmented datasets show promising results, but they are inherently limited because of the lack of access to explicit causal structure. In this paper, we present an alternative approach that relies on non-counterfactual data augmentation. Our proposal instead relies on using noisy, cost-efficient data augmentations that preserve semantics associated with the target aspect. Our approach then relies on modelling invariances between different versions of the data to improve robustness. A comprehensive suite of experiments shows that our proposal significantly improves upon strong pre-trained baselines on both standard and robustness-specific datase
    
[^11]: 利用超复空间上融合多模态信号进行极端抽象文本摘要

    Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents. (arXiv:2306.13968v1 [cs.CL])

    [http://arxiv.org/abs/2306.13968](http://arxiv.org/abs/2306.13968)

    本文介绍了mTLDR数据集，这是一个多模态数据集，利用超复空间上融合多模态信号进行极端抽象文本摘要的任务。mTLDRgen模型成功实现了TL;DR生成的多模态系统。

    

    在已有注释摘要和丰富数据的基础上，科学文本摘要领域已经取得了显著的进展。然而，多个输入模态（例如视频和音频）的利用尚未得到彻底探索。目前，科学多模态输入的文本摘要系统往往采用较长的目标摘要，如摘要，在文本摘要任务中表现不佳。在本文中，我们通过利用多个输入模态来处理极端抽象文本摘要（即TL;DR生成）的新任务。为此，我们介绍了mTLDR，这是一个首创的数据集，包括视频、音频和文本，以及作者撰写的摘要和专家注释的摘要。mTLDR数据集搜集了来自不同学术会议记录总共4182个实例，如ICLR、ACL和CVPR。随后，我们提出了mTLDRgen，这是一种适用于超复杂空间上极端抽象摘要的编码解码深度神经网络架构。我们在mTLDR数据集上获得了显著的结果，并报告了首个成功实施TL;DR生成的多模态系统的结果。

    The realm of scientific text summarization has experienced remarkable progress due to the availability of annotated brief summaries and ample data. However, the utilization of multiple input modalities, such as videos and audio, has yet to be thoroughly explored. At present, scientific multimodal-input-based text summarization systems tend to employ longer target summaries like abstracts, leading to an underwhelming performance in the task of text summarization.  In this paper, we deal with a novel task of extreme abstractive text summarization (aka TL;DR generation) by leveraging multiple input modalities. To this end, we introduce mTLDR, a first-of-its-kind dataset for the aforementioned task, comprising videos, audio, and text, along with both author-composed summaries and expert-annotated summaries. The mTLDR dataset accompanies a total of 4,182 instances collected from various academic conference proceedings, such as ICLR, ACL, and CVPR. Subsequently, we present mTLDRgen, an encod
    
[^12]: 多方会话中的情感反转推理

    Emotion Flip Reasoning in Multiparty Conversations. (arXiv:2306.13959v1 [cs.CL])

    [http://arxiv.org/abs/2306.13959](http://arxiv.org/abs/2306.13959)

    本文旨在识别说话者情感翻转背后的推动者，提出了一个数据集和神经架构c进行支持。

    

    在对话中，说话者可能有不同的情感状态，它们的动态在理解情感话语中起着重要作用。然而，仅仅检测情感并不足以完全理解会话过程中发生的说话者特定情感变化。为了高效地理解说话者的情感动态，有必要确定导致说话者情感翻转的原因或推动者。在本文中，我们探讨了名为Instigator based Emotion Flip Reasoning (EFR) 的任务，旨在识别会话中说话者情感翻转背后的推动者。例如，从快乐到愤怒的情感翻转可能是由威胁这样的推动者引起的。为了实现这个任务，我们提出了一个包括符合情感心理学标准的EFR推动者标签的数据集MELD-I。为了评估数据集，我们提出了一个新颖的神经架构c。

    In a conversational dialogue, speakers may have different emotional states and their dynamics play an important role in understanding dialogue's emotional discourse. However, simply detecting emotions is not sufficient to entirely comprehend the speaker-specific changes in emotion that occur during a conversation. To understand the emotional dynamics of speakers in an efficient manner, it is imperative to identify the rationale or instigator behind any changes or flips in emotion expressed by the speaker. In this paper, we explore the task called Instigator based Emotion Flip Reasoning (EFR), which aims to identify the instigator behind a speaker's emotion flip within a conversation. For example, an emotion flip from joy to anger could be caused by an instigator like threat. To facilitate this task, we present MELD-I, a dataset that includes ground-truth EFR instigator labels, which are in line with emotional psychology. To evaluate the dataset, we propose a novel neural architecture c
    
[^13]: 揭示COVID-19谣言的情感载体以及它们对印度和美国疫苗接种的影响

    Characterizing the Emotion Carriers of COVID-19 Misinformation and Their Impact on Vaccination Outcomes in India and the United States. (arXiv:2306.13954v1 [cs.CL])

    [http://arxiv.org/abs/2306.13954](http://arxiv.org/abs/2306.13954)

    本研究对 COVID-19 谣言的情感载体及其对疫苗接种产生的影响在印度和美国进行了探讨，结果表明带有愤怒和恐惧等情感的谣言载体对疫苗接种结果有重要影响。

    

    COVID-19 Infodemic对全球的健康行为和结果产生了前所未有的影响。尽管许多研究侧重于理解谣言的定性和定量方面，包括情感分析，但对谣言的情感载体及其在地理上的差异的理解存在差距。在本研究中，我们对情感载体及其对印度和美国的疫苗接种率的影响进行了表征。从230万条推文中创建了手动标记的数据集，并与三个公开可用的数据集（CoAID，AntiVax，CMU）进行整合，以训练深度学习模型以进行谣言分类。利用Plutchik Transformers分析了谣言标记推文的行为方面，以确定每个推文的情感。进行了时间序列分析以研究谣言对空间和时间特征的影响。此外，使用transformed features进行分类以了解与谣言相关的情感。我们的结果表明，带有愤怒和恐惧等情感的谣言载体对美国和印度的疫苗接种结果产生了显著影响。本研究提供了有关COVID-19谣言情感方面及其对健康行为的影响的见解。

    The COVID-19 Infodemic had an unprecedented impact on health behaviors and outcomes at a global scale. While many studies have focused on a qualitative and quantitative understanding of misinformation, including sentiment analysis, there is a gap in understanding the emotion-carriers of misinformation and their differences across geographies. In this study, we characterized emotion carriers and their impact on vaccination rates in India and the United States. A manually labelled dataset was created from 2.3 million tweets and collated with three publicly available datasets (CoAID, AntiVax, CMU) to train deep learning models for misinformation classification. Misinformation labelled tweets were further analyzed for behavioral aspects by leveraging Plutchik Transformers to determine the emotion for each tweet. Time series analysis was conducted to study the impact of misinformation on spatial and temporal characteristics. Further, categorical classification was performed using transforme
    
[^14]: 面向土耳其地址解析的预训练语言模型比较研究

    Comparison of Pre-trained Language Models for Turkish Address Parsing. (arXiv:2306.13947v1 [cs.CL])

    [http://arxiv.org/abs/2306.13947](http://arxiv.org/abs/2306.13947)

    本研究比较、评估了多语言和针对土耳其的BERT、DistilBERT、ELECTRA和RoBERTa模型在土耳其地址解析上的性能，结果发现针对土耳其的模型表现更佳。

    

    基于Transformer的预训练模型，如BERT及其变种，通过在大型语料库上的训练，在自然语言处理（NLP）任务中取得了巨大的成功。大多数学术研究都是基于英语进行的;然而，多语言和特定语言的研究数量正在稳步增加。此外，一些研究声称，针对特定语言的模型在各种任务中优于多语言模型。因此，社区倾向于针对其案例研究的语言来训练或微调模型。本文针对土耳其地图数据，全面评估了多语言和土耳其BERT、DistilBERT、ELECTRA和RoBERTa。此外，我们还提出了一个多层感知器（MLP）用于微调BERT，以及标准的一层微调方法。对于数据集，我们构建了一个质量相对较高的中等规模地址解析语料库。在这个数据集上进行的实验表明，

    Transformer based pre-trained models such as BERT and its variants, which are trained on large corpora, have demonstrated tremendous success for natural language processing (NLP) tasks. Most of academic works are based on the English language; however, the number of multilingual and language specific studies increase steadily. Furthermore, several studies claimed that language specific models outperform multilingual models in various tasks. Therefore, the community tends to train or fine-tune the models for the language of their case study, specifically. In this paper, we focus on Turkish maps data and thoroughly evaluate both multilingual and Turkish based BERT, DistilBERT, ELECTRA and RoBERTa. Besides, we also propose a MultiLayer Perceptron (MLP) for fine-tuning BERT in addition to the standard approach of one-layer fine-tuning. For the dataset, a mid-sized Address Parsing corpus taken with a relatively high quality is constructed. Conducted experiments on this dataset indicate that
    
[^15]: 非监督地将动名词的论据映射到相应的动词标签上

    Unsupervised Mapping of Arguments of Deverbal Nouns to Their Corresponding Verbal Labels. (arXiv:2306.13922v1 [cs.CL])

    [http://arxiv.org/abs/2306.13922](http://arxiv.org/abs/2306.13922)

    本文提出了一种非监督机制，基于上下文化的单词表示，将动名词的论据映射到相应动词构造的通用依存关系上，从而在不需要语义本体的情况下丰富了依存关系树。

    

    动名词是常用的用于描述事件或行为及其论据的动词名词化形式。然而，许多NLP系统，特别是基于模式的系统，忽略了处理这种名词化的结构。而针对名词化结构的论据的解决方案基于语义注释，并需要语义本体，使得它们的应用仅限于一小部分名词。我们提出采用更加句法化的方式，将动名词的论据映射到相应动词构造的通用依存关系上。我们提出了一种非监督机制，基于上下文化的单词表示，它允许使用与相应动词情况相同的标签，将通用依存树丰富了描述动名词的论据的依赖关系弧。通过与动词情况相同的标签集共享，已经针对动词开发的模式也可以应用于动名词。

    Deverbal nouns are nominal forms of verbs commonly used in written English texts to describe events or actions, as well as their arguments. However, many NLP systems, and in particular pattern-based ones, neglect to handle such nominalized constructions. The solutions that do exist for handling arguments of nominalized constructions are based on semantic annotation and require semantic ontologies, making their applications restricted to a small set of nouns. We propose to adopt instead a more syntactic approach, which maps the arguments of deverbal nouns to the universal-dependency relations of the corresponding verbal construction. We present an unsupervised mechanism -based on contextualized word representations -- which allows to enrich universal-dependency trees with dependency arcs denoting arguments of deverbal nouns, using the same labels as the corresponding verbal cases. By sharing the same label set as in the verbal case, patterns that were developed for verbs can be applie
    
[^16]: GPT-4能够在需要高度专业领域知识的任务中支持文本数据分析吗？

    Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise?. (arXiv:2306.13906v1 [cs.CL])

    [http://arxiv.org/abs/2306.13906](http://arxiv.org/abs/2306.13906)

    研究评估了GPT-4在分析法律文件中的表现，发现GPT-4在注释指南的提示下能够与习惯的注释人员表现相当，并且可以通过批处理预测实现成本的显著降低。此外，还展示了如何通过分析GPT-4的预测结果来改善模型性能。这些发现可以帮助法律文本领域的研究人员和从业者提高效率和准确性。

    

    我们评估了生成式预训练转换器（GPT-4）在需要高度专业领域知识的任务中分析文本数据的能力。具体而言，我们专注于分析法院意见以解释法律概念的任务。我们发现，GPT-4在注释指南的提示下与训练有素的法律学生注释者的表现相当。我们观察到，通过批处理预测，可以在相对较小的性能损失下实现成本的显著降低。然而，采用连续思考提示并没有显著提高这项任务的性能。此外，我们演示了如何分析GPT-4的预测结果以识别和减轻注释指南的缺陷，并随后改善模型的性能。最后，我们观察到该模型非常脆弱，因为提示中的小型格式相关更改对预测结果产生了很大的影响。这些发现可以被法律文本分析领域的研究人员和从业者利用，以提高他们工作的效率和准确性。

    We evaluated the capability of generative pre-trained transformers~(GPT-4) in analysis of textual data in tasks that require highly specialized domain expertise. Specifically, we focused on the task of analyzing court opinions to interpret legal concepts. We found that GPT-4, prompted with annotation guidelines, performs on par with well-trained law student annotators. We observed that, with a relatively minor decrease in performance, GPT-4 can perform batch predictions leading to significant cost reductions. However, employing chain-of-thought prompting did not lead to noticeably improved performance on this task. Further, we demonstrated how to analyze GPT-4's predictions to identify and mitigate deficiencies in annotation guidelines, and subsequently improve the performance of the model. Finally, we observed that the model is quite brittle, as small formatting related changes in the prompt had a high impact on the predictions. These findings can be leveraged by researchers and pract
    
[^17]: 利用生成模型进行语义轨迹分析的时空叙事？

    Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis. (arXiv:2306.13905v1 [cs.CL])

    [http://arxiv.org/abs/2306.13905](http://arxiv.org/abs/2306.13905)

    本文通过生成语言模型对语义轨迹进行分析和生成合成语义轨迹数据，为从城市规划到个性化推荐引擎和商业策略等一系列应用做出贡献。

    

    本文提出了一种使用生成语言模型对语义轨迹跟踪进行分析和生成合成语义轨迹数据（SST）的愿景。利用深度学习的进步，如自然语言处理（NLP）、计算机视觉等领域的进展，我们旨在创建智能模型，可以研究不同上下文中的语义轨迹，预测未来趋势，增强机器对动物、人类、货物等移动情况的理解，提高人机交互，并为从城市规划到个性化推荐引擎和商业策略等一系列应用做出贡献。

    In this paper, we lay out a vision for analysing semantic trajectory traces and generating synthetic semantic trajectory data (SSTs) using generative language model. Leveraging the advancements in deep learning, as evident by progress in the field of natural language processing (NLP), computer vision, etc. we intend to create intelligent models that can study the semantic trajectories in various contexts, predicting future trends, increasing machine understanding of the movement of animals, humans, goods, etc. enhancing human-computer interactions, and contributing to an array of applications ranging from urban-planning to personalized recommendation engines and business strategy.
    
[^18]: 通过生成问题陈述的语言变体解决数学问题

    Math Word Problem Solving by Generating Linguistic Variants of Problem Statements. (arXiv:2306.13899v1 [cs.CL])

    [http://arxiv.org/abs/2306.13899](http://arxiv.org/abs/2306.13899)

    该论文提出了一个通过生成问题文本语言变体的方法来解决数学问题，该方法利用DeBERTa作为编码器，同时引入了一个挑战性的数据集用于评估模型性能。结果表明，该框架在两个基准数据集以及作者提出的数据集上优于现有技术水平的模型，证明了其推导正确的解决方案表达式的能力。

    

    数学推理艺术是智力进展的基本支柱，是培养人类独创性的核心催化剂。最近，研究人员已发表了大量围绕解决数学语言问题（MWP）的作品，这是迈向通用AI的重要步骤。这些现有模型容易依赖于肤浅的启发式和虚假的相关性来推导解决方案表达式。为了改善这一问题，在本文中，我们提出了一个基于生成问题文本语言变体的MWP求解器框架。该方法涉及解决每个不同变体的问题并选择得票最多的预测表达式。我们使用DeBERTa（具有解码增强的BERT和分离注意力）作为编码器，以利用其丰富的文本表示和增强的遮罩解码器来构造解决方案表达式。此外，我们引入了一个具有挑战性的数据集$\mathrm{P\small{ARA}\normalsize}_\mathrm{gen}$-MWP，以评估模型在生成和解析变体问题方面的性能。我们的结果显示，所提出的框架在两个基准数据集和我们提出的$\mathrm{P\small{ARA}\normalsize}_\mathrm{gen}$-MWP上都优于现有技术水平的模型，证明了其通过理解和推理问题文本的语义细微差别来推导正确的解决方案表达式的能力。

    The art of mathematical reasoning stands as a fundamental pillar of intellectual progress and is a central catalyst in cultivating human ingenuity. Researchers have recently published a plethora of works centered around the task of solving Math Word Problems (MWP) $-$ a crucial stride towards general AI. These existing models are susceptible to dependency on shallow heuristics and spurious correlations to derive the solution expressions. In order to ameliorate this issue, in this paper, we propose a framework for MWP solvers based on the generation of linguistic variants of the problem text. The approach involves solving each of the variant problems and electing the predicted expression with the majority of the votes. We use DeBERTa (Decoding-enhanced BERT with disentangled attention) as the encoder to leverage its rich textual representations and enhanced mask decoder to construct the solution expressions. Furthermore, we introduce a challenging dataset, $\mathrm{P\small{ARA}\normalsi
    
[^19]: 早期ArXiving对论文被接受的因果影响估计

    Estimating the Causal Effect of Early ArXiving on Paper Acceptance. (arXiv:2306.13891v1 [cs.CL])

    [http://arxiv.org/abs/2306.13891](http://arxiv.org/abs/2306.13891)

    本研究使用数据和因果推断方法，研究了早期ArXiving论文对其被ICLR会议接受的影响。结果显示，早期ArXiving可能会对论文被接受的机会产生影响，但这种影响微乎其微，并且不因作者引用次数和机构排名等因素有所不同。

    

    在论文提交同行审查前发布预印本会产生什么影响？由于没有进行随机对照试验，因此我们需要利用观察数据来回答这个问题。我们使用了ICLR会议（2018-2022）的数据，并应用因果推断方法来估计在审阅期前删除论文（早期arXiving）对论文被会议接受的影响。调整了18个混杂因素，如主题、作者和质量，我们可以得出因果效应的估计值。然而，由于质量是一种难以估计的构建，因此我们使用负面结果控制方法，将论文引用次数作为对照变量，以消除质量混杂效应。我们的结果表明，早期arXiving可能会对论文被接受的机会产生一定程度的影响。然而，当存在影响时，这种影响在作者引用次数和机构排名分组后并没有显著差异。这表明早期arXiving对ICLR会议接受论文的影响微乎其微。

    What is the effect of releasing a preprint of a paper before it is submitted for peer review? No randomized controlled trial has been conducted, so we turn to observational data to answer this question. We use data from the ICLR conference (2018--2022) and apply methods from causal inference to estimate the effect of arXiving a paper before the reviewing period (early arXiving) on its acceptance to the conference. Adjusting for 18 confounders such as topic, authors, and quality, we may estimate the causal effect. However, since quality is a challenging construct to estimate, we use the negative outcome control method, using paper citation count as a control variable to debias the quality confounding effect. Our results suggest that early arXiving may have a small effect on a paper's chances of acceptance. However, this effect (when existing) does not differ significantly across different groups of authors, as grouped by author citation count and institute rank. This suggests that early
    
[^20]: L3Cube-MahaSent-MD：一种多域 Marathi 情感分析数据集和 Transformer 模型

    L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models. (arXiv:2306.13888v1 [cs.CL])

    [http://arxiv.org/abs/2306.13888](http://arxiv.org/abs/2306.13888)

    本论文介绍了 L3Cube-MahaSent-MD，这是一个多领域的 Marathi 情感分析数据集。在其中，针对每个领域，我们构建了包含 1.5 万个样本的子数据集。我们微调了不同的单语和多语 BERT 模型，并报告了 MahaBERT 模型的最佳准确性。我们的研究提出了利用低资源多领域数据集进行情感分析的需求。

    

    在低资源语言（如 Marathi）中进行情感分析一直受到相应数据集的限制。本文提出了 L3Cube-MahaSent-MD，它是一个多领域 Marathi 情感分析数据集，包括电影评论、普通推文、电视节目字幕和政治推文等四个不同领域。该数据集包含约 6 万个手动标记的样本，覆盖了三种不同情感 - 正面、负面和中性。我们针对每个领域创建了一个子数据集，每个子数据集包含 1.5 万个样本。这是 Indic 情感领域内的第一个全面的多领域情感分析数据集。我们针对这些数据集微调了不同的单语和多语 BERT 模型，并报告了 MahaBERT 模型的最佳准确性。我们还进行了广泛的领域内和领域间分析，从而凸显了低资源多域数据集的需求。数据集和模型均可在 https://github.com/l3cube-pune/MarathiN 获取。

    The exploration of sentiment analysis in low-resource languages, such as Marathi, has been limited due to the availability of suitable datasets. In this work, we present L3Cube-MahaSent-MD, a multi-domain Marathi sentiment analysis dataset, with four different domains - movie reviews, general tweets, TV show subtitles, and political tweets. The dataset consists of around 60,000 manually tagged samples covering 3 distinct sentiments - positive, negative, and neutral. We create a sub-dataset for each domain comprising 15k samples. The MahaSent-MD is the first comprehensive multi-domain sentiment analysis dataset within the Indic sentiment landscape. We fine-tune different monolingual and multilingual BERT models on these datasets and report the best accuracy with the MahaBERT model. We also present an extensive in-domain and cross-domain analysis thus highlighting the need for low-resource multi-domain datasets. The data and models are available at https://github.com/l3cube-pune/MarathiN
    
[^21]: IERL: 可解释的集成表示学习--结合众包知识与分布式语义表示

    IERL: Interpretable Ensemble Representation Learning -- Combining CrowdSourced Knowledge and Distributed Semantic Representations. (arXiv:2306.13865v1 [cs.CL])

    [http://arxiv.org/abs/2306.13865](http://arxiv.org/abs/2306.13865)

    该论文提出了一种利用众包知识与分布式语义表示相结合的方法，以产生可解释的集成表示，从而减少语言模型产生不一致性输出的问题。

    

    大型语言模型（LLMs）以分布式语义的形式编码单词的含义。分布式语义从大量数据中捕捉语言标记（单词、短语和句子）之间的共同统计模式。LLMs在用于测试模型对输入标记的含义理解的通用语言理解评估（GLUE）任务中表现卓越。然而，最近的研究表明，当处理很少在训练中出现过的输入，或与不同语境相关的输入（例如，在语言生成任务中出现的已知幻觉现象）时，LLMs往往会产生意外、不一致或错误的文本输出。众包和专家策划的知识图谱（如ConceptNet）旨在从一组紧密定义的上下文中捕捉单词的含义。因此，LLMs可以受益于利用这样的知识上下文来减少输出中的不一致性。我们提出了一种新颖的集成学习方法，将众包知识与LLMs的分布式语义表示相结合，以产生可解释的集成表示。

    Large Language Models (LLMs) encode meanings of words in the form of distributed semantics. Distributed semantics capture common statistical patterns among language tokens (words, phrases, and sentences) from large amounts of data. LLMs perform exceedingly well across General Language Understanding Evaluation (GLUE) tasks designed to test a model's understanding of the meanings of the input tokens. However, recent studies have shown that LLMs tend to generate unintended, inconsistent, or wrong texts as outputs when processing inputs that were seen rarely during training, or inputs that are associated with diverse contexts (e.g., well-known hallucination phenomenon in language generation tasks). Crowdsourced and expert-curated knowledge graphs such as ConceptNet are designed to capture the meaning of words from a compact set of well-defined contexts. Thus LLMs may benefit from leveraging such knowledge contexts to reduce inconsistencies in outputs. We propose a novel ensemble learning m
    
[^22]: 预训练真的比元学习更好吗？

    Is Pre-training Truly Better Than Meta-Learning?. (arXiv:2306.13841v1 [cs.LG])

    [http://arxiv.org/abs/2306.13841](http://arxiv.org/abs/2306.13841)

    在少样本学习中，当数据集的正式多样性较低时，预训练模型（PT）胜过模型无关元学习（MAML）。当正式多样性较高时，MAML更好。

    

    在少样本学习的背景下，目前普遍认为固定的预训练模型（PT）加上在评价时微调最后一层，胜过标准的元学习算法。我们通过深入的实证研究和广泛的数据集比较PT和模型无关元学习（MAML）这些说法。与以前的工作不同，我们强调使用相同的体系结构、相同的优化器，以及所有模型都训练到收敛。关键地，我们使用一个更严格的统计工具——效应量（Cohen's d）——来确定使用PT与使用MAML之间的模型差异的实际意义。然后使用一个预先提出的度量——多样性系数——来计算数据集的平均正式多样性。使用这种分析，我们证明了以下事实：1. 当数据集的正式多样性较低时，PT在平均意义上胜过MAML；2. 当正式多样性较高时，MAML胜过PT。

    In the context of few-shot learning, it is currently believed that a fixed pre-trained (PT) model, along with fine-tuning the final layer during evaluation, outperforms standard meta-learning algorithms. We re-evaluate these claims under an in-depth empirical examination of an extensive set of formally diverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike previous work, we emphasize a fair comparison by using: the same architecture, the same optimizer, and all models trained to convergence. Crucially, we use a more rigorous statistical tool -- the effect size (Cohen's d) -- to determine the practical significance of the difference between a model trained with PT vs. a MAML. We then use a previously proposed metric -- the diversity coefficient -- to compute the average formal diversity of a dataset. Using this analysis, we demonstrate the following: 1. when the formal diversity of a data set is low, PT beats MAML on average and 2. when the formal diversity is hi
    
[^23]: 超越规模：多样性系数作为数据质量指标证明了LLMs是在形式多样的数据上预先训练的

    Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])

    [http://arxiv.org/abs/2306.13840](http://arxiv.org/abs/2306.13840)

    本论文提出使用多样性系数作为LLM预训练数据质量的指标，研究表明公开可用的LLM数据集的多样性系数很高。

    

    当前，预先训练强大的大语言模型(LLMs)的趋势主要集中在模型和数据集规模的扩大。然而，预先训练数据的质量对于训练强大的LLMs来说是一个重要因素，但它是一个模糊的概念，尚未完全表征。因此，我们使用最近提出的Task2Vec多样性系数来基于数据质量的形式方面，超越规模本身。具体而言，我们测量公开可用的预先训练数据集的多样性系数，以证明它们的形式多样性高于理论的下限和上限。此外，为了建立对多样性系数的信心，我们进行可解释性实验，并发现该系数与多样性的直观属性相吻合，例如，随着潜在概念数量的增加，它增加。我们得出结论，多样性系数是可靠的，表明公开可用的LLM数据集的多样性系数很高，并推测它可以作为预训练LLMs模型的数据质量指标。

    Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
    
[^24]: NLP Transformer中的双螺旋结构

    The Double Helix inside the NLP Transformer. (arXiv:2306.13817v1 [cs.AI])

    [http://arxiv.org/abs/2306.13817](http://arxiv.org/abs/2306.13817)

    本文介绍了一个NLP Transformer中分析位置、句法、语义和上下文信息的框架，揭示了位置信息通过螺旋路径在深层中自我分离，并在编码器和解码器侧生成词性聚类。提出了替代在语义嵌入中添加位置信息的方法。

    

    我们介绍了一个NLP Transformer中分析不同信息类型的框架。在这个方法中，我们区分了四层信息：位置、句法、语义和上下文。我们还提出，常见的在语义嵌入中添加位置信息的做法是次优的，提议使用线性加法(Linar-and-Add)的方法。我们的分析揭示了位置信息在深层中的自我分离。我们展示了嵌入向量的位置组成部分遵循螺旋的路径，在编码器侧和解码器侧都是如此。我们另外还展示，编码器侧的概念维度生成了词性(PoS)的聚类。在解码器侧，我们展示使用二元语法的方法有助于揭示下一个标记的词性聚类。我们的方法为阐明通过NLP Transformer的深层信息处理铺平了道路。

    We introduce a framework for analyzing various types of information in an NLP Transformer. In this approach, we distinguish four layers of information: positional, syntactic, semantic, and contextual. We also argue that the common practice of adding positional information to semantic embedding is sub-optimal and propose instead a Linear-and-Add approach. Our analysis reveals an autogenetic separation of positional information through the deep layers. We show that the distilled positional components of the embedding vectors follow the path of a helix, both on the encoder side and on the decoder side. We additionally show that on the encoder side, the conceptual dimensions generate Part-of-Speech (PoS) clusters. On the decoder side, we show that a di-gram approach helps to reveal the PoS clusters of the next token. Our approach paves a way to elucidate the processing of information through the deep layers of an NLP Transformer.
    
[^25]: 多模态双重注意力变换器实现跨语音情感识别

    Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v1 [cs.CL])

    [http://arxiv.org/abs/2306.13804](http://arxiv.org/abs/2306.13804)

    提出一种多模态双重注意力变换器（MDAT）模型，利用预训练模型进行多模态特征提取，通过引入图形注意和共同关注机制来捕捉不同情感的跨模态依赖，并使用最少的目标语言数据实现改进的跨语言情感识别结果。

    

    尽管语音情感识别（SER）取得了近期的进展，但最先进的系统无法在跨语言环境中实现改进的性能。本文提出了一种多模态双重注意力变换器（MDAT）模型，以改进跨语言SER。我们的模型利用预训练模型进行多模态特征提取，并配备双重注意机制，包括图形注意和共同关注，以捕获不同模态之间的复杂依赖关系，并使用最少的目标语言数据实现改进的跨语言SER结果。此外，我们的模型还利用变换器编码器层进行高层特征表示，以提高情感分类准确性。MDAT在各个阶段执行特征表示的细化，并为分类层提供情感显着特征。这种新颖方法还确保了模态特定的情感信息的保存，同时增强了交叉模态。

    Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems are unable to achieve improved performance in cross-language settings. In this paper, we propose a Multimodal Dual Attention Transformer (MDAT) model to improve cross-language SER. Our model utilises pre-trained models for multimodal feature extraction and is equipped with a dual attention mechanism including graph attention and co-attention to capture complex dependencies across different modalities and achieve improved cross-language SER results using minimal target language data. In addition, our model also exploits a transformer encoder layer for high-level feature representation to improve emotion classification accuracy. In this way, MDAT performs refinement of feature representation at various stages and provides emotional salient features to the classification layer. This novel approach also ensures the preservation of modality-specific emotional information while enhancing cross-modality 
    
[^26]: 新冠疫苗相关情感分析：从研发到推广

    An analysis of vaccine-related sentiments from development to deployment of COVID-19 vaccines. (arXiv:2306.13797v1 [cs.SI])

    [http://arxiv.org/abs/2306.13797](http://arxiv.org/abs/2306.13797)

    本文分析了全球范围内在COVID-19大流行期间推特上的情感态度，结果表明情感极性得分与病例数量及其变化之间存在联系。疫情的前半段情感极性得分发生了剧烈变化，后来趋于稳定，暗示疫苗的推广对于人们的讨论方向产生了影响。

    

    病毒爆发和疫苗接种计划的历史上一直存在着反疫苗情绪。COVID-19大流行中，人们对疫苗的恐惧和不确定性在Twitter等社交媒体平台上有很好的表达。本文通过深度学习模型分析了全球范围内在COVID-19大流行期间推特上的情感态度，并提供了相关的可视化。结果表明推特上的情感极性得分与病例数量及其变化之间存在联系，在疫情重大波动期间情感极性得分的变化很大。与此同时，本文还发现疫情的前半段情感极性得分发生了剧烈变化，后来趋于稳定，暗示疫苗的推广对于人们的讨论方向产生了影响。

    Anti-vaccine sentiments have been well-known and reported throughout the history of viral outbreaks and vaccination programmes. The COVID-19 pandemic had fear and uncertainty about vaccines which has been well expressed on social media platforms such as Twitter. We analyse Twitter sentiments from the beginning of the COVID-19 pandemic and study the public behaviour during the planning, development and deployment of vaccines expressed in tweets worldwide using a sentiment analysis framework via deep learning models. In this way, we provide visualisation and analysis of anti-vaccine sentiments over the course of the COVID-19 pandemic. Our results show a link between the number of tweets, the number of cases, and the change in sentiment polarity scores during major waves of COVID-19 cases. We also found that the first half of the pandemic had drastic changes in the sentiment polarity scores that later stabilised which implies that the vaccine rollout had an impact on the nature of discuss
    
[^27]: 解构分类器：针对文本分类模型的数据重构攻击

    Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models. (arXiv:2306.13789v1 [cs.CL])

    [http://arxiv.org/abs/2306.13789](http://arxiv.org/abs/2306.13789)

    本文提出了一种针对文本分类模型的数据重构攻击，称为Mix And Match攻击，该攻击利用了分类模型基于LLM的特点，该攻击已被证明在随机和有机测试数据集上是有效的。

    

    自然语言处理（NLP）模型越来越受到现实世界应用的青睐，如文本分类。然而，它们对隐私攻击是脆弱的，包括旨在提取用于训练模型的数据的数据重构攻击。大多数以前关于数据重构攻击的研究都集中在LLM上，而分类模型被认为更安全。在这项工作中，我们提出了一种新的有针对性的数据重构攻击称为Mix And Match攻击，它利用了大多数分类模型基于LLM的事实。Mix And Match攻击使用目标模型的基础模型生成候选令牌，然后使用分类头修剪它们。我们广泛展示了攻击的有效性，使用了随机与有机的金丝雀。这项工作突出了在分类模型中考虑与数据重构攻击相关的隐私风险的重要性，并提供了有关如何提高模型的隐私保护的见解。

    Natural language processing (NLP) models have become increasingly popular in real-world applications, such as text classification. However, they are vulnerable to privacy attacks, including data reconstruction attacks that aim to extract the data used to train the model. Most previous studies on data reconstruction attacks have focused on LLM, while classification models were assumed to be more secure. In this work, we propose a new targeted data reconstruction attack called the Mix And Match attack, which takes advantage of the fact that most classification models are based on LLM. The Mix And Match attack uses the base model of the target model to generate candidate tokens and then prunes them using the classification head. We extensively demonstrate the effectiveness of the attack using both random and organic canaries. This work highlights the importance of considering the privacy risks associated with data reconstruction attacks in classification models and offers insights into po
    
[^28]: 通过OCR后的文本处理进行简历信息提取

    Resume Information Extraction via Post-OCR Text Processing. (arXiv:2306.13775v1 [cs.CL])

    [http://arxiv.org/abs/2306.13775](http://arxiv.org/abs/2306.13775)

    本文提出了利用OCR后的文本处理和YOLOv8对象识别进行简历信息提取的方法，并成功优于以往基于NLP的模型对简历中教育和经验信息进行提取。

    

    自然语言处理领域中其中一项主要任务信息提取（IE）在简历中日益增加重要性。以往的研究中，通常是使用NLP模型对简历中的信息进行句子分类提取。本研究旨在通过预处理（如光学字符识别（OCR）和对象识别）来分类提取简历中的所有文本组。数据集包含286份简历，其中涵盖了IT行业5个不同职位（教育、经验、才能、个人和语言）的要求，并且使用BERT、BERT-t、DistilBERT、RoBERTa 和 XLNet等模型进行实验。与此同时，YOLOv8模型的结果也进行了比较。研究表明，通过使用OCR后的文本处理和YOLOv8对象识别，该方法比先前的基于NLP的模型在简历信息提取方面表现更好，特别是在教育信息和经验信息提取方面分别达到了0.94和0.91的F1分数。

    Information extraction (IE), one of the main tasks of natural language processing (NLP), has recently increased importance in the use of resumes. In studies on the text to extract information from the CV, sentence classification was generally made using NLP models. In this study, it is aimed to extract information by classifying all of the text groups after pre-processing such as Optical Character Recognition (OCT) and object recognition with the YOLOv8 model of the resumes. The text dataset consists of 286 resumes collected for 5 different (education, experience, talent, personal and language) job descriptions in the IT industry. The dataset created for object recognition consists of 1198 resumes, which were collected from the open-source internet and labeled as sets of text. BERT, BERT-t, DistilBERT, RoBERTa and XLNet were used as models. F1 score variances were used to compare the model results. In addition, the YOLOv8 model has also been reported comparatively in itself. As a resul
    
[^29]: CHiME-7 DASR 挑战赛：多设备远程会议转录在多样化环境中

    The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios. (arXiv:2306.13734v1 [eess.AS])

    [http://arxiv.org/abs/2306.13734](http://arxiv.org/abs/2306.13734)

    CHiME-7 DASR 挑战赛旨在进行在远场环境下多设备远程会议转录的鲁棒语音识别，并在三种不同场景中评估系统性能。参与者可以使用开源预训练模型和数据集。

    

    CHiME 挑战赛在鲁棒语音识别系统的开发和评估中发挥了重要作用。我们在第七届 CHiME 挑战赛中引入了 CHiME-7 远程自动语音识别 (DASR) 任务，该任务包括在远场环境下使用多个、可能是异构的录音设备进行联合 ASR 和人声分离。与之前的挑战不同的是，我们评估3个不同的场景下的系统性能：CHiME-6、DiPCo 和 Mixer 6。目标是让参与者设计一个能够在不知道先验信息的情况下横跨不同阵列几何和用例的单个系统。另一个与之前 CHiME 不同的是，参与者可以使用开源预训练模型和数据集。

    The CHiME challenges have played a significant role in the development and evaluation of robust speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separat
    
[^30]: 为Conformer Transducer语音识别系统建立高效紧凑的上下文表示

    Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems. (arXiv:2306.13307v1 [eess.AS])

    [http://arxiv.org/abs/2306.13307](http://arxiv.org/abs/2306.13307)

    本研究旨在为Conformer Transducer语音识别系统建立高效紧凑的上下文表示，通过特定的注意力汇聚层实现跨话语的信息集成，取得了显著的性能提升。

    

    当前的自动语音识别系统主要在话语级别进行训练和评估。可以纳入长范围的跨话语上下文信息。本文提出，在Conformer-Transducer编码器中使用特殊设计的注意力汇聚层，通过高效缓存的前面话语历史向量，学习了紧凑的低维跨话语上下文特征。在1000小时的Gigaspeech语料库上的实验表明，所提出的上下文化流Conformer-Transducer在开发和测试数据上都比仅使用话语内部上下文的基线模型具有显著的字错率降低（0.7％到0.5％绝对，4.3％到3.1％相对）。

    Current ASR systems are mainly trained and evaluated at the utterance level. Long range cross utterance context can be incorporated. A key task is to derive a suitable compact representation of the most relevant history contexts. In contrast to previous researches based on either LSTM-RNN encoded histories that attenuate the information from longer range contexts, or frame level concatenation of transformer context embeddings, in this paper compact low-dimensional cross utterance contextual features are learned in the Conformer-Transducer Encoder using specially designed attention pooling layers that are applied over efficiently cached preceding utterances history vectors. Experiments on the 1000-hr Gigaspeech corpus demonstrate that the proposed contextualized streaming Conformer-Transducers outperform the baseline using utterance internal context only with statistically significant WER reductions of 0.7% to 0.5% absolute (4.3% to 3.1% relative) on the dev and test data.
    
[^31]: 显式句法引导神经文本生成

    Explicit Syntactic Guidance for Neural Text Generation. (arXiv:2306.11485v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11485](http://arxiv.org/abs/2306.11485)

    本研究提出了一种基于句法引导的文本生成模式，通过自上而下的形式结构树生成序列，在自回归基线中表现优异，具有可解释性、可控制性和多样性的优点。

    

    大多数现有文本生成模型都遵循序列对序列范例。生成语法表明人类通过学习语言语法来生成自然语言文本。我们提出了一种句法引导的生成模式，它通过自上而下的形式结构树生成序列。解码过程可以分解为两个部分：（1）在源句子的词汇化句法上下文中为每个成分预测填充文本；（2）映射和扩展每个成分以构建下一级句法上下文。因此，我们提出了一种结构束搜索方法，用于分层地查找可能的句法结构。对于释义生成和机器翻译的实验结果表明，所提出的方法优于自回归基线，同时还表现出可解释性、可控制性和多样性的有效性。

    Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a top-down direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence; (2) mapping and expanding each constituent to construct the next-level syntax context. Accordingly, we propose a structural beam search method to find possible syntax structures hierarchically. Experiments on paraphrase generation and machine translation show that the proposed method outperforms autoregressive baselines, while also demonstrating effectiveness in terms of interpretability, controllability, and diversity.
    
[^32]: 一种新颖的基于对照的方面情感分析方法

    A novel Counterfactual method for aspect-based sentiment analysis. (arXiv:2306.11260v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11260](http://arxiv.org/abs/2306.11260)

    本文提出了一种新颖的基于对照的数据增强方法，用于生成具有相反情感极性的观点表达，并利用T5模型检索掩码，该方法在三个ABSA数据集上表现优于当前增强方法。

    

    方面情感分析 (ABSA) 是一项细粒度的情感评估任务，它分析评估方面的情感极性。然而，以往的工作仅关注观点表达的识别，而忽略了观点表达的多样性对ABSA任务的影响。为解决这个问题，我们提出了一种新颖的基于对照的数据增强方法，用于生成具有相反情感极性的观点表达。具体而言，我们使用集成梯度来识别和屏蔽观点表达。然后将反向标签的提示组合到原始文本中，并最终利用预训练语言模型 (PLM) T5 来检索掩码。实验结果表明，所提出的对照数据增强方法在三个ABSA数据集 (即Laptop，Restaurant和MAMS) 上的表现优于当前的增强方法。

    Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation task, which analyze the emotional polarity of the evaluation aspects. However, previous works only focus on the identification of opinion expressions, forget that the diversity of opinion expressions also has great impacts on the ABSA task. To mitigate this problem, we propose a novel counterfactual data augmentation method to generate opinion expression with reversed sentiment polarity. Specially, the integrated gradients are calculated to identify and mask the opinion expression. Then, a prompt with the reverse label is combined to the original text, and a pre-trained language model (PLM), T5, is finally employed to retrieve the masks. The experimental results show the proposed counterfactual data augmentation method perform better than current augmentation methods on three ABSA datasets, i.e. Laptop, Restaurant and MAMS.
    
[^33]: 基于低秩和稀疏逼近的大型语言模型结构化压缩 LoSparse

    LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation. (arXiv:2306.11222v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11222](http://arxiv.org/abs/2306.11222)

    LoSparse 是一种新的语言模型压缩技术，通过低秩矩阵和稀疏矩阵逼近权重矩阵，结合了低秩逼近和裁剪的优点，可以显著降低语言模型大小和复杂度，并在多个自然语言任务中表现优异。

    

    Transformer 模型在多种自然语言任务中取得了显著的成果，但它们通常过于庞大，需要大量的内存和计算资源。为了降低这些模型的大小和复杂性，我们提出了 LoSparse（低秩和稀疏逼近）一种新的模型压缩技术，通过低秩矩阵和稀疏矩阵之和逼近权重矩阵。我们的方法结合了低秩逼近和裁剪的优点，同时避免了它们的局限性。低秩逼近压缩了神经元中的一致和表达力强的部分，而裁剪则消除了神经元中的不一致和表达力不强的部分。裁剪增强了低秩逼近的多样性，低秩逼近防止了裁剪丢失表达力强的神经元过多。我们在自然语言理解、问答和自然语言生成任务上评估了我们的方法。实验结果表明，我们的方法明显优于现有的压缩方法。

    Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compre
    
[^34]: 文本到图像扩散模型中的能量交叉注意力用于贝叶斯上下文更新

    Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])

    [http://arxiv.org/abs/2306.09869](http://arxiv.org/abs/2306.09869)

    本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。

    

    尽管文本到图像扩散模型在图像生成任务中表现出色，但最近的研究提出了一个问题，即生成的图像有时无法捕捉到文本提示的预期语义内容，这种现象通常被称为语义错位。为了解决这个问题，我们提出了一种新颖的基于能量的模型（EBM）框架。具体而言，我们首先在去噪自编码器的每个交叉注意力层中制定潜在图像表示和文本嵌入的EBM。然后，我们获得上下文向量的对数后验梯度，可以更新和转移到后续的交叉注意力层，从而隐式地最小化嵌套层次的能量函数。我们的潜在EBMs还允许零样本组合生成，即通过不同上下文的交叉注意力输出的线性组合。通过大量实验，我们证明了所提出的方法在处理各种图像生成任务方面非常有效，并可以显著降低文本提示和生成图像之间的语义错位现象。

    Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
    
[^35]: 使用大型语言模型探索MIT数学和EECS课程

    Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models. (arXiv:2306.08997v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.08997](http://arxiv.org/abs/2306.08997)

    通过使用大型语言模型，翻译了MIT数学和EECS课程中的4550个题目，开发出一个可以自动评分的模型，并探索了课程、问题和答案之间的关系。

    

    我们整理了一个综合性数据集，包括了获取学位所需的所有MIT数学和电气工程及计算机科学（EECS）课程的题目集、期中考试和期末考试中的4550个问题和解决方案。我们评估了大型语言模型实现任何MIT数学和EECS专业毕业要求的能力。我们的结果表明，GPT-3.5成功解决了整个MIT课程的三分之一，而GPT-4在题目中不包含图像的测试集上经过提示工程后达到了完美的解决率。我们在此数据集上对开源大型语言模型进行了微调。我们采用GPT-4自动评分，提供了课程、问题和答案类型的详细性能分析。通过将问题嵌入低维空间，我们探索了问题、主题和课程之间的关系，并发现哪些问题和课程是解决其他问题和课程所必需的。

    We curate a comprehensive dataset of 4,550 questions and solutions from problem sets, midterm exams, and final exams across all MIT Mathematics and Electrical Engineering and Computer Science (EECS) courses required for obtaining a degree. We evaluate the ability of large language models to fulfill the graduation requirements for any MIT major in Mathematics and EECS. Our results demonstrate that GPT-3.5 successfully solves a third of the entire MIT curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate on a test set excluding questions based on images. We fine-tune an open-source large language model on this dataset. We employ GPT-4 to automatically grade model responses, providing a detailed performance breakdown by course, question, and answer type. By embedding questions in a low-dimensional space, we explore the relationships between questions, topics, and classes and discover which questions and classes are required for solving other questions and classes
    
[^36]: MOFI: 从含噪实体标注的图像中学习图像表示

    MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])

    [http://arxiv.org/abs/2306.07952](http://arxiv.org/abs/2306.07952)

    MOFI 提出了一种新的方法，自动从含噪图像文本对中为图像指定实体标签，创建了一个新的大规模数据集 I2E，通过研究不同的训练配方，学习到了能够有效学习图像表示的模型。

    

    本文提出了一种新的视觉基础模型 MOFI，旨在从含噪实体标注的图像中学习图像表示。MOFI 与以往的工作有两点不同：（i）预训练数据，（ii）训练配方。在数据方面，我们引入了一种新方法，自动从含噪图像文本对中为图像指定实体标签。我们使用命名实体识别模型从 alt-text 中提取实体，然后使用 CLIP 模型选择正确的实体作为图像的标签。这种方法简单易行，不需要昂贵的人工注释，并且可以轻松扩展到从 web 上挖掘的数十亿个图像文本对。通过这种方法，我们创建了 Image-to-Entities（I2E）这一新的大规模数据集，其中包含 10 亿张图像和 200 万个不同的实体，涵盖了野外丰富的视觉概念。基于 I2E 数据集，我们研究了不同的训练配方，包括有监督的预训练、对比度预训练。

    We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train
    
[^37]: 使用混合语言特征提高土耳其文本可读性的研究

    Exploring Hybrid Linguistic Features for Turkish Text Readability. (arXiv:2306.03774v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.03774](http://arxiv.org/abs/2306.03774)

    本文结合神经网络模型和各语言层面上的特征，开发出一种先进的土耳其文本可读性工具，发现了影响土耳其文本可读性的关键语言特征。

    

    本文首次对土耳其文本的自动可读性评估进行了全面研究。我们结合了最先进的神经网络模型和词汇、形态句法、语法和话语水平的语言特征，开发了一个先进的可读性工具。我们评估了传统可读性公式与现代自动方法的效果，并确定了影响土耳其文本可读性的关键语言特征。

    This paper presents the first comprehensive study on automatic readability assessment of Turkish texts. We combine state-of-the-art neural network models with linguistic features at lexical, morphosyntactic, syntactic and discourse levels to develop an advanced readability tool. We evaluate the effectiveness of traditional readability formulas compared to modern automated methods and identify key linguistic features that determine the readability of Turkish texts.
    
[^38]: ChatGPT信息的图神经网络用于股票价格预测

    ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.03763](http://arxiv.org/abs/2306.03763)

    该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。

    

    ChatGPT已在各种自然语言处理（NLP）任务中展示了出色的能力。然而，它从时间文本数据（尤其是财经新闻）推断动态网络结构的潜力仍是一个未开发的领域。在这项研究中，我们介绍了一个新的框架，利用ChatGPT的图推断能力来增强图神经网络（GNN）。我们的框架巧妙地从文本数据中提取出不断变化的网络结构，并将这些网络结构融合到图神经网络中，进行后续的预测任务。股票价格预测的实验结果表明，我们的模型始终优于基于深度学习的最新基准。此外，基于我们模型的产出构建的组合展示出更高的年化累计回报、更低的波动性和最大回撤。这种卓越表现突显了ChatGPT用于基于文本的网络推断和金融预测应用的潜力。

    ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
    
[^39]: 通过大型语言模型扩展基于证据的教学设计专业知识

    Scaling Evidence-based Instructional Design Expertise through Large Language Models. (arXiv:2306.01006v1 [cs.CL])

    [http://arxiv.org/abs/2306.01006](http://arxiv.org/abs/2306.01006)

    本文探讨了如何利用大型语言模型扩展基于证据的教学设计专业知识，通过两个案例研究展示了GPT-4在教学设计中的应用，并提供了使用LLMs的最佳实践和未来LLMs为所有学习者提供个性化教育内容的愿景。

    

    本文探讨了如何利用大型语言模型（LLMs），特别是GPT-4，来拓展教学设计领域的基于证据的专业知识。我们致力于弥合理论教育研究和实际实施之间的差距，探讨了AI驱动内容生成的优缺点，并强调了人工监督来确保教育材料的质量的必要性。通过两个详细的案例研究，我们展示了如何使用GPT-4创建不同课程的复杂高阶评估和积极学习组件。从我们的经验中，我们提供了在教学设计任务中有效使用LLMs的最佳实践，如利用模板，微调，处理意外的输出，实现LLM链，引用参考文献，评估输出，创建评分标准和生成干扰项。我们还分享了一个愿景，未来LLMs可以为所有背景和水平的学习者提供易于接受和个性化的教育内容。

    This paper presents a comprehensive exploration of leveraging Large Language Models (LLMs), specifically GPT-4, in the field of instructional design. With a focus on scaling evidence-based instructional design expertise, our research aims to bridge the gap between theoretical educational studies and practical implementation. We discuss the benefits and limitations of AI-driven content generation, emphasizing the necessity of human oversight in ensuring the quality of educational materials. This work is elucidated through two detailed case studies where we applied GPT-4 in creating complex higher-order assessments and active learning components for different courses. From our experiences, we provide best practices for effectively using LLMs in instructional design tasks, such as utilizing templates, fine-tuning, handling unexpected output, implementing LLM chains, citing references, evaluating output, creating rubrics, grading, and generating distractors. We also share our vision of a f
    
[^40]: SQL-PaLM：针对Text-to-SQL的改进大语言模型适应性

    SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.00739](http://arxiv.org/abs/2306.00739)

    本文提出了一种基于大语言模型的Text-to-SQL模型SQL-PaLM，使用了面向Text-to-SQL的基于执行的自一致提示方法，在Spider上实现了77.3%的测试套件准确度，并显着超越以前的最新技术的方法。

    

    大语言模型（LLMs）的一个令人印象深刻的新兴功能是生成代码，包括用于数据库的结构化查询语言（SQL）。对于将自然语言文本转换为SQL查询的任务，即Text-to-SQL，LLMs的适应性至关重要，具体取决于使用的适应性数据量。本文提出了一种基于LLM的Text-to-SQL模型SQL-PaLM，利用了PaLM-2，推动了两种设置的最新进展。Few-shot SQL-PaLM基于面向Text-to-SQL的基于执行的自一致提示方法，可在Spider上实现77.3%的测试套件准确度，据我们所知，这是第一个通过显着较大的微调超越以前的最新技术的方法。此外，我们证明经过精细调整的SQL-PALM可进一步提高1%的性能。为了将SQL-PaLM应用于实际场景，我们进一步评估了其对其他挑战的稳健性。

    One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other chall
    
[^41]: 癌症实体的关联和分类的机器学习方法

    Machine Learning Approach for Cancer Entities Association and Classification. (arXiv:2306.00013v1 [cs.CL])

    [http://arxiv.org/abs/2306.00013](http://arxiv.org/abs/2306.00013)

    本研究利用命名实体识别和文本分类的机器学习方法自动从大量生物医学文献中提取癌症相关实体和实体间关系，以帮助推进癌症研究进展。

    

    根据世界卫生组织（WHO）的数据，癌症是全球第二大死因。不同类型癌症的科学研究以每年发布大量的研究文章的速度不断增长。与基因相关的药物、诊断、风险、症状、治疗等的信息和知识是帮助探索和推进癌症研究进展的重要因素。手动筛选这么大量的文章非常费时费力，很难制定任何假设。本研究使用两种最为重要的自然语言处理（NLP）功能，实体识别和文本分类，从生物医学文献中发现知识。命名实体识别（NER）借助用户友好的界面和内置字典识别并提取与癌症相关的预定义实体。文本分类采用机器学习方法，帮助探究癌症实体之间的关系。

    According to the World Health Organization (WHO), cancer is the second leading cause of death globally. Scientific research on different types of cancers grows at an ever-increasing rate, publishing large volumes of research articles every year. The insight information and the knowledge of the drug, diagnostics, risk, symptoms, treatments, etc., related to genes are significant factors that help explore and advance the cancer research progression. Manual screening of such a large volume of articles is very laborious and time-consuming to formulate any hypothesis. The study uses the two most non-trivial NLP, Natural Language Processing functions, Entity Recognition, and text classification to discover knowledge from biomedical literature. Named Entity Recognition (NER) recognizes and extracts the predefined entities related to cancer from unstructured text with the support of a user-friendly interface and built-in dictionaries. Text classification helps to explore the insights into the 
    
[^42]: MemeGraphs: 将网络文化表情包与知识图谱相连

    MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v1 [cs.LG])

    [http://arxiv.org/abs/2305.18391](http://arxiv.org/abs/2305.18391)

    该论文提出了一种使用场景图和知识图谱结构化表达网络文化表情包的方法，并将其用于分类。结果显示该方法相比使用学习表达式的模型有所改善。

    

    网络文化表情包是一种在社交媒体和互联网上流行的传播趋势和观点的形式，结合了图像和文本的模式。它们可以表达幽默和讽刺，但也可能含有冒犯性的内容。自动分析和分类网络文化表情包具有挑战性，因为其解释依赖于对视觉元素、语言和背景知识的理解。因此，重要的是有意义地表示这些来源以及它们之间的交互，以便将表情包作为整体分类。在这项工作中，我们提出使用场景图作为表示图像中物体及其视觉关系的结构化表达方式，并将知识图谱作为网络文化表情包分类的结构化表示，使用基于Transformer的架构。我们将我们的方法与ImgBERT进行比较，后者使用仅学习（而不是结构化）的表达式进行多模式建模，我们观察到始终有所改善。我们还提供了一个具有人工图注释的数据集，供比较。

    Memes are a popular form of communicating trends and ideas in social media and on the internet in general, combining the modalities of images and text. They can express humor and sarcasm but can also have offensive content. Analyzing and classifying memes automatically is challenging since their interpretation relies on the understanding of visual elements, language, and background knowledge. Thus, it is important to meaningfully represent these sources and the interaction between them in order to classify a meme as a whole. In this work, we propose to use scene graphs, that express images in terms of objects and their visual relations, and knowledge graphs as structured representations for meme classification with a Transformer-based architecture. We compare our approach with ImgBERT, a multimodal model that uses only learned (instead of structured) representations of the meme, and observe consistent improvements. We further provide a dataset with human graph annotations that we compa
    
[^43]: 语言模型是有限实用说话者

    Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17760](http://arxiv.org/abs/2305.17760)

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。

    

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。特别地，我们展示了经过人类反馈的强化学习微调的大型语言模型（Ouyang等人，2022）具有概念上类似于 快与慢思考模型（Kahneman，2011）的思维模型，而这种思维模型被心理学家们归因于人类。我们讨论了从人类反馈中的强化学习作为快与慢思考模型的局限性，并提出了扩展这个框架的途径。本研究实质上凸显了采用认知概率建模方法来获得对语言模型的理解、评估和推进方面的深刻见解的价值。

    How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
    
[^44]: SAIL: Search-Augmented Instruction Learning （SAIL：搜索增强的指令学习）

    SAIL: Search-Augmented Instruction Learning. (arXiv:2305.15225v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15225](http://arxiv.org/abs/2305.15225)

    这篇论文提出了一种称为SAIL的搜索增强指令学习方法，该方法以内部和外部搜索引擎生成的搜索结果为基础，实现了语言生成和指令跟踪能力。通过构建一个包含指令、接地信息和响应三元组的新的搜索基础训练集来实现这一目标。在训练过程中，模型需要学习过滤干扰段落并生成目标响应。

    

    大型语言模型通过指令微调得到了显著改进，但仍缺乏透明性和利用最新知识和信息的能力。在本文中，我们提出了搜索增强的指令学习（SAIL），它以内部和外部搜索引擎生成的搜索结果为基础，实现了语言生成和指令跟踪能力。通过指令微调语料库，我们从不同的搜索API和领域收集每个训练用例的搜索结果，并构建了一个包含（指令，接地信息，响应）三元组的新的搜索基础训练集。然后在构建的训练集上对LLaMA-7B模型进行微调。由于收集的结果包含不相关和争议的语言，模型需要学习在 可信赖 的搜索结果上进行接地，过滤出干扰的段落，并生成目标响应。搜索结果去噪过程需要显式的、可信任的信息。

    Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy inform
    
[^45]: 不对称学习率的分离式理性化: 一种灵活的Lipschitz限制

    Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint. (arXiv:2305.13599v1 [cs.LG])

    [http://arxiv.org/abs/2305.13599](http://arxiv.org/abs/2305.13599)

    本文提出了一种名为DR的灵活的方法，它通过不对称的学习率来解决由合作博弈引发的退化问题，该方法能够在两个基准测试中显著改善表现。

    

    通常情况下，自说明理性化模型通过合作博弈构建，其中生成器从输入文本中选择最易理解的部分作为原理，接着预测器基于所选择的原理进行预测。然而，这种合作博弈可能会引发退化问题，预测器过度拟合于由尚未训练好的生成器生成的信息不足的部分，反过来导致生成器收敛于趋向于选择无意义的部分的次优模型。本文从理论上将退化问题与预测器的Lipschitz连续性联系起来。随后，我们实验性地提出了一种名为DR的简单而有效的方法，可以自然、灵活地约束预测器的Lipschitz常数，并解决了退化问题。DR方法的主要思想是将生成器和预测器分离，为它们分配不对称的学习率。在两个广泛使用的基准测试中进行的一系列实验表明，我们的DR方法能够显著改善现有方法的表现。

    A self-explaining rationalization model is generally constructed by a cooperative game where a generator selects the most human-intelligible pieces from the input text as rationales, followed by a predictor that makes predictions based on the selected rationales. However, such a cooperative game may incur the degeneration problem where the predictor overfits to the uninformative pieces generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. In this paper, we theoretically bridge degeneration with the predictor's Lipschitz continuity. Then, we empirically propose a simple but effective method named DR, which can naturally and flexibly restrain the Lipschitz constant of the predictor, to address the problem of degeneration. The main idea of DR is to decouple the generator and predictor to allocate them with asymmetric learning rates. A series of experiments conducted on two widely used benchm
    
[^46]: 上下文化的短语预测网络在端到端语音识别中的应用

    Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.12493](http://arxiv.org/abs/2305.12493)

    本研究引入了一个上下文短语预测网络用于基于注意力的深度偏置方法，通过计算偏置损失以帮助训练上下文化模型，在多种端到端语音识别模型上实现了显著的WER降低，相对于基线模型相对WER提高了12.1％，上下文短语的WER相对降低了40.5％。

    

    上下文信息在语音识别技术中发挥着至关重要的作用，将其融入端到端语音识别模型近年来引起了极大的兴趣。然而，先前的深度偏置方法缺乏偏置任务的显式监督。本研究引入了一个上下文短语预测网络用于基于注意力的深度偏置方法。该网络利用上下文嵌入预测发音中的上下文短语，并计算偏置损失以帮助训练上下文化模型。我们的方法在多种端到端语音识别模型上实现了显著的单词错误率(WER)降低。对LibriSpeech语料库的实验结果表明，在基线模型上，我们提出的模型相对WER提高了12.1％，上下文短语的WER相对降低了40.5％。此外，通过应用上下文短语过滤策略，我们还有效消除了使用更大的偏置列表时的WER降级现象。

    Contextual information plays a crucial role in speech recognition technologies and incorporating it into the end-to-end speech recognition models has drawn immense interest recently. However, previous deep bias methods lacked explicit supervision for bias tasks. In this study, we introduce a contextual phrase prediction network for an attention-based deep bias method. This network predicts context phrases in utterances using contextual embeddings and calculates bias loss to assist in the training of the contextualized model. Our method achieved a significant word error rate (WER) reduction across various end-to-end speech recognition models. Experiments on the LibriSpeech corpus show that our proposed model obtains a 12.1% relative WER improvement over the baseline model, and the WER of the context phrases decreases relatively by 40.5%. Moreover, by applying a context phrase filtering strategy, we also effectively eliminate the WER degradation when using a larger biasing list.
    
[^47]: 连锁符号提示激发了大型语言模型中的规划能力

    Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v1 [cs.CL])

    [http://arxiv.org/abs/2305.10276](http://arxiv.org/abs/2305.10276)

    本文提出了自然语言规划（NLP）的基准，旨在研究LLMs在需要理解并在文本中相应进行操作的复杂规划任务中的表现。同时提出了一种新方法CoS，使用简化的符号空间表示法来表示复杂的环境。

    

    本文旨在研究LLMs在需要理解通过自然语言模拟的虚拟空间环境并在文本中相应进行操作的复杂规划任务中的表现。我们提出了一个名为自然语言规划（NLP）的基准，它由一组新颖的任务组成：Brick World、基于NLVR的操作和自然语言导航。我们发现当前流行的LLMs（如ChatGPT）仍然缺乏复杂规划的能力。这引出了一个问题——LLMs是否对自然语言中描述的环境有良好的理解，或者其他替代方法（如符号表示）是否更加简单，因此更容易被LLMs理解？为此，我们提出了一种名为CoS（Chain-of-Symbol Prompting）的新方法，在链式中间思考步骤中使用简化的符号空间表示法来表示复杂的环境。CoS易于使用，不需要对LLMs进行额外的培训。

    In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question -- do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. 
    
[^48]: 创造性数据生成：以文本和诗歌为重点的综述

    Creative Data Generation: A Review Focusing on Text and Poetry. (arXiv:2305.08493v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08493](http://arxiv.org/abs/2305.08493)

    本文研究了创造性数据的生成，特别关注了以诗歌为重点的自然语言生成。深入探讨了该领域面临的挑战和机遇。

    

    机器学习的快速发展导致了自动数据生成的激增，使得区分自然或人类生成的数据和机器生成的数据越发具有挑战性。尽管取得了这些进展，创造性数据生成仍然是一个挑战。本文旨在研究和理解创造力的本质，既包括一般情况下的创造力，也包括自然语言生成领域内的创造力。我们回顾了各种创造性写作设备和任务，并特别关注诗歌的生成。我们旨在揭示创造性数据生成领域的挑战和机遇。

    The rapid advancement in machine learning has led to a surge in automatic data generation, making it increasingly challenging to differentiate between naturally or human-generated data and machine-generated data. Despite these advancements, the generation of creative data remains a challenge. This paper aims to investigate and comprehend the essence of creativity, both in general and within the context of natural language generation. We review various approaches to creative writing devices and tasks, with a specific focus on the generation of poetry. We aim to shed light on the challenges and opportunities in the field of creative data generation.
    
[^49]: 多语言LLMs是更好的跨语言上下文学习者与对齐效果。

    Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment. (arXiv:2305.05940v1 [cs.CL])

    [http://arxiv.org/abs/2305.05940](http://arxiv.org/abs/2305.05940)

    针对跨语言ICL中无法对准输入输出空间的问题，我们提出了一种新的提示构建策略X-InSTA，可以同时对源语言和目标语言的语境进行编码和对齐，从而提高跨语言ICL的效率。

    

    随着大语言模型能够在没有任何梯度更新的情况下推断出以少数标记样本为条件的测试标签，上下文学习(ICL)成为可能。启用ICL的大语言模型为在低资源环境下规避复发性注释成本提供了有希望的前进步伐。然而，过去只有少数几项研究探究了跨语言设置下的ICL，这在从高资源语言到低资源语言转移标签知识的需要下至关重要。为了弥合这一鸿沟，我们首次对跨语言文本分类的ICL进行了深入分析。我们发现，在跨语言ICL的情况下，普遍选择随机的输入-标签对来构建提示上下文的模式严重受限于输入和输出空间的缺乏对准。为了缓解这一问题，我们提出了一种新的提示构建策略——跨语言上下文源-目标对齐（X-InSTA）。通过注入共同训练的阈值元素，X-InSTA可以同时对源语言和目标语言的语境进行编码和对齐，从而提高跨语言ICL的效率。

    In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy -- Cross-lingual In-context Source-Target Alignment (X-InSTA). With an injected co
    
[^50]: 基于基础模型的系统设计框架

    A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])

    [http://arxiv.org/abs/2305.05352](http://arxiv.org/abs/2305.05352)

    本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    

    最近推出了大型语言模型(LLM)的聊天机器人，如ChatGPT，这引起了人们对基础模型的广泛关注。基础模型被广泛认为将成为未来人工智能系统的基石。由于基础模型处于早期阶段，基于基础模型的系统设计尚未得到系统地探索。人们对在软件架构中引入基础模型的影响知之甚少。因此，在本文中，我们提出了一个基于基础模型的系统分类法，对基础模型和基于基础模型的系统的特点进行了分类和比较。我们的分类法包括三个类别：基础模型预训练和微调、基于基础模型的系统架构设计和负责任的AI设计。这个分类法为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
    
[^51]: 超越分类：最先进的语言模型中的财务推理

    Beyond Classification: Financial Reasoning in State-of-the-Art Language Models. (arXiv:2305.01505v1 [cs.CL])

    [http://arxiv.org/abs/2305.01505](http://arxiv.org/abs/2305.01505)

    本研究探讨了大语言模型在财务推理领域的潜在应用，对任务制定、数据生成、提示方法和评估能力等方面进行了详细研究，最终在各种数据集大小上对参数规模为2.8B至13B的各种GPT变体进行基准测试。

    

    大语言模型(LLMs)由1000亿及以上的参数组成，在复杂的多步推理任务中表现出了非凡的能力。然而，这种通用的进展应用在很少领域中，例如临床或法律领域，而财务推理领域基本上未被探索。据我们所知，LLMs解决财务推理问题的能力从未被研究过，并且它是否可以在任何规模上完成仍未知。为了填补这一知识空白，本研究对LLMs在财务领域的潜在应用进行了全面调查。调查包括对一系列主题的详细探讨，包括任务制定，合成数据生成，提示方法和评估能力。此外，本研究在不同的数据集大小上对参数规模为2.8B至13B的各种GPT变体进行基准测试，包括有无指导调整。

    Large Language Models (LLMs), consisting of 100 billion or more parameters, have demonstrated remarkable ability in complex multi-step reasoning tasks. However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored. To the best of our knowledge, the ability of LLMs to solve financial reasoning problems has never been dealt with, and whether it can be performed at any scale remains unknown. To address this knowledge gap, this research presents a comprehensive investigation into the potential application of LLMs in the financial domain. The investigation includes a detailed exploration of a range of subjects, including task formulation, synthetic data generation, prompting methods, and evaluation capability. Furthermore, the study benchmarks various GPT variants with parameter scales ranging from 2.8B to 13B, with and without instruction tuning, on diverse dataset sizes.
    
[^52]: 基于SearChain的复杂知识密集型任务中精确、可信和可追溯内容生成的研究

    Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks. (arXiv:2304.14732v1 [cs.CL])

    [http://arxiv.org/abs/2304.14732](http://arxiv.org/abs/2304.14732)

    提出了一个名为SearChain的新型框架，以改进LLM生成的内容的准确性、可信度和可追溯性，从而提高复杂知识密集型任务的表现。SearChain通过深度集成LLM和信息检索（IR）实现，其思路是通过构造查询链，将多跳问题进行分解，最终指导LLM生成正确的答案。

    

    随着ChatGPT等大型语言模型（LLM）的广泛应用，如何使LLM生成的内容准确可信在复杂知识密集型任务中变得非常重要。本文提出了一种名为Search-in-the-Chain（SearChain）的新型框架，以改进多跳问题回答等典型复杂知识密集型任务中LLM生成内容的准确性、可信度和可追溯性。SearChain是一个深度集成LLM和信息检索（IR）的框架。在SearChain中，LLM构建查询链，作为多跳问题的分解。链的每个节点都是由IR导向的查询-答案对，以及由LLM生成的该查询的答案。IR验证、完善和跟踪链中每个节点的信息，以指导LLM构建正确的查询链，并最终回答多跳问题。SearChain使LLM从一次性答案转变为多步答案，从而提高了生成内容的准确性和可信度。实验结果表明，SearChain在准确性和可靠性方面优于其他最先进的方法。

    With the wide application of Large Language Models (LLMs) such as ChatGPT, how to make the contents generated by LLM accurate and credible becomes very important, especially in complex knowledge-intensive tasks. In this paper, we propose a novel framework called Search-in-the-Chain (SearChain) to improve the accuracy, credibility and traceability of LLM-generated content for multi-hop question answering, which is a typical complex knowledge-intensive task. SearChain is a framework that deeply integrates LLM and information retrieval (IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition of the multi-hop question. Each node of the chain is a query-answer pair consisting of an IR-oriented query and the answer generated by LLM for this query. IR verifies, completes, and traces the information of each node of the chain, so as to guide LLM to construct the correct chain-of-query, and finally answer the multi-hop question. SearChain makes LLM change from trying to gi
    
[^53]: 通过模块化线性化注意力机制改进自回归自然语言处理任务

    Improving Autoregressive NLP Tasks via Modular Linearized Attention. (arXiv:2304.08453v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08453](http://arxiv.org/abs/2304.08453)

    本文提出模块化线性化注意力机制（MLA）以最大化推理质量并实现速度提升，并在多个自回归自然语言处理任务上验证了该方法，包括语音到文本神经机器翻译（S2T NMT）、语音到文本同声传译（SimulST）和自回归文本到频谱图任务，在TTS上具有高效率收益，在NMT和SimulST的训练和推理过程中表现出竞争性能。

    

    多种自然语言处理任务需要的模型必须在最终应用于边缘或其他资源受限制的环境中高效且小型。尽管先前的研究已将这些模型的大小减小，但在不影响性能的前提下提高计算效率仍然很困难，特别是对于自回归任务而言。本文提出了一种模块化线性化注意力机制（MLA），它结合了多个有效的注意力机制，包括cosFormer，以最大化推理质量并实现显着的速度提升。我们在几个自回归自然语言处理任务上验证了这种方法，包括语音到文本神经机器翻译（S2T NMT）、语音到文本同声传译（SimulST）和自回归文本到频谱图任务，在TTS上具有高效率收益，在NMT和SimulST的训练和推理过程中表现出竞争性能。

    Various natural language processing (NLP) tasks necessitate models that are efficient and small based on their ultimate application at the edge or in other resource-constrained environments. While prior research has reduced the size of these models, increasing computational efficiency without considerable performance impacts remains difficult, especially for autoregressive tasks. This paper proposes {modular linearized attention (MLA), which combines multiple efficient attention mechanisms, including cosFormer, to maximize inference quality while achieving notable speedups. We validate this approach on several autoregressive NLP tasks, including speech-to-text neural machine translation (S2T NMT), speech-to-text simultaneous translation (SimulST), and autoregressive text-to-spectrogram, noting efficiency gains on TTS and competitive performance for NMT and SimulST during training and inference.
    
[^54]: 神经网络中符号的出现与语义理解和交流

    Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])

    [http://arxiv.org/abs/2304.06377](http://arxiv.org/abs/2304.06377)

    本文介绍了一种名为SEA-net的神经网络解决方案，可以生成符号，实现语义理解和交流。这些符号可以捕捉到组成性语义信息，并呈现类似自然语言的内在结构。

    

    能够创造有意义的符号，并熟练地将它们用于更高的认知功能，如交流、推理、规划等，是人类智能的重要和独特之处。 目前，深度神经网络仍远远落后于人类创造符号进行这些高级认知功能的能力。本文提出了一种名为SEA-net的解决方案，使神经网络具有符号创造、语义理解和交流能力。SEA-net生成动态配置网络以执行特定任务的符号。这些符号捕捉了组成性语义信息，使系统能够通过纯符号操作或交流获得新功能。此外，我们发现这些自动生成的符号呈现出类似自然语言的内在结构，表明在人类大脑和人工神经网络中生成和理解符号的共同框架。我们希望这将成为将来发展人工智能的助推器。

    Being able to create meaningful symbols and proficiently use them for higher cognitive functions such as communication, reasoning, planning, etc., is essential and unique for human intelligence. Current deep neural networks are still far behind human's ability to create symbols for such higher cognitive functions. Here we propose a solution, named SEA-net, to endow neural networks with ability of symbol creation, semantic understanding and communication. SEA-net generates symbols that dynamically configure the network to perform specific tasks. These symbols capture compositional semantic information that enables the system to acquire new functions purely by symbolic manipulation or communication. In addition, we found that these self-generated symbols exhibit an intrinsic structure resembling that of natural language, suggesting a common framework underlying the generation and understanding of symbols in both human brains and artificial neural networks. We hope that it will be instrum
    
[^55]: 有效地对齐跨语言会话任务的提示调整跨语言转移学习

    Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])

    [http://arxiv.org/abs/2304.01295](http://arxiv.org/abs/2304.01295)

    本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。

    

    针对自然语言处理任务，跨语言转移的语言模型已被广泛研究，但是对于会话任务的研究相对较少。本文提出了XSGD，这是一个由Schema-Guided Dialogue（SGD）翻译成105种其他语言的平行大规模多语种会话数据集。为了实现对齐的跨语言表示方法，我们开发了一种有效的基于提示调整的方法来学习对齐提示。我们还研究了两种不同的分类器：NLI-based和vanilla分类器，并测试了对齐提示所实现的跨语言能力。我们在两个对话任务（插槽填充和意图分类）上评估了我们模型的跨语言泛化能力。

    Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
    
[^56]: ChatDoctor：使用医学领域知识在LLaMA模型上微调的医疗聊天模型

    ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v1 [cs.CL])

    [http://arxiv.org/abs/2303.14070](http://arxiv.org/abs/2303.14070)

    本文介绍了一种在医学领域利用LLaMA模型微调的医疗聊天模型ChatDoctor。经过700多种疾病和其相应症状、药品和医疗检查的收集和处理，这种模型具有理解患者需求、提供建议和帮助的潜力。这些先进的语言模型集成到医疗保健中可以极大地改进医疗专业人员和患者的沟通方式。

    

    最近，在一般领域中应用的大型语言模型（LLM），例如ChatGPT，已经表现出仿佛是人类讲话般的成功。然而，这样的语言模型并没有经过个别且仔细为医学领域学习，导致诊断准确度低且不能给出正确的医疗诊断、药品等建议。为了解决这个问题，我们收集了700多种疾病及其相应症状、推荐药品和所需医疗检查，然后生成了5K名医患的对话。通过微调医患对话模型，这些模型具有了理解患者需求、提供明智建议并在各种医疗相关领域提供宝贵帮助的巨大潜力。将这些先进的语言模型集成到医疗保健中，可以彻底改变医疗专业人员和患者的沟通方式，最终改善整体质量。

    Recent large language models (LLMs) in the general domain, such as ChatGPT, have shown remarkable success in following instructions and producing human-like responses. However, such language models have not been learned individually and carefully for the medical domain, resulting in poor diagnostic accuracy and inability to give correct recommendations for medical diagnosis, medications, etc. To address this issue, we collected more than 700 diseases and their corresponding symptoms, recommended medications, and required medical tests, and then generated 5K doctor-patient conversations. By fine-tuning models of doctor-patient conversations, these models emerge with great potential to understand patients' needs, provide informed advice, and offer valuable assistance in a variety of medical-related fields. The integration of these advanced language models into healthcare can revolutionize the way healthcare professionals and patients communicate, ultimately improving the overall quality 
    
[^57]: 从宽到深：维度提升网络用于参数高效的知识图谱嵌入

    From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])

    [http://arxiv.org/abs/2303.12816](http://arxiv.org/abs/2303.12816)

    本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。

    

    知识图谱嵌入（KGE）将实体和关系映射到向量表示对于下游任务非常重要。传统的KGE方法需要相对高维的实体表示来保留知识图谱的结构信息，但会导致庞大的模型参数。最近的方法通过采用低维实体表示来降低模型参数，同时开发技术（例如知识蒸馏）来补偿降维。然而，这样的操作会导致模型精度下降和模型参数减少有限。具体来说，我们将所有实体表示的级联视为嵌入层，那么采用高维实体表示的传统KGE方法等同于扩展嵌入层的宽度以获得表现力。为了在不牺牲准确度的情况下实现参数效率，我们相反地增加深度，并提出一个更深的实体嵌入网络。

    Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
    
[^58]: 使用RNN模型学习转换和对齐

    Learning Transductions and Alignments with RNN Seq2seq Models. (arXiv:2303.06841v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.06841](http://arxiv.org/abs/2303.06841)

    本文研究了RNN seq2seq模型在学习四种转换任务方面的能力，并发现其只能逼近符合训练或分布内数据的映射，不能学习底层函数；文章建立了一个新的复杂性层次结构，用于无注意力RNN seq2seq模型，而不是字符串转换的复杂性层次结构。

    

    本文研究了循环神经网络序列到序列(RNN seq2seq)模型在学习四种转换任务：恒等、反转、完全重复和二次复制。这些转换在有限状态转换器下被广泛研究，并具有逐渐增加的复杂性。我们发现，RNN seq2seq模型只能逼近符合训练或分布内数据的映射，而不能学习底层函数。尽管注意力机制使学习更加高效和鲁棒，但它并不能克服分布外的泛化限制。我们建立了一个新的复杂性层次结构来学习这四个任务的无注意力RNN seq2seq模型，它可以用正式语言的复杂性层次结构来解释，而不是字符串转换的复杂性层次结构。RNN的变种也在结果中发挥作用。特别地，我们证明简单的RNN seq2seq模型无法计算输入长度。

    The paper studies the capabilities of Recurrent-Neural-Network sequence to sequence (RNN seq2seq) models in learning four transduction tasks: identity, reversal, total reduplication, and quadratic copying. These transductions are traditionally well studied under finite state transducers and attributed with increasing complexity. We find that RNN seq2seq models are only able to approximate a mapping that fits the training or in-distribution data, instead of learning the underlying functions. Although attention makes learning more efficient and robust, it does not overcome the out-of-distribution generalization limitation. We establish a novel complexity hierarchy for learning the four tasks for attention-less RNN seq2seq models, which may be understood in terms of the complexity hierarchy of formal languages, instead of string transductions. RNN variants also play a role in the results. In particular, we show that Simple RNN seq2seq models cannot count the input length.
    
[^59]: SpikeGPT：带有脉冲神经网络的生成预训练语言模型

    SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13939](http://arxiv.org/abs/2302.13939)

    本论文提出了一种称之为SpikeGPT的生成语言模型，使用二进制、事件驱动脉冲激活单元进行训练，克服了SNN训练中的挑战性。该模型可以用于大规模语言生成任务。

    

    随着大型语言模型的规模不断扩大，所需的计算资源也随之增加。脉冲神经网络（SNN）已成为一种能够利用稀疏和事件驱动激活减少模型推理计算开销的节能深度学习方法。虽然它们在许多计算机视觉任务上已经具有竞争力，但SNN的训练也被证明更具挑战性。因此，它们的性能落后于现代深度学习，我们尚未看到SNN在语言生成方面的有效性。在本文中，我们受到Receptance Weighted Key Value（RWKV）语言模型的启发，成功实现了“SpikeGPT”，它是一种具有二进制、事件驱动脉冲激活单元的生成语言模型。我们在两种模型变体上训练了所提出的模型：45M和216M参数。据我们所知，SpikeGPT是迄今最大的反向传播训练SNN模型，使其适用于非脉冲模型通常解决的大规模语言生成任务。

    As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suita
    
[^60]: 知识图谱上复杂查询答案的序列查询编码

    Sequential Query Encoding For Complex Query Answering on Knowledge Graphs. (arXiv:2302.13114v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13114](http://arxiv.org/abs/2302.13114)

    本文提出一种名为SQE的序列查询编码方法，将复杂查询答案编码为一个序列，从而实现快速和强大的知识图谱推理。

    

    复杂查询答案是知识图谱推理的重要和基础任务。查询编码被提出作为复杂查询答案的快速而强大的解决方案。在编码过程中，大多数现有的QE方法首先将逻辑查询解析为可执行的计算有向无环图(DAG)，然后使用神经网络对操作符进行参数化，最后递归执行这些神经网络化的操作符。然而，参数化和执行范式可能会潜在地过于复杂，它可以通过单一的神经网络编码器进行结构简化。与此同时，像LSTM和Transformer这样的序列编码器被证明对于编码相关任务的语义图非常有效。受此启发，我们提出了序列查询编码(SQE)作为编码CQA查询的替代方案。SQE首先使用基于搜索的算法将计算图线性化为一系列标记，然后使用序列编码器将这些标记编码为向量。

    Complex Query Answering (CQA) is an important and fundamental task for knowledge graph (KG) reasoning. Query encoding (QE) is proposed as a fast and robust solution to CQA. In the encoding process, most existing QE methods first parse the logical query into an executable computational direct-acyclic graph (DAG), then use neural networks to parameterize the operators, and finally, recursively execute these neuralized operators. However, the parameterization-and-execution paradigm may be potentially over-complicated, as it can be structurally simplified by a single neural network encoder. Meanwhile, sequence encoders, like LSTM and Transformer, proved to be effective for encoding semantic graphs in related tasks. Motivated by this, we propose sequential query encoding (SQE) as an alternative to encode queries for CQA. Instead of parameterizing and executing the computational graph, SQE first uses a search-based algorithm to linearize the computational graph to a sequence of tokens and th
    
[^61]: 基于预训练和微调语言模型在对话中提取话语结构

    Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues. (arXiv:2302.05895v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.05895](http://arxiv.org/abs/2302.05895)

    该论文探讨了如何利用预训练语言模型和微调任务在对话中构建话语结构，提出了无监督和半监督方法，并在STAC语料库上取得了令人鼓舞的结果。

    

    话语处理因数据稀缺而受到困扰，特别是在对话中。因此，我们探索了基于预训练语言模型（PLMs）的注意力矩阵来构建对话的话语结构的方法。我们研究了多种微调任务，并表明专门针对对话的句子排序任务表现最佳。为了定位和利用PLMs中的话语信息，我们提出了一种无监督和半监督方法。我们的提议在STAC语料库上取得了令人鼓舞的结果，无监督方法和半监督方法的F1分别为57.2和59.3。当限制在投影树上时，我们的得分提高到了63.3和68.1。

    Discourse processing suffers from data sparsity, especially for dialogues. As a result, we explore approaches to build discourse structures for dialogues, based on attention matrices from Pre-trained Language Models (PLMs). We investigate multiple tasks for fine-tuning and show that the dialogue-tailored Sentence Ordering task performs best. To locate and exploit discourse information in PLMs, we propose an unsupervised and a semi-supervised method. Our proposals achieve encouraging results on the STAC corpus, with F1 scores of 57.2 and 59.3 for unsupervised and semi-supervised methods, respectively. When restricted to projective trees, our scores improved to 63.3 and 68.1.
    
[^62]: 感知和预测：基于自监督语音表示的语音增强损失函数

    Perceive and predict: self-supervised speech representation based loss functions for speech enhancement. (arXiv:2301.04388v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2301.04388](http://arxiv.org/abs/2301.04388)

    本文证明了干净和带噪声语音的特征编码之间的距离对语音质量和可懂度的评估有显著作用，使用此距离作为损失函数在语音增强方面效果优于传统方法。

    

    近期有关语音增强的研究探讨了利用自监督语音表示来辅助神经语音增强模型的训练。然而，很多相关研究侧重于使用自监督语音表示模型的最深或最终输出，而不是较早的特征编码。这种方式使用自监督表示经常缺乏充分的动机。本文证明了干净和带噪声语音的特征编码之间的距离与心理声学衡量标准以及人类平均意见得分显著相关。实验使用此距离作为损失函数，并使用感知语音质量评估(PESQ)和短时语音客观质量评估(STOI)等客观度量证明了其优于基于STFT频谱距离和其他常见语音增强文献中的损失函数的性能。

    Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and shor
    
[^63]: 我有足够的知识回答吗？探究知识库问答的可回答性。

    Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions. (arXiv:2212.10189v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10189](http://arxiv.org/abs/2212.10189)

    本文探究了在知识库问答中的可回答性问题，使用新的GrailQAbility基准KBQA数据集，发现现有的KBQA模型在处理无法回答问题时性能下降，并对无法回答的检测存在问题，需要进一步研究来使KBQA系统对无法回答具有鲁棒性。

    

    在对知识库进行自然语言问答时，缺失的事实、不完整的模式和有限的范围自然地导致许多问题无法回答。虽然在其他问答环境中已经探讨了可回答性，但对于知识库问答（KBQA）尚未进行研究。我们首先识别了各种形式的知识库不完整性，使得问题无法回答，并通过有系统地调整GrailQA（一个仅包含可回答问题的流行KBQA数据集）来创建具有无法回答问题的新的GrailQAbility基准KBQA数据集。在三个最先进的KBQA模型的实验中，我们发现即使在适当调整无法回答的问题后，所有三个模型的性能也会下降。此外，这些模型常常因错误的原因检测出无法回答，并发现特定形式的无法回答尤其难以处理。这强调了进一步研究使KBQA系统对无法回答具有鲁棒性的必要性。

    When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first identifying various forms of KB incompleteness that make questions unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset with only answerable questions). Experimenting with three state-of-the-art KBQA models, we find that all three models suffer a drop in performance even after suitable adaptation for unanswerable questions. In addition, these often detect unanswerability for wrong reasons and find specific forms of unanswerability particularly difficult to handle. This underscores the need for further research in making KBQA systems robust to unanswerability
    
[^64]: 多样的演示提高了上下文组合泛化能力

    Diverse Demonstrations Improve In-context Compositional Generalization. (arXiv:2212.06800v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.06800](http://arxiv.org/abs/2212.06800)

    本文提出了一种选择多样化演示来鼓励模型在组合泛化的测试集上表现良好的方法，并在三个组合泛化语义解析数据集中取得了显著的提高。

    

    在 i.i.d 语义解析拆分中，上下文学习表现出了很大的成功，其中训练集和测试集是从同一分布中抽取的。在这个设置中，模型通常会使用与输入话语类似的演示来提示。然而，在组合泛化的设置中，模型在输出具有训练集中不存在的结构的测试集上进行测试时，选择类似的演示是不够的，因为通常没有任何示例足够接近于输入。本文提出了一种选择多样的演示的方法，旨在共同涵盖输出程序中所需的所有结构，以鼓励模型从这些演示中推广到新的结构。我们在三个组合泛化语义解析数据集中进行了实证研究，结果表明，在纯上下文学习设置中，结合多样的演示可以显著提高模型的性能，同时还可以与fi进行组合。

    In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input. In this work, we propose a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations. We empirically show that combining diverse demonstrations with in-context learning substantially improves performance across three compositional generalization semantic parsing datasets in the pure in-context learning setup and when combined with fi
    
[^65]: PaLM的翻译提示：评估策略和性能

    Prompting PaLM for Translation: Assessing Strategies and Performance. (arXiv:2211.09102v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09102](http://arxiv.org/abs/2211.09102)

    本文通过对PaLM的研究评估了少量样例提示翻译的各种策略，并发现样例的质量是最重要的因素。此外，研究表明PaLM的性能虽然令人印象深刻，但仍然落后于最先进的监督系统。

    

    在多语言但非并行文本上训练的大型语言模型(LLMs)展示了出色的翻译能力。我们对Pathways语言模型(PaLM)进行了深入研究，这是迄今为止经过类似训练的LLMs中表现最强的机器翻译(MT)模型之一。我们研究了选择少量样例进行提示的各种策略，得出结论是样例的质量是最重要的因素。使用优化后的提示，我们重新评估了PaLM在最新的测试集、现代MT度量和人工评价方面的MT能力，并发现它的表现虽然令人印象深刻，但仍然落后于最先进的监督系统。最后，我们提供了一份对PaLM的MT输出进行分析的报告，这揭示了一些有趣的特性和未来工作的前景。

    Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM's MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM's MT output which reveals some interesting properties and prospects for future work.
    
[^66]: 探究神经语言模型对估计概率词语的理解

    Probing neural language models for understanding of words of estimative probability. (arXiv:2211.03358v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.03358](http://arxiv.org/abs/2211.03358)

    本研究探究了神经语言处理模型对于估计概率词语的理解能力，使用UNLI数据集和构建WEP数据集进行实验，发现语言模型在预测WEP的存在时很有效，但没有完全捕捉到与每个词相关联的共识概率水平。

    

    估计概率词语(WEP)是陈述的可信度表达（例如，可能，或许，很有可能，怀疑，不可能等）。多项调查表明，人类评估者在为WEP分配数值概率水平时存在一致性。例如， Fagen-Ulmschneider(2015)的调查中，“很有可能”对应着中位数0.90+-0.08的几率。本研究测量了神经语言处理模型捕捉与每个WEP相关联的共识概率水平的能力。首先，我们使用UNLI数据集(陈等人，2020)，将前提和假设与其感知的联合概率p相联系，构建提示，例如“[前提]。[WEP]，[假设]。”并评估语言模型是否可以预测WEP共识概率水平是否接近于p。其次，我们构建了一个基于WEP的概率推理数据集，测试语言模型是否能够使用WEP组合进行推理。在提示“[事件A]很有可能。[事件B]不可能。[结果]会发生的概率是多少？ "时，人们可以根据WEP词汇的可信度估算概率水平。我们对两个现有的神经语言处理模型进行了实验，并观察到虽然它们在预测WEP的存在时很有效，但它们没有完全捕捉到与每个词相关联的共识概率水平。

    Words of estimative probability (WEP) are expressions of a statement's plausibility (probably, maybe, likely, doubt, likely, unlikely, impossible...). Multiple surveys demonstrate the agreement of human evaluators when assigning numerical probability levels to WEP. For example, highly likely corresponds to a median chance of 0.90+-0.08 in Fagen-Ulmschneider (2015)'s survey. In this work, we measure the ability of neural language processing models to capture the consensual probability level associated to each WEP. Firstly, we use the UNLI dataset (Chen et al., 2020) which associates premises and hypotheses with their perceived joint probability p, to construct prompts, e.g. "[PREMISE]. [WEP], [HYPOTHESIS]." and assess whether language models can predict whether the WEP consensual probability level is close to p. Secondly, we construct a dataset of WEP-based probabilistic reasoning, to test whether language models can reason with WEP compositions. When prompted "[EVENTA] is likely. [EVEN
    
[^67]: KSAT：知识注入的自我关注变压器——整合多个领域特定的上下文

    KSAT: Knowledge-infused Self Attention Transformer -- Integrating Multiple Domain-Specific Contexts. (arXiv:2210.04307v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.04307](http://arxiv.org/abs/2210.04307)

    KSAT使用外部知识源引入知识引导偏见来整合多个领域特定上下文的自我关注结构。

    

    领域特定的语言理解需要整合多个相关的上下文信息。例如，“我拿着一把枪，对我的生活感到很糟糕，如果明天我不醒来，这可能不是最糟糕的事情”，其中包含自杀和抑郁症相关的行为（多个上下文）。自注意力结构中的领域特定性通过在相关领域特定资源的摘录上进行微调（数据集和外部知识-与自杀和抑郁症相关的心理健康诊断的医学教科书章节）来处理。我们提出了一种修改后的自我关注结构KSAT，通过使用外部知识源实现了多个领域特定上下文的整合。KSAT在每个知识源的专门自我关注层中引入知识引导偏见来完成这一点。此外，KSAT提供了控制学习和知识利用之间权衡的机制。

    Domain-specific language understanding requires integrating multiple pieces of relevant contextual information. For example, we see both suicide and depression-related behavior (multiple contexts) in the text ``I have a gun and feel pretty bad about my life, and it wouldn't be the worst thing if I didn't wake up tomorrow''. Domain specificity in self-attention architectures is handled by fine-tuning on excerpts from relevant domain specific resources (datasets and external knowledge - medical textbook chapters on mental health diagnosis related to suicide and depression). We propose a modified self-attention architecture Knowledge-infused Self Attention Transformer (KSAT) that achieves the integration of multiple domain-specific contexts through the use of external knowledge sources. KSAT introduces knowledge-guided biases in dedicated self-attention layers for each knowledge source to accomplish this. In addition, KSAT provides mechanics for controlling the trade-off between learning 
    
[^68]: PROD：渐进式蒸馏用于密集检索

    PROD: Progressive Distillation for Dense Retrieval. (arXiv:2209.13335v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2209.13335](http://arxiv.org/abs/2209.13335)

    本文提出了一种渐进式蒸馏方法PROD，用于密集检索，通过逐步改进学生模型来填补教师和学生之间的差距，并在五个基准数据集上取得了最先进的性能。

    

    知识蒸馏是将强教师的知识传递给高效学生模型的有效方法。然而，通常情况下预期的更好的教师会导致经过蒸馏后学生更糟。为了填补这一差距，本文提出了一种用于密集检索的PROgressive Distillation (PROD)方法，包括教师渐进式蒸馏和数据渐进式蒸馏两个阶段，从而逐步提高学生的检索绩效。在五个被广泛使用的基准数据集（MS MARCO Passage、TREC Passage 19、TREC Document 19、MS MARCO Document和自然问题）上进行了大量实验验证，PROD在密集检索的蒸馏方法中表现出最先进的性能。代码和模型将会发布。

    Knowledge distillation is an effective way to transfer knowledge from a strong teacher to an efficient student model. Ideally, we expect the better the teacher is, the better the student. However, this expectation does not always come true. It is common that a better teacher model results in a bad student via distillation due to the nonnegligible gap between teacher and student. To bridge the gap, we propose PROD, a PROgressive Distillation method, for dense retrieval. PROD consists of a teacher progressive distillation and a data progressive distillation to gradually improve the student. We conduct extensive experiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19, TREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves the state-of-the-art within the distillation methods for dense retrieval. The code and models will be released.
    
[^69]: 基于Transformer的大语料库语义相似度分析的认知研究

    A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.11716](http://arxiv.org/abs/2207.11716)

    本文通过使用Transformer在U.S Patent Phrase to Phrase Matching Dataset上进行语义相似度分析，提高了算法效率，达到了令人满意的结果。

    

    语义相似度分析和建模是当今自然语言处理许多先驱应用中基本认可的任务。由于顺序模式识别的感知，许多神经网络（如RNN和LSTM）在语义相似度建模方面取得了令人满意的结果。但是，由于它们无法以非顺序方式处理信息，因此这些解决方案被认为效率低下，从而导致上下文提取不当。Transformer因其非顺序数据处理和自我关注等优势而成为最先进的架构。本文使用传统和基于transformer的技术对美国专利短语进行语义相似度分析和建模。我们对四种不同版本的解码增强BERT-DeBERTa进行实验，并通过K折交叉验证来提高其性能。实验结果证明了我们的方法的有效性。

    Semantic similarity analysis and modeling is a fundamentally acclaimed task in many pioneering applications of natural language processing today. Owing to the sensation of sequential pattern recognition, many neural networks like RNNs and LSTMs have achieved satisfactory results in semantic similarity modeling. However, these solutions are considered inefficient due to their inability to process information in a non-sequential manner, thus leading to the improper extraction of context. Transformers function as the state-of-the-art architecture due to their advantages like non-sequential data processing and self-attention. In this paper, we perform semantic similarity analysis and modeling on the U.S Patent Phrase to Phrase Matching Dataset using both traditional and transformer-based techniques. We experiment upon four different variants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by performing K-Fold Cross-Validation. The experimental results demonstrate our me
    
[^70]: DiSCoMaT：材料科学文章中基于远程监督的表格组成提取

    DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.01079](http://arxiv.org/abs/2207.01079)

    本文提出了一个新型挑战任务，即通过远程监督方式从科学文章中的表格中提取有关材料组成的信息。为此，研究者创建了一个包含4408个远程监督表格和1475个手动注释的开发和测试表格的训练数据集，并提出了一个强基线——DiSCoMaT。

    

    从科学领域文章中的表格中提取有关材料组成的信息是知识库策划的重要组成部分。然而，现有的表格提取器假定您已经了解表格结构和格式，而科学表格中可能没有这些先前的知识。本文研究了一种特定且具有挑战性的表格提取问题：提取材料（例如玻璃，合金）的组成。我们首先观察到材料科学研究人员使用各种表格样式组织类似的组成，这需要一个智能模型来理解表格和提取组成。因此，我们将其定义为机器学习领域的新型挑战，并创建了一个由4408个远程监督表格和1475个手动注释的开发和测试表格组成的训练数据集。我们还提出了DiSCoMaT，它是一个针对该问题的强基线。

    A crucial component in the curation of KB for a scientific domain is information extraction from tables in the domain's published articles -- tables carry important information (often numeric), which must be adequately extracted for a comprehensive machine understanding of an article. Existing table extractors assume prior knowledge of table structure and format, which may not be known in scientific tables. We study a specific and challenging table extraction problem: extracting compositions of materials (e.g., glasses, alloys). We first observe that materials science researchers organize similar compositions in a wide variety of table styles, necessitating an intelligent model for table understanding and composition extraction. Consequently, we define this novel task as a challenge for the ML community and create a training dataset comprising 4,408 distantly supervised tables, along with 1,475 manually annotated dev and test tables. We also present DiSCoMaT, a strong baseline geared t
    
[^71]: 语言模型作为知识嵌入

    Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.12617](http://arxiv.org/abs/2206.12617)

    该论文提出了一种使用语言模型来推导知识嵌入的方法LMKE，它旨在提高对丰富的长尾实体的表示能力并解决基于描述的先前方法的问题，实验结果表明该方法在多个基准数据集上实现了最先进的性能。

    

    知识嵌入是通过将实体和关系嵌入到连续向量空间中来表示知识图谱的一种方法。现有的方法主要是基于结构或基于描述。基于结构的方法学习表示，以保留知识图谱的内在结构。它们不能很好地表示现实世界知识图谱中有限结构信息下丰富的长尾实体。基于描述的方法利用文本信息和语言模型。在这个方向上的先前方法几乎无法超越基于结构的方法，并且存在昂贵的负采样和限制性描述需求等问题。在本文中，我们提出了LMKE，采用语言模型来推导知识嵌入，旨在丰富长尾实体的表示并解决基于描述的先前方法的问题。我们用对比学习框架来表述基于描述的知识嵌入学习，以提高训练和评价的效率。实验结果表明，LMKE在多个基准数据集上实现了最先进的性能，超越了基于结构和基于先前描述的方法。

    Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental resul
    
[^72]: 统一实例和知识对齐预训练用于基于方面的情感分析

    Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis. (arXiv:2110.13398v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.13398](http://arxiv.org/abs/2110.13398)

    本论文提出了一种统一实例和知识对齐预训练框架，能够有效解决预训练和下游ABSA数据集之间的领域偏移问题，提高了基于方面的情感分析的性能，达到了最先进水平。

    

    基于方面的情感分析（ABSA）旨在确定对某个方面的情感倾向。由于昂贵且有限的标记数据，预训练策略已成为ABSA的事实标准。然而，预训练和下游ABSA数据集之间总是存在严重的领域偏移，直接微调时的知识转移效果不佳，导致下游任务表现亚优化。为了缓解这种领域偏移，我们引入了一个统一的对齐预训练框架，包括实例和知识层面的对齐，将其融入到预训练和微调流程中。具体而言，我们首先设计了一种新颖的分阶段检索采样方法，从大规模的预训练数据集中选择与目标领域相关的实例，从而实现预训练和目标领域实例的对齐（第一阶段）。然后，我们引入了基于知识指导的策略，进一步桥接知识层面的领域差异。我们在基准ABSA数据集上评估了我们的方法，并展示了其超越强基线的卓越性能。我们的方法在多个ABSA数据集上实现了最先进的结果，包括SemEval 2014任务4、SemEval 2015任务12和SemEval 2016任务5。

    Aspect-based Sentiment Analysis (ABSA) aims to determine the sentiment polarity towards an aspect. Because of the expensive and limited labelled data, the pretraining strategy has become the de-facto standard for ABSA. However, there always exists severe domain shift between the pretraining and downstream ABSA datasets, hindering the effective knowledge transfer when directly finetuning and making the downstream task performs sub-optimal. To mitigate such domain shift, we introduce a unified alignment pretraining framework into the vanilla pretrain-finetune pipeline with both instance- and knowledge-level alignments. Specifically, we first devise a novel coarse-to-fine retrieval sampling approach to select target domain-related instances from the large-scale pretraining dataset, thus aligning the instances between pretraining and target domains (First Stage). Then, we introduce a knowledge guidance-based strategy to further bridge the domain gap at the knowledge level. In practice, we 
    
[^73]: 强健的视觉问答需要合成对抗样本来训练

    Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2110.01013](http://arxiv.org/abs/2110.01013)

    本文提出了一种新的模型无关的对抗样本合成和训练（CSST）策略，可以有效解决当前视觉问答模型的语言偏差问题，显著改善模型的性能并具备理想的可视化解释和问题敏感性。

    

    当前的视觉问答模型往往只捕捉训练数据集中的表面语言相关性，并且不能很好地推广到具有不同问答分布的测试集中。为了减少这些语言偏差，最近的视觉问答研究引入了一个辅助的仅问题模型来规范有针对性的VQA模型的训练，并在诊断基准测试中取得了主导地位。但是，由于复杂的模型设计，这些基于集成的方法无法具备理想的VQA模型的两个不可或缺的特征：1）可视化解释：模型在做决策时应依赖于正确的视觉区域。2）问题敏感：模型对问题中的语言变化应具有敏感性。为此，我们提出了一种新的模型无关的对抗样本合成和训练（CSST）策略。经过CSST训练后，VQA模型被迫关注所有关键对象和单词，从而显著改善了视觉问答的性能。

    Today's VQA models still tend to capture superficial linguistic correlations in the training set and fail to generalize to the test set with different QA distributions. To reduce these language biases, recent VQA works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on diagnostic benchmarks for out-of-distribution testing. However, due to complex model design, these ensemble-based methods are unable to equip themselves with two indispensable characteristics of an ideal VQA model: 1) Visual-explainable: The model should rely on the right visual regions when making decisions. 2) Question-sensitive: The model should be sensitive to the linguistic variations in questions. To this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing and Training (CSST) strategy. After training with CSST, VQA models are forced to focus on all critical objects and words, which significantly improves both visu
    
[^74]: 基于说话人感知的CRF用于对话行为分类

    Speaker-change Aware CRF for Dialogue Act Classification. (arXiv:2004.02913v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2004.02913](http://arxiv.org/abs/2004.02913)

    本文提出了一种新的CRF模型，称为Speaker-change Aware CRF，以考虑对话行为分类中的说话人变化，实验结果表明其优于先前的方法。

    

    最近在对话行为（DA）分类方面的研究将该任务作为一个序列标注问题，使用神经网络模型以及作为最后一层的条件随机场（CRF）。CRF模型根据输入话语序列来建模目标DA标签序列的条件概率。然而，该任务还涉及到另一个重要的输入序列，即说话人序列，而先前的工作在这方面没有做出考虑。为了解决这个限制，本文提出了一种简单的CRF层的修改，该层考虑到说话人变化。对SwDA语料库的实验表明，我们修改后的CRF层优于原始层，在某些DA标签上的差距非常大。此外，可视化展示了我们的CRF层可以以端到端的方式学习在说话人交替条件下DA标签对之间的有意义的、复杂的转移模式。代码已公开发布。

    Recent work in Dialogue Act (DA) classification approaches the task as a sequence labeling problem, using neural network models coupled with a Conditional Random Field (CRF) as the last layer. CRF models the conditional probability of the target DA label sequence given the input utterance sequence. However, the task involves another important input sequence, that of speakers, which is ignored by previous work. To address this limitation, this paper proposes a simple modification of the CRF layer that takes speaker-change into account. Experiments on the SwDA corpus show that our modified CRF layer outperforms the original one, with very wide margins for some DA labels. Further, visualizations demonstrate that our CRF layer can learn meaningful, sophisticated transition patterns between DA label pairs conditioned on speaker-change in an end-to-end way. Code is publicly available.
    

